CN115529835A - Neural blending for novel view synthesis - Google Patents
Neural blending for novel view synthesis Download PDFInfo
- Publication number
- CN115529835A CN115529835A CN202180004259.1A CN202180004259A CN115529835A CN 115529835 A CN115529835 A CN 115529835A CN 202180004259 A CN202180004259 A CN 202180004259A CN 115529835 A CN115529835 A CN 115529835A
- Authority
- CN
- China
- Prior art keywords
- images
- image
- depth
- view
- input
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000002156 mixing Methods 0.000 title claims abstract description 120
- 230000001537 neural effect Effects 0.000 title description 14
- 230000015572 biosynthetic process Effects 0.000 title description 4
- 238000003786 synthesis reaction Methods 0.000 title description 4
- 238000000034 method Methods 0.000 claims abstract description 116
- 238000013528 artificial neural network Methods 0.000 claims abstract description 84
- 239000002131 composite material Substances 0.000 claims abstract description 70
- 239000003086 colorant Substances 0.000 claims abstract description 35
- 230000004044 response Effects 0.000 claims abstract description 12
- 230000015654 memory Effects 0.000 claims description 37
- 238000012545 processing Methods 0.000 claims description 34
- 230000006870 function Effects 0.000 claims description 22
- 230000004927 fusion Effects 0.000 claims description 16
- 238000007499 fusion processing Methods 0.000 claims description 7
- 230000008569 process Effects 0.000 description 34
- 238000004891 communication Methods 0.000 description 18
- 238000009877 rendering Methods 0.000 description 17
- 238000010586 diagram Methods 0.000 description 16
- 238000004422 calculation algorithm Methods 0.000 description 13
- 238000004590 computer program Methods 0.000 description 9
- 238000012549 training Methods 0.000 description 8
- 230000009471 action Effects 0.000 description 7
- 238000005516 engineering process Methods 0.000 description 7
- 230000008901 benefit Effects 0.000 description 5
- 230000003993 interaction Effects 0.000 description 5
- 230000033001 locomotion Effects 0.000 description 5
- 239000000203 mixture Substances 0.000 description 5
- 238000013527 convolutional neural network Methods 0.000 description 4
- 230000000694 effects Effects 0.000 description 4
- 230000003287 optical effect Effects 0.000 description 4
- 230000004888 barrier function Effects 0.000 description 3
- 238000004364 calculation method Methods 0.000 description 3
- 239000011521 glass Substances 0.000 description 3
- 210000003128 head Anatomy 0.000 description 3
- 238000005070 sampling Methods 0.000 description 3
- 230000000007 visual effect Effects 0.000 description 3
- 208000035126 Facies Diseases 0.000 description 2
- 230000003190 augmentative effect Effects 0.000 description 2
- 230000005540 biological transmission Effects 0.000 description 2
- 230000001413 cellular effect Effects 0.000 description 2
- 238000012937 correction Methods 0.000 description 2
- 238000010237 hybrid technique Methods 0.000 description 2
- 238000003702 image correction Methods 0.000 description 2
- 239000004973 liquid crystal related substance Substances 0.000 description 2
- 238000013507 mapping Methods 0.000 description 2
- 230000002194 synthesizing effect Effects 0.000 description 2
- 241000238370 Sepia Species 0.000 description 1
- 238000004458 analytical method Methods 0.000 description 1
- 230000020411 cell activation Effects 0.000 description 1
- 239000003795 chemical substances by application Substances 0.000 description 1
- 238000013135 deep learning Methods 0.000 description 1
- 230000001419 dependent effect Effects 0.000 description 1
- 239000010408 film Substances 0.000 description 1
- 238000007667 floating Methods 0.000 description 1
- 230000014509 gene expression Effects 0.000 description 1
- 238000005286 illumination Methods 0.000 description 1
- 238000012886 linear function Methods 0.000 description 1
- 239000000463 material Substances 0.000 description 1
- 230000003278 mimic effect Effects 0.000 description 1
- 230000006855 networking Effects 0.000 description 1
- 230000001902 propagating effect Effects 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 238000004088 simulation Methods 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 125000006850 spacer group Chemical group 0.000 description 1
- 239000013589 supplement Substances 0.000 description 1
- 230000002123 temporal effect Effects 0.000 description 1
- 239000010409 thin film Substances 0.000 description 1
- 230000003313 weakening effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T5/00—Image enhancement or restoration
- G06T5/50—Image enhancement or restoration by the use of more than one image, e.g. averaging, subtraction
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T15/00—3D [Three Dimensional] image rendering
- G06T15/10—Geometric effects
- G06T15/20—Perspective computation
- G06T15/205—Image-based rendering
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T15/00—3D [Three Dimensional] image rendering
- G06T15/10—Geometric effects
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T15/00—3D [Three Dimensional] image rendering
- G06T15/10—Geometric effects
- G06T15/20—Perspective computation
-
- G06T3/18—
-
- G06T5/70—
-
- G06T5/80—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/50—Depth or shape recovery
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/50—Depth or shape recovery
- G06T7/55—Depth or shape recovery from multiple images
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/10—Image acquisition modality
- G06T2207/10016—Video; Image sequence
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/10—Image acquisition modality
- G06T2207/10024—Color image
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/10—Image acquisition modality
- G06T2207/10028—Range image; Depth image; 3D point clouds
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/20—Special algorithmic details
- G06T2207/20016—Hierarchical, coarse-to-fine, multiscale or multiresolution image processing; Pyramid transform
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/20—Special algorithmic details
- G06T2207/20084—Artificial neural networks [ANN]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/20—Special algorithmic details
- G06T2207/20212—Image combination
- G06T2207/20221—Image fusion; Image merging
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/30—Subject of image; Context of image processing
- G06T2207/30196—Human being; Person
- G06T2207/30201—Face
Abstract
Systems and methods are described for receiving a plurality of input images, a plurality of depth images, and a plurality of view parameters to generate a virtual view of a target subject. The system and method may generate a plurality of warped images based on at least one of the plurality of input images, the plurality of view parameters, and the plurality of depth images. In response to providing the plurality of depth images, the plurality of view parameters, and the plurality of warped images to the neural network, the systems and methods may receive, from the neural network, blending weights for assigning colors to pixels of a virtual view of the target subject, and may generate a composite image according to the view parameters based on the blending weights and the virtual view.
Description
Technical Field
The present specification relates generally to methods, devices, and algorithms for use in synthesizing three-dimensional (3D) content.
Background
Conventional object rendering typically involves intensive computational efforts in order to generate realistic imagery. If the object is in motion, additional computational effort may be used to generate a realistic image of the object. Such rendering can include modeling the appearance of the object using a neural network. However, the model may generate images with extraneous noise and geometric artifacts.
Disclosure of Invention
Systems and methods described herein may perform image-based rendering using an input image and predefined view parameters to generate (e.g., synthesize) a novel (e.g., not-seen) view of a video and/or image based on the input image. Image-based rendering of unseen views can utilize a warping process for received input images. In general, warping processes can cause geometric inaccuracies and view and/or image-related effects that can create artifacts when contributions from different input views are mixed together. The systems and methods described herein use deep learning techniques that employ Neural Networks (NNs) to mix image content to obtain an image-based rendering of novel views. Specific blending weights are learned and used to combine the input image contributions into a final composite view. The blending weights are generated to provide the following advantages: a composite image is generated that exhibits reduced view and/or image correlation effects and a reduced number of image artifacts.
A technical challenge that may arise when using NN, warping processes and/or mixing weights is the lack of sufficiently accurate geometry so that the NN (e.g., a convolutional neural network) can select the appropriate mixing weights in order to avoid image artifacts. The systems and methods described herein may address this technical challenge by using a learned mix of color and depth views of the input images and/or employing multi-resolution mixing techniques to select pixel colors that provide accurate images with reduced image artifacts. For example, blending weights may be applied to heavily weighted projected (e.g., probabilistically provided) pixel colors that are likely to be correct and accurate relative to the ground truth image, while weakening the weights of projected pixel colors that are unlikely to be correct and accurate for a given ground truth image.
To employ such hybrid technologies, the systems and methods described herein may utilize one or more witness cameras in addition to specific on-board system cameras (e.g., color cameras, infrared cameras, etc.). The witness camera may supervise the content used to generate the novel view. For example, the witness camera may be a high resolution camera that may function to provide ground truth data. The generated novel view is compared to ground truth data received from (e.g., obtained by) the witness camera. In some implementations, the image details of the novel view can be scored based on image details captured by the witness camera when generating the novel view.
In some implementations, the systems and methods described herein account for training losses. For example, the system can generate training data with various captured scenes to minimize losses in order to provide high quality novel view synthesis while reducing temporal flicker artifacts in the synthesized views. In some implementations, the systems and methods described herein can also employ occlusion inference to correct artifacts in the synthesized novel view.
A system of one or more computers may be configured to perform particular operations or actions by installing software, firmware, hardware, or a combination thereof on the system that, alone or in combination, in operation, causes the system to perform the actions. One or more computer programs may be configured to perform particular operations or actions by including instructions that, when executed by data processing apparatus, cause the apparatus to perform the actions.
In one general aspect, systems and methods are described for receiving a plurality of input images, receiving a plurality of depth images associated with a target subject in at least one of the plurality of input images, receiving a plurality of view parameters for generating a virtual view of the target subject, and generating a plurality of warped images based on at least one of the plurality of input images, the plurality of view parameters, and the plurality of depth images. In response to providing the plurality of depth images, the plurality of view parameters, and the plurality of warped images to the neural network, the systems and methods may receive, from the neural network, a blending weight for assigning a color to a pixel of a virtual view of the target subject. Systems and methods may generate a composite image from view parameters based on the blending weights and the virtual view.
These and other aspects can include one or more of the following, alone or in combination. In some implementations, systems and methods may include reconstructing a consensus surface using a geometric fusion process on a plurality of depth images to generate a geometric fusion model, generating a plurality of reprojected images based on a plurality of input images and the consensus surface, and in response to providing the plurality of depth images, the plurality of view parameters, and the plurality of reprojected images to a neural network, the systems and methods may receive additional blending weights from the neural network for assigning colors to pixels in a composite image.
In some implementations, the systems and methods may further include providing the neural network with depth differences between the geometric fusion model and depths observed in the plurality of depth images, and the method further includes correcting occlusions detected in the composite image based on the depth differences. In some implementations, the plurality of input images are color images captured according to predefined view parameters associated with at least one camera capturing the plurality of input images and/or the plurality of depth images each include a depth map associated with the at least one camera capturing at least one of the plurality of input images, at least one occlusion map, and/or a depth map associated with a ground truth image captured by the at least one witness camera at a time corresponding to the capture of the at least one of the plurality of input images. In some implementations, the blending weight is configured to assign a blended color to each pixel of the composite image.
In some implementations, the neural network is trained based on minimizing an occlusion loss function between a composite image generated by the neural network and a ground truth image captured by the at least one witness camera. In some implementations, the composite image is an uncaptured view of a target subject generated for a three-dimensional video conference.
In some implementations, generating the plurality of warped images based on the plurality of input images, the plurality of view parameters, and at least one of the plurality of depth images includes determining candidate projections of colors associated with the plurality of input images into an uncaptured view using the at least one of the plurality of depth images, wherein the uncaptured view includes at least a portion of an image feature of the at least one of the plurality of input images.
In another general aspect, an image processing system is described, in particular for performing the method of any one of the preceding claims. The image processing system may include at least one processing device and a memory storing instructions that, when executed, cause the system to perform operations comprising: the method includes receiving a plurality of input images captured by an image processing system, receiving a plurality of depth images captured by the image processing system, receiving a plurality of view parameters associated with an uncaptured view that is in turn associated with at least one of the plurality of input images, and generating a plurality of warped images based on the plurality of input images, the plurality of view parameters, and at least one of the plurality of depth images. In response to providing the plurality of depth images, the plurality of view parameters, and the plurality of warped images to the neural network, the system may include receiving, from the neural network, a blending weight for assigning a color to a pixel of the uncaptured view. The system may further include generating a composite image according to the blending weight, wherein the composite image corresponds to the uncaptured view.
These and other aspects can include one or more of the following, alone or in combination. In some implementations, the plurality of input images are color images captured by the image processing system according to predefined view parameters associated with the image processing system and/or the plurality of depth images include a depth map associated with at least one camera capturing at least one of the plurality of input images, at least one occlusion map, and/or a depth map associated with a witness camera of the image processing system.
In some implementations, the blending weight is configured to assign a blended color to each pixel of the composite image. In some implementations, the neural network is trained based on minimizing an occlusion loss function between a composite image generated by the neural network and a ground truth image captured by the at least one witness camera. In some implementations, the composite image is a novel view generated for a three-dimensional video conference.
In another general aspect, a non-transitory machine-readable medium is described having instructions stored thereon that, when executed by a processor, cause a computing device to receive a plurality of input images, receive a plurality of depth images associated with a target subject in at least one of the plurality of input images, and receive a plurality of view parameters for generating a virtual view of the target subject. The machine-readable medium may also be configured to reconstruct a consensus surface using a geometric fusion process on the plurality of depth images to generate a geometric fusion model of the target subject, generate a plurality of reprojected images based on the plurality of input images, the plurality of view parameters, and the consensus surface. In response to providing the plurality of depth images, the plurality of view parameters, and the plurality of re-projected images to the neural network, the machine-readable medium may receive, from the neural network, blending weights for assigning colors to pixels of a virtual view of the target subject, and generate a composite image from the view parameters based on the blending weights and the virtual view.
These and other aspects can include one or more of the following, alone or in combination. In some implementations, the machine-readable medium further includes providing the depth difference between the geometric fusion model and the depth observed in the plurality of depth images to a neural network, and correcting for occlusions detected in the composite image based on the depth difference. In some implementations, the plurality of input images are color images captured according to predefined view parameters associated with at least one camera capturing the plurality of input images and/or the plurality of depth images include a depth map associated with the at least one camera capturing at least one of the plurality of input images, at least one occlusion map, and/or a depth map associated with a ground truth image captured by the at least one witness camera at a time corresponding to the capture of the at least one of the plurality of input images.
In some implementations, the blending weight is configured to assign a blended color to each pixel of the composite image. In some implementations, the neural network is trained based on minimizing an occlusion loss function between a composite image generated by the neural network and a ground truth image captured by the at least one witness camera. In some implementations, the composite image is a novel view for three-dimensional video conferencing. In some implementations, the neural network is further configured to perform multi-resolution blending to assign pixel colors to pixels in the composite image, the multi-resolution blending triggering providing the image pyramid as an input to the neural network to trigger receiving from the neural network multi-resolution blending weights for a plurality of scales and an opacity value associated with each scale.
These and other aspects may include one or more of the following alone or in combination. According to some aspects, the methods, systems, and computer-readable media claimed herein may include one or more (e.g., all) of the following features (or any combination thereof).
Implementations of the described technology may include hardware, methods or processes on a computer-accessible medium, or computer software. The details of one or more implementations are set forth in the accompanying drawings and the description below. Other features will be apparent from the description and drawings, and from the claims.
Drawings
Fig. 1 is a block diagram illustrating an example 3D content system for displaying composite content on a display device according to embodiments described throughout this disclosure.
Fig. 2 is a block diagram of an exemplary system for compositing content for rendering on a display, according to an embodiment described throughout this disclosure.
Fig. 3 is a block diagram illustrating an example of re-projecting an input image to a target camera viewpoint according to an embodiment described throughout the present disclosure.
Fig. 4 is a block diagram of an example flow diagram for generating synthetic content for rendering on a display using neural mixing techniques, according to an embodiment described throughout this disclosure.
Fig. 5 is a block diagram of an example flow diagram for generating hybrid weights according to an embodiment described throughout this disclosure.
Fig. 6 is a flow diagram illustrating one example of a process for generating synthetic content using neural mixing techniques according to an implementation described throughout this disclosure.
FIG. 7 illustrates an example of a computer device and a mobile computer device that may be used with the techniques described herein.
Like reference symbols in the various drawings indicate like elements.
Detailed Description
This document describes examples related to generating novel (e.g., not seen) views of image content. Examples described herein may synthesize (e.g., generate) real-time novel views based on captured video content and/or image content. For example, image-based rendering techniques may be used to synthesize novel views of moving image content (e.g., objects, users, scene content, image frames, etc.) using a learning mix of color views and depth views.
The systems and methods described herein may generate novel color images with fewer artifacts than conventional systems. For example, the systems and methods described herein may correct certain image noise and loss function analysis to generate a novel image with less depth inaccuracy and less occlusion. The correction may be performed by employing a Neural Network (NN) to learn to detect and correct an image region containing a visibility error. In addition, the NN can learn and predict color values for novel views using a blending algorithm that constrains the output values to a linear combination of the re-projected input colors retrieved from the color input images.
In operation, a process may retrieve (e.g., capture, obtain, receive, etc.) multiple input images and data (e.g., target view parameters) to predict a novel view (e.g., a non-visible color image) by combining color image streams of input images (e.g., views) from the same scene (e.g., image content in the scene). The color image streams may be provided to the NN to employ neural rendering techniques to enhance low-quality output from real-time image capture systems (e.g., 3D video conferencing systems such as telepresence systems). For example, the novel views may be predicted color images generated by the systems and techniques described herein. The predicted image may be generated by providing the input images and the combined color image stream (e.g., and/or a re-projection or representation of such input images) to the NN to allow the NN to learn certain mixing weights in order to assign pixel colors to the predicted color image. The learned blending weights can be applied to generate pixel colors for the novel color image. The learned blending weights may also be used to generate other novel views of the image content represented in one or more provided input images.
In some implementations, the NN described herein may model view-related effects to predict future user movement (e.g., motion) in order to mitigate erroneous projections of artifacts caused by the noise properties of the particular geometry information used to generate the user's images and/or the geometry information received from the camera capturing the user and/or the information received from the image processing performed on the user's images.
In some implementations, the systems and methods described herein can train one or more NNs (e.g., convolutional NNs, such as U-nets) to predict images in the viewpoints of individual witness cameras, which can be used, for example, to provide surveillance of output color images. The witness camera may act as a ground truth camera for the image capture and/or processing system described herein. In some implementations, two or more witness cameras may be used as training data for the NN. The two or more witness cameras may represent one or many pairs of witness cameras.
In some implementations, the systems and methods can utilize captured input images, predefined parameters associated with a desired novel output view, and/or occlusion maps that include depth differences and depth maps. The depth disparity may be generated using a view from a color camera between a surface closest to the novel view and a surface of the camera view. The depth disparity may be used for occlusion inference to correct occlusion views and/or other errors in the generated image. In some implementations, the depth map may include a depth map from a view captured by a witness camera.
In some implementations, the systems and methods described herein can reconstruct consensus (consensus) surfaces (e.g., geometric surfaces) by geometric fusion of input depth images. In some implementations, the systems and methods described herein can use depth information, such as separately captured depth images and/or pleasing surfaces, to determine the projection of input colors into novel views.
In some implementations, the systems and methods described herein can generate a color image (e.g., a color image) for the novel view by assigning a mixed color to each pixel in the novel view. The mixed color may be determined using the color input image and the mixing weights determined by the NN as described herein. In some implementations, the blending weights are regularized by a loss function. In some implementations, the novel view is a weighted combination of one or more pixel color values of the image projected from the original input image into the novel view.
As used herein, a novel (e.g., unseen) view may include image content and/or video content that has been interpreted (e.g., synthesized, interpolated, simulated, etc.) based on one or more frames of image content and/or video content captured by a camera. For example, interpretation of camera-captured image content and/or video content may be used in conjunction with the techniques described herein to create unseen versions and views (e.g., gestures, expressions, angles, etc.) of the captured image content and/or video content.
In some implementations, for example, the techniques described herein can be used to synthesize images that are expressively accurate and realistic for display on a screen of a 2D or 3D display used in a multi-way 2D or 3D video (e.g., telephone) conference. The techniques described herein may be used to generate and display accurate and realistic views (e.g., image content, video content) of a user in a video conference. The views include unseen views that are traditionally difficult to depict in 3D without significant image artifacts.
The systems and methods described herein provide the advantage of generating novel views without significant image artifacts by using one or more witness cameras and NNs to learn mixing weights based on multi-view color input images and noise occlusion cues. The learned blending weights can ensure that occlusion and color artifacts are corrected in the resulting output image. In addition, the learned blending weights and one or more witness cameras can be used by the systems described herein to ensure that image content not captured in the input image can be used to accurately predict novel views associated with image content in the input image. For example, because the blending weights are learned and evaluated relative to the witness camera image, accurate predictions can be made for image portions of the scene that were not captured or represented in the original input image.
In some implementations, the techniques described herein may be used for entertainment purposes in movies, videos, shorts, game content, virtual or augmented reality content, or other formats that include images of users that may benefit from the prediction techniques described herein. For example, the techniques described herein may be used to generate novel views for mobile characters rendered in image and/or video content.
In some implementations, the techniques described herein may be used by virtual assistant devices or other intelligent agents that may perform image processing to identify objects, recreate objects, and/or generate composite images from objects using the techniques described herein.
Fig. 1 is a block diagram illustrating an example 3D content system 100 for displaying content in a stereoscopic display device according to embodiments described throughout this disclosure. For example, the 3D content system 100 may be used by multiple users to conduct video conference communication in 3D (e.g., telepresence sessions) and/or to access augmented and/or real content. In general, the system of fig. 1 may be used to capture video and/or images and/or scenes of a user during a 2D or 3D video conference and use the systems and techniques described herein to generate a novel view based on the captured content to facilitate rendering of accurate images depicting the novel view in the video conference session. The system 100 may benefit from the use of the techniques described herein because such techniques may generate and display real-time novel views that accurately represent users, for example, within a video conference. For example, the novel view may be provided in 2D and/or 3D to another user via the system 100 for display.
As shown in fig. 1, a first user 102 and a second user 104 access a 3D content system 100. For example, users 102 and 104 may access 3D content system 100 to participate in a 3D telepresence session. In such an example, the 3D content system 100 can allow each of the users 102 and 104 to see a highly realistic and visually consistent representation of each other, thereby facilitating the users to interact in a manner similar to each other's physical presence.
Each user 102, 104 may use a corresponding 3D system for a 3D telepresence session. Here, user 102 accesses 3D system 106, and user 104 accesses 3D system 108. The 3D systems 106, 108 may provide functionality related to 3D content including, but not limited to, capturing images for 3D display, processing and presenting image information, and processing and presenting audio information. The 3D system 106 and/or the 3D system 108 may constitute a collection of sensing devices integrated into one unit. The 3D system 106 and/or the 3D system 108 may include some or all of the components described with reference to fig. 2 and 8.
The 3D content system 100 may include one or more 2D or 3D displays. Here, a 3D display 110 is depicted for the 3D system 106 and a 3D display 112 is depicted for the 3D system 108. The 3D displays 110, 112 may use any of a variety of types of 3D display technologies to provide stereoscopic views to respective viewers (e.g., the user 102 or the user 104). In some implementations, the 3D displays 110, 112 can be stand-alone units (e.g., self-supporting or hung on a wall). In some implementations, the 3D displays 110, 112 may include or have access to wearable technology (e.g., controllers, AR glasses, etc.). In some implementations, the displays 110, 112 can be 2D displays.
In general, the displays 110, 112 may provide imagery that approximates the 3D optical characteristics of physical objects in the real world without the use of Head Mounted Display (HMD) devices. In general, the displays described herein may include a flat panel display that covers a lenticular lens (e.g., a microlens array) and/or a parallax barrier to redirect images to multiple different viewing areas associated with the display.
In some implementations, the displays 110, 112 may include high resolution and glasses-free lenticular 3D displays. For example, the displays 110, 112 may include a microlens array (not shown) including a plurality of lenses (e.g., microlenses) having glass spacers coupled (e.g., bonded) to the microlenses of the display. The microlenses may be designed such that, from a selected viewing position, a left eye of a user of the display may view a first set of pixels and a right eye of the user may view a second set of pixels (e.g., where the second set of pixels is mutually exclusive from the first set of pixels).
In some exemplary displays, there may be a single location that provides a 3D view of the image content (e.g., user, object, etc.) provided by such a display. The user can sit in a single position to experience proper parallax, minimal distortion, and realistic 3D images. If the user moves to a different physical location (or changes head position or eye gaze position), the image content (e.g., the user, objects worn by the user, and/or other objects) may begin to appear less realistic, 2D, and/or distorted. The systems and techniques described herein may reconfigure image content projected from a display to ensure that a user can move around, yet experience a suitable parallax, low distortion rate, and realistic 3D image in real-time. Thus, the systems and techniques described herein provide the advantage of maintaining and providing 3D image content and objects for display to a user regardless of any user movement that occurs while the user is viewing the 3D display.
As shown in fig. 1, the 3D content system 100 may be connected to one or more networks. Here, a network 114 is connected to the 3D system 106 and the 3D system 108. The network 114 may be a publicly available network (e.g., the internet) or a private network, as just two examples. The network 114 may be wired, wireless, or a combination of both. The network 114 may include or utilize one or more other devices or systems, including but not limited to one or more servers (not shown).
The 3D systems 106, 108 may include a number of components related to the capture, processing, transmission, or reception of 3D information and/or the presentation of 3D content. The 3D systems 106, 108 may include one or more cameras for capturing image content and/or video (e.g., visible and IR image data) of images to be included in the 3D presentation. In the depicted example, the 3D system 106 includes cameras 116 and 118. For example, the cameras 116 and/or 118 may be disposed substantially within a housing of the 3D system 106 such that an objective lens or lens of the respective camera 116 and/or 118 captures image content through one or more openings in the housing. In some implementations, the cameras 116 and/or 118 may be separate from the housing, such as in the form of a standalone device (e.g., with a wired and/or wireless connection to the 3D system 106). The cameras 116 and 118 may be positioned and/or oriented to capture a sufficiently representative view of a user (e.g., the user 102).
Although the cameras 116 and 118 will generally not obscure the view of the 3D display 110 of the user 102, the placement of the cameras 116 and 118 may be arbitrarily selected. For example, one of the cameras 116, 118 may be positioned somewhere above the face of the user 102 and the other somewhere below the face. For example, one of the cameras 116, 118 may be positioned somewhere to the right of the face of the user 102 and the other somewhere to the left of the face. For example, the 3D system 108 may include cameras 120 and 122 in a similar manner. Additional cameras may also be present. For example, a third camera may be placed near or behind the display 110.
In some implementations, the 3D systems 106, 108 can include one or more witness cameras 119, 121. The witness cameras 119, 121 may be used to capture high quality images (e.g., witness camera images 132), which may represent ground truth images. Images captured by witness camera 119 and/or camera 121 may be used with the techniques described herein to be used as a comparison when generating novel views and calculating losses and corrections for such losses. In general, images captured by witness cameras 119, 121 may be captured at substantially the same time as corresponding ones of other images (e.g., frames) captured by cameras 116, 118, 120, 122, 124, and/or 126 and combinations of such cameras and/or camera pods (pods). In some implementations, a witness camera image 134 may be captured and used as training data for one or more NNs to generate novel views.
In some implementations, the 3D systems 106, 108 may include one or more depth sensors to capture depth data to be used in 3D rendering. Such a depth sensor may be considered part of a depth capture component in the 3D content system 100 for characterizing a scene captured by the 3D systems 106 and/or 108 to properly represent the scene on a 3D display. Further, the system may track the position and orientation of the viewer's head so that a 3D presentation can be rendered by an appearance corresponding to the viewer's current perspective. Here, the 3D system 106 includes a depth sensor 124, which may also represent an infrared camera. In a similar manner, the 3D system 108 may include a depth sensor 126. Any of a variety of types of depth sensing or depth capture may be used to generate the depth data.
In some implementations, each camera 116, 118, 119, and 124 can represent multiple cameras in the cabin. For example, depth sensor 124 may be housed in a camera bay with camera 116 and/or camera 118. In some implementations, three or more phase pods may be placed around and/or behind the display 110, and each pod may include a camera 124 (e.g., a depth sensor/camera) and one or more cameras 116, 118. Similarly, three or more phase pods may be placed around and/or behind the display 112, and each pod may include a camera 126 (e.g., a depth sensor/camera) and one or more cameras 120, 122.
In operation of the system 106, secondary stereo depth capture may be performed. For example, a light point may be used to illuminate a scene, and stereo matching may be performed between two respective cameras. Such illumination may be accomplished using waves of a selected wavelength or range of wavelengths. For example, infrared light (IR) may be used. For example, the depth data may include or be based on any information related to the scene that reflects a distance between a depth sensor (e.g., depth sensor 124) and an object in the scene. For content in an image corresponding to an object in a scene, the depth data reflects a distance (or depth) to the object. For example, the spatial relationship between the camera and the depth sensor may be known and may be used to correlate an image from the camera with a signal from the depth sensor to generate depth data for the image.
Images captured by the 3D content system 100 may be processed and later displayed as a 3D presentation. As shown in the example of fig. 1, a 3D image of the user 104 is presented on a 3D display 110. In this way, the user 102 may perceive the 3D image 104' (e.g., of the user) as a 3D representation of the user 104, and the user 104 may be remote from the user 102. Similarly, the 3D image 102' is presented on the 3D display 112. In this way, the user 104 may perceive the 3D image 102' as a 3D representation of the user 102.
The 3D content system 100 may allow participants (e.g., users 102, 104) to engage in audio communications with each other and/or others. In some implementations, the 3D system 106 includes a speaker and a microphone (not shown). For example, the 3D system 108 may similarly include a speaker and a microphone. As such, the 3D content system 100 may allow the users 102 and 104 to participate in 3D telepresence sessions with each other and/or with others. In general, the systems and techniques described herein may operate with system 100 to generate image content and/or video content for display between users of system 100.
In operation of the system 100, a set of input images 132 may be captured by the cameras 116, 118, 119, 124 and/or 120, 121, 122, and 126. For example, the input images may include a witness camera image 134 and an RGB color image 136. In some implementations, the system 100 may also generate and/or otherwise obtain the depth image 138. In one example, the depth image 138 may be generated by performing one or more stereo calculations from a pair of IR images retrieved from an IR camera as described above. The input image 132 may be used as a basis for predicting an output image that is a linear combination of the re-projected colors from the input image. In some implementations, the input image 132 may include two or more color images representing a re-projected color image (e.g., red Green Blue (RGB)) captured at known (e.g., predetermined, predefined) view parameters. In some implementations, the input image 132 also includes one or more depth images 138 that are computed (e.g., generated) at known view parameters. The input images 132 may be used in conjunction with specific camera parameters, view parameters, and/or NN blending algorithm 140 to generate novel views for display on displays 110 and/or 112.
Fig. 2 is a block diagram of an example system for compositing content for rendering on a display, according to implementations described throughout this disclosure. System 200 may be used as or included in one or more implementations described herein, and/or may be used to perform operations for one or more examples of the synthesis, processing, simulation, or presentation of image content described herein. The overall system 200 and/or one or more of its individual components may be implemented according to one or more examples described herein.
System 200 may include one or more 3D systems 202. In the depicted example, 3D systems 202A, 202B, through 202N are shown, where the index N indicates an arbitrary number. The 3D system 202 may provide for the capture of visual and audio information for 2D or 3D rendering, and may forward the 2D or 3D information for processing. Such information may include images of the scene, depth data about the scene, parameters associated with image capture, and/or audio from the scene. The 2D/3D system 202 may be used as or included in the systems 106 and 108 and the 2D/3D displays 110 and 112 (FIG. 1). Although systems 202B and 202N do not depict the same modules as depicted in system 202A, each module in system 202A may also be present in systems 202B and 202N.
The system 200 may include multiple cameras, as shown by camera 204. Any type of light sensing technology may be used to capture the image, such as the type of image sensor used in ordinary digital cameras. The cameras 204 may be of the same type, or of different types. For example, the camera location may be placed anywhere on the 3D system (e.g., system 106). In some implementations, each system 202A, 202B, and 202N includes three or more phase pods, each camera pod including a depth camera (e.g., depth sensor 206 and/or one or more pairs of IR cameras whose content is analyzed using stereo algorithms to infer depth images) and one or more color cameras. In some implementations, the systems 202A, 202B, and 202N also include one or more witness cameras (not shown) that can capture images to be used as ground truth images in generating novel views and/or, for example, for training neural networks.
The system 202A includes a depth sensor 206. In some implementations, the depth sensor 206 operates by propagating an IR signal onto the scene and detecting a response signal. For example, the depth sensor 206 may generate and/or detect the light beams 128A and/or 128B and/or 130A and/or 130B. In some implementations, the depth sensor 206 may be used to calculate the occlusion map. The system 202A also includes at least one microphone 208 and a speaker 210. In some implementations, the microphone 208 and the speaker 210 may be part of the system 106.
Further, the system 202 includes a 3D display 212 that can present 3D images. In some embodiments, the 3D display 212 may be a stand-alone display, while in some other embodiments, the 3D display 212 may be integrated into AR glasses, a head-mounted display device, or the like. In some implementations, the 3D display 212 operates using parallax barrier technology. For example, the parallax barrier may comprise parallel vertical stripes of substantially opaque material (e.g., an opaque film) placed between the screen and the viewer. Due to parallax between the viewer's respective eyes, different portions (e.g., different pixels) of the screen are viewed by the respective left and right eyes. In some implementations, the 3D display 212 operates using a lenticular lens. For example, alternating rows of lenses may be placed in front of the screen, the rows directing light from the screen to the left and right eyes, respectively, of a viewer.
The system 200 can include a computing system 214, the computing system 214 can perform certain tasks of data processing, data modeling, data coordination, and/or data transmission. In some implementations, the computing system 214 can also generate images, blend weights, and perform neural processing tasks. In some implementations, the computing system 214 is an image processing system. The computing system 214 and/or components thereof can include some or all of the components described with reference to fig. 8.
The computing system 214 includes an image processor 216 that may generate 2D information and/or 3D information. For example, the image processor 216 may receive (e.g., obtain) one or more input images 132 and/or view parameters 218 and may generate image content for further processing by an image warping engine 220, a mixing weight generator 222, and/or the NN 224. The input image 132 may include a captured color (e.g., RGB, YUV, CMYK, CIE, RYB) image.
The view parameters 218 may include camera parameters associated with the capture of a particular input image 132 and/or associated with the capture of an image to be generated (e.g., synthesized). In general, the view parameters 218 may represent camera model approximations. The view parameters 218 may include any or all of a view direction, pose, camera view angle, lens distortion, and/or intrinsic and extrinsic parameters of the camera.
The image processor 216 also includes (and/or generates and/or receives) an occlusion map 226, a depth map 228, a UV map 230, target view parameters 232, a loss function 234, and mesh proxy geometry 236.
The occlusion map 226 may encode the signed distance between the surface point determined to be closest to the target viewpoint and the camera capturing the surface. Positive values may indicate that a point is occluded by the view. Accordingly, the system 200 may configure the blending weight generator 222 (and the NN 224) to not use positive distance in determining the blending weights 242 because such occluded image content will not provide accurate rendered data content when generating new or novel views based on captured images. In some implementations, the occlusion map 226 can be used to evaluate the depth difference between the depth observed in a particular view and the geometric fusion model associated with that view.
The UV map 230 is generated from the visible content in the input image 132. In particular, UV map 230 represents the projection of a 2D image onto a 3D model surface in order to perform texture mapping to generate features that may be used to generate a composite image (e.g., a novel view).
The target view parameter 232 represents a view parameter of the novel synthesized image (i.e., a view parameter for generating a virtual view of a target subject). The target view parameters 232 may include image parameters and/or camera parameters associated with the image to be generated (e.g., synthesized). The target view parameters 232 may include view direction, pose, camera view angle, and the like.
The loss function 234 may evaluate a difference between the ground truth image and a predicted image, where the predicted image is predicted based on a combination of visible light information captured for the frames, IR light captured for the frames, and blending weights associated with color and/or depth. The loss functions 234 may include functions that describe any or all image errors, image holes, image error projection artifacts, and the like.
In some implementations, the loss function 234 can include a reconstruction loss based on a reconstruction difference between the activated segmented ground truth image mapped to the layers in the NN and the activated segmented predicted image mapped to the layers in the NN. The segmented ground truth image may be segmented by a ground truth mask to remove background pixels and the segmented predicted image may be segmented by a prediction mask to remove background pixels. The prediction mask may be predicted based on a combination of both visible light information captured for the frame and IR light captured for the frame.
The mesh proxy geometry 236 may represent a set of K proxies P i,1 ,…,P i,K The rough geometry of (i.e., a grid of rectangles, triangles, etc. with UV coordinates). For example, the 2D image may be projected onto a 3D proxy model surface to generate mesh proxy geometry 236. The proxy may be used as a version of the actual geometry used to represent the particular image content. In operation, the system 200 uses the proxy geometry principle to encode geometry using a set of coarse proxy surfaces (e.g., mesh proxy geometry 236) and shape, albedo, and view-related effects.
The image warping engine 220 may be configured to receive one or more input images (e.g., frames, streams) and/or other capture/feature parameter data and generate one or more output images (e.g., frames, streams) that preserve features. The image warping engine 220 may utilize the capture/feature parameter data to reconstruct the input image in some manner. For example, the image warping engine 220 may generate a reconstructed candidate color image from the input image, where each pixel in the reconstructed image is a candidate pixel for a new composite image corresponding to one or more input images.
In some implementations, the image warping engine 220 may perform functions on the input image at the pixel level to preserve small-scale image features. In some implementations, the image warping engine 220 may use a non-linear or linear function to generate the reconstructed image.
The blending weight generator 222 includes a blending algorithm 238 and a visibility score 240. The blending algorithm 238 may be used to generate blending weights 242. The mixing algorithm 238 may be accessed, in particular, via the NN224 to generate the mixing weights 242. Blending weights 242 represent values for particular pixels of an image that can be used to contribute to aspects of the pixels in the result (e.g., the final, novel image). Blending algorithm 238 comprises a heuristic-based algorithm for calculating blending weights for rendering a particular set of depth images and/or fusion geometries representing depth images. The blending algorithm receives as input the multi-view color image and the noise-occlusion cues to learn output blending weights for a novel view (e.g., a novel composite image). In some implementations, the texture (e.g., received from the camera pod) and visibility score 240 for the target view and the input image may also be provided as inputs to the blending algorithm 238.
The visibility score 240 may represent the visibility of a particular pixel or feature of an object in a captured image. Each visibility score 240 may represent a single scalar value that indicates which portions (e.g., pixels, features, etc.) of an image are visible in a particular view of an input image. For example, if the leftmost side of the user's face is not visible in the user's input image, the visibility score 240 representing the leftmost pixel of the user's face may be weighted low, while other areas that may be well viewed and/or captured in the input image may be weighted high. The visibility score may be considered when generating blending weights 242 for novel views (e.g., images).
The neural network 224 includes an embedder network 244 and a generator network 246. The embedder network 244 includes one or more convolutional layers and downsample layers. The generator network 246 includes one or more convolutional layers and upsampling layers.
The inpainter (in-painter) 254 may generate content (e.g., pixels, regions, etc.) that may be missing from a particular texture or image based on a local neighborhood of pixels around a particular missing content portion. In some implementations, the inpainter 254 may utilize the blending weights 242 to determine how to inpaint a particular pixel, region, etc. The inpainter 254 may utilize the output from the NN224 to predict a particular background/foreground mask for rendering. In some implementations, the inliner 254 may be used with the image correction engine 252 to pull and push hole fills. This can be performed in images with regions/pixels where depth information is missing, which may result in no output color being predicted by the NN 224. The image correction engine 252 may trigger the inpainter to color a particular region/pixel in the image.
Once the blending weight 242 is determined, the system 214 may provide the weight to the neural renderer 248. The neural renderer 248 may generate an intermediate representation of the object (e.g., user) and/or scene, for example, utilizing the NN 244 (or another NN). For example, with an object-specific convolutional network, the neural renderer 248 may incorporate view-dependent effects by simulating the difference between the true appearance (e.g., the reference true phase) and the diffuse reflectance reprojection.
In operation, the system 200 may receive a stereoscopic fusion pipeline that produces (1) a depth map corresponding to each of the three color camera images and (2) a depth value D in the synthesized view from the target viewpoint to the nearest surface point determined for each output pixel t . For example, the image capture system may include at least three camera pods. Each phase cabin may include one or more color cameras and depth cameras (e.g., camera 204, depth sensor 206). In some implementations, the image capture system may additionally include a witness camera cabin. In this example, the system may perform geometric warping to convert the information from the three facies cabins into a target image space of the witness facies cabin. In particular, for each k of the three color cameras, reproducing a color (e.g., RGB) image, a calculation may be made
Then will beNN 224. The network can predict the image W where each channel is a non-negative floating point value for each input color image pixel. The system 214 may then construct an output image I N 。
In some implementations, the system 214 may perform multi-resolution blending using the multi-resolution blending engine 256. The multi-resolution blending engine 256 may employ the image pyramid as an input to a convolutional neural network (e.g., NN 224/414) that generates blending weights at multiple scales at the opacity values associated with each scale. In operation, the multi-resolution hybrid engine 256 may employ a two-stage trained end-to-end convolutional network process. Engine 256 may utilize multiple source cameras.
As described herein, the composite view 250 represents a 3D stereoscopic image of content (e.g., VR/AR objects, a user, a scene, etc.) with appropriate parallax and viewing configurations for the two eyes associated with a user accessing a display (e.g., display 212) based at least in part on the calculated mixing weights 242. The usage system 214 may determine at least a portion of the composite view 250 based on output from a neural network (e.g., the NN 244) each time a user moves head position while viewing the display and/or each time a particular image changes on the display. In some implementations, the composite view 250 represents the user's face and other features of the user that surround the user's face within the view that captured the user's face. In some implementations, for example, composite view 250 represents the entire field of view captured by one or more cameras associated with telepresence system 202A.
In some embodiments, the processor (not shown) of systems 202 and 214 may include (or be in communication with) a Graphics Processing Unit (GPU). In operation, the processor may include or have access to memory, storage, and other processors (e.g., CPUs). To assist in graphics and image generation, the processor may communicate with the GPU to display images on a display device (e.g., display device 212). The CPU and GPU may be connected via a high-speed bus such as PCI, AGP, or PCI-Express. The GPU may be connected to the Display through another high speed interface (e.g., HDMI, DVI, or Display Port). Generally, a GPU may render image content in the form of pixels. Display device 212 may receive image content from the GPU and may display the image content on a display screen.
Although not depicted in fig. 2, additional graphs, such as feature graphs, may be provided to one or more NNs 224 to generate image content. The feature map may be generated by analyzing the image to generate features for each pixel of the image. Such features may be used to generate feature maps and texture maps that may be provided to the hybrid weight generator 222 and/or NN224 to help generate the hybrid weights 242.
Fig. 3 is a block diagram illustrating an example of a re-projection of an input image to a target camera viewpoint according to implementations described throughout this disclosure. For example, the system 200 may be used to generate a re-projection of an image to be used as an input image for the NN. Warping the image may include re-projecting the captured input image 132 to the target camera viewpoint using the fused depth (from the depth image) to target the camera viewpoint. In some implementations, the input image 132 is already in the form of a reprojected image. In some implementations, image warping engine 220 performs warping.
For example, image warping engine 220 may back-project target image point x 302 onto a ray. Image warping engine 220 may then find point X304 at a distance d from target camera 308. Next, image warping engine 220 may project X to pod image point X '306, which is a distance d' from pod camera 310. The following equations [1] - [3] describe this calculation:
next, the image warping engine 220 may bilinearly sample the texture camera image at x', as shown by equations [4] and [5] below:
fig. 4 is a block diagram of an example flow diagram 400 for generating synthetic content for rendering on a display using neural mixing techniques, according to an implementation described throughout this disclosure. The graph 400 may generate data (e.g., multi-view color images, noise occlusion cues, depth data, etc.) to be provided to a blending algorithm via a neural network. The neural network can then learn the output mixing weights.
In this example, a plurality of input images 402 may be obtained (e.g., received). For example, the system 202A may capture a plurality of input images 402 (e.g., image frames, video). The input image 402 may be a color image. The input image 402 may also be associated with a depth image captured at substantially the same time as the input image. For example, the depth image may be captured by an infrared camera.
The computing system 214 may use the input image color and the depth image to warp (e.g., reproject) the input image 402 into a reprojected image 404. For example, warping engine 220 may re-project input image 402 into an output view that represents a desired novel view. In particular, the warping engine 220 may retrieve colors from the input image 402 and warp the colors into the output view using the depth view associated with the input image. In general, each input image may be warped into a single reprojected view. Thus, if four input images are retrieved, the warping engine 220 may generate four reprojected views, each associated with a single input image. The reprojected image 404 serves as a candidate color that may be selected for a pixel in the novel composite output image. Depth views captured at substantially the same time as the input image 402 may be used to generate a depth map 406 and an occlusion map 408 (similar to the depth map 228 and the occlusion map 226).
The reprojected image 404 may be used to generate a weighted sum image 410 that represents a weighted combination of colors for the pixels. The weighted sum image 410 may also take into account a ground truth image 412. Ground truth images 412 may be captured by one or more witness cameras.
The re-projected image 404, the depth map 406, and the occlusion map 408 may be provided to the NN 414, which, as shown in fig. 4, is a convolutional neural network having a U-Net shape. Other NNs are of course possible. In one non-limiting example, the NN 414 input may include three color RGB images, an occlusion map, and a target view depth map, which may utilize about fourteen channels.
In some implementations, the NN 414 may also be provided with a plurality of view parameters 415. The view parameters 415 may relate to a desired novel view (e.g., image). The view parameters 415 may include any or all of a view direction, pose, camera view angle, lens distortion, and/or intrinsic and extrinsic parameters of the camera (virtual or actual camera).
The NN 414 may generate a blending weight 416 for each of the re-projected images 404 to determine how to combine the colors of the re-projected images 404 to generate an accurate novel output image. The reprojected image 404 may be computed by warping the input image 402, for example, to a novel view based on the depth image 406. The NN 414 may use the blending weights 416 and the re-projected image 404 to generate a blended texture image 418, such as by blending at least a portion of the re-projected image 404 with each other using the blending weights 416. The hybrid texture image 418 may be used to generate an image associated with each camera pod associated with the input image 402 and thus, in turn, the reprojected image 404. In this example, three camera pods are used to capture three color images (e.g., input image 402) and three depth images (e.g., represented by depth map 406). Accordingly, three corresponding image views are output, as shown by image 420. The novel view can be synthesized using image 418 and image 420, as shown by composite image 422.
In operation, the NN 414 may use the blending weights 416 to determine how to combine the reprojected colors associated with the reprojected image 404 to generate an accurate composite image 422. The NN 414 may determine the blending weight by learning over the space of the predefined output views.
The network architecture of the NN 414 may be a deep neural network that is a U-Net shaped network with all convolutional layers using the same fill values and rectifying linear cell activation functions. The output may include hybrid weights 416 for the three re-projected images 404, one channel per phase of the nacelle, where the output weights are generated according to equation [6 ]:
W’＝10 -2 * W +1/3, clamped (clamp) to [0,1%] [6]
Graph 400 may be performed to account for training loss. For example, reconstruction losses, perceptual losses on the mixed color image, and integrity losses may be determined and used to improve the resulting composite image 422.
In operation, the system 200 may utilize several aspects to generate a per-pixel penalty value. For example, it can be as equation [7 ]]Showing a novel view image I for a texture camera I N And neural mixture weight W i ：
And an invalid target depth mask with no input having RGB values may be represented as I Mask 。
In particular, an example loss function may be represented by the following equation [8 ]:
wherein D = alpha r L 1 +α c VGG represents reconstruction and perceptual loss. In other words, it can be as equation [9 ]]As shown, the reconstruction and perceptual loss on the mixed color image is represented:
D：＝L 1 +αL VGG ，D(I N ⊙I Mask ，I Witness ⊙I Mask ) [9]
the integrity loss on the network output mixing weights for each x, y pixel coordinate can be expressed as shown in equation [10 ]:
L compl (W)＝∑ x ∑ y |(∑ k w k，x，y )-1| [10]
the occlusion loss on the network can be expressed as shown in equation [11 ]:
L occl (C，O)＝||c x，y || 1 if o is x，y ＞τ [11]
In some implementations, the NN 414 may be trained based on minimizing an occlusion loss function (i.e., equation [8 ]) between the composite image 422 generated by the NN 414 and the ground truth image 412 captured by the at least one witness camera.
Fig. 5 is a block diagram of an example flow diagram for generating hybrid weights according to an implementation described throughout this disclosure. For example, this example can employ convolution NN (e.g., convolution U-Net) to process the pixels of each input view. A multi-layer perceptron (MLP) may be used to generate the blending weights in order to assign each pixel of the proposed composite view. The blending weights generated by MLP can be used to combine features from the input images/views.
In some implementations, generating the blending weights may involve the use of multi-resolution blending techniques. The multi-resolution hybrid technique employs an end-to-end convolutional network process after two-stage training. These techniques utilize multiple source cameras. For example, the system 202A may capture one or more input images (e.g., RGB color images) from each of the first, second, and third phase nacelles 502, 504, and 506. Similarly, and at substantially the same time, the pods 502-504 can each capture (or compute) a depth image corresponding to a particular input image.
At least three color source input images and at least three source depth images can be provided to convolution networks 508A, 508B, and 508C (e.g., convolution U-Net) to generate a feature map that embeds view-related information. For example, one or more feature maps (not shown) may represent features of the input image in a feature space. In particular, for each input image/depth image 502-504, a feature map (e.g., feature maps 510A, 510B, and 510C) may be generated using the extracted features of the image. In some implementations, the input image can include two color source images and a single depth image. In such an example, the system 500 may use a single depth image to re-project each of the two color input images into the output view.
Feature maps 510A-510C may be used to generate UV maps 512A, 512B, and 512C. For example, the feature maps 510A-510C may be used to generate UV maps 512A-C from the visible content in the input images 502-504. UV maps 512A-512C represent the projection of a 2D image onto a 3D model surface in order to perform texture mapping to generate features that can be used to generate a composite image (e.g., a novel view). The output neural texture is still in the source camera image coordinates.
The respective feature maps 510A-510C may each be sampled along with the respective UV maps 512A-512C and witness camera parameters 514. For example, the system 500 may use a witness camera as the target camera for generating the composite novel image. Witness (e.g., target) camera parameters 514 may be predefined. Each of the respective sampling feature maps 510A-510C and UV maps 512A-C can be used with the parameters 514 and sampled with the occlusion map and depth map 516. Sampling may include using pre-computed UV maps 512A-512C from the fusion geometry (e.g., mesh proxy geometry 236) to warp each of the differentiated sampling layers of the neural texture.
The sampled content may be used by a multi-layer perceptron (MLP) NN 518 to generate occlusion maps, depth maps, etc. of the sampled features from all source machine views. From these figures, MLP 518 may generate a set of blending weights 520. For example, the per-pixel MLP 518 map may include sampled features from any number of source camera views that can be used to generate a set of blending weights 520. Such blending weights 520 may be used to generate a composite image.
In some implementations, the processes described herein may incorporate multi-resolution mixing techniques. For example, the multi-resolution blending technique may be performed, for example, by the multi-resolution blending engine 256 and may employ the image pyramid as an input to a convolutional neural network (e.g., NN 224/414) that generates blending weights at multiple scales at the opacity values associated with each scale.
The output blending weights at each scale are used to construct an output color image using the input reprojected color image at that scale, forming an output image pyramid. Each level of this pyramid is then weighted by the associated opacity value and upsampled to the original scale. The resulting set of images is then summed to construct the final output image. This is advantageous due to the fact that: there are small holes (due to missing geometries) in the input re-projected image, and the demagnification and then magnification process fills the missing regions with neighboring pixel values. In addition, the process may generate softer contours that are more visually appealing than conventional blending techniques.
In some implementations, the input pyramid can be constructed by downsampling bilinear reprojected colors of the reprojected images, not pre-multiplying by (un-pre-multiplex) the downsampled effective depth mask (e.g., map), upsampling back to a predefined (e.g., original) resolution, and not pre-multiplying by the upsampled effective depth mask. For each layer, the flowchart may add an output layer decoder (for mixing weights and alpha), upsample to a predefined (e.g., original resolution), adjust the additional background alpha at the highest resolution, normalize alpha using the soft max function, and mix with the reprojected color and background.
The multi-resolution hybrid technique employs an end-to-end convolutional network process after two-stage training. For each stage, for example, the multi-resolution blending technique may add an output layer decoder (e.g., on the blending weights and alpha losses). These techniques may compute RGB images, add losses, multiply by α, and cascade to determine candidate RGB images. The candidate RGB image may be upsampled. The upsampled candidate images may be used to generate an output image (e.g., a novel view/synchronization image) taking into account the loss.
In operation, the techniques utilize multiple source cameras. For example, the system 202A may capture one or more input images (e.g., RGB color images) from each of the first, second, and third phase nacelles 502, 504, and 506. Similarly, and at substantially the same time, the pods 502-504 can each capture a depth image corresponding to a particular input image.
Regardless of how the output viewpoint moves, multi-resolution blending may use the same 3D point in the scene graph to the same point location on the feature map. This may ensure that no 2D convolution is performed and, therefore, the output includes the same mixing weights for the point locations, since the input features are fixed.
Fig. 6 is a flow diagram illustrating one example of a process 600 for generating synthetic content using neural mixing techniques according to an implementation described throughout this disclosure. Process 600 is described with respect to example implementations of systems 100 and/or 200 and/or systems 500 and/or 800 of fig. 1 and 2, but it should be appreciated that the method can be implemented by systems having other configurations. In general, one or more processors and memory on system 202 and/or computing system 214 may be used to perform process 600.
At a high level, the process 600 may utilize a color input image, a depth image corresponding to the input image, and view parameters associated with a desired novel view corresponding to at least a portion of content within the input image. The process 600 may provide the above elements, or a version of the above elements, to a neural network to receive blending weights for determining a particular pixel color and depth for a desired novel view. The views may be used with the blending weights to generate a novel output image.
At block 602, the process 600 may include receiving a plurality of input images. For example, system 202A (or other image processing system) may use cameras (e.g., camera 204) to capture input images from two or more camera pods. In general, the plurality of input images are color images captured according to predefined view parameters. However, in some implementations, the plurality of input images may be gradient images of a single color (e.g., sepia, grayscale, or other gradient colors). The predefined view parameters may include camera parameters associated with the capture of a particular input image 132 (e.g., input image 402) and/or associated with the capture of an image to be generated (e.g., synthesized). In some implementations, the view parameters may include any or all of a view direction, pose, camera view angle, lens distortion, and/or intrinsic and extrinsic parameters of the camera. In some implementations, the plurality of input images can include a plurality of target subjects captured within a frame of the image. The target subject may include a user, a background, a foreground, a physical object, a virtual object, a gesture, a hair style, a wearable device, and the like.
At block 604, the process 600 may include receiving a plurality of depth images associated with a target subject in at least one of a plurality of input images. For example, system 202A may capture depth image 138 at substantially the same capture time as the input image (e.g., RGB color image 136). The depth image may capture a target subject that is also captured in one or more of the plurality of input images. The depth images may each include a depth map (e.g., map 228) associated with the at least one camera 204 that captured at least one of the plurality of input images 132, at least one occlusion map 226, and a depth map (e.g., via the target view parameters 232) associated with a ground truth image captured by the at least one witness camera at a time corresponding to the capture of the at least one of the plurality of input images. In short, the system 200 may consider the depth of the input image and the depth of the desired target view (or other determined target view) of the witness camera when generating the blending weights 242 for the target view.
At block 606, the process 600 may include receiving a plurality of view parameters for generating a virtual view of the target subject. For example, the view parameters may relate to a desired novel view (e.g., a novel composite image relating to a novel (e.g., virtual) view that was not previously captured by the camera). For example, the view parameters may include target parameters for a witness camera used to capture content at substantially the same time as the color image 136 and the depth image 138. The view parameters may include predefined lens parameters, viewing direction, pose, and specific intrinsic and/or extrinsic parameters of a camera configured to capture the novel view.
At block 608, the process 600 may include generating a plurality of warped images based on at least one of the plurality of input images, the plurality of view parameters, and the plurality of depth images. For example, the image warping engine 220 may use the input image 132 to generate a warped image by reprojecting the input image 132 into a reprojected version of the image 132. Warping may be performed to determine a projection of the input colors of the input image 132 into the novel view using depth information (e.g., a separate depth image or a geometrically pleasing surface). Warping may generate a reprojected image (e.g., image 404) by obtaining colors from one or more original input views and using the depth image (e.g., depth map 406 and occlusion map 408) to manipulate the colors for the novel view (e.g., image). Each input image may be used to generate a separate re-projection. The re-projected image (e.g., image 404) may represent pixels of candidate colors that may be used in the novel composite image.
In some implementations, the process 600 may include generating a plurality of warped images based on at least one of the plurality of input images, the plurality of view parameters, and the plurality of depth images by determining candidate projections of colors associated with the plurality of input images 402 into the uncaptured view (i.e., novel view/image, virtual view/image) using at least one of the plurality of depth images (e.g., the depth map 406 and the occlusion map 408). The non-captured view may include at least a portion of an image feature of at least one of the plurality of input images. For example, if the input image includes an object, the uncaptured view may consider at least a portion of the object, colors, pixels, etc.
At block 610, the process 600 may include receiving, from a neural network (e.g., NN224, NN 414, NN 508A-C), a blending weight 416 for assigning a color to a pixel of a virtual view (e.g., unseen image/unseen view) of a target subject (e.g., user 104'). In some implementations, the target subject can include or be based on at least one element captured in at least one frame of the plurality of input images 402. The blending weights 416 may be received in response to providing the plurality of depth images (e.g., the depth image 138 and/or the depth map 406 and/or the occlusion map 408), the plurality of view parameters 415, and the plurality of warped images (e.g., the reprojected image 404) to the NN 414. The NN 414 may generate the mixing weights 416 to indicate a probabilistic manner in which the colors of the combined reprojected image 404 to provide a likely and realistic output image that realistically represents the target subject. In some implementations, blending weight 416 is configured to assign a blended color to each pixel of a virtual view (i.e., a novel and/or unseen and/or previously unseen view), resulting in assignment of such blended color to an output composite image (e.g., composite image 422). For example, blending weights 416 are used to blend at least portions of the reprojected images 404 with one another.
At block 612, the process 600 may include generating a composite image from the view parameters based on the blending weights and the virtual view. The composite image 422 may represent an image captured using parameters related to an uncaptured view (e.g., not captured by a physical camera, generated from a virtual or physical camera into a virtual view, etc.), which may represent a view that is not visible (e.g., not captured by any camera of the image system, but instead synthesized). The composite image 422 may be generated for a three-dimensional (e.g., telepresence) video conference and/or during a three-dimensional (e.g., telepresence) video conference. For example, the composite image 422 may be generated in real-time during a video conference to provide an error corrected and accurate image of the user or content being captured by a camera associated with the video conference. In some implementations, the composite image 422 represents a novel view generated for a three-dimensional video conference. In some implementations, the composite image represents an uncaptured view of the target subject generated for a three-dimensional video conference.
In operation, blending weights are applied to pixels in the virtual view according to the view parameters. The resulting virtual view may include pixel colors generated using the blending weights for the target subject. For example, a color image of the virtual view may be used to generate a composite view from view parameters associated with the virtual camera.
In some implementations, the process 600 may additionally perform a geometric fusion process. In some implementations, the process 600 may perform a geometric fusion process rather than providing a separate depth image per input image. For example, the process 600 may use a geometric fusion process on the plurality of depth images to reconstruct a desired surface (e.g., a geometric proxy) to generate a geometric fusion model.
The geometric fusion model may be used to replace multiple views of depth image data (e.g., captured depth views of image content) with updated (e.g., computed) views of depth image data. The updated depth views may be generated as views of image content that contain depth data from the captured depth views and additionally contain image and/or depth information from each of any other available captured depth views of the image content. The one or more updated depth views may be used by the NN 414, for example, to synthesize additional (and new) views of the object by synthesizing additional (and new) blending weights using geometrically fused depth image data and image and depth information associated with multiple other views of the object. Any number of algorithms may be used to fuse the depth image data to replace each (input) depth view with a new depth view incorporating depth data information from several other depth views. In some implementations, the system 200 can use a geometric fusion model to generate depth data (e.g., a depth map) that can be used in connection with the cause of an occlusion in order to correct for such occlusion loss.
The process 600 may then generate a plurality of reprojected images based on the plurality of input images and the consensus surface used to generate the geometrically fused depth image data and provide the geometrically fused depth image data (along with the plurality of view parameters 415 and the plurality of reprojected images 404) to the NN 414. In response, the process 600 may include receiving the mixing weights 416 from the NN 414 and/or additional mixing weights generated using the pleasing surface depth image data for assigning colors to pixels in the composite image 422.
In some implementations, the process 600 may further include providing the NN 414 with a depth difference between the geometric fusion model and a depth observed in the plurality of depth images. For example, the depth disparity may be used to correct for occlusions detected in the composite image 422. In some implementations, the NN 414 may be trained based on minimizing an occlusion loss function between a composite image generated by the NN 414 and ground truth images 412 captured by at least one witness camera (e.g., associated with the system 202A), as described in detail with respect to fig. 4. In some implementations, process 400 may be performed using a single depth image instead of multiple depth images.
In some implementations, the NN 414 is further configured to perform multi-resolution blending to assign pixel colors to pixels in the composite image. In operation, multi-resolution mixing may trigger providing the image pyramid as input to the NN 414 to trigger receiving multi-resolution mixing weights (e.g., additional mixing weights 520) for multiple scales from the NN 414 and may additionally receive opacity values associated with each scale.
Fig. 7 shows an example of a computer device 700 and a mobile computer device 750 that can be used with the described techniques. Computing device 700 may include a processor 702, memory 704, a storage device 706, a high-speed interface 708 connecting to memory 704 and to a plurality of high-speed expansion ports 710, and a low-speed interface 712 connecting to low-speed bus 714 and storage device 706. The components 702, 704, 706, 708, 710, and 712, are interconnected using various buses, and may be mounted on a common motherboard or in other manners as appropriate. The processor 702 can process instructions for execution in the computing device 700, including instructions stored in the memory 704 or on the storage device 706 to display graphical information for a GUI on an external input/output device, such as display 716 coupled to high speed interface 708. In some embodiments, multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory. Moreover, multiple computing devices 700 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system).
The memory 704 stores information in the computing device 700. In one embodiment, the memory 704 is a volatile memory unit or units. In another embodiment, memory 704 is a non-volatile memory unit or units. The memory 704 may also be another form of computer-readable medium, such as a magnetic or optical disk.
The storage device 706 is capable of providing mass storage for the computing device 700. In one embodiment, the storage device 706 may be or contain a computer-readable medium, such as a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations. The computer program product may be tangibly embodied in an information carrier. The computer program product may also contain instructions that, when executed, perform one or more methods, such as the methods described herein. The information carrier may be a computer-readable medium or a machine-readable medium, such as the memory 704, the storage device 706, or memory on processor 702.
The high-speed controller 708 manages bandwidth-intensive operations for the computing device 700, while the low-speed controller 712 manages lower bandwidth-intensive operations. Such allocation of functions is merely exemplary. In one embodiment, high-speed controller 708 is coupled to memory 704, display 716 (e.g., through a graphics processor or accelerator), and to high-speed expansion ports 710, which high-speed expansion ports 710 may receive various expansion cards (not shown). Low-speed controller 712 may be coupled to storage device 706 and low-speed expansion port 714. The low-speed expansion port, which may include various communication ports (e.g., USB, bluetooth, ethernet, wireless ethernet), may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device, such as a switch or router, for example, through a network adapter.
As illustrated in the figures, computing device 700 may be implemented in a number of different forms. It may be implemented, for example, as a standard server 720, or multiple times in a group of such servers. Further, it may be implemented as part of a rack server system 724. Further, it may be implemented in a personal computer such as a laptop computer 722. Alternatively, components from computing device 700 may be combined with other components in a mobile device (not shown), such as device 750. Each of these devices may contain one or more computing devices 700, 750, and an entire system may be made up of multiple computing devices 700, 750 communicating with each other.
Computing device 750 includes a processor 752, a memory 764, input/output devices such as a display 754, a communication interface 766, and a transceiver 768, among other components. The device 750 may also be provided with a storage device, such as a microdrive or other device, to provide additional storage. Each of the components 750, 752, 764, 754, 766, and 768, are interconnected using various buses, and several of the components may be mounted on a common motherboard or in other manners as appropriate.
The processor 752 can execute instructions in the computing device 750, including instructions stored in the memory 764. The processor may be implemented as a chipset of chips that include separate pluralities of analog and digital processors. The processor may provide, for example, for coordination of the other components of the device 750, such as control of user interfaces, applications run through device 750, and wireless communication through device 750.
The memory 764 stores information in the computing device 750. The memory 764 can be implemented as one or more of a computer-readable medium or media, a volatile memory unit or units, or a non-volatile memory unit or units. Further, expansion memory 784 may be provided and connected to device 750 through expansion interface 782, where expansion interface 782 may include, for example, a SIMM (Single in line memory Module) card interface. Such expansion memory 784 may provide additional storage space for device 750, and may also store applications or other information for device 750. Specifically, expansion memory 784 may include instructions to carry out or supplement the processes described above, as well as secure information. Thus, for example, expansion memory 784 may be a security module for device 750 and may be programmed with instructions that permit secure use of device 750. In addition, secure applications may be provided via the SIMM card along with additional information, such as placing identification information into the SIMM card in a non-blackable manner.
The memory may include, for example, flash memory and/or NVRAM memory, as described below. In one embodiment, a computer program product may be tangibly embodied in an information carrier. The computer program product contains instructions which, when executed, perform one or more methods, such as the methods described above. The information carrier may be a computer-or machine-readable medium, such as the memory 764, expansion memory 784, or memory on processor 752, that is received, for example, through transceiver 768 or external interface 762.
Device 750 may communicate wirelessly through communication interface 766, which may include digital signal processing circuitry where necessary. Communication interface 766 may provide for communications under various modes or protocols, such as GSM voice calls, SMS, EMS, or MMS messaging, CDMA, TDMA, PDC, WCDMA, CDMA2000, or GPRS, among others. Such communication may occur, for example, through radio-frequency transceiver 768. In addition, short-range communications may be made, such as using a bluetooth, wiFi, or other such transceiver (not shown). In addition, GPS (Global positioning System) receiver module 780 may provide additional navigation-and location-related wireless data to device 750, which may be used as appropriate by applications running on device 750.
Device 750 may also communicate audibly using audio codec 760, where audio codec 760 may receive spoken information from a user and convert it to usable digital information. Likewise, audio codec 760 may generate audible sound for a user, such as through a speaker, e.g., in a headset of device 750. Such sound may include sound from voice telephone calls, may include recorded sound (e.g., voice messages, music files, etc.) and may also include sound generated by applications operating on device 750.
As illustrated in the figures, the computing device 750 may be implemented in a number of different forms. For example, it may be implemented as a cellular telephone 780. It may also be implemented as part of a smart phone 782, a personal digital assistant, or other similar mobile device.
Various embodiments of the systems and techniques described here can be implemented in digital electronic circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof. These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
These computer programs (also known as programs, software applications or code) include machine instructions for a programmable processor, and can be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. As used herein, the terms "machine-readable medium" and "computer-readable medium" refer to any computer program product, apparatus and/or device (e.g., magnetic discs, optical disks, memory, programmable Logic Devices (PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term "machine-readable signal" refers to any signal used to provide machine instructions and/or data to a programmable processor.
To provide for interaction with a user, the systems and techniques described here can be implemented on a computer having a display device (e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor) for displaying information to the user and a keyboard and a pointing device (e.g., a mouse or a trackball) by which the user can provide input to the computer. Other types of devices may also be used to provide for interaction with a user; for example, feedback provided to the user can be any form of sensory feedback (e.g., visual feedback, auditory feedback, or tactile feedback); and input from the user can be received in any form, including acoustic, speech, or tactile input.
The systems and techniques described here can be implemented in a computing system that includes a back-end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front-end component (e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an embodiment of the systems and techniques described here), or any combination of such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a Local Area Network (LAN), a Wide Area Network (WAN), and the Internet.
The computing system may include clients and servers. A client and server are substantially remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
In some embodiments, the computing device shown in fig. 7 may include sensors that interface with a virtual reality headset (VR headset/HMD device 790). For example, one or more sensors included on the computing device 750 or other computing devices shown in fig. 7 may provide input to the VR headset 790 or, in general, to the VR space. The sensors may include, but are not limited to, touch screens, accelerometers, gyroscopes, pressure sensors, biometric sensors, temperature sensors, humidity sensors, and ambient light sensors. The computing device 750 may use the sensors to determine the absolute position and/or detected rotation of the computing device in the VR space, which is then used as an input to the VR space. For example, the computing device 750 may be incorporated as a virtual object into a VR space such as a controller, laser pointer, keyboard, weapon, and so forth. When merged into the VR space, positioning the computing device/virtual object by the user may allow the user to position the computing device to view the virtual object in some manner in the VR space.
In some embodiments, one or more input devices included on or connected to computing device 750 may be used as input to the VR space. The input devices may include, but are not limited to, a touch screen, a keyboard, one or more buttons, a touch pad, a pointing device, a mouse, a trackball, a joystick, a camera, a microphone, a headset or earpiece with input capabilities, a game controller, or other connectable input devices. When incorporating a computing device into the VR space, a user interacting with an input device included on the computing device 750 may cause a particular action to occur in the VR space.
In some embodiments, one or more output devices included on the computing device 750 may provide output and/or feedback to a user of the VR headset 790 in the VR space. The output and feedback may be visual, tactile or audio. The output and/or feedback may include, but is not limited to, rendering a VR space or virtual environment, vibrating, turning on and off or flashing and/or flashing one or more lights or flashes, sounding an alarm, playing a ringtone, playing a song, and playing an audio file. Output devices may include, but are not limited to, vibration motors, vibration coils, piezoelectric devices, electrostatic devices, light Emitting Diodes (LEDs), flashlights, and speakers.
In some embodiments, the computing device 750 may be placed within the VR headset 790 to create a VR system. The VR headset 790 may include one or more positioning elements that allow a computing device 750 (such as a smartphone 782) to be placed in place within the VR headset 790. In such embodiments, the display of smartphone 782 may render a stereoscopic image representing the VR space or virtual environment.
In some embodiments, the computing device 750 may appear as another object in a computer-generated 3D environment. User interactions with the computing device 750 (e.g., rotation, shaking, touch to a touch screen, finger sliding across a touch screen) may be interpreted as interactions with objects in the VR space. As just one example, the computing device may be a laser pointer. In such an example, the computing device 750 appears as a virtual laser pointer in a computer-generated 3D environment. As the user manipulates the computing device 750, the user in the VR space sees the movement of the laser pointer. The user receives feedback from interactions with the computing device 750 in the VR environment on the computing device 750 or VR headset 790.
In some embodiments, computing device 750 may include a touch screen. For example, a user can interact with the touchscreen in a particular way that can mimic what happens on the touchscreen and what happens in the VR space. For example, a user may zoom in and out on content displayed on a touch screen with a pinch-type action. This pinching action on the touch screen may cause the information provided in the VR space to be magnified. In another example, a computing device may be rendered as a virtual book in a computer-generated 3D environment. In the VR space, the pages of the book may be displayed in the VR space, and sliding the user's finger across the touchscreen may be interpreted as flipping/turning the pages of the virtual book. As each page is flipped/flipped, in addition to seeing page content changes, audio feedback may be provided to the user, such as the sound of flipping pages in a book.
In some embodiments, one or more input devices other than a computing device (e.g., mouse, keyboard) may be rendered in a computer-generated 3D environment. The rendered input device (e.g., rendered mouse, rendered keyboard) may be used to render in the VR space to control objects in the VR space.
In addition, the logic flows depicted in the figures do not require the particular order shown, or sequential order, to achieve desirable results. In addition, other steps may be provided, eliminated, from the illustrated flows, and other components may be added to, or deleted from, the system. Accordingly, other embodiments are within the scope of the following claims.
Claims (21)
1. A computer-implemented method, comprising:
receiving a plurality of input images;
receiving a plurality of depth images associated with a target subject in at least one of the plurality of input images;
receiving a plurality of view parameters for generating a virtual view of the target subject;
generating a plurality of warped images based on the plurality of input images, the plurality of view parameters, and at least one depth image of the plurality of depth images;
in response to providing the plurality of depth images, the plurality of view parameters, and the plurality of warped images to a neural network, receiving, from the neural network, a blending weight for assigning a color to a pixel of the virtual view of the target subject; and
generating a composite image according to the view parameters based on the blending weights and the virtual view.
2. The computer-implemented method of claim 1, further comprising:
reconstructing a desired surface using a geometric fusion process on the plurality of depth images to generate a geometric fusion model;
generating a plurality of reprojected images based on the plurality of input images and the desired surface; and
in response to providing the plurality of depth images, the plurality of view parameters, and the plurality of re-projected images to the neural network, receiving additional blending weights from the neural network for assigning colors to pixels in the composite image.
3. The computer-implemented method of claim 2, further comprising providing the neural network with depth differences between the geometrically fused model and depths observed in the plurality of depth images, and further comprising correcting occlusions detected in the composite image based on the depth differences.
4. The computer-implemented method of any of the preceding claims, wherein:
the plurality of input images are color images captured according to predefined view parameters associated with at least one camera capturing the plurality of input images; and/or
The plurality of depth images each include a depth map associated with at least one camera that captured at least one of the plurality of input images, at least one occlusion map, and/or a depth map associated with a ground truth image captured by at least one witness camera at a time corresponding to capture of at least one of the plurality of input images.
5. The computer-implemented method of any of the preceding claims, wherein the blending weight is configured to assign a blended color to each pixel of the composite image.
6. The computer-implemented method of any of the preceding claims, wherein the neural network is trained based on minimizing an occlusion loss function between the composite image generated by the neural network and ground truth images captured by at least one witness camera.
7. The computer-implemented method of any of the preceding claims, wherein the composite image is an uncaptured view of the target subject generated for a three-dimensional video conference.
8. The computer-implemented method of any of the preceding claims, wherein generating the plurality of warped images based on the plurality of input images, the plurality of view parameters, and at least one of the plurality of depth images comprises determining candidate projections of colors associated with the plurality of input images into an uncaptured view using the at least one of the plurality of depth images, the uncaptured view comprising at least a portion of an image feature of the at least one of the plurality of input images.
9. An image processor system for performing the method according to any one of the preceding claims, the system comprising:
at least one processing device; and
a memory storing instructions that, when executed, cause the system to perform operations comprising:
receiving a plurality of input images captured by the image processing system;
receiving a plurality of depth images captured by the image processing system;
receiving a plurality of view parameters associated with an uncaptured view associated with at least one input image of the plurality of input images;
generating a plurality of warped images based on the plurality of input images, the plurality of view parameters, and at least one depth image of the plurality of depth images;
in response to providing the plurality of depth images, the plurality of view parameters, and the plurality of warped images to a neural network, receiving, from the neural network, a blending weight for assigning colors to pixels of the uncaptured view; and
generating a composite image according to the blending weights, wherein the composite image corresponds to the uncaptured view.
10. The image processing system of claim 9, wherein:
the plurality of input images are color images captured by the image processing system according to predefined view parameters associated with the image processing system; and/or
The plurality of depth images includes a depth map associated with at least one camera capturing at least one of the plurality of input images, at least one occlusion map, and/or a depth map associated with a witness camera of the image processing system.
11. The image processing system according to claim 9 or 10, wherein the blending weight is configured to assign a blended color to each pixel of the composite image.
12. The image processing system of any of claims 9 to 11, wherein the neural network is trained based on minimizing an occlusion loss function between the composite image generated by the neural network and ground truth images captured by at least one witness camera.
13. The image processing system of any of claims 9 to 12, wherein the composite image is a novel view generated for a three-dimensional video conference.
14. A non-transitory machine-readable medium having instructions stored thereon, which when executed by a processor, cause a computing device to:
receiving a plurality of input images;
receiving a plurality of depth images associated with a target subject in at least one of the plurality of input images;
receiving a plurality of view parameters for generating a virtual view of the target subject;
reconstructing a consensus surface using a geometric fusion process on the plurality of depth images to generate a geometric fusion model of the target subject;
generating a plurality of reprojected images based on the plurality of input images, the plurality of view parameters, and the consensus surface;
in response to providing the plurality of depth images, the plurality of view parameters, and the plurality of re-projected images to a neural network, receiving, from the neural network, a blending weight for assigning colors to pixels of the virtual view of the target subject; and
generating a composite image according to the view parameters based on the blending weights and the virtual view.
15. The non-transitory machine-readable medium of claim 14, further comprising:
providing depth differences between the geometric fusion model and depths observed in the plurality of depth images to the neural network, and correcting occlusions detected in the composite image based on the depth differences.
16. The non-transitory machine-readable medium of claim 14 or 15, wherein:
the plurality of input images are color images captured according to predefined view parameters associated with at least one camera capturing the plurality of input images; and/or
The plurality of depth images includes a depth map associated with at least one camera that captured at least one of the plurality of input images, at least one occlusion map, and/or a depth map associated with a ground truth image captured by at least one witness camera at a time corresponding to capture of at least one of the plurality of input images.
17. The non-transitory machine readable medium of any of claims 14 to 16, wherein the blending weight is configured to assign a blended color to each pixel of the composite image.
18. The non-transitory machine readable medium of any of claims 14 to 17, wherein the neural network is trained based on minimizing an occlusion loss function between the composite image generated by the neural network and a ground truth image captured by at least one witness camera.
19. The non-transitory machine readable medium of any of claims 14 to 18, wherein the composite image is a novel view for a three-dimensional video conference.
20. The non-transitory machine-readable medium of any of claims 14 to 19, wherein the neural network is further configured to perform multi-resolution blending to assign pixel colors to pixels in the composite image, the multi-resolution blending triggering provision of an image pyramid as an input to the neural network to trigger receipt of multi-resolution blending weights for multiple scales and opacity values associated with each scale from the neural network.
21. The non-transitory machine readable medium of any of claims 14 to 20, wherein the instructions, when executed by the processor, cause the computing device to perform the method of any of claims 1 to 8.
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2021/070362 WO2022216333A1 (en) | 2021-04-08 | 2021-04-08 | Neural blending for novel view synthesis |
Publications (1)
Publication Number | Publication Date |
---|---|
CN115529835A true CN115529835A (en) | 2022-12-27 |
Family
ID=75625694
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202180004259.1A Pending CN115529835A (en) | 2021-04-08 | 2021-04-08 | Neural blending for novel view synthesis |
Country Status (7)
Country | Link |
---|---|
US (1) | US20220398705A1 (en) |
EP (1) | EP4091141A1 (en) |
JP (1) | JP2023524326A (en) |
KR (1) | KR102612529B1 (en) |
CN (1) | CN115529835A (en) |
TW (1) | TWI813098B (en) |
WO (1) | WO2022216333A1 (en) |
Families Citing this family (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
KR20220146900A (en) * | 2021-04-26 | 2022-11-02 | 삼성전자주식회사 | Electronic device including processing circuit for generating depth information using luminance data, and depth information generation method |
US20230196662A1 (en) * | 2021-12-20 | 2023-06-22 | Nvidia Corporation | Image blending using one or more neural networks |
US20230252714A1 (en) * | 2022-02-10 | 2023-08-10 | Disney Enterprises, Inc. | Shape and appearance reconstruction with deep geometric refinement |
KR102648938B1 (en) * | 2023-02-15 | 2024-03-19 | 고려대학교 산학협력단 | Method and apparatus for 3D image reconstruction based on few-shot neural radiance fields using geometric consistency |
Family Cites Families (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7965902B1 (en) * | 2006-05-19 | 2011-06-21 | Google Inc. | Large-scale image processing using mass parallelization techniques |
US9681154B2 (en) * | 2012-12-06 | 2017-06-13 | Patent Capital Group | System and method for depth-guided filtering in a video conference environment |
US20160142700A1 (en) * | 2014-11-19 | 2016-05-19 | Ginni Grover | Measuring Accuracy of Image Based Depth Sensing Systems |
US10958887B2 (en) * | 2019-01-14 | 2021-03-23 | Fyusion, Inc. | Free-viewpoint photorealistic view synthesis from casually captured video |
US10970911B2 (en) * | 2019-02-21 | 2021-04-06 | Facebook Technologies, Llc | Graphics processing chip with machine-learning based shader |
US10930054B2 (en) * | 2019-06-18 | 2021-02-23 | Intel Corporation | Method and system of robust virtual view generation between camera views |
US11928787B2 (en) * | 2020-07-29 | 2024-03-12 | Intel Corporation | Deep novel view synthesis from unstructured input |
CN112614060A (en) * | 2020-12-09 | 2021-04-06 | 深圳数联天下智能科技有限公司 | Method and device for rendering human face image hair, electronic equipment and medium |
-
2021
- 2021-04-08 CN CN202180004259.1A patent/CN115529835A/en active Pending
- 2021-04-08 KR KR1020217041885A patent/KR102612529B1/en active IP Right Grant
- 2021-04-08 JP JP2021577153A patent/JP2023524326A/en active Pending
- 2021-04-08 EP EP21720665.5A patent/EP4091141A1/en active Pending
- 2021-04-08 WO PCT/US2021/070362 patent/WO2022216333A1/en unknown
- 2021-04-08 US US17/754,392 patent/US20220398705A1/en active Pending
- 2021-12-16 TW TW110147092A patent/TWI813098B/en active
Also Published As
Publication number | Publication date |
---|---|
US20220398705A1 (en) | 2022-12-15 |
JP2023524326A (en) | 2023-06-12 |
WO2022216333A1 (en) | 2022-10-13 |
KR20220140402A (en) | 2022-10-18 |
TW202240530A (en) | 2022-10-16 |
TWI813098B (en) | 2023-08-21 |
KR102612529B1 (en) | 2023-12-11 |
EP4091141A1 (en) | 2022-11-23 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
EP3959688B1 (en) | Generative latent textured proxies for object category modeling | |
US10096157B2 (en) | Generation of three-dimensional imagery from a two-dimensional image using a depth map | |
US20220130111A1 (en) | Few-shot synthesis of talking heads | |
KR102612529B1 (en) | Neural blending for new view synthesis | |
CN109791704B (en) | Texture rendering method, system and device based on multi-layer UV mapping for free-running FVV application | |
US11335077B1 (en) | Generating and modifying representations of dynamic objects in an artificial reality environment | |
US11451758B1 (en) | Systems, methods, and media for colorizing grayscale images | |
US20230316810A1 (en) | Three-dimensional (3d) facial feature tracking for autostereoscopic telepresence systems | |
US11410387B1 (en) | Systems, methods, and media for generating visualization of physical environment in artificial reality | |
EP4233310A1 (en) | Dynamic resolution of depth conflicts in telepresence | |
Thatte et al. | Real-World Virtual Reality With Head-Motion Parallax | |
US20240153223A1 (en) | Reliable Depth Measurements for Mixed Reality Rendering | |
US20220232201A1 (en) | Image generation system and method | |
Thatte | Cinematic virtual reality with head-motion parallax | |
Wetzstein | Augmented and virtual reality | |
JP2023546693A (en) | Geometric fusion of depth data on the transmit side | |
CN115661408A (en) | Generating and modifying hand representations in an artificial reality environment |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |