CN117377983A - System and method for machine learning model with convolution and attention - Google Patents
System and method for machine learning model with convolution and attention Download PDFInfo
- Publication number
- CN117377983A CN117377983A CN202280026409.3A CN202280026409A CN117377983A CN 117377983 A CN117377983 A CN 117377983A CN 202280026409 A CN202280026409 A CN 202280026409A CN 117377983 A CN117377983 A CN 117377983A
- Authority
- CN
- China
- Prior art keywords
- attention
- computer
- convolution
- implemented method
- network
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000010801 machine learning Methods 0.000 title claims abstract description 129
- 238000000034 method Methods 0.000 title claims abstract description 108
- 230000007246 mechanism Effects 0.000 claims abstract description 41
- 230000003068 static effect Effects 0.000 claims abstract description 20
- 239000011159 matrix material Substances 0.000 claims abstract description 16
- 230000003044 adaptive effect Effects 0.000 claims abstract description 15
- 230000004044 response Effects 0.000 claims abstract description 8
- 238000010606 normalization Methods 0.000 claims description 15
- 230000005284 excitation Effects 0.000 claims description 7
- 230000007423 decrease Effects 0.000 claims description 4
- 238000001125 extrusion Methods 0.000 claims description 3
- 230000008569 process Effects 0.000 description 42
- 238000012545 processing Methods 0.000 description 20
- 238000012549 training Methods 0.000 description 15
- 230000015654 memory Effects 0.000 description 14
- 238000013528 artificial neural network Methods 0.000 description 13
- 238000010586 diagram Methods 0.000 description 12
- 230000000007 visual effect Effects 0.000 description 9
- 239000008186 active pharmaceutical agent Substances 0.000 description 7
- 238000013527 convolutional neural network Methods 0.000 description 6
- 230000008901 benefit Effects 0.000 description 5
- 230000006870 function Effects 0.000 description 5
- 238000004891 communication Methods 0.000 description 4
- 230000003993 interaction Effects 0.000 description 4
- 230000004048 modification Effects 0.000 description 4
- 238000012986 modification Methods 0.000 description 4
- 230000011218 segmentation Effects 0.000 description 4
- 238000001514 detection method Methods 0.000 description 3
- 230000006872 improvement Effects 0.000 description 3
- 230000000306 recurrent effect Effects 0.000 description 3
- 238000013519 translation Methods 0.000 description 3
- 230000000694 effects Effects 0.000 description 2
- 238000005516 engineering process Methods 0.000 description 2
- 238000003709 image segmentation Methods 0.000 description 2
- 230000009471 action Effects 0.000 description 1
- 238000007792 addition Methods 0.000 description 1
- 230000004075 alteration Effects 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 238000007906 compression Methods 0.000 description 1
- 230000006835 compression Effects 0.000 description 1
- 238000013144 data compression Methods 0.000 description 1
- 230000004927 fusion Effects 0.000 description 1
- 230000007787 long-term memory Effects 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 238000011176 pooling Methods 0.000 description 1
- 230000006403 short-term memory Effects 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/40—Extraction of image or video features
- G06V10/44—Local feature extraction by analysis of parts of the pattern, e.g. by detecting edges, contours, loops, corners, strokes or intersections; Connectivity analysis, e.g. of connected components
- G06V10/443—Local feature extraction by analysis of parts of the pattern, e.g. by detecting edges, contours, loops, corners, strokes or intersections; Connectivity analysis, e.g. of connected components by matching or filtering
- G06V10/449—Biologically inspired filters, e.g. difference of Gaussians [DoG] or Gabor filters
- G06V10/451—Biologically inspired filters, e.g. difference of Gaussians [DoG] or Gabor filters with interaction between the filter responses, e.g. cortical complex cells
- G06V10/454—Integrating the filters into a hierarchical structure, e.g. convolutional neural networks [CNN]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/06—Physical realisation, i.e. hardware implementation of neural networks, neurons or parts of neurons
- G06N3/063—Physical realisation, i.e. hardware implementation of neural networks, neurons or parts of neurons using electronic means
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/82—Arrangements for image or video recognition or understanding using pattern recognition or machine learning using neural networks
Abstract
A computer-implemented method of performing computer vision with reduced computational cost and increased accuracy can include: obtaining, by a computing system comprising one or more computing devices, input data comprising an input tensor having one or more dimensions; providing, by the computing system, the input data to a machine-learned convolutional attention network comprising two or more network stages; and receiving, by the computing system, a machine learning prediction from the machine learning convolution attention network in response to providing the input data to the machine learning convolution attention network. The convolution attention network can include at least one attention block, wherein the attention block includes a relative attention mechanism comprising a sum of a static convolution kernel and an adaptive attention matrix. This provides improved generalization, capacity and efficiency of the convolved attention network over some existing models.
Description
RELATED APPLICATIONS
The present application claims priority and benefit from U.S. provisional patent application No.63/194,077, filed 5/27 at 2021. U.S. provisional patent application No.63/194,077 is hereby incorporated by reference in its entirety.
Technical Field
The present disclosure relates generally to machine learning. More specifically, the present disclosure relates to systems and methods of machine learning models with convolution and attention.
Background
Machine learning refers to a class of learning algorithms that provide predictions for input data. Convolutional neural networks or CNNs are a class of machine learning models that employ convolutional frames in neural networks. Transducers are a class of machine-learning models that employ an attention mechanism to weight different portions of input data. Existing methods of combining convolution and attention face drawbacks such as increased computational cost.
Disclosure of Invention
Aspects and advantages of embodiments of the disclosure will be set forth in part in the description which follows, or may be learned from the description, or may be learned by practice of the embodiments.
One example aspect of the present disclosure relates to a computer-implemented method of performing computer vision with reduced computational cost and increased accuracy. A computer-implemented method includes obtaining, by a computing system including one or more computing devices, input data including an input tensor having one or more dimensions. The computer-implemented method includes providing, by a computing system, input data to a machine-learned convolutional attention network, the machine-learned convolutional attention network comprising two or more network phases, each of the two or more network phases comprising one of an attention phase or a convolution phase. The computer-implemented method includes, in response to providing input data to a machine-learning convolutional attention network, receiving, by a computing system, a machine-learning prediction from the machine-learning convolutional attention network. The attention phase includes a relative attention mechanism including a sum of a static convolution kernel and an adaptive attention matrix.
Another example aspect of the invention relates to a computer-implemented method of performing computer vision with reduced computational cost and increased accuracy. A computer-implemented method includes obtaining, by a computing system including one or more computing devices, input data including an input tensor having one or more dimensions. A computer-implemented method includes providing, by a computing system, input data to a machine-learning convolution attention network. The machine-learning convolution attention network includes a downsampling stage configured to reduce spatial resolution relative to an input tensor; and one or more attention blocks comprising a relative attention mechanism comprising a sum of a static convolution kernel and an adaptive attention matrix. The computer-implemented method includes, in response to providing input data to a machine-learning convolutional attention network, receiving, by a computing system, a machine-learning prediction from the machine-learning convolutional attention network.
Another example aspect of the invention relates to a computer-implemented method of performing computer vision with reduced computational cost and increased accuracy. A computer-implemented method includes obtaining, by a computing system including one or more computing devices, input data including an input tensor having one or more dimensions. The computer-implemented method includes providing, by a computing system, input data to a machine-learned convolutional attention network, the machine-learned convolutional attention network including a plurality of network stages. The plurality of network phases includes an S0 phase comprising a two-layer convolutional dry-line network; stage S1, including a convolution block with squeeze excitation; s2, a stage comprising a convolution block; s3, a stage comprising a convolution block; stage S4, which includes an attention block; and a S5 stage comprising an attention block. Each of the S4 stage and the S5 stage includes a relative attention mechanism including a sum of a static convolution kernel and an adaptive attention matrix. The spatial resolution is reduced at each of the plurality of network stages. The number of channels is increased at each of the plurality of network stages. The computer-implemented method includes, in response to providing input data to a machine-learned convolutional attention network, receiving, by a computing system, a machine-learned prediction from the machine-learned convolutional attention network.
Other aspects of the disclosure relate to various systems, apparatuses, non-transitory computer-readable media, user interfaces, and electronic devices.
These and other features, aspects, and advantages of various embodiments of the present disclosure will become better understood with reference to the following description and appended claims. The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate exemplary embodiments of the disclosure and together with the description, serve to explain the principles of the disclosure.
The accompanying drawings describe example embodiments of the proposed technology in more detail. The accompanying appendix is incorporated in and forms a part of the present disclosure. However, the present disclosure is not limited to the example embodiments provided in the attached appendix.
Drawings
A detailed discussion of embodiments directed to one of ordinary skill in the art is set forth in the specification in view of the accompanying drawings, wherein:
FIG. 1A depicts a block diagram of an example computing system that performs computer vision with reduced computing costs and increased accuracy, according to an example embodiment of the present disclosure.
FIG. 1B depicts a block diagram of an example computing device performing computer vision with reduced computing cost and increased accuracy, according to an example embodiment of the present disclosure.
FIG. 1C depicts a block diagram of an example computing device performing computer vision with reduced computing cost and increased accuracy, according to an example embodiment of the present disclosure.
Fig. 2 depicts a block diagram of an example convolved attention network (coanet) model, according to an example embodiment of the disclosure.
FIG. 3 depicts a block diagram of an example convolved attention network model, according to an example embodiment of the present disclosure.
FIG. 4 depicts a block diagram of an example convolved attention network model, according to an example embodiment of the present disclosure.
FIG. 5 depicts a flowchart of an example method of performing computer vision with reduced computational cost and increased accuracy in accordance with an example embodiment of the present disclosure.
Repeated reference characters in the drawings are intended to represent like features in the various embodiments.
Detailed Description
In general, the present disclosure relates to systems and methods of machine learning models with convolution and attention. In particular, systems and methods according to example aspects of the present disclosure can include a convolution block and/or an attention block. According to example aspects of the disclosure, the attention block can include a relative attention mechanism. In particular, example aspects of the present disclosure recognize that the above-described relative attentiveness can be considered a natural mix of deep convolution and content-based attentiveness. Further, example aspects of the present disclosure recognize that both depth convolution and self-attention can be represented as weighted sums of values in receptive fields. Thus, the relative attention mechanism can include the sum of the static convolution kernel and the self-attention matrix. The sum can be applied before and/or after SoftMax normalization by the relative attention mechanism. As one example, the relative attention mechanism (e.g., applied prior to SoftMax normalization) may be mathematically represented by:
As another example, the relative attention mechanism (e.g., applied after SoftMax normalization) may be mathematically represented by:
in the above equation, the depth convolution kernel w i-j Is an input-independent parameter for a static value of a given index in the input tensor (i, j) (e.g., a relative shift between indexes i-j, where the dependence on the relative shift, rather than a particular value, is referred to as a translational equivalence, which can improve generalization under a finite-sized dataset), x i And x j Input and output at location i, respectively, and g is a global receptive field (e.g., the entire set of locations).
The use of global receptive fields (e.g., as opposed to limited local receptive fields traditionally employed in convolutional networks) can provide improved capabilities to capture complex relational interactions between different spatial locations, which can be desirable when dealing with higher-level concepts. Denominator terms can also be referred to as the attention weight a i，j . The attention weights can be determined jointly by the depth convolution kernel and the translational equivalence of the input-adaptive input-output pairs, which can provide two characteristics of varying degrees, improving generalization, capacity, and/or accuracy of the model.
These attention blocks with relative self-attention can be employed in a network with convolution and attention (referred to herein as the "coanet" model) to provide improved fusion of benefits from convolution and attention. For example, in addition to the high accuracy and efficiency associated with convolutional networks, the model can have over-fitting, lower computational cost, reduced memory usage, and/or robustness to smaller parameter sizes, while additionally providing the ability to learn complex relational interactions between spatial locations in the input data associated with the transducer.
Systems and methods according to example aspects of the present disclosure (e.g., employing an attention block with relative attention) can provide a number of technical effects and benefits, including improvements to computer technology. As one example, systems and methods according to example aspects of the present disclosure can unify convolution and attention to provide improved generalization, model capacity, and/or efficiency. For example, systems and methods according to example aspects of the present disclosure can more effectively manage trade-offs between improved generalization (e.g., similar to a convolutional network) and improved model capacity (e.g., similar to a transformer). For example, some example embodiments of the present disclosure are capable of achieving the performance of the prior art under different data sizes and computational budgets.
The improvements provided by the proposed model architecture (e.g., generalization and/or model capacity) can in turn provide improved accuracy of the model, particularly over invisible input data, improved range of input data types and/or dimensions, reduced consumption of computing resources (e.g., faster computing speed, fewer computing cycles, reduced processor or memory usage, etc.), and/or other improvements over existing models. In particular, the model as presented herein enables performance comparable to prior art convolutional neural networks while having a smaller number of parameters. As one example, an example implementation of the coanet model can achieve comparable first level (top 1) accuracy on the ImageNet dataset, with only 40% parameter count and 70% FLOP.
As another example technical effect, the hybrid convolution and attention architecture described herein enables more efficient use of dedicated hardware, such as a processor (e.g., a graphics processing unit), dedicated to performing convolution and attention mechanisms. For example, the convolution stage of the proposed hybrid model can be performed by hardware dedicated to convolution operations, while the attention stage of the proposed hybrid model can be performed by hardware dedicated to attention operations. For example, the convolution operation of the convolution stage of the proposed hybrid model can be performed in parallel by a plurality of processors.
Systems and methods according to example aspects of the present disclosure can be applied to a variety of machine learning tasks, particularly tasks that traditionally employ convolutional neural networks. As an example, the machine learning task can be a computer vision task such as object detection, object recognition, image classification, semantic segmentation, video recognition, video classification, video segmentation, and the like. As another example, the machine learning task can be a multi-modal application, such as an application involving additional signals (e.g., visual signals), e.g., image captions, video captions, and the like.
Referring now to the drawings, example embodiments of the present disclosure will be discussed in more detail.
FIG. 1A depicts a block diagram of an example computing system 100 that performs computer vision with reduced computing costs and increased accuracy, according to an example embodiment of the disclosure. The system 100 includes a user computing device 102, a server computing system 130, and a training computing system 150 communicatively coupled by a network 180.
The user computing device 102 can be any type of computing device, such as, for example, a personal computing device (e.g., a laptop computer or desktop computer), a mobile computing device (e.g., a smart phone or tablet), a game console or controller, a wearable computing device, an embedded computing device, or any other type of computing device.
The user computing device 102 includes one or more processors 112 and memory 114. The one or more processors 112 can be any suitable processing device (e.g., processor core, microprocessor, ASIC, FPGA, controller, microcontroller, etc.), and can be an operatively connected processor or processors. The memory 114 can include one or more non-transitory computer-readable storage media such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, and the like, as well as combinations thereof. The memory 114 is capable of storing data 116 and instructions 118 that are executed by the processor 112 to cause the user computing device 102 to perform operations.
In some implementations, the user computing device 102 can store or include one or more machine learning models 120. For example, the machine learning model 120 can be or otherwise include various machine learning models, such as a neural network (e.g., deep neural network) or other types of machine learning models, including nonlinear models and/or linear models. The neural network can include a feed forward neural network, a recurrent neural network (e.g., a long and short term memory recurrent neural network), a convolutional neural network, or other form of neural network. Some example machine learning models can utilize an attention mechanism such as self-attention. For example, some example machine learning models can include a multi-headed self-attention model (e.g., a transformer model). An example machine learning model 120 (e.g., a coanet model) is discussed with reference to fig. 2-3.
In some implementations, one or more machine learning models 120 can be received from the server computing system 130 over the network 180, stored in the user computing device memory 114, and then used or otherwise implemented by the one or more processors 112. In some implementations, the user computing device 102 can implement multiple parallel instances of a single machine learning model 120 (to perform parallel computer vision across multiple instances of the coanet model).
Additionally or alternatively, one or more machine learning models 140 can be included in the server computing system 130 in communication with the user computing device 102 according to client server relationships or otherwise stored and implemented by the server computing system 130. For example, the machine learning model 140 can be implemented by the server computing system 140 as part of a web service (e.g., a computer vision service such as image classification, service). In this way, one or more models 120 can be stored and implemented at the user computing device 102 and/or one or more models 140 can be stored and implemented at the server computing system 130.
The user computing device 102 can also include one or more user input components 122 that receive user input. For example, the user input component 122 can be a touch-sensitive component (e.g., a touch-sensitive display screen or touchpad) that is sensitive to a touch (e.g., a finger or stylus) of a user input object. The touch sensitive component can be used to implement a virtual keyboard. Other example user input components include a microphone, a conventional keyboard, or other device by which a user can provide user input.
The server computing system 130 includes one or more processors 132 and memory 134. The one or more processors 132 can be any suitable processing device (e.g., a processor core, microprocessor, ASIC, FPGA, controller, microcontroller, etc.), and can be an operatively connected processor or processors. Memory 134 can include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, and the like, as well as combinations thereof. The memory 134 is capable of storing data 136 and instructions 138 that are executed by the processor 132 to cause the server computing system 130 to perform operations.
In some implementations, the server computing system 130 includes or is otherwise implemented by one or more server computing devices. Where the server computing system 130 includes multiple server computing devices, such server computing devices are capable of operating in accordance with a sequential computing architecture, a parallel computing architecture, or some combination thereof.
As described above, the server computing system 130 can store or otherwise include one or more machine learning models 140. For example, model 140 can be or otherwise include various machine learning models. Example machine learning models include neural networks or other multi-layer nonlinear models. Example neural networks include feed forward neural networks, deep neural networks, recurrent neural networks, and convolutional neural networks. Some example machine learning models can utilize an attention mechanism such as self-attention. For example, some example machine learning models can include a multi-headed self-attention model (e.g., a transformer model). An example model 140 is discussed with reference to fig. 2-3.
The user computing device 102 and/or the server computing system 130 can train the models 120 and/or 140 via interactions with a training computing system 150 communicatively coupled via a network 180. The training computing system 150 can be separate from the server computing system 130 or can be part of the server computing system 130.
The training computing system 150 includes one or more processors 152 and memory 154. The one or more processors 152 can be any suitable processing device (e.g., processor core, microprocessor, ASIC, FPGA, controller, microcontroller, etc.), and can be an operatively connected processor or processors. The memory 154 can include one or more non-transitory computer-readable storage media such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, and the like, as well as combinations thereof. The memory 154 is capable of storing data 156 and instructions 158 that are executed by the processor 152 to cause the training computing system 150 to perform operations. In some implementations, the training computing system 150 includes or is otherwise implemented by one or more server computing devices.
Training computing system 150 can include a model trainer 160, which model trainer 160 trains machine learning models 120 and/or 140 stored at user computing device 102 and/or server computing system 130 using various training or learning techniques, such as, for example, error back propagation. For example, the loss function can be counter-propagated through the model to update one or more parameters of the model (e.g., a gradient based on the loss function). Various loss functions can be used, such as mean square error, likelihood loss, cross entropy loss, hinge loss, and/or various other loss functions. Gradient descent techniques can be used to iteratively update parameters through multiple training iterations.
In some implementations, performing error back-propagation can include performing truncated back-propagation over time. Model trainer 160 can perform a variety of generalization techniques (e.g., weight decay, sag, etc.) to enhance the generalization ability of the trained model.
In particular, model trainer 160 can train machine learning models 120 and/or 140 based on a set of training data 162. The training data 162 can include, for example, a corpus or other data set of task-specific training data, such as an image classification database (e.g., imageNet, JFT 300M, etc.).
In some implementations, if the user has provided permission, the training examples can be provided by the user computing device 102. Thus, in such embodiments, the model 120 provided to the user computing device 102 can be trained by the training computing system 150 based on user-specific data received from the user computing device 102. In some cases, this process can be referred to as personalizing the model.
Model trainer 160 includes computer logic for providing the required functionality. Model trainer 160 can be implemented in hardware, firmware, and/or software that controls a general purpose processor. For example, in some implementations, model trainer 160 includes program files stored on a storage device, loaded into memory, and executed by one or more processors. In other implementations, model trainer 160 includes one or more sets of computer-executable instructions stored in a tangible computer-readable storage medium, such as RAM, a hard disk, or an optical or magnetic medium.
The network 180 can be any type of communication network, such as a local area network (e.g., an intranet), a wide area network (e.g., the internet), or some combination thereof, and can include any number of wired or wireless links. In general, communications over network 180 can be carried via any type of wired and/or wireless connection using various communication protocols (e.g., TCP/IP, HTTP, SMTP, FTP), coding or formats (e.g., HTML, XML), and/or protection schemes (e.g., VPN, secure HTTP, SSL).
The machine learning model described in this specification may be used for various tasks, applications, and/or use cases.
In some implementations, the input to the machine learning model of the present disclosure can be image data. The machine learning model is capable of processing the image data to generate an output. As an example, the machine learning model can process the image data to generate an image recognition output (e.g., recognition of the image data, potential embedding of the image data, encoded representation of the image data, hashing of the image data, etc.). As another example, the machine learning model can process image data to generate an image segmentation output. As another example, the machine learning model can process image data to generate an image classification output. As another example, the machine learning model can process the image data to generate an image data modification output (e.g., a change in the image data, etc.). As another example, the machine learning model can process the image data to generate an encoded image data output (e.g., an encoded and/or compressed representation of the image data, etc.). As another example, the machine learning model can process image data to generate an amplified image data output. As another example, the machine learning model can process image data to generate a prediction output.
In some implementations, the input of the machine learning model of the present disclosure can be text or natural language data. The machine learning model is capable of processing text or natural language data to generate an output. As an example, the machine learning model can process natural language data to generate a language encoded output. As another example, the machine learning model can process text or natural language data to generate a potential text-embedded output. As another example, the machine learning model can process text or natural language data to generate a translation output. As another example, the machine learning model can process text or natural language data to generate a classification output. As another example, the machine learning model can process text or natural language data to generate a text segmentation output. As another example, the machine learning model can process text or natural language data to generate semantic intent output. As another example, the machine learning model can process text or natural language data to generate an enlarged text or natural language output (e.g., higher quality text or natural language data than the input text or natural language, etc.). As another example, the machine learning model can process text or natural language data to generate a predictive output.
In some implementations, the input to the machine learning model of the present disclosure can be speech data. The machine learning model is capable of processing speech data to generate an output. As an example, the machine learning model can process speech data to generate a speech recognition output. As another example, the machine learning model can process speech data to generate speech translation output. As another example, the machine learning model can process speech data to generate a potentially embedded output. As another example, the machine learning model can process the speech data to generate an encoded speech output (e.g., an encoded and/or compressed representation of the speech data, etc.). As another example, the machine learning model can process the speech data to generate an amplified speech output (e.g., higher quality speech data than the input speech data, etc.). As another example, the machine learning model can process the speech data to generate a textual representation output (e.g., a textual representation of the input speech data, etc.). As another example, the machine learning model can process speech data to generate a predictive output.
In some implementations, the input of the machine learning model of the present disclosure can be potentially encoded data (e.g., a potential spatial representation of the input, etc.). The machine learning model is capable of processing the potentially encoded data to generate an output. As an example, the machine learning model can process the potentially encoded data to generate the recognition output. As another example, the machine learning model can process the potentially encoded data to generate a reconstructed output. As another example, the machine learning model can process the potentially encoded data to generate a search output. As another example, the machine learning model can process the potentially encoded data to generate a reaggregation output. As another example, the machine learning model can process the potentially encoded data to generate a prediction output.
In some implementations, the input to the machine learning model of the present disclosure can be statistical data. The statistical data can be, represent, or otherwise include data calculated and/or computed from some other data source. The machine learning model is capable of processing the statistical data to generate an output. As an example, the machine learning model can process the statistical data to generate an identification output. As another example, the machine learning model can process the statistical data to generate a prediction output. As another example, the machine learning model can process the statistical data to generate a classification output. As another example, the machine learning model can process the statistical data to generate a segmentation output. As another example, the machine learning model can process the statistical data to generate a visual output. As another example, the machine learning model can process the statistical data to generate a diagnostic output.
In some implementations, the input to the machine learning model of the present disclosure can be sensor data. The machine learning model is capable of processing the sensor data to generate an output. As one example, the machine learning model can process the sensor data to generate an identification output. As another example, the machine learning model can process the sensor data to generate a prediction output. As another example, the machine learning model can process the sensor data to generate a classification output. As another example, the machine learning model can process the sensor data to generate a segmented output. As another example, the machine learning model can process the sensor data to generate visual output. As another example, the machine learning model can process the sensor data to generate a diagnostic output. As another example, the machine learning model can process the sensor data to generate a detection output.
In some cases, the machine learning model can be configured to perform tasks that include encoding input data for reliable and/or efficient transmission or storage (and/or corresponding decoding). For example, the task may be an audio compression task. The input may comprise audio data and the output may comprise compressed audio data. In another example, the input includes visual data (e.g., one or more images or videos), the output includes compressed visual data, and the task is a visual data compression task. In another example, the task may include generating an embedding for input data (e.g., input audio or visual data).
In some cases, the input includes visual data and the task is a computer visual task. In some cases, the input includes pixel data for one or more images and the task is an image processing task. For example, the image processing task can be an image classification, wherein the output is a set of scores, each score corresponding to a different object class and representing a likelihood that one or more images describe an object belonging to the object class. The image processing task may be object detection, wherein the image processing output identifies one or more regions in the one or more images, and for each region, the region describes a likelihood of the object of interest. As another example, the image processing task can be image segmentation, wherein the image processing output defines a respective likelihood for each category in the predetermined set of categories for each pixel in the one or more images. For example, the set of categories can be foreground and background. As another example, the set of categories can be object classes. As another example, the image processing task can be depth estimation, where the image processing output defines a respective depth value for each pixel in the one or more images. As another example, the image processing task may be motion estimation, where the network input includes a plurality of images, and the image processing output defines, for each pixel of one of the input images, a motion of a scene depicted at the pixel between the images in the network input.
In some cases, the input includes audio data representing a spoken utterance and the task is a speech recognition task. The output may include a text output mapped to the spoken utterance. In some cases, the task includes encrypting or decrypting the input data. In some cases, tasks include microprocessor performance tasks such as branch prediction or memory address translation.
FIG. 1A illustrates one example computing system that can be used to implement the present disclosure. Other computing systems can also be used. For example, in some implementations, the user computing device 102 can include a model trainer 160 and a training data set 162. In such an embodiment, the model 120 can be trained and used locally at the user computing device 102. In some such implementations, the user computing device 102 can implement the model trainer 160 to personalize the model 120 based on user-specific data.
Fig. 1B illustrates a block diagram of an example computing device 10, performed in accordance with an example embodiment of the present disclosure. Computing device 10 can be a user computing device or a server computing device.
Computing device 10 includes a plurality of applications (e.g., applications 1 through N). Each application contains its own machine learning library and machine learning model. For example, each application can include a machine learning model. Example applications include text messaging applications, email applications, dictation applications, virtual keyboard applications, browser applications, and the like.
As shown in fig. 1B, each application is capable of communicating with a plurality of other components of the computing device, such as, for example, one or more sensors, a context manager, a device status component, and/or additional components. In some implementations, each application can communicate with each device component using an API (e.g., public API). In some implementations, the APIs used by each application are specific to that application.
Fig. 1C illustrates a block diagram of an example computing device 50, performed in accordance with an example embodiment of the present disclosure. The computing device 50 can be a user computing device or a server computing device.
Computing device 50 includes a plurality of applications (e.g., applications 1 through N). Each application communicates with a central intelligent layer. Example applications include text messaging applications, email applications, dictation applications, virtual keyboard applications, browser applications, and the like. In some implementations, each application can communicate with the central intelligence layer (and the models stored therein) using APIs (e.g., public APIs across all applications).
The central intelligence layer includes a plurality of machine learning models. For example, as shown in FIG. 1C, a respective machine learning model can be provided for each application and managed by a central intelligent layer. In other implementations, two or more applications can share a single machine learning model. For example, in some embodiments, the central intelligence layer can provide a single model for all applications. In some implementations, the central intelligence layer is included in the operating system of the computing device 50 or otherwise implemented by the operating system of the computing device 50.
The central intelligence layer is capable of communicating with the central device data layer. The central device data layer can be a centralized data repository for computing devices 50. As shown in fig. 1C, the central device data layer can communicate with a plurality of other components of the computing device, such as, for example, one or more sensors, a context manager, a device status component, and/or an add-on component. In some implementations, the central device data layer can communicate with each device component using an API (e.g., a proprietary API).
Fig. 2 depicts a block diagram of an example convolved attention network (coanet) model 200, according to an example embodiment of the disclosure. In some implementations, the model 200 is trained to receive a set of input data 202 that describes, for example, image data or other task-specific input data, and as a result of receiving the input data 202, provide output data 204 that is responsive to a particular machine learning task, such as a computer vision task (e.g., image classification).
According to example aspects of the disclosure, the model 200 can include a downsampling stage 210. The downsampling stage 210 is capable of reducing the spatial resolution of the input data 202. For example, if the input data 202 includes a tensor, the downsampling stage 210 may reduce the spatial resolution such that the output of the downsampling stage 210 has at least one dimension or resolution that is lower than at least one dimension or resolution of the tensor of the input data 202. Additionally and/or alternatively, the downsampling stage 210 may increase the number of channels relative to the input data. In some implementations, the downsampling stage 210 can be or can include a convolution trunk. The convolution trunk can have a positive stride, for example a stride greater than 10.
Additionally and/or alternatively, the model 200 can include one or more attention blocks 212. The attention block 212 is capable of receiving downsampled input data from the downsampling stage 202 and generating output data 204. The attention block 212 can implement a relative attention mechanism. In some implementations, the attention block 212 can be a transducer block that operates similar to a transducer network.
According to example aspects of the present disclosure, the attention block 212 can include a relative attention mechanism. The relative attention mechanism can include a sum of a static convolution kernel and an adaptive attention matrix. The sum can be applied before and/or after SoftMax normalization by the relative attention mechanism. As one example, the relative attention mechanism (e.g., applied prior to SoftMax normalization) may be mathematically represented by:
as another example, the relative attention mechanism (e.g., applied after SoftMax normalization) may be mathematically represented by:
in the above equation, the depth convolution kernel w i-j Is an input-independent parameter for a static value of a given index in the input tensor (i, j) (e.g., a relative shift between indexes i-j, where the dependence on the relative shift, rather than a particular value, is referred to as a translational equivalence, which can improve generalization under a finite-sized dataset), x i And x j Input and output at location i, respectively, and g is a global receptive field (e.g., the entire set of locations).
The use of global receptive fields (e.g., as opposed to limited local receptive fields traditionally employed in convolutional networks) can provide improved capabilities to capture complex relational interactions between different spatial locations, which can be desirable when dealing with higher-level concepts. Denominator terms can also be referred to as the attention weight a i,j . The attention weights can be jointly determined by the depth convolution kernel and the translational equivalence of the input-adaptive input-output pairs, which can provide two characteristics of varying degrees, improving generalization, capacity, and/or accuracy of the model.
Fig. 3 depicts a block diagram of an example convolved attention network (coanet) model 300, in accordance with an example embodiment of the present disclosure. In some implementations, the model 300 is trained to receive a set of input data 202 that describes, for example, image data or other task-specific input data, and as a result of receiving the input data 202, provide output data 204 that is responsive to a particular machine learning task, such as a computer vision task (e.g., image classification).
The machine learning convolution attention network 300 can include two or more network stages (e.g., 302, 304, 306, 308, and 310). Each of the two or more network phases can be or can include one of an attention phase or a convolution phase such that the convolution phase is sequentially preceding the attention phase. As one example, in some implementations, the two or more network phases can include an S0 phase 302, an S1 phase 304, an S2 phase 306, an S3 phase 308, and an S4 phase 310. Each of these phases can be a convolution phase comprising one or more convolution blocks (e.g., MBConv blocks) or an attention phase comprising one or more attention blocks with a relative attention mechanism. As another example, in some implementations, the convolution block is capable of performing a depth-separable convolution (e.g., over multiple channels). Additionally and/or alternatively, in some implementations, the convolution block can perform an inverse bottleneck convolution. In some embodiments, the spatial resolution gradually decreases over two or more network phases. In some implementations, the number of channels can be increased (e.g., doubled) at any stage (such as at least one of S1 stage 304, S2 stage 306, S3 stage 308, or S4 stage 310).
In some embodiments, S0 stage 302 includes a two-layer convolutional dry-line network. Additionally and/or alternatively, the S1 stage 304 can include one or more convolution blocks with squeeze excitation. The one or more convolution blocks of the S1 stage and/or other convolution stages can include an MBConv block. The MBConv block can be configured to expand the channel size from the original channel size of the input of the one or more convolution blocks and then project the expanded channel size back to the original channel size. As another example, in some implementations, the convolution block is capable of performing a depth-separable convolution (e.g., over multiple channels). Additionally and/or alternatively, in some implementations, the convolution block can perform an inverse bottleneck convolution. In some embodiments, the width of S0 stage 302 is less than or equal to the width of S1 stage 304. In some implementations, each of the S0 stage 302, S1 stage 304, and S4 stage 310 (e.g., exactly) includes two blocks, and each of the S2 stage 306 and S3 stage 308 includes more than two blocks. For example, in one particular embodiment, the two or more network phases include an S0 phase 302, an S1 phase 304, an S2 phase 306, an S3 phase 308, and an S4 phase 310, the S0 phase 302 includes a two-layer convolutional backbone network, the S1 phase 304 includes a convolutional block with extrusion excitation, the S2 phase 306 includes a convolutional block, the S3 phase 308 includes an attention block, and the S4 phase 310 includes an attention block, wherein each of the S3 phase 308 and the S4 phase 310 includes a relative attention mechanism configured to determine a sum of a static convolutional kernel and an adaptive attention matrix.
The attention block and/or stage (e.g., S3 and/or S4 stages 308, 310) can include a relative attention mechanism according to example aspects of the present disclosure. The relative attention mechanism can include a sum of a static convolution kernel and an adaptive attention matrix. The sum can be applied before and/or after SoftMax normalization by the relative attention mechanism. As one example, the relative attention mechanism (e.g., applied prior to SoftMax normalization) may be mathematically represented by:
as another example, the relative attention mechanism (e.g., applied after SoftMax normalization) may be mathematically represented by:
in the above equation, the depth convolution kernel w i-j Is an input-independent parameter for a static value of a given index in the input tensor (i, j) (e.g., a relative shift between indexes i-j, where the dependence on the relative shift, rather than a particular value, is referred to as a translational equivalence, which can improve generalization under a finite-sized dataset), x i And x j Input and output at location i, respectively, and g is a global receptive field (e.g., the entire set of locations).
Fig. 4 depicts a block diagram of an example convolved attention network (coanet) model 400, in accordance with an example embodiment of the present disclosure. As shown in FIG. 4, the model 400 can include S0, S1, S2, S3, and S4 phases. For example, the S0 phase or trunk phase can include two (e.g., 3 x 3) convolutional layers (e.g., with a stride of 2). Additionally, the convolutions S1 stage and S2 stage can each include a 1 x 1 convolutions layer, a 3 x 3 deconvolutions layer, and a 1 x 1 convolutions layer. Additionally, the attention (e.g., S3 and S4) phases can each include a relative attention mechanism and a feed forward network. The model can additionally include a global pooling layer and a fully connected layer to produce a model output. Each stage can be repeated up to the designed number of times.
Fig. 5 depicts a flowchart of an example method performed in accordance with an example embodiment of the present disclosure. Although fig. 5 depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particular illustrated order or arrangement. The various steps of method 500 may be omitted, rearranged, combined, and/or modified in various ways without departing from the scope of the present disclosure.
The method 500 can include, at 502, obtaining (e.g., by a computing system including one or more computing devices) input data including an input tensor having one or more dimensions. For example, the input tensor can be a two-dimensional tensor having a length and/or a width. Additionally and/or alternatively, the input tensor can have one or more channels. In some implementations, for example, the input tensor may be or include image data, such as an image having a length, a width, and/or a plurality of color channels.
The method 500 can include, at 504, providing input data to a machine learning convolution attention network (e.g., by a computing system). The machine learning convolution attention network can be any suitable network in accordance with example aspects of the present disclosure, such as networks 200 and/or 300 of fig. 2 and/or 3.
For example, in some implementations, the machine-learning convolution attention network can include two or more network phases, each of the two or more network phases including one of an attention phase or a convolution phase, such that the convolution phase is sequentially preceding the attention phase. As one example, in some embodiments, the two or more network phases can include an S0 phase, an S1 phase, an S2 phase, an S3 phase, and an S4 phase. Each of these phases can be a convolution phase comprising one or more convolution blocks (e.g., MBConv blocks) or an attention phase comprising one or more attention blocks with a relative attention mechanism. In some embodiments, the spatial resolution gradually decreases over two or more network phases. In some embodiments, the number of channels can be increased (e.g., doubled) at any stage (e.g., at least one of the S1, S2, S3, or S4 stages).
In some embodiments, the S0 stage includes a two-layer convolutional dry-line network. Additionally and/or alternatively, the S1 stage can include one or more convolution blocks with squeeze excitation. The one or more convolution blocks of the S1 stage and/or other convolution stages can include an MBConv block. The MBConv block can be configured to expand the channel size from the original channel size of the input of the one or more convolution blocks and then project the expanded channel size back to the original channel size. As another example, in some implementations, the convolution block is capable of performing a depth-separable convolution (e.g., over multiple channels). Additionally and/or alternatively, in some implementations, the convolution block can perform an inverse bottleneck convolution. In some embodiments, the width of the S0 stage is less than or equal to the width of the S1 stage. In some embodiments, each of the S0, S1, and S5 phases (e.g., exactly) includes two blocks, and each of the S2 and S3 phases includes more than two blocks. For example, in one particular embodiment, the two or more network phases include an S0 phase, an S1 phase, an S2 phase, an S3 phase, and an S4 phase, the S0 phase including a two-layer convolutional dry line network, the S1 phase including convolutions with extrusion excitation, the S2 phase including convolutions, the S3 phase including attentives, and the S4 phase including attentives, wherein each of the S3 and S4 phases includes a relative attentiveness mechanism configured to determine a sum of the static convolution kernel and the adaptive attentiveness matrix.
As another example, in some implementations, the machine-learned convolutional attention network can include a downsampling stage configured to reduce spatial resolution relative to the input tensor and one or more attention blocks including a relative attention mechanism. The downsampling stage can reduce spatial resolution to improve the feasibility of performing computations. For example, if the input data includes a tensor, the downsampling stage may reduce the spatial resolution such that the output of the downsampling stage has at least one dimension or resolution that is lower than the at least one dimension or resolution of the tensor of the input data. Additionally and/or alternatively, the downsampling stage may increase the number of channels relative to the input data. In some implementations, the downsampling stage can be or can include a convolution trunk. The convolution trunk can have a positive stride, for example a stride greater than 10.
The attention block and/or stage (e.g., S3 and/or S4 stage) can include a relative attention mechanism according to example aspects of the present disclosure. The relative attention mechanism can include a sum of a static convolution kernel and an adaptive attention matrix. The sum can be applied before and/or after SoftMax normalization by the relative attention mechanism. As one example, the relative attention mechanism (e.g., applied prior to SoftMax normalization) may be mathematically represented by:
As another example, the relative attention mechanism (e.g., applied after SoftMax normalization) may be mathematically represented by:
in the above equation, the depth convolution kernel w i-j Is an input-independent parameter for a static value of a given index in the input tensor (i, j) (e.g., a relative shift between indexes j-j, where the dependence on the relative shift, rather than a particular value, is referred to as a translational equivalence, which can improve generalization under a finite-sized dataset), x i And x j Input and output at location i, respectively, and g is a global receptive field (e.g., the entire set of locations).
The method 500 can include, at 506, receiving, by the computing system, a machine learning prediction from the machine learning convolutional attention network in response to providing the input data to the machine learning convolutional attention network. The machine learning prediction can be a task-specific machine learning prediction. As an example, the output can be a computer vision output, such as a classification output (e.g., a classification vector), an object recognition output, and so forth. Alternatively, the machine learning prediction can be an intermediate prediction or representation, such as embedding in a potential or learning space.
The techniques discussed herein refer to servers, databases, software applications, and other computer-based systems, as well as actions taken and information sent to and received from such systems. The inherent flexibility of computer-based systems allows for a variety of possible configurations, combinations, and divisions of tasks and functions between components. For example, the processes discussed herein can be implemented using a single device or component or multiple devices or components working in combination. The database and application can be implemented on a single system or can be distributed across multiple systems. The distributed components can operate sequentially or in parallel.
While the present subject matter has been described in detail with reference to various specific example embodiments thereof, each example is provided by way of explanation and not limitation of the present disclosure. Alterations, modifications and equivalents of these embodiments will readily occur to those skilled in the art after having appreciated the foregoing description. Accordingly, this subject disclosure does not preclude inclusion of such modifications, variations and/or additions to the present subject matter as would be readily apparent to one of ordinary skill in the art. For instance, features illustrated or described as part of one embodiment, can be used with another embodiment to yield a still further embodiment. Accordingly, the present disclosure is intended to cover such alternatives, modifications, and equivalents.
Claims (29)
1. A computer-implemented method of performing computer vision with reduced computational cost and increased accuracy, the method comprising:
obtaining, by a computing system comprising one or more computing devices, input data comprising an input tensor having one or more dimensions;
providing, by the computing system, the input data to a machine-learned convolutional attention network comprising two or more network phases including one or more attention phases and one or more convolution phases, wherein at least one of the one or more attention phases includes a relative attention mechanism configured to determine a sum of a static convolution kernel and an adaptive attention matrix; and
In response to providing the input data to the machine-learning convolution attention network, a machine-learning prediction is received by the computing system from the machine-learning convolution attention network.
2. The computer-implemented method of any preceding claim, wherein the two or more network phases comprise an S0 phase, an S1 phase, an S2 phase, an S3 phase, and an S4 phase.
3. The computer-implemented method of claim 2, wherein the S0 phase comprises a two-layer convolutional dry-line network.
4. A computer-implemented method according to claim 2 or 3, wherein the S1 stage comprises one or more convolution blocks with squeeze excitation.
5. The computer-implemented method of claim 4, wherein the one or more convolution blocks of the S1 stage comprise an MBConv block configured to expand a channel size from an original channel size of an input of the one or more convolution blocks and then project the expanded channel size back to the original channel size.
6. The computer-implemented method of any of claims 2-5, wherein each of the S2, S3, or S4 stages comprising a convolution stage comprises an MBConv block.
7. The computer-implemented method of any of claims 2-6, wherein a number of lanes is doubled for at least one of the S1 stage, the S2 stage, the S3 stage, or the S4 stage.
8. The computer-implemented method of any of claims 2-7, wherein a width of the S0 stage is less than or equal to a width of the S1 stage.
9. The computer-implemented method of any of claims 2-8, wherein each of the S0 phase, the S1 phase, and the S5 phase comprises two blocks, and wherein each of the S2 phase and the S3 phase comprises more than two blocks.
10. The computer-implemented method of any preceding claim, wherein spatial resolution gradually decreases over the two or more network phases.
11. The computer-implemented method of any preceding claim, wherein the sum of the static convolution kernel and the adaptive attention matrix is applied prior to SoftMax normalization by the relative attention mechanism.
12. The computer-implemented method of any preceding claim, wherein the input data comprises image data.
13. The computer-implemented method of any preceding claim, wherein the machine-learning prediction comprises a computer vision output.
14. The computer-implemented method of any preceding claim, wherein the machine-learned prediction comprises a classification output.
15. The computer-implemented method of any preceding claim, wherein the one or more convolution stages precede the one or more attention stages of the two or more network stages sequentially.
16. A computer-implemented method of performing computer vision with reduced computational cost and increased accuracy, the method comprising:
obtaining, by a computing system comprising one or more computing devices, input data comprising an input tensor having one or more dimensions;
providing, by the computing system, the input data to a machine-learning convolution attention network, the machine-learning convolution attention network comprising:
a downsampling stage configured to reduce a spatial resolution relative to the input tensor; and
one or more attention blocks comprising a relative attention mechanism configured to determine a sum of a static convolution kernel and an adaptive attention matrix;
In response to providing the input data to the machine-learning convolution attention network, a machine-learning prediction is received by the computing system from the machine-learning convolution attention network.
17. The computer-implemented method of claim 16, wherein the downsampling stage comprises a convolutional trunk.
18. The computer-implemented method of claim 16 or 17, wherein the convolution trunk has a stride greater than 10.
19. The computer-implemented method of any of claims 16-18, wherein the input data comprises image data.
20. The computer-implemented method of any of claims 16-19, wherein the machine-learning prediction comprises a computer vision output.
21. The computer-implemented method of any of claims 16-20, wherein the machine learning prediction includes a classification output.
22. The computer-implemented method of any of claims 16-21, wherein the sum of the static convolution kernel and the adaptive attention matrix is applied prior to SoftMax normalization by the relative attention mechanism.
23. A computer-implemented method of performing computer vision with reduced computational cost and increased accuracy, the method comprising:
Obtaining, by a computing system comprising one or more computing devices, input data comprising an input tensor having one or more dimensions;
providing, by the computing system, the input data to a machine-learning convolution attention network, the machine-learning convolution attention network comprising a plurality of network phases including:
s0, wherein the S0 comprises a two-layer convolution trunk network;
an S1 stage, wherein the S1 stage comprises a convolution block with extrusion excitation;
s2, wherein the S2 comprises a convolution block;
an S3 stage, the S3 stage comprising an attention block;
an S4 stage, the S4 stage comprising an attention block; and
wherein each of the S3 phase and the S4 phase includes a relative attention mechanism configured to determine a sum of a static convolution kernel and an adaptive attention matrix;
wherein spatial resolution decreases at each of the plurality of network phases;
wherein the number of channels increases at each of the plurality of network phases; and
in response to providing the input data to the machine-learning convolution attention network, a machine-learning prediction is received by the computing system from the machine-learning convolution attention network.
24. The computer-implemented method of claim 23, wherein the input data comprises image data.
25. The computer-implemented method of claim 23 or 24, wherein the machine-learning prediction comprises a computer vision output.
26. The computer-implemented method of claim 23 or 24 or 25, wherein the machine-learned prediction comprises a classification output.
27. The computer-implemented method of claim 23 or 24 or 25 or 26, wherein the sum of the static convolution kernel and the adaptive attention matrix is applied prior to SoftMax normalization by the relative attention mechanism.
28. A system comprising one or more processors configured to perform the method of any of the preceding claims.
29. One or more non-transitory computer-readable media storing instructions that, when executed by one or more processors, cause the one or more processors to implement the method of any of claims 1-27.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202163194077P | 2021-05-27 | 2021-05-27 | |
US63/194,077 | 2021-05-27 | ||
PCT/US2022/031304 WO2022251602A1 (en) | 2021-05-27 | 2022-05-27 | Systems and methods for machine-learned models having convolution and attention |
Publications (1)
Publication Number | Publication Date |
---|---|
CN117377983A true CN117377983A (en) | 2024-01-09 |
Family
ID=82115984
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202280026409.3A Pending CN117377983A (en) | 2021-05-27 | 2022-05-27 | System and method for machine learning model with convolution and attention |
Country Status (5)
Country | Link |
---|---|
US (2) | US11755883B2 (en) |
EP (1) | EP4288939A1 (en) |
JP (1) | JP2024517056A (en) |
CN (1) | CN117377983A (en) |
WO (1) | WO2022251602A1 (en) |
Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN110189334A (en) * | 2019-05-28 | 2019-08-30 | 南京邮电大学 | The medical image cutting method of the full convolutional neural networks of residual error type based on attention mechanism |
US20210064955A1 (en) * | 2019-09-03 | 2021-03-04 | Here Global B.V. | Methods, apparatuses, and computer program products using a repeated convolution-based attention module for improved neural network implementations |
CN112464792A (en) * | 2020-11-25 | 2021-03-09 | 北京航空航天大学 | Remote sensing image ship target fine-grained classification method based on dynamic convolution |
US20210142106A1 (en) * | 2019-11-13 | 2021-05-13 | Niamul QUADER | Methods and systems for training convolutional neural network using built-in attention |
Family Cites Families (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP6936592B2 (en) * | 2017-03-03 | 2021-09-15 | キヤノン株式会社 | Arithmetic processing unit and its control method |
-
2022
- 2022-05-27 US US17/827,130 patent/US11755883B2/en active Active
- 2022-05-27 WO PCT/US2022/031304 patent/WO2022251602A1/en active Application Filing
- 2022-05-27 JP JP2023557195A patent/JP2024517056A/en active Pending
- 2022-05-27 EP EP22731945.6A patent/EP4288939A1/en active Pending
- 2022-05-27 CN CN202280026409.3A patent/CN117377983A/en active Pending
-
2023
- 2023-07-19 US US18/355,243 patent/US20230359862A1/en active Pending
Patent Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN110189334A (en) * | 2019-05-28 | 2019-08-30 | 南京邮电大学 | The medical image cutting method of the full convolutional neural networks of residual error type based on attention mechanism |
US20210064955A1 (en) * | 2019-09-03 | 2021-03-04 | Here Global B.V. | Methods, apparatuses, and computer program products using a repeated convolution-based attention module for improved neural network implementations |
US20210142106A1 (en) * | 2019-11-13 | 2021-05-13 | Niamul QUADER | Methods and systems for training convolutional neural network using built-in attention |
CN112464792A (en) * | 2020-11-25 | 2021-03-09 | 北京航空航天大学 | Remote sensing image ship target fine-grained classification method based on dynamic convolution |
Non-Patent Citations (1)
Title |
---|
JONGCHAN PARK 等: "A Simple and Light-Weight Attention Module for Convolutional Neural Networks", 《INTERNATIONAL JOURNAL OF COMPUTER VISION》, 28 January 2020 (2020-01-28) * |
Also Published As
Publication number | Publication date |
---|---|
US20230359862A1 (en) | 2023-11-09 |
JP2024517056A (en) | 2024-04-19 |
US20220383069A1 (en) | 2022-12-01 |
WO2022251602A1 (en) | 2022-12-01 |
EP4288939A1 (en) | 2023-12-13 |
US11755883B2 (en) | 2023-09-12 |
WO2022251602A9 (en) | 2023-09-07 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11361546B2 (en) | Action recognition in videos using 3D spatio-temporal convolutional neural networks | |
US20230359865A1 (en) | Modeling Dependencies with Global Self-Attention Neural Networks | |
US11875269B2 (en) | Large scale generative neural network model with inference for representation learning using adversarial training | |
EP4033412A2 (en) | Method and apparatus with neural network training | |
KR20220148274A (en) | Self-supervised representation learning using bootstrapped latent representations | |
US20230274527A1 (en) | Systems and Methods for Training Multi-Class Object Classification Models with Partially Labeled Training Data | |
Huttunen | Deep neural networks: A signal processing perspective | |
WO2022019913A1 (en) | Systems and methods for generation of machine-learned multitask models | |
Yi et al. | Elanet: effective lightweight attention-guided network for real-time semantic segmentation | |
US11948090B2 (en) | Method and apparatus for video coding | |
US20230053618A1 (en) | Recurrent unit for generating or processing a sequence of images | |
US20230351203A1 (en) | Method for knowledge distillation and model genertation | |
CN115186825A (en) | Full attention with sparse computational cost | |
CN117377983A (en) | System and method for machine learning model with convolution and attention | |
US20220245428A1 (en) | Machine-Learned Attention Models Featuring Omnidirectional Processing | |
US20220245917A1 (en) | Systems and methods for nearest-neighbor prediction based machine learned models | |
US20240135187A1 (en) | Method for Training Large Language Models to Perform Query Intent Classification | |
CN113365072B (en) | Feature map compression method and device, computing equipment and storage medium | |
US20230419082A1 (en) | Improved Processing of Sequential Data via Machine Learning Models Featuring Temporal Residual Connections | |
US20220245432A1 (en) | Machine-Learned Attention Models Featuring Echo-Attention Layers | |
US20230394306A1 (en) | Multi-Modal Machine Learning Models with Improved Computational Efficiency Via Adaptive Tokenization and Fusion | |
US20230419721A1 (en) | Electronic device for improving quality of image and method for improving quality of image by using same | |
WO2024020107A1 (en) | Task-specific prompt recycling for machine-learned models that perform multiple tasks | |
WO2024035416A1 (en) | Machine-learned models for multimodal searching and retrieval of images | |
CN115803753A (en) | Multi-stage machine learning model synthesis for efficient reasoning |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |