CN117980896A - Automatic generation of immersive interfaces - Google Patents
Automatic generation of immersive interfaces Download PDFInfo
- Publication number
- CN117980896A CN117980896A CN202180102530.5A CN202180102530A CN117980896A CN 117980896 A CN117980896 A CN 117980896A CN 202180102530 A CN202180102530 A CN 202180102530A CN 117980896 A CN117980896 A CN 117980896A
- Authority
- CN
- China
- Prior art keywords
- content
- immersive
- immersive interface
- text content
- text
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000000034 method Methods 0.000 claims abstract description 68
- 230000000007 visual effect Effects 0.000 claims abstract description 59
- 230000004044 response Effects 0.000 claims description 21
- 238000009877 rendering Methods 0.000 claims description 3
- 238000004590 computer program Methods 0.000 abstract description 5
- 230000000694 effects Effects 0.000 description 36
- 230000008569 process Effects 0.000 description 31
- 238000003860 storage Methods 0.000 description 21
- 238000012545 processing Methods 0.000 description 19
- 201000004792 malaria Diseases 0.000 description 13
- 230000015654 memory Effects 0.000 description 13
- 238000004458 analytical method Methods 0.000 description 12
- 238000007726 management method Methods 0.000 description 12
- 238000010411 cooking Methods 0.000 description 11
- 230000008520 organization Effects 0.000 description 11
- 230000008901 benefit Effects 0.000 description 8
- 230000003993 interaction Effects 0.000 description 8
- 238000004891 communication Methods 0.000 description 7
- 239000000284 extract Substances 0.000 description 7
- 230000004048 modification Effects 0.000 description 6
- 238000012986 modification Methods 0.000 description 6
- 241001465754 Metazoa Species 0.000 description 5
- 230000003190 augmentative effect Effects 0.000 description 5
- 238000013500 data storage Methods 0.000 description 5
- 238000003058 natural language processing Methods 0.000 description 5
- PXHVJJICTQNCMI-UHFFFAOYSA-N Nickel Chemical compound [Ni] PXHVJJICTQNCMI-UHFFFAOYSA-N 0.000 description 4
- 230000009471 action Effects 0.000 description 4
- 238000010586 diagram Methods 0.000 description 4
- 238000009826 distribution Methods 0.000 description 4
- 238000001914 filtration Methods 0.000 description 4
- 230000002452 interceptive effect Effects 0.000 description 4
- 230000009193 crawling Effects 0.000 description 3
- 238000005516 engineering process Methods 0.000 description 3
- 235000013305 food Nutrition 0.000 description 3
- 230000036541 health Effects 0.000 description 3
- 230000003287 optical effect Effects 0.000 description 3
- 230000001360 synchronised effect Effects 0.000 description 3
- 238000013519 translation Methods 0.000 description 3
- 230000014616 translation Effects 0.000 description 3
- 238000012384 transportation and delivery Methods 0.000 description 3
- 230000001413 cellular effect Effects 0.000 description 2
- 238000006243 chemical reaction Methods 0.000 description 2
- 235000014510 cooky Nutrition 0.000 description 2
- 238000001514 detection method Methods 0.000 description 2
- 230000006870 function Effects 0.000 description 2
- 238000003709 image segmentation Methods 0.000 description 2
- 230000010354 integration Effects 0.000 description 2
- 239000000203 mixture Substances 0.000 description 2
- 229910052759 nickel Inorganic materials 0.000 description 2
- 230000002085 persistent effect Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000004513 sizing Methods 0.000 description 2
- 230000003068 static effect Effects 0.000 description 2
- 230000009466 transformation Effects 0.000 description 2
- 230000001131 transforming effect Effects 0.000 description 2
- 230000007704 transition Effects 0.000 description 2
- 208000035473 Communicable disease Diseases 0.000 description 1
- 238000007792 addition Methods 0.000 description 1
- 230000003796 beauty Effects 0.000 description 1
- 230000006399 behavior Effects 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 230000015572 biosynthetic process Effects 0.000 description 1
- 239000003086 colorant Substances 0.000 description 1
- 238000013523 data management Methods 0.000 description 1
- 239000004744 fabric Substances 0.000 description 1
- 239000012634 fragment Substances 0.000 description 1
- 238000009432 framing Methods 0.000 description 1
- 230000007274 generation of a signal involved in cell-cell signaling Effects 0.000 description 1
- 238000007654 immersion Methods 0.000 description 1
- 230000006872 improvement Effects 0.000 description 1
- 208000015181 infectious disease Diseases 0.000 description 1
- 238000011835 investigation Methods 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 230000004807 localization Effects 0.000 description 1
- 230000014759 maintenance of location Effects 0.000 description 1
- 235000012054 meals Nutrition 0.000 description 1
- 238000005259 measurement Methods 0.000 description 1
- 238000002156 mixing Methods 0.000 description 1
- 230000006855 networking Effects 0.000 description 1
- 230000035764 nutrition Effects 0.000 description 1
- 235000016709 nutrition Nutrition 0.000 description 1
- 238000002360 preparation method Methods 0.000 description 1
- 230000005180 public health Effects 0.000 description 1
- 238000003786 synthesis reaction Methods 0.000 description 1
- 238000012549 training Methods 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/957—Browsing optimisation, e.g. caching or content distillation
- G06F16/9577—Optimising the visualization of content, e.g. distillation of HTML documents
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/34—Browsing; Visualisation therefor
- G06F16/345—Summarisation for human users
Abstract
The present disclosure provides systems, methods, and computer program products for performing automatic generation of an immersive interface. For example, the computing device may perform automatic generation of the immersive interface by: the method includes analyzing a web-based resource that includes text content, extracting a plurality of text content segments from the web-based resource, obtaining visual content and audio content associated with each respective text content segment of the plurality of text content segments, generating target content for an audiovisual display of the web-based resource based on combining at least a portion of each respective text content segment with the visual content and the audio content for the respective text content segment, and providing data describing the generated target content to a computing device for presenting the audiovisual display of the web-based resource.
Description
Technical Field
The present disclosure relates generally to computer systems. More particularly, the present disclosure relates to automatic generation of immersive interfaces for distributed computing devices via computer networks and internet services.
Background
Worldwide, more than 2.5 million cubic (quintillion) bytes of data are created per day. Technology providers store gigabytes of content that users can access through internet search engines and other tools. In addition, users perform over 50 hundred million searches worldwide per day to query and access various forms of content. Most of this information exists as long form text (long-form text) with little or no accompanying visual or audio content.
Content curation (content duration) generally refers to the collection of information related to a particular topic and the selection, organization, and presentation of such information in an interesting and meaningful way. Content curation is a time-consuming process requiring specific skills and specialized software. Furthermore, existing methods of content curation are expensive, typically consider only a small portion of the available content, and quickly become outdated in view of the increasing amount of new information generated daily. Furthermore, most content cannot be provided in different formats, languages, and interfaces, which would otherwise benefit various users around the world.
Disclosure of Invention
Aspects and advantages of embodiments of the disclosure will be set forth in part in the description which follows, or may be learned from the description, or may be learned by practice of the embodiments.
One example aspect of the disclosure relates to a system comprising one or more processors and one or more non-transitory computer-readable media storing instructions that, when executed by the one or more processors, cause the one or more processors to perform operations for automatically generating an immersive interface. For example, the operations may include analyzing a web-based resource that includes text content, extracting a plurality of text content segments (segments) from the web-based resource, obtaining visual content and audio content associated with each respective text content segment of the plurality of text content segments, generating target content for an audiovisual display of the web-based resource based on combining at least a portion of each respective text content segment with the visual content and the audio content for the respective text content segment, and providing data describing the generated target content to a computing device for presenting the audiovisual display of the web-based resource.
Another example aspect of the disclosure relates to a computer-implemented method for performing automatic generation of an immersive interface. For example, a computer-implemented method may include analyzing a network-based resource that includes text content, extracting a plurality of text content segments from the network-based resource, obtaining visual content and audio content associated with each respective text content segment of the plurality of text content segments, generating target content for an audiovisual display of the network-based resource based on combining at least a portion of each respective text content segment with the visual content and the audio content for the respective text content segment, and providing data describing the generated target content to a computing device for presenting the audiovisual display of the network-based resource.
In yet another example of the present disclosure, a non-transitory computer-readable medium stores instructions that, when executed by one or more processors, cause the one or more processors to automatically generate an immersive interface, for example, by analyzing a web-based resource that includes text content, extracting a plurality of text content segments from the web-based resource, obtaining visual content and audio content associated with each respective text content segment in the plurality of text content segments. Generating target content for the audiovisual display of the network-based resource based on combining at least a portion of each respective text content segment with the visual content and the audio content about the respective text content segment, and providing data describing the generated target content to the computing device for rendering the audiovisual display of the network-based resource.
Other aspects of the disclosure relate to various apparatuses, computer systems, non-transitory computer-readable media, computer-implemented methods, user interfaces, and electronic devices. These and other features, aspects, and advantages of various embodiments of the present disclosure will become better understood with reference to the following description and appended claims. The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate embodiments of the disclosure and together with the description, serve to explain the principles of interest.
Drawings
A detailed discussion of embodiments directed to one of ordinary skill in the art is set forth in the specification in view of the accompanying drawings, wherein:
fig. 1 depicts a block diagram of an example system for performing automatic generation of an immersive interface in accordance with an example embodiment of the present disclosure.
Fig. 1A depicts an example illustration showing an example of source content and an example of an immersive interface automatically generated from the example source content in accordance with an example embodiment of the present disclosure.
Fig. 2 depicts a flowchart of an example method for performing automatic generation of an immersive interface in accordance with an example embodiment of the present disclosure.
Fig. 2A depicts an illustration of an example for performing automatic generation of an immersive interface in accordance with an example embodiment of the present disclosure.
An example illustration according to an example embodiment of the present disclosure includes two example immersive interfaces generated from example segments of long form text source content.
Fig. 3 depicts a flowchart of an example method for performing automatic generation of an immersive interface in response to a user request in accordance with an example embodiment of the present disclosure.
Fig. 4 depicts an example illustration including two example immersive interfaces generated from example segments (pieces) of long form text source content in accordance with example embodiments of the present disclosure.
Fig. 5 depicts a flowchart of an example method for processing a request to generate an immersive interface in accordance with an example embodiment of the present disclosure.
FIG. 6 depicts a block diagram of an example computer system that may be used to perform one or more operations, according to an example embodiment of the present disclosure.
Detailed Description
Reference will now be made in detail to the embodiments, one or more examples of which are illustrated in the drawings. Each example is provided by way of explanation of the embodiments and not limitation of the disclosure. Indeed, it will be apparent to those skilled in the art that various modifications and variations can be made to the embodiments without departing from the scope or spirit of the disclosure. For example, features illustrated or described as part of one embodiment can be used with another embodiment to yield still a further embodiment. Accordingly, aspects of the present disclosure are intended to cover such modifications and variations.
In general, the present disclosure relates to improving various types of interfaces presented to a user, including but not limited to long form text content, using automatically generated immersive interfaces that can be created in various immersive formats. In particular, examples described in this disclosure enable automatic transformation of source content into an immersive user interface. Furthermore, the present disclosure also enables users to automatically convert their own content into an immersive user interface. Further, examples of the present disclosure provide improvements over existing methods for automated processing and integration of large and varied forms of text, visual, audio, and other types of content. Further, a platform is provided according to an example embodiment that enables a content provider to automatically convert content into an immersive user interface and present (surface) the content to a user via a search engine and other techniques.
Many users have grown accustomed to viewing shorter and more attractive (engaging) forms of content, which has recently gained popularity. For example, many users prefer to view short form video (video), annotated visual (visual) content, and social media posts, rather than lengthy articles and other types of long form text content. Furthermore, more than 15 billions of people began using the internet for the first time during 2015 to 2020, and it is expected that another billion users began using the internet during 2020 to 2025. Many of these new users connect to the internet via smart phones and increasingly use voice and video as tools to find information and services online. In addition, many internet users cannot benefit from content that exists in an unfamiliar language, lengthy text content that is difficult to consume on a small display, high resolution content that is difficult or impossible to consume based on a computer network or device limitation, content that is stored in an unsupported or incompatible format, and the like.
Many traditional forms of content, such as text content, may present (present) information primarily or entirely in a single manner (e.g., primarily or entirely in long form text). However, such content may be edited, enhanced, combined, and/or otherwise integrated with one or more other forms of content (e.g., audio content, visual content, augmented reality content, virtual reality content, etc.) in order to present facts, storytelling, and/or delivery messages to a user. The curation and editing of traditional content is a manual, time consuming and expensive process requiring advanced skills and specialized software. As a result, curation and editing are not typically performed on most types of content, including user-generated content, due to the associated complexity and expense. Thus, while most content, including user-generated content, may be otherwise improved and enhanced to benefit others, such content remains unedited. Furthermore, the content is typically in a non-immersive, legacy format and is not presented in a manner appropriate to the current hardware and sophistication of the user. Thus, various users, content owners, content publishers, and others around the world may benefit from the automatic generation of an immersive interface.
The present disclosure provides examples of performing automatic generation of an immersive interface based on transforming and integrating various forms of source content into one or more new pieces of target content to generate an immersive interface for a user. In examples of the disclosure, a computer system may perform automatic generation of an immersive interface, for example, by analyzing one or more pieces of source content, selecting a plurality of content segments from the one or more pieces of source content to generate target content, automatically identifying additional content associated with each content segment for inclusion in the target content with the respective content segment, generating the target content based at least in part on combining each respective content segment with the additional content identified for the respective content segment, and providing the target content to a computing device for presentation to a user. In an example embodiment, the summarization (summarization) engine may be trained on specific categories, such as discipline, genre, language, and target audience. Depending on the summarization engine parameters, content may be generated with multiple outputs having different text. Examples may include long form text, quick summaries, more detailed summaries, summary plus examples, and so forth.
One or more immersive interfaces can be generated based on a set of inputs. As an example, multiple immersive interfaces may be generated based on the same set of inputs by changing parameters of an immersive interface generation pipeline (pipeline). Example parameters such as summarization parameters, image selection parameters (different corpora), and/or language may be changed by changing one or more parameters, multiple user interfaces may be generated from the same set of inputs. As a specific example, consider a multi-motion broadcast event that may include image feeds (feeds) of different live events. A filtering service may be provided to select the appropriate image to generate 70 outputs (e.g., including 25 languages) for different locales (locales). After the event, the immersive interface generation system may use different filtering services and/or different parameters to generate various outputs from the same content for different requirements or different target audience.
In some examples, the immersive interface generation system may be used in a variety of different contexts to generate attractive target content based on source content. In some examples, the immersive interface generation system may be provided to the user for use on the user computing device (e.g., as a software distribution that may be installed on the user computing device). In this context, the user may provide the content to the immersive interface generation system (e.g., by directly entering text or by providing a link to the source content). The immersive interface generation system can generate attractive target content on the user computing device and the user can distribute as desired.
In another example, the immersive interface generation system may be provided as a service to third party content producers. For example, a content producer (e.g., a particular content producer, a group of content producers, or an entity) may provide content to a platform that includes an immersive interface generation system. In some examples, the provided content may be input directly to the content platform by the content producer. In other examples, the original content may be associated with a link provided by the content producer to the immersive interface generation system. The immersive interface generation system may access content based on the links and capture the content (e.g., crawling the content from linked web pages). The content provided may be in a format convenient to the content producer, such as plain text formatted, formatted as text articles, or not formatted. The immersive interface generation system may process the provided content to generate attractive target content including additional audio and/or visual media not included in the provided content. As described above, processing the provided content to produce attractive target content may include summarizing the content or extracting one or more key phrases. Processing may include formatting the provided content according to a particular template for display in one or more different display form factors. Additional content (e.g., media content that was not originally provided) may be identified and added to the target content, and a translation of the content may be generated.
Once the attractive target content has been generated for a particular piece of provided content, the attractive target content may be provided for access by the user via that platform that includes the immersive interface generation system. Thus, in some examples, the content platform may include an immersive interface generation system and allow a content producer to provide content in a first format (e.g., original text, article format, etc.) and automatically generate attractive target content. The attractive target content may be shared with the user in one or more new formats and one or more new languages on the content platform.
In another example, a content platform independent of the immersive interface generation system can use the immersive interface generation system to generate attractive target content for content that has been published on the content platform. Generating the attractive target content may allow the content platform to provide the content to the user in a more attractive manner (e.g., multimedia presentation), in a variety of formats (e.g., in a format more suitable for smartphones), and in additional languages. Making content more attractive and available in additional formats and languages may allow the content platform to provide more useful services to users accessing its content.
In an example embodiment, the immersive interface system may enable tracking of content, such as for providing revenue or revenue distribution. As an example, the immersive interface system can track content provided to the system by the content provider. The system may convert or otherwise generate an immersive interface based at least in part on content provided by the content provider. The system may track the original content and/or the converted content to enable, for example, revenue sharing and/or distribution. For example, the system may track the number of times the conversion content is accessed or otherwise provided to the user. In an example embodiment, revenue associated with converting content may be shared between the immersive interface system and the original content provider.
The systems, methods, and computer program products described herein provide a number of technical effects and benefits. As one example, the embodiments described in this disclosure perform automated analysis and integration of large and diverse forms of distributed content, and automatically transform and generate new immersive interfaces more efficiently and with less computing resources (e.g., less processing power, less memory usage, less power consumption, etc.) that would otherwise be wasted using manual investigation, creation, editing, and discovery processes.
Referring now to the drawings, example aspects of the present disclosure will be discussed in more detail.
Example computing environment for automatically generating immersive interfaces
Fig. 1 depicts a block diagram of an example computing environment 100 for performing automatic generation of an immersive interface in accordance with an example embodiment of the present disclosure. The example computing environment 100 includes one or more server machines 110, one or more search server machines 120, one or more user devices 130, and a data store (storage) 150 communicatively coupled via a network 102.
The network 102 may be a public network (e.g., the internet), a private network (e.g., a Local Area Network (LAN) or a Wide Area Network (WAN)), or any combination thereof. In examples, network 102 may include the internet, one or more intranets, a wired network, a wireless network, and/or other suitable types of communication networks. Network 102 may also include a wireless telecommunication network (e.g., a cellular network) adapted to communicate with other communication networks, such as the internet. In addition, network 102 may include one or more short-range wireless networks.
Each server machine 110 may include one or more network (web) servers 112 and/or one or more application servers 114. Web server 112 may provide text, audio, images, video, or any other content to server machine 110 or other sources (e.g., search server machine 120, user device 130, and data store 150), and from server machine 110 or other sources (e.g., search server machine 120, user device 130, and data store 150). Web server 112 may also provide web-based application services, business logic, and interfaces to server machine 110, search server machine 120, and user device 130. Web server 112 may also send and receive text, audio, video, images, and/or other content to and from search server machine 120, user device 130, etc. For example, various forms, versions, and types of content (e.g., content 118, 138 and/or content residing in data store 150, etc.) may be stored for storage, sharing, and/or distribution.
In an example, one or more web servers 112 are coupled to one or more application servers 114, the application servers 114 providing application services, data, business logic, and/or APIs to server machines 110, search server machines 120, user devices 130, and/or other computing devices (not shown). In some examples, application server 114 independently provides one or more such services without using web server 112. In an example, web server 112 may provide server machine 110, search server machine 120, and user device 130 with access to one or more application server 114 services associated with an automatic immersive interface generation system (e.g., immersive interface generation system 140). In an example, such functionality may also be provided as part of one or more different web applications, stand-alone applications, systems, plug-ins, web browser extensions, application Programming Interfaces (APIs), and the like. In some examples, the plug-ins and extensions may be commonly referred to, individually or collectively, as "add-on" components.
In some examples, one or more web servers 112 are coupled to one or more application servers 114, capable of providing a content platform available to a user of user device 130. The immersive interface generation system 140 can generate attractive target content based on the content 118 in a first format (e.g., a predominantly text-based format). The content platform may host (host) both the original content 118 and the attractive target content. In some examples, the content platform hosts only attractive (engaging) generated based on the original content. In some examples, the original content provided by the content producer is converted to attractive target content by the immersive interface generation system 140. Once the attractive target content is generated, it may be made available to the user of the content platform and the original content may be discarded.
Server machine 110 includes a local data store 116 and may access other data stores 150 to store and access various forms of content (e.g., content 118). In an example, server machine 110 may provide, be associated with, or be used in conjunction with one or more cloud-based or network-based services and applications, such as an internet search engine, a social networking site, a cloud storage provider, a content sharing site, an image or photo sharing site, a video sharing site, a news publishing site, enterprise software, and/or any other site, service, or application that stores, processes, coordinates, generates, and/or displays content provided by a user and/or of any other source or type. In various examples, such sites, services, and/or applications may be accessed by a user via one or more applications 132 running on respective user devices 130.
Search server machine 120 may generally be any computer system, device, or other machine capable of receiving, locating, obtaining, retrieving, indexing, and/or searching for any form of content. For example, search server machine 120 may discover or otherwise locate content available from any authorized and accessible location, including, but not limited to, data store 150, the internet, an intranet, a content repository, a content management system, a document management system, and the like. Search server machine 120 may also include one or more web servers (not shown) and/or application servers (not shown).
In various examples, search server machine 120 may locate and process content by systematically crawling crawling through available content and/or associated metadata. For example, any type of web content or other content may be crawled using hyperlinks, references, and/or any other references present in the source content to identify and locate additional content for analysis, search index generation, and/or other processing.
In various examples, search server machine 120 may analyze various types of content to generate search index 122, search index 122 facilitating quick and accurate retrieval of relevant content in response to a search query. For example, a web crawler (crawler) may locate web or other content that is analyzed and used to create a search index 122, the search index 122 allowing a search engine to quickly identify and retrieve relevant content for search queries submitted via user devices 130. In some examples, search server machine 120 may store or maintain search index 122 locally and/or a reference search index for other services generated and/or stored elsewhere. Moreover, search server machine 120 may cache various forms of content, e.g., for use in association with analyzing content, generating search index 122, retrieving search results in response to a search query, and so forth. In various examples, such cached information may be stored locally on search server machine 120, in data storage 150, and/or elsewhere accessible to search server machine 120.
In various examples, search server machine 120 may receive and process search queries received from user device 130. The search queries may include, but are not limited to, information queries related to a particular topic, navigation queries to search a particular web site or web page of a party (a party), transaction queries (transactional query) reflecting a user's interest in performing a particular activity (like cooking a particular meal, repairing a torn fabric, repairing a flat), and/or generally any other type of search query. Search server machine 120 may generally receive one or more types of search queries via a search interface associated with user device 130, application 132, user interface 134, and the like. For example, search server machine 120 may generally receive and process search queries submitted as text, audio, spoken words, and/or visual information (e.g., visual content such as photographs or other images, video clips, etc.).
In various examples, search server machine 120 may process search queries by accessing and searching one or more search indexes 122 based on the search queries to quickly identify and return relevant search results. Search index 122 generally refers to any index data structure used by a computer system, device, or software application to provide for the quick and accurate retrieval of information, for example, in response to a search query. Examples of search index 122 may generally include, but are not limited to, an inverted index, a b-tree index, a bitmap index, an n-gram index, and the like. In various examples, search server machine 120 identifies relevant search results in response to a search query (e.g., an internet search engine query) and returns them to user device 130, application 132, and/or user interface 134.
User device 130 may typically be a Personal Computer (PC), a laptop computer, a mobile phone, a tablet computer, a server computer, a wearable computing device, or any other type of computing device (i.e., a client machine). The user device 130 may run an Operating System (OS) that manages hardware and software of the corresponding device. A browser application (not shown) may run on the user device 130. The browser application may be a web browser that may access content and services provided by server machine 110, search server machine 120, or a combination of server machine 110 and/or search server machine 120. Other types of computer programs and scripts may also be run on the user device 130.
User device 130 may include one or more applications 132, a user interface 134, a data store 136, and content 138. User device 130 may generally execute or run one or more applications 132 or client components or versions of applications 132 that interoperate with one or more server machines 110 and/or one or more search server machines 120. For example, the application 132 may include or provide access to one or more immersive interface generation systems 140 and locally and/or remotely operated services.
Each application 132 may generally provide a user interface 134, the user interface 134 allowing a user to submit user inputs (e.g., commands, instructions, search queries, etc.) and receive various text, graphical, and/or audio outputs associated with the corresponding application 132 running on the user device 130. For example, a user may typically provide user input to the application 132 via a user input component of the respective user device 130, such as a touch-sensitive component (e.g., a touch-sensitive display screen or touchpad) that is sensitive to touch by a user input object (e.g., a finger or stylus), a virtual keyboard, a traditional keyboard, a mouse, a microphone, a traditional keyboard, or by other means by which the user may provide input to the application 132 via the device. The user may also receive output from the application 132 via one or more user interfaces 134, the one or more user interfaces 134 being provided via a display, the user device 130, a computing system, and/or any other type of device.
Applications 132 may include, for example, web browsers, content management, document management, search engines, maps, navigation, social networks, photo sharing, video sharing, storage services, digital wallets, mobile wallets, and/or any other type of "application" running on user device 130. Further, the application 132 may have corresponding websites, services, and/or APIs that may be used in association with the application 132, or separate (separate) and apart (apart) from the application 132. In some examples, the application 132 may include an immersive interface generation system 140, the immersive interface generation system 140 allowing a user to generate attractive target content on the user device 130 based on content provided by the user. In some examples, the user may provide text directly (e.g., enter text into the immersive interface generation system 140). In other examples, the user may identify a content source (e.g., based on a web link) from which the immersive interface generation system 140 may crawl (script) source content. The data store 136 can generally include any removable or non-removable storage device associated with the user device 130, and the user device 130 can also access other various storage devices (e.g., the data store 150) via the network 102.
In an example, server machine 110, search server machine 120, user device 130, and/or other computing devices may each store and access various forms of content 118, 138 (e.g., source content, target content), including content from data store 150 and/or elsewhere. In examples, content may generally refer to any text data, audio data, visual data, graphics data, animation, images, video, multimedia, augmented reality data, virtual reality data, and/or any other data used to represent or describe any type of content in various forms or formats of any dimension (e.g., 2D, 3D, etc.). In various examples, the content may be original or newly captured, pre-processed or partially edited, professionally edited, curated, user generated, and/or in any other form.
The content may include any type of text content. Text content generally refers to any content including letters, numbers, words, phrases, sentences, paragraphs, and the like. The text content may include titles, notes, messages, web pages, articles, stories, books and/or any other type of written or printed work in digital and/or any other format. In various examples, the text content may be long form text content consisting primarily or entirely of text.
The content may also include any type of audio content. Audio content generally refers to any sound including, but not limited to, speech, audible sound, inaudible sound, sound effects, sound clips (clips), background or other types of music, voice-over audio, text-to-speech audio, and the like. Audio may generally be presented alone and/or in combination with other forms and types of content.
The content may also include any type of visual content such as images, still graphics, animated graphics, video, and the like. The image may generally include any visual or graphical representation, such as a photograph or screenshot (screenshot) captured by a camera, computing device, and/or other device. The images may include, for example, portrait, square (square), panoramic, and/or any other type of image. The visual content may also include, for example, one or more segments (segments) or one or more pieces (pieces) of original and/or edited content, such as a series (burst) of photos, video clips, movie trailers, montages, music videos, highlight (HIGHLIGHT REEL) videos, advertisement videos, and the like.
Video content generally refers to a set of consecutive image frames representing a scene in motion. For example, a series of successive images may be captured consecutively or later reconstructed to produce the effect of a moving picture, which may include camera motion and/or motion of content within the scene. Video content may be presented in a variety of formats including, but not limited to, analog, digital, two-dimensional video, and three-dimensional video. Further, the video content may include any collection of movies, video clips, or animated images to be displayed in sequence. The video data may comprise digital video having a sequence of still image frames that may also be stored as image data. Furthermore, each image frame may represent a snapshot (snapshot) of a scene that has been captured according to a time interval.
Data storage 150 generally refers to persistent storage that is capable of holding various types of content, such as text, audio, images, video, and graphics. In some examples, data store 150 may include a network-attached file server or cloud storage, while in other examples, data store 150 may include other forms of persistent storage, such as object-oriented databases, relational databases, and the like. In some examples, the data store may include one or more content repositories (repository) that store digital content and may provide services such as data organization, data management, version control (version), data retention policies, indexing, searching, retrieval, and the like. For example, the content repository may include one or more content management systems and/or document management systems controlled, operated, and/or provided by one or more different parties.
In various examples, the data store 150 may include user-generated content (e.g., user-generated documents, images, videos, etc.) uploaded by a user via the user device 130 and/or content provided by one or more other parties. The data may be added to the data storage 150, for example, as discrete files (e.g., structured text documents, unstructured text documents, HTML files, XML files, spreadsheet files, database files, multimedia files, moving Picture Experts Group (MPEG) files, windows Media Video (WMV) files, joint Photographic Experts Group (JPEG) files, graphics Interchange Format (GIF) files, portable Network Graphics (PNG) files, etc.), or as components of a single compressed file (e.g., zip file).
In various examples, server machine 110, search server machine 120, and/or user device 130 run, execute, or otherwise utilize immersive interface generation system 140. For example, user device 130 may include one or more applications 132 associated with services (e.g., immersive interface generation system 140) provided by one or more server machines 110 and/or search server machines 120. For example, various types of computing devices (e.g., smart phones, smart televisions, tablet computers, smart wearable devices, smart home computer systems, smart assistant devices, etc.) may use dedicated apps and/or APIs to access services provided by server machine 110 and/or search server machine 120, issue commands to server machine 110 and/or search server machine 120, and/or receive content from server machine 110 and/or search server machine 120 without accessing or using web pages.
In an example, the functions performed by one or more of server machine 110 and/or search server machine 120 may be performed in whole or in part by one or more other machines and/or user devices 130. Server machine 110 and/or search server machine 120 may be accessed as a service provided by a system or device via an appropriate Application Programming Interface (API) and data feed (data feed) and is thus not limited to use with web sites. Further, server machine 110, search server machine 120, and/or user device 130 may be associated with and/or serviced by one or more immersive interface generation systems 140, e.g., as supplied or provided by one or more different parties.
In various examples, the immersive interface generation system 140 performs automatic generation of the immersive interface by automatically transforming the source content into attractive target content including a combination of at least three or more of text, audio, still images, animated images, graphics, video, interactive user interface elements, and the like. In examples of the present disclosure, the immersive interface generation system 140 automatically generates an immersive interface using a rigorous computerized process that rapidly transforms primary (basic) source content, which exists primarily or entirely in one form, into a dynamic and attractive user interface in a manner that was not previously performed.
In an example, the immersive interface generation system 140 generates one or more immersive interfaces associated with a particular topic, theme, person, category, activity, article, recipe, news event, sports event, or the like. The one or more immersive interfaces can be generated from one or more sources and can be generated in various formats. Such immersive interfaces may be generated, for example, at different points in time. For example, one or more immersive interfaces may be generated prior to or upon anticipation of a user request, in response to or based in part on one or more user requests or events, additional generation, modification, and/or transformation being performed based on various attributes associated with the user request (e.g., user preferences, physical computing device capabilities, operating system, network settings, application features and settings, location, language, etc.).
In an example, the immersive interface generation system 140 generates one or more immersive interfaces in response to search queries submitted by users. For example, the immersive interface generation system 140 can automatically generate one or more immersive interfaces based on one or more of audio, visual, text, and/or other inputs or information associated with the search query. In various examples, one or more generated immersive interfaces may be sent or otherwise provided to the computing device in response to the search query. In various examples, the generated immersive interface may be stored and cached for use and reuse and later provided to one or more other computing devices in response to other search queries of the user (e.g., similar, related, and/or original (exact) search queries).
In an example, the immersive interface generation system 140 determines one or more pieces of source content to analyze based on the request. The request may generally refer to a search query, instruction, command, or any other type of input or request received from a user and/or machine. In some examples, the request may include source content to be analyzed, information identifying the source content to be analyzed, information allowing the immersive interface generation system 140 to locate and/or identify one or more pieces of source content to be analyzed, and the like. For example, the request may include one or more general or specific locations of the source content in a data store (e.g., data store 116, 136, 150) and/or any type of content repository. The request may also include information regarding attributes of the source content or one or more other criteria that allow the immersive interface generation system 140 to identify or otherwise determine the source content to analyze to generate the immersive interface.
In an example, the immersive interface generation system 140 analyzes text, audio, and/or visual content associated with the request, such as input associated with a search query, to determine source content. The immersive interface generation system 140 can also analyze the organization and structure of words, phrases, or other information associated with the request to determine one or more pieces of source content to analyze in association with the request. In some examples, immersive interface generation system 140 may determine one or more pieces of source content to analyze based on one or more results returned from search server machine 120 in response to the search query.
In an example, the immersive interface generation system 140 analyzes one or more pieces of source content based on the request. For example, one or more pieces of structured and/or unstructured content may be analyzed to identify candidate content segments for generating an immersive interface. In some examples, the immersive interface generation system 140 analyzes different pieces of source content (e.g., files) and selects one or more portions of source content from each of the one or more pieces of source content. For example, one or more sentences may be selected from different source content files (web pages, articles, books, etc.) and organized into logically smooth outline, summaries, stories, messages, and/or any other form or type of content.
In an example, the immersive interface generation system 140 analyzes each selected content segment and, for each selected content segment, assigns a respective weight to each of the one or more objects, activities, and/or relationships present in the respective content segment, e.g., to help identify additional related content that may be combined with or otherwise included in the immersive interface with the respective selected content segment in the immersive interface. For example, text or other information may be extracted from each selected content segment and analyzed to identify entities, activities, and/or relationships in each particular selected content segment and to assign weights to each of one or more of the entities, activities, and/or relationships represented in the particular content segment. Content may be extracted from any type of content including text, images, and video. For example, OCR may be performed on one or more images, or human/object recognition (recognation) may be performed on images. Further, multiple content sources may be used to generate an immersive interface. For example, text may be extracted from one source and images or the like extracted from another source. People, objects, animals, scenes, locations, landmarks, activities, and/or other types of information may be determined based on analyzing text, audio, visual, and/or other forms of selected content segments. In various examples, each of the one or more entities, activities, and/or relationships in a particular piece of content may be weighted based on one or more of organization, ordering, location, salience, duration, association, interaction, and/or any other aspect of information determined based on analyzing the selected piece of content, for example.
An immersive interface may be generated based on one or more sets of inputs. For example, multiple immersive interfaces may be generated based on the same set of inputs by changing parameters to the immersive interface generation pipeline. Example parameters such as summarization parameters, image selection parameters (different corpora), and/or language may be changed by changing one or more parameters, multiple user interfaces may be generated from the same set of inputs. As a specific example, consider a multi-motion broadcast event that may include image feeds of different live events. A filtering service may be provided to select the appropriate image to generate 70 outputs (e.g., including 25 languages) for different locales. After the event, the immersive interface generation system may use different filtering services and/or different parameters to generate various outputs from the same content for different requirements or different target audience.
In an example, the immersive interface generation system 140 identifies additional content associated with each of the selected content segments for inclusion in the immersive user interface with the respective content segments. For example, the immersive interface generation system 140 may query or otherwise search one or more additional content collections (collections) individually to identify one or more additional pieces of content that correspond to, are related to, or otherwise match each selected piece of content. For example, the immersive interface generation system 140 may analyze, score, and rank (rank) pieces of additional content located in the data store 150 or other locations based on weights respectively determined for each of one or more entities, activities, and/or relationships in a particular selected piece of content.
In an example, the immersive interface generation system 140 determines one or more pieces of additional content to combine with each content segment based on scoring and/or ordering the pieces of additional content according to similarity to each selected content segment. For example, the immersive interface generation system 140 can select one or more pieces of additional content and/or one or more types of additional content that are related to a particular piece of content. In some examples, the immersive interface generation system 140 scores, or scores and ranks, the additional visual content and/or the additional audio content in view of weights assigned to each of the one or more entities, activities, and/or relationships identified in the text-based selected source content. In this way, the immersive interface generation system 140 can identify, select, and obtain one or more different types of additional content corresponding to each selected piece of content to automatically generate an immersive interface that combines each selected piece of source content (e.g., one content type) with the corresponding additional content (e.g., one or more additional content types).
In an example, the immersive interface generation system 140 automatically generates one or more pieces of logo content for the immersive user interface. The one or more immersive interfaces can be generated from one or more sources and can be generated in various formats. For example, the immersive interface generation system 140 can combine, mix, or otherwise combine each particular selected content segment with its corresponding additional content to generate one or more associated types of target content. This type of targeted content may include, but is not limited to, one or more timed or non-timed display slides, multimedia clips, and/or any other type of targeted content, which may be included as part of an immersive user interface. In various examples, the immersive interface generation system 140 processes each of the plurality of selected text content segments by combining each particular content segment with its corresponding additional visual content and corresponding additional audio content to generate an immersive user interface.
In an example, the immersive interface generation system 140 automatically generates a piece of logo content for each of the plurality of selected content segments based at least in part on combining each selected content segment with additional content of at least two or more additional content types. In some examples, the immersive interface generation system 140 generates a piece of logo content based on combining text from the selected piece of content with additional audio content and additional visual content corresponding to the text from the selected piece of content.
By way of example, and for illustrative purposes only, fig. 1A depicts an example illustration showing an example of source content and an example of immersive interface 190 automatically generated from the example source content in accordance with an example embodiment of the present disclosure. For example, the illustration of example source content 180 generally shows an example of source content that is manually scrolled on a display of a mobile device. The illustration of the example immersive interface 190 generally shows an example of an immersive interface generated from an example of the source content 180. In an example, such an immersive interface can generally be presented automatically and without interruption. In this way, a user can typically view and listen to various enhanced information provided via a series of generated immersive interfaces without performing repeated and/or non-repeated interactions.
In an example, the immersive interface generation system 140 automatically generates the immersive user interface by integrating a plurality of generated target content together. In some examples, a set of two or more pieces of logo content may be integrated together into an immersive user interface, such as a display slide, a multimedia slide, a set of multimedia clips, and/or in other forms or arrangements as new content. In various examples, the immersive user interface may include any one or more of animations, automatic user interface progress (progression), transitions between different pieces of integrated content, and the like. In various examples, the immersive interface generation system 140 may generate a set of immersive display slides, multimedia slides, or multimedia clips for the user interface, such as tile user interfaces or interactive story pages with automated seamless user interface progression (flow cohesively) flowing in close-coupled with a configured or predefined cadence (pace). In some examples, the immersive interface generation system 140 generates the immersive user interface as part of a standardized file, set of files, or package (package) that may be used with one or more different types of user interfaces and/or user interface controls (e.g., animated quick user interface controls, storyboard user interface controls, augmented reality user interfaces, virtual reality interfaces, and/or any other type of user interface or user interface control capable of providing an immersive interface to a user). In various examples, the immersive interface generation system 140 generates an immersive interface that is automatically presented to the user, allowing any user to view and understand the information being presented without prior experience, knowledge, or training.
In an example, the immersive interface generation system 140 provides an automatically generated immersive interface to one or more computing devices for presentation, display, communication, or any other type of delivery to one or more users. In various examples, the automatically generated immersive interface may be provided to the user via the user device 130. For example, the immersive interface generation system 140 can provide one or more generated immersive interfaces in response to search queries or other types of user requests submitted to the server machine 110 and/or the search server machine 120. In various examples, the immersive interface generation system 140 provides the generated immersive interface in one or more forms, including, but not limited to, presentation as an animated tile in a tile-based user interface, as content in a feed, as a post in a social media feed or on a social media site, as a carousel (carousel) user interface or post as a set of story pages, and/or generally as part of any other type of user interface or in various other forms. In some examples, the immersive interface may be generated in a web story (web story) format that mixes audio, video, images, animations and/or text that conveys a dynamic immersive experience to the user as compared to one or more long form text documents.
In various examples, the immersive interface generation system 140 allows a user to automatically generate an immersive interface from one or more types of available content. For example, any type of application, website, system, and/or service associated with the immersive interface generation system 140 can generally allow individual users, such as influencers, content owners, content providers, content partners, content licensees, or any type of entity, to automatically generate high-quality immersive user interfaces quickly without the need for specialized software applications or skills. In some examples, any number of users or entities (1, 10, hundreds, thousands, millions, etc.), each may use the immersive interface generation system 140 to automatically generate an immersive interface using its own content, shared content, licensed content, public domain content, and/or any other source of licensed content. The user may then share and/or publish the generated immersive interface with other people, e.g., via a network, social media, content sharing site, etc.
In an example embodiment, the system may receive feedback from the user and incorporate the feedback into the generated content. The publisher may be provided with an application tool that enables the publisher to edit the generated content. In addition, the system may analyze consumer behavior on the generated content and refine the engine to modify the content to make it more user friendly and useful. In some examples, ad units may be programmatically (programmatically) added to the generated content. Furthermore, the analysis may be programmatically added to the generated content. Such analysis may enable tracking, measurement, feedback loop generation, and engine-plus-output refinement.
Example methods for performing automatic generation of an immersive interface
Fig. 2 depicts a flowchart of an example method 200 for performing automatic generation of an immersive interface in accordance with an example embodiment of the present disclosure. The method 200 and other processes described herein are illustrated as a set of blocks that specify operations performed, but are not necessarily limited to the orders or combinations of operations by the respective blocks shown. Accordingly, one or more of the various portions of method 200 may be omitted, rearranged, combined, and/or adjusted in various ways without departing from the scope of this disclosure.
One or more portions of method 200, as well as other processes described herein, may be implemented by one or more computing devices, such as, for example, one or more computing devices of a computing environment, e.g., as shown in example computing environment 100 of fig. 1 (e.g., one or more of server machine 110, search server machine 120, user device 130, etc.). Although portions of the following discussion may be made with respect to a particular computing environment, such references are by way of example only. The operations are not limited to being performed by one entity or multiple entities operating on a device or in any particular computing environment. As such, any one or more portions of these processes may be implemented as operations on hardware components of the devices described herein.
At (202), a computing system analyzes one or more pieces of source content. In an example, the immersive interface generation system 140 analyzes one or more pieces of source content associated with one or more items, which may include, but are not limited to, particular topics, subjects, people, categories, activities, articles, recipes, news events, sporting events, and the like. For example, the immersive interface generation system 140 can receive a request, such as a user search query or request, to generate an immersive interface. In various examples, such requests and other types of requests may generally include information that names, describes, is associated with, or otherwise directly or indirectly identifies information that may be used to obtain one or more pieces of source content to generate an immersive user interface.
In an example, the immersive interface generation system 140 receives a request for (pertain) a theme such as malaria (malaria), which is a mosquito-borne infectious disease affecting people and animals in various areas of the world. The request may include or be associated with text, audio, and/or visual content. For example, the request may be a text search query, a voice search query, an image query, or visual content including the word "malaria" or a phrase including the word "malaria" (e.g., such as "WHAT IS MALARIA", "sympms of malaria", "PREVENT MALARIA", etc.). In other examples, the request may be a request from a content provider or other entity (e.g., a medical provider, national public health agency, medical journal, publisher, or any other type of entity) to generate one or more immersive interfaces with items about "malaria" or any other type. In various examples, source content may be searched, located, and identified in a data store 116, 136, 150, database, data repository, content management system, document management system, anywhere else that resides across one or more networks 102 (such as an intranet or the internet) and/or content or associated metadata to identify relevant source content.
In an example, the immersive interface generation system 140 of the computing system identifies and obtains one or more pieces of source content for use in generating one or more immersive interfaces. In some examples, one or more identified pieces of source content may be translated into a standard language for analysis. In various examples, each of the one or more identified pieces of source content may be analyzed to determine one or more relevant segments, regions, portions, sub-portions, words, phrases, sentences, paragraphs, pages, and/or any other identifiable or other relevant portions of the pieces of source content for generating the immersive interface.
By way of example, and for illustrative purposes only, fig. 2A shows a non-limiting example of performing automatic generation of an immersive interface in accordance with an example embodiment of the present disclosure. For example, such examples may be used as part of processing each of a plurality of different pieces of source content to generate a plurality of corresponding immersive interfaces to be provided to a computing device for presentation to a user.
At (204), the computing system selects one or more content segments from one or more pieces of source content for use in generating target content. In an example, the immersive interface generation system 140 selects one or more content segments from each of the one or more pieces of source content for use in generating the immersive interface. For example, the immersive interface generation system 140 may select one or more words, phrases, sentences, paragraphs, portions, and/or any other subset of content from each of one or more documents (such as long form text documents) based on analyzing various source content in view of the request.
In an example, the immersive interface generation system 140 analyzes one or more pieces of source content and determines one or more portions from each of one or more of the source documents for generating the immersive user interface. For example, the immersive interface generation system 140 can identify and select a single (SINGLE PIECE) source content (e.g., licensedArticle, licensed or owned web content, and/or other example licensed source content), or one or more different portions from each of a plurality of different pieces of source content may be identified and selected for use in generating an immersive user interface. In some examples, the immersive interface generation system 140 processes the plurality of selected source content, which may include, for example, merging, summarizing, sorting, reordering, and/or by performing any other process involving the plurality of selected source content in preparation for generating the immersive interface. In some examples, the system may refine and/or recreate the text to summarize the content, rather than the original (exact) sentence in the source text.
In an example, the immersive interface generation system 140 identifies and obtains one or more different pieces of source content associated with an item (e.g., a person, object, animal, scene, location, landmark, activity, and/or other type of information type) from each of the one or more pieces of source content. In some examples, the immersive interface generation system 140 can summarize each of the one or more selected pieces of source content into one or more phrases or sentences of gist with respect to the one piece of source content. In some examples, the immersive interface generation system 140 summarizes each of the one or more pieces of source content as one, two, or three, or any number of phrases and/or sentences. In various examples, the immersive interface generation system 140 can separate and organize a plurality of different raw and/or summarized source content into a series of logically flowing (logically flowing) phrases or sentences that are selected for generating the immersive interface. Moreover, in various examples, the immersive interface generation system 140 can perform translations of any one or more of such phrases and/or sentences such that the phrases and/or sentences are represented in a standard or common language.
In various examples, the immersive interface generation system 140 analyzes each of the one or more selected pieces of source content and identifies, for example, one or more entities, activities, and/or relationships in each particular selected piece of content and assigns a weight to each of one or more of the entities, activities, and/or relationships represented in the particular piece of content. For example, people, objects, animals, scenes, locations, landmarks, activities, and/or other types of information may be determined based on analyzing selected pieces of content in text, audio, visual, and/or other forms. In various examples, each of the one or more entities, activities, and/or relationships in a particular piece of content may be weighted based on one or more of organization, ordering, location, salience, duration, association, interaction, and/or any other aspect of information determined (ascertain) from analyzing the selected piece of content, for example.
As an example, the immersive interface generation system 140 may analyze the selected content segments, such as "ni and samum are cooking food (NICK AND SAM ARE cooking up a storm)", and then the immersive interface generation system 140 may identify entities, activities, and/or relationships based on the analysis of the selected content segments of the examples. The immersive interface generation system 140 may also assign weights to each of the entities, activities, and/or relationships identified in the example selected content segments. In one example, assigning weights to example content segments may include "nickel (0.6)", "Sam (samm)" (0.5) "," cookie (0.4) ", and" storm (0.1) ". In various examples, topics and associated weights in the selected content segments may be analyzed, identified, and weighted based on the organization and order of information in the selected content segments. In various examples, topics in the selected content segments may also be analyzed, identified, and weighted in view of associated information obtained from knowledge graphs, knowledge bases, or any other type of system or data structure that provides information about entities, objects, events, relationships, circumstances, and the like, for example. In various examples, the immersive interface generation system 140 can automatically identify corresponding additional content using the identified topics and associated weights of such identified topics found in the selected content segments to combine with such selected content segments to generate the immersive interface.
At (206), the immersive interface generation system 140 automatically identifies additional content for inclusion in the target content with each of the respective content segments. In an example, the immersive interface generation system 140 processes each of the one or more selected source content to identify, compare, and select one or more additional pieces of content to be combined with each respective source content in the immersive interface. For example, the immersive interface generation system 140 can search, locate, obtain, score, rank, compare, evaluate, and/or perform various other processes to identify corresponding (e.g., related, highly related, and/or most related) additional content for inclusion in the immersive interface with each of the one or more pieces of source content.
In an example, the immersive interface generation system 140 identifies entities, activities, relationships, and/or other concepts in or otherwise associated with a piece of selected source content. The immersive interface generation system 140 may then extract and assign weights to each of one or more of such identified entities, activities, relationships, and/or other concepts in each of the one or more different selected pieces of source content. In various examples, the immersive interface generation system 140 uses information associated with the identified entities, activities, relationships, and/or other concepts to query, search, locate, obtain, score, rank, compare, evaluate, and/or perform various other processes to identify additional content compatible with, related to, or otherwise corresponding to each particular selected source content. In various examples, the immersive interface generation system 140 accesses additional content from any authorized and permitted location, such as, for example, data stores 116, 136, 150 and/or elsewhere, including, but not limited to, the internet, an intranet, a content repository, a content management system, a document management system, and the like.
In various examples, the immersive interface generation system 140 determines additional content to analyze based on one or more license management software applications and/or systems. License management software applications and systems generally refer to any tool that allows a user or organization to track, manage, and/or obtain information about license status, license agreements, license terms, license rules, and license compliance associated with various types of assets, including but not limited to content, third party content, partner content, public domain content, any other type of content, software applications, hardware, technology, and the like. In some examples, the license management software applications and systems can include notification information, attribute information, and/or any other type of information that the immersive interface generation system 140 can include, such as additional content used in the immersive interface.
In various examples, the immersive interface generation system 140 obtains and analyzes one or more pieces of additional content to determine additional content to be matched and combined with a particular one of the selected pieces of source content in the immersive interface. For example, the immersive interface generation system 140 can obtain additional content in any form or type, such as images, video, audio, and text, based on weights associated with entities, activities, relationships, and/or other concepts identified in a piece of selected source content. The immersive interface generation system 140 may then analyze the additional content and determine a relevance score based on weights associated with the entities, activities, relationships, and/or other concepts identified in the one piece of selected source content.
In various examples, the immersive interface generation system 140 analyzes and scores multiple pieces of additional content, such as images, video, audio, and text, based on how closely each piece corresponds to or otherwise matches the selected source content. For example, the immersive interface generation system 140 can perform any type of visual, audio, or text analysis in order to score any piece of additional content. In some examples, the immersive interface generation system 140 analyzes the visual content using image segmentation, image classification, object detection, image recognition, object recognition, and/or any other type of analysis of visual additional content (e.g., images, video, multimedia content with visual components, etc.). In some examples, the immersive interface generation system 140 analyzes the audio content to detect, recognize, interpret, identify various types of sounds, speech, noise, interactions, and/or any other aspect of additional content associated with the audio (e.g., recordings, sound clips, sounds associated with multimedia such as video, etc.). In some examples, the immersive interface generation system 140 analyzes the text, for example, using text processing, natural language processing, and/or any other type of processing, to identify additional content for various aspects associated with the text. Content may be extracted from any type of content including text, images, and video. For example, OCR may be performed on one or more images, or person/object recognition may be performed on images.
In one example, where one piece of selected source content is "nix and samum is cooking a delicacy (NICK AND SAM ARE cooking up a storm)", the immersive interface generation system 140 can search for authorized additional content based on the identified concepts and the weights of the selected source content. For example, the immersive interface generation system 140 may use concepts and weights that are determined based on natural language processing, analyzing knowledge graphs, and/or in one or more other ways. For example, in the present example, such concepts and weights may be "nickel (0.6)", "Sam (samm)" (0.5) "," cookie (cooking) "(0.4) and" storm (0.1) ". As such, the immersive interface generation system 140 can analyze and rank the various additional content based on such concepts and weights. For example, the additional content (such as an image with both ni and samm) may be scored higher than the additional content that includes only one of ni or samm. Furthermore, the content including both nikk and samm in the kitchen may be scored higher than if nikk and samm walk outdoors. Furthermore, the additional content of the nix and samum cooks in the kitchen may be scored and ranked highest in the examples mentioned based on the closest match to the selected source content.
In an example, the immersive interface generation system 140 selects one or more additional pieces of content for each of the selected source content. For example, the immersive interface generation system 140 may select one or more pieces of additional content based on scoring, ordering, or otherwise evaluating the attributes of the additional content as compared to the attributes of the selected source content. In some examples, the immersive interface generation system 140 selects one or more pieces of additional visual content, such as one or more photos, images, video clips, and the like. The immersive interface generation system 140 can also select one or more pieces of additional audio content and/or additional text content to combine with the selected source content to generate an immersive interface.
In some examples, the immersive interface generation system 140 selects additional audio content based on text-to-speech (TTS) audio generated from the source content. For example, the immersive interface generation system 140 can generate text-to-speech content or voice-over associated with a particular language, region, dialect, place, genre, licensed characters, licensed voices, user preferences, and the like. In various examples, the additional audio content is not limited to text-to-speech and may include any one or more of a variety of types of audio, including but not limited to background music, sound clips, sound effects, actual speech narration, text-to-speech audio, and/or other types of audio content, may be presented alone and/or with the selected source content in the immersive user interface.
At (208), the immersive interface generation system 140 generates target content based at least in part on combining each respective content segment with the corresponding identified additional content. In an example, the immersive interface generation system 140 automatically generates one or more pieces of logo content for the immersive user interface. For example, the immersive interface generation system 140 can automatically combine, merge, mix, edit, curate, arrange, resize, and/or otherwise generate a new piece of targeted content for the immersive interface from the selected content segments and the corresponding additional content identified for the selected content segments. In some examples, a piece of generated target content may include, but is not limited to, one or more timed or non-timed display slides, multimedia clips, and/or any other form or type of target content, which may be included as part of an immersive user interface.
In various examples, the immersive interface generation system 140 processes the selected content segments of a single content type (e.g., one of text, audio, or visual content) and automatically generates target content of the immersive interface including two, three, four, and/or any number of additional content types (e.g., text, audio, visual, augmented reality, virtual reality, etc.). In one example, the immersive interface generation system 140 can automatically generate a new piece of immersive target content from a single type of source content by editing, arranging, and/or combining selected segments of the source content with corresponding additional visual content and corresponding additional audio content to generate target content including at least three types of content for the immersive user interface. For example, selected source content including text such as "nice and samm is cooking food (NICK AND SAM ARE cooking up a storm)" may be automatically combined into an item of target content with additional visual content including nice and samm being cooked together in the kitchen. Additionally, automatically generated text-to-speech of the selected source content text and/or any other additional audible content (e.g., cooking sounds, nike and samum talk cooked sounds, etc.) may also be combined for use in the immersive interface.
In various examples, the immersive interface generation system 140 automatically integrates the plurality of generated target content together into the immersive user interface. For example, a collection of two or more automatically generated pieces of target content may be integrated together into an immersive user interface that includes a plurality of different content types (text, audio, video, etc.). In some examples, the immersive interface generation system 140 may generate a set of immersive display slides, multimedia slides, or multimedia clips for the user interface, such as a tile user interface or an interactive story page with automatic seamless user interface progression (e.g., without user interaction) that closely combines flows at a configured or predefined cadence. As such, in some examples, the immersive interface generation system 140 can automatically transform lengthy paragraphs (LENGTHY PASSAGES) of long form text that are difficult to understand, navigate, and require repeated scrolling or other user interactions (e.g., on various types of mobile devices) into new audio-visual and immersive forms for new internet users and other users of any skill level. In some examples, the immersive interface generation system 140 can also automatically transform video content (e.g., raw, lengthy, etc.) into summaries that are presented via the immersive content. Further, the immersive interface generation system 140 may be used to transform a large existing corpus of non-visual content into an immersive interface.
At (210), the immersive interface generation system 140 provides the generated target content to the computing device for presentation to the user. In an example, the immersive interface generation system 140 transmits the automatically generated immersive interface to one or more computing devices for presentation, display, communication to one or more users. In some examples, the immersive interface generation system 140 can provide one or more generated immersive interfaces to the user device 130 in response to a search query or any other type of user request. The generated immersive interface may be provided in one or more forms including, but not limited to, as an animated tile in a tile-based user interface, as content in a feed, as a post in a social media feed or on a social media site, as a carousel user interface or post, as a set of story pages, and/or generally as part of any other type of user interface or in various other forms for presentation. In some examples, the immersive interface may be provided as an accelerated moving page and/or in a web story format that mixes various audio, video, images, graphics, animations and/or text in order to provide a dynamic immersive experience to the user.
Fig. 3 depicts a flowchart of an example method 300 for performing automatic generation of an immersive interface in response to a user request in accordance with an example embodiment of the present disclosure. The method 300 and other processes described herein are illustrated as a collection of blocks that specify operations to be performed, but are not necessarily limited to the orders or combinations of operations by the respective blocks shown. Accordingly, one or more of the various portions of method 300 may be omitted, rearranged, combined, and/or adjusted in various ways without departing from the scope of this disclosure.
One or more portions of method 300, as well as other processes described herein, may be implemented by one or more computing devices, such as, for example, one or more computing devices of a computing environment, e.g., as shown in example computing environment 100 of fig. 1 (e.g., one or more of server machine 110, search server machine 120, user device 130, etc.). Although portions of the following discussion may be made with respect to a particular computing environment, such references are by way of example only. The operations are not limited to being performed by one entity or multiple entities operating on a device or in any particular computing environment. As such, any one or more portions of these processes may be implemented as operations on hardware components of the devices described herein.
At (302), a computing system receives a request from a computing device. In an example, the immersive interface generation system 140 receives a request associated with a search query or any other type of information request. In some examples, the request may be received based on selection of a hyperlink or a user interface event. For example, a user may submit a request for information regarding one or more selectable topics, categories, channels, interests, or other selectable information presented via a user interface (e.g., one or more sports categories, food types, recipes, health topics, nutrition topics, beauty topics, music, entertainment, news, etc.).
In one example, the immersive interface generation system 140 analyzes the request and determines one or more content sources to analyze based on the request. In some examples, the immersive interface generation system 140 analyzes available content sources and determines that one or more previously generated immersive interfaces correspond to the request. In some examples, the immersive interface generation system 140 can return information about existing immersive interfaces in response to the request, for example, wherein one or more immersive interfaces have been previously generated for new events, topics, searches, events, and the like. For example, the immersive interface generation system 140 of the search server machine 120 may generate an immersive interface based on trend (trending) topics (e.g., top 1, 5, 10, n) associated with the internet or other type of search engine. In this way, one or more pre-generated immersive interfaces may then be returned in response to the relevant search queries or user selections of such topics.
In various examples, the immersive interface generation system 140 analyzes information associated with the request to determine source content to analyze. For example, the immersive interface generation system 140 can perform one or more types of processing on the request, including, but not limited to, natural language processing. In some examples, immersive interface generation system 140 is used in association with search server machine 120 and/or search index 122 to identify relevant source content and/or additional content to analyze based on the request. In some examples, the immersive interface generation system 140 may analyze information associated with the request to determine one or more locations of the data stores 116, 136, 150, content repositories, document management systems, partner systems, collections, clips, and/or others to identify and obtain source content associated with the request. In some examples, the immersive interface generation system 140 obtains information from one or more license management software applications and/or systems and analyzes, at least in part, to identify and obtain source content for the request.
At (304), the computing system analyzes the one or more pieces of source content in view of the request. In an example, the immersive interface generation system 140 locates, obtains, and analyzes one or more pieces of source content based on the request. In various examples, the immersive interface generation system 140 locates and analyzes one or more pieces of source content that provide knowledge and facts about the topic associated with the request.
In various examples, the one or more pieces of source content may be crowd-sourced (crowdsourced) content (e.g., one or moreArticles or other types of crowd-sourced content in any language). Other examples of source content may include, but are not limited to, source content in any form (e.g., file, feed, form, stream, etc.) obtained from any permitted source (e.g., created and/or otherwise owned by a user or organization, from a partner content provider or distributor, from a licensor, from a government agency, from a public domain, etc.).
In various examples, the immersive interface generation system 140 of the computing system analyzes the source content and/or associated metadata to identify one or more pieces of source content that include fact information or other data related to the one or more items associated with the request. For example, the immersive interface generation system 140 can identify relevant source content associated with a particular topic, theme, person, category, activity, article, recipe, news event, sports event, or the like. In addition, the immersive interface generation system 140 can also analyze one or more identified pieces of source content associated with the request to identify particular segments from such source content for use in generating the immersive interface. In some examples, the immersive interface generation system 140 translates each of the one or more pieces of source content into a standard or common language.
At (306), the computing system extracts at least one content segment from each of the one or more pieces of source content for generating an immersive interface. In an example, the immersive interface generation system 140 analyzes each of the one or more identified pieces of source content including the fact information and/or data associated with the request. In various examples, the immersive interface generation system 140 analyzes the source content to identify one or more particular portions of each of the one or more pieces of source content used to generate the immersive interface.
In various examples, the immersive interface generation system 140 extracts one or more content segments from each of the one or more pieces of source content for generating content of the immersive interface. For example, the immersive interface generation system 140 can extract one or more words, phrases, sentences, paragraphs, sections, and/or any other subset of content from each of one or more documents (such as long form text documents) based on analyzing the one or more pieces of source content. In an example, the immersive interface generation system 140 generates a summary of the extracted content segments for generating an immersive interface. In some examples, the immersive interface generation system 140 uses one or more forms of natural language processing to analyze the various source content individually and/or collectively and extract summaries of the selected content segments. For example, the immersive interface generation system 140 may perform various types of processing to organize, sort, reorder, merge, and/or otherwise generate logical and tightly-coupled summaries from a set of content segments associated with generating the immersive interface (cohesive summary). In one example, the immersive interface generation system 140 automatically analyzes one or more pieces of source content, each piece of source content including long form text (e.g., text of 1, 10, 100, 1000, 10000, 100000, or any number of pages), extracts content pieces based on the long form text of the source content, and generates a brief summary from the selected content pieces (e.g., 4, 8, 12 sentences, or phrases) for creating the immersive interface.
In various examples, the immersive interface generation system 140 analyzes each of the one or more sentences, phrases, or other fragments of the summarized source content to identify and assign weights to each of the one or more entities, activities, and/or relationships in the summarized source content. For example, people, objects, animals, scenes, locations, landmarks, activities, and/or other types of information may be identified in summarized source content based on analyzing the summarized source content in text, audio, visual, and/or other forms.
In an example, each of the one or more entities, activities, and/or relationships of a particular summarized source content may be weighted, e.g., based on one or more of organization, ordering, location, salience, duration, association, interaction, and/or any other attribute or aspect of information determined based on analyzing the particular summarized source content. In some examples, entities, activities, and/or relationships may be identified, extracted, and/or weighted based on information associated with knowledge graphs, knowledge bases, or any other type of system or data structure that provides information describing various attributes associated with entities, objects, events, relationships, conditions, and the like. In various examples, the immersive interface generation system 140 utilizes entities, activities, and/or relationships identified and weighted from source content to identify additional content to match source content in the immersive interface.
At (308), the computing system obtains additional content corresponding to each of the one or more respective content segments. In an example, the immersive interface generation system 140 automatically identifies and obtains additional content for inclusion in the target content with each of the respective content segments based on identified and weighted entities, activities, and/or relationships found in the source content (e.g., original, modified, and/or summarized source content). For example, the immersive interface generation system 140 can search, locate, obtain, analyze, score, rank, compare, evaluate, and/or perform various other processes on additional content that is available and authorized for identifying one or more pieces of additional content for inclusion in the immersive interface with the particular pieces of source content.
In various examples, the immersive interface generation system 140 identifies one or more sets of additional content authorized and available for analysis based on information available from one or more license management, software applications, and/or systems. For example, the license management software application or system can provide information indicating additional content owned by the user or other party generating the immersive interface, additional content available from the collaborative organization and the publisher, additional content licensed from one or more other parties, additional content available from the public domain, and/or any other source of additional licensed content authorized for use.
In various examples, the immersive interface generation system 140 obtains and analyzes one or more pieces of additional content to determine additional content to be matched and combined with a particular selected piece of source content in the immersive interface. For example, the immersive interface generation system 140 may execute one or more queries based on the identified entities, activities, relationships, and/or other concepts in the source content and their associated weights to identify, analyze, score, and/or rank one or more additional pieces of content, such as images, video, audio, and text, based on how closely each of the additional pieces of content is related to the selected source content.
The immersive interface generation system 140 may perform any type of visual, audio, or text analysis to evaluate, score, and/or rank any piece of additional content. In some examples, the immersive interface generation system 140 analyzes the visual content using image segmentation, image classification, object detection, image recognition, object recognition, and/or any other type of analysis of visual additional content (e.g., images, video, multimedia content with visual components, etc.). In some examples, the immersive interface generation system 140 analyzes the audio content to detect, identify, interpret, identify various types of sounds, speech, noise, interactions, and/or any other aspect of additional content associated with the audio (e.g., recordings, sound clips, sounds associated with multimedia, such as video, etc.). In some examples, the immersive interface generation system 140 analyzes the text, for example, using text processing, natural language processing, and/or any other type of processing, to identify additional content for various aspects associated with the text.
In an example, the immersive interface generation system 140 selects one or more pieces of additional content for inclusion in the immersive interface with the source content segments based on scoring, ordering, or otherwise evaluating the attributes of the additional content as compared to the attributes of the respective pieces of source content. In some examples, the immersive interface generation system 140 selects one or more additional visual content, such as one or more images, one or more video clips or clips, for inclusion with a particular piece of text source content. The immersive interface generation system 140 can also select one or more additional audio content and/or additional text content for inclusion with a particular piece of source content.
At (310), the computing system generates audio for the respective content segments. In an example, the immersive interface generation system 140 generates audio of one or more source content and/or one or more additional content for use in the immersive interface. For example, the immersive interface generation system 140 can generate audio for each sentence or phrase in a piece of source content and/or an additional piece of content. In some examples, the immersive interface generation system 140 includes and/or adds markup language to text and/or metadata associated with text. For example, the immersive interface generation system 140 of the computing system may add a Speech Synthesis Markup Language (SSML) or any other type of markup, including, but not limited to, embedded voice commands, text-to-speech (TTS) markup, or any type of similar information that supports the generation and playback of text-based audio. In some examples, the immersive interface generation system 140 includes timestamp information for supporting synchronous presentation of text or synchronous highlighting of text when corresponding text-to-speech audio is played in the immersive interface (HIGHLIGHTING).
At (312), the computing system determines a template for generating the immersive interface. In an example, the immersive interface generation system 140 determines and selects an immersive interface template for generating one or more immersive interfaces. For example, the immersive interface generation system 140 may determine and select an immersive interface template from a plurality of different immersive interface templates available for use. In some examples, the immersive interface generation system 140 can determine and select one or more immersive interface templates to be automatically used based on one or more of information associated with the request, one or more user preferences, user selections, information or attributes associated with the source content, one or more content categories (e.g., news, articles, health, government, work, education, sports, art, entertainment, cooking, etc.) determined from the source content and/or additional content, and the like. In some examples, the immersive interface generation system 140 can determine and select the immersive interface template automatically, by default, or based on user input.
In an example, the immersive interface generation system 140 can determine and select an immersive interface template from a plurality of immersive interface templates associated with a category and/or for a particular use. The immersive interface templates may generally provide support for generating an immersive interface that combines text, visual, and audio content in various forms. The immersive interface template may also provide support for combining various forms of content in the augmented reality and virtual reality interfaces. In general, an immersive interface template can generally provide a set of one or more style attributes associated with an immersive interface, which can include attributes present in the original content and/or style attributes applied as part of an automated editing process. In various examples, the immersive interface may provide a predetermined or standardized format for a type of content based on style attributes that may generally include, but are not limited to, layout, format, length, presentation, framing, positioning, size, ratio, angle, motion, focus, scaling, animation, conversion, timing, automatic navigation, user interface controls, video rate, color, lighting, audio rate, background image, attributes, notifications, text, and/or any other attributes associated with the various types of immersive interfaces.
At (314), the computing system automatically generates an immersive interface based on the selected content segments, the identified additional content corresponding to the respective content segments, and the generated audio. In an example, the immersive interface generation system 140 automatically generates an immersive interface from a plurality of different source content segments based on automatically editing, mixing, combining, resizing, and/or arranging each selected content segment and corresponding additional content (e.g., one or more pieces of additional visual, audio, and/or textual content) in view of the immersive interface template to generate each new target content for inclusion in the immersive interface. In some examples, the immersive interface generation system 140 also combines the generated audio, such as text-to-speech narration (narration), with each of the one or more selected content segments and corresponding additional content as each new piece of target content is generated for inclusion in the immersive interface.
In an example, the immersive interface generation system 140 automatically generates each of the one or more new pieces of target content of the immersive interface based on the attributes and information associated with the immersive interface template. For example, the immersive interface template may include information about details for performing automatic generation of the immersive interface. Such details may include, but are not limited to, default and/or specific background colors, layering of elements, layout, placement of different types of content, aspect ratios, launch page, default message, default graphics, logo, branding, visual content placement and sizing, resolution and quality of visual content, formatting across immersive interface segments, sizing, self-propelling, aspects of one or more of the various style attributes, language, localization, and the like.
In an example, the immersive interface generation system 140 automatically generates a new piece of target content for the immersive interface based on the selected piece of content and its corresponding additional content. For example, the immersive interface generation system 140 can analyze the associated visual content and automatically determine and extract one or more relevant portions of the visual content. The immersive interface generation system 140 may also determine the placement of text content for combination with the extracted visual content. In addition, the immersive interface generation system 140 can determine and apply animation, graphics, audio, timing, and/or other information of a new piece of content. In various examples, the immersive interface generation system 140 can perform one or more other operations to mix, combine, or otherwise integrate one or more portions of the selected content segments with one or more portions of the additional content, animation, effects, generated audio, and/or other content to automatically create a new piece of target content for the immersive interface.
In an example, the immersive interface generation system 140 automatically generates an immersive interface, such as a display slide, a multimedia clip, and/or a collection in other forms or arrangements as new content, by integrating different logo content automatically generated from selected source content, additional content, and/or other sources into a single immersive user interface. In some examples, the immersive user interface may include any one or more of animations, timed or automatic progress, transitions, and the like. In various examples, the immersive interface generation system 140 may generate a set of immersive display slides, multimedia slides, or multimedia clips for a user interface, such as a tile user (e.g., a sliding tile) interface or an interactive story page. In some examples, the immersive interface generation system 140 generates an immersive interface provided to the user device, allowing any user to view and understand information being presented via immersion, regardless of skill level or experience.
At (316), the computing system provides the generated immersive interface to one or more devices. In an example, the immersive interface generation system 140 provides the generated target content to one or more computing devices for presentation, display, and/or communication to one or more users. In some examples, the immersive interface generation system 140 can provide one or more immersive interfaces to the search server machine 120 for indexing and ordering. For example, a search server machine may analyze, index, rank, publish, and/or provide one or more generated immersive interfaces to one or more computing devices in response to a search query or other request. The immersive interface generation system 140 can automatically generate one or more immersive interfaces, for example, in response to a request or prior to receiving the request. In some examples, the immersive interface generation system 140 can update one or more generated immersive interfaces based on information associated with the user request. For example, the immersive interface generation system 140 can convert, transform, update, and/or otherwise modify one or more existing immersive interfaces based on any other information including, but not limited to, user preferences, detecting language associated with a user or request, receiving location information from a device associated with a request, and/or allowing the immersive interface generation system 140 to automatically transform and customize (tailor) new and/or existing immersive interfaces for a particular user or group of users based on available information.
In various examples, the immersive interface generation system 140 may provide the immersive interface in one or more forms, including, but not limited to, as an animated tile in a tile-based user interface, as content in a feed, as a post on social media, as a carousel user interface or post, as a set of story pages, and/or generally as part of any other type of user interface or in any other form for communication to a user. In some examples, the immersive interface may be provided as an accelerated moving page and/or in a web story format that mixes various audio, video, images, graphics, animations and/or text. In some examples, users and content publishers may store the generated immersive interface in data store 150 (e.g., cloud storage) and share, publish, or otherwise communicate the generated immersive interface via social media sites, content sharing sites, news sites, and/or via other various types of computing systems, services, and networks.
Fig. 4 depicts an example illustration 400 including two example immersive interfaces generated from an example piece of long form text source content in accordance with example embodiments of the present disclosure. The example illustration 400 includes an example piece of long-form text source content 402 displayed on a mobile device, an example portion of an immersive interface 404 that is automatically generated in one language based on the long-form text source content 402, and an example portion of a second immersive interface 406 that is generated in another language based on the long-form text source content 402.
In an example, the immersive interface generation system 140 receives a request to generate one or more immersive interfaces. For example, the request may include information about the long-form text source content 402 and/or other pieces of source content. The request may also be a search query for malaria or a command from a content provider or other user to generate immersive content for malaria. In an example, the immersive interface generation system 140 can locate and utilize the long form text source content 402 to generate one or more immersive interfaces associated with malaria. In one example, the immersive interface generation system 140 automatically generates an immersive interface for malaria from the long form text source content 402 of english (e.g., including an example portion of the immersive interface 404). In one example, the immersive interface generation system 140 automatically generates an immersive interface for malaria (e.g., including an example portion of the second immersive interface 406) from the long form text source content 402 of the hindi.
In an example, the immersive interface generation system 140 automatically generates an immersive interface in one language (e.g., english) and/or for a particular geographic location (e.g., the united states) or place (e.g., florida) based on the long form text source content 402. In another example, the immersive interface generation system 140 automatically generates an immersive interface in a different language (e.g., hindi) and/or for a different geographic location (e.g., india) or place (e.g., new dery) based on the long form text source content 402. For example, different versions of the immersive interface may be generated based on location information associated with the request, user preferences, information associated with the user account, and the like.
In an example, the immersive interface generation system 140 generates a new immersive interface based on the existing immersive interface. In one example, the immersive interface generation system 140 can automatically generate an immersive interface for a hindi based on existing immersive interfaces for english. For example, the immersive interface generation system 140 can automatically translate the language of an existing immersive interface into a different language for a new immersive interface based on the location information or preferences of the user associated with the request. Additionally or alternatively, the system may summarize the content again using a region-specific summarization engine. This allows the system to reduce the complexity of the text (e.g., sentence structure, vocabulary, terms) introduced by the translation. The immersive interface generation system 140 can also generate a new immersive interface using different immersive interface templates associated with a particular country or region. In addition, the immersive interface generation system 140 can also identify and utilize new additional content (e.g., country or region specific visual content, audio content, text-to-speech, narration, etc.) associated with a particular country or place that is familiar and understandable to the user when generating the new immersive interface. As an example, the system may use region-specific content, such as images or videos associated with a particular region. As a specific example, the system may use images of farmers from the inner plaska for users in the united states and images of farmers from thailand for users in thailand.
Fig. 5 depicts a flowchart of an example method 500 for processing a request to generate an immersive interface in accordance with an example embodiment of the present disclosure. The method 500 and other processes described herein are illustrated as a collection of blocks that specify operations to be performed, but are not necessarily limited to the orders or combinations of operations to be performed by the respective blocks. Accordingly, one or more of the various portions of method 500 may be omitted, rearranged, combined, and/or adjusted in various ways without departing from the scope of this disclosure.
One or more portions of method 500, as well as other processes described herein, may be implemented by one or more computing devices, such as, for example, one or more computing devices of a computing environment, e.g., as shown in example computing environment 100 of fig. 1 (e.g., one or more of server machine 110, search server machine 120, user device 130, etc.). Although portions of the following discussion may be made with respect to a particular computing environment, such references are by way of example only. The operations are not limited to being performed by one entity or multiple entities operating on a device or in any particular computing environment. As such, any one or more portions of these processes may be implemented as operations on hardware components of the devices described herein.
At (502), a computing system receives a request to generate an immersive interface. In some examples, the computing system may include a content platform that provides content to a user of a user device (e.g., user device 130 in fig. 1). In an example, the immersive interface generation system 140 receives a request to generate one or more immersive interfaces. For example, the request may be received from a content owner, a content publisher, an organization, an individual user, and/or any other type of entity seeking to generate an immersive interface based on any existing authorized content. In an example, the request may be received in association with any type of software application, site, or system that provides social media, content sharing, image or photo sharing, video sharing, content publishing, content editing, and/or any other type of service.
In various examples, the immersive interface generation system 140 allows any number of users or entities (1, 10, hundreds, thousands, millions, etc.) to generate immersive interfaces using their own content and/or other authorized content permitted by one or more other parties. As an example, access to the immersive interface generation system 140 services can be provided to a user directly or indirectly via a computing device, an application (e.g., application 132), a user interface (e.g., user interface 134), a website, a computing system, an Application Programming Interface (API), a web service, or the like. In some examples, the request may include information indicative of one or more immersive interface templates and/or information about one or more pieces of source content used to generate the immersive interface.
In some examples, the system enables a user to view, approve, edit, publish, or download an immersive interface generated by the system. The immersive interface may be hosted locally by the user, the publisher, and/or remotely by the immersive interface generation system. In another example, search server machine 120 may expose immersive interface generation system capabilities in a Search Console (Search Console). In a manner similar to a video or image search experience, a user may be notified when new content is available based on the user's existing content. After viewing the content, they may edit, publish, download, or even approve the content for other purposes (e.g., display it in a search result or other screen/application).
At (504), the computing system provides one or more immersive interface templates for selection. In an example, the immersive interface generation system 140 provides one or more immersive interface templates that may be selected for generating the immersive content. For example, one or more immersive interface templates may be presented to the user to select to use their own content and/or other authorized content to generate an immersive interface. The immersive interface templates may be grouped together for presentation to the user, e.g., based on subject matter, channel, influencer, creator, and/or category, such as news, articles, health, government, work, education, sports, art, entertainment, cooking, travel, sports, exercise, and the like.
In an example, at least one immersive interface template may be selected by default. In some examples, the immersive interface generation system 140 receives a selection of an immersive interface template and generates an immersive interface using the selected template. The immersive interface generation system 140 can also generate a corresponding immersive interface using one or more default immersive interface templates. In some examples, the immersive interface generation system 140 receives a selection of a plurality of immersive interface templates and generates a different immersive interface using each immersive interface template based on common source content, e.g., to allow a user to compare, select, publish, share, and/or discard one or more of the generated immersive interfaces.
At (506), the computing system receives user content for generating an immersive interface. In an example, the immersive interface generation system 140 receives information from a user regarding one or more pieces of source content for generating one or more immersive interfaces. For example, a user may provide one or more pieces of text, images, photographs, audio and/or video source content. In some examples, the source content may be user-generated content associated with or shared with a user account. In some examples, a user may specify one or more particular pieces of source content. The user may also provide a location that includes one or more collections of source content. In an example, the user provides information about the source content and/or the source content itself from accounts associated with a photo sharing service, a video sharing service, a storage service, social media, and/or one or more other services, which the immersive interface generation system 140 can access and utilize to automatically generate an immersive interface from the source content. In one example, the source content may include, but is not limited to, user-generated or user-provided documents, text, writing, social media posts, images, pictures, photographs, videos, recordings, and the like.
At (508), the computing system automatically generates an immersive interface from the user content based on the selected immersive interface template. In an example, the immersive interface generation system 140 analyzes the source content, selects one or more content segments from each source content, summarises the selected content segments, divides the summarized selected content into a plurality of pieces, identifies and assigns a respective weight to each of one or more objects, activities, and/or relationships present in each summarized content, identifies and scores additional content associated with each summarized content for inclusion in the immersive user interface with the respective content segments, determines one or more additional content to be combined with each summarized content based on the scores or rankings associated with the scores, generates text-to-speech for text associated with the one or more summarized content and/or the corresponding additional content, generates an item of target content for the immersive interface by combining each summarized content with the corresponding additional content and the generated text-to-speech, and creates the immersive interface based on integrating the generated target content into the immersive interface.
At (510), the computing system prepares an automatically generated immersive interface for publication. In an example, the immersive interface generation system 140 allows a user to preview, select, approve, store, update, regenerate, discard, and/or perform one or more other actions associated with an automatically generated immersive interface. In some examples, the immersive interface generation system 140 allows a user to select one or more of a plurality of automatically generated immersive interfaces for storage and/or publication. In some examples, the immersive interface generation system 140 provides the automatically generated immersive interface to the search server machine 120 for analysis, indexing, and/or ordering. In some examples, the immersive interface generation system 140 may also provide the immersive interface generation system 140 to the server machine 110 for publication and/or distribution, for example, in response to a request from the user device 130.
At (512), the computing system provides the automatically generated immersive interface to the one or more computing devices. In an example, the immersive interface generation system 140 outputs or otherwise transmits the automatically generated immersive interface to one or more computing devices (e.g., the server machine 110, the search server machine 120, the user device 130, and/or any other type of computing device) for presentation, display, communication, or any other type of delivery to one or more users. In some examples, the immersive interface may be hosted on one or more servers using conventional content hosting techniques. The immersive interface may also be made local and shareable without involving a server.
Example devices and systems
FIG. 6 shows a diagrammatic representation of example machine in the form of a computer system 600 within which a set of instructions, for causing the machine to perform any one or more of the operations discussed herein, may be executed. In other examples, the machine may be connected (e.g., networked) to other machines in a LAN, intranet, extranet, or the internet. The machine may operate in the capacity of a server or a client machine in a client-server network environment, or as a peer machine in a peer-to-peer (or distributed) network environment. The machine may be a Personal Computer (PC), a tablet PC, a set-top box (STB), a Personal Digital Assistant (PDA), a cellular telephone, a wearable computing device, a network device, a server, a network router, switch or bridge, or any machine capable of executing a set of instructions (sequential or otherwise) that specify actions to be taken by that machine. Furthermore, while only a single machine is illustrated, the term "machine" shall also be taken to include any collection of machines that individually or jointly execute a set (or multiple sets) of instructions to perform any one or more of the operations discussed herein.
Computer system 600 includes at least one processing device (e.g., processor 602), a main memory 604 (e.g., read Only Memory (ROM), flash memory, dynamic Random Access Memory (DRAM) (such as Synchronous DRAM (SDRAM), double data rate (DDR SDRAM), or DRAM (RDRAM)), a static memory 606 (e.g., flash memory, static Random Access Memory (SRAM), etc.), and a data storage device 618, which communicate with each other via a bus 630.
The processor 602 represents one or more general-purpose processing devices, such as a microprocessor, central processing unit, or the like. More specifically, the processor 602 may be a Complex Instruction Set Computing (CISC) microprocessor, a Reduced Instruction Set Computing (RISC) microprocessor, a Very Long Instruction Word (VLIW) microprocessor, or a processor implementing other instruction sets or processors implementing a combination of instruction sets. The processor 602 may also be one or more special purpose processing devices such as an Application Specific Integrated Circuit (ASIC), a Field Programmable Gate Array (FPGA), a Digital Signal Processor (DSP), a network processor, or the like. The processor 602 is configured to execute instructions 622 for performing the operations discussed herein.
Computer system 600 may also include a network interface device 608. The computer system 600 may also include a video display unit 610 (e.g., a Liquid Crystal Display (LCD) or Cathode Ray Tube (CRT)), an alphanumeric input device 612 (e.g., a keyboard), a cursor control device 614 (e.g., a mouse), and a signal generation device 616 (e.g., a speaker).
The data storage device 618 may include a computer-readable storage medium 628 having stored thereon one or more sets of instructions 622 (e.g., software computer instructions) embodying any one or more of the examples described herein. The instructions 622 may also reside, completely or at least partially, within the main memory 604 and/or within the processor 602 during execution thereof by the computer system 600, the main memory 604 and the processor 602 also constituting computer-readable storage media. The instructions 622 may be transmitted or received over the network 620 via the network interface device 608.
In one example, the instructions 622 include instructions for one or more modules of an automatic immersive interface generation system (e.g., the immersive interface generation system 140 of fig. 1) and/or a software library (library) containing methods of invoking the immersive interface generation system 140. While the computer-readable storage medium 628 (machine-readable storage medium) is shown by way of example to be a single medium, the term "computer-readable storage medium" should be taken to include a single medium or multiple media (e.g., a centralized or distributed database, and/or associated caches and servers) that store the one or more sets of instructions. The term "computer-readable storage medium" may also include any medium that is capable of storing, encoding or carrying a set of instructions for execution by the machine and that cause the machine to perform any one or more of the operations of the present disclosure. Accordingly, the term "computer-readable storage medium" should be taken to include, but is not limited to, solid-state memories, optical media, and magnetic media.
Many details are set forth in the preceding description. However, it will be apparent to one of ordinary skill in the art having the benefit of the present disclosure that the present disclosure may be practiced without the specific details. In some instances, well-known structures and devices are shown in block diagram form, rather than in detail, in order to avoid obscuring the present disclosure.
Some portions of the detailed description have been presented in terms of procedures and symbolic representations of operations on data bits within a computer memory. Here, a process is generally considered to be a self-consistent (self-consistency) sequence of steps leading to a desired result. The steps are those requiring physical manipulations of physical quantities. Usually, though not necessarily, these quantities take the form of electrical or magnetic signals capable of being stored, transferred, combined, compared, and otherwise manipulated. It has proven convenient at times, principally for reasons of common usage, to refer to these signals as bits, values, elements, symbols, characters, terms, numbers, or the like.
It should be borne in mind, however, that all of these and similar terms are to be associated with the appropriate physical quantities and are merely convenient labels applied to these quantities. Unless specifically stated otherwise as apparent from the following discussions, it is appreciated that throughout the specification discussions utilizing terms such as "analyzing," "determining," "identifying," "adjusting," "sending," "receiving," "processing," or the like, refer to the action and processes of a computer system, or similar electronic computing device, that manipulates and transforms data represented as physical (e.g., electronic) quantities within the computer system's registers and memories into other data similarly represented as physical quantities within the computer system memories or registers or other such information storage, transmission or display devices.
Certain examples of the present disclosure also relate to an apparatus for performing the operations herein. The apparatus may be configured for the intended purpose or it may comprise a computer selectively activated or reconfigured by a computer program stored in the computer. Such a computer program may be stored in a computer readable storage medium, such as, but is not limited to, any type of disk including floppy disks, optical disks, CD-ROMs, and magnetic-optical disks, read-only memories (ROMs) Random Access Memories (RAMs), EPROMs, EEPROMs, magnetic or optical cards, or any type of media suitable for storing electronic instructions.
It is to be understood that the above description is intended to be illustrative, and not restrictive. Many other examples will be apparent to those of skill in the art upon reading and understanding the above description. The scope of the disclosure should, therefore, be determined with reference to the appended claims, along with the full scope of equivalents to which such claims are entitled.
Additional disclosure
The technology discussed herein refers to servers, databases, software applications, and other computer-based systems, as well as actions taken and information sent to and from such systems. The inherent flexibility of computer-based systems allows for a variety of possible configurations, combinations, and divisions of tasks and functions between and among components. For example, the processes discussed herein may be implemented using a single device or component or multiple devices or components working in combination. The database and applications may be implemented on a single system or distributed across multiple systems. The distributed components may operate sequentially or in parallel.
While the present subject matter has been described in detail with respect to various specific example embodiments thereof, each example is provided by way of explanation and not limitation of the present disclosure. Modifications, variations and equivalents to those embodiments will readily occur to those skilled in the art upon attaining an understanding of the foregoing. Accordingly, the subject disclosure does not preclude inclusion of such modifications, variations and/or additions to the present subject matter as would be readily apparent to one of ordinary skill in the art. For example, features illustrated or described as part of one embodiment can be used with another embodiment to yield still a further embodiment. Accordingly, the present disclosure is intended to cover such alternatives, modifications, and equivalents.
Claims (20)
1. A computer-implemented method, comprising:
analyzing, by the one or more processors, web-based resources including text content;
Extracting, by the one or more processors, a plurality of text content segments from the network-based resource;
Obtaining, by the one or more processors, visual content and audio content associated with each respective text content segment for each text content segment of the plurality of text content segments;
Generating, by one or more processors, target content for an audiovisual display of a network-based resource, wherein generating the target content includes combining at least a portion of each respective text content segment from a plurality of text content segments with visual content and audio content pertaining to the respective text content segment; and
Data describing the generated target content is provided by the one or more processors to the computing device for rendering an audiovisual display of the network-based resource.
2. The computer-implemented method of claim 1, further comprising:
Network-based resources are obtained by the one or more processors based at least in part on information associated with the request.
3. The computer-implemented method of claim 2, wherein the request is a search query received from a computing device.
4. The computer-implemented method of claim 3, wherein the data describing the generated target content is provided in response to a search query.
5. The computer-implemented method of claim 1, further comprising:
Determining, by the one or more processors, a template for generating target content for audiovisual display of the network-based resource, and
Wherein the generating target content for the audiovisual display of the network-based resource is based at least in part on the determined template.
6. The computer-implemented method of claim 1, further comprising:
Audio content is generated by the one or more processors based on converting one or more of the text content segments to speech.
7. The computer-implemented method of claim 1, further comprising:
Analyzing, by the one or more processors, each of the text content segments from the web-based resource; and
The method includes determining, by one or more processors, for each of a plurality of text content segments, a weight of one or more items identified in each text content segment, respectively.
8. A computing system, comprising:
A non-transitory computer readable medium; and
One or more processors communicatively coupled to a non-transitory computer-readable medium, wherein the one or more processors execute instructions from the non-transitory computer-readable medium that cause the computing system to:
Analyzing web-based resources including text content;
Extracting a plurality of text content segments from a web-based resource;
for each text content segment of the plurality of text content segments, obtaining visual content and audio content associated with each respective text content segment;
Generating target content for audiovisual display of a network-based resource, wherein generating the target content includes combining at least a portion of each respective text content segment from a plurality of text content segments with visual content and audio content pertaining to the respective text content segment; and
Data describing the generated target content is provided to a computing device for presenting the audiovisual display of the network-based resource.
9. The computing system of claim 8, wherein the computing system is further to:
Network-based resources are obtained by the one or more processors based at least in part on information associated with the request.
10. The computing system of claim 9, wherein the request is a search query received from a computing device.
11. The computer-implemented method of claim 10, wherein the data describing the generated target content is provided in response to a search query.
12. The computing system of claim 8, wherein the computing system is further to:
Determining templates for generating target content for audiovisual display of network-based resources, and
Wherein the generating target content for the audiovisual display of the network-based resource is based at least in part on the determined template.
13. The computing system of claim 8, wherein the computing system is further to:
audio content is generated based on converting one or more of the text content segments into speech.
14. The computing system of claim 8, wherein the computing system is further to:
Analyzing each of the text content segments from the web-based resource; and
For each of a plurality of text content segments, a weight of one or more items respectively identified in each text content segment is determined.
15. A non-transitory computer-readable medium having instructions that, when executed by one or more processors associated with a computing device, cause the computing device to:
Analyzing web-based resources including text content;
Extracting a plurality of text content segments from a web-based resource;
for each text content segment of the plurality of text content segments, obtaining visual content and audio content associated with each respective text content segment;
Generating target content for audiovisual display of a network-based resource, wherein generating the target content includes combining at least a portion of each respective text content segment from a plurality of text content segments with visual content and audio content pertaining to the respective text content segment; and
Data describing the generated target content is provided to a computing device for rendering an audiovisual display of the network-based resource.
16. The non-transitory computer-readable medium of claim 15, wherein the computing device is further to:
network-based resources are obtained based at least in part on information associated with the request.
17. The non-transitory computer-readable medium of claim 16, wherein the request is a search query received from a computing device and the data describing the generated target content is provided in response to the search query.
18. The non-transitory computer-readable medium of claim 15, wherein the computing device is further to:
Determining templates for generating target content for audiovisual display of network-based resources, and
Wherein the generating target content for the audiovisual display of the network-based resource is based at least in part on the determined template.
19. The non-transitory computer-readable medium of claim 15, wherein the computing device is further to:
audio content is generated based on converting one or more of the text content segments into speech.
20. The non-transitory computer-readable medium of claim 15, wherein the computing device is further to:
Analyzing each of the text content segments from the web-based resource; and
For each of a plurality of text content segments, a weight of one or more items respectively identified in each text content segment is determined.
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2021/042701 WO2023003555A1 (en) | 2021-07-22 | 2021-07-22 | Automated generation of immersive interfaces |
Publications (1)
Publication Number | Publication Date |
---|---|
CN117980896A true CN117980896A (en) | 2024-05-03 |
Family
ID=77338872
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202180102530.5A Pending CN117980896A (en) | 2021-07-22 | 2021-07-22 | Automatic generation of immersive interfaces |
Country Status (2)
Country | Link |
---|---|
CN (1) | CN117980896A (en) |
WO (1) | WO2023003555A1 (en) |
Family Cites Families (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11816436B2 (en) * | 2018-07-24 | 2023-11-14 | MachEye, Inc. | Automated summarization of extracted insight data |
-
2021
- 2021-07-22 WO PCT/US2021/042701 patent/WO2023003555A1/en unknown
- 2021-07-22 CN CN202180102530.5A patent/CN117980896A/en active Pending
Also Published As
Publication number | Publication date |
---|---|
WO2023003555A1 (en) | 2023-01-26 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US10325397B2 (en) | Systems and methods for assembling and/or displaying multimedia objects, modules or presentations | |
US10096145B2 (en) | Method and system for assembling animated media based on keyword and string input | |
US9372926B2 (en) | Intelligent video summaries in information access | |
US9218414B2 (en) | System, method, and user interface for a search engine based on multi-document summarization | |
Sharda et al. | Tourism blog visualizer for better tour planning | |
JP2014032656A (en) | Method, device and program to generate content link | |
US11166000B1 (en) | Creating a video for an audio file | |
US10783192B1 (en) | System, method, and user interface for a search engine based on multi-document summarization | |
Bellini et al. | Modeling performing arts metadata and relationships in content service for institutions | |
US11651039B1 (en) | System, method, and user interface for a search engine based on multi-document summarization | |
Kalender et al. | Videolization: knowledge graph based automated video generation from web content | |
US20200293160A1 (en) | System for superimposed communication by object oriented resource manipulation on a data network | |
WO2012145561A1 (en) | Systems and methods for assembling and/or displaying multimedia objects, modules or presentations | |
US20210375324A1 (en) | Automatic modification of values of content elements in a video | |
CN111523069B (en) | Method and system for realizing electronic book playing 3D effect based on 3D engine | |
CN117980896A (en) | Automatic generation of immersive interfaces | |
Niu | Hierarchical relationships in the bibliographic universe | |
Trilsbeek et al. | Increasing the future usage of endangered language archives | |
KR101396020B1 (en) | Method for providing authoring service of multimedia contents using authoring tool | |
KR102251513B1 (en) | Method and apparatus for generating contents for learning based on celeb's social media information using machine learning | |
Steiner et al. | I-search: a multimodal search engine based on rich unified content description (rucod) | |
Steiner | DC proposal: Enriching unstructured media content about events to enable semi-automated summaries, compilations, and improved search by leveraging social networks | |
Jangra et al. | Metadata standards for content description: Microdata, Microformats and JSON-LD | |
Liu et al. | Analysis of multimedia application and research based on web pages | |
DeCesare | Navigating multimedia: How to find internet video resources for teaching, learning, and research |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication |