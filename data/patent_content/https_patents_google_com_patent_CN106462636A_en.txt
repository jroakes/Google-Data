CN106462636A - Clarifying audible verbal information in video content - Google Patents
Clarifying audible verbal information in video content Download PDFInfo
- Publication number
- CN106462636A CN106462636A CN201580033369.5A CN201580033369A CN106462636A CN 106462636 A CN106462636 A CN 106462636A CN 201580033369 A CN201580033369 A CN 201580033369A CN 106462636 A CN106462636 A CN 106462636A
- Authority
- CN
- China
- Prior art keywords
- user
- media content
- items
- content
- client device
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/80—Generation or processing of content or additional data by content creator independently of the distribution process; Content per se
- H04N21/81—Monomedia components thereof
- H04N21/8126—Monomedia components thereof involving additional data, e.g. news, sports, stocks, weather forecasts
- H04N21/8133—Monomedia components thereof involving additional data, e.g. news, sports, stocks, weather forecasts specifically related to the content, e.g. biography of the actors in a movie, detailed information about an article seen in a video program
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/70—Information retrieval; Database structures therefor; File system structures therefor of video data
- G06F16/78—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/783—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
- G06F16/7844—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content using original textual content or text extracted from visual content or transcript of audio data
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F17/00—Digital computing or data processing equipment or methods, specially adapted for specific functions
- G06F17/40—Data acquisition and logging
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06Q—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES; SYSTEMS OR METHODS SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES, NOT OTHERWISE PROVIDED FOR
- G06Q10/00—Administration; Management
- G06Q10/06—Resources, workflows, human or project management; Enterprise or organisation planning; Enterprise or organisation modelling
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/26—Speech to text systems
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/20—Servers specifically adapted for the distribution of content, e.g. VOD servers; Operations thereof
- H04N21/23—Processing of content or additional data; Elementary server operations; Server middleware
- H04N21/233—Processing of audio elementary streams
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/20—Servers specifically adapted for the distribution of content, e.g. VOD servers; Operations thereof
- H04N21/23—Processing of content or additional data; Elementary server operations; Server middleware
- H04N21/235—Processing of additional data, e.g. scrambling of additional data or processing content descriptors
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/20—Servers specifically adapted for the distribution of content, e.g. VOD servers; Operations thereof
- H04N21/23—Processing of content or additional data; Elementary server operations; Server middleware
- H04N21/239—Interfacing the upstream path of the transmission network, e.g. prioritizing client content requests
- H04N21/2393—Interfacing the upstream path of the transmission network, e.g. prioritizing client content requests involving handling client requests
Abstract
A method at a server includes: receiving a user request to clarify audible verbal information associated with a media content item playing in proximity to a client device, where the user request includes an audio sample of the media content item and a user query, and the audio sample corresponds to a portion of the media content item proximate in time to issuance of the user query; in response to the user request: identifying the media content item and a first playback position in the media content corresponding to the audio sample; in accordance with the first playback position and identity of the media content item, obtaining textual information corresponding to the user query for a respective portion of the media content item; and transmitting to the client device at least a portion of the textual information.
Description
Technical field
This application describes the system and method for explaining the audible utterance information in video content.
Background technology
When viewing has the video of audible language content (song or the dialogue said etc.), user may be unclear
Content is heard on ground, or even gives sufficient attention and miss completely and hear content due to leaving a little while or not.
If user wants to hear content again, video can be backed by user has the position for not knowing language content so as to again
Secondary hear language content, or attempt on-line search information to find out language content.The two methods are all poorly efficient and/or invalid
's.Video is refunded to hear again the time for being spent of language content prolongation user's viewing video, and due to causing language
Say content unclear be probably the language content inherent character (for example, accent), problem may not accounted for.User is made to exist
Line search information makes user be distracted attention from video in itself, and possibly invalid, because the information with regard to language content
May not be that user is obtainable by searching for.
Content of the invention
For Voice ＆ Video content, user currently can not search for pleasant to the ear in such content via voice or text input
The word for arriving or the lyrics, and and then these words are immediately or shortly quickly and easily become after user hears
Clearly link up and be appreciated that.Word that embodiment specifically described herein is passed in audio or video or the lyrics and its up and down
Text (for example, immediately) after being heard is provided a user with and explains the word or the lyrics and its context.In some embodiment party
In formula, the word that hears and right is determined using the video from content, audio frequency and with combine of mating of the technology of general captions
Automatically assume the word and its context afterwards as the result of inquiry.Therefore, it can be presented in audio or video to user
Accurate word, sentence or the lyrics (for example, hearing during TV program, film or in music) that passes on, not exclusively interior
The mark or title of appearance and its associated metadata.Additionally, with regarding that user searches for and seeks to have been watched with regard to them or heard
The explanation of the word in frequency or audio content, can provide a user with regard to the context of word or emotion and/or its source
Additional information.Can provide for the request of the user's inquiry with regard to the word that passes under the situation of a large amount of scopes and background, bag
Include but be not limited to：The lyrics, the background that hears in commercial advertisement or perform in film and TV in TV performance theme song
The song of middle broadcasting, performer's talk, press Communique, the fact that read out by newsreader or politician, the dialogue of commentator,
Foreign language word (and/or its translation) and foreign place name (and/or its translation).And it is possible to recognize and present the pass of broadcaster
When will start or the commentary that terminates and the news with regard to the film of middle distribution at the cinema in new performance or series.
Word and its context are automatically illustrated or are explained by determining the vision that hears and/or audio content so that
Beholder can easy search for and be present in the word for illustrating and having explained of reception and registration or song in audio or video content
Word.
Used as another advantage, the technology makes it possible to realize accurate Search Results；For example, the embodiment is potential
Ground allows all TVs, order video and song to cause word to be quickly and easily made apparent from ground immediately after being heard
Link up and be appreciated that.
In addition, the word explains that feature can provide useful Search Results, even if word is not to be spoken by original
Person or singer say or sing.For example, if song " being sung by turning over " (for example, by different artisies or band performance) and simultaneously
The performing artist of non-original record, the feature can still provide the explanation of the lyrics in song.Even if video or audio frequency are live
And be never recorded (for example, live concert, live sport event, highlight), using the audio frequency in various data bases
Fingerprint recognition signal and the single contamination for mating with captions, it is also possible to provide a user with explanation.
Furthermore it is possible to by the explanation or illustrate feature with other service combinations to be that the user of user media content carries
For additional or subsidiary details, the such as search in search engine, or allow users to potentially watch again or receive
Them are listened to seek the information of the content of its explanation.
Another advantage is the information that can obtain the entity relevant with the word for being sought its explanation.For example, can obtain
With regard to performer or their role and he doing what information in the scene；The letter of the meaning of singer and her song
Breath；For sports commentary word, the background of sport event and the information in place is obtained；And news program is directed to, obtain news event
The source of thing and the information of background.
Enable a viewer to easily and rapidly understand that the word that passes in Voice ＆ Video content can also direct
User searches out knowledge using search engine and from video and music service in new context.
According to some embodiments, the combination using technology is determining Current Content.Can be according to taking from audio or video
The metadata of what content is being played in the identification of business (for example, broadcaster, music service, video content stream provider), or
Person determines content by the combination of video, audio frequency and matching technique with detection service.First number from content detection service
According to and/or result be used to via mating with various content identification data storehouses and determine Current Content.Via match time stab or
Closed-caption or general captions are determining position of the user in content.Identifying content and user is being determined in the content
In the case of interior position, in the appropriate and related time or in user's request, the content that has just heard from user is automatic
Ground compiling " closed-caption " or " general captions " is simultaneously presented to user.Can be via online (for example, on the world wide web (www)
Join available information to find context or the emotion of word, sentence or content.
According to some embodiments, user watches or listens to audio or video content, such as film or music, and does not listen
To the word that hears in audio frequency and/or video or the lyrics (for example, it is unclear that or inaudible sentence or the lyrics) or needs
Which is explained.In some embodiments, user inquiry service (via text, voice or with specific service application program in
User interface interacts), and based on the content automatically compile the audio frequency that has just heard result summary (for example, " general
Captions ", the text of transcript, lyrics text) and it is presented to user.
According to some embodiment there is provided in order to explain the method for the audible utterance information in video content, system with
And computer-readable recording medium.Receive and be associated in order to the items of media content that explains with play just near the client device
The user's request of audible utterance information.The user's request includes the audio sample of items of media content and user's inquiry.Audio sample
The part of the issue of user's inquiry is temporally close to corresponding to items of media content.In response to user's request, recognize in media
Hold the first playback position corresponding to audio sample in item and media content, according to the first playback position and the mark of items of media content
Know, for items of media content appropriate section obtain corresponding to user inquiry text message, and to client device send
At least a portion of text information.
Description of the drawings
Figure 1A-Figure 1B is the block diagram for illustrating the distributed client-server system according to some embodiments.
Fig. 2 is the block diagram of the structure for illustrating the example server system according to some embodiments.
Fig. 3 A is the block diagram of the structure for illustrating the example client end equipment according to some embodiments.
Fig. 3 B is the block diagram of the structure for illustrating the example client end equipment according to some embodiments.
Fig. 4 illustrates the Example data structure according to some embodiments.
Fig. 5 be illustrate according to some embodiments on the second device show with play on the first device in have
The flow chart of the general view of the process of the quotation content of pass.
Fig. 6 be illustrate according to some embodiments on the second device show with play on the first device in have
The flow chart of the general view of the process of the video content summary of pass.
Fig. 7 A, 7B and 7C are the illustrative screenshot according to some embodiments.
Fig. 8 A and 8B are the illustrative screenshot according to some embodiments.
Fig. 9 illustrate according to some embodiments for for recognize and store quotation method flow chart.
Figure 10 A- Figure 10 B is illustrated according to some embodiments for recognizing flow chart of the quotation for presenting.
Figure 11 illustrates the flow chart for the method for assuming quotation is used for according to some embodiments.
Figure 12 illustrates the flow chart for the method for being used for the summary for generating items of media content according to some embodiments.
Figure 13 is illustrated according to some embodiments for the method that generates the summary of items of media content with regard to the time period
Flow chart.
Figure 14 A- Figure 14 F is the illustrative screenshot according to some embodiments.
Figure 15 A- Figure 15 B is illustrated according to some embodiments for physically assuming the information with regard to items of media content
Flow chart.
Figure 16 is the general of the process of the text message for illustrating the display explanation audible utterance information according to some embodiments
The flow chart that lookes at.
Figure 17 A- Figure 17 B is the illustrative screenshot according to some embodiments.
Figure 18 A- Figure 18 C illustrates audible for be associated with items of media content for explanation according to some embodiments
The flow chart of the method for utterance information.
Identical reference number refers to corresponding part throughout each figure.
Specific embodiment
Method and system specifically described herein is disclosed for showing content of text to explain in visitor on a client device
The system of audible language content in the media content that or plays in the end equipment of family (for example, dialogue, song lyrics) and
Method.Such method and system provides the beholder acquisition beholder to video content and may miss or listen unclear
The effective means of the explanation of audible language content.
Various embodiments will be investigated in detail now, its example will be shown in the drawings.In the following detailed description, illustrate
Many specific detail are to provide the thorough understandings of the present invention and the embodiment.However, it is possible to not have these specific
Implement the present invention in the case of details.In other cases, do not describe in detail well-known method, program, component with
And circuit is in order to avoid unnecessarily obscure each side of embodiment.
Figure 1A is the block diagram for illustrating distributed system 100, and the distributed system 100 includes：Client device 102, client
End equipment 140, communication network 104, server system 106, video content system 112, one or more content host 170, can
The one or more social networkies 172 of selection of land and alternatively one or more search engines 174.Server system 106 is by logical
Communication network 104 is coupled to client device 102, client device 140, video content system 112, content host 170, social activity
Network 172 and search engine 174.
The function combinations of 112 server system 106 of video content system can be become individual server system.Some
In embodiment, server system 106 is embodied as individual server system, and in other embodiments, is implemented these as
The distributed system of multiple servers.Illustrate just to convenient, below server system 106 is described as in individual server
Realize in system.In some embodiments, video content system 112 is embodied as individual server system, and real at other
Apply in mode, implement these as the distributed system of multiple servers.Illustrate just to convenient, below by video content system
112 are described as realizing in individual server system.
Communication network 104 can be any wired or wireless LAN (LAN) and/or wide area network (WAN), such as inline
Net, extranet or the Internet.Communication network 104 provides client device 102 and 140, server system 106, video content system
Communication capacity between system 112, content host 170, social networkies 172 and search engine 174 is sufficient.In some enforcements
In mode, communication network 104 uses TCP/IP (TCP/IP) using HTML (Hypertext Markup Language) (HTTP)
To transmit information.HTTP allows client device 102 and 140 to access via the available various resources of communication network 104.However,
Various embodiments specifically described herein are not limited to the use of any specific protocol.
In some embodiments, system 106 includes front-end server 114, and which promotes server system 106 and network
Communication between 104.Front-end server 114 receives content information 142 from client 102 and/or client 140.In some realities
Apply in mode, content information 142 is video flowing or one part.In some embodiments, content information 142 is derived from client
(part for the video flowing that such as plays on client 102 is one or more with the part for the video flowing that plays on end 102
Fingerprint).In some embodiments, front-end server 114 is configured to 140 transmission content of client device.In some realities
Apply in mode, front-end server 114 is configured to send the content link for pointing to content.In some embodiments, front end clothes
Business device 114 is configured to send or receive one or more video flowings.
According to some embodiments, video or video flowing are the sequences of the image or frame for representing operating scene.Ying Jiang
Video is differentiated with image.Video is per second to show many images or frame.For example, video shows 30 successive frames each second.Compare
Under, image is not associated with any other image.
In some embodiments, server system 106 includes to store the customer data base 130 of user data.Some
In embodiment, customer data base 130 is distributed data base.
In some embodiments, server system 106 includes content identifier module 118, and which is included in order to from client
102 and/or client 140 receive content information 142, the content information is mated with the user supplied video content using fingerprints in fingerprint database 120
And mating recognizing the video content for just presenting at client device 102 (for example, based on content information and user supplied video content using fingerprints
" video content item " such as film, TV series collection, video clipping or any other distinctive video content segments) mould
Block.In some embodiments, content identifier module also recognizes that the current location in audio content (for example, is just set in client
Position in the video content for presenting on standby 102 or wherein how far).Present bit in the mark and video content of video content
Put and entity module 144 is passed to, relevant with the video content for being recognized or many in its identification entity data bak 122
Individual entity.
In some embodiments, server system 106 includes the fingerprint database 120 of storage content fingerprint.As herein
Used in user supplied video content using fingerprints be the content of video flowing and/or audio stream and/or corresponding with video flowing and/or audio stream general
Any kind of compression of captions/closed caption (subtitle/caption) data or close-coupled represent or sign.Some
In embodiment, fingerprint can represent that the editing of video flowing or audio stream or corresponding general captions/closed caption data is (such as several
Second, a few minutes or a few houres) or part.Or, fingerprint can represent video flowing or audio stream or general captions/closed caption data
The single moment (for example, the fingerprint of the fingerprint of the single frame of video or the audio frequency being associated with the frame of video or correspond to
The fingerprint of the general captions/closed caption of the frame of video).Further, since video content is changed over, the video content
Corresponding fingerprint will also be changed over.In some embodiments, fingerprint database 120 is distributed data base.
In some embodiments, client device 102 includes video module 110, and which receives from video content system 112
Video content 126, content information 142 is extracted simultaneously from the video content 126 (for example, video flowing) just played on client 102
Content information 142 is sent to server 106.
Client device 102 is to be connectable to communication network 104 in some embodiments, receive video content (example
Such as, video flowing), extract information from the video content and assume any suitable computer of video content on the display device 108
Equipment.In some embodiments, client device 102 is the Set Top Box for including to receive and present the component of video flowing.
For example, client device 102 could be for receiving Set Top Box, the digital video recorder of wired TV and/or satellite TV
(DVR), digital media receiver, TV tuner, computer and/or output TV signal any other equipment.Some other
In embodiment, client device 102 is computer, laptop computer, panel computer equipment, net book, mobile phone, intelligence
Phone, panel computer equipment, game station, multimedia player device or video content can be received (for example, as logical
Cross the video flowing of network 104) any other equipment.In some embodiments, client device 102 is in display device 108
Upper display of video streams.In some embodiments, client device 102 is conventional TV display, and which is connected to the Internet simultaneously
Display digit and/or simulation TV content via air broadcast or satellite or cable connection.
In some embodiments, display device 108 could be for assuming any display of video content to user.
In some embodiments, display device 108 is display or the computer monitor of TV, and which is configured to from client
102 receive and show audio and video frequency signal or other digital contents.In some embodiments, display device 108 is that have
CPU, memorizer and it is configured to receive and show in audio and video frequency signal or other numerals from client 102
The electronic equipment of the display of appearance.For example, display device can be lcd screen, panel computer equipment, mobile phone, projector
Or other types of video display system.Display 108 can be coupled to client 102 via wirelessly or non-wirelessly connecting.
In some embodiments, client device 102 receives video content 126 via TV signal 138.As this paper institute
The TV signal for using is to include electricity corresponding to the audio frequency of TV channel and/or video component, light or other types of data
Transmitting medium.In some embodiments, TV signal 138 is the aerial TV broadcast singal in land or in cable system or satellite system
The signal that distributes on system and/or broadcast.In some embodiments, TV signal 138 is sent out by network connection as data
Send.For example, client device 102 can receive video flowing from Internet connection.Herein sometimes by the audio frequency of TV signal and
Video component is referred to as audio signal and video signal.In some embodiments, TV signal is corresponded to just in display device
The TV channel for showing on 108.
In some embodiments, video content 126 is live telecast system (for example, the television content of first run, straight
Broadcast event).In some embodiments, video content 126 is the previous content that broadcasts (for example, in broadcast or non-broadcast channel
The replay of upper exhibition, in order to adapt to the broadcast later of the content of time zones differences).In some embodiments, video content 126 is
Recorded content (the content that for example, record is played back in DVR and from which；It is downloaded to nonvolatile memory completely and is stored in
The content for wherein and from which playing back).In some embodiments, video content is the content (such as Online Video) of streaming.
Video content 126 includes sub-audible sound or content.Sub-audible sound or content include audible utterance information or content (example
Such as, the dialogue that says or speech, have the song of the lyrics) and alternatively include audible non-linguistic information or content (for example, tone,
Acoustics, the song without the lyrics).In some embodiments, sub-audible sound is carried on the audio frequency in video content 126
In track.In certain embodiments, TV signaling bearer is for the letter of the sub-audible sound corresponding with the audio track on TV channel
Breath.In some embodiments, sub-audible sound is by the speaker (example being associated with display device 108 or client device 102
Such as, speaker 109) produce.
In some embodiments, the audible utterance information (speech that for example, says in TV signaling bearer and audio track
Language, the Radix Aconiti Kusnezoffii that says and/or dialogue, song lyrics) corresponding general captions or closed caption (for example, closed-caption) letter
Breath or data.General captions or closed caption are the text transcriptions of the linguistic information in video content.General captions or hiding word
Curtain can be simultaneously presented together with corresponding video content.For convenience's sake, hereinafter by general captions and hide
Captions are commonly referred to as " captions (subtitle) " and general captions/closed caption data are referred to as " caption data ".
Client device 140 can be any appropriate computer equipment for being connectable to communication network 104, such as count
Calculation machine, laptop computer, panel computer equipment, net book, the Internet letter newsstand (kiosk), personal digital assistant, mobile electricity
Words, game station or any other equipment that can communicate with server system 106.Client device 140 generally includes one
Or multiple processors, the nonvolatile memory of such as hard disk drive and display.Client device 140 can also be with defeated
Enter equipment, such as keyboard and mouse (as shown in Figure 3).In some embodiments, client device 140 includes to touch screen display
Show device.
In some embodiments, client device 140 is connected to display device 128.Display device 128 can be used
In any display for assuming video content to user.In some embodiments, display device 128 be TV display or
Computer monitor, which is configured to receive and show audio and video frequency signal or other digital contents from client 128.At certain
In a little embodiments, display device 128 is with CPU, memorizer and to be configured to receive simultaneously from client 140
Show the electronic equipment of the display of audio and video frequency signal or other digital contents.In some embodiments, display device
128 is the video display system of lcd screen, panel computer equipment, mobile phone, projector or any other type.Some
In embodiment, client device 140 is connected to display device 128.In some embodiments, display device 128 includes
Or be otherwise connected to produce raising for the audible stream corresponding with the audio frequency ingredient of TV signal or video flowing
Sound device.
In some embodiments, client device 140 is connected to client device via wirelessly or non-wirelessly connecting
102.Exist in some embodiments of such connection wherein, client device 140 is alternatively according to by client device 102
The instruction of offer, information and/or digital content (being referred to as the second screen message) and operate.In some embodiments, client
End equipment 102 issues instruction to client device 140, and the instruction makes client device 140 in display 128 and/or speaker
Assume the digital content of the digital content that supplements or with regard to being presented by client 102 on a display device 108 on 129.Some its
In its embodiment, server 106 issues instruction to client device 140, and the instruction promotes client device 140 in display
128 and/or speaker 129 on present supplement or digital content with regard to just being presented by client 102 on a display device 108 number
Word content.
In some embodiments, client device 140 includes mike, and which enables client device in client
102 receive sound (audio content) from client 102 when playing video content 126.Mike enables client device 102
With video content 126 played/viewing and store the audio content/sound rail being associated with the video content 126.With with
Herein for the same way described in client 102, client device 140 can be in this information locally stored and and then to clothes
Business 106 transmission content information 142 of device, content information 142 is any one or more in the following：The audio content of storage
Fingerprint, audio content itself, the fingerprint of the part/segment of audio content or the part of audio content.So, server 106
The video content 126 that is just playing on client 102 can be recognized, even if just the electronics of playing/viewing content sets in the above
Standby is not that equipment (such as, older television set) is realized in the Internet, is not linked to the Internet (temporarily or permanently) therefore
It is unable to transmission content information 142, or does not have the media information relevant with video content 126 is recorded or fingerprint recognition
Ability.Such arrangement (that is, 140 storage content information 142 of wherein the second screen equipment send it to server 106) permits
Family allowable receives the second screen content for being triggered in response to content information from server 106, and (no matter beholder is just at what
TV is watched in position) and the information (such as with video content 126 entity relevant information) relevant with video content 126.
In some embodiments, mike also enables client device 140 to receive phonetic entry from user.The language
Sound input is included for example in order to execute order and the inquiry to information or content or the request of specific operation.
In some embodiments, the content information 142 for being sent to server 106 from client 102 or 140 includes following
In any one or more：The fingerprint of the caption data for being stored, caption data itself, the part/segment of caption data or
The fingerprint of the part of caption data.So, server 106 can recognize the video content 126 that is just playing on client 102,
Even if the audio volume level in such as client 102 is too low, audio content can not audibly be detected by client device 140
To, the audio content distortion that exported by client 102 (for example, due to the poor transmission from video content system 112, by
Delayed in the disposal ability at client 102), even if or speaker 109 otherwise may not operate.
In some embodiments, client device 140 includes one or more applications 127.As herein in more detail
Ground is discussed, one or more applications 127 receive and present the information that receives from server 106, in such as video content
Entity and the information (being referred to as " entity information " 132) with regard to the entity in video content.In some embodiments, one or
Multiple applications 127 are received from server 106 and assume the information information-related with audible utterance in video content.In some realities
Apply in mode, application 127 includes assistance application.Assistance application is obtained based on multi-signal and assumes the information with regard to user,
The including but not limited to demographic information of user, the current location of equipment and/or user, the calendar of user, the contact of user
List, the social networkies of user, the search history of user, the position of the network browsing history, equipment and/or user of user are gone through
History, the statement preference of user, the content viewing history of user and it is currently being rendered to the content of user.
Server 106 includes entity data bak or storage vault 122.Entity data bak 122 is associated with video content
The data base of entity.As used herein entity is any clearly presence or the object being associated with video content.At certain
In a little embodiments, without limitation, entity include title, people, place, music, object, product, quotation and
Awards.For example, title includes movie title, serial title (for example, TV series title) and collection of drama title is (for example,
Collection of TV plays title).People includes cast member (for example, performer), cast member (for example, director, producer, music composition
Person etc.), the role in story, contest player, contest judge, host, guest and the people for mentioning.Place is included in story
Position, shooting ground and the position that mentions.Music includes the song used in video content and melody.Object is included in story
Object (for example, the light sword in " Star War ").Product include any any goods that mentions in video content or show,
Service or item (for example, mention book, due to product placement including the product in video content).Quotation is included to consider oneself as
The spoken dialog fragment of frequency content, the lines that is such as said by the role in video content or non-myth and epigram are (for example,
" be willing to former power with you with ").Awards include any awards (for example, the optimal man being associated with video content segments and its entity
Performer, the best achievement in directing, the best song etc.).It should be appreciated that these examples are non exhaustive, and the entity of other species is
Possible.
In some embodiments, entity data bak 122 also includes to indicate the chart network of the association between entity.Example
Such as, movie property (for example, as the movie title entity of the entity for representing film) is linked to its cast member's entity, play
Position entities, quotation entity etc. in group membership's entity, story.Chart network is realized using any proper data structure.
In some embodiments, entity data bak 122 also include when occur in video content item with regard to entity, quilt
Mention or be uttered the information of (for example, in the case of quotation).For example, for movie property, entity data bak 122 is deposited
Storage when occur with regard to such as specific role or cast member (for example, actual on screen), in activity scene (even if
In whole persistent period of the activity scene in film not on screen) information.This type of information can be stored as in video
Hold the time range (for example, 22 in item:30-24:47 time range means role or cast member from 30 seconds 22 minutes
Labelling is occurred in video content item to 47 seconds 24 minutes labellings).Similarly, entity data bak 122 is stored with regard in video
Hold when place in item occurs or be mentioned, when played song or melody be, when quotation is uttered, when object occurs
Or be mentioned, product when occur or be mentioned etc. information.
In some embodiments, the entity in entity data bak 122 is also associated with the non-physical outside entity data bak.
For example, the people's entity in entity data bak 122 can include to point to the chain of the web page of the News Stories being associated with that people
Connect.
In some embodiments, server 106 also includes the text corresponding to the audible utterance information in video content
Information.Text information be can display information, take text writing form, corresponding to the audible utterance information in video content.
The song that text message includes the transcription of the speech (for example, dialogue, aside etc.) that says in such as video, sung in video
The translation of the lyrics and the speech that says or the lyrics and/or Roman phonetic.The source of text message include for example caption data and
Online document (for example, by the web page of 170 trustship of content host and other documents, by 170 trustship of content host lyrics number
According to storehouse).
In some embodiments, server 106 by text message and point to text message source link (for example,
Point to the link of the document of trustship at the content host 170) it is stored in entity data bak 122.In entity data bak 122,
Corresponding text message and respective sources (people for for example, singing the lyrics, the people for saying speech, can be sung wherein with corresponding entity
Go out or say the movie or television serial of the lyrics or speech) associated.In some other embodiments, text message and refer to
The link of Xiang Qiyuan is stored in the independent data base in server 106 or storage vault (not shown).Individually data base/storage
Corresponding text message and respective sources in storehouse can be associated with the corresponding entity in entity data bak 122.Rise for convenience
See, explained below assumes that the link of text message and its source of sensing is stored in entity data bak 122.
In some embodiments, server 106 includes entity module 144, summary module 146, quotation module 148, stream
Row degree module 150 and audio frequency explanation module 152.Entity module 144 recognizes and extracts the entity relevant with video content and incites somebody to action
The entity of extraction is stored in entity data bak 122.In some embodiments, entity module 144 is from video content (for example,
From content information 142) and extract from other sources (for example, by web page of 170 trustship of content host) relevant with video content
Entity.In some embodiments, entity module 144 also select from entity data bak 122 one or more entities and by its
Front-end server 114 is presented to client device (for example, client device 140) to be sent to for presenting.
Summary module 146 generates the summary of video content.As used herein summary is associated with video content
The list of entity (entity for for example, occurring in video content or mentioning).In some embodiments, including in summary
Entity is to be confirmed as, with based on one or more popularity criterions, the reality that generally popular video content item is associated
Body, describes its details below；Summary is to generate with regard to video content item and do not carry out personalization for specific user.Some
In embodiment, including in the entity in summary being and being confirmed as in the video popular generally and with regard to specific user
Hold the associated entity of item；Summary is generated with regard to video content item and by personalized for specific user.In some realities
Apply in mode, including the entity in summary be be determined to be in the time period of definition (for example, certain moon, some day, certain
One week, specific hour (for example, Primetime hour) in some day etc.) generally popular video content is associated
The entity of (but being not necessarily all associated with same video content item)；Summary is not to generate with regard to particular video frequency content item
's.
Quotation module 148 recognizes the quotation in video content.The dialogue that video content is said with many.However, not
All lines of the dialogue that says or wording are all that interesting or popular or well-known or specific title or people are earnestly hoped
's.In some embodiments, quotation module 148 determine in the dialogue (that is, quotation) said with reference to popularity module 150 which
A little lines or wording are popular or well-known etc. (for example, based on for example refer to online and share), and therefore make
It is stored in entity data bak 122 for distinctive (distinct) entity.Quotation module 148 is analyzed by 170 He of content host
The non-video content (such as document (for example, web page) and social networkies) of 172 trustship of social networkies, to determine video content
In which lines of the dialogue that says and wording be shared, refer to or comment on, and be therefore worth as distinctive
Quotation entity is distinguishing.
Popularity module 150 determines the popularity of entity based on one or more criterions.In some embodiments, flow
Row degree module 150 determine real-time popularity (for example, the popularity in the nearest hour) and last popularity or
Popularity in the range of long period (popularity for example, year-to-date, popularity all the time etc.).
Distributed system 100 also includes one or more content host 170, one or more social networkies 172 and
Individual or multiple search engines 174.170 trustship of content host may be used to determine the content of the popularity of entity, such as wherein may be used
To refer to and comment on the web page of entity.Similarly, social networkies 172 are also included wherein it can be mentioned that and commenting on the interior of entity
Hold (for example, in user comment and model).Additionally, in social networkies 172, can share content, this is provided for entity
Popularity another tolerance.Search engine 174 can from client device 102 or 140 receive corresponding to entity inquiry, and
Return relevant information.
In some embodiments, server 106 includes audio frequency explanation module 152.Audio frequency explanation module 152 recognizes video
Audible information in content, and corresponding text message (for example, transcription, translation, Roman phonetic) is obtained from each introduces a collection, including
The caption data being associated with video content and online document.Audio frequency explanation module 152 by text message and/or point to source chain
Connect and be stored in entity data bak 122.In some embodiments, text information is by server 106 from audible utterance information
Generate.For example, audible utterance information can be processed by voice recognition module (not shown) to turn the speech in audible utterance information
Change text into.In some embodiments, text message is processed further to generate additional textual information.For example, from captions number
Can be translated by machine translation module (not shown) and the translation of generation transcription according to the transcription of the speech for obtaining.In some embodiments
In, audio frequency explanation module 152 recognizes audible utterance information and phase based on the position in the mark and video content of video content
Answer text message；By knowing that video content is the position (how many minutes for example, entered in video) in what and video content,
Audible utterance information can be obtained and corresponding text message (for example, obtains the word of the position in being synchronized to video content
Curtain).
In some embodiments, front-end server 114 from 140 receive user of client device ask, it include with just
The corresponding content information 142 of the video of broadcasting on client device 102, and its audio content is by client device 140
Pickup.User's request is to explain the request of the audible utterance information in audio content.Recognized based on content information 142 and regard
Frequently, and recognize the text message responded by the request and client device 140 is sent it to for presenting.
Figure 1B depicts the distributed system 180 similar with the distributed system 100 that describes in Figure 1A.In fig. ib, visitor
The feature of family end equipment 102 and 104 (Figure 1A) and component are incorporated in client device 182.In distributed system 180,
Client device 182 is received and assumes video content 126.Content information 142 is sent to server 106 by client device 182.
Server 106 recognizes video content and to 182 sending entity information 132 of client device for presenting.In other side, point
Cloth system 180 is same or like with distributed system 100.Therefore, details is not repeated here.
Fig. 2 is the block diagram for illustrating the server system 106 according to some embodiments.Server system 106 is generally included
One or more processing unit (CPU) 202, one or more networks or other communication interfaces 208, memorizer 206 and it is used for
One or more communication bus 204 by these component interconnection.Communication bus 204 alternatively includes to interconnect system component and control
Make the Circuits System (sometimes referred to as chipset) of the communication between which.Memorizer 206 includes high-speed random access memory, such as
DRAM, SRAM, DDR RAM or other random access solid state memory devices；And may include nonvolatile memory, such as one
Individual or multiple disk storage equipments, optical disc memory apparatus, flash memory device or other non-volatile solid-state memory devices.Memorizer
206 alternatively can include to be located remotely from the one or more storage devices at CPU 202.Memorizer 206 is (including memorizer
Non-volatile and volatile memory devices in 206) include non-transitory computer-readable storage media.In some embodiment party
In formula, the non-transitory computer-readable storage media storage following procedure of memorizer 206 or memorizer 206, module or data
Structure or its subset, including：Operating system 216, network communication module 218, content identifier module 118, fingerprint database 120,
Entity data bak 122, customer data base 130, entity module 144, summary module 146, quotation module 148 and popularity module
150.
Operating system 216 is included for processing various basic system services and the journey for executing the task of depending on hardware
Sequence.
Network communication module 218 is via one or more communications network interfaces 208 (wired or wireless) and one or more
Communication network (the Internet, other wide area networks, LAN, Metropolitan Area Network (MAN) etc.) is promoting the communication with miscellaneous equipment.
Fingerprint database 120 stores one or more user supplied video content using fingerprints 232.Fingerprint 232 includes that title 234, fingerprint audio is believed
Breath 236 and/or fingerprint video information 238 and associated with list 239.Title 234 recognizes corresponding user supplied video content using fingerprints 232.Example
Such as, title 234 can include the title of association TV programme, film or advertisement.In some embodiments, fingerprint audio information
The fingerprint of the editing (such as several seconds, a few minutes or a few houres) of 236 audio contents for including video flowing or audio stream or other pressures
Contracting represents.In some embodiments, fingerprint video information 238 includes editing (such as several seconds, a few minutes or several little of video flowing
When) fingerprint.In some embodiments, fingerprint 232 include to represent the fingerprint of a part of the caption data of video flowing or its
It represents.Fingerprint 232 in fingerprint database 120 is updated periodically.
Customer data base 124 includes the user data 240 for one or more users.In some embodiments, use
Include user identifier 242 and demographic information 244 in the user data of relative users 240-1.User identifier 242 is recognized
User.For example, user identifier 242 can be the IP address being associated with client device 102 or selected by user or by
The alphanumeric values for uniquely identifying user of server-assignment.Demographic information 244 includes the characteristic of the relative users.People
Mouth statistical information may include one or more in the group being made up of the following：Age, sex, income, geographical position, religion
Educate, wealth, religion, race, nationality, marital status, household size, employment state and political political parties and groups ownership.In some enforcements
In mode, for relative users user data also include one or more in the following：Search history (for example, user
Have been filed on to the search inquiry of search engine), content-browsing history (web page that for example, is watched by user) and content disappear
Take history (video that for example, user has watched).
Content identifier module 118 receives content information 142 from client 102 or 140, and recognize in client 102 or
The video content for presenting at 140.Content identifier module 118 includes fingerprint matching module 222.In some embodiments, content
Identification module 118 also includes fingerprint generation module 221, its other media content for preserving from content information 142 or by server
Generate fingerprint.
Fingerprint matching module 222 is by least a portion of content information (or content information for being generated by fingerprint generation module
142 fingerprint) mate with the fingerprint 232 in fingerprint database 120.The fingerprint 242 of coupling is sent to entity module 144, its
Retrieve the entity being associated with the fingerprint 242 of coupling.Fingerprint matching module 222 includes that the content for receiving from client 102 is believed
Breath 142.Content information 142 includes audio-frequency information 224, video information 226, user identifier 229 and alternatively caption data
(not shown).User identifier 229 recognizes the user being associated with client 102 or 140.For example, user identifier 229 is permissible
It is the IP address being associated with client device 102 or is selected by user or uniquely identify user's by server-assignment
Alphanumeric values.In some embodiments, contextual audio information 224 includes the video flowing that plays on client device 102
Or the editing (such as several seconds, a few minutes or a few houres) of audio stream.In some embodiments, audio content information 226 includes
The editing (such as several seconds, a few minutes or a few houres) of the video flowing that plays on client device 102.
Entity data bak 122 includes the entity being associated with video content.Entity number is further described below with reference to Fig. 4
According to storehouse 122.
Entity module 144 is based on the fingerprint 242 for being mated or other criterions from the entity number being associated with video content item
According to selection entity in storehouse.Selected entity can be subset (for example, the entity mould of the entity that quotes in the fingerprint 242 for being mated
Block 144 selects most popular in the entity that quotes in the fingerprint 242 for being mated).
Summary module 146 generates the summary of video content.Summary includes with regard to video content item or the time with regard to defining
Entity in the popular video content item of Duan Eryan.
Quotation module 148 is from video content itself (for example, using caption data) and from non-video content (for example, in web
Referring in the page and social networkies, shared and comment on) quotation in identification video content.
Popularity module 150 determines and updates the popularity of entity in entity data bak 122.
Audio frequency explanation module 152 recognizes the audible utterance information in video content, and obtains corresponding to the audible utterance
The text message of information.
In some embodiments, summary info 146, quotation module 148 and popularity module 150 are entity module
144 submodule.
Each in said elements can be stored in previously mentioned memory devices one or more in, and
Each in module or program corresponds to the instruction set for executing above-mentioned functions.The instruction set can be by one or more places
Reason device (for example, CPU 202) is executed.The module or program (that is, content identifier module 118) of above-identified need not be implemented as
Single software program, program or module, and therefore in various embodiments can be by the various sub-combinations of these modules
Or otherwise rearrange.In some embodiments, memorizer 206 can store the module data knot of above-identified
The subset of structure.Additionally, memorizer 206 can store the add-on module data structure being not described above.
Although Fig. 2 shows server system, Fig. 2 is more intended to various in one group of server as may be present in
The function description of feature is not as the structural representation of embodiment specifically described herein.In practice, and as ability
The technical staff in domain recognizes, separately shown item can be combined, and some items can be separated.For example, in fig. 2
Separately shown some items (for example, operating system 216 and network communication module 218) can be realized on a single server, and
And single item can be realized by one or more servers.Be used for realize server system 106 server actual quantity and
How in-between assigned characteristics will be different according to different embodiments, and may partly depend on system and make in peak value
Amount with the data service that must process during the period and during average use time.
Fig. 3 A is the block diagram for illustrating the client device 102 according to some embodiments.Client device 102 is generally included
One or more processing unit (CPU) 302, one or more networks or other communication interfaces 308, memorizer 306 and it is used for
One or more communication bus 304 by these component interconnection.Communication bus 304 alternatively includes to interconnect system component and control
Make the Circuits System (sometimes referred to as chipset) of the communication between which.Client device 102 can also include user interface, its bag
Include display device 313 and keyboard and/or mouse (or other pointer device) 314.Memorizer 306 includes that high random access is stored
Device, such as DRAM, SRAM, DDR RAM or other random access solid state memory devices；And may include non-volatile memories
Device, such as one or more disk storage equipments, optical disc memory apparatus, flash memory device or other nonvolatile solid state storages set
Standby.Memorizer 306 alternatively can include to be located remotely from the one or more storage devices at CPU 302.Memorizer 306 or
Non-volatile memory devices of the person as an alternative in memorizer 306 include non-transitory computer-readable storage media.Some
In embodiment, the computer-readable recording medium storage following procedure of memorizer 306 or memorizer 306, module data knot
Structure or its subset, including：Operating system 316, network communication module 318,110 data 320 of video module.
Client device 102 is included for receiving the video input/output 330 with outputting video streams.In some embodiment party
In formula, video input/output 330 is configured to receive video flowing from wireless radio transmission, satellite transmission and cable plant.Some
In embodiment, video input/output 330 is connected to Set Top Box.In some embodiments, 330 quilt of video input/output
It is connected to satellite dish (satellite dish).In some embodiments, video input/output 330 is connected
To antenna.In some embodiments, client device 102 received by network interface 308 video flowing (for example, by because
Special net is receiving video flowing), contrary with by video input.
In some embodiments, client device 102 includes the TV tuner for receiving video flowing or TV signal
332.
Operating system 316 is included for processing various basic system services and the journey for executing the task of depending on hardware
Sequence.
Network communication module 318 is via one or more communications network interfaces 308 (wired or wireless) and one or more
Communication network (the Internet, other wide area networks, LAN, Metropolitan Area Network (MAN) etc.) is promoting the communication with miscellaneous equipment.
Data 320 include video flowing 126.
Video module 126 is from 126 export content information 142 of video flowing.In some embodiments, content information 142 is wrapped
Include audio-frequency information 224, video information 226, user identifier 229 or its any combinations.User identifier 229 recognizes that client sets
Standby 102 user.For example, user identifier 229 can be the IP address being associated with client device 102 or be selected by user
Select or the alphanumeric values for uniquely identifying user by server-assignment.In some embodiments, audio-frequency information 224 includes
The editing (such as several seconds, a few minutes or a few houres) of video flowing or audio stream.In some embodiments, video information 226 can
To include the editing (such as several seconds, a few minutes or a few houres) of video flowing.In some embodiments, content information 142 includes
Caption data corresponding to video flowing.In some embodiments, video information 226 and audio-frequency information 224 be from client
The video flowing 126 that plays on 102 or played is derived.Video module 126 can generate the content for corresponding video flowing 346
Multiple set of information 142.
Each in the element of above-mentioned identification can be stored in one or more in previously mentioned memory devices
In, and each in module or program corresponds to the instruction set for executing above-mentioned functions.The instruction set can be by one
Or multiple processor (for example, CPU 302) execute.The module or program (that is, instruction set) of above-identified need not be implemented as
Single software program, program or module, and therefore in various embodiments can be by the various sub-combinations of these modules
Or otherwise rearrange.In some embodiments, memorizer 306 can store the module data knot of above-identified
The subset of structure.Additionally, memorizer 306 can store the add-on module data structure being not described above.
Although Fig. 3 A shows client device, Fig. 3 A is more intended to each in client device as may be present in
Plant the structural representation of the function description not as embodiment specifically described herein of feature.In practice, and as this
Technical staff's understanding in field is pointed to, and separately shown item can be combined, and some items can be separated.
Fig. 3 B is the block diagram for illustrating the client device 140 according to some embodiments.Client device 140 is generally included
One or more processing unit (CPU) 340, one or more networks or other communication interfaces 345, memorizer 346 and it is used for
One or more communication bus 341 by these component interconnection.Communication bus 341 alternatively includes to interconnect system component and control
Make the Circuits System (sometimes referred to as chipset) of the communication between which.Client device 140 can also include user interface, its bag
Include display device 343 and keyboard and/or mouse (or other pointer device) 344.Memorizer 346 includes that high random access is stored
Device, such as DRAM, SRAM, DDR RAM or other random access solid state memory devices；And may include non-volatile memories
Device, such as one or more disk storage equipments, optical disc memory apparatus, flash memory device or other nonvolatile solid state storages set
Standby.Memorizer 346 alternatively can include to be located remotely from the one or more storage devices at CPU 340.Memorizer 346 or
Non-volatile memory devices of the person as an alternative in memorizer 346 include non-transitory computer-readable storage media.Some
In embodiment, the computer-readable recording medium storage following procedure of memorizer 346 or memorizer 346, module data knot
Structure or its subset, including：Operating system 347, network communication module 348, figure module 349 and application 355.
Operating system 347 is included for processing various basic system services and the journey for executing the task of depending on hardware
Sequence.
Network communication module 348 is via one or more communications network interfaces 345 (wired or wireless) and one or more
Communication network (the Internet, other wide area networks, LAN, Metropolitan Area Network (MAN) etc.) is promoting the communication with miscellaneous equipment.
Client device 140 includes one or more applications 355.In some embodiments, application 355 includes browser
Application 355-1, media application 355-2 and assistance application 355-3.Browser application 355-1 shows web page.Media application
355-2 plays video and music, display image and manages playlist 356.(which is also referred to as " intelligent personal assistants " to assistance application
Application) 355-3 shows the information (video that is watching with user for for example, being provided relevant with user at present by server 106
Relevant entity 357；Appointment on the horizon；Traffic on route to be advanced), and execute or user relevant with user
Being asked for task or service (for example, transmission is reminded to notify the late, schedule of friend's supper appointment to update, call restaurant).
Application 328 is not limited to apply as discussed above.
Each in said elements can be stored in previously mentioned memory devices one or more in, and
Each in module or program corresponds to the instruction set for executing above-mentioned functions.The instruction set can be by one or more places
Reason device (for example, CPU 340) is executed.The module or program (that is, instruction set) of above-identified need not be implemented as individually soft
Part program, program or module, and therefore in various embodiments can be by the various sub-combinations of these modules or with which
Its mode is rearranged.In some embodiments, memorizer 306 can store the son of the module data structure of above-identified
Collection.Additionally, memorizer 306 can store the add-on module data structure being not described above.
Although Fig. 3 B shows client device, Fig. 3 B is more intended to each in client device as may be present in
Plant the structural representation of the function description not as embodiment specifically described herein of feature.In practice, and as this
The technical staff in field recognizes, separately shown item can be combined, and some items can be separated.
Fig. 4 illustrates the entity data structure 426 being stored in entity data bak 122 according to some embodiments.Accordingly
Entity 428 include entity identifier (entity ID) 448, entity type 450, entity name 452, other entities quoted
454th, to non-physical, 458, popularity metric 460 and alternatively additional information are quoted.In some embodiments, entity ID
448 uniquely identify corresponding entity 428.Entity type 450 recognizes the type of entity 428.For example, in entity data bak 122
The entity type 450 of corresponding entity 428 indicates that corresponding entity 428 is title, people, place, music, object, product, quotation
And awards.In some embodiments, entity type 450 also indicate subtype (for example, for people, performer or play staff
Staff or role or competitor or judge or host or guest or the people for mentioning).Entity name 452 is named for entity.
For example, depending on entity, entity name is title, name, place name, song or melody title, the object of movie or television performance
Title, name of product, the actual words of quotation or awards title.454 instructions are quoted to other entities 428 to other entities
Quote (for example, with its entity ID 448).For example, the entity 428 corresponding to movie title includes the performer's telogenesis to film
Member, cast member, role, place etc. quote 454.Quotation entity include to say quotation wherein video content (film,
TV performance etc.) and the quoting of people (performer, role etc.) of quotation is said in video content.In due course, real to other
The data of example 456 that quotes including occurring with regard to other entities or be mentioned of body.For example, the example of movie title entity
The time range that 456 data include cast member or role when occur or when product is mentioned etc..To non-physical
Quote 458 include to be not stored in entity data bak 122 as entity but still relevant with entity 428 content
(for example, point to mention entity web page link) quote.Popularity metric 460 provides the importance of entity file 428
Tolerance.In some embodiments, tolerance 460 is determined by popularity module 150.In some embodiments, popularity degree
Amount includes history and real-time popularity.
Show quotation
Fig. 5 be illustrate according to some embodiments on the second device show with play on the first device in have
The flow chart of the process 500 of the quotation content of pass.Fig. 5 provides the method for discussing in more detail in the discussion of Figure 10 A- Figure 11
1000 and 1100 general view.Video content system 112 sends video flowing (501) to client 102.Video flowing is by client
Equipment 102 is received and shows (502).When video flowing is played, determines the content information from video flowing and send it to
Server 106 (506).As described in other places in this application, in some embodiments, from the content information bag of video flowing
Include the audio frequency of video flowing and/or one or more editings (such as several seconds, a few minutes of video component or corresponding subtitle data
Or a few houres) or by client device 102 from the audio frequency of video flowing and/or video component or corresponding subtitle data
Fingerprint or other signatures that one or more editing (such as several seconds, a few minutes or a few houres) generate.In some embodiments,
Content information is formatted, and therefore which can be by easily compared with the user supplied video content using fingerprints for storing on the server.Server 106
Receive content information and the content information is mated (508) with user supplied video content using fingerprints.
In some embodiments, when video flowing is played, client device 140 is from from client device 102
(for example, the mike in client 140 is from client 1,020 for the audio output corresponding with the audio frequency ingredient of video flowing
Take audio output) determine content information.Client 140 determines content information and the content information is sent to server 106；Visitor
Family end 140 replaces 102 execution step 506 of client.
In some embodiments, before runtime by server (for example, using fingerprint generation module 221) from by
The media content (for example, audio frequency and/or video clipping or frame of video) that third party user uploads generates user supplied video content using fingerprints.At certain
In a little embodiments, by server (for example, using fingerprint generation module 221) in real time (for example, live) or operationally
Between before from the media content (for example, audio frequency and/or video clipping or frame of video) for having received from video content system 112
Generate user supplied video content using fingerprints.
Determine one or more quotations for being associated with the fingerprint for being mated and alternatively one or more of the other entity
(512)；Quotation is the lines or wording that says in video content, and other entities can include to say in video content
Performer/the role of quotation.In some embodiments, quotation determined by be the most popular quotation to video content item or
Part close to the video content item for presenting.An as used herein part close to video content item means
Close to currently assuming part in video content item on time.For example, if video content item is just 20:00 mark is broadcast
Put, then close to 20:The quotation of 00 labelling or the part including such quotation will be included from 20:00 labelling was risen in the definition time
In the range of (for example, plus/minus 15 minutes) quotation for saying.Quotation, one or more corresponding visual cues (affordance) with
And alternatively other entities are sent to client 140 (514).In some embodiments, via client to communication network
104 connection of the connection directly or between client 140 and client 102 is indirectly and by quotation and visible light
Rope is sent to client 140.In some embodiments, as the replacement for sending visual cues to client 140, server
106 send instruction to generate to the application (for example, assistance application 355-3 of Fig. 3 B) for being configured to present quotation and other entities
And assume corresponding visual cues at client 140.Client device 140 receive quotation, visual cues and alternatively other
Entity (516).Assume quotation and visual cues and alternatively other entities (518).In some embodiments, in time
With the video flowing 126 that is just played by client 102 mutually in phase on the display device 128 being associated with client device 140
Show one or more quotations and visual cues.For example, the quotation for presenting is included before current position of appearing in video
The quotation that has been said in predefined time period (for example, nearest half an hour from current location).In some embodiments, draw
Text includes the quotation after current position of appearing in video streaming.These quotations on the horizon can be prevented from showing until should
Till the position of the video flowing that quotation on the horizon is uttered in this place is presented, to avoid upsetting video content as user
The story of a play or opera.
Visual cues are included for activation to the various operations of corresponding quotation or the visual cues of action.In some enforcements
In mode, corresponding visual cues correspond to corresponding action；User select quotation and and then activate corresponding visual cues with
Activate the corresponding actions to selecting quotation.In some other embodiments, quotation shown by each have respective one or
The set of multiple visual cues；User activates the visual cues to corresponding quotation to activate the actions menu to corresponding quotation
Or action of the activation to corresponding quotation.The action and operation that can be activated with regard to quotation is described further below.
Show the summary of popular entity
Fig. 6 be illustrate according to some embodiments on the second device show with play on the first device in have
The flow chart of the process 600 of the summary of pass.Fig. 6 provides the method 1200 for discussing in more detail in the discussion of Figure 12-Figure 13
With 1300 general view.Video content system 112 sends video flowing (601) to client 102.Video flowing is by client device
102 receive and show (602).When video flowing is played, determines the content information from video flowing and send it to service
Device 106 (606).As described in other places in this application, in some embodiments, the content information from video flowing includes to regard
The audio frequency of frequency stream and/or one or more editings (such as several seconds, a few minutes or several of video component or corresponding subtitle data
Hour) or by client device 102 from one of the audio frequency of video flowing and/or video component or corresponding subtitle data or
Fingerprint or other signatures that multiple editing (such as several seconds, a few minutes or a few houres) generate.In some embodiments, content letter
Breath is formatted, and therefore which can be by easily compared with the user supplied video content using fingerprints for storing on the server.In server 106 is received
The content information is simultaneously mated (608) by appearance information with user supplied video content using fingerprints.
In some embodiments, when video flowing is played, client device 140 is from from client device 102
(for example, the mike in client 140 is from client 1,020 for the audio output corresponding with the audio frequency ingredient of video flowing
Take audio output) determine content information.Client 140 determines content information and the content information is sent to server 106；Visitor
Family end 140 replaces 102 execution step 606 of client.
In some embodiments, before runtime by server (for example, using fingerprint generation module 221) from by
The media content (for example, audio frequency and/or video clipping or frame of video) that third party user uploads generates user supplied video content using fingerprints.At certain
In a little embodiments, by server (for example, using fingerprint generation module 221) in real time (for example, live) or operationally
Between before from media content (for example, audio frequency and/or video clipping or the video for having received from video content system 112
Frame) generate user supplied video content using fingerprints.
Determine the summary (612) being associated with the fingerprint for being mated；The summary includes the most popular reality to video content item
Body.The summary is sent to client 140 (614).In some embodiments, via the company of client to communication network 104
Connect the connection directly or between client 140 and client 102 and indirectly summary is sent to client 140.
Client device 140 receives the summary (616).The summary is presented (618).In some embodiments, by client 102
The video flowing 126 for carrying out in (for example, at the end of video content item) after having completed, on display device 128 show
The summary.In some other embodiments, presenting or assuming end for any particular video frequency content item is being not dependent on
Time, assume the summary.
For showing example UI of quotation
Fig. 7 A, 7B and 7C illustrate the illustrative screenshot according to some embodiments.Each first client of diagram of Fig. 7 A, 7B and 7C
End 102 and the screenshotss of the second client 140.First client 102 play video content, and the second client 140 play with
The relevant quotation content of the video content of broadcasting in first client 102.Diagram in Fig. 7 A, 7B and 7C should substantially be illustrated as
Exemplary and nonrestrictive.In some embodiments, in response to server 106 by client fingerprint be stored in server
On user supplied video content using fingerprints coupling and the instruction of the second client device 140/application generation example is downloaded to by being serviced device 106 and is cut
Screen.In some embodiments, the user supplied video content using fingerprints in response to server 106 by client fingerprint with storage on the server
Join, by be stored on the second client device 140 instruction/application (such as browser, assistance application or other pre-configured should
With) in response to generating the illustrative screenshot from server 106 in order to show the instruction of certain content.
Fig. 7 A illustrates the screenshotss of the first client device 102 and the second client device 140.First client 102 shows
One for saying in TV series collection 702 and display 706 (for example, the assistance application) of application of the second client 140, collection of drama 702
Or multiple quotations 708 and the visual cues 710 corresponding to corresponding quotation 708.When collection of drama 702 is in the first client 102
During broadcasting, the content information that derives from collection of drama 702 is sent to server system 106 by the first client 102.As an alternative, second
The content information that derives from the audio output corresponding to collection of drama 702 from the first client 102 is sent to clothes by client 140
Business device system 106.Server system 106 mates content information so as to identification collection of drama 702 with user supplied video content using fingerprints.In identification and content
After the user supplied video content using fingerprints of information matches, server 106 determines (to be said with the relevant one or more quotations of collection of drama 702 in collection of drama
Go out) and the quotation and corresponding visual cues are sent to the second client device 140 for presenting.Second client device
140 assume quotation 708 and corresponding visual cues 710.Quotation 708 also includes when be uttered in collection of drama 702 for the quotation
Corresponding timestamp.In some embodiments, send additional information together with quotation and visual cues (for example, to say
The entity of quotation).
In some embodiments, user selects quotation (for example, by clicking on or tapping quotation 708) to recall with regard to drawing
The additional information of text.For example, as shown in fig.7b, if selected for quotation 708-2, then the square frame for being used for quotation 708-2 expands
With display additional information.In the expansion square frame for quotation 708-2, assume the more information being associated with quotation, such as exist
The entity (performer, role) of quotation is said in collection of drama 702.
As shown in fig. 7c, user can select the visual cues 710 of quotation 708-1 to recall the action with regard to quotation
Menu 712.Menu 712 includes the various actions to quotation 708-1 that can be activated.For example, user can ask to see pass
More multiple entity (and making those entities be shown on display), shared quotation 708-1 in quotation 708-1 is (for example, in social network
In network 172, by Email, by text message etc.), play and include video clipping (for example, collection of drama 702 of quotation 708-1
A part), in search engine 174 search quotation 708-1 (for example, quotation 708-1 is submitted to search engine as inquiry
174), entity (performer for for example, saying quotation or the role, wherein with regard to quotation 708-1 is searched in search engine 174
Say collection of drama and the serial of the quotation), commented on and indicated the interest to collection of drama 702 to quotation 708-1 and by quotation
708-1 is included in the instruction of interest.In some embodiments, activation comment action triggers are at the second client 140
Show on display text input interface so as to input with regard to quotation 708-1 comment, which can be stored in server system
At 106.In some embodiments, activation interest indicates that action triggers submit to the interest to collection of drama 702 to refer to social networkies 172
Show, and the interest indicates to include quotation 708-1.
For showing example UI of the summary of popular entity
Fig. 8 A and 8B diagram is according to the illustrative screenshot of some embodiments.Fig. 8 A diagram the first client 102 and the second visitor
The screenshotss at family end 140, and Fig. 8 B illustrates the screenshotss of the second client 140.In fig. 8 a, the first client 102 plays video
Content, and after playing video content at the first client 102, the second client 140 shows and in the first client 102
The summary of the relevant entity of the video content of upper broadcasting.In the fig. 8b, the second client 140 shows the time period with regard to definition
Summary with regard to the entity of video content.Diagram in Fig. 8 A and 8B substantially should be illustrated as example and nonrestrictive.At certain
In a little embodiments, the illustrative screenshot is that the instruction/application for downloading to the second client device 140 by being serviced device 106 is generated
's.In some embodiments, the illustrative screenshot (is such as browsed by the instruction/application being stored on the second client device 140
Device, assistance application or other pre-configured applications) generate.
Fig. 8 A illustrates the screenshotss of the first client device 102 and the second client device 140.First client 102 shows
TV programme 802.After the playback of program 802 terminates, display 806 (for example, the assistance application) of application of the second client 140,
One or more entities relevant with program 802 (for example, in program 802 by popularity front 5 people) and corresponding to corresponding
Entity 808 visual cues 810.When program 802 is played in the first client 102, the first client 102 will be from program
802 content informations that derives are sent to server system 106.As an alternative, the second client 140 will be from from the first client
The content information that 102 audio output corresponding to program 802 is derived is sent to server system 106.Server system 106 will
Content information is mated with user supplied video content using fingerprints so as to identification program 802.After the user supplied video content using fingerprints for mating with content information are recognized, clothes
Business device 106 determines the one or more entities being associated with program 802, and determines its popularity (for example, based in social networkies
With refer to number of times in web page).After program 802 completes to play, server system 106 is by summary together with most popular reality
Body 808 (for example, first 5) and corresponding visual cues 810 are sent collectively to the second client device 140 for presenting.Second
Client device 140 assumes entity 808 and corresponding visual cues 810.User can select visual cues 810 to recall with regard to phase
The menu of the action of entity is answered, as the visual cues 710 in Fig. 7 A- Fig. 7 C.
In some embodiments, it is generally most popular for the selected most popular entity of summary, without pin
Any personalization of interest and preference and history to user.In some embodiments, selected most popular for summary
Entity be by the interest of user and preference and history and popularity generally take into account most popular.
Fig. 8 B illustrates the screenshotss of the second client device 140.Server system 106 determines and in the time period of definition
The popularity of the video content associated entity of user is inside presented to.Server system 106 is by summary together with for the time
Section most popular entity 812 (for example, first 5) and corresponding visual cues 814 be sent collectively to the second client device 140 with
In presenting.Second client device 140 assumes entity 812 and corresponding visual cues 814.User can select visual cues 814
To recall the menu of the action with regard to corresponding entity, as the visual cues 810 in Fig. 8 A.
It should be appreciated that " popularity " of as used herein entity (for example, quotation etc.) is not only referred to reality
The affirmative of body or the interest of praise, but also can more generally refer to the interest to entity, as mentioned by, shared and inquiry
Indicated by number of times and any other appropriate criterion.Therefore, popularity metric 460 is the tolerance of the interest level to entity.
Recognize and store quotation
Fig. 9 illustrates the method 900 for recognizing and storing quotation according to some embodiments.With one or more
Method 900 is executed at the server system 106 of processor and memorizer.
From the identification of multiple documents and the associated plurality of quotation of media content (902).106 (for example, quotation module of server
148) analysis is by the document (or more generally, any content of text) of content host 170 and 172 trustship of social networkies with identification
With items of media content and more specifically associated with video content (such as, film and TV programme and Online Video) draw
Text.The example of analyzed document or content includes web page and social network profile, timeline and feeds.At certain
In a little embodiments, the document that is analyzed includes certain types of document, such as has editor's review, social comment and quotes
TV performance and other online articles of film and the web page of document.In some embodiments, these particular kind of texts
Shelves are extracted from the content host for being listed in white list as the document with these types.Server system 106 is analyzed
The document is to find quoting and quotation itself to video content quotation.
Recognize the corresponding items of media content (906) associated with quotation.Server system 106 recognize quotation from regard
Frequency content, i.e., say the video content of the quotation wherein.
In some embodiments, identification with quotation be associated corresponding items of media content include by quotation with corresponding
Associated closed caption data coupling (908) of items of media content.Server system 106 is for the caption data of video content
And mate the quotation from document identification.Coupling indicates that the video content item that quotation is corresponded to the caption data of coupling is associated.
The corresponding popularity metric (910) of quotation is determined according to one or more popularity criterions.In some enforcements
In mode, popularity criterion includes one or more in the following：The search inquiry amount of corresponding quotation, corresponding quotation
The quantity (912) for referring to number of times and the document including corresponding quotation in social networkies.It is right that server system 106 determines
The popularity metric 460 of each quotation for being recognized.Popularity module 150 determines the popularity of quotation based on many criterions.
The criterion includes：How many users have searched for quotation (volumes of searches of quotation), quotation in social networkies 172 in search engine 174
In (for example, in social media model and push away in literary (tweet)) be mentioned how many times and the document including corresponding quotation
The quantity of (such as web page).In some embodiments, analysis is used to identify the identical document of quotation in step 902
Deng to determine the popularity metric to quotation.In some embodiments, when the popularity of quotation is measured, to quotation specific
(above with reference to the certain types of document (editor's review etc.) described in step 902) is given attached for referring in the content of type
Weighted value.
In some embodiments, popularity module 150 also determines the popularity of quotation in real time.For example, for quotation
Refer to and shared and search inquiry to quotation etc. and analyze document and other contents, which popularity module 150 can detect
A little quotations have other recent trends of the popularity of peak value or quotation and change in the recent period in popularity.
The corresponding popularity metric of the association between corresponding quotation and corresponding items of media content and quotation is deposited
Storage is in physical storage storehouse (914).Quotation is stored in entity data bak 122 as entity 428.Each quotation entity includes
454 are quoted to other entities, which indicates quotation and the associating between entity that be cited.Each quotation entity is also included as in step
The popularity metric 460 to quotation for determining in rapid 910, and which can be updated periodically.
In some embodiments, for items of media content is given, will be one or more be associated with items of media content
Associated storage between entity and the corresponding quotation associated with corresponding items of media content is in entity data bak (916).
As described above, entity data bak 122 is quoted to other entities for entity storage, which indicates the association between entity.At certain
In a little embodiments, which is mapped to the chart data structure of the connection in entity data bak 122 between mapping entity.Solid data
Storehouse 122 includes the entity corresponding to video content item, it include to people (for example, performer, the visitor being associated with video content item
People etc.) the quoting of corresponding entity.For the people's for saying dialogue in video content item being associated with video content item
Subset, its corresponding people's entity includes quoting to the entity corresponding with the quotation that is said by this subset of people.Therefore, entity
Data base 122 be stored in for corresponding video content item the entity (for example, people's entity) being associated with video content item and with
Association between the associated quotation of video content item.
Identification quotation is to present
Figure 10 A- Figure 10 B is illustrated according to some embodiments for recognizing quotation for the method 1000 that presents.?
Method 1000 is executed at server system 106 with one or more processors and memorizer.
The items of media content (1002) is currently presented close to first user by identification.Server system 106 is from client
102 or 140 receive content information 142.Content information 142 is corresponding to the items of media content (example for just presenting on client 102
Such as, video content item).User is assumed close to client 102 video content item can be watched, even if he is not actually seeing
See the video content item.Also, as set forth above, it is possible to constitute from the audio frequency corresponding to video content item from client 102
Part and the audio output export content information 142 for being perceived by the mike in client 140.Assume user in client 140
Nearby (for example, client 140 is held in his handss), client 140 just can play video content on client 102
During item, perception is to assume the instruction of video content item close to user from the audio output of client 102.
In some embodiments, the identification of items of media content is using fingerprint (for example, by content information and fingerprint database
Fingerprint in 120 compares).Additionally, entitled " the Methods for Displaying for submitting on June 30th, 2011
Content on a Second Device that is Related to the Content Playing on a First
The U.S. of Device (method of the related content of content for showing on the second device to just play on the first device) "
Number of patent application 13/174, describes the more details with regard to recognizing content using fingerprint in 612, the patent application is by entirety
Be herein incorporated by reference.
In some embodiments, recognizing currently is including just to determine close to the items of media content that first user is presented
A part (1004) in the items of media content for presenting close to first user.Server system 106 can not only be recognized
In client 102 play video content item, and be just capable of identify that play on client 102 which part (for example,
Present in video content item where and video content item start or end how far apart).In step 1002, as
A part for items of media content identification process, determines the part being currently being rendered；Server system 106 recognizes items of media content
Be what and current in items of media content is assumed where.
One or more first quotations being associated with items of media content in identification physical storage storehouse, wherein, this first
Quotation is determined as popular (1006) according to one or more popularity criterions.Server system 106 is from physical storage
Storehouse 122 recognizes and selects one or more quotations.These quotations are associated with items of media content；These quotations are items of media content
A part for the interior dialogue that says.Selected quotation be in the quotation for being determined by server system 106 based on popularity metric
460 and the most popular quotation that is associated with items of media content.Popularity metric is determined according to one or more criterions.
In some embodiments, popularity criterion includes one or more in the following：First user is to corresponding
The search inquiry amount of quotation, corresponding quotation amount to search inquiry amount, corresponding quotation referring to time in social networkies
Number and including corresponding quotation predefined species document quantity (1008).For determining the criterion bag of quotation popularity
Include one or more in the following：User/multiple users amount to search (volumes of searches), the quotation for performing how many pairs of quotations
How many times and quotation being mentioned in document (for example, web page), how many times are shared in social networkies.In some embodiment party
In formula, with regard to referring in a document, document of the server system 106 pairs in predefined species is (such as comprising editor's review, society
Hand over the web page of comment, or other web pages for referring to film and TV) in quotation refer to more heavily and weighting；Predefined text
Shelves plant referring in the document of apoplexy due to endogenous wind, and than referring in the document outside the predefined species, the popularity weighting to quotation is more.
In some embodiments, popularity criterion includes one or more real-time criterion (1010).Server system 106
The real-time popularity of quotation can be determined based on one or more real-time criterions.Criterion simply can consider closely in real time
Any of above criterion (for example, the criterion described in step 1008) of phase time range.For example, the volumes of searches of measurement in real time can
To include the volumes of searches in nearest 15 minutes or volumes of searches per minute.Criterion provides the recent change of quotation popularity in real time
The tolerance (such as trend and peak value) (i.e. the real-time popularity of quotation) of change.
In some embodiments, the first quotation is determined as popular in real time according to popularity criterion
(1012).Server system 106 is recognized in real time and selects the popular quotation being associated with items of media content.In some enforcements
In mode, server system 106 consideration history and real-time popularity when quotation is selected, and by one than another more
How to weight.Note that specifically described herein for recognizing that this of popular quotation and other methods be also applied for recognizing other
The popular entity of type.
In some embodiments, the first quotation is close to presenting close to first user in the items of media content
The part (1014) of items of media content.Server system 106 is determining that (expression is current for the part of the items of media content for presenting
Playback position) after (1004), recognize and select the quotation close to the part (and as described above, being popular).Such as
Fruit quotation is said in time predefined from present playback position, then quotation is close to the part.For example, it is possible to will be
The quotation that says in nearest 15 minutes from present playback position is considered as close to the part.
In some embodiments, the quotation of the part that " close to " is being presented includes current in items of media content
The quotation that says in the range of certain time after position.Server system 106 can recognize will arrive in items of media content
The quotation for coming, describes its more detail below.
First quotation and the one or more visual cues associated to the first quotation are sent to related with first user
The client device (1016) of connection.Entity information 132 is sent to server system 106 client 140 being associated with user.
Entity information 132 includes selected quotation 708 and corresponding visual cues 710.Client 140 shows quotation 708 and corresponding visible light
Rope 710.
The selection of the first visual cues in visual cues is received, wherein, the first visual cues are corresponding to the first quotation
Quotation is associated (1018).At client 140, user select corresponding in quotation visual cues 710 (for example,
As shown in fig.7b, corresponding to the visual cues 710 of quotation 708-1).As shown in fig. 7c, this opens for execution and quotation
The menu (for example, visual cues) of the option 712 of the associated action of 708-1.User selects the option visible light in menu 712
One in rope, and the selection is sent to server system 106 by client 140.
According to the selection of the first visual cues, the operation (1020) associated with corresponding quotation is executed.Server system
106 according to selected visual cues execution action.For example, if user have selected " shared quotation " option, server system
106 models for sending shared quotation 708-1 in social networkies 174, user is with account and service in the social networkies 174
Device system 106 has been given the access for representing user to model by user.
In some embodiments, each corresponding visual cues is provided for a phase corresponding with the first quotation
One or more options (1022) of interaction.For example, when the option visual cues in menu 712 are chosen, can show and institute
The relevant additional option of option, and user can select any additional option.
In some embodiments, the operation associated with corresponding quotation is executed including any in the following
Individual：The information relevant with corresponding quotation is sent to show at client device to client device；Share and draw accordingly
Text；The media segment for including corresponding quotation is sent to client device to show at client device；Initiate with phase
The quotation that answers is used as the search of search inquiry；Initiate the search for the entity relevant with corresponding quotation；To client device
Text input interface is provided, which is configured to receive the input of the comment with regard to corresponding quotation；Or sharing media content item
In interest instruction, the interest indicates to include corresponding quotation as closed caption (1024).By selecting in menu 712
Any option, the additional information (for example, entity) relevant with quotation can be sent to client with commander server system 106 by user
End 140 is for showing, shared quotation (on social networkies, by Email, by message etc.), to client 140
The video clipping including quotation is sent, is used quotation to search for as query execution, (for example, said using the entity relevant with quotation
The role of quotation) search for as query execution, instruct client device 140 and show the text for input with regard to the comment of quotation
The instruction of the interest in inputting interface, or the shared video content item for including quotation.
In some embodiments, recognize and close to the items of media content after the part that first user is presented
A part of associated one or more second quotation (1026), detect the presenting of further part close to first user
(1028) detection for presenting, and according to further part, by the second quotation and one or more with what the second quotation was associated
Visual cues are sent to the client device (1030) being associated with first user.As described above, in video content item close to
The quotation of current location can include the certain time scope of (that is, after the current portions being just presented) after current location
The quotation that inside says.Server system 106 recognizes these " on the horizon " quotations, and waits and send it to client and set
Standby 140, till the part for actually saying these quotations reaches client 102.When server system 106 is in client 102
Detect when just assuming the part of " on the horizon " quotation, should the quotation of " on the horizon " be sent to client device
140.Therefore, server system 106 " can prefetch " quotation for arriving after a while in video content item, but prevented until
Till which is actually uttered in video content item, in order to avoid upset video for a user.
Assume quotation
Figure 11 illustrates the method 1100 for assuming quotation according to some embodiments.Hold at client 140 or 182
Row method 1000.
The items of media content (1102) is currently presented close to first user by detection.For example, at client device 140
Mike perceives the audio output from client 102.Application 127 in client device 140 is derived interior from the audio output
Content information 142 is simultaneously sent to server system 106 by appearance information 142, in server system 106, for fingerprint database 120
In fingerprint matching content information 142 to recognize the video content item corresponding to audio output.Server 106 recognize and select with
The associated and being serviced device system 106 of video content item is defined as the quotation of popular (for example, with high popularity metric 460).
These quotations 708 and corresponding visual cues 710 are sent to client 140.
Show the one or more popular quotation associated to items of media content and one or more corresponding visual cues, its
In, each visual cues is provided for corresponding with the popular quotation one one or more options (1104) for interacting.
Client device 140 receives and shows quotation 708 and corresponding visual cues 710.Each visual cues 710 is opened when being activated
The menu 712 of option (itself being visual cues) is to interact with corresponding quotation 708.
The user for receiving the first visual cues corresponding to popular quotation accordingly activates (1106).Can according to activation first
Sight line rope, executes and the operation (1108) that accordingly popular quotation is associated.User selects the option visible light in options menu 712
Rope, its selection is received by client device 140.Client device 140 execute in combination with server system 106 corresponding to
The action of selected visual cues or operation.For example, if action is to share quotation, server 106 is shared in social networkies
Quotation, and show shared procedure on client device 140.
In some embodiments, the operation associated with corresponding popular quotation is executed including any in the following
One：Show the information relevant with corresponding popular quotation；Share popular accordingly quotation；Showing includes popular quotation accordingly
Media segment；Initiate using popular quotation accordingly as the search of search inquiry；Initiating to be directed to is had with corresponding popular quotation
The search of the entity of pass；Show and be configured to receive the text input interface with regard to the input of the comment of popular quotation accordingly；
Or the instruction of the interest in sharing media content item, the instruction of the interest includes that popular quotation is used as closed caption accordingly
(1110).By selecting any option in menu 712, user can instruct client device 140 with 106 phase of server system
The additional information (for example, entity) relevant with quotation is sent in combination for showing to client 140, shared quotation is (in society
Hand on network, by Email, by message etc.), the video clipping including quotation is sent to client 140, using quotation
Search is executed as inquiry, searched to execute as inquiry using the entity (role for for example, saying quotation) relevant with quotation
Rope, instructs client device 140 and shows text input interface for input with regard to the comment of quotation, or shared includes that this draws
The instruction of the interest in the video content item of text.
Generate content summary
Figure 12 illustrates the method 1200 for generating the summary of items of media content according to some embodiments.Have one
Method 1200 is executed at the server system 106 of individual or multiple processors and memorizer.
That detects items of media content assumes (1202).Recognize items of media content and one relevant with items of media content or many
Individual entity (1204).When just video content item is assumed at client 102, client 102 or client 140 are by content information
142 are sent to server 106.Server 106 recognizes video content item using content information 142.Server 106 also recognize with
The associated one or more entities of video content item.
The corresponding interest level (1206) to the entity for being recognized is determined based on one or more signals.Server
106 determine interest level (for example, the popularity metric to the entity for being recognized using one or more signals or criterion
460).Server 106 determines these interest levels for amounting to.
In some embodiments, one or more of signals include one or more in the following：Accordingly
The entity amount of referring to accordingly in a document, the corresponding queries to corresponding entity, the phase to corresponding items of media content
The total (1208) of the history of the media consumption of the queries, the total of the query history of user and user answered.For determining
The signal of interest level or criterion include volumes of searches to entity and to items of media content, the total of the query history of user, with
And user has consumed the total of the history of what items of media content.Other possible signals are included above for the popular of quotation
Signal described in the determination of degree, such as referring to and the shared quantity in social networkies in a document.
In some embodiments, signal includes one or more in the following：The position of user, the population of user
The media consumption history (1210) of statistical property, the query history of user and user.Signal may include the specific letter of user
Number, the history of the items of media content consumption of the such as demographic information of position, user, the query history of user and user.
In some embodiments, the corresponding interest water in recognized entity is determined based on one or more signals
The flat corresponding interest level for including in the entity recognized with regard to user's determination.When together with other signals (for example, above in step
Those described in rapid 1208) when being used together the peculiar signal of the user for describing in step 1210, server 106 can determine
With regard to the user's and total interest level to entity.
Based on determined by interest level selecting the subset (1214) of entity.Server 106 is selected and items of media content
The associated entity with high total interest level (for example, first 5 in interest level).
In some embodiments, the subset of entity is selected to include to select reality based on the interest level for determining with regard to user
The subset (1216) of body.Server 106 can select the entity that the user being associated with video content item is most interested in, and
It is not the entity with high total interest level.As an alternative, server 106 considers user's and total when entity is selected
Both interest levels, but the interest level of user is higher weighted.Any one in both modes, server 106 with
Mode more for user individual selects entity.
The selected subset of entity is sent to the client device of user to assume (1218) at client device.Institute
Entity 808 is selected to be sent to client device 140 by the summary as items of media content 802 so as at client device 140
Show.
Figure 13 is illustrated according to some embodiments for the method that generates the summary of items of media content with regard to the time period
1300.Method 1300 is executed at the server system 106 with one or more processors and memorizer.
That detects multiple items of media content assumes (1302).Recognize items of media content and be directed to each corresponding media
Content item recognizes the relevant one or more entities (1304) of items of media content corresponding with this.When the client just in user sets
Standby when above assuming video content item, client device (for example, client 102 or 140) is by the content information 142 of video content item
It is sent to server 106.Server 106 recognizes video content item using content information 142.Server 106 is also recognized with often
The associated one or more entities of individual recognized corresponding video content item.
Determine the corresponding interest water in recognized entity with regard to the time period for defining based on one or more signals
Put down (1306).Server 106 determines the interest level (example to the entity for being recognized using one or more signals or criterion
Such as, popularity metric 460).Server 106 determines that these interest levels of time period that amount to and with regard to defining are (for example, fixed
Interest level in the time period of justice).The signal for being used can be identical with above with reference to those described in Figure 12.
In some embodiments, the time period of definition is any one in the following：The hour of definition, definition
The time range (1038) of day, the moon of definition or definition.Can be with regard to the hour (for example, 8 a.m.) for defining, the day of definition
(for example, Monday), the time range (for example, Primetime hour) of the moon (for example, May) of definition or definition and determine real
The interest level of body.The time period of definition can also be combinations of the above.For example, the time period of definition can be definition day
Definition time range (for example, in the Primetime hour on Thursday).
The interest level for being determined based on the time period with regard to defining is selecting the subset (1310) of entity.Server 106 is selected
Select entity (for example, the definition being associated with items of media content with high total interest level within the time period of definition
First 5 in the interest level of time period).
The selected subset of entity is sent to the client device of user to assume (1312) at client device.Institute
Entity 812 is selected to be sent to client device 140 by the summary as the items of media content for defining the time period so as in client
Show at end equipment 140.
In some embodiments, summary includes top story (for example, News Stories).For example, server 106 recognizes matchmaker
Entity in body content item.The search of server 106 refers to entity and popular story (for example, the document comprising news article
Deng).Server 106 is recognized and most popular document is included in summary in these documents.In some embodiments, lead to
The important keyword (people and Places for for example, mentioning in story) that crosses in identification story is recognizing the story of entity.Shared weight
The story of keyword is wanted to be clustered together.Content (for example, caption data) for items of media content mates these important passes
Key word is to find the story with regard to the entity relevant with items of media content.Determine the popularity of these stories, and in summary
Show most popular.
In some embodiments, generate and show the summary of items of media content in real time.For example, media are assumed
During content item, that detects items of media content and items of media content currently assumes/playback position.Server 106 generates from currently and is in
The summary of (for example, nearest 15 minutes) the summary is sent to client device 140 in the range of the certain time that existing position is risen
To present to user.This summary is as items of media content is presented and is continually updated or refreshes.
In some embodiments, can in response to the search inquiry of user and in response to items of media content viewing and
Execute presenting for relevant with quotation as above information and content summary.For example, when user is from television program search quotation
When, can be additional to or be alternative in Search Results, show quotation relevant information mentioned above.If user's searching television section
Mesh, can be additional to or be alternative in Search Results, and the summary of display program (for example, for youngest collection of drama, is directed to nearest one
Individual month etc.).
Select in response to user and show entity information
In some embodiments, client device 140 in response to user select and show with regard to just set in client
The information of the relevant entity of the video content item that presents on standby 102.For example, client device 140 is before entity information is shown
Detect client device 102 and be energized (or at least determining that client device 102 is energized or may be energized).When
When client device 102 is detected or is defined as being energized, client device 140 points out user selection.The user selects to indicate
User in order to detect the video content item for currently presenting authorizes.Selected according to user and combine server system 106, client
Equipment 140 assumes the information with regard to the entity relevant with the video content item for detecting.
In some embodiments, (or the server system for combining with client device 140 of client device 140
106) detect or determine whether client 102 is energized (and alternatively, whether client device 102 is active, with the free time
Or contrary in sleep pattern).In distributed system 100, client device 140 determines whether client device 102 is led to
Electricity, with such it is assumed that i.e. in the case that client device 102 is energized, user is currently in use client device 102 and sees
See content (for example, film, TV programme).In some embodiments, if client device 102 and client device 140
(for example, both client devices 102 and 140 are connected to public network by same router to be communicatively connected to consolidated network
Network, for example, as in home network), then client device 140 determines visitor by the communication with client device 102
The power supply status of family end equipment 102.
For example, be energized when client device 102 (or additionally, be active, and idle or be in sleep pattern phase
When instead), 102 broadcast or multicast message of client device (for example, using SSDP (SSDP)) is to public network
The presence of the miscellaneous equipment notification client end equipment 102 in network.Client device 140 receives message from client device 102, and
And based on receiving message from client device 102 and determine that client device 102 is energized.Also, client device 140 can
With periodically to the equipment in public network send broadcast or multicast search message with determine client device 102 whether still by
Energising.Client device 102 receives search message and sends response message to client device 140.Client device 140 is received
Response message, and based on receiving message from client device 102 and determine that client device 102 is energized.Client sets
Standby 140 notice client device 102 of server system 106 is to turn on.According to the notice that client device 102 is energized, clothes
Business device system 106 determines that client device 102 is energized and provides instruction or content (for example, entity letter to client device 140
Cease and in order to show the instruction of such entity information).
In some embodiments, if client device 140 can not be determined according to the communication with client device 102
(for example, client device 102 and 140 is connected to different networks, client device to the power supply status of client device 102
102 are not linked to any network, and client device 102 is not configured to be connected to any network (for example, client device
102 is not that the Internet is realized), client device 102 is not configured to broadcast/multi broadcast there is (for example, client device in which
102 do not support SSDP or another like agreement)), then client device 140 determines client based on one or more signals and sets
Standby 102 power supply status；Client device 140 guesses based on the signal whether client device 102 is energized.In some realities
Apply in mode, signal includes one or more in the following：The current location of client device 140, current time and
Data (for example, the search history of user) with regard to user.
In some embodiments, client device 140 determines its current location (for example, using in client device 140
Location equipment, such as global positioning system (GPS) module (not shown)).The current location of client device 140 is to indicate to use
Whether family is in (assuming that client device 140 and user are close to each other) and therefore may be currently in use client device 102 is seen
See the signal of video content (for example, seeing TV).
In some embodiments, client device 140 recognizes current time.Current time is that instruction user may be
See the signal of TV (for example, if current time is the Primetime hour scope of television-viewing, it is determined that user is likely to
It is currently in use client device 102 and watches video content).
In some embodiments, client device 140 is communicated with server system 106 and obtains the data with regard to user
(for example, from customer data base 130).User data include the search history of such as user, the media consumption history of user and
The TV viewing history of user.Whether these history are instruction users to video content (for example, TV performance, film) and correlation
(for example, if user frequently searches for TV relevant information, user is more likely to TV phase for information signal of special interest
Pass information is interested).
In some embodiments, on client device 140 client is made in application (for example, assistance application 355-3)
102 determinations whether being energized.Therefore, application can be activated whenever application (for example, start, be brought to foreground) when carry out really
Determining or the determination is periodically carried out when in backstage.
Client device 140 (for example, the second screen equipment, such as smart phone, laptop computer or flat calculating
Machine) based on these signals and/or the presence notice from client device as above 102 (for example, TV or Set Top Box)
And determine whether (possibility) is energized client device 102.If equipment is confirmed as being energized, as shown in fig. 14 a,
Application (for example, assistance application 355-3) on client device 140 shows visual cues 1402-1 (for example, " card ").Visually
Clue can be by (for example, the card) that applies in multiple visual cues 1402 for simultaneously showing.Visual cues 1402-
1 includes to invite user's choice cards (for example, execute on card and tap gesture, click on card) to receive with regard to may be
The prompting of the information of the entity in the video content item that plays on client device 102.For example, visual cues 1402-1 include
" who is on TV？This card is tapped to find out "；The prompting invite user tap card 1402-1 obtain with regard to user just
In the relevant information of the people of no matter any performance of viewing.
Visual cues 1402-1 be have selected in response to user, application activating is played on client device 102 in order to detect
Media content process.In some embodiments, application activating audio detection process, wherein, from client device 102
Audio output by client device 140 (for example, by the mike in client 140) pickup.Generate from the audio frequency of pickup
Content information 142 simultaneously sends it to server 106 to determine the items of media content that plays on client device 102 (above
Its details is described, and is not repeated here).In some other embodiments, application and client device 102 (for example, with
Media player applications on client device 102) communicated to determine that what content is played just on client device 102
(for example, by the metadata from 102 index playing items of media content of client device or caption data).In some embodiments
In, visual cues 1402-1 show the afoot information of instruction content detection.
For example, in Figure 14 B, after visual cues 1402-1 that user is selected in Figure 14 A, visual cues 1402-1 are existing
Afoot instruction " just in detection content ... " is detected in display content.
In some embodiments, content detection has the time limit.Therefore, if applied in definition time limit (for example, 12 seconds)
The items of media content just played inside not can determine that, then visual cues 1402-1 show error message (for example, " content detection mistake
Lose ").
If content detection is successfully (for example, to detect the collection of TV plays that plays just on client device 102
1401), then the application on client device 140 shows the 1404 (example of visual cues of the entity information of the content for including to detect
Such as, card).For example, as shown in Figure 14 C, visual cues 1404 are shown on client device 140.Visual cues 1404 are wrapped
Include the entity information of collection of TV plays 1401.For example, visual cues 1404-1 include that (or collection of drama 1401 belongs to for collection of TV plays 1401
TV series) cast.Each cast member in visual cues 1404-1 can individually be easily selected by a user (example
Such as, tap)；Each cast member in card 1404-1 is single visual cues.
Figure 14 C is also shown in suitable according to the z of visual cues 1404 after visual cues 1404-1 on client device 140
Visual cues 1404-2 that sequence shows.Visual cues 1404-2 can be easily selected by a user so that visual cues 1404-2 are according to z
Order shifts to an earlier date.Figure 14 D shows visual cues 1404-2 for showing before visual cues 1404-1.Visual cues 1404-2
The entity information of also display TV collection of drama 1401.For example, visual cues 1404-2 show the people for mentioning in collection of TV plays 1401
(in role in the historic role for example, mentioned in story, story etc., the news that mentions in collection of drama but do not occur
People).In some embodiments, it is that referred everyone has single card.In some other embodiments, there is display
" people for mentioning " card, such as visual cues 1404-2 of the list of the people for mentioning in collection of TV plays 1401.Mention having
People list " people for mentioning " card in, similar to the performer in cast card 1404-1 as above how to be can
Individually select, list in card everyone individually can select；Everyone in " people for mentioning " list is independent
Visual cues.In some embodiments, exist with regard to showing or in independent visual cues card column in visual cues card
Go out which entity, the quantity of entity for showing or listing and what shows the restriction of separate visual clue card for.Example
Such as, the quantity can be limited to maximum quantity (for example, five).As another example, show or be listed in items of media content
In present playback position define the entity mentioned in time range.For example, if in the current location of collection of drama 1401 most
People be refer in nearly 30 minutes, then which is qualified shown or lists, but if which was mentioned outside nearest 30 minutes
, then it is not eligible for shown or lists.
On card 1404, shown entity can be relevant with the items of media content for presenting on client device 102
Any entity.Therefore, shown entity is not necessarily just people；Shown entity can include place, quotation, music etc..
Additionally, as in the above summary, the entity for showing in card 1404 can be shown with content summary format.?
In some embodiments, the entity for showing on card 1404 is the mark of the determination based on the content for detecting from solid data
Storehouse 122 is derived.In some embodiments, on card 1404 show entity be from by internet search engine in response to
(for example, actor name is performed in this case for the visual cues on card shown by having selected in response to user
Search inquiry be the title of respective media item and combining for actor name) and the Search Results that returned of inquiry initiated are derived
's.In some embodiments, the entity for showing on card 1404 be from from entity data bak 122 and Internet search knot
One or more combination in the information of fruit is derived.
When user have selected from the visual cues with list of entities entity (performer for example, from card 1401-1,
From the people that card 1404-2 is mentioned) or have selected visual cues (card for example, with a people) with corpus separatum
When, show the information of selected entity.In some embodiments, shown information is the additional information with regard to selected entity
(for example, biographic information, other factural informations).For example, as shown in Figure 14 E, if selected in visual cues 1404-2
" people 1 ", then show the biographic information with regard to people 1, the list of the people that replacement is previously mentioned in visual cues 1404-2.Some its
In its embodiment, shown information be wherein selected entity be search inquiry search Search Results.For example, if choosing
" people 1 " in visual cues 1404-2 has been selected, then search engine 174 has been submitted to the search inquiry of people 1, and as Figure 14 F
Shown in, show the list of the people that Search Results replace mentioning in visual cues 1404-2.In Figure 14 E- Figure 14 F, user
The list in visual cues 1404-2 can be returned to by activating back 1408 etc..
In some embodiments, the entity for showing on card is the type based on the items of media content for detecting.When
When server 106 recognizes the items of media content that is just playing, server 106 also recognizes the type of items of media content (for example, based on matchmaker
Metadata of body content item etc.).For example, if collection of drama 1401 is to fabricate program (for example, TV drama, sitcom, film),
Then open 1401-1 and show cast.If collection of drama 1401 is with the non-of host and participant (for example, guest, competitor)
Imaginary program (for example, midnight talk show, record performance, news interview performance, game performance), then card 1404-1 shows master
Hold list rather than the cast of people and participant.
Certainly, cast, host and guest, the people that mentions etc. can only show entity and corresponding visible light
The example of rope 1404.Other entities that can show in card 1404 and information include collection of drama summary, cast member, mention
Place, the list of songs that hears in items of media content etc..
It should be appreciated that the items of media content for being detected as playing on client device 102 can be live or time shift
(for example, from digital record play, program request).
Figure 15 A- Figure 15 B illustrates the method 1500 for showing the information relevant with entity according to some embodiments.
Method 1500 is executed at client device (for example, the client device 140) place with one or more processors and memorizer.
Whether detection user is watching items of media content (1502).Client device 140 is by detection for watching matchmaker
Whether the internal client device (for example, client device 102) for holding just is being energized to carry out this is detected, client device 102
Power supply status are the agencies for whether watching items of media content to user.For example, client device 140 is using from client
The presence notice of equipment 102 and/or signal (for example, current time, current location etc.) are determining whether client device 102 leads to
Electricity.
After detecting user and watching items of media content, assume offer first user to user on a user device
Selectable option is to receive first visual cues (1504) of the information with regard to the entity relevant with items of media content.Client sets
Standby 140 show visual cues 1402-1 detecting client device 102 and being energized after, its invitation user reception with regard to matchmaker
The information of the relevant entity (for example, people) of body content item (for example, collection of TV plays 1401).User select visual cues 1402-1 with
Initiate the process for receive information.
In some embodiments, user equipment is chosen from the group being made up of the following：Computer, mobile phone,
Smart phone, panel computer equipment, multimedia player device and game player equipment (1506).Client device 140
Can be any one in the following：Computer (desk computer or laptop computer), mobile phone, smart phone,
Panel computer equipment, multimedia player and game player equipment.Any one in these equipment can have detection
User is watching the application (for example, assistance application 355-3) of the power supply status of media content and client device 102 and is showing
There are the visual cues of the invitation to receive information.
User in response to the option selects (1508), to the programme information from items of media content at user equipment
Sampled, including one or more (1510) in the audio signal and closed caption of items of media content, and will section
Mesh information is sent to server (1512).Used as response, server identification items of media content simultaneously generates recognized media content
One or more users of item may be selected user option, and send one of offer second user selectable option to user equipment
Or multiple second visual cues.In response to the user option of visual cues 1402-1, client device is by (all to programme information
As the audio output from client device 102, caption data and items of media content metadata) carry out sampling to detect in visitor
What is being played in family end equipment 102.Programme information is sent to server 106 as content information 142.Server 106
The items of media content that is playing on client device 102 is detected based on content information 142, and is generated and to client
Equipment 140 sends one or more visual cues of recognized items of media content.New visual cues are corresponded to regard to being known
The entity of other items of media content.
For example, the user in response to card 1401-1 selects, and client device 140 is to 106 transmission content information of server
142.Server 106 recognizes the collection of drama 1401 that plays on client device 102 using content information 142.Server is then
From entity data bak 122 recognize with regard to collection of drama 1401 entity (for example, cast member, host and guest, in collection of drama 1401
In mention people, place etc.), and the visual cues 1404 for generating corresponding to these entities for being recognized (or as an alternative, will
The entity that recognized and be sent to client device 140 in order to generate the instruction of corresponding visual cues (for example, card) 1404).
Visual cues 1404 are sent to client device 140 by server 106, there show visual cues 1404.
In some embodiments, server recognizes the type of items of media content, and based on the media content for being recognized
Type and generate second user may be selected user option (1514).Server 106 recognize items of media content type and
The mark of items of media content.According to type, server 106 for items of media content identification different entities and is based on type and generate can
Sight line rope.For example, if type is to fabricate program, server 106 generates the card for listing cast member.If type is
Non- imaginary program with host and other participants, then the card of host and participant is listed in the generation of server 106.
In some embodiments, items of media content is selected from：Live media content item or time-shifted media content item (1516).
Items of media content can be live (for example, the first run of TV programme collection of drama, live news event, live sport event)
Or (content for example, from recordings broadcasting, the on-demand content) of time shift.To any one in two ways, client 140 will
The content information 142 of items of media content is sent to server 106, its identification items of media content.
Show the second visual cues (1518) on a user device.For example, as shown in Figure 14 C- Figure 14 D, in client
Show card 1404 on equipment 140.
In response to user select the second visual cues in one, show on a user device with regard to the media for being recognized
The information (1520) of the relevant corresponding entity of content item.For example, as shown in Figure 14 E- Figure 14 F, in response to choice cards
" people 1 " in 1404-2, shows the Search Results with regard to the information of people 1 or to people 1.
In some embodiments, second user selectable option is corresponding to the corresponding of the items of media content with regard to being recognized
Entity, and show that the information with regard to the corresponding entity relevant with the items of media content for being recognized includes on a user device
Show the information (1522) with regard to selecting corresponding corresponding entity with the user of in the second visual cues.For example,
In the card 1404-1 and 1404-1 as shown in Figure 14 C- Figure 14 D, listed cast member and the people for mentioning are cards
Interior independent visual cues.Any one and the people that mentions that user can be individually chosen in cast member, recall with regard to
Selected cast member or any information that mentions.
In some embodiments, the information with regard to corresponding entity includes the fact that entity corresponding with this is relevant information
(1524).As in Figure 14 E, the information with regard to entity includes the biographic information for example with regard to people and other factural informations, such as
Information with regard to place etc..
In some embodiments, the information with regard to corresponding entity includes the searching as inquiry with the corresponding entity
The Search Results of rope.As in Figure 14 F, the information with regard to entity includes for example there is search knot of the entity as the search of inquiry
Really.
In some embodiments, server 106 (for example, is existed from other sources by analyzing items of media content and quoting
On-line documentation, other information service) data and build entity data bak 122.The analysis of items of media content is included from media content
Item is received, the data of retrieval or extraction for example corresponding to audio track, caption data and metadata.According to audio track data
Deng server 106 recognizes the entity (for example, people, place, music, quotation etc.) and this that mentions in items of media content or occur
When a little entities occur or are mentioned in items of media content.For example, server 106 can be by audio track data etc.
Any proper noun that mentions is considered as potential entity, and quotes other information data source to be confirmed.Server 106
Items of media content search document (for example, web page) can be directed to find the potential entity for finding in audio track data
Refer to.If the number of times that mentions in a document and the quality that alternatively these refer to exceed threshold value, potential entity is identified
Be for being added to the entity of entity data bak 122.In addition, server 106 can quote other data sources to help recognize.Example
Such as, server 106 can quote music information source (for example, songs/music identification service, musical database) to help recognize
The music that plays in items of media content or mention.
Show text message
Figure 16 be illustrate according to the display on the second device of some embodiments with playing on the first device or
The flow chart of the process 1600 of the corresponding text message of audible utterance information in the content that person has played.Figure 16 is provided
The general view of the method 1800 for discussing in more detail in the discussion of Figure 18 A- Figure 18 C.Video content system 112 is to client
102 send video flowing 126 (1602).Video flowing is received by client device 102 and shows (1604).Played in video flowing
When, the user's request (1606) of the audible utterance information in the video flowing for explain broadcasting is received by client device 140.?
In some embodiments, (for example, pressed the button or select using phonetic entry or other inputs by the user of client device 140
Select icon) issuing user's request.User's request is processed by assistance application 355-3 and sends it to server 106.Voice is defeated
(for example, " he/her is firm to enter to be identified as explaining the word or expression of user's request of audible utterance information including server 106
What has just been said？”；" what that is？”；" I does not understand last part " etc.).Phonetic entry can include explicit or implicit
Time refers to (for example, " what he has said in five seconds before？”；" what she has just said？") and/or specific speaker or singer
Explicit or implicit refer to, the speaker for no matter mentioning or singer are that (for example, " she has said assorted for true man or fictional character
?？”；" what Sheldon (Cooper) just said ").In some embodiments, input may also include instruction particularization and ask
Ask word or expression (for example, in order to ask the Roman capitals romanization to Japanese song " what its Roman capitals are ", in order to
Request translator of Chinese " that a line Chinese be what the meaning？”).
Determine the content information from video flowing and send it to server 106 (1608).Other as in this application
Place is described, in some embodiments, includes audio frequency and/or the video component of video flowing from the content information of video flowing
Or corresponding subtitle data one or more editings (such as several seconds, a few minutes or a few houres) or by client device 102 from
The audio frequency of video flowing and/or video component or corresponding subtitle data one or more editings (such as several seconds, a few minutes or
A few houres) fingerprint that generates or other signatures.For example, content information is from the audible utterance information of video flowing.In some enforcements
In mode, when video flowing is played, client device 140 is from the audio frequency from client device 102 with video flowing
The corresponding audio output of appearance determines content information, and (for example, the mike in client 140 is defeated from the pickup audio frequency of client 102
Go out).Client 140 determines content information and the content information is sent to server 106.In some embodiments, content
Information is formatted, and therefore which can be by easily compared with the user supplied video content using fingerprints for storing on the server.Server 106 is received
The content information is simultaneously mated (1610) by content information with user supplied video content using fingerprints.
In some embodiments, before runtime by server (for example, using fingerprint generation module 221) from by
The media content (for example, audio frequency and/or video clipping or frame of video) that third party user uploads generates user supplied video content using fingerprints.At certain
In a little embodiments, by server (for example, using fingerprint generation module 221) in real time (for example, live) or operationally
Between before from media content (for example, audio frequency and/or video clipping or the video for having received from video content system 112
Frame) generate user supplied video content using fingerprints.
The position (1612) in video flowing is determined based on user's request.Server 106 determines that in video flowing, user just please
Solve the position that releases.The determination is at least based on content information, because content information is most while sending out in time with user's request
Raw.Content information includes the content in video flowing with the simultaneous position of user's request, and thereby indicate that video
Position and the default video stream position of user's request that in stream, user may be interested.In some embodiments, it is contemplated that use
Some objects are dully heard and carry out the delay of customer responsiveness between user's request, default video position is configured in family
Certain number of seconds (for example, one second, two seconds) based on content information before defined location.The determination is also based on user please
Whether the peculiar details in asking, such as user's request include referring to for time.For example, if the phonetic entry in user's request is
" what he says in 10 seconds before？", then the user default video stream position position of about 10 seconds of may adjusting the distance is interested.The opposing party
Face, if the phonetic entry in user's request is that " what he has said？", then user's request does not refer to the time, and is based on content
The default location of information be determined by position.
Obtain corresponding to determined by the text message of position send it to client 140 (1614).Server 106
Obtain the text message corresponding to position determined by video flowing.Server 106 consults entity data bak 122 first to obtain
Obtain text message.If text message is not in entity data bak 122, server 106 consults other sources, such as video flowing
Document in caption data or content host 170.In some embodiments, the text message for being obtained includes true in institute of institute
Text message in certain time scope near fixed position (for example, determined by plus/minus five seconds near position).
After server 106 obtains text message, text message is sent to client 140.
Client 140 receives text message (1616), and assumes text message (1618).Auxiliary at client 140
Application 355-3 displays to the user that text message.User can also be carried out to different text messages (such as, corresponding to shown
Text message position before or after the text message of position or the text message of different speaker) user please
Ask.
In some embodiments, client 102 and client 140 are same equipment, and client 102 and 140
Function and feature are present in same equipment (for example, Figure 1B, client 182).For example, user may include assistance application
Video flowing 126, the sound of assistance application 355-3 and server 106 is watched on the same smart phone of 355-3 or tablet PC
Frequency explanation module 152 is cooperated to explain the audible utterance information in video flowing 126.In such embodiment, in client 140
Upper execution and/or be herein attributed to some or all operations of the first client 102 and be used to by 140 trustship of client
Execute the associated software module data structure of those operations.In order to explain, can also be in the feelings for not quoting associated video content
Under condition, audio-frequency information or content application are described herein as being applied to the audio frequency being associated with video flowing or video content
One or more in the operation of information or content.This is being applied to the music data with language content, audio books, is broadcasting
Can have when visitor, broadcast program, broadcast (the naming a few) of music video and the music event with language content
Help.
Figure 17 A-17B illustrates the illustrative screenshot according to some embodiments.Each first client of diagram of Figure 17 A- Figure 17 B
End 102 and the screenshotss of the second client 140.First client 102 plays video content, and the second client 140 is played and explained
The text message of the audible utterance information in the video content that plays in the first client 102.Diagram in Figure 17 A- Figure 17 B
Should substantially be illustrated as exemplary and nonrestrictive.In some embodiments, the illustrative screenshot be by under being serviced device 106
It is downloaded to instruction/application generation of the second client device 140.In some embodiments, in response to from server 106
In order to show the instruction of certain content, by the instruction/application (the such as browser, auxiliary that are stored on the second client device 140
Application or other pre-configured applications) generate the illustrative screenshot.
Figure 17 A illustrates the screenshotss of the first client device 102 and the second client device 140.First client 102 shows
Primary importance 1701-1 in TV series collection 1701 and the card of display application of the second client 140 (for example, assistance application)
Piece object 1702.When collection of drama 1701 is played just in the first client 102, the second client 140 is from may dully
Hear user's receive user request of the speech in collection of drama 1701.The request can take the phonetic entry for asking " what that is "
Form.As shown in figure 17 a, the phonetic entry shows in card 1702, confirms so as to user or cancels the request.Work as reception
To during request, client device 140 also generates content information from the audio output from the first client 102.User can select
Card (for example, if card shows on the touchscreen, then tapping card) or idle card are selected with confirmation request, or
Waving card (waved on the touch screen for for example, showing card wherein to the left and sweep) is swept to cancel the request.If inquiry is identified, please
Summation content information is sent to server 106.Server 106 processes the request, obtains text message in response to the request,
And text message is sent to the second client 140.
When the second client 140 receives text message, in card 1702, show text message, as institute in Figure 17 B
Show.In Figure 17 B, show user's request and the text message in response to user's request in card 1702.For example, as indicated,
Text message is the lines " I Love You " near the position 1701-1 of collection of drama 1701 and " I also likes you ".Server 106 is also
Additional information can be sent together with text message.For example, it is possible to text message is matched with entity information, by spy
Determine lines to be associated with specific speaker.As shown in Figure 17 B, together with the identity of the role for saying lines in collection of drama 1701
Show text message (role A says " I Love You ", and role B says " I also likes you ").Thus, user can ask text further
This information (in such as collection of drama 1701 no matter what text message current location be), request are corresponding to by specific actors or angle
The text envelope of the text message of the lines that color is said or request before or after primary importance 1701-1 in certain time
Breath.
Figure 18 A- Figure 18 C illustrates the audible utterance being associated for explanation according to some embodiments with items of media content
The method 1800 of information.Method 1800 is executed at the server system 106 with one or more processors and memorizer.
Server receives what the items of media content of (1802) for explaining with just play close to client device was associated
The user's request of audible utterance information, wherein, user's request includes the audio sample of items of media content and user's inquiry, wherein,
Audio sample is temporally close to issue the part of user's inquiry corresponding to items of media content.Server 106 is set from client
Standby (for example, client device 140) are received in order to explain the user's request for playing the audible linguistic information in video.In video
When Rong Xiangzheng is played on client device 102, user to client device 140 carry out for explain in video content item can
Listen the user's request of utterance information.For example, user's request can be taken the form of phonetic entry (for example, " what that is？”).Replace
Selection of land, can be used by selecting the predefined user interface object (for example, button, icon) being associated with such request
Family is asked.
When client device 140 receives user's request, client device 140 is from the client for just playing video content item
End equipment 102 picks up audio output, and generates including the audio frequency sample from the audio output from client device 102
This content information 142.Audio sample corresponding to being temporally close in video content item (for example, concurrently, delayed about
The 1-2 second) position of user's request is received by user equipment 140.For example, if when video content item is in 5:Connect during 32 labelling
User's request is received, then the user's request is included close to 5 in video content item:The audio sample of 32 labellings.
User's request includes that user inquires about.If having carried out user's request with phonetic entry, inquire about based on phonetic entry.
If having carried out user's request using predefined user interface object (for example, button or icon), then inquiry be with regard to user circle
Face as predefined inquiry.
The user's request for being carried out with phonetic entry includes word and expression.The word and expression specifies user's inquiry.For example,
Inquiry can be that (for example, " what that is to logical finger？”).Logical refer to inquiry for such, it is assumed that to close to receiving user
The default user inquiry of the explanation of any audible utterance information that says in the video content of the time of request or sing.The inquiry
Can be either explicitly or implicitly with regard to time or people.For example, if inquiry includes (for example, " what Alfred says to name？") or property
(for example, " what she says for other pronoun " he " or " she "？"), then the inquiry be in video content close to receiving user's request
The people by referred name (if inquiry includes name) of time or the property mated with sex pronoun others (if
If inquiry includes sex pronoun) say or the explanation of any audible utterance information that sings in one.If inquiry is specified
(for example, " what lines before are to relative time within 5 seconds？", " show to me and return the lyrics of 10 seconds ") or absolute time (example
Such as, " what the lyrics in song at 30 seconds are？"), then inquiry be in video content close to absolute time or with regard to receive
One in the explanation of any audible utterance information that says or sing of relative time during user's request.Inquiry can show
Formula and/or impliedly (for example, " what she has said in 5 seconds before with regard to time and/or people？", " Sheldon was in 5 minutes labellings
Place has said anything？”).
Inquiry can include the inquiry to becoming privileged information.For example, for Roman phonetic (for example, inquiry can be included
For Chinese dialogue phonetic romanization " how that lines phonetic spells？") or for translating (for example, for translating into
French " what that lines is gallice？") inquiry.
In response to user's request (1814), in server identification (1816) items of media content and media content, audio frequency is corresponded to
First playback position of sample；According to the first playback position and the mark of items of media content, obtain (1818) and correspond to media
The text message of user's inquiry of the appropriate section of content item；And at least a portion of text message is sent (1830) to visitor
Family end equipment.Server 106 is recognized corresponding in video content item and media content using the content information including audio sample
Position in audio sample.A timing of server 106 (for example, the audio frequency explanation module 152) acquisition near recognized position
Between in the range of items of media content text message (text for example, from caption data, the lyrics from database of song lyrics
Deng), and at least a portion of text message is sent to client device 140 for presenting.Because server 106 is recognized
Position in items of media content and items of media content, so server 106 knows that the audible language content that request is explained is words
Sound or song or both.Server 106 according to audible utterance information be speech or song or both and from one or
Multiple sources obtain text message.For speech, source includes such as caption data and speech recognition.Text message is provided in video
Close to the speech in any audible utterance information for being uttered or singing of the position for being recognized or the written table of the lyrics in appearance
Show or transcribe.In some embodiments, server 106 (for example, audio frequency explanation module 152) consult physical storage storehouse 122 with
Check for recognized video content item and determined by the text message of position whether be stored in entity data bak 122
In.If text message is found in physical storage storehouse 122, retrieve text message from physical storage storehouse 122.If in entity
Text message is not found in storage vault 122, then audio frequency explanation module 122 consults other sources (for example, captions number of text message
According to, online document etc.) to find text message.
In some embodiments, audible utterance information is associated with multiple speakers；And user's request include with regard to
Corresponding speaker explains the request (1804) of audible utterance information.The audible utterance information can include multiple speakers or many
The speech of individual singer, and user's request includes to explain the inquiry of audible utterance information with regard to corresponding speaker or singer.Example
Such as, if request includes name or sex pronoun, the request being serviced device 106 is construed to for explaining by referred name
People or mate with sex pronoun or the request of audible utterance information that the people that quoted by the sex pronoun says or sings.For every
Individual such inquiry, the text message for being sent to client device 140 corresponds to the people for quoting by referred name or by pronoun
The text message of the audible utterance information that says or sing.
In some embodiments, items of media content is live television programming (1806).Video content item can be live
TV programme (for example, live event, first run collection of TV plays).For live telecast content, the source of text message can be
Including the caption data in the video flowing of live telecast content.
In some embodiments, items of media content is the television content (1808) of previous broadcast.Video content item is permissible
Be previous broadcast TV programme (for example, re-run, in a time zone the first run broadcast after for another time zone again
Broadcast).For the television content of previous broadcast, the source of text message can be included in the captions in the video flowing of television content
Data or the caption data being stored in physical storage storehouse 122.
In some embodiments, items of media content is recorded content (1810).Video content item can be just played
Recorded content (for example, content of the record in DVR；It is stored in interior in CD, hard disk driver or other nonvolatile memories
Hold).For recorded content, the source of text message can be included in (for example, the adjoint captions of the caption data in recorded content
Data) or the caption data that is stored in physical storage storehouse 122.
In some embodiments, items of media content is the content (1812) of streaming.Video content item can be streaming
Video (for example, Online Video).Video to streaming, the source of text message can be included in the captions number in the video of streaming
According to or the text that generated using speech recognition.
In some embodiments, text message includes song lyrics (1820).Server 106 is based on items of media content
Position in identification and items of media content and know the audible utterance information at the position be speech or song or both.Such as
Fruit audible utterance information is song, then text message includes the lyrics of song.
In some embodiments, text message includes the transcription (1822) of speech.Server 106 is based on items of media content
Identification and items of media content in position and know the audible utterance information at the position be speech or song or both.
If audible utterance information is speech, text message includes the transcription of speech.
In some embodiments, text message includes the caption data (1824) being associated with items of media content.Captions
Data can be the source of text message；Text message includes the text for extracting from caption data.
In some embodiments, text message includes the information (1826) for obtaining from online document.Text message is permissible
It is to obtain from the document of trustship or data base at content host 170.For example, it is possible to obtain the lyrics from database of song lyrics.Can
To obtain the lyrics, lyrics romanization and the lyrics translation of the song to particular genre from specific website (for example, in Japanese
Japanese animation in the song that finds, the Japanese animation song website as lyrics source and its web page, and corresponding sieve
Horseization and translation).
In some embodiments, text message includes the audible utterance information of the appropriate section corresponding to items of media content
Translation (1828).For example, server 106 is known audible based on the position in the identification and items of media content of items of media content
Utterance information is to adopt the language different from required by user's request or different from user Preferred Language.Server 106
Obtain the text message of the translation (for example, the lyrics from website are translated) as audible utterance information and send it to client
End 140.
In some embodiments, server receives (1832) in order to the media explained with play close to client device
The second user request of the associated audible utterance information of content item, wherein, second user request includes the of items of media content
Two audio samples and second user inquiry, and wherein, the second audio sample is temporally close to corresponding to items of media content
The part of the issue of second user inquiry.After text message is sent to client 140, client 140 can receive use
To explain another request of audible utterance information.New request can be with formerly ask unrelated new request or with formerly please
Ask relevant request.For example, about request, the timing for example before the text message in response to first request can be asked
Between or certain amount lines at audible voice content explanation.Client 140 sends new request, server to server 106
106 are received and are processed.
(1834) are asked in response to second user, server recognizes right in (1836) second items of media content and media content
Should be in the second playback position of the second audio sample；According to the second playback position and the mark of items of media content, obtain (1838)
Correspond to the second text message of the second user inquiry of the appropriate section to items of media content；And by the second text message
At least a portion sends (1840) and arrives client device.Server 106 similarly processes new request with formerly request.Server
Position in 106 identification items of media content and item, obtains text message in response to the request, and by text message at least
A part is sent to client 140.
In some embodiments, server is obtained or generates the translation of (1842) text message, and by the translation
At least a portion sends (1844) and arrives client device, and wherein, described at least a portion of translation is corresponding to text message extremely
A few part.Server 106 is being obtained using the text message with the audible utterance information of audible utterance information identical language
Afterwards, can obtain or generate the translation of text message.Server 106 can obtain translation from the document content host 170, or
Person generates translation using machine translation.Text message and corresponding translation are sent to client 140 for aobvious by server 106
Show.
In some embodiments, image information and text message are sent to client device 140 by server 106.
Image information include items of media content corresponding to being sent to client device 140 with by server 106 in media content
The frame of text message identical position or editing or segment.Image information is together with text message at client device 140
Show.Image information provides the context of text message (for example, by being shown in which to say or singing corresponding to text message
The editing of audible language content or frame).
In some embodiments, assume text envelope in the same client end equipment for playing items of media content wherein
Breath.For example, (for example, in browser window in the media player interface that items of media content shows just on client device 102
Mouthful in) play when, (for example, be adjacent to media player interface near the media player interface on client device 102
Region in) show text message.User can carry out explaining that the user of the audible utterance information in items of media content please
Ask, and items of media content is automatically jumped in response to user's request and defined location.Even if working as items of media content
Play and when showing text message on display 140 in client 102, client 102 can also from server 106 or from
Client device 140 receives instruction so that items of media content is jumped in response to user's request and defined location.More generally,
The equipment that broadcasting items of media content can be instructed retreats items of media content or advances to want understanding for explanation with user
The position of language information.
Although it will be appreciated that herein various elements can be described using term " first ", " second " etc., these
Element should not be limited by these terms.These terms are only used for differentiating an element with another.For example, it is possible to will
First contact person is referred to as the second contact person, and similarly the second contact person can be referred to as the first contact person, and which changes description
Meaning, as long as being occurred all by as one man renaming for " the first contact person " and occurring all by for the second contact person
Cause ground renaming.First contact person and the second contact person both contact person, and which is not same contact person.
Terminology used in this article merely for the sake of the purpose for describing particular implementation and is not limiting as right
Require.Unless context is otherwise clearly indicated, as used in the description of embodiment and claims
Singulative " ", " one " and " being somebody's turn to do " be also intended to include plural form.It will also be understood that as used herein term
"and/or" refers to and covers one or more any in the item listed of association and is possible to combine.Will be further understood that
Be that term " including " and/or "comprising" specify the feature, entirety, step, operation, element when using in this manual
And/or the presence of component, but be not excluded for one or more of the other feature, entirety, step, operation, element, component and/or its
The presence of group or additional.
Depending on context, as used herein term " if " can be understood as meaning " when ... " or
" ... afterwards " or " in response to determine " or " according to determining " or " in response to detecting " condition noted earlier be genuine.
Likewise it is possible to by phrase " if it is determined that [foregoing condition is genuine] " or " if [foregoing condition is true
] " or " when [foregoing condition is genuine] " be understood as meaning " after determination " or " in response to determining " or
" according to determining " or " after detecting " or " in response to detecting " foregoing condition are genuine, depending on upper and lower
Text.
Various embodiments will be investigated in detail now, its example will be shown in the drawings.In the following detailed description, illustrate
Many specific detail are to provide the thorough understandings of the present invention and the embodiment.However, it is possible to not have these specific
Implement the present invention in the case of details.In other cases, do not describe in detail well-known method, program, component with
And circuit is in order to avoid unnecessarily make each side of embodiment ambiguous hard to understand.
For purposes of illustration, description above has been described with reference to particular implementation.However, property described above discussion
Be not intended to limit or limit the invention to disclosed precise forms.In view of above teachings, can have many
Modifications and changes.Select and describe embodiment be in order to best explain the principle of the invention and its practical application, with so that
Others skilled in the art can best using have be suitable for envision special-purpose various modifications this
Bright and various embodiments.
Claims (22)
1. a kind of method, including：
At server：
Receive in order to explain and the audible utterance information being associated close to the items of media content of client device broadcasting
User's request, wherein, the user's request includes the audio sample of the items of media content and user's inquiry, wherein, the sound
Frequency sample is corresponding to the part of the issue for being temporally close to user inquiry in the items of media content；And
In response to the user's request：
Recognize the items of media content and corresponding to the first playback position in the media content of the audio sample；
According to the identification of first playback position and the items of media content, obtain corresponding to be used for the items of media content
The corresponding text message of partial user inquiry；And
At least a portion of the text message is sent to the client device.
2. method according to claim 1, wherein, the text message includes song lyrics.
3. method according to claim 1, wherein, the text message includes the transcription of speech.
4. method according to claim 1, wherein, the audible utterance information is associated with multiple speakers；And institute
Stating user's request includes the request in order to explain the audible utterance information with regard to corresponding speaker.
5. method according to claim 1, wherein, the text message includes the word being associated with the items of media content
Curtain data.
6. method according to claim 1, wherein, the text message includes the information for obtaining from online document.
7. the method according to any one of claim 1-6, also includes：
Receive in order to understanding that the items of media content that explains with play close to the client device is associated
The second user request of language information, wherein, second user request include the second audio sample of the items of media content with
Second user is inquired about, and wherein, second audio sample is corresponding to being temporally close to described the in the items of media content
The part of the issue of two users inquiry；And
Ask in response to the second user：
Identification is corresponding to the second playback position in the corresponding media content of second audio sample；
According to the identification of second playback position and the items of media content, obtain and be used for the items of media content
The second user of appropriate section inquires about the second corresponding text message；And
At least a portion of second text message is sent to the client device.
8. the method according to any one of claim 1-6, wherein, the text message is included corresponding to described
The translation of the audible utterance information of the appropriate section of items of media content.
9. the method according to any one of claim 1-6, also includes：
Obtain or generate the translation of the text message；And
At least a portion of the translation is sent to the client device, wherein, described at least a portion of the translation
Described at least a portion corresponding to the text message.
10. the method according to any one of claim 1-6, wherein, the items of media content is live electricity
Depending on content.
11. methods according to any one of claim 1-6, wherein, the items of media content is previous broadcast
Television content.
12. methods according to any one of claim 1-6, wherein, the items of media content is the interior of recording
Hold.
13. methods according to any one of claim 1-6, wherein, the items of media content is the interior of streaming
Hold.
A kind of 14. server systems, including：
Memorizer；
One or more processors；And
Storage in which memory and is configured for by one or more journeys of one or more of computing devices
Sequence, one or more of programs are included for following instruction：
Receive in order to explain and the audible utterance information being associated close to the items of media content of client device broadcasting
User's request, wherein, the user's request includes the audio sample of the items of media content and user's inquiry, wherein, the sound
Frequency sample is corresponding to the part of the issue for being temporally close to user inquiry in the items of media content；And
In response to the user's request：
Recognize the items of media content and corresponding to the first playback position in the media content of the audio sample；
According to first playback position and the identification of the items of media content, obtain and be used for the items of media content
The user of appropriate section inquires about corresponding text message；And
At least a portion of the text message is sent to the client device.
15. systems according to claim 14, wherein, the audible utterance information is associated with multiple speakers；And
The user's request includes the request in order to explain the audible utterance information with regard to corresponding speaker.
16. systems according to claim 14, also include the instruction for following operation：
Receive in order to understanding that the items of media content that explains with play close to the client device is associated
The second user request of language information, wherein, second user request include the second audio sample of the items of media content with
Second user is inquired about, and wherein, second audio sample is corresponding to being temporally close to described the in the items of media content
The part of the issue of two users inquiry；And
Ask in response to the second user：
The second playback position in the media content of audio sample is stated in identification corresponding to described second；
According to the identification of second playback position and the items of media content, obtain and be used for the items of media content
The second user of appropriate section inquires about the second corresponding text message；And
At least a portion of second text message is sent to the client device.
17. systems according to claim 14, also include the instruction for following operation：
Obtain or generate the translation of the text message；And
At least a portion of the translation is sent to the client device, wherein, described at least a portion of the translation
Described at least a portion corresponding to the text message.
A kind of 18. non-transitory computer-readable storage media, the non-transitory computer-readable storage media storage will be by
One or more programs that computer system with memorizer and one or more processors is executed, one or more of journeys
Sequence includes：
For receiving the audible utterance letter being associated in order to the items of media content that explains with play close to client device
The instruction of the user's request of breath, wherein, the user's request includes the audio sample of the items of media content and user's inquiry, its
In, the audio sample is corresponding to the part of the issue for being temporally close to user inquiry in the items of media content；
And
In response to the user's request：
For recognizing the finger of the first playback position in the items of media content and the media content corresponding to the audio sample
Order；
For being obtained according to the identification of first playback position and the items of media content and being used for the media content
The user of the appropriate section of item inquires about the instruction of corresponding text message；And
For at least a portion of the text message to be sent to the instruction of the client device.
19. computer-readable recording mediums according to claim 18, wherein, the audible utterance information is spoken with multiple
Person is associated；And the user's request includes the request in order to explain the audible utterance information with regard to corresponding speaker.
20. computer-readable recording mediums according to claim 18, also include：
For receive in order to explain with close to the client device play the items of media content be associated can
The instruction for listening the second user of utterance information to ask, wherein, the second user request includes the second of the items of media content
Audio sample and second user inquiry, wherein, second audio sample is corresponded to and is connect in the items of media content in time
It is bordering on the part of the issue of second user inquiry；And
Ask in response to the second user：
Instruction for the second playback position in media content of the identification corresponding to second audio sample；
For being obtained according to the identification of second playback position and the items of media content and being used for the media content
The second user of the appropriate section of item inquires about the instruction of the second corresponding text message；And
For at least a portion of second text message to be sent to the instruction of the client device.
A kind of 21. server systems, including：
Memorizer；
One or more processors；And
Storage in which memory and is configured for by one or more journeys of one or more of computing devices
Sequence, one or more of programs include the instruction for requiring the method described in any one of 1-13 for perform claim.
A kind of 22. non-transitory computer-readable storage media, the non-transitory computer-readable storage media storage will be by
One or more programs that computer system with memorizer and one or more processors is executed, one or more of journeys
Sequence includes to require the instruction described in any one of 1-13 for perform claim.
Applications Claiming Priority (9)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/311,204 | 2014-06-20 | ||
US14/311,204 US9946769B2 (en) | 2014-06-20 | 2014-06-20 | Displaying information related to spoken dialogue in content playing on a device |
US14/311,211 US9805125B2 (en) | 2014-06-20 | 2014-06-20 | Displaying a summary of media content items |
US14/311,218 | 2014-06-20 | ||
US14/311,211 | 2014-06-20 | ||
US14/311,218 US9838759B2 (en) | 2014-06-20 | 2014-06-20 | Displaying information related to content playing on a device |
US14/488,213 | 2014-09-16 | ||
US14/488,213 US10206014B2 (en) | 2014-06-20 | 2014-09-16 | Clarifying audible verbal information in video content |
PCT/US2015/036756 WO2015196115A1 (en) | 2014-06-20 | 2015-06-19 | Clarifying audible verbal information in video content |
Publications (2)
Publication Number | Publication Date |
---|---|
CN106462636A true CN106462636A (en) | 2017-02-22 |
CN106462636B CN106462636B (en) | 2021-02-02 |
Family
ID=53761488
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201580033369.5A Active CN106462636B (en) | 2014-06-20 | 2015-06-19 | Interpreting audible verbal information in video content |
Country Status (4)
Country | Link |
---|---|
US (5) | US10206014B2 (en) |
EP (3) | EP3158479B1 (en) |
CN (1) | CN106462636B (en) |
WO (1) | WO2015196115A1 (en) |
Cited By (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN108012173A (en) * | 2017-11-16 | 2018-05-08 | 百度在线网络技术（北京）有限公司 | A kind of content identification method, device, equipment and computer-readable storage medium |
CN108337559A (en) * | 2018-02-06 | 2018-07-27 | 杭州政信金服互联网科技有限公司 | A kind of live streaming word methods of exhibiting and system |
CN109313901A (en) * | 2017-04-21 | 2019-02-05 | 索尼公司 | Information processing unit, reception device and information processing method |
CN110381359A (en) * | 2019-06-26 | 2019-10-25 | 北京奇艺世纪科技有限公司 | A kind of method for processing video frequency, device, computer equipment and storage medium |
CN110390927A (en) * | 2019-06-28 | 2019-10-29 | 北京奇艺世纪科技有限公司 | Audio-frequency processing method, device, electronic equipment and computer readable storage medium |
CN110717066A (en) * | 2019-10-11 | 2020-01-21 | 掌阅科技股份有限公司 | Intelligent searching method based on audio electronic book and electronic equipment |
CN111433845A (en) * | 2017-11-28 | 2020-07-17 | 乐威指南公司 | Method and system for recommending content in the context of a conversation |
CN111798872A (en) * | 2020-06-30 | 2020-10-20 | 联想(北京)有限公司 | Processing method and device for online interaction platform and electronic equipment |
CN113056922B (en) * | 2018-11-23 | 2024-02-02 | 索尼集团公司 | Receiver device including a native broadcaster application |
Families Citing this family (30)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8977255B2 (en) | 2007-04-03 | 2015-03-10 | Apple Inc. | Method and system for operating a multi-function portable electronic device using voice-activation |
US10206014B2 (en) * | 2014-06-20 | 2019-02-12 | Google Llc | Clarifying audible verbal information in video content |
US9946769B2 (en) | 2014-06-20 | 2018-04-17 | Google Llc | Displaying information related to spoken dialogue in content playing on a device |
US9838759B2 (en) | 2014-06-20 | 2017-12-05 | Google Inc. | Displaying information related to content playing on a device |
US9805125B2 (en) | 2014-06-20 | 2017-10-31 | Google Inc. | Displaying a summary of media content items |
US10362391B2 (en) * | 2014-10-24 | 2019-07-23 | Lenovo (Singapore) Pte. Ltd. | Adjusting audio content based on audience |
US10739960B2 (en) * | 2015-09-22 | 2020-08-11 | Samsung Electronics Co., Ltd. | Performing application-specific searches using touchscreen-enabled computing devices |
US10075751B2 (en) * | 2015-09-30 | 2018-09-11 | Rovi Guides, Inc. | Method and system for verifying scheduled media assets |
US10349141B2 (en) | 2015-11-19 | 2019-07-09 | Google Llc | Reminders of media content referenced in other media content |
US10412447B2 (en) * | 2015-12-16 | 2019-09-10 | Gracenote, Inc. | Dynamic video overlays |
US9609397B1 (en) * | 2015-12-28 | 2017-03-28 | International Business Machines Corporation | Automatic synchronization of subtitles based on audio fingerprinting |
US10034053B1 (en) | 2016-01-25 | 2018-07-24 | Google Llc | Polls for media program moments |
EP3408761A1 (en) * | 2016-01-25 | 2018-12-05 | Google LLC | Media program moments guide |
EP3520361B1 (en) * | 2016-10-03 | 2022-04-06 | Telepathy Labs, Inc. | System and method for social engineering identification and alerting |
US10777201B2 (en) * | 2016-11-04 | 2020-09-15 | Microsoft Technology Licensing, Llc | Voice enabled bot platform |
US10846331B2 (en) * | 2016-12-29 | 2020-11-24 | Google Llc | Music recommendations from trending queries |
US9854324B1 (en) | 2017-01-30 | 2017-12-26 | Rovi Guides, Inc. | Systems and methods for automatically enabling subtitles based on detecting an accent |
KR102068182B1 (en) * | 2017-04-21 | 2020-01-20 | 엘지전자 주식회사 | Voice recognition apparatus and home appliance system |
DK201770427A1 (en) | 2017-05-12 | 2018-12-20 | Apple Inc. | Low-latency intelligent automated assistant |
DK179496B1 (en) | 2017-05-12 | 2019-01-15 | Apple Inc. | USER-SPECIFIC Acoustic Models |
US10755700B2 (en) | 2017-06-23 | 2020-08-25 | Ascension Health Alliance | Systems and methods for operating a voice-based artificial intelligence controller |
CN109658919A (en) * | 2018-12-17 | 2019-04-19 | 深圳市沃特沃德股份有限公司 | Interpretation method, device and the translation playback equipment of multimedia file |
US11347471B2 (en) * | 2019-03-04 | 2022-05-31 | Giide Audio, Inc. | Interactive podcast platform with integrated additional audio/visual content |
US11537690B2 (en) * | 2019-05-07 | 2022-12-27 | The Nielsen Company (Us), Llc | End-point media watermarking |
US11487815B2 (en) * | 2019-06-06 | 2022-11-01 | Sony Corporation | Audio track determination based on identification of performer-of-interest at live event |
US11722749B2 (en) * | 2019-07-31 | 2023-08-08 | Rovi Guides, Inc. | Systems and methods for providing content relevant to a quotation |
WO2021132738A1 (en) * | 2019-12-23 | 2021-07-01 | 엘지전자 주식회사 | Display device and method for operating same |
US20220291743A1 (en) * | 2021-03-11 | 2022-09-15 | Apple Inc. | Proactive Actions Based on Audio and Body Movement |
CN113066498B (en) * | 2021-03-23 | 2022-12-30 | 上海掌门科技有限公司 | Information processing method, apparatus and medium |
US11804215B1 (en) * | 2022-04-29 | 2023-10-31 | Apple Inc. | Sonic responses |
Citations (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20040004599A1 (en) * | 2002-07-03 | 2004-01-08 | Scott Shepard | Systems and methods for facilitating playback of media |
CN1972395A (en) * | 2006-12-08 | 2007-05-30 | 清华大学深圳研究生院 | Multimedia home gateway and its implementation method for program recording, recovery, suspension |
CN101329867A (en) * | 2007-06-21 | 2008-12-24 | 西门子（中国）有限公司 | Method and device for playing speech on demand |
CN101499915A (en) * | 2008-02-03 | 2009-08-05 | 突触计算机系统（上海）有限公司 | Method and apparatus for providing multimedia content description information for customer in Internet |
US7788080B2 (en) * | 2001-11-19 | 2010-08-31 | Ricoh Company, Ltd. | Paper interface for simulation environments |
US8433431B1 (en) * | 2008-12-02 | 2013-04-30 | Soundhound, Inc. | Displaying text to end users in coordination with audio playback |
Family Cites Families (150)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20020120925A1 (en) | 2000-03-28 | 2002-08-29 | Logan James D. | Audio and video program recording, editing and playback systems using metadata |
US20030093790A1 (en) | 2000-03-28 | 2003-05-15 | Logan James D. | Audio and video program recording, editing and playback systems using metadata |
US6934963B1 (en) | 1998-09-22 | 2005-08-23 | United Video Properties, Inc. | Interactive television program guide with passive content |
US7209942B1 (en) * | 1998-12-28 | 2007-04-24 | Kabushiki Kaisha Toshiba | Information providing method and apparatus, and information reception apparatus |
CN1312615C (en) | 1999-07-03 | 2007-04-25 | 纽约市哥伦比亚大学托管会 | Fundamental entity-relationship models for the genetic audio visual data signal description |
US6317882B1 (en) | 1999-12-21 | 2001-11-13 | Thomas D. Robbins | System and method for automatically reminding a user of a receiver that a broadcast is on a data stream |
US20020092021A1 (en) | 2000-03-23 | 2002-07-11 | Adrian Yap | Digital video recorder enhanced features |
US7281220B1 (en) * | 2000-05-31 | 2007-10-09 | Intel Corporation | Streaming video programming guide system selecting video files from multiple web sites and automatically generating selectable thumbnail frames and selectable keyword icons |
US8010988B2 (en) | 2000-09-14 | 2011-08-30 | Cox Ingemar J | Using features extracted from an audio and/or video work to obtain information about the work |
US6651253B2 (en) * | 2000-11-16 | 2003-11-18 | Mydtv, Inc. | Interactive system and method for generating metadata for programming events |
US7565367B2 (en) | 2002-01-15 | 2009-07-21 | Iac Search & Media, Inc. | Enhanced popularity ranking |
ATE465597T1 (en) | 2002-11-08 | 2010-05-15 | Koninkl Philips Electronics Nv | RECOMMENDATION DEVICE AND METHOD FOR RECOMMENDING CONTENT |
US10664138B2 (en) | 2003-03-14 | 2020-05-26 | Comcast Cable Communications, Llc | Providing supplemental content for a second screen experience |
US20050015491A1 (en) | 2003-05-16 | 2005-01-20 | Markel Corporation | Systems, methods, and articles of manufacture for dynamically providing web services |
KR20050028150A (en) * | 2003-09-17 | 2005-03-22 | 삼성전자주식회사 | Mobile terminal and method for providing user-interface using voice signal |
ATE363120T1 (en) * | 2003-11-10 | 2007-06-15 | Koninkl Philips Electronics Nv | AUDIO DIALOGUE SYSTEM AND VOICE-CONTROLLED BROWSING PROCESS |
US7707039B2 (en) | 2004-02-15 | 2010-04-27 | Exbiblio B.V. | Automatic modification of web pages |
CN101142591A (en) * | 2004-04-19 | 2008-03-12 | 兰德马克数字服务有限责任公司 | Content sampling and identification |
JP4251634B2 (en) | 2004-06-30 | 2009-04-08 | 株式会社東芝 | Multimedia data reproducing apparatus and multimedia data reproducing method |
US20080275764A1 (en) | 2005-04-25 | 2008-11-06 | Wilson Eric S | Content management system and method |
US8180037B1 (en) | 2005-06-30 | 2012-05-15 | Sprint Spectrum L.P. | Automated registration for mobile device reminders |
US8751502B2 (en) * | 2005-11-29 | 2014-06-10 | Aol Inc. | Visually-represented results to search queries in rich media content |
JP2009524273A (en) | 2005-11-29 | 2009-06-25 | グーグル・インコーポレーテッド | Repetitive content detection in broadcast media |
US8132103B1 (en) | 2006-07-19 | 2012-03-06 | Aol Inc. | Audio and/or video scene detection and retrieval |
US20070244902A1 (en) | 2006-04-17 | 2007-10-18 | Microsoft Corporation | Internet search-based television |
US9173001B1 (en) | 2006-06-27 | 2015-10-27 | Verizon Patent And Licensing Inc. | Media content access systems and methods |
US8108535B1 (en) | 2006-06-30 | 2012-01-31 | Quiro Holdings, Inc. | Methods, systems, and products for selecting images |
US7664744B2 (en) | 2006-07-14 | 2010-02-16 | Yahoo! Inc. | Query categorizer |
US8850464B2 (en) | 2006-10-09 | 2014-09-30 | Verizon Patent And Licensing Inc. | Systems and methods for real-time interactive television polling |
US8079048B2 (en) | 2006-12-15 | 2011-12-13 | At&T Intellectual Property I, L.P. | System and method of scheduling an event related to an advertisement |
US20080154870A1 (en) * | 2006-12-26 | 2008-06-26 | Voice Signal Technologies, Inc. | Collection and use of side information in voice-mediated mobile search |
US7801888B2 (en) | 2007-03-09 | 2010-09-21 | Microsoft Corporation | Media content search results ranked by popularity |
US8631440B2 (en) | 2007-04-30 | 2014-01-14 | Google Inc. | Program guide user interface |
US7983915B2 (en) | 2007-04-30 | 2011-07-19 | Sonic Foundry, Inc. | Audio content search engine |
US8099315B2 (en) | 2007-06-05 | 2012-01-17 | At&T Intellectual Property I, L.P. | Interest profiles for audio and/or video streams |
JP4952433B2 (en) | 2007-08-08 | 2012-06-13 | ソニー株式会社 | Information processing apparatus and method, and information processing system |
US20090083281A1 (en) | 2007-08-22 | 2009-03-26 | Amnon Sarig | System and method for real time local music playback and remote server lyric timing synchronization utilizing social networks and wiki technology |
US8275764B2 (en) | 2007-08-24 | 2012-09-25 | Google Inc. | Recommending media programs based on media program popularity |
TWI359573B (en) | 2007-12-12 | 2012-03-01 | Inventec Corp | Electric device |
US8060525B2 (en) | 2007-12-21 | 2011-11-15 | Napo Enterprises, Llc | Method and system for generating media recommendations in a distributed environment based on tagging play history information with location information |
US7487096B1 (en) * | 2008-02-20 | 2009-02-03 | International Business Machines Corporation | Method to automatically enable closed captioning when a speaker has a heavy accent |
US9378286B2 (en) | 2008-03-14 | 2016-06-28 | Microsoft Technology Licensing, Llc | Implicit user interest marks in media content |
US8140562B1 (en) | 2008-03-24 | 2012-03-20 | Google Inc. | Method and system for displaying real time trends |
US8312376B2 (en) | 2008-04-03 | 2012-11-13 | Microsoft Corporation | Bookmark interpretation service |
JP2011525727A (en) | 2008-05-26 | 2011-09-22 | コーニンクレッカ フィリップス エレクトロニクス エヌ ヴィ | Method and apparatus for presenting a summary of content items |
WO2010018586A2 (en) | 2008-08-14 | 2010-02-18 | Tunewiki Inc | A method and a system for real time music playback syncronization, dedicated players, locating audio content, following most listened-to lists and phrase searching for sing-along |
US8122094B1 (en) | 2008-11-05 | 2012-02-21 | Kotab Dominic M | Methods for performing an action relating to the scheduling of an event by performing one or more actions based on a response to a message |
US8516533B2 (en) | 2008-11-07 | 2013-08-20 | Digimarc Corporation | Second screen methods and arrangements |
US20110063503A1 (en) | 2009-07-06 | 2011-03-17 | Brand Steven M | Synchronizing secondary content to a multimedia presentation |
US8707381B2 (en) | 2009-09-22 | 2014-04-22 | Caption Colorado L.L.C. | Caption and/or metadata synchronization for replay of previously or simultaneously recorded live programs |
US20110078020A1 (en) | 2009-09-30 | 2011-03-31 | Lajoie Dan | Systems and methods for identifying popular audio assets |
US8677400B2 (en) | 2009-09-30 | 2014-03-18 | United Video Properties, Inc. | Systems and methods for identifying audio content using an interactive media guidance application |
CN102741842A (en) | 2009-12-04 | 2012-10-17 | Tivo有限公司 | Multifunction multimedia device |
US20110218946A1 (en) | 2010-03-03 | 2011-09-08 | Microsoft Corporation | Presenting content items using topical relevance and trending popularity |
WO2011119775A1 (en) | 2010-03-23 | 2011-09-29 | Google Inc. | Organizing social activity information on a site |
US8572488B2 (en) * | 2010-03-29 | 2013-10-29 | Avid Technology, Inc. | Spot dialog editor |
US20110246383A1 (en) | 2010-03-30 | 2011-10-06 | Microsoft Corporation | Summary presentation of media consumption |
US8645125B2 (en) * | 2010-03-30 | 2014-02-04 | Evri, Inc. | NLP-based systems and methods for providing quotations |
US9264785B2 (en) | 2010-04-01 | 2016-02-16 | Sony Computer Entertainment Inc. | Media fingerprinting for content determination and retrieval |
US8560583B2 (en) | 2010-04-01 | 2013-10-15 | Sony Computer Entertainment Inc. | Media fingerprinting for social networking |
US9191639B2 (en) | 2010-04-12 | 2015-11-17 | Adobe Systems Incorporated | Method and apparatus for generating video descriptions |
US9159338B2 (en) * | 2010-05-04 | 2015-10-13 | Shazam Entertainment Ltd. | Systems and methods of rendering a textual animation |
US20120191231A1 (en) | 2010-05-04 | 2012-07-26 | Shazam Entertainment Ltd. | Methods and Systems for Identifying Content in Data Stream by a Client Device |
US8994311B1 (en) | 2010-05-14 | 2015-03-31 | Amdocs Software Systems Limited | System, method, and computer program for segmenting a content stream |
US8731939B1 (en) * | 2010-08-06 | 2014-05-20 | Google Inc. | Routing queries based on carrier phrase registration |
TWI408672B (en) * | 2010-09-24 | 2013-09-11 | Hon Hai Prec Ind Co Ltd | Electronic device capable display synchronous lyric when playing a song and method thereof |
US9241195B2 (en) | 2010-11-05 | 2016-01-19 | Verizon Patent And Licensing Inc. | Searching recorded or viewed content |
US20120131060A1 (en) | 2010-11-24 | 2012-05-24 | Robert Heidasch | Systems and methods performing semantic analysis to facilitate audio information searches |
US10440402B2 (en) | 2011-01-26 | 2019-10-08 | Afterlive.tv Inc | Method and system for generating highlights from scored data streams |
US9262612B2 (en) * | 2011-03-21 | 2016-02-16 | Apple Inc. | Device access using voice authentication |
KR101894389B1 (en) | 2011-04-21 | 2018-10-05 | 삼성전자주식회사 | Method and apparatus for connecting between devices |
US20120278331A1 (en) | 2011-04-28 | 2012-11-01 | Ray Campbell | Systems and methods for deducing user information from input device behavior |
US9035163B1 (en) * | 2011-05-10 | 2015-05-19 | Soundbound, Inc. | System and method for targeting content based on identified audio and multimedia |
US8843584B2 (en) | 2011-06-02 | 2014-09-23 | Google Inc. | Methods for displaying content on a second device that is related to the content playing on a first device |
US20120311624A1 (en) | 2011-06-03 | 2012-12-06 | Rawllin International Inc. | Generating, editing, and sharing movie quotes |
US9262522B2 (en) | 2011-06-30 | 2016-02-16 | Rednote LLC | Method and system for communicating between a sender and a recipient via a personalized message including an audio clip extracted from a pre-existing recording |
US20130031162A1 (en) | 2011-07-29 | 2013-01-31 | Myxer, Inc. | Systems and methods for media selection based on social metadata |
US20110289532A1 (en) | 2011-08-08 | 2011-11-24 | Lei Yu | System and method for interactive second screen |
WO2013037081A1 (en) | 2011-09-12 | 2013-03-21 | Intel Corporation (A Corporation Of Delaware) | Using multimedia search to identify products |
WO2013040533A1 (en) | 2011-09-16 | 2013-03-21 | Umami Co. | Second screen interactive platform |
US8433577B2 (en) | 2011-09-27 | 2013-04-30 | Google Inc. | Detection of creative works on broadcast media |
KR101952170B1 (en) | 2011-10-24 | 2019-02-26 | 엘지전자 주식회사 | Mobile device using the searching method |
US8989521B1 (en) * | 2011-11-23 | 2015-03-24 | Google Inc. | Determination of dance steps based on media content |
KR20140097250A (en) | 2011-11-30 | 2014-08-06 | 톰슨 라이센싱 | Method, apparatus and system for enabling the recall of content of interest for subsequent review |
US9245254B2 (en) * | 2011-12-01 | 2016-01-26 | Elwha Llc | Enhanced voice conferencing with history, language translation and identification |
US8607276B2 (en) | 2011-12-02 | 2013-12-10 | At&T Intellectual Property, I, L.P. | Systems and methods to select a keyword of a voice search request of an electronic program guide |
CN102419945A (en) | 2011-12-09 | 2012-04-18 | 上海聚力传媒技术有限公司 | Method, device, equipment and system for presenting display information in video |
US20130149689A1 (en) | 2011-12-10 | 2013-06-13 | Lee M. DeGross | Pop up educational content and/or dictionary entry for images and complementing trivia |
US9135291B2 (en) | 2011-12-14 | 2015-09-15 | Megathread, Ltd. | System and method for determining similarities between online entities |
US8868558B2 (en) * | 2011-12-19 | 2014-10-21 | Yahoo! Inc. | Quote-based search |
US8949872B2 (en) | 2011-12-20 | 2015-02-03 | Yahoo! Inc. | Audio fingerprint for content identification |
US9009025B1 (en) | 2011-12-27 | 2015-04-14 | Amazon Technologies, Inc. | Context-based utterance recognition |
US20130173796A1 (en) | 2011-12-30 | 2013-07-04 | United Video Properties, Inc. | Systems and methods for managing a media content queue |
US8917971B2 (en) | 2011-12-30 | 2014-12-23 | United Video Properties, Inc. | Methods and systems for providing relevant supplemental content to a user device |
US8484203B1 (en) | 2012-01-04 | 2013-07-09 | Google Inc. | Cross media type recommendations for media items based on identified entities |
US20130185711A1 (en) | 2012-01-13 | 2013-07-18 | Daniel Morales | Mobile phone movie quote application |
US9576334B2 (en) | 2012-03-26 | 2017-02-21 | Max Abecassis | Second screen recipes function |
US20130291019A1 (en) | 2012-04-27 | 2013-10-31 | Mixaroo, Inc. | Self-learning methods, entity relations, remote control, and other features for real-time processing, storage, indexing, and delivery of segmented video |
US20130311408A1 (en) | 2012-05-15 | 2013-11-21 | Comcast Cable Communications, Llc | Determining and Predicting Popularity of Content |
US9317500B2 (en) * | 2012-05-30 | 2016-04-19 | Audible, Inc. | Synchronizing translated digital content |
US9792285B2 (en) | 2012-06-01 | 2017-10-17 | Excalibur Ip, Llc | Creating a content index using data on user actions |
US9965129B2 (en) | 2012-06-01 | 2018-05-08 | Excalibur Ip, Llc | Personalized content from indexed archives |
US9237386B2 (en) | 2012-08-31 | 2016-01-12 | Google Inc. | Aiding discovery of program content by providing deeplinks into most interesting moments via social media |
US20140089424A1 (en) | 2012-09-27 | 2014-03-27 | Ant Oztaskent | Enriching Broadcast Media Related Electronic Messaging |
US9727644B1 (en) | 2012-09-28 | 2017-08-08 | Google Inc. | Determining a quality score for a content item |
US8886011B2 (en) | 2012-12-07 | 2014-11-11 | Cisco Technology, Inc. | System and method for question detection based video segmentation, search and collaboration in a video processing environment |
US20140200888A1 (en) * | 2013-01-11 | 2014-07-17 | Ruwei Liu | System and Method for Generating a Script for a Web Conference |
US20170026681A9 (en) | 2013-02-13 | 2017-01-26 | Peel Technologies, Inc. | Notice-based digital video recording system and method |
US9282075B2 (en) | 2013-02-20 | 2016-03-08 | Comcast Cable Communications, Llc | Collaborative composition of multimedia communications |
US20140255003A1 (en) | 2013-03-05 | 2014-09-11 | Google Inc. | Surfacing information about items mentioned or presented in a film in association with viewing the film |
US9479594B2 (en) | 2013-03-14 | 2016-10-25 | Comcast Cable Communications, Llc | Methods and systems for pairing devices |
US20150149482A1 (en) | 2013-03-14 | 2015-05-28 | Google Inc. | Using Live Information Sources To Rank Query Suggestions |
US10447826B2 (en) | 2013-03-14 | 2019-10-15 | Google Llc | Detecting user interest in presented media items by observing volume change events |
US10109021B2 (en) | 2013-04-02 | 2018-10-23 | International Business Machines Corporation | Calculating lists of events in activity streams |
US20150012840A1 (en) | 2013-07-02 | 2015-01-08 | International Business Machines Corporation | Identification and Sharing of Selections within Streaming Content |
WO2015024743A1 (en) | 2013-08-19 | 2015-02-26 | Doowapp Limited | Method and arrangement for processing and providing media content |
US20150067061A1 (en) | 2013-08-30 | 2015-03-05 | Milyoni, Inc. | Systems and methods for predicting and characterizing social media effectiveness |
US10194189B1 (en) | 2013-09-23 | 2019-01-29 | Amazon Technologies, Inc. | Playback of content using multiple devices |
US9456237B2 (en) | 2013-12-31 | 2016-09-27 | Google Inc. | Methods, systems, and media for presenting supplemental information corresponding to on-demand media content |
US10373611B2 (en) * | 2014-01-03 | 2019-08-06 | Gracenote, Inc. | Modification of electronic system operation based on acoustic ambience classification |
US9462230B1 (en) | 2014-03-31 | 2016-10-04 | Amazon Technologies | Catch-up video buffering |
US9535990B2 (en) | 2014-05-20 | 2017-01-03 | Google Inc. | Systems and methods for generating video program extracts based on search queries |
US9536329B2 (en) | 2014-05-30 | 2017-01-03 | Adobe Systems Incorporated | Method and apparatus for performing sentiment analysis based on user reactions to displayable content |
US9430463B2 (en) * | 2014-05-30 | 2016-08-30 | Apple Inc. | Exemplar-based natural language processing |
US20150356102A1 (en) | 2014-06-05 | 2015-12-10 | Mobli Technologies 2010 Ltd. | Automatic article enrichment by social media trends |
US9838759B2 (en) | 2014-06-20 | 2017-12-05 | Google Inc. | Displaying information related to content playing on a device |
US9805125B2 (en) * | 2014-06-20 | 2017-10-31 | Google Inc. | Displaying a summary of media content items |
US9946769B2 (en) | 2014-06-20 | 2018-04-17 | Google Llc | Displaying information related to spoken dialogue in content playing on a device |
US10206014B2 (en) * | 2014-06-20 | 2019-02-12 | Google Llc | Clarifying audible verbal information in video content |
US9756393B2 (en) | 2014-07-31 | 2017-09-05 | At&T Intellectual Property I, L.P. | Recording option for advertised programs |
US20160042766A1 (en) * | 2014-08-06 | 2016-02-11 | Echostar Technologies L.L.C. | Custom video content |
US10074360B2 (en) * | 2014-09-30 | 2018-09-11 | Apple Inc. | Providing an indication of the suitability of speech recognition |
US9767102B2 (en) | 2014-12-01 | 2017-09-19 | Comcast Cable Communications, Llc | Content recommendation system |
US10057651B1 (en) | 2015-10-05 | 2018-08-21 | Twitter, Inc. | Video clip creation using social media |
US10349141B2 (en) * | 2015-11-19 | 2019-07-09 | Google Llc | Reminders of media content referenced in other media content |
US9609397B1 (en) * | 2015-12-28 | 2017-03-28 | International Business Machines Corporation | Automatic synchronization of subtitles based on audio fingerprinting |
US9990176B1 (en) * | 2016-06-28 | 2018-06-05 | Amazon Technologies, Inc. | Latency reduction for content playback |
US10474753B2 (en) * | 2016-09-07 | 2019-11-12 | Apple Inc. | Language identification using recurrent neural networks |
US9854324B1 (en) * | 2017-01-30 | 2017-12-26 | Rovi Guides, Inc. | Systems and methods for automatically enabling subtitles based on detecting an accent |
US10607082B2 (en) * | 2017-09-09 | 2020-03-31 | Google Llc | Systems, methods, and apparatus for image-responsive automated assistants |
US10445429B2 (en) * | 2017-09-21 | 2019-10-15 | Apple Inc. | Natural language understanding using vocabularies with compressed serialized tries |
WO2019173045A1 (en) * | 2018-03-08 | 2019-09-12 | Frontive, Inc. | Methods and systems for speech signal processing |
US10874947B2 (en) * | 2018-03-23 | 2020-12-29 | Sony Interactive Entertainment LLC | Connecting a player to expert help in real-time during game play of a gaming application |
US10635462B2 (en) * | 2018-05-23 | 2020-04-28 | Microsoft Technology Licensing, Llc | Skill discovery for computerized personal assistant |
US11114085B2 (en) * | 2018-12-28 | 2021-09-07 | Spotify Ab | Text-to-speech from media content item snippets |
US11675563B2 (en) * | 2019-06-01 | 2023-06-13 | Apple Inc. | User interfaces for content applications |
US11176942B2 (en) * | 2019-11-26 | 2021-11-16 | Vui, Inc. | Multi-modal conversational agent platform |
WO2022093927A1 (en) * | 2020-10-28 | 2022-05-05 | Christiana Care Health System, Inc. | System and method for automated voice-based healthcare planning using a supplemental clinician user interface |
US11290773B1 (en) * | 2020-12-09 | 2022-03-29 | Rovi Guides, Inc. | Systems and methods to handle queries comprising a media quote |
US11915694B2 (en) * | 2021-02-25 | 2024-02-27 | Intelligrated Headquarters, Llc | Interactive voice system for conveyor control |
-
2014
- 2014-09-16 US US14/488,213 patent/US10206014B2/en active Active
-
2015
- 2015-06-19 EP EP15744395.3A patent/EP3158479B1/en active Active
- 2015-06-19 EP EP19189325.4A patent/EP3579118B1/en active Active
- 2015-06-19 EP EP22196679.9A patent/EP4123481A1/en active Pending
- 2015-06-19 WO PCT/US2015/036756 patent/WO2015196115A1/en active Application Filing
- 2015-06-19 CN CN201580033369.5A patent/CN106462636B/en active Active
-
2019
- 2019-01-04 US US16/240,608 patent/US10638203B2/en active Active
-
2020
- 2020-04-13 US US16/847,224 patent/US11064266B2/en active Active
-
2021
- 2021-07-12 US US17/372,909 patent/US11425469B2/en active Active
-
2022
- 2022-08-22 US US17/892,501 patent/US20220408163A1/en not_active Abandoned
Patent Citations (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7788080B2 (en) * | 2001-11-19 | 2010-08-31 | Ricoh Company, Ltd. | Paper interface for simulation environments |
US20040004599A1 (en) * | 2002-07-03 | 2004-01-08 | Scott Shepard | Systems and methods for facilitating playback of media |
CN1972395A (en) * | 2006-12-08 | 2007-05-30 | 清华大学深圳研究生院 | Multimedia home gateway and its implementation method for program recording, recovery, suspension |
CN101329867A (en) * | 2007-06-21 | 2008-12-24 | 西门子（中国）有限公司 | Method and device for playing speech on demand |
CN101499915A (en) * | 2008-02-03 | 2009-08-05 | 突触计算机系统（上海）有限公司 | Method and apparatus for providing multimedia content description information for customer in Internet |
US8433431B1 (en) * | 2008-12-02 | 2013-04-30 | Soundhound, Inc. | Displaying text to end users in coordination with audio playback |
Cited By (15)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN109313901A (en) * | 2017-04-21 | 2019-02-05 | 索尼公司 | Information processing unit, reception device and information processing method |
CN109313901B (en) * | 2017-04-21 | 2024-04-16 | 索尼公司 | Information processing apparatus, receiving apparatus, and information processing method |
CN108012173A (en) * | 2017-11-16 | 2018-05-08 | 百度在线网络技术（北京）有限公司 | A kind of content identification method, device, equipment and computer-readable storage medium |
US11164571B2 (en) | 2017-11-16 | 2021-11-02 | Baidu Online Network Technology (Beijing) Co., Ltd. | Content recognizing method and apparatus, device, and computer storage medium |
US11716514B2 (en) | 2017-11-28 | 2023-08-01 | Rovi Guides, Inc. | Methods and systems for recommending content in context of a conversation |
CN111433845B (en) * | 2017-11-28 | 2024-04-12 | 乐威指南公司 | Method and system for recommending content in context of session |
CN111433845A (en) * | 2017-11-28 | 2020-07-17 | 乐威指南公司 | Method and system for recommending content in the context of a conversation |
CN108337559A (en) * | 2018-02-06 | 2018-07-27 | 杭州政信金服互联网科技有限公司 | A kind of live streaming word methods of exhibiting and system |
CN113056922B (en) * | 2018-11-23 | 2024-02-02 | 索尼集团公司 | Receiver device including a native broadcaster application |
CN110381359A (en) * | 2019-06-26 | 2019-10-25 | 北京奇艺世纪科技有限公司 | A kind of method for processing video frequency, device, computer equipment and storage medium |
CN110381359B (en) * | 2019-06-26 | 2022-03-08 | 北京奇艺世纪科技有限公司 | Video processing method and device, computer equipment and storage medium |
CN110390927B (en) * | 2019-06-28 | 2021-11-23 | 北京奇艺世纪科技有限公司 | Audio processing method and device, electronic equipment and computer readable storage medium |
CN110390927A (en) * | 2019-06-28 | 2019-10-29 | 北京奇艺世纪科技有限公司 | Audio-frequency processing method, device, electronic equipment and computer readable storage medium |
CN110717066A (en) * | 2019-10-11 | 2020-01-21 | 掌阅科技股份有限公司 | Intelligent searching method based on audio electronic book and electronic equipment |
CN111798872A (en) * | 2020-06-30 | 2020-10-20 | 联想(北京)有限公司 | Processing method and device for online interaction platform and electronic equipment |
Also Published As
Publication number | Publication date |
---|---|
US20210345012A1 (en) | 2021-11-04 |
US20220408163A1 (en) | 2022-12-22 |
US20200245037A1 (en) | 2020-07-30 |
US11064266B2 (en) | 2021-07-13 |
WO2015196115A1 (en) | 2015-12-23 |
US10638203B2 (en) | 2020-04-28 |
US10206014B2 (en) | 2019-02-12 |
EP4123481A1 (en) | 2023-01-25 |
US20150373428A1 (en) | 2015-12-24 |
US20190141413A1 (en) | 2019-05-09 |
US11425469B2 (en) | 2022-08-23 |
CN106462636B (en) | 2021-02-02 |
EP3158479B1 (en) | 2019-09-04 |
EP3579118A1 (en) | 2019-12-11 |
EP3579118B1 (en) | 2022-10-26 |
EP3158479A1 (en) | 2017-04-26 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN106462636A (en) | Clarifying audible verbal information in video content | |
US20200245039A1 (en) | Displaying Information Related to Content Playing on a Device | |
US11354368B2 (en) | Displaying information related to spoken dialogue in content playing on a device | |
US11350173B2 (en) | Reminders of media content referenced in other media content | |
CN108140056A (en) | Media program moment guide | |
US20150370864A1 (en) | Displaying Information Related to Spoken Dialogue in Content Playing on a Device | |
EP3158476B1 (en) | Displaying information related to content playing on a device |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
C06 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
CB02 | Change of applicant information | ||
CB02 | Change of applicant information |
Address after: American CaliforniaApplicant after: Google limited liability companyAddress before: American CaliforniaApplicant before: Google Inc. |
|
GR01 | Patent grant | ||
GR01 | Patent grant |