CN112352441A - Enhanced environmental awareness system - Google Patents
Enhanced environmental awareness system Download PDFInfo
- Publication number
- CN112352441A CN112352441A CN201980043847.9A CN201980043847A CN112352441A CN 112352441 A CN112352441 A CN 112352441A CN 201980043847 A CN201980043847 A CN 201980043847A CN 112352441 A CN112352441 A CN 112352441A
- Authority
- CN
- China
- Prior art keywords
- electrical
- sound
- new sound
- input signal
- signal
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04R—LOUDSPEAKERS, MICROPHONES, GRAMOPHONE PICK-UPS OR LIKE ACOUSTIC ELECTROMECHANICAL TRANSDUCERS; DEAF-AID SETS; PUBLIC ADDRESS SYSTEMS
- H04R25/00—Deaf-aid sets, i.e. electro-acoustic or electro-mechanical hearing aids; Electric tinnitus maskers providing an auditory perception
- H04R25/43—Electronic input selection or mixing based on input signal analysis, e.g. mixing or selection between microphone and telecoil or between microphones with different directivity characteristics
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04R—LOUDSPEAKERS, MICROPHONES, GRAMOPHONE PICK-UPS OR LIKE ACOUSTIC ELECTROMECHANICAL TRANSDUCERS; DEAF-AID SETS; PUBLIC ADDRESS SYSTEMS
- H04R3/00—Circuits for transducers, loudspeakers or microphones
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/1807—Speech classification or search using natural language modelling using prosody or stress
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04R—LOUDSPEAKERS, MICROPHONES, GRAMOPHONE PICK-UPS OR LIKE ACOUSTIC ELECTROMECHANICAL TRANSDUCERS; DEAF-AID SETS; PUBLIC ADDRESS SYSTEMS
- H04R25/00—Deaf-aid sets, i.e. electro-acoustic or electro-mechanical hearing aids; Electric tinnitus maskers providing an auditory perception
- H04R25/50—Customised settings for obtaining desired overall acoustical characteristics
- H04R25/505—Customised settings for obtaining desired overall acoustical characteristics using digital signal processing
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04R—LOUDSPEAKERS, MICROPHONES, GRAMOPHONE PICK-UPS OR LIKE ACOUSTIC ELECTROMECHANICAL TRANSDUCERS; DEAF-AID SETS; PUBLIC ADDRESS SYSTEMS
- H04R25/00—Deaf-aid sets, i.e. electro-acoustic or electro-mechanical hearing aids; Electric tinnitus maskers providing an auditory perception
- H04R25/50—Customised settings for obtaining desired overall acoustical characteristics
- H04R25/505—Customised settings for obtaining desired overall acoustical characteristics using digital signal processing
- H04R25/507—Customised settings for obtaining desired overall acoustical characteristics using digital signal processing implemented by neural network or fuzzy logic
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04R—LOUDSPEAKERS, MICROPHONES, GRAMOPHONE PICK-UPS OR LIKE ACOUSTIC ELECTROMECHANICAL TRANSDUCERS; DEAF-AID SETS; PUBLIC ADDRESS SYSTEMS
- H04R25/00—Deaf-aid sets, i.e. electro-acoustic or electro-mechanical hearing aids; Electric tinnitus maskers providing an auditory perception
- H04R25/55—Deaf-aid sets, i.e. electro-acoustic or electro-mechanical hearing aids; Electric tinnitus maskers providing an auditory perception using an external connection, either wireless or wired
- H04R25/552—Binaural
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04R—LOUDSPEAKERS, MICROPHONES, GRAMOPHONE PICK-UPS OR LIKE ACOUSTIC ELECTROMECHANICAL TRANSDUCERS; DEAF-AID SETS; PUBLIC ADDRESS SYSTEMS
- H04R5/00—Stereophonic arrangements
- H04R5/033—Headphones for stereophonic communication
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/78—Detection of presence or absence of voice signals
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04R—LOUDSPEAKERS, MICROPHONES, GRAMOPHONE PICK-UPS OR LIKE ACOUSTIC ELECTROMECHANICAL TRANSDUCERS; DEAF-AID SETS; PUBLIC ADDRESS SYSTEMS
- H04R2225/00—Details of deaf aids covered by H04R25/00, not provided for in any of its subgroups
- H04R2225/39—Aspects relating to automatic logging of sound environment parameters and the performance of the hearing aid during use, e.g. histogram logging, or of user selected programs or settings in the hearing aid, e.g. usage logging
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04R—LOUDSPEAKERS, MICROPHONES, GRAMOPHONE PICK-UPS OR LIKE ACOUSTIC ELECTROMECHANICAL TRANSDUCERS; DEAF-AID SETS; PUBLIC ADDRESS SYSTEMS
- H04R2225/00—Details of deaf aids covered by H04R25/00, not provided for in any of its subgroups
- H04R2225/41—Detection or adaptation of hearing aid parameters or programs to listening situation, e.g. pub, forest
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04R—LOUDSPEAKERS, MICROPHONES, GRAMOPHONE PICK-UPS OR LIKE ACOUSTIC ELECTROMECHANICAL TRANSDUCERS; DEAF-AID SETS; PUBLIC ADDRESS SYSTEMS
- H04R2225/00—Details of deaf aids covered by H04R25/00, not provided for in any of its subgroups
- H04R2225/43—Signal processing in hearing aids to enhance the speech intelligibility
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04R—LOUDSPEAKERS, MICROPHONES, GRAMOPHONE PICK-UPS OR LIKE ACOUSTIC ELECTROMECHANICAL TRANSDUCERS; DEAF-AID SETS; PUBLIC ADDRESS SYSTEMS
- H04R25/00—Deaf-aid sets, i.e. electro-acoustic or electro-mechanical hearing aids; Electric tinnitus maskers providing an auditory perception
- H04R25/35—Deaf-aid sets, i.e. electro-acoustic or electro-mechanical hearing aids; Electric tinnitus maskers providing an auditory perception using translation techniques
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04R—LOUDSPEAKERS, MICROPHONES, GRAMOPHONE PICK-UPS OR LIKE ACOUSTIC ELECTROMECHANICAL TRANSDUCERS; DEAF-AID SETS; PUBLIC ADDRESS SYSTEMS
- H04R25/00—Deaf-aid sets, i.e. electro-acoustic or electro-mechanical hearing aids; Electric tinnitus maskers providing an auditory perception
- H04R25/35—Deaf-aid sets, i.e. electro-acoustic or electro-mechanical hearing aids; Electric tinnitus maskers providing an auditory perception using translation techniques
- H04R25/353—Frequency, e.g. frequency shift or compression
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04R—LOUDSPEAKERS, MICROPHONES, GRAMOPHONE PICK-UPS OR LIKE ACOUSTIC ELECTROMECHANICAL TRANSDUCERS; DEAF-AID SETS; PUBLIC ADDRESS SYSTEMS
- H04R25/00—Deaf-aid sets, i.e. electro-acoustic or electro-mechanical hearing aids; Electric tinnitus maskers providing an auditory perception
- H04R25/35—Deaf-aid sets, i.e. electro-acoustic or electro-mechanical hearing aids; Electric tinnitus maskers providing an auditory perception using translation techniques
- H04R25/356—Amplitude, e.g. amplitude shift or compression
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04S—STEREOPHONIC SYSTEMS
- H04S2400/00—Details of stereophonic systems covered by H04S but not provided for in its groups
- H04S2400/13—Aspects of volume control, not necessarily automatic, in stereophonic sound systems
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04S—STEREOPHONIC SYSTEMS
- H04S2400/00—Details of stereophonic systems covered by H04S but not provided for in its groups
- H04S2400/15—Aspects of sound capture and related signal processing for recording or reproduction
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04S—STEREOPHONIC SYSTEMS
- H04S7/00—Indicating arrangements; Control arrangements, e.g. balance control
- H04S7/30—Control circuits for electronic adaptation of the sound field
Abstract
Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for an enhanced situational awareness system to present new and important sounds to a user with an enhanced listening experience. In one aspect, a method comprises: detecting a new sound in the current auditory environment that was not present prior to the detection; adjusting the electrical input signal determined to be characteristic of the new sound to generate an adjusted electrical signal; generating an electrical output signal from the conditioned electrical signal and the electrical input signal that was never determined to be characteristic of the new sound; and providing the electrical output signal to the audio output transducer.
Description
Background
Various hearing assistant devices modify the auditory environment to provide an enhanced auditory experience. Examples of such devices include conventional hearing aids, personal sound amplification devices, and augmented reality systems. Some of these devices may determine content that the user is interested in hearing (e.g., using eye gaze) and provide an enhancement of the auditory environment.
However, the ability of people to understand multiple sources of hearing at once is limited. Moreover, this ability degrades with age, especially when age-related hearing loss is active.
Disclosure of Invention
This specification describes technologies relating to an enhanced situational awareness system presenting new sounds to a user with an enhanced listening experience.
In general, one innovative aspect of the subject matter described in this specification can be embodied in methods that include the actions of a system including an audio output transducer that converts an electrical output signal into acoustic sound; and a signal processing device receiving as input an electrical input signal and in data communication with the audio output transducer to provide an electrical output signal to the audio output transducer, and wherein the signal processing device is arranged to execute instructions that cause the signal processing device to perform operations comprising: the method includes detecting a new sound in the current auditory environment that did not exist prior to the detecting based on the electrical input signal and data describing the current auditory environment, adjusting the electrical input signal determined to be characteristic of the new sound to generate an adjusted electrical signal, generating an electrical output signal from the adjusted electrical signal determined to be characteristic of the new sound and the electrical input signal not determined to be characteristic of the new sound, and providing the electrical output signal to the audio output transducer.
These and other embodiments may each optionally include one or more of the following features. In some embodiments, the data describing the current auditory environment includes data generated from an electrical input signal received prior to detection of the electrical input signal for a new sound.
In some embodiments, the system further comprises a plurality of audio input transducers that convert acoustic sound into electrical input signals, wherein the signal processing device is in data communication with the audio input transducers and detecting new sounds in the current auditory environment that were not present prior to detection based on the electrical input signals and the data descriptive of the current auditory environment comprises detecting new sounds from the electrical input signals provided by the audio input transducers.
In some embodiments, generating the adjusted electrical signal determined to be characteristic of the new sound includes storing the electrical input signal determined to be characteristic of the new sound for the duration of the current sound of interest.
In some embodiments, generating the electrical output signal from the conditioned electrical signal and the electrical input signal not determined to be characteristic of the new sound comprises generating the electrical output signal from the electrical input signal not determined to be characteristic of the new sound and the stored electrical input signal determined to be characteristic of the new sound.
In some implementations, generating the adjusted electrical signal determined to be characteristic of the new sound includes monitoring the electrical input signal to detect an absence of the current sound of interest, wherein storing the electrical input signal determined to be characteristic of the new sound occurs during the monitoring. Monitoring the electrical input signal to detect the absence of the current sound of interest may include detecting a prosodic signal indicating an end of the current sound of interest.
In some embodiments, generating the electrical output signal from the conditioned electrical signal and the electrical input signal not determined to be characteristic of the new sound comprises inserting a stored electrical input signal determined to be characteristic of the new sound in the generated electrical output signal when the current sound of interest is not present.
In some implementations, the new sound is a sound generated by speech (e.g., a person speaking or a computer audio file of a person speaking).
In some implementations, when the adjusted electrical signal is used to cause an audio output transducer to generate an audio output, an adjusted electrical input signal determined to be characteristic of the new sound is generated to cause a frequency shift in the new sound.
In some embodiments, based on the electrical input signal and the data descriptive of the current auditory environment, detecting a new sound that was not present in the current auditory environment prior to the detection includes determining a directional component indicative of a source of the new sound. Generating the electrical output signal from the adjusted electrical signal and the electrical input signal that is not determined to be characteristic of the new sound may comprise generating the electrical output signal such that the electrical output signal causes the audio output transducer to generate the new sound as originating from a location defined by the directional component.
In some embodiments, adjusting the electrical input signal determined to be characteristic of the new sound to generate the adjusted electrical signal includes adjusting an amplitude of the electrical input signal determined to be characteristic of the new sound.
Other embodiments of this aspect include corresponding methods, apparatus, and computer programs.
Particular embodiments of the subject matter described in this specification can be implemented to realize one or more of the following advantages. In a dialogue situation where the hearing instrument is focused on one particular source, the enhanced situational awareness system may make the user aware of the new sound by enhancing the audibility of the sound source. The new sound may be adjusted to fit the auditory environment such that it is presented to the user at a time and place that is easily separated from the foreground sound, and in a manner that allows the user's auditory system to easily separate the foreground from the background.
A real-time and personalized auditory environment may be provided to the user, wherein the system may detect and identify new sounds of interest to the user and highlight only the new sounds of interest in the user's auditory environment. By highlighting only the new sound that is the sound of interest to the user, the system may provide the user with an auditory environment with reduced background noise and less distracting sounds. Further, the system may consider the current sound of interest in the user's auditory environment and determine the importance or level of interest of the new sound detected in the user's auditory environment relative to the current sound of interest and adjust the presentation of the new sound accordingly. For example, the virtual reality experience may be augmented by grading sounds in real-time such that the user's focus is drawn to new important sounds in the virtual environment of the virtual reality experience and/or alerting of new important sounds in the auditory environment outside of the virtual reality experience.
The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Drawings
FIG. 1 is a block diagram of an example operating environment for an enhanced situational awareness system.
FIG. 2 is a flow diagram of example operations of an enhanced situational awareness system.
FIG. 3 is a flow chart of an example process for generating an electrical output signal.
Fig. 4A is a flow diagram of another example process for generating an electrical output signal.
Fig. 4B is a flow diagram of another example process for generating an electrical output signal.
FIG. 5 is a flow diagram of another example process for generating an electrical output signal.
Like reference numbers and designations in the various drawings indicate like elements.
Detailed Description
SUMMARY
The technology in this patent application is intended to help users become aware of new auditory objects in their environment. This technique enhances the audibility of new sources when the user is in a focused listening situation. This technique achieves two main features: and (4) detecting and presenting. The detection involves identifying a new sound in the current auditory environment, and the presentation involves adjusting the presentation of the new sound so that the user is more likely to notice the new sound.
More particularly, the technology in this patent application relates to detecting a new sound in the user's auditory environment, adjusting the new sound relative to the auditory environment, and presenting the adjusted new sound and auditory environment to the user. The user's auditory environment may include both naturally generated sounds and machine-generated sounds. In some implementations, the user's auditory environment is presented to the user through a filtering process, for example, where sound is processed using beamforming (e.g., to focus the user's attention to one particular sound source), background noise attenuation (e.g., to reduce unnecessary noise), or level adjustment of natural sounds vs machine-generated sound.
Adjusting the new sound to emphasize the new sound in the user's auditory environment may be by imposing a time delay in the presentation of the new sound (e.g., during a pause in the auditory environment), by a frequency transformation of the new sound (e.g., distinguishing new sound frequencies relative to the auditory environment), by adjusting the amplitude of the new sound (e.g., increasing the relative volume of the new sound to attract attention), or a combination thereof. The system may determine the type of adjustment for the new sound based in part on the type of sound being adjusted. For example, when a new sound is presented to a user in the user's auditory environment, the new sound types of speech and non-speech may be adjusted differently from each other to better highlight the new sound. The type of adjustment of the new sound may be based in part on the determined importance of the new sound (e.g., based on a classification of the sound) such that the presentation of the adjusted new sound more or less interferes with the user's auditory environment. The type of adjustment of the new sound may be based in part on user preferences such that when the new sound is presented to the user in the user's auditory environment, the user may select a type of adjustment for a particular type of sound (e.g., a class of sound).
These features and additional features are described in more detail below.
Example operating Environment
Fig. 1 is a block diagram of an example environment 100 in which an enhanced situational awareness system 102 provides an enhanced listening experience to a user 104. The auditory environment of the user 104 includes all sounds presented to the user 104 (e.g., all sounds within the user's audible range). The auditory environment may include sound from a real environment (e.g., audio signals in the user's surrounding area), or sound from a virtual environment (e.g., synthesized audio signals), or a combination thereof.
In some implementations, the auditory environment can include frequencies outside of the nominal audible range of the user 104 (e.g., treble sounds or bass sounds). The audible range of the sound presented to the user 104 in the auditory environment may depend in part on the sensitivity of the audio input transducer 106 (e.g., the conversion efficiency of the audio input transducer 106 to convert the sound pressure to a voltage).
The enhanced situational awareness system 102 optionally includes an audio input transducer 106, the audio input transducer 106 may receive audio input sounds from one or more audio sources 108a, 108b in the auditory environment of the user 104. The audio input transducer 106 may convert audio input signals from one or more sources 108a, 108b into an electrical input signal 110. The audio sources 108a, 108b are humans, animals, sound emitting devices, or environmental events that generate sound and are presented to the user 104. The sources 108a, 108b may include humans (e.g., human speech), environmental noise (e.g., weather, traffic), alerts (e.g., sirens, alarms), animals (e.g., dog calls), and so forth.
The enhanced situational awareness system 102 may also receive an electrical input signal 110, the electrical input signal 110 being characteristic of sound originating from one or more virtual sources 112. The electrical input signal 110 includes, for example, a stored electrical signal (e.g., data stored as part of a sound file and electronically processed), and an electrical signal that may be generated from a stored audio file (e.g., an mp3 file) (e.g., by a computer). The electrical input signal 110 may be provided by a virtual reality system, an augmented reality system, a computer, a media playback device (e.g., mp3 player, video player), a user's smart phone, or other device as a characteristic of sound originating from the virtual source 112, where the sound originating from the virtual source 112 is generated by providing the electrical input signal 110 to one or more audio output transducers (e.g., speakers, headphones).
The electrical input signal 110 may be used to describe, for example, a current auditory environment representative of the user 104. In some embodiments, the additional data may also be used to describe the current auditory environment. For example, examples of such data describing the current auditory environment of the user 104 may include information related to the content of the field of view of the user 104 (e.g., in a virtual or augmented reality experience) as well as audio data buffered from the last period of time (e.g., the last three seconds). The enhanced situational awareness system 102 may define a background level of the current auditory environment by monitoring changes in a spectrogram of the current auditory environment. Background estimation and subtraction algorithms can be used to remove background noise from the current auditory environment, e.g., to perform speech enhancement. An exemplary process for performing Speech enhancement is described in IEEE Transactions on Acoustics, Speech, and Signal Processing, Vol.32, No. 6, p.1109-. Other procedures may also be used.
The enhanced situational awareness system 102 may monitor sounds in the area surrounding the user 104 to detect new sounds. For example, the enhanced situational awareness system 102 may continuously or periodically scan the area around the user (e.g., beam steering around a radius around the user's surroundings) to check for new sounds, and process the audio input and electrical input signals 110 to detect new sounds in the auditory environment. The electrical input signal 110 provided by the audio input transducer 106, the virtual source 112 or a combination of the audio input transducer 106 and the virtual source 112 is provided to a new sound detector 114.
In some implementations, the new sound is detected based on the electrical input signal 110 and data describing the current auditory environment in the electrical input signal 110 provided from the audio input transducer 106. For example, the new sound may be a person speaking, a door closing, or an alarm in an area around the user 104.
In some implementations, the new sound is detected based on the electrical input signal 110 and data describing the current auditory environment in the electrical input signal 110 provided from the virtual source 112. For example, the new sound may be a virtual human speaking in a virtual reality experience in which the user 104 is engaged, or alternatively, a real human speaking in a physical environment in which the user is located.
The new sound detector 114 may detect a new sound in the auditory environment that was not present prior to detection based on the electrical input signal 110 and data describing the current auditory environment. Depending on the actual origin of the new sound, it may be determined that the new sound originates from the sources 108a, 108b and/or from the virtual source 112.
More specifically, the new sound is a sound that was determined not to be part of the auditory environment prior to the new sound being detected by the new sound detector 114. The new sound may be, for example, a person speaking, an alarm, a door closed, a dog barking, or other sound detected in the auditory environment of the user 104 and not present prior to such detection. In one example, the new sound is a sound generated by speech (e.g., a person speaking). In another example, the new sound is not a speech signal (e.g., a driving car).
The electrical signal characteristics of the new sound detected by the new sound detector 114 are provided to an electrical signal conditioner 116. The electrical signal conditioner 116 may perform one or more adjustments (e.g., time delays, frequency or pitch shifts, amplitude shifts, etc.) to the electrical input signal determined to be characteristic of the new sound to generate a conditioned electrical signal.
The adjustment of the electrical input signal determined to be characteristic of the new sound may depend in part on the classification of the new sound. The adjustment of the electrical input signal determined to be of the new characteristic may be based in part on the classification of the new sound, causing the audio output from the output electrical signal generated from the adjusted electrical signal to be more or less intrusive to the user's auditory environment. The importance of the type sound to the user (e.g., classification as a sound from an alarm), the relative location of the source of the new sound (e.g., source behind the user compared in the user's field of view), the familiarity of the new sound (e.g., the user's spouse talk compared to a commercial radio station), each can affect the type of adjustment made to the electrical input signal determined to be characteristic of the new sound. In one example, the location of the source of the new sound outside the field of view of the user 104 may result in a first type of adjustment (e.g., a time delay of the new sound) and the location of the source of the new sound within the field of view of the user 104 may result in a second type of adjustment (e.g., a frequency offset of the new sound). The conditioned electrical signal is discussed in more detail below with reference to fig. 3, 4A, and 4B.
In some implementations, the adjustment of the electrical input signal determined to be characteristic of the new sound may depend in part on user preferences. The user 104 may indicate to the enhanced situational awareness system 102 the type of adjustment that the user prefers the new sound. In one example, the user 104 may specify a type of adjustment (e.g., frequency shift) for the new sound based on the type of sound (e.g., voice-type sound). In another example, the user 104 may specify the type of adjustment (e.g., amplitude offset) for all new sounds.
In some implementations, the adjustment of the electrical input signal determined to be characteristic of the new sound may depend in part on the classification of the current sound of interest in the user's auditory environment. For example, when it is determined that the current sound of interest is a speech-type sound (e.g., the user 104 is engaged in a conversation), a time delay adjustment may be applied to the electrical input signal determined to be characteristic of the new sound. In another example, when it is determined that the current sound of interest is a non-speech type sound (e.g., the user 104 is listening to music), an amplitude offset adjustment may be applied to the electrical input signal determined to be characteristic of the new sound.
In some implementations, the adjustment of the electrical input signal determined to be characteristic of the new sound may depend in part on the circumstances (e.g., time and physical space) of the user's auditory environment. When a new sound is detected, the user may find that a particular new sound is more or less important based on the user's situation, and the type of adjustment of the electrical input signal determined to be characteristic of the new sound may differ depending on the importance. For example, the user's auditory environment may be a workplace setting (e.g., the user is at work), while the new sound is a telephone ring tone, which may be very important to the user. As a result, an electrical input signal determined to be characteristic of a telephone ringtone may be adjusted by a shift in amplitude (e.g. made louder). In another example, a coworker sneezing in the user's workplace may be less important to the user such that the electrical input signal determined to be characteristic of the sneezing may be time delayed.
In some implementations, the new sound detector 114 can detect a number of new sounds present in the current auditory environment. The new sound detector 114 may rank the plurality of new sounds (e.g., by the relative importance of the new sounds to the user) to determine the type of adjustment or the degree to which the electrical input signal is determined that the characteristics of each new sound should be adjusted. For example, the new sound detector 114 may detect two new sounds in the current auditory environment of the user 104: a person speaking out of the user's 104 field of view and the user's mobile phone ring tone. The new sound detector may determine that the person speaking is ranked higher than the user's mobile phone ring tone and adjust the electrical input signal for each sound accordingly. In another example, the new sound detector 114 may detect two new sounds in the current auditory environment of the user 104: a person speaking in the field of view of the user 104 and a person speaking outside the field of view of the user 104. The new sound detector may determine that the ranking of the person speaking in the user's field of view is higher than the ranking of the user outside the user's field of view (e.g., because it may prevent audio/visual dyssynchrony).
In some embodiments, the electrical input signal and the adjusted electrical signal determined not to be characteristic of the new sound are stored as stored electrical signals 118. How long the electrical signal is stored may depend, for example, on the duration of the new sound. For example, the adjusted electrical signal may be stored in the stored electrical signal 118 until it is determined that the current sound of interest is not present (e.g., the current person is speaking a complete statement). Alternatively, only the last N seconds of the electrical signal for the new sound may be stored.
The electrical output signal generator 120 may generate an electrical output signal from the adjusted electrical signal and the electrical input signal that is not determined to be characteristic of the new sound. In some embodiments, the electrical output signal generator 120 receives the stored electrical signals 118, the electrical signals 118 including the conditioned electrical signals and electrical input signals that are not determined to be characteristic of the new sound.
In some embodiments, the new sound detector 114 does not detect the new sound such that no electrical signal is conditioned, and the new sound detector 114 serves as a channel for the received electrical input signal 110 and provides it to the electrical output signal generator 120.
The electrical output signals are provided to one or more audio output transducers 122, which convert the electrical output signals into acoustic sound. The acoustic sound may then be provided to one or more audio output devices 124 to present the acoustic sound to the user 104. The audio output device 124 may include, for example, an earphone, a speaker, a hearing aid, and the like.
In some implementations, the audio output device 124 can be a component of a virtual reality or augmented reality system. The audio output device 124 may provide audio for a virtual reality or augmented reality system that also includes a visual presentation of a virtual reality or augmented reality experience. In one example, the augmented reality system includes wearable "smart" glasses that include audio output devices 124 (e.g., earpieces, integrated speakers).
The processes performed by the new sound detector 116, the electrical signal conditioner 116, and the electrical output signal generator 120 are performed by one or more signal processing devices 126. The signal processing device 126 may be in data communication with the audio output transducer 122, or the audio output transducer and the audio input transducer 106. The signal processing device 126 may receive the electrical input signal 110 as an input and may provide an electrical output signal to the audio output transducer 122.
Example operation of an enhanced Environment awareness System
FIG. 2 is a flow diagram of example operations 200 of the enhanced situational awareness system 102. The current auditory environment and new sound may include sound from real sources (e.g., sources 108a, 108b), virtual sources (e.g., virtual source 112), or a combination of real and virtual sources (e.g., in an augmented reality experience).
A new sound that is not present in the current auditory environment is detected (202). The new sound detector 114 may detect a new sound from the electrical input signal 110 received by the new sound detector 114, wherein the electrical input signal 110 may originate from the virtual source 112, an audio input transducer converting audio signals from one or more sources 108a and 108b, or a combination thereof. In some implementations, the variance of the spectrogram calculated for the sound in the current auditory environment of the user 104 can be used to determine whether a new sound is present in the auditory environment. For example, the current auditory environment may be captured by looking at statistics of a spectrogram of the current auditory environment or a modulated spectrogram of the current auditory environment, and deviations from normal may be detected as a new sound.
For example, the new sound detector 114 may use machine learning techniques to identify new sounds that are different from the current auditory environment. A statistical model may be used to characterize the electrical input signal 110 and determine whether the electrical input signal 110 is characteristic of a new sound (e.g., determined to be outside the range of the current auditory environment). The new sound detector 114 may evaluate the electrical input signal 110 using one or more of an ambient sound background model, an auditory significance model, a binaural localization model, and a speech detector to determine whether a new sound is present in the current auditory environment. The new sound detector may use other processes to determine whether a new sound is present in the current auditory environment.
For example, the ambient sound background model may be used to identify background noise in the electrical input signal so that the new sound detector 114 may better separate potential new sounds in the current auditory environment from the background noise. One example of how enhanced awareness system 102 may use a voice activity detector to distinguish between new sounds (e.g., may detect a new voice signal) is described in IEEE Transactions on Audio, Speech, and Language Processing, Vol.14, No. 3, page 920-930, N.Mesgarani et al, in 2006, 5, "diagnosis of Speech from Speech based on multiscale spatial-temporal modulation". In another example, an Audio event classifier may be used to detect when a new type of sound is present, as described in "CNN architecture for Large-Scale Audio Classification" by Hershey, Shawn et al, which has been published at ICASSP 2017, with the latest revision of 1/10 in 2017 (arXIv: 1609.09430v 2). Other procedures may also be used. The enhanced situational awareness system 102 may distinguish between foreground sounds (e.g., important sounds that are relevant to the user) and background sounds (e.g., unimportant sounds that are not relevant to the user).
In another example, the auditory significance model may be used to determine what is perceptually significant to the user 104 in the auditory environment. The user's 104 response to sounds detected in the auditory environment by the enhanced situational awareness system 102 (e.g., gaze tracking, electroencephalographic signals) may be monitored in order to determine what sounds are perceived by the user 104. One method of determining auditory significance is described in "Modeling audio attentions" by Kaya, EM et al in the cosmetic transactions of the Royal Society B, Vol.372, issue No. 1714, 19/2/2017. For example, a new phonetic topic may be identified using the system described in Broderick, Michael P. et al, "Electrical scientific coefficients of semiconductor discrete reflection of natural, cognitive speed" in Current Biology volume 28, No. 5, 803-809.e 3. Additionally, the brain electrical signals may be used to determine when the sound is interesting (e.g., should be adjusted to attract attention to the sound in the user's auditory environment), for example, as in Conf Proc IEEE Eng Med Biol soc.2017 month 7; 2017: 1644- "Neural decoding of atomic selection in multi-space environments with access to isolated sources" of J.et al in 1647. Other procedures may also be used.
In another example, a binaural localization model may be used to determine the orientation of the new sound source with respect to the user 104. For example, the new sound detector 114 may use a binaural localization model to determine the relative location of the new sound source to the user 104 (e.g., behind the user, in front of the user). The position information extracted using the binaural localization model may be used by the enhanced situational awareness system 102 to determine the type of adjustment that should be made to the new sound. In one example, directional Sound may be synthesized by creating auditory level differences (ILD) and auditory time differences (ITD) using the process described in "A Structural Model for binary Sound Synthesis" by Brown, C.Phillip et al, in IEEE Transactions on Speech and Audio Processing, volume 6, volume 5, 12 of 1998. Other procedures may also be used.
The new sound detector 114 may determine the category of the new sound. Sounds determined to be present in the auditory environment may be classified (e.g., as an alarm, human speech, music, wind), and the importance of each sound category may be determined.
The new sound detector 114 may classify the new sound, for example, using neural network/machine learning, which includes the type of sound source (e.g., human, animal, machine), the importance of the new sound (e.g., important or unimportant), the user's familiarity with the new sound (e.g., voice vs door closed from a known human), the location of the sound source relative to the user (e.g., source in front of the user or behind the user). The new sound detector 114 may further classify the voice type of the new sound (e.g., dialogue speech, alarm speech) as being classified as a new sound based on speech. For example, the new sound detector 114 can distinguish between a voice of an alarm type (e.g., "Look out beyond you |") and a voice of a conversation type (e.g., "Nice to meet you (see you with a high pleasure")). For example, emotions and/or mood changes may be detected using the system described in Acoustics, Speech and Signal Processing,1998, Proceedings of the 1998IEEE International Conference on, Seattle, WA,1998, page 985-. In another example, new sounds may be identified and classified using the AudioSet technique described by Hershey, Shawn et al, which has been published on ICASSP 2017, newly revised on 2017 at 1/10 (arXiv: 1609.09430v 2). Other procedures may also be used.
In one example, the speech detector may be used to detect whether human speech is present in the electrical input signal, for example, by using noise reduction (e.g., via spectral subtraction), feature detection of the electrical input signal, and classification of features of the electrical input signal to determine whether the electrical input signal is characteristic of human speech. One exemplary method for performing Speech detection is described in IEEE Transactions on Audio, Speech, and Language Processing, Vol.14, volume 3, page 920-930, Mesgarani, N et al, in 2006, 5, trade of Speech from Speech based on multiscale spatial-temporal modulation. Other procedures may also be used.
In some implementations, the enhanced situational awareness system 102 can use data provided from an augmented reality or virtual reality system to determine whether a new sound is present (e.g., data provided by a virtual reality environment in which the new sound is present). For example, the virtual reality system may provide data to the enhanced situational awareness system 102 regarding what audio/visual is presented to the user 104 in the virtual reality experience.
The electrical input signal 110 determined to be characteristic of the new sound is adjusted to generate an adjusted electrical signal (204). The electrical signal conditioner 116 may receive the electrical input signal 110 from the new sound detector 114 that is determined to be characteristic of the new sound and perform one or more adjustments to the electrical input signal. The adjustment of the electrical input signal determined to be characteristic of the new sound may comprise a frequency shift, a time delay, an amplitude shift, or a combination thereof. More details of the type of adjustment for the electrical input signal determined to be characteristic of the new sound are described with reference to fig. 3, 4A and 4B.
An electrical output signal is generated (206) from the conditioned electrical signal and the electrical input signal that was never determined to be characteristic of the new sound. The electrical output signal may be generated by the electrical output signal generator 120 from the stored electrical signal 118 or the electrical input signal 110 (e.g., an electrical input signal that is not determined to be characteristic of a new sound). In some implementations, the new sound detector 114 does not detect a new sound in the current auditory environment of the user 104, and the electrical output signal generator 120 can generate an electrical output signal from the electrical input signal 110.
The electrical output signals are provided to an audio output transducer 122(208) to convert the electrical output signals into audio signals, which may be provided to one or more audio output devices 124 (e.g., headphones or speakers) to produce sound for the user 104. For example, an electrical output signal generated from the conditioned electrical signal and an electrical input signal that is not characteristic of the new sound are provided to the audio output transducer 122.
Generating a conditioned electrical signal
One way to notify the user of the new sound is to present the new sound after there is no more sound of current interest to the user. This may be done by storing an electrical signal indicative of the new sound of interest. In some implementations, the adjusted electrical signal determined to be characteristic of the new sound is generated by storing the electrical input signal (e.g., the stored electrical signal 118) over the duration of the current sound of interest (e.g., the time delay of the new sound). Fig. 3 is a flow chart of an example process 300 for generating an electrical output signal. During the duration of the current sound of interest, the electrical input signal determined to be characteristic of the new sound is stored (302). The current sound of interest may be a sound that the user 104 is focusing on (e.g., by monitoring eye gaze, electroencephalogram signals, etc.). For example, the current sound of interest may be a speaker with which the user 104 is speaking, which may be determined by the user 104 looking at the speaker. In another example, a current sound of interest may be determined in a virtual reality or augmented reality system by determining what a user is looking at (e.g., using gaze tracking) or what is presented to the user in a visual component of a virtual reality or augmented reality experience (e.g., content projected on a screen).
The duration of the current sound of interest may be determined during the interval that the current sound of interest is providing audio input to the enhanced awareness system 102. For example, during the duration of active participation of the speaker.
In some implementations, the current sound of interest is provided by the virtual/augmented reality system, and the duration of the current sound of interest may be provided by the virtual/augmented reality system to the enhanced situational awareness system 102. For example, if the user 104 is listening to a person speaking in a virtual reality experience in a virtual reality system, the virtual reality system may provide the enhanced situational awareness system 102 with information about the user's experience and how long the person in the virtual reality experience will be actively speaking.
The electrical input signal is monitored to detect the absence of a current sound of interest (304). The absence of the current sound of interest may be determined by monitoring an electrical input signal characteristic of the current sound of interest. The absence of the current sound of interest may be a pause or an end of the current sound of interest. In one example, the current sound of interest is conversational speech, where a speaker's pause in the conversation may be detected by monitoring prosodic signals (e.g., inflection point, emphasis, contrast of speech, focus of speech) of the current sound of interest. Prosodic signals may be the conclusion of pitch drops in the speaker's mood, indicating a statement or expressed idea.
In some implementations, the current sound of interest is provided to the user through a virtual/augmented reality experience by a virtual/augmented reality system, where the virtual/augmented reality system can provide information to the enhanced situational awareness system 102 as to when no current sound of interest will occur. For example, a user may play a game over a virtual reality system such that the virtual reality system has present and future knowledge of the occurrence, and may provide the enhanced situational awareness system 102 with information regarding when there will be gaps in the current sound of interest in the virtual reality game.
Upon determining that the current sound of interest is not present, the stored electrical input signal determined to be characteristic of the new sound is inserted into the generated electrical output signal (306). For example, the electrical input signal characteristics of the alarm in the user's auditory environment may be stored (e.g., in the stored electrical signal 118) until the person speaking with the user 104 pauses, at which time the electrical signal characteristics of the alarm are inserted into the electrical output signal.
An electrical output signal is generated (308) from the electrical input signal that is not determined to be characteristic of the new sound and the stored electrical input signal that is determined to be characteristic of the new sound. In some implementations, the new sound is provided to the user 104 after a period of delay that is dependent on the enhanced situational awareness system 102 detecting the absence of the current sound of interest. In other words, the electrical output signals from the conditioned electrical signal and the electrical input signal that is not characteristic of the new sound are generated by inserting the stored electrical input signal (e.g., in the stored electrical signal 118) as characteristic of the new sound when the absence of the current sound of interest is detected.
In one example, the enhanced situational awareness system 102 may monitor a current sound of interest (e.g., a person speaking with the user 104) and delay providing a new sound (e.g., a telephone ring tone) to the user 104. The delay of the new sound may be accomplished by storing the electrical input signal characteristics of the telephone ring tone (e.g., in the stored electrical signal 118) until the person's speech with the user pauses or their speech ends (e.g., apnea, complete statement).
Fig. 4A is a flow diagram of another example process 400 for generating an electrical output signal. When the adjusted electrical signal is used to cause an audio output transducer to generate an audio output, the electrical input signal determined to be characteristic of the new sound is adjusted to cause a frequency shift of the new sound (402). In some embodiments, the frequency shift in the new sound may shift the pitch of the new sound, where a high pitch sound corresponds to a high frequency sound wave and a low pitch sound corresponds to a low frequency sound wave.
For example, when the new sound is played through the audio output device, the frequency shift may shift the frequency of the new sound to a lower frequency, such that the lower frequency results in the new sound being within the audible range of the user 104 and outside of the range of the foreground sound.
In another example, the frequency shift may be to shift the pitch of the new sound to a different pitch than the pitch of the sounds included in the current auditory environment (e.g., to raise the pitch of the new sound to be different from the background sound).
An adjusted electrical input signal determined to be characteristic of the new sound is generated from the frequency shifted electrical input signal determined to be characteristic of the new sound (404). The conditioned electrical input signal may be stored (e.g., in the stored electrical signal 118) and provided to the electrical output signal generator 120 for incorporation in the electrical output signal.
Fig. 4B is a flow diagram of another example process 410 for generating an electrical output signal. When the adjusted electrical signal is used to cause an audio output transducer to generate an audio output, the electrical input signal determined to be characteristic of the new sound is adjusted to adjust the amplitude in the new sound. In some implementations, the amplitude adjustment in the new sound may be to increase or decrease the amplitude of the new sound, where an increased amplitude corresponds to a greater perceived sound by the user 104 and a decreased amplitude corresponds to a quieter perceived sound by the user 104.
For example, the amplitude adjustment may be to adjust the amplitude of the new sound to a higher amplitude when the new sound is played through the audio output device, such that the higher amplitude results in the new sound perceived by the user 104 being louder relative to the auditory environment.
An adjusted electrical input signal determined to be characteristic of the new sound is generated from the amplitude adjusted electrical input signal determined to be characteristic of the new sound (414). The conditioned electrical input signal may be stored (e.g., in the stored electrical signal 118) and provided to the electrical output signal generator 120 for incorporation in the electrical output signal.
Fig. 5 is a flow diagram of another example process 500 for generating an electrical output signal. In some implementations, a directional component indicative of a source (e.g., 108a, 108b) of a new sound that was not present in the current auditory environment prior to detection is determined based on the electrical input signal and data descriptive of the current auditory environment (502). The directional component includes information of the location of the source 108a of the new sound relative to the user 104 (e.g., in front of the user 104, behind the user 104, above/below the user 104, etc.). For example, the new sound may be a person speaking with the user 104 and who is behind the user 104. Information about the directional component may be stored in the stored electrical signal 118 and used by the electrical output signal generator 120 in generating the electrical output signal of the new sound.
An electrical output signal is generated from the conditioned electrical signal and the electrical input signal that is not determined to be characteristic of the new sound. An electrical output signal is generated such that the electrical output signal causes the audio output transducer 122 to generate a new sound as originating from a location defined by the directional component (504). The user may perceive the audio signal provided to the user 104 by the audio output device 124 as originating from a defined location of the directional component (e.g., by outputting panning, by synthesizing three-dimensional sound positions by varying binaural time difference (ITD) and binaural level difference (ILD) cues, or by full three-dimensional sound rendering). In one example, surround sound using multiple audio output devices 124 may provide audio signals to the user 104 such that the user perceives the audio signals as originating from the location defined by the directional component.
In some implementations, the augmented reality or virtual reality experience provides new sound as originating from a location defined by the directional component in accordance with the presence of a virtual source in the user's virtual/augmented reality experience. For example, the virtual reality experience shows the user that a person is speaking on the user's left side and presents the user with an audio signal that the person is speaking so that the user perceives that the person is speaking on their left side.
As the user 104 moves relative to the source, the directional component of the source may change. In one example, if the source is behind the user, the audio signal may be presented to the user with a directional component indicating that the source is behind the user. If the user 104 then turns to face the source, the directional component indicative of the source will reflect this relative motion and will indicate that the source is in front of the user.
In some implementations, the enhanced situational awareness system 102 monitors the user's 104 response to the presentation of new sounds and responsively adjusts the presentation of the auditory environment. For example, if the user 104 reacts to the presentation of the new sound (e.g., turns to face the source of the new sound, stops speaking, focuses attention), the enhanced situational awareness system 102 may determine that the new sound is the current sound of interest of the user 104. For example, an electroencephalogram (EEG) signal from the user 104 indicating that the user is concentrating on attention, accelerometer data from a wearable device of the user (e.g., smart glasses, headphones), a change in the user's voice (e.g., the user stops speaking) may be used to determine the user's 104 reaction to a new sound.
In another example, if the user 104 does not react to the presentation of the new sound (e.g., continues to speak, does not indicate concentration), the enhanced situational awareness system 102 may determine that the new sound is not the current sound of interest of the user 104. The enhanced situational awareness system 102 may attenuate the new sound to a background level (e.g., reduce the amplitude of the new sound) or adjust the new sound to a lower priority in the user's auditory environment.
In some implementations, the enhanced situational awareness system 102 can use the user's 104 reaction to the presentation of the new sound to refine one or more machine learning models that the new sound detector 114 uses to detect and identify new sounds that may be of interest to the user 104. For example, if the user 104 ignores the presentation of the new sound (e.g., a window is open), the enhanced situational awareness system 102 may de-emphasize the same classified sound for future new sounds detected. In another example, if the user 104 reacts and focuses on the presentation of a new sound (e.g., spouse speaking), the enhanced situational awareness system 102 may emphasize the importance of the same classification of sound (e.g., the user's spouse should always be presented as a new sound of interest). The model may be trained using the responses to sound presentations during a training period to distinguish between new sounds of interest to the user 104 and new sounds not of interest to the user 104, such that when an electrical output signal generated by the conditioned electrical signal is provided to the audio output transducer 122, only the electrical input signal that is characteristic of the new sound of interest is conditioned by the electrical signal conditioner 116 to highlight the new sound of interest.
In some implementations, the system can process speech input from the user to process the new sound according to the user's verbal instructions. For example, the user may not be interested in the new sound, and may say "I don't wait to listen to that new sound", and the system may suppress the electrical signal that is characteristic of the new sound. One example use case is a sound headphone that lets the user know the physical environment sound. The user may be listening to a ring tune and when the user's phone rings, the system may present the user with a new sound of the phone ring. The user may say "I don't want to listen to that new sound" and the headset will no longer present the user with a telephone ring tone, e.g. may isolate the user from the sound of the physical environment. Instead, assume that the user is listening to a melody and that there is noise in the physical environment, e.g., a washing machine imbalance or a phone ring. If the user is interested in the new sound, the user may say: "I wait to listen more closely". The system may then continue to present new sounds to the user while suppressing the current sound of interest, e.g., the electrical signal used to generate the symphony music is paused or suppressed.
Embodiments of the subject matter and the operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions, encoded on computer storage media for execution by, or to control the operation of, data processing apparatus.
The computer storage media may be or be included in a computer-readable storage device, a computer-readable storage substrate, a random or serial access memory array or device, or a combination of one or more of them. Further, although the computer storage medium is not a propagated signal, the computer storage medium can be a source or destination of computer program instructions encoded in an artificially generated propagated signal. The computer storage medium may also be or be contained in one or more separate physical components or media (e.g., multiple CDs, disks, or other storage devices).
The operations described in this specification may be implemented as operations performed by data processing apparatus on data stored on one or more computer-readable storage devices or received from other sources.
The term "data processing apparatus" encompasses all types of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, a system on a chip, or a plurality or combination of the foregoing. The apparatus can comprise special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). The apparatus can include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, a cross-platform runtime environment, a virtual machine, or a combination of one or more of them. The apparatus and execution environment may implement a variety of different computing model infrastructures, such as web services, distributed computing, and grid computing infrastructures.
A computer program (also known as a program, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment. A computer program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform actions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read-only memory or a random access memory or both. Important elements of a computer include a processor for performing actions in accordance with instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, a computer need not have such devices. Moreover, a computer may be embedded in another device, e.g., a mobile telephone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game player, a Global Positioning System (GPS) receiver, or a portable storage device (e.g., a Universal Serial Bus (USB) flash drive), to name a few. Devices suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disks; and CD ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube), OLED (organic light emitting diode), or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse and a trackball, by which the user can provide input to the computer. Other types of devices can also be used to provide for interaction with a user; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input. In addition, the computer is able to interact with the user by sending and receiving documents to and from the device used by the user; for example, by sending a web page to a web browser on the user device of the user in response to a request received from the web browser.
Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front-end component (e.g., a user computer having a graphical user interface or a web browser through which a user can interact with an implementation of the subject matter described in this specification), or a combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include local area networks ("LANs") and wide area networks ("WANs"), the internet (e.g., the internet), and peer-to-peer networks (e.g., ad hoc peer-to-peer networks).
The computing system may include a user and a server. For example, a user and a server are generally remote from each other and typically interact through a communication network. The relationship of user and server arises by virtue of computer programs running on the respective computers and having a user-server relationship to each other. In some embodiments, the server transmits data (e.g., HTML pages) to the user device (e.g., for the purpose of displaying data to and receiving user input from a user interacting with the user device). Data generated at the user device (e.g., a result of the user interaction) may be received from the user device at the server.
While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any features or of what may be claimed, but rather as descriptions of features specific to particular embodiments. Certain features that are described in this specification in the context of separate implementations can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous. Moreover, the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
Thus, particular embodiments of the present subject matter have been described. Other embodiments are within the scope of the following claims. In some cases, the actions recited in the claims can be performed in a different order and still achieve desirable results. In addition, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some embodiments, multitasking and parallel processing may be advantageous.
Claims (20)
1. A system, comprising:
an audio output transducer that converts an electrical output signal into acoustic sound; and
a signal processing device that receives as input an electrical input signal and is in data communication with the audio output transducer to provide the electrical output signal to the audio output transducer, and wherein the signal processing device is arranged to execute instructions that cause the signal processing device to perform operations comprising:
detecting, based on the electrical input signal and data describing a current auditory environment, a new sound in the current auditory environment that was not present prior to the detecting;
adjusting the electrical input signal determined to be characteristic of the new sound to generate an adjusted electrical signal;
generating an electrical output signal from the adjusted electrical signal determined to be characteristic of the new sound and an electrical input signal that was not determined to be characteristic of the new sound; and
providing the electrical output signal to the audio output transducer.
2. The system of claim 1, wherein the data descriptive of the current auditory environment comprises data generated from an electrical input signal received prior to detection of the received electrical input signal of the new sound.
3. The system of claim 2, further comprising a plurality of audio input transducers that convert acoustic sound into electrical input signals, and wherein:
the signal processing device is in data communication with the audio input transducer; and is
Detecting the new sound that was not present in the current auditory environment prior to the detecting based on the electrical input signal and the data descriptive of the current auditory environment comprises detecting the new sound from the electrical input signal provided from the audio input transducer.
4. The system of claim 1, wherein:
generating the adjusted electrical signal determined to be characteristic of the new sound comprises storing the electrical input signal determined to be characteristic of the new sound for the duration of the current sound of interest; and is
Generating the electrical output signal from the adjusted electrical signal and the electrical input signal not determined to be characteristic of the new sound includes generating the electrical output signal from the electrical input signal not determined to be characteristic of the new sound and the stored electrical input signal determined to be characteristic of the new sound.
5. The system of claim 3, wherein:
generating the adjusted electrical signal determined to be characteristic of the new sound comprises:
monitoring the electrical input signal to detect the absence of a current sound of interest;
and wherein storing the electrical input signal determined to be characteristic of the new sound occurs during the monitoring.
6. The system of claim 5, wherein monitoring the electrical input signal to detect the absence of a current sound of interest comprises detecting a prosodic signal indicating an end of the current sound of interest.
7. The system of claim 3, wherein generating the electrical output signal from the conditioned electrical signal and the electrical input signal not determined to be characteristic of the new sound comprises inserting the stored electrical input signal determined to be characteristic of the new sound in the generated electrical output signal in the absence of the current sound of interest.
8. The system of claim 3, wherein the new sound is a sound generated by speech.
9. The system of claim 1, wherein:
generating the adjusted electrical input signal determined to be characteristic of the new sound comprises: adjusting the electrical input signal determined to be characteristic of the new sound to cause a frequency shift in the new sound when the adjusted electrical signal is used to cause the audio output transducer to generate an audio output.
10. The system of claim 1, wherein detecting a new sound in the current auditory environment that was not present in the current auditory environment prior to the detection based on the electrical input signal and data descriptive of the current auditory environment comprises determining a directional component indicative of a source of the new sound.
11. The system of claim 10, wherein generating an electrical output signal from the adjusted electrical signal and an electrical input signal not determined to be characteristic of the new sound comprises generating the electrical output signal such that the electrical output signal causes the audio output transducer to generate the new sound originating from a location defined by the directional component.
12. The system of claim 1, wherein adjusting the electrical input signal determined to be characteristic of the new sound to generate an adjusted electrical signal comprises adjusting an amplitude of the electrical input signal determined to be characteristic of the new sound.
13. A method implemented in a data processing apparatus, comprising:
receiving as input an electrical input signal by the signal processing means;
detecting, based on the electrical input signal and data describing a current auditory environment, a new sound in the current auditory environment that was not present prior to the detecting;
adjusting the electrical input signal determined to be characteristic of the new sound to generate an adjusted electrical signal;
generating an electrical output signal from the adjusted electrical signal determined to be characteristic of the new sound and an electrical input signal that was not determined to be characteristic of the new sound; and
the electrical output signal is provided to an audio output transducer.
14. The method of claim 13, wherein the data descriptive of the current auditory environment comprises data generated from an electrical input signal received prior to detecting the received electrical input signal of the new sound.
15. The method of claim 14, further comprising:
converting acoustic sound into the electrical input signal; and
detecting the new sound from the electrical input signal provided from the audio input transducer.
16. The method of claim 13, wherein:
generating the adjusted electrical signal determined to be characteristic of the new sound comprises storing the electrical input signal determined to be characteristic of the new sound for the duration of the current sound of interest; and is
Generating the electrical output signal from the adjusted electrical signal and the electrical input signal not determined to be characteristic of the new sound includes generating the electrical output signal from the electrical input signal not determined to be characteristic of the new sound and the stored electrical input signal determined to be characteristic of the new sound.
17. The method of claim 16, wherein:
generating the adjusted electrical signal determined to be characteristic of the new sound comprises:
monitoring the electrical input signal to detect the absence of a current sound of interest;
and wherein storing the electrical input signal determined to be characteristic of the new sound occurs during the monitoring.
18. The method of claim 17, wherein monitoring the electrical input signal to detect the absence of a current sound of interest comprises detecting a prosodic signal indicating an end of the current sound of interest.
19. The method of claim 15, wherein generating the electrical output signal from the conditioned electrical signal and the electrical input signal not determined to be characteristic of the new sound comprises inserting the stored electrical input signal determined to be characteristic of the new sound in the generated electrical output signal in the absence of the current sound of interest.
20. The method of claim 13, wherein adjusting the electrical input signal determined to be characteristic of the new sound to generate an adjusted electrical signal comprises adjusting an amplitude of the electrical input signal determined to be characteristic of the new sound.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US16/020,721 | 2018-06-27 | ||
US16/020,721 US10536786B1 (en) | 2018-06-27 | 2018-06-27 | Augmented environmental awareness system |
PCT/US2019/038099 WO2020005672A1 (en) | 2018-06-27 | 2019-06-20 | Augmented environmental awareness system |
Publications (2)
Publication Number | Publication Date |
---|---|
CN112352441A true CN112352441A (en) | 2021-02-09 |
CN112352441B CN112352441B (en) | 2022-07-12 |
Family
ID=67138240
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201980043847.9A Active CN112352441B (en) | 2018-06-27 | 2019-06-20 | Enhanced environmental awareness system |
Country Status (4)
Country | Link |
---|---|
US (1) | US10536786B1 (en) |
EP (1) | EP3695618B1 (en) |
CN (1) | CN112352441B (en) |
WO (1) | WO2020005672A1 (en) |
Families Citing this family (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11252497B2 (en) * | 2019-08-09 | 2022-02-15 | Nanjing Zgmicro Company Limited | Headphones providing fully natural interfaces |
DE102019218808B3 (en) * | 2019-12-03 | 2021-03-11 | Sivantos Pte. Ltd. | Method for training a hearing situation classifier for a hearing aid |
US11659331B2 (en) * | 2021-01-22 | 2023-05-23 | Toyota Motor Engineering & Manufacturing North America, Inc. | Systems and methods for audio balance adjustment |
US11689878B2 (en) * | 2021-09-07 | 2023-06-27 | Qualcomm Incorporated | Audio adjustment based on user electrical signals |
CN113782058B (en) * | 2021-09-15 | 2023-07-18 | 深圳市豪恩声学股份有限公司 | Dynamic audio perception tracking system and method, storage medium, and earphone |
WO2024010501A1 (en) * | 2022-07-05 | 2024-01-11 | Telefonaktiebolaget Lm Ericsson (Publ) | Adjusting an audio experience for a user |
Citations (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7903825B1 (en) * | 2006-03-03 | 2011-03-08 | Cirrus Logic, Inc. | Personal audio playback device having gain control responsive to environmental sounds |
CN102984636A (en) * | 2011-08-15 | 2013-03-20 | 奥迪康有限公司 | Control of output modulation in a hearing instrument |
US20150230033A1 (en) * | 2014-01-17 | 2015-08-13 | Okappi, Inc. | Hearing Assistance System |
CN104937954A (en) * | 2013-01-09 | 2015-09-23 | 听优企业 | Method and system for self-managed sound enhancement |
CN105850154A (en) * | 2013-12-20 | 2016-08-10 | 微软技术许可有限责任公司 | Adapting audio based upon detected environmental acoustics |
CN106062746A (en) * | 2014-01-06 | 2016-10-26 | 哈曼国际工业有限公司 | System and method for user controllable auditory environment customization |
US20170188168A1 (en) * | 2015-12-27 | 2017-06-29 | Philip Scott Lyren | Switching Binaural Sound |
Family Cites Families (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
EP2605541A1 (en) * | 2011-12-14 | 2013-06-19 | Oticon A/S | Voice recorder for use with a hearing device |
US9332359B2 (en) * | 2013-01-11 | 2016-05-03 | Starkey Laboratories, Inc. | Customization of adaptive directionality for hearing aids using a portable device |
US9270244B2 (en) | 2013-03-13 | 2016-02-23 | Personics Holdings, Llc | System and method to detect close voice sources and automatically enhance situation awareness |
US10750293B2 (en) * | 2016-02-08 | 2020-08-18 | Hearing Instrument Manufacture Patent Partnership | Hearing augmentation systems and methods |
US9922655B2 (en) | 2016-05-31 | 2018-03-20 | International Business Machines Corporation | System, method, and recording medium for controlling dialogue interruptions by a speech output device |
US10409548B2 (en) * | 2016-09-27 | 2019-09-10 | Grabango Co. | System and method for differentially locating and modifying audio sources |
US9886954B1 (en) * | 2016-09-30 | 2018-02-06 | Doppler Labs, Inc. | Context aware hearing optimization engine |
-
2018
- 2018-06-27 US US16/020,721 patent/US10536786B1/en active Active
-
2019
- 2019-06-20 WO PCT/US2019/038099 patent/WO2020005672A1/en unknown
- 2019-06-20 EP EP19735181.0A patent/EP3695618B1/en active Active
- 2019-06-20 CN CN201980043847.9A patent/CN112352441B/en active Active
Patent Citations (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7903825B1 (en) * | 2006-03-03 | 2011-03-08 | Cirrus Logic, Inc. | Personal audio playback device having gain control responsive to environmental sounds |
CN102984636A (en) * | 2011-08-15 | 2013-03-20 | 奥迪康有限公司 | Control of output modulation in a hearing instrument |
CN104937954A (en) * | 2013-01-09 | 2015-09-23 | 听优企业 | Method and system for self-managed sound enhancement |
CN105850154A (en) * | 2013-12-20 | 2016-08-10 | 微软技术许可有限责任公司 | Adapting audio based upon detected environmental acoustics |
CN106062746A (en) * | 2014-01-06 | 2016-10-26 | 哈曼国际工业有限公司 | System and method for user controllable auditory environment customization |
US20150230033A1 (en) * | 2014-01-17 | 2015-08-13 | Okappi, Inc. | Hearing Assistance System |
US20170188168A1 (en) * | 2015-12-27 | 2017-06-29 | Philip Scott Lyren | Switching Binaural Sound |
Also Published As
Publication number | Publication date |
---|---|
EP3695618B1 (en) | 2023-09-06 |
EP3695618A1 (en) | 2020-08-19 |
CN112352441B (en) | 2022-07-12 |
US20200007993A1 (en) | 2020-01-02 |
US10536786B1 (en) | 2020-01-14 |
WO2020005672A1 (en) | 2020-01-02 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN112352441B (en) | Enhanced environmental awareness system | |
US20220159403A1 (en) | System and method for assisting selective hearing | |
US10817251B2 (en) | Dynamic capability demonstration in wearable audio device | |
US9293133B2 (en) | Improving voice communication over a network | |
US20230045237A1 (en) | Wearable apparatus for active substitution | |
KR20170071585A (en) | Systems, methods, and devices for intelligent speech recognition and processing | |
TW201820315A (en) | Improved audio headset device | |
Zhang et al. | Sensing to hear: Speech enhancement for mobile devices using acoustic signals | |
US20230164509A1 (en) | System and method for headphone equalization and room adjustment for binaural playback in augmented reality | |
Bramsløw et al. | Improving competing voices segregation for hearing impaired listeners using a low-latency deep neural network algorithm | |
CN116324969A (en) | Hearing enhancement and wearable system with positioning feedback | |
JPWO2018030149A1 (en) | INFORMATION PROCESSING APPARATUS AND INFORMATION PROCESSING METHOD | |
Fukumori et al. | Optical laser microphone for human-robot interaction: speech recognition in extremely noisy service environments | |
Omologo | A prototype of distant-talking interface for control of interactive TV | |
US20230267942A1 (en) | Audio-visual hearing aid | |
US11935557B2 (en) | Techniques for detecting and processing domain-specific terminology | |
US20230229383A1 (en) | Hearing augmentation and wearable system with localized feedback | |
US20230171543A1 (en) | Method and device for processing audio signal by using artificial intelligence model | |
WO2024058147A1 (en) | Processing device, output device, and processing system | |
Shankar | Real-Time Single and Dual-Channel Speech Enhancement on Edge Devices for Hearing Applications | |
CN115250646A (en) | Auxiliary listening method and device | |
Cullen et al. | Vocate: Auditory Interfaces for Location-based Services |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |