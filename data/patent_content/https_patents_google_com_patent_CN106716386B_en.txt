CN106716386B - Hardware assisted memory compression management using page filters and system MMU - Google Patents
Hardware assisted memory compression management using page filters and system MMU Download PDFInfo
- Publication number
- CN106716386B CN106716386B CN201580050274.4A CN201580050274A CN106716386B CN 106716386 B CN106716386 B CN 106716386B CN 201580050274 A CN201580050274 A CN 201580050274A CN 106716386 B CN106716386 B CN 106716386B
- Authority
- CN
- China
- Prior art keywords
- memory
- page
- compressed
- inactive
- pages
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
- G06F12/0802—Addressing of a memory level in which the access to the desired data or data block requires associative addressing means, e.g. caches
- G06F12/0891—Addressing of a memory level in which the access to the desired data or data block requires associative addressing means, e.g. caches using clearing, invalidating or resetting means
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
- G06F12/0802—Addressing of a memory level in which the access to the desired data or data block requires associative addressing means, e.g. caches
- G06F12/0893—Caches characterised by their organisation or structure
- G06F12/0897—Caches characterised by their organisation or structure with two or more cache hierarchy levels
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
- G06F12/10—Address translation
- G06F12/1009—Address translation using page tables, e.g. page table structures
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
- G06F12/10—Address translation
- G06F12/1027—Address translation using associative or pseudo-associative address translation means, e.g. translation look-aside buffer [TLB]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
- G06F12/12—Replacement control
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/06—Digital input from, or digital output to, record carriers, e.g. RAID, emulated record carriers or networked record carriers
- G06F3/0601—Interfaces specially adapted for storage systems
- G06F3/0602—Interfaces specially adapted for storage systems specifically adapted to achieve a particular effect
- G06F3/0604—Improving or facilitating administration, e.g. storage management
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/06—Digital input from, or digital output to, record carriers, e.g. RAID, emulated record carriers or networked record carriers
- G06F3/0601—Interfaces specially adapted for storage systems
- G06F3/0602—Interfaces specially adapted for storage systems specifically adapted to achieve a particular effect
- G06F3/061—Improving I/O performance
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/06—Digital input from, or digital output to, record carriers, e.g. RAID, emulated record carriers or networked record carriers
- G06F3/0601—Interfaces specially adapted for storage systems
- G06F3/0628—Interfaces specially adapted for storage systems making use of a particular technique
- G06F3/0638—Organizing or formatting or addressing of data
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/06—Digital input from, or digital output to, record carriers, e.g. RAID, emulated record carriers or networked record carriers
- G06F3/0601—Interfaces specially adapted for storage systems
- G06F3/0628—Interfaces specially adapted for storage systems making use of a particular technique
- G06F3/0653—Monitoring storage devices or systems
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/06—Digital input from, or digital output to, record carriers, e.g. RAID, emulated record carriers or networked record carriers
- G06F3/0601—Interfaces specially adapted for storage systems
- G06F3/0628—Interfaces specially adapted for storage systems making use of a particular technique
- G06F3/0655—Vertical data movement, i.e. input-output transfer; data movement between one or more hosts and one or more storage devices
- G06F3/0656—Data buffering arrangements
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/06—Digital input from, or digital output to, record carriers, e.g. RAID, emulated record carriers or networked record carriers
- G06F3/0601—Interfaces specially adapted for storage systems
- G06F3/0668—Interfaces specially adapted for storage systems adopting a particular infrastructure
- G06F3/0671—In-line storage system
- G06F3/0673—Single storage device
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/06—Digital input from, or digital output to, record carriers, e.g. RAID, emulated record carriers or networked record carriers
- G06F3/0601—Interfaces specially adapted for storage systems
- G06F3/0668—Interfaces specially adapted for storage systems adopting a particular infrastructure
- G06F3/0671—In-line storage system
- G06F3/0683—Plurality of storage devices
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
- G06F12/0802—Addressing of a memory level in which the access to the desired data or data block requires associative addressing means, e.g. caches
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
- G06F12/0802—Addressing of a memory level in which the access to the desired data or data block requires associative addressing means, e.g. caches
- G06F12/0864—Addressing of a memory level in which the access to the desired data or data block requires associative addressing means, e.g. caches using pseudo-associative means, e.g. set-associative or hashing
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
- G06F12/12—Replacement control
- G06F12/121—Replacement control using replacement algorithms
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2212/00—Indexing scheme relating to accessing, addressing or allocation within memory systems or architectures
- G06F2212/10—Providing a specific technical effect
- G06F2212/1016—Performance improvement
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2212/00—Indexing scheme relating to accessing, addressing or allocation within memory systems or architectures
- G06F2212/10—Providing a specific technical effect
- G06F2212/1041—Resource optimization
- G06F2212/1044—Space efficiency improvement
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2212/00—Indexing scheme relating to accessing, addressing or allocation within memory systems or architectures
- G06F2212/40—Specific encoding of data in memory or cache
- G06F2212/401—Compressed data
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2212/00—Indexing scheme relating to accessing, addressing or allocation within memory systems or architectures
- G06F2212/50—Control mechanisms for virtual memory, cache or TLB
- G06F2212/502—Control mechanisms for virtual memory, cache or TLB using adaptive policy
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2212/00—Indexing scheme relating to accessing, addressing or allocation within memory systems or architectures
- G06F2212/60—Details of cache memory
- G06F2212/601—Reconfiguration of cache memory
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2212/00—Indexing scheme relating to accessing, addressing or allocation within memory systems or architectures
- G06F2212/60—Details of cache memory
- G06F2212/604—Details relating to cache allocation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2212/00—Indexing scheme relating to accessing, addressing or allocation within memory systems or architectures
- G06F2212/65—Details of virtual memory and virtual address translation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2212/00—Indexing scheme relating to accessing, addressing or allocation within memory systems or architectures
- G06F2212/68—Details of translation look-aside buffer [TLB]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2212/00—Indexing scheme relating to accessing, addressing or allocation within memory systems or architectures
- G06F2212/68—Details of translation look-aside buffer [TLB]
- G06F2212/683—Invalidation
-
- Y—GENERAL TAGGING OF NEW TECHNOLOGICAL DEVELOPMENTS; GENERAL TAGGING OF CROSS-SECTIONAL TECHNOLOGIES SPANNING OVER SEVERAL SECTIONS OF THE IPC; TECHNICAL SUBJECTS COVERED BY FORMER USPC CROSS-REFERENCE ART COLLECTIONS [XRACs] AND DIGESTS
- Y02—TECHNOLOGIES OR APPLICATIONS FOR MITIGATION OR ADAPTATION AGAINST CLIMATE CHANGE
- Y02D—CLIMATE CHANGE MITIGATION TECHNOLOGIES IN INFORMATION AND COMMUNICATION TECHNOLOGIES [ICT], I.E. INFORMATION AND COMMUNICATION TECHNOLOGIES AIMING AT THE REDUCTION OF THEIR OWN ENERGY USE
- Y02D10/00—Energy efficient computing, e.g. low power processors, power management or thermal management
Abstract
Methods and systems are provided for managing memory using hardware-based page filters designed to distinguish active pages from inactive pages (hot and cold pages, respectively) so that inactive pages can be compressed before a page fault occurs. The method and system are designed to: lower cost, longer battery life, and faster user response are achieved. While existing methods for memory management are based on pixel or frame buffer compression, the methods and systems provided are focused on the programs (e.g., general data structures) of the CPU. Memory compression that focuses on hardware acceleration for offloading the CPU translates higher power efficiency (e.g., ASIC power is about 100 times lower than CPU) and higher performance (e.g., ASIC is about 10 times faster than CPU) and also allows hardware-assisted memory management to offload the OS/kernel, which significantly increases response time.
Description
Cross Reference to Related Applications
This application claims priority to U.S. provisional patent application serial No. 62/060,949, filed on 7/10/2014, the disclosure of which is incorporated herein by reference in its entirety.
Background
Existing approaches for memory compression typically focus on pixel/reference frame memory compression, which is often applied to GPU (graphics processor unit), ISP (image signal processor), video, and/or display streams. While such techniques can achieve some degree of bandwidth reduction, none address or can reduce memory footprint.
Disclosure of Invention
This summary introduces a selection of concepts in a simplified form to provide a basic understanding of some aspects of the disclosure. This summary is not an extensive overview of the disclosure and is not intended to identify key or critical elements of the disclosure or to delineate the scope of the disclosure. This summary merely presents some of the concepts of the disclosure as a prelude to the detailed description provided below.
The present disclosure relates generally to methods and systems for managing memory. More particularly, aspects of the present disclosure relate to hardware-based page filters designed to distinguish active pages from inactive pages and compress inactive pages before a page fault occurs.
One embodiment of the present disclosure is directed to a method for memory compression management, comprising: determining that a page in the virtual memory space is inactive using a page filter; removing working memory address information of an inactive page from a page table of a corresponding operating system; determining a location in a working memory to allocate compression information for the inactive page; allocating a compressed memory address of the inactive page; updating a translation look-aside buffer with the compressed memory address of the inactive page; and writing the compressed memory address to the compressed memory.
In another embodiment, the method for memory compression management further comprises: the corresponding page table entry for the inactive page is removed from the page table of the corresponding operating system.
In another embodiment, the method for memory compression management further comprises: an interrupt is raised to the memory management unit of the operating system to allocate an address in the compressed memory of the inactive page.
Another embodiment of the disclosure is directed to a system for memory compression management, the system comprising at least one processor and a non-transitory computer-readable medium coupled to the at least one processor, the non-transitory computer-readable medium storing instructions that, when executed by the at least one processor, cause the at least one processor to: determining, using a hardware page filter, that a page in the virtual memory space is inactive; removing working memory address information of the inactive page from the corresponding page table; determining a location in a working memory to allocate compression information for the inactive page; allocating a compressed memory address of the inactive page; updating an associated cache with the compressed memory address of the inactive page; and writing the compressed memory address of the inactive page to the compressed memory.
In another embodiment, the at least one processor in the system for memory compression management is further caused to: the corresponding page table entry for the inactive page is removed from the page table.
In yet another embodiment, the at least one processor in the system for memory compression management is further caused to: the associative cache is updated with the compressed memory address of the inactive page using a memory management unit.
In yet another embodiment, the at least one processor in the system for memory compression management is further caused to: an interrupt is raised to the memory management unit to allocate an address in the compressed memory of the inactive page.
Yet another embodiment of the present disclosure is directed to a method for memory compression management, the method comprising: detecting an inactive page in a virtual memory space using a hardware page filter; compressing the inactive page prior to the page fault; and providing information about the compressed inactive pages to a kernel of the corresponding operating system.
In another embodiment, the method for memory compression management further comprises: in a page table of the operating system, a starting physical address of the compressed memory of each of the pages in the virtual memory space is recorded.
In yet another embodiment, the method for memory compression management further comprises: in the page table, compression information for each of the pages in the virtual memory space is recorded.
In yet another embodiment, the method for memory compression management further comprises: an interrupt is raised to update a page table entry in the kernel with information about the compressed inactive page.
In one or more other embodiments, the methods and systems described herein may optionally include one or more of the following additional features: updating, by a memory management unit of an operating system, a translation lookaside buffer (e.g., an associative cache) with a compressed memory address of an inactive page; updating the translation look-aside buffer (or associated cache) with the compressed memory address of the inactive page to maintain translation information between virtual memory, compressed memory, and working memory; the page filter determines that a page in the virtual memory space is inactive based on the reference count of the page being below a threshold count; the page filter determines that a page in the virtual memory space is inactive based on a reference count of the page being below a threshold count during a predetermined period of time; and/or cause an interrupt in response to a capacity eviction or reference count saturation.
Further scope of applicability of the present disclosure will become apparent from the detailed description given hereinafter. It should be understood, however, that the detailed description and the specific examples, while indicating preferred embodiments, are given by way of illustration only, since various changes and modifications within the spirit and scope of the disclosure will become apparent to those skilled in the art from this detailed description.
Drawings
These and other objects, features, and characteristics of the present disclosure will become more fully apparent to those skilled in the art from a study of the following detailed description when taken in conjunction with the appended claims and accompanying drawings, all of which form a part of this specification. In the drawings:
FIG. 1 is a block diagram illustrating an example system for hardware assisted memory compression management using a page filter and a system memory management unit in accordance with one or more embodiments described herein.
FIG. 2 is a block diagram illustrating an example main memory page table to which compression information has been added according to one or more embodiments described herein.
FIG. 3 is a flow diagram illustrating an example method for hardware-based page profiling using page filters according to one or more embodiments described herein.
FIG. 4 is a block diagram illustrating an example operation of a system memory management unit in accordance with one or more embodiments described herein.
FIG. 5 is a flow diagram illustrating an example method of compressing memory using a page filter and a system memory management unit in accordance with one or more embodiments described herein.
FIG. 6 is a flow diagram illustrating an example method for decompressing memory using a system memory management unit in accordance with one or more embodiments described herein.
FIG. 7 is a block diagram illustrating an example computing device configured for hardware assisted memory compression management using a page filter and a system memory management unit in accordance with one or more embodiments described herein.
Headings are provided herein for convenience only and do not necessarily affect the scope or meaning of what is claimed in this disclosure.
In the drawings, for convenience in understanding and for convenience, the same reference numerals and any acronyms identify elements or acts having the same or similar structures or functions. The drawings will be described in detail in the following detailed description.
Detailed Description
Various examples and embodiments will now be described. The following description provides specific details for a thorough understanding and enabling description of these examples. It will be apparent, however, to one skilled in the relevant art that one or more embodiments described herein may be practiced without many of these details. Likewise, one skilled in the relevant art will also appreciate that one or more embodiments of the disclosure may include many other obvious features not described in detail herein. Additionally, some well known structures or functions may not be shown or described in detail below to avoid unnecessarily obscuring the relevant description.
To implement virtual memory, a computer system needs to have special memory management hardware, commonly referred to as an MMU (memory management unit). Without an MMU, when a CPU (computer processing unit) accesses a RAM (random access memory), the actual location within the RAM does not change (e.g., a particular memory address is always the same physical location within the RAM). However, with MMUs, memory addresses are processed by a translation step before each memory access. As such, it is possible to have a given memory address point to a first physical address once and point to a second physical address once. Because the resources required to separately track virtual-to-physical translations for gigabytes of memory can be excessive, the MMU divides the RAM into pages that are contiguous sections of memory of a set size that the MMU handles as a single entity. Thus, physical memory can be viewed as an array of fixed-size slots called page frames, each of which can contain a single virtual memory page.
To record where each virtual page of the address space is placed in physical memory, the operating system maintains a per-process data structure called a Page Table (PT). The primary role of the page table is to store the address translation of each of the virtual pages of the address space, informing each page of the location that resides in physical memory. In order to translate a virtual address generated by a particular process, it is necessary to first split it into two components: virtual Page Number (VPN) and offset within the page.
It is then possible with the VPN to index the page table and determine in which physical frame the virtual page resides. For example, by using a page table, it is possible to determine the corresponding Physical Frame Number (PFN) (sometimes also referred to as physical page number or PPN) of a virtual page. The virtual address may then be translated by replacing the VPN with a PFN (or PPN). It should be understood that the offset is not converted (it remains unchanged) because the offset only indicates the desired byte within the page.
A page fault is a sequence of events that occur when a program attempts to access (e.g., request) data (or code) that is in its address space but not currently located in the real memory (e.g., RAM) of the operating system. The operating system must handle the page fault by causing the accessed data to be resident in memory in some way, allowing the program to continue to operate as if the page fault had never occurred. For example, if the CPU presents the desired address to the MMU, and the MMU does not have a translation for that address, the MMU interrupts the CPU and causes software to be executed (commonly referred to as a page fault handler). The page fault handler then determines the operations that must be completed to resolve the page fault (e.g., fetch data from the virtual memory space and load the data into RAM).
When a process (e.g., a process associated with a program) requests access to data in its memory, it is the responsibility of the operating system to map the virtual address provided by the process to the physical address of the actual memory where the data is stored. A page table is where the operating system stores its mappings of virtual addresses to physical addresses, where each mapping is also referred to as a "page table entry" (PTE).
A Translation Lookaside Buffer (TLB) is an associative cache of Page Table Entries (PTEs), where each block is a single PTE. If an entry (corresponding to a virtual page) is not in the TLB, a TLB "miss" (miss) occurs. If the entry is also not in the real memory of the operating system (e.g., it has been "swapped out"), a page fault also occurs. The TLB and page table together form a translation unit that maps from virtual addresses to physical addresses.
Embodiments of the present disclosure relate to methods and systems for managing memory by using hardware-based page filters designed to distinguish active pages from inactive pages (sometimes referred to herein as "hot" pages and "cold" pages, respectively) such that inactive pages are compressed before a page fault occurs.
As will be described in more detail below, the method and system of the present disclosure are designed to: lower cost, longer battery life, and faster user response, among other things, are achieved. For example, in accordance with at least one embodiment, lower cost is achieved, at least in terms of capacity, since 4GB of DRAM (dynamic random access memory) is made to behave like 8GB of DRAM. In this sense, the methods and systems of the present disclosure trade compressed computing energy (e.g., a few pJ/Op) for DRAM cost (e.g., a few dollars per GB). In terms of extending battery life, fewer DRAM accesses (GB/s) means less power consumption (mW), and thus, the methods and systems described herein may trade off on-chip compression computational energy (e.g., a few pJ/Op) for off-chip memory reference energy (e.g., a few hundred pJ/B). In addition, faster user response is achieved by utilizing a hardware assisted MMU (e.g., μ s) instead of an OS/kernel (e.g., ms).
While existing methods for memory management are based on pixel or frame buffer compression, the methods and systems of the present disclosure place an emphasis on the programs (e.g., general data structures) of the CPU. For example, in accordance with one or more embodiments, the methods and systems described herein utilize a ZRAM (which, as described further below, provides a form of virtual memory compression) that targets a general heap of programs. Placing emphasis on hardware accelerated memory compression to offload the CPU (offload) translates into higher power efficiency (e.g., ASIC power is about 100 times lower than CPU) and higher performance (e.g., ASIC is about 10 times faster than CPU), and also allows hardware assisted memory management to offload the OS/cores, which significantly increases response time. Additional details regarding the ZRAM and its utilization in accordance with the disclosed method and system are provided below.
The ZRAM provides a form of virtual memory compression. The ZRAM compresses the actual block of RAM to make more RAM available to the operating system. The kernel (e.g., a computer program for managing input/output requests from a software application by translating such requests into data processing instructions for the CPU) dynamically compresses the program's memory without the program's knowledge of it ("transparent" memory compression). This compression is achieved by the virtual address space of the program and the request paging. The kernel may unmap pages from the program's page tables and compress those pages. When a compressed page is accessed (e.g., requested by a program), the page fault handler reads the PTE to locate the page from the compression pool space, decompresses the page, and links the page back to the page table of the program.
FIG. 1 is an example system 100 for hardware assisted memory compression management. According to one or more embodiments described herein, system 100 may include a page filter 110, a system memory management unit 130 (system MMU), a Last Level Cache (LLC)120, a Fabric (Fabric)140, a primary uncompressed memory controller (wide I/O2 or WIO2 Ct1)160, a primary uncompressed memory with a wide I/O2 interface standard (WIO2 RAM)180, a low power double data rate memory controller (e.g., LPDDR3 Ctl)150, and a backup compressed memory with LPDDR3 interface standard 170 (ZRAM).
In accordance with at least one embodiment, page filter 110 may be configured to detect inactive (or "cold") pages, while system MMU 130 maintains a victim TLB (translation lookaside buffer). For example, LLC 120 may be an 8MB on-chip Static Random Access Memory (SRAM), and fabric 140 is an on-chip interconnect that moves commands/packets between various agents (e.g., components, elements, etc.) of the system, including: such as CPU, GPU, on-chip SRAM cache, off-chip DRAM, etc. Further, the LPDDR3 Ct 1150 is a memory controller that interfaces with the JEDEC LPDDR3 zmam 170 (backup compressed memory). Although LPDDR3 zr am170 is provided for capacity reasons, a main working memory WIO 2RAM 180 is included in the system 100 to provide working bandwidth. The WIO 2RAM 180 is a dedicated (e.g., high bandwidth, smaller capacity) memory provided in addition to the on-chip MMU. It should be appreciated that while WIO2 is an industry standard, the use of WIO2 and an on-chip MMU with LPDDR3 is a new method to achieve improved speed and improved memory management.
In subsequent sections, "capacity evictions" and "reference count saturations" are used in the context (context) that describes various features of a page filter (e.g., page filter 110 in the example system 100 shown in fig. 1), in accordance with one or more embodiments of the present disclosure. For example, a page filter may have 1024 entries that keep track of the activity of 4KB pages of the system level (not the CPU). In this context, the page filter is an on-chip cache because the main page table resides in DRAM. When a new page must be monitored, the cache controller will evict the old entry (an action called "capacity eviction"). Similarly, when the reference count of an entry reaches its maximum value (i.e., "reference count saturates"), the cache controller will raise an interrupt to notify the MMU of the kernel to update the main page table.
According to one or more embodiments described herein, hardware-based page profiling may be used when an interrupt is raised to update the PTEs of the kernel during capacity eviction or reference count saturation. For example, one history table may track 4KB pages, while another history table tracks larger (e.g., 4MB) pages. Each page may have a Least Recently Used (LRU) history or reference count to see how often the page is referenced over a period of time, which may be adjustable. A threshold may be defined and when a capacity eviction is required, an interrupt is raised to evict/compress the unused pages. As described above, capacity eviction is when a new page must be monitored and an old entry must be evicted from the history table.
Various features and operations of a page filter within an example process of hardware-based page profiling in accordance with one or more embodiments of the present disclosure are described below. In the following description, reference is sometimes made to corresponding features and/or operations illustrated in fig. 3.
FIG. 3 illustrates an example process 300 for hardware-based page profiling using a page filter. According to one or more embodiments described herein, the example process 300 of page profiling may utilize a page filter (e.g., the page filter 110 in the example system 100 shown in fig. 1) designed to distinguish active ("hot") pages from inactive ("cold") pages such that inactive pages are compressed before a page fault occurs. Further details regarding one or more of blocks 305-320 in the example process 300 of hardware-based page profiling are provided in the following sections.
The PTE provides a history of page references so that the kernel can decide which page to compress. Active or "hot" pages should not be compressed. The page filter is on-chip and the page tables are in DRAM. Since the on-chip table has limited entries, capacity evictions occur in the page filter. Whenever there is a capacity eviction, the page table entry in the DRAM is updated with the latest reference count.
As described above, when reference count saturation occurs, the maximum reference count has been reached (e.g., 255 or greater for an 8-bit counter), meaning that the history can no longer be tracked. In such a case, the entry may be evicted, or the counter may be reset and the associated reference count value added to the page table entry in the DRAM.
According to at least one embodiment, the DRAM page table may be reset based on a predetermined manner (e.g., every hour) or dynamically (e.g., according to thresholds being met) in order to prevent count saturation in the DRAM.
According to at least one embodiment, the page filter is designed to provide additional information to the kernel. While the kernel already has some information, the page filter may add further information by, for example, reference counting and/or LRU vectors.
Further, in at least one embodiment, the operating system may provide hints to the kernel that enable the kernel to override (override) the hardware-based page filter. Such an arrangement may allow a page to remain "hot" regardless of the page being unused/referenced. Such hints may also be used to overwrite compression of a page, which may take too long to decompress. The page size is determined by the OS and then designed into hardware. Another fixed parameter is the size of the on-chip history table. For example, a microcontroller implementation may allow these features to be configurable within resource constraints. The microcontroller may use ROM or external RAM to save its state.
FIG. 4 illustrates example operations (400) of a system memory management unit (e.g., system MMU 130 in example system 100 shown in FIG. 1 and described in detail above). In accordance with one or more embodiments of the present disclosure, a system MMU may be configured to perform hardware assisted (e.g., hardware accelerated) memory compression management by caching page table entries.
For example, according to at least one embodiment, the hardware-assisted ZRAM may be responsible for caching Virtual Page Numbers (VPNs), Physical Page Numbers (PPNs), and compressed page numbers (ZPN). It should be understood that in the context of this disclosure, the main working memory (physical memory) may be uncompressed. For example, the compressed memory space (e.g., 3GB of LPDDR3) may become 6GB to 9GB of compressed ZRAM.
In existing approaches, the main working memory may attempt to map virtual addresses to physical addresses. However, more information is needed for compression. As such, adding compression information means that physical pages from the LPDDR3 space can be mapped to multiple compressed pages with a granularity of, for example, 4 KB. Thus, a 4KB physical page can be mapped to two compressed pages.
In accordance with one or more embodiments of the present disclosure, the compression information described above may be added to the page table. Fig. 2 illustrates an example of such a main memory page table to which compression information has been added. In addition, the start address of each page may also be recorded in a page table where the start address is the physical address of the compressed memory. It should be noted that the start address is recorded in the page table in addition to mapping virtual addresses to physical addresses. In this manner, virtual addresses are mapped to physical addresses, and the physical addresses are separated into compressed and uncompressed memory.
As described above, the CPU has the TLB. When an entry is not mapped, the entry is allocated to the system level MMU, which means that a page is sorted out from the working DRAM and removed from the current mapping to the compressed space. The kernel is then triggered to do some compression work and the entry is allocated. When compression is complete, the allocated entry is updated in the main memory table with information indicating the physical location to which the compressed page is allocated. The on-chip data structure (e.g., the system MMU) is then updated with the compressed page information.
For example, when a user attempts to exchange between different tabs that the user has opened in a web browser, the assigned entry may be read. Specifically, the web page of the tag that the user swapped to is compressed and is now being decompressed as a result of the user swapping to the tag. In such a scenario, the compressed entry is read to determine where to fetch the compressed memory space and move it to the uncompressed space.
As described above, a TLB miss results in a page fault. Thus, in accordance with at least one embodiment of the present disclosure, some portion of the DRAM table may be saved in the on-chip MMU so that entries may be obtained (e.g., retrieved, requested, etc.) faster than a lookup at the DRAM.
Furthermore, once the virtual and physical pages corresponding to an entry are remapped to the CPU TLB (e.g., a compressed page is moved from a compressed region to an uncompressed working memory), the entry may be removed from the MMU because it is now allocated in the CPU TLB. In another example, page table entries may also be invalidated/removed by the kernel MMU in the event that, for example, a process is terminated or a tag is removed. It should be understood that entries are not removed from virtual memory until the program is completed or terminated, at which point the program may clear the virtual address space.
According to at least one embodiment, the cache operation engine is configured to unmap physical pages from the CPU TLB and flush any lines from the Last Level Cache (LLC). Flushing a row from the LLC is necessary because hardware-triggered compression requires removing compressed content from the cache to keep the cache coherent with the memory. For example, a cache operation may occur concurrently with an allocation when the DRAM extracts something and moves it to a compressed space. In another example, a cache operation may be triggered by a hardware engine deciding when to compress a page. In yet another example, a cache operation may be triggered by a kernel evicting a page from memory.
FIG. 5 illustrates an example process of compressing memory using a page filter and a system memory management unit (e.g., page filter 110 and system MMU 130 in example system 100 shown in FIG. 1 and described in detail above). Various details regarding one or more of blocks 505 through 530 in the example process 500 of using a page filter and a system MMU for memory compression are provided below.
According to one or more embodiments described herein, a page is considered (e.g., classified, deemed, determined, etc.) to be a cold page when the reference count for a given page is determined to be less than a certain threshold (which may be predetermined or may be dynamically determined/adjusted during operation). The result of this occurrence is: the system MMU flushes the corresponding page table entries from the cache or the kernel MMU handles page faults (standard memory evictions).
If the on-chip page filter detects a cold page, the system MMU will request the physical page number from the cache. For example, 4KB of data may be fetched and compressed, and the compressed memory allocated to the page table entries. The kernel then looks up its memory page tables and decides where to allocate the compressed page. Once the kernel decides which physical page gets the compressed page information, the on-chip MMU updates the victim TLB to maintain the translation between virtual, compressed, and working memory. Thus, the compressed page is evicted from the working memory into the compressed memory. According to one or more embodiments described herein, a Direct Memory Access (DMA) engine may be configured to write compressed pages to a compressed memory (e.g., LPDDR3 ZRAM170 in the example system 100 shown in FIG. 1). Address allocation occurs first and then the content is saved to compressed memory.
FIG. 6 illustrates an example process of decompressing memory using a system memory management unit (e.g., the system MMU 130 in the example system 100 shown in FIG. 1 and described in detail above). Various details regarding one or more of blocks 605-630 in the example process 600 for memory decompression using a system MMU are provided below.
In accordance with one or more embodiments, a context switch can be when a user switches between tabs in a web browser, for example. For example, the web page to which the user may attempt to switch may have been compressed, and thus, the TLB will not have the physical address mapped in working memory, causing a page fault. Such an event may initiate (e.g., start) a prefetch process. For example, it may be assumed that a program has multiple pages that need to be fetched and filled. As such, the example process of memory decompression is designed to not only get (e.g., retrieve, request, fetch, etc.) the currently requested page, but also to get the subsequently contiguous requested page.
Look up virtual and compressed addresses and raise an interrupt to handle page faults by finding the space of physical page numbers allocated in the working memory. While a level 1 memory (the level 1 memory is the main working memory) allocation is in progress, a compressed page may be fetched from a level 2 memory (the level 2 memory is the compressed memory) and decompressed. MMU page table updates and decompressions should be done at approximately the same time (decompression may be faster). The level 1 memory may be allocated and updated in a page table, and then the uncompressed memory may be written to the allocated level 1 memory space.
FIG. 7 is a high-level block diagram of an exemplary computer (700) configured for hardware-assisted memory compression management using a page filter and a system memory management unit. For example, according to one or more embodiments described herein, the computer (700) may be configured to perform hardware-based page profiling to update PTEs of the kernel during capacity eviction or reference count saturation. In a very basic configuration (701), a computing device (700) typically includes one or more processors (710) and a system memory (720). A memory bus (730) may be used for communication between the processor (710) and the system memory (720).
Depending on the desired configuration, the processor (710) may be any type of processor, including but not limited to: a microprocessor (μ Ρ), a microcontroller (μ C), a Digital Signal Processor (DSP), or any combination thereof. The processor (710) may include one more levels of cache, such as a level one cache (711) and a level two cache (712), a processor core (713), and registers (714). The processor Core (713) may include an Arithmetic Logic Unit (ALU), a Floating Point Unit (FPU), a digital signal processing Core (DSP Core), or any combination thereof. The memory controller (716) may also be used with the processor (710), or in some embodiments, the memory controller (715) may be an internal part of the processor (710).
Depending on the desired configuration, system memory (720) may be any type of memory, including but not limited to: volatile memory (such as RAM), non-volatile memory (such as ROM, flash memory, etc.), or any combination thereof. The system memory (720) typically includes an operating system (721), one or more applications (722), and program data (724). The application (722) may include a hardware assisted memory compression management system (723) that uses page filters and a system MMU to efficiently compress and decompress memory.
The program data (724) may include stored instructions that, when executed by one or more processing devices, implement systems and methods for hardware assisted memory compression management. Further, according to at least one embodiment, the program data (724) may include page reference count data (725) that may relate to a page profiling operation in which a page is determined to be inactive when, for example, the reference count of the page is below a certain threshold. In some embodiments, an application (722) may be arranged to operate with program data (724) on an operating system (721).
The computing device (700) may have additional features or functionality, and additional interfaces to facilitate communications between the basic configuration (701) and any required devices and interfaces.
System memory (720) is an example of computer storage media. Computer storage media include, but are not limited to: RAM, DRAM, SARM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, Digital Versatile Disks (DVD) or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can be accessed by computing device 700. Any such computer storage media may be part of device (700).
The computing device (700) may be implemented as part of a small form factor portable (or mobile) electronic device such as a cellular telephone, a smartphone, a Personal Digital Assistant (PDA), a personal media player device, a tablet computer (tablet), a wireless web watch device, a personal headset device, an application specific device, or a hybrid device that include any of the above functions. The computing device (700) may also be implemented as a personal computer including both laptop computer and non-laptop computer configurations.
The foregoing detailed description has set forth various embodiments of the devices and/or processes via the use of block diagrams, flowcharts, and/or examples. Insofar as such block diagrams, flowcharts, and/or examples contain one or more functions and/or operations, it will be understood by those within the art that each function and/or operation within such block diagrams, flowcharts, or examples can be implemented, individually and/or collectively, by a wide range of hardware, software, firmware, or virtually any combination thereof. In one embodiment, portions of the subject matter described herein may be implemented via an Application Specific Integrated Circuit (ASIC), a Field Programmable Gate Array (FPGA), a Digital Signal Processor (DSP), or other integrated format. However, those skilled in the art will recognize that some aspects of the embodiments disclosed herein, in whole or in part, can be equivalently implemented in integrated circuits, as one or more computer programs running on one or more computers, as one or more programs running on one or more processors, as firmware, or as virtually any combination thereof, and that designing the circuitry and/or writing the code for the software and or firmware would be well within the skill of one of skill in the art in light of this disclosure.
In addition, those skilled in the art will appreciate that the mechanisms of the subject matter described herein are capable of being distributed as a program product in a variety of formats, and that an illustrative embodiment of the subject matter described herein applies regardless of the particular type of non-transitory signal bearing medium used to actually carry out the distribution. Examples of non-transitory signal bearing media include, but are not limited to, the following: recordable type media such as floppy disks, hard disk drives, Compact Disks (CDs), Digital Video Disks (DVDs), digital tapes, computer memory, etc.; and a transmission type medium such as a digital and/or an analog communication medium (e.g., a fiber optic cable, a waveguide, a wired communications link, a wireless communication link, etc.).
With respect to nearly any plural and/or singular terms used herein, those having skill in the art can translate from the plural to the singular and/or from the singular to the plural as is appropriate to the context and/or application. For clarity, various single/plural permutation combinations may be explicitly set forth herein.
Thus, particular embodiments of the present subject matter have been described. Other embodiments are within the scope of the following claims. In some cases, the actions recited in the claims can be performed in a different order and still achieve desirable results. In addition, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some embodiments, multitasking and parallel processing may be advantageous.
Claims (10)
1. A method (500) for memory compression management, comprising:
determining that a page in a working memory associated with a virtual memory space is inactive, the working memory (180) being a wide input/output random access memory;
flushing a page table entry from a cache (120), the cache being a static random access memory and the page table entry indicating a physical location of the inactive page in the working memory;
determining (515) a physical location in a compressed memory (170), the compressed memory being a low power double data rate random access memory to write compressed pages, the compressed pages being compressed from the determined inactive pages; and
writing (530) the compressed page to the determined physical location in the compressed memory.
2. The method of claim 1, wherein a translation lookaside buffer is updated with the determined physical location of the compressed page to maintain translation information between the working memory, the compressed memory, and the virtual memory space.
3. The method of claim 1, wherein a page filter determines that the page in the working memory is inactive based on a reference count of the page being below a threshold count.
4. The method of claim 1, wherein a page filter determines that the page in the working memory is inactive based on a reference count of the page being below a threshold count during a predetermined period of time.
5. A system (100) for memory compression management, the system comprising:
at least one processor; and
a non-transitory computer-readable medium coupled to the at least one processor, the non-transitory computer-readable medium storing instructions that, when executed by the at least one processor, cause the at least one processor to:
determining, using a page filter (110), that a page in working memory (180) associated with a virtual memory space is inactive, the working memory being a wide input/output random access memory;
flushing page table entries from a cache (120) using a memory management unit, the cache being a static random access memory;
determining, using the memory management unit, a physical location in a compressed memory (170), the compressed memory being a low power double data rate random access memory; and
writing, using a direct memory access engine, compressed pages to the determined physical location in the compressed memory, the compressed pages compressed from the determined inactive pages.
6. The system of claim 5, wherein the at least one processor is further caused to:
updating, using a memory management unit (130), a translation lookaside buffer with the determined physical location of the compressed page.
7. The system of claim 6, wherein the translation look-aside buffer is updated with the determined physical location of the compressed page to maintain translation information between the working memory, the compressed memory, and the virtual memory space.
8. The system of claim 5, wherein the page filter determines that the page in the working memory is inactive based on a reference count of the page being below a threshold count.
9. The system of claim 5, wherein the page filter determines that the page in the working memory is inactive based on a reference count of the page being below a threshold count during a predetermined period of time.
10. A method for memory compression management, the method comprising:
detecting an inactive page in a working memory (180) associated with a virtual memory space, the working memory (180) being a wide input/output random access memory;
compressing the inactive page prior to a page fault;
recording, in a main memory page table located within the working memory, a start address indicating a physical location of an inactive page compressed in a compressed memory (170), the compressed memory being a low power double data rate random access memory; and
writing the compressed inactive page to the compressed memory.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201462060949P | 2014-10-07 | 2014-10-07 | |
US62/060,949 | 2014-10-07 | ||
PCT/US2015/054496 WO2016057670A1 (en) | 2014-10-07 | 2015-10-07 | Hardware-assisted memory compression management using page filter and system mmu |
Publications (2)
Publication Number | Publication Date |
---|---|
CN106716386A CN106716386A (en) | 2017-05-24 |
CN106716386B true CN106716386B (en) | 2020-05-29 |
Family
ID=54337914
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201580050274.4A Active CN106716386B (en) | 2014-10-07 | 2015-10-07 | Hardware assisted memory compression management using page filters and system MMU |
CN201580043956.2A Active CN106663060B (en) | 2014-10-07 | 2015-10-07 | Method and system for cache lines duplicate removal |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201580043956.2A Active CN106663060B (en) | 2014-10-07 | 2015-10-07 | Method and system for cache lines duplicate removal |
Country Status (4)
Country | Link |
---|---|
US (2) | US9740631B2 (en) |
EP (2) | EP3204860A1 (en) |
CN (2) | CN106716386B (en) |
WO (2) | WO2016057672A1 (en) |
Families Citing this family (28)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9892054B2 (en) | 2014-10-07 | 2018-02-13 | Google Llc | Method and apparatus for monitoring system performance and dynamically updating memory sub-system settings using software to optimize performance and power consumption |
US9740631B2 (en) | 2014-10-07 | 2017-08-22 | Google Inc. | Hardware-assisted memory compression management using page filter and system MMU |
WO2016130915A1 (en) * | 2015-02-13 | 2016-08-18 | Google Inc. | Transparent hardware-assisted memory decompression |
US10496543B2 (en) | 2016-03-31 | 2019-12-03 | Samsung Electronics Co., Ltd. | Virtual bucket multiple hash tables for efficient memory in-line deduplication application |
US10678704B2 (en) | 2016-03-29 | 2020-06-09 | Samsung Electronics Co., Ltd. | Method and apparatus for enabling larger memory capacity than physical memory size |
US9983821B2 (en) | 2016-03-29 | 2018-05-29 | Samsung Electronics Co., Ltd. | Optimized hopscotch multiple hash tables for efficient memory in-line deduplication application |
US10528284B2 (en) | 2016-03-29 | 2020-01-07 | Samsung Electronics Co., Ltd. | Method and apparatus for enabling larger memory capacity than physical memory size |
US9966152B2 (en) | 2016-03-31 | 2018-05-08 | Samsung Electronics Co., Ltd. | Dedupe DRAM system algorithm architecture |
CN109314103B (en) | 2016-06-30 | 2023-08-15 | 英特尔公司 | Method and apparatus for remote field programmable gate array processing |
US20180004668A1 (en) * | 2016-06-30 | 2018-01-04 | Intel Corporation | Searchable hot content cache |
CN107579916B (en) * | 2016-07-04 | 2021-03-23 | 新华三技术有限公司 | Forwarding table entry access method and device |
US9946660B2 (en) * | 2016-07-29 | 2018-04-17 | Hewlett Packard Enterprise Development Lp | Memory space management |
US20180060235A1 (en) * | 2016-08-30 | 2018-03-01 | Intel Corporation | Non-volatile memory compression devices and associated methods and systems |
US10282436B2 (en) * | 2017-01-04 | 2019-05-07 | Samsung Electronics Co., Ltd. | Memory apparatus for in-place regular expression search |
US10061698B2 (en) | 2017-01-31 | 2018-08-28 | Qualcomm Incorporated | Reducing or avoiding buffering of evicted cache data from an uncompressed cache memory in a compression memory system when stalled write operations occur |
US10649889B2 (en) * | 2017-06-04 | 2020-05-12 | Apple Inc. | Method and apparatus for managing kernel memory of data processing systems |
CN109710396B (en) | 2017-10-26 | 2023-08-22 | 华为技术有限公司 | Method and device for information acquisition and memory release |
CN110377534B (en) * | 2018-04-13 | 2023-11-17 | 华为技术有限公司 | Data processing method and device |
CN110659225A (en) * | 2018-06-28 | 2020-01-07 | 华为技术有限公司 | Memory management method and related device |
US10776028B2 (en) * | 2018-07-10 | 2020-09-15 | EMC IP Holding Company LLC | Method for maximum data reduction combining compression with deduplication in storage arrays |
US10628072B2 (en) * | 2018-08-21 | 2020-04-21 | Samsung Electronics Co., Ltd. | Scalable architecture enabling large memory system for in-memory computations |
US11249900B2 (en) * | 2018-10-29 | 2022-02-15 | Vmware, Inc. | Efficiently purging non-active blocks in NVM regions using virtblock arrays |
US11379373B2 (en) | 2019-08-13 | 2022-07-05 | Micron Technology, Inc. | Memory tiering using PCIe connected far memory |
KR20210049602A (en) | 2019-10-25 | 2021-05-06 | 삼성전자주식회사 | A computing device and a method for operating the computing device |
US11797207B2 (en) * | 2020-08-03 | 2023-10-24 | Cornell University | Base and compressed difference data deduplication |
CN117203625A (en) * | 2021-03-23 | 2023-12-08 | 华为技术有限公司 | Method for virtual memory management in computer |
CN113568940A (en) * | 2021-08-04 | 2021-10-29 | 北京百度网讯科技有限公司 | Data query method, device, equipment and storage medium |
CN117651021B (en) * | 2024-01-25 | 2024-04-30 | 苏州萨沙迈半导体有限公司 | Filter, control method and device thereof and electric equipment |
Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JPH08241241A (en) * | 1995-01-23 | 1996-09-17 | Internatl Business Mach Corp <Ibm> | Method and system for improvement of performance of memory in restricted virtual memory environment by reduction of paging activity |
US5699539A (en) * | 1993-12-30 | 1997-12-16 | Connectix Corporation | Virtual memory management system and method using data compression |
CN101770430A (en) * | 2008-12-29 | 2010-07-07 | J·鲁德利克 | Method and apparatus to profile RAM memory objects for displacement with nonvolatile memory |
Family Cites Families (55)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5696927A (en) * | 1995-12-21 | 1997-12-09 | Advanced Micro Devices, Inc. | Memory paging system and method including compressed page mapping hierarchy |
US5978888A (en) | 1997-04-14 | 1999-11-02 | International Business Machines Corporation | Hardware-managed programmable associativity caching mechanism monitoring cache misses to selectively implement multiple associativity levels |
US5991847A (en) * | 1997-06-06 | 1999-11-23 | Acceleration Software International Corporation | Data pattern caching for speeding up write operations |
US6393525B1 (en) * | 1999-05-18 | 2002-05-21 | Intel Corporation | Least recently used replacement method with protection |
US6434669B1 (en) | 1999-09-07 | 2002-08-13 | International Business Machines Corporation | Method of cache management to dynamically update information-type dependent cache policies |
US6532520B1 (en) | 1999-09-10 | 2003-03-11 | International Business Machines Corporation | Method and apparatus for allocating data and instructions within a shared cache |
US7089391B2 (en) * | 2000-04-14 | 2006-08-08 | Quickshift, Inc. | Managing a codec engine for memory compression/decompression operations using a data movement engine |
US6556952B1 (en) | 2000-05-04 | 2003-04-29 | Advanced Micro Devices, Inc. | Performance monitoring and optimizing of controller parameters |
US7047382B2 (en) | 2000-11-29 | 2006-05-16 | Quickshift, Inc. | System and method for managing compression and decompression and decompression of system memory in a computer system |
US6877081B2 (en) | 2001-02-13 | 2005-04-05 | International Business Machines Corporation | System and method for managing memory compression transparent to an operating system |
US6516397B2 (en) * | 2001-04-09 | 2003-02-04 | Hewlett-Packard Company | Virtual memory system utilizing data compression implemented through a device |
US7032158B2 (en) * | 2001-04-23 | 2006-04-18 | Quickshift, Inc. | System and method for recognizing and configuring devices embedded on memory modules |
US6792510B1 (en) | 2002-02-14 | 2004-09-14 | Novell, Inc. | System and method for updating a cache |
US7035979B2 (en) | 2002-05-22 | 2006-04-25 | International Business Machines Corporation | Method and apparatus for optimizing cache hit ratio in non L1 caches |
US6910106B2 (en) | 2002-10-04 | 2005-06-21 | Microsoft Corporation | Methods and mechanisms for proactive memory management |
US7134143B2 (en) * | 2003-02-04 | 2006-11-07 | Stellenberg Gerald S | Method and apparatus for data packet pattern matching |
US7383399B2 (en) | 2004-06-30 | 2008-06-03 | Intel Corporation | Method and apparatus for memory compression |
CN101147174B (en) * | 2004-10-15 | 2011-06-08 | 微软公司 | System and method for managing communication and/or storage of image data |
US7246205B2 (en) | 2004-12-22 | 2007-07-17 | Intel Corporation | Software controlled dynamic push cache |
US20060179258A1 (en) | 2005-02-09 | 2006-08-10 | International Business Machines Corporation | Method for detecting address match in a deeply pipelined processor design |
TW200828273A (en) * | 2006-12-28 | 2008-07-01 | Genesys Logic Inc | Hard disk cache device and method |
EP2168060A4 (en) * | 2007-05-10 | 2012-10-03 | Nitrosphere Corp | System and/or method for reducing disk space usage and improving input/output performance of computer systems |
US7895242B2 (en) | 2007-10-31 | 2011-02-22 | Microsoft Corporation | Compressed storage management |
JP5090941B2 (en) * | 2008-01-29 | 2012-12-05 | 株式会社日立製作所 | Storage subsystem and storage system |
EP2283045A1 (en) * | 2008-05-20 | 2011-02-16 | Crystal Clear Partnership | Separation of polysaccharides by charge density gradient |
US8458404B1 (en) | 2008-08-14 | 2013-06-04 | Marvell International Ltd. | Programmable cache access protocol to optimize power consumption and performance |
KR101618634B1 (en) | 2009-01-07 | 2016-05-09 | 삼성전자주식회사 | Non-Volatile memory, page dynamic allocation apparatus and page mapping apparatus therefor, and page dynamic allocation method and page mapping method therefor |
US8332586B2 (en) | 2009-03-30 | 2012-12-11 | Hitachi, Ltd. | Information processing system for measuring the cache effect in a virtual capacity |
US8112585B2 (en) | 2009-04-30 | 2012-02-07 | Netapp, Inc. | Method and apparatus for dynamically switching cache policies |
CN101572552B (en) * | 2009-06-11 | 2012-07-18 | 哈尔滨工业大学 | High-speed lossless data compression system based on content addressable memory |
US8620939B2 (en) * | 2010-01-25 | 2013-12-31 | Sepaton, Inc. | System and method for summarizing data |
US8677071B2 (en) | 2010-03-26 | 2014-03-18 | Virtualmetrix, Inc. | Control of processor cache memory occupancy |
US20110242427A1 (en) * | 2010-04-01 | 2011-10-06 | Timothy Ramsdale | Method and System for Providing 1080P Video With 32-Bit Mobile DDR Memory |
US8886664B2 (en) * | 2010-05-13 | 2014-11-11 | Microsoft Corporation | Decreasing duplicates and loops in an activity record |
US8484405B2 (en) | 2010-07-13 | 2013-07-09 | Vmware, Inc. | Memory compression policies |
US8458145B2 (en) | 2011-01-20 | 2013-06-04 | Infinidat Ltd. | System and method of storage optimization |
KR20130031046A (en) * | 2011-09-20 | 2013-03-28 | 삼성전자주식회사 | Flash memory device and data manage method thererof |
US9311250B2 (en) * | 2011-12-19 | 2016-04-12 | Intel Corporation | Techniques for memory de-duplication in a virtual system |
US8706971B1 (en) | 2012-03-14 | 2014-04-22 | Netapp, Inc. | Caching and deduplication of data blocks in cache memory |
CN104169892A (en) * | 2012-03-28 | 2014-11-26 | 华为技术有限公司 | Concurrently accessed set associative overflow cache |
US8930612B2 (en) | 2012-05-31 | 2015-01-06 | Seagate Technology Llc | Background deduplication of data sets in a memory |
CN103176752A (en) * | 2012-07-02 | 2013-06-26 | 晶天电子（深圳）有限公司 | Super-endurance solid-state drive with Endurance Translation Layer (ETL) and diversion of temp files for reduced Flash wear |
US9043570B2 (en) | 2012-09-11 | 2015-05-26 | Apple Inc. | System cache with quota-based control |
KR20140035082A (en) * | 2012-09-13 | 2014-03-21 | 삼성전자주식회사 | Method for managing memory |
US20140089600A1 (en) | 2012-09-27 | 2014-03-27 | Apple Inc. | System cache with data pending state |
US8938417B2 (en) * | 2013-02-22 | 2015-01-20 | International Business Machines Corporation | Integrity checking and selective deduplication based on network parameters |
US9436604B2 (en) | 2013-03-13 | 2016-09-06 | Futurewei Technologies, Inc. | System and method for software/hardware coordinated adaptive performance monitoring |
US9298637B2 (en) * | 2013-03-13 | 2016-03-29 | International Business Machines Corporation | Dynamic caching module selection for optimized data deduplication |
US20140281155A1 (en) * | 2013-03-14 | 2014-09-18 | Lsi Corporation | Storage device assisted data de-duplication |
GB2512604A (en) | 2013-04-03 | 2014-10-08 | Ibm | Flexibly storing defined presets for configuration of a storage controller |
US9292449B2 (en) * | 2013-12-20 | 2016-03-22 | Intel Corporation | Cache memory data compression and decompression |
US9563251B2 (en) | 2013-12-28 | 2017-02-07 | Intel Corporation | Representing a cache line bit pattern via meta signaling |
US9792063B2 (en) | 2014-01-15 | 2017-10-17 | Intel Corporation | Deduplication-based data security |
US9740631B2 (en) | 2014-10-07 | 2017-08-22 | Google Inc. | Hardware-assisted memory compression management using page filter and system MMU |
US9892054B2 (en) | 2014-10-07 | 2018-02-13 | Google Llc | Method and apparatus for monitoring system performance and dynamically updating memory sub-system settings using software to optimize performance and power consumption |
-
2015
- 2015-10-07 US US14/877,484 patent/US9740631B2/en active Active
- 2015-10-07 EP EP15784523.1A patent/EP3204860A1/en not_active Withdrawn
- 2015-10-07 CN CN201580050274.4A patent/CN106716386B/en active Active
- 2015-10-07 CN CN201580043956.2A patent/CN106663060B/en active Active
- 2015-10-07 US US14/877,523 patent/US9785571B2/en active Active
- 2015-10-07 WO PCT/US2015/054499 patent/WO2016057672A1/en active Application Filing
- 2015-10-07 EP EP15784524.9A patent/EP3204859B1/en active Active
- 2015-10-07 WO PCT/US2015/054496 patent/WO2016057670A1/en active Application Filing
Patent Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5699539A (en) * | 1993-12-30 | 1997-12-16 | Connectix Corporation | Virtual memory management system and method using data compression |
JPH08241241A (en) * | 1995-01-23 | 1996-09-17 | Internatl Business Mach Corp <Ibm> | Method and system for improvement of performance of memory in restricted virtual memory environment by reduction of paging activity |
CN101770430A (en) * | 2008-12-29 | 2010-07-07 | J·鲁德利克 | Method and apparatus to profile RAM memory objects for displacement with nonvolatile memory |
Also Published As
Publication number | Publication date |
---|---|
US20160098356A1 (en) | 2016-04-07 |
CN106663060B (en) | 2019-11-19 |
CN106716386A (en) | 2017-05-24 |
EP3204859A1 (en) | 2017-08-16 |
CN106663060A (en) | 2017-05-10 |
US9785571B2 (en) | 2017-10-10 |
EP3204859B1 (en) | 2020-12-02 |
WO2016057670A1 (en) | 2016-04-14 |
US9740631B2 (en) | 2017-08-22 |
WO2016057672A1 (en) | 2016-04-14 |
US20160098353A1 (en) | 2016-04-07 |
EP3204860A1 (en) | 2017-08-16 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN106716386B (en) | Hardware assisted memory compression management using page filters and system MMU | |
US10203901B2 (en) | Transparent hardware-assisted memory decompression | |
US10474583B2 (en) | System and method for controlling cache flush size | |
KR101713051B1 (en) | Hybrid Memory System and Management Method there-of | |
US10310985B2 (en) | Systems and methods for accessing and managing a computing system memory | |
US10282308B2 (en) | Method and apparatus for reducing TLB shootdown overheads in accelerator-based systems | |
US20140181461A1 (en) | Reporting access and dirty pages | |
JP5583274B2 (en) | Method for managing computer memory, corresponding computer program product, and data storage device therefor | |
TWI526832B (en) | Methods and systems for reducing the amount of time and computing resources that are required to perform a hardware table walk (hwtw) | |
KR20140035082A (en) | Method for managing memory | |
US20100332693A1 (en) | Direct memory access in a computing environment | |
US20160342340A1 (en) | Page replacement algorithms for use with solid-state drives | |
US9875191B2 (en) | Electronic device having scratchpad memory and management method for scratchpad memory | |
US20190324914A1 (en) | Method, Apparatus, and Non-Transitory Readable Medium for Accessing Non-Volatile Memory | |
US8543791B2 (en) | Apparatus and method of reducing page fault rate in virtual memory system | |
US9792228B2 (en) | Enhancing lifetime of non-volatile cache by injecting random replacement policy | |
TW201447584A (en) | Method and apparatus for preventing unauthorized access to contents of a register under certain conditions when performing a hardware table walk (HWTW) | |
JP4915756B2 (en) | Method and system for speeding up address translation | |
US8417903B2 (en) | Preselect list using hidden pages | |
US10565111B2 (en) | Processor | |
US9767043B2 (en) | Enhancing lifetime of non-volatile cache by reducing intra-block write variation | |
US7769979B1 (en) | Caching of page access parameters | |
TW201441817A (en) | Data caching system and method | |
US20240111687A1 (en) | Translating Virtual Memory Addresses to Physical Memory Addresses |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
CB02 | Change of applicant information | ||
CB02 | Change of applicant information |
Address after: American CaliforniaApplicant after: Google limited liability companyAddress before: American CaliforniaApplicant before: Google Inc. |
|
GR01 | Patent grant | ||
GR01 | Patent grant |