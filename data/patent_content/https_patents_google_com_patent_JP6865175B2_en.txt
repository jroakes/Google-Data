JP6865175B2 - Systems and methods for biomechanical visual signals to interact with real and virtual objects - Google Patents
Systems and methods for biomechanical visual signals to interact with real and virtual objects Download PDFInfo
- Publication number
- JP6865175B2 JP6865175B2 JP2017556858A JP2017556858A JP6865175B2 JP 6865175 B2 JP6865175 B2 JP 6865175B2 JP 2017556858 A JP2017556858 A JP 2017556858A JP 2017556858 A JP2017556858 A JP 2017556858A JP 6865175 B2 JP6865175 B2 JP 6865175B2
- Authority
- JP
- Japan
- Prior art keywords
- user
- keyboard
- key
- display
- eyes
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 238000000034 method Methods 0.000 title claims description 270
- 230000000007 visual effect Effects 0.000 title description 292
- 210000001508 eye Anatomy 0.000 claims description 583
- 230000004424 eye movement Effects 0.000 claims description 328
- 230000033001 locomotion Effects 0.000 claims description 148
- 230000003565 oculomotor Effects 0.000 claims description 138
- 230000006870 function Effects 0.000 claims description 124
- 238000001514 detection method Methods 0.000 claims description 49
- 230000004044 response Effects 0.000 claims description 20
- 230000002093 peripheral effect Effects 0.000 claims description 15
- 238000012986 modification Methods 0.000 claims description 5
- 230000004048 modification Effects 0.000 claims description 5
- 238000012790 confirmation Methods 0.000 claims description 4
- 102000014150 Interferons Human genes 0.000 claims 3
- 108010050904 Interferons Proteins 0.000 claims 3
- 229940079322 interferon Drugs 0.000 claims 3
- 230000035939 shock Effects 0.000 claims 1
- 230000008569 process Effects 0.000 description 83
- 230000008859 change Effects 0.000 description 44
- 238000005259 measurement Methods 0.000 description 42
- 238000012546 transfer Methods 0.000 description 39
- 230000000694 effects Effects 0.000 description 38
- 210000003811 finger Anatomy 0.000 description 37
- 210000003128 head Anatomy 0.000 description 30
- 230000001629 suppression Effects 0.000 description 30
- 210000004027 cell Anatomy 0.000 description 29
- 230000009471 action Effects 0.000 description 25
- 238000012545 processing Methods 0.000 description 25
- 230000004913 activation Effects 0.000 description 24
- 238000001994 activation Methods 0.000 description 24
- 230000003190 augmentative effect Effects 0.000 description 24
- 239000011159 matrix material Substances 0.000 description 24
- 230000004397 blinking Effects 0.000 description 23
- 230000001149 cognitive effect Effects 0.000 description 23
- 230000002829 reductive effect Effects 0.000 description 23
- 230000001965 increasing effect Effects 0.000 description 21
- 238000009877 rendering Methods 0.000 description 21
- 230000007246 mechanism Effects 0.000 description 20
- 241000699666 Mus <mouse, genus> Species 0.000 description 18
- 230000004886 head movement Effects 0.000 description 16
- 210000004247 hand Anatomy 0.000 description 15
- 230000006854 communication Effects 0.000 description 14
- 210000001747 pupil Anatomy 0.000 description 14
- 201000004569 Blindness Diseases 0.000 description 13
- 238000013528 artificial neural network Methods 0.000 description 13
- 238000004422 calculation algorithm Methods 0.000 description 13
- 230000019771 cognition Effects 0.000 description 13
- 238000004891 communication Methods 0.000 description 13
- 238000013479 data entry Methods 0.000 description 13
- 230000003993 interaction Effects 0.000 description 13
- 230000015654 memory Effects 0.000 description 13
- 230000003287 optical effect Effects 0.000 description 13
- 238000003825 pressing Methods 0.000 description 13
- 239000003086 colorant Substances 0.000 description 11
- 238000013507 mapping Methods 0.000 description 11
- 230000005043 peripheral vision Effects 0.000 description 11
- 210000001525 retina Anatomy 0.000 description 11
- 230000008901 benefit Effects 0.000 description 10
- 238000003860 storage Methods 0.000 description 10
- 230000001720 vestibular Effects 0.000 description 10
- 230000001133 acceleration Effects 0.000 description 9
- 238000012937 correction Methods 0.000 description 9
- 239000003607 modifier Substances 0.000 description 9
- 230000035945 sensitivity Effects 0.000 description 9
- 230000007704 transition Effects 0.000 description 9
- 241000282412 Homo Species 0.000 description 8
- 238000013459 approach Methods 0.000 description 8
- 210000005252 bulbus oculi Anatomy 0.000 description 8
- 210000000744 eyelid Anatomy 0.000 description 8
- 230000001976 improved effect Effects 0.000 description 8
- 230000001537 neural effect Effects 0.000 description 8
- 230000004461 rapid eye movement Effects 0.000 description 8
- 238000012549 training Methods 0.000 description 8
- 230000005540 biological transmission Effects 0.000 description 7
- 230000009977 dual effect Effects 0.000 description 7
- 230000004438 eyesight Effects 0.000 description 7
- 238000012935 Averaging Methods 0.000 description 6
- 238000010586 diagram Methods 0.000 description 6
- 238000010801 machine learning Methods 0.000 description 6
- 230000009467 reduction Effects 0.000 description 6
- 230000006399 behavior Effects 0.000 description 5
- 230000007423 decrease Effects 0.000 description 5
- 238000009826 distribution Methods 0.000 description 5
- 230000007613 environmental effect Effects 0.000 description 5
- 238000003780 insertion Methods 0.000 description 5
- 230000037431 insertion Effects 0.000 description 5
- 230000002452 interceptive effect Effects 0.000 description 5
- 210000003205 muscle Anatomy 0.000 description 5
- 230000002269 spontaneous effect Effects 0.000 description 5
- 230000003213 activating effect Effects 0.000 description 4
- 238000012152 algorithmic method Methods 0.000 description 4
- 210000004556 brain Anatomy 0.000 description 4
- 238000004364 calculation method Methods 0.000 description 4
- 230000006998 cognitive state Effects 0.000 description 4
- 210000000695 crystalline len Anatomy 0.000 description 4
- 238000006073 displacement reaction Methods 0.000 description 4
- 230000014759 maintenance of location Effects 0.000 description 4
- 210000002589 oculomotor nerve Anatomy 0.000 description 4
- 230000008447 perception Effects 0.000 description 4
- 230000003068 static effect Effects 0.000 description 4
- 210000000707 wrist Anatomy 0.000 description 4
- 208000001692 Esotropia Diseases 0.000 description 3
- 241000699670 Mus sp. Species 0.000 description 3
- 208000004350 Strabismus Diseases 0.000 description 3
- 206010044565 Tremor Diseases 0.000 description 3
- 208000003464 asthenopia Diseases 0.000 description 3
- 230000015572 biosynthetic process Effects 0.000 description 3
- 238000006243 chemical reaction Methods 0.000 description 3
- 239000011248 coating agent Substances 0.000 description 3
- 238000000576 coating method Methods 0.000 description 3
- 238000005520 cutting process Methods 0.000 description 3
- 230000003247 decreasing effect Effects 0.000 description 3
- 238000005516 engineering process Methods 0.000 description 3
- 238000011156 evaluation Methods 0.000 description 3
- 238000011049 filling Methods 0.000 description 3
- 230000005021 gait Effects 0.000 description 3
- 238000007429 general method Methods 0.000 description 3
- 230000035479 physiological effects, processes and functions Effects 0.000 description 3
- 230000035790 physiological processes and functions Effects 0.000 description 3
- 230000003252 repetitive effect Effects 0.000 description 3
- 210000003813 thumb Anatomy 0.000 description 3
- 241000282326 Felis catus Species 0.000 description 2
- 230000006978 adaptation Effects 0.000 description 2
- 230000003044 adaptive effect Effects 0.000 description 2
- 239000000853 adhesive Substances 0.000 description 2
- 230000001070 adhesive effect Effects 0.000 description 2
- 238000004458 analytical method Methods 0.000 description 2
- 239000011324 bead Substances 0.000 description 2
- 210000003169 central nervous system Anatomy 0.000 description 2
- 230000008878 coupling Effects 0.000 description 2
- 238000010168 coupling process Methods 0.000 description 2
- 238000005859 coupling reaction Methods 0.000 description 2
- 238000013135 deep learning Methods 0.000 description 2
- 230000001934 delay Effects 0.000 description 2
- 230000003111 delayed effect Effects 0.000 description 2
- 230000001419 dependent effect Effects 0.000 description 2
- 210000005069 ears Anatomy 0.000 description 2
- 238000000605 extraction Methods 0.000 description 2
- 210000000720 eyelash Anatomy 0.000 description 2
- 210000000887 face Anatomy 0.000 description 2
- 230000001815 facial effect Effects 0.000 description 2
- 238000001914 filtration Methods 0.000 description 2
- 230000005057 finger movement Effects 0.000 description 2
- 238000007667 floating Methods 0.000 description 2
- 230000014509 gene expression Effects 0.000 description 2
- 230000000977 initiatory effect Effects 0.000 description 2
- 238000004519 manufacturing process Methods 0.000 description 2
- 239000003550 marker Substances 0.000 description 2
- 238000012544 monitoring process Methods 0.000 description 2
- 210000001328 optic nerve Anatomy 0.000 description 2
- 238000004091 panning Methods 0.000 description 2
- 230000036961 partial effect Effects 0.000 description 2
- 108091008695 photoreceptors Proteins 0.000 description 2
- 238000006116 polymerization reaction Methods 0.000 description 2
- 239000000047 product Substances 0.000 description 2
- 230000035484 reaction time Effects 0.000 description 2
- 230000011514 reflex Effects 0.000 description 2
- 230000001373 regressive effect Effects 0.000 description 2
- 230000002207 retinal effect Effects 0.000 description 2
- 238000012552 review Methods 0.000 description 2
- 230000006403 short-term memory Effects 0.000 description 2
- 230000011664 signaling Effects 0.000 description 2
- 238000004088 simulation Methods 0.000 description 2
- 230000006886 spatial memory Effects 0.000 description 2
- 238000007619 statistical method Methods 0.000 description 2
- 230000002123 temporal effect Effects 0.000 description 2
- 230000036962 time dependent Effects 0.000 description 2
- 238000013519 translation Methods 0.000 description 2
- 239000010981 turquoise Substances 0.000 description 2
- 238000001429 visible spectrum Methods 0.000 description 2
- 230000002747 voluntary effect Effects 0.000 description 2
- XLYOFNOQVPJJNP-UHFFFAOYSA-N water Substances O XLYOFNOQVPJJNP-UHFFFAOYSA-N 0.000 description 2
- 230000003936 working memory Effects 0.000 description 2
- RDEIXVOBVLKYNT-VQBXQJRRSA-N (2r,3r,4r,5r)-2-[(1s,2s,3r,4s,6r)-4,6-diamino-3-[(2r,3r,6s)-3-amino-6-(1-aminoethyl)oxan-2-yl]oxy-2-hydroxycyclohexyl]oxy-5-methyl-4-(methylamino)oxane-3,5-diol;(2r,3r,4r,5r)-2-[(1s,2s,3r,4s,6r)-4,6-diamino-3-[(2r,3r,6s)-3-amino-6-(aminomethyl)oxan-2-yl]o Chemical compound OS(O)(=O)=O.O1C[C@@](O)(C)[C@H](NC)[C@@H](O)[C@H]1O[C@@H]1[C@@H](O)[C@H](O[C@@H]2[C@@H](CC[C@@H](CN)O2)N)[C@@H](N)C[C@H]1N.O1C[C@@](O)(C)[C@H](NC)[C@@H](O)[C@H]1O[C@@H]1[C@@H](O)[C@H](O[C@@H]2[C@@H](CC[C@H](O2)C(C)N)N)[C@@H](N)C[C@H]1N.O1[C@H](C(C)NC)CC[C@@H](N)[C@H]1O[C@H]1[C@H](O)[C@@H](O[C@@H]2[C@@H]([C@@H](NC)[C@@](C)(O)CO2)O)[C@H](N)C[C@@H]1N RDEIXVOBVLKYNT-VQBXQJRRSA-N 0.000 description 1
- 241001270131 Agaricus moelleri Species 0.000 description 1
- 208000019901 Anxiety disease Diseases 0.000 description 1
- 206010002953 Aphonia Diseases 0.000 description 1
- 101100272279 Beauveria bassiana Beas gene Proteins 0.000 description 1
- 238000006873 Coates reaction Methods 0.000 description 1
- 206010057315 Daydreaming Diseases 0.000 description 1
- 206010011878 Deafness Diseases 0.000 description 1
- 208000003098 Ganglion Cysts Diseases 0.000 description 1
- 241000668842 Lepidosaphes gloverii Species 0.000 description 1
- 206010025421 Macule Diseases 0.000 description 1
- 241000124008 Mammalia Species 0.000 description 1
- 241001465754 Metazoa Species 0.000 description 1
- OAICVXFJPJFONN-UHFFFAOYSA-N Phosphorus Chemical compound [P] OAICVXFJPJFONN-UHFFFAOYSA-N 0.000 description 1
- 241000590419 Polygonia interrogationis Species 0.000 description 1
- 206010039740 Screaming Diseases 0.000 description 1
- 206010041349 Somnolence Diseases 0.000 description 1
- 208000005400 Synovial Cyst Diseases 0.000 description 1
- 210000003484 anatomy Anatomy 0.000 description 1
- 230000036506 anxiety Effects 0.000 description 1
- 210000000617 arm Anatomy 0.000 description 1
- 238000003491 array Methods 0.000 description 1
- 238000005452 bending Methods 0.000 description 1
- 230000003592 biomimetic effect Effects 0.000 description 1
- 230000000903 blocking effect Effects 0.000 description 1
- 210000004204 blood vessel Anatomy 0.000 description 1
- 210000001638 cerebellum Anatomy 0.000 description 1
- 230000002301 combined effect Effects 0.000 description 1
- 230000000295 complement effect Effects 0.000 description 1
- 238000004883 computer application Methods 0.000 description 1
- 239000004020 conductor Substances 0.000 description 1
- 239000000470 constituent Substances 0.000 description 1
- 238000010276 construction Methods 0.000 description 1
- 210000004087 cornea Anatomy 0.000 description 1
- 230000001351 cycling effect Effects 0.000 description 1
- 231100000895 deafness Toxicity 0.000 description 1
- 238000012217 deletion Methods 0.000 description 1
- 230000037430 deletion Effects 0.000 description 1
- 238000013461 design Methods 0.000 description 1
- 230000008034 disappearance Effects 0.000 description 1
- 238000002224 dissection Methods 0.000 description 1
- 208000002173 dizziness Diseases 0.000 description 1
- 229940079593 drug Drugs 0.000 description 1
- 239000003814 drug Substances 0.000 description 1
- 230000005670 electromagnetic radiation Effects 0.000 description 1
- 230000006397 emotional response Effects 0.000 description 1
- 230000002708 enhancing effect Effects 0.000 description 1
- 230000007717 exclusion Effects 0.000 description 1
- 230000004384 eye physiology Effects 0.000 description 1
- 230000008921 facial expression Effects 0.000 description 1
- 210000001097 facial muscle Anatomy 0.000 description 1
- 239000012530 fluid Substances 0.000 description 1
- 238000011010 flushing procedure Methods 0.000 description 1
- 210000002683 foot Anatomy 0.000 description 1
- 210000000873 fovea centralis Anatomy 0.000 description 1
- 210000002149 gonad Anatomy 0.000 description 1
- 238000009499 grossing Methods 0.000 description 1
- 125000001475 halogen functional group Chemical group 0.000 description 1
- 208000016354 hearing loss disease Diseases 0.000 description 1
- 238000010438 heat treatment Methods 0.000 description 1
- 208000013057 hereditary mucoepithelial dysplasia Diseases 0.000 description 1
- 238000005286 illumination Methods 0.000 description 1
- 238000010191 image analysis Methods 0.000 description 1
- 238000003384 imaging method Methods 0.000 description 1
- 230000009474 immediate action Effects 0.000 description 1
- 230000005764 inhibitory process Effects 0.000 description 1
- 230000003902 lesion Effects 0.000 description 1
- 230000004301 light adaptation Effects 0.000 description 1
- 239000007788 liquid Substances 0.000 description 1
- 230000007774 longterm Effects 0.000 description 1
- 238000012423 maintenance Methods 0.000 description 1
- 230000000873 masking effect Effects 0.000 description 1
- 239000000463 material Substances 0.000 description 1
- 238000007620 mathematical function Methods 0.000 description 1
- 230000001404 mediated effect Effects 0.000 description 1
- 230000003340 mental effect Effects 0.000 description 1
- 229910044991 metal oxide Inorganic materials 0.000 description 1
- 150000004706 metal oxides Chemical class 0.000 description 1
- 230000003278 mimic effect Effects 0.000 description 1
- 238000010295 mobile communication Methods 0.000 description 1
- 230000037191 muscle physiology Effects 0.000 description 1
- 230000004379 myopia Effects 0.000 description 1
- 208000001491 myopia Diseases 0.000 description 1
- 210000005036 nerve Anatomy 0.000 description 1
- 210000004126 nerve fiber Anatomy 0.000 description 1
- 230000004751 neurological system process Effects 0.000 description 1
- 230000002232 neuromuscular Effects 0.000 description 1
- 210000002569 neuron Anatomy 0.000 description 1
- 230000007935 neutral effect Effects 0.000 description 1
- 239000013307 optical fiber Substances 0.000 description 1
- 230000010355 oscillation Effects 0.000 description 1
- 230000001151 other effect Effects 0.000 description 1
- 230000037361 pathway Effects 0.000 description 1
- 230000010363 phase shift Effects 0.000 description 1
- 229910052698 phosphorus Inorganic materials 0.000 description 1
- 239000011574 phosphorus Substances 0.000 description 1
- 230000004962 physiological condition Effects 0.000 description 1
- 238000012805 post-processing Methods 0.000 description 1
- 238000002360 preparation method Methods 0.000 description 1
- 230000002787 reinforcement Effects 0.000 description 1
- 230000036387 respiratory rate Effects 0.000 description 1
- 230000004043 responsiveness Effects 0.000 description 1
- 230000033764 rhythmic process Effects 0.000 description 1
- 230000001711 saccadic effect Effects 0.000 description 1
- 210000003786 sclera Anatomy 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000035807 sensation Effects 0.000 description 1
- 230000001568 sexual effect Effects 0.000 description 1
- 230000008054 signal transmission Effects 0.000 description 1
- 239000013589 supplement Substances 0.000 description 1
- 230000001360 synchronised effect Effects 0.000 description 1
- 238000012360 testing method Methods 0.000 description 1
- 230000001131 transforming effect Effects 0.000 description 1
- 230000001960 triggered effect Effects 0.000 description 1
- 230000004304 visual acuity Effects 0.000 description 1
- 210000000857 visual cortex Anatomy 0.000 description 1
- 230000002618 waking effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/011—Arrangements for interaction with the human body, e.g. for user immersion in virtual reality
- G06F3/013—Eye tracking input arrangements
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0484—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range
- G06F3/0485—Scrolling or panning
-
- A—HUMAN NECESSITIES
- A61—MEDICAL OR VETERINARY SCIENCE; HYGIENE
- A61B—DIAGNOSIS; SURGERY; IDENTIFICATION
- A61B3/00—Apparatus for testing the eyes; Instruments for examining the eyes
- A61B3/10—Objective types, i.e. instruments for examining the eyes independent of the patients' perceptions or reactions
- A61B3/113—Objective types, i.e. instruments for examining the eyes independent of the patients' perceptions or reactions for determining or recording eye movement
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/017—Head mounted
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/017—Head mounted
- G02B27/0172—Head mounted characterised by optical features
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F1/00—Details not covered by groups G06F3/00 - G06F13/00 and G06F21/00
- G06F1/16—Constructional details or arrangements
- G06F1/1613—Constructional details or arrangements for portable computers
- G06F1/163—Wearable computers, e.g. on a belt
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F1/00—Details not covered by groups G06F3/00 - G06F13/00 and G06F21/00
- G06F1/16—Constructional details or arrangements
- G06F1/1613—Constructional details or arrangements for portable computers
- G06F1/1633—Constructional details or arrangements of portable computers not specific to the type of enclosures covered by groups G06F1/1615 - G06F1/1626
- G06F1/1684—Constructional details or arrangements related to integrated I/O peripherals not covered by groups G06F1/1635 - G06F1/1675
- G06F1/1686—Constructional details or arrangements related to integrated I/O peripherals not covered by groups G06F1/1635 - G06F1/1675 the I/O peripheral being an integrated camera
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F1/00—Details not covered by groups G06F3/00 - G06F13/00 and G06F21/00
- G06F1/16—Constructional details or arrangements
- G06F1/1613—Constructional details or arrangements for portable computers
- G06F1/1633—Constructional details or arrangements of portable computers not specific to the type of enclosures covered by groups G06F1/1615 - G06F1/1626
- G06F1/1684—Constructional details or arrangements related to integrated I/O peripherals not covered by groups G06F1/1635 - G06F1/1675
- G06F1/1694—Constructional details or arrangements related to integrated I/O peripherals not covered by groups G06F1/1635 - G06F1/1675 the I/O peripheral being a single or a set of motion sensors for pointer control or gesture input obtained by sensing movements of the portable computer
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/011—Arrangements for interaction with the human body, e.g. for user immersion in virtual reality
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/017—Gesture based interaction, e.g. based on a set of recognized hand gestures
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/02—Input arrangements using manually operated switches, e.g. using keyboards or dials
- G06F3/023—Arrangements for converting discrete items of information into a coded form, e.g. arrangements for interpreting keyboard generated codes as alphanumeric codes, operand codes or instruction codes
- G06F3/0233—Character input methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/02—Input arrangements using manually operated switches, e.g. using keyboards or dials
- G06F3/023—Arrangements for converting discrete items of information into a coded form, e.g. arrangements for interpreting keyboard generated codes as alphanumeric codes, operand codes or instruction codes
- G06F3/0233—Character input methods
- G06F3/0236—Character input methods using selection techniques to select from displayed items
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/03—Arrangements for converting the position or the displacement of a member into a coded form
- G06F3/0304—Detection arrangements using opto-electronic means
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/03—Arrangements for converting the position or the displacement of a member into a coded form
- G06F3/041—Digitisers, e.g. for touch screens or touch pads, characterised by the transducing means
- G06F3/042—Digitisers, e.g. for touch screens or touch pads, characterised by the transducing means by opto-electronic means
- G06F3/0425—Digitisers, e.g. for touch screens or touch pads, characterised by the transducing means by opto-electronic means using a single imaging device like a video camera for tracking the absolute position of a single or a plurality of objects with respect to an imaged reference surface, e.g. video camera imaging a display or a projection screen, a table or a wall surface, on which a computer generated image is displayed or projected
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/03—Arrangements for converting the position or the displacement of a member into a coded form
- G06F3/041—Digitisers, e.g. for touch screens or touch pads, characterised by the transducing means
- G06F3/042—Digitisers, e.g. for touch screens or touch pads, characterised by the transducing means by opto-electronic means
- G06F3/0425—Digitisers, e.g. for touch screens or touch pads, characterised by the transducing means by opto-electronic means using a single imaging device like a video camera for tracking the absolute position of a single or a plurality of objects with respect to an imaged reference surface, e.g. video camera imaging a display or a projection screen, a table or a wall surface, on which a computer generated image is displayed or projected
- G06F3/0426—Digitisers, e.g. for touch screens or touch pads, characterised by the transducing means by opto-electronic means using a single imaging device like a video camera for tracking the absolute position of a single or a plurality of objects with respect to an imaged reference surface, e.g. video camera imaging a display or a projection screen, a table or a wall surface, on which a computer generated image is displayed or projected tracking fingers with respect to a virtual keyboard projected or printed on the surface
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0484—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0484—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range
- G06F3/04842—Selection of displayed objects or displayed text elements
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0484—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range
- G06F3/04845—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range for image manipulation, e.g. dragging, rotation, expansion or change of colour
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10G—REPRESENTATION OF MUSIC; RECORDING MUSIC IN NOTATION FORM; ACCESSORIES FOR MUSIC OR MUSICAL INSTRUMENTS NOT OTHERWISE PROVIDED FOR, e.g. SUPPORTS
- G10G1/00—Means for the representation of music
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/0101—Head-up displays characterised by optical features
- G02B2027/014—Head-up displays characterised by optical features comprising information/image processing systems
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/017—Head mounted
- G02B2027/0178—Eyeglass type
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/0179—Display position adjusting means not related to the information to be displayed
- G02B2027/0187—Display position adjusting means not related to the information to be displayed slaved to motion of at least a part of the body of the user, e.g. head, eye
Description
関連出願
本願は、２０１５年８月１５日に提出された同時係属米国仮出願第６２／０２５７０７号、２０１５年１１月６日に提出された同時係属米国仮出願第６２／２５２３４７号、および２０１６年４月７日に提出された同時係属米国仮出願第６２／３１９７５１号の優先権を主張し、２０１４年５月９日に提出された米国仮出願第６１／９９１４３５号、２０１４年７月１３日に提出された米国仮出願第６２／０２３９４０号、２０１４年７月２２日に提出された米国仮出願第６２／０２７７７４号、２０１４年７月２２日に提出された米国仮出願第６２／０２７７７７号、２０１４年８月１９日に提出された米国仮出願第６２／０３８９８４号、２０１４年８月１９日に提出された米国仮出願第６２／０３９００１号、２０１４年９月４日に提出された米国仮出願第６２／０４６０７２号、２０１４年１１月４日に提出された米国仮出願第６２／０７４９２０号、および２０１４年１１月４日に提出された米国仮出願第６２／０７４９２７号の優先権を出張する、全てが２０１５年５月９日に提出された同時係属米国特許出願第１４／７０８２３４号、第１４／７０８２４１号および第１４／７０８２２９号の一部継続出願であり、これらの出願の全体が参照により本明細書に組み込まれる。
Related Applications This application applies to co-pending US provisional application No. 62/025707 filed on August 15, 2015, co-pending US provisional application No. 62/252347 filed on November 6, 2015, and 2016. US Provisional Application No. 61/991435, filed May 9, 2014, July 13, 2014, claiming priority for Simultaneous Pending US Provisional Application No. 62/319751 filed on April 7, 2014. US Provisional Application No. 62/023940 filed in, US Provisional Application No. 62/027774 filed on July 22, 2014, US Provisional Application No. 62/027777 filed on July 22, 2014. , U.S. Provisional Application No. 62/039884 filed on August 19, 2014, U.S. Provisional Application No. 62/039001 filed on August 19, 2014, U.S.A. filed on September 4, 2014. Priority of Provisional Application No. 62/046072, US Provisional Application No. 62/074920 filed on November 4, 2014, and US Provisional Application No. 62/074927 filed on November 4, 2014. Business trips, all of which are partial continuations of simultaneously pending US Patent Applications Nos. 14/708234, 14/708241 and 14/708229, all filed May 9, 2015, and all of these applications. Is incorporated herein by reference.
著作権表示
この特許文書の開示の一部には、著作権保護の対象となるものが含まれている。著作権者は、米国特許商標庁の特許ファイルまたは記録に掲載された特許文書または特許開示の複製に対しては異議を唱えないが、その他の場合、すべての著作権を留保する。著作権保護は、以下の明細書および図面に記載されているソフトウェア、スクリーンショットおよびデータに適用し、すべての権利が留保される。
Copyright Notice Some of the disclosures in this patent document are subject to copyright protection. The copyright holder does not object to the reproduction of the patent document or patent disclosure contained in the US Patent and Trademark Office patent file or record, but otherwise reserves all copyrights. Copyright protection applies to the software, screenshots and data described in the following specifications and drawings, and all rights are reserved.
発明の分野
本発明は、一般的に、任意の補助的な入力サポートと共に、主にユーザの視線を用いて、ユーザの意図を識別する（ＤＩＵ：Discerning the Intent of a User）ことによって、コンピュータ装置および他の装置を制御し且つコンピュータ装置および他の装置と対話するためのシステムおよび方法に関する。このシステムは、人間-機械インターフェイス（ＨＭＩ）分野、装着型コンピュータ分野、人間生理学分野、画像処理分野、および深層学習分野の技術を利用する。このシステムは、目立たない頭部装着型視線追跡装置および／または遠隔視線追跡ハードウェア内に実装することができる。頭部装着型視線追跡装置および／または遠隔視線追跡ハードウェアは、必要に応じて、頭部装着型ディスプレイ（ＨＭＤ）、遠隔ディスプレイおよび／または他の装着可能なセンサまたはアクチュエータを備えることができる。このシステムは、ローカルコンピュータ装置またはリモートコンピュータ装置を制御し、ローカルコンピュータ装置またはリモートコンピュータ装置と対話するための、使い易く、直観的で適応性のある入力メカニズムをユーザに提供することができる。
Fields of Invention The present invention generally relates to a computer device by identifying a user's intentions (DIU: Discerning the Intent of a User), primarily using the user's line of sight, with any auxiliary input support. And systems and methods for controlling and interacting with computer devices and other devices. The system utilizes technologies in the human-machine interface (HMI) field, wearable computer field, human physiology field, image processing field, and deep learning field. The system can be implemented within a discreet head-mounted eye tracking device and / or remote eye tracking hardware. Head-mounted gaze tracking devices and / or remote gaze tracking hardware can optionally include a head-mounted display (HMD), remote display and / or other wearable sensors or actuators. The system can provide the user with an easy-to-use, intuitive and adaptive input mechanism for controlling the local computer device or the remote computer device and interacting with the local computer device or the remote computer device.
背景
コンピュータマウス、ジョイスティックおよびその他の手動追跡装置は、人間と機械との対話中に、位置情報を指定するためのユビキタスツールである。装着型コンピュータの出現によって、例えば、一般的に適切な操作を行うために静止面を必要とする巨大且つ目立つ装置は、身体に装着するように設計された装置の携帯性と合わない。
Background Computer mice, joysticks and other manual tracking devices are ubiquitous tools for specifying location information during human-machine interaction. With the advent of wearable computers, for example, large and prominent devices that generally require a stationary surface for proper operation do not match the portability of devices designed to be worn on the body.
視線追跡を用いて、ディスプレイを視認し、ディスプレイ上の仮想オブジェクトまたは装置ユーザの環境内の実在オブジェクトに関連する位置情報を意図的に指定することができる。しかしながら、目は、通常の人間活動中にも広く使用される。したがって、対話および制御を行うために眼球位置を入力データストリームとして使用する場合、その課題は、眼球運動に基づいてユーザの意図を識別する（ＤＩＵ）ことである。本発明のシステムおよび方法の目的の１つは、通常の日常活動に関連する眼球運動と、装置を制御し且つ装置と対話する意図を持つ意識的または自発的な眼球運動（本明細書では「視覚信号」と呼ぶ）とを区別することである。 Eye tracking can be used to visually recognize the display and intentionally specify location information associated with a virtual object on the display or a real object in the device user's environment. However, the eye is also widely used during normal human activity. Therefore, when using eye position as an input data stream for dialogue and control, the task is to identify the user's intent based on eye movement (DIU). One of the objects of the systems and methods of the present invention is eye movements associated with normal daily activities and conscious or voluntary eye movements intended to control and interact with the device (as used herein, ". It is to distinguish it from "visual signal").
人が自分の環境を視認し環境と対話する能力を維持しながら、眼球運動の意図を識別するためには、新しいパラダイムが必要である。 A new paradigm is needed to identify the intent of eye movements while maintaining the ability of one to see and interact with one's environment.
概要
上記に鑑み、本発明は、実質的に１つ以上の意図的な眼球運動に基づいて、１人以上のユーザの様々な意図または操作目的を実質的に連続的に識別するためのシステムおよび方法を提供する。
Overview In view of the above, the present invention is a system for substantially continuously identifying various intentions or operational purposes of one or more users based on substantially one or more intentional eye movements. Provide a method.
一実施形態によれば、検出装置を用いて、ユーザの片目または両目の眼球運動に少なくとも部分的に基づいて、ユーザの意図を決定するためのグラフィカルユーザインターフェイスを提供するための方法は、検出装置を用いて、ユーザの片目または両目が瞬きをすることを特定するステップと、検出装置を用いて、ユーザの片目または両目の眼球運動が瞬きの後に所定の後期瞬き期間に入ることを確認するステップと、検出装置を用いて、ユーザの片目または両目の眼球運動の速度が所定の瞬き後期眼球運動の閾値速度を超えることに基づいて、眼球運動が衝動性動眼であることを判断するステップと、衝動性動眼に関連する操作を行うステップとを含む。 According to one embodiment, a method for using a detector to provide a graphical user interface for determining a user's intent based at least in part on the eye movements of one or both eyes of the user is a detector. And a step of identifying that one or both eyes of the user blink, and a step of confirming that the eye movements of the user's one or both eyes enter a predetermined late blink period after blinking. A step of determining that the eye movement is an impulsive eye movement based on the speed of the eye movement of one or both eyes of the user exceeding the threshold speed of the predetermined blinking late eye movement using the detection device. Includes steps to perform operations related to the impulsive eye.
別の実施形態によれば、検出装置およびディスプレイを用いて、ユーザの片目または両目の眼球運動に少なくとも部分的に基づいて、ユーザの意図を伝えるためのグラフィカルユーザインターフェイスを提供するための方法は、検出装置を用いて、ユーザの片目または両目がオブジェクトに向けられていることを特定するステップと、検出装置を用いて、オブジェクトからディスプレイ上のターゲット位置に向けられたユーザの片目または両目の衝動性動眼を特定するステップと、衝動性動眼の特定に少なくとも部分的に基づいて、衝動性動眼中にディスプレイ上に表示された１つ以上のオブジェクトの位置をディスプレイ上のターゲット位置の周りの所定の領域に移動するステップと、検出装置を用いて、衝動性動眼がターゲット位置から所定の距離内に完了したことを確認するステップと、検出装置を用いて、表示されたオブジェクトのうち１つを追跡する１つ以上の注視運動を判断することによって、追跡されるオブジェクトを特定するステップと、ターゲット位置および追跡されるオブジェクトの一方または両方に関連する操作を行うステップとを含む。 According to another embodiment, a method of using a detector and a display to provide a graphical user interface for communicating a user's intent based at least in part on the eye movements of one or both eyes of the user. The step of using the detector to identify that one or both eyes of the user are pointing at the object, and the impulsiveness of the user's one or both eyes that the detector is used to point the object to the target position on the display. Based on the steps to identify the moving eye and at least partly based on the identification of the impulsive eye, the position of one or more objects displayed on the display during the impulsive moving eye is a predetermined area around the target position on the display. A step of moving to, a step of using a detector to confirm that the impulsive eye has been completed within a predetermined distance from the target position, and a step of using a detector to track one of the displayed objects. It includes the step of identifying the object to be tracked by determining one or more gaze movements and the step of performing operations related to the target position and one or both of the tracked objects.
別の実施形態によれば、検出装置を用いて、ユーザの片目または両目の眼球運動に少なくとも部分的に基づいて、ユーザの意図を伝えるためのグラフィカルユーザインターフェイスを提供するための方法は、検出装置を用いて、ユーザの片目または両目が第１キーボード上の第１キーに向けられていることを特定するステップと、検出装置を用いて、ユーザの片目または両目が第１キーから第２キーボード上の第２キーに向けられた第１衝動性動眼を特定するステップと、検出装置を用いて、第１衝動性動眼が第２キーの位置から所定の距離内に完了したことを確認するステップと、ユーザによる第２キーの認識を待たずに、第１キーおよび第２キーの一方または両方に関連する操作を行うステップとを含む。 According to another embodiment, a method for using a detector to provide a graphical user interface for communicating a user's intent, at least partially based on the eye movements of the user's one or both eyes, is the detector. And using the detector to identify that one or both eyes of the user are directed to the first key on the first keyboard, and using the detector, one or both eyes of the user are on the first key to the second keyboard. A step of identifying the first impulsive eye that is directed to the second key of the key, and a step of confirming that the first impulsive eye is completed within a predetermined distance from the position of the second key by using a detection device. Includes steps to perform operations related to one or both of the first and second keys without waiting for the user to recognize the second key.
さらに別の実施形態によれば、検出装置を用いて、ユーザの片目または両目の眼球運動に少なくとも部分的に基づいて、ユーザの意図を伝えるためのグラフィカルユーザインターフェイスを提供するための方法は、複数のキーを有する第１領域を含む第１キーボードと、複数のキーを有する第２領域を含む第２キーボードとを提供するステップと、検出装置を用いて、ユーザの片目または両目が第１キーボードの第１領域に向けられていることを特定するステップと、検出装置を用いて、ユーザの片目または両目が第１キーボード上の第１キーに向けられていることを特定するステップと、検出装置を用いて、ユーザの片目または両目が第１キーから第２キーボードの第２領域に向けられた第１衝動性動眼を特定するステップと、検出装置を用いて、第１衝動性動眼が第２キーボードの第２領域内で完了したことを確認するステップと、ユーザによる第２キーボードの第２領域の認識を待たずに、第１キーに関連する操作を行うステップとを含む。 According to yet another embodiment, there are multiple methods for using a detector to provide a graphical user interface for communicating a user's intent, at least partially based on the eye movements of the user's one or both eyes. A step of providing a first keyboard comprising a first area having the keys of and a second keyboard including a second area having a plurality of keys, and using a detector, one or both eyes of the user of the first keyboard. A step of identifying that the user is directed to the first region, a step of identifying that one or both eyes of the user are directed to the first key on the first keyboard using the detector, and a detector. Using the step of identifying the first impulsive eye with one or both eyes of the user directed from the first key to the second area of the second keyboard, and using the detector, the first impulsive eye is the second keyboard. This includes a step of confirming that the keyboard has been completed in the second area of the keyboard, and a step of performing an operation related to the first key without waiting for the user to recognize the second area of the second keyboard.
さらに別の実施形態によれば、検出装置を用いて、ユーザの片目または両目の眼球運動に少なくとも部分的に基づいて、ユーザの意図を伝えるためのグラフィカルユーザインターフェイスを提供するための方法は、複数のアイコンを有する第１領域を含む第１フィールドと、複数のアイコンを有する第２領域を含む第２フィールドとをディスプレイ上に形成するステップと、検出装置を用いて、ユーザの片目または両目が第１フィールドの第１領域に向けられていることを特定するステップと、検出装置を用いて、ユーザの片目または両目が第１フィールド内の第１アイコンに向けられていることを特定するステップと、検出装置を用いて、ユーザの片目または両目が第１アイコンから第２フィールドの第２領域に向けられた第１衝動性動眼を特定するステップと、検出装置を用いて、第１衝動性動眼が第２フィールドの第２領域内で完了したことを確認するステップと、第１衝動性動眼が第１アイコンから第２領域内で完了したことを確認すると、ユーザによる第２領域の認識を待たずに、第１アイコンに関連する操作を行うステップとを含む。 According to yet another embodiment, there are multiple methods for using a detector to provide a graphical user interface for communicating a user's intent, at least partially based on the eye movements of one or both eyes of the user. A step of forming a first field including a first area having the icon of the above and a second field including a second area having a plurality of icons on the display, and using a detection device, one or both eyes of the user can be the first. A step of identifying that the user is directed to the first area of the field, and a step of using the detector to identify that one or both eyes of the user are directed to the first icon in the first field. The step of identifying the first impulsive eye with the user's one or both eyes directed from the first icon to the second area of the second field using the detector, and the first impulsive eye with the detector. When the step of confirming that the first impulsive eye has been completed in the second area of the second field and the confirmation that the first impulsive eye has been completed in the second area from the first icon, the user does not have to wait for the recognition of the second area. Including a step of performing an operation related to the first icon.
別の実施形態によれば、キーボードおよび検出装置を用いて、ユーザの片目または両目の眼球運動に少なくとも部分的に基づいて、ユーザの意図を伝えるためのグラフィカルユーザインターフェイスを提供するための方法は、検出装置を用いて、ユーザの片目または両目がキーボード上の第１キーに向けられていることを特定するステップと、検出装置を用いて、ユーザの片目または両目がキーボード上の第１キーから第２キーに向けられた第１衝動性動眼を特定するステップと、検出装置を用いて、第１衝動性動眼が第２キーの位置から所定の距離内に完了したことを確認するステップと、ユーザによる第２キーの認識を待たずに、第１キーおよび第２キーの一方または両方に関連する操作を行うステップとを含む。 According to another embodiment, a method for using a keyboard and a detector to provide a graphical user interface for communicating a user's intent, at least in part, based on the eye movements of one or both eyes of the user. The steps of using the detector to identify that one or both eyes of the user are directed to the first key on the keyboard, and using the detector, the first to first key of the user's eyes or both eyes on the keyboard. A step of identifying the first impulsive eye moving toward the two keys, a step of confirming that the first impulsive moving eye is completed within a predetermined distance from the position of the second key by using a detection device, and a user. It includes a step of performing an operation related to one or both of the first key and the second key without waiting for the recognition of the second key by.
さらに別の実施形態によれば、キーボードおよび検出装置を用いて、ユーザの片目または両目の眼球運動に少なくとも部分的に基づいて、ユーザの意図を伝えるためのグラフィカルユーザインターフェイスを提供するための方法は、検出装置を用いて、ユーザの片目または両目がキーボード上の第１キーに向けられていることを特定するステップと、検出装置を用いて、ユーザの片目または両目がキーボード上の第１キーから第２キーに向けられた第１衝動性動眼を特定するステップと、検出装置を用いて、ユーザの片目または両目がキーボード上の第２キーに再び注視する１つ以上の矯正的な衝動性動眼を特定するステップと、検出装置を用いて、１つ以上の矯正的な衝動性動眼の少なくとも１つが第２キーの位置から所定の距離内に完了したことを確認するステップと、ユーザによる第２キーの認識を待たずに、第１キーおよび第２キーの一方または両方に関連する操作を行うステップとを含む。 According to yet another embodiment, a method for using a keyboard and a detector to provide a graphical user interface for communicating a user's intent, at least partially based on the eye movements of one or both eyes of the user. , Using a detector to identify that one or both eyes of the user are directed to the first key on the keyboard, and using the detector to identify one or both eyes of the user from the first key on the keyboard. One or more corrective impulsive eye movements in which one or both eyes of the user re-evaluate the second key on the keyboard using a step of identifying the first impulsive eye movements directed to the second key and a detector. A step of identifying, a step of confirming that at least one of one or more corrective impulsive eye movements has been completed within a predetermined distance from the position of the second key, and a second step by the user using a detector. It includes a step of performing an operation related to one or both of the first key and the second key without waiting for the recognition of the key.
さらに別の実施形態によれば、キーボードおよび検出装置を用いて、ユーザの片目または両目の眼球運動に少なくとも部分的に基づいて、ユーザの意図を伝えるためのグラフィカルユーザインターフェイスを提供するための方法は、検出装置を用いて、ユーザの片目または両目がキーボード上の第１キーの位置から所定の距離内に向けられていることを特定するステップと、検出装置を用いて、ユーザの片目または両目がキーボード上の第１キーから第２キーに向けられた第１衝動性動眼を特定するステップと、検出装置を用いて、第１衝動性動眼が第２キーの位置から所定の距離内に完了したことを確認するステップと、ユーザの注意を引かないようにディスプレイを変更することなく、第１キーおよび第２キーの一方または両方に関連する操作を行うステップとを含む。 According to yet another embodiment, a method for using a keyboard and a detector to provide a graphical user interface for communicating a user's intent, at least in part, based on the eye movements of one or both eyes of the user. , The step of using a detector to identify that one or both eyes of the user are directed within a predetermined distance from the position of the first key on the keyboard, and using the detector, the user's one or both eyes The first impulsive eye was completed within a predetermined distance from the position of the second key using the step of identifying the first impulsive eye directed from the first key to the second key on the keyboard and the detection device. It includes a step of confirming that and a step of performing an operation related to one or both of the first and second keys without changing the display so as not to attract the user's attention.
さらに別の実施形態によれば、キーボード、ディスプレイおよび検出装置を用いて、ユーザの片目または両目の眼球運動に少なくとも部分的に基づいて、ユーザの意図を伝えるためのグラフィカルユーザインターフェイスを提供するための方法は、検出装置を用いて、ユーザの片目または両目がキーボード上の第１位置に向けられていることを特定するステップと、検出装置を用いて、ユーザの片目または両目がディスプレイ上の第１位置から自動充填作動ターゲット位置に位置する自動充填作動ターゲット（auto-fill activation target）に向けられた第１衝動性動眼を特定するステップと、ディスプレイ上の自動充填作動ターゲット位置の周りの所定の領域内にデータセット内の１つ以上の完結要素を表示するステップと、１つ以上の完結要素のうち１つを選択したことを示す選択眼球運動を実行することによって、選択した完結要素を特定するステップと、選択した完結要素をデータセットに追加するステップとを含む。 According to yet another embodiment, a keyboard, display and detector are used to provide a graphical user interface for communicating the user's intent based at least in part on the eye movements of the user's one or both eyes. The method uses a detector to identify that the user's one or both eyes are directed to a first position on the keyboard, and the detector is used to identify the user's one or both eyes as the first on the display. A step to identify the first impulsive eye for an auto-fill activation target located at a position to an auto-fill activation target and a predetermined area around the auto-fill activation target position on the display. Identify the selected complete element by performing a step of displaying one or more complete elements in the dataset and a selective eye movement indicating that one of the one or more complete elements has been selected. Includes steps and steps to add selected completion elements to the dataset.
さらに別の実施形態によれば、キーボードおよび検出装置を用いて、ユーザの片目または両目の眼球運動に少なくとも部分的に基づいて、ユーザの意図を伝えるためのグラフィカルユーザインターフェイスを提供するための方法は、検出装置を用いて、ユーザの片目または両目がキーボード上の第１キーの位置から所定の距離内に向けられていることを特定するステップと、検出装置を用いて、ユーザの片目または両目がキーボード上の第１キーから第２キーに向けられた第１衝動性動眼を特定するステップと、検出装置を用いて、ユーザの片目または両目がキーボード上の１つ以上の追加キーに向けられた１つ以上の追加の衝動性動眼を特定するステップと、第１衝動性動眼および１つ以上の追加の衝動性動眼から、眼球運動のパターンを特定するステップと、眼球運動のパターンを神経ネットワークの入力として用いて、１つ以上の英数字を分類するステップと、１つ以上の英数字に関連する操作を行うステップとを含む。 According to yet another embodiment, a method for using a keyboard and a detector to provide a graphical user interface for communicating a user's intent, at least partially based on the eye movements of one or both eyes of the user. , Using a detector to identify that one or both eyes of the user are pointing within a predetermined distance from the position of the first key on the keyboard, and using the detector to identify one or both eyes of the user. Using a step to identify the first impulsive eye movement directed from the first key to the second key on the keyboard and a detector, one or both eyes of the user were directed to one or more additional keys on the keyboard. A step of identifying one or more additional impulsive eyes, a step of identifying a pattern of eye movements from a first impulsive eye and one or more additional impulsive eyes, and a pattern of eye movements in a neural network. It includes a step of classifying one or more alphanumeric characters as input and a step of performing an operation related to the one or more alphanumeric characters.
別の実施形態によれば、検出装置およびディスプレイを用いて、ユーザの片目または両目の眼球運動に少なくとも部分的に基づいて、ユーザの意図を伝えるためのグラフィカルユーザインターフェイスを提供するための方法は、検出装置を用いて、ユーザの片目または両目が第１位置に向けられていることを特定するステップと、検出装置を用いて、ユーザの片目または両目がディスプレイ上の第１位置から第１作動ターゲット位置に位置する第１作動ターゲットに向けられた第１衝動性動眼を特定するステップと、ディスプレイから第１作動ターゲットを除去し、ディスプレイ上で、第１作動ターゲット位置と異なる第２作動ターゲット位置に第２作動ターゲットを形成するステップと、検出装置を用いて、ユーザの片目または両目がディスプレイ上の第１作動ターゲット位置から第２位置に向けられた第２衝動性動眼を特定するステップと、検出装置を用いて、第２衝動性動眼が第２位置から所定の距離内に完了したことを確認するステップと、検出装置を用いて、ユーザの片目または両目がディスプレイ上の第２位置から第２作動ターゲット位置に向けられた第３衝動性動眼を特定するステップと、第１位置、第１作動ターゲット、第２位置および第２作動ターゲットのうち１つ以上に関連する操作を行うステップとを含む。 According to another embodiment, a method of using a detector and a display to provide a graphical user interface for communicating a user's intent based at least in part on the eye movements of one or both eyes of the user. A step of using a detector to identify that one or both eyes of the user are directed to a first position, and a detector that uses a detector to indicate that one or both eyes of the user are from the first position to the first actuation target on the display. The step of identifying the first impulsive eye moving toward the first actuating target located at the position and the removal of the first actuating target from the display to a second actuating target position on the display that is different from the first actuating target position. A step of forming a second actuating target and a step of identifying a second impulsive eye with the user's one or both eyes directed from the first actuating target position to the second position on the display using a detector. A step of using a device to confirm that the second impulsive eye has been completed within a predetermined distance from the second position, and a detection device that allows one or both eyes of the user to move from the second position to the second on the display. Includes a step of identifying a third impulsive eye moving towards an actuating target position and a step of performing an operation related to one or more of a first position, a first actuating target, a second position and a second actuating target. ..
さらに別の実施形態によれば、検出装置を用いて、ユーザの片目または両目の眼球運動に少なくとも部分的に基づいて、ユーザの意図を伝えるためのグラフィカルユーザインターフェイスを提供するための方法は、検出装置を用いて、ユーザの片目または両目が目盛り内の注視位置に向けられていることを特定するステップと、検出装置を用いて、ユーザの片目または両目が目盛り内の注視位置から作動ターゲット位置に位置する作動ターゲットに向けられた衝動性動眼を特定するステップと、検出装置を用いて、衝動性動眼が作動ターゲット位置から所定の距離内に完了したことを確認するステップと、目盛りのある位置に対する目盛り内の注視位置および作動ターゲットの一方または両方に関連する操作を行うステップとを含む。 According to yet another embodiment, a method for using a detector to provide a graphical user interface for communicating a user's intent, at least partially based on the eye movements of the user's one or both eyes, is detection. The device is used to identify that one or both eyes of the user are directed to the gaze position in the scale, and the detector is used to move the user's one or both eyes from the gaze position in the scale to the actuating target position. For a graduated position, a step of identifying the occlusal eye directed to a positioned actuating target, a step of using a detector to confirm that the impulsive eye is completed within a predetermined distance from the actuating target position, and a graduated position. Includes steps to perform operations related to the gaze position within the scale and one or both of the actuating targets.
別の実施形態によれば、検出装置を用いて、ユーザの片目または両目の眼球運動に少なくとも部分的に基づいて、ユーザの意図を識別するためのグラフィカルユーザインターフェイスを提供するための方法は、検出装置を用いて、ユーザの片目または両目がオブジェクト位置に位置するオブジェクトから所定の距離内に向けられていることを特定するステップと、検出装置を用いて、ユーザの片目または両目がオブジェクトからターゲット位置に位置するターゲットに向けられた衝動性動眼を特定するステップと、検出装置を用いて、衝動性動眼がターゲット位置から所定の距離内に完了したことを確認するステップと、ターゲットを認識する前に、オブジェクト、オブジェクト位置、ターゲットおよびターゲット位置のうち１つ以上に関連する操作を行うステップとを含み、操作は、ユーザの視覚注意を引かないように構成されたユーザフィードバックを提供することを含む。 According to another embodiment, a method for using a detector to provide a graphical user interface for identifying a user's intent based at least in part on the eye movements of one or both eyes of the user is detection. The device is used to identify that one or both eyes of the user are directed within a predetermined distance from the object located at the object position, and the detector is used to locate the user's one or both eyes from the object to the target position. A step to identify the impulsive eye moving toward the target located in, a step to confirm that the impulsive moving eye is completed within a predetermined distance from the target position by using a detection device, and a step before recognizing the target. , An object, an object position, a target, and a step of performing an operation related to one or more of the target positions, the operation comprising providing user feedback configured to not draw the user's visual attention.
さらに別の実施形態によれば、検出装置および表示装置を用いて、ユーザの片目または両目の眼球運動に少なくとも部分的に基づいて、ユーザの意図を決定するためのグラフィカルユーザインターフェイスを提供するための方法は、検出装置を用いて、ユーザの片目または両目がオブジェクト位置に位置するオブジェクトに向けられていることを特定するステップと、検出装置を用いて、ユーザの片目または両目がオブジェクト位置から第１ターゲット位置に位置する第１ターゲットに向けられた第１衝動性動眼を特定するステップと、第１衝動性動眼中に、表示装置を用いて、第１ターゲットに対するユーザの視界を変更することによって、ギャップ効果を生成するステップと、検出装置を用いて、ユーザの片目または両目が第１ターゲット位置から第２ターゲット位置に位置する第２ターゲットに向けられた第２衝動性動眼を特定するステップと、検出装置を用いて、第２衝動性動眼が第２ターゲット位置から所定の距離内に完了したことを確認するステップと、オブジェクト、オブジェクト位置、第１ターゲット、第１ターゲット位置、第２ターゲット、および第２ターゲット位置のうち１つ以上に関連する操作を行うことを含む。 According to yet another embodiment, the detector and display device are used to provide a graphical user interface for determining a user's intent based at least in part on the eye movements of one or both eyes of the user. The method uses a detector to identify that the user's one or both eyes are directed at an object located at the object position, and the detector is used to first identify the user's one or both eyes from the object position. By changing the user's view of the first target using a display device during the step of identifying the first impulsive eye moving toward the first target located at the target position and during the first impulsive moving eye. A step of generating a gap effect and a step of using a detector to identify a second impulsive eye with one or both eyes of the user directed from a first target position to a second target located at a second target position. A step of confirming that the second impulsive eye has been completed within a predetermined distance from the second target position using a detector, and an object, an object position, a first target, a first target position, a second target, and Includes performing operations related to one or more of the second target positions.
さらに別の実施形態によれば、ディスプレイおよび検出装置を用いて、ユーザの片目または両目の眼球運動に少なくとも部分的に基づいて、ユーザの意図を決定するためのグラフィカルユーザインターフェイスを提供するための方法は、ユーザから離れるように向けられたシーンカメラを用いて、ユーザの環境内に位置するオブジェクトを特定するステップと、オブジェクトテンプレートのデータベースを用いて、ユーザの環境内に位置する１つ以上のオブジェクトが重要度の高いオブジェクトであるか否かを判断するステップと、通信装置を用いて、ディスプレイ上で表示される着信データセットを受信するステップと、重要度の高いオブジェクトがユーザの環境内に存在しないと判断された場合に限り、ディスプレイ上で着信データセットを表示するステップとを含む。 According to yet another embodiment, a method for using a display and a detector to provide a graphical user interface for determining a user's intent based at least in part on the eye movements of one or both eyes of the user. Uses a scene camera aimed away from the user to identify objects located within the user's environment, and uses a database of object templates to identify one or more objects located within the user's environment. A step to determine if is a high importance object, a step to receive an incoming data set displayed on the display using a communication device, and a high importance object exists in the user's environment. Includes a step of displaying the incoming data set on the display only if it is determined not to.
別の実施形態によれば、シーンカメラおよび検出装置を用いて、２名の装着型装置ユーザの片目または両目の眼球運動に少なくとも部分的に基づいて、ユーザの意図を決定するための方法は、第１装着型装置ユーザに関連する第１装着型装置に動作可能に接続されたシーンカメラを用いて撮影された１つ以上のシーンカメラ画像から、第２装着型装置ユーザの片目または両目を含む領域を特定するステップと、画像認識を用いて、第２装着型装置ユーザの片目または両目が所定の角度範囲で第１装着型装置ユーザに向けられていることを特定するステップと、第２装着型装置ユーザの片目または両目が所定の時間で第１装着型装置ユーザに向けられていることを確認するステップと、第１装着型装置ユーザに関連する第１装着型装置と、第２装着型装置ユーザに関連する第２装着型装置との間の電子通信を許可するステップと含む。 According to another embodiment, a method for determining a user's intent using a scene camera and a detector, at least partially based on the eye movements of one or both eyes of two wearable device users. Includes one or both eyes of the second wearable device user from one or more scene camera images taken with a scene camera operably connected to the first wearable device associated with the first wearable device user. A step of identifying an area, a step of using image recognition to identify that one or both eyes of the second wearable device user are directed at the first wearable device user within a predetermined angle range, and a second wear. A step of confirming that one or both eyes of the mold device user are aimed at the first wearable device user at a predetermined time, a first wearable device associated with the first wearable device user, and a second wearable type. Includes a step of allowing electronic communication with a second wearable device associated with the device user.
別の実施形態によれば、ディスプレイ、１つ以上のシーンカメラおよび検出装置を用いて、ユーザの片目または両目の眼球運動に少なくとも部分的に基づいて、ユーザの意図を決定するためのグラフィカルユーザインターフェイスを提供するための方法は、ユーザの環境に向けられた１つ以上のシーンカメラを用いて、ユーザの環境内のオブジェクトを特定するステップと、通信装置を用いて、ディスプレイ上で表示される１つ以上の着信データセットを受信するステップと、１つ以上の所定のユーザ嗜好に基づいて、ディスプレイ上でユーザに表示されるデータセットの選択および選択時間を決定するステップとを含む。 According to another embodiment, a graphical user interface for determining a user's intent using a display, one or more scene cameras and detectors, based at least in part on the eye movements of the user's one or both eyes. The method for providing is to identify an object in the user's environment using one or more scene cameras directed to the user's environment, and to be displayed on a display using a communication device. It comprises receiving one or more incoming datasets and determining the selection and selection time of the datasets to be displayed to the user on the display based on one or more predetermined user preferences.
さらに別の実施形態によれば、装着型装置ユーザに関連する装着型装置のシーンカメラおよび検出装置を用いて、ユーザの片目または両目の眼球運動に少なくとも部分的に基づいて、装着型装置ユーザの意図を決定するための方法は、装着型装置のシーンカメラを用いて撮影された１つ以上のシーンカメラ画像から、観察される個人の片目または両目を含む領域を特定するステップと、検出装置およびシーンカメラを用いて、装着型装置ユーザの視線が所定の角度範囲で観察される個人の片目または両目に向けられることを特定するステップと、画像認識を用いて、観察される個人の片目または両目が所定の角度範囲で装着型装置ユーザに向けられていることを特定するステップと、装着型装置を用いて、操作を行うステップとを含む。 According to yet another embodiment, the wearable device user's scene camera and detection device associated with the wearable device user are used to at least partially based on the eye movements of the user's one or both eyes. The method for determining the intent is to identify the area containing one or both eyes of the individual to be observed from one or more scene camera images taken with the scene camera of the wearable device, and the detector and A scene camera is used to identify that the wearable device user's line of sight is directed to one or both eyes of the individual observed in a predetermined angular range, and image recognition is used to identify the individual's one or both eyes to be observed. Includes a step of identifying that is directed at the wearable device user in a predetermined angle range, and a step of performing an operation using the wearable device.
別の実施形態によれば、シーンカメラおよび１つ以上のビーコンを用いて、２名の装着型装置ユーザの片目または両目の眼球運動に少なくとも部分的に基づいて、ユーザの意図を決定するための方法は、第１装着型装置ユーザに関連する第１装着型装置に動作可能に接続されたシーンカメラを用いて撮影された１つ以上のシーンカメラ画像から、第２装着型装置ユーザに関連する第２装着型装置からの１つ以上のビーコンを含む領域を特定するステップと、シーンカメラを用いて、第２装着型装置ユーザに関連する第２装着型装置からの１つ以上のビーコンによって発信されたコードを特定するステップと、アクセスコードのデータベースを用いて、ビーコンによって発信されたコードが第２装着型装置ユーザに関する情報のデータベースへのアクセスを許可することを確認するステップと、第１装着型装置ユーザに関連する第１装着型装置と、第２装着型装置ユーザに関する情報を含むデータベースとの間の電子通信を許可するステップとを含む。 According to another embodiment, a scene camera and one or more beacons are used to determine a user's intent based at least in part on the eye movements of one or both eyes of two wearable device users. The method relates to a second wearable device user from one or more scene camera images taken with a scene camera operably connected to the first wearable device user associated with the first wearable device user. Transmitted by one or more beacons from the second wearable device associated with the second wearable device user, using a scene camera and a step to identify an area containing one or more beacons from the second wearable device. A step to identify the code that has been used, a step to confirm that the code transmitted by the beacon allows access to the database of information about the second wearable device user, and a step to use the database of access codes, and the first wear. It includes a step of allowing electronic communication between the first wearable device associated with the type device user and a database containing information about the second wearable device user.
さらに別の実施形態によれば、検出装置を用いて、ユーザの片目または両目の眼球運動に少なくとも部分的に基づいて、ユーザの意図を決定するための方法は、検出装置を用いて、ユーザの片目または両目が意図的な眼球運動であると決定されたことを特定するステップと、眼球運動テンプレートのデータベースを用いて、意図的な眼球運動に関連する視線操作を特定するステップと、実質的に同時にユーザの片目または両目に関連していない入力装置によって生成されたユーザ入力を特定するステップと、入力装置によって生成されたユーザ入力に関連する装置操作を特定するステップと、入力階層のデータベースに基づいて、意図的な眼球運動に関連する視線操作およびユーザ入力に関連する装置操作のうち１つを行うステップとを含む。 According to yet another embodiment, a method for determining a user's intent using a detector, at least partially based on the eye movements of the user's one or both eyes, is a method of using the detector to determine the user's intent. A step of identifying that one or both eyes were determined to be intentional eye movements, and a step of identifying eye manipulations associated with intentional eye movements using a database of eye movement templates, and substantially Based on a database of input hierarchies, at the same time identifying user inputs generated by input devices that are not related to one or both eyes of the user, and identifying device operations related to user inputs generated by the input device. It includes a step of performing one of a line-of-sight operation related to intentional eye movement and a device operation related to user input.
さらに別の実施形態によれば、検出装置およびポインティング装置を用いて、ユーザの片目または両目の眼球運動に少なくとも部分的に基づいて、１つ以上のディスプレイ上でポインタを移動するための方法は、その位置がポインティング装置によって少なくとも部分的に制御されるポインタのソースディスプレイ上のソース位置を特定するステップと、検出装置を用いて、ユーザの片目または両目がターゲットディスプレイ上のターゲット位置に向けられていることを特定するステップと、ポインティング装置を用いて、ソースディスプレイ上のソース位置に対して、所定の方向範囲内の方向でターゲットディスプレイ上のターゲット位置に向かったポインティング装置の移動を確認するステップと、ターゲット位置から所定の距離内で、ターゲットディスプレイ上の新しいポインタ位置にポインタを表示するステップとを含む。 According to yet another embodiment, a method for moving a pointer on one or more displays using a detector and a pointing device, at least partially based on the eye movements of one or both eyes of the user. The user's one or both eyes are directed to the target position on the target display using the step of identifying the source position on the source display of the pointer whose position is at least partially controlled by the pointing device and the detector. A step of identifying that, and a step of confirming the movement of the pointing device toward the target position on the target display in a direction within a predetermined direction range with respect to the source position on the source display by using the pointing device. Includes the step of displaying the pointer at a new pointer position on the target display within a predetermined distance from the target position.
別の実施形態によれば、ディスプレイおよび検出装置を用いて、ユーザの片目または両目の眼球運動に少なくとも部分的に基づいて、ユーザの意図を決定するためのグラフィカルユーザインターフェイスを提供するための方法は、楽譜の一部分をディスプレイに表示するステップと、検出装置を用いて、ユーザの片目または両目が表示された楽譜の部分内の注視位置に向けられていることを特定するステップと、楽譜を含むデータベースを用いて、楽譜内の１つ以上の切断位置を特定するステップと、検出装置を用いて、注視位置が楽譜内の切断位置の少なくとも１つから所定の距離内に位置することを確認するステップと、
楽譜の新しい部分を表示するステップと含む。
According to another embodiment, a method for using a display and a detector to provide a graphical user interface for determining a user's intent based at least in part on the eye movements of one or both eyes of the user. A database containing the score, a step of displaying a portion of the score on the display, a step of using a detector to identify that one or both eyes of the user are directed to a gaze position within the portion of the displayed score, and a database containing the score. To identify one or more cutting positions in the score using, and to confirm that the gaze position is within a predetermined distance from at least one of the cutting positions in the score using a detector. When,
Includes steps to display new parts of the score.
さらに別の実施形態によれば、検出装置、シーンカメラおよびディスプレイを用いて、乗り物を制御しているユーザの片目または両目の眼球運動に少なくとも部分的に基づいて、ユーザの意図を伝えるためのグラフィカルユーザインターフェイスを提供するための方法は、シーンカメラに動作可能に接続されたディスプレイ上に、乗り物の環境のビデオ画像を表示するステップと、検出装置を用いて、ユーザの片目または両目がディスプレイ上の乗り物の環境のビデオ画像内のターゲット位置に向けられていることを特定するステップと、検出装置を用いて、ユーザの片目または両目がディスプレイ上のターゲット位置からディスプレイ上の作動ターゲットに向けられた第１衝動性動眼を特定するステップと、検出装置を用いて、第１衝動性動眼が作動ターゲットの位置から所定の距離内に完了したことを確認するステップと、ディスプレイ上のターゲット位置によって表された乗り物の環境内の目標位置に向かって、乗り物を移動するステップとを含む。 According to yet another embodiment, a detector, a scene camera and a display are used to convey the user's intent, at least in part, based on the eye movements of one or both eyes of the user controlling the vehicle. The method for providing a user interface is to display a video image of the vehicle environment on a display operably connected to the scene camera, and using a detector, the user's one or both eyes are on the display. A step of identifying that the vehicle environment is directed to a target position in a video image and a detector that directs one or both eyes of the user from the target position on the display to the actuating target on the display. Represented by one step of identifying the impulsive eye, a step of using a detector to confirm that the first impulsive eye was completed within a predetermined distance from the position of the actuating target, and the position of the target on the display. Includes steps to move the vehicle towards a target position within the vehicle's environment.
さらに別の実施形態によれば、検出装置を用いて、乗り物を運転しているユーザの片目または両目の眼球運動の少なくとも一部に基づいて、ユーザの意図を伝えるためのグラフィカルユーザインターフェイスを提供するための方法は、検出装置を用いて、ユーザの片目または両目がオブジェクト位置に位置するオブジェクトに向けられていることを特定するステップと、検出装置を用いて、ユーザの片目または両目がオブジェクト位置からディスプレイ上のターゲット位置に位置する作動ターゲットに向けられた第１衝動性動眼を特定するステップと、検出装置を用いて、第１衝動性動眼がターゲット位置から所定の距離内に完了したことを確認するステップと、検出装置を用いて、ユーザの片目または両目が所定の時間および所定の方向範囲で、乗り物の移動方向に向けられたことを確認するステップと、オブジェクトおよび作動ターゲットのうち一方または両方に関連する操作を行うステップとを含む。 According to yet another embodiment, the detector is used to provide a graphical user interface for communicating the user's intent based on at least a portion of the eye movements of one or both eyes of the user driving the vehicle. The method for this is to use a detector to identify that the user's one or both eyes are directed at the object located at the object position, and to use the detector to identify the user's one or both eyes from the object position. Confirm that the first impulsive eye has been completed within a predetermined distance from the target position by using the step of identifying the first impulsive eye moving toward the operating target located at the target position on the display and the detection device. Steps to ensure that one or both eyes of the user are oriented in the direction of movement of the vehicle at a given time and in a given directional range using a detector, and one or both of the object and the actuating target. Includes steps to perform operations related to.
さらに別の実施形態によれば、検出装置を用いて、ユーザの片目または両目の眼球運動に少なくとも部分的に基づいて、ユーザのメモリを増強するためのグラフィカルユーザインターフェイスを提供するための方法は、検出装置を用いて、ユーザの片目または両目が記憶オブジェクトの位置から所定の距離内に向けられていることを特定するステップと、ユーザの片目または両目がオブジェクトの位置から、ターゲット位置に位置する記憶作動ターゲットに向けられた第１衝動性動眼を特定するステップと、検出装置を用いて、第１衝動性動眼がターゲット位置から所定の距離内に完了したことを確認するステップと、メモリ装置を用いて、記憶オブジェクトに関連するデータセットを複数のデータセット内に位置し且つ記憶データセット索引に対応する位置に記憶し、記憶データセット索引を１で増分する記憶操作を行うステップとを含む。 According to yet another embodiment, a method for using a detector to provide a graphical user interface for augmenting a user's memory based at least in part on the eye movements of one or both eyes of the user. A step of using a detector to identify that one or both eyes of the user are oriented within a predetermined distance from the position of the storage object, and a memory of the user's one or both eyes located at the target position from the position of the object. Using a memory device, a step of identifying the first impulsive eye moving toward the working target, a step of confirming that the first impulsive eye is completed within a predetermined distance from the target position by using a detection device, and a memory device. It includes a step of storing the data set related to the storage object in a plurality of data sets and in a position corresponding to the storage data set index, and performing a storage operation in which the storage data set index is incremented by 1.
別の実施形態によれば、検出装置およびディスプレイを用いてユーザの片目または両目の眼球運動に少なくとも部分的に基づいて、ユーザのメモリを増強するためのグラフィカルユーザインターフェイスを提供するための方法は、検出装置を用いて、ユーザの片目または両目がディスプレイ上の検索オブジェクト位置に位置する検索オブジェクトに向けられていることを特定するステップと、検出装置を用いて、ユーザの片目または両目がディスプレイ上の検索オブジェクト位置から、ターゲット位置に位置する検索作動ターゲットに向けられた第１衝動性動眼を特定するステップと、検出装置を用いて、第１衝動性動眼がターゲット位置から所定の距離内に完了したことを確認するステップと、メモリ装置を用いて、複数のデータセット内に位置し且つ検索データセット索引に対応する位置から検索データセットを検索する検索操作を行うステップとを含む。 According to another embodiment, a method for using a detector and a display to provide a graphical user interface for augmenting a user's memory based at least in part on the eye movements of the user's one or both eyes. The steps of using the detector to identify that one or both eyes of the user are aimed at the search object located at the location of the search object on the display, and using the detector, the user's one or both eyes are on the display. The first impulsive eye is completed within a predetermined distance from the target position by using the step of identifying the first impulsive eye moving toward the search operating target located at the target position from the search object position and the detection device. This includes a step of confirming that, and a step of performing a search operation for searching the search data set from a position located in a plurality of data sets and corresponding to the search data set index by using the memory device.
別の実施形態によれば、検出装置を用いて、ユーザの片目または両目の眼球運動に少なくとも部分的に基づいて、ユーザのメモリを増強するためのグラフィカルユーザインターフェイスを提供するための方法は、検出装置を用いて、オブジェクトから記憶作動ターゲットの位置の所定の距離に向けられた衝動性動眼を検出した場合、オブジェクトに関連するデータセットを複数のデータセット内に位置し且つ記憶データセット索引に対応する位置に記憶し、記憶データセット索引を１で増分する記憶操作を行うステップと、検出装置を用いて、オブジェクトから検索作動ターゲットの位置の所定の距離に向けられた衝動性動眼を検出した場合、複数のデータセット内に位置し且つ検索データセット索引に対応する位置から検索データセットを検索する検索操作を行うステップと、検出装置を用いて、上方の作動ターゲットに向けられた衝動性動眼を検出した場合、検出データセット索引を１で増分してから、検索データセット索引を選択するステップと、検出装置を用いて、下方の作動ターゲットに向けられた衝動性動眼を検出した場合、検出データセット索引を１で減分してから、検索データセット索引を選択するステップと、検出装置を用いて、オブジェクトから削除作動ターゲットの位置の所定の距離に向けられた衝動性動眼を検出した場合、複数のデータセット内に位置し且つ検索データセット索引に対応する位置から検索データセットを削除する削除操作を行い、記憶データセット索引を１で減分するステップとを含む。 According to another embodiment, a method for using a detector to provide a graphical user interface for augmenting a user's memory, at least partially based on the eye movements of the user's one or both eyes, is detection. When the device is used to detect an impulsive moving eye directed from an object to a predetermined distance from the location of a memory-activated target, the dataset associated with the object is located within multiple datasets and supports the stored dataset index. When a step of performing a storage operation of storing at the desired position and incrementing the stored data set index by 1 and a detection device are used to detect an impulsive eye that is directed to a predetermined distance from the object at the position of the search operation target. , The step of performing a search operation to search the search dataset from the position corresponding to the search dataset index, which is located in multiple datasets, and the impulsive eye movement toward the upper working target using the detection device. If detected, increment the detection dataset index by 1 and then select the search dataset index, and if the detector is used to detect an impulsive eye toward the lower working target, the detection data If the set index is decremented by 1 and then the search dataset index is selected and the detector is used to detect an impulsive eye from the object directed to a given distance at the location of the delete actuating target. It includes a step of performing a delete operation to delete a search dataset from a location in a plurality of datasets and corresponding to the search dataset index, and decrementing the stored dataset index by 1.
さらに別の実施形態によれば、ユーザの片目または両目に向けられたた１つ以上の眼球カメラおよびユーザの環境に向けられたシーンカメラを用いて、ユーザの片目または両目の方向に少なくとも部分的に基づいて、文書のセキュリティを強化するためのユーザインターフェイスを提供するための方法は、１つ以上の眼球カメラを用いて、ユーザの片目または両目の１つ以上の画像を撮影するステップと、ユーザの片目または両目の１つ以上の画像を既知ＩＤ−目テンプレートのデータベースと比較することによって、ユーザのＩＤを決定するステップと、シーンカメラを用いて、ユーザの片目または両目によって閲覧される文書を特定するステップと、１つ以上の眼球カメラを用いて、ユーザの片目または両目が文書に向けられていることを確認するステップと、特定されたユーザが文書を閲覧したことを示すように、文書を電子的に承認するステップとを含む。 According to yet another embodiment, one or more eye cameras directed at the user's one or both eyes and a scene camera directed at the user's environment are used, at least partially in the direction of the user's one or both eyes. Based on, the method for providing a user interface for enhancing the security of a document is to take one or more images of the user's one or both eyes with one or more eye cameras, and the user. A step to determine a user's ID by comparing one or more images of one or both eyes with a database of known ID-eye templates, and a document viewed by the user's one or both eyes using a scene camera. A document to identify and to use one or more eye cameras to ensure that one or both eyes of the user are pointing at the document and to indicate that the identified user has viewed the document. Includes steps to electronically approve.
さらに別の実施形態によれば、ディスプレイ、頭部動作検出装置および眼球位置検出装置を用いて、ユーザの片目または両目の眼球運動に少なくとも部分的に基づいて、ユーザの意図を決定するためのグラフィカルユーザインターフェイスを提供するための方法は、ディスプレイ上に、複数のオブジェクトを表示するステップと、眼球位置検出装置を用いて、ユーザの片目または両目が表示された複数のオブジェクトのうち１つに向けられていることを特定することによって、観察されているオブジェクトを特定するステップと、頭部動作検出装置を用いて、第１方向に沿ったユーザの頭部動作を特定するステップと、眼球運動検出装置を用いて、ユーザの片目または両目の眼球運動を確認するステップと、頭部動作の間にユーザが眼球運動によって観察されているオブジェクトを続けて観察するか否かに基づいて、眼球運動が前庭性眼球運動であるか否かを判断するステップと、眼球運動が前庭性眼球運動ではないと判断した場合、表示された複数のオブジェクトをディスプレイ上で第１方向に沿って移動するステップとを含む。 According to yet another embodiment, a display, a head motion detector and an eye position detector are used to graphically determine the user's intent based at least in part on the eye movements of one or both eyes of the user. A method for providing a user interface is directed to one of a plurality of objects in which one or both eyes of the user are displayed, using a step of displaying multiple objects on the display and an eye position detector. A step of identifying the observed object by identifying the eye movement, a step of identifying the user's head movement along the first direction using the head movement detection device, and an eye movement detection device. Based on the step of confirming the eye movements of the user's one or both eyes and whether or not the user continues to observe the object being observed by the eye movements during the head movement, the eye movements are vestibular. It includes a step of determining whether or not the eye movement is a sexual eye movement, and a step of moving a plurality of displayed objects along a first direction on the display when it is determined that the eye movement is not a vestibular eye movement. ..
さらに別の実施形態によれば、検出装置およびシーンカメラを用いて、ユーザの片目または両目の眼球運動に少なくとも部分的に基づいて、ユーザの意図を伝えるためのユーザインターフェイスを提供するための方法は、検出装置を用いて、ユーザの片目または両目がユーザの環境内の第１注視位置に向けられていることを特定するステップと、シーンカメラを用いて、ユーザの環境内の第１注視位置に位置する第１注視オブジェクトを特定するステップと、オブジェクトテンプレートのデータベースを用いて、第１注視オブジェクトを特定することによって、第１特定された注視オブジェクトを作成するステップと、検出装置を用いて、ユーザの片目または両目がユーザの環境内の第２注視位置に向かって視線を移動する眼球運動を特定するステップと、シーンカメラを用いて、ユーザの環境内の第２注視位置に位置する第２注視オブジェクトを特定するステップと、オブジェクトテンプレートのデータベースを用いて、第２注視オブジェクトを特定することによって、第２特定された注視オブジェクトを作成するステップと、作動可能なオブジェクトのデータベースを用いて、第２特定された注視オブジェクトが作動ターゲットに相当することを確認するステップと、第１特定された注視オブジェクト、第１注視位置、第２特定された注視オブジェクト、および第２注視位置のうち１つ以上に関連する操作を行うステップとを含む。 According to yet another embodiment, a method for using a detector and a scene camera to provide a user interface for communicating a user's intent based at least in part on the eye movements of one or both eyes of the user. Use the detector to identify that one or both eyes of the user are directed to the first gaze position in the user's environment, and use the scene camera to reach the first gaze position in the user's environment. A step of identifying a first gaze object to be located, a step of creating a first gaze object by identifying the first gaze object using a database of object templates, and a user using a detection device. A step of identifying eye movements in which one or both eyes move their line of sight toward a second gaze position in the user's environment, and a second gaze located in the second gaze position in the user's environment using a scene camera. A second step of creating a second identified gaze object by identifying a second gaze object using a database of object templates, and a second step of creating a second identified gaze object using a database of operable objects. One or more of the first identified gaze object, the first gaze position, the second identified gaze object, and the second gaze position, with the step of verifying that the identified gaze object corresponds to the working target. Includes steps to perform related operations.
さらに別の実施形態によれば、検出装置、シーンカメラおよびディスプレイを用いて、ユーザの片目または両目の眼球運動に少なくとも部分的に基づいて、ユーザの意図を伝えるためのユーザインターフェイスを提供するための方法は、検出装置を用いて、ユーザの片目または両目がユーザの環境内の注視位置に向けられていることを特定するステップと、シーンカメラを用いて、ユーザの環境内の注視位置に位置する注視オブジェクトを特定するステップと、オブジェクトテンプレートのデータベースを用いて、注視オブジェクトを特定することによって、特定された注視オブジェクトを作成するステップと、検出装置を用いて、ユーザの片目または両目が特定された注視オブジェクトからディスプレイ上の１つ以上の作動ターゲットのうち１つに向かって視線を移動する１つ以上の眼球運動を特定するステップと、検出装置を用いて、１つ以上の眼球運動が１つ以上の作動ターゲットのうち１つから所定の距離内に完了したことを確認することによって、選択された作動ターゲットを作成するステップと、第１特定された注視オブジェクトおよび選択された作動ターゲットのうち一方または両方に関連する操作を行うステップとを含む。 According to yet another embodiment, a detector, a scene camera and a display are used to provide a user interface for communicating a user's intent based at least in part on the eye movements of one or both eyes of the user. The method uses a detector to identify that one or both eyes of the user are directed to a gaze position in the user's environment, and a scene camera is used to locate the gaze position in the user's environment. The step of identifying the gaze object, the step of creating the identified gaze object by identifying the gaze object using the database of object templates, and the detection device were used to identify one or both eyes of the user. One or more eye movements using a detection device and a step of identifying one or more eye movements that move the line of sight from the gaze object to one of the one or more actuating targets on the display. One of the first identified gaze object and the selected actuation target, the step of creating the selected actuation target by confirming that it was completed within a predetermined distance from one of the above actuation targets. Or include steps to perform operations related to both.
別の実施形態によれば、検出装置を用いて、ユーザの認知状態、生理状態または眼球運動履歴に少なくとも部分的に基づいて、電子ディスプレイを備えたグラフィカルユーザインターフェイス内で、ユーザの意図を示す信号の認識を調整するための方法は、ユーザの認知状態、神経状態、生理状態、および眼球運動履歴の少なくとも１つを観察するステップと、検出装置を用いて、ユーザの片目または両目の所定の動きによって実行され得るユーザの意図を伝える眼球運動によって示された少なくとも１つの既知の視覚信号を特定するステップと、少なくとも１つの既知の視覚信号を特定する前にまたは最中に、ユーザの認知状態、生理状態または眼球運動の１つ以上に基づいて、少なくとも１つの既知信号を認識するための許容差を調整するステップとを含む。 According to another embodiment, a detector is used to indicate a user's intent within a graphical user interface with an electronic display, at least in part based on the user's cognitive, physiological or eye movement history. A method for adjusting the perception of a user is a step of observing at least one of the user's cognitive state, neural state, physiological state, and eye movement history, and a predetermined movement of the user's one or both eyes using a detection device. The step of identifying at least one known visual signal indicated by eye movements that conveys the user's intent that may be performed by, and the cognitive state of the user before or during identifying at least one known visual signal. It comprises adjusting the tolerance for recognizing at least one known signal based on one or more of physiological conditions or eye movements.
さらに別の実施形態によれば、検出装置を用いて、ユーザの認知状態、生理状態または眼球運動履歴に少なくとも部分的に基づいて、電子ディスプレイを備えたグラフィカルユーザインターフェイス内で、ユーザの意図を示す信号の認識に対する応答を調整するための方法は、ユーザの認知状態、神経状態、生理状態および眼球運動履歴のうち少なくとも１つを観察するステップと、ユーザの片目または両目の所定の眼球運動、ユーザの片目または両目の瞳孔拡大、およびユーザの片目または両目の瞳孔縮小のうち１つ以上を含み、ユーザの意図を伝えるための少なくとも１つの既知の視覚信号を特定するステップと、少なくとも１つの既知信号を特定する前にまたは最中に、ユーザの認知状態、生理状態、または眼球運動のうち少なくとも１つに基づいて、少なくとも１つの既知の視覚信号を認識するステップと、応答のタイミング、応答の一部として形成されたグラフィック画像の選択、応答の一部として形成されたグラフィック要素の変遷、応答の任意部分のタイミング、および応答の一部として取られた操作のうち少なくとも１つを変更することによって、少なくとも１つの既知の視覚信号に対する応答を調整するステップとを含む。 According to yet another embodiment, the detector is used to indicate the user's intent within a graphical user interface with an electronic display, at least partially based on the user's cognitive, physiological or eye movement history. Methods for adjusting the response to signal recognition include the step of observing at least one of the user's cognitive state, neural state, physiological state and eye movement history, and the predetermined eye movement of the user's one or both eyes, the user. A step of identifying at least one known visual signal to convey the user's intent, including one or more of one-eye or both-eye pupil enlargement and one or more of the user's one-eye or both-eye pupil reduction, and at least one known signal. A step of recognizing at least one known visual signal based on at least one of the user's cognitive state, physiological state, or eye movement, and the timing of the response, one of the responses, before or during the identification of. By changing at least one of the selection of the graphic image formed as a part, the transition of the graphic element formed as part of the response, the timing of any part of the response, and the operation taken as part of the response. Includes the step of adjusting the response to at least one known visual signal.
さらに別の実施形態によれば、ヘッドセットおよび検出装置を用いて、ユーザの片目または両目の眼球運動に少なくとも部分的に基づいて、ユーザの意図を決定するためのグラフィカルユーザインターフェイスを提供するための方法は、ヘッドセットを用いて、ユーザの頭部動作を決定するステップと、ユーザの頭部動作の振幅および振動数の一方または両方に基づいて、眼球運動の厳密性範囲を決定するステップと、検出装置を用いて、ユーザの片目または両目の１つ以上の眼球運動が眼球運動の厳密性範囲内の１つ以上の眼球運動テンプレートと一致することを確認するステップと、１つ以上の眼球運動に関連する操作を行うステップとを含む。 According to yet another embodiment, a headset and a detector are used to provide a graphical user interface for determining a user's intent based at least in part on the eye movements of one or both eyes of the user. The method includes a step of determining the user's head movement using a headset and a step of determining the exact range of eye movement based on one or both of the amplitude and frequency of the user's head movement. A step of verifying that one or more eye movements of the user's one or both eyes match one or more eye movement templates within the strict range of eye movements and one or more eye movements using a detector. Includes steps to perform operations related to.
別の実施形態によれば、ヘッドセットを用いて、ユーザの片目または両目の眼球運動に少なくとも部分的に基づいて、ユーザの意図を決定するためのグラフィカルユーザインターフェイスを提供するための方法は、ヘッドセットを用いて、ユーザの頭部の環境照明を決定するステップと、環境照明の光度および変化の一方または両方に基づいて、眼球運動の厳密性範囲を決定するステップと、ヘッドセットに搭載された眼球運動検出装置を用いて、ユーザの片目または両目の１つ以上の眼球運動が眼球運動の厳密性範囲内の１つ以上の眼球運動テンプレートと一致することを確認するステップと、１つ以上の眼球運動に関連する操作を行うステップとを含む。 According to another embodiment, the method for using a headset to provide a graphical user interface for determining a user's intent based at least in part on the eye movements of one or both eyes of the user is head. Mounted on the headset, using the set to determine the ambient lighting of the user's head, and determining the range of rigor of eye movement based on one or both of the luminosity and variation of the ambient lighting. A step of using an eye movement detector to verify that one or more eye movements of one or both eyes of the user match one or more eye movement templates within the strict range of eye movements and one or more steps. Includes steps to perform operations related to eye movements.
さらに別の実施形態によれば、検出装置およびディスプレイを用いて、ユーザの片目または両目の眼球運動に少なくとも部分的に基づいて、ユーザの意図を決定するためのグラフィカルユーザインターフェイスを提供するための方法は、検出装置を用いて、ユーザの片目または両目がディスプレイ上の最初の注視オブジェクト位置に位置する最初の注視オブジェクトに向けられていることを特定するステップと、ディスプレイに表示されるオブジェクトの好ましい拡大中心のデータベースを用いて、最初の注視オブジェクトの位置から所定の距離内に位置する全てのオブジェクトをディスプレイ上で拡大するための好ましい中心位置を決定するステップと、検出装置を用いて、最初の注視オブジェクトの位置から所定の距離内に位置する全てのオブジェクトをディスプレイ上で拡大する間に、ユーザの片目または両目が追跡オブジェクトに追従することを特定するステップと、検出装置を用いて、追跡オブジェクトが拡大の間に所定の距離または所定の時間で追跡されていることを確認することによって、選択オブジェクトを特定するステップと、選択オブジェクトに関連する操作を行うステップとを含む。 According to yet another embodiment, a method for using a detector and a display to provide a graphical user interface for determining a user's intent based at least in part on the eye movements of one or both eyes of the user. Uses a detector to identify that one or both eyes of the user are directed to the first gaze object located at the first gaze object position on the display, and the preferred enlargement of the object displayed on the display. Using the center database, the steps to determine the preferred center position for magnifying all objects located within a given distance from the position of the first gaze object on the display, and using the detector, the first gaze While magnifying all objects located within a given distance from the object's position on the display, the tracking object uses a detection device and a step to identify that one or both eyes of the user follow the tracking object. Includes steps to identify the selected object by ensuring that it is being tracked at a given distance or at a given time during the expansion, and to perform operations related to the selected object.
さらに別の実施形態によれば、検出装置およびディスプレイを用いて、ユーザの片目または両目の眼球運動に少なくとも部分的に基づいて、ユーザの意図を決定するためのグラフィカルユーザインターフェイスを提供するための方法は、検出装置を用いて、ユーザの片目または両目がディスプレイ上の最初の注視オブジェクト位置に位置する最初の注視オブジェクトに向けられていることを特定するステップと、ディスプレイに表示されるオブジェクトの好ましい拡大中心のデータベースを用いて、最初の注視オブジェクトの位置から所定の距離内に位置する全てのオブジェクトをディスプレイ上で拡大するための好ましい中心位置を決定するステップと、検出装置を用いて、最初の注視オブジェクトの位置から所定の距離内に位置する全てのオブジェクトをディスプレイ上で拡大する間に、ユーザの片目または両目が追跡オブジェクトに追従することを特定するステップと、検出装置を用いて、追跡オブジェクトが拡大の間に所定の距離または所定の時間で追跡されていることを確認することによって、選択オブジェクトを特定するステップと、選択オブジェクトに関連する操作を行うステップとを含む。 According to yet another embodiment, a method for using a detector and a display to provide a graphical user interface for determining a user's intent based at least in part on the eye movements of one or both eyes of the user. Uses a detector to identify that one or both eyes of the user are directed to the first gaze object located at the first gaze object position on the display, and the preferred enlargement of the object displayed on the display. Using the center database, the steps to determine the preferred center position for magnifying all objects located within a given distance from the position of the first gaze object on the display, and using the detector, the first gaze While magnifying all objects located within a given distance from the object's position on the display, the tracking object uses a detection device and a step to identify that one or both eyes of the user follow the tracking object. Includes steps to identify the selected object by ensuring that it is being tracked at a given distance or at a given time during the expansion, and to perform operations related to the selected object.
別の実施形態によれば、検出装置、ディスプレイおよびキーボードを用いて、ユーザの片目または両目の眼球運動に少なくとも部分的に基づいて、ユーザの意図を決定するためのグラフィカルユーザインターフェイスを提供するための方法は、検出装置を用いて、ユーザの片目または両目が複数の選択可能なターゲットを含むディスプレイ上の領域に向けられていることを特定するステップと、各々が１つ以上の選択可能なターゲットに関連する複数のキーボード記号をディスプレイ上に重ね合わせるステップと、キーボードを用いて、ユーザが１つ以上の選択可能なターゲットに関連する１つのキーボード記号に対応するキーボード上のキーを選択したことを確認することによって、１つ以上の選択ターゲットを特定するステップと、１つ以上の選択ターゲットに関連する操作を行うステップとを含む。 According to another embodiment, a detector, display and keyboard are used to provide a graphical user interface for determining a user's intent based at least in part on the eye movements of one or both eyes of the user. The method uses a detector to identify that one or both eyes of the user are directed to an area on the display that contains multiple selectable targets, each with one or more selectable targets. Steps to overlay multiple related keyboard symbols on the display and use the keyboard to verify that the user has selected a key on the keyboard that corresponds to one keyboard symbol associated with one or more selectable targets. By doing so, it includes a step of identifying one or more selected targets and a step of performing an operation related to the one or more selected targets.
さらに別の実施形態によれば、検出装置、ディスプレイおよびマイクロフォンを用いて、ユーザの片目または両目の眼球運動に少なくとも部分的に基づいて、ユーザの意図を決定するためのグラフィカルユーザインターフェイスを提供するための方法は、検出装置を用いて、ユーザの片目または両目が複数の選択可能なターゲットを含むディスプレイ上の領域に向けられていることを特定するステップと、各々が１つ以上の選択可能なターゲットに関連する複数の記号をディスプレイ上に重ね合わせるステップと、マイクロフォンおよび音声テンプレートのデータベースを用いて、ユーザが１つ以上の選択可能なターゲットに関連する１つの記号に対応する音声を生成しことを確認することによって、１つ以上の選択ターゲットを特定するステップと、１つ以上の選択ターゲットに関連する操作を行うステップとを含む。 According to yet another embodiment, the detector, display and microphone are used to provide a graphical user interface for determining the user's intent based at least in part on the eye movements of the user's one or both eyes. The method uses a detector to identify that one or both eyes of the user are directed to an area on the display that contains multiple selectable targets, each of which has one or more selectable targets. Using a database of microphones and voice templates with the step of overlaying multiple symbols related to on the display, the user can generate a voice corresponding to one symbol associated with one or more selectable targets. By confirming, it includes a step of identifying one or more selected targets and a step of performing an operation related to the one or more selected targets.
さらに別の実施形態によれば、検出装置およびディスプレイを用いて、ユーザの意図を伝えるための注視方向を決定するための方法は、検出装置を用いて、ディスプレイ上の目標オブジェクト位置に位置する目標オブジェクトを見ているユーザの右目の注視位置を特定するステップと、検出装置を用いて、ディスプレイ上の目標オブジェクトを見ているユーザの左目の注視位置を特定するステップと、左目の注視位置および右目の注視位置のうち、目標オブジェクト位置に近い目の注視位置を決定するステップと、目標オブジェクト位置に近い目の注視位置の決定に基づいて、目標オブジェクト位置の所定の範囲内に、優位眼の範囲を割り当てるステップとを含む。 According to yet another embodiment, the method for determining the gaze direction for communicating the user's intention using the detection device and the display is to use the detection device to determine the target located at the target object position on the display. The step of identifying the gaze position of the right eye of the user who is looking at the object, the step of identifying the gaze position of the left eye of the user who is looking at the target object on the display using the detection device, and the gaze position and right eye of the left eye. Within the predetermined range of the target object position, the range of the dominant eye is based on the step of determining the gaze position of the eye close to the target object position and the determination of the gaze position of the eye close to the target object position. Includes steps to assign.
別の実施形態によれば、検出装置およびディスプレイを用いて、ユーザの眼球運動に少なくとも部分的に基づいて、ユーザの意図を伝えるための注視方向を決定するための方法は、検出装置を用いて、ユーザの目が第１オブジェクト距離に現れる第１オブジェクトに向けられていることを特定するステップと、検出装置を用いて、ユーザの目が第１オブジェクト距離と異なる第２オブジェクト距離に現れる第２オブジェクトに向けられていることを特定するステップと、ディスプレイを用いて、ディスプレイ上の第２オブジェクトの位置およびサイズの一方または両方を修正するステップと、ユーザの目に見られる場合に第１オブジェクトと第２オブジェクトが最大限に重合することを確認するステップと、第１オブジェクトの位置および第２オブジェクトの位置に基づいて、ユーザの目の視軸を決定するステップとを含む。 According to another embodiment, a method of using a detector and a display to determine a gaze direction to convey a user's intent, at least in part, based on the user's eye movements, uses the detector. , The step of identifying that the user's eyes are directed at the first object that appears at the first object distance, and the second object that the user's eyes appear at a second object distance that is different from the first object distance, using the detector. The step of identifying that the object is aimed, the step of using the display to modify one or both of the position and size of the second object on the display, and the first object when visible to the user. It includes a step of confirming that the second object is maximally overlapped and a step of determining the visual axis of the user's eyes based on the position of the first object and the position of the second object.
さらに別の実施形態によれば、検出装置およびディスプレイを用いて、装置ユーザの片目または両目の注視位置に少なくとも部分的に基づいて、送信帯域幅内で画像をディスプレイに表示するための方法は、検出装置を用いて、ユーザの片目または両目がディスプレイ上の注視位置に向けられていることを特定するステップと、注視位置に基づいて、ディスプレイ上で、ユーザの視野内の中心窩視覚領域を特定するステップと、視認可能なオブジェクトのデータベースに基づいて、表示されたオブジェクトのうち、装置ユーザに対して関連性の高いオブジェクトを特定するステップと、ユーザの視野内の他のオブジェクトに比べて、中心窩視野に位置するオブジェクトおよび関連性の高いオブジェクトの一方または両方を高解像度でディスプレイにレンダリングするステップとを含む。 According to yet another embodiment, a method for displaying an image on a display within a transmission bandwidth using a detector and a display, at least partially based on the gaze position of one or both eyes of the device user. The detector is used to identify the central fossa visual area in the user's field of view on the display, based on the steps to identify that one or both eyes of the user are directed to the gaze position on the display. And the steps to identify the displayed objects that are more relevant to the device user, based on the database of visible objects, and the center compared to other objects in the user's field of view. Includes steps to render one or both of the objects located in the fossa field and the relevant objects on the display in high resolution.
さらに別の実施形態によれば、検出装置およびディスプレイを用いて、装置ユーザの片目または両目の注視位置に少なくとも部分的に基づいて、送信帯域幅内で画像をディスプレイ上で表示するための方法は、検出装置を用いて、ユーザの片目または両目がディスプレイ上の注視位置に向けられていることを特定するステップと、注視位置に基づいて、ディスプレイ上で、注視位置を囲む所定の領域内の中心窩視覚領域を特定するステップと、中心窩視野領域外のユーザの視野に基づいて、ディスプレイ上で、非中心窩視覚領域を特定するステップと、非中心窩領域に位置するオブジェクトを低色量でディスプレイにレンダリングするステップとを含む。 According to yet another embodiment, a method for displaying an image on a display within a transmission bandwidth using a detector and a display, at least partially based on the gaze position of one or both eyes of the device user. , A step of identifying that one or both eyes of the user are directed to a gaze position on the display using a detector, and a center within a predetermined area surrounding the gaze position on the display based on the gaze position. The step of identifying the fossa visual area and the step of identifying the non-central fossa visual area on the display based on the user's field of view outside the central fossa visual field, and the low color amount of objects located in the non-central fossa area Includes steps to render to the display.
さらに別の実施形態によれば、視線検出装置およびディスプレイを用いて、ユーザの片目または両目の注視位置に少なくとも部分的に基づいて、所定の送信帯域幅内で画像をディスプレイ上で表示するための方法は、視線検出装置を用いて、ユーザの片目または両目がディスプレイ上の注視位置に向けられていることを特定するステップと、注視位置に基づいて、ディスプレイ上で、ユーザの視野内の高解像度表示領域を特定するステップと、高解像度表示領域外のユーザの視界に位置するディスプレイ上の他のオブジェクトに比べて、ディスプレイ上の高解像度表示領域に位置するオブジェクトを高解像度でレンダリングするステップと、ユーザの片目または両目に到達する光量に基づいて、ディスプレイ上の高解像度表示領域のサイズおよび形状のうち一方を変更するステップとを含む。 According to yet another embodiment, the line-of-sight detector and display are used to display an image on the display within a predetermined transmission bandwidth, at least partially based on the gaze position of one or both eyes of the user. The method uses a gaze detector to identify that one or both eyes of the user are directed to a gaze position on the display, and a high resolution in the user's field of view on the display based on the gaze position. The step of identifying the display area and the step of rendering the object located in the high resolution display area on the display in higher resolution than the other objects on the display located in the user's view outside the high resolution display area. It includes changing one of the size and shape of a high resolution display area on the display based on the amount of light reaching the user's one or both eyes.
別の実施形態によれば、検出装置を用いて、ユーザの片目または両目の眼球運動に少なくとも部分的に基づいて、ユーザの意図を決定するための視線を判断するための方法は、検出装置を用いて、ユーザの片目または両目がオブジェクトの基準位置に対する第１注視位置に向けられていることを特定するステップと、オブジェクトに関連する顕著性マップを用いて、オブジェクトの基準位置に対する第１注視位置に基づいて、第１重み係数を決定するステップと、検出装置を用いて、ユーザの片目または両目がオブジェクトの基準位置に対する第２注視位置に向けられていることを特定するステップと、オブジェクトに関連する顕著性マップを用いて、オブジェクトの基準位置に対する第２注視位置に基づいて、第２重み係数を決定するステップと、第１注視位置に第１重み係数を乗算し、第２注視位置に第２重み係数を乗算することによって、加重平均注視位置を決定するステップとを含む。 According to another embodiment, a method of using a detector to determine a line of sight for determining a user's intent based at least in part on the eye movements of the user's one or both eyes is to use the detector. Use the steps to identify that one or both eyes of the user are oriented to the first gaze position with respect to the object's reference position, and the first gaze position with respect to the object's reference position using the saliency map associated with the object. To determine the first weighting factor based on, and to use the detector to identify that one or both eyes of the user are oriented to the second gaze position relative to the object's reference position, and related to the object. Using the saliency map to determine the second weighting factor based on the second gaze position with respect to the reference position of the object, the first gaze position is multiplied by the first weighting factor, and the second gaze position is the second. 2 Includes a step of determining a weighted average gaze position by multiplying by a weighting factor.
さらに別の実施形態によれば、検出装置およびディスプレイを用いて、ユーザの片目または両目の眼球運動に少なくとも部分的に基づいて、ユーザの意図を伝えるためのグラフィカルユーザインターフェイスを提供するための方法は、検出装置を用いて、ユーザの片目または両目がディスプレイ上の第１位置に向けられていることを特定するステップと、検出装置を用いて、第１位置から領域選択オブジェクトに向けられたユーザの片目または両目の第１衝動性動眼が領域選択オブジェクトの位置から所定の距離内に完了したことを確認するステップと、検出装置を用いて、ユーザの片目または両目がディスプレイ上の第２位置に向けられていることを特定するステップと、検出装置を用いて、第２位置から作動ターゲットに向けられたユーザの片目または両目の第２衝動性動眼が作動ターゲットの位置から所定の距離内に完了したことを確認するステップと、ディスプレイ上に投影されたオブジェクトに基づいて、第１位置および第２位置に隅を有する矩形領域によって囲まれた全てのオブジェクトを特定するステップと、第１位置および第２位置に隅を有する矩形領域によって囲まれたディスプレイ上の全てのオブジェクトについて、作動ターゲットに関連する操作を行うステップとを含む。 According to yet another embodiment, a method for using a detector and a display to provide a graphical user interface for communicating a user's intent based at least in part on the eye movements of one or both eyes of the user. , A step of using a detector to identify that one or both eyes of the user are directed to a first position on the display, and a detector that uses a detector to point the user's area selection object from the first position. A step of confirming that the first impulsive moving eye of one or both eyes is completed within a predetermined distance from the position of the region selection object and the detection device are used to point the user's one or both eyes to the second position on the display. The second impulsive eye of the user's one or both eyes directed from the second position to the actuating target was completed within a predetermined distance from the position of the actuating target using the step of identifying that and the detector. A step to confirm that, and a step to identify all objects surrounded by rectangular areas with corners in the first and second positions, based on the objects projected on the display, and the first and second positions. Includes steps to perform operations related to the actuation target for all objects on the display surrounded by a rectangular area with corners in position.
本発明の他の態様および特徴は、添付図面と併せて、以下の詳細な説明を考慮することによって、明らかになるであろう。 Other aspects and features of the invention will become apparent by considering the following detailed description in conjunction with the accompanying drawings.
本発明は、以下の例示的な図面に関連して詳細な説明を参照することによって、より完全に理解されるであろう。全ての図面において、同様の参照番号は、同様の要素または動作を示している。現在の例示的な実施形態は、添付図面に示されている。 The present invention will be more fully understood by reference to the detailed description in connection with the exemplary drawings below. In all drawings, similar reference numbers indicate similar elements or behaviors. Current exemplary embodiments are shown in the accompanying drawings.
例示的な実施形態の詳細な説明
視覚信号中のタイミングの考察
眼球運動を解読することによってユーザの意図を識別するために、アルゴリズムフィルタは、眼球の神経筋制御における生理制約（例えば、動きの範囲、最大速度、水平軸および垂直軸に沿った動きの差）と、急速眼球運動および瞬きに起因する視覚情報の認知処理における中断とを考慮する必要がある。視界の認知処理における中断は、衝動性動眼中に大きな動きボケなくまたは瞬き中に停止（すなわち、短期間の暗闇）なく、環境を観察するために必要である。人間は、衝動性動眼および瞬き中に網膜信号の視覚処理を抑制することによって、このような中断を回避する。これらの神経処理は、各々衝動性動眼抑制（saccadic suppression）および瞬き抑制（blink suppression）として呼ばれる。
Detailed Description of Illustrative Embodiments In order to identify the user's intent by decoding eye movements, algorithmic filters are used for physiological constraints in the control of the neuromuscular muscles of the eye (eg, the range of movement). , Maximum velocity, difference in movement along the horizontal and vertical axes) and interruptions in the cognitive processing of visual information due to rapid eye movements and blinking. Interruptions in the cognitive processing of visual field are necessary for observing the environment without major motion blur during impulsive motion or stop during blinking (ie, short-term darkness). Humans avoid such interruptions by suppressing the visual processing of retinal signals during impulsive oculomotor and blinking. These neural processes are called saccadic suppression and blink suppression, respectively.
衝動性動眼抑制（衝動性動眼マスキングとも呼ばれる）は、実際に、衝動性動眼が始まる前に開始する。この観察は、衝動性動眼抑制の少なくとも一部が中枢神経系によって媒介されるということを証明するための証拠として使用されている。衝動性動眼抑制は、衝動性動眼を含む急速眼球運動中に存在する動きボケに対する自覚を防止する。衝動性動眼抑制（ErdmannおよびDodgeによって１８９８年に初めて行われたその現象の科学観察）の簡単実証は、眼球運動をしている人が鏡で急速眼球運動を見ることができず、その人（または鏡）を見ている他の観察者が急速眼球運動をはっきりと見ることができるという事実を示している。 Impulsive oculomotor suppression (also called impulsive oculomotor masking) actually begins before the onset of impulsive occlusal eye. This observation has been used as evidence to prove that at least some of the impulsive oculomotor nerve suppression is mediated by the central nervous system. Impulsive oculomotor suppression prevents awareness of motion blurring present during rapid eye movements, including impulsive oculomotor eye movements. A brief demonstration of impulsive oculomotor inhibition (the first scientific observation of the phenomenon by Erdmann and Dodge in 1898) is that a person doing eye movements cannot see rapid eye movements in the mirror, and that person ( It shows the fact that other observers looking at the mirror) can clearly see the rapid eye movements.
衝動性動眼抑制は、一般的に１００〜１５０ミリ秒持続し、急速眼球運動が停止すると、直ちに（例えば、数ミリ秒内に）終了する。衝動性動眼抑制は、通常、衝動性動眼が終了した時に停止する、または人工条件下目に投影された画像が動きを示さない時に停止する。視覚シーンの移動中に急速停止抑制の実証は、人が急速に移動する乗り物から外を見るときに、シーンの移動速度を（衝動性）眼球運動に簡単に合わせることによって、オブジェクトを視認する能力である。静止画像が網膜に数ミリ秒間投影されても、急速に移動するシーンを視認することができる。これらの観察は、（中枢神経系と異なる）網膜レベルの神経処理が衝動性動眼抑制の制御または維持に関与することを示唆している。 Impulsive oculomotor suppression generally lasts 100 to 150 milliseconds and ends immediately (eg, within a few milliseconds) when rapid eye movement ceases. Impulsive oculomotor suppression usually ceases when the impulsive occlusal eye ends, or when the image projected on the eye under artificial conditions shows no movement. Demonstration of rapid stop suppression during movement of a visual scene is the ability to see objects by easily adjusting the speed of movement of the scene to (impulsive) eye movements when a person looks out of a rapidly moving vehicle. Is. Even if a still image is projected onto the retina for a few milliseconds, a rapidly moving scene can be visually recognized. These observations suggest that retinal-level neural processing (as opposed to the central nervous system) is involved in the control or maintenance of impulsive oculomotor nerve suppression.
視線が目標位置に到達してから約５０〜６０ミリ秒という短い期間の間に、瞳孔、水晶体および角膜輪部を含む眼内構造は、減衰振動の形で続けて動く。脳によって処理された場合、特に水晶体の動きは、網膜に到達する画像に動きボケを引き起こす。このような動きボケを避けるために、衝動性動眼抑制は、眼内構造の減衰振動中に継続的に実行される。 During the short period of about 50-60 ms after the line of sight reaches the target position, the intraocular structure, including the pupil, lens and corneal ring, continues to move in the form of damped vibrations. When processed by the brain, the movement of the crystalline lens in particular causes motion blur in the image reaching the retina. To avoid such motion blur, impulsive oculomotor suppression is continuously performed during damped oscillations of the intraocular structure.
瞬き抑制は、上眼瞼が瞳孔を覆って、殆どの光線が網膜に届かないようにするときに、「暗闇」に対する認知を抑える。瞬き抑制は、瞬きが自発的であるかまたは非自発的（すなわち、自動的または反射的）であるかに拘わらず発生し、一般的に両眼に同時に発生する。瞬き抑制は、眼瞼の遮光機能をバイパスして光を網膜に人工的に導入するときにも発生する。 Blink suppression suppresses perception of "darkness" when the upper eyelid covers the pupil and prevents most light rays from reaching the retina. Blink suppression occurs regardless of whether the blink is spontaneous or involuntary (ie, automatic or reflexive) and generally occurs simultaneously in both eyes. Blink suppression also occurs when light is artificially introduced into the retina by bypassing the light-shielding function of the eyelids.
瞬き抑制は、瞬きの前に始まり、２００〜２５０ミリ秒間持続し、上眼瞼が瞳孔を覆わなくなると終了する。上眼瞼の最大速度は、方向によって異なり、下方方向の場合７００°／秒に達し、上方方向の場合２０００°／秒に達する。眠気などの要因は、眼瞼の最大速度および眼瞼の閉じ期間に影響を与える。 Blink suppression begins before blinking, lasts for 200-250 milliseconds, and ends when the upper eyelid no longer covers the pupil. The maximum velocity of the upper eyelid varies depending on the direction and reaches 700 ° / sec in the downward direction and 2000 ° / sec in the upward direction. Factors such as drowsiness affect the maximum speed of the eyelids and the duration of eyelid closure.
人間は、平均で１５〜２０回／分の瞬きをする。上述した瞬き抑制の持続時間を用いて推論すると、瞬きによって、人間は約５〜８％の時間に機能的に「失明」する。類似の推論によって、典型的な条件下、人間は１分間に約４０回の衝動性動眼を行う。上述した衝動性動眼抑制の持続時間を用いて推論すると、衝動性動眼抑制によって、人間は約７〜１０％の時間に機能的に「失明」する。これらの抑制の総合効果によって、典型的な人間は、約１５％の時間に機能的に「失明」する。この値は、「白日夢」と呼ばれている人間の認知負荷、疲労度、注意力、ストレス、薬物治療、照明、および行われている作業（例えば、読書、会話）を含む多くの要因に依存して変化し得る。いくつかの用途において特に関連深いことは、表示装置を見ているときに、瞬き速度が実質的に減少する（最大５倍）ことである。 Humans blink 15 to 20 times / minute on average. Inferring using the duration of blink suppression described above, blinking causes humans to functionally "blind" in about 5-8% of the time. By similar reasoning, under typical conditions, humans perform about 40 impulsive oculomotor eyes per minute. Inferring using the duration of impulsive oculomotor suppression described above, impulsive oculomotor suppression causes humans to functionally "blind" in about 7-10% of the time. Due to the combined effect of these suppressions, a typical human is functionally "blind" in about 15% of the time. This value includes many factors, called "daydreaming", including human cognitive load, fatigue, attention, stress, medication, lighting, and the work being done (eg, reading, conversation). Can change depending on. Of particular relevance in some applications is the substantial reduction in blink speed (up to 5 times) when looking at the display device.
人間の瞬きと衝動性動眼とは相互に影響する。衝動性動眼が瞬きの直後（例えば、最大２００ミリ秒）に発生する場合、衝動性動眼の加速度、ピーク速度および減速度が減少する。一方、瞬きが衝動性動眼の後に発生する場合、瞬きの動力学が著しく影響されない。最大効果は、瞬きが約１００ミリ秒で衝動性動眼に先行する場合に生じる。その結果、衝動性動眼の持続時間は、衝動性動眼距離に基づいて計算された持続時間よりも約３分の１に長くなる。したがって、眼球運動を衝動性動眼に分類するための閾値速度は、３０°／秒を超える典型的な（すなわち、瞬きに続いていない）瞬き閾値から、２０°／秒を超える瞬き後閾値まで減らすことができる。 Human blinking and impulsive oculomotor influence each other. If the impulsive occlusal eye occurs immediately after blinking (eg, up to 200 ms), the acceleration, peak velocity and deceleration of the impulsive occlusal eye are reduced. On the other hand, if the blink occurs after the impulsive googlym, the dynamics of the blink are not significantly affected. The maximum effect occurs when the blink precedes the impulsive googlymium in about 100 ms. As a result, the duration of the impulsive oculomotor eye is about one-third longer than the duration calculated based on the impulsive occlusal eye distance. Therefore, the threshold speed for classifying eye movements as impulsive oculomotor is reduced from a typical (ie, not following blink) blink threshold above 30 ° / sec to a post-blink threshold greater than 20 ° / sec. be able to.
瞬きは、その後の両眼離反運動に影響を与えることもできる。少なくとも一部の影響は、両眼離反運動中に小さな衝動性動眼に与えたものである。瞬きが衝動性動眼に影響を与える１つの可能性は、瞬き中に共有された運動前野神経回路を抑制することによって、衝動性動眼を開始するプロセスに影響を与えることである。この共有された神経回路は、凝視によって誘発された瞬きに役割を果たすこともできる。人間および他の哺乳類では、長い時間の凝視は、瞬きを引き起こす可能性を約２０％に増加する。 Blinking can also affect subsequent binocular detachment movements. At least some of the effects were on small impulsive oculomotors during binocular detachment. One possibility that blink affects the oculomotor nerve is to affect the process of initiating the oculomotor nerve by suppressing the shared premotor neural circuits during the blink. This shared neural circuit can also play a role in gaze-induced blinks. In humans and other mammals, long-term gaze increases the likelihood of causing blinking by about 20%.
例示的な実施形態において、視覚信号の伝達中に、機能的な失明は、ユーザの反応時間の期待値に影響を与える可能性がある。眼球動力学の中断および変化は、視覚信号の選択基準を考慮する必要がある。眼球動力学の中断および変化は、瞬きの発生が一般的に視覚信号の形成と同期されていないときに発生する。さらに、矯正的な衝動性動眼などの急速眼球運動は、いくつかの視覚信号中に点在する。さらに、衝動性動眼の前に発生した瞬きは、衝動性動眼の動力学を変更する。したがって、目による対話の性能を最適化するために、ユーザの認知（ユーザによって使用される場合）、選択プロセス、および／または制御された表示オブジェクトの出現または消失のタイミングを制御するときに、瞬き期間および他の機能的な失明期間を検出し、考慮しなければならない。 In an exemplary embodiment, during the transmission of a visual signal, functional blindness can affect the expected value of the user's reaction time. Discontinuities and changes in ocular dynamics require consideration of visual signal selection criteria. Disruptions and changes in ocular dynamics occur when the occurrence of blinks is generally out of sync with the formation of visual signals. In addition, rapid eye movements such as corrective impulsive oculomotor eye are interspersed in some visual signals. In addition, the blink that occurs before the impulsive occlusal eye modifies the dynamics of the impulsive oculomotor eye. Therefore, blinking when controlling the user's perception (when used by the user), the selection process, and / or the timing of the appearance or disappearance of controlled display objects in order to optimize the performance of visual interaction. Periods and other functional blindness periods must be detected and considered.
具体的な例として、一連の時間間隔の短い衝動性動眼を伴う作動シーケンスの間に注意を引くように衝動性動眼の終了時に直ちにオブジェクトを導入する場合、オブジェクトの導入タイミングに瞬きの有無を考慮しなければならない。瞬きが衝動性動眼の直前に発生した場合、衝動性動眼の追加の持続時間（すなわち、衝動性動眼距離に基づいて計算された時間を超えた部分）によって、新規導入されたオブジェクトの表示を遅らせる必要がある。また、瞬き後の衝動性動眼の持続時間の増加を例えば、異なる作動シーケンスの要素となり得る滑動性追跡眼球運動、前庭性眼球運動または固視のいずれかとして誤って解釈しないことも重要である。 As a specific example, if an object is introduced immediately at the end of the impulsive gonem so as to draw attention during an operation sequence involving a series of short time intervals of the impulsive eye, the presence or absence of blinking should be considered when the object is introduced. Must. If the blink occurs just before the oculomotor eye, the additional duration of the impulsive occlusal eye (ie, the portion beyond the time calculated based on the impulsive oculomotor distance) delays the display of the newly introduced object. There is a need. It is also important not to misinterpret the increase in the duration of the impulsive oculomotor eye after blinking as either gliding pursuit eye movement, vestibular eye movement or fixation, which can be elements of different operating sequences, for example.
一般的に、イベントのタイミングの制御または測定に基づく視覚信号は、衝動性動眼抑制および瞬き抑制中に視覚処理の欠如を考慮しなければならない。これらは、短時間に示されたメニューの選択、ユーザの決定を行う時間、反応時間の測定、潜意識的な眼球運動を刺激または回避するように刺激の提供、ギャップ効果を生成するようにターゲットを除去するタイミング、変化失明（change blindness）を利用するように画像を導入または変更するタイミング、シーン内の要素を変更するための視覚抑制を利用した講談師をサポートするツールの実装などを含む。 In general, visual signals based on control or measurement of event timing must take into account the lack of visual processing during impulsive oculomotor and blink suppression. They target short-lived menu selections, time to make user decisions, measure reaction times, provide stimuli to stimulate or avoid latent eye movements, and generate gap effects. Includes timing of removal, timing of introducing or changing images to take advantage of change blindness, and implementation of tools to support talkers using visual suppression to change elements in the scene.
別の考えとして、衝動性動眼は、画像変位の衝動性動眼抑制および／または所謂「ブランク効果」に関与する。画像変位の衝動性動眼抑制とは、衝動性動眼中に、視覚系に認識されないように、ターゲット位置を一定量で移動する認知処理である。認知されずにオブジェクトを移動できる範囲は、最大２°であり、概ね中心窩視野の標準サイズである。認知処理は、短期的な視覚記憶および／またはトランス衝動性動眼記憶に関与すると考えられている。 As another idea, the impulsive oculomotor is involved in the impulsive oculomotor suppression and / or the so-called "blank effect" of image displacement. Impulsive oculomotor suppression of image displacement is a cognitive process that moves a target position by a fixed amount so that it is not recognized by the visual system during impulsive oculomotor eye. The maximum range in which an object can be moved without being recognized is 2 °, which is approximately the standard size of the foveal field of view. Cognitive processing is thought to be involved in short-term visual memory and / or transimpulsive oculomotor memory.
興味深いことは、オブジェクトを再導入する前の５０〜３００ミリ秒に、（目標オブジェクトが存在しない）「ブランク」フィールドを挿入することによって、オブジェクトの移動を検出する能力を改善できることである。衝動性動眼の後に検出されたオブジェクトの位置に少量の「スロップ」を許すことによって、予想されたターゲット位置と実際のターゲット位置との間の差から起因する動きの認知を回避できると考えられる。 Interestingly, the ability to detect object movement can be improved by inserting a "blank" field (where the target object does not exist) 50-300 milliseconds before reintroducing the object. By allowing a small amount of "slop" in the position of the object detected after the impulsive occlusal eye, it is thought that the perception of movement due to the difference between the expected target position and the actual target position can be avoided.
例示的な実施形態において、視覚信号を伝達する間に、ブランク効果を利用して、衝動性動眼中に、（必要に応じて）注意を引くことなく、目標領域に位置する１つ以上のオブジェクトの位置を変位させることができる。逆に、簡単なブランクフィールドの挿入を用いて、ユーザの視線をターゲット位置にある動いているオブジェクトに「誘導」することを支援することができる。この動いているオブジェクトは、例えば、１つ以上のオブジェクトを表示させることによって、ユーザの滑動性追跡眼球運動に基づいた意図的な追跡によって検知されたＮ個のオブジェクト（Ｎは、通常０よりも大きい正整数である）から１つのオブジェクトを選択する初期段階にあってもよい。 In an exemplary embodiment, one or more objects located in the target area without drawing attention (if necessary) during the impulsive eye movement, utilizing the blank effect while transmitting the visual signal. The position of can be displaced. Conversely, a simple blank field insertion can be used to help "guide" the user's line of sight to a moving object at the target location. This moving object is N objects (N is usually greater than 0) detected by intentional tracking based on the user's gliding tracking eye movements, for example by displaying one or more objects. It may be in the initial stage of selecting one object from (which is a large positive integer).
衝動性動眼の変動性
短い距離の衝動性動眼は、ターゲットをオーバーシュートし、（約５°を超える）長い距離の衝動性動眼は、ターゲットをアンダーシュートする傾向がある。通常、長い距離の衝動性動眼は、ターゲット距離の９０％をカバーし、矯正的な衝動性動眼は、１０％をカバーする。求心性衝動性動眼は、遠心性衝動性動眼より精確である傾向がある。アンダーシュートまたはオーバーシュートの後に行われた矯正的な衝動性動眼は、長い待機時間または短い待機時間で待機することがある。矯正的な衝動性動眼を迅速に実行することができ（すなわち、動的アンダーシュートまたはオーバーシュート）、または数百ミリ秒で待機させることもできる（すなわち、滑降アンダーシュートまたはオーバーシュート）。ターゲットの特性、主に輝度は、矯正的な衝動性動眼の待機時間に影響を与えることができる。輝度が中心窩閾値を下回る場合、待機時間は、大幅に増加することがある。
Vulnerability of Impulsive Googlymium Short-distance impulsive gonadly eyes tend to overshoot the target, and long-distance impulsive googlymia (> about 5 °) tend to undershoot the target. Usually, a long-distance impulsive eye will cover 90% of the target distance, and a corrective impulsive eye will cover 10%. The afferent impulsive occlusal eye tends to be more accurate than the efferent impulsive occult eye. Corrective impulsive eye movements performed after an undershoot or overshoot may wait with a long or short wait time. Corrective impulsive googlymia can be performed quickly (ie, dynamic undershoot or overshoot), or it can be kept waiting for hundreds of milliseconds (ie, downhill undershoot or overshoot). Target characteristics, primarily brightness, can affect the waiting time of the corrective impulsive oculomotor eye. If the brightness is below the foveal threshold, the waiting time may increase significantly.
目が暗闇の中に非可視ターゲットに向けられた場合または目に特定の病変を有する場合、衝動性動眼は、遅くなる傾向がある。衝動性動眼の持続時間は、衝動性動眼のターゲットおよびシーケンス以外の視覚刺激によって影響されることもある。 Impulsive googlymia tends to be slow when the eye is aimed at an invisible target in the dark or has specific lesions in the eye. The duration of the impulsive occlusal eye may also be influenced by visual stimuli other than the target and sequence of the impulsive occlusal eye.
まとめると、例示的な実施形態において、指向性衝動性動眼の有無を検出するためのアルゴリズムフィルタは、上述した遅延および変動性を考慮する必要がある。また、輝度を制御することによって、（矯正的な衝動性動眼を含む）衝動性動眼および他の形態の眼球運動の速度を最適化することができる。全体的な輝度が制御されていない場合に、アルゴリズムフィルタは、一連の衝動性動眼を特定する時に、輝度の上昇または低下による全般的な影響を考慮する必要がある。 In summary, in an exemplary embodiment, the algorithmic filter for detecting the presence or absence of directional impulsive occlusal eye needs to take into account the delay and variability described above. Also, by controlling the brightness, the speed of impulsive oculomotor eyes (including corrective impulsive occlusal eyes) and other forms of eye movement can be optimized. When the overall brightness is not controlled, the algorithm filter needs to consider the overall effect of increasing or decreasing the brightness when identifying a series of impulsive oculomotor eyes.
以下の記載において、例えば、「デュアル」衝動性動眼または「シングル」衝動性動眼を言及する場合、このような意図的な衝動性動眼を他の衝動性動眼を含む他の眼球運動から認識および区別しなければならない。これらの介在性の眼球運動は、１つ以上の（一般的に短距離の）矯正的な衝動性動眼、瞬き、振顫および浮動などを含む。 In the following description, for example, when referring to a "dual" impulsive oculomotor eye or a "single" impulsive occlusal eye, the recognition and distinction of such an intentional impulsive occlusal eye from other eye movements including other impulsive occlusal eyes. Must. These intervening eye movements include one or more (generally short distance) corrective impulsive oculomotor eyes, blinking, swaying and floating.
また、上述したように、衝動性動眼中に視線が目標位置に到着した場合、瞳孔および（一般的により狭い範囲で）角膜輪部は、減衰振動のような変位を示すことができる。これらの振動の一次振動数は、一般的に約２０Ｈｚ（すなわち、５０ミリ秒の周期）である。一般的には、約５０〜６０ミリ秒の間にこの減衰振動を検出することができるため、１〜３個の減衰振動を観測することができる。衝動性動眼抑制は、この期間中も持続する。さもなければ、観察されたシーンに動きボケが現れるであろう。 Also, as mentioned above, when the line of sight arrives at the target position during impulsive eye movement, the pupil and (generally in a narrower range) the corneal ring can exhibit a displacement such as damped vibration. The primary frequency of these vibrations is generally about 20 Hz (ie, a period of 50 milliseconds). Generally, since this damped vibration can be detected within about 50 to 60 milliseconds, 1 to 3 damped vibrations can be observed. Impulsive oculomotor suppression persists during this period. Otherwise, motion blur will appear in the observed scene.
認知を行わない衝動性動眼
衝動性動眼中に視線が目標位置に到着した後に生じた減衰振動を含み、衝動性動眼抑制中に、目標オブジェクトの認知が不可能である。したがって、例示的な実施形態において、意図を伝えるように設計された衝動性動眼において、目標オブジェクトの認知が意図を伝えるための必須要素ではない場合、減衰振動が発生している時間が意図を伝えるための必須要素ではない。換言すれば、認知を必要とする場合、認知時間（２００〜２５０ミリ秒）並びに衝動性動眼中に視線が目標位置に到着した後にユーザが機能的に失明になる減衰振動時間（５０〜６０ミリ秒）は、意図を伝えるための意図的な眼球運動の速度を制限する。認知が必要ではない場合、両方の期間も必要しない。
Impulsive oculomotor eye without cognition Including damped vibration generated after the line of sight arrives at the target position during impulsive oculomotor eye, recognition of the target object is impossible during impulsive oculomotor eye suppression. Therefore, in an exemplary embodiment, in an impulsive googlymium designed to convey an intention, if the cognition of the target object is not an essential element to convey the intention, the time during which the damped vibration occurs conveys the intention. Not an essential element for. In other words, if cognition is required, cognitive time (200-250 ms) and damped vibration time (50-60 ms) that causes the user to be functionally blind after the line of sight reaches the target position during impulsive motion. Seconds) limits the speed of intentional eye movements to convey intent. If cognition is not required, neither period is required.
例示的な実施形態において、視覚信号言語（eye signal language）において、経験豊富なユーザは、衝動性動眼シーケンスのターゲットであるオブジェクトの各々または全てを完全に認識することなく、意図を伝えるように設計された多くの衝動性動眼を行うことができる。ユーザの経験によって認知を行われなかった場合、衝動性動眼間の間隔を大幅に短縮することができ、１つ以上のユーザの意図をより迅速に伝達することができる。 In an exemplary embodiment, in the eye signal language, an experienced user is designed to convey intent without fully recognizing each or all of the objects that are the target of the impulsive oculomotor sequence. Many impulsive occult eyes can be performed. If cognition is not performed due to the user's experience, the distance between the impulsive oculomotor eyes can be significantly shortened, and the intention of one or more users can be transmitted more quickly.
経験豊富なユーザは、一般的に、記憶誘導型衝動性動眼により多く依存することによって、意図を伝える。可視ターゲットは、一般的に記憶誘導型衝動性動眼にとって必要であるが、ターゲットを完全に検査する（すなわち、完全に認知する）必要はない。また、選択オブジェクトは、一般的に傍中心窩視界または周辺視界に位置する。以下の「高関連性オブジェクトの高解像度レンダリング」部分に説明するように、一般的には、オブジェクトの詳細を完全に認識しなくても、オブジェクトの位置を十分に認識することができる。 Experienced users generally convey intent by relying more on memory-guided impulsive oculomotor eyes. Visible targets are generally required for memory-guided impulsive oculomotor eyes, but do not require complete examination (ie, full recognition) of the target. Also, the selected object is generally located in the parafoveal or peripheral field of view. In general, the position of an object can be fully recognized without fully recognizing the details of the object, as described in the "High Resolution Rendering of Highly Relevant Objects" section below.
例示的な実施形態において、オブジェクトを完全に認知することなく、第１オブジェクトに対する衝動性動眼を行うことができる。したがって、第１オブジェクトを完全に認知することなく、任意の時間で、操作を行うことができ、および／または第２オブジェクトに対する次の衝動性動眼を行うことができる。衝動性動眼の弾道学プロファイルに基づいて衝動性動眼の注視位置を決定した直後に、第１オブジェクトに向かった移動に関連する操作を行うことができる。したがって、次の「意図的な」（すなわち、意図を伝えるための）眼球運動（または意図を伝えるための他のモダリティ）を意識的に認識する前に、１つ以上の操作を行うことができる。 In an exemplary embodiment, the impulsive eye movement for the first object can be performed without fully recognizing the object. Therefore, the operation can be performed at any time without fully recognizing the first object, and / or the next impulsive eye movement for the second object can be performed. Immediately after determining the gaze position of the impulsive oculomotor eye based on the ballistic profile of the impulsive oculomotor eye, operations related to the movement toward the first object can be performed. Therefore, one or more operations can be performed before consciously recognizing the next "intentional" (ie, other modality to convey the intention) eye movement (or other modality to convey the intention). ..
認知される前に第１オブジェクトの表示を取り消すこともできる。これによって、所謂「ギャップ効果」を生成する。本明細書の他の箇所に記載されているように、ギャップ効果は、第１オブジェクトを観察している（または完全に認知している）目を「解放」し、別のオブジェクトへの急速な衝動性動眼を促進することができる。 It is also possible to cancel the display of the first object before it is recognized. This creates the so-called "gap effect". As described elsewhere herein, the gap effect "releases" the eye observing (or fully recognizing) the first object and rapidly to another object. Impulsive oculomotor eye can be promoted.
デュアル衝動性動眼選択シーケンス（ＤＳＳＳ）による複数の選択（例えば、キーボード）
前述したように、視覚信号言語において、滑動性追跡眼球運動を用いて、特定の追跡オブジェクトを視覚的に追跡することによって、Ｎ個のオブジェクトから１つのオブジェクトの選択を行うことができる。このメカニズムによって、限られた選択数（典型的には２〜８）で、比較的少数の逐次選択シーケンス（典型的には１から数個までのシーケンス）を行う場合、視覚信号の形成中に、特定の眼球運動の「流れ」が特に効果的である。シーケンスおよび／または選択の数の最大値は、滑動性追跡眼球運動に基づくメカニズムの使用を制限するものではなく、快適で効率的な使用に関する提案である。
Multiple selections by dual impulsive oculomotor selection sequence (DSSS) (eg keyboard)
As described above, in the visual signal language, one object can be selected from N objects by visually tracking a specific tracking object by using the sliding tracking eye movement. By this mechanism, when performing a relatively small number of sequential selection sequences (typically one to several sequences) with a limited number of selections (typically 2-8), during the formation of the visual signal. , The "flow" of certain eye movements is particularly effective. The maximum number of sequences and / or choices does not limit the use of gliding tracking eye movement-based mechanisms and is a suggestion for comfortable and efficient use.
選択数Ｎが大きくなると、急速な衝動性動眼を用いて選択を行うことによって、生理学的な効率が良くなる（すなわち、より少ない時間および労力を要する）。大規模なＮ選択プロセスの代表的且つ特に有用な例は、タイピングである。英語では、タイピングは、２６個の文字、１０個の数字、および／またはいくつかの他の「特殊」記号（例えば、「＄」、「＠」、「＃」など）、句読点（例えば、「、」、「。」、「？」など）および機能／コマンド（例えば、大文字キー、シフトキーおよびコントロールキー、入力の終了など）を含むことができる。 As the number of selections N increases, the physiological efficiency improves (ie, requires less time and effort) by making selections with a rapid impulsive googlymium. A representative and particularly useful example of a large-scale N-selection process is typing. In English, typing is 26 letters, 10 numbers, and / or some other "special" symbols (eg, "$", "@", "#", etc.), punctuation marks (eg, "". , ”,“. ”,“? ”, Etc.) and functions / commands (eg, uppercase keys, shift and control keys, end of typing, etc.) can be included.
以下の記載において、「キーボード」という用語は、入力に一般的に使用された標準的なキーボード（例えば、所謂ＱＷＥＲＴＹ）、代替的な入力配置を有するキーボード、数値入力に一般的に使用されたキーパッド、または英数字（英字および／または数字）、句読点、方向矢印、所謂特殊文字、画像、機能キー、描画機能、テキスト編集機能、ショートカット（すなわち、機能の集合）などを含み得る他の選択マトリックスを含むことができる。曜日、月、時間の所定の増分（例えば、１５分ごと）、国の州または地域、色、フォントおよび／またはサイズの選択、連絡先、ソフトウェアアプリケーション、機械制御機能などを含む可能な選択の特定セットに適合するように、キーボードを改変することができる。キーボードまたは選択マトリックスは、実在物（すなわち、物理的なオブジェクト）または仮想物（すなわち、ディスプレイの一部の投影）であってもよく、実在キーと仮想キーの組み合わせ（中央ディスプレイ装置上に投影されたキーおよび表示領域外に配置された追加の物理キー）であってもよい。 In the following description, the term "keyboard" refers to a standard keyboard commonly used for input (eg, so-called QWERTY), a keyboard with an alternative input arrangement, and keys commonly used for numerical input. Pads, or other selection matrices that can contain alphanumeric characters (alphanumeric characters and / or numbers), punctuation marks, direction arrows, so-called special characters, images, function keys, drawing functions, text editing functions, shortcuts (ie, sets of functions), etc. Can be included. Identifying possible selections, including day, month, given increments of time (eg, every 15 minutes), country or region, color, font and / or size selection, contacts, software applications, machine control features, etc. The keyboard can be modified to fit the set. The keyboard or selection matrix may be a real object (ie, a physical object) or a virtual object (ie, a projection of a portion of the display), a combination of real and virtual keys (projected onto a central display device). Keys and additional physical keys located outside the display area).
また、「キーボード」（すなわち、選択マトリックス）内の個々のキーは、より高レベルの思想および／または概念を表すことができる。キーボードの選択は、単語、語句、または長文を表すことがある。キーは、中国語の記号／意味に関連するアイコンまたは図形を含んでもよい。また、キーは、特定の言語に関連していない共通物を含む静的絵標識または動的（すなわち、アニメーション化）絵文字を含んでもよい。記号、画像および／または絵文字は、以前に入力されたキー、特定のユーザ、時間、閲覧時のみ、および／または他の文脈要素によって、時間と共に変化してもよい。 Also, individual keys within the "keyboard" (ie, the selection matrix) can represent higher levels of thought and / or concepts. Keyboard choices may represent words, phrases, or long sentences. The key may include an icon or graphic associated with a Chinese symbol / meaning. The key may also include static pictograms or dynamic (ie, animated) pictograms that contain commonalities that are not related to a particular language. Symbols, images and / or pictograms may change over time due to previously entered keys, specific users, time, browsing only, and / or other contextual elements.
眼球運動によって入力を行うときの優先制約は、急速眼球運動中に、（例えば、長くなった滞在期間の平均化または注視領域の「ズームイン」などの統計手法を使用せず）視線ベクトルを決定する精度である。（振顫、マイクロ衝動性動眼、浮動、および他の生物学的態様の存在を含む）システムの注視精度は、衝動性動眼を使用する選択プロセス中に指し示すことができる別個の領域または区域の数を制限する。高度に区切られた表示領域に対するこの制限を克服するための方法として、複数の衝動性動眼を用いて、一連の選択を各々行うことである。この方法は、全体的な選択グループのより小さいサブセットからより小さな枝を選択することができる「選択木」（selection tree）として考えられてもよい。 The priority constraint when inputting by eye movement determines the line-of-sight vector during rapid eye movement (eg, without using statistical techniques such as averaging longer stays or "zooming in" the gaze area). Accuracy. The gaze accuracy of the system (including the presence of tremor, microimpulsive oculomotor, floating, and other biological aspects) is the number of separate regions or areas that can be pointed to during the selection process using the impulsive occlusal eye. To limit. A way to overcome this limitation on highly partitioned display areas is to make a series of selections, each with multiple impulsive oculomotor eyes. This method may be thought of as a "selection tree" that allows smaller branches to be selected from a smaller subset of the overall selection group.
特に最初の衝動性動眼に応じて選択肢を動的に変化する方法を用いて、３つ以上の衝動性動眼を連続的に行うことによって、選択を指定することができる。しかしながら、これは、一般的に非常に大きなＮ（例えばＮ＞５０）の場合のみ必要である。したがって、デュアル衝動性動眼選択シーケンス（ＤＳＳＳ）使用が最も簡単である。以下、デュアル衝動性動眼選択シーケンスを詳細に記載する。３つ以上の衝動性動眼は、同様の一般原則を用いて、各々の選択を行うことができる。 The choice can be specified by continuously performing three or more impulsive oculomotors, in particular using a method that dynamically changes the choices in response to the first impulsive occlusal eye. However, this is generally only necessary for very large N (eg N> 50). Therefore, it is easiest to use dual impulsive oculomotor selection sequence (DSSS). The dual impulsive oculomotor selection sequence will be described in detail below. Three or more impulsive oculomotors can make their choices using similar general principles.
また、全ての可能な選択を行うために、同様な数のレイヤ（または選択木構造の「枝」）を使用する必要もない。作動に必要な衝動性動眼を少なくするように、一般的に使用された選択および／または「重要な」選択を設定してもよい。以下でより詳細に説明するように、図６は、デュアル衝動性動眼選択シーケンスを用いて殆どの選択を行う配置２７０を示す例である。しかしながら、所謂「改行」２７３の選択に関連する操作は、１回の衝動性動眼によって行われる。
Nor is it necessary to use a similar number of layers (or "branches" of the selected tree structure) to make all possible selections. Commonly used choices and / or "significant" choices may be set to reduce the impulsive occlusal eye required for operation. As described in more detail below, FIG. 6 is an example showing an
視覚信号言語要素を開発する際に考慮しなければならない第２制約は、ユーザが「ターゲット」位置を捜索し、発見することを可能にすることである。いくつかの場合（すなわち、一般的に少数の選択を含む場合）、熟練したユーザは、一連の共通選択位置を知ることができ、記憶された位置に基づいて記憶誘導型衝動性動眼を行うことができる。しかしながら、殆どの場合、選択を行う前に潜在的なターゲットの選択肢を検索することをユーザに許可しなければならない。通覧および検索は、一般的に、衝動性動眼に関与する。したがって、視覚信号成分は、１）任意の数の眼球捜索運動を可能にし、２）選択処理を実行する意図的な衝動性動眼を区別できるようにしなければならない。 A second constraint that must be considered when developing the visual signal language element is to allow the user to search for and find the "target" location. In some cases (ie, generally involving a small number of choices), a skilled user can know a series of common selection positions and perform memory-guided impulsive oculomotor eye based on the memorized position. Can be done. However, in most cases, the user must be allowed to search for potential target choices before making a selection. Browsing and searching generally involve impulsive oculomotor eye. Therefore, the visual signal component must be able to 1) enable any number of eye search movements and 2) distinguish the intentional impulsive oculomotor eye performing the selection process.
また、ターゲット位置に注視する場合、「意図的な」眼球運動を示すことであっても、または単に環境を視覚的に捜索することであっても、１つ以上の矯正的な衝動性動眼および／または他の形態の眼球運動（例えば、振顫、震動）を行う必要がある。これらの眼球運動は、一般的に、衝動性動眼および／または他の形態の自発運動（例えば、滑動性追跡眼球運動、前庭庭眼球運動）の間に介入できる非自発的な眼球運動である。ターゲット位置の注視（並びに眼球運動のタイミングおよび他の特性）を決定するように設計されたアルゴリズムは、これらの運動の介入を可能にしなければならない。 Also, when looking at the target position, one or more corrective impulsive eye movements, whether showing "intentional" eye movements or simply visual exploration of the environment. / Or other forms of eye movement (eg, tremor, tremor) need to be performed. These eye movements are generally non-spontaneous eye movements that can intervene during impulsive eye movements and / or other forms of spontaneous movements (eg, gliding pursuit eye movements, vestibular garden eye movements). Algorithms designed to determine gaze of target position (as well as timing and other characteristics of eye movements) must allow intervention of these movements.
例えば、ターゲット位置に注視する場合、最初の衝動性動眼（予測または測定）は、ターゲットから所定の閾値距離内に収まらない場合がある。しかしながら、１つ以上の矯正的な衝動性動眼によって、ユーザの視線が徐々にターゲットに近づくことができる。１つ以上の矯正的な衝動性動眼によってユーザの視線がターゲットからの所定の閾値距離内にあると判断された場合、ターゲットが選択されたと考えられ、関連する操作を開始することができる。 For example, when gazing at the target position, the first impulsive oculomotor eye (prediction or measurement) may not be within a predetermined threshold distance from the target. However, one or more corrective impulsive oculomotor eyes allow the user's line of sight to gradually approach the target. If the user's line of sight is determined to be within a predetermined threshold distance from the target by one or more corrective impulsive eyes, the target is considered selected and the associated operation can be initiated.
図１Ａは、４（水平）×３（垂直）の視線選択グリッド２００に制限されたＤＳＳＳ英語アルファベットキーボードの例である。各グリッドは、４つの文字／機能またはグリッド位置内の４つの文字／機能のうち１つを特定する選択の特定（２０３ａ、２０３ｂ、２０３ｃ、２０３ｄ）を含む。文字／機能または選択処理を表すアイコンは、一般的に、グリッドの中央領域に位置し、グリッド位置内の眼球運動を正確に追跡するための中央焦点（すなわち、隣接するグリッドに干渉する可能性のあるエッジから離れる）を提供する。
FIG. 1A is an example of a DSSS English alphabet keyboard restricted to a 4 (horizontal) x 3 (vertical) line-of-
最初に所望の文字／機能（および他の３つの文字／機能）を含むグリッド位置に（一般的に衝動性動眼によって）視線を移動し、その後、文字／機能を含むグリッド位置内の文字の位置に対応するグリッド位置の選択（２０３ａ、２０３ｂ、２０３ｃ、２０３ｄ）に視線を移動することによって、任意の文字２０１を選択することができる。次の文字／機能を含むグリッド位置に再び視線を移動し、その後、グリッド位置内の文字の位置に対応するグリッド位置の選択（２０３ａ、２０３ｂ、２０３ｃ、２０３ｄ）に視線を移動することによって、次の文字を選択することができる。このような一対の眼球運動を繰り返すことによって、任意の数の文字または記号を選択することができる。
First move the line of sight to a grid position containing the desired character / function (and the other three characters / functions) (generally by impulsive occlusal eyes), then the position of the character within the grid position containing the character / function. Any
一例として、図１Ｂは、視覚信号および図１Ａに示す配置のキーボードを用いて、単語「少年」を入力する一連の眼球運動を示している。眼球運動は、破線（例えば、２０５）によって標示される。眼球運動は、文字「ｂ」を含むグリッド位置２０４ａを選択することによって始まる。文字「ｂ」が文字グリッド位置２０４ａに位置している右上象限に対応する選択グリッド位置２０３ｂに視線を移動する（２０５）によって、文字「ｂ」を選択することができる。次に、文字「ｏ」を含むグリッド位置２０４ｂに視線を移動して、その後、文字「ｏ」が文字グリッド位置２０４ｂに位置している左下象限に対応する選択グリッド位置２０３ｃに視線を移動するによって、文字「ｏ」を選択することができる。文字「ｙ」を含むグリッド位置に視線を移動して、その後、左上象限を特定するグリッド位置２０３ａに視線を移動する（２０６）によって、文字「ｙ」を選択することができる。
As an example, FIG. 1B shows a series of eye movements in which the word "boy" is entered using a visual signal and a keyboard arranged as shown in FIG. 1A. Eye movements are marked by dashed lines (eg, 205). Eye movements begin by selecting grid position 204a containing the letter "b". The character "b" can be selected by moving the line of sight to the selection grid position 203b corresponding to the upper right quadrant in which the character "b" is located at the character grid position 204a (205). Next, the line of sight is moved to the grid position 204b containing the character "o", and then the line of sight is moved to the selected grid position 203c corresponding to the lower left quadrant in which the character "o" is located at the character grid position 204b. , The letter "o" can be selected. The letter "y" can be selected by moving the line of sight to a grid position that includes the letter "y" and then moving the line of sight to the
単語の終わりは、「スペース」２０７または他の句読点（例えば、「、」、「。」、「？」など）によって指定することができる。必要に応じて、例えば、スマートフォンおよび他の装置に見られる機能に類似する「オートフィル」機能の選択によって、単語の一部および／または句読点を完成させることができる。特別な文字／機能（例えば、しばしば改行２０８に関連する「←」）を用意して、この機能を実行することができる。用意された別の特殊記号（図１Ａに示す「＾」２０２）をトリガすることによって、他の機能および代替配置のキーボードを利用することができる。この経路は、例えば、数字、記号、大文字および他の機能を利用することができる。 The end of a word can be specified by "space" 207 or other punctuation marks (eg, ",", ".", "?", Etc.). If desired, part of the word and / or punctuation can be completed, for example, by selecting an "autofill" feature similar to that found on smartphones and other devices. Special characters / functions (eg, "←" often associated with newline 208) can be provided to perform this function. Other functions and alternative keyboard arrangements can be utilized by triggering another prepared special symbol (“^” 202 shown in FIG. 1A). This route can utilize, for example, numbers, symbols, uppercase letters and other functions.
図２Ａおよび図２Ｂは、視覚信号言語を用いて入力を行うためのキーボード配置２１０の別の例を示している。この場合、３つの文字（例えば、２１１）が、視線で選択可能なグリッド位置に各々縦に配置される。最初に文字／機能を含むグリッド位置に注視する場合、上方選択位置２１３ａ、中間選択位置２１３ｂまたは下方選択位置２１３ｃに各々注視することによって、グリッド位置における上方の文字、中間の文字または下方の文字を選択することができる。この例では、選択グリッドの位置２１３ａ、２１３ｂおよび２１３ｃは、ディスプレイ全体の右端に配置されている。
2A and 2B show another example of a
図２Ａに示す配置は、一般的により大きな生理動作範囲および制御を有する水平方向の眼球運動を優先的に支持する。また、この配置は、比較的簡単で直観的であり、初心者による簡単な単語入力に特に適している。この配置は、例えば、全ての２６個の英文字および入力の終わりおよび／または異なるキーボード配置に切り替える希望を示すために使用することができる「＾」機能文字を含む。 The arrangement shown in FIG. 2A preferentially supports horizontal eye movements that generally have a larger range of physiology and control. In addition, this arrangement is relatively simple and intuitive, and is particularly suitable for simple word input by beginners. This layout includes, for example, all 26 alphabetic characters and the "^" functional character that can be used to indicate the end of input and / or the desire to switch to a different keyboard layout.
図２Ｂは、一連の眼球運動を示す別の例示である。この場合、図２Ａに示す配置のキーボードを用いて、単語「ｃａｔ」を入力する。図１Ｂと同様に、衝動性動眼は、破線（例えば、２１５）によって示されている。一連の眼球運動は、文字「ｃ」を含むグリッド位置２１４を選択することによって始まる。文字「ｃ」がグリッド位置２１４内の下方の文字であるため、下方の文字の選択に対応する作動グリッド位置２１３ｃに注視する（２１５）ことによって、文字「ｃ」を選択することができる。次に、文字「ａ」を含む同一のグリッド位置２１４に再び注視した後、文字「ａ」がグリッド位置２１４の上方の文字であるため、上方の文字／機能を特定する選択グリッド位置２１３ａに注視することによって、文字「ａ」を選択することができる。同様に、文字「ｔ」を含むグリッド位置に注視した後、グリッド位置の中央に位置する文字を特定するグリッド位置２１３ｂに注視する（２１６）ことによって、文字「ｔ」を選択することができる。
FIG. 2B is another example showing a series of eye movements. In this case, the word "cat" is input using the keyboard arranged as shown in FIG. 2A. Similar to FIG. 1B, the impulsive googlymium is indicated by a dashed line (eg, 215). A series of eye movements begins by selecting
これらのキーボード配置において、選択可能な文字／機能の最大数は、文字／機能グリッド位置の数と選択位置の数との積である。例えば、図１Ａには８個の文字／機能グリッド位置および４個の選択位置が存在するため、選択可能な文字／機能の最大数は、８×４＝３２である。図２Ａには、９個の文字／機能グリッド位置および３個の選択位置が存在するため、選択可能な文字／機能の最大数は、９×３＝２７である。 In these keyboard layouts, the maximum number of characters / functions that can be selected is the product of the number of character / function grid positions and the number of selected positions. For example, since there are 8 character / function grid positions and 4 selection positions in FIG. 1A, the maximum number of characters / functions that can be selected is 8 × 4 = 32. Since there are 9 character / function grid positions and 3 selection positions in FIG. 2A, the maximum number of characters / functions that can be selected is 9 × 3 = 27.
より一般的に、キーボード内の選択可能なグリッド位置の数が一定である場合、文字／機能位置および選択位置の数を概ね等しくすることによって、最大数の選択を達成することができる。例えば、図３は、３行×４列のグリッド配置２２０の例を示している。この配置は、６個の文字／機能位置および６個の選択位置を有するため、合計３６個の可能な選択をもたらすことができる。これによって、眼球運動に基づいて、全ての英文字および１０個の他の特殊文字または特殊記号を選択することができる。６個の選択位置は、２列、すなわち左列２２３ａおよび右列２２３ｂ、および３行、すなわち上方行２２４ａ、中間行２２４ｂおよび下方行２２４ｃに配置される。
More generally, if the number of selectable grid positions in the keyboard is constant, the maximum number of selections can be achieved by approximately equalizing the number of character / function positions and selection positions. For example, FIG. 3 shows an example of a
図３において、各選択グリッド位置の中央領域のアイコンは、文字／機能選択位置内の相対位置（すなわち、左、右、上方、中央、および下方位置）に対応する。したがって、相対位置を使用して、特定の文字／機能を選択する。より一般的に、別のメカニズムまたは視覚指標を別々にまたは組み合わせて使用して、文字／機能を選択することができる。別のメカニズムまたは視覚指標は、文字／機能の色、フォント、輝度またはサイズ、背景色、境界特性および／または他の視覚指示のマッチングを含む。 In FIG. 3, the icons in the central region of each selection grid position correspond to relative positions within the character / function selection position (ie, left, right, top, center, and bottom positions). Therefore, relative positions are used to select specific characters / functions. More generally, different mechanisms or visual indicators can be used separately or in combination to select characters / functions. Other mechanisms or visual indicators include matching text / function colors, fonts, brightness or size, background colors, boundary characteristics and / or other visual indications.
表示面積／視線追跡面積に制限がある場合、眼球運動に基づいた選択を行うための列または行を単に１つで追加することで、単一画面内の選択肢の数を大幅に拡大することができる。図４Ａは、５列×３列の選択グリッドの例示的な配置２５０を示している。図３と同様に、６個の選択位置は、２列、すなわち左列２５３ａおよび右列２５３ｂ、および３行、すなわち上方行２５４ａ、中間行２５４ｂおよび下方行２５４ｃに配置される。文字／機能２５１は、左側の３列２５２ａ、２５２ｂおよび２５２ｃに含まれたものを指定することによって選択される。この配置を使用する場合、単一のＤＳＳＳで、合計９×６＝５４つの文字または記号の選択を行うことができる。この配置は、２６個の英文字、１０個の数字、４個の移動キー（左、右、上および下）および１４個の特殊文字または記号の同時表示に十分である。この増加した選択は、大文字、数字入力、画面の移動、句読点、訂正、他の画面配置または記号の利用などの機能を実行するように、視覚信号言語に遥かに高い効率性および柔軟性を提供することができる。
If your display area / line-of-sight tracking area is limited, you can significantly increase the number of choices in a single screen by simply adding one column or row to make eye movement-based selections. it can. FIG. 4A shows an
同様の視線グリッド構成を有しても、単に仮想的に表示された文字／機能のセットを切り替えることによって、配置を切り替えることができるため、異なる配置のキーボードを形成することができる。例えば、図４Ｂは、５列×３行の選択グリッドの代替的な配置の一例である。この例において、英字２５５、数字２５６、特殊文字および記号２５７は、ディスプレイの個別領域に分類される。例えば、散文の作成、（例えば、選択位置間の眼球運動を最小限にするように数字を配置する場合における）数字の入力、ウェブ上の移動および入力、特定の作業（例えば、チェックリストの入力）、描画および／または個人的な好みに適合するように、最適化可能なキーボードの数は、実質的に無限である。
Even with a similar line-of-sight grid configuration, the layout can be switched by simply switching the set of virtually displayed characters / functions, so that keyboards with different layouts can be formed. For example, FIG. 4B is an example of an alternative arrangement of a selection grid of 5 columns x 3 rows. In this example, the
図５は、視覚信号による入力を可能にする「仮想キーボード」配置２６０の別の例である。この場合、視覚信号による４列×４行の選択グリッドは、手動入力を行う人間-機械インターフェイス（ＨＭＩ）に広く使用されている周知の「ＱＷＥＲＴＹ」キーボードにほぼ一致するように配置されている。この例において、文字「２６１」は、標準の「ＱＷＥＲＴＹ」英語キーボードの配置と同様の順序で配置されている。この配置がより一般的に使用される文字を上方の行に沿って配置するように設計されたため、仮想キーボードの上方の行２６６に沿って選択位置を配置することによって、一般的に使用される文字を選択する際の眼球運動距離を最小限に抑えることができる。
FIG. 5 is another example of a "virtual keyboard"
４つの選択グリッド位置２６６は、１２個の文字／機能グリッド位置の各位置内の４つの可能な左から右への文字（合計で４８個の可能な文字／機能）の位置に対応する。この例示的な配置は、英文字に加えて、移動キー（すなわち、左、右、上および下）クラスタ２６４ａおよび２６４ｂ、大文字「Ａ」２６５によって表された大文字機能、１個ブロック２６２内のスペースおよび句読点機能、殆どの標準キーボードと同様の配置を使用する最も右側の列２６３内の数字キーパッド、他の特殊記号および追加のメニュー選択をもたらす機能キー２０２を含む。
The four selection grid positions 266 correspond to the positions of four possible left-to-right characters (48 possible characters / functions in total) within each position of the twelve character / function grid positions. This exemplary arrangement is in addition to the alphabetic characters, the move keys (ie left, right, up and down)
同様の方法を用いて、任意のサイズおよび形状で、視覚信号による入力を可能にするキーボード配置を構成することができる。全ての使用可能なＤＳＳＳ組み合わせを使用する必要はない（すなわち、一部の位置を空白にしてもよい）。選択位置を特定の一側に配置する必要はなく、連続領域内に配置する必要もない（例えば、選択位置は、左端境界または右端境界から分割されてもよい）。キーボードは、例えば、入力されているテキストおよび／または行われているキーボード入力の検索基準に最も適合する英数字表現および画像項目を含むことができるより多面的な表示システム内に組み込むこともできる。これらのテキストおよび画像は、キーボードの周りに（すなわち、任意の一側または全ての側に沿って）配置されてもよく、またはキーボードの中央領域内に配置されてもよい。 Similar methods can be used to configure keyboard layouts of any size and shape that allow visual signal input. It is not necessary to use all available DSSS combinations (ie, some positions may be blank). The selection position need not be located on one particular side and does not need to be located within a continuous area (eg, the selection position may be split from the left or right boundary). The keyboard can also be incorporated, for example, into a more multifaceted display system that can contain alphanumeric representations and image items that best match the search criteria for the text being entered and / or the keyboard input being made. These texts and images may be placed around the keyboard (ie, along any one or all sides) or within the central area of the keyboard.
図６は、非連続の選択領域を有する代替的な選択配置を示す４（水平）×３（垂直）グリッドに制限されたＤＳＳＳ英語アルファベットキーボードの例示的な配置２７０である。図１Ａと同様に、最初に、ディスプレイ２７０の中央領域２７２から４つの文字または記号からなるクラスタを選択する。その後、表示領域の４隅２７１ａ、２７１ｂ、２７１ｃおよび２７ｄｄのうち１つに注視することによって、４つの文字／機能のうち１つを選択することができる。４隅のうち１つを選択する選択処理は、直感的で個別な（すなわち、異なる方向）衝動性動眼およびアルゴリズム的に特定可能な衝動性動眼を容易にする。
FIG. 6 is an
また、図６の配置２７０は、同様の選択モードで全ての選択を行わない選択木の状況を示している。配置２７０内の殆どの選択は、デュアル衝動性動眼シーケンスを用いて行われる。ディスプレイ上の最も右側の所謂「改行」文字２７３に関連する任意の機能は、（すなわち、１回の衝動性動眼を用いて行った）選択の直後に実行することができる。
Further, the
視覚信号を用いてタイピングしている間または他の選択処理の間のフィードバックは、視覚的な表示に制限されない。（例えば、装置がユーザの意図を推測するときの）文字、単語、語句または他の表示は、音声フォーマットで（すなわち、合成音声または発話音声を使用して）提供することができる。（例えば、ユーザが「誤った」入力をした場合）装置による短い期間の振動などの触覚フィードバックを組み込むこともできる。全てのフィードバックモダリティを個別に使用することができ、組み合わせて使用することもできる。様々な用途において、異なるモダリティを使用することができる。 Feedback during typing with visual signals or during other selection processes is not limited to visual display. Letters, words, phrases or other indications (eg, when the device infers the user's intent) can be provided in audio format (ie, using synthetic or spoken speech). Tactile feedback, such as short-term vibration by the device (eg, if the user makes an "wrong" input), can also be incorporated. All feedback modality can be used individually or in combination. Different modality can be used in a variety of applications.
オフディスプレイターゲット
ディスプレイのエッジの近くにターゲットを配置することによって表示領域内の選択可能なターゲットの数を最大化する方法は、２０１５年５月９日に出願され、「実在オブジェクトおよび仮想オブジェクトと対話するための生体力学に基づく視覚信号を行うためのシステムおよび方法」と題された米国出願第１４／７０８２３４号に記載されている。当該出願の全体が参照により本明細書に組み込まれる。この一般的な方法は、ターゲットを任意の表示領域の外側に完全にまたは少なくとも部分的に配置することによって拡張することができる。
Off-Display Targets A method of maximizing the number of selectable targets in the display area by placing targets near the edges of the display was filed on May 9, 2015, "Interacting with Real and Virtual Objects. It is described in US Application No. 14/708234 entitled "Systems and Methods for Providing Biomechanically Based Visual Signals". The entire application is incorporated herein by reference. This general method can be extended by placing the target completely or at least partially outside any display area.
視認可能なオフディスプレイターゲットをユーザに提供する（すなわち、ユーザに「期待するもの」を提供する）ために、ターゲットは、一般的に装置ユーザの視野内に存在する標識（例えば、アイコン、画像、英数字、記号、ディスプレイの視認可能なエッジ）を含むことができる。これらのターゲットは、装置の制御によって点灯、消灯または調節できる１つ以上の発光ダイオード（ＬＥＤ）、２次表示灯または投光灯などの（メインディスプレイとは独立する）動的素子を含むことができる。 In order to provide the user with a visible off-display target (ie, to provide the user with "what to expect"), the target is generally a sign (eg, an icon, an image, etc.) that is in the field of view of the device user. It can contain alphanumeric characters, symbols, visible edges of the display). These targets may include dynamic elements (independent of the main display) such as one or more light emitting diodes (LEDs) that can be turned on, off or adjusted under the control of the device, secondary indicators or floodlights. it can.
ターゲット（例えば、アイコン、記号、ＬＥＤ）を視認可能な面に貼り付けることができる。投影されたターゲットまたはターゲットの一部は、１つ以上の表面からの反射、所謂導波路を用いた指向型投影、１つ以上の光ファイバ、屈折光学素子、シャッタおよび／または他の光制御機構を用いた誘導光を含む１つ以上のリモート光源を含むことができる。 Targets (eg, icons, symbols, LEDs) can be attached to visible surfaces. The projected target or part of the target is a reflection from one or more surfaces, a directional projection using a so-called waveguide, one or more optical fibers, refracting optics, shutters and / or other optical control mechanisms. It is possible to include one or more remote light sources including induced light using the above.
また、オフディスプレイターゲットは、特定のディスプレイの結像面に制限されない。例えば、オフディスプレイターゲットは、ディスプレイの結像面の前面または背面に現れてもよい。これによって、オフディスプレイターゲットへの眼球運動が意図的な作動シーケンスであるか否かを判断する際に、両眼離反運動（および若いユーザの眼球の水晶体の可能な形状変化）を考慮に入れることができる。オフディスプレイターゲットが様々な深さで形成された場合、特定のターゲットの選択基準内にある両眼離反運動のレベル（すなわち、認識深さ）を使用することができる。 Also, the off-display target is not limited to the image plane of a particular display. For example, the off-display target may appear in front of or behind the image plane of the display. This allows binocular detachment (and possible changes in the lens lens of the young user's eye) to be taken into account when determining whether eye movement to the off-display target is an intentional sequence of action. Can be done. When off-display targets are formed at various depths, the level of binocular detachment movement (ie, cognitive depth) within the selection criteria of a particular target can be used.
また、オフディスプレイターゲットは、ユーザが移動する時に（全体の環境に対して）動く可能性があるユーザの一部（例えば、親指、指、手）であってもよい。さらに、オフディスプレイターゲットは、ユーザによって所持されてもよい（例えば、手袋、指輪、服装の一部）。仮想のオフディスプレイターゲットは、近くの携帯電話またはタブレットなどの別の表示装置に表示されてもよい。 The off-display target may also be a portion of the user (eg, thumb, finger, hand) that may move (relative to the overall environment) as the user moves. In addition, the off-display target may be possessed by the user (eg, gloves, rings, parts of clothing). The virtual off-display target may be displayed on another display device such as a nearby mobile phone or tablet.
機械的に利用可能であれば、（必要に応じて）オフディスプレイターゲットを選択する時に、ユーザに何らかのフィードバック（例えば、ＬＥＤの点灯または消灯、光色の変化）を提供することができる。オフディスプレイターゲットが変更可能でない場合（例えば、表面に貼り付けられた「貼り付け」アイコンまたは画像）、フィードバックは、傍中心窩視界または周辺視界を含むディスプレイ上の他の位置に提供されてもよい。 If mechanically available, it is possible to provide some feedback (eg, LED on / off, light color change) to the user when selecting an off-display target (if desired). If the off-display target is not modifiable (eg, a "paste" icon or image pasted on the surface), feedback may be provided to other locations on the display, including parafoveal or peripheral vision. ..
多くの場合、特に経験豊富なユーザにとって、オフディスプレイターゲットのフィードバックは特に必要ない。デュアル衝動性動眼選択シーケンス（ＤＳＳＳ）は、オフディスプレイターゲットのフィードバックが一般的に必要でない衝動性動眼信号言語の特に有用な例である。ＤＳＳＳ中にオフディスプレイターゲットへの衝動性動眼が作動に使用された場合、結果として生じた動作（例えば、選択された文字または画像の表示、文字または単語の音声生成）は、一般的に、作動選択が行われたことをユーザに十分に知らせることができる。 Off-display target feedback is often not particularly needed, especially for experienced users. The Dual Impulsive Eye Selection Sequence (DSSS) is a particularly useful example of an impulsive eye signal language that generally does not require feedback from off-display targets. When an impulsive occlusal eye to an off-display target is used for activation during DSSS, the resulting behavior (eg, display of selected letters or images, speech generation of letters or words) is generally activated. The user can be fully informed that the selection has been made.
図７は、４列（２８０ａ）×３行（２８０ｂ）の選択マトリクスを示す例である。この選択マトリクスは、表示領域の４隅の上方に４つの追加のターゲット位置２８１ａ、２８１ｂ、２８１ｃおよび２８ｄ１を有する。図７において、オフディスプレイターゲット２８１ａ、２８１ｂ、２８１ｃおよび２８１ｄに対応する空間領域は、破線で示されている。表示された選択肢とオフディスプレイターゲット２８１ａ、２８１ｂ、２８１ｃおよび２８１ｄとの組み合わせを用いて、ＤＳＳＳによって、例えば、全ての英字、数字およびいくつかの特殊記号（例えば、シーケンスの終結を示すように用意されたもの２８２を含む）から、選択を行うことができる。図６と比較すると、４隅にオフディスプレイターゲット２８１ａ、２８１ｂ、２８１ｃおよび２８１ｄを配置することによって、４列（２８０ａ）×３行（２８０ｂ）の選択マトリクスで１６個の可能な選択を追加できる（すなわち、追加的に示された４つのマトリクス位置の各位置に４つの選択を配置できる）ことが分かる。
FIG. 7 is an example showing a selection matrix of 4 columns (280a) × 3 rows (280b). This selection matrix has four
図８は、８個のオフディスプレイターゲット２９１ａ、２９１ｂ、２９１ｃ、２９１ｄ、２９２ａ、２９２ｂ、２９２ｃおよび２９２ｄを使用する別の例示的な配置を示している。図７と同様に、表示されたターゲットは、４列×３行の選択マトリックスを含み、オフディスプレイターゲット２９１ａ、２９１ｂ、２９１ｃ、２９１ｄ、２９２ａ、２９２ｂ、２９２ｃおよび２９２ｄは、破線で示されている。表示された選択可能な項目は、英語アルファベットの全ての文字、１０個の数字、および入力の完了を示すためによく使用された所謂「リターン」（または「改行」）キー２９５を含む特殊文字および記号を含む。
FIG. 8 shows another exemplary arrangement using eight off-
図８の配置を使用する場合、データ入力は、中央に表示された４列×３行のマトリックスから、４つまでの可能な選択肢を含むグループを指定してから、４つの選択肢から１つを選択することを示すように、４隅のオフディスプレイターゲット２９１ａ、２９１ｂ、２９１ｃおよび２９ｄに注視することを含む。図７に示すプロセスと同様に、この方法は、表示された選択マトリックス内の任意の文字または記号を特定することができる。
When using the arrangement shown in FIG. 8, the data entry is from a centrally displayed matrix of 4 columns x 3 rows, specifying a group containing up to 4 possible choices, and then selecting one of the four choices. It involves gazing at the four corner off-
その後、ユーザは、必要に応じて、オフディスプレイ「変更子」ターゲット２９２ａ、２９２ｂ、２９２ｃおよび２９２ｄのうち１つに注視することによって、選択をさらに変更することができる。典型的なキーボードと同様に、これらの変更子ターゲットは、例えば、（多くの場合、上向き矢印として表示され、しばしば大文字の入力に使用される）「シフトキー」２９２ａ、「制御キー」（略称Ｃｔｒｌ）２９２ｂ、「代替キー」（略称Ａｌｔ）２９２ｃ、一般的に数字に関連する「機能キー」（略称「Ｆ」）２９２ｄに関連する補助機能を実行することができる。標準的なキーボードと同様に、シフト数字キー２９４に関連するものような変更を介して利用可能な選択肢は、初心者を支援するために表示された選択マトリクスに示されてもよい。
The user can then further change the selection by looking at one of the off-display "modifier"
中央に表示された選択マトリックス内の任意の位置に再び注視することによって、選択を終える。ユーザが変更マトリクスターゲット位置２９２ａ、２９２ｂ、２９２ｃおよび２９２ｄに注視することなく、選択マトリクスに注視する場合、初期の文字／機能選択が変更されない。変更子ターゲットに対して一連の衝動性動眼を行うことによって、複数の変更子（ＣｔｒｌおよびＳＨＩＦＴなど）を指定することができる。所望であれば、変更子ターゲット２９２ａ、２９２ｂ、２９２ｃおよび２９２ｄは、位置ターゲット２９１ａ、２９１ｂ、２９１ｃおよび２９１ｄの前に選択されてもよく、後に選択されてもよい。
The selection is completed by looking again at any position in the selection matrix displayed in the center. If the user gazes at the selection matrix without looking at the change
図８に示す体系によって、視覚信号を用いて、標準的なキーボード上の文字および特殊機能のほぼ全ての組み合わせを選択することができる。標準ＱＷＥＲＴＹキーボードの右側の特殊記号は、表示された選択マトリックスの右下領域２９３から使用することができる。これによって、（例えば、ウェブページを閲覧および応答する時に）視覚信号を用いて、標準的なキーボードの入力を簡単に置換することができる。
The system shown in FIG. 8 allows visual signals to be used to select almost any combination of characters and special features on a standard keyboard. The special symbols on the right side of the standard QWERTY keyboard are available from the lower
オンディスプレイターゲット位置とオフディスプレイターゲットとの組み合わせによって、特に視野の比較的小さい部分を占める装着型ディスプレイに、可能な選択肢の数を劇的に増加することができる。例えば、図８に示された４列×３列の選択マトリックスの場合、４隅の４つのオフディスプレイターゲットおよび４つのオフディスプレイ変更子ターゲットを含むため、単一の（オプション）変更子に制限された場合に、合計１２×４×４＝１９２個の可能な選択を行うことができる。第２の（すなわち、４つのうち１つを選択する）オプション変更子を設けた場合、可能な選択肢の数を７６８まで増加することができる。 The combination of the on-display target position and the off-display target can dramatically increase the number of possible options, especially for wearable displays that occupy a relatively small area of the field of view. For example, the 4-column x 3-column selection matrix shown in FIG. 8 is limited to a single (optional) modifier because it contains four off-display targets in the four corners and four off-display modifier targets. If so, a total of 12 x 4 x 4 = 192 possible selections can be made. With a second (ie, choosing one of four) option modifiers, the number of possible choices can be increased to 768.
したがって、この一般的な方法は、大きな可能性プールから選択を行うときに特に有用である。このことは、文字の入力（図７および図８に示すように一度に１文字）、単語または語句の選択、英数字の入力、多くの画像またはデータセット（特に選択木構造に含まれたもの）からの選択、項目リストからの選択などを含む。 Therefore, this general method is particularly useful when making selections from a large pool of possibilities. This includes character input (one character at a time as shown in FIGS. 7 and 8), word or phrase selection, alphanumeric input, many images or datasets (especially those included in the selection tree structure). ), Selection from the item list, etc. are included.
より一般的には、オフディスプレイターゲットの数は、単一の選択可能な領域であってもよく、装置着用者の視界内で確実に識別可能な多くのターゲットであってもよい。オフディスプレイターゲットは、ディスプレイの周辺領域と重なってもよく、表示領域に隣接してもよく、１つ以上の表示領域から離れていてもよい。目標領域のサイズは、例えば、表示領域の一辺を越える任意位置に注視することを含むように変化してもよい。オンディスプレイターゲットおよびオフディスプレイターゲットの両方の形状は、例えば、正方形、長方形、六角形および円形のいずれかを含むことができる。 More generally, the number of off-display targets may be a single selectable area or many targets that can be reliably identified within the device wearer's field of view. The off-display target may overlap the peripheral area of the display, may be adjacent to the display area, or may be separated from one or more display areas. The size of the target area may vary, for example, to include gazing at an arbitrary position beyond one side of the display area. The shape of both the on-display target and the off-display target can include, for example, any of squares, rectangles, hexagons and circles.
作動ターゲット（activation target）の周辺位置および／またはオフディスプレイ位置の別の重要な利点は、表示領域内の不注意な作動を回避することである。特定の文字または機能を選択するとき、ユーザは、所望の選択肢を見つけるために必要とされる多くの眼球運動を行うことができる。「作動」が周辺位置（すなわち、表示領域から離れた位置）への眼球運動によって行われるため、捜索中に殆ど作動されない。周辺位置で作動された場合、新しい選択は、中央領域に戻ることのみによって開始される。これによって、作動作業から検索処理および初期選択処理を空間的および方向的に分けることができ、不注意な作動の可能性を大幅に低減する。 Another important advantage of the peripheral and / or off-display position of the activation target is to avoid inadvertent activation within the display area. When selecting a particular character or function, the user can perform many eye movements required to find the desired option. Since the "actuation" is performed by eye movement to a peripheral position (ie, a position away from the display area), it is hardly activated during the search. When activated in the peripheral position, the new selection is initiated only by returning to the central area. As a result, the search process and the initial selection process can be separated spatially and directionally from the operation work, and the possibility of inadvertent operation is greatly reduced.
ユーザが（例えば、障害による視線追跡が悪くなることによって）必要以上の不注意な作動をする傾向があった場合、オフディスプレイターゲット２９１ａ、２９１ｂ、２９１ｃ、２９１ｄおよび変更子ターゲット２９２ａ、２９２ｂ、２９２ｃ、２９２ｄを表示領域から遠くなるように設ける。例えば、ユーザが経験を積んで不注意な作動の回数を減らした場合、これらのターゲットを表示領域の近くに移設することができ、これによって、目の移動距離を減少し、結果として選択時間および目の疲労を軽減することができる。
Off-
シングル衝動性動眼選択シーケンス（ＳＳＳＳ）による複数の選択
表示および視覚信号のために拡張された視野を利用できるときに、Ｎ個の選択のうち１つを選択することを、１回の眼球運動（すなわち、１回の衝動性動眼）で１つの選択を行う方法に変更することができる。ＳＳＳＳは、クラスタ内に配置された選択セットを利用して、選択肢に注視してから、視線を移動して次の選択を実行することができる新しいクラスタに注視することによって、１つのクラスタ内の特定の項目を選択することができ、選択行動を示すことができる。
Multiple selections with a single impulsive oculomotor selection sequence (SSSS) When an expanded visual field is available for display and visual signals, selecting one of the N selections can be a single eye movement (1 eye movement). That is, it is possible to change to a method of making one selection with one impulsive oculomotor eye). SSSS utilizes a selection set located within a cluster to focus on the choices and then move the line of sight to a new cluster where the next selection can be made, thereby within one cluster. A specific item can be selected and a selection action can be shown.
図９Ａは、日付を選択するためのディスプレイ配置３００の一例である。この場合、１回の衝動性動眼で１つの選択肢（すなわち、月、日）を選択することができる。一番左側のクラスタ３０１は、月を指定する。中間クラスタ３０２は、１ヶ月内の日数を指定する。一番右側のクラスタ３０３は、操作を指定する。任意の日付の指定は、左側のクラスタ３０１内の月に注視してから、視線を移動して中央のクラスタ３０２内の日数に注視し、その後、視線を移動して右端のクラスタ３０３内の操作に注視することによって、行うことができる。
FIG. 9A is an example of a
図９Ａは、「６月４日」という日付を入力するための眼球運動の例示的なシーケンスを示している。衝動性動眼は、破線で示されている。「６月」（Jun）３０４に注視してから視線を移動することによって、月を選択する。その後、数字「４」に注視してから視線を移動することによって、日を選択する。「ENTER」キー３０５ｂに注視することによって、選択された日付をアプリケーションのデータ入力シーケンスに適用することができる。アプリケーションは、例えば、表示を変更して、追加情報の入力を可能にすることができる。 FIG. 9A shows an exemplary sequence of eye movements for entering the date "June 4th". Impulsive googlymium is indicated by a broken line. Select the month by gazing at "June" 304 and then moving your gaze. After that, the day is selected by paying attention to the number "4" and then moving the line of sight. By looking at the "ENTER" key 305b, the selected date can be applied to the application's data entry sequence. The application can, for example, change the display to allow input of additional information.
図９Ａにおいて、選択可能な代替操作項目は、例えば、前の質問に対する回答を再入力することを可能にするように、データ入力の後方に後退する「BACK」キー３０５ａを含む。この例において、任意の時間に「SKIP」キー３０５ｃに注視することによって、日付入力なしでアプリケーションを進めることができる。選択がクラスタに注視してから視線を移動するときにのみ行われるため、所望の選択を選択するように、クラスタ内で任意の数の捜索的な眼球運動を容易に行うことができる。単に日付クラスタに注視することによって、不適切に選択された月を修正することができる。数値クラスタ３０６内の記号「←」は、例えば、異なるキーボード配置を呼び出すための特殊機能のために予備することができる。
In FIG. 9A, selectable alternative operation items include, for example, a "BACK" key 305a that retracts backwards in data entry to allow re-entry of answers to previous questions. In this example, by gazing at the "SKIP" key 305c at any time, the application can proceed without date entry. Since the selection is made only when the gaze is focused on the cluster and then the line of sight is moved, any number of searchable eye movements within the cluster can be easily performed to select the desired selection. You can correct improperly selected months by simply looking at the date cluster. The symbol "←" in the
図９Ｂは、日付を入力するための別のキーボード配置３０７を示している。この配置は、（例えば、バイザーの上方または下部領域に沿った表示を容易にするために）垂直方向の眼球運動が厳しく制限されている場合および／または１位数字に従って日を表す数値を並べることが望ましい場合（一部のユーザにとってより直感的である）に使用され得る。図９Ｂの眼球運動の例示的なＳＳＳＳは、５月８日という日付を指定している。
FIG. 9B shows another
図１０Ａは、さらに別のキーボード配置３１０を示している。この配置は、例えば、眼球運動の水平範囲が制限されている場合に使用され得る。この場合、２つの別々の数字クラスタ３１２、３１３から日を指定することによって日付を選択するために、もう１つの衝動性動眼が必要となる。左から右のクラスタは、月３１１、日の１０位数字３１２、日の１位数字３１３および操作３１４の指定を可能にする。例示的な眼球運動シーケンス３１５は、ＳＳＳＳを用いて「１２月２５日」という日付を選択したことを示している。
FIG. 10A shows yet another
図１０Ｂは、眼球運動の垂直範囲が（３行のみに）制限されていることを除いて、図１０Ａに示されたものと類似するキーボードの変形配置３１６である。この配置３１６において、１０位数字クラスタ３１２をバイパスすることは、先頭の数字がゼロであることが理解され得るため、日を選択するための１０位数字に「０」が含まれていない。図示された一連の眼球運動３１７は、ＳＳＳＳを用いて「１２月７日」という日付を指定したことを示している。
FIG. 10B is a modified
選択セット（例えば、文字、数字、機能など）が繰り返して同様である場合、選択セットの一部または全部の重複部分を表示することによって、ＳＳＳＳを処理することができる。クラスタ間で交互に前後移動するまたはクラスタ間で移行することによって、任意の長さのシーケンスを選択することができる。 If the selection set (eg, letters, numbers, functions, etc.) is repeated and similar, SSSS can be processed by displaying some or all overlapping parts of the selection set. Sequences of any length can be selected by alternating back and forth between clusters or migrating between clusters.
図１１Ａは、例えば、電話番号をダイヤルするために使用され得る左右に並んだ標準的な数字キーパッド配置３２０を示している。図１１Ａに示されたキーパッド配置３２１および３２２は、典型的な電話機に見られるものと同様である。代わりに、「７８９」を含む行および「１２３」を含む行を交換すると、典型的なコンピュータのキーボードおよび計算器の配置に変換される。
FIG. 11A shows, for example, a side-by-side standard
２つのキーパッドに左右交替で注視することによって、任意長さの番号を指定することができる。いずれか一方のキーパッドは、数値入力の開始点として使用されてもよい。特定のアプリケーションに十分な桁数（電話番号、パスコードなど）が入力されたときにまたは特殊記号（例えば、「＊」または「＃」）が入力されたときに、入力を停止することができる。図１１Ａに示されたＳＳＳＳ眼球運動経路３２３は、番号「１４８７６６」を入力する例である。
By gazing at the two keypads alternately left and right, a number of arbitrary length can be specified. Either keypad may be used as a starting point for numeric input. You can stop typing when enough digits (phone number, passcode, etc.) are entered for a particular application, or when special symbols (eg, "*" or "#") are entered. .. The SSSS
図１１Ｂは、数値入力用の代替配置３２４を示している。この配置３２４は、使用が簡単且つ直観的である。２行の数字３２５ａ、３２５ｂおよび特殊文字／機能「←」に上下交替で注視することによって、任意の番号を入力することができる。垂直方向の寸法が制限されたため、この配置３２４を例えばディスプレイの上方または下方のエッジに配置することができ、支障になる可能性を低減する。図１１Ｂに示されたＳＳＳＳ眼球運動経路３２６は、図１１Ａと同様の番号「１４８７６６」を入力する例である。
FIG. 11B shows an
図１２Ａは、計算器のＳＳＳＳ配置３３０を示している。この例において、同様の計算器ディスプレイ３３１ａ、３３１ｂが左右に並べて配置されている。数字および機能の両方は、同様の計算器ディスプレイ３３１ａ、３３１ｂの間に視線を左右に移動して選択することができる。破線３３２は、３４および８５のを計算するために使用された眼球運動を示している。乗算の結果（例えば、２８９０）は、最後に「＝」記号３３３に視線を移動した直後に表示される（３３９）。
FIG. 12A shows the
図１２Ｂは、ディスプレイの動的制御によってユーザの入力プロセスを支援することができる簡単な状況を示している。図１２Ａと同様の計算器配置３３０を使用するが、計算器の左側ディスプレイ３３１ａおよび右側ディスプレイ３３１ｂの両方は、（左側ディスプレイ３３１ａに示すように）最初に記号「／」、「＊」、「＋」および「＝」を表示しなくてもよい。その理由は、数値がなければ、記号を用いて計算を実行することができないからである。第１数値（例えば、ディスプレイ３３１ａ上の「２」）が選択されると、算術演算子が直ちに（例えば、衝動性動眼３３４の間に）反対側のディスプレイ３３１ｂに表示される（３３５）。このときに、追加の数値情報が提供されるまで算術演算および等号の機能を実行することができないため、等号（すなわち、「＝」）が表示されていない（３３６）。
FIG. 12B shows a simple situation in which the dynamic control of the display can assist the user's input process. A
連続の入力が同様であっても、ＳＳＳＳディスプレイを同様にする必要がない。図１３は、数値の高速入力に最適化された科学計算器の例示的な配置を示している。垂直方向において、上方ディスプレイ３４１ａおよび下方ディスプレイ３４１ｂ内のグリッド位置の内容は、互いに鏡像になっている。頻繁に使用される数字は、近くに配置され、上方ディスプレイ３４１ａおよび下方ディスプレイ３４１ｂの中央の内側行３４２ｃ、３４３ａを占める。共通の機能は、中間行３４２ｂ、３４３ｂを占める。使用される機能および他の操作は、表示領域の中央から最も遠い列３４２ａ、３４３ｃに配置される。この構成は、より一般的な選択（特に垂直方向）を行うように、より短くより速い眼球運動をサポートする。より少なく使用される外側行の機能の選択は、一般的に、より長い眼球運動を行う必要がある。
The SSSS display does not need to be similar, even if the continuous inputs are similar. FIG. 13 shows an exemplary arrangement of scientific calculators optimized for fast numerical input. In the vertical direction, the contents of the grid positions in the
図１３に示すように、視覚信号３４５を用いて行われた例示的な計算３４９は、ｓｉｎ（３０°）／４を計算する。科学計算器で一般的に見られる算術機能およびその他の操作（例えば、表示消去、入力消去）に加えて、フラグおよびの他の設定条件、例えば、三角関数が度または弧度を単位とするか否かは、視線を適切な位置３４４に移動することによって確立することができる。
As shown in FIG. 13, an
図１３は、手動（または他の種類の）入力に最適化された配置が視覚信号に最適ではないという概念を示す例である。したがって、１本以上の指の動きに基づいて設計された配置（例えば、数字キーパッド）は、視覚信号の発生速度、使用の容易さおよび／または目の疲労に基づく視覚信号には最適ではない可能性がある。 FIG. 13 is an example showing the concept that an arrangement optimized for manual (or other type) input is not optimal for visual signals. Therefore, arrangements designed based on the movement of one or more fingers (eg, numeric keypads) are not optimal for visual signals based on the rate of occurrence of the visual signal, ease of use and / or eye fatigue. there is a possibility.
図１４は、ＱＷＥＲＴＹキーボード用のＳＳＳＳ配置３５０の一例を示している。この例において、配置３５０の上方セクション３５１ａおよび下方セクション３５１ｂは、垂直方向上互いに鏡像になっている。標準的なＱＷＥＲＴＹキーボードの上方に配置された、頻繁に使用される文字の複製行３５２ｃ、３５３ａが互いに近くに配置される。標準的なキーボードの中央行は、上方セクション３５１ａおよび下方セクション３５１ｂの中間行３５２ｂ、３５３ｂに配置される。より少なく使用される文字は、最も離れた行３５２ａ、３５３ｃに配置される。図１４の視覚信号３４５は、単語「ＨＥＬＬＯ」を入力してから、次の単語のためにスペース記号（すなわち、「＿」）３５４を入力した。
FIG. 14 shows an example of an
経験豊富な装置ユーザは、（記憶に基づいて）配置内の選択肢の位置を知ることができる。この場合、空間記憶を用いて、眼球の捜索運動を排除し、単に一連の選択肢に注視することによって、選択速度をさらに加速することができる。大きな選択衝動性動眼の終わりに起こり得る小さな矯正的な衝動性動眼（例えば、閾値距離未満で移動した衝動性動眼）は、アルゴリズム的に特定され、別個の選択として考慮されなくてもよい。記憶誘導型選択シーケンスは、より短くよく知られている電話番号、パスコード、普通名前などに特に有用であり得る。 Experienced device users can know the location of choices within the deployment (based on memory). In this case, spatial memory can be used to further accelerate selection speed by eliminating eye search movements and simply focusing on a set of options. Small corrective occlusal occlusal eyes that can occur at the end of a large selective gonocytic eye (eg, impulsive occlusal that has moved below a threshold distance) are algorithmically identified and need not be considered as separate choices. Memory-guided selection sequences can be particularly useful for shorter, well-known phone numbers, passcodes, common names, and the like.
図１５Ａは、１回の（記憶誘導型）衝動性動眼で１桁の数字を選択することによって、数列を入力するための４行×３列の数字キーパッド配置３６０の一例である。視覚信号言語が（例えば、「反復」操作を行うために）一般的に停留に依存しないため、反復入力（例えば、反復数字）が必要な場合に問題が生じる。この問題は、「反復」選択３６２を提供することによって解決される。反復数字または機能の入力は、入力位置から反復選択３６２の位置に視線を移動することによって実行することができる。数字または機能をさらに繰り返す必要がある場合、数字または機能を入力した位置に視線を戻すことができる。この前後プロセスを何度も繰り返してもよい。
FIG. 15A is an example of a 4-row x 3-column
別の実施形態において、「反復」機能３６２の位置に表示されるアイコンは、繰り返す回数を示す数字（または他の文字／機能）であってもよい。これは、以前の入力に応じて、潜在的な選択が変化する動的制御ディスプレイの一例である。選択が行われた場合、反復位置３６２は、その選択が反復入力を引き起こすように、選択の重複画像を表示する。反復位置３６２の動的変更は、注意および関連する非自発の眼球運動を引かないように（例えば、本明細書の他の箇所に記載されているように）「目に見えないほどに可視である」（invisibly visible）ように行うことができる。
In another embodiment, the icon displayed at the position of the "repeat"
点線３６１で示すように、図１５Ａは、数字「１５６６４」を入力するための例示的な眼球運動シーケンスを示している。この例において、繰り返し数字（すなわち「６」）は、「繰り返し」位置３６２を用いて入力される。数値入力の完結は、指定位置３６３（多くのオペレーティングシステムでは一般的に「改行」として示される）。短い数列中の誤入力は、例えば、単にこの数列を終了し（３６３）、再入力することによって、対処することができる。（国／地域コードの認識によって桁数を決定する）電話番号などの既知番号の選択を指定すると共に、「改行」位置３６３を使用して、ダイヤルを開始してもよく、または誤ったダイヤルを避けるために、入力された番号をダイヤルすることなく入力シーケンスを中止してもよい。後者の場合、最後の桁が入力されると、ダイヤルが自動的に行われる。
As shown by the dotted
図１５Ｂは、１回の（記憶誘導型）衝動性動眼で１桁の数字を選択することによって、数列を入力するための３行×４列の数字キーパッド配置３６５の代替例である。この配置は、例えば、より高い精度および範囲を利用して、（垂直方向に対して）水平方向の眼球運動を判断することができる。
FIG. 15B is an alternative example of a 3-row x 4-column
記憶誘導型眼球運動を容易にする単純な表示配置を用いて、数字、英数字のサブセット、機能を表すアイコン、共通のメニューシーケンス、一群の人を選択することができる人間の顔または他のグラフィカルな表現などから、選択を行うことができる。 Numbers, a subset of alphanumeric characters, functional icons, a common menu sequence, a human face or other graphical that allows you to select a group of people, with a simple display arrangement that facilitates memory-guided eye movements. You can make a selection from various expressions.
対話式ディスプレイは、（例えば、拡張現実ディスプレイの場合）背景を有しなく、別個の背景または（例えば、透明度、輝度、色などを制御することによって）他の画像またはビデオ上に重ね合わせた背景を有してもよい。ディスプレイ要素は、例えば、必要に応じて使用されるために、ユーザの視野内の中心に配置されてもよく、周辺に配置されてもよい。 Interactive displays do not have a background (eg, in the case of augmented reality displays) and have a separate background or a background superimposed on another image or video (eg, by controlling transparency, brightness, color, etc.). May have. The display element may be centered or peripheral in the user's field of view, for example for use as needed.
ディスプレイは、静的であってもよく、動的であってもよい。動的ディスプレイは、ユーザの入力を促進することができ、支援することができる。例えば、上述したように、計算器を使用する場合（通常、数字を入力した後に演算を行う場合）、少なくとも１個の数字が入力されなければ、演算記号（例えば、＋、−、＊、／）が表示されなくてもよく、または、結果の計算に十分な情報がなければ、結果記号／選択（例えば、等号）が表示されなくてもよい。電話番号を入力する場合、最後の数字を入力する時に表示（色、輝度など）が変化してもよく、または入力が終わる時に自動的に電話接続を行ってもよい。データ入力プロセスの様々な段階に、注意を引くために、１つ以上の方法で（例えば、明るさ、色、透明度、背景、境界線などを制御することによって）、最も頻繁に使用される選択肢および／または最も重要な選択肢をハイライトすることができる。 The display may be static or dynamic. The dynamic display can facilitate and assist the user's input. For example, as described above, when using a calculator (usually when performing an operation after inputting a number), if at least one number is not input, an operation symbol (for example, +,-, *, / ) May not be displayed, or the result symbol / selection (eg, equal number) may not be displayed if there is not enough information to calculate the result. When entering a telephone number, the display (color, brightness, etc.) may change when the last number is entered, or the telephone connection may be made automatically when the entry is completed. The most frequently used choices in one or more ways (eg, by controlling brightness, color, transparency, background, borders, etc.) to draw attention to various stages of the data entry process. And / or the most important choices can be highlighted.
複数および／または可動の作動ターゲット
他の例示的な実施形態において、（一般的に、記憶誘導型衝動性動眼によって選択された）作動ターゲットをディスプレイ上の通常の位置から別の位置に移動することによって、行われた選択に関するある特徴または性質を示すことが望ましい場合がある。追加的にまたは代替的に、（２つ以上のターゲットから）特定のターゲットの選択が作動だけでなく、Ｎ個のオブジェクトから１つのオブジェクトの選択を示す複数の作動ターゲットを表示してもよい。
Multiple and / or Movable Activating Targets In other exemplary embodiments, moving the actuating target (generally selected by the memory-guided impulsive oculomotor eye) from its normal position on the display to another position May be desirable to show certain characteristics or properties of the choices made. Additional or alternative, the selection of a particular target (from two or more targets) may display not only the activation, but also multiple activation targets indicating the selection of one object from N objects.
図１６Ａおよび図１６Ｂは、可動の作動ターゲットを示している。作動ターゲットの位置３７３ａ、３７３ｂは、選択された数列のパネル３７２内の各々の数字の挿入位置を示す。選択された数字２７２の順番を用いて、例えば、電話番号、個人識別番号、クレジットカード番号などを特定することができる。数字は、視覚信号によって、数字キーパッド３７０から選択される。数字キーパッド３７０は、１０個の数字、修正を行うために使用された「バックスペース」記号３７５、数値入力の終了を示すために使用された「改行」記号３７６を含む。
16A and 16B show a movable actuating target. The
図１６Ａにおいて、数列の入力は、第１数字（この例では、「３」）の選択から始まる。数字選択列３７０内の「３」から数字表示領域３７２内の作動位置３７３ａまで衝動性動眼３７４を実行することによって、「３」を第１数字として入力する。この場合、図１６Ｂに示すように、作動位置３７３ｂが右に１桁分シフトし、入力された「３」が表示される。これによって、数字が既に入力され、新しい作動位置３７３ｂによって示されたディスプレイ上の数字パネル３７２内の位置で次の数字を入力することができることを示す視覚的フィードバックをユーザに提供する。
In FIG. 16A, the input of the sequence begins with the selection of the first number ("3" in this example). By executing the
図１６Ｂに示す例示的なシーケンスに入力される次の数字は、「５」である。装置ユーザは、任意の回数の眼球運動によって数字キーパッドから次の数字を捜索し、数字キーパッドの外部に目を通すか、または所望の数字に直接的に注視する（３７７）。所望の数字から新たな作動ターゲットの位置３７３ｂへの衝動性動眼を行うと、次の数字が入力される。
The next number entered in the exemplary sequence shown in FIG. 16B is "5". The device user searches the numeric keypad for the next digit by any number of eye movements and either looks outside the numeric keypad or gazes directly at the desired digit (377). When an impulsive eye movement is performed from the desired number to the
したがって、入力を受け入れ、それに応じて作動ターゲットの位置をシフトするプロセスを繰り返すことによって、任意の長さの数値を入力することができる。シーケンスの終了は、所定の桁の数値の入力または特定の記号（例えば、「改行」記号３７６）の入力によって判断されてもよい。同様の方法を使用して、英数字データ、複数選択問題の回答（例えば、試験）、一連の単語または語句、一組の記号または画像からの選択などを入力することができる。 Therefore, by repeating the process of accepting the input and shifting the position of the working target accordingly, it is possible to input a numerical value of any length. The end of the sequence may be determined by the input of a number of predetermined digits or the input of a particular symbol (eg, the "line feed" symbol 376). Similar methods can be used to enter alphanumeric data, answers to multiple choice questions (eg, exams), a series of words or phrases, a selection from a set of symbols or images, and so on.
図１〜図８は、複数の作動ターゲットが存在する状況を示している。これらの例において、Ｎ個のオブジェクトから１つのオブジェクトの選択によって、作動時に文字グループから選択される文字を特定する。文字、音節、単語、語句、数字、画像、ファイル名、データセット、記号、アイコン、絵文字、機械機能、メニュー選択、適用されたパラメータまたは特性、ソフトウェアアプリケーションなどを含むグループから選択を行う時に、同様の方法（すなわち、Ｎ個の可能性および作動から１つの選択）を使用することができる。さらに、グループをクラスタ化することによって、ＵＭＩをより直観的にするために、第１選択の発見および／または作動ターゲットの指向を容易にすることができる。例えば、ある国の州を指定する場合、選択肢を（例えば、アルファベット順ではなく）その国の地理位置に従った選択マトリックス内に空間的に配列してもよい。更なる例として、一定範囲のソフトウェアアプリケーションから選択を行う場合、ビデオの表示を伴う選択肢の全ては、下方方向に沿って（ディスプレイの下方に位置する）作動ターゲットに注視することによって、選択されるように配置されてもよい。 1 to 8 show a situation in which a plurality of operating targets exist. In these examples, the selection of one object out of N objects identifies the character selected from the character group during activation. Similar when making a selection from a group that includes letters, syllables, words, phrases, numbers, images, filenames, datasets, symbols, icons, pictograms, machine features, menu selections, applied parameters or characteristics, software applications, etc. (Ie, one choice from N possibilities and actions) can be used. In addition, clustering the groups can facilitate the discovery of the first choice and / or the orientation of the actuation target in order to make the UMI more intuitive. For example, when specifying a state in a country, the choices may be spatially arranged in a selection matrix according to the geographic location of the country (eg, not in alphabetical order). As a further example, when making a selection from a range of software applications, all options with video display are selected by looking down at the working target (located below the display). It may be arranged as follows.
割り当てられた作動ターゲットまたは「スタート」オブジェクト
最も一般的なケースにおいて、視覚信号言語は、眼球運動の追跡を使用して、１つ以上のディスプレイ上の実在オブジェクトおよび仮想オブジェクトの任意の組み合わせを有するユーザの意図を特定することができる。１つ以上のディスプレイ上の仮想オブジェクトの位置を含む特性は、ネットワークを介して接続されたリモートプロセッサ、サーバまたはストレージ設備と同様に、装置またはこの装置を含むシステムに知られてもよい。したがって、装置は、１つ以上の仮想オブジェクトに向けられた眼球運動に意味を容易に割り当てることができる。
Assigned Actuation Target or "Start" Object In most common cases, the visual signal language uses eye movement tracking to allow a user to have any combination of real and virtual objects on one or more displays. Can identify the intent of. Characteristics including the location of virtual objects on one or more displays may be known to the device or the system containing the device, as well as remote processors, servers or storage equipment connected over the network. Therefore, the device can easily assign meaning to eye movements directed at one or more virtual objects.
しかしながら、装置着用者の環境における実在オブジェクトの（位置を含む）特性は、一般的に装置によって制御されない。例えば、会議室にいる装置ユーザによって見られたオブジェクトは、自動車を運転している間に見られたものとは実質的に異なる。したがって、視覚信号言語は、このような現実世界の変化に対応できなければならない。対応は、装置制御ディスプレイを使用せず、ユーザの意図を特定する状況に対応することを含む。 However, the properties (including position) of a real object in the device wearer's environment are generally not controlled by the device. For example, the objects seen by a device user in a conference room are substantially different from those seen while driving a car. Therefore, the visual signal language must be able to respond to such changes in the real world. Response includes responding to situations that identify the user's intent without using the device control display.
上述したように、可能な選択を特定するときにまたは実在オブジェクトの特定の選択を行うときに、聴覚的フィードバックまたは他のフィードバックを提供することができる。このシナリオに含まれる重要な側面は、選択された操作を実行または作動するための眼球運動の特定である。換言すれば、ディスプレイがない場合、何かが作動アイコンまたは「スタート」アイコンの役割を果たすか？
例示的な実施形態において、これは、１つ以上の割り当て可能な「スタート」オブジェクトの理解につながる。換言すれば、実世界の環境内の１つ以上のオブジェクトは、永続的または一時的に、１つ以上の「スタート」アイコンとして使用することができる。ディスプレイを使用する場合、特定された実在オブジェクトへの機能の割当は、ユーザが視覚的な補強、信号、アクチベーションまたはディスプレイ生成の遷移グラフィックスの有無に拘わらず、実在オブジェクトを見ることを含む視覚信号言語に適合する。
As mentioned above, auditory feedback or other feedback can be provided when identifying possible choices or when making certain choices of real objects. An important aspect involved in this scenario is the identification of eye movements to perform or activate the selected operation. In other words, if there is no display, does something act as an activation icon or a "start" icon?
In an exemplary embodiment, this leads to an understanding of one or more assignable "start" objects. In other words, one or more objects in the real world environment can be used permanently or temporarily as one or more "start" icons. When using a display, assigning functionality to an identified real object involves visual signals that include viewing the real object by the user with or without visual reinforcement, signal, activation, or display-generated transition graphics. Fits the language.
例示的な実施形態において、最初に実在オブジェクトに注視してから、視線をディスプレイ上の「表示」アイコンに移動し、その後、新たに表示された「割当」アイコンに注視することによって、実在オブジェクトに「スタート」機能または作動機能を割り当てられることができる。他の例示的な眼球運動結合シーケンスは、視線をオブジェクトから表示された「スタート」スポットまたは他の可能な作動スポットに移動する衝動性動眼、またはオブジェクトを表示されたオブジェクトおよび他の実在オブジェクトに組み込む衝動性動眼および／または追跡シーケンスを含む。シーンカメラによって観察されたように、現実世界の移動オブジェクトの動き追跡およびユーザによって動かされたオブジェクトの視線追跡、例えば、オブジェクトに指しているユーザの指または移動しているオブジェクトに追従する指を追跡する視線追跡は、オブジェクトの機能の割り当ておよび／または選択または作動に使用できる言語に追加することができる。 In an exemplary embodiment, the real object is first gazed at, then the line of sight is moved to the "display" icon on the display, and then the newly displayed "assign" icon is gazed at. A "start" function or activation function can be assigned. Other exemplary eye movement coupling sequences incorporate an impulsive googlymium that moves the line of sight from an object to a displayed "start" spot or other possible action spot, or an object into a displayed object and other real objects. Includes impulsive oculomotor eye and / or tracking sequence. Tracking the movement of moving objects in the real world and the line of sight of objects moved by the user, as observed by the scene camera, for example, tracking the user's finger pointing at the object or the finger following the moving object. Gaze tracking can be added to the language that can be used to assign and / or select or act on the function of the object.
この実施形態の一例は、本物の本または仮想の本または画面上の本に書かれた単語、画像またはページなどのオブジェクトを指している指を追跡することによって、以下の３つのうち、いずれか１つを行うことができる。 An example of this embodiment is one of the following three by tracking a finger pointing at an object such as a word, image or page written in a real or virtual book or a book on the screen. You can do one.
１）単語をタッチまたは指した時に、その単語または関連する文章、段落またはページの読み上げを実行することができる。 1) When a word is touched or pointed, the word or related sentences, paragraphs or pages can be read aloud.
２）任意のオブジェクトをタッチするまたはを指すことに基づいて、音声の再生または他の操作を実行することができる。例えば、ユーザが本の画像または絵を注視するまたは指すまたはタッチするときに、音声を再生することができ、または動画キャラクタが注視、タッチまたは注視タッチの組み合わせに反応することができる。 2) You can play audio or perform other operations based on touching or pointing to any object. For example, when a user gazes, points to, or touches an image or picture in a book, the audio can be played, or the moving image character can react to a combination of gaze, touch, or gaze touch.
３）オブジェクトに機能を割り当てることによって、相互作用（例えば、タッチ、押し潰しまたは他の相互作用、指し、くすぐり、注視）で、オブジェクトの音声、移動または他の変化などの動作を引き起こすことができる。 3) By assigning a function to an object, the interaction (eg, touch, crush or other interaction, pointing, tickling, gaze) can cause an action such as voice, movement or other change of the object. ..
例示的な実施形態において、ディスプレイを有しない状況において、眼球運動を用いて、「スタート」機能または作動機能を割り当てることができる。このような割り当ては、所望の実在オブジェクトに注視すること、続いて認識可能な独特の眼球運動を行うことを含む。例として、（例えば、所定の最小移動回数を有する）複数の眼球運動は、実在オブジェクトの周りに略円形のパターンで行われてもよく、または、ユーザは、所謂内斜視信号を用いて、作動機能を割り当てもよい。 In an exemplary embodiment, eye movements can be used to assign a "start" or actuation function in the absence of a display. Such an assignment involves gazing at the desired real object, followed by a unique recognizable eye movement. As an example, multiple eye movements (eg, having a predetermined minimum number of movements) may be performed in a substantially circular pattern around a real object, or the user may operate using a so-called esotropia signal. Functions may be assigned.
また、追加の例示的な実施形態として、ユーザは、注視位置と、自発的に制御されている身体動作とを組み合わせることによって、作動制御を割り当てることができる。ユーザが実在オブジェクトから特定の身体部分（例えば、指、親指、手、足）に視線を移動した場合、この身体部分の注視および／または所定の動きを用いて、ユーザがこの実在オブジェクトを作動ターゲットまたは「スタート」ターゲットにしたい意向を示すことができる。代わりに、ユーザが（例えば、特定の指またはポインティング装置を用いて）オブジェクトに指していると見なされた場合に、指したオブジェクトを作動ターゲットにすることができる。同様に、ユーザがオブジェクトを拾い上げ、所定の時間および／または移動パターンで観察する場合、（いずれかの場所に置かれた）このオブジェクトは、作動ターゲットまたは「スタート」ターゲットとして見なされてもよい。 Also, as an additional exemplary embodiment, the user can assign motion control by combining a gaze position with a spontaneously controlled body movement. When the user moves his or her line of sight from a real object to a specific body part (eg, fingers, thumbs, hands, feet), the user activates this real object using the gaze and / or predetermined movement of this body part. Or you can indicate your intention to be a "start" target. Alternatively, if the user is deemed to be pointing at an object (eg, using a particular finger or pointing device), the pointed object can be the actuation target. Similarly, if a user picks up an object and observes it at a given time and / or movement pattern, the object (placed somewhere) may be considered as a working target or a "start" target.
実在の作動オブジェクトまたはスタートオブジェクトは、例えば、壁上のマーク、標識、数字、単語、扉、灯などを含む環境内の任意のものであってもよい。オブジェクトは、印刷物、おもちゃ、他の不動なオブジェクトまたは可動なオブジェクトであってもよい。作動機能または「スタート」機能は、実在オブジェクトの一部、例えば、テレビジョンの端部、窓枠の隅部、ドアの取手に割り当てられてもよい。機能は、一意に特定可能な（すなわち、１つ以上の特定可能な区別的特性を有する）特定のオブジェクトまたは当技術分野で知られているオブジェクト認識アルゴリズムを用いて識別できる光スイッチ、電気コンセント、ペン、本、靴などの一般的なオブジェクトクラスに割り当てることができる。 The real working object or start object may be anything in the environment, including, for example, marks, signs, numbers, words, doors, lights, etc. on the wall. The object may be printed matter, toys, other immovable objects or movable objects. Activating or "starting" functions may be assigned to parts of a real object, such as the edge of a television, the corner of a window frame, or the handle of a door. Features include optical switches, electrical outlets, which can be uniquely identified (ie, have one or more identifiable distinctive properties) or can be identified using object recognition algorithms known in the art. It can be assigned to common object classes such as pens, books, and shoes.
１つ以上の実在オブジェクトに「スタート」機能または作動機能を割り当てた後、視覚信号言語の他の要素を使用することができる。例えば、リモート制御されている照明灯を点灯または消灯に切り替えたいユーザは、まずスイッチに注視し、その後視線を「スタート」オブジェクトに移動することができる。一般的に、実在の作動ターゲットは、作動されたときに（実在の）選択メニューを表示できないため、注視された１つ以上の実在オブジェクトの（ＩＤを含む）コンテキストに大きく依存する。例えば、スイッチは照明灯を点灯または消灯することができる一方、（ユーザが特定のドアを開くように認証された場合に）ドアを見ると、ドアを開くことができる。コンテキストは、他の要素、例えば、時刻（例えば、夜間には点灯のみ）、活動履歴（ユーザが消された照明灯の点灯のみ）、環境の明るさ、環境に音声または認識可能な発話の有無を含むことができる。 After assigning a "start" or actuation function to one or more real objects, other elements of the visual signal language can be used. For example, a user who wants to turn a remotely controlled light on or off can first gaze at the switch and then move his gaze to the "start" object. In general, a real actuation target is largely dependent on the context (including ID) of one or more real objects being watched, as it cannot display the (real) selection menu when activated. For example, a switch can turn a light on or off, while looking at a door (when the user is authenticated to open a particular door) can open the door. Contexts include other factors, such as time of day (eg, only lights at night), activity history (only lights that are turned off by the user), brightness of the environment, and the presence or absence of voice or recognizable speech in the environment. Can be included.
複数の作動ターゲットまたは「スタート」オブジェクト
視覚信号言語は、単一の「スタート」位置または作動位置に制限されない。更なる実施形態において、異なる位置に位置する複数の「スタート」アイコンまたは作動ターゲットに、異なる操作を割り当てることができる。これによって、ユーザは、「スタート」位置に注視するときに、作動に加えて、Ｎ個の作動ターゲットから１つのオブジェクトを素早く選択することができる。
Multiple actuation targets or "start" objects The visual signal language is not limited to a single "start" position or actuation position. In a further embodiment, different operations can be assigned to multiple "start" icons or actuation targets located at different locations. This allows the user to quickly select one object from N actuation targets in addition to actuation when gazing at the "start" position.
上記で概説したように、現実環境内の複数のオブジェクトに、（コンテキストを考慮して）同一または異なる機能を割り当てもよい。現実環境には多数の異なるオブジェクトが存在するため、ユーザは、多数のオブジェクトに機能を割り当てることによって、信号伝達効率を高めることができる。このことは、オフィスまたは家族部屋などの安定した既知環境に特に有効である。 As outlined above, multiple objects in the real environment may be assigned the same or different functionality (in consideration of context). Since there are many different objects in the real environment, the user can improve the signal transmission efficiency by assigning functions to many objects. This is especially useful in stable, known environments such as offices or family rooms.
ユーザのディスプレイ環境がより制限された場合でも、いくつかのアプリケーションにとって、複数の作動位置を使用する方がより効果的であり得る。この一般概念の例は、図１〜図８に示されている。これらの例において、複数の作動ターゲットを用いて、文字、数字および他の機能の可能な選択グループから選択を特定する。４つの作動ターゲットを４隅またはディスプレイの４辺に配置することは、特に効果的であり、使い易い。作動ターゲットを所定の方向に沿って空間的に十分に隔離することによって、ユーザによる選択肢の特定がより直感的になり、（空間的に隔離されたターゲットによって）装置の意図しない作動を回避する。 Even if the user's display environment is more restricted, it may be more effective to use multiple operating positions for some applications. Examples of this general concept are shown in FIGS. 1-8. In these examples, multiple actuation targets are used to identify selections from possible selection groups of letters, numbers and other functions. Placing the four actuating targets at the four corners or on the four sides of the display is particularly effective and easy to use. By sufficiently spatially isolating the actuation target along a predetermined direction, the user's identification of choices becomes more intuitive and avoids unintended actuation of the device (by the spatially sequestered target).
衝動性動眼を用いて連続範囲から選択を行う
前述したように、「連続」入力範囲に基づいてユーザ入力を指定する（例えば、スピーカ音量を制御する）ために、滑動性追跡眼球運動を使用して、ユーザが選択可能な時間および／または距離に亘ってオブジェクトを追跡することができる。更なる例示的な実施形態において、衝動性動眼を用いて、連続範囲または「目盛り」から選択を行うことができる。目盛りは、任意種類のグラフィック表現である。ユーザは、視線を用いて表示された目盛りの全範囲から（任意数の可能な位置から）特定の位置を指定することに基づいて、入力を指定することができる。具体的には、選択を特定するように、表示された目盛りの最下位値および最上位値の位置に対して、注視位置を決定することができる。
Making a selection from a continuous range using the impulsive oculomotor eye As mentioned above, use gliding tracking eye movements to specify user input (eg, control speaker volume) based on a "continuous" input range. The object can be tracked over a user-selectable time and / or distance. In a further exemplary embodiment, the impulsive googlymium can be used to make a selection from a continuous range or a "scale". The scale is any kind of graphic representation. The user can specify the input based on specifying a specific position (from any number of possible positions) from the entire range of the scale displayed using the line of sight. Specifically, the gaze position can be determined with respect to the position of the lowest value and the highest value of the displayed scale so as to specify the selection.
衝動性動眼を用いて連続範囲から選択を行うときの重要な課題は、衝動性動眼が本質的に不連続であり、１つのターゲット位置から別のターゲット位置にジャンプするという事実から生じる。したがって、衝動性動眼を用いて連続選択を行うときの重要な考慮点は、（一般的に、時間的および空間的な平均化を使用して）特定のターゲットに向けられた衝動性動眼内の測定位置をフィルタリングすることである。 An important challenge when making a selection from a continuous range with the impulsive occlusal eye arises from the fact that the impulsive occlusal eye is essentially discontinuous and jumps from one target position to another. Therefore, an important consideration when making continuous selections with the impulsive oglymium is within the impulsive gonadly eye directed to a particular target (generally using temporal and spatial averaging). It is to filter the measurement position.
前述したように、「何も見えない」領域に衝動性動眼を行うことは生理学的に困難である。したがって、衝動性動眼を用いて連続選択を行う場合においても、衝動性動眼のターゲットとして個別の焦点位置を提供することが重要である。図１７Ａは、グラフィックまたは目盛り３８０の例を示している。目盛り３８０は、衝動性動眼に基づいて連続選択を行うために使用され得る焦点（すなわち、注視ターゲット）位置を有する。
As mentioned above, it is physiologically difficult to perform impulsive oculomotor eye in the "invisible" area. Therefore, it is important to provide individual focal positions as targets for the impulsive oculomotor eye, even when continuous selection is performed using the impulsive oculomotor eye. FIG. 17A shows an example of a graphic or
焦点位置は、定規と同様に中心軸３８５を横切る等間隔に排列された目盛マーク３８１ａ、３８１ｂを用いて形成される。軸３８５と目盛マーク３８１ａ、３８１ｂとの交差点は、焦点を形成する。目盛マークの反復性を解消するために、巻尺のマークと同様に、規則的な間隔で長い目盛マーク３８１ｂを短い目盛マーク３８１ａの間に挿入する。目盛マーク、点、矢印、色の変化などの標記を用いてそれぞれ形成された目盛りに沿って焦点を配置することによって、連続選択の他の図形表示をダイアル盤、（液体充填の）機械温度計の表示、注射器の指示などに適用することができる。任意の方向（水平方向、垂直方向、径方向状および／または周方向を含む）に沿って衝動性動眼を行うことによって、選択を示すことができる。
The focal position is formed by using the scale marks 381a and 381b arranged at equal intervals across the
図１７Ａにおいて、例示的な基準注視位置は、黒丸で示されている。図１７Ｂに示すように、これらの基準位置３８２の移動平均に基づいて、目盛り３８０の全体の選択範囲に対して最初の選択を示す図形表示３８３がユーザに提供される。この時点で、ユーザは、左側の焦点位置３８４ａまたは右側の焦点位置３８４ｂに衝動性動眼を行うことによって、選択値を減少または増加することができる。必要に応じて、左側または右側の衝動性動眼によって選択を行う速度は、ユーザが最初の選択から視線を左側または右側に移動した距離に基づくことができる。ユーザが選択に満足すると、選択処理が完了したことを示すように、目盛りの領域から視線を離すことまたは作動ターゲットに視線を向けることができる。
In FIG. 17A, exemplary reference gaze positions are indicated by black circles. As shown in FIG. 17B, based on the moving averages of these
マルチモーダルフィードバック
更なる例示的な実施形態において、ユーザ選択中のフィードバックは、表示された目盛りまたは他の目盛り表現に関連するもの以外のモダリティを用いて行われてもよい。例えば、音声ボリュームを調整する場合、音量の選択を支援するために、選択処理中に（指定のレベルで）サウンドを生成することができる。更なる例として、表示された画像の明るさ、色、色相、透明度、明暗度、サイズ、詳細レベルおよび／または形状は、選択処理中にユーザによって行われた選択に対して、比例、反比例または非線形の態様で変更することができる。
Multimodal Feedback In a further exemplary embodiment, feedback during user selection may be provided using modality other than that related to the displayed tick marks or other tick marks. For example, when adjusting the audio volume, sound can be generated (at a specified level) during the selection process to assist in volume selection. As a further example, the brightness, color, hue, transparency, brightness, size, level of detail and / or shape of the displayed image may be proportional, inversely proportional or proportional to the selection made by the user during the selection process. It can be changed in a non-linear manner.
フィードバックは、ユーザ装置に対して遠隔（しかしながら、通信している）装置に提供されてもよい。遠隔モダリティの例は、遠隔ディスプレイの画面および照明の変更、モバイル装置によって生成された音を含む音の遠隔生成、環境温度の制御、デジタル読出しなどの標識などを含む。 Feedback may be provided to devices that are remote (but communicating) to the user device. Examples of remote modality include changing the screen and lighting of remote displays, remote generation of sounds, including sounds produced by mobile devices, environmental temperature control, signs such as digital readout, and the like.
他の非ディスプレイ関連モダリティの使用は、ユーザの視線を引くディスプレイの変化を回避することによって、選択処理中に不注意な眼球運動の減少に役立つことができる。更なる例示的な実施形態において、ある形態のマルチモーダルフィードバックを用いて、別の形態のモダリティの変化を表すことができる。例えば、ピッチ（すなわち、サウンドの振動数）を用いてズームのレベルを示す場合、画像の拡大または「ズーム」倍率は、聴覚信号として表すことができる。同様にまたは別の例として、ビデオ表示中のフレームレート、テキストのスクロールレート、静止画表示中の「ページめくり」レートなどの任意種類の「レート」に関連する制御の選択中に、クリック音（または他の音）の繰り返しレートを増加および／または減少させることができる。 The use of other non-display related modality can help reduce inadvertent eye movements during the selection process by avoiding changes in the display that gaze at the user. In a further exemplary embodiment, one form of multimodal feedback can be used to represent another form of modality change. For example, when using pitch (ie, the frequency of sound) to indicate the level of zoom, the magnification or "zoom" magnification of the image can be expressed as an auditory signal. Similarly or as another example, a click sound (as well as another example) during the selection of controls related to any kind of "rate" such as frame rate during video display, text scroll rate, "page turning" rate during still image display. Or other sounds) can increase and / or decrease the repetition rate.
追加の例示的な実施形態において、代表画像の微妙な変化は、「目に見えないほどに可視である」（invisibly visible）ようにすることができる。換言すれば、ユーザが視覚要素の存在を認知的に認識しているときに、オブジェクトが現在の視点からユーザの傍中心窩または周辺視野で感知または確認することができるが、ユーザが視覚的要素の存在の可能性を認知的に認識していないときに、オブジェクトが「不可視」であるまたは認知されない。これは、ユーザの視線をターゲット、アイコン、または制御されている状態の変化を示す指示に引き込まないように実行されてもよい。 In an additional exemplary embodiment, subtle changes in the representative image can be made "invisibly visible". In other words, when the user is cognitively aware of the presence of a visual element, the object can perceive or confirm from the current point of view in the user's fovea centralis or peripheral vision, but the user has the visual element. An object is "invisible" or unrecognized when it is not cognitively aware of its possible existence. This may be done so that the user's line of sight is not drawn to a target, icon, or instruction indicating a change in controlled state.
これらの方法は、衝動性動眼による選択を行う間に不注意な眼球運動を容易に引き起こす（すなわち、注意力を分散する）ように画像を激しくズームすることなど、ディスプレイの激しい変化をフィードバックにすることを回避する。ユーザは、特定の（すなわち、記憶された）ピッチまたはクリック回数が特定のズームレベルまたは特定の繰り返しレートを表すことをすぐに知ることができる。同様の方法は、他の連続入力選択に適用することができる。 These methods feed back drastic changes in the display, such as zooming the image violently to easily cause inadvertent eye movements (ie, disperse attention) while making selections with impulsive eye movements. Avoid that. The user can immediately see that a particular (ie, remembered) pitch or click count represents a particular zoom level or particular repeat rate. A similar method can be applied to other continuous input selections.
特に、聴覚フィードバックは、実在オブジェクトを含む選択を行う時に有用である。例えば、ユーザが、（例えば、注視位置と共にシーンカメラを用いて）装置によって認識される照明スイッチのようなオブジェクトに注視する場合、オブジェクトが既に認識されており、操作に利用可能であるという指示が、聴覚指示（例えば、ピン、チン、リン、クリック、チャイムなど）で示されてもよい。その後、視線をオブジェクトから作動アイコンまたは「スタート」アイコンに移動すると、点灯などの操作を引き起こすことができる。 In particular, auditory feedback is useful when making selections that include real objects. For example, when the user gazes at an object such as a lighting switch recognized by the device (eg, using a scene camera with the gaze position), the instruction that the object is already recognized and available for operation is instructed. , May be indicated by auditory instructions (eg, pin, chin, phosphorus, click, chime, etc.). Then, moving the line of sight from the object to the activation icon or the "start" icon can trigger operations such as lighting.
追加の実施形態において、聴覚フィードバックおよび／または他の（例えば、振動、嗅覚、触覚）フィードバックを調律することによって、実在オブジェクトのいくつかの性質を示すことができる。例えば、オブジェクトが顔であり、（オブジェクト認識アルゴリズムを用いて）顔のデータベースから特定された場合、ある種類の聴覚指示（例えば、第２トーンが高くなるダブルトーン）をフィードバックとしてユーザに提供することができる。一方、オブジェクトが顔として認識されたが、顔のデータベースから特定されなかった場合、異なる聴覚変調、振幅および／または指示（例えば、第２トーンが低くなるダブルトーン）をフィードバックとしてユーザに提供することができる。いずれの場合、ユーザは、衝動性動眼による作動を行うことができる。しかしながら、作動による操作は、装置によるオブジェクトの認識および／または特定に応じて、異なる場合がある。 In additional embodiments, some properties of real objects can be demonstrated by tuning auditory feedback and / or other (eg, vibration, olfactory, tactile) feedback. For example, if the object is a face and is identified from the face database (using an object recognition algorithm), provide the user with some kind of auditory instruction (eg, a double tone with a higher second tone) as feedback. Can be done. On the other hand, if the object is recognized as a face but not identified from the face database, provide the user with different auditory modulation, amplitude and / or indication (eg, a double tone with a lower second tone) as feedback. Can be done. In either case, the user can perform the operation by the impulsive oculomotor eye. However, the operation by operation may differ depending on the recognition and / or identification of the object by the device.
これらの場合に触覚フィードバックを提供することでき、または振動、温度、電圧、圧力を含む目に関連する対話に応じて触覚フィードバックを提供することができる。振動、温度、電圧、圧力のいずれかまたはその組合せは、情報の所望の通信に応じて変化することができる。このような触覚フィードバックは、手袋、指輪、ブレスレット、ネックレス、頭部装着物、下着などに接続した装着可能な装置を介して、伝達することができる。 Tactile feedback can be provided in these cases, or can be provided in response to eye-related interactions including vibration, temperature, voltage, and pressure. The vibration, temperature, voltage, pressure, or combination thereof can vary depending on the desired communication of information. Such tactile feedback can be transmitted via wearable devices connected to gloves, rings, bracelets, necklaces, headwear, underwear and the like.
更なる実施形態において、音源を定位する立体音響性質および／または能力は、ユーザにフィードバックを与えるための別の可能性を提供する。このことは、一般的に２つ以上のスピーカを用いて達成することができるが、ユーザ装置のフィードバックシステムの構成要素として１対のイヤプラグまたはヘッドフォンを使用すると、特に効果的である。基本的に、音源の方向の感知は、主に両耳に聞こえた音の位相差によって達成され、距離の感知は、両耳に聞こえた音の振幅差によって達成される。この一般的な現象は、当技術分野では頭部伝達関数（ＨＲＴＦ）として知られている。 In a further embodiment, the stereophonic properties and / or ability to localize the sound source provide another possibility for giving feedback to the user. This can generally be achieved with two or more speakers, but it is particularly effective to use a pair of earplugs or headphones as a component of the feedback system of the user device. Basically, the sense of the direction of the sound source is achieved mainly by the phase difference of the sounds heard by both ears, and the sense of the distance is achieved by the difference in the amplitude of the sounds heard by both ears. This common phenomenon is known in the art as a head related transfer function (HRTF).
感知された方向および距離を別々にまたは共に使用して、ユーザにフィードバックを提供することができる。断続モード（例えば、個々のクリックまたは他の音）または連続モード（例えば、連続サイレン）で、方向および／または距離の変化を使用することができる。音の方向は、何らかの形で実行されている操作に関連付けられてもよく（例えば、ディスプレイ上のオブジェクトを左に動かす場合、左側から出るように聞こえる音を生成する）、または実行されている操作と完全に無関係であってもよい（音が右側から出る場合「はい」を示し、音が左側から出る場合「いいえ」を示す）。 The perceived directions and distances can be used separately or together to provide feedback to the user. Directional and / or distance changes can be used in intermittent mode (eg, individual clicks or other sounds) or continuous mode (eg, continuous siren). The direction of the sound may be associated with the operation being performed in some way (for example, if you move an object on the display to the left, it will produce a sound that sounds like it is coming out of the left side), or the operation being performed. May be completely irrelevant (indicates "yes" if the sound comes from the right side and "no" if the sound comes out from the left side).
同様に、感知された距離は、何らかの形で実行されている操作に関連付けられてもよく（例えば、画像をズームインする時に音が近くに現れ、ユーザが正解に迫るときに音が近くに現れ、または遠隔装置に設定をした時に遠音が感知される）、または実行されている操作と完全に無関係であってもよい（例えば、ユーザの近くに現れる音がメッセージの緊急性を示す）。 Similarly, the perceived distance may be associated with some form of operation being performed (eg, a sound appears nearby when zooming in on an image, and a sound appears nearby when the user approaches the correct answer, Alternatively, a distant sound may be detected when the remote device is set up), or it may be completely unrelated to the operation being performed (for example, a sound appearing near the user indicates the urgency of the message).
物理的なオブジェクトに適用されたギャップ効果
ギャップ効果は、視覚信号言語内でユーザのやり取りを加速するための基本ツールとして使用される。前述したように、衝動性動眼の結果として見られようとしているオブジェクトは、視界から取り除かれ、ユーザの視線を「解放」することによって、（オブジェクトをユーザの視界に残す場合よりも）速く他のオブジェクトを見ることができる。適切な時間でディスプレイ上の仮想オブジェクトを除去すると、ギャップ効果が生じることができる。
Gap effects applied to physical objects Gap effects are used as a basic tool for accelerating user interaction within the visual signal language. As mentioned earlier, an object that is about to be seen as a result of impulsive oculomotor eye is removed from the field of view and by "releasing" the user's line of sight, the other object is faster (than leaving the object in the user's field of view). You can see the object. Gap effects can occur by removing virtual objects on the display in a timely manner.
物理的なオブジェクトまたは実在オブジェクトを見る場合、通常、衝動性動眼中にユーザの視野から実在オブジェクトを除去することができない。しかしながら、拡張現実（ＡＲ）装置または複合現実（ＭＲ）装置を使用すると、視界からオブジェクトを除去することが可能である。この例示的な実施形態において、最初に、ＡＲ装置またはＭＲ装置の１つ以上の領域を実質的に透明にすることによって、ＡＲ装置またはＭＲ装置を通して実在オブジェクトを見ることができる。その後、ユーザが作動ターゲットに注視する場合、実在の作動ターゲットの方向に対応するＡＲ装置またはＭＲ装置の透明度を減少するまたはなくすことによって、ユーザの視界からオブジェクトを除去することができる。 When looking at a physical or real object, it is usually not possible to remove the real object from the user's visual field during impulsive eye movements. However, augmented reality (AR) or mixed reality (MR) devices can be used to remove objects from view. In this exemplary embodiment, the real object can be seen through the AR or MR device by first making one or more areas of the AR or MR device substantially transparent. If the user then gazes at the actuating target, the object can be removed from the user's field of view by reducing or eliminating the transparency of the AR or MR device that corresponds to the orientation of the real actuating target.
必要であれば、部分的または完全に不透明な領域は、ユーザの注意を引かない中立地帯にされてもよい。これは、例えば、単色、色の空間的勾配、擬似ランダムノイズ、見慣れた画像、またはオブジェクトの周囲の可視領域に基づいて作動ターゲットの背景の推定を含む。さらに、ユーザの注意を引かないように、透明度の低い領域のエッジの周りの領域は、不透明（すなわち、オブジェクトのユーザの視界を妨げる）から完全透明（すなわち、オブジェクトが見える）になるまで徐々に移行することができる。これによって、ユーザが透明度の低い領域の絵部を検知するおよび／または見る可能性が低くなる。 If desired, the partially or completely opaque area may be a neutral zone that does not attract the user's attention. This includes, for example, estimating the background of the working target based on monochromatic, spatial gradient of color, pseudo-random noise, familiar images, or visible areas around the object. In addition, to avoid getting the user's attention, the area around the edge of the less transparent area gradually changes from opaque (ie, obstructing the user's view of the object) to fully transparent (ie, the object is visible). Can be migrated. This reduces the likelihood that the user will detect and / or see the picture area in the less transparent area.
衝動性動眼中および／または衝動性動眼後に、ＡＲ技術を用いて目標オブジェクトの視認を完全にまたは部分的にブロックすると、実際の作動ターゲットを用いたギャップ効果を作ることができる。オブジェクトを再表示するために、ユーザの注意を引かないように設計された前述した技術を用いて、ＡＲ装置の透明度を戻すことができる。例えば、時間が十分にある場合またはユーザが（例えば、衝動性動眼中および／または瞬き中）機能的に「失明」である場合、透明度を徐々に戻してもよい。 Using AR technology to completely or partially block the visibility of the target object during and / or after impulsive oculomotor eye movement can create a gap effect with the actual working target. In order to redisplay the object, the transparency of the AR device can be restored using the above-mentioned technique designed not to attract the user's attention. For example, the transparency may be gradually restored if there is sufficient time or if the user is functionally "blind" (eg, during impulsive eye movement and / or blinking).
コーテシ事項
コーテシ（courtesy）は、全ての対話式および／または所謂「スマート」装置の共通事項である。多くの場合、例えばビジネス事情および社会事情によって、ユーザは、緊急通信以外の全てのサービスを一時停止することによって、着信電話またはデータの受信を制限したい場合がある。しかしながら、日常生活中、ビジネス事情および社会事情の前に、装置を「コーテシ」モードまたは「サイレンス」モードに設置することを容易に忘れてしまうことがある。これらの事情が終わったときに、同様に、装置を通常操作に戻すことを忘れてしまうこともある。
Courtesy Matters Courtesy is a common denominator for all interactive and / or so-called "smart" devices. In many cases, for example, due to business and social circumstances, a user may wish to restrict incoming calls or data reception by suspending all services except emergency communications. However, in daily life, it is easy to forget to install the device in "courtesy" mode or "silence" mode before business and social circumstances. Similarly, when these circumstances are over, it may be forgotten to return the device to normal operation.
所定の好み設定に基づいて、ヘッドセットおよび／またはディスプレイは、コーテシモードにする必要のある状況（例えば、関連性の高いオブジェクトおよび／または個人）を認識した場合に、着信通知および情報を一時的に「フィルタリング」することができる。例えば、装置は、着用者が会話中であることを認識することができる。この状況は、マイクを用いて記録された着用者の声の認識および／または環境に向いているシーンカメラを用いて記録された装置着用者の方向に話している人の画像の認識から決定することができる。シーンカメラ画像内の人の視線方向（例えば、装置着用者に向けられている場合または装置着用者と十分近い場合、中断されるべきではない親密または個人的な接触を示唆する）および近くにいる人の口が動いているかに特に注意を払う必要がある。「コーテシモード」を呼び出すべきか否かを自動的に決定するために使用できる他の情報は、時刻、曜日（例えば、勤務日対週末）、地理位置、屋内環境、屋外環境、および他のパターンを含む。 Based on certain preference settings, the headset and / or display will temporarily notify you of incoming calls and information when it recognizes a situation (eg, a relevant object and / or individual) that needs to be in courtesy mode. Can be "filtered". For example, the device can recognize that the wearer is in a conversation. This situation is determined from the recognition of the wearer's voice recorded with a microphone and / or the recognition of an image of a person speaking in the direction of the device wearer recorded with an environmentally friendly scene camera. be able to. The direction of the person's line of sight in the scene camera image (eg, when directed at or close enough to the device wearer, suggesting intimate or personal contact that should not be interrupted) and close Particular attention should be paid to whether a person's mouth is moving. Other information that can be used to automatically determine whether to call "Courtesy Mode" includes time, day of the week (eg workday vs. weekend), geographic location, indoor environment, outdoor environment, and other patterns. Including.
特定の人物、環境または場所を認識したときに、コーテシモードを呼び出すことができる。例えば、特定の人（例えば、ボスまたは牧師）と話すときに、中断が不適切である。航空機の内部を認識した場合、コーテシモード（もしくは「機内モード」）を呼び出すことができる。学校、病院、映画館、および他の施設内のエリアは、地理位置および／またはシーンカメラを使用した特定に基づいて「コーテシモード」ゾーンとして特定することができる。（自動車内部テンプレートのデータベースに基づいて）ユーザの優先注意を必要とする任意（自動車または他の機械）の制御つまみと共に、自動車の内部を認識することができる。 You can call the courtesy mode when you recognize a specific person, environment or location. For example, interruptions are inappropriate when talking to a particular person (eg, a boss or minister). If it recognizes the inside of the aircraft, it can call the courtesy mode (or "airplane mode"). Areas within schools, hospitals, cinemas, and other facilities can be identified as "courtesy mode" zones based on geographic location and / or identification using scene cameras. The interior of the vehicle can be recognized, along with any (automobile or other machine) control knob that requires the user's priority (based on a database of vehicle internal templates).
最も一般的な意味において、着用者の注意散漫を許すべきでない状況または他の人または主体が注意散漫を感知できる状況は、「コーテシモード」機能を用いて管理することができる。「コーテシ」状況または中断できない状況を認識するように訓練された神経ネットに基づく画像解析を用いて、１つ以上のコーテシモードを実施することができる。 In the most general sense, situations where the wearer's distraction should not be tolerated or where another person or subject can perceive distraction can be managed using the "coating mode" function. One or more courtesy modes can be performed using neural net-based image analysis trained to recognize "coating" or uninterrupted situations.
ユーザの好みに基づいて、異なるレベルのデータフィルタリングを予め設定することができる。例示的なシナリオとして、いくつかの状況において、電子メールおよび他の通知をブロックしながら、テキストおよびアラートを許可することができる。情報の表示または保留（すなわち、後で表示される）は、情報源および／または現在の活動に依存し得る。例えば、サイクリングまたはハイキング中に生理学的モニタリング（例えば、心拍数、呼吸数）およびナビゲーション情報を表示することができるが、活動が完了するとこれらをさらに表示しない。他の状況において、緊急であると見なされたものを除いて、全ての通信がブロックまたはバッファされる（すなわち、収集されるが表示されない）。 Different levels of data filtering can be preset based on user preference. As an exemplary scenario, text and alerts can be allowed while blocking email and other notifications in some situations. The display or hold of information (ie, displayed later) may depend on the source and / or current activity. For example, physiological monitoring (eg, heart rate, respiratory rate) and navigation information can be displayed during cycling or hiking, but not further when the activity is complete. In other situations, all communications are blocked or buffered (ie, collected but not visible) except those deemed urgent.
同様に、状況に応じて、通知の様式を変更することができる。異なるコーテシモード状況において、一般的に可聴音に関与する通知は、単に画面メッセージを表示するおよび／または聞こえない振動を行う「サイレンス」モードに切り換えることができる。 Similarly, the format of the notification can be changed depending on the situation. In different courtesy mode situations, notifications that typically involve audible sound can be switched to "silence" mode, which simply displays screen messages and / or vibrates inaudibly.
別の例示的な実施形態において、コーテシモードになる前に、通知を遅延してもよい。会話を中断しないように、遅延が数秒または数分であってもよい。重要な会議を中断しないように、遅延をはるかに長く、例えば、数時間に延長することもできる。「コーテシモード」状態の表示は、視覚方法、聴覚方法、または他の刺激（例えば、触覚メカニズムを介して）によって、ユーザに提供されてもよい。また、目、声、手、頭、脳波（ＥＥＧ）または他の入力方法を用いて、コーテシモードの制御および／または承認を実行することができる。 In another exemplary embodiment, the notification may be delayed before entering courtesy mode. The delay may be seconds or minutes so as not to interrupt the conversation. The delay can be extended to much longer, for example several hours, so as not to interrupt important meetings. The indication of the "coating mode" state may be provided to the user by a visual, auditory, or other stimulus (eg, via a tactile mechanism). Also, eye, voice, hand, head, electroencephalogram (EEG) or other input methods can be used to perform control and / or approval of coatess mode.
緊急通知時にユーザの注意引き
場合によって、他の活動に関係なく、ユーザの注意を特定の情報またはイベントに引く必要がある。例えば、装置ユーザまたは他人に即時の害を与える可能性がある場合（例えば、ユーザの環境における所謂「アクティブシューティング」状況、建物固有の火災警報など）、装置ユーザを警告する必要性がある。装置ユーザの視線に関する知識は、ユーザの注意を引き、状況に関連する情報を提供するプロセスを支援することができる。
User Attention During Emergency Notification In some cases, the user's attention needs to be drawn to specific information or events, regardless of other activities. For example, there is a need to warn the device user if it can cause immediate harm to the device user or others (eg, so-called "active shooting" situations in the user's environment, building-specific fire alerts, etc.). Knowledge of the device user's line of sight can assist in the process of drawing the user's attention and providing information related to the situation.
例示的な実施形態において、緊急情報を受信すると、ユーザ視線の一般的な方向における視野（例えば、拡張現実ヘッドセット、仮想現実ヘッドセット、または複合現実ヘッドセット）に指示を表示する。この指示は、ユーザの中心窩視線領域、傍中心窩領域、または周辺領域内に明示的に配置することができる。必要に応じて、システムは、ユーザが通知を確認するまでおよび／または行動を取るまで、ユーザの視線を動的に追跡することができる。この視覚指示は、アイコン、記号、単語、状況に関連するサムネイル画像、または画像の一部分であってもよい。視覚指示は、注意を引くように表示される（例えば、衝動性動眼抑制または失明抑制を含むユーザの失明期間中ではなく、高明暗度および高速で導入される）。視覚指示は、聴覚刺激または触覚刺激などの他の警報メカニズムを伴ってもよい。 In an exemplary embodiment, upon receiving emergency information, instructions are displayed in the field of view in the general direction of the user's line of sight (eg, an augmented reality headset, a virtual reality headset, or a mixed reality headset). This instruction can be explicitly placed within the user's foveal gaze area, parafoveal area, or peripheral area. If desired, the system can dynamically track the user's line of sight until the user confirms the notification and / or takes action. This visual instruction may be an icon, symbol, word, thumbnail image associated with the situation, or a portion of the image. Visual instructions are displayed to attract attention (eg, introduced at high brightness and high speed, not during the user's blindness period, including impulsive oculomotor or blindness suppression). Visual instructions may be accompanied by other alarm mechanisms such as auditory or tactile stimuli.
最初の通知が行われると、装置着用者が視覚的に追従できるように視覚指示を移動するまたは「運ぶ」（sweep）。これによって、装置着用者は、特定の領域、場合によって領域の特定部分に関する情報またはイベントを象徴的に表現する空間経路を追跡することができ、記憶することができる。例えば、緊急時に受信するテキストメッセージは、テキストメッセージを検索ことができる領域に移動することができる。テキストメッセージを検索する領域内に、移動は、緊急テキストメッセージの特定の送信者に関連する位置を視覚的に示すことができる。同様に、緊急電子メールは、特定の送信者に送信するように、電子メールを検索するために保留された領域に示されてもよい。緊急画像またはビデオのリンクは、緊急画像またはビデオを検索できる位置に移動されてもよい。様々な種類の緊急情報は、（種類に関係なく）情報の専用領域に移動されてもよく、および／または他の領域にコピーされてもよい。 When the first notification is given, the visual instructions are moved or "sweep" so that the device wearer can visually follow. This allows the device wearer to track and remember spatial paths that symbolically represent information or events about a particular area, and in some cases a particular part of the area. For example, a text message received in an emergency can be moved to an area where the text message can be searched. Within the area of searching for text messages, the move can visually indicate the location associated with a particular sender of an urgent text message. Similarly, emergency emails may be shown in a reserved area for searching emails to be sent to a particular sender. The emergency image or video link may be moved to a location where the emergency image or video can be searched. Various types of emergency information may be moved (regardless of type) to a dedicated area of information and / or copied to another area.
このような移動中に、ユーザの注視行動を監視することによって、ユーザが滑動性追跡眼球運動または衝動性動眼信号を用いて、刺激を追従していることを確認することができる。ユーザが所定の時間にこのような運動を実行していない場合、ユーザの注意がとらえられたこと、通知が承認されたことおよび／または行動がとられたことを保証するように、移動を繰り返す。ユーザの視線を追跡するシステムの能力は、重要状況において、ユーザの注意を引くための強力な手段を提供する。 By monitoring the gaze behavior of the user during such movement, it is possible to confirm that the user is following the stimulus by using the gliding pursuit eye movement or the impulsive eye movement signal. If the user does not perform such an exercise at a given time, repeat the move to ensure that the user's attention has been taken, the notification has been approved and / or action has been taken. .. The ability of the system to track the user's line of sight provides a powerful means of attracting the user's attention in critical situations.
掃引された指示に視覚的に追従すると、ユーザは、（視覚信号言語を用いて）指示を即座に作動して、緊急通知に関する更なる詳細を検索することができる。代わりに、ユーザは、単に通知の空間位置を「記憶」し、後で情報を検索することができる。この方法は、長期間に（例えば、自動車の運転中または他の機械の操作中に）ユーザの注意力を分散せずまたはユーザが望ましくない視覚的な活動を行うことなく、緊急通知をユーザに警告する。 Visually following the swept instructions allows the user to immediately activate the instructions (using a visual signal language) to search for further details regarding the emergency notification. Instead, the user can simply "remember" the spatial location of the notification and retrieve the information later. This method gives the user an emergency notification over a long period of time (eg, while driving a car or operating another machine) without distracting the user's attention or performing unwanted visual activity. Warning.
追加の実施形態において、装置による緊急通知の処理は、通知の内容に依存してもよい。ユーザは、視覚的警告プロセスを引き起こすのに十分緊急である通知を事前に設定することができる。情報源、通知の種類（例えば、テキスト、画像）、緊急性の分類、振動数履歴、時間、位置、ユーザの活動、他人の近接性、ユーザの履歴、過去に通知を無視したかまたはそのような通知に対する行動を取ったか、およびその他の要因に基づいて、事前設定を行うことができる。視覚的警告プロセスが実行されるか否かは、装置ユーザによって行われた見かけ活動に依存し得る。例えば、乗り物を運転している時に高優先度の通知のみを提示することができ、相対的に活動していない期間中に、低優先度の通知（例えば、テキストメッセージ）を提示することができる。 In additional embodiments, the processing of the emergency notification by the device may depend on the content of the notification. The user can preset notifications that are urgent enough to trigger the visual alert process. Source of information, type of notification (eg text, image), classification of urgency, frequency history, time, location, user activity, proximity of others, user history, past notifications ignored or so Can be pre-configured based on actions taken in response to such notifications and other factors. Whether or not the visual warning process is performed may depend on the apparent activity performed by the device user. For example, only high priority notifications can be presented when driving a vehicle, and low priority notifications (eg, text messages) can be presented during periods of relative inactivity. ..
オンラインモバイル状態およびプライバシー管理
（事前の確認または認証なく）装着型コンピュータ装置で初期接続を有効化する場合、関連するネットワークアドレス（例：ＵＲＬアドレス、ＭＡＣアドレス、ＩＰアドレス）、または電子通信を行うための他の種類および／またはプロトコルを有する装着型コンピュータ装置に視聴者をリングする時に、問題が生じることがある。顔認識などの方法によって視聴者を特定できる場合、（プライバシー設定に基づいて許可された場合）個人に関連する装置または「アドレス」をデータベースから取り出すことができる。しかしながら、ユーザまたはハードウェア認識がない場合、互いに近接しているユーザの装置間の明確な接続またはリンクを自動的に開始する方法はない。
Online Mobile Status and Privacy Management (without prior confirmation or authentication) When enabling an initial connection on a wearable computer device, to perform relevant network addresses (eg URL addresses, MAC addresses, IP addresses), or electronic communications. Problems can arise when ringing the viewer to a wearable computer device that has other types and / or protocols. If the viewer can be identified by methods such as facial recognition, the device or "address" associated with the individual can be retrieved from the database (if permitted based on privacy settings). However, in the absence of user or hardware awareness, there is no way to automatically initiate a clear connection or link between devices of users in close proximity to each other.
非常に近い場合、一方の装置のユーザは、別の装置のユーザの識別（例えば、バーコード、ＱＲコード（登録商標））を読み取り、解釈することによって、ネットワークアドレスを入手することができる。しかしながら、この方法は、距離があった場合に困難であり、外部接続が望ましくないとき（識別コードを物理的に除去しない場合）、ユーザが「プライバシーモード」に入ることができない。 When very close, the user of one device can obtain the network address by reading and interpreting the identification of the user of the other device (eg, barcode, QR code®). However, this method is difficult when there is a distance, and the user cannot enter "privacy mode" when an external connection is not desirable (without physically removing the identification code).
装置および当該装置着用者を別の装置（および装置着用者）に関連付けることができる単純且つ省電力方法は、装置に取り付けられ、他の装置着用者のシーンカメラが読み取ることができる１つ以上の電磁放射源（ビーコン）を使用することである。パルス状（例えば、フラッシング、オン−オフ）赤外線ＬＥＤビーコンは、この用途のための便利な一例である。例えば、ユーザに視認不可であり且つユーザの注意力を引かない波長であって、相補型金属酸化物半導体（ＣＭＯＳ）または電荷結合素子（ＣＣＤ）を備えた）シーンカメラによって検出することができる近赤外波長（例えば、８００〜１３００ｎｍ）を選択することができる。 A simple and power-saving method that allows a device and its wearer to be associated with another device (and device wearer) is one or more that are attached to the device and can be read by a scene camera of another device wearer. It is to use an electromagnetic radiation source (beacon). A pulsed (eg, flushing, on-off) infrared LED beacon is a convenient example for this application. For example, a wavelength that is invisible to the user and does not attract the user's attention and can be detected by a scene camera (with a complementary metal oxide semiconductor (CMOS) or charge-coupled device (CCD)). The infrared wavelength (eg, 800-1300 nm) can be selected.
広角投射角（１８０度に近い）を備えた赤外線ＬＥＤは、例えば、鼻上方の中間梁に取り付けて使用すると、最も便利である。図１８Ａは、ヘッドセット４９０および中間梁４９１に取り付けられた単一ビーコンの一例を示している。このようなビーコンを備えた装置着用者の顔を見ることができる任意装置着用者は、（ビーコンが何らかの理由で遮断された場合を除き）ビーコンの発信を検出することができる。
Infrared LEDs with a wide-angle projection angle (close to 180 degrees) are most convenient to use, for example, by attaching them to an intermediate beam above the nose. FIG. 18A shows an example of a single beacon attached to the
装置の側面に取り付けられた２つ（または２つ以上）のＬＥＤは、より多くの消費電力を消耗するが、あらゆる角度で容易に検出され得る。図１８Ｂは、耳当て部のヒンジ領域に配置された２つのビーコン４９２ａ、４９２ｂの一例を示している。代替的な実施形態において、頭部上方に支持部を有するヘルメットまたは頭部装着装置の場合、頭部上方の近くに広角投影ビーコンを配置することができる。頭部の周りに配置された複数のビーコンは、全ての視野角から検出を行う能力を保証することができる。
Two (or more) LEDs mounted on the sides of the device consume more power, but can be easily detected at any angle. FIG. 18B shows an example of two
パルス式ビーコンを用いて（画像プロセスによって）、装置および当該装置のユーザを初期検出することができ、装置着用者の状態を示すこともできる。接続状態の表示は、典型的な電話会議に使用された状態表示と同様であってもよい。状態表示の例として、例えば、「利用可能」、「一時的にオフライン」、「利用可能なグループ」、「連絡先のみ」または「利用不可」を含む。これらの表示は、装置着用者にプライバシー管理を与え、ユーザによってリアルタイム変更することができ、および／または装置によって自動的に変更することができる。位置、認識された装置着用者の環境（例えば、会話しているか否か）、日時、電子カレンダ、視覚信号活動などに基づいて、状態を自動的に変更することができる。 A pulsed beacon can be used (by an imaging process) to initially detect the device and the user of the device, and can also indicate the state of the device wearer. The connection status display may be similar to the status display used in a typical conference call. Examples of status indications include, for example, "Available", "Temporarily Offline", "Available Groups", "Contacts Only" or "Not Available". These displays give the device wearer privacy control, can be changed in real time by the user, and / or can be changed automatically by the device. The state can be automatically changed based on the position, the recognized device wearer's environment (eg, whether or not they are talking), the date and time, the electronic calendar, the visual signal activity, and so on.
装置着用者の環境内で別のヘッドセットおよび識別ビーコンを位置付けるための便利な方法は、ビーコンをオンにする（即ち、アクティブ信号を発信する）場合の空間的に配列された画像から、ビーコンをオフにする場合の空間的に配列された画像を減算することを含む。このように２つ（または２つ以上）の画像間の差分は、主にビーコン領域に現れる。このような「差分画像」を用いて、ビーコンの位置を特定し、（例えば、ビーコン振動数特性を測定することによって）そのビーコンが別の装置のものであることを確認することができる。 A convenient way to position another headset and identification beacon within the device wearer's environment is to place the beacon from a spatially arranged image when the beacon is turned on (ie, emits an active signal). Includes subtracting spatially arranged images when turned off. In this way, the difference between two (or two or more) images appears mainly in the beacon region. Such a "difference image" can be used to locate the beacon and confirm (eg, by measuring the beacon frequency characteristics) that the beacon belongs to another device.
ビーコンのパルスが観察カメラのフレームレートに同期されていない場合、（連続画像を減算することによってビーコンを容易に追跡／特定するように）カメラの画像が最大の輝度レベルおよび最小の輝度レベルに少なくとも１つのフレームを含むことを保証しながら、ビーコンをオン／オフに変調できる最大の振動数を決定することが重要である。連続画像内においてフレームレートがパルスビーコンの変調レートの少なくとも４倍である場合、非同期カメラによって収集された画像は、（ビーコンのオン時間およびオフ時間が等しい場合）最大の輝度レベルおよび最小の輝度レベルを有するフレームを含むことが保証される。以下に説明するように、最大の輝度レベルおよび最小の輝度レベルを基準として用いて、情報を符号化するためにパルス幅または振幅変調が行われているか否かを判断することができる。一例として、カメラの撮影速度が６０フレーム／秒である場合、１５Ｈｚでビーコンを変調することによって、カメラ画像から最大および最小ビーコンレベルを検出することができる。 If the beacon pulses are not synchronized to the observation camera frame rate, the camera image will be at least at maximum and minimum brightness levels (so that the beacon can be easily tracked / identified by subtracting continuous images). It is important to determine the maximum frequency at which the beacon can be modulated on / off, ensuring that it contains one frame. If the frame rate in the continuous image is at least 4 times the modulation rate of the pulse beacon, the image collected by the asynchronous camera will have the maximum and minimum brightness levels (if the beacon on and off times are equal). It is guaranteed to include a frame with. As described below, the maximum and minimum luminance levels can be used as a reference to determine if pulse width or amplitude modulation has been performed to encode the information. As an example, when the shooting speed of the camera is 60 frames / sec, the maximum and minimum beacon levels can be detected from the camera image by modulating the beacon at 15 Hz.
図１９Ａは、パルスビーコン５００と非同期カメラ５０１ａ、５０１ｂのフレームレートとの間の関係を示すタイミング図である。図面には、カメラのフレームレートは、ビーコンのサイクルレートの４倍である。タイミング図５００において、ビーコンの「オン」状態は、ハイレベル５０３ａで示され、ビーコンの「オフ」状態は、ローレベル５０３ｂで示される。ビーコンの全周期５０２は、オン状態５０３ａおよびオフ状態５０３ｂの両方を含む。図１９Ａにおいて、破線５０６は、ビーコンの各周期において低レベルから高レベルに遷移する基準時間を示している。
FIG. 19A is a timing diagram showing the relationship between the
カメラのタイミング５０１ａ、５０１ｂは、各画像フレームの取得の終了時間または新しいフレームの取得の開始時間を示す縦線条によって表される。上方のカメラのタイミング図５０１ａにおいて、フルフレーム５０４ａは、ビーコン５００が「オン」である間に取得される。別のフレーム５０４ｂは、ビーコン５００がオフである間に取得される。カメラのタイミングをシフトする場合（５０１ｂ）、ビーコンが「オン」であるとき（５０５ａ）に少なくとも１つのフルフレームが取得され、ビーコンが「オフ」であるとき（５０５ｂ）に少なくとも１つの別のフルフレームが取得される。カメラとビーコンとの同期に関係なく、フルオン画像およびフルオフ画像を取得することができる。
The
現在の電気通信規格では遅いが、装置（および関連するユーザ）の状態をこのようなビーコンに暗号化することができる。ビーコンの特徴を動的に変化することによって、暗号化状態を反映することができる。暗号化の例は、シリアル通信（例えば、ＲＳ−２３２プロトコル）に使用された非同期通信規格に類似したパルス幅変調（すなわち、オンオフ位相の時間の制御）、位相シフトまたは振幅変調を含む。（通常、数値として暗号化された）限定数の状態オプションを使用する場合、低速のカメラフレームレートであっても、例えば１秒以内に状態情報を決定し、発信することができる。 Although slow in current telecommunications standards, the state of the device (and associated users) can be encrypted into such a beacon. The encrypted state can be reflected by dynamically changing the characteristics of the beacon. Examples of encryption include pulse width modulation (ie, control of on-off phase time), phase shift or amplitude modulation similar to the asynchronous communication standard used for serial communication (eg, RS-232 protocol). When using a limited number of state options (usually encrypted numerically), state information can be determined and transmitted, for example, within 1 second, even at slow camera frame rates.
装置着用者が外部接続を許可するように設定した場合、接続を可能にするために、ネットワーク「アドレス」または他の接続情報を交換する必要がある。ローカルエリアに装置着用者が１人しかいない場合、装置「発見」プロセスによって、すなわち、（例えば、Bluetooth（登録商標）またはWiFi（登録商標）を用いて）制限せずアドレスを発信することによって、アドレスを交換することができる。しかしながら、アドレスおよび資格情報をより安全に交換するために、特に近くに他の装置ユーザが２人以上存在する可能性がある場合、特定の装置（見られている装置）に特有する交信プロセスが必要である。 If the device wearer is configured to allow external connections, network "addresses" or other connection information must be exchanged to allow the connection. If there is only one device wearer in the local area, by the device "discovery" process, i.e., by sending an unrestricted address (eg, using Bluetooth® or WiFi®). Addresses can be exchanged. However, in order to exchange addresses and credentials more securely, communication processes specific to a particular device (the device being viewed) may be present, especially if there may be more than one other device user nearby. is necessary.
接触または「接続」の可能性がある場合、視認カメラをビーコン源周囲の非常に小さな関心領域に一時的に切り替え、取得速度を毎秒数百または数千フレームに増加することができる。ビーコンに暗号化された情報のマイクロバーストは、ＵＲＬ、ＩＰアドレス、ＭＡＣアドレス、または中央データベース内のルックアップテーブルに使用され得るポインタなどのネットワーク位置情報を含むことができる。暗号化は、低フレームレート状態の決定に使用されたものと同様の技術または異なる技術を使用することができる。例えば、共通のシリアルプロトコル（例えば、ＲＳ−２３２）を使用することができる。主な違いは、より高いフレームレートが対話の速度を増加させることができるため、ＵＲＬなどのより複雑な情報を指定できることである。 If there is a possibility of contact or "connection", the visual camera can be temporarily switched to a very small area of interest around the beacon source and the acquisition speed can be increased to hundreds or thousands of frames per second. A microburst of information encrypted on a beacon can include network location information such as URLs, IP addresses, MAC addresses, or pointers that can be used for lookup tables in a central database. Encryption can use a technique similar to or different from that used to determine the low frame rate state. For example, a common serial protocol (eg RS-232) can be used. The main difference is that higher frame rates can increase the speed of dialogue, allowing you to specify more complex information such as URLs.
図１９Ｂは、マイクロバースト５０８を示すタイミング図である。このマイクロバーストは、ネットワーク位置情報をビーコンの１周期のパルスに暗号化している。装置ユーザが接続を許可するときに、このようなマイクロバーストは、ビーコン５０７の通常周期に定期的に点在されることができる。
FIG. 19B is a timing diagram showing the
類推すると、ビーコンは、ＱＲまたはバーコードの識別機能を電子的に実行するが、この識別機能は、長い距離で操作されてもよく、ヘッドセットの着用者によって制御されてもよい。必要に応じて、特別に見られている人の資格情報の交換は、プライバシーを維持しながら、目立たないように行うことができ、典型的には１秒未満で行うことができる。 By analogy, the beacon electronically performs a QR or barcode identification function, which may be operated over long distances or controlled by the headset wearer. If desired, the exchange of specially viewed person credentials can be unobtrusive, while maintaining privacy, typically in less than a second.
高フレームレートカメラの代替的な実施形態は、より多くの情報を暗号化して送信するように、複数のビーコンを実質的に同時に発信することを含む。情報の暗号化は、２つ以上のビーコン間の位相差を含むことができる。１つ以上のカメラの高フレームレートに切り替える代わりに、受信装置に１つ以上の単一ピクセル検出装置（例えば、フォトダイオード）を使用することができる。その後、例えば「接続経路」を暗号化するマイクロバーストを解読するために、高レートでこの検出装置をサンプリングすることができる。情報の伝送は、テレビ、スピーカシステムなどと通信するために一般的に使用されたＩＲ遠隔制御装置と機能的に類似している。混雑状況にクロストークを回避するために、このような検出装置を方向感受性ものにする必要がある。 An alternative embodiment of a high frame rate camera comprises transmitting multiple beas at substantially the same time so that more information is encrypted and transmitted. Information encryption can include a phase difference between two or more beacons. Instead of switching to a higher frame rate for one or more cameras, one or more single pixel detectors (eg, photodiodes) can be used for the receiver. The detector can then be sampled at a high rate, for example to decrypt microbursts that encrypt the "connection path". The transmission of information is functionally similar to the IR remote control devices commonly used to communicate with televisions, speaker systems, and the like. Such detectors need to be directional sensitive to avoid crosstalk in crowded situations.
相互視線の特定および応用
人と人の間の対面会話は、日常生活に遍在する。相互視線は、会話の開始および維持、並びに「誰が何を言ったか」の特定および登録という点で、対面会話に不可欠な役割を果たしている。
Identification and application of mutual gaze Face-to-face conversations between people are ubiquitous in daily life. Mutual gaze plays an integral role in face-to-face conversations in terms of starting and maintaining conversations, as well as identifying and registering "who said what."
例示的な実施形態において、シーンカメラ画像に組み合わせた（すなわち、装置ユーザの視線を環境に重ね合わせた）視線追跡を用いて、装置着用者が見ている人を特定することができる。別の装置着用者が上記装置着用者を振り返って見ている（すなわち、同様の方法で特定された）場合、両方の装置着用者は、資格情報を交換することができる。相互視線が維持される限り、二人の間の対話および会話を行うことができる。上述したように、資格情報の交換は、例えば、装置のビーコンを介した初期の認証を含むことができる。 In an exemplary embodiment, line-of-sight tracking combined with a scene camera image (ie, superimposing the device user's line of sight on the environment) can be used to identify the person the device wearer is looking at. If another device wearer looks back at the device wearer (ie, identified in a similar manner), both device wearers may exchange credentials. Dialogue and conversation between the two can take place as long as mutual gaze is maintained. As mentioned above, the exchange of credentials can include, for example, initial authentication via the device beacon.
所定のユーザの好みに基づいて、システムは、必要に応じて、視線の交換を異なる時間に行うことを可能にすることができる。例えば、第１ユーザが一定の時間で第２ユーザを見てから視線を離した場合、ユーザが選択した所定の時間に第１ユーザが視線を離したとしても第２ユーザが振り返って第１ユーザを見た場合、システムは、資格情報の交換を許可することができる。更なる例として、一対のユーザは、会議または他の集会の間に両者が互い視線を合わせたことがあれば、後で再接続を行うことができる。 Based on a given user's preference, the system can allow line-of-sight exchanges to occur at different times, if desired. For example, when the first user looks at the second user for a certain period of time and then looks away, the second user looks back and looks back even if the first user looks away at a predetermined time selected by the user. If you see, the system can allow the exchange of credentials. As a further example, a pair of users may later reconnect if they have looked at each other during a meeting or other meeting.
会話に参加している人の一方が装置を着用していない場合、他方の装置着用者のシーンカメラを用いて、相互視線の存在を特定することもできる。装置着用者のシーンカメラビデオに基づいて、画像認識を用いて、環境内の人が装置着用者に振り返って見ているか否かを判断することができる。人が特定の方向に見ているか否かの特定は、当該技術分野では「視線ロック」として知られている。装置着用者に対する視線ロックは、顔の向き、顔の表情、および／または視線方向に基づいて特定されてもよい。いくつかの実施形態において、アルゴリズム方法は、カメラ画像に基づいた機械学習（神経ネットワーク）を含むことができる。殆どのユーザは、誰かに見られているときに自然に気付くため、視線ロック（すなわち、見られていること）を自然に特定することができる。 If one of the participants in the conversation is not wearing the device, the scene camera of the other device wearer can also be used to identify the presence of mutual gaze. Based on the device wearer's scene camera video, image recognition can be used to determine if a person in the environment is looking back at the device wearer. The identification of whether or not a person is looking in a particular direction is known in the art as "line-of-sight lock". The line-of-sight lock for the device wearer may be specified based on facial orientation, facial expression, and / or line-of-sight direction. In some embodiments, the algorithmic method can include machine learning (nerve network) based on camera images. Most users naturally notice when someone is looking at them, so they can naturally identify the line-of-sight lock (ie, what they are looking at).
視線ロックの特定は、仮想世界のアバタに適用することもできる。テレビ会議、シミュレーションおよびゲームなどの状況において、より現実的な会話を可能にするために、特定の個人を見るようにアバタを有効化することができる。視線ロックは、アバタによって（一般的に電気通信リンクを介して）直接伝達されてもよく、アバタのシーンカメラ画像に基づいて検出されてもよい。 Gaze lock identification can also be applied to avatars in the virtual world. In situations such as video conferencing, simulations and games, avatars can be enabled to look at specific individuals to allow for more realistic conversations. The line-of-sight lock may be transmitted directly by the avatar (generally via a telecommunications link) or may be detected based on the avatar's scene camera image.
相互視線が１）２つの異なる装置着用者からの画像、または２）単一の装置着用者による視線追跡およびシーンカメラ画像に対する視線ロックの検出、または３）アバタによる視線ロックに基づくことによって、以下に示すいくつかの方法で、相互視線を利用することができる。
１．ユーザが一度も会ったことがない場合、（本明細書に記載されているように、プライバシーが考慮されていると仮定して）、他のユーザに関する情報を装置着用者に表示することによって、接続を許可することができる。
２．ユーザが以前に会ったことがある場合、再接続に必要な情報（例えば、名前、以前に会った時間など）を表示することによって、接続を許可することができる。
３．相互視線を行う間に会話または他の情報を交換した場合、交換に関する記憶情報（例えば、音声、画像、テキスト）は、相互視線を行う間に発生したものとしてタグ付けされる。これは、情報検索中にコンテキストの提供を支援することができる。
４．複数のユーザ間の会話または対話において、相互視線を記録しておよび／または対話中に情報（例えば、音声、画像、テキスト）にタグ付けることができる。これらのデータは、データセット内の「誰が何を言ったか」という問題に対処する共に、各ユーザの関心のある話題またはユーザグループの興味のある話題の識別を支援することができる。
By reciprocal gaze 1) based on images from two different device wearers, or 2) gaze tracking by a single device wearer and detection of gaze locks on scene camera images, or 3) gaze locks by avatars: Mutual gaze can be utilized in several ways as shown in.
1. 1. If a user has never met (assuming privacy is taken into account as described herein), by displaying information about other users to the device wearer. You can allow the connection.
2. If the user has met before, the connection can be allowed by displaying the information needed to reconnect (eg, name, time of previous meeting, etc.).
3. 3. If a conversation or other information is exchanged during the mutual gaze, the memory information about the exchange (eg, voice, image, text) is tagged as occurring during the mutual gaze. This can help provide context during information retrieval.
4. In a conversation or dialogue between multiple users, mutual gaze can be recorded and / or information (eg, voice, image, text) can be tagged during the dialogue. These data can address the issue of "who said what" in the dataset and help identify topics of interest to each user or group of users.
装着型装置を含むマルチモーダル制御
視覚信号は、人間-機械インターフェイス（ＨＭＩ）対話、特に装着型および／またはモバイルコンピュータ状況において強力であるが、ＨＭＩ対話の１つ以上の他のモダリティと組み合わせられると、その有用性を大幅に拡張することができる。例えば、一般的には、視覚信号を音声コマンドおよび／または意図的な眼球運動を知らせるおよび／または強調するための点頭などの補足動作に混ぜることが「快適」である。意図的な眼球運動は、衝動性動眼、一連の衝動性動眼、両眼離反運動、前庭性眼球運動、または滑動性追跡眼球運動を含み得る。
Multimodal controlled visual signals, including wearable devices, are powerful in human-machine interface (HMI) interactions, especially in wearable and / or mobile computer situations, but when combined with one or more other modality of HMI interactions. , Its usefulness can be greatly extended. For example, it is generally "comfortable" to mix visual signals with voice commands and / or supplementary movements such as instillation to signal and / or emphasize intentional eye movements. Intentional eye movements may include impulsive googly, a series of impulsive eyes, binocular detachment movements, vestibular eye movements, or gliding pursuit eye movements.
このようなマルチモーダル制御に関する問題点は、いつでも包括的な管理を与える入力の種類に対処することである。ほぼ同一の時間で異なるソースから起因する２つの作動シーケンスを受信した場合（例えば、どのシーケンスを最初に実行するか）および／または論理上一貫性のない２つ以上のシーケンスを受信した場合（例えば、ほぼ同一の時間で画像の全画面表示および電子メールの全画面表示をほぼ同時に要求する場合）両義性が生じる可能性がある。 The problem with such multimodal control is dealing with the types of inputs that always give comprehensive control. When two operating sequences from different sources are received at about the same time (eg, which sequence is executed first) and / or when two or more sequences that are logically inconsistent are received (eg,). Ambiguity can occur (when requesting full-screen display of images and full-screen display of emails at about the same time).
入力の優先順位は、特定のアプリケーションに依存することができ、および／または時々刻々に変化することができる。例えば、文字、数字、単語、語句または文章を含む仕様が必要される場合、所望の入力を話すことがしばしばより容易でより迅速である。例示的な実施形態において、アプリケーションは、このような時に音声入力を「傾聴する」ことができる。例えば、音声入力が装置着用者に特有であることを確認するために、音声入力をさらに純化することができる。 Input priorities can depend on a particular application and / or can change from moment to moment. For example, if a specification involving letters, numbers, words, phrases or sentences is required, it is often easier and faster to speak the desired input. In an exemplary embodiment, the application can "listen" to the voice input at such times. For example, the voice input can be further refined to ensure that the voice input is specific to the wearer of the device.
多くの種類のＨＭＩ入力は、装置の制御に関連せず、日常活動の一部である変化または動きを利用する。異なる入力は、変化がＨＭＩ対話にとって「意図的」である（すなわち、信号の構成要素である）かまたは装置の制御に関連しないかを解釈する感受性が異なる。これは、様々な入力の優先順位を決定する役割を果たすことができる。この範囲の一方側において、コンピュータマウスを動かし、マウスボタンの１つをクリックすることは、常に機械制御を示すことを意図している。この範囲の反対側において、眼球運動は、日常活動の不可欠な部分である。日常活動において、視覚信号言語は、捜索的な眼球運動および他の形態の眼球運動から、意図的な眼球運動を容易に区別するように特別に設計されている。 Many types of HMI inputs are not related to control of the device and utilize changes or movements that are part of daily activities. Different inputs have different sensitivities to interpret whether the change is "intentional" to the HMI dialogue (ie, a component of the signal) or not related to control of the device. It can play a role in prioritizing various inputs. Moving a computer mouse and clicking one of the mouse buttons on one side of this range is intended to always indicate mechanical control. On the other side of this range, eye movements are an integral part of daily activities. In daily activities, the visual signal language is specially designed to easily distinguish intentional eye movements from exploratory eye movements and other forms of eye movements.
表１は、状態変化が意図的である（すなわち、ＨＭＩ対話に向ける）ことを示す初期または固有信頼度と共に、いくつかの例示的な入力源を示している。テーブルの略上半分は、意図的な対話接触（近接検出の場合、近接接触）を行う装置の入力モダリティを含む。表１の中央欄における星印の数の増加は、（フォローオン分析なく）最初に検出された状態の変化が意図的ものであることを示す信頼度の増加を示している。 Table 1 shows some exemplary input sources, with initial or intrinsic confidence indicating that the state change is intentional (ie, directed towards an HMI dialogue). The substantially upper half of the table contains the input modality of the device for making intentional interactive contact (proximity contact in the case of proximity detection). An increase in the number of asterisks in the central column of Table 1 indicates an increase in confidence that the first detected state change (without follow-on analysis) is intentional.
マウスボタンまたはキーボードを押すことは、殆ど例外なく、意図的なＨＭＩ対話に関連している。偶発的なタッチ、タップまたはスワイプが時々起こるが、スイッチ、スクリーン、パッド、または他の近接感応型装置にタッチすることは、通常意図的である。多くの装置には、動きを検出し、動かされるまたは揺らされる時に反応する加速度計が組み込まれている。装着型装置は、特に、絶えず動かされている。したがって、動きが意図的であるか否かを決定するために、動きの特徴（例えば、振幅、一次振動数および／または持続時間）を考慮しなければならない。 Pressing a mouse button or keyboard is almost exclusively associated with intentional HMI dialogue. Although accidental touches, taps or swipes occur from time to time, touching a switch, screen, pad, or other proximity-sensitive device is usually intentional. Many devices have built-in accelerometers that detect movement and react when moved or shaken. Wearable devices, in particular, are constantly being moved. Therefore, in order to determine whether the movement is intentional or not, the characteristics of the movement (eg, amplitude, primary frequency and / or duration) must be considered.
表１の下部は、一般的により間接的に感知される入力モダリティを含む。点頭、手振りおよび他の身体部分によって行われた特定の動きは、一般的にＨＭＩ信号として解釈されるために、特定のパターンおよび／または一連の動きを必要とする。さもなければ、これらの動きは、他の活動に関連する。失明または聾唖ではない場合、話しまたは眼球運動は、日常活動に広く存在する。したがって、このような潜在的入力の「意図」レベルを慎重に評価しなければならない。 The lower part of Table 1 contains input modality that is generally more indirectly perceived. Specific movements performed by instillation, hand gestures and other body parts require a specific pattern and / or series of movements to be generally interpreted as an HMI signal. Otherwise, these movements are related to other activities. In the absence of blindness or deafness, speaking or eye movements are widespread in daily activities. Therefore, the "intention" level of such potential inputs must be carefully evaluated.
固有信頼度および「意図スコア」は、所定の入力の優先順位を決定する役割を果たす。「意図スコア」は、特定の動き、ジェスチャまたは視覚信号が選択シーケンスまたは作動シーケンスのパターンと一致する度合いに基づいて計算されてもよい。高度に一致するパターンは、装置ユーザの側で意図的である可能性が高い。モダリティの固有信頼度、意図スコアおよび特定の応用の状況（例えば、１つ以上のモダリティが特定のモダリティの入力に対して所期または便利であるか否か）の全ては、入力のフィルタリングおよび優先順位付けに使用されてもよい。 The intrinsic confidence and the "intention score" play a role in determining the priority of a given input. The "intention score" may be calculated based on the degree to which a particular movement, gesture or visual signal matches the pattern of a selection sequence or action sequence. Highly matching patterns are likely to be intentional on the part of the device user. The modality's intrinsic confidence, intent score, and specific application context (eg, whether one or more modality is desired or convenient for the input of a particular modality) are all input filtering and priorities. It may be used for ranking.
表１の一番右の列は、例示的なアプリケーションの例示的な「階層」または優先順位を列挙する。一般的に、全ての入力モダリティが常に利用可能とは限らない。利用可能な場合でも、一部の応用に使用できないモダリティもある。例えば、モバイル電話装置との対話に視覚信号を使用する場合、モバイル装置に関連する入力モダリティおよび眼球運動を追跡するための頭部装着装置（マイクを含む）によって検知されたモダリティを使用して、意図を伝えることができる。この種類の対話の場合、（表１に示すように）入力モダリティの例示的な優先順位は、次のようになる。 The rightmost column of Table 1 lists exemplary "hierarchies" or priorities for exemplary applications. In general, not all input modality is always available. Even if available, some modality may not be available for some applications. For example, when using visual signals to interact with a mobile phone device, using the input modality associated with the mobile device and the modality detected by a head-worn device (including a microphone) to track eye movements, Can convey the intention. For this type of dialogue, the exemplary priorities of input modality (as shown in Table 1) are:
１．モバイル装置の画面を使用するタッチ制御は、自発的な動きに基づいているため、一般的に意図的である。したがって、全てのタッチコマンドは、直ちに実行される。 1. 1. Touch control using the screen of a mobile device is generally intentional because it is based on spontaneous movement. Therefore, all touch commands are executed immediately.
２．移動装置の振動は、非意図的または意図的であってもよい。したがって、装置の揺れの判断に高厳密性を適用することができる。閾値（例えば、振幅、振動数、持続時間、方向）を超える場合、検出値を入力として使用する。 2. The vibration of the mobile device may be unintentional or intentional. Therefore, high rigor can be applied to the judgment of the shaking of the device. If the threshold (eg amplitude, frequency, duration, direction) is exceeded, the detected value is used as input.
３．同様に、頭部動作は一般的に遍在する。しかしながら、予め定義されたパターン（例えば、頭部振りは「はい」または「いいえ」を意味する）に一致する大幅の頭部動きがあった場合、このような入力に対して行動をすぐに取ることができる。 3. 3. Similarly, head movements are generally ubiquitous. However, if there is significant head movement that matches a predefined pattern (eg, head swing means "yes" or "no"), take immediate action in response to such input. be able to.
４．次に優先度の低いモダリティは、優先度の高いモダリティまたは完全に点在するモダリティに関係なく、選択または実行することができる視覚信号である。例えば、視覚信号を用いて選択を行うことができ、モバイル装置上のタッチ制御を用いて、行動または作動を指定することができる。 4. The next lowest priority modality is a visual signal that can be selected or executed regardless of the higher priority modality or the fully interspersed modality. For example, visual signals can be used to make selections, and touch controls on mobile devices can be used to specify actions or actions.
５．この例において、最も優先度の低いモダリティは、音声入力である。この場合、音声認識は、視覚信号または他の優先度の高い対話手段によって作動された場合に限り、使用される。したがって、音声の録音または音声のテキストへの転記は、例えば、他のモダリティによって指示された時に（例えば、テキストメッセージまたは電子メールが作成されている時に）制限される。 5. In this example, the lowest priority modality is voice input. In this case, speech recognition is used only when activated by a visual signal or other high priority dialogue means. Thus, recording audio or transcribing audio to text is restricted, for example, when directed by another modality (eg, when a text message or email is being created).
上述した例において、異なるモダリティの優先順位を動的にすることができる。例えば、キーワードまたは語句が検出された場合、音声制御の優先度を上げて、視覚信号および／または他のモダリティを取り替えることができる。この場合、音声ストリームが存在する限り、音声コマンドが実行される。静寂期間中、データ入力のより便利な手段として考えられる時に、および／または１つ以上の特定のキーワードまたは語句を介してまたは他の何らかの指示によって、音声制御がオフにされたまたは優先度が低下した時に、視覚信号を点在させることができる。 In the example described above, the priorities of different modality can be made dynamic. For example, if a keyword or phrase is detected, voice control can be prioritized to replace visual signals and / or other modality. In this case, the voice command is executed as long as the voice stream exists. Voice control is turned off or deprecated during periods of silence, when considered as a more convenient means of data entry, and / or through one or more specific keywords or phrases or by some other instruction. At that time, visual signals can be scattered.
２つ以上の入力モダリティが実質的に同時に検出された場合、階層の低いモダリティに関連する操作は、遅延して実行するまたはまったく実行しない。より低い優先度の操作を最終的に実行するか否かは、上述したように、入力が装置ユーザによって意図的に行われた「意図スコア」または測定信頼度に依存することができる。意図スコアは、例えば、眼球運動が作動ターゲットに向かうまたは追従する度合いに依存する。 If two or more input modality are detected at substantially the same time, the operations associated with the lower modality may be delayed or not performed at all. Whether or not to finally perform a lower priority operation can depend on the "intention score" or measurement confidence that the input was intentionally made by the device user, as described above. The intent score depends, for example, on the degree to which eye movements move toward or follow the working target.
マルチモーダルＨＭＩを使用する場合のもう１つの考慮点は、視覚信号によって指定された位置の精度と他のモダリティによって指定された位置の精度との違いである。眼球運動は、非常に速いが、角度解像度に根本的な制限がある。すなわち、注視位置が瞳孔および中心窩の大きさ並びに他の光学理由に基づいた視線追跡から決定されるという制限がある。対照的に、コンピュータマウスの場合、例えば、腕、手およびマウスを遠方に動かすのに時間がかかるが、（カーソルの移動に対するマウスの適切なスケーリングに加えて）手の精巧な運動技能を用いて、ディスプレイ上の位置もしくは仮想世界または現実世界の視界内の位置を精確に指定することができる。 Another consideration when using a multimodal HMI is the difference between the accuracy of the position specified by the visual signal and the accuracy of the position specified by other modality. Eye movements are very fast, but there is a fundamental limitation on angular resolution. That is, there is a limitation that the gaze position is determined by gaze tracking based on the size of the pupil and fovea as well as other optical reasons. In contrast, with a computer mouse, for example, it takes time to move the arm, hand, and mouse far away, but with the elaborate movement skills of the hand (in addition to the proper scaling of the mouse for cursor movement). , The position on the display or the position in the virtual world or the real world can be specified accurately.
視覚信号補助型マルチモーダル制御（「移送」）
このような精度および制御の違いを認識して、視覚信号ユーザインターフェイスは、目が「一番できること」をするようにし、他のモダリティの機能を考慮するマルチモーダル入力を利用する。例えば、ディスプレイの広い領域をカバーする眼球運動を含むシーン捜索は、目によって最もよく行われる。その後、眼球運動によって指示された場合または高精度装置（例えば、コンピュータマウス、トラックボール、デジタルペン）上で動きが検出された場合、高精度装置からの入力が制御を引き継ぐことができる。
Visual signal assisted multimodal control ("transfer")
Recognizing these differences in precision and control, the visual signal user interface utilizes multimodal inputs that allow the eye to do "the best thing" and take into account the capabilities of other modality. For example, scene searches involving eye movements that cover a large area of the display are best performed by the eye. Subsequent input from the precision device can take over control if instructed by eye movements or if movement is detected on a precision device (eg, computer mouse, trackball, digital pen).
実際に、目が長い距離運動および捜索を最も良く行うため、マルチモーダル装置の動き感度を高精度（対距離）に設定することができる。目で注意を特定の領域に「移送」した後、精度を最大化にするように、このような装置の動き感度を大きく変更してもよい。 In fact, the motion sensitivity of the multimodal device can be set with high accuracy (against distance) for best performing long-distance movements and searches with eyes. After visually "transferring" attention to a particular area, the motion sensitivity of such a device may be significantly modified to maximize accuracy.
他のマルチモーダル制御と共に視覚信号を含む利点は、以下にある。
１．速度。目は、指、手、腕または他の身体の部分よりも、大きな円弧距離を速く通過することができる。
The advantages of including visual signals along with other multimodal controls are:
1. 1. speed. The eyes can pass through larger arc distances faster than fingers, hands, arms or other parts of the body.
２．簡単。装置着用者の意図を実行するすることに関与するオブジェクトを見るまたは集中することが、殆どのプロセスの自然部分である。 2. Simple. Seeing or concentrating on the objects involved in carrying out the device wearer's intentions is a natural part of most processes.
３．疲労軽減。指関節、手首、肘または他の身体部分に関わる生体力学に比べて、眼球の動きが本質的に疲れない。 3. 3. Fatigue reduction. Compared to biomechanics involving knuckles, wrists, elbows or other body parts, eye movements are essentially less tiring.
４．特殊機能。視認のフィードバックは、操作（例えば、ドライブシミュレータ、高解像度表現など）の実行および／または他の装置からの多次元入力と共に目によって観察された２次元（または両眼離反運動を含む場合に３次元）位置の組み合わせに不可欠な要素である。 4. Special function. Visual feedback is two-dimensional (or three-dimensional if it involves binocular detachment) observed by the eye along with performing operations (eg, drive simulator, high resolution representation, etc.) and / or multidimensional input from other devices. ) It is an indispensable element for the combination of positions.
これらの利点を具現化する具体例として、情報の読み取り、閲覧および移動指示、画像またはビデオの編集および表示、作図、作曲などの広範囲典型的な応用にディスプレイの観察を検討する。眼球の意図的な動きを用いて、ポインタまたは他の標識（例えば、カーソル）を対象領域に迅速に「移送する」ことができる。 As specific examples that embody these advantages, we consider observing displays for a wide range of typical applications such as reading information, viewing and moving instructions, editing and displaying images or videos, drawing, and composing. The intentional movement of the eye can be used to quickly "transfer" a pointer or other marker (eg, a cursor) to the area of interest.
容易性および迅速性に加えて、このプロセスのもう１つの利点は、ポインタまたはカーソルの現在位置を「発見する」（すなわち、視覚的に捜索する）必要がないことである。現行の応用において、カーソル、特に多くのテキストまたは画像に隠されているカーソルを視覚的に「発見する」ことは、時間がかかり、気を散らすことである。しかしながら、例えば、コンピュータマウスを使用する場合、ターゲット位置に向かってどの方向の動きを行うかを判断するために、カーソルを発見する必要がある。 In addition to ease and speed, another advantage of this process is that you do not have to "find" (ie, visually search) the current position of the pointer or cursor. In current applications, visually "discovering" cursors, especially those hidden in many texts or images, is time consuming and distracting. However, for example, when using a computer mouse, it is necessary to find the cursor in order to determine which direction to move toward the target position.
例示的な実施形態において、ポインタの移動を開始する時に、装置着用者が見ているターゲット位置に向かって一般的な方向（すなわち、所定の方向範囲）に沿って、ポインタを移動する。この初期移動は、カーソル、ジョイスティック、トラックボール、デジタルペンなどのポインティング装置を用いて実行することができる。開始されると、新しい位置（すなわち、ユーザの視線によって示されたターゲット位置）へのポインタの移動は、迅速に（例えば、中間位置にビデオフレームを迅速に表示する）または瞬間的に（例えば、介在するビデオフレームなしで）行うことができる。代替的にまたは追加的に、ターゲット位置に向かう意図的な衝動性動眼またはターゲット位置から離れる意図的な衝動性動眼を介して、視覚信号言語内で、ターゲットの位置移動を開始してもよい。 In an exemplary embodiment, when the pointer begins to move, the pointer is moved along a general direction (ie, a predetermined directional range) towards the target position seen by the device wearer. This initial movement can be performed using a pointing device such as a cursor, joystick, trackball, or digital pen. Once started, moving the pointer to a new position (ie, the target position indicated by the user's line of sight) can be done quickly (eg, quickly displaying a video frame in the middle position) or momentarily (eg, for example). Can be done (without intervening video frames). Alternatively or additionally, the movement of the target may be initiated within the visual signal language via a deliberate impulsive eye moving towards the target position or a deliberate impulsive eye moving away from the target position.
（所望の場合）新たな焦点領域を示すフィードバックを装置着用者に提供してもよい。このフィードバックは、可視カーソルまたはポインタ、領域内の選択可能なオブジェクトの強調表示、背景の変更、境界の追加などの形態であってもよい。必要に応じて、視覚的フィードバックは、高速移送の後にオブジェクトがターゲット位置に近づく時にオブジェクトの「減速」を表示することを含むことができる。 Feedback may be provided to the device wearer (if desired) to indicate a new focal area. This feedback may be in the form of a visible cursor or pointer, highlighting selectable objects in the area, changing the background, adding boundaries, and so on. If desired, visual feedback can include displaying an object's "deceleration" as it approaches the target position after a fast transfer.
焦点領域に「移送」する際、別のＨＭＩ装置をシームレスに（すなわち、特定のコマンドシーケンスなしで）使用して、選択を制御および作動することができる。タッチパッド、コンピュータマウス、ジョイスティックなどの装置は、（視覚信号に比べて）より高い精度を提供する。この高精度を使用して、装置着用者の視力に至るまでの空間的範囲にわたって、選択を行うことができる。実際に、このような装置は、大きな範囲をカバーする必要がないため、小さい精密な動きを優先するように、装置の感度を調整することができる。この感度調整は、静的または動的で適応性がある。例えば、ポインタを所望のターゲットの付近に配置する視覚信号（例えば、衝動性動眼）による移送の終わりに、ポインティング装置の感度を一時的に減少することができる（ポインタをより短い距離に移動するためにより大きな物理的移動を必要とする）。 When "transferring" to the focal area, another HMI device can be used seamlessly (ie, without a specific command sequence) to control and activate the selection. Devices such as touchpads, computer mice, and joysticks provide higher accuracy (compared to visual signals). This high precision can be used to make selections over a spatial range up to the eyesight of the device wearer. In fact, such devices do not need to cover a large area, so the sensitivity of the device can be adjusted to prioritize small precision movements. This sensitivity adjustment is static or dynamic and adaptive. For example, the sensitivity of the pointing device can be temporarily reduced (to move the pointer to a shorter distance) at the end of transfer by a visual signal (eg, impulsive occlusal eye) that places the pointer near the desired target. Requires greater physical movement).
「移送」は、仮想現実ディスプレイ、拡張現実ディスプレイ、タブレット、携帯電話、スマート装置、投影画像、ディスプレイモニタ、サイネージなどを含む幅広いディスプレイに関与することができる。「移送」は、単一のディスプレイに行われてもよく、２つ以上のディスプレイ装置の間に行われてもよい。これらのディスプレイは、１つ以上の処理装置によって制御されてもよい。ポインタの「移送」をサポートする「視標追跡」は、隣接または非隣接の表示可能なオブジェクト上の１つ以上の表示をサポートするように配置された１つ以上の視線追跡機構によって有効化することができる。表示は、空間内のオブジェクトの３次元またはホログラフィック投影の形にした「仮想」ものであってもよい。視野内の移送は、同一の表示モードで動作する装置に制限されない。例えば、ポインタまたは他の標識の移送は、スマートフォンの画面から、ホログラフィック投影にシームレスに行うことができ、スマートウォッチ、テレビ、医療装置のスクリーンおよび／または他のスクリーンにシームレスに行うことができる。 "Transfer" can involve a wide range of displays including virtual reality displays, augmented reality displays, tablets, mobile phones, smart devices, projected images, display monitors, signage and more. The "transfer" may be performed on a single display or between two or more display devices. These displays may be controlled by one or more processing devices. "Optical tracking", which supports "transfer" of pointers, is enabled by one or more eye tracking mechanisms arranged to support one or more displays on adjacent or non-adjacent visible objects. be able to. The display may be "virtual" in the form of a three-dimensional or holographic projection of an object in space. Transfer within the field of view is not limited to devices operating in the same display mode. For example, the transfer of pointers or other signs can be seamlessly done from the screen of a smartphone to a holographic projection, and seamlessly to the screen of a smartwatch, television, medical device and / or other screen.
視覚フィードバックと同時に機能（すなわち、上述した第４点の一部）を実行する能力は、他の入力（例えば、コンピュータのマウス、タッチパッド、デジタルペンなど）と組み合わせて眼球運動を用いて、３次元空間内で３次元仮想オブジェクトを視覚化し、制御する場合に、特に有用である。殆どの一般的なコンピュータ入力装置（例えば、マウス、トラックボールなど）に見られる２次元制御は、空間における３次元オブジェクトの遠近および他の特徴を制御する自由度が不十分である。２次元制御は、しばしば、様々な次元の制御を特定の制御、例えば、水平方向の平行移動、垂直方向の平行移動、ズーム、水平方向の回転、垂直方向の回転、軸方向の回転に分ける。 The ability to perform a function (ie, part of the fourth point mentioned above) at the same time as visual feedback is 3 using eye movements in combination with other inputs (eg, computer mouse, touchpad, digital pen, etc.). It is especially useful for visualizing and controlling 3D virtual objects in 3D space. The 2D controls found in most common computer input devices (eg, mice, trackballs, etc.) lack the freedom to control the perspective and other features of 3D objects in space. Two-dimensional control often divides various dimensions of control into specific controls, such as horizontal translation, vertical translation, zoom, horizontal rotation, vertical rotation, and axial rotation.
（両眼離反運動を含み）目を用いて、オブジェクトの表面上の位置または３次元空間内の位置に注視し、視点を基点として用いてシーンを扱うことは、遥かに自然（すなわち、直観且つ迅速）である。例えば、ピボットポイントに注視しながらタッチパッドを使用する場合、（指を左右に快速移動することによって）対象物を水平方向に回転することができ、または（指を上下に快速移動することによって）対象物を垂直方向に回転することができ、または（２本の指を円状に快速移動することによって）対象物を軸方向に回転することができる。意図またはその後の作動は、マウスクリック、スクリーンタッチなどの自発的な行動によって誤って示される可能性がある。目が制御メカニズムの（比喩的および本当の）中心部分であるため、対象領域から目へのフィードバックは即時である。 It is much more natural (ie, intuitive and intuitive) to use the eyes (including binocular divergence) to gaze at a position on the surface of an object or within a three-dimensional space and use the viewpoint as a starting point to handle the scene. Quick). For example, if you use the touchpad while gazing at the pivot point, you can rotate the object horizontally (by moving your finger left and right quickly) or (by moving your finger up and down quickly). The object can be rotated vertically, or the object can be rotated axially (by rapidly moving two fingers in a circular motion). Intention or subsequent action can be erroneously indicated by voluntary actions such as mouse clicks and screen touches. Feedback from the area of interest to the eye is immediate because the eye is the (figurative and true) central part of the control mechanism.
３次元世界の中でオブジェクトをより自然に見ることができるだけでなく、視線を入力として含む場合、オブジェクトをより自然に、迅速に且つ直感的に「編集」することもできる。構成要素を増減するまたは特性（例えば、領域の色、表面テクスチャおよび構成材料）を変更する場合、領域を見ながら、他の入力モダリティを用いて変更を行うことが快適である。また、領域の大きさ、形状、または特性の変更は、視線を離れることなく、（例えば、他の視覚指示、スクリーンまたは視点を用いて）瞬時に視覚化することができる。 Not only can you see the object more naturally in the three-dimensional world, but you can also "edit" the object more naturally, quickly and intuitively if you include the line of sight as input. When increasing or decreasing components or changing properties (eg, area color, surface texture and constituent materials), it is comfortable to look at the area and use other input modality to make the change. Also, changes in the size, shape, or properties of the area can be instantly visualized (eg, using other visual instructions, screens, or viewpoints) without leaving the line of sight.
これらのマルチモーダル方法は、しばしば「フィッツ法則」と呼ばれている人間工学制限またはＨＭＩ制限を克服することができる。この法則は、目標領域までの移動に必要とされた時間がターゲットまでの距離とターゲットの大きさとの比に依存し、ターゲットの大きさがターゲット選択プロセスの精確さによって決定されることを示唆している。眼球運動は、大きな距離をすぐにカバーすることができる。より高い空間解像度を有するマルチモーダル入力は、より小さい（およびより多くの）ターゲットを入力することができる。したがって、（通常、情報理論においてビット毎秒として記載される）選択速度は、視覚信号を含むマルチモーダル入力によって増加することができる。 These multimodal methods can overcome ergonomic or HMI limits, often referred to as "Fitz's law". This law suggests that the time required to travel to the target area depends on the ratio of the distance to the target to the size of the target, and the size of the target is determined by the accuracy of the target selection process. ing. Eye movements can quickly cover large distances. Multimodal inputs with higher spatial resolution can input smaller (and more) targets. Therefore, the selection speed (usually described as bits per second in information theory) can be increased by a multimodal input containing a visual signal.
視覚信号言語内のマルチモーダル入力の能力を実証する例として、オブジェクトまたはシーンを描画する（すなわち、写生する）プロセスが挙げられる。この場合、眼球運動を用いて、行動を行う対象としてのオブジェクトの位置を特定することができる。オブジェクトは、動いている可能性があり、短時間しか現れない可能性もある。異なるオブジェクト間の衝動性動眼を用いて、同様の操作を一連のオブジェクトに適用することができる。音声コマンドを用いて、「コピー」、「ペースト」、「消去」または「色塗り」などの操作を指定することができる。頭部動作を用いて、仮想オブジェクトの位置および／または回転を「微調整する」ことができる。（例えば、拡張現実感装置を使用する場合）カラーパレットを表示するタッチスクリーンまたは仮想スクリーンを用いて、指のジェスチャーと共に、色を選択することができる。 An example of demonstrating the ability of multimodal input within a visual signal language is the process of drawing (ie, sketching) an object or scene. In this case, the eye movement can be used to specify the position of the object as the target of the action. The object may be moving and may only appear for a short time. Similar operations can be applied to a series of objects using the impulsive occlusal eye between different objects. You can use voice commands to specify operations such as "copy," "paste," "erase," or "color." Head movements can be used to "fine-tune" the position and / or rotation of virtual objects. Colors can be selected with finger gestures using a touch screen or virtual screen that displays a color palette (eg, when using an augmented reality device).
異なる時間に、異なる方法で異なる入力モードを適用することができる。マルチモーダル入力の組み合わせによって、装着型装置および／またはモバイル装置を用いて、描画などの複雑な機能を完全に実行することができる。 Different input modes can be applied at different times and in different ways. The combination of multimodal inputs allows wearable and / or mobile devices to fully perform complex functions such as drawing.
視覚信号による調整したマルチモーダル制御
上述した実施形態において、例えば、感知した意図に基づいて、２つ以上の入力メカニズムを切り替えることによって、視覚信号を他の形態の機械制御と共に使用することができる。更なる実施形態において、視覚信号が他の入力の機能を「調整」または強化することを可能にすることによって、快適で直感的な方法でマルチモーダル制御を大きく拡張することができる。このようにして、異なる形態のマルチモーダル制御の威力を同時に利用することができる。
Multimodal Control Adjusted by Visual Signals In the embodiments described above, visual signals can be used with other forms of mechanical control, for example, by switching between two or more input mechanisms based on the sensed intent. In a further embodiment, the multimodal control can be greatly extended in a comfortable and intuitive way by allowing visual signals to "tune" or enhance the functionality of other inputs. In this way, the power of different forms of multimodal control can be utilized at the same time.
図２０Ａおよび図２０Ｂは、カーソル、ポインタ、または他の追跡位置５１１ａ（簡潔性のために、以下「ポインタ」と呼ぶ）のマルチモーダル移動の視覚信号に基づた調整の特に有用な例に含まれたステップを示している。ポインタ（カーソル、追跡位置など）は、移動可能な記号表現（矢印、十字線、アイコン、円など）を含むことができる。これらの記号表現は、場合によって不可視なり、場合によって強調表示、背景の変更、境界の追加、色の変更、オブジェクトサイズの変化などのフィートバックを提供する。図２０Ａにおいて、ユーザは、ディスプレイ５１０上で黒点として示された位置５１２に注視する。視線の測定には不確実性があるため、視線の位置は、注視されているオブジェクトの周りの領域５１３内に決定される。カーソルまたはポインタ５１１ａは、ディスプレイの他の任意位置にあってもよい。
20A and 20B are included as particularly useful examples of visual signal-based adjustment of the cursor, pointer, or
マウス、トラックボール、ジョイスティックまたは他の制御装置を用いて、カーソル５１１ａの位置を制御することができる。しかしながら、調整されたマルチモーダル制御の間に、ポインタが概ね視線方向５１３に沿って移動されるとき、連続移動の閾値距離に達した場合、カーソル５１ａは、注視領域５１３に瞬時に移動または「ジャンプ」する（５１５）。図２０Ｂに示すように、ポインタまたは他の追跡位置の迅速な移動（５１５）は、時間を大幅に節約し、マウス、トラックボール、ジョイスティックまたは他の制御装置の繰り返しの手動操作による疲労および／または他の影響を軽減する。場合によって、長い距離を通過するために、手または装置を持ち上げる動作を含む複数の手の動作が必要とされるであろう。
The position of the
ポインタの調整されたマルチモーダル制御は、本明細書において、ポインタの「移送」と呼ばれる。図２０Ｂに示すように、瞬間的な移動の後、ポインタ５１ｂは、一般的にユーザの中心窩視野に入る。一実施形態において、ポインタは、一般的なベクトルに沿って、ポインタの元の位置から、ユーザの視点に向って移動するように再現してもよい。この場合、ポインティング装置の動きは、再現時のポインタの動きを制御する。前述したように、ユーザがポインタを所望の位置に到着するように指示して、ポインティング装置をクリックまたは作動することによって動作を取るまで、ポインタの再現時に、（制御が容易になるように）ポインティング装置の感度が低減される。したがって、少ない眼球運動でまたは眼球運動を行わず、ユーザは、後で１つ以上の制御装置を用いて高精度で操作されるポインタを見ることができる。その結果、移動するポインタの精密制御が可能になる。このポインタは、概ね制御方向に沿って移動し、制御方向に沿って見られるが、最小限の努力でディスプレイ５１０内の広い領域をスキップすることができる。
Adjusted multimodal control of pointers is referred to herein as "transfer" of pointers. As shown in FIG. 20B, after momentary movement, the pointer 51b generally enters the user's foveal field of view. In one embodiment, the pointer may be reproduced so as to move from the original position of the pointer toward the user's viewpoint along a general vector. In this case, the movement of the pointing device controls the movement of the pointer at the time of reproduction. As mentioned above, pointing (for ease of control) during pointer reproduction until the user directs the pointer to reach the desired position and takes action by clicking or activating the pointing device. The sensitivity of the device is reduced. Thus, with less eye movement or no eye movement, the user can later see a pointer that is manipulated with high precision using one or more control devices. As a result, precise control of the moving pointer becomes possible. This pointer moves roughly along the control direction and is seen along the control direction, but with minimal effort it is possible to skip a large area within the
このスキームは、より高い精度を有する他の制御装置からのコマンド入力を受け入れながら、眼が迅速に動く能力を同時に利用する。ポインタに対する視線の方向および／または制御装置の移動速度を考慮して、閾値距離を通過したときに移送を行うことができる。代わりに、ユーザは、制御装置を用いて、注視位置に向かう急速移動が望ましいことを「ジェスチャ」することができる。ジェスチャは、急速移動（すなわち、閾値速度を超える）、装置を（例えば、前後）振ること、１つ以上の移動パターン（例えば、円形移動、固定方向の移動など）などを含むことができる。代わりに、押圧ボタン（または近接感知、音声認識、頭振り、瞬きなどを含む他のスイッチ機構）を用いて、注視位置に向かうポインタの急速移動が望ましいことを示すことができる。 This scheme simultaneously utilizes the ability of the eye to move quickly while accepting command input from other controllers with higher accuracy. Considering the direction of the line of sight with respect to the pointer and / or the moving speed of the control device, the transfer can be performed when the threshold distance is passed. Instead, the user can use the control device to "gesture" that rapid movement towards the gaze position is desirable. Gestures can include rapid movement (ie, exceeding a threshold speed), shaking the device (eg, back and forth), one or more movement patterns (eg, circular movement, fixed direction movement, etc.) and the like. Alternatively, press buttons (or other switch mechanisms including proximity sensing, speech recognition, head swing, blinking, etc.) can be used to indicate that rapid movement of the pointer towards the gaze position is desirable.
装置ユーザは、優先設定を作成することによって、移送プロセスの実行を引き起こすモード（すなわち、ジェスチャ、動き、音声作動など）、閾値（例えば、方向の範囲、移動距離など）および他の変数（例えば、特定単語の発話、瞬き時間など）を指定することができる。また、ポインタが視点の近く（すなわち、ポインタと注視位置との間の所定の距離内）にある場合、移送は、一般的に有利ではないため、自動的に無効化されてもよい。 By creating preference settings, the device user can create modes (ie, gestures, movements, voice activations, etc.), thresholds (eg, range of directions, distance traveled, etc.) and other variables (eg, distance traveled) that trigger the execution of the transport process. You can specify the utterance of a specific word, blinking time, etc.). Also, if the pointer is near the viewpoint (ie, within a predetermined distance between the pointer and the gaze position), the transfer is generally not advantageous and may be automatically disabled.
ポインタ位置における１つ以上の瞬時ジャンプの代替方法は、制御装置の感度または空間「ゲイン」を変更することを含む。「ゲイン」は、制御装置の動きまたは信号表示と、ポインタによってカバーされた距離との（一般的に比例する）関係を指す。例えば、制御装置の「ゲイン」は、カーソル位置と注視位置との間の距離に比例する。より高いゲイン（すなわち、所定の制御操作または他の入力に対してより大きなカーソル移動速度）の場合、ディスプレイ内のより大きな距離をより迅速に通過することができる。ポインタの急速移動を形成するまたはプロファイルすることができる。例えば、ポインタの移動は、急速移動の始めおよび終わりに加速段階および減速段階を有し、始めと終わりとの間に高速段階を含むことができる。これらのスキームは、ポインタの移動を平滑化することによって、移送中に注意散漫を引き起こす可能性を低減することができる。 An alternative method of one or more instantaneous jumps at a pointer position involves changing the sensitivity or spatial "gain" of the controller. "Gain" refers to the (generally proportional) relationship between the movement or signal display of a controller and the distance covered by the pointer. For example, the "gain" of the control device is proportional to the distance between the cursor position and the gaze position. Higher gains (ie, higher cursor movement speeds for a given control operation or other input) allow faster passage through larger distances within the display. A rapid movement of the pointer can be formed or profiled. For example, pointer movement may have acceleration and deceleration stages at the beginning and end of rapid movement, and may include high speed stages between the beginning and end. These schemes can reduce the possibility of causing distraction during transfer by smoothing the movement of the pointer.
移送は、ディスプレイの数またはディスプレイの種類に制限されない。例えば、ポインタは、仮想現実ヘッドセット上の位置から、スマートフォン上のディスプレイに「移送」されてもよい。プラットフォーム間の移送は、装着型コンピュータを利用する応用において特に重要である。例えば、表示領域が一装置では制限されるが、別の装置では使用可能である。他の装置への急速移送または装置間の急速移送を行うことができる。「移送」が空間領域を急速スキップするため、装置間の物理的なギャップは、「移送」プロセスに影響を与えない。実際に、このようなギャップを渡る移送は、直感的である。 Transport is not limited to the number of displays or the type of display. For example, the pointer may be "transferred" from a position on the virtual reality headset to the display on the smartphone. Transfer between platforms is especially important in wearable computer applications. For example, the display area is limited in one device, but can be used in another device. Rapid transfer to or between devices can be performed. Physical gaps between devices do not affect the "transfer" process, as the "transfer" skips spatial areas rapidly. In fact, transfers across such gaps are intuitive.
プラットフォーム間移送は、拡張現実ディスプレイまたは仮想現実ディスプレイ、スマートフォン、腕時計、タブレット、ラップ上方ディスプレイ、ディスプレイモニタ、ディスプレイモニタアレイ、サイネージなどを含み得る。ディスプレイは、固定なものであってもよく、可動なものであってもよい。ポインタ制御は、装置内のプロセッサおよびBluetooth（登録商標）、WiFi（登録商標）および／または他の種類の移動通信などの装置間通信プロトコルを用いて実行される。高速の電気通信は、装置間のシームレス（すなわち、大幅な遅延のない）移送を容易にするために不可欠である。 Inter-platform transfers can include augmented reality or virtual reality displays, smartphones, watches, tablets, lap-up displays, display monitors, display monitor arrays, signage, and more. The display may be fixed or movable. Pointer control is performed using an in-device processor and device-to-device communication protocols such as Bluetooth®, WiFi® and / or other types of mobile communications. High-speed telecommunications is essential to facilitate seamless (ie, significant delay-free) transfer between devices.
１つ以上のポインタに割り当てられた関連物も、ポインタと共に移送されてもよい。例えば、ポインタが画像またはテキストの一部に関連付けられているまたは「固定」されている場合、その画像またはテキストに関連する情報は、追加の装置のディスプレイおよび処理素子に移送されてもよい。移送され得る関連物の例は、画像、ビデオ、テキスト、数字、ファイル、アプリケーション、（アプリケーションによって実行される）特定の機能などを含む。この種類の移送は、視覚信号言語内の多くの機能の基礎となる。 Related items assigned to one or more pointers may also be transferred with the pointers. For example, if the pointer is associated with or "fixed" to a portion of the image or text, the information associated with that image or text may be transferred to the display and processing elements of the additional device. Examples of related objects that can be transferred include images, videos, text, numbers, files, applications, specific functions (performed by the application), and so on. This type of transfer underlies many functions within the visual signal language.
図２１Ａおよび図２１Ｂは、関連機能の「移送」を含むプラットフォーム間視覚信号伝達プロセスを示している。図２１Ａは、スマートフォンまたはタブレットなどのモバイル装置の表示領域５２０を示している。表示領域５２０は、様々な実行可能な機能を表すアイコン５２３と、視覚信号言語の一部として選択され得る作動アイコン列５２４とを含む。タッチ、視覚信号、音声コマンドなどの任意の組み合わせを用いて、ディスプレイ５２０を操作することによって、所望の動作を行うことができる。
21A and 21B show an inter-platform visual signaling process that includes a "transfer" of related functions. FIG. 21A shows a
例えば、ユーザがスマートウォッチ５２５上でテキストメッセージを取得したい場合、テキストメッセージ５２１を表すアイコンを、ウォッチ５２５の領域に「移送」することによって、この取得プロセスを開始することができる。移送操作を引き起こす設定は、視覚信号、タッチスクリーン、音声、または他の入力モダリティの任意の組み合わせによって実行されてもよい。移送は、装置間の空間的なギャップを通過できる１つ以上の眼球運動５２２（一般的に衝動性動眼）によって行われる。必要に応じて、受信装置（この場合、時計５２５）上の図形フィードバックは、移送プロセスを示す画像または動画を含むことができる。図形フィードバックは、例えば、眼球運動の方向に沿って、装置５２５のディスプレイの中心に向って快速移動するアイコン、サムネイルまたは他の図形表示であってもよい。
For example, if the user wants to retrieve a text message on the
図２１Ｂに示すように、一旦機能が「移送」されると、視覚信号および／またはタッチおよび音声を含む任意の他の制御機構を用いて、所望の操作をさらに行うことができる。例えば、既存の表示（すなわち、図２１Ａにおいて読み易い時計５２６ａ）を最小化することができる（５２６ｂ）。視覚信号および／または他のマルチモーダル入力を用いて、移送された機能を処理することもできる。例えば、図２１Ｂに示すように、テキストメッセージを読み上げることができる。
As shown in FIG. 21B, once the function has been "transferred", any other control mechanism, including visual signals and / or touch and audio, can be used to further perform the desired operation. For example, the existing display (ie, the easy-to-
プラットフォーム間の移送によって、各装置の強みを活用することができる。図２１Ａおよび図２１Ｂに示す場合、タブレット５２０は、（表示スペースによって）遥かに大きな選択アレイを有することができる。スマートウォッチは、外出先でも便利な読み取り装置である。複数のディスプレイ（例えば、「壁」状に配置された５×５個のディスプレイ）に亘ってマルチプラットフォーム移送を行う場合、移送プロセスを使用して、ポインタの位置を決定することができる。
Transfers between platforms allow us to leverage the strengths of each device. As shown in FIGS. 21A and 21B, the
図２２Ａおよび図２２Ｂは、マルチモーダル技術を用いて書類の一部のテキストを指定し、その後、指定されたテキストを「カット」して、書類の別の位置に「ペースト」するように、ポインタを「移送する」ことの利点をさらに示している。上述したように、一部の応用において、指定されたテキストは、異なる装置のディスプレイに貼り付けることができる。図２２Ａは、本文５３１を含むディスプレイ５３０を示している。この場合、ユーザは、テキスト５３２ａの第１行文章を書類５３１の下方のパングラム文の例に入れるように、第１行文章を移動したい。
22A and 22B use a multimodal technique to specify a portion of the text of the document, followed by a pointer to "cut" the specified text and "paste" it elsewhere in the document. It further demonstrates the benefits of "transporting" the paste. As mentioned above, in some applications, the specified text can be pasted onto the display of a different device. FIG. 22A shows a
視覚信号およびマルチモーダル入力（例えば、マウス、トラックパッド）の任意の組み合わせを用いて、（例えば、マウスのボタンをクリックすることによって）選択されたテキストの文頭を指定することによって、ポインタ５３３ａを第１行文章の文頭に移動する。次に、第１行文章５３５の文末付近の領域に視線を移動し、マルチモーダル（例えば、マウス、トラックパッド）制御を使用するプロセスを続けることによって、ポインタの位置を第１行文章の文末に向かって「移送」される（５３４）。注視方向（すなわち、第１行文章５３５の文末）への移動は、瞬間的または高速であってもよい。
ポインタは、（必要に応じて）注視位置に近づくにつれて、減速するように見せられてもよい。これによって、ポインタが入力コントロールにシームレスに応答すると共に、応答特性が動的に変化するという快適な感覚が得られる。その結果、画面の大部分を迅速に横断すると共に、高精度で語句の文末（すなわち、カット＆ペースト領域の終点）を指定することができ、時間および労力を大幅に節約する。 The pointer may appear to decelerate as it approaches the gaze position (if necessary). This provides a comfortable feeling that the pointer responds seamlessly to the input control and the response characteristics change dynamically. As a result, you can quickly traverse most of the screen and specify the end of a phrase (ie, the end point of the cut and paste area) with high accuracy, saving a lot of time and effort.
図２２Ｂに示すように、選択されたテキスト５３５の文末が指定されると、ユーザは、テキストの移動先である領域に向ける衝動性動眼（５３６）を行う。再度、操作の「ペースト」部分は、「移送」プロセスから恩恵を受ける。衝動性動眼は、マルチモーダル入力と共に、ポインタを迅速に注視領域に移送した後、マルチモーダル制御を用いてポインタをより精密に案内することによって、文章５３２ｂを挿入する位置５３３ｂを指定することができる。マウスまたはトラックボールのボタン押しまたは他のユーザ指示（例えば、音声コマンド）を行うことによって、操作を完了する。
As shown in FIG. 22B, when the end of the selected
例示的な実施形態において、移送機構を実現するための視覚信号は、遠隔または装着可能なオブジェクトまたは装置（頭部装着型装置を含む）上にまたはその近くに配置された検出装置を用いて取得されてもよく、１人以上のユーザの目を見ることができる１つ以上のシーンカメラを用いて取得されてもよい。 In an exemplary embodiment, visual signals for implementing a transfer mechanism are obtained using a detector located on or near a remote or wearable object or device (including a head-worn device). It may be acquired using one or more scene cameras that can see the eyes of one or more users.
マルチモーダル制御中に視覚信号のフィードバック
視覚的選択および／またはフィードバックを装置ユーザに提供する際に、マルチモーダル制御に特別な配慮が必要である。常に全ての可能なマルチモーダル選択を提供することは、インターフェイスを体験するユーザに混乱を与えてしまう。
Feedback of visual signals during multimodal control Special consideration is required for multimodal control when providing visual selection and / or feedback to the device user. Providing all possible multimodal choices at all times confuses the user experiencing the interface.
このような混乱を克服するための例示的な方法は、中心窩強調表示を含む。この実施形態において、ユーザがある領域に注視している時に、この領域内（すなわち、概ね中心窩視野内）の選択可能な項目がユーザに表示されてもよい。表示は、光輪、領域の輪郭線、フォントの変更、オブジェクトのサイズ変更または他の表示形式を含む色または輝度の強調表示、前景色の変化または背景の違いで行われる。中心窩視野内の選択可能な項目の動的表示は、多数の選択項目が表示画面を乱雑にする状況を回避することができる。また、表示されている領域および選択可能な項目に関するフィードバックをユーザ（初心者）に提供する。 Illustrative methods for overcoming such confusion include foveal highlighting. In this embodiment, when the user is gazing at an area, selectable items within this area (ie, generally within the foveal field of view) may be displayed to the user. Display is done with halos, area contours, font changes, object resizing or color or brightness highlighting, including other display formats, foreground changes or background differences. The dynamic display of selectable items in the foveal field of view can avoid situations where a large number of select items clutter the display screen. It also provides the user (beginner) with feedback on the areas displayed and the items that can be selected.
選択表示の除去を慎重に制御し、除去順序を決定することも重要である。最終選択（すなわち、操作を行うために更なる選択が必要ないもの）の場合、選択された操作を続行するために、選択されていない項目の強調表示（または他の表示）を直ちに除去する必要がある。 It is also important to carefully control the removal of the selection display and determine the removal order. For the final selection (ie, one that does not require further selection to perform the operation), the highlighting (or other display) of the unselected item must be immediately removed in order to continue the selected operation. There is.
マルチモーダル制御を使用すると、マウスまたはトラックボールなどの本質的に意図的な入力（表１を参照）の最初の動きが、視覚信号を使用して特定の選択を行わない信号として使用することができ、視覚信号メニューを取り消すことを可能にする。アルゴリズム的には、視覚信号表示要素の包含および取消を都合よく実施する１つの方法は、透明度を制御できる画像「レイヤ」をこれらの要素に割り当てることを含む。必要に応じて、ディスプレイと同様のサイズの層（または層内の領域）の透明度を低く（すなわち、不透明に）することによって、視覚信号要素を表示することができる。必要に応じて、注意力を引く傾向を低減するように、視覚信号要素を徐々に表示することができる。不必要なときに、層内の１つ以上の領域を透明にすることによって、視覚信号層の１つ以上の領域を除去することができる。 With multimodal control, the first movement of an essentially intentional input (see Table 1), such as a mouse or trackball, can be used as a signal that does not make a specific selection using a visual signal. It allows you to undo the visual signal menu. Algorithmically, one method of conveniently performing inclusion and cancellation of visual signal display elements involves assigning image "layers" with controllable transparency to these elements. If desired, the visual signal element can be displayed by reducing the transparency (ie, opaque) of a layer (or area within the layer) that is similar in size to the display. If necessary, visual signal elements can be gradually displayed to reduce the tendency to draw attention. By making one or more areas within the layer transparent when not needed, one or more areas of the visual signal layer can be removed.
視覚信号に基づいた音楽の読取
視覚信号の特定の応用は、音楽の読取、演奏および／または視聴を含む。物理またはタッチスクリーン世界において、音楽家は、物理的な捲りまたはスクリーンスワイプを用いてページを捲るために、楽譜ページの終わりに演奏を一時停止しなければならない。このことは、演奏中に望ましくない一時停止を引き起こし、音楽の連続性および流れを乱す可能性がある。
Reading Music Based on Visual Signals Certain applications of visual signals include reading, playing and / or listening to music. In the physical or touch screen world, a musician must pause playing at the end of a score page in order to turn the page using a physical turn or screen swipe. This can cause unwanted pauses during the performance, disrupting the continuity and flow of the music.
一実施形態において、視覚信号および追跡情報を統合することによって音楽中の音楽家の演奏位置を決定し、音楽のスマートスクロールを有効化することによって、演奏の中断を回避する。１ページに複数の行を含む音楽の標準ページの場合、音楽家がページの終わりに到達すると、ページ捲りが実行される。音楽家が当該ページの音楽を完全に演奏して、次のページに移動するように、必要に応じて、視線の移動速度に基づいて、ページを徐々に捲る。 In one embodiment, the visual signal and tracking information are integrated to determine the playing position of the musician in the music, and smart scrolling of the music is enabled to avoid interruptions in the performance. For a standard page of music that contains multiple lines per page, page turning is performed when the musician reaches the end of the page. If necessary, gradually turn the page based on the speed of movement of the line of sight so that the musician plays the music of the page completely and moves to the next page.
図２３は、ディスプレイ５７０に表示された楽譜５７１の例を示している。音楽家の視線（必ずしも演奏位置に限らない）がページの最後の音符５７２の領域に到達すると、ページ捲りを行う。ユーザの好みに基づいて、ページ捲りは、「連続的な」行スクロールモードまたは伝統的なページ捲りモード、すなわち、ページまたはディスプレイの一部または全部をスクロールまたはフリップすることによって行うことができる。
FIG. 23 shows an example of the
後者の方法の利点は、既存の楽譜を単にスキャンして、視線追跡を含むアプローチに簡単に移植できるという事実を含む。また、現代の音楽家は、この捲り方法に慣れている。この方法を使用する場合、演奏中に情報が失われないように、スクロールのタイミングを制御することが重要である。 The advantages of the latter method include the fact that existing sheet music can simply be scanned and easily ported to approaches involving eye tracking. Also, modern musicians are accustomed to this method of turning. When using this method, it is important to control the timing of scrolling so that no information is lost during the performance.
楽譜を含むデータベースは、ディスプレイ内の変更を行う好ましい区切り点（すなわち、表示変更を行いたい楽譜内の位置）を含むことができる。代替的または追加的に、表示された楽譜の終わり（または他の基準位置）から所定の距離内に１つ以上の区切り点を設置することができる。 A database containing a score can include a preferred break point in the display where the change is made (ie, a position in the score where the display change is desired). Alternatively or additionally, one or more break points can be placed within a predetermined distance from the end of the displayed score (or other reference position).
代替のスクロール方法において、楽譜は、複数の行を１つの水平「ストリップ」に連結するようにフォーマットされる。図２４は、例示として、楽譜ストリップ５８０の一部を示している。この楽譜ストリップ内のセグメントまたは「ウィンドウ」５８１は、音楽家に表示される。ユーザの注視位置５８２が表示された楽譜の領域の終わりに近づくと、楽譜は、前進５８３（すなわち、右から左へ）して、表示ウィンドウ内に入る。音楽家は、楽譜のテンポ、視覚信号および／または聴覚フィードバックに基づいた追加のフィードバックに従ったユーザの視線によって決定されたスクロール速度で、単一行の楽譜の全体を画面上で連続的にスクロールすることができる。既存の楽譜をスキャンして再フォーマットするために必要とされる努力にも拘わらず、この方法は、一部の状況、特に小さなディスプレイ（仮想現実ディスプレイ、拡張現実ディスプレイまたは複合現実ディスプレイを含む）が望ましい状況には好ましい。一部のユーザにとって、１行の連続スクロールは、より快適で直感的である。
In an alternative scrolling method, the score is formatted to connect multiple lines into one horizontal "strip". FIG. 24 shows a part of the
ページのインテリジェントスクロールは、しばしば音楽家のスキルレベルを反映する視覚動作に依存する。音楽家は、一般的に、演奏されている音符の先のいくつかの小節を見ている。このことは、スキルレベルによって異なる。未熟の音楽家は、すぐに演奏される音符のみを見ているが、熟練の音楽家は、１種の事前準備として、演奏されている音符の先のいくつかの小節を読むことができる。これは、「初見演奏」（sight reading）、すなわち、練習なしに楽譜を初めて読んで正確に演奏する能力として知られた音楽家のスキルレベルに関連している。熟練の音楽家は、多くの小節を先に読むことができるため、これから行う演奏、および適切なリズムで唇、腕、指、胸および／または他の身体部分を動かすことに十分な時間を与え、音楽を正確に演奏することができる。 Intelligent scrolling of pages often relies on visual movements that reflect the skill level of the musician. Musicians generally look at a few bars beyond the note being played. This depends on the skill level. An immature musician sees only the notes that are played immediately, but a skilled musician can read a few bars ahead of the notes that are being played as a kind of preparation. This is related to "sight reading", the skill level of musicians known for their ability to read and accurately play music for the first time without practice. Experienced musicians can read many measures first, giving them plenty of time to play and move their lips, arms, fingers, chest and / or other body parts with the proper rhythm. You can play music accurately.
例示的な実施形態において、楽譜表示の視覚信号に基づく制御を用いて、初見でより複雑な音楽を演奏するように、音声および注視位置に基づいて、音楽家を採点するおよび／または訓練することができる。図２５Ａおよび図２５Ｂに示すように、音楽家が演奏している音符および読んでいる音符の間の距離を比較することによって、採点およびフィードバックを行うことができる。 In an exemplary embodiment, musicians may be scored and / or trained based on audio and gaze position to play more complex music at first glance, using visual signal-based control of the musical score display. it can. As shown in FIGS. 25A and 25B, scoring and feedback can be provided by comparing the distance between the note being played and the note being read by the musician.
図２５Ａは、楽譜５９０を示している。この場合、演奏位置５９１ａと（演奏位置の先の）注視位置５９２ａとの間の距離を測定する。演奏位置は、楽器で演奏されている音符の検知に基づいて、または楽器によって生成され、マイクを用いて検知された音の音声認識に基づいて、決定することができる。図２５Ｂは、図２５Ａと同様の楽譜を示しているが、この場合、訓練および／またはフィードバックによって、演奏位置５９１ｂと注視位置５９２ｂとの間の距離が大きくなった。
FIG. 25A shows the
演奏する間に、一般的な視覚信号言語を用いて、（例えば、異なる楽譜のアーカイブから）楽譜の選択、ユーザ好みの設定、他の音楽家または指揮者との無言交流、他人からの要求の応答、または、再録音が必要な演奏部分の（リアルタイム）表示を含む様々な機能を制御することができる。電子制御機器の場合、視覚信号を使用して、１つ以上の機器を部分的にまたは完全に制御する（例えば、パラメータを設定する）ことができる。このような操作は、視覚信号言語の一般的な構造および選択シーケンスを用いて、個別に実行することができる。 While playing, use common visual signal languages to select scores (eg, from archives of different scores), set user preferences, interact silently with other musicians or conductors, respond to requests from others. Or, you can control various functions, including a (real-time) display of the playing part that needs to be re-recorded. In the case of electronically controlled devices, visual signals can be used to partially or completely control (eg, set parameters) one or more devices. Such operations can be performed individually using the general structure and selection sequence of the visual signal language.
視覚信号言語スクラッチパッド
多くのコンピュータ装置のオペレーティングシステムに利用されている普遍的機能または作業は、情報ブロックを１つの位置またはアプリケーションから、別の位置またはアプリケーションに転送することである。情報の種類は、文字、単語または単語群、数字または数字群、写真、図面、音声ファイル、ビデオ、機能または用途を表すアイコン、フォルダまたは他の情報のクラスタ表現などを含むことができる。視覚信号言語を用いてこのような情報を転送することを可能にする例示的な方法は、「スクラッチパッド」の使用である。「スクラッチパッド」は、視覚信号プロセス中に複数の機能を果たすことができる。スクラッチパッドは、１）転送または保管のために、データを整理することができ、２）データの一部を分けることができ、３）必要に応じて、データをある種類から別の種類に変換する（例えば、音声をテキストに変換する）ことができ、４）データセットを保存および検索するための保管リポジトリとして機能することができ、および５）転送プロセスを指示し且つ動作させる中心位置として機能することができる。
Visual Signal Language Scratch Pad A universal function or task utilized in the operating systems of many computer devices is to transfer a block of information from one location or application to another. Information types can include letters, words or groups of words, numbers or groups of numbers, photographs, drawings, audio files, videos, icons representing functions or uses, folders or cluster representations of other information. An exemplary method of making it possible to transfer such information using a visual signal language is the use of "scratch pads". The "scratch pad" can perform multiple functions during the visual signal process. Scratch pads can 1) organize data for transfer or storage, 2) separate parts of data, and 3) convert data from one type to another as needed. Can (eg, convert voice to text), 4) can act as a storage repository for storing and retrieving datasets, and 5) can act as a central location to direct and operate the transfer process. can do.
情報源は、ワードプロセッサ、スプレッドシート、画像およびビデオの閲覧ソフト、テキストの受信ソフト、メールの受信ソフト、Ｗｅｂブラウザなどのアプリケーションによって生成されたデータを含む。また、情報源は、カメラ、マイクロフォン、ＧＰＳ、時計、加速度計、センサなどの装置の出力を含む。情報の宛先は、ワードプロセッサ、スプレッドシート、画像およびビデオの編集ソフト、テキストの送信ソフト、メールサーバ、ウェブページの選択および／または応答などを含む。 Sources of information include data generated by applications such as word processors, spreadsheets, image and video viewing software, text receiving software, email receiving software, and web browsers. Sources of information also include the output of devices such as cameras, microphones, GPS, clocks, accelerometers, and sensors. Destinations of information include word processors, spreadsheets, image and video editing software, text sending software, mail servers, web page selection and / or responses, and the like.
図２６は、視覚信号により制御されたスクラッチパッドの例示的な表示配置６００である。この表示配置は、視覚信号を用いて情報をスクラッチパッドに保存することができる領域６０１と、視覚信号を用いてスクラッチパッドから情報を取り出すことができる領域６０２と、以前にスクラッチパッドに保存された情報を単独で表示するために「スクロールアップ」する領域６０３と、以前にスクラッチパッド保存された情報を単独で表示するために「スクロールダウン」する（すなわち、より最近の情報を表示する）領域６０４とを含む。
FIG. 26 is an
スクラッチパッドに情報を保存するプロセスは、まず、オブジェクトに注視して、その後、視線をオブジェクト６０５の領域からディスプレイ上のオブジェクト「保存」領域６０１に移動することを含む。上述したように、保存された情報は、多くの異なる種類を含むことができる。さらに、オブジェクトは、仮想ディスプレイ６００または現実世界から抽出することができる。その後、オブジェクト認識を用いて、オブジェクトのＩＤを登録することができ、および／またはオブジェクトの画像を格納することができる。
The process of storing information in the scratchpad involves first gazing at the object and then moving the line of sight from the area of the
新しく格納されたオブジェクトのサムネイルは、スクラッチパッドの「保存」領域６０１および「取出」領域６０２の両方に配置される。「保存」領域から情報を直接に抽出することができないため、「保存」領域６０１内の情報の格納は、単に最近情報が格納されたことを装置ユーザに知らせることである。情報の取り出しは、「取出」領域６０２を用いて行われる。
Thumbnails of newly stored objects are placed in both the "save"
スクラッチパッドから情報を取得するプロセスは、次のステップを含む。具体的には、
１．ユーザは、格納された情報のアーカイブ表現（例えば、サムネイル）を分けるために、必要に応じて、スクロールアップ６０３および／またはスクロールダウン６０４を行うことができる。スクロールは、「取出」領域６０２から「スクロールアップ」領域６０３または「スクロールダウン」領域６０４のいずれかに視線を移動する衝動性動眼を実行し、その後、「取出」領域６０２に視線を戻すことを含む。ユーザの視線が「スクロールアップ」領域６０３または「スクロールダウン」領域６０４に停留する場合、隣接する「取出」領域が中心窩視野内にあり、ユーザが見ることができるため、スクロールは、繰り返される。
２．便宜上、スクラッチパッドから分けられ、不要になった項目は、スクラッチパッドから削除されてもよい。削除は、例えば、「取出」領域６０２をバイパスして、視線を「スクロールアップ」領域６０３から「スクロールダウン」領域６０４に移動するまたはその逆の衝動性動眼を行うことによって、達成することができる。削除操作を行った後、削除された項目が領域６０３のものであるかまたは領域６０４のものであるかに応じて、「取出」領域に表示される項目は、スクラッチパッドの「アップ」リストまたは「ダウン」リストの次の項目になる。
３．スクラッチパッドから所望の項目を分けた後、ユーザは、項目を挿入するためのディスプレイ領域を捜索することができる。挿入位置は、例えば、ワードプロセッサまたはテキストメッセージ内の位置、スプレッドシート内のセル、ウェブページ内の位置などであり得る。視線を所望の挿入位置６０６から「取出」領域６０２に移動することによって、取り出しを完了する。
The process of retrieving information from the scratch pad involves the following steps: In particular,
1. 1. The user can scroll up 603 and / or scroll down 604 as needed to separate archive representations (eg, thumbnails) of the stored information. Scrolling performs an impulsive eye movement that moves the line of sight from either the "scroll up"
2. For convenience, items that are separated from the scratch pad and are no longer needed may be removed from the scratch pad. Deletion can be achieved, for example, by bypassing the "extract"
3. 3. After separating the desired item from the scratch pad, the user can search the display area for inserting the item. The insertion position can be, for example, a position in a word processor or text message, a cell in a spreadsheet, a position in a web page, and so on. Extraction is completed by moving the line of sight from the desired
上述した「スクロール」方法の代わりにまたはそれに加えて、転送するスクラップパッドの項目を分離するために、スクラッチパッドを「検索」することができる。これは、音声入力を使用すると特に強力である。しかしながら、ディスプレイ上に分けられたテキストまたは画像もしくは装置によって取り込まれた画像または音声は、検索および比較のソースとして使用されてもよい。必要に応じて、スクラッチパッド内の検索を可能にするために、変換を実行することができる。例えば、複数モデルの検索を可能にするために、（スクラッチパッド内で音声を直接に比較しない場合）音声をテキスト（または他の表現）に変換してもよい。同様に、オブジェクトの特定および／または音声の認識を用いて、スクラッチパッド内の項目を特定することによって、検索を促進することができる。スクラッチパッド内の最も適切な項目は、転送するために、「取出」領域６０２に入れることができる。
Instead of or in addition to the "scrolling" method described above, scratchpads can be "searched" to separate items of scrappads to transfer. This is especially powerful when using voice input. However, the text or images separated on the display or the images or sounds captured by the device may be used as a source of search and comparison. If desired, conversions can be performed to allow searching within the scratchpad. For example, speech may be converted to text (or other representation) to allow searching for multiple models (without direct comparison of speech within the scratch pad). Similarly, object identification and / or speech recognition can be used to facilitate searches by identifying items within the scratch pad. The most suitable item in the scratch pad can be placed in the "removal"
視線を「スクロールアップ」領域６０３に停留することによって、例えば、捜索に音声入力を使用することを装置に警告することができる。視線を「スクロールアップ」領域６０３に停留することによって、「取出」領域６０２に現在に格納されている情報の前の（すなわち、より先にスクラッチパッドに格納された）情報のサブセットを検索することを示すことができる。逆に、視線を「スクロールダウン」領域６０４に停留することによって、捜索に音声入力を使用することを装置に警告することができ、「取出」領域６０２に現在に格納されている情報の後に格納された情報のサブセットを検索することを示すことができる。視線を「取出」領域６０２に停留することによって、スクラッチパッド内の全ての情報を検索することを示すことができる。これは、ユーザの視線の「停留」操作が適切であるため、スクラッチパッド内の検索結果が認知される、視覚信号言語内の少ないインスタンスの１つである。
By stopping the line of sight in the "scroll-up"
図２６において、スクラッチパッド内の保存領域６０１Ａおよび取出領域６０２に示されたアイコンは、スクラッチパッドが空である時の例示的なプレースホルダである。スクラッチパッドの「保存」領域６０１または「取出」領域のいずれかに情報を入ると、スクラッチパッド内の情報を示すサムネイルは、各々の領域に表示される。
In FIG. 26, the icons shown in the storage area 601A and the
ドロンおよび他の遠隔乗り物（vehicle）の視覚信号制御
視覚信号言語は、位置および／または方向を迅速且つ／または反復的に指定する必要がある場合、および／または他の身体部分による制御、例えば、手によるジョイスティックまたはコンピュータマウスの制御などを減らすまたは無くすことが望ましい場合に、特に有用である。１つ以上のカメラを搭載したドローン（すなわち、無人飛行器）または他の遠隔制御乗り物（例えば、ボート、潜水艦）の飛行および／または遠隔測定（telemetry）は、このような例である。
Visual signal control of Delon and other remote vehicles When the visual signal language needs to specify position and / or direction quickly and / or iteratively, and / or control by other body parts, eg, It is especially useful when it is desirable to reduce or eliminate manual control of the joystick or computer mouse. Flight and / or telemetry of drones (ie, unmanned aerial vehicles) or other remote controlled vehicles (eg, boats, submarines) equipped with one or more cameras is such an example.
このような乗り物を飛行させるときの主な機能は、「どこに飛ぶ」（または、地上、海洋または水中の乗り物の場合、「どこに行く」）を指定することである。殆どの場合、このことは、障害を克服し、「飛行禁止」領域を避けるために、風、水流、他の環境条件および他の要因を考慮して、連続的または定期的に行われる。 The main function when flying such a vehicle is to specify "where to fly" (or "where to go" for ground, ocean or underwater vehicles). In most cases, this is done continuously or regularly, taking into account wind, water currents, other environmental conditions and other factors, in order to overcome obstacles and avoid "no-fly" areas.
３次元の飛行方向を指定する時に、特に直感的且つ有用な方法は、ドローンまたは乗り物の視点からのターゲット位置／方向を「見る」ことである。ドロンまたは乗り物に搭載された１台以上のカメラからのリアルタイム遠隔測定および画像は、拡張現実ヘッドセット、仮想現実ヘッドセット、複合現実ヘッドセット、タブレット、スマートフォン、スマートウォッチ、ラップトップコンピュータ、またはデスクトップコンピュータのモニタで見ることができる。視覚信号制御を行うための様々な選択は、生ビデオ画像の上または周囲に重ねることができる。乗り物の制御は、視覚信号言語を用いて実行され、必要に応じて、他の（例えば、手動、音声）制御によって支援される。 A particularly intuitive and useful method when specifying a three-dimensional flight direction is to "see" the target position / direction from the perspective of the drone or vehicle. Real-time remote measurements and images from one or more cameras on board Delon or the vehicle can be augmented reality headsets, virtual reality headsets, mixed reality headsets, tablets, smartphones, smartwatches, laptop computers, or desktop computers. You can see it on your monitor. Various choices for visual signal control can be overlaid on or around the raw video image. Vehicle control is performed using a visual signal language and is assisted by other (eg, manual, audio) controls as needed.
視覚信号言語の他の要素と同様に、一般的に環境を捜索するための捜索的な眼球運動と、ドローンまたは乗り物を制御する意図を伝えるための「意図的な」眼球運動とを区別することが重要である。ディスプレイ内のターゲット位置から作動位置または一連の位置に視線を移動する意図的な衝動性動眼を行うことによって、目標方向（ＧＰＳなどの他の機器を使用する場合、距離）を特定することができる。このような目標位置は、静止であってもよく、（乗り物に追跡されるように）動いてもよい。１つ以上の目標は、異なる時点で表示されなくてもよい。 To distinguish between searchable eye movements, which generally search the environment, and "intentional" eye movements, which convey the intent to control a drone or vehicle, as well as other elements of visual signal language. is important. The target direction (distance when using other devices such as GPS) can be specified by performing intentional impulsive oculomotor eye movement that moves the line of sight from the target position in the display to the operating position or a series of positions. .. Such target positions may be stationary or may move (as tracked by the vehicle). One or more goals may not be displayed at different times.
部分的なまたは完全なハンズフリー制御を達成するために、視覚信号を用いて、乗り物の他の運動特性を制御することもできる。これは、例えば、前進速度、スロットル、制御面（control surface）のサイズおよび方向、プロペラピッチなどを含む。また、視覚信号言語を用いて、「ホーム帰還」、荷物卸し、停空飛翔、警告または指示信号の発行などの他の作業を行うことができる。視覚信号を用いて、メニューを介して状態表示を照会することができ、飛行計画を確認または変更することができ、または飛行ポイントを表示およびマークすることができる。 Visual signals can also be used to control other motion characteristics of the vehicle to achieve partial or complete hands-free control. This includes, for example, forward speed, throttle, control surface size and direction, propeller pitch, and the like. The visual signal language can also be used to perform other tasks such as "home return", unloading, flying at rest, issuing warning or instruction signals. Visual signals can be used to query status indications via menus, to review or change flight plans, or to display and mark flight points.
乗り物の運転中の視覚追跡および視覚指示
更なる例示的な実施形態において、視線制御および視覚信号言語の要素を用いて乗り物を運転しているときに、特別な配慮を与えなければならない。乗り物は、自動車、トラック、重機、ボート、雪上車、全地形万能車などを含む。命を脅かす潜在的な状況を避けるために、視覚指示に加えて、短時間の中断しかできない運転者の視線を用いて、運転環境を探査しなければならないという特別な配慮がある。
Visual tracking and visual instruction while driving a vehicle In a further exemplary embodiment, special consideration must be given when driving a vehicle using elements of gaze control and visual signal language. Vehicles include automobiles, trucks, heavy equipment, boats, snowmobiles, all-terrain vehicles, and more. There is a special consideration that the driving environment must be explored using the driver's line of sight, which can only be interrupted for a short period of time, in addition to visual instructions, to avoid potential life-threatening situations.
運転者の安全に関与する機関は、適切な条件の下で、前方の道路から視線を逸らす時間が１．６秒〜２．０秒を超えないようにすることを推奨している（異なる機関の推奨値に変動することがある）。殆どの運転者は、実際に約２秒間脇見をすると、不安感があると報告している。基準として、（例えば、ヘッドアップディスプレイに関連していない）従来の電波を調整するのにかかる平均時間は、１．６秒である。しかしながら、このような時間の幅広い変動が報告されている。 Institutions involved in driver safety recommend that, under appropriate conditions, the time to divert the line of sight from the road ahead should not exceed 1.6 to 2.0 seconds (different agencies). It may fluctuate to the recommended value of). Most drivers report anxiety when they actually look aside for about two seconds. As a reference, the average time it takes to adjust a conventional radio wave (eg, not related to a heads-up display) is 1.6 seconds. However, such wide fluctuations in time have been reported.
運転中に、運転者の注視位置を組み込むことで、快適性および安全性の両方に多くの利点が生まれる。１）運転者情報の表示または他の形式は、運転者が前方の道路を見たか否かおよび見た時間に基づいて修正される。表示装置は、不適当な時間に注意を引くことおよび／または認知負荷を軽減することを避けるために、運転条件（例えば、高速道路対駐車場）および運転者の行動に動的に適応することができる。２）視覚信号制御および表示フィードバックのタイミングは、運転者が所定の時間後に前方の道路に視線を向けるように設計されてもよい。３）乗り物が（例えば、自動制動を引き起こす可能性がある）緊急状況を検知して、運転者が緊急状況の方向に見ていない場合、運転者の視線を変えるための措置を講じることができる。 Incorporating the driver's gaze position while driving offers many benefits for both comfort and safety. 1) The display or other form of driver information is modified based on whether or not the driver has seen the road ahead and how long he has seen it. The display device should dynamically adapt to driving conditions (eg, highway vs. parking) and driver behavior to avoid drawing attention to inappropriate times and / or reducing cognitive load. Can be done. 2) The timing of visual signal control and display feedback may be designed so that the driver directs his or her eyes to the road ahead after a predetermined time. 3) If the vehicle detects an emergency (eg, which can cause automatic braking) and the driver is not looking in the direction of the emergency, measures can be taken to change the driver's line of sight. ..
視覚信号言語による選択および作動の迅速性を利用して、仮想もの（例えば、コンソールディスプレイ、拡張現実アイウェア、ヘッドアップディスプレイなど）、特に比較的大型ものを用いた安全な運転を促進することができる。前述したように、眼球の衝動性動眼は、人体内で最も速い動きである。コンソールディスプレイ上の位置に視線を移動する衝動性動眼（しばしばメモリガイドされる）の後、選択および／または作動を行うための１つ以上の衝動性動眼は、一般的に、同様の機能を実行するように、タッチ感応画面またはボタン上の位置に対してジェスチャまたは作動を行う手の動きよりも短い時間で実行することができる。視覚信号を使用して、ハンドルから手を離すことなく、全ての運転制御を行うことも可能である。 Utilizing the speed of selection and operation by visual signal language, it is possible to promote safe driving with virtual objects (for example, console displays, augmented reality eyewear, head-up displays, etc.), especially relatively large ones. it can. As mentioned above, the impulsive oculomotor eye of the eye is the fastest movement in the human body. After an impulsive eye (often memory-guided) that moves the line of sight to a position on the console display, one or more impulsive eyes for selection and / or operation generally perform a similar function. As such, it can be performed in less time than the movement of the hand performing the gesture or action on the touch-sensitive screen or position on the button. It is also possible to use visual signals to control all driving without taking your hands off the steering wheel.
特定の運転事例において、運転進路から視線を逸らす時間をさらに減らすために、状況依存型メニュー選択および流線型メニュー選択を採用することができる。前者の例として、無線機能を制御するための仮想「ボタン」は、ナビゲーション地図の表示中に直接に利用できない。後者の例として、作動位置、例えば、ディスプレイの４隅のうち１つに視線を移動するシングル衝動性動眼を用いて、機能選択および作動の両方を行うことができる。 In certain driving cases, context-sensitive menu selection and streamlined menu selection can be employed to further reduce the time it takes to divert the line of sight from the driving path. As an example of the former, virtual "buttons" for controlling wireless functions are not directly available while displaying the navigation map. As an example of the latter, both function selection and activation can be performed using a single impulsive oculomotor eye that moves the line of sight to the operating position, eg, one of the four corners of the display.
対照的に、非運転応用において、画像または地図との対話は、ズームイン、ズームアウト、パン、オブジェクトの選択、オブジェクトの識別、カットおよびペーストなどの多くの操作から、選択を行うことに関与する。視覚信号言語において、多くの操作から選択を行うことは、一般的に、少なくとも２つの衝動性動眼（所謂「表示」操作）、すなわち、メニューを生成するための衝動性動眼、およびメニューから操作を選択するための衝動性動眼を必要とする。 In contrast, in non-driving applications, interacting with an image or map involves making selections from many operations such as zooming in, zooming out, panning, selecting objects, identifying objects, cutting and pasting. In a visual signal language, making a selection from many operations generally involves at least two impulsive eyes (so-called "display" operations), i.e. the impulsive eyes to generate a menu, and operations from the menu. Requires impulsive oculomotor eye to choose.
しかしながら、運転応用において、地図を見るときに、選択は、例えば、画面の４隅の１つに視線を移動する衝動性動眼に制限される。４隅に関連する操作は、（地図上で見た位置を中心に）ズームイン、ズームアウト、パン（地図上で見た方向）、および地図表示から離れる（例えば、高次メニューに戻る）ことを含む。 However, in driving applications, when viewing a map, the choice is limited to, for example, the impulsive oculomotor eye that moves the line of sight to one of the four corners of the screen. Operations related to the four corners include zooming in, zooming out (centering on the position viewed on the map), panning (direction viewed on the map), and leaving the map display (for example, returning to the higher-level menu). Including.
制限された状況依存型機能の迅速選択および作動は、運転進路から視線を逸らす時間を短縮し、運転に直接関連しない認知負荷を軽減する。また、運転者が前方の道路を先に見てから、運転進路から視線を短時間逸らし、選択を行うように強制的にすることもできる。前述したように、前方の道路を見ている時に、注意を引かない方法で表示を変更する（例えば、衝動性動眼および／または瞬き失明の期間中、輝度および／または色を徐々に変更する（変化失明を利用する）など）方法は、運転者の安全性をさらに高めることができる。安全な運転行動をさらに促進するために、運転者の注視方向に関する知識を用いて、運転者に情報を提供するタイミングおよび内容を変更することができる。 The rapid selection and activation of restricted context-sensitive features reduces the amount of time the driver diverts his or her line of sight and reduces cognitive load that is not directly related to driving. It is also possible to force the driver to look ahead on the road first and then divert his or her line of sight from the driving path for a short time to make a choice. As mentioned above, when looking at the road ahead, change the display in a non-obtrusive way (eg, gradually change the brightness and / or color during periods of impulsive occlusal eye and / or blink blindness (eg). (Using change blindness), etc.) can further enhance the safety of the driver. In order to further promote safe driving behavior, knowledge about the driver's gaze direction can be used to change the timing and content of providing information to the driver.
運転中、運転者は、運転進路から視線を短時間離れて、選択可能なオブジェクトに移動することがある。選択可能なオブジェクトは、乗り物の内部のコンソールまたはヘッドアップディスプレイ上に投影された地図上に表示された位置などの仮想オブジェクトであってもよく、または照明を制御するノブのような物理的なオブジェクトであってもよい。視覚信号を用いてディスプレイの作動を行うための代替的な実施形態において、運転者は、選択可能なオブジェクトを見て特定した後、視線を運転進路に戻すことができる。この時に、運転者は、直前に見たオブジェクトに関連する機能を作動することができる。音声コマンド（すなわち、１つ以上のキーワードの音声認識）、スイッチ（例えば、ハンドルに配置されたスイッチ）、点頭、または類似の指示を含む多くの方法で、作動を行うことができる。異なる運転者は、異なる作動モードを好むことがある。異なる時間に異なる作動モードを使用してもよい。 While driving, the driver may move away from the driving path for a short period of time to a selectable object. The selectable object may be a virtual object, such as a position displayed on a map projected onto the console or heads-up display inside the vehicle, or a physical object, such as a knob that controls lighting. It may be. In an alternative embodiment for operating a display using visual signals, the driver can look at and identify selectable objects and then return his gaze back to the driving path. At this time, the driver can activate the function related to the object seen immediately before. The operation can be performed in many ways, including voice commands (ie, voice recognition of one or more keywords), switches (eg, switches located on the steering wheel), punctuation, or similar instructions. Different drivers may prefer different modes of operation. Different operating modes may be used at different times.
異なる作動モード（１つ以上の作動位置に視線を移動する衝動性動眼、ボタンの手動押下、音声など）を使用し、異なる制御を作動することができる。運転者による制御の例は、（照明つまみを見ることによって）照明の調整、（窓のトグル制御を見ることによって）窓の昇降、（ラジオを見ることによって）ラジオ局の選択、（音声プレーヤーを見ることによって）プレイリスト内の歌またはポッドキャストの繰り上げ、（音量制御ノブを見ることによって）音量および他の音声特性の調整、（適切なノブおよび／または指示灯を見ることによって）空気流の方向および強さの調整、（指示灯を見ることによって）局部温度の制御、（ミラー位置制御ノブを見ることによって）ミラー位置の調整、（適切なボタンを見ることによって）座席制御の変更、（ワイパー制御を見ることによって）ワイパー速度の調整などを含むことができる。殆どの場合、運転者は、視線制御および（従来の）手動制御の両方を利用することができる。 Different controls can be activated using different modes of operation (impulsive oculomotor eye that moves the line of sight to one or more operating positions, manual button presses, voice, etc.). Examples of driver control include adjusting the lights (by looking at the wipes), raising and lowering windows (by looking at the window toggle control), selecting a radio station (by looking at the radio), and (voice player). Advance a song or podcast in a playlist (by looking), adjust volume and other audio characteristics (by looking at the volume control knob), direction of airflow (by looking at the appropriate knob and / or indicator light) And strength adjustment, local temperature control (by looking at the indicator light), mirror position adjustment (by looking at the mirror position control knob), seat control change (by looking at the appropriate button), wiper It can include adjusting the wiper speed (by looking at the controls) and so on. In most cases, the driver can utilize both line-of-sight control and (conventional) manual control.
視線を前方の道路に戻すように運転者を促すことは、非運転機能に関連する時間を短縮することによって、安全運転を促進する。運転者の短期間記憶または作業視覚記憶内の時間に作動を行うことによって、車の運転に直接関連しない作業の認知負荷をさらに低減することができる。地図上の目標位置の位置を発見および特定することなどの複雑操作は、地図に対する一連の短時間注視を行い、各々の注視の後、視線を運転進路に戻してから、作動を行うことによって、行うことができる。このような一連の短時間注視は、運転安全にとって最適と考えられる。さらに、運転者がディスプレイを直接見ていない間に、ディスプレイに対する変化（例えば、地図上で行われたパンまたはズーム機能）を作動することによってディスプレイを変更すると、動的に変化するディスプレイを見ることに関連する視覚処理をさらに低減することができる。 Encouraging the driver to return his gaze to the road ahead promotes safe driving by reducing the time associated with non-driving features. By operating at a time within the driver's short-term memory or work visual memory, the cognitive load of work not directly related to driving the vehicle can be further reduced. Complex operations such as finding and identifying the position of the target position on the map are performed by performing a series of short-time gazes on the map, returning the line of sight to the driving course after each gaze, and then performing the operation. It can be carried out. Such a series of short-time gaze is considered to be optimal for driving safety. In addition, changing the display by activating changes to the display (eg, pan or zoom features made on the map) while the driver is not looking directly at the display will see a dynamically changing display. The visual processing associated with can be further reduced.
異なるディスプレイおよび対話式装置が利用可能な場合、異なる方法を運転者に提供することができる。例えば、ＡＲディスプレイおよびコンソールディスプレイの両方が利用可能な場合、ＡＲディスプレイに制限量の情報を短期間で表示することができる。しかしながら、ナビゲーションなどのより複雑な操作を行う場合、視覚信号に支援されているコンソール表示を使用した方が安全である。ＡＲ装置とコンソールを制御するプロセッサとの間に、直接通信を行うことができる。代替的または追加的に、ＡＲ装置上のシーンカメラを用いて、コンソール上に表示された選択肢を見ることができ、視覚信号によるユーザの選択を支援することができる。 Different methods can be provided to the driver when different displays and interactive devices are available. For example, if both an AR display and a console display are available, the AR display can display a limited amount of information in a short period of time. However, when performing more complex operations such as navigation, it is safer to use a console display that is assisted by visual signals. Direct communication can be performed between the AR device and the processor that controls the console. Alternatively or additionally, a scene camera on the AR device can be used to see the choices displayed on the console and assist the user's choice by visual signal.
運転者正面のつまみを見ることできると、一貫した安全運転を促進する。記憶誘導型頭部および眼球運動によって、最小限の運転経験で、（大きい角度許容度で）視線を車輛内の既知の位置に誘導する。対照的に、大量の情報を小さなヘッドアップディスプレイなどの小さな空間に投影すると、限られた空間に特定の制御を位置決め且つ特定することがより困難になる。異なる時間に異なる情報をディスプレイに投影する場合、この状況は、さらに悪化する。運転者は、所望の表示メニューと対話するために、待機する必要または他のルックアップ操作を行う必要がある。 Being able to see the knobs in front of the driver promotes consistent and safe driving. Memory-guided head and eye movements guide the line of sight (with large angular tolerance) to known locations within the vehicle with minimal driving experience. In contrast, projecting large amounts of information onto a small space, such as a small heads-up display, makes it more difficult to position and identify a particular control in a limited space. This situation is exacerbated when different information is projected onto the display at different times. The driver needs to wait or perform other lookup operations to interact with the desired display menu.
特定されたドキュメントおよび装置ユーザの関連付け
虹彩認証および他の手段に基づいて装置着用者を特定する方法は、２０１５年５月９日に出願され、「視覚信号の特定および連続生体認証を行うためのシステムおよび方法」と題された特許出願第１４／７０８２４１号に記載されている。当該出願の全体は、参照により本明細書に組み込まれる。多くのセキュアアプリケーションは、文書、フォーム、契約または他のデータセットと、特定される装置着用者との関連付けを要求する。これらのアプリケーションは、法律、財務、ライセンス、セキュリティ、告知同意、医療などの幅広い分野を含む。
Association of Identified Documents and Device Users A method of identifying device wearers based on iris recognition and other means was filed May 9, 2015, "for identifying visual signals and performing continuous biometrics. It is described in Patent Application No. 14/708241 entitled "Systems and Methods". The entire application is incorporated herein by reference. Many secure applications require the association of documents, forms, contracts or other datasets with the identified device wearer. These applications include a wide range of areas such as law, finance, licensing, security, announcement consent, and healthcare.
例示的な実施形態において、装置着用者は、実際の（すなわち、物理的な）または仮想の（すなわち、ディスプレイ上で見た）ドキュメントを見ることができ、装置着用者の環境に向く１つ以上のシーンカメラを用いて、ドキュメントを認証することができる。ドキュメントの内容、埋め込まれたＱＲコード、埋め込まれたバーコード、署名の認識、または他のセキュリティ機能の位置および認証に基づいて、ドキュメントを認証することができる。署名の認識は、画像認識技術および／または神経ネットワークの深層学習による分類を含む機械学習技術を用いて、行うことができる。 In an exemplary embodiment, the device wearer can see real (ie, physical) or virtual (ie, as seen on the display) documents, one or more suitable for the device wearer's environment. Documents can be authenticated using the scene camera of. Documents can be authenticated based on the content of the document, embedded QR code, embedded barcode, signature recognition, or the location and authentication of other security features. Signature recognition can be performed using image recognition techniques and / or machine learning techniques including deep learning classification of neural networks.
ドキュメントを認証した後、装置着用者によって閲覧され、虹彩認識（例えば、虹彩コード）および／または他の手段に基づいて認証されたものとして、ドキュメントに電子タグを付けることができる。必要に応じて、視覚信号言語またはキーボードの使用または音声認識などの他の方法を用いて、ドキュメントの電子表現を変更することができる。変更は、確認された装置着用者のＩＤに基づいた「署名」を挿入することを含むことができる。署名は、一般的にドキュメントに関連する手書き形式（例えば、重合画像）であってもよく、または各人に一意に関連する「電子署名」（すなわち、数値コード）であってもよい。追加的にまたは代替的に、バーコードまたはＱＲコードをドキュメントに添付することによって、認証されたユーザがこのドキュメントを閲覧したことを示すことができる。 After authenticating the document, the document can be electronically tagged as being viewed by the device wearer and authenticated based on iris recognition (eg, iris code) and / or other means. If desired, the electronic representation of the document can be modified using a visual signal language or other methods such as keyboard use or speech recognition. The modification can include inserting a "signature" based on the identified device wearer's ID. The signature may be in a handwritten form generally associated with the document (eg, a polymerized image), or may be an "electronic signature" (ie, a numeric code) that is uniquely associated with each person. Additional or alternative, by attaching a barcode or QR code to the document, it can be indicated that an authenticated user has viewed this document.
最後に、署名は、ユーザの視線データの一部または全体を記述するメタデータを含むことができる。ユーザの視線データは、ユーザがドキュメントを視覚的に閲覧することを示す視覚信号、タイムスタンプ付き注視時間、注視シーケンスマッピング、瞳孔拡大、および発生する他の生体信号のうち、１つ以上を含み得る。このような情報は、告知同意の確認に有用であり、情報の閲覧を検証するだけでなく、契約の理解または合意を示す示唆を提供するための方法にも有用である。 Finally, the signature can include metadata that describes some or all of the user's line-of-sight data. The user's gaze data may include one or more of a visual signal indicating that the user visually browses the document, a time stamped gaze time, gaze sequence mapping, pupil enlargement, and other biological signals generated. .. Such information is useful for confirming announcing consent and as a method not only for verifying the viewing of the information, but also for providing suggestions for understanding or agreeing to the contract.
このような「署名」および／または変更されたドキュメントは、ドキュメントの物理的交換に相当する方法で交換することができる。（例えば、ＧＰＳから決定された）署名を行った時間、日付および位置は、ドキュメントに埋め込まれてもよい。 Such "signature" and / or modified documents can be exchanged in a manner equivalent to the physical exchange of documents. The time, date and location of the signature (as determined from GPS, for example) may be embedded in the document.
仮想ディスプレイ「壁」
仮想現実ディスプレイ、拡張現実感ディスプレイ、複合現実ディスプレイ、および他のヘッドマウントディスプレイシステムは、仮想ディスプレイ「壁」または拡張デスクトップを投影するメカニズムを提供する。これらの壁または有効面は、大きな仮想表示領域を含み、「壁」の１つの領域は、装置着用者がいつでも見ることができるように投影される。しかしながら、装置ユーザを完全に囲む可能性のある仮想ディスプレイの全ての領域は、表示および対話のためにプログラムに従って維持される。仮想壁または仮想面は、任意のサイズ、装置着用者の実感距離、形状、または曲率を有してもよい。アプリケーションは、改良されたワークステーション環境、対話型表示および娯楽を含む。
Virtual display "wall"
Virtual reality displays, augmented reality displays, mixed reality displays, and other head-mounted display systems provide a mechanism for projecting virtual display "walls" or augmented desktops. These walls or effective surfaces include a large virtual display area, and one area of the "wall" is projected so that the device wearer can see it at any time. However, all areas of the virtual display that may completely surround the device user are programmatically maintained for display and interaction. The virtual wall or surface may have any size, actual distance, shape, or curvature of the device wearer. Applications include an improved workstation environment, interactive display and entertainment.
仮想ディスプレイ壁は、視線交流または視覚信号言語に理想的である。ユーザは、好み設定または特定の応用に基づいて、異なる対話モードを選択することができる。装置着用者の視野に投影された領域は、頭部動作、装置着用者の視線方向および／または視覚信号コマンドに基づいて制御することができる。 Virtual display walls are ideal for line-of-sight exchange or visual signal languages. The user can select different dialogue modes based on preference settings or specific applications. The area projected into the device wearer's field of view can be controlled based on head movements, device wearer's line-of-sight direction and / or visual signal commands.
図２７は、装置ユーザ６１１と典型的な仮想壁またはディスプレイ６１０との対話を示している。装置ユーザ６１１の（破線で示された）視野６１２内で、仮想壁６１０の一部の領域６１３は、閲覧のために、表示装置（例えば、拡張現実ヘッドセットまたは仮想現実ヘッドセット）に投影される。領域６１３は、任意のサイズで仮想表示領域６１０内の任意の位置に投影されてもよい。場合によって、領域６１３のサイズは、特定の用途に応じて自動的に変更されてもよい。
FIG. 27 shows the interaction between the
当該技術分野（例えば、Oculus VR、USAによって製造された装置）において、仮想壁６１０の一部の可視領域６１３の制御は、一般的に頭部動作に基づいて行われる。例えば、頭部動作によって、仮想環境内の静止オブジェクトが静止に見えるように、表示された画像を頭部動作の方向に沿って円弧移動するようにスクロールする。しかしながら、例示的な実施形態において、眼球運動の動力学をさらに考慮することによって、可視領域の制御を改良することができる。例えば、頭が動いている間に、目がある位置を見続ける前庭性眼球運動が検出されると、可視領域を静止に維持することができる。これによって、関心のあるオブジェクトが存在する時に、装置ユーザに対して過度のディスプレイ移動を回避することができる。
In the art (eg, Oculus VR, a device manufactured by USA), control of a portion of the
一方、非前庭性眼球運動に関連する頭部動作がある例示的な実施形態において、装置ユーザは、一般的に関心のあるオブジェクトを意図的に変えている。この場合、シーンの変更が予想される。シーンを移動するための非常に愉快で「自然な」方法の１つは、（前述した）衝動性動眼抑制の期間中に表示の変更を行うことである。このようにして、装置ユーザが機能的に失明であるときにシーンが変更されるため、シーンの変更に関連する目眩しを軽減することができる。瞬き抑制の主な生理的目的が常に感知される変更を低減することであるため、衝動性動眼抑制の期間中に行われるシーンの変更は、「生物誘発」技術に分類され得る。 On the other hand, in an exemplary embodiment where there are head movements associated with non-vestibular eye movements, the device user is deliberately altering an object of general interest. In this case, the scene is expected to change. One of the most entertaining and "natural" ways to move the scene is to make display changes during the period of impulsive oculomotor eye suppression (discussed above). In this way, the scene is changed when the device user is functionally blind, so that dizziness associated with the scene change can be reduced. Scene changes made during the period of impulsive oculomotor eye suppression can be classified as "biologically induced" techniques, as the main physiological purpose of blink suppression is to reduce constantly perceived changes.
視覚信号言語において、ディスプレイの視線選択可能な領域をどのように配置するか、およびどのように表示するかを選択することができる。第１例として、「スクリーンロック」と呼ばれ、選択可能な領域は、仮想表示領域６１０内の固定位置に配置される。便宜上、特定の選択可能な領域は、様々な位置に重複に配置されてもよい。図２７に示すように、３つの同様な作動ターゲット６１６ａ、６１６ｂおよび６１６ｃは、仮想表示領域６１０の上方に沿って重複に配置される。場合によって、選択可能な領域は、仮想ディスプレイの単一領域に配置されてもよい。この場合、選択可能な領域は、状況によって、当該領域内の「スクリーンロック」アイコンまたは他の記号として、１つの位置のみで利用可能である。
In the visual signal language, it is possible to select how to arrange the line-of-sight selectable area of the display and how to display it. As a first example, called a "screen lock", the selectable area is located at a fixed position within the
視覚信号言語の別の実施形態において、視線選択可能な領域（例えば、作動ターゲット）は、装置着用者の視野内の固定位置に配置されてもよい。この方法は、「ヘッドロック」と呼ばれる。ユーザの視線を選択可能な領域に向かって誘導するアイコンまたは他の記号は、仮想ディスプレイ上に重合され、視認可能な領域６１３が仮想表示領域６１０内で（例えば、ユーザの頭部動作によって）移動される時に移動する。
In another embodiment of the visual signal language, the line-of-sight selectable region (eg, actuation target) may be located at a fixed position in the field of view of the wearer of the device. This method is called "headlock". An icon or other symbol that directs the user's line of sight towards a selectable area is superimposed on the virtual display so that the
上述したように、頭部により制御されたオブジェクトの位置は、必要に応じて、前庭性眼球運動ではないと判断された頭部動作のみによって制御されてもよい。換言すれば、視覚信号を行う期間中に、特定のオブジェクトを見ながら頭を動かす（すなわち、前庭性眼球運動を生成する）ことによって、投影／表示されたオブジェクトの位置を動かすことなく、頭を動かすことが可能である。 As described above, the position of the object controlled by the head may be controlled only by the head movement determined not to be a vestibular eye movement, if necessary. In other words, by moving the head while looking at a particular object (ie, generating vestibular eye movements) during the period of visual signaling, the head is moved without moving the position of the projected / displayed object. It is possible to move it.
追加の実施形態において、視線による対話を行うために、「スクリーンロック」方法および「ヘッドロック」方法の両方を同時に使用してもよい。例えば、仮想ディスプレイ６１０のコンテンツに関連する選択可能な領域（例えば、テキストの編集に関連する選択）は、「スクリーンロック」モードでテキストコンテンツの近くに配置されてもよい。同時に、より一般的なナビゲーション機能および（例えば、新しいアプリケーションを起動するための）制御機能に関連する選択可能な領域は、「ヘッドロック」モードで常に利用可能に配置されてもよい。これによって、より一般的なナビゲーション機能をより容易に見つけることができる（すなわち、記憶誘導型衝動性動眼を促進する）。 In additional embodiments, both the "screen lock" method and the "head lock" method may be used simultaneously for eye-gaze dialogue. For example, selectable areas related to the content of the virtual display 610 (eg, selections related to editing text) may be placed close to the text content in "screen lock" mode. At the same time, selectable areas related to more general navigation functions and control functions (eg, for launching new applications) may be placed always available in "headlock" mode. This makes it easier to find more general navigation functions (ie, promotes memory-guided impulsive oculomotor eye).
「スクリーンロック」モードおよび「ヘッドロック」モードの両方に関連するアイコンまたは他の記号の表示は、アイコンおよび／または記号の透明度を制御することによって行うことができる。淡いアイコンは、余分な乱雑なく、ディスプレイ上のコンテンツを閲覧することを可能にする。例えば、前回の作動から時間の増加および／または装置ユーザの経験レベルなどの状況によって保証された場合、不透明度を増加する（すなわち、透明度を減少する）ことができる。 The display of icons or other symbols associated with both "screen lock" and "head lock" modes can be achieved by controlling the transparency of the icons and / or symbols. The faint icon allows you to browse the content on the display without extra clutter. For example, opacity can be increased (ie, transparency is reduced) if guaranteed by conditions such as increased time since the last operation and / or the level of experience of the device user.
キーボード入力の改善
ヘッドセット装置（例えば、仮想現実ディスプレイ、拡張現実ディスプレイまたはヘッドアップディスプレイ）と共にキーボードを使用する場合、ヘッドセット装置および／または任意の付属品は、キーボードに対するビューを物理的に制限または妨げる可能性がある。例えば、仮想現実装置は、現実世界に対する全てのビューを完全にブロックする。このことは、一般的にキーボードの使用を妨げる。その理由は、多くのユーザが、入力する時にまたは機能に関連するキーを押す時に、両手を見る必要がある。所謂「ブラインドタッチ」できる人でさえ、タイピングを行う前に、手を（キーボードの）適切な位置に置いているか否かを一般的に目で確認することがある。
Improved keyboard input When using a keyboard with a headset device (eg, a virtual reality display, augmented reality display or heads-up display), the headset device and / or any accessory physically limits the view to the keyboard or May interfere. For example, a virtual reality device completely blocks all views of the real world. This generally hinders the use of keyboards. The reason is that many users need to look at both hands when typing or pressing key related to a function. Even those who can do so-called "blind touch" may generally visually check that their hands are in the proper position (on the keyboard) before typing.
例示的な実施形態において、装着可能なヘッドセットおよび（装着可能または遠隔の）ディスプレイは、カメラを用いてキーボードの領域を表示し、（指およびキー位置を含む）カメラ画像をディスプレイの可視領域に重ねることによって、キーボードおよび手の視認妨害を克服することができる。このようにして、下方（または他の方向）の実在キーボードを見る必要がなく、キーボードに対する指の位置を確認することができる。キーボードを視認するためのカメラは、任意種類の頭部装着装置、頸部または胸部、支柱などの支持体（例えば、身体以外もの）に取り付けられてもよく、またはディスプレイ画面の頂端から下方に見る装置として取り付けられてもよい。 In an exemplary embodiment, a wearable headset and a (wearable or remote) display use a camera to display an area of the keyboard and bring the camera image (including finger and key positions) into the visible area of the display. By stacking, it is possible to overcome the visual obstruction of the keyboard and hands. In this way, you can see the position of your finger relative to the keyboard without having to look down (or in any other direction) the real keyboard. The camera for viewing the keyboard may be attached to any type of head-mounted device, a support such as the neck or chest, struts (eg, something other than the body), or viewed downwards from the top of the display screen. It may be attached as a device.
指およびキーの位置の重合表示は、視覚信号と併用されると、特に強力である。視覚信号言語は、眼球運動によってカバーされた距離および意図の伝達に必要とされた時間の点から「効率的な」眼球運動を利用するように構築されている。下方の物理キーボードに向く視線は、言語要素から離れるため、多くの時間を費やし、（キーボードに向かった長い距離の衝動性動眼によって）目の疲れを引き起こす。さらに、ユーザの視線をディスプレイ内の領域に戻すプロセスは、一般的に、ディスプレイ上の所望の注視位置に戻るために、長い距離の衝動性動眼、１つ以上の矯正的な衝動性動眼、およびしばしば捜索的な衝動性動眼を必要とする。 Superposed display of finger and key positions is particularly powerful when used in combination with visual signals. Visual signal languages are constructed to take advantage of "efficient" eye movements in terms of the distance covered by eye movements and the time required to convey intent. The line of sight towards the physical keyboard below spends a lot of time moving away from the language element, causing eye strain (due to the long-distance impulsive eye movement towards the keyboard). In addition, the process of returning the user's line of sight to an area within the display generally involves a long-distance impulsive eye, one or more corrective impulsive eyes, and one or more corrective impulsive eyes to return to the desired gaze position on the display. Often requires a searchable impulsive oculomotor eye.
ユーザの手およびキーボードの画像をリアルタイムに投影することによって、キーボード入力中に手を見ることに伴う目（および頭）の動作を大幅に減らすことができる。殆どの場合、手およびキーボードの仮想画像または投影画像は、作業を行っている間に、周辺視界、傍中心窩視界または中心窩視界に位置するように配置されてもよい。これによって、装置ユーザは、ディスプレイの空間的基準および／または他の情報を失うことなく、ディスプレイ上の１つ以上の関心領域と、キーボード入力中に指およびキーの画像との表示を交互に行うことができる。 By projecting images of the user's hand and keyboard in real time, eye (and head) movements associated with looking at the hand during keyboard input can be significantly reduced. In most cases, the virtual or projected image of the hand and keyboard may be arranged to be located in the peripheral, parafoveal or foveal field of view while performing the work. This allows the device user to alternate between displaying one or more areas of interest on the display with images of fingers and keys during keyboard input without losing the spatial reference and / or other information on the display. be able to.
図２８は、頭部装着装置６５０を示している。頭部装着装置６５０は、キーボードを視認するためのカメラ６５１を含み、キーボードカメラ６５１は、キーボード６５２および装置着用者の手６５３ａ、６５３ｂがキーボードカメラ６５１の視野６５４内にあるように配向される。いくつかの実施形態において、装置ユーザは、キーボード６５２および手６５３ａ、６５３ｂの位置を見えない場合がある。
FIG. 28 shows the head-mounted
図２９は、キーボードカメラ６５１によって記録されたキーボード６６２および装置ユーザの手６６３ａ、６６３ｂをディスプレイ６６０上でリアルタイムで見るように、ディスプレイ６６０に投影した、図２８と同様の頭部装着装置６５０を示している。ディスプレイ６６０は、装置着用者の（破線で表された）視野６６１ａ、６６１ｂに位置する限り、頭部装着装置６５０の一部（例えば、拡張現実ディスプレイ、仮想現実ディスプレイまたはヘッドアップディスプレイ）であってもよく、または遠隔に位置するもの（例えば、タブレットモニタ、スマート装置モニタ、ラップトップモニタ、コンピュータモニタ）であってもよい。
FIG. 29 shows a head-mounted
図２９に示す例示的な作業は、装置ユーザによる入力を含む。文字および単語は、位置６６５でテキスト本体６６４に付加される。必要に応じて、ユーザは、下方に投影されたキーボード６６２および手６６３ａ、６６３ｂの画像を見ることによって、入力中の相対位置を決定することができる。これによって、実在キーボード６５２および手６５３ａ、６５３ｂを見ることに比べて、眼球運動が少なくなり、関連する認知処理も少なくなる。
The exemplary work shown in FIG. 29 includes input by the device user. Letters and words are added to the
投影されたキーボード６６２および手６６３ａ、６６３ｂの画像は、装置ユーザに適合するように、ディスプレイ６６０の任意位置に配置されてもよい。また、投影された画像は、任意のサイズまたは任意の透明度（すなわち、他の表示項目を完全に遮蔽する程不透明から、重ね合わられる場合画像が殆ど見えない程透明）に作られてもよい。（眼球運動を最小限に抑えるために）タイピングまたは他の視覚活動を行っている領域に、画像を半透明に投影してもよい。透明度は、ユーザによって制御されてもよく、自動的に調整されてもよい。例えば、入力ミスがあった場合に、投影画像の透明度を低くする（すなわち、よく見えるにする）ことができる。
The projected images of the
他の例示的な実施形態において、拡張モードで、物理キーボードおよび／または投影されたキーボードの使用を最適化することができる。物理キーボードは、直接に見ることができないときに手および指の位置決めを支援するように、（触感によって特定可能な）機械的なガイドまたは基準位置を含むことができる。触覚によって特定可能な１つ以上のガイドポストまたはガイド面および基準面は、キーボードの任意位置に配置されてもよい。 In other exemplary embodiments, the use of the physical keyboard and / or the projected keyboard can be optimized in extended mode. The physical keyboard can include mechanical guides (identifiable by tactile sensation) or reference positions to assist in positioning the hands and fingers when they are not directly visible. One or more guideposts or guide planes and reference planes that are tactilely identifiable may be located at any position on the keyboard.
実際に任意のキーボード配置に対応できるように、投影されたキーボードを開発することができる。投影されたキーの表面は、当技術分野で周知の画像認識技術を用いて、ビデオ画像から認識することができる。こられの画像において、任意の記号または文字を投影されたキーの表面に重合することができる。このようにして、ユーザが完全に任意のキーボードを選択することができる。これによって、様々な言語に使用されている文字または記号、身体障害または特定の応用（例：主にスプレッドシートに数値を入力すること）に適応することができ、および／または視覚信号言語を含む様々な設定における性能を最適化するように配置をカスタマイズすることができる。 Projected keyboards can be developed to accommodate any keyboard layout in practice. The surface of the projected key can be recognized from the video image using image recognition techniques well known in the art. In these images, any symbol or letter can be superimposed on the surface of the projected key. In this way, the user can select a completely arbitrary keyboard. This allows it to be adapted to letters or symbols used in various languages, disabilities or specific applications (eg, entering numbers primarily in spreadsheets) and / or includes visual signal languages. The placement can be customized to optimize performance in various settings.
後者の特に有用な例は、キーボードを用いて、眼球運動が「意図的な」ものであるか否かを特定する能力を含む。親指の押下などの急速動きは、視覚信号を意図的なものとして解釈することを迅速に示すことができ、結果としての作動を行うことができる。このようなキーボード「ボタン」は、コンピュータマウスのボタン押しまたはタッチ感応タッチパッドの押しと同様の方法で使用することができる。このようにして、キーボードから手を離すことなく、小さな手の動きおよび急速眼球運動で、カーソルおよび／または他のポインタの動きを制御することができる。 A particularly useful example of the latter includes the ability to use a keyboard to identify whether eye movements are "intentional" or not. Rapid movements, such as the pressing of the thumb, can quickly indicate that the visual signal is interpreted as intentional, and the resulting action can be performed. Such keyboard "buttons" can be used in a manner similar to pressing a computer mouse button or pressing a touch-sensitive touchpad. In this way, small hand movements and rapid eye movements can control the movement of the cursor and / or other pointers without taking the keyboard off.
代替的または追加的に、キーボードは、埋め込まれたタッチ感応面、方向ボタン、小型ジョイスティックまたは押し棒、小型トラックボールまたは類似の機構を使用する位置制御を含むことができる。これらは、視覚信号を用いた位置制御に比べてより精確な位置制御が必要な状況に使用され得る。コンピュータマウス、遠隔タッチパッドまたはジョイスティックの位置を特定し、その後それらを使用する必要性を排除して、手を比較的静止状態に保つことによって、殆ど種類のキーボード入力をさらに強化する（すなわち、入力速度を増加させるおよび／または認知負荷および関連する疲労度を減少させる）。 Alternatively or additionally, the keyboard can include position control using an embedded touch sensitive surface, direction buttons, a small joystick or push bar, a small trackball or similar mechanism. These can be used in situations where more precise position control is required compared to position control using visual signals. By locating the computer mouse, remote touchpad or joystick and then eliminating the need to use them and keeping the hands relatively stationary, most types of keyboard input are further enhanced (ie, input). Increase speed and / or reduce cognitive load and associated fatigue).
代替または追加の実施形態において、様々な技術を用いて、（通常、手の位置）特に指の位置を感知することによって、上述したカメラ画像を補うことができる。指とキーの近接度は、当技術分野に知られている近接感知技術を用いて推定することができる。例えば、いくつかの近接感知技術は、大量の水分を含む指の誘電特性と、空気（すなわち、指が存在しない場合）の誘電特性との差に基づく。したがって、（誘電特性に依存する）キーの近傍の静電容量を測定することによって、指の有無を判断することができる。他の技術は、指が存在するときに指から反射される光の有無を測定することを含む。 In alternative or additional embodiments, various techniques can be used to supplement the camera image described above by sensing (usually the position of the hand), especially the position of the fingers. The proximity of the finger to the key can be estimated using proximity sensing techniques known in the art. For example, some proximity sensing techniques are based on the difference between the dielectric properties of a finger containing a large amount of water and the dielectric properties of air (ie, in the absence of the finger). Therefore, the presence or absence of a finger can be determined by measuring the capacitance in the vicinity of the key (depending on the dielectric property). Other techniques include measuring the presence or absence of light reflected from a finger when it is present.
特定のキーの近くに指の有無を示す表示は、上記のキーボードのカメラ画像上または別の仮想キーボード上に示されてもよい。指の有無を示す表示は、小さなライトの画像、キーの表面上の文字または記号の変化、重合された仮想指の輪郭または画像、または各キー上に指の存在を示す他の表現を含むことができる。 The indication of the presence or absence of a finger near a particular key may be shown on the camera image of the above keyboard or on another virtual keyboard. The finger presence / absence indication should include an image of a small light, a change in letters or symbols on the surface of the key, a superimposed virtual finger contour or image, or other representation of the presence of the finger on each key. Can be done.
更なる実施形態において、以下に基づいて、指の存在をさらに区別することができる。１）上述した近接感応技術を用いて、指の近接度を感知することができる。２）タッチ感応技術を用いて、指とキーとの接触を判断することができる。３）キーの押しボタンスイッチ操作によって、キーの実際の押圧を測定することができる。 In a further embodiment, the presence of a finger can be further distinguished based on: 1) The proximity of the finger can be sensed by using the proximity sensitivity technique described above. 2) Touch-sensitive technology can be used to determine the contact between a finger and a key. 3) Key push button The actual push of the key can be measured by operating the switch.
これらの差別の各々は、上述した仮想キーボードにさらに示されてもよく、または装置着用者に使用されている実在キーボードのカメラ画像に重ねられてもよい。また、これらの差別を用いて、キーボードの入力を変更することもできる。例えば、単にキーにタッチすることによって、小文字の文字を入力することができる。一方、キーを完全に押下することによって、大文字および／または他の特殊記号を入力することができる。これによって、複数のキー入力または代替のキーボード配置を避けることができ、および／または異なる順列の入力を生成するために手および手首を動かす必要性を低減することができる。一般的に、キーの下向打ちを除いて、指は、静止のままである。 Each of these discriminations may be further shown on the virtual keyboard described above, or superimposed on the camera image of the real keyboard used by the device wearer. You can also use these discriminations to change keyboard input. For example, you can enter lowercase letters by simply touching a key. On the other hand, you can enter uppercase letters and / or other special symbols by pressing the key completely. This avoids multiple keystrokes or alternative keyboard layouts and / or reduces the need to move the hands and wrists to generate different permutation inputs. In general, the finger remains stationary, except for the downward striking of the key.
これらの方法を組み合わせるように設計することによって、データ入力中の眼球運動および指の動作の両方を最小限に抑え、人間-機械インターフェイス（ＨＭＩ）の速度を高めることができ、および／または疲労度につながる認知負荷を軽減することができる。したがって、これらの方法は、指および目の両方の動きを最適化することができる。 By designing to combine these methods, both eye movements and finger movements during data entry can be minimized, human-machine interface (HMI) speed can be increased, and / or fatigue. It is possible to reduce the cognitive load that leads to. Therefore, these methods can optimize the movement of both fingers and eyes.
乱れのない衝動性動眼による視線入力
例示的な実施形態において、「視線入力」とは、（ユーザの片目または両目で）キーボードまたは他の選択マトリクスを見ることによって、文字および／または他の記号を入力することを意味する。キーボードまたは選択マトリクスは、実在（すなわち、物理的なオブジェクト）または仮想（すなわち、ディスプレイの一部として投影されるオブジェクト）であってもよい。視線入力の基本は、認知負荷を最小限に抑えながら、意図的な眼球運動を用いて、ユーザの意図をできるだけ迅速に伝えることができる文字および他の記号の入力である。
Line-of-sight input by undisturbed impulsive occlusal eye In an exemplary embodiment, "line-of-sight input" refers to characters and / or other symbols by looking at the keyboard or other selection matrix (with one or both eyes of the user). Means to enter. The keyboard or selection matrix can be real (ie, physical objects) or virtual (ie, objects projected as part of the display). The basis of gaze input is the input of letters and other symbols that can convey the user's intentions as quickly as possible using intentional eye movements while minimizing cognitive load.
例えば、頭部装着装置に取り付けられ、片目または両目に向けられた１つ以上の検出装置を用いて、眼球運動を監視することができる。頭部装着装置は、拡張現実ディスプレイ、複合現実ディスプレイ、または仮想現実ディスプレイを含むことができる。代替的にまたは追加的に、携帯電話、タブレット、ラップトップコンピュータまたは類似の装置などのモバイル装置に設けられ、片目または両目に向けられた１つ以上の検出装置を用いて、眼球運動を遠隔に監視することができる。また、中央処理装置、壁掛けディスプレイ、読み専用ディスプレイ、サイネージまたは類似の装置と通信するデスクトップコンピュータまたは周辺装置（所謂「周辺機器」）に設けられ、片目または両目に向けられた検出装置を用いて、眼球運動を監視することもできる。 For example, eye movements can be monitored using one or more detectors attached to a head-worn device and directed to one or both eyes. The head-worn device can include an augmented reality display, a mixed reality display, or a virtual reality display. Alternatively or additionally, remote eye movements are performed using one or more detectors, which are provided on mobile devices such as mobile phones, tablets, laptop computers or similar devices and are directed to one or both eyes. Can be monitored. Also, using a detection device provided on a central processing unit, wall-mounted display, read-only display, desktop computer or peripheral device (so-called "peripheral device") that communicates with signage or similar devices and directed to one or both eyes. Eye movements can also be monitored.
多くの市販視線入力システムの一特徴は、視線停留（すなわち、１つ以上の眼が所定の時間を超えて注視し続けること）、目の瞬き、または顔面筋肉の動きを用いて、文字または記号の選択を示す。選択を示すことは、特に視線を特定の文字または記号の位置に向ける急速な衝動性動眼に比べて、かなりの時間を必要とする。このような選択を示すことを無くすと、視線入力の速度（すなわち、単位時間に入力された文字、記号またはコンテンツの数）を実質的に増加することができる。 One feature of many commercial gaze input systems is the use of gaze retention (ie, one or more eyes continue to gaze for more than a given period of time), eye blinking, or facial muscle movements to make letters or symbols. Indicates the selection of. Showing choice requires a considerable amount of time, especially compared to the rapid impulsive oculomotor eye, which directs the line of sight to the position of a particular letter or symbol. Eliminating such choices can substantially increase the speed of gaze input (ie, the number of characters, symbols or content entered per unit time).
本明細書の例示的な実施形態の一態様は、ユーザが視線の停留、目の瞬きまたは非眼筋肉動きによって一般的に中断されない一連の衝動性動眼を生成することである。ユーザは、実質的に遅延せず、一連の衝動性動眼を介して、選択される一文字から次の文字に視線を容易に移動する。システムは、眼球運動の１つ以上のビデオストリームに基づいて、中断のない逐次的な方法で、一文字から次の文字までの意図的な衝動性動眼を認識する。 One aspect of an exemplary embodiment of the present specification is for the user to generate a series of impulsive oculomotor eyes that are generally uninterrupted by gaze retention, eye blinking or non-eye muscle movements. The user easily moves his or her line of sight from one selected character to the next through a series of impulsive oculomotor eyes with virtually no delay. The system recognizes the deliberate impulsive oculomotor eye from one letter to the next in an uninterrupted, sequential manner based on one or more video streams of eye movements.
本明細書の実施形態の別の態様は、視線入力を行っている時に、一般的に視覚フィードバックを表示しないことである。フィードバックを表示する場合（例えば、選択された文字を強調表示すること、単語または文章内に新たに入力された文字を表示すること、キーボードの配置を変更すること）、フィードバックを表示することによって、タイピストが一般的にフィードバックを「認識」しなければならない。このような視覚的な認識は、最小限の時間（１つの文字または記号を完全に認識するのに２００〜２５０ミリ秒）を必要とし、入力の最大速度を制限する。実際、フィードバックの認識は、（少なくとも１つの矯正的な衝動性動眼がさらに関与しても）殆どの衝動性動眼よりも一般的に長い時間を必要とする。 Another aspect of the embodiments herein is that visual feedback is generally not displayed when performing line-of-sight input. If you want to display feedback (for example, highlighting selected characters, displaying newly typed characters in a word or sentence, changing the keyboard layout), by displaying feedback Typists generally have to "recognize" feedback. Such visual recognition requires a minimum amount of time (200-250 milliseconds to fully recognize a character or symbol) and limits the maximum speed of input. In fact, the recognition of feedback generally requires longer time than most impulsive eyes (even with the additional involvement of at least one corrective impulsive eye).
人間の場合、視覚系が（非中心窩領域を含む）視野内の急激な変化を無視することは認知上困難である。したがって、迅速な視線入力の要因は、視野を殆ど変化しないように維持することである。現在の視線入力システムにおいて、一般的に入力されている文字または文章の強調表示は、（追加の眼球運動を伴う）注意引きおよびその後の認知／認識によって、達成可能な最大入力速度を大幅に減少してしまう。 In humans, it is cognitively difficult for the visual system to ignore sudden changes in the visual field (including the non-foveal region). Therefore, a factor for rapid gaze input is to keep the visual field unchanged. In current line-of-sight input systems, highlighting commonly typed characters or sentences significantly reduces the maximum achievable input speed with attention (with additional eye movements) and subsequent cognition / recognition. Resulting in.
また、入力速度の増加は、入力プロセス自体のボトルネックを取り除くことによって、認知の流れを改善することができる。一般的に、殆どの人が（手動または視線）入力よりも速い速度で思考することができる。視線入力を簡素化および高速化することによって、一連の思考を容易に結び付けることができる。 Also, increasing input speed can improve cognitive flow by removing bottlenecks in the input process itself. In general, most people can think faster than (manual or gaze) input. By simplifying and speeding up gaze input, a series of thoughts can be easily linked.
追加の実施形態において、認知的な視覚処理および視線入力に関連する制御に影響を与えないように、非視覚的なフィードバックをユーザに提供することができる。例示的な実施形態において、フィードバックは、眼球運動による選択を示す聴覚指示で提供されてもよい。視覚的な選択を行うときに、個々の文字、単語または語句は、音声形式で放送されてもよい。追加的にまたは代替的に、触覚フィードバックをユーザに提供することができる。聴覚フィードバックおよび触覚フィードバックの両方を含む例は、システムが各単語を認識したときの発声、および各語句を決定したときの短い触覚刺激を含む。 In additional embodiments, non-visual feedback can be provided to the user so as not to affect the controls associated with cognitive visual processing and gaze input. In an exemplary embodiment, feedback may be provided by auditory instructions indicating eye movement selection. When making visual choices, individual letters, words or phrases may be broadcast in audio format. Additional or alternative, tactile feedback can be provided to the user. Examples that include both auditory and tactile feedback include vocalizations when the system recognizes each word, and short tactile stimuli when determining each phrase.
視覚フィードバックによって誘起された停留（および殆どの他の形態の固定）および認識を無くすと、視線入力を行うタイピストの中心窩視野が静的でシンプルな視覚キーボードになり、衝動性動眼によって文字および他の記号を迅速に選択することができる。入力時のフィードバックがないため、エラーを訂正する傾向も減少する。このようなシステムでは、機械に基づくエラー訂正に対する依存性が一般的に大きくなる。一般的に、入力中に初期訂正されなかった殆どのエラーは、タイピストがドキュメントの全体的にレビューする時に訂正される。 Eliminating the retention (and most other forms of fixation) and cognition induced by visual feedback, the foveal visual field of the typist performing gaze input becomes a static and simple visual keyboard, with impulsive eye movements for letters and others. You can quickly select the symbol. Since there is no feedback on input, the tendency to correct errors is also reduced. In such systems, the dependence on machine-based error correction is generally high. In general, most errors that were not initially corrected during input are corrected when the typist reviews the entire document.
更なる実施形態において、神経ネットワーク（ＮＮ）を用いて、特定の文字および／または一連の文字の選択に関連する眼球運動を「学習」することができる。機械学習の分野において、ＮＮ訓練の一般方法（例えば、バックプロパゲーション）および実装（中央処理装置、グラフィック処理装置およびハードウェアベース加速の配置を含む）は、当該技術分野において周知である。 In a further embodiment, neural networks (NNs) can be used to "learn" eye movements associated with the selection of specific letters and / or series of letters. In the field of machine learning, general methods of NN training (eg, backpropagation) and implementations (including placement of central processing units, graphics processing units and hardware-based acceleration) are well known in the art.
神経ネットを訓練することによって、個々の文字、音節、単語、一連の文字および／または他の記号の選択に関連するユーザの一連の眼球運動を認識することができる。ＮＮへの入力は、１）眼球運動中に片目または両目のビデオシーケンス、または２）実在または仮想のキーボードまたは選択マトリクスに対して片目または両目の視線の追跡座標を含むことができる。前者の場合、ＮＮは、文字または他の記号に関連する特定の眼球運動を認識することに加えて、ビデオ画像内の片目または両目の位置を追跡する機能を行う。後者の場合、ＮＮへの入力は、一連の視線座標およびキーボードまたは選択マトリックスの相対位置を含む。 By training the neural net, it is possible to recognize a series of eye movements of the user associated with the selection of individual letters, syllables, words, series of letters and / or other symbols. Inputs to the NN can include 1) a video sequence of one or both eyes during eye movement, or 2) tracking coordinates of the line of sight of one or both eyes against a real or virtual keyboard or selection matrix. In the former case, the NN functions to track the position of one or both eyes in the video image, in addition to recognizing specific eye movements associated with letters or other symbols. In the latter case, the input to the NN includes a set of line-of-sight coordinates and the relative position of the keyboard or selection matrix.
視線入力ＮＮの訓練は、１）グローバル（すなわち、視線入力を行うタイピストの集団からのデータに基づく）、２）個別（すなわち、ユーザによって生成された訓練データに基づく）、または３）２つの方法の組み合わせ（すなわち、グローバルＮＮは、個別ＮＮトレーニングの開始フレームワークとして使用される）で行われてもおい。訓練は、視線入力でシステムに知られている文字列を入力することによって実行されてもよい。ＮＮを訓練するために、理想的には、多くの異なる組み合わせの文字、音節および単語を採用すべきである。一般的には、視線入力でこのような組み合わせを含む短い文章また記事を入力することが適切である。 Training for gaze input NN can be done in 1) global (ie, based on data from a population of typists performing gaze input), 2) individually (ie, based on user-generated training data), or 3) two methods. (Ie, the global NN is used as the starting framework for individual NN training). The training may be performed by entering a string known to the system with gaze input. Ideally, many different combinations of letters, syllables and words should be adopted to train the NN. In general, it is appropriate to enter a short sentence or article containing such a combination in the line-of-sight input.
各々の人は、視線を異なる文字または記号に向ける衝動性動眼を行う方法が異なる場合がある。例えば、個人は、選択マトリックスの周辺の近くに位置する文字または記号の中心まで眼球運動を伸ばすことができない。しかしながら、ユーザがこのような文字または記号を選択するように同様の眼球運動を維持する限り、ＮＮは、この眼球運動を選択マトリックスの周辺に位置する文字、記号、またはオブジェクトの集合を選択するものとして認識することができる。 Each person may have different ways of performing impulsive oculomotor eye movements that direct their gaze to different letters or symbols. For example, an individual is unable to extend eye movements to the center of a letter or symbol located near the perimeter of the selection matrix. However, as long as the user maintains a similar eye movement to select such a character or symbol, the NN selects a set of characters, symbols, or objects located around this eye movement selection matrix. Can be recognized as.
ユーザの解剖学的構造、眼筋の生理機能および認知処理に基づいて、特定の位置に向ける眼球運動に関する経路および速度プロファイルは、異なるユーザによって大幅に異なる可能性がある。類推すると、異なる人は、一般的に異なる運動歩容で歩き、異なるパターンの腕動きで野球を投げる。異なるユーザは、視線で異なる種類の文字および単語を入力するときに、異なるパターンの「視線歩容」を示すことができる。異なるユーザは、ユーザの環境を単純に捜索するための衝動性動眼に比べて、意図的な衝動性動眼中に異なる「視線歩容」を示すこともできる。ターゲット位置に向けた眼球運動の距離および方向（すなわち、完全なパターン）は、ユーザ特有の注視プロファイルに影響を与える可能性がある。ＮＮおよび／または他のアルゴリズム法は、このような違いを考慮して、選択を行った時間および／または特定の眼球運動の「意図」の程度を決定することができる。個人に対して訓練または較正された神経ネットまたはアルゴリズム法は、一般的に、集団平均に基づく方法よりも堅牢である。 Based on the user's anatomy, eye muscle physiology and cognitive processing, the pathway and velocity profiles for eye movements towards a particular position can vary significantly from one user to another. By analogy, different people generally walk with different gaits and throw baseball with different patterns of arm movements. Different users can show different patterns of "line-of-sight gait" when entering different types of letters and words with their line of sight. Different users may also exhibit different "line-of-sight gaits" during the intentional impulsive oculomotor eye, as compared to the impulsive oculomotor eye for simply searching the user's environment. The distance and direction of eye movement towards the target position (ie, the perfect pattern) can affect the user-specific gaze profile. NN and / or other algorithmic methods can take these differences into account to determine the time of selection and / or the degree of "intention" of a particular eye movement. Neural nets or algorithmic methods trained or calibrated for individuals are generally more robust than methods based on population means.
（文字単位またはキー単位の選択に制限されなく）一連の眼球運動を生成および認識する能力は、迅速な視線入力の基礎である。手動入力の速いタイピストは、一連の文字を繋ぐための指の動きを制御する（一般的に小脳に由来すると考えられる）所謂筋肉記憶を使用する。このような入力は、（多くの反復練習によって）意識的な努力を減らす。 The ability to generate and recognize a series of eye movements (without being limited to character-by-character or key-by-key selection) is the basis of rapid gaze input. Fast-handed typists use so-called muscle memory, which controls finger movements to connect a series of letters (generally thought to come from the cerebellum). Such input reduces conscious effort (by many repetitive exercises).
同様に、記憶型衝動性動眼を用いて、視線入力中に共通の文字列を繋ぐことができる。例えば、単語「the」は、この単語を構成する３つの文字に精確に注視できない一連の眼球運動を伴い得る。しかしながら、単語「the」の入力に関連する眼球運動が一貫している限り、これらの眼球運動を認識して、関連する結果として単語「the」を生成する。同様の方法を用いて、結合して文章を形成する音節、単語、語句、長文、または概念を分類することができる。 Similarly, the memory-type impulsive occlusal eye can be used to connect common character strings during line-of-sight input. For example, the word "the" may involve a series of eye movements that do not allow precise attention to the three letters that make up the word. However, as long as the eye movements associated with the input of the word "the" are consistent, they will recognize these eye movements and generate the word "the" as a related result. Similar methods can be used to classify syllables, words, phrases, long sentences, or concepts that combine to form a sentence.
個々の文字、文字のグループ、音節、単語、語句および／または文章に関連する眼球運動を認識するように、神経ネットワークを訓練することができる。認識された個々の文字、文字のグループ、音節、単語、語句および／または文章は、以前に認識されたおよび／または格納された文字列に連結（すなわち、追加）することができる。これらの文字列は、入力ドキュメントの一部を形成してもよく、および／または他の形態の操作に対するコンポーネントまたは入力であってもよい。 Neural networks can be trained to recognize eye movements associated with individual letters, groups of letters, syllables, words, phrases and / or sentences. Individual recognized letters, groups of letters, syllables, words, phrases and / or sentences can be concatenated (ie, added) to previously recognized and / or stored strings. These strings may form part of the input document and / or may be components or inputs to other forms of operation.
追加の実施形態において、必要に応じて、視線入力は、通常単語間に入れるスペース、句読点（例えば、コンマ、ピリオド、セミコロン、疑問符）、および／または語句および文章を形成するために使用される他の要素（例えば、引用符、括弧、大括弧）の挿入を省くことができる。これらの要素は、他の入力システムにおいて、一般的に選択メニュー（例えば、キーボード）の重要な部分を占め、および／または異なる選択メニューおよび／または配置に前後切り替えることによって入力処理の流れを乱す（すなわち、認知負荷を増加させる）。 In additional embodiments, if desired, gaze input is usually used to form spaces between words, punctuation marks (eg, commas, periods, semicolons, question marks), and / or words and sentences. Elements (eg, quotation marks, parentheses, parentheses) can be omitted. These elements generally occupy an important part of the selection menu (eg, the keyboard) in other input systems, and / or disrupt the flow of input processing by switching back and forth between different selection menus and / or arrangements (for example, keyboards). That is, it increases the cognitive load).
例示的な実施形態において、このような単語間のスペース、句読点および他の文法要素を挿入するアルゴリズムを導入することができる。これらのアルゴリズムは、所謂現在の「スペルチェッカ」または「文法チェッカ」と類似している。しかしながら、スペースが通常単語間に入れてないことおよび／または句読点が通常欠けていることを具体的に考慮するように、視線入力アルゴリズムを開発する必要がある。迅速な視線入力プロセスに特有の特徴をテキストに挿入すると、システムは、より一般的なスペルチェックおよび文法チェックと同様の操作を行うことができる。 In exemplary embodiments, algorithms can be introduced to insert such inter-word spaces, punctuation marks and other grammatical elements. These algorithms are similar to the so-called current "spelling checkers" or "grammar checkers". However, it is necessary to develop a line-of-sight input algorithm specifically to take into account the fact that spaces are usually not placed between words and / or punctuation is usually missing. By inserting features into the text that are unique to the rapid gaze input process, the system can perform operations similar to the more general spelling and grammar checking.
例示的な実施形態において、視線入力した文章の機械に基づく補正は、視線入力プロセスの特性を考慮する必要がある。特性は、１つ以上の衝動性動眼に関連する「確信度」を含む。確信度は、特定の文字または文字列を入力した確信に対する評価を示し、「意図」に対する評価レベルを含むことができる。これらの評価は、既知の文字列を入力する時に過去または記録された眼球運動と、監視された眼球運動との類似度に依存する。 In an exemplary embodiment, the machine-based correction of the gaze-input text needs to take into account the characteristics of the gaze-input process. Properties include "confidence" associated with one or more impulsive oculomotor eyes. The confidence level indicates an evaluation of the belief that a specific character or character string is input, and can include an evaluation level for "intention". These assessments depend on the similarity between past or recorded eye movements when entering a known string and monitored eye movements.
視線入力は、キーボード（または他の選択マトリクス）配置に関連して測定された眼球運動に基づいて、代替文字または文字セットを機械に基づく補正アルゴリズムに提供することもできる。例えば、眼球運動が主に文字「ｔ」に向けられているように見えるときに、目の視線が（標準的なＱＷＥＲＴＹキーボード配置上の）文字「ｔ」のターゲット位置の左に少し向けられた場合、装置ユーザが実際に意図した文字が「ｒ」である可能性がある。視線入力した文字の「最良の推定値」だけでなく、測定された眼球運動に基づいて１つ以上の代替文字または文字列および／または確信度を補正アルゴリズムに与えることによって、エラー訂正を行う時に、１つ以上の代替文字を考慮することができる。 Line-of-sight input can also provide alternative characters or character sets to machine-based correction algorithms based on eye movements measured in connection with keyboard (or other selection matrix) placement. For example, when eye movements appear to be primarily directed at the letter "t", the line of sight of the eye is slightly directed to the left of the target position for the letter "t" (on a standard QWERTY keyboard layout). In that case, the character actually intended by the device user may be "r". When making error correction by giving the correction algorithm one or more alternative characters or strings and / or certainty based on the measured eye movements as well as the "best estimate" of the gaze-entered character. One or more alternative characters can be considered.
追加の実施形態において、視線入力は、必要に応じて、大文字の入力を避けることができる。大文字は、選択メニューの大部分を占有し、および／または異なる文字セットの切換えによって、入力プロセスの流れを妨げることができる。アルゴリズムを含むことによって、適切な位置（例えば、文章の始め、適切な名前、特定の頭字語）に大文字を入力することができる。 In additional embodiments, the line-of-sight input can avoid the input of uppercase letters, if desired. Uppercase letters occupy most of the selection menu and / or can interfere with the flow of the input process by switching between different character sets. By including the algorithm, you can enter uppercase letters at the appropriate positions (eg, the beginning of a sentence, the appropriate name, a particular acronym).
いくつかの状況において、必要に応じて、数字の入力は、数字キーパッドと同様のメニューを用いて行うことができる。この方法は、計算器またはスプレッドシートを用いて多くの数値を数字として入力する場合、より効率的である。同様に、有限数の要素のリスト（例えば、州、市、国、住所、月、年、色、フォントのリスト）からの１つ以上の要素の選択は、切り替え可能なメニュー選択を表示することによって、より効率的に行うことができる。 In some situations, if desired, numeric input can be done using a menu similar to the numeric keypad. This method is more efficient when entering many numbers as numbers using a calculator or spreadsheet. Similarly, selecting one or more elements from a finite number of element lists (eg, a list of states, cities, countries, addresses, months, years, colors, fonts) displays a switchable menu selection. This can be done more efficiently.
しかしながら、必要に応じて、一般化テキストの迅速入力を行うために、数字キーパッドを提供せず、偶にしか現れない数字を「入力する」（すなわち、スペルアウトする）方がより効率的である。後処理アルゴリズムを含むことによって、このような数値語をチェックし、適切なときに数字に変換することができる。例えば、最終に入力された文章において、「一七七六」または「千七百七十六」を「１７７６」として表示することができる。 However, if necessary, it is more efficient to "enter" (ie, spell out) numbers that appear only by chance, without providing a numeric keypad, for quick entry of generalized text. is there. By including a post-processing algorithm, such numeric words can be checked and converted to numbers at the appropriate time. For example, in the last input sentence, "1776" or "1776" can be displayed as "1776".
例示的な実施形態において、視線入力を行うタイピストによって使用され得る別の任意の要素は、各単語または文章の終わりに短い「一時停止」を挿入する能力である。一時停止は、特定の機能文字の認識または特定の機能文字に視線の停留を示せず、むしろ、一時停止は、単語、語句または文章の終わりを示す指示メカニズムと、視線入力を行うタイピストが次の単語または語句に進むために思想を集中させる短い時間との両方として機能することができる。経験によれば、この方法は、視線入力のペースを取るための自然で快適な方法である。 In an exemplary embodiment, another optional element that can be used by a gaze input typist is the ability to insert a short "pause" at the end of each word or sentence. Pause does not indicate the recognition of a particular functional character or the retention of the line of sight to a particular functional character, but rather the pause is the instruction mechanism that indicates the end of a word, phrase or sentence, and the typist who inputs the line of sight: It can serve both as a short time to focus thoughts on a word or phrase. Experience has shown that this method is a natural and comfortable way to pace gaze input.
まとめると、現代の文法チェッカおよびスペルチェッカのアルゴリズムに比べて、視線で入力した文字列の意図の機械に基づく識別をより前向き（proactive）になるように設計することができる。意図の識別は、一般的に、スペース、大文字および句読点が自動的に挿入されたことを前提とする。視線入力したコンテンツの書式設定は、文体スタイルに一致するように、見出しおよび段落の自動挿入、数字または他の記号の変換を含む。不適切な単語または語句が現れた場合に、標準的な文法ルールまたは辞書に基づいた訂正に加えて、特定の文字または文字列の信頼度並びにキーボード配置に基づいた代替の組み合わせを考慮してもよい。 In summary, it can be designed to be more proactive in the machine-based identification of the intent of a gaze-entered string compared to modern grammar and spell checker algorithms. Intention identification generally assumes that spaces, uppercase letters and punctuation are automatically inserted. Gaze Formatting of entered content includes automatic insertion of headings and paragraphs, conversion of numbers or other symbols to match stylistic style. In the event of inappropriate words or phrases, in addition to corrections based on standard grammatical rules or dictionaries, even considering alternative combinations based on the reliability of a particular character or string and keyboard layout. Good.
追加の実施形態において、必要に応じて、視線入力は、任意の数の「ショートカット」を含むことができる。ショートカットは、長い列の文字、言葉、文章または画像を表す視線入力用の特殊記号または文字列を含む。また、ショートカットは、特定の識別可能な眼球運動（例えば、内斜視操縦、ディスプレイから特定の方向に沿った脇見、文字選択に関連しない急速な衝動性動眼シーケンス）、または眼の周囲領域の動き（例えば、片方または両方の眼瞼の動き、寄り目、目の部分閉じまたは完全閉じ）。 In additional embodiments, the line-of-sight input can optionally include any number of "shortcuts". Shortcuts include special symbols or strings for gaze input that represent long lines of letters, words, sentences or images. Shortcuts can also be specific identifiable eye movements (eg, esotropia, inattentiveness from the display, rapid impulsive eye movement sequences not related to character selection), or movements of the peri-eye area (eg,). For example, movement of one or both eyelids, cross-eyed eyes, partially closed or fully closed eyes).
このようなショートカットを使用して、共通単語または語句（例えば、名前、挨拶、頻繁に尋ねられる質問の回答）、チェックリストなどのような共通質問の回答、および／または会話の一部として含まれ得る画像を迅速に挿入することができる。ショートカット（すなわち、特殊記号の視線選択）を使用して、キーボード配置を切り替えることもできる。このようなメニューの切り替えは、数字キーパッドを使用した数値の入力、有限数の要素（例えば、年、月、日）からなる選択シーケンスを使用した日付の入力、および類似のデータ入力などの状況に適用することができる。しかしながら、上述したように、メニューの切り替えは、一般的に、他の中断されない視線入力よりも遅い。したがって、視線入力の速度を最適に維持するために、メニューの頻繁切り替えを避けるべきである。 Use such shortcuts to include common words or phrases (eg, names, greetings, answers to frequently asked questions), answers to common questions such as checklists, and / or as part of a conversation. The resulting image can be inserted quickly. You can also use shortcuts (ie, gaze selection of special symbols) to switch keyboard layouts. Such menu switching can be done in situations such as entering numbers using the numeric keypad, entering dates using a selection sequence of finite number of elements (eg, year, month, day), and entering similar data. Can be applied to. However, as mentioned above, menu switching is generally slower than other uninterrupted line-of-sight inputs. Therefore, frequent menu switching should be avoided in order to maintain the optimum line-of-sight input speed.
さらに関連する実施形態において、ユーザは、データおよび／またはコンセプの入力速度の向上を支援するために、必要に応じて、単語、音節、語句、文章、画像および／または記号または機能の任意の所定シーケンス（または同様の種類のデータ入力）の所謂「自動充填」または自動完成を含むことができる。現在の自動充填実装とは異なり、視覚信号による１つ以上の自動充填選択肢の表示および選択は、ユーザが視線をディスプレイ上の１つ以上の自動充填作動位置に移動した場合に限り行われる。ユーザが視線を作動位置に移動した場合に限り、選択項目が表示される。これによって、現在の（視線誘導型ではない）自動充填実装の共通特徴である、入力処理中に多数の単語補完選択肢を見るために必要とされる数多くの注意分散（すなわち、検索、検査）眼球運動を回避することができる。 In a further related embodiment, the user optionally prescribes words, syllables, phrases, sentences, images and / or symbols or functions to assist in improving the input speed of data and / or concepts. It can include so-called "auto-filling" or auto-completion of sequences (or similar types of data entry). Unlike current autofill implementations, visual signal display and selection of one or more autofill options is performed only when the user moves his or her line of sight to one or more autofill operating positions on the display. The selection item is displayed only when the user moves the line of sight to the operating position. This allows for the many attention-distributed (ie, search, test) eyes required to see a large number of word completion choices during input processing, which is a common feature of current (non-gaze-guided) automatic filling implementations. Exercise can be avoided.
さらに、１つ以上の可能な選択肢は、（ユーザの現在の視線方向を考慮て）ユーザの中心窩視野に表示される。このようにして、可能な選択肢を探すための眼球運動をさらに行う必要がない。選択肢をユーザの中心窩視野に表示することによって、表示される選択肢のサイズおよび／または選択肢の数を制限することができる。選択は、可能な選択肢の拡大表現（本明細書の他の箇所に記載）を追従することによって、または選択肢から作動ターゲットに向かって視線を移動することによって、行うことができる。 In addition, one or more possible options are displayed in the user's foveal field of view (considering the user's current gaze direction). In this way, there is no need for further eye movements to look for possible options. By displaying the choices in the user's foveal field of view, the size and / or number of choices displayed can be limited. The selection can be made by following an expanded representation of the possible options (described elsewhere herein) or by moving the line of sight from the options towards the working target.
さらに、熟練したユーザは、所望の選択肢、特に共通単語、語句および／またはデータ入力要素の選択肢が明らかに利用可能である可能性が高い場合に、自動充填機構を呼び出す最適な時間を学習することができる。さらに、選択肢は、完全な語句、完全な名前、挨拶、共通のデータ入力要素などを含むことができる。これらの方法を組み合わせることによって、視線入力の速度を大幅に向上させるこｔができる。 In addition, a seasoned user should learn the optimal time to call the automatic filling mechanism when the desired choices, especially the choices of common words, phrases and / or data entry elements, are clearly likely to be available. Can be done. In addition, choices can include complete words, complete names, greetings, common data entry elements, and so on. By combining these methods, the speed of line-of-sight input can be significantly improved.
回帰神経回路網を使用した意図の識別
深部神経回路網を含む機械学習技術は、眼球運動のコースを（事前に）予測または予想するだけでなく、視覚信号および他の眼球運動の意図の識別に特に有用である。装置着用者の意図を予測および識別する方法は、２０１５年５月９日に出願され、「実在オブジェクトおよび仮想オブジェクトと対話するための生体力学的に基づく視覚信号用のシステムおよび方法」と題された特許出願第１４／７０８２３４号に記載されている。当該出願の全体は、参照により本明細書に組み込まれる。開示され方法において、眼球運動の最近の履歴が、バッファに入れられ、神経ネットワークの入力（例えば、先入れ先出し）として使用される。
Identifying Intentions Using Regressive Neural Networks Machine learning techniques, including deep neural networks, not only predict or predict (in advance) the course of eye movements, but also identify visual signals and other eye movement intentions. Especially useful. A method for predicting and identifying the device wearer's intent was filed May 9, 2015, entitled "Systems and Methods for Biomechanically Based Visual Signals for Interacting with Real and Virtual Objects." It is described in Patent Application No. 14/708234. The entire application is incorporated herein by reference. In the disclosed method, the recent history of eye movements is buffered and used as a neural network input (eg, first-in first-out).
別の実施形態は、回帰神経ネットワークの使用を含む。回帰神経ネットワークは、接続内に反復サイクルが存在するように、（「バック」方向に沿って）相互接続された神経ネットワークの一種である。これらの相互接続によって、最近の「メモリ」をネットワーク自体に組み込むことが可能になる。このような構造は、眼球運動の明示的な履歴を（上述した）入力ノードに供給する必要性を排除することができる。むしろ、各眼球運動が測定され、順次にネットワークに供給され、最近の運動履歴の相互作用および重みがネットワーク学習プロセスによって決定される。 Another embodiment involves the use of a regression neural network. Regressive neural networks are a type of neural network that are interconnected (along the "back" direction) so that there are iterative cycles within the connection. These interconnects allow modern "memory" to be incorporated into the network itself. Such a structure can eliminate the need to provide an explicit history of eye movements to the input node (described above). Rather, each eye movement is measured and sequentially fed to the network, and recent movement history interactions and weights are determined by the network learning process.
眼球運動の最近の履歴は、上述した視線入力中に意図を識別する際に特に有用である。迅速な視線入力中に意図の識別は、文字単位の入力だけでなく、コンテンツ入力時に２つ以上の文字、完全な単語および／または完全な語句の組み合わせに関連する識別可能な一連の眼球運動を含む。さらに、音節、単語間のギャップ、カンマに関連するものを含む句読点が通常発生する位置、および文章の末尾に関連する眼球運動（必要に応じて、眼球運動の一時停止を含む）を識別することができる。したがって、例示的な実施形態において、視線入力時の意図を識別するために使用された神経ネットアーキテクチャは、拡張された語句および短い停止（すなわち、約２０秒まで）を組み込むのに十分な履歴を含むことができる。 The recent history of eye movements is particularly useful in identifying intent during the gaze input described above. Intention identification during rapid gaze input involves not only character-by-character input, but also a series of identifiable eye movements associated with a combination of two or more characters, a complete word and / or a complete phrase when entering content. Including. In addition, identify syllables, gaps between words, where punctuation marks normally occur, including those related to commas, and eye movements associated with the end of the sentence, including pausing eye movements, if necessary. Can be done. Thus, in an exemplary embodiment, the neural net architecture used to identify intent during gaze input has sufficient history to incorporate extended phrases and short stops (ie, up to about 20 seconds). Can include.
装置着用者の環境内のボタンおよびつまみ
追加の例示的な実施形態において、「仮想ボタン」または物理ボタンは、装置着用者の視野内の任意の認識可能な表面上に表示され、「仮想的に押される」または作動されることができる。「仮想ボタン」は、標準的なキーボード、数字パッド、壁スイッチなどに見られる機能と同様の機能を行うことができ、視覚信号言語内のターゲットアイコンに関連する補助機能を行うことができる。さらに、入力は、例えば、スピーカ音量を選択するための入力として使用される「スライダ」のように、連続的に可変であってもよい。仮想ボタンおよび／またはつまみの要素は、制御面を移動させる能力または（制御面に対して）装置ユーザを移動させる能力を含む。制御面の画像認識を使用する場合、仮想ボタンおよび他のつまみの位置は、制御面に対して不動のままであってもよい。
Buttons and knobs in the device wearer's environment In an additional exemplary embodiment, a "virtual button" or physical button is displayed on any recognizable surface within the device wearer's field of view and is "virtually". Can be "pushed" or activated. "Virtual buttons" can perform functions similar to those found on standard keyboards, number pads, wall switches, etc., and can perform auxiliary functions related to target icons in the visual signal language. Further, the input may be continuously variable, for example, a "slider" used as an input for selecting speaker volume. The virtual button and / or knob element includes the ability to move the control surface or the device user (relative to the control surface). When using control surface image recognition, the positions of the virtual buttons and other knobs may remain immobile with respect to the control surface.
シーンカメラを用いて、基準面の位置（例えば、エッジ、色の変化、スポット、パターン）に関連して配置された仮想ボタン、物理ボタンおよび連続つまみと共に、制御面を認識することができる。装置ユーザがボタンの領域を見ながら、制御面上の仮想ボタン／つまみを仮想的に「押圧」または「スライド」する時に限り、作動を行う。画像処理技術を用いて、１本以上の指が制御面を押した（すなわち、２進制御）または制御面に沿ってスライドした（すなわち、連続的な制御）か否かを認識することによって、作動を判断する。 The scene camera can be used to recognize the control surface, along with virtual buttons, physical buttons and continuous knobs arranged in relation to the position of the reference surface (eg, edges, color changes, spots, patterns). The operation is performed only when the device user virtually "presses" or "slides" the virtual button / knob on the control surface while looking at the area of the button. By recognizing whether one or more fingers pressed the control surface (ie, binary control) or slid along the control surface (ie, continuous control) using image processing techniques. Judge the operation.
便利な「仮想制御面」の一例として、装置ユーザの腕の皮膚表面が挙げられる。手首および肘に沿った腕の側面は、反対側の手の指でタッチ操作および摺動操作を行うための基準位置を提供する。このような制御の一例として、例えば、装置ユーザの手首に最も近い仮想ボタンを押圧することによって、装置による（テキストを音声に変換するソフトウェアを用いて）最近のテキストメッセージの発声を（フリップフロップ方式で）開始および停止することができる。腕の中間部分の仮想スライダを用いて、音声ボリュームを制御することができる。肘に近い仮想ボタンを用いて、一連のテキストメッセージを「スキップする」またはテキストメッセージに「戻る」ことができる。仮想ボタンを含むアームが移動する場合、装置着用者が移動する場合、および／または装置着用者の頭部および眼球の両方が移動する（例えば、前庭眼反射）場合であっても、このような操作を行うことができる。 An example of a convenient "virtual control surface" is the skin surface of the device user's arm. The sides of the arm along the wrist and elbow provide a reference position for touch and sliding operations with the fingers of the opposite hand. As an example of such control, for example, by pressing the virtual button closest to the wrist of the device user, the device can utter a recent text message (using software that converts text to speech) (flip-flop method). Can be started and stopped. You can control the audio volume using the virtual slider in the middle of the arm. You can "skip" a series of text messages or "return" to a text message using a virtual button near your elbow. Such as when the arm containing the virtual button moves, when the device wearer moves, and / or when both the device wearer's head and eyeball move (eg, vestibular eye reflex). You can perform operations.
「制御面」（すなわち、物理的なオブジェクトまたは表面と仮想つまみとの組み合わせ）の別の例として、通常会議室および劇場に配置され、画像を投影するためのホワイトボードが挙げられる。シーンカメラを用いて、基準位置（例えば、ホワイトボードのエッジ）を決定することによって、制御面を特定することができる。制御面に注視することによっておよび１本以上の指で制御面をタッチするまたは制御面に沿ってスライドすることによって、入力を特定することができ、これによって、アプリケーションの制御および他の操作を行うことができる。 Another example of a "control surface" (ie, a combination of a physical object or surface and a virtual knob) is a whiteboard, usually placed in conference rooms and theaters, for projecting images. A control surface can be identified by determining a reference position (eg, the edge of a whiteboard) using a scene camera. Inputs can be identified by gazing at the control surface and by touching or sliding along the control surface with one or more fingers, thereby controlling the application and performing other operations. be able to.
追加の実施形態において、制御面または「作動」面にオブジェクトまたは記号（例えば、図画、文字、グラフィックス）を設けることができる。例えば、ホワイトボードまたは他の表面に任意の記号を描画し、次いで、視覚信号言語または他の入力手段を用いて「意味」または関連する動作を割り当てることができる。例えば、表面上に文字「Ｗ」を描画し、「Ｗ」に対する作動（例えば、注視または指押し）があった場合、一連のコマンドを実行することによって、現在の「天気」を表示することができる。代わりに、認識可能な図形（例えば、「Ｗ」）または物理的なオブジェクトに動作（例えば、現在天気の表示）を事前に割り当てることができる。 In additional embodiments, objects or symbols (eg, drawings, letters, graphics) can be provided on the control or "actuating" surfaces. For example, any symbol can be drawn on a whiteboard or other surface and then assigned a "meaning" or related action using a visual signal language or other input means. For example, if the letter "W" is drawn on the surface and there is an action (eg, gaze or finger press) on the "W", the current "weather" can be displayed by executing a series of commands. it can. Alternatively, an action (eg, a display of the current weather) can be pre-assigned to a recognizable figure (eg, "W") or a physical object.
別の例として、例えば、紙またはプラスチックで支持された発光接着剤（すなわち、「ステッカー」）を用いて、色付き点、記号、またはカットアウト（以下、アイコン記号と総称する）を装置ユーザの腕などの表面に（例えば、一時的に）貼り付けるまたは接着することができる。様々な作業を行うように、異なるアイコン記号に割り当てることができる。これらのアイコン記号は、装置着用者の視野の任意位置に配置することができる。これによって、装置着用者は、制御入力が豊富な環境を形成し、カスタマイズすることができる。 As another example, a colored dot, symbol, or cutout (hereinafter collectively referred to as an icon symbol) is applied to the arm of the device user using, for example, a luminescent adhesive (ie, a "sticker") supported by paper or plastic. It can be attached or adhered (for example, temporarily) to a surface such as. It can be assigned to different icon symbols to perform different tasks. These icon symbols can be placed at any position in the field of view of the device wearer. This allows the device wearer to create and customize an environment rich in control inputs.
上述したように、仮想つまみまたは（物理的な）アイコン記号に関連する動作は、予め記号および／または相対位置に割り当てることができ、使用中に記号および／または相対位置に割り当てることができ、または両方の組み合わせによって割り当てることができる。仮想つまみまたは実在つまみに動作を割り当てるときに、コンテキストを考慮することもできる。例えば、テキストメッセージを表示している間、スライダを用いて、音量を制御することができる。しかしながら、写真を表示している場合、（制御面の同一位置に設けられた）スライダを用いて、写真のサムネイルを迅速に閲覧することができる。 As mentioned above, actions related to virtual knobs or (physical) icon symbols can be pre-assigned to symbols and / or relative positions, and can be assigned to symbols and / or relative positions during use, or. It can be assigned by a combination of both. You can also consider the context when assigning actions to virtual or real knobs. For example, you can use the slider to control the volume while displaying a text message. However, when displaying a photo, a slider (provided at the same position on the control surface) can be used to quickly browse the thumbnail of the photo.
追加の実施形態において、遠隔ディスプレイおよび／または拡張現実ヘッドセットのディスプレイを含むディスプレイを用いて、制御または作動を行うことができる。換言すれば、操作を関連付けたオブジェクトが視覚的に特定された場合、ディスプレイ上の１つ以上の仮想ターゲットに対する１つ以上の衝動性動眼（または他の眼球運動）を行うことによって、操作を開始することができる。特に拡張現実ヘッドセット内の複数の（別個の操作を引き起こすことができる）別個の作動ターゲットを用いて、環境内の複数のオブジェクトに関連付けた一連の操作を迅速に行うことができる。 In additional embodiments, displays including remote displays and / or displays in augmented reality headsets can be used to control or operate. In other words, when the object associated with the operation is visually identified, the operation is initiated by performing one or more impulsive eye movements (or other eye movements) on one or more virtual targets on the display. can do. In particular, multiple separate actuation targets (which can trigger separate operations) within an augmented reality headset can be used to quickly perform a series of operations associated with multiple objects in the environment.
通知によってトリガされる状況依存型視覚信号の作動
装着型コンピュータ装置、ポータブルコンピュータ装置および固定型コンピュータ装置において、装置ユーザに着信情報を通知するための一般的な方法は、着信情報をディスプレイ内のサムネイル、タイトル、メッセージの最初の数個の単語および／または他の視覚表示に一時的に重合することである。これらのものは、一般的に、表示装置上の固定位置（例えば、右上角）に形成され、ユーザに周知である。通知は、例えば、着信テキストメッセージ、電話メッセージ、電子メール、警告、目覚通知、カレンダイベント、更新サービス、データストリーム（例えば、天気、株式市場レポート、ＧＰＳの位置）の重要な変更を含むことができる。本発明のシステムおよび方法を用いて、これらのコンピュータ装置は、時間依存方式で、ディスプレイ上の領域に時間依存情報を「共有」することができる。
Activation of context-sensitive visual signals triggered by notifications In wearable computer devices, portable computer devices, and fixed computer devices, a common way to notify device users of incoming call information is to thumbnail the incoming call information in the display. , Title, the first few words of the message and / or other visual display. These are generally formed in a fixed position on the display device (eg, the upper right corner) and are well known to the user. Notifications can include, for example, incoming text messages, phone messages, emails, alerts, alerts, calendar events, update services, significant changes to data streams (eg weather, stock market reports, GPS locations). .. Using the systems and methods of the invention, these computer devices can "share" time-dependent information into areas on the display in a time-dependent manner.
このような通知は、一般的に、装置ユーザの迅速な一瞥（すなわち、一時的な通知の位置に向かう衝動性動眼およびその位置から離れる衝動性動眼の両方）を引き起こす。ユーザは、一瞥によって、通知に反応することができる。視覚信号言語は、このような一時的な通知に応じた「作動」を可能にすることによって、着信情報に効率的に反応する機会を提供する。作動は、一時的な通知の位置から「作動」ターゲットに視線を移動する衝動性動眼によって行われる。ユーザが通知位置から非作動ターゲット位置に視線を移動した場合（例えば、視線を元の作業に戻した場合）または衝動性動眼を行う前に通知が消えた場合、作動は行われない。 Such notifications generally cause a quick glance at the device user (ie, both the impulsive oculomotor eye towards and away from the position of the temporary notification). The user can respond to the notification with a glance. The visual signal language provides an opportunity to respond efficiently to incoming information by enabling "operation" in response to such temporary notifications. The actuation is performed by an impulsive googlymium that moves the line of sight from the position of the temporary notification to the "actuated" target. If the user moves his or her line of sight from the notification position to the inactive target position (eg, returns the line of sight to the original work) or if the notification disappears before performing the impulsive eye movement, no action is taken.
作動は、状況に依存してもよい。すなわち、通知内のデータ種類に応じて、適切なソフトウェアツールを起動することができる。例えば、電子メールの受信通知を有効にすると、電子メールの読み上げおよび表示を行うためのソフトウェアの作動および／または起動を引き起こすことができる。着信テキストメッセージは、テキストメッセージの表示およびそれに対する応答を行うためのソフトウェアの作動および／または起動を引き起こすことができる。カレンダイベントの通知を有効にすると、イベントに関する詳細を表示することができ、イベントの参加に関する応答を送信することができる。 The operation may depend on the situation. That is, an appropriate software tool can be started according to the data type in the notification. For example, enabling email notification can trigger the activation and / or activation of software for reading and displaying email. Incoming text messages can trigger the activation and / or activation of software for displaying and responding to text messages. If you enable notification of calendar events, you can view details about the event and send a response about attending the event.
選択厳密性の動的制御
多数の条件は、視覚信号（例えば、衝動性動眼、滑動性追跡眼球運動）言語要素を正確に生成するためのユーザの能力に影響を与えることができる。同様に、多数の条件は、眼球運動を精確に測定し、識別するための装置の能力に影響を与えることができる。場合によって、例えば、でこぼこ道路で運転をするとき、バスまたは自転車に乗っるとき、ボートを操縦しているとき、スキーをしているとき、走っているとき、単に歩いているときに、ユーザおよび装置性能の両が影響を受ける可能性もある。
Dynamic Control of Selective Strictness Numerous conditions can affect the user's ability to accurately generate visual signals (eg, impulsive oculomotor eye, gliding pursuit eye movement) language elements. Similarly, a number of conditions can affect the ability of the device to accurately measure and discriminate eye movements. In some cases, for example, when driving on bumpy roads, riding a bus or bicycle, maneuvering a boat, skiing, running, simply walking, the user and Both equipment performances can be affected.
これらの条件の下、言語要素の識別精度に影響を与える主な原因は、加速度と、圧縮可能な表面によって支持された圧縮可能な物体または少なくとも１つの自由度で自由に移動できる物体を移動できる合成力（ニュートン法則によって、力は、物体の質量に比例する）との積である。片目または両目に加速度を加えた場合、これらの３つのメカニズムの全ては、ユーザの眼球運動システムによって決められた生理学的制御（意図的な制御を含む）を超えて、眼球を動かす可能性がある。ユーザの頭部領域の加速度は、眼球（特に、眼球の流体充満領域）を圧迫することができ、眼球を眼窩内または眼窩外に押すことができ、および／または眼窩内で眼球を（垂直および／または水平に）回転させることができる。これらの動きは、一般的にわずかであるが、視線推定に関わる幾何学によって、意図的な眼球運動の精確な追跡に大きな影響を与えることができる。 Under these conditions, the main factors that affect the identification accuracy of language elements are acceleration and the ability to move compressible objects supported by compressible surfaces or objects that are free to move with at least one degree of freedom. It is the product of the combined force (according to Newton's law, the force is proportional to the mass of the object). When acceleration is applied to one or both eyes, all three of these mechanisms can move the eye beyond the physiological controls (including intentional controls) determined by the user's eye movement system. .. The acceleration of the user's head area can compress the eyeball (particularly the fluid-filled area of the eyeball), push the eyeball into orbital or out of the orbit, and / or push the eyeball (vertical and) within the orbit. / Or can be rotated horizontally). These movements are generally insignificant, but the geometry involved in gaze estimation can have a significant impact on the accurate tracking of intentional eye movements.
同様に、頭部装着装置または他の装置に加速度を加えた場合、装置自体の一部が撓むことによって、装置とユーザとの間の接触点（例えば、鼻、耳の上方）または全てを圧迫することができ、または装置が滑ることによって、眼球位置を検知する検出装置（例えば、１つ以上のカメラ）の相対位置を変化させることができる。眼球位置を検知する検出装置が携帯電話またはタブレットに取り付けられているおよび／またはユーザによって保持されている場合、状況は一般的に悪くなる。１つ以上の検出装置を含む装置の質量および装置を保持することに関与するテコ作用は、力を増加させ、片目または両目に対する装置の動きを増加させることができる。 Similarly, when acceleration is applied to a head-worn device or other device, a portion of the device itself bends, causing the contact point between the device and the user (eg, nose, above the ear) or all. The relative position of the detection device (eg, one or more cameras) that detects the eye position can be changed by compressing or sliding the device. The situation is generally exacerbated when a detector that detects eye position is attached to the cell phone or tablet and / or held by the user. The mass of the device, including one or more detectors, and the leverage involved in holding the device can increase the force and increase the movement of the device with respect to one or both eyes.
視覚信号言語要素の識別精度に影響を与える可能性のある他の条件は、環境照明（屋内対屋外）、瞳孔のサイズおよび応答性、睫毛および／または他の障害物の瞬間位置、日常活動（例えば、寝起き、真昼に明るい環境にいる、夜遅く）に伴って変化し得る眼瞼の通常位置を含む。 Other conditions that can affect the identification accuracy of visual signal language elements are ambient lighting (indoor vs. outdoor), pupil size and responsiveness, momentary position of eyelashes and / or other obstacles, daily activity (daily activity). For example, it includes the normal position of the eyelids that can change with waking up, in a bright environment at noon, late at night).
例示的な実施形態において、視覚信号言語の選択およびその他の構成要素を作成する厳密性は、予期の視線追跡精度に依存して、動的に変更することができる。装置によって監視され得る外部条件に加えて、ユーザが装置を使用する経験、（例えば、ユーザによって行われた「復帰」操作または他の訂正操作によって決められた）ユーザの過去の精度、時間（典型的な一日に対する履歴の推定精度に基づくこともできる）ユーザが（例えば、視覚信号言語を用いて）指定した好み設定に依存して、厳密性を変更することができる。 In an exemplary embodiment, the choice of visual signal language and the rigor of creating other components can be dynamically changed depending on the expected gaze tracking accuracy. In addition to the external conditions that can be monitored by the device, the user's experience of using the device, the user's past accuracy, time (typically determined by a "return" operation or other correction operation performed by the user). The rigor can be changed depending on the preference settings specified by the user (eg, using a visual signal language) (which can also be based on the estimation accuracy of the history for a typical day).
頭部装着装置を着用するときに、頭部領域に適用された加速度の大きさおよび頻度は、頭部装着装置に１つ以上の加速度計を埋め込むこと、眼球を監視する１つ以上のカメラを用いて眼球および瞳孔の急な側方運動を検出すること、および／または１つ以上の外向きシーンカメラを用いて画像内の環境の全体的な動きを捕捉することを含む多くの方法を用いて、推定することができる。 When wearing a head-mounted device, the magnitude and frequency of acceleration applied to the head area can be determined by embedding one or more accelerometers in the head-mounted device, one or more cameras that monitor the eyeball. Using many methods, including detecting sudden lateral movements of the eye and pupil, and / or capturing the overall movement of the environment in the image with one or more outward-facing scene cameras. Can be estimated.
後者の方法は、一定の時間に亘って１つ以上のシーンカメラから撮影された画像を比較することによって、画像内の背景が頭部動作と一致して並進（画像内の少量の回転運動が可能である）移動したか否かを判断することができる。（装置ユーザの頭部に取り付けられている）シーンカメラの角速度は、背景が１つのカメラ画像から次のカメラ画像に（単位時間で）並進移動した距離にほぼ比例する。画像内の背景の移動は、画像認識技術を用いて、装置ユーザに対して移動し得る個々のオブジェクトの移動から分離されてもよい。 The latter method compares images taken from one or more scene cameras over a period of time so that the background in the image translates in line with the head movement (a small amount of rotational movement in the image). It is possible to determine whether or not it has moved. The angular velocity of the scene camera (mounted on the device user's head) is approximately proportional to the distance (in units of time) the background travels translationally from one camera image to the next. The movement of the background in the image may be separated from the movement of individual objects that can be moved to the device user using image recognition techniques.
環境照明（および追跡精度に与える影響）は、視線を追跡する検出装置内の全体的な輝度レベルまたは装置内に埋め込まれた１つ以上の環境光検出装置（例えば、フォトダイオード）からの全体的な輝度レベルから推定することができる。環境照明は、１つ以上のシーンカメラから撮影された画像に基づいて推定することもできる。 Environmental lighting (and its effect on tracking accuracy) is the overall brightness level within the detector that tracks the line of sight or the overall from one or more ambient light detectors (eg, photodiodes) embedded within the device. It can be estimated from the brightness level. Environmental lighting can also be estimated based on images taken from one or more scene cameras.
例示的な実施形態において、多くの方法を利用して、選択を行うための厳密性を変更することができる。これらの方法は、単独でまたは組み合わせて使用することができる。これらの方法は、視覚信号メニュー選択のサブセットまたは個々の選択に全体的に適用することができる。 In an exemplary embodiment, many methods can be utilized to change the rigor for making a selection. These methods can be used alone or in combination. These methods can be applied globally to a subset of visual signal menu selections or individual selections.
選択の厳密性を調整する比較的簡単な方法は、選択を行う空間的な目標範囲の制御を含む。殆どの場合、うまく設計された目標オブジェクトは、ユーザの視線を目標オブジェクトの最適（通常中央）位置に導くための中心焦点（区別可能な明暗度、明るさおよび色を有する）を含む。特定の選択を行うためのターゲット空間範囲に視線を移動した場合、アルゴリズムは、その眼球運動が視覚信号言語の意図的な要素であることを判断することができる。 A relatively simple way to adjust the rigor of selection involves controlling the spatial target range for making the selection. In most cases, a well-designed target object will include a central focus (having distinguishable brightness, brightness and color) to guide the user's line of sight to the optimal (usually centered) position of the target object. When the line of sight is moved to a target spatial range to make a particular selection, the algorithm can determine that the eye movement is an intentional element of the visual signal language.
空間的な目標範囲は、円形、楕円形、正方形、または長方形を含む任意の形状とすることができる。空間的な目標範囲のサイズおよび／または形状を制御することによって、厳密性を調整することができる。一般的に、目標範囲のサイズを減少すると、ターゲット選択の厳密性が増大し、精確な注視位置を決定する不確実性が増加するため、不注意による作動の回数が減少する。逆に、より理想的な眼球運動を記録する条件下、目標範囲のサイズが増加すると、正確な眼球運動を行う必要性が減少するため、ユーザがより迅速に視覚信号運動を生成し、認知負荷を低減させることができる。 The spatial target range can be any shape, including circular, elliptical, square, or rectangular. Strictness can be adjusted by controlling the size and / or shape of the spatial target range. In general, reducing the size of the target range increases the rigor of target selection and increases the uncertainty of determining the exact gaze position, thus reducing the number of inadvertent actions. Conversely, under conditions that record more ideal eye movements, increasing the size of the target range reduces the need for accurate eye movements, allowing the user to generate visual signal movements more quickly and cognitive load. Can be reduced.
厳密性は、検出された眼球運動の時間範囲を制御することによって、調整することができる。例えば、精確な視線追跡を行うための条件が最適である時間に、衝動性動眼の注視位置の予測および衝動性動眼の視線が作動ターゲット領域に注視するという決定をより早く行うことができる。これによって、その後の視覚信号言語要素の変更をより迅速に行うことが可能になる。その結果、ターゲット位置のアイコンまたはオブジェクトを変更するまたは除去することによって、ギャップ効果を生成することができる。「公開」選択処理の間に、殆どまたは全く遅延せず、追加のメニュー選択を提示することができる。さらに、画面上で「公開」メニューを利用できる長さは、より精確な視線追跡が利用可能な場合に低減され得る（すなわち、より精確な視線追跡によって促進される）。 Severity can be adjusted by controlling the time range of detected eye movements. For example, at a time when the conditions for accurate gaze tracking are optimal, it is possible to predict the gaze position of the impulsive occlusal eye and determine that the gaze of the impulsive occlusal eye gazes at the working target region earlier. This makes it possible to make subsequent changes to the visual signal language element more quickly. As a result, a gap effect can be generated by modifying or removing the icon or object at the target location. Additional menu selections can be presented with little or no delay during the "public" selection process. In addition, the length of availability of the "Publish" menu on the screen can be reduced if more precise eye tracking is available (ie, promoted by more accurate eye tracking).
一般的に、測定値に不確実性がある場合、測定値を複数回測定し、測定値を平均化することによって、精度を向上させることができる（測定されている項目が測定時間内に比較的静止している場合）。このような統計学的手法は、以下でより詳細に説明する所謂「厳密性マップ」を考慮することができる。理想的な記録条件ではない場合、ユーザに注視されている実在オブジェクトまたは仮想オブジェクト（またはオブジェクトの一部）を確実に識別するために、より多くの眼球位置の測定（すなわち、検出装置による画像フレームの収集）を行う必要がある。この方法は、２つ以上の近接オブジェクトが存在する状況に特に有用である。 In general, when there is uncertainty in the measured value, the accuracy can be improved by measuring the measured value multiple times and averaging the measured values (measured items are compared within the measurement time). When stationary). Such statistical methods can take into account the so-called "rigidity maps" described in more detail below. More eye position measurements (ie, image frames by the detector) to ensure that a real or virtual object (or part of an object) that is being watched by the user is identified when the recording conditions are not ideal. Need to be collected). This method is especially useful in situations where there are two or more melee objects.
更なる例示的な実施形態において、高い厳密性条件の場合、視覚信号選択および作動プロセスの間に、より多くの測定を検討する必要がある。逆に、低い厳密性条件の場合、より少ない測定を行うことによって、予定の信頼度を達成することができ、シーケンシャル視覚信号をより迅速に行うことができる。厳密性の計算に使用される上記の条件に加えて、眼球位置が実質的に静止していると予想されるときの測定値の変化レベルを用いて、測定値内の「ノイズ」の大きさを推定することができる。ノイズが増加する場合、空間測定値の確実性の低下を補うために、厳密性を増加させる必要がある。 In a further exemplary embodiment, for high rigor conditions, more measurements need to be considered during the visual signal selection and actuation process. Conversely, under low rigor conditions, less measurement can achieve expected reliability and sequential visual signals can be performed more quickly. In addition to the above conditions used to calculate the rigor, the magnitude of "noise" within the measurement is used, using the level of change in the measurement when the eye position is expected to be substantially stationary. Can be estimated. If the noise increases, the rigor needs to be increased to compensate for the reduced certainty of the spatial measurements.
不注意な作動（ＩＡ）の回避
視覚信号言語に関する殆どの記述は、様々な操作を選択して行うプロセスに集中しているが、いわゆる不注意な作動（ＩＡ）の回避も検討する必要がある。ＩＡは、装置着用者の意図しない操作の実行を引き起こす１つ以上の眼球運動から生じる。例えば、ＩＡは、選択シーケンスと重複し、選択シーケンスとして解釈される実在オブジェクトを見ることに関わる眼球運動から生じることがある。別の例は、ユーザの注意が意図的な視線選択および／または作動シーケンスの特徴を模倣する注意分散な眼球運動を生成する、ユーザに「叫ぶ」オブジェクトを表示することを含む。
Avoiding inadvertent actuation (IA) Most descriptions of visual signal languages focus on the process of selecting and performing various operations, but avoidance of so-called inadvertent actuation (IA) should also be considered. .. IA results from one or more eye movements that cause the device wearer to perform unintended manipulations. For example, IA may result from eye movements that overlap with the selection sequence and involve viewing a real object that is interpreted as the selection sequence. Another example involves displaying a "screaming" object to the user, where the user's attention produces attention-distributed eye movements that mimic the characteristics of intentional gaze selection and / or action sequences.
ＩＡは、いくつかの場合（例えば、ゲームプレイ）に単に迷惑であるが、いくつかの使用例において、ＩＡは、回避しなければならない実質的な危険である。例えば、拡張現実ディスプレイの一部領域に情報の投影を可能にするＩＡは、現実世界の視認を妨害してしまう。このことは、活動を妨害してしまい、機械操作の場合に、危険である。同様に、ＩＡは、予期せぬ音を生成してしまい、装置着用者を驚かせる。 IA is simply annoying in some cases (eg gameplay), but in some use cases IA is a substantial danger that must be avoided. For example, an IA that allows information to be projected onto a portion of an augmented reality display interferes with real-world visibility. This interferes with activity and is dangerous in the case of machine operation. Similarly, the IA produces unexpected sounds that surprise the wearer of the device.
例示的な実施形態において、ＩＡを回避するための方法として、選択および作動を行うプロセスに「摩擦」を加えることである。上述したように、様々な使用例が様々なリスクおよび／またはＩＡによる有効性の喪失を引き起こすため、異なるレベルの摩擦が必要とされる。摩擦を増減するための方法は、個々にまたは組み合わせて使用して、広い範囲の摩擦レベルを生成することができる。摩擦レベルは、個々の選択および／または作動シーケンスに対して予め設定されてもよく、または、視覚信号言語の内容、ユーザの経験、ＩＡの最新記録、最近の眼球運動の速度などの要因に基づいて動的に調整されてもよい。 In an exemplary embodiment, a way to avoid IA is to add "friction" to the process of selection and operation. As mentioned above, different levels of friction are required as different use cases cause different risks and / or loss of effectiveness due to IA. Methods for increasing or decreasing friction can be used individually or in combination to produce a wide range of friction levels. The friction level may be preset for individual selection and / or operation sequences, or is based on factors such as visual signal language content, user experience, IA up-to-date records, and recent eye movement speeds. May be dynamically adjusted.
上述したように、視覚信号の摩擦を制御する最も簡単な方法は、眼球運動の目標領域のサイズおよび形状の制御である。一般的に、目標領域のサイズを小さくすると摩擦が増大し、ＩＡの数が減少する。目標領域のサイズの縮小は、眼球運動が１つ以上のターゲットを「見逃す」ことによっていくつかの選択または作動シーケンスの不出来を犠牲にして実行される。その結果、視覚信号言語シーケンスを遅らせる（および困らせる）可能性がある。したがって、物理追跡の精度に影響を与える可能性のある（上記の）環境条件に加えて、ターゲットの見逃し回数およびその後の再試行回数（同一のターゲットの方向に向けて重複に行った成功の眼球運動および不成功の眼球運動）に基づいて、厳密性を動的に変更することができる。 As mentioned above, the simplest way to control the friction of visual signals is to control the size and shape of the target area of eye movement. In general, reducing the size of the target area increases friction and reduces the number of IA. Reducing the size of the target area is performed at the expense of some selection or failure of the actuation sequence by eye movements "missing" one or more targets. As a result, it can delay (and annoy) the visual signal language sequence. Therefore, in addition to the environmental conditions (above) that can affect the accuracy of physical tracking, the number of missed targets and the number of subsequent retries (overlapping successful eyeballs towards the same target). The rigor can be changed dynamically based on movement and unsuccessful eye movements).
更なる実施形態において、ＩＡを回避するために適用された摩擦は、眼球運動の正確な分析に依存し得る。ある位置の特定のオブジェクトの意図的な観察は、しばしば所謂矯正的な衝動性動眼を伴う。このような衝動性動眼によって、観察者の視線は、観察しているオブジェクトにより正確に指向する。矯正的な衝動性動眼は、オブジェクトを観察する意図を表すため、「意図的な」ものとして解釈することができる。矯正的な衝動性動眼を用いて、領域内のオブジェクトを選択するための摩擦を減少することができる。 In a further embodiment, the friction applied to avoid the IA may depend on an accurate analysis of eye movements. Intentional observation of a particular object at a location is often accompanied by so-called corrective impulsive oculomotor eye. With such an impulsive oculomotor eye, the observer's line of sight is more accurately directed by the object being observed. Corrective impulsive oculomotor eye can be interpreted as "intentional" because it expresses the intention of observing the object. Corrective impulsive googlymia can be used to reduce friction for selecting objects in the area.
同様に、長い距離に亘る衝動性動眼は、ターゲットをアンダーシュートする傾向があるため、ターゲット位置に焦点を絞るように、同一方向の矯正的な衝動性動眼を必要とする。視覚信号言語は、長い距離の衝動性動眼の軌道を外挿することによって、この傾向を認識することができる。長い距離の衝動性動眼がターゲットに向けられているがアンダーシュートする（多くの場合、最大２０％）場合、視覚信号言語は、ターゲットが選択される（摩擦を低減する）と見なし、選択に関連する操作を行う。また、必要に応じて、システムは、初期の衝動性動眼の注視位置と投影されたターゲットとの間に可能なターゲットがないことを確認して、初期衝動性動眼の後に、投影されたターゲットに向ける１つ以上の矯正的な衝動性動眼が続く可能性があることを確認する。 Similarly, long-distance impulsive eyes tend to undershoot the target and therefore require corrective impulsive eyes in the same direction to focus on the target position. Visual signal language can recognize this tendency by extrapolating the trajectory of the impulsive oculomotor eye over long distances. If a long-distance impulsive occlusal eye is aimed at the target but undershoots (often up to 20%), the visual signal language considers the target to be selected (reduces friction) and is associated with the selection. Do the operation to do. Also, if necessary, the system confirms that there is no possible target between the gaze position of the initial impulsive eye and the projected target, and after the initial impulsive eye, the projected target. Make sure that one or more corrective impulsive oculomotor eyes that are directed may follow.
逆に、短い距離の衝動性動眼は、意図した注視位置をオーバーシュートする傾向がある。長い距離の衝動性動眼と同様に、視覚信号言語は、この傾向を考慮することができる。短い距離の衝動性動眼が検出され、ターゲットが（最大約２０％で）短縮されたターゲット経路内にある場合、ターゲットが選択されると見なし、摩擦を低減し、選択に関連する操作を行うことができる。 Conversely, short-distance impulsive oculomotors tend to overshoot the intended gaze position. Similar to long-distance impulsive oculomotor eyes, visual signal languages can take this tendency into account. If a short-distance impulsive occlusal eye is detected and the target is within the shortened target path (up to about 20%), the target is considered to be selected, friction is reduced, and selection-related operations are performed. Can be done.
衝動性動眼中に移動した距離に基づいて、ターゲットをアンダーシュートまたはオーバーシュートする傾向を認識することは、作動ターゲット（例えば、「スタート」または「公開」ターゲット）に向けられた衝動性動眼などの意図を伝える意図的な眼球運動に特に有用である。さらに、前述したように、衝動性動眼が本質的に「衝撃的」であるため、移動距離および注視位置を予測することができる。したがって、長い距離の衝動性動眼がターゲットをアンダーシュートし、短い距離の衝動性動眼がターゲットをオーバーシュートする傾向は、ユーザの意図をより迅速に特定する要素として予測され得る。 Recognizing the tendency to undershoot or overshoot a target based on the distance traveled during the impulsive occlusal eye, such as an impulsive occlusal eye directed at a working target (eg, a "start" or "public" target). It is especially useful for intentional eye movements that convey intent. Furthermore, as mentioned above, the impulsive oculomotor eye is "shocking" in nature, so that the distance traveled and the position of gaze can be predicted. Therefore, the tendency of long-distance impulsive occlusal eyes to undershoot the target and short-distance impulsive occlusal eyes to overshoot the target can be predicted as a factor that more quickly identifies the user's intention.
更なる実施形態において、意図的な衝動性動眼の意図したターゲット位置は、測定された（予測された）衝動性動眼の注視位置および衝動性動眼が長い角度距離（例えば＞１０°）を移動したか否かに基づいて計算されてもよい。衝動性動眼が長い角度距離を移動した（または移動すると予測された）場合、測定される衝動性動眼と同一方向（所定の方向範囲内）であるが、衝動性動眼の注視位置を（最大約２０％）超える選択ターゲットは、「選択された」と見なすことができる。このような選択は、その後にターゲットに近づく任意の矯正的な衝動性動眼の前に行うことができ（すなわち、時間を節約する）、またはユーザの視線方向が実際に意図したターゲットに到達していない場合でも行うことができる（すなわち、厳密性を選択的に調整する）。 In a further embodiment, the intended target position of the intentional impulsive eye is the measured (predicted) gaze position of the impulsive eye and the impulsive eye moved a long angular distance (eg> 10 °). It may be calculated based on whether or not. When the occlusal eye moves (or is predicted to move) a long angular distance, it is in the same direction (within a predetermined directional range) as the measured impulsive eye, but the gaze position of the impulsive eye is (up to about about). Selection targets that exceed 20%) can be considered "selected". Such a choice can then be made in front of any corrective impulsive oculomotor eye that approaches the target (ie, saves time), or the user's gaze direction actually reaches the intended target. It can be done even if it is not (ie, the rigor is selectively adjusted).
同様に、衝動性動眼が短い角度距離（例えば、＜５°）で移動した（または移動すると予測された）場合、測定される衝動性動眼と同一方向（所定の方向範囲内）であるが、衝動性動眼によって（最大約２０％）バイパスされた選択ターゲットは、「選択された」と見なすことができる。このような選択は、ターゲットに近づく任意の矯正的な衝動性動眼の前に行うことができ（すなわち、時間を節約する）、またはユーザの視線方向が実際に意図したターゲットに到達していない場合でも行うことができる（すなわち、厳密性を選択的に調整する）。 Similarly, if the impulsive occlusal eye moves (or is predicted to move) over a short angular distance (eg, <5 °), it is in the same direction (within a predetermined directional range) as the measured impulsive occlusal eye. Selected targets that are bypassed by the impulsive oculomotor eye (up to about 20%) can be considered "selected". Such a choice can be made in front of any corrective impulsive oculomotor eye approaching the target (ie, saving time), or if the user's gaze direction does not actually reach the intended target. Can also be done (ie, selectively adjust the rigor).
このような方法は、意図的な眼球運動に基づいて、装置ユーザの意図のより正確且つ迅速な識別を支援することができる。 Such a method can assist in more accurate and rapid identification of the device user's intentions based on intentional eye movements.
拡張特徴を有する追跡による選択
視覚解像度（visual resolution）は、画面（例えば、モニタ、ＨＭＤ、タブレット）上に表示されたオブジェクト（例えば、テキスト、記号、スプレッドシートのセル）の特徴サイズを決定するための基準として頻繁に使用される。注視解像度（gaze resolution、すなわち、視線方向に基づいて位置を特定する能力）は、一般的に視覚解像度よりも少なくとも１桁低い。これによって、ユーザの視覚解像度内にあるが、ユーザの注視解像度よりも小さいサイズを有する特徴またはオブジェクトから、視覚信号により１つ以上の位置を選択しなければならない場合、特定の視覚信号言語方法が必要とされる。この状況は、例えば、スプレッドシート内のセルを編集する場合、１次元リストまたは２次元アイコン列にオブジェクトを挿入する場合、または文字列内の単語を置換する場合に発生する。
Selection by tracking with enhanced features Visual resolution is for determining the feature size of objects (eg text, symbols, spreadsheet cells) displayed on the screen (eg monitors, HMDs, tablets). Frequently used as a standard for. Gaze resolution (the ability to locate based on gaze direction) is generally at least an order of magnitude lower than visual resolution. This provides a particular visual signal language method when one or more positions must be selected by the visual signal from features or objects that are within the user's visual resolution but have a size smaller than the user's gaze resolution. Needed. This situation occurs, for example, when editing a cell in a spreadsheet, inserting an object into a one-dimensional list or two-dimensional icon string, or replacing a word in a string.
眼球運動の検出および使用は、キーボードまたはマイクを使用する他の入力モードと併用されると、これらの例示的な状況に特に強力である。例えば、スプレッドシートまたはリストに値を入力する場合、モダリティ（例えば、タイピングまたは音声）からデータ入力を抽出すると同時に、視線でスプレッドシートまたはリスト内の位置を指定することを可能にすることによって、データ入力の効率を大いに改善することができる。 The detection and use of eye movements is particularly powerful in these exemplary situations when used in combination with other input modes that use a keyboard or microphone. For example, when entering a value in a spreadsheet or list, the data can be extracted from the modality (eg typing or voice) while at the same time allowing the line of sight to specify its position in the spreadsheet or list. Input efficiency can be greatly improved.
例示的な実施形態において、選択可能な特徴が注視解像度よりも小さい状況において視覚信号による選択を可能にする方法は、注視されている領域を拡大または徐々に「ズームイン」することを含む。拡大は、ビデオシーケンスに対して行われた拡大の逐次増加を含む。すなわち、領域および領域内のオブジェクトは、滑動性追跡眼球運動、一連の短い追跡衝動性動眼、またはこれら２つの組み合わせを用いて、特定のオブジェクトまたは拡大されている領域を追跡できる倍率内で拡大される。換言すれば、領域の拡大率のビデオ表示は、眼球運動が領域内のオブジェクトまたは点を容易に追従することができる径方向速度で動くように制限される。一般的な基準として、拡大されている領域内のオブジェクトの移動速度は、約３０°／秒未満に制限される。 In an exemplary embodiment, a method of allowing visual signal selection in situations where the selectable features are less than the gaze resolution comprises enlarging or gradually "zooming in" the gaze area. The enlargement includes a gradual increase of the enlargement made to the video sequence. That is, the area and the objects within the area are magnified within a magnification that can track a particular object or the magnified area using a gliding pursuit eye movement, a series of short pursuit impulsive eye movements, or a combination of the two. To. In other words, the video display of the magnification of the area is restricted so that the eye movements move at a radial velocity that can easily follow an object or point in the area. As a general rule, the speed of movement of objects within the expanded area is limited to less than about 30 ° / sec.
領域の拡大または拡張を実行している間に、ユーザは、領域内のオブジェクトを追跡することによって、そのオブジェクトの選択を示すことができる。オブジェクトが所定の時間または距離に亘って追跡された場合、オブジェクトの選択を示すことができ、スイッチまたはキーボードの押下、またはユーザによって生成された可聴信号（例えば、キーワード、語句、クリック）などの他の意図表示を行った場合、オブジェクトの選択を示すことができる。 While performing an area expansion or expansion, the user can indicate a selection of objects in the area by tracking them. If an object is tracked over a given time or distance, it can indicate the selection of the object, such as a switch or keyboard press, or a user-generated audible signal (eg, keyword, phrase, click), etc. When the intention of is displayed, it is possible to indicate the selection of the object.
図３０Ａおよび図３０Ｂは、拡大中に追跡眼球運動を用いてスプレッドシート７００内の特定の「セル」を選択するプロセスを示している。図３０Ａに示すように、例示的なスプレッドシート７００は、ドル値を囲む多くの要素または「セル」７０１を含む。スプレッドシート７００内の各セルのサイズは、実用の時間量（例えば、＜１秒）にユーザの視線を測定できる解像度よりも小さい。（球体７０２ａによって示された）視線の推定位置は、スプレッドシート７０３の左上隅から下方へ２セルと右側へ２セルとの間の領域にある。現時点では、編集（または他の目的）を行うためのセルの特定が不可能である。
30A and 30B show the process of selecting a particular "cell" in
その後、個々のオブジェクトを追跡できる速度で、視線の推定位置７０２ａの周りの領域を拡大する。図３０Ｂは、拡大処理中の単一のフレームを示している。拡大された領域７０４は、元のスプレッドシート７００の一部の要素およびオブジェクトを含む。この図において、スプレッドシート内の特定のセルを追跡するための眼球運動の一般的な方向は、矢印７０５によって示される。ユーザの追跡性眼球運動によって、スプレッドシート７０６ａ内の特定のセル、例えば、値＄５．９８を含むセルを特定することができる。セルを特定した後、ユーザは、必要に応じて、セルに対して修正（例えば、削除、編集、コピー、貼り付け）を行うことができる。
Then, the area around the estimated
領域を徐々に拡大する場合、領域内のオブジェクトおよび特徴は、拡大の中心から離れ、追跡の最大速度を超えてディスプレイ上で移動し始めることがある。この状況に対処するために、時間の関数として、または必要に応じて、領域内の特徴の大きさおよび相対位置、ユーザの経験および／または好みなどの他の因子および時間の関数として、拡大速度または「ズーム」速度を変更することができる。例えば、拡大処理の後期に拡大率を小さくすることによって、拡大される領域の周辺の近くのオブジェクトの速度は、滑動性追跡眼球運動および／または短い衝動性動眼に基づく視線追跡の生理限界内に制限することができる。 When the area is gradually expanded, objects and features within the area may move away from the center of expansion and begin to move on the display beyond the maximum tracking speed. To address this situation, as a function of time, or optionally, as a function of other factors and time, such as the size and relative position of features within the region, the user's experience and / or preference, and the rate of expansion. Or you can change the "zoom" speed. For example, by reducing the magnification later in the enlargement process, the velocity of objects near the periphery of the area to be enlarged is within the physiological limits of gliding tracking eye movements and / or gaze tracking based on short impulsive oculomotor eyes. Can be restricted.
別の例示的な実施形態において、異なる方向の拡大率は、異なってもよい。例えば、殆どのユーザにとって、水平方向の眼球運動の範囲は、垂直方向の眼球運動の範囲よりも大きい。したがって、領域の拡大は、垂直方向に比べて水平方向により大きくしてもよい。表示された領域のエッジまたはコーナーに近い領域またはユーザの中心窩視野の領域を拡大する場合に、同様の方法を適用することができる。例えば、エッジまたはコーナーに近い辺または領域は、他の方向の拡大に比べてより遅く拡大されてもよい。その結果、オブジェクトが表示可能な領域から離れる前に、ユーザがオブジェクトを選択するための表示時間が長くなる。 In another exemplary embodiment, the magnification in different directions may be different. For example, for most users, the range of horizontal eye movements is larger than the range of vertical eye movements. Therefore, the expansion of the area may be larger in the horizontal direction than in the vertical direction. A similar method can be applied to magnify the area near the edges or corners of the displayed area or the area of the user's foveal field of view. For example, an edge or region near an edge or corner may grow slower than it grows in other directions. As a result, the display time for the user to select an object is increased before the object leaves the viewable area.
拡大の速度および径方向プロファイルに加えて、拡大処理を左右するもう１つの重要な変数は、拡大の中心位置である。例示的な実施形態において、この中心は、現在の注視位置、最近の注視位置の移動平均（または他の統計値）、または以前に指定された領域、例えば、所謂「公開」操作中に特定された位置であってもよい。 In addition to the expansion speed and radial profile, another important variable that influences the expansion process is the center position of the expansion. In an exemplary embodiment, this center is identified during a current gaze position, a moving average (or other statistic) of a recent gaze position, or a previously specified area, eg, a so-called "open" operation. It may be in the same position.
代わりに、視覚信号言語要素（例えば、「公開」または他のメニュー選択）を使用して、眼球運動を連続拡大モードにすることができる。このモードにおいて、ディスプレイの領域は、表示されると同時に、拡大される。拡大しているオブジェクトに追従する場合、そのオブジェクトに基づいて選択を行う。それ以外の場合、画面の異なる領域が注視される時に、選択を行うまでまたは拡大モードをオフにするまで、（通常、最初にゆっくりと）拡大を開始する。 Alternatively, a visual signal language element (eg, "publish" or other menu selection) can be used to put the eye movement into continuous magnifying mode. In this mode, the area of the display is expanded as soon as it is displayed. If you want to follow an expanding object, make a selection based on that object. Otherwise, when different areas of the screen are gazed, the enlargement begins (usually slowly at the beginning) until a selection is made or the enlargement mode is turned off.
上述の拡大される領域を示す方法に拘わらず、ディスプレイ上のオブジェクトの文脈を用いて、拡大中心の位置を調整または「微調整」することもできる。このような調整は、（拡大中心で引き伸ばされたオブジェクトの一部分ではなく）オブジェクトの全体がユーザの最初の注視方向から異なる方向に沿って外側に放射状に拡大されるように、行われててもよい。 Regardless of the method of indicating the magnified area described above, the context of the object on the display can also be used to adjust or "fine-tune" the position of the magnifying center. Even if such adjustments are made so that the entire object (rather than a portion of the object stretched at the center of magnification) is radiated outward along a different direction from the user's initial gaze direction. Good.
拡大中心の調整は、径方向外側の眼球運動が拡大される全てのオブジェクトを（同様に良好に）追従するように実行されてもよい。このような調整は、特定のオブジェクトが（すなわち、中心にある）移動しないため、または多くの選択可能な近接オブジェクトにクラスタ化されているため、または最初からユーザの注視方向から離れているため、特定のオブジェクトを選択し易いまたは難い状況を避けることができる。 The adjustment of the center of magnification may be performed so that the radial lateral eye movement follows all objects to which it is magnified (similarly well). Such adjustments are due to the fact that a particular object does not move (ie, is in the center), or is clustered into many selectable proximity objects, or because it is far from the user's gaze direction from the beginning. You can avoid situations where it is easy or difficult to select a particular object.
拡大中心の調整は、スプレッドシートから特定のセルを選択するプロセスを用いて、容易に説明することができる。拡大中心を（各セルに対して）スプレッドシートセルの水平方向および垂直方向の両方の中間に位置するように調整することによって、選択可能なセルを概ねユーザの注視位置から径方向に沿って拡大することができる。一例として、図３０Ａに示すように、ユーザの注視位置７０２ａからの拡大中心を、スプレッドシート７００内の水平セルおよび垂直セルの境界７０２ｂの近傍の交点に調整することができる。図３０Ｂに示すように、拡大処理中に、この交点７０２ｂに隣接するセル７０６ａ、７０６ｂ、７０６ｃ、７０６ｄ（および他の遠く離れているセル）のいずれかを追跡および選択することができる。
Adjustment of the center of expansion can be easily explained using the process of selecting specific cells from the spreadsheet. By adjusting the center of expansion (for each cell) to be in the middle of both the horizontal and vertical directions of the spreadsheet cells, the selectable cells are expanded approximately radially from the user's gaze position. can do. As an example, as shown in FIG. 30A, the center of expansion from the user's
拡大領域内のオブジェクトを追跡することによって選択を指定する能力は、視覚信号言語の基本的要素である。この方法は、特に視覚信号言語と互換性があるように設計されていない状況においても使用され得る。例えば、（視覚信号に関係なく設計された）ウェブ上のページを閲覧する場合、ユーザは、特定のエリアの拡大および／または選択したい場合がある。このエリアは、１つ以上の選択、ハイパーリンクおよび／またはデータ入力ポイントを含む可能性がある。拡大処理中に眼球運動を用いて所望の要素を追跡することによって、所望の選択、ハイパーリンク、またはデータエントリポイントを有効化することができる。 The ability to specify choices by tracking objects in the magnified area is a fundamental element of visual signal language. This method can also be used in situations where it is not specifically designed to be compatible with visual signal languages. For example, when browsing a page on the web (designed regardless of the visual signal), the user may want to zoom in and / or select a particular area. This area may contain one or more selections, hyperlinks and / or data entry points. The desired selection, hyperlink, or data entry point can be activated by tracking the desired element using eye movements during the magnifying process.
拡大領域からの選択を行った後、ユーザ対話を進めるために、（コンテキストによって）いくつかの可能なフィードバックモダリティを使用することができる。例えば、１）直ちにディスプレイを拡大直前の状態に戻すことができ、選択された要素を示すように、必要に応じて指示（例えば、フォント、色、輝度の変化）を含むことができる。２）拡大領域は、入力（スプレッドシートのセル、テキストなど）が完了するまで、拡大されたままに維持される。３）更なる選択（すなわち、サブメニューの選択）を可能にするために、追加の移動、拡大および／または選択可能なオブジェクトを拡大領域内に導入してもよい。４）選択の結果によって、画面の内容を別の画面に切換えることができる。 After making a selection from the magnified area, some possible feedback modality (depending on the context) can be used to facilitate the user dialogue. For example, 1) the display can be immediately returned to its pre-magnification state and can optionally include instructions (eg, changes in font, color, brightness) to indicate the selected element. 2) The expanded area remains expanded until the input (spreadsheet cells, text, etc.) is completed. 3) Additional moveable, magnified and / or selectable objects may be introduced within the magnified area to allow for further selection (ie, submenu selection). 4) Depending on the result of selection, the contents of the screen can be switched to another screen.
重合されたマーカによる視線選択の増強
更なる例示的な実施形態において、鋭敏な視覚解像度と比較的に低い視線追跡解像度との間の不一致の問題に対処するために、重合されたマーカおよび補助入力を測定される注視位置に結合することによって、ユーザ選択および／作動を効率的に行うことができる。補助入力は、Ｎ個の選択肢から１つの選択肢を指定することができる任意の装置、例えば、キーボード、キーパッド、コンピュータマウス、トラックボール、音声認識用いて特定されたキーワードのうち１つ、スイッチ、点頭、指ジェスチャーまたはハンドジェスチャーなどであってもよい。
Enhancement of line-of-sight selection with polymerized markers In a further exemplary embodiment, polymerized markers and auxiliary inputs are used to address the problem of discrepancies between sensitive visual resolution and relatively low line-of-sight tracking resolution. By coupling to the gaze position to be measured, user selection and / operation can be performed efficiently. Auxiliary inputs are any device that can specify one of N choices, such as a keyboard, keypad, computer mouse, trackball, one of the keywords identified using speech recognition, a switch, and so on. It may be a head, a finger gesture, a hand gesture, or the like.
視線を用いて、ディスプレイから、ユーザの関心領域の大体位置を指定することができる。眼球運動を用いてこのような領域を特定することは、最小の認知負荷で、迅速に（すなわち、最大９００°／秒の角速度で）領域の大体位置を指定する目の能力を利用している。大体位置が単一の選択可能な項目を含む場合、視覚信号言語内の典型的な作動シーケンス（例えば、「スタート」または「公開」シーケンス）を用いて、作動を進めることができる。しかしながら、視線追跡解像度を超えるまたはそれに近い複数の選択が視線領域に存在する場合、ユーザが特定の選択を示すために追加のステップを行う必要がある。この状況の例として、スプレッドシート内の１つ以上のセル、テキスト本体内の（例えば、削除または挿入）位置、サムネイル画像またはアイコンのグリッド、単語の選択列、チェックリストまたは他の連続項目、ポップダウンメニューなどを挙げることができる。 The line of sight can be used to specify the approximate location of the user's area of interest from the display. Identifying such regions using eye movements utilizes the ability of the eye to quickly (ie, at angular velocities up to 900 ° / sec) roughly position the region with minimal cognitive load. .. If the approximately position contains a single selectable item, the operation can be advanced using a typical operation sequence within the visual signal language (eg, a "start" or "public" sequence). However, if there are multiple selections in the line-of-sight area that exceed or are close to the line-of-sight tracking resolution, additional steps need to be taken by the user to indicate a particular selection. Examples of this situation are one or more cells in a spreadsheet, position (eg, delete or insert) in the body of the text, a grid of thumbnail images or icons, a word selection column, a checklist or other contiguous item, pop. Down menu etc. can be mentioned.
複数の可能な選択肢を含むディスプレイ上の注視領域は、「候補固視領域」（ＣＦＡ）と呼ばれる。ユーザがＣＦＡを短時間固視すると、ＣＦＡ内の全ての可能な選択肢にマーカを配置することができる。便宜上、マーカは、「重合された」と呼ばれる。しかしながら、マーカは、部分的に透明にされまたは背景に重合されてもよく、ＣＦＡ内のオブジェクトと区別する色または輝度レベルから作成されまたは輪郭だけで示されてもよく、および／またはＣＦＡ内のオブジェクトを完全に遮蔽しないように、類似の方法で示されてもよい。 A gaze area on a display that contains a plurality of possible options is referred to as a "candidate fixation area" (CFA). When the user gazes at the CFA for a short time, markers can be placed on all possible options within the CFA. For convenience, the markers are referred to as "polymerized". However, the markers may be partially transparent or superposed on the background, may be created from a color or brightness level that distinguishes them from the objects in the CFA, or may be indicated only by contours, and / or within the CFA. It may be shown in a similar way so as not to completely obscure the object.
代替的には、ユーザによって生成された任意の信号を用いて、重合処理を開始することができる。このような信号の例として、キーボード上の任意のキー（例えば、スペースバー）を押すこと、キーワードを発声すること、スイッチを押すことなどを含む。 Alternatively, any signal generated by the user can be used to initiate the polymerization process. Examples of such signals include pressing any key on the keyboard (eg, the space bar), uttering a keyword, pressing a switch, and the like.
ＣＦＡに重合されたマーカの例として、数字、文字、（殆どのキーボードに示される）特殊機能、方向矢印、記号、異なる色の斑点、音声を表すオブジェクトなどが挙げられる。ＣＦＡ内の特定の要素の選択は、必要に応じてイベントの作動と共に、所望の要素の上面に重合されたマーカに基づいて指定することによって実行される。例えば、重合されたマーカが数字を含む場合、所望の選択に重合されている（例えば、キーボードまたは数字キーパッド上の）数字を入力することは、選択処理によって（重合された数字の下方の）要素を選択することを示す。 Examples of markers polymerized on the CFA include numbers, letters, special features (shown on most keyboards), direction arrows, symbols, spots of different colors, objects representing sounds, and so on. The selection of a particular element within the CFA is carried out by specifying based on the markers superimposed on the top surface of the desired element, with the action of the event as needed. For example, if the polymerized markers contain numbers, entering the numbers that are polymerized (eg, on the keyboard or number keypad) to the desired selection is by the selection process (below the polymerized numbers). Indicates to select an element.
代わりにまたは組み合わせて、重合された文字または色に対応するキーを打つことによって、または重合された記号または単語に対応する音声を発声することによって、選択を行うことができる。記号が方向（例えば、左、右、左上など）を表す場合、特定のキー（例えば、「矢印」キー）または多数のキーのいずれかを使用して、所定の方向を示すことができる。後者の例として、重合された記号が右上方向を示す場合、右上の選択を示すために使用されるキーは、標準の「ＱＷＥＲＴＹ」キーボード上の（右上領域に位置する）「Ｐ」または「Ｏ」キーである。 Selections can be made instead or in combination by hitting the key corresponding to the polymerized letter or color, or by uttering the voice corresponding to the polymerized symbol or word. If the symbol represents a direction (eg, left, right, top left, etc.), either a particular key (eg, the "arrow" key) or a number of keys can be used to indicate a given direction. As an example of the latter, if the superimposed symbol points to the upper right, the key used to indicate the upper right selection is the "P" or "O" (located in the upper right area) on a standard "QWERTY" keyboard. Is the key.
図３１Ａは、例えばテキストドキュメントの形成中にフォント特性およびカット＆ペーストを制御するために使用され得る９個のワード選択肢からなる３×３グリッド７５０の一例を示している。この例示的な事例において、９個の選択肢からなるＣＦＡグリッド７５０は、ディスプレイの小さい領域で邪魔にならないように形成することができる。測定された注視位置の不確定性は、円形「雲」７５２として示され、グリッド７５０内の各選択要素（例えば、７５１ａ）のサイズを超えている。したがって、単一の眼球運動に基づいて、選択を行うことができない。
FIG. 31A shows an example of a
図３１Ｂは、図３１Ａに示されたＣＦＡグリッド７５０に重合された９個の選択可能な数字（例えば、７５３ａ、７５３ｂ）を示している。マーカ数字（例えば、７５３ａ、７５３ｂ）は、典型的な数字キーパッドと同様の配置で配置される。キーパッドまたはキーボードが利用可能でない代替の例示的な実施形態において、数字を発話し、その後、当技術分野に周知の音声認識方法を用いて認識することができる。９個の数字うち１つを選択すると、重合数字の下方の単語に関連する操作を実行することができる。例えば、図３１Ｂに示すテーブルを用いて、「７」を選択した（７５３ｂ）場合、後のテキストを入力するための「太字」属性７５４ｂがオンまたはオフに切り換えられる。同様に、「１」を選択した（７５３ａ）場合、システムは、テキスト入力中の文字または単語を「削除」する（７５４ａ）。
FIG. 31B shows nine selectable numbers (eg, 753a, 753b) polymerized on the
図３１Ｃは、図３１Ａに示された３×３選択肢からなるＣＦＡグリッド７５０に重合された記号の代替セットを示している。この場合、方向矢印（例えば、７５５ａ）が可能な選択肢に重合される。その後、方向矢印に関連する個々のキーを押すことによって、またはキーボードまたは他のデータ入力装置上のある基準点に対して矢印の方向にある任意のキーを押すことによって、個々の選択を行うことができる。例えば、標準のＱＷＥＲＴＹキーボードの中心を基準点として使用する場合、文字「Ａ」または「Ｓ」を用いて、図３１Ｃに示されたＣＦＡグリッド内の「取り消し線」機能に関連する最も左側の選択を示すことができる。
FIG. 31C shows an alternative set of symbols polymerized on the
選択を行った後、重合されたマーカを消すことができ、および／または同様のキーボードまたは補助装置を使用した追加のエントリを入力として特定の要素に提供することができる。例えば、データ入力プロセスの一部として、英数字をスプレッドシート内の特定のセルに入力することができる。所定の時間で表示した後（すなわち、選択処理を開始しない場合）、またはユーザがディスプレイの別の領域を注視する場合、重合されたマーカを消してもよい。 After making the selection, the polymerized markers can be erased and / or additional entries using a similar keyboard or auxiliary device can be provided as input to the particular element. For example, you can enter alphanumeric characters into specific cells in a spreadsheet as part of the data entry process. After displaying at a predetermined time (ie, if the selection process is not started), or if the user gazes at another area of the display, the polymerized markers may be erased.
追加の実施形態において、ディスプレイ上の複数の位置で記号を繰り返すことによってまたは複数の選択可能な項目に（大きな）記号を重ね合わせることによって、重合された各記号に複数の選択を関連付けることができる。この場合、（選択可能な項目が操作を表す場合）複数の操作を実行することができ、複数の項目に操作を適用することができる。重合されたマーカによって増強された視線選択は、何回でも繰り返すことができる。 In additional embodiments, multiple selections can be associated with each superimposed symbol by repeating the symbol at multiple locations on the display or by overlaying the (large) symbol on multiple selectable items. .. In this case, a plurality of operations can be executed (when the selectable item represents an operation), and the operation can be applied to a plurality of items. The line-of-sight selection enhanced by the polymerized markers can be repeated any number of times.
視線較正および眼の優位性
較正の重要要素は、片目または両目の１つ以上の特徴（例えば、瞳孔、角膜輪部）の位置を、ディスプレイ（例えば、ＨＭＤ、モバイル装置、リモートモニタ）上の注視位置、または装置ユーザの実際の環境内の注視位置に変換するためのマッピングスキーム（例えば、１つ以上の数学関数、ルックアップテーブル、補間方法）を形成することである。較正スキームは、照明源、基準物および／または頭部装着装置上のカメラの既知の相対位置または測定の相対位置に加えて、片目または両目の解剖モデルに基づくことができる。集団平均、装置ユーザの既知特徴（例えば、年齢、民族起源）に基づいた推定、（例えば、較正手順中に）測定された装置着用者の特性、またはこれらのスキームの組み合わせに基づいて、この解剖モデルのパラメータを選定することができる。
Eye Calibration and Eye Superiority An important element of calibration is the location of one or more features of one or both eyes (eg, pupil, corneal ring), gaze on a display (eg, HMD, mobile device, remote monitor). Forming a mapping scheme (eg, one or more mathematical functions, a look-up table, an interpolation method) for transforming a position or gaze position in the device user's actual environment. The calibration scheme can be based on an anatomical model of one or both eyes, in addition to the known relative position or measurement relative position of the camera on the illumination source, reference and / or head-mounted device. This dissection is based on population means, estimates based on known characteristics of the device user (eg, age, ethnic origin), measured device wearer characteristics (eg during the calibration procedure), or a combination of these schemes. You can select the parameters of the model.
例示的な実施形態において、殆どの録画において両目を追跡するときに、較正および視線追跡スキームに優位眼を考慮する必要がある。優位眼とは、視覚入力を行う時に、他方の目よりも、一方の目を利用する傾向である。優位眼と利き手との間に相関がある。約７０％の人は、右優位眼である。いくつかの場合（例えば、野球を打つ場合の内斜視および利き手）、眼帯または光ディフューザを用いて、優位眼を変更することが可能である。約３０％の人は、左優位眼である。僅かな数の人は、眼の優位性を示さない。 In an exemplary embodiment, it is necessary to consider the dominant eye in the calibration and gaze tracking scheme when tracking both eyes in most recordings. The dominant eye is the tendency to use one eye rather than the other when performing visual input. There is a correlation between the dominant eye and the dominant hand. About 70% of people have a right-dominant eye. In some cases (eg, esotropia and dominant hand when hitting baseball), an eyepatch or optical diffuser can be used to change the dominant eye. About 30% of people have a left-dominant eye. A few people do not show eye dominance.
眼の優位性の最も極端な例として、極端な角度で見るときに、鼻梁は、見ている領域から最も離れた目の視界を物理的に妨げることがある。しかしながら、殆どの人は、この角度に到達する前に、よく優位眼をシフトする。通常右優位眼の人は、視界中心の左側を見る時に、左目で見るように切換える。同様に、通常左優位眼の人は、視界中心の右側の領域を見る時に、右目で見るように切換える。個人の間に広い変動が存在するが、平均的に、優位眼から反対側の眼の切り換えは、視野中心から約１５°で行われる。 As the most extreme example of eye dominance, the bridge of the nose can physically obstruct the view of the eye farthest from the area of view when viewed at extreme angles. However, most people often shift their dominant eye before reaching this angle. Normally, a person with a right-dominant eye switches to see with the left eye when looking at the left side of the center of vision. Similarly, a person with a left-dominant eye usually switches to see with the right eye when looking at the area to the right of the center of vision. Although there are wide variations between individuals, on average, switching from the dominant eye to the opposite eye occurs at about 15 ° from the center of the visual field.
優位眼を考慮していない場合、非利き眼からのベクトルまたは両目の間の位置または両目の周りの位置に基づいて視線方向を決定する時に、ユーザが認知的に選択したオブジェクトを誤って特定してしまう可能性がある。視線追跡のために装着型装置に使用された近眼ディスプレイ、イルミネータ、および検出装置にとって、優位眼を考慮することは、特に重要である。現在利用可能な視線追跡システムは、左右の眼の測定値の平均値（場合によって、重み付け平均値）を使用する。これらのシステムは、優位眼の空間（特に水平方向）依存性を考慮していない。したがって、較正およびその後の注視位置の決定は、注視位置の関数としての眼の優位性を考慮しなければならず、精確な（例えば、＜２°）注視位置を決定するために個人間の幅広い変動を考慮しなければならない。 Without considering the dominant eye, the user incorrectly identifies the object cognitively selected when determining the gaze direction based on the vector from the non-dominant eye or the position between the eyes or around the eyes. There is a possibility that it will end up. It is especially important to consider the dominant eye for myopia displays, illuminators, and detectors used in wearable devices for eye tracking. Currently available eye tracking systems use the average of left and right eye measurements (possibly a weighted average). These systems do not take into account the spatial (especially horizontal) dependence of the dominant eye. Therefore, calibration and subsequent determination of gaze position must take into account the superiority of the eye as a function of gaze position and is widespread among individuals to determine an accurate (eg <2 °) gaze position. Fluctuations must be taken into account.
例示的な実施形態において、パターンに従って移動しているオブジェクトの連続的な追跡を用いて、１）較正時に使用される（上記で説明した）マッピングスキームのパラメータを決定することができ、２）優位眼を一方の目から他方の目にシフトする遷移ゾーンの位置を決定することができる。円形、矩形、一連の線分、多角形、または丸めたエッジを有する多角形などの幅広いパターンのうち、１つ以上に従って移動している焦点の追跡を使用することができる。 In an exemplary embodiment, continuous tracking of objects moving according to a pattern can be used to 1) determine the parameters of the mapping scheme (described above) used during calibration and 2) dominance. The position of the transition zone that shifts the eye from one eye to the other can be determined. Focus tracking can be used that is moving according to one or more of a wide range of patterns, such as circles, rectangles, series of lines, polygons, or polygons with rounded edges.
図３２は、「無限大記号」（「連珠形」または「横向きの数字８」とも呼ばれる）のパターン内の焦点の追跡を示している。図３２の破線７２０で表された連珠形の一部は、より多くの点をディスプレイ７２１の水平遠方範囲に配置することによって、水平方向に利用可能なより広い範囲の眼球運動を利用する。この例において、装置着用者は、高明暗度の焦点を含む移動ディスク７２３の中央領域を追従するように指示される。この焦点は、ユーザの注意力を移動ディスク７２３の中心に集中させることができる。装置着用者は、（矢印７２４で示すように）パターンを１回以上繰り返して、連珠形７２０を追跡する。焦点の移動速度は、予め決められてもよく、または例えばディスプレイ７２１の遠方範囲内のより遅い眼球運動を可能にするように、時間および／または位置の関数として変化してもよい。
FIG. 32 shows focus tracking within a pattern of "infinity symbols" (also called "renju" or "
曲線近似を用いて、測定された眼球位置（例えば、瞳孔または角膜輪部の中心）をディスプレイの位置に変換するマッピング関数の「最適」パラメータを決定することができる。例えば、マッピング関数は、回転範囲が制限された一次式であってもよく、多項式であってもよい。勾配降下などの技術を用いた曲線近似は、当該技術分野において周知である。 Curve approximation can be used to determine the "optimal" parameter of the mapping function that transforms the measured eye position (eg, the center of the pupil or corneal ring) to the position of the display. For example, the mapping function may be a linear expression with a limited rotation range or a polynomial. Curve approximation using techniques such as gradient descent is well known in the art.
較正時に、視線追跡は、表示されたパターン（ｘ、ｙ）の座標が以下のように表される時間の関数として進行する。 At the time of calibration, the line-of-sight tracking proceeds as a function of time when the coordinates of the displayed pattern (x, y) are represented as follows.
（ｘ，ｙ）＝ｆ（ｔ）
式中、ｆ（ｔ）は、追跡パターン（例えば、連珠形、円、長方形）を定義する式である。上述した拡大領域内のオブジェクトの追跡と同様に、ターゲットの速度は、滑動性追跡眼球運動、一連の短い追跡衝動性動眼、またはこれらの２つの組み合わせを用いて追跡できるように制限される。
(X, y) = f (t)
In the formula, f (t) is a formula that defines a tracking pattern (for example, a bead shape, a circle, or a rectangle). Similar to the tracking of objects in the magnified area described above, the speed of the target is limited so that it can be tracked using a sliding tracking eye movement, a series of short tracking impulsive eyes, or a combination of the two.
追跡パターンおよび速度に関係なく、追跡ターゲット（図３２の７２３）を表示する時間と、装置ユーザが視覚反応をした時間との間に遅延が存在する。この遅延Δｔは、画像から目の位置座標（ｘｉ，ｙｉ）を取得するときのハードウェアおよびソフトウェアの遅延だけでなく、視神経を介して網膜上で検出された信号を視覚野に伝達する「生物学的」遅延、装置着用者が視覚情報を処理するための認知時間、眼球を動かす筋肉に運動信号を送る時間、および筋肉が力を生成し、その後に検出される眼球運動を引き起こすための時間を含むことができる。目のターゲット位置座標（ｘｔ，ｙｔ）は、次のように表すことができる。 Regardless of the tracking pattern and speed, there is a delay between the time the tracking target (723 in FIG. 32) is displayed and the time the device user makes a visual response. The delay Δt, the position coordinates of the eyes from the image (x i, y i) as well hardware and software delays when acquiring the transmits the detected signal on the retina through the optic nerve to the visual cortex "Biological" delay, cognitive time for the device wearer to process visual information, time to send motion signals to the muscles that move the eye, and because the muscles generate force and cause subsequent detected eye movements. Time can be included. The target position coordinates (x t , y t ) of the eye can be expressed as follows.
（ｘｔ，ｙｔ）＝ｆ（ｔ−Δｔ）
したがって、目の測定位置を注視位置にマッピングするために使用される最適パラメータに加えて、システム遅延Δｔは、最初に不明である。例示的な実施形態において、初期の曲線近似を行うために、システム平均値に基づいて、初期値（例えば、３０ミリ秒）をΔｔに割り当てることができる。これによって、取得された目の位置座標（ｘｉ，ｙｉ）と初期ターゲット位置座標（ｘｔ，ｙｔ）との間の曲線近似を行うことができる。必要に応じて、マッピングされた目の位置座標（ｘｉ，ｙｉ）を図３２に示された追跡パターンに重合することができる。マッピングされた例示的な座標位置は、×（７２２）によって示される。
(X t , y t ) = f (t−Δt)
Therefore, in addition to the optimal parameters used to map the eye measurement position to the gaze position, the system delay Δt is initially unknown. In an exemplary embodiment, an initial value (eg, 30 ms) can be assigned to Δt based on the system average value to perform an initial curve approximation. As a result, it is possible to perform a curve approximation between the acquired eye position coordinates (x i , y i ) and the initial target position coordinates (x t , y t). If necessary, mapped eye coordinates (x i, y i) can be polymerized in the track pattern shown in Figure 32. An exemplary coordinate position mapped is indicated by x (722).
表示された目標パターン（ｘ，ｙ）と対応のマッピングされた座標との相互共分散を計算することによって、相互共分散のピークから、システムの遅延Δｔを測定する（必要におじて、平均化する）ことができる。この時点で、マッピングされた注視位置の外れ値（例えば、表示されたターゲットの全ての測定値の標準偏差の２倍を超えるもの）を廃棄することができる。この排除処理は、時間（すなわち、ｆ（ｔ−Δｔ））およびターゲットとマッピングされた注視座標との間の距離の両方を考慮する。外れ値を廃棄した後、最適パラメータの再決定し、システム遅延を再確定するプロセスを複数回に繰り返すことによって、例えば、目の瞬き、不正確な視線追跡の結果および／または装置着用者の不注意に起因する外れ値をさらに排除することができる。 Measure the system delay Δt from the peak of the mutual covariance by calculating the mutual covariance between the displayed target pattern (x, y) and the corresponding mapped coordinates (averaging, if necessary). can do. At this point, the outliers of the mapped gaze position (eg, more than twice the standard deviation of all measurements of the displayed target) can be discarded. This exclusion process takes into account both time (ie, f (t−Δt)) and the distance between the target and the mapped gaze coordinates. By repeating the process of redetermining the optimal parameters and redetermining the system delay multiple times after discarding the outliers, for example, blinking of the eyes, inaccurate eye tracking results and / or device wearer inattention. Outliers due to attention can be further eliminated.
例示的な実施形態において、上記のプロセスは、左目および右目に対して別々に実行されてもよい。左目または右目のみの使用を確実にするために、ターゲットを一方の目に投影するまたは他方の目によるターゲットの視認をブロックすることができる。その結果、左目および右目を表示座標に別々にマッピングするための較正パラメータ（およびΔｔ）を得ることができる。 In an exemplary embodiment, the above process may be performed separately for the left and right eyes. To ensure the use of only the left or right eye, the target can be projected into one eye or blocked from being seen by the other eye. As a result, calibration parameters (and Δt) for separately mapping the left and right eyes to the display coordinates can be obtained.
精確な較正を保証するために、追跡パターンを複数回に繰り返すことによって、曲線近似を行うための追加のデータポイントを取得することができる。（例えば、相関係数で示された）曲線近似の精度および／または廃棄された外れ値の数に依存して、追跡パターンを繰り返してもよい。 Additional data points for curve approximation can be obtained by repeating the tracking pattern multiple times to ensure accurate calibration. The tracking pattern may be repeated, depending on the accuracy of the curve fit (eg, indicated by the correlation coefficient) and / or the number of outliers discarded.
更なる例示的な実施形態において、ターゲットは、その後、両目に同時に投影されてもよい。この場合、（Δｔを考慮して）左目用の眼球位置および較正または右目用の眼球位置および較正が表示された目標パターンにより良く対応するか否かを決定する必要がある。このような比較に基づいて、ディスプレイの異なる領域に眼の優位性を示す視覚「マップ」を構成することができる。 In a further exemplary embodiment, the target may then be projected simultaneously to both eyes. In this case, it is necessary to determine (taking into account Δt) whether the eye position and calibration for the left eye or the eye position and calibration for the right eye correspond better to the displayed target pattern. Based on such comparisons, visual "maps" can be constructed that show the superiority of the eye in different areas of the display.
図３３は、視覚優性マップの一例を示している。この例において、目標パターン７３０内の対応する位置と一致する左目の注視位置は、「Ｌ」７３２ａによって表される。ディスプレイ７３１上の目標パターン７３０内の対応する位置と一致する右目の注視位置は、「Ｒ」７３２ｂによって表される。図３３には、３つの異なる領域、すなわち、１）左目の測定および較正がターゲット位置により正確にマッピングする領域７３４ａ、２）右目の測定および較正がターゲット位置により正確にマッピングする領域７３４ｃ、および３）優位眼が混在している「遷移ゾーン」７３４ｂが存在する。
FIG. 33 shows an example of a visual dominance map. In this example, the gaze position of the left eye that coincides with the corresponding position in the
このような優位眼領域の特定によって、眼の位置に依存して、マッピング操作を行うことができる。左目ゾーンに位置をマッピングする場合、左目の較正を使用してもよい。右目ゾーンに位置をマッピングする場合、右目の較正を使用してもよい。図３３に示されていないが、異なるゾーンを決定する際に、眼球の垂直位置を考慮することもできる。さらに、見られているオブジェクトの特徴（例えば、大きさ、輝度）が目の優位性に影響を与える可能性があるという経験的な証拠がある。ディスプレイの特性が既知（例えば、制御されている）場合、左目または右目の較正を使用すべきか否かを決定する際に、これらのディスプレイ特性を考慮してもよい。本明細書に記載のように、これらの考慮事項は、作動ターゲットを設計および表示する際に特に重要であり得る。 By identifying the dominant eye region in this way, the mapping operation can be performed depending on the position of the eye. When mapping the position to the left eye zone, left eye calibration may be used. When mapping the position to the right eye zone, right eye calibration may be used. Although not shown in FIG. 33, the vertical position of the eye can also be considered when determining different zones. In addition, there is empirical evidence that the characteristics of the object being viewed (eg, size, brightness) can affect eye dominance. If the characteristics of the display are known (eg, controlled), these display characteristics may be taken into account when deciding whether to use left-eye or right-eye calibration. As described herein, these considerations can be of particular importance in designing and displaying working targets.
図３３に示すように、左目または右目の較正プロセスの選択は、遷移ゾーンにおいて直ちに明らかにならない場合がある。例示的な実施形態において、任意の遷移ゾーンにいくつかの方法を使用することができる。
１．図３３に示すように、垂直ラインの右側の「Ｌ」７３３ａおよびラインの左側の「Ｒ」７３３ｂの総数を最小化するように、水平位置（すなわち、垂直ライン７３５）を計算することができる。この位置は、左優位眼および右優位眼を区別するための閾値として使用される。
２．左目および右目の選択ターゲットが遷移ゾーン内にある場合、左目マッピングおよび右目マッピングの両方を計算することができる。いずれかを選択する場合、選択された操作を行うことができる。このスキームは、不注意な作動をわずかに増加する可能性がある。
３．左眼較正および右眼較正の両方に基づいた平均位置を使用してもよい。
４．左目および右目のマッピングによって示された領域における較正中に左眼の較正位置および右眼の較正位置の重み付けが「Ｌ」および「Ｒ」位置の分布に依存する場合、重み付け平均を使用することができる。
As shown in FIG. 33, the choice of left-eye or right-eye calibration process may not be immediately apparent in the transition zone. In an exemplary embodiment, several methods can be used for any transition zone.
1. 1. As shown in FIG. 33, the horizontal position (ie, the vertical line 735) can be calculated to minimize the total number of "L" 733a on the right side of the vertical line and "R" 733b on the left side of the line. This position is used as a threshold to distinguish between the left-dominant eye and the right-dominant eye.
2. Both left-eye and right-eye mappings can be calculated if the left-eye and right-eye selection targets are within the transition zone. If you select either, you can perform the selected operation. This scheme can slightly increase inadvertent activity.
3. 3. An average position based on both left eye and right eye calibrations may be used.
4. If the weighting of the left eye calibration position and the right eye calibration position depends on the distribution of the "L" and "R" positions during calibration in the area indicated by the left and right eye mappings, the weighted average can be used. it can.
ユーザの視軸および視線の較正
所謂「光軸」が「視軸」と異なることは、当該技術分野において周知である。光軸は、光が角膜、瞳孔および水晶体の中心を通って、曲げずに、網膜を含む眼の後壁に到達する線である。視力（およびその後の認知）の中心である黄斑および中心窩は、この光軸から平均５°で鼻側に変位するため、精確な視線追跡を行うために、視軸を光軸から区別することが不可欠である。
Calibration of the user's visual axis and line of sight It is well known in the art that the so-called "optical axis" is different from the "optical axis". The optical axis is the line through which light passes through the center of the cornea, pupil, and lens and reaches the posterior wall of the eye, including the retina, without bending. The macula and fovea, which are the centers of visual acuity (and subsequent cognition), are displaced nasally at an average of 5 ° from this optical axis, so the axis should be distinguished from the optical axis for accurate eye tracking. Is indispensable.
視軸を決定する従来の方法は、片目を開いた人の視野にロッドを一列に整列することを含む（腕の長さに保持された鉛筆を用いて、同様の原理を示すことができる）。この場合、ロッドの前面によって（径方向対称的に）遮蔽されるため、ロッドの背面を見ることができない。意図的にこの方向に位置決められると、ロッドの中心軸は、観察者の視軸に沿って指向する。このプロセスは、任意の方向で繰り返すことができる。 Traditional methods of determining the axis of view include aligning the rods in the field of view of a person with one eye open (a similar principle can be demonstrated using a pencil held at arm length). .. In this case, the back surface of the rod cannot be seen because it is shielded (diametrically symmetrically) by the front surface of the rod. When intentionally positioned in this direction, the central axis of the rod points along the observer's visual axis. This process can be repeated in any direction.
例示的な実施形態において、視軸を決定するために、上述した従来のロッドに基づく技術と同様の原理のいくつかを使用するディスプレイに基づく方法は、視野の領域に近視野および遠視野ディスク（または他の任意の形状）を一列に整列することを含む。装置着用者は、遠視野オブジェクトが近視野オブジェクトによって最大に遮蔽されるようにオブジェクトを整列するように指示される。この技術の本質的な要素ではないが、近視野オブジェクトおよび遠視野オブジェクトに異なる色を与えることができる。装置着用者は、近視野オブジェクトを用いて、可能な限り遠視野オブジェクトの色を（径方向対称的に）均一に「隠す」または遮蔽するように指示される。 In an exemplary embodiment, a display-based method that uses some of the same principles as the conventional rod-based technique described above to determine the visual axis is a near-field and far-field disc in the area of the field of view. Or any other shape), including arranging in a row. The device wearer is instructed to align the far-field objects so that they are maximally blocked by the near-field objects. Although not an essential element of this technique, it is possible to give different colors to near-field and far-field objects. The device wearer is instructed to use the near-field object to "hide" or shield the color of the far-field object as uniformly (diametrically symmetrically) as possible.
拡張現実（ＡＲ）の場合、近視野オブジェクトは、ＡＲディスプレイ上に投影された実質的に不透明なディスクまたは他の形状の物体であってもよい。遠視野オブジェクトは、ディスプレイ上（例えば、モバイル装置、タブレット、モニタ）上に表示された仮想オブジェクトまたは実在オブジェクトであってもよい。遠視野オブジェクトがディスプレイ上に表示された仮想オブジェクトである場合、整列プロセス中に、近視野ＡＲオブジェクトまたは遠視野仮想オブジェクトを移動することができる。異なる位置（すなわち、注視方向）でこのプロセスを繰り返すことによって、視軸の複数の測定値を得るための追加の整列を生成することができる。 For augmented reality (AR), the near-field object may be a substantially opaque disk or other shaped object projected onto an AR display. The far-field object may be a virtual or real object displayed on a display (eg, a mobile device, tablet, monitor). If the far-field object is a virtual object displayed on the display, the near-field AR object or far-field virtual object can be moved during the alignment process. By repeating this process at different positions (ie, gaze directions), additional alignment can be generated to obtain multiple measurements of the visual axis.
遠視野実在オブジェクトの場合、オブジェクトは、較正のために装置着用者の環境に意図的に追加されてもよく、またはオブジェクトは、自然な形状を有しまたは装置着用者の環境に認識され得る物体の一部であってもよい。前者の例として、特異に着色された紙ディスク、または接着剤でディスプレイの隅に貼り付けられた１つ以上のＬＥＤ光源によって生成されたパターンを挙げることができる。後者の例として、殆どの表示装置の４隅の「Ｌ字型」（ベゼル）領域を挙げることができる。領域の形状は、外向きカメラまたは所謂「シーン」カメラを使用して得られた画像または形状テンプレートのデータベースに基づいて認識されてもよい。 For distant-view real objects, the object may be intentionally added to the device wearer's environment for calibration, or the object may have a natural shape or be recognized by the device wearer's environment. May be part of. Examples of the former include uniquely colored paper discs or patterns generated by one or more LED light sources affixed to the corners of the display with an adhesive. An example of the latter is the "L-shaped" (bezel) area at the four corners of most display devices. The shape of the region may be recognized based on a database of images or shape templates obtained using an outward facing camera or a so-called "scene" camera.
図３４は、携帯電話またはタブレット７４１などの表示装置の実在の４隅７４８ａ、７４８ｂ、７４８ｃ、７４８ｄが視軸整列ツールとして使用される例を示している。表示装置７４１の隅７４０の構造は、１）このような装置の機械的設計に基づくテンプレートのデータベース、２）装置着用者の環境を視覚化するシーンカメラ７４６および光学装置７４７を用いて取得された画像、または３）データベースがシーンカメラ画像から認識されたオブジェクト（またはオブジェクトの一部）の正確な寸法を提供するように、両者の組み合わせに基づいて認識することができる。
FIG. 34 shows an example in which the actual four
整列オブジェクト７４０またはその一部の形状が認識されると、拡張現実ディスプレイ７４３内の仮想オブジェクト７４２として形状を再構築する。次いで、装置着用者は、指示に基づいて、ディスプレイ７４１または目の位置を、仮想オブジェクト７４２が遠視野オブジェクトを最大限に覆い隠すように整列される位置７４４に移動する。（遠視野オブジェクト７４０に対する）頭部／目の位置７４４において、目７４４の視軸に沿って見たときに、近視野オブジェクト７４２および遠視野オブジェクト７４０の端部は、完全に整列される（７４５ａ、７４５ｂ）。遠視野オブジェクト７４０の中心位置と、対応する近視野オブジェクト７４２の中心位置とは、３次元空間において、装置着用者の目７４４の視軸の中心を通過する線７４９を画定する。
When the shape of the aligned
追加の実施形態において、装置着用者は、指示に基づいて、近視野オブジェクト７４２がちょうど遠視野オブジェクト７４０に重ねる（すなわち、正確に同様の大きさを有する）ように、遠視野オブジェクト７４０にさらに近づくおよび／または遠視野オブジェクト７４０から遠ざけるように移動することができる。（例えば、オブジェクトのデータベースに基づいて）遠視野オブジェクト７４０の寸法が既知である場合、近視野オブジェクト７４２の寸法が装置によって管理される（したがって既知である）ため、遠視野オブジェクト７４０と目の視覚中心（すなわち、７４５ａと７４５ｂと７４９との交点）との間の距離は、三角法を用いて計算することができる。この距離によって、光軸７４９に沿った単一の点を選択された目の視野の中心に割り当てることができる。
In an additional embodiment, the device wearer, based on instructions, further approaches the far-
更なる実施形態において、（頭部装着装置内の仮想基準の表示が９０°、１８０、および２７０°回転される場合）４隅７４８ａ、７４８ｂ、７４８ｃ、７４８ｄまたは他の遠視野基準位置の任意数のうち１つを用いて、視軸整列プロセスを繰り返して行うことができる。（例えば、視野の中心までの距離を決定するための上記の方法を利用しない場合）これらの測定値の各々によって定義された視軸の交点を用いて、特定の装置ユーザの視軸および／または視覚中心の位置を特定するまたは（例えば、より精確に）さらに定義することができる。
In a further embodiment, any number of four
さらに別の実施形態において、最近に決定された視軸の中心に対して、装置着用者の目の１つ以上の特定可能な点（例えば、瞳孔の中心、角膜輪部の中心、強膜上の特定可能な血管）の位置を決定することができる。視軸とこれらの特定可能な点との間の差は、その後、「視軸オフセット」とみなされ得る。これによって、装置は、目の１つ以上の特定可能な点の測定値に基づいて、装置着用者の視軸（すなわち、装置着用者が「見ている」方向）を計算することができる。 In yet another embodiment, one or more identifiable points of the device wearer's eye (eg, the center of the pupil, the center of the corneal ring, on the sclera) with respect to the recently determined center of the visual axis. The location of the identifiable blood vessel) can be determined. The difference between the visual axis and these identifiable points can then be considered a "visual axis offset". This allows the device to calculate the device wearer's visual axis (ie, the direction the device wearer is "seeing") based on measurements of one or more identifiable points of the eye.
追加の実施形態において、遠視野オブジェクトは、スクリーン上に表示された仮想オブジェクト（例えば、ディスク、ポリゴン）であってもよい。この場合、ＡＲまたはＭＲ（すなわち、複合現実）頭部装着装置と遠視野表示装置との間で通信を使用して、遠視野仮想オブジェクトの表示特性（例えば、サイズ、位置）を制御することができる。図３４に示された方法と同様の方法を用いて、ユーザの視軸および視覚中心を決定することができる。この較正プロセスにおいて遠視野仮想オブジェクトのサイズ、位置、向きおよび他の特性を制御することによって、装置着用者による整列プロセスをさらに支援することができる。 In additional embodiments, the far-field object may be a virtual object (eg, disk, polygon) displayed on the screen. In this case, communication between the AR or MR (ie, mixed reality) head-mounted device and the far-field display device can be used to control the display characteristics (eg, size, position) of the far-field virtual object. it can. A user's visual axis and visual center can be determined using a method similar to that shown in FIG. By controlling the size, position, orientation and other characteristics of the far-field virtual object in this calibration process, the alignment process by the device wearer can be further assisted.
更なる実施形態において、最初に閉じた目を開き、開いた目を閉じることによって、上記のプロセスを繰り返すことによって、各目の視覚中心を決定することができる。ビデオストリームを各目に別々に投影する機構において、一方の目に対する評価を行っている間に、他方の目に投影されるビデオストリームをオフにするまたは静止にする。視線計算に両眼の視軸の測定値を含むことによって、視線精度、特に両眼離反運動を伴う視線精度を向上することができる。 In a further embodiment, the visual center of each eye can be determined by repeating the above process by first opening the closed eye and then closing the open eye. In a mechanism that projects a video stream separately to each eye, the video stream projected to the other eye is turned off or stationary while the evaluation for one eye is being performed. By including the measured values of the visual axes of both eyes in the line-of-sight calculation, the line-of-sight accuracy, particularly the line-of-sight accuracy accompanied by the binocular detachment movement can be improved.
両目が開いた状態で、視軸を測定するプロセスをさらに繰り返してもよい。この場合、いくつかの視線方向および一部の視距離において、上述したように（個人間にばらつきがあるため）、測定を優位眼に割り当てることができる。（一般的にまっすぐ前方に見えている）他の視線方向および（一般的に頭部から離れた）距離において、所謂「自我中心」（egocenter）を決定することができる。自我中心は、通常、両眼の中間に位置する基準位置であり、この基準位置から、観察者の環境を見るときに距離および方向が認知的に感知される。いくつかの応用（例えば、一人称シューティングゲームおよび他の形態のゲーム、運転シミュレーション、乳児の教育）において、自我中心の知識および使用は、仮想環境をより現実的にするのを支援することができる。したがって、装置ユーザの自我中心に対して、距離および方向の測定および／または表示を行うことができる。このような測定は、装置着用者の認知に含まれる認知プロセスをより正確に表すおよび／または定量化することができる。 The process of measuring the visual axis may be repeated with both eyes open. In this case, the measurements can be assigned to the dominant eye, as described above (because of the variability among individuals) in some gaze directions and some gaze distances. The so-called "egocenter" can be determined at other gaze directions (generally looking straight ahead) and at a distance (generally away from the head). The egocentricity is usually a reference position located in the middle of both eyes, from which the distance and direction are cognitively perceived when looking at the observer's environment. In some applications (eg, first-person shooters and other forms of games, driving simulations, baby education), egocentric knowledge and use can help make virtual environments more realistic. Therefore, the distance and direction can be measured and / or displayed with respect to the egocentricity of the device user. Such measurements can more accurately represent and / or quantify the cognitive processes involved in the cognition of the device wearer.
近視野整列オブジェクトおよび遠視野整列オブジェクトの両方が単一ディスプレイ内に含まれる仮想現実（ＶＲ）ヘッドセットおよびＡＲヘッドセットの場合、上述した装置着用者の視軸を決定するための方法を利用するために、画像投影の正確なモードに依存する追加の考慮を行わなければならない。 For virtual reality (VR) headsets and AR headsets where both near-field alignment objects and far-field alignment objects are contained within a single display, the method for determining the visual axis of the device wearer described above is utilized. Therefore, additional considerations must be made that depend on the exact mode of image projection.
一度に１つのみの平面または「深さ」に投影できるＶＲまたはＡＲヘッドセットの場合、視軸整合プロセスは、交互に（連続に）投影された画像フレームに感知された深さを切り替えることによって行うことができる。異なる画像を迅速に連続して表示することは、例えば、従来／過去のテレビジョンセットに使用された「インターレース」技術に類似している。図３４に示したのと同様の整列は、１つの平面に表示された近視野オブジェクトと、別の（インターレースされた）フレームに表示された遠視野オブジェクトとを用いて行われる。 For VR or AR headsets that can project onto only one plane or "depth" at a time, the axis alignment process is by switching the perceived depth to alternating (continuously) projected image frames. It can be carried out. Displaying different images in rapid succession is similar to, for example, the "interlacing" technique used in traditional / past television sets. An alignment similar to that shown in FIG. 34 is performed using a near-field object displayed in one plane and a far-field object displayed in another (interlaced) frame.
空間内の点からベクトルとして光を導く所謂「光照射視野」（light field）を使用するＶＲおよびＡＲヘッドセットにおいて、近視野オブジェクトと遠視野オブジェクトの両方を同一のフレームに投影することができる。このような構成において、整列は、遠視野オブジェクトが最大限に（且つ径方向均一に）遮蔽されるように、近視野オブジェクトまたは遠視野オブジェクトを移動することを含むことができる。 In VR and AR headsets that use the so-called "light field" that guides light as a vector from a point in space, both near-field and far-field objects can be projected onto the same frame. In such a configuration, alignment can include moving the near-field or far-field object so that the far-field object is maximally (and radially uniformly) shielded.
追加の実施形態において、近視野オブジェクトおよび遠視野オブジェクトの両方が仮想物である場合、多数の方法のいずれかを使用して、整列プロセス中に一方のオブジェクトまたは他方のオブジェクト（または、必要に応じて両方）を移動することができる。これは、様々な方法のいずれかで行うことができる。例えば、
１）装置着用者の近くにキーボードがある場合、方向矢印（または他のキー）を繰り返して押すことによって、左、右、上、下の移動を示すことができる。
２）目または頭部装着装置タッチ感応領域（またはタブレットまたは携帯電話などの他のタッチ感応面）に、移動機能（左、右、上、下）を割り当てることができる。
３）装置着用者は、頭部動作（すなわち、回転、傾き、またはその両方）を用いてオブジェクトを「動かす」ことができる。必要に応じて、移動の度合いは、（例えば、加速度計またはシーンカメラの動きを用いて測定された）頭部動作の度合いに比例することができる。
４）「左」、「右」、「上」または「下」の発話などの音声コマンドを用いて、近視野オブジェクトおよび／または遠視野オブジェクトを整列することができる。
In additional embodiments, if both the near-field and far-field objects are virtual objects, one of the objects or the other (or optionally) is used during the alignment process using one of a number of methods. Both) can be moved. This can be done in any of a variety of ways. For example
1) If the keyboard is near the wearer of the device, repeated pressing of the directional arrow (or other key) can indicate left, right, up, and down movements.
2) The movement function (left, right, up, down) can be assigned to the eye or head-worn device touch-sensitive area (or other touch-sensitive surface such as a tablet or mobile phone).
3) The device wearer can "move" the object using head movements (ie, rotation, tilt, or both). If desired, the degree of movement can be proportional to the degree of head movement (eg, measured using accelerometer or scene camera movement).
4) Near-field and / or far-field objects can be aligned using voice commands such as "left", "right", "up" or "down" utterances.
追加の実施形態において、整列プロセスを高速化するために、各コマンド（すなわち、左、右、上または下）によって引き起こされた仮想移動を動的に調整することができる。初期のコマンドは、近視野オブジェクトまたは遠視野オブジェクトの粗動を引き起こす。繰り返した同一方向のコマンド（例えば、「右」、「右」）は、移動の幅に影響を与えない。しかしながら、反対方向のコマンドは、移動の幅（すなわち、所謂「増分）を減少する（例えば、半分にカットする）。装置着用者が最適な整列に向かって収束するにつれて、増分が徐々に小さなり、より精確な制御を可能にする。この技術は、アナログ／デジタル変換の「逐次近似」方法に類似している。水平軸および垂直軸の較正中に、増分のサイズを別々に追跡および調整することができる。 In additional embodiments, the virtual movement caused by each command (ie, left, right, up or down) can be dynamically adjusted to speed up the alignment process. Early commands cause fluttering of near-field or far-field objects. Repeated commands in the same direction (eg, "right", "right") do not affect the width of movement. However, commands in the opposite direction reduce the width of movement (ie, the so-called "increment) (eg, cut in half). As the device wearer converges towards optimal alignment, the increment gradually decreases. This technique is similar to the "sequential approximation" method of analog-to-digital conversion. The size of the increments can be tracked and adjusted separately during horizontal and vertical axis calibration.
高関連性オブジェクト（ＨＲＯ）の高解像度レンダリング
観察者が見ている中央の表示領域に投影された最高解像度の画像に視線追跡を組み合わせる技術は、当該技術分野において周知である。観察者が直接に見ていない領域に低解像度の画像を投影することによって、全体のユーザ表示を生成するための帯域幅が低減される。殆どの実施形態において、視野の周辺領域には、オブジェクトは、より低い空間解像度および／またはより少ない細部の投影で表示される。いくつかの実施例において、シーン中心の高解像度領域と周辺の低解像度領域との間に中間解像度領域が存在する。これによって、最大８０％の帯域の節約を実現した。
High-Resolution Rendering of Highly Relevant Objects (HROs) Techniques for combining line-of-sight tracking with the highest resolution image projected into the central display area viewed by the observer are well known in the art. By projecting a low resolution image into an area that the observer is not directly looking at, the bandwidth for generating the entire user display is reduced. In most embodiments, in the peripheral area of the field of view, the object is displayed with lower spatial resolution and / or less detail projection. In some embodiments, there is an intermediate resolution region between the high resolution region in the center of the scene and the low resolution region in the periphery. This has resulted in up to 80% bandwidth savings.
この技法は、当技術分野において、一般的に「フォビエットレンダリング」（Foveated Rendering）、「フォビエット像」、または「フォビエットグラフィックス」と呼ばれている。「フォビエット」（Foveated）という用語は、少なくともいくつかの点で、人間の視覚系の生理機能に合わせた生成画像の結果として生じる。生物学的に、中心窩は、視力が最も高い網膜の小さな領域である。中心窩は、視野全体の約２°の領域内から画像を検出するように機能するが、脳に情報を伝到達する視神経束の神経線維の約半分は、中心窩からの情報を運ぶ。 This technique is commonly referred to in the art as "Foveated Rendering", "Foveated Rendering", or "Foveated Graphics". The term "Foveated" arises, at least in some respects, as a result of generated images tailored to the physiology of the human visual system. Biologically, the fovea is a small area of the retina with the highest vision. The fovea functions to detect images from within an area of about 2 ° of the entire visual field, but about half of the nerve fibers in the optic nerve bundle that reach the brain carry information from the fovea.
多くの状況において、観察者の周辺視野内の一部のオブジェクトの低い空間解像度のレンダリングは、観察者の視覚経験および意思決定能力を損なう可能性がある。例示的な実施形態において、高空間解像度表示技術を用いて、観察者の傍中心窩にある、特に周辺視野にある高関連性オブジェクト（ＨＲＯ）をレンダリングすることが有利である。ＨＲＯに関連する１つ以上の領域を選択的に高解像度レンダリングすることによって、このようなオブジェクトとの効果的な対話を犠牲にすることなく、全体的な帯域幅の節約を実現することができる。 In many situations, low spatial resolution rendering of some objects in the observer's peripheral vision can impair the observer's visual experience and decision-making ability. In an exemplary embodiment, it is advantageous to use high spatial resolution display techniques to render highly relevant objects (HROs) in the observer's parafovea, especially in the peripheral vision. By selectively rendering one or more areas associated with the HRO in high resolution, overall bandwidth savings can be achieved without sacrificing effective interaction with such objects. ..
傍中心窩および周辺視覚システムは、画像処理（例えば、細部の区別または色の識別）のいくつかの点で弱いが、オブジェクトの空間位置（動きを含む）または明滅を判断する能力において特に強い。傍中心窩および周辺視界内の位置を決定する能力は、例えば曲芸をしながら、入って来る発射体を回避する活動によく証明される。 The parafovea and peripheral visual systems are weak in some respects in image processing (eg, detail distinction or color discrimination), but are particularly strong in their ability to determine the spatial position (including movement) or blinking of an object. The ability to locate the parafovea and peripheral vision is well demonstrated in activities to avoid incoming projectiles, for example, while performing acrobatics.
人間と機械の対話の際にＨＲＯの位置が特に重要である条件が（例えば、仮想現実ディスプレイシステム、拡張現実感ディスプレイシステム、複合現実ディスプレイシステム、または遠隔ディスプレイシステムに）多く存在する。例えば、視覚信号を行っている間に、装置ユーザは、作動アイコンの大体領域を記憶することができる。その後、（しばしば細部の認知が欠けている）傍中心窩または周辺視力を用いて、精確な位置を検出することができる。殆どの場合（位置が特定の機能に関連する場合）、空間記憶に少なくとも部分的に基づいて、傍中心窩状または周辺視野内のオブジェクトまたは作動ターゲットに直接に向かう衝動性動眼を行うことが「快適」であると感じられる。このようなターゲットに向かう衝動性動眼は、視覚信号言語の基本要素である。 There are many conditions (eg, in virtual reality display systems, augmented reality display systems, mixed reality display systems, or remote display systems) where the location of the HRO is particularly important during human-machine interaction. For example, while performing a visual signal, the device user can memorize an approximate area of the activation icon. The parafovea or peripheral vision (often lacking recognition of details) can then be used to detect the exact location. In most cases (when position is associated with a particular function), it is possible to perform an impulsive oculomotor eye directly toward an object or working target in the parafoveal or peripheral vision, at least in part based on spatial memory. It feels "comfortable". Impulsive oculomotor eye toward such a target is a basic element of visual signal language.
例示的な実施形態において、このようなＨＲＯを高解像度で表示することができる。最初に低い解像度で表示された場合、このようなＨＲＯに向けられた衝動性動眼およびその後（例えば、新たな中心窩視野に入ることによって）より高い解像度の切り替えは、ＨＲＯの外観に急速な変化を引き起こし、１つ以上の捜索的な眼球運動または「驚愕する」眼球運動を自然に引き付ける。このことは、視覚信号言語に関連する意図的な動きを妨げる可能性がある。高解像度のＨＲＯディスプレイを維持することによって、このような状況を回避することができる。 In an exemplary embodiment, such an HRO can be displayed in high resolution. When initially displayed at low resolution, such impulsive oculomotor eyes directed at the HRO and subsequent higher resolution switching (eg, by entering a new foveal field of view) cause a rapid change in the appearance of the HRO. Naturally attracts one or more exploratory or "amazing" eye movements. This can interfere with the intentional movement associated with the visual signal language. This situation can be avoided by maintaining a high resolution HRO display.
新しく表示された追加の選択可能なアイコン（または他のＨＲＯ）の次の表示は、高解像度を用いて、新しい周辺視野内に表示されてもよい。これらの新しいアイコンの空間位置は、これらのアイコンの各々を直接に見るための衝動性動眼で特定されてもよい。同様の方法を繰り返すことによって、経験豊富な装置ユーザは、傍中心窩または周辺視野内の新たに表示された特定の（すなわち、ユーザによって意図的に選択された）ターゲットにターゲットに視線を直接に移動する衝動性動眼に特に心地よく感じる。必要に応じて、ユーザの目の中心窩領域に完全に感知されないように、これらの１つ以上の衝動性動眼を複数回に行うことができる。 The next display of the newly displayed additional selectable icon (or other HRO) may be displayed in the new peripheral vision using high resolution. The spatial position of these new icons may be identified by the impulsive oculomotor eye to see each of these icons directly. By repeating the same method, an experienced device user can directly gaze at a specific (ie, intentionally selected by the user) newly displayed target in the parafovea or peripheral vision. I feel particularly comfortable with the moving impulsive oculomotor eye. If desired, one or more of these impulsive oculomotors can be performed multiple times so that they are not completely perceived by the foveal region of the user's eye.
追加の実施形態において、新たな潜在的な目標オブジェクトの導入は、ユーザの注意を引くことなく（すなわち、オブジェクトを「目に見えないほどに可視である」ように）１つ以上のオブジェクトを導入するように、１つ以上の技術を用いて行うことができる。前に説明したように、このような導入は、ユーザが機能的に盲目的である１つ以上の期間（例えば、瞬きおよび／または１つ以上の衝動性動眼）中にオブジェクトを導入することによって行ってもよい。（単独でまたは組み合わせて使用される）１つ以上の「目に見えないほどに可視である」オブジェクトを導入するための他の方法は、背景に比べて、オブジェクトの解像度、細部、色量、輝度、サイズ、透明度および／または明暗度を徐々に変化することを含む。これらの方法のいくつかは、観察者がオブジェクトまたは背景の外観のゆっくりとした変化に気付かなくなる「変化失明」に関連する原理を使用する。 In additional embodiments, the introduction of new potential goal objects introduces one or more objects without the user's attention (ie, making the objects "invisible"). As such, it can be done using one or more techniques. As explained earlier, such an introduction is by introducing the object during one or more periods in which the user is functionally blind (eg, blinking and / or one or more impulsive oculomotor eyes). You may go. Other ways to introduce one or more "invisible" objects (used alone or in combination) are the object's resolution, detail, color, compared to the background. Includes gradual changes in brightness, size, transparency and / or brightness. Some of these methods use principles related to "change blindness" in which the observer goes unnoticed by slow changes in the appearance of the object or background.
更なる実施形態において、ユーザの意図しない注意を引くことなく、潜在的なオブジェクトを慎重に移動することができる。前に説明したように、相当な量（例えば、２°まで）のオブジェクトの移動は、衝動性動眼後の失明期間に許容され得る。これは、「ブランク効果」として知られており、衝動性動眼が予想のターゲット位置に完全に視線を移動ししていないときに移動を感知する可能性を抑制することができる。１つ以上の潜在的なターゲット物の更なる移動が、ユーザの捜索的な眼球運動を驚愕しないまたは引かないように設計された方法で導入されてもよい。このような移動は、（「変化失明」の要素を用いて）遅くてもよく、特に高解像度特徴、明るい特徴、大規模な特徴、詳細な特徴、不透明、対照色、または同様の視覚属性のうち１つ以上であり得る１つ以上のオブジェクトによって、特定の方向の注意を引かないように設計されてもよい。同様に、１つ以上のオブジェクトが目立たない（すなわち、１つ以上のオブジェクトの変化が他のオブジェクトの類似または対応する変化の前に生じない）ように、オブジェクトの移動タイミングまたは視覚属性の変化タイミングの均一性を保証することによって、捜索的または驚愕的な眼球運動を回避することができる。 In a further embodiment, potential objects can be carefully moved without the user's unintended attention. As previously described, a significant amount of object movement (eg, up to 2 °) can be tolerated during the period of blindness after impulsive oculomotor eye movement. This is known as the "blank effect" and can reduce the possibility that the impulsive occlusal eye will perceive movement when it has not completely moved its line of sight to the expected target position. Further movement of one or more potential targets may be introduced in a manner designed so as not to astonish or pull the user's searchable eye movements. Such movements may be slow (using the "change blindness" element), especially for high resolution features, bright features, large features, detailed features, opacity, contrast colors, or similar visual attributes. One or more objects, which may be one or more of them, may be designed so as not to draw attention in a particular direction. Similarly, the timing of object movement or change of visual attributes so that one or more objects are unobtrusive (ie, changes in one or more objects do not occur before similar or corresponding changes in other objects). By guaranteeing the uniformity of the eye movements, searchable or astonishing eye movements can be avoided.
ＨＲＯの追加の例として、例えば、一人称シューティングゲーム内の味方または敵対目標、講談または会話に議論されたオブジェクト、高レベル決定を行う時に構想を示す記号または心像（例えば、数学操作、流れ図、構成部品、回路図、主要な事実）が挙げられる。全ての記号および／または心像を中心窩視野に同時に維持する必要がなく、またはこれらのオブジェクトが短期記憶または作業記憶にあることを確実にするために中心窩視野にこれらのオブジェクトを繰り返して走査する必要性なく、ユーザがこのようなオブジェクト（およびこれらのオブジェクトが表すもの）の存在を知ることは有用である。 Additional examples of HROs include, for example, ally or hostile targets in first-person shooters, objects discussed in storytelling or conversation, symbols or mental images that represent ideas when making high-level decisions (eg, math operations, schematics, components). , Schematic, main facts). It is not necessary to keep all symbols and / or heart images in the foveal visual field at the same time, or iteratively scans these objects in the foveal visual field to ensure that they are in short-term or working memory. It is useful for the user to know the existence of such objects (and what they represent) without the need.
更なる例示的な実施形態において、ＨＲＯの高解像度レンダリングは、非中心窩オブジェクトの識別、知的な関連付け、および空間的な位置づけを支援するだけでなく、ＨＲＯに向かう衝動性（または他の形態の頭部または眼球）運動が存在する場合、前述したように、新しい焦点の領域のレンダリング特性が殆ど変わらない。これは、（新しい視覚特徴の導入に関連する）驚愕または捜索的な反射および／または新しい視覚情報の導入に関連する（すなわち、前に記載された「見慣れている」認知原理を用いた）認知処理を回避することができる。その代わりに、オブジェクトの感知に時間がかかると、オブジェクトは、衝動性動眼（および任意数の後続の矯正的な衝動性動眼）を行う前に観察者の記憶および傍中心窩視野または周辺視野に基づいて予期されたものとは変わらないものとして感知される。 In a further exemplary embodiment, high resolution rendering of the HRO not only assists in the identification, intellectual association, and spatial positioning of non-foveal objects, but also the impulsivity (or other form) towards the HRO. In the presence of (head or eye) movements, as mentioned above, the rendering characteristics of the new focal area are almost unchanged. This is related to startle or search reflexes (related to the introduction of new visual features) and / or the introduction of new visual information (ie, using the "familiar" cognitive principles described above). Processing can be avoided. Instead, if the object takes a long time to perceive, the object will be in the observer's memory and parafoveal or peripheral vision before performing an impulsive eye (and any number of subsequent corrective impulsive eyes). It is perceived as the same as expected based on it.
ＨＲＯの高解像度レンダリングの実装にはある程度の自由度がある。同様の画像においても、１つ以上のスキームを使用することができる。例えば、１）オブジェクトを表す任意形状の領域を高解像度でレンダリングし、低解像度の背景に重合することができる。２）１つ以上のＨＲＯを含む全視野の矩形領域を高解像度でレンダリングすることができる。３）１つ以上のオブジェクトの周りの円形領域を高解像度でレンダリングすることができる。上記場合の各々において、高解像度レンダリングの度合いは、（オプションとして）観察者の現在の（すなわち、中心窩）視野からの距離に従って変化してもよい。 There is some freedom in implementing high resolution rendering for HRO. One or more schemes can be used in similar images. For example, 1) an area of arbitrary shape representing an object can be rendered at high resolution and superimposed on a low resolution background. 2) It is possible to render a rectangular area of the entire field of view including one or more HROs with high resolution. 3) The circular area around one or more objects can be rendered in high resolution. In each of the above cases, the degree of high resolution rendering may (optionally) vary according to the distance from the observer's current (ie, fovea) field of view.
いくつかの状況において、ディスプレイの高解像度領域または所謂「中心窩」領域のサイズを変更することが望ましい場合がある。一般的に１°〜３°の範囲における人間の中心窩サイズの生物学的な変動を用いて、中央の高解像度表示領域にサイズを割り当てることができる。また、いくつかの用途または視線対話において、高解像度領域のサイズを調整することが有利であり得る。例えば、人間の作業記憶（例えば、高レベルの意思決定）に属すると考えられたいくつかの別個の要素を必要とする認知プロセスの間に、これらの要素（またはこれらの要素を表す記号）は、拡大された高レベルまたは高解像度視野に表示されてもよい。対照的に、異なる方向から移動してくる高速移動オブジェクトおよび／または「脅威」オブジェクトを含むゲーム中に、「中心窩」視野または高解像度視野を、より大きな時間解像度（すなわち、より高い表示フレームレート）および／または特定のオブジェクト（例えば、ＨＲＯ）の高解像度レンダリングを可能にするサイズに縮小することができる。 In some situations it may be desirable to resize the high resolution area or so-called "fovea" area of the display. Biological variations in human foveal size, typically in the range of 1 ° to 3 °, can be used to assign sizes to the central high resolution display area. It may also be advantageous to adjust the size of the high resolution area in some applications or line-of-sight dialogues. For example, during a cognitive process that requires several separate elements that are considered to belong to human working memory (eg, high-level decision making), these elements (or symbols that represent these elements) , May be displayed in a magnified high level or high resolution field of view. In contrast, during a game containing fast moving objects and / or "threat" objects moving from different directions, the "foveal" or high resolution field of view has a higher time resolution (ie, higher display frame rate). ) And / or can be reduced to a size that allows high resolution rendering of certain objects (eg, HRO).
上述したように、「フォビエットレンダリング」などの用語は、ディスプレイ解像度の変化を説明するために当技術分野で一般的に使用されている。人間−機械インターフェイス（ＨＭＩ）の観点から、中心窩が別個の解剖特徴を有する網膜内の構造であるが、中心窩の生理学的（すなわち、機能的）領域は、実際には視力およびＨＭＩに最も関連している。「感知視野サイズ」（perceptive field size、ＰＦＳ）という用語は、既に当該技術分野に使用され、認知に使用された中心窩内の領域および周囲の領域を記述する。 As mentioned above, terms such as "Vietto rendering" are commonly used in the art to describe changes in display resolution. From a human-machine interface (HMI) perspective, the fovea is a structure within the retina with distinct anatomical features, but the physiological (ie, functional) region of the fovea is actually the most acuity and HMI. It is related. The term "perceptive field size" (PFS) describes the area within and around the fovea that has already been used in the art and used for cognition.
光強度は、人のＰＦＳを変えることができる。したがって、例示的な実施形態において、観察されている高解像度（すなわち、所謂「フォビエット」）レンダリング領域のサイズは、網膜に到達する光量に従って変化し得る。この光強度は、ＶＲディスプレイのような、より高度に管理された視覚環境に表示されているものに関する知識から決定することができる。代替的にまたは追加的に、特にＡＲまたはＭＲ装置を使用する場合、１つ以上のフォトダイオード、フォトトランジスタまたは他の光センサを用いて、目の近傍の光を測定することができる。１つ以上の光学フィルタを用いて、特定の波長範囲（以下を参照）を選択することができる。 Light intensity can change a person's PFS. Thus, in an exemplary embodiment, the size of the observed high resolution (ie, so-called "foviet") rendering area can vary with the amount of light reaching the retina. This light intensity can be determined from knowledge of what is displayed in a more highly controlled visual environment, such as a VR display. Alternatively or additionally, especially when using an AR or MR device, one or more photodiodes, phototransistors or other photosensors can be used to measure the light in the vicinity of the eye. A particular wavelength range (see below) can be selected using one or more optical filters.
より具体的には、網膜に到達する光量が減少するにつれて、ＰＦＳが増加する。したがって、この変動に対応するように、高解像度レンダリングの領域のサイズを増加させることができる。逆に、光強度が増加すると、ＰＦＳが減少するため、高解像度レンダリング領域のサイズを減少させることができる。生理学的には、この変化は、豊富な光が存在するときに中心窩のより中央の領域に高濃度の（比較的に光感受性が低いが色情報をエンコードする）円錐体を使用する視覚から、中心窩の中心領域から離れて移動するとより大きな密度で表された光感受性の高い桿状体を使用する視覚にシフトするという概念と一致する。 More specifically, as the amount of light reaching the retina decreases, PFS increases. Therefore, the size of the high resolution rendering area can be increased to accommodate this variation. On the contrary, as the light intensity increases, the PFS decreases, so that the size of the high-resolution rendering area can be reduced. Physiologically, this change is from the visual sense of using a high concentration (relatively less photosensitive but encoding color information) cones in the more central region of the fovea in the presence of abundant light. Consistent with the notion that moving away from the central region of the fovea shifts to vision using a more densely represented photosensitive rod.
さらに具体的には、可視スペクトルの青緑色部分における（すなわち、約４００〜５５０ｎｍの波長を有する）光は、感知視野のサイズを変更するが、赤色光は、サイズに少ない影響を与えるまたは影響を与えない。したがって、例示的な実施形態において、ディスプレイの中央視野領域内の高解像度レンダリングは、特に可視スペクトルの青緑色部分における光の有無に基づいて制御されてもよい。 More specifically, light in the turquoise portion of the visible spectrum (ie, having a wavelength of about 400-550 nm) resizes the sensing field of view, while red light has a small effect or effect on size. Do not give. Thus, in an exemplary embodiment, high resolution rendering within the central visual field region of the display may be controlled, especially based on the presence or absence of light in the turquoise portion of the visible spectrum.
更なる実施形態において、（例えば、ＰＦＳに基づいた）中央表示領域のレンダリングの変更は、目が新しい照明条件に適応するのに必要な時間を考慮に入れることができる。円錐体の適応は直ぐに始まるが、完全に適応するまで数分が必要である。桿状体の適応はより長くなり、完全に適応するまで２０〜３０分が必要である。したがって、例示的な実施形態において、異なる照明条件に適応するための移行は、視覚調整の時間および幅に対応して、（ディスプレイの高いフレームレートに比べて）徐々に且つ（例えば、光適応対時間曲線の指数形に概ね一致させるように）漸進的に行うことができる。 In a further embodiment, the rendering change of the central display area (eg, based on PFS) can take into account the time required for the eye to adapt to the new lighting conditions. Adaptation of the cone begins immediately, but it takes a few minutes to fully adapt. The adaptation of the rod is longer and requires 20-30 minutes to fully adapt. Thus, in an exemplary embodiment, the transition to adapt to different lighting conditions is gradual (eg, light adaptation pairs), corresponding to the time and width of the visual adjustment (compared to the high frame rate of the display). It can be done progressively (so that it roughly matches the exponential form of the time curve).
本明細書の他の箇所でより詳細に説明したように、短い距離（例えば、＜５°）の衝動性動眼は、意図したターゲット位置をオーバーシュートする傾向があり、長い距離（例えば、＜１０°）の衝動性動眼は、意図したターゲット位置をアンダーシュートする傾向がある。更なる実施形態において、これらの傾向および追従誤差を用いて、眼球運動中に意図したターゲット位置の周囲により高い解像度でレンダリングされた（所謂「中心窩」）領域のサイズおよび形状を構造化することができる。 As described in more detail elsewhere herein, short-distance (eg, <5 °) impulsive occlusal eyes tend to overshoot the intended target position and long-distance (eg, <10). Impulsive oculomotor eyes of °) tend to undershoot the intended target position. In a further embodiment, these trends and follow-up errors are used to structure the size and shape of the region rendered at higher resolution (so-called "fovea") around the intended target position during eye movement. Can be done.
例えば、予想された衝動性動眼の目的注視位置の周りの領域に加えて、衝動性動眼と同様の方向に目標領域を越えて延在する追加の領域は、長い距離の衝動性動眼が検出されたときに、高解像度でレンダリングされてもよい。この延在は、長い距離の衝動性動眼（一般的に約２０°）のアンダーサートを修正するために典型的に１つ以上の矯正的な衝動性動眼が発生することを予想し得る。長い距離の衝動性動眼が検出されるとすぐにこのような領域を高解像度でレンダリングすることによって、ユーザは、長い距離の衝動性動眼中におよびその後の矯正的な衝動性動眼中に、高解像度の（所謂「中心窩」）画像を見ることができる。これによって、ユーザが意図したターゲット位置を漸進的に絞るときに、領域の解像度の変化（突然の混乱）を回避することができる。 For example, in addition to the area around the intended gaze position of the expected impulsive occlusal eye, an additional area extending beyond the target area in the same direction as the impulsive oculomotor eye is detected over a long distance. At that time, it may be rendered in high resolution. This prolongation can be expected to result in typically one or more corrective impulsive eyes to correct the undersert of long-distance impulsive eyes (typically about 20 °). By rendering such areas in high resolution as soon as a long-distance impulsive eye is detected, the user can perform high during the long-distance impulsive eye and subsequent corrective impulsive eye. You can see the resolution (so-called "fovea") image. This makes it possible to avoid changes in the resolution of the area (sudden confusion) when the target position intended by the user is gradually narrowed down.
同様に、短い距離の衝動性動眼は、意図したターゲット位置をオーバーシュートする傾向がある。したがって、例示的な実施形態において、高解像度で目標の注視領域をレンダリングすることに加えて、より短い距離の衝動性動眼のオーバーシュートの傾向を考慮して、（注視領域に隣接する）視線移動の開始位置と注視位置との間の追加の領域を高解像度でレンダリングすることができる。これによって、１つ以上の領域の再レンダリングは、一般的に１つ以上の矯正的な衝動性動眼が視線を意図した注視位置（初期の衝動性動眼の開始位置の大体方向）に向かって戻す区域に最小化される。 Similarly, short-distance impulsive occlusal eyes tend to overshoot the intended target position. Therefore, in an exemplary embodiment, in addition to rendering the target gaze area at high resolution, the gaze movement (adjacent to the gaze area) takes into account the tendency of overshoots of the impulsive oculomotor eye over shorter distances. The additional area between the starting position and the gaze position can be rendered in high resolution. Thereby, the re-rendering of one or more areas generally returns the gaze position (roughly the starting position of the initial impulsive gonocly eye) to the intended gaze position of the one or more corrective gonocly eye. Minimized to area.
更なる実施形態において、眼球運動のターゲット位置に基づく高解像度領域のサイズおよび／または形状は、視線位置または予測および／または測定された注視位置を決定する際の予想誤差の度合いを考慮することができる。注視位置の測定における誤差の度合い（または逆に、信頼度）は、以下を含む多くの要因に基づいて推定されてもよい。これらの要因は、特定のユーザおよび／または最近の使用中（特に、ユーザが既知のオブジェクト位置を見ていると特定された場合）に視線位置の全体的な測定の変動、ヘッドセットおよび／またはユーザ頭部の領域に存在し得る（前述した）振動および他の機械的な加速度、照明の均一性および／または強度、ビデオ画像内の（所謂「ショット」）ノイズのランダム度、視線追跡の測定を不明瞭にする可能性のある障害物（例えば、睫毛）の存在、目の振顫または振動などの追加的な動きの存在、瞬きの発生、および、眼の構造（例えば、テンプレートとの適合度）を特定する際に、１つ以上のアルゴリズム法によって決定された多数の要因を含む。 In a further embodiment, the size and / or shape of the high resolution region based on the target position of eye movement may take into account the degree of prediction error in determining the line-of-sight position or predicted and / or measured gaze position. it can. The degree of error in measuring the gaze position (or conversely, reliability) may be estimated based on a number of factors, including: These factors include fluctuations in the overall measurement of gaze position, headset and / or during specific users and / or recent use (especially if the user is identified as looking at a known object position). Measurements of vibrations and other mechanical accelerations (described above) that may be present in the area of the user's head, lighting uniformity and / or intensity, randomness of noise (so-called "shots") in video images, eye tracking. The presence of obstacles (eg, eyelashes) that can obscure the presence of, the presence of additional movements such as eye sway or vibration, the occurrence of blinks, and the fit of the eye structure (eg, fitting with the template). Degrees) include a number of factors determined by one or more algorithmic methods.
さらに、注視位置を予測または算出する場合、速度に基づく（空間的および時間的）誤差および（個人によって異なる可能性のある）眼球運動の非弾道的速度プロファイルに対する傾向を考慮に入れることができる。 In addition, velocity-based (spatial and temporal) errors and tendencies for non-ballistic velocity profiles of eye movements (which may vary from individual to individual) can be taken into account when predicting or calculating gaze positions.
例示的な実施形態において、高解像度でレンダリングされた表示領域のサイズおよび／または形状は、視線の測定および／または（速度に基づく）注視位置の予測の予想誤差の推定値に基づいて動的に調整することができる。一般的に、誤差の推定値が増加した場合、注視位置の不確実性の増大に対応するために、高解像度領域のサイズを増加させることができる。これによって、ユーザがディスプレイの高解像度表示を実際に見ていることをさらに保証する。 In an exemplary embodiment, the size and / or shape of the display area rendered at high resolution is dynamically based on an estimate of the expected error in gaze measurement and / or gaze position prediction (based on speed). Can be adjusted. In general, as the estimated error increases, the size of the high resolution region can be increased to accommodate the increased uncertainty in the gaze position. This further ensures that the user is actually looking at the high resolution display of the display.
同様に、優先方向（例えば、ディスプレイのエッジの近く）における最近の動きの速度および／または視線測定値の疑わしいノイズに基づいて、領域の形状は、見かけ動きの方向、動きの幅、および／またはノイズの度合いおよび分布に基づいて調整することができる。例えば、ノイズ測定値および／または不確実性がディスプレイのエッジ近くで一般的に増加する場合、増加したノイズ／不確実性に対応するために、高解像度領域の形状をディスプレイのエッジに向かう方向に拡大することができる。 Similarly, based on the suspicious noise of recent motion speeds and / or line-of-sight measurements in the preferred direction (eg, near the edge of the display), the shape of the area is the apparent motion direction, motion width, and / or It can be adjusted based on the degree and distribution of noise. For example, if noise measurements and / or uncertainty generally increase near the edge of the display, then the shape of the high resolution area is oriented towards the edge of the display to accommodate the increased noise / uncertainty. Can be expanded.
追加の例示的な実施形態において、選択された表示領域（例えば、非ＨＲＯおよび／または中心窩視野）内の帯域幅を減少させる代替または追加の方法は、このような領域の色量を低減することを含む。画像の色見えは、赤、緑および青（すなわち、ＲＧＢ）強度の基礎表示に加えて、色相（hue）、濃淡（tint）、彩度（colorfulness）、クロマ（chroma）、飽和度（saturation）および輝度（luminance）などの用語の使用を含む多くの異なる方法で度々記述される。色量の減少は、１つ以上の表示可能な色の全体的な範囲（すなわち、最も暗い色から最も明るい色まで）を減少させることおよび／または解像度を減少させること（すなわち、特定色の選択可能な強度の数を減らすこと）を含む。代わりに、他の色に対する１つの色の比または相対強度を計算（および伝送）する方式において、このような相対強度の範囲および／または解像度を低減することができる。色の範囲および／または解像度を低減することによって、（ピクセルまたはピクセル群内の）各色を表すのに必要とされるビット数を低減し、その結果、画像を伝送するために必要な帯域幅を低減する。 In additional exemplary embodiments, alternative or additional methods of reducing bandwidth within selected display areas (eg, non-HRO and / or foveal field of view) reduce the amount of color in such areas. Including that. The color appearance of an image is a basic representation of red, green and blue (ie RGB) intensities, as well as hue, tint, colorfulness, chroma, and saturation. And often described in many different ways, including the use of terms such as luminance. Reducing the amount of color reduces the overall range of one or more displayable colors (ie, from the darkest to the brightest) and / or reduces the resolution (ie, the selection of a particular color). To reduce the number of possible intensities). Alternatively, such relative intensity ranges and / or resolutions can be reduced in methods of calculating (and transmitting) the ratio or relative intensity of one color to another color. By reducing the color range and / or resolution, the number of bits required to represent each color (in a pixel or group of pixels) is reduced, resulting in the bandwidth required to transmit the image. Reduce.
多くの点で、非中心窩表示領域の色量の減少を含む実施形態は、生体模倣型である。上述したように、視覚システムは、一般的に、周辺視野内のオブジェクトの細部およびオブジェクトの色の両方を識別することに弱い。したがって、傍中心窩および／または周辺領域内の色量および／または細部を低減することによって帯域幅を減少させるスキームは、人間の視覚システムによる画像の処理能力と一致する。 In many respects, embodiments that include a reduction in the amount of color in the non-foveal display area are biomimetic. As mentioned above, visual systems are generally vulnerable to identifying both object details and object colors in the peripheral vision. Thus, schemes that reduce bandwidth by reducing color and / or detail within the parafovea and / or peripheral region are consistent with the ability of the human visual system to process images.
追加の実施形態において、オブジェクトの関連性をいくつかの方法で決定することができる。具体的に、１）典型的なソフトウェアアプリケーションの場合、アプリケーション開発者は、表示された画面内の異なるオブジェクトに関連性を与えることができる。視覚信号に関連するメニュー選択、講談中にキー文字の画像、本文の見出しおよびページ番号、ハイパーリンク、および着信のテキストは、高い関連性を与えるオブジェクトの例である。２）実在オブジェクトと仮想オブジェクトの任意の組み合わせを含むシーンに基づいて、シーン内の特定のオブジェクトの関連性は、当技術分野で周知の画像認識技術を用いたオブジェクトの識別に基づいてアルゴリズム的に決定される。例えば、全ての顔または特定の顔を高解像度で表示することができる。同様に、少なくとも視線を引き付けることを予期して、ユーザに「警告する」（すなわち、視覚的な呼び出しまたはかけ声）アイコンを高解像度で表示することができる。３）１つ以上のオブジェクトによって実行される特定のアクティビティのマシンに基づく認識は、高い関連性状態をトリガすることができる。例えば、環境内の顔が怒っているまたは話しプロセスに関連していると特定された場合、その関連性を増加させることができる。４）神経ネットおよび他の機械学習手法を用いて、特定のユーザに対して（例えば、繰り返し選択によって）重要であるオブジェクトを決定することができる。例えば、ユーザが１日の特定の時間に野球の得点を見る場合、機械学習手法は、得点が更新されると、高解像度で表示することができる。 In additional embodiments, the relevance of objects can be determined in several ways. Specifically, 1) In the case of a typical software application, the application developer can give relevance to different objects in the displayed screen. Menu selections related to visual signals, key character images during a storytelling, body headings and page numbers, hyperlinks, and incoming text are examples of highly relevant objects. 2) Based on a scene containing any combination of real and virtual objects, the relevance of a particular object in the scene is algorithmically based on object identification using image recognition techniques well known in the art. It is determined. For example, all faces or specific faces can be displayed in high resolution. Similarly, a "warning" (ie, visual call or shout) icon can be displayed in high resolution to the user, at least in anticipation of attracting the line of sight. 3) Machine-based awareness of a particular activity performed by one or more objects can trigger a highly relevant state. For example, if a face in the environment is identified as angry or related to the talking process, that relevance can be increased. 4) Neural nets and other machine learning techniques can be used to determine objects that are important to a particular user (eg, by repeated selection). For example, when a user sees a baseball score at a specific time of the day, the machine learning method can display the score in high resolution when the score is updated.
追加の実施形態において、関連性は、特定のオブジェクトに割り当てられ得る任意の数の条件によって調整されてもよい。例えば、スポーツスコアが最初に発表されたときに関連性が高いが、時間と共に関連性が低下する。一年の特定の時間帯に、特定のスポーツに関する画像に高い関連性を与えてもよい。一日の特定の時間および／または一週の特定の日に、公共交通のスケジュールおよび／または告知に与えられた関連性を増加させることができる。予定されたカレンダイベントの前の一定の時間に関連性を与えることができる。 In additional embodiments, the relevance may be adjusted by any number of conditions that can be assigned to a particular object. For example, the sports score is highly relevant when it is first published, but becomes less relevant over time. Images related to a particular sport may be highly relevant at a particular time of the year. The relevance given to public transport schedules and / or announcements can be increased at specific times of the day and / or specific days of the week. Relevance can be given at a certain time before the scheduled calendar event.
ユーザの近くに位置するおよび／またはユーザに向かって移動していると表示されているオブジェクトに、高い関連性を与えることができる。瞬時注視位置に比べて、特定の方向に位置するオブジェクトに、より高い関連性を与えることができる。例えば、文字／記号が一般的に左から右および上から下に配置された言語（例えば、英語、フランス語、ドイツ語）で読書を行う場合、テキストおよび／またはオブジェクトを読む順序に視線を向かうことを予期して、注視位置の右側および／または下方に位置する文字および単語をより高い解像度で表示することができる。 Objects that are located near the user and / or appear to be moving towards the user can be highly relevant. It is possible to give a higher relevance to an object located in a specific direction as compared with the instantaneous gaze position. For example, when reading in a language in which letters / symbols are generally arranged from left to right and from top to bottom (eg English, French, German), look at the order in which you read the text and / or objects. In anticipation of this, letters and words located to the right and / or below the gaze position can be displayed at a higher resolution.
場合によって、絶対位置（すなわち、ディスプレイに対する相対位置）が関連性の決定要因の１つであってもよい。意図的な記憶誘導型衝動性動眼の潜在的なターゲットは、このカテゴリーにおける特に有用なクラスである。例えば、視覚信号言語において、１つ以上の作動ターゲットおよび／または選択可能なオブジェクトは、所定の応用または殆どの応用において、同様の位置に現れる可能性がある。潜在的なターゲットとして、このような潜在的なＵＲＯターゲットをより高い空間解像度および／または色解像度で表示することができる。 In some cases, the absolute position (ie, the position relative to the display) may be one of the determinants of the relevance. Potential targets for intentional memory-guided impulsive oculomotor eye are a particularly useful class in this category. For example, in a visual signal language, one or more actuating targets and / or selectable objects can appear in similar positions in a given application or most applications. As potential targets, such potential URO targets can be displayed at higher spatial and / or color resolutions.
オブジェクトの関連性に等級をつける（すなわち、連続範囲内のスコアを与える）ことができる。多くの関連性の高いオブジェクトを同時に表示する場合、利用可能な帯域幅を抑えるために、（高い関連性の範囲にあるが）比較的に関連性の低いオブジェクトを少なくとも一時的に解像度を下げて表示することができる。 You can grade the relevance of objects (ie, give them a score within a continuous range). When displaying many relevant objects at the same time, reduce the resolution of relatively irrelevant objects (although in the high association range) at least temporarily to reduce the available bandwidth. Can be displayed.
１つ以上の処理装置および／または送信装置による全体的な処理要件を低減し、その結果として消費電力を低減するために、帯域幅の削減は、他の方法と相乗的に使用されてもよい。これらの方法は、モバイル装置の望ましくない加熱をさらに低減し、バッテリの寿命を延ばすことができる。 Bandwidth reduction may be used synergistically with other methods to reduce the overall processing requirements of one or more processors and / or transmitters and, as a result, reduce power consumption. .. These methods can further reduce unwanted heating of mobile devices and extend battery life.
表示が必要ではないときに、例えば、（上述したように）ユーザが機能的な失明しているとき（例えば、瞬き抑制または衝動性動眼抑制の期間）に、ディスプレイを（一時的に）更新しないおよび／または一部の表示／処理素子を電子的にオフにすることができる。これらの時間に、送信を一時停止してもよい。代わりに、ユーザが機能的な失明状態から完全な視力に戻るときの必要性を予期して、表示情報の送信を継続してもよい。例えば、機能的な失明の期間中、高解像度の中央（中心窩）視野の表示を一時的に中断するが、表示領域の静止（例えば、背景）領域および／または周辺領域を更新してもよい。このようなスキームは、利用可能な時間と帯域幅を最大限に活用することができる。 Do not (temporarily) update the display when the display is not needed, for example (as described above) when the user is functionally blind (eg, during the period of blink suppression or impulsive eye movement suppression). And / or some display / processing elements can be electronically turned off. At these times, transmission may be paused. Alternatively, the transmission of display information may continue in anticipation of the need for the user to return from functional blindness to full vision. For example, during a period of functional blindness, the display of the high resolution central (foveal) visual field may be temporarily interrupted, but the stationary (eg, background) and / or peripheral areas of the display area may be updated. .. Such schemes can make the best use of available time and bandwidth.
ユーザの注意点を決定するためのオブジェクト顕著性の導入
生理学的に、人間の視覚情報の処理は、まず、網膜内の神経節細胞のレベルで始まる。これらの細胞は、中央領域の光受容体と環状の周囲領域の光受容体とに当たる光の差が存在する場合に、ニューロンが興奮する所謂「中央−周囲受容野」パターン内に配置される。この構成によって、その後に脳に送信されるエッジの位置（すなわち、所定のレベルを超える空間的な勾配）を検出することができる。
Introduction of object prominence to determine user attention Physiologically, the processing of human visual information begins at the level of ganglion cells in the retina. These cells are placed within the so-called "central-peripheral receptive field" pattern in which neurons are excited when there is a difference in light that hits the photoreceptors in the central region and the photoreceptors in the annular perimeter region. With this configuration, it is possible to detect the position of the edge (ie, the spatial gradient above a predetermined level) that is subsequently transmitted to the brain.
脳が主にエッジを使ってオブジェクトを特定するという事実は、観察されたオブジェクトの全体の異なる領域が異なる視覚的注意を受けるという証拠である。輝度および／または色の変化に高い明暗度がある領域は、一般的に最も多くの視覚的注意を受ける。したがって、観察者の注意点は、必ずしもオブジェクトの幾何学的中心ではなく、高明暗度の領域に集中することが多い。このことは、１つ以上のオブジェクトの特徴位置または基準位置（例えば、オブジェクトの中心）に対して、観察者が最も頻繁に見ている位置を経験的に特定する、所謂「顕著性マップ」の構築および使用をもたらした。 The fact that the brain uses edges primarily to identify objects is evidence that the entire different area of the observed object receives different visual attention. Areas with high brightness and / or color changes receive the most visual attention in general. Therefore, the observer's attention is often focused on the high light and dark areas, not necessarily the geometric center of the object. This is a so-called "saliency map" that empirically identifies the position most frequently viewed by the observer with respect to the feature or reference position of one or more objects (eg, the center of the object). Brought construction and use.
オブジェクト（またはオブジェクトの一部）の「顕著性」は、オブジェクトの物理的な特徴のみによって決定されない。顕著性は、観察者の（場合によって時々変化する）関心に依存する。例えば、殆ど種類（人間または他の動物）の顔面上の目の周りの領域は、高顕著性領域として頻繁に見られる。場合によって、観察者は、オブジェクトまたはオブジェクトの一部の顕著性に影響を与えることを「学習」することができる。例えば、最初に人を驚かせる（または何らかの他の情動反応を生成する）可能性のあるオブジェクトを繰り返して見ることによって、オブジェクトおよびその特徴がより親しくなり、最終的にはオブジェクト全体の顕著性を低減することを学習することができる。 The "saliency" of an object (or part of an object) is not determined solely by the physical characteristics of the object. Severity depends on the observer's (sometimes changing) interests. For example, the area around the eyes on the face of most types (humans or other animals) is often seen as a highly prominent area. In some cases, the observer can "learn" to affect the saliency of an object or part of an object. For example, by repeatedly looking at an object that may first surprise a person (or generate some other emotional response), the object and its features become more familiar and ultimately the overall prominence of the object. You can learn to reduce.
顕著性によって、オブジェクトの一部の特徴は、他の特徴よりも頻繁に見られる。その結果、１つ以上の測定値に基づいて視線の焦点および／またはオブジェクトを決定するための任意の「平均化」方法または他の統計方法は、好ましい注視方向／位置を考慮しなければならない。典型的には、一連の空間的測定値を平均化することは、ある基準位置に対する誤差の径方向対称（例えば、ガウス分布）分布を仮定する。しかしながら、１つ以上のオブジェクト内に好ましい注視位置が存在する場合、この仮定（すなわち、径方向対称）が一般的に満たされず、複数の測定値を統計的に処理するために、所謂「確率密度関数」（ＰＤＦ）が必要となる。 Due to saliency, some features of an object are seen more often than others. As a result, any "averaging" method or other statistical method for determining the focus and / or object of the line of sight based on one or more measurements must consider the preferred gaze direction / position. Typically, averaging a set of spatial measurements assumes a radial symmetric (eg, Gaussian distribution) distribution of errors with respect to a reference position. However, if there are preferred gaze positions within one or more objects, this assumption (ie, radial symmetry) is generally not met and the so-called "probability density" is used to statistically process multiple measurements. A "function" (PDF) is required.
例示的な実施形態において、（例えば、オブジェクトの高明暗度領域に基づいて）仮定されたまたは経験的に測定された顕著性マップを使用して、表示された特定のオブジェクトに関連するＰＤＦを決定することができる。ＰＤＦが予測または測定されると、各オブジェクトに関連する個々のＰＤＦを考慮して、（例えば、加重平均を用いて）複数の注視位置の測定値を結合することができる。重み係数は、ＰＤＦを用いて空間分布に基づいて計算される。 In an exemplary embodiment, a hypothesized or empirically measured saliency map (eg, based on the object's high brightness area) is used to determine the PDF associated with the particular object displayed. can do. Once the PDF is predicted or measured, multiple gaze position measurements can be combined (eg, using a weighted average), taking into account the individual PDFs associated with each object. The weighting factor is calculated based on the spatial distribution using PDF.
結合された（例えば、加重平均された）注視位置の測定値は、個々の注視位置の測定値に比べて、一般的に、ユーザの注意の推定により高い信頼度および空間精度を提供する。このような方法は、最新の視線追跡システム内のカメラのフレームレート（すなわち、単位時間に取得され得る注視位置の測定値の最大数に対応する）が増加するにつれて、実行可能且つ有価値になる。結合された注視位置の推定値を用いて、ユーザの注意（例えば、衝動性動眼の目的位置）が特定のオブジェクトの中心（または他の基準位置）から所定の距離内に位置するか否かを判断することによって、選択を表示するおよび／または選択されたオブジェクト（および／または他のパラメータ）に関連する操作を開始する際に、特に有用である。顕著性および結果として生じたＰＤＦを考慮する結合された測定値の使用は、視覚信号言語による表示および／または選択されているオブジェクト（および逆に、不注意な作動の減少）を決定する際により高い精度および一貫性をもたらす。 Combined (eg, weighted averaged) gaze position measurements generally provide higher reliability and spatial accuracy by estimating user attention than individual gaze position measurements. Such a method becomes viable and valuable as the frame rate of the camera in the modern line-of-sight tracking system (ie, corresponding to the maximum number of gaze position measurements that can be obtained per unit time) increases. .. Using the combined gaze position estimate, whether the user's attention (eg, the target position of the impulsive eye) is within a given distance from the center (or other reference position) of a particular object. By making a judgment, it is especially useful in displaying selections and / or initiating operations related to the selected object (and / or other parameters). The use of combined measurements to account for saliency and the resulting PDF is more dependent on determining the display and / or selected object in the visual signal language (and conversely, the reduction of inadvertent movement). Provides high accuracy and consistency.
図３５は、オブジェクト７６０の予期顕著性マップの一例を示す図である。例示的なオブジェクトは、単純な二等辺三角形７６０からなる。顕著性が最も高いと推定される焦点は、三角形７６０の辺が交差する位置に配置される。図３５に示された顕著性マップにおいて、顕著性の度合いがグレー目盛り７６１ａ、７６１ｂ、７６１ｃで表されている（薄黒い色は、最も顕著な領域を表す）。オブジェクト（すなわち、二等辺三角形）７６０の幾何中心は、十字線７６２によって示される。
FIG. 35 is a diagram showing an example of an expected saliency map of the
顕著性マップ７６１ａ、７６１ｂ、７６１ｃは、オブジェクト中心７６２を通る破線として描かれた垂直線７６３ｂの左右に等しい領域を含む。したがって、結合された水平注視位置を決定する時に、水平方向の注視測定値に同様の重みを付けてもよい。しかしながら、オブジェクト中心７６２を通る破線として描かれた水平線７６３ａより上方の７６１ａに比べて、下方の顕著な特徴７６１ｂ、７６１ｃの数が多い。したがって、オブジェクト７６２の幾何中心の上方よりも、観察者による注視の測定値が下方により多くあると期待される。
The saliency maps 761a, 761b, 761c include regions equal to the left and right of the
図３５に示す単純例において、中央の水平線７６３ａの上方に比べて水平線７６３ａの下方に２倍の顕著性領域が存在する。オブジェクトを見る観察者は、最初にオブジェクトの幾何中心７６２の下方を見る可能性、および／または、その後（マイクロ衝動性動眼、振顫期間中）中央の水平線７６３ａ下方のオブジェクトを観察する可能性が２倍である。したがって、水平線７６３ａの下方に比べて、水平線７６３ａの上方に２倍の観測値が得られたため、水平線７６３ａ下方の各測定値に比べて、水平線７６３ａ上方の各測定値に重みを付けべきである。この重み付けによって、他の全ての誤差源がランダムであると仮定すると、多くの注視測定値から得られた加重平均は、オブジェクト７６２の真の幾何中心に収束する。
In the simple example shown in FIG. 35, there is a twice as prominent region below the
領域およびクラスタの視線選択
多くのＨＭＩ（人間-機械インターフェイス）システムの基本要素は、オブジェクトのクラスタまたはグループを指定し、オブジェクトと対話する機能に関与する。このようなオブジェクトの例は、データファイル、アプリケーション、画像、機能を表すアイコン、名前、住所、電話番号、テキスト内の単語、ハイパーリンク、ビデオ、録音、外面などの集合を含む。現行システムにおいて、典型的なスクリーンに基づくＧＵＩ（グラフィカルユーザインターフェイス）は、ユーザが（例えば、タッチスクリーンまたはコンピュータマウスを用いて）画面上で、クラスタ内の所望のオブジェクトの表現を含む１つ以上のほぼ矩形領域を指定することを可能にすることによって、クラスタのグループ化を処理する。操作は、例えば、クラスタ内の全てのオブジェクトまたは機能に対して同時に実行されてもよい。
Area and Cluster Line-of-Sight Selection Many basic elements of HMI (human-machine interface) systems are involved in the ability to specify and interact with a cluster or group of objects. Examples of such objects include sets of data files, applications, images, function icons, names, addresses, phone numbers, words in text, hyperlinks, videos, recordings, exteriors, and so on. In current systems, a typical screen-based GUI (graphical user interface) is one or more on which the user (eg, using a touch screen or computer mouse) contains a representation of the desired object in the cluster. Handles cluster grouping by allowing you to specify a nearly rectangular area. The operation may be performed simultaneously on, for example, all objects or functions in the cluster.
視覚信号を行っている間に、急速な衝動性動眼の本質的な「ポイントツーポイント」性質、および各衝動性動眼中に目が注視する視覚可能な項目（すなわち、ターゲット）を提供する必要があるということから問題が生じる。このことは、眼球運動の生理学を考慮したＧＵＩなしに、ユーザの眼球運動によって任意の大きさの領域を特定することを困難にする。 While performing the visual signal, it is necessary to provide the essential "point-to-point" nature of the rapidly impulsive oculomotor eye, and the visible items (ie, targets) that the eye gazes at during each impulsive oculomotor eye. The problem arises from the fact that there is. This makes it difficult to identify a region of arbitrary size by the user's eye movements without a GUI that takes into account the physiology of eye movements.
例示的な実施形態において、ディスプレイ上のオブジェクトのクラスタは、特定可能な一連の衝動性動眼を考慮して選択されてもよい。一連の衝動性動眼によって指定された領域内の全てのオブジェクトは、選択の一部として含まれる。任意の数のオブジェクトを任意のサイズまたは形状の領域に包含するように、複数の領域を順次に選択または追加することができる。 In an exemplary embodiment, the cluster of objects on the display may be selected in view of a series of identifiable impulsive eyes. All objects within the area specified by the set of impulsive oculomotor eyes are included as part of the selection. Multiple areas can be sequentially selected or added to include any number of objects in any area of any size or shape.
視覚信号言語において、隅の位置から「領域選択」ターゲットに視線を移動する衝動性動眼によって、選択可能な領域の隅を指定することができる選択される領域は、一般的に長方形である。しかしながら、例えば、実質的に１次元の要素リストから複数の要素を選択する場合、他の選択形状が可能である。「領域選択」ターゲットは、領域の１つの境界の選択以外に、特定の機能に関連付けられていない。 In the visual signal language, the selected area is generally rectangular, which allows the corners of the selectable area to be specified by the impulsive oculomotor eye that moves the line of sight from the corner position to the "area selection" target. However, for example, when selecting a plurality of elements from a substantially one-dimensional element list, other selection shapes are possible. The "area selection" target is not associated with any particular function other than the selection of one boundary of the area.
領域を指定するために使用される眼球運動の特定の順列には、様々な変動がある。このような一連の眼球運動の本質的要素は、１）２回の衝動性動眼を用いて、矩形領域の２つの（隣接しない）隅を指定する能力、２）必要に応じて、追加の眼球運動を用いて、追加の領域をクラスタに追加する能力、３）１つ以上の領域の不注意な選択を修正または「廃棄」する能力、および４）選択された１つ以上の領域内のオブジェクト（オブジェクトによって表される機能を含む）に対して選択可能な操作を行う能力を含む。 There are various variations in the particular permutation of eye movements used to specify the region. The essential elements of such a series of eye movements are 1) the ability to specify two (non-adjacent) corners of a rectangular area using two impulsive eyes, and 2) additional eyeballs as needed. The ability to use motion to add additional areas to a cluster, 3) the ability to modify or "discard" inadvertent selection of one or more areas, and 4) objects in one or more selected areas. Includes the ability to perform selectable operations on (including the functions represented by objects).
「領域選択」ターゲットを表示させる操作は、（作動領域の数を最小限に抑えるために）所謂「スタート」選択シーケンスに埋め込むことができる。しかしながら、多くのアプリケーションにおいて、全て（または少なくとも殆ど）の時に「領域選択」ターゲットを使用可能にする方が使いやすく、効率的である。また、大きな表示領域に少ない数のオブジェクトが存在する場合、視線が他の視認可能なオブジェクトがない箇所に注視する視覚な「休憩領域」を提供するために、ライングリッドまたはドットマトリクスなどのグラフィック指示を背景に重合するまたはそれに含めると便利である。 The operation of displaying the "region selection" target can be embedded in the so-called "start" selection sequence (to minimize the number of working regions). However, in many applications it is easier and more efficient to enable the "region selection" target at all (or at least most) times. Also, if there are a small number of objects in a large display area, graphic instructions such as a line grid or dot matrix to provide a visual "rest area" where the line of sight looks where there are no other visible objects. It is convenient to polymerize or include in the background.
図３６Ａ〜Ｃは、ディスプレイ７７０上に表示された可能なテキストメッセージまたは電子メールの受信者リスト７７１から複数の人を選択するための一連の眼球運動７７３ａ、７７３ｂを示している。図３６Ａにおいて、ディスプレイ７７０上の領域７７５ａ内の１グループの受信者（Peter、Mary、Dave、C.J.）は、まず、（移動方向を示す矢印を含む実線で示すように）選択された領域７７５ａの隅７７２ａから「領域選択」アイコン（この場合、点線で描かれた中央に黒点を含む正方形７７４ａ）までの衝動性動眼を行うことによって選択される。その後、ユーザは、所望の要素を含む領域７７５ａの第２隅７７２ｂを探すために（作動ターゲット７７４ｂに対する衝動性動眼を行わない限り）任意の数の追加の眼球運動を行うことができる。ユーザは、第２隅を見つけると、領域７７５ａの第２隅７７２ｂから領域選択ターゲット７７４ａまでの衝動性動眼７７３ｂを行うことによって、選択された名前を有する領域７７５ａを画定することができる。
36A-C show a series of
この時点で、領域選択ターゲット７７４ａから作動ターゲット７７４ｂまでの衝動性動眼が、最初に選択された領域７７５ａ内の４つの名前に基づいて、（例えば、受信者に電子メールを送信する）操作を開始することができる。しかしながら、図３６Ｂに示すように、この操作に別の受信者の名前を追加する場合、選択された第２領域７７５ｂの第１隅７７２ｃは、隅７７２ｃから領域選択ターゲット７７４ａまでの衝動性動眼７７３ｃによって指定されてもよい。選択された第１領域７７５ａと同様に、ユーザは、選択された要素を含む第２領域７７５ｂの第２隅７７２ｄを探すために（作動ターゲット７７４ｂに対する衝動性動眼を行わない限り）任意の数の捜索的な眼球運動を行うことができる。ユーザは、第２隅を見つけると、領域７７５ｂの第２隅７７２ｄから、領域選択ターゲット７７４ａまでの衝動性動眼７７３ｄを行うことによって、選択された追加の名前（例えば、Art、Kris）を含む領域７７５ｂを画定することができる。別の方法として、隅７７２ａ、７７２ｂの選択順序を逆にしてもよい。
At this point, the impulsive occlusal eye from the
更なる名前をリストに追加しない場合、ユーザは、選択された領域７７５ｂの第２隅７７２ｄから作動ターゲット７７４ｂまでの（矢印の付いた破線７７７で示された）衝動性動眼を行うことによって、電子メール受信者のリストに第２組の名前を追加し、作動シーケンス内の（例えば、電子メールを送信する）機能を行うことができる。代替的に（眼球運動の全体の観点から見て効率が低いが）、領域選択ターゲット７７４ａに視線７７３ｄを移動した後、作動ターゲット７７４ｂに直接に視線を移動する衝動性動眼は、選択された機能を合併した第１組７７５ａおよび第７７５ｂ組の受信者に同様に適用することができる。
If no further name is added to the list, the user performs an impulsive eye movement (indicated by the dashed
図３６Ｃは、１つの追加の名前（Phil、７７６）を電子メール受信者のリストに追加するために行われる一連の追加の眼球運動を示している。追加の受信者７７６を探すために、領域選択ターゲット７７４ａからの衝動性動眼７７３ｅを行うことができる。追加の受信者を見つけると、単一の選択（Phil、７７６）から作動ターゲット７７４ｂまでの衝動性動眼７７３ｆを行うことによって、単一の要素をリストに追加し、選択された操作を実行することができる。この場合、単一の要素のみを選択するため、ディスプレイ７７０上に領域を定義する必要がない。しかしながら、領域選択ターゲット７７４ａを用いて、単一の要素を含む領域を選択し、その後、領域選択ターゲット７７４ａから作動ターゲット７７４ｂまでの衝動性動眼を行うことによって、同等の結果を達成することができる。
FIG. 36C shows a series of additional eye movements performed to add one additional name (Phil, 776) to the list of email recipients. Impulsive
特定の実施形態において説明した様々な構成要素および特徴は、意図した実施形態の用途に応じて、追加、削除、および／または他の実施形態のもので置換され得ることが理解される。 It is understood that the various components and features described in a particular embodiment may be added, deleted, and / or replaced with those of other embodiments, depending on the intended use of the embodiment.
さらに、代表的な実施形態を説明する際に、方法および／またはプロセスを特定の順序のステップとして記載する場合がある。しかしながら、方法またはプロセスは、本明細書に記載の特定の順序のステップに依存しない限り、記載された特定の順序のステップに制限されるべきではない。当業者が理解するように、他の順序のステップも可能である。したがって、明細書に記載されたステップの特定の順序は、特許請求の範囲に対する限定として解釈されるべきではない。 In addition, methods and / or processes may be described as steps in a particular sequence when describing typical embodiments. However, the method or process should not be restricted to the specific sequence of steps described unless it relies on the specific sequence of steps described herein. Other sequence steps are possible, as those skilled in the art will understand. Therefore, the particular order of steps described in the specification should not be construed as a limitation on the claims.
本発明は、様々な修正および代替形態が可能であるが、その特定の例示が図面に示されており、本明細書に詳細に記載されている。本発明は、開示された特定の形態または方法に制限されるものではなく、その逆、本発明は、添付の特許請求の範囲に含まれる全ての改変物、等価物および代替物を包含するものであることを理解されたい。 Various modifications and alternatives are possible of the present invention, the particular embodiment of which is shown in the drawings and described in detail herein. The present invention is not limited to the particular form or method disclosed, and vice versa, the invention includes all modifications, equivalents and alternatives within the appended claims. Please understand that.
Claims (28)
前記検出装置を用いて、前記ユーザの片目または両目が第１キーボード上の第１キーに向けられていることを特定するステップと、
前記検出装置を用いて、前記ユーザの片目または両目が前記第１キーから、所定の距離内にない、前記第１キーボードと同じ組のキーを有する第２キーボード上の第２キーに向けられた第１の１つまたは複数の衝動性動眼を特定するステップと、
前記検出装置を用いて、前記第１の１つまたは複数の衝動性動眼に対する前記第２キーの位置に向かう第１の１つまたは複数の補正された衝動性動眼が前記第２キーの位置から前記所定の距離内で完了したことを確認するステップと、
前記ユーザによる前記第２キーの認識を待たずに、前記第１キーおよび前記第２キーの一方または両方に関連する操作を行うステップとを含む、方法。 A method of using a detector to provide a graphical user interface for communicating the user's intent, at least in part, based on the eye movements of one or both eyes of the user.
Using the detection device, a step of identifying that one or both eyes of the user are directed to a first key on the first keyboard.
Using the detection device, one or both eyes of the user are directed from the first key to a second key on a second keyboard that has the same set of keys as the first keyboard and is not within a predetermined distance. The first step of identifying one or more impulsive oculomotors,
Using the detection device, the first one or more corrected impulsive occult eyes towards the position of the second key with respect to the first one or more impulsive eyes are from the position of the second key. The step of confirming that the work was completed within the predetermined distance, and
A method comprising performing an operation related to one or both of the first key and the second key without waiting for the user to recognize the second key.
前記検出装置を用いて、前記ユーザの片目または両目が前記第２キーから前記第１キーボードの第３キーに向けられた第２の１つまたは複数の衝動性動眼を特定するステップと、
前記検出装置を用いて、前記第２の１つまたは複数の衝動性動眼に対する第２の１つ又は複数の衝動性動眼が前記第１キーボードの前記第３キーから所定の距離内で完了したことを確認するステップと、
前記ユーザによる前記第１キーボードの前記第３キーの認識を待たずに、前記第３キーおよび前記第２キーのうちの一つ又は両方に関連する第２操作を行うステップとをさらに含む、請求項１〜７のいずれかに記載の方法。 A step of identifying the second key when one or both eyes of the user are pointed at the second keyboard using the detection device.
Using the detection device, one or more eyes of the user identify a second one or more impulsive oculomotor eyes directed from the second key to the third key of the first keyboard.
Using the detection device, the second one or more impulsive eyes for the second one or more impulsive eyes are completed within a predetermined distance from the third key of the first keyboard. Steps to check and
The claim further includes a step of performing a second operation related to the third key and one or both of the third key and the second key without waiting for the user to recognize the third key of the first keyboard. Item 8. The method according to any one of Items 1 to 7.
前記第２キーは、前記ディスプレイ上に表示されていない前記ユーザの視野に位置する、請求項１〜１３のいずれかに記載の方法。 The first key is located on the virtual keyboard displayed on the display.
The method according to any one of claims 1 to 13, wherein the second key is located in the user's field of view that is not displayed on the display.
前記第２キーボードは、前記第１キーボードに隣接しており、同じ向きにある、請求項１〜１６のいずれかに記載の方法。 The second keyboard has the same set of keys as the first keyboard in the same layout.
The method according to any one of claims 1 to 16, wherein the second keyboard is adjacent to the first keyboard and has the same orientation.
前記検出装置を用いて、前記ユーザの片目または両目が前記第１または第２キーボード上の第１キーの位置から所定の距離内に向けられていることを特定するステップを備え、前記第１キーボードまたは前記第２キーボードの少なくとも一つのキーは、単一の衝動性動眼を用いてアクティブになる一方、前記第１キーボードまたは前記第２キーボードの他のキーは、二つの衝動性動眼を用いてアクティブになり、前記第２キーボードは前記ユーザの両目に対して前記第１キーボードの垂直に隣接して同じ方向に配置されており、前記第２キーボードは、前記第１キーボードと同じレイアウトで同じキーの組を有しており、前記第１キーボードおよび前記第２キーボードのうちの１つまたは複数の最も頻繁に使用されるキーは、前記第１キーボードおよび前記第２キーボードの複数の行のうちの１または複数の中央の行を占めており、
前記検出装置を用いて、前記ユーザの片目または両目が前記キーボード上の前記第１キーから、所定の距離内にない、前記第１または第２のキーボード上の第２キーに向けられた第１の１つまたは複数の衝動性動眼を特定するステップと、
前記検出装置を用いて、前記１つまたは複数の衝動性動眼に対する前記第２キーの位置に向かう、１または複数の補正された衝動性動眼が前記第２キーの前記位置から所定の距離内で完了したことを確認するステップと、
前記第１キーおよび前記第２キーの一方または両方に関連する操作を行うステップとを含む、方法。 A method of using a first keyboard , a second keyboard and a detector to provide a graphical user interface for communicating the user's intent, at least partially based on the eye movements of one or both eyes of the user. ,
Using the detection device, comprising the step of identifying that one or both eyes of the user are directed within a predetermined distance from the position of the first key on the first or second keyboard, before Symbol first The keyboard or at least one key on the second keyboard is activated with a single impulsive eye, while the other keys on the first keyboard or the second keyboard are activated with two impulsive eyes. When activated, the second keyboard is arranged vertically adjacent to the user's eyes in the same direction as the first keyboard, and the second keyboard has the same layout and the same keys as the first keyboard. The most frequently used key of one or more of the first keyboard and the second keyboard is a plurality of lines of the first keyboard and the second keyboard. Occupies one or more central rows and
Using the detection device, the user's one or both eyes are directed from the first key on the keyboard to a second key on the first or second keyboard that is not within a predetermined distance. And the steps to identify one or more impulsive oculomotors
Using said detection device, said toward the position of the second key for one or more impulsivity oculomotor, one or more of the corrected shock volatility oculomotor is within a predetermined distance from the position of the second key Steps to confirm that it was completed in
A method comprising performing an operation related to one or both of the first key and the second key.
前記ユーザの一方または両方の目が前記第１または第２のキーボード上の前記第１キーの位置から所定の距離内にあるとき、前記検出装置で識別した後に、前記第１または第２のキーボード上の前記第２キーの最も頻繁に使用される特徴の輝度を変更することをさらに含む、請求項１８〜２２のいずれかに記載の方法。 The first keyboard and the second keyboard includes off display modified target for at least frequently used functions associated with one of the key one of the keyboard, the method comprising:
When the eyes of one or both of the user is in the first or second within a predetermined distance from the position of the first key on the keyboard, after identification by the detection device, the first or second keyboard The method of any of claims 18-22, further comprising changing the brightness of the most frequently used feature of the second key above.
少なくとも第１キーボードを表示するように構成されたプロジェクタと、
検出装置とを備え、前記検出装置は、
ユーザの片目または両目が第１キーボードの第１キーに向けられたときを識別するように構成されており、前記第１キーボードの少なくとも１つのキーは、単一の断続的な目の動きを使用して起動可能であり、前記第１キーボードの他のキーは、２つの断続的な目の動きを使用して起動可能であり、
前記第１キーから、予め定められた距離内にない、第２キーボードの第２キーに向かう前記ユーザの片目または両目の第１の１つまたは複数の断続性運動を識別し、
前記第１の１つまたは複数の断続性運動について前記第２キーの位置に向かう１つまたは複数の修正された断続性運動が、前記第２キーの位置から所定の距離以内に完了したことを確認するように構成されており、
プロセッサを備え、前記プロセッサは、前記ユーザによる第２キーの知覚を待つことなく、前記第１の１つまたは複数の断続性運動について前記１つまたは複数の修正された断続性運動が、前記第２キーの位置から前記所定の距離以内に完了したという確認に応答して、前記第１キーおよび前記第２キーの一方または両方に関連付けられた動作を実行するように構成されている、グラフィカルユーザインターフェイス。 A graphical user interface that determines a user's intent based at least in part on the movement of one or both eyes of the user.
With a projector configured to display at least the first keyboard,
The detection device is provided with a detection device.
It is configured to identify when one or both eyes of the user are directed to the first key of the first keyboard, at least one key of the first keyboard using a single intermittent eye movement. The other keys on the first keyboard can be activated using two intermittent eye movements.
Identifying one or more intermittent movements of the user's one or both eyes towards the second key of the second keyboard, which is not within a predetermined distance from the first key.
For the first one or more intermittent movements One or more modified intermittent movements towards the second key position have been completed within a predetermined distance from the second key position. It is configured to check and
The processor comprises the said one or more modified intermittent movements for the first one or more intermittent movements without waiting for the user to perceive the second key. A graphical user configured to perform an operation associated with one or both of the first key and the second key in response to confirmation that the two keys have been completed within the predetermined distance from the position of the two keys. Interface.
前記ユーザの片目または両目が前記第２キーボードに向けられたときに前記第２キーを識別し、
前記第２キーから前記第１キーボードの第３キーに向かう、前記ユーザの片目または両目の第２の１つまたは複数の断続性運動を識別し、
前記第２の１つまたは複数の断続性運動に対する第２の１つまたは複数の修正された断続性運動が、前記第１キーボードの前記第３キーから指定された距離内で完了したことを確認するように構成されており、
前記プロセッサは、前記ユーザによる前記第１キーボードの前記第３キーの知覚を待つことなく、前記第２の１つまたは複数の断続性運動に対する前記第２の１つまたは複数の修正された断続性運動が、前記第１キーボードの前記第３キーから前記指定された距離内で完了したという確認に応答して、前記第３キーおよび前記第２キーの一方または両方に関連付けられた動作を実行するように構成されている、請求項２４〜２７のいずれかに記載のグラフィカルユーザインターフェイス。 The detector identifies the second key when one or both eyes of the user are pointed at the second keyboard.
Identifying the second one or more intermittent movements of the user's one or both eyes from the second key to the third key of the first keyboard.
Confirm that the second modified intermittent movement for the second one or more intermittent movements has been completed within the specified distance from the third key of the first keyboard. Is configured to
The processor has the second one or more modified intermittentness to the second one or more intermittent movements without waiting for the user to perceive the third key of the first keyboard. In response to confirmation that the exercise has been completed within the specified distance from the third key of the first keyboard, perform the operation associated with one or both of the third key and the second key. 24. The graphical user interface according to any of claims 24 to 27.
Applications Claiming Priority (7)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201562205707P | 2015-08-15 | 2015-08-15 | |
US62/205,707 | 2015-08-15 | ||
US201562252347P | 2015-11-06 | 2015-11-06 | |
US62/252,347 | 2015-11-06 | ||
US201662319751P | 2016-04-07 | 2016-04-07 | |
US62/319,751 | 2016-04-07 | ||
PCT/US2016/047105 WO2017031089A1 (en) | 2015-08-15 | 2016-08-15 | Systems and methods for biomechanically-based eye signals for interacting with real and virtual objects |
Publications (3)
Publication Number | Publication Date |
---|---|
JP2018530798A JP2018530798A (en) | 2018-10-18 |
JP2018530798A5 JP2018530798A5 (en) | 2020-07-02 |
JP6865175B2 true JP6865175B2 (en) | 2021-04-28 |
Family
ID=58051236
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2017556858A Active JP6865175B2 (en) | 2015-08-15 | 2016-08-15 | Systems and methods for biomechanical visual signals to interact with real and virtual objects |
Country Status (7)
Country | Link |
---|---|
EP (1) | EP3335096B1 (en) |
JP (1) | JP6865175B2 (en) |
KR (1) | KR102196975B1 (en) |
CN (1) | CN108351685B (en) |
DE (1) | DE112016003719T5 (en) |
GB (1) | GB2561455B (en) |
WO (1) | WO2017031089A1 (en) |
Families Citing this family (78)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10884493B2 (en) | 2013-06-20 | 2021-01-05 | Uday Parshionikar | Gesture based user interfaces, apparatuses and systems using eye tracking, head tracking, hand tracking, facial expressions and other user actions |
US10489577B2 (en) * | 2016-08-11 | 2019-11-26 | Onenigma LLC | Identifying one or more users based on typing pattern and/or behavior |
US10627900B2 (en) * | 2017-03-23 | 2020-04-21 | Google Llc | Eye-signal augmented control |
US10216260B2 (en) * | 2017-03-27 | 2019-02-26 | Microsoft Technology Licensing, Llc | Selective rendering of sparse peripheral displays based on element saliency |
US11312458B2 (en) | 2017-04-25 | 2022-04-26 | Bae Systems Plc | Watercraft |
GB2561852A (en) * | 2017-04-25 | 2018-10-31 | Bae Systems Plc | Watercraft |
GB2561882A (en) * | 2017-04-27 | 2018-10-31 | Nokia Technologies Oy | Virtual reality viewport adaption |
CA3065131A1 (en) * | 2017-05-31 | 2018-12-06 | Magic Leap, Inc. | Eye tracking calibration techniques |
GB2563276B (en) * | 2017-06-09 | 2021-12-01 | Advanced Risc Mach Ltd | Virtual reality systems |
KR20200035003A (en) * | 2017-07-27 | 2020-04-01 | 소니 주식회사 | Information processing apparatus, information processing method, and program |
EP3672478A4 (en) | 2017-08-23 | 2021-05-19 | Neurable Inc. | Brain-computer interface with high-speed eye tracking features |
CN111052042B (en) | 2017-09-29 | 2022-06-07 | 苹果公司 | Gaze-based user interaction |
CN109697000B (en) * | 2017-10-24 | 2022-09-16 | 阿里巴巴集团控股有限公司 | Resource allocation method and related equipment, display method and related equipment |
US10484530B2 (en) | 2017-11-07 | 2019-11-19 | Google Llc | Sensor based component activation |
AU2018367510A1 (en) * | 2017-11-14 | 2020-06-25 | Vivid Vision, Inc. | Systems and methods for visual field analysis |
EP3716220B1 (en) * | 2017-11-20 | 2024-02-21 | Rakuten Group, Inc. | Information processing device, information processing method, and information processing program |
KR102601622B1 (en) | 2017-12-14 | 2023-11-10 | 매직 립, 인코포레이티드 | Contextual rendering of virtual avatars |
WO2019144196A1 (en) * | 2018-01-25 | 2019-08-01 | Psykinetic Pty Ltd | Eye-typing method, system and computer readable medium |
FR3077900B1 (en) * | 2018-02-12 | 2020-01-17 | Thales | PERIPHERAL VISION IN A MAN-MACHINE INTERFACE |
CN112041788B (en) | 2018-05-09 | 2024-05-03 | 苹果公司 | Selecting text input fields using eye gaze |
US11604661B2 (en) * | 2018-06-03 | 2023-03-14 | Apple Inc. | Determining relevant information based on third party information and user interactions |
US11450070B2 (en) | 2018-06-20 | 2022-09-20 | Hewlett-Packard Development Company, L.P. | Alerts of mixed reality devices |
WO2019245550A1 (en) | 2018-06-20 | 2019-12-26 | Hewlett-Packard Development Company, L.P. | Alerts of mixed reality devices |
DE102018210824B4 (en) * | 2018-07-02 | 2023-02-09 | Audi Ag | Method and system for illuminating an area in an area surrounding a vehicle |
US10581940B1 (en) * | 2018-08-20 | 2020-03-03 | Dell Products, L.P. | Head-mounted devices (HMDs) discovery in co-located virtual, augmented, and mixed reality (xR) applications |
US10616565B2 (en) * | 2018-08-21 | 2020-04-07 | The Boeing Company | System and method for foveated simulation |
KR20200029785A (en) * | 2018-09-11 | 2020-03-19 | 삼성전자주식회사 | Localization method and apparatus of displaying virtual object in augmented reality |
US10664050B2 (en) * | 2018-09-21 | 2020-05-26 | Neurable Inc. | Human-computer interface using high-speed and accurate tracking of user interactions |
WO2020076300A1 (en) * | 2018-10-09 | 2020-04-16 | Hewlett-Packard Development Company, L.P. | Selecting a display with machine learning |
WO2020081075A1 (en) | 2018-10-17 | 2020-04-23 | Google Llc | Processing fundus camera images using machine learning models trained using other modalities |
CN111281762A (en) * | 2018-12-07 | 2020-06-16 | 广州幻境科技有限公司 | Vision rehabilitation training method and system |
CN109324417A (en) * | 2018-12-13 | 2019-02-12 | 宜视智能科技（苏州）有限公司 | Typoscope and its control method, computer storage medium |
CN109799838B (en) * | 2018-12-21 | 2022-04-15 | 金季春 | Training method and system |
US11514602B2 (en) | 2018-12-25 | 2022-11-29 | Samsung Electronics Co., Ltd. | Method and apparatus for gaze estimation |
KR102237659B1 (en) * | 2019-02-21 | 2021-04-08 | 한국과학기술원 | Method for input and apparatuses performing the same |
CN109917914B (en) * | 2019-03-05 | 2022-06-17 | 河海大学常州校区 | Interactive interface analysis and optimization method based on visual field position |
JP7215246B2 (en) * | 2019-03-08 | 2023-01-31 | 株式会社Ｊｖｃケンウッド | Display device, display method, and display program |
EP3709263A1 (en) * | 2019-03-14 | 2020-09-16 | Siemens Healthcare GmbH | Method and system for monitoring a biological process |
CN111766939A (en) * | 2019-03-15 | 2020-10-13 | 苹果公司 | Attention direction on optical transmission display |
WO2020209171A1 (en) * | 2019-04-09 | 2020-10-15 | オムロン株式会社 | Iinformation processing device, information processing system, information processing method, and information processing program |
US11228810B1 (en) | 2019-04-22 | 2022-01-18 | Matan Arazi | System, method, and program product for interactively prompting user decisions |
US11882268B2 (en) * | 2019-05-30 | 2024-01-23 | Kyocera Corporation | Head-up display system and movable object |
US10997773B2 (en) * | 2019-06-13 | 2021-05-04 | Facebook Technologies, Llc | Dynamic tiling for foveated rendering |
US10976816B2 (en) | 2019-06-25 | 2021-04-13 | Microsoft Technology Licensing, Llc | Using eye tracking to hide virtual reality scene changes in plain sight |
US10901502B2 (en) * | 2019-06-27 | 2021-01-26 | Facebook, Inc. | Reducing head mounted display power consumption and heat generation through predictive rendering of content |
CN110399930B (en) * | 2019-07-29 | 2021-09-03 | 北京七鑫易维信息技术有限公司 | Data processing method and system |
KR102313622B1 (en) | 2019-08-21 | 2021-10-19 | 한국과학기술연구원 | Biosignal-based avatar control system and method |
CN110703916B (en) * | 2019-09-30 | 2023-05-09 | 恒信东方文化股份有限公司 | Three-dimensional modeling method and system thereof |
CA3058447A1 (en) * | 2019-10-10 | 2021-04-10 | Spacecard Inc. | A system for generating and displaying interactive mixed-reality video on mobile devices |
KR102128894B1 (en) * | 2019-10-10 | 2020-07-01 | 주식회사 메디씽큐 | A method and system for eyesight sensing of medical smart goggles |
CN110956082B (en) * | 2019-10-17 | 2023-03-24 | 江苏科技大学 | Face key point detection method and detection system based on deep learning |
US11430414B2 (en) * | 2019-10-17 | 2022-08-30 | Microsoft Technology Licensing, Llc | Eye gaze control of magnification user interface |
TWI734259B (en) * | 2019-11-14 | 2021-07-21 | 雲想科技股份有限公司 | Electronic signature authentication device and method |
US11176637B2 (en) | 2019-12-03 | 2021-11-16 | Facebook Technologies, Llc | Foveated rendering using eye motion |
PL3839411T3 (en) * | 2019-12-17 | 2023-12-27 | John Cockerill Defense SA | Smart system for controlling functions in a turret of a combat vehicle |
US11526159B2 (en) * | 2020-02-14 | 2022-12-13 | Rockwell Automation Technologies, Inc. | Augmented reality human machine interface testing |
CN111443796B (en) * | 2020-03-10 | 2023-04-28 | 维沃移动通信有限公司 | Information processing method and device |
WO2021194487A1 (en) * | 2020-03-25 | 2021-09-30 | Hewlett-Packard Development Company, L.P. | Head-related transfer functions with antropometric measurements |
CN111861900A (en) * | 2020-06-05 | 2020-10-30 | 北京迈格威科技有限公司 | Video image matting and noise reduction method and device |
CN111930234A (en) * | 2020-08-05 | 2020-11-13 | 西安闻泰电子科技有限公司 | Input method and device based on eye control, electronic equipment and storage medium |
DE102020210173A1 (en) | 2020-08-11 | 2022-02-17 | Volkswagen Aktiengesellschaft | Method and system for supporting a driver of a vehicle when there are bottlenecks in a route of the vehicle |
KR102495213B1 (en) | 2020-11-05 | 2023-02-07 | 한국전자통신연구원 | Apparatus and method for experiencing augmented reality-based screen sports |
CA3206042A1 (en) | 2021-01-22 | 2022-07-28 | Sarah Nancy STAAB | Visual data management system and method |
CN112952838B (en) * | 2021-01-29 | 2023-08-11 | 中国电力科学研究院有限公司 | Intelligent tide analysis method and device based on eye movement equipment |
CN112906346B (en) * | 2021-02-09 | 2023-07-04 | 江苏徐工工程机械研究院有限公司 | System and method for designing character size of engineering machinery display |
US11769134B2 (en) * | 2021-03-22 | 2023-09-26 | International Business Machines Corporation | Multi-user interactive ad shopping using wearable device gestures |
WO2022265654A1 (en) * | 2021-06-18 | 2022-12-22 | Hewlett-Packard Development Company, L.P. | Image analysis to switch audio devices |
US20230030433A1 (en) * | 2021-07-27 | 2023-02-02 | App-Pop-Up Inc. | System and method for adding and simultaneously displaying auxiliary content to main content displayed via a graphical user interface (gui) |
CN114115230B (en) * | 2021-10-25 | 2023-10-03 | 武汉理工大学 | Man-machine cooperative ship remote driving control method, system, device and medium |
KR102655829B1 (en) * | 2021-11-08 | 2024-04-08 | 엄태경 | Monitor for posture correction and adjusting method of the same |
WO2023104286A1 (en) * | 2021-12-07 | 2023-06-15 | Ericsson | Rendering of virtual keyboards in virtual environments |
CN114265501B (en) * | 2021-12-22 | 2024-01-26 | 南京紫金体育产业股份有限公司 | Dynamic identification system for motion scene |
JP2023115808A (en) | 2022-02-08 | 2023-08-21 | キヤノン株式会社 | Electronic device, electronic device control method, program, and recording medium |
CN114420131B (en) * | 2022-03-16 | 2022-05-31 | 云天智能信息(深圳)有限公司 | Intelligent voice auxiliary recognition system for weak eyesight |
US11620000B1 (en) * | 2022-03-31 | 2023-04-04 | Microsoft Technology Licensing, Llc | Controlled invocation of a precision input mode |
EP4258085A1 (en) * | 2022-04-04 | 2023-10-11 | Deutsche Telekom AG | Control of a cursor when using virtual screens |
CN115686292B (en) * | 2023-01-03 | 2023-07-25 | 京东方艺云(杭州)科技有限公司 | Information display method and device, electronic equipment and storage medium |
CN117632330A (en) * | 2023-10-12 | 2024-03-01 | 浙江大学 | Interactive target layout method and system of eye control interface in virtual environment |
Family Cites Families (24)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US4737040A (en) * | 1985-02-15 | 1988-04-12 | Moon Tag Y | Keyboard device and method for entering Japanese language text utilizing Romaji character notation |
CA2126142A1 (en) * | 1994-06-17 | 1995-12-18 | David Alexander Kahn | Visual communications apparatus |
US7438414B2 (en) * | 2005-07-28 | 2008-10-21 | Outland Research, Llc | Gaze discriminating electronic control apparatus, system, method and computer program product |
US8120577B2 (en) * | 2005-10-28 | 2012-02-21 | Tobii Technology Ab | Eye tracker with visual feedback |
BRPI0712837B8 (en) * | 2006-06-11 | 2021-06-22 | Volvo Tech Corporation | method for determining and analyzing a location of visual interest |
FR2912274B1 (en) * | 2007-02-02 | 2009-10-16 | Binocle Sarl | METHOD FOR CONTROLLING A VOLUNTARY OCULAR SIGNAL, IN PARTICULAR FOR SHOOTING |
WO2009019760A1 (en) * | 2007-08-07 | 2009-02-12 | Osaka Electro-Communication University | Moving object detector, moving object detection method, pointing device, computer program and storage medium |
US20120105486A1 (en) * | 2009-04-09 | 2012-05-03 | Dynavox Systems Llc | Calibration free, motion tolerent eye-gaze direction detector with contextually aware computer interaction and communication methods |
US9213405B2 (en) * | 2010-12-16 | 2015-12-15 | Microsoft Technology Licensing, Llc | Comprehension and intent-based content for augmented reality displays |
US8885877B2 (en) * | 2011-05-20 | 2014-11-11 | Eyefluence, Inc. | Systems and methods for identifying gaze tracking scene reference locations |
US8994522B2 (en) * | 2011-05-26 | 2015-03-31 | General Motors Llc | Human-machine interface (HMI) auto-steer based upon-likelihood to exceed eye glance guidelines |
US9766700B2 (en) * | 2011-12-14 | 2017-09-19 | Intel Corporation | Gaze activated content transfer system |
KR101850035B1 (en) * | 2012-05-02 | 2018-04-20 | 엘지전자 주식회사 | Mobile terminal and control method thereof |
CN104395857A (en) * | 2012-05-09 | 2015-03-04 | 英特尔公司 | Eye tracking based selective accentuation of portions of a display |
US8917238B2 (en) * | 2012-06-28 | 2014-12-23 | Microsoft Corporation | Eye-typing term recognition |
CN103631365B (en) * | 2012-08-22 | 2016-12-21 | 中国移动通信集团公司 | A kind of terminal input control method and device |
US9176581B2 (en) * | 2012-09-28 | 2015-11-03 | Intel Corporation | System and method for inferring user intent based on eye movement during observation of a display screen |
KR20140073730A (en) * | 2012-12-06 | 2014-06-17 | 엘지전자 주식회사 | Mobile terminal and method for controlling mobile terminal |
US20140173407A1 (en) * | 2012-12-17 | 2014-06-19 | Empire Technology Development Llc | Progressively triggered auto-fill |
CN105339866B (en) * | 2013-03-01 | 2018-09-07 | 托比股份公司 | Interaction is stared in delay distortion |
US10025378B2 (en) * | 2013-06-25 | 2018-07-17 | Microsoft Technology Licensing, Llc | Selecting user interface elements via position signal |
EP3057508B1 (en) * | 2013-10-17 | 2020-11-04 | Children's Healthcare Of Atlanta, Inc. | Methods for assessing infant and child development via eye tracking |
KR20160109443A (en) * | 2015-03-11 | 2016-09-21 | 주식회사 비주얼캠프 | Display apparatus using eye-tracking and method thereof |
KR101857466B1 (en) * | 2017-06-16 | 2018-05-15 | 주식회사 비주얼캠프 | Head mounted display and method for calibrating the same |
-
2016
- 2016-08-15 EP EP16837677.0A patent/EP3335096B1/en active Active
- 2016-08-15 JP JP2017556858A patent/JP6865175B2/en active Active
- 2016-08-15 GB GB1803620.2A patent/GB2561455B/en active Active
- 2016-08-15 WO PCT/US2016/047105 patent/WO2017031089A1/en active Application Filing
- 2016-08-15 KR KR1020177031497A patent/KR102196975B1/en active IP Right Grant
- 2016-08-15 DE DE112016003719.8T patent/DE112016003719T5/en not_active Ceased
- 2016-08-15 CN CN201680031080.4A patent/CN108351685B/en active Active
Also Published As
Publication number | Publication date |
---|---|
WO2017031089A1 (en) | 2017-02-23 |
CN108351685B (en) | 2022-07-08 |
KR20180083252A (en) | 2018-07-20 |
JP2018530798A (en) | 2018-10-18 |
CN108351685A (en) | 2018-07-31 |
GB2561455A (en) | 2018-10-17 |
GB201803620D0 (en) | 2018-04-18 |
EP3335096B1 (en) | 2022-10-05 |
EP3335096A4 (en) | 2019-03-06 |
GB2561455B (en) | 2022-05-18 |
DE112016003719T5 (en) | 2018-05-09 |
EP3335096A1 (en) | 2018-06-20 |
KR102196975B1 (en) | 2020-12-30 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
JP6865175B2 (en) | Systems and methods for biomechanical visual signals to interact with real and virtual objects | |
US10564714B2 (en) | Systems and methods for biomechanically-based eye signals for interacting with real and virtual objects | |
US11747618B2 (en) | Systems and methods for sign language recognition | |
CN112507799B (en) | Image recognition method based on eye movement fixation point guidance, MR glasses and medium | |
US10082940B2 (en) | Text functions in augmented reality | |
US11899735B2 (en) | Systems and methods for altering display parameters for users with epilepsy | |
Ding | User-generated vocabularies on Assistive/Access Technology | |
US20230401795A1 (en) | Extended reality based digital assistant interactions | |
US20240152256A1 (en) | Devices, Methods, and Graphical User Interfaces for Tabbed Browsing in Three-Dimensional Environments | |
US20240103676A1 (en) | Methods for interacting with user interfaces based on attention | |
US20150054747A1 (en) | Circular Keyboard | |
Ward | Testing methods to shift visual attention from wearable head-up displays to real-world locations | |
WO2022238936A2 (en) | Systems and methods for making websites accessible |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20180831 |
|
A711 | Notification of change in applicant |
Free format text: JAPANESE INTERMEDIATE CODE: A711Effective date: 20180926 |
|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A821Effective date: 20180926 |
|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20190805 |
|
A621 | Written request for application examination |
Free format text: JAPANESE INTERMEDIATE CODE: A621Effective date: 20190805 |
|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20200515 |
|
A871 | Explanation of circumstances concerning accelerated examination |
Free format text: JAPANESE INTERMEDIATE CODE: A871Effective date: 20200515 |
|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20200520 |
|
A975 | Report on accelerated examination |
Free format text: JAPANESE INTERMEDIATE CODE: A971005Effective date: 20200821 |
|
A977 | Report on retrieval |
Free format text: JAPANESE INTERMEDIATE CODE: A971007Effective date: 20200826 |
|
A131 | Notification of reasons for refusal |
Free format text: JAPANESE INTERMEDIATE CODE: A131Effective date: 20200901 |
|
A601 | Written request for extension of time |
Free format text: JAPANESE INTERMEDIATE CODE: A601Effective date: 20201201 |
|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20210129 |
|
TRDD | Decision of grant or rejection written | ||
A01 | Written decision to grant a patent or to grant a registration (utility model) |
Free format text: JAPANESE INTERMEDIATE CODE: A01Effective date: 20210309 |
|
A61 | First payment of annual fees (during grant procedure) |
Free format text: JAPANESE INTERMEDIATE CODE: A61Effective date: 20210405 |
|
R150 | Certificate of patent or registration of utility model |
Ref document number: 6865175Country of ref document: JPFree format text: JAPANESE INTERMEDIATE CODE: R150 |
|
R250 | Receipt of annual fees |
Free format text: JAPANESE INTERMEDIATE CODE: R250 |