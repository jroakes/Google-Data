CN113168354A - System and method for selecting and providing available actions to a user from one or more computer applications - Google Patents
System and method for selecting and providing available actions to a user from one or more computer applications Download PDFInfo
- Publication number
- CN113168354A CN113168354A CN201980081101.7A CN201980081101A CN113168354A CN 113168354 A CN113168354 A CN 113168354A CN 201980081101 A CN201980081101 A CN 201980081101A CN 113168354 A CN113168354 A CN 113168354A
- Authority
- CN
- China
- Prior art keywords
- application
- user
- output
- computing device
- indicator
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
- 230000009471 action Effects 0.000 title claims abstract description 243
- 238000004883 computer application Methods 0.000 title claims abstract description 188
- 238000000034 method Methods 0.000 title claims description 36
- 230000004044 response Effects 0.000 claims abstract description 55
- 238000013473 artificial intelligence Methods 0.000 claims description 31
- 230000003993 interaction Effects 0.000 claims description 13
- 238000010801 machine learning Methods 0.000 claims description 3
- 238000012549 training Methods 0.000 description 13
- 238000012552 review Methods 0.000 description 11
- 238000013519 translation Methods 0.000 description 11
- 230000014616 translation Effects 0.000 description 11
- 230000000007 visual effect Effects 0.000 description 10
- 230000004308 accommodation Effects 0.000 description 9
- 238000013528 artificial neural network Methods 0.000 description 9
- 238000010586 diagram Methods 0.000 description 8
- 230000007613 environmental effect Effects 0.000 description 8
- 238000004891 communication Methods 0.000 description 7
- 230000010006 flight Effects 0.000 description 7
- 238000012545 processing Methods 0.000 description 6
- 230000008569 process Effects 0.000 description 5
- 241001417524 Pomacanthidae Species 0.000 description 4
- 238000005516 engineering process Methods 0.000 description 4
- 230000008901 benefit Effects 0.000 description 3
- 230000000306 recurrent effect Effects 0.000 description 3
- 208000032041 Hearing impaired Diseases 0.000 description 2
- 230000002730 additional effect Effects 0.000 description 2
- 230000004075 alteration Effects 0.000 description 2
- 235000013305 food Nutrition 0.000 description 2
- 238000012986 modification Methods 0.000 description 2
- 230000004048 modification Effects 0.000 description 2
- 238000007792 addition Methods 0.000 description 1
- 239000003086 colorant Substances 0.000 description 1
- 238000013461 design Methods 0.000 description 1
- 230000000694 effects Effects 0.000 description 1
- 230000006870 function Effects 0.000 description 1
- 235000012054 meals Nutrition 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 230000011664 signaling Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/46—Multiprogramming arrangements
- G06F9/54—Interprogram communication
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L67/00—Network arrangements or protocols for supporting network services or applications
- H04L67/50—Network services
- H04L67/60—Scheduling or organising the servicing of application requests, e.g. requests for application data transmissions using the analysis and optimisation of the required network resources
- H04L67/63—Routing a service request depending on the request content or context
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/953—Querying, e.g. by the use of web search engines
- G06F16/9535—Search customisation based on user profiles and personalisation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/953—Querying, e.g. by the use of web search engines
- G06F16/9537—Spatial or temporal dependent retrieval, e.g. spatiotemporal queries
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
- G06N3/0442—Recurrent networks, e.g. Hopfield networks characterised by memory or gating, e.g. long short-term memory [LSTM] or gated recurrent units [GRU]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N5/00—Computing arrangements using knowledge-based models
- G06N5/02—Knowledge representation; Symbolic representation
- G06N5/022—Knowledge engineering; Knowledge acquisition
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
Abstract
A computing system may be configured to input a model input including context data into a machine-learned model and receive a model output describing one or more semantic entities referenced by the context data. The computing system may be configured to provide data describing the one or more semantic entities to the computer application and to receive application output from the respective computing application in response to providing the data describing the one or more semantic entities to the computer application. The application output received from each computer application may describe available actions of the corresponding computer application with respect to one or more semantic entities. The computing system may be configured to provide at least one indicator to a user describing available actions of the corresponding computer application with respect to the one or more semantic entities.
Description
Technical Field
The present disclosure relates generally to interaction between a computing system and a computer application operable on the computing system. More particularly, the present disclosure relates to a system and related method for selecting and providing available actions to a user from one or more computer applications.
Background
Computing devices (e.g., desktop computers, laptop computers, tablet computers, smart phones, wearable computing devices, etc.) are ubiquitous in modern society. They may support communication between their users, providing their users with information about their environment, current events, the entire world, etc. A large number of different computer applications are operable on such computing devices to perform a wide variety of actions. Typically, a user must manually select a particular computer application based on the action the user wishes to perform.
Disclosure of Invention
Aspects and advantages of embodiments of the present disclosure will be set forth in part in the description which follows, or may be learned by practice of the embodiments.
One example aspect of the present disclosure is directed to a computing system. The computing system may include at least one processor. The computing system may include a machine-learned model configured to receive a model input including context data, and in response to receipt of the model input, output a model output describing one or more semantic entities referenced by the context data. A computing system may include one or more computer applications. The computing system may include at least one tangible, non-transitory computer-readable medium storing instructions that, when executed by at least one processor, cause the at least one processor to perform operations. The operations may include inputting a model input into the machine-learned model and receiving an output as the machine-learned model. The model output may describe one or more semantic entities referenced by the context data. The operations may include providing data describing one or more semantic entities to one or more computer applications. The operations may include receiving one or more application outputs from one or more computing applications, respectively, in response to providing data describing the one or more semantic entities to the one or more computer applications. The application output received from each computer application may describe one or more available actions of the corresponding computer application with respect to one or more semantic entities. The operations may include providing at least one indicator to a user of the computing system. The at least one indicator may describe at least one of the one or more available actions of the corresponding computer application with respect to the one or more semantic entities.
Another example aspect of the present disclosure is directed to a computer-implemented method for selecting and providing available actions to a user from one or more computer applications. The method may include inputting, by one or more computing devices, a model input including context data into a machine-learned model, the machine-learned model configured to receive the model input, and outputting, in response to receipt of the model input, a model output, the model output describing one or more semantic entities referenced by the context data. The method may include receiving, by one or more computing devices, the model output as an output of a machine-learned model. The model output may describe one or more semantic entities referenced by the context data. The method may include providing, by one or more computing devices, data describing one or more semantic entities to one or more computer applications. The method may include receiving, by the one or more computing devices, one or more application outputs from the one or more computing applications, respectively, in response to providing the data describing the one or more semantic entities to the one or more computer applications. The application output received from each computer application may describe one or more available actions of the corresponding computer application with respect to one or more semantic entities. The method may include providing, by one or more computing devices, at least one indicator to a user of the computing system. The at least one indicator may describe at least one of the one or more available actions of the corresponding computer application with respect to the one or more semantic entities.
Other aspects of the disclosure are directed to various systems, apparatuses, non-transitory computer-readable media, user interfaces, and electronic devices.
These and other features, aspects, and advantages of various embodiments of the present disclosure will become better understood with reference to the following description and appended claims. The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate exemplary embodiments of the disclosure and together with the description, serve to explain the relevant principles.
Drawings
A detailed discussion of embodiments directed to one of ordinary skill in the art is set forth in the specification, which makes reference to the appended figures, in which:
fig. 1A depicts a block diagram of an example computing system, according to an example embodiment of the present disclosure.
Fig. 1B depicts a block diagram of an example computing system, according to an example embodiment of the present disclosure.
Fig. 1C depicts a block diagram of an example computing system, according to an example embodiment of the present disclosure.
FIG. 1D depicts an example user computing device configured to provide data describing one or more semantic entities to one or more computer applications, receive application output from the computer applications, and provide indicators describing available actions that may be performed by the computer applications to a user.
Fig. 2A depicts a machine-learned recognition model according to an example embodiment of the present disclosure.
Fig. 2B depicts a machine-learned ranking model according to an example embodiment of the present disclosure.
FIG. 3 depicts a flow diagram of an example method for selecting and providing available actions to a user from one or more computer applications.
FIG. 4 depicts an example mobile computing device displaying a pointer in a user interface showing available actions from a computer application, in accordance with aspects of the present disclosure.
Fig. 5A depicts an example mobile computing device in a first state where the mobile computing device is displaying a text message exchange in a user interface, in accordance with aspects of the present disclosure.
Fig. 5B depicts the mobile computing device of fig. 5A in a second state in which the mobile computing device is providing indicators from multiple computer applications of available actions with respect to recognized text from the text message exchange of fig. 5A, in accordance with aspects of the present disclosure.
FIG. 6 depicts an example mobile computing device displaying a text message notification in a user interface and indicators from multiple computer applications regarding available actions for recognized text from a text message, in accordance with aspects of the present disclosure.
Fig. 7A depicts an example mobile computing device in a first state in which ambient audio referencing a historic person is detected and an indicator is displayed in a lock screen of the mobile computing device, according to aspects of the present disclosure.
FIG. 7B depicts the example mobile computing device of FIG. 7A in a second state in which indicators describing available actions with respect to a historian character are displayed, in accordance with aspects of the present disclosure.
Fig. 7C depicts the example mobile computing device of fig. 7A in a third state, where the indicator has been "bookmarked" for later viewing, in accordance with aspects of the present disclosure.
Fig. 8A depicts an example mobile computing device in a first state with video and indicators displayed in a user interface of the mobile device, in accordance with aspects of the present disclosure.
Fig. 8B depicts the example mobile computing device in a second state in which additional indicators describing available actions associated with the movie, such as purchasing movie tickets, are displayed in the user interface, in accordance with aspects of the present disclosure.
Fig. 9A depicts an example mobile computing device in a first state with a video and a pointer displayed in a user interface, in accordance with aspects of the present disclosure.
FIG. 9B depicts the example mobile computing device of FIG. 9A in a second state, where the available actions associated with the indicator of FIG. 9A are being performed.
Fig. 10A depicts an example mobile computing device displaying text in a user interface of the mobile computing device, in accordance with aspects of the present disclosure.
Fig. 10B depicts the example mobile computing device of fig. 10A in a second state in which the movie title is highlighted in the text and an indicator describing available actions with respect to the movie title is displayed, in accordance with aspects of the present disclosure.
Fig. 10C depicts the example mobile computing device of fig. 10A in a third state in which indicators describing further available actions with respect to movie titles are displayed, in accordance with aspects of the present disclosure.
Fig. 11A depicts an example mobile computing device in a first state with text highlighted and an indicator displayed in a user interface of the mobile computing device, in accordance with aspects of the present disclosure.
FIG. 11B depicts the example mobile computing device 1100 of FIG. 11A in a second state in which indicators are displayed that provide additional information about a selected action described by one of the indicators of FIG. 11A, in accordance with aspects of the present disclosure.
Fig. 12A depicts an example mobile computing device in which text is displayed in a user interface of the mobile computing device, in accordance with aspects of the present disclosure.
FIG. 12B depicts the example mobile computing device of FIG. 12A in a second state in which indicators describing available actions with respect to text portions are displayed, in accordance with aspects of the present disclosure.
FIG. 12C depicts the example mobile computing device of FIG. 12A in a third state in which additional information regarding the selected indicator of FIG. 12A is displayed, along with additional indicators describing further available actions, displayed in accordance with aspects of the present disclosure.
Fig. 13A depicts an example mobile computing device in a first state in which the mobile computing device has been processing audio during a phone call, in accordance with aspects of the present disclosure.
Fig. 13B depicts the example mobile computing device of fig. 13A in a second state in which indicators describing available actions with respect to semantic entities captured from audio of a phone call are displayed, in accordance with aspects of the present disclosure.
Fig. 13C depicts the example mobile computing device of fig. 13A in a third state in which indicators describing further available actions with respect to semantic entities captured from audio of a phone call are displayed, in accordance with aspects of the present disclosure.
Fig. 14A depicts an example mobile computing device in camera mode, where images from a camera of the mobile computing device are displayed in a user interface of the mobile computing device, in accordance with aspects of the present disclosure.
Fig. 14B depicts the example mobile computing device of fig. 14A in a second state in which indicators describing available actions with respect to landmarks depicted in images from a camera are displayed, in accordance with aspects of the present disclosure.
Fig. 14C depicts the example mobile computing device of fig. 14A in a third state in which indicators describing further available actions with respect to landmarks depicted in images from the camera are displayed, in accordance with aspects of the present disclosure.
Fig. 15A depicts an example mobile computing device displaying text in a user interface of the mobile computing device, in accordance with aspects of the present disclosure.
FIG. 15B depicts the example mobile computing device of FIG. 15A in a second state in which indicators describing available actions with respect to text portions are displayed, in accordance with aspects of the present disclosure.
FIG. 15C depicts the example mobile computing device of FIG. 15A in a third state in which indicators describing further available actions with respect to text portions are displayed, in accordance with aspects of the present disclosure.
Fig. 16A depicts an example mobile computing device in a first state in which environmental audio in spanish spoken language is detected and an indicator describing available actions is displayed in a lock screen of the mobile computing device, the available actions including translating the spanish spoken language into english according to aspects of the present disclosure.
FIG. 16B depicts the example mobile computing device of FIG. 16A in a second state, in which indicators describing available actions with respect to Spanish spoken language are displayed.
FIG. 16C depicts the example mobile computing device of FIG. 16A in a third state in which indicators describing further available actions with respect to translation are displayed, in accordance with aspects of the present disclosure.
Fig. 17A depicts an example mobile computing device in which an indicator showing available actions is displayed in a lock screen of the mobile computing device based on calendar data including upcoming flights, in accordance with aspects of the present disclosure.
FIG. 17B depicts the example mobile computing device of FIG. 17A in a second state in which indicators describing further available actions with respect to calendar data are displayed, in accordance with aspects of the present disclosure.
Fig. 18A depicts an example mobile computing device in which a pointer is displayed in a lock screen of the mobile computing device showing available actions based on the location of the mobile computing device, in accordance with aspects of the present disclosure.
Fig. 18B depicts the example mobile computing device of fig. 18A in a second state in which indicators are displayed that describe further available actions based on the location of the mobile computing device, in accordance with aspects of the present disclosure.
Repeated reference numerals in the various figures are intended to identify identical features in the various implementations.
Detailed Description
SUMMARY
Example aspects of the present disclosure are directed to systems and related methods for selecting and providing available actions to a user from one or more computer applications. The systems and related methods herein can provide a system or platform (e.g., including a predefined Application Programming Interface (API)) by which a computing system can intelligently provide hints, suggestions, or hints (e.g., within a "surface chip") to a user regarding contextually relevant actions that can be performed by one or more computing applications, such as an application located on a mobile device. In doing so, the system and related methods may assist the user in performing technical tasks by way of a guided human-machine interaction process, as discussed in more detail further below. The computing system may include an artificial intelligence system (e.g., including one or more machine-learned models) that detects semantic entities from contextual data (e.g., information displayed, detected, or otherwise processed by the computing system). The computing system may query or otherwise interact with the computer application, for example, through a predefined API, to receive available context-related actions from the computer application. The predefined API may describe rules, protocols, or other information about how the computing system and computer application interact. One or more indicators (e.g., visual indicators) may be provided to the user to indicate available actions of the computer application. This configuration or platform may encourage application developers to design applications that are configured to interface with the computing systems described herein. As a result, the computing system can provide relevant prompts, suggestions, or hints from the computer application to the user in a more reliable, intelligent, and helpful manner, for example, to guide the user in performing technical tasks.
As one example, the computing system may identify one or more semantic entities, such as restaurant names, referenced by text displayed in the user interface. The computing system may provide data describing the semantic entities to the computer application (e.g., according to a predefined API or other protocol). The data may include a restaurant name, and/or additional information regarding a location or context in which the restaurant name is displayed in a user interface (e.g., in a text messaging application, in an article displayed in a web browsing application, etc.). The computing system may receive (e.g., according to a predefined API or other protocol) one or more application outputs from the computer application in response to the data describing the semantic entity. For example, a computer application may format, communicate, and/or alert the computing system with respect to application output according to rules or protocols specified by a predefined API. The computing system may provide an indicator to the user that describes or suggests an available action from the computer application (e.g., translate text displayed with a restaurant name, make a reservation using a reservation application, read a comment using a comment application, and/or navigate to the restaurant using a navigation application).
Importantly, the user may be provided with controls that allow the user to make selections as to whether and when the systems, programs, or features described herein may enable the collection of user information (e.g., environmental audio, text presented in a user interface, etc.). In addition, certain data may be processed in one or more ways before being stored or used, such that personally identifiable information is removed. For example, the identity of the user may be processed such that no personally identifiable information is determined for the user. Thus, the user may have control over what information the user collects, how the information is used, and what information is provided to the user.
According to one aspect of the present disclosure, a computing system may include a machine-learned model (e.g., a recognition model) and one or more computer applications. The machine-learned model may be configured to receive a model input including context data and output a model output describing one or more semantic entities referenced by the model input. Examples of context data may include text displayed in a user interface, audio played or processed by a computing system, audio detected by a computing system, information about a location of a user (e.g., a location of a mobile computing device of a computing system), calendar data, and/or contact data. For example, the context data may include ambient audio detected by a microphone of the computing system and/or telephone audio processed during a telephone call. The calendar data may describe future events or plans (e.g., flights, hotel reservations, dinner plans, etc.). Example semantic entities that may be described by the model output include words or phrases identified in text and/or audio. Additional examples include information about the user's location, such as city names, state names, street names, names of nearby attractions, and so forth.
The computing system may be configured to provide data describing the one or more semantic entities to one or more computer applications. The data may be or may include semantic entities and/or additional contextual information about the semantic entities. For example, the data may include, for example, translations of text into other languages, text subtitles for audio played by the computing system, information about entry into one or more particular buildings (e.g., for a wheelchair user), or names of songs identified in video played by the computing system. The data may also include other associated information, such as the name of the artist performing the song.
The computing system may be configured to receive one or more application outputs from one or more computing applications in response to providing data describing one or more semantic entities. The application output received from each computer application may describe one or more available actions of the corresponding computer application with respect to one or more semantic entities. To use the above examples, the application output may include a translation to text in one or more other languages, text captioning for audio played by the computing system (e.g., for a hearing impaired user), information about entry into one or more particular buildings (e.g., for a wheelchair user), or the name of a song and/or artist that may be streamed by the music streaming application. As another example, in response to receiving a semantic entity that includes a city name (e.g., a city that the user plans to visit), the application output may include potential lodging and/or car rentals that are available from the travel reservation application at the city.
The computing system may be configured to provide at least one indicator to a user that describes one or more available actions of a corresponding computer application. The indicator may be or may include a graphical indicator presented in the user interface, such as a word or phrase describing the available actions. For example, in the above example with respect to song titles, the indicator may be or may include a button labeled "Play (Play)" next to the song title identified in the video. As another example, the graphical indicator may include a translation to text in other languages, a text caption of the played audio, and/or other accessibility information, such as the location of a wheelchair portal. As another example, the graphical indicator may be or may include a movie time and/or theater location based on semantic entities including a movie title that is displayed in the user interface or included in the ambient audio (e.g., audio processed by the computing device and/or detected by a microphone of the computing device). As yet another example, the indicator may comprise an audio indicator that is played to the user. For example, the audio indicator may include a voice or other sound that asks the user whether the text or environmental audio present in the user interface should be translated into a different language.
The indicator may also include or describe identifying information about the computer application that may be used to perform the action. As one example, in some implementations, a computer application may provide a stylized output describing aesthetic features, which may include identifying information. The computing system may display an indicator in the user interface based on the stylized output. The aesthetic features may include one or more of a name, logo, font, color, shape, location within a user interface, and/or any other suitable visual characteristic of the computer application.
In some implementations, the computing system may include an artificial intelligence system (e.g., "Google Assistant"). The artificial intelligence system may include a machine-learned model and perform some or all of the operations described herein. The artificial intelligence system can be separate and distinct from, but capable of communicating with, one or more computer applications. Via a predefined application programming interface, the artificial intelligence system can provide data describing one or more semantic entities to the computer application and can receive application output from the computing application, respectively.
In some implementations, some or all of the operations described herein may be performed proactively, without requiring user input for their performance. As a user uses the computing device, the computing system (e.g., artificial intelligence system) may identify the context data without requiring user input requesting such action. For example, the system may proactively provide an interpretation of text, text captioning of audio, and/or other accessibility information without a prior user input request for that information. For example, where a user is known to be interested in receiving certain types of information, such information may be provided. For example, a user may be reading an article displayed in a user interface. The computing system may input some or all of the text of the article into a machine-learned model and receive a model output that describes semantic entities within the text. Semantic entities may include names or products, unobstructed entrances and exits, people, events, or other items of interest. The computing system may present one or more indicators in the user interface that describe the available actions from the computer application. The indicator may include emphasis (emphasis) of the semantic entity (e.g., highlighting, outlining, underlining, etc.) optionally in conjunction with further information about the available actions. For example, the semantic entities may include locations of unobstructed entrances and/or exits of buildings, and the indicators may inform the user that the navigation application is available to navigate the user to the entrances/exits along a suitable unobstructed route (e.g., a route suitable for a wheelchair). In another example, the semantic entity may include a name of a product mentioned in the text of the article, and the indicator may inform the user that a shopping application (e.g., Amazon application) is available to perform an action, such as purchasing an item.
However, in some implementations, at least the operation of providing at least one indicator may be performed reactively in response to user input. The user may perform an action that specifically requests an available action (e.g., long press, double click, speak a request, etc.) from one or more computer applications. Alternatively, the user may perform actions that do not specifically request an available action, such as highlighting text to copy and paste. The computing system may provide an indicator in response to the user highlighting the text. For example, a user may highlight the name of a city, and the computing system may provide an indicator describing available actions with respect to the city (e.g., booking accommodation in the city).
In some implementations, a computing system may be configured to provide an indicator that describes less than all available actions corresponding to application output received by the computing system. In other words, the computing system may receive multiple application outputs (e.g., from multiple computer applications) that describe multiple available actions.
The computing system may select a subset, such as a proper subset, of the plurality of available actions to provide to the user, for example, based on various factors or data. Examples include relevance to one or more semantic entities, past user interactions, types of one or more semantic entities, or types of one or more available actions.
As an example, the available actions may be selected based on a comparison between the type of semantic entity and the type of available action. Semantic entity types may include sights (e.g., restaurants, landmarks, etc.), media (e.g., text, songs, videos, etc.), locations (e.g., cities, streets, etc.), products, and/or people (e.g., historical characters, authors, contacts, etc.). The available action types may include navigation, subscriptions (e.g., lodging, dining, etc.), displaying media (e.g., captioned text from audio, transliterated text, songs, images, video, etc.), and/or providing information that is not necessarily executable (e.g., displaying menus, captioned text, translations, or other information). The available actions may be selected by matching the appropriate available action type to the semantic entity. For example, in response to providing the media type semantic entity and receiving a plurality of available action types, the computing system may select (or prefer) the available actions that include displaying the media.
As another example, the available actions may be selected based on past user interactions. Example past user interactions include receiving preferences for a particular type of action available or for actions from a particular computer application or group of computer applications. Thus, the computing system may customize the selection of available actions from the computer application for the user of the computing device.
In some implementations, the selection of the available actions may be performed by a machine-learned model. The "ranked machine learned model" may be configured to receive input describing one or more available actions described by output received from each computer application. In response to receipt of the input, the machine-learned model may be configured to output a ranking output describing a ranking of the available actions. The computing system may be configured to input inputs describing available actions into the ranking machine learned model and receive ranking outputs describing a ranking of respective outputs as outputs of the ranking machine learned model. The computing system may be configured to select an available action for presentation to the user based on the ranking output. For example, the highest ranked action (e.g., top single result, top two actions, top three actions, etc.) may be selected for presentation to the user.
In some implementations, the computing system may be configured to store information, such as "bookmarks," that include or describe indicators for later viewing by the user. The computer system may be configured to detect a user input action requesting that the pointer be saved for later viewing. The computing system may display a "bookmark" icon associated with (e.g., displayed near) the indicator. The user may tap (tap) or touch the "bookmark" icon to save the indicator to memory for later viewing. The user may view the saved indicator in a panel of "saved for later" (saved for later) "which may be retrieved when the user wishes to revisit the previously provided indicator.
In some implementations, the saved indicators can be combined together and/or saved, for example, based on contextual data associated with the generation and/or display of the indicators. The stored indicators may be combined together as related to a particular entity, event, or location, and/or may be combined together based on other information. For example, saved indicators may be grouped together as related to a person (e.g., a contact) based on the available actions and/or the type of computer application associated with the indicators, the location of the mobile computing device when the indicators are provided to the user, the location of the focus of the available actions (e.g., the city in which the list of available accommodations is located, the destination city where the tickets are provided), and/or time groupings based on dates or date ranges (e.g., the indicators may be grouped together for a particular vacation or trip).
In some implementations, a computing system may be configured to perform some or all of the operations described herein, e.g., locally on a mobile computing device. The mobile device may store the machine-learned model described herein and execute the machine-learned model locally. Local execution on the user's mobile device may reduce overall system latency as well as reduce network traffic with one or more back-end server computers located remotely from the mobile device. The user may be notified or signaled before the information is sent out of the device (e.g., for cloud computing). Such signaling may improve user confidence by using the features described herein. For example, a user may be particularly interested in knowing whether or when certain types of contextual data (e.g., ambient audio detected with a microphone and/or audio collected during a phone call) are being transmitted out of the device. Thus, in some implementations, some or all of the context data may remain on the device unless consent is received from the user to allow the context data to be transmitted from the device.
The systems and methods of the present disclosure may provide a number of technical effects and benefits. As noted above in some places, the systems and related methods can operate in an proactive manner to assist a user. In doing so, the systems and methods may reduce the number of user inputs for a given system, e.g., via a user's mobile device, thereby saving computing and power-related resources (e.g., requests or searches by a search engine) that may otherwise be needed to process such inputs. Implementation on the device of the method, for example, through machine-learned models stored and executed locally at the user device, can shorten the latency of providing information to assist the user, and in addition, can reduce network traffic that might otherwise be required to request/provide such information from a server computer at a remote location. Proactively providing prompts and/or other types of information discussed above may also provide for efficient use of screen real estate (real estate) at the user device by presenting the information in a manner that facilitates guided human-machine interaction, e.g., to navigate a wheelchair user to an unobstructed entrance to a building, or to provide a hearing impaired person with text captioning of technical information from audio.
Referring now to the drawings, example embodiments of the disclosure will be discussed in further detail.
Example apparatus and System
FIG. 1A depicts a block diagram of an example computing system 100, the example computing system 100 selecting and providing available actions to a user from one or more computer applications, according to an example embodiment of the present disclosure. The system 100 may include a user computing device 102, a server computing system 130, and/or a training computing system 150 communicatively coupled through a network 180.
The user computing device 102 may be any type of computing device, such as, for example, a personal computing device (e.g., a laptop or desktop computer), a mobile computing device (e.g., a smartphone or tablet), a gaming console or controller, a wearable computing device, an embedded computing device, or any other type of computing device.
The user computing device 102 includes one or more processors 112 and memory 114. The one or more processors 112 may be any suitable processing device (e.g., processor core, microprocessor, ASIC, FPGA, controller, microcontroller, etc.) and may be one processor, or a plurality of processors operatively connected. Memory 114 may include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, a disk, etc., as well as combinations thereof. The memory 114 may store data 116 and instructions 118, which data 116 and instructions 118 are executed by the processor 112 to cause the user computing device 102 to perform operations.
The user computing device 102 may store or include one or more computer applications 119. The computer application may be configured to perform various operations and provide application output as described herein.
The user computing device 102 may store or include an artificial intelligence system 120. The artificial intelligence system 120 may perform some or all of the operations described herein. The artificial intelligence system 120 can be separate and distinct from the one or more computer applications 119, but can communicate with the one or more computer applications 119.
The artificial intelligence system 120 can include one or more recognition models 122 and/or one or more ranking models 124. The recognition model 122 and/or the one or more ranking models 124 may be or may additionally include various machine-learned models, such as neural networks (e.g., deep neural networks) or other multi-layered non-linear models. The neural network may include a recurrent neural network (e.g., a long-short term memory recurrent neural network), a feed-forward neural network, or other form of neural network. An example recognition model 122 is discussed with reference to FIG. 2A. An example ranking model 124 is discussed with reference to FIG. 2B.
In some implementations, the one or more recognition models 122 and/or the one or more ranking models 124 can be received from the server computing system 130 over the network 180, stored in the user computing device memory 114, and used or implemented by the one or more processors 112. In some implementations, the user computing device 102 may implement multiple parallel instances of a single recognition model 122 (e.g., to perform parallel recognition operations across multiple instances of the recognition model 122). In some implementations, the user computing device 102 may implement multiple parallel instances of the ranking model 124 (e.g., to perform parallel ranking operations across multiple instances of the ranking model 124).
More specifically, the recognition model 122 may be configured to recognize one or more semantic entities described by the context data. The recognition model 122 can be configured to receive a model input that includes context data and output a model output that describes one or more semantic entities referenced by the model input. Examples of context data may include text displayed in a user interface, audio played or processed by a computing system, audio detected by a computing system, information about a location of a user (e.g., a location of a mobile computing device of a computing system), calendar data, and/or contact data. For example, the context data may include ambient audio detected by a microphone of the computing system and/or telephone audio processed during a telephone call. The calendar data may describe future events or plans (e.g., flights, hotel reservations, dinner plans, etc.). Example semantic entities that may be described by the model output include words or phrases identified in text and/or audio. Additional examples include information about the user's location, such as city names, state names, street names, names of nearby attractions, and so forth.
The ranking model 124 may be configured to receive input describing one or more available actions described by the output received from each computer application. In response to receipt of the input, the machine-learned model may be configured to output a ranking output describing a ranking of the available actions. The computing system may be configured to input inputs describing available actions into the ranking machine learned model and receive ranking outputs describing a ranking of the respective outputs as outputs of the ranking machine learned model. The computing system may be configured to select an available action for presentation to the user based on the ranking output. For example, the highest ranked action (e.g., top single result, top two actions, top three actions, etc.) may be selected for presentation to the user.
Additionally or alternatively, the artificial intelligence system 140 can be included in, or stored and implemented by, a server computing system 130, the server computing system 130 communicating with the user computing device 102 according to a client-server relationship. For example, the artificial intelligence system 140 can include a recognition model 142 and/or an ordering model 144. The recognition model 142 and/or the ranking model 144 may be implemented by the server computing system 140 as part of a web service. Thus, one or more models 122, 124 may be stored and implemented at the user computing device 102, and/or one or more models 142, 144 may be stored and implemented at the server computing system 130.
The user computing device 102 may also include one or more user input components 122 that receive user input. For example, the user input component 122 may be a touch-sensitive component (e.g., a touch-sensitive display screen or touchpad) that is sensitive to touch by a user input object (e.g., a finger or stylus). The touch sensitive component may be used to implement a virtual keyboard. Other example user input components include a microphone, a conventional keyboard, or other device through which a user may input communications.
The server computing system 130 includes one or more processors 132 and memory 134. The one or more processors 132 may be any suitable processing device (e.g., processor core, microprocessor, ASIC, FPGA, controller, microcontroller, etc.) and may be one processor, or operatively connected processors. Memory 134 may include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, a disk, etc., as well as combinations thereof. The memory 134 may store data 136 and instructions 138, which data 136 and instructions 138 are executed by the processor 132 to cause the server computing system 130 to perform operations.
In some implementations, the server computing system 130 includes or is otherwise implemented by one or more server computing devices. Where the server computing system 130 includes multiple server computing devices, such server computing devices may operate according to a sequential computing architecture, a parallel computing architecture, or some combination thereof.
As described above, the server computing system 130 may store or otherwise include an artificial intelligence system, which may include one or more machine-learned models 142, 144. For example, the models 142, 144 may be or may otherwise include various machine-learned models, such as neural networks (e.g., deep recurrent neural networks) or other multi-layer nonlinear models. An example model 140 is discussed with reference to fig. 2A and 2B.
The server computing system 130 may train the model 140 via interaction with a training computing system 150 communicatively coupled through a network 180. The training computing system 150 may be separate from the server computing system 130 or may be part of the server computing system 130.
In particular, the model trainer 160 may train one or more of the models 122, 124, 142, 144 based on the set of training data 142. The training data 142 may include, for example, publicly available data sets such as labeled or unlabeled images, sounds, and the like.
In some implementations, the training examples may be provided by the user computing device 102 if the user has provided consent (e.g., based on communications previously provided by the user of the user computing device 102). Thus, in such implementations, the model 120 provided to the user computing device 102 may be trained by the training computing system 150 on user-specific communication data received from the user computing device 102. In some instances, this process may be referred to as personalizing the model.
The model trainer 160 includes computer logic for providing the desired functionality. The model trainer 160 may be implemented in hardware, firmware, and/or software that controls a general purpose processor. For example, in some implementations, model trainer 160 includes program files stored on a storage device, loaded into memory, and executed by one or more processors. In other implementations, model trainer 160 includes one or more sets of computer-executable instructions stored in a tangible computer-readable storage medium, such as a RAM hard disk or an optical or magnetic medium.
FIG. 1A illustrates one example computing system that can be used to implement the present disclosure. Other computing systems may also be used. For example, in some implementations, the user computing device 102 may include a model trainer 160 and a training data set 162. In such implementations, the model 120 may be trained and used locally at the user computing device 102. In some such implementations, the user computing device 102 may implement the model trainer 160 to personalize the model 120 based on user-specific data.
FIG. 1B depicts a block diagram of an example computing device 10 for selecting and providing available actions to a user from one or more computer applications, according to an example embodiment of the present disclosure. Computing device 10 may be a user computing device (e.g., a mobile computing device) or a server computing device.
As shown in fig. 1B, each application may communicate with many other components of the computing device, such as one or more sensors, a context manager, a device state component, and/or additional components. In some implementations, each application may communicate with each device component using an API (e.g., a public API). In some implementations, the API used by each application is application specific.
Fig. 1C depicts a block diagram of an example computing device 50, performed in accordance with an example embodiment of the present disclosure. Computing device 50 may be a user computing device or a server computing device.
The central smart inlay includes a number of machine-learned models. For example, as shown in fig. 1C, a respective machine-learned model (e.g., model) may be provided for each application and managed by a central intelligence layer. In other implementations, two or more applications may share a single machine-learned model. For example, in some implementations, the central smart tier may provide a single model (e.g., a single model) for all applications. In some implementations, the central smart inlay is included within or otherwise implemented by the operating system of the computing device 50.
The central smart inlay may communicate with the central device data plane. The central device data layer may be a centralized repository of data for the computing device 50. As shown in fig. 1C, the central device data layer may communicate with many other components of the computing device, such as one or more sensors, a context manager, a device state component, and/or additional components. In some implementations, the central device data layer may communicate with each device component using an API (e.g., a private API).
FIG. 1D depicts an example user computing device 170 (e.g., a mobile computing device) configured to select and provide available actions to a user from one or more computer applications 172, in accordance with aspects of the present disclosure. More specifically, the user computing device 170 may be configured to provide data 174 describing one or more semantic entities to one or more computer applications 172. The data may be or may include a semantic entity and/or additional contextual information about the semantic entity. For example, the data may include the names of songs identified in a video played by the computing system. The data may also include the name of the artist performing the song. The computing system may provide data describing the semantic entities 174 to the computer application according to a predefined API or other protocol.
The user computing device 170 may be configured to receive one or more application outputs 176 from the one or more computing applications 172, respectively, in response to providing data 174 describing the one or more semantic entities to the one or more computer applications 172. The application output 174 received from each computer application may describe one or more available actions of the corresponding computer application 172 with respect to one or more semantic entities. In the above example, the application output 174 may include the names of songs and/or artists that may be streamed by a music streaming (streaming) application. As another example, in response to receiving data 174 that includes a semantic entity containing the name of a city (e.g., a city that the user is planning to visit), application output 176 may include potential lodging and/or car rentals available from the travel reservation application at the city. The computer application may format, transmit, and/or alert the computing system with respect to the application output according to rules or protocols (e.g., as specified by a predefined API).
The user computing device 170 may be configured to provide at least one indicator 178 to the user, the indicator 178 describing one or more available actions of the corresponding computer application 172. The indicators 178 may be or may include graphical indicators presented in the user interface, such as words or phrases describing available actions. For example, in the above example, the indicator may be or may include a button labeled "play" next to the song title identified in the video. As another example, the graphical indicator may be or may include a movie time and/or theater location based on semantic entities including a movie title that is displayed in the user interface or included in the ambient audio (e.g., audio processed by the computing device and/or detected by a microphone of the computing device). As yet another example, the indicator may comprise an audio indicator that is played to the user. For example, the audio indicator may include a voice or other sound that asks the user whether the text or environmental audio appearing in the user interface should be translated into another language. However, it should be understood that any of the graphical indicators described herein may be provided in an audio format within the scope of the present disclosure.
The indicator may also include or describe identifying information about the computer application that may be used to perform the action. As one example, in some implementations, a computer application may provide a stylized output describing aesthetic features, which may include identifying information. The computing system may display an indicator in the user interface based on the stylized output. The aesthetic features may include one or more of a name, logo, font, color, shape, location within a user interface, and/or any other suitable visual characteristic of the computer application.
Example model arrangements
Fig. 2A depicts a block diagram of an example recognition model 202, according to an example embodiment of the present disclosure. In some implementations, recognition model 202 may be configured to receive a model input that includes context data 204 and output a model output that describes one or more semantic entities 206 referenced by the model input. Examples of the context data 204 may include text displayed in a user interface, audio played or processed by a computing system, audio detected by a computing system, information about a location of a user (e.g., a location of a mobile computing device of a computing system), calendar data, and/or contact data. For example, the context data 204 may include ambient audio detected by a microphone of the computing system and/or audio processed by the computing device (e.g., during a phone call or while playing media such as video, podcasts, etc.). The calendar data may describe future events or plans (e.g., flights, hotel reservations, dinner plans, etc.). Example semantic entities that may be described by the model output include words or phrases identified in text and/or audio. Additional examples include information about the user's location, such as city names, state names, street names, names of nearby attractions, and so forth.
FIG. 2B depicts a ranking model 250 that may be configured to receive input describing one or more available actions 252 described by the output received from each computer application. In response to receipt of the input, the machine-learned model may be configured to output a ranking output 254 that describes a ranking of the available actions. The computing system may be configured to input inputs describing available actions 252 into the ranked machine-learned model 250 and receive ranked outputs 254 describing the rankings of the respective outputs as outputs of the ranked machine-learned model 250. The computing system may be configured to select the available actions 252 for presentation to the user based on the ranking output 254. For example, the highest ranked action (e.g., top single result, top two actions, top three actions, etc.) may be selected for presentation to the user.
In some implementations, the computing system may include an artificial intelligence system (e.g., "google assistant"). The artificial intelligence system may include one or more of the machine-learned models 202, 250 described above with reference to fig. 2A and 2B. The artificial intelligence system may perform some or all of the operations described herein.
The artificial intelligence system can be separate and distinct from, but capable of communicating with, one or more computer applications. Via a predefined application programming interface, the artificial intelligence system can provide data describing one or more semantic entities to the computer application and can receive application output from the computing application, respectively.
Example method
FIG. 3 depicts a flowchart of an example method 300 for selecting and providing available actions to a user from one or more computer applications, according to an example embodiment of the present disclosure. Although fig. 3 depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particular illustrated order or arrangement. The various steps of method 300 may be omitted, rearranged, combined, and/or adapted in various ways without departing from the scope of the present disclosure.
At 302, the computing system may be configured to input model inputs into the machine-learned model, e.g., a ranked machine-learned model such as described above with reference to fig. 2A. The model input may include context data. Examples of context data may include text displayed in a user interface, audio played or processed by a computing system, audio detected by a computing system, information about a location of a user (e.g., a location of a mobile computing device of a computing system), calendar data, and/or contact data. For example, the context data may include ambient audio detected by a microphone of the computing system and/or telephone audio processed during a telephone call. The calendar data may describe future events or plans (e.g., flights, hotel reservations, dinner plans, etc.).
In some implementations, the computing system may include an artificial intelligence system (e.g., "google assistant"). The artificial intelligence system may include a ranking machine learned model and may perform some or all of the operations described herein. The artificial intelligence system can be separate and distinct from, but capable of communicating with, one or more computer applications. Via a predefined application programming interface, the artificial intelligence system can provide data describing one or more semantic entities to the computer application and can receive application output from the computing application, respectively.
At 304, the computing system may be configured to receive a model output describing one or more semantic entities referenced by the context data as an output of a machine-learned model (e.g., a ranked machine-learned model). Example semantic entities that may be described by the model output include words or phrases identified in text and/or audio described by the context data. Additional examples include information about the user's location, such as city names, state names, street names, names of nearby attractions, and so forth.
At 306, the computing system may be configured to provide data describing the one or more semantic entities to one or more computer applications. The data may be or may include semantic entities and/or additional contextual information about the semantic entities. For example, the data may include the names of songs identified in a video played by the computing system. The data may also include the name of the artist performing the song. The computing system may provide data describing the semantic entities to the computer application according to a predefined API or other protocol.
At 308, the computing system may be configured to receive one or more application outputs from the one or more computing applications, respectively, in response to providing the data describing the one or more semantic entities to the one or more computer applications. The application output received from each computer application may describe one or more available actions of the corresponding computer application with respect to one or more semantic entities. In the examples described above, the application output may include the names of songs and/or artists that may be streamed by the music streaming application. As another example, in response to receiving a semantic entity that includes a name of a city (e.g., a city that the user plans to visit), the application output may include potential lodging and/or car rentals available from the travel reservation application at the city.
At 310, the computing system may be configured to provide at least one indicator to a user that describes one or more available actions of a corresponding computer application. The indicator may be or may include a graphical indicator presented in the user interface, such as a word or phrase describing the available actions. For example, in the above example, the indicator may be or may include a button labeled "play" next to the song title identified in the video. As another example, the graphical indicator may be or may include a movie time and/or theater location based on semantic entities including a movie title that is displayed in the user interface or included in the ambient audio (e.g., audio processed by the computing device and/or detected by a microphone of the computing device). As yet another example, the indicator may comprise an audio indicator that is played to the user. For example, the audio indicator may include a voice or other sound that asks the user whether the text or environmental audio present in the user interface should be translated into a different language. However, it should be understood that any of the graphical indicators described herein may be provided in an audio format within the scope of the present disclosure.
The indicator may also include or describe identifying information about the computer application that may be used to perform the action. As one example, in some implementations, a computer application may provide a stylized output describing aesthetic features, which may include identifying information. The computing system may display an indicator in the user interface based on the stylized output. The aesthetic features may include one or more of a name, logo, font, color, shape, location within a user interface, and/or any other suitable visual characteristic of the computer application.
Example implementation
FIG. 4 illustrates an example implementation of aspects of the present disclosure. More specifically, fig. 4 depicts an example mobile computing device 400 that displays an indicator 402 in a user interface 401, the indicator 402 showing available actions from a computer application, in accordance with aspects of the present disclosure. The mobile computing device 400 may correspond to the user computing device 102 of fig. 1A. The mobile computing device 400 may store or otherwise implement a computer application. In this example, the mobile computing device 400 can display the restaurant review generated by the restaurant review application "Eater.
The mobile computing device 400 may be configured to input a model input including context data into a recognition machine learned model, e.g., a model as described with reference to fig. 2A. In this example, the context data may include some or all of the text displayed in the user interface 401.
The mobile computing device 400 can be configured to receive a model output describing one or more semantic entities referenced by the context data. In this example, the model output may include highlighted text. More specifically, in this example, the user has highlighted the city name "Port Angeles" in the text displayed in user interface 401.
The mobile computing device 400 may be configured to provide data describing semantic entities to one or more computer applications. In this example, the data may include the name "Port Angeles". The mobile computing device 400 may provide data to several computer applications, including a lodging application, a navigation application, and/or other suitable applications. The mobile computing device 400 may provide data according to a predefined API or other protocol.
Additionally, the one or more computer applications may be "third party" computer applications. In other words, some or all of the computer applications may be developed by entities other than developers of operating systems and/or artificial intelligence systems operable on the mobile computing device.
The mobile computing device 400 may be configured to select a computer application from a plurality of applications operable on a computing system to which to provide data describing one or more semantic entities based on a comparison between the model output and corresponding information about the plurality of applications. In this example, the mobile computing device 400 (e.g., an artificial intelligence system) can be configured to compare the model output (e.g., including "Port angels") to information about potential applications, such as an accommodation application, a navigation application, a movie tickets application, a movie reviews application, a restaurant reservation application, and so forth. The mobile computing device 400 may select one or more applications to which to provide data (e.g., including "Port angels"). In this example, the mobile computing device 400 (e.g., an artificial intelligence system) may select an accommodation application and/or a navigation application based on a comparison of information (e.g., typical inputs and/or outputs) about the potential applications. For example, an accommodation application and/or a navigation application may be selected because typical inputs for such an application include the name of a city.
The mobile computing device 400 can be configured to receive application output from the computing applications, respectively, in response to providing data describing the semantic entities to the computer applications. The computer application may format, transmit, and/or alert the computing system with respect to the application output according to rules or protocols (e.g., as specified by a predefined API). The application output received from each computer application may describe one or more available actions of the corresponding computer application with respect to the semantic entity. In this example, the application output includes output from the lodging application "Airbnb". The application output may include information about available accommodation options at Port Angeles. The application output may include output from a navigation application and/or a search application.
The mobile computing device 400 may select a subset of the plurality of available actions described by the application output to provide to the user. The mobile computing device 400 can select a subset of the available actions based on various factors or data. Examples include a relevance to one or more semantic entities, a location of a mobile computing device 400 of the computing system, past user interactions, a type of one or more semantic entities, or a type of one or more available actions.
In this example, the mobile computing device 400 can receive application output from the lodging application, the navigation application, the search application, and/or additional applications (e.g., restaurant reservation application, restaurant review application, etc.) that describe respective available actions. For example, based on past user interactions, the mobile computing device 400 may select an application output from the lodging applications to display to the user. Examples of past user interactions may include an increase in usage of the lodging application, an increase in engagement of an indicator describing an action from the lodging application, and the like, as compared to other applications. Thus, the computing system may customize the selection of available actions from the computer application for the user of the computing device.
In some implementations, the selection of the available actions may be performed by a machine-learned model, such as "rank machine-learned model" described with reference to FIG. 2B. The mobile computing device 400 may be configured to input inputs describing available actions into the ranking machine learned model and receive ranking outputs describing a ranking of respective outputs as outputs of the ranking machine learned model. The mobile computing device 400 may be configured to select available actions for presentation to the user based on the ranked outputs. For example, the highest ranked action (e.g., top single result, top two actions, top three actions, etc.) may be selected for presentation to the user. In this example, the mobile computing device 400 can input model input describing application output from two or more computer applications, such as an accommodation application, a navigation application, a search application, and/or an additional application (e.g., a restaurant reservation application, a restaurant review application, etc.). The mobile computing device 400 can receive the ranked model output, including the computer application, as output from the ranked machine-learned model. The mobile computing device 400 can select an available action to provide to the user based on the ranking. In this example, the mobile computing device 400 may select the available actions from the lodging application to provide to the user.
More specifically, the mobile computing device 400 may provide an indicator 402 describing available actions from the lodging application for display in the user interface 401. In this example, the indicator 402 includes a name and a logo of the lodging application. If the user performs input for the pointer (e.g., a user touch action using a touch screen with respect to the mobile computing device 400), the mobile computing device 400 may open the lodging application (or provide output from the lodging application for display in the user interface 401) to show available lodging options in Port Angeles from the lodging application.
An indicator 402 may be provided in the panel 404. Additional available actions or indicators 406 may be displayed in the panel 404. For example, in this example, an option to copy or share highlighted text plus an option to select text all is also presented in panel 404. In addition, a vertical ellipse 408 may be displayed in the panel 404. In response to receiving the user input action for the vertical ellipse 408, the mobile computing device 400 may display additional indicators from additional computer applications and/or additional information about the currently displayed indicators 402, 406 in the panel 404.
The mobile computing device 400 can display the indicator 410 in the operating system level navigation bar 412. The operating system level navigation bar 412 can be displayed during use of multiple applications and/or at a "home screen" of the mobile computing device. In this example, indicator 410 in navigation bar 412 includes text providing "Airbnb booking".
As described above, the indicator 402 may also include or describe identifying information about the computer application that is available to perform the action (e.g., a logo or name of the computing application). In some implementations, the computer application may provide a stylized output describing aesthetic features, which may include identifying information. In this example, the lodging application may provide for display a stylized output that includes a logo for the lodging application. The stylized output may include various aesthetic features such as fonts, colors, shapes, locations within the user interface, and/or any other suitable visual characteristics of the indicators 402.
In some implementations, the mobile computing device 400 can provide an audio indicator that can be played to the user. For example, the audio indicator may include a voice or other sound that asks the user whether the text or environmental audio present in the user interface should be translated into a different language. In this example, the audio indicator may include speech that speaks an english translation of spanish spoken language. However, it should be understood that any of the graphical indicators described herein may be provided in an audio format within the scope of the present disclosure.
Additionally, in some implementations, the mobile computing device 400 can provide a visual signal 414 (e.g., in the navigation bar 412). The visual signal 414 may indicate a state of the mobile computing device. For example, the visual signal 414 may signal the user when contextual data (e.g., text displayed in a user interface) is being or is not being sent out of the device. As another example, the visual signal 414 may indicate a status of the artificial intelligence system. The visual signals 414 may indicate when the artificial intelligence system is collecting context data, performing one or more operations configured to retrieve available actions from the computer application (e.g., "process" the context data), and/or a confidence level associated with the relevance of the indicators 402, 410 provided to the user.
Fig. 5A and 5B illustrate another example implementation of aspects of the present disclosure. More specifically, fig. 5A depicts the example mobile computing device 500 in a first state in which the mobile computing device 500 is displaying a text message exchange in a user interface 502, in accordance with aspects of the present disclosure. Fig. 5B depicts the mobile computing device 500 of fig. 5A in a second state, in which the mobile computing device is providing indicators of available actions from multiple computer applications with respect to recognized text from the text message exchange of fig. 5A, in accordance with aspects of the present disclosure. The mobile computing device 500 may input a model input including context data into a recognition model, e.g., a model as described above with reference to fig. 2A. In this example, the context data may include some or all of the text displayed in the user interface 502. The recognition model may output a model output that describes one or more semantic entities referenced by the context data, such as the name of a restaurant "Sushi Tomi" and/or one or more pieces of information about the text message exchange. Additional examples of semantic entities that may be described by the model output include the name of the person with whom the user exchanges text messages and/or the word "lunch (lunch)".
The mobile computing device 500 may be configured to provide data describing one or more semantic entities to one or more computer applications. In this example, the data may include semantic entities. The mobile computing device 400 may provide data to several computer applications, including a subscription application and/or a navigation application. The mobile computing device 400 may provide data according to a predefined API or other protocol.
The mobile computing device 500 may be configured to receive application output from the computing applications in response to providing data describing one or more semantic entities to the computer applications, respectively. The computer application may format, transmit, and/or alert the computing system with respect to the application output according to rules or protocols (e.g., as specified by a predefined API). The application output received from each computer application may describe one or more available actions of the corresponding computer application with respect to the semantic entity. In this example, the application output includes output from the subscription application "Open Table" and output from the navigation application "Google Maps". The application output may include information about where to make a reservation and/or the restaurant Sushi Tomi.
The mobile computing device 500 may be configured to provide one or more indicators to a user of the computing system by displaying the indicators in the user interface 401. For example, the mobile computing device 500 may display an indicator 552 that describes the available actions for subscribing using the subscribing application. The indicator 552 may be displayed in a "half shelf" 554. Half shelf 554 may be located at a lower portion of user interface 502.
The mobile computing device 500 may display an indicator 556 that describes the available actions from the navigation application. For example, the available actions may include calling a restaurant (represented by icon 558), navigating to the restaurant (represented by icon 560), and/or viewing the location of the restaurant (represented by icon 562) on a map. The indicator 556 can include additional information about the restaurant, such as a photograph 564 from the restaurant website and/or map 566.
The mobile computing device 500 can display a pointer 568 that describes available actions including searching for restaurants using the search application. The mobile computing device 500 can display a pointer 568 and/or a pointer 570, the pointer 568 describing available actions including searching for restaurants using a search application, the pointer 570 describing available actions including viewing business hours for restaurants, for example, in a web browser application or a restaurant review/reservation application.
Fig. 6 depicts an example mobile computing device 600 that displays a text message notification 602 in a user interface 604 along with indicators of available actions from multiple computer applications with respect to recognized text from a text message, in accordance with aspects of the present disclosure. More specifically, the mobile computing device 600 may display a text message notification in a lock screen 606 displayed in the user interface 604.
In this example, the context data may include the text of the received text message and/or the name of the sender. The semantic entities may include one or more of the following: date ("day 6.8"), event ("christis's wedding"), or place of presence ("plantary"), which is described by contextual data.
The mobile computing device 600 may be configured to provide data describing one or more semantic entities to a computer application. In this example, the data may include one or more semantic entities. The mobile computing device 400 may provide data according to a predefined API or other protocol.
The mobile computing device 600 may be configured to receive one or more application outputs from a computer application. The computer application may format, transmit, and/or alert the computing system with respect to the application output according to rules or protocols (e.g., as specified by a predefined API). In this example, the application output may include information about available actions including navigating from the navigation application to arbor and/or information about available actions including creating a calendar event from a calendar application.
The mobile computing device 600 may be configured to provide an indicator 608 and/or an indicator 610, the indicator 608 describing available actions including navigating to an arbor using a navigation application, the indicator 610 describing creation of a calendar event from a calendar application.
The mobile computing device 600 may display one or more indicators 612 that include suggested responses. For example, the intelligent system may analyze the context data and generate suggested responses. Alternatively, the mobile computing device 600 may receive the indicator 612 that includes a suggested response from a computer application separate from the intelligent system, e.g., as described above with respect to the other indicators 608, 610.
Fig. 7A depicts the example mobile computing device 700 in a first state in which ambient audio referencing a historic person is detected and an indicator 702 is displayed in a lock screen 702 displayed in a user interface 704 of the mobile computing device 700, according to aspects of the present disclosure. For example, the mobile computing device 700 may be configured to detect ambient audio (represented by audio symbol 706) using a microphone.
The mobile computing device 700 may be configured to input a model input including context data into a recognition machine learned model, e.g., a model as described with reference to fig. 2A. In this example, the context data may include some or all of the detected ambient audio.
The mobile computing device 700 can be configured to receive a model output that describes one or more semantic entities referenced by context data. In this example, the model output may include the name of the historical persona, Enrico Fermi, and/or additional information about the historical persona or about sentences or contexts that mention the name of the historical persona. For example, if the name of a historical character is spoken in a question, e.g., "which books were written by Enrico Fermi? "or" who is Enrico Fermi? "the model output may describe additional information about the question or may include additional semantic entities (e.g.," books ") from the question.
The mobile computing device 700 can be configured to provide data describing semantic entities to one or more computer applications. In this example, the data may include the name "Enrico Fermi" and/or "book". The mobile computing device 700 can provide data to several computer applications, including a search application, a shopping application, and/or other suitable applications. The mobile computing device 400 may provide data according to a predefined API or other protocol.
The mobile computing device 700 can be configured to receive application output from the computing applications, respectively, in response to providing data describing the semantic entities to the computer applications. The computer application may format, transmit, and/or alert the computing system with respect to application output according to rules or protocols (e.g., as specified by a predefined API). The application output received from each computer application may describe one or more available actions of the corresponding computer application with respect to the semantic entity. In this example, the application output includes output from the search application and the shopping application "Amazon". The application output from the search application output may include relevant information about the semantic entity, such as results from an internet query that includes the semantic entity. In this example, the application output from the search application may include text about Enrico Fermi from a Wikipedia (Wikipedia) article. The application output from the shopping application may include available actions including purchasing one or more products related to the semantic entity. In this example, the application output from the shopping application may include a book written by or about Enrico Fermi that may be purchased using the shopping application.
FIG. 7B depicts the example mobile computing device 700 of FIG. 7A in a second state, in which indicators describing available actions with respect to a historian character mentioned in the environmental audio are displayed. More specifically, an indicator 720 may be displayed that describes information received from the search application. In this example, the indicator 720 includes text from a web page query performed on the historical persona.
An indicator 722 can be displayed that includes available actions that can be performed by the shopping application with respect to the semantic entity. More specifically, items 724 related to semantic entities may be offered for sale. In this example, the items 724 may include books written by or about the historical personality, and/or books written about disciplines (e.g., physics) related to the historical personality.
In some implementations, the mobile computing device 700 may be configured to store information, such as "bookmarks," that include or describe the indicators 720, 722 for later viewing by the user. For example, the mobile computing device 700 may display a bookmark icon 726. In response to a user input requesting storage of one or more indicators 720, 722 for later viewing (e.g., a user touch action against bookmark icon 726), mobile computing device 700 may store information describing indicators 720, 722 for later viewing and/or display a "save-to-standby" panel, e.g., as described below with reference to fig. 7C.
Fig. 7C depicts the example mobile computing device 700 of fig. 7A in a third state, where the indicator has been "bookmarked" for later viewing, in accordance with aspects of the present disclosure. The mobile computing device 700 may display the saved indicator 740 in a "saved standby" panel 742, which panel 742 may be retrieved when the user wishes to revisit the previously provided indicator 740. In some implementations, saved indicators 740 may be combined and/or saved, for example, based on context data associated with the generation and/or display of indicators 740. In this example, multiple saved indicators 740 may be combined together as being related to a particular person (e.g., contact). Additional examples include saving or grouping indicators based on: the type of computer application and/or available action associated with the indicator, the location of the mobile computing device when the indicator is provided to the user, the location of the focus of the available action (e.g., the city in which the list of available accommodations is located, the destination city where the ticket is provided), and/or time groupings based on date or date range (e.g., the indicators may be grouped together for a particular vacation or trip).
Fig. 8A depicts the example mobile computing device 800 in a first state in which the video 802 and the indicator 804 are displayed in the user interface 806 of the mobile computing device 800, in accordance with aspects of the present disclosure. In this example, the video 802 may be or may include a movie trailer, movie review, or other portion of a movie. The mobile computing device 800 can input the model input into the recognition model. The model input may include one or more frames of the video 802 and/or a portion of the audio from the video 802, e.g., as described above with reference to fig. 2A. The recognition model may output a model output that describes one or more semantic entities. For example, semantic entities may include words spoken in a movie. In some implementations, the recognition model can be configured to recognize movie titles, and the semantic entities can include movie titles.
The mobile computing device 800 may be configured to provide data describing semantic entities to one or more computer applications. In this example, the data may include a title of the movie and/or words spoken in the movie. The mobile computing device 800 may provide data to several computer applications, including a search application, a navigation application, an application for viewing movie times, and/or an application for purchasing movie tickets. The mobile computing device 800 may provide data according to a predefined API or other protocol.
The mobile computing device 800 may be configured to receive application output from the computing applications in response to providing data describing the semantic entities to the computer applications, respectively. The computer application may format, transmit, and/or alert the computing system with respect to the application output according to rules or protocols (e.g., specified by a predefined API). The application output received from each computer application may describe one or more available actions of the corresponding computer application with respect to one or more semantic entities. In this example, the application output may include output from a search application, a navigation application, an application for viewing movie times, and/or an application for purchasing movie tickets. The application output may describe the available actions that may be performed with each computer application with respect to the movie.
Fig. 8B depicts the example mobile computing device 800 of fig. 8A in a second state, wherein additional indicators 850, 852, 854 are displayed in the user interface 806. The additional indicators 850, 852, 854 may be associated with available actions for the movie, such as purchasing movie tickets. Additional indicators 850, 852, 854 may be displayed in the panel 856. Common characteristics of the available actions, such as the name of the theater, may be displayed in the panel 856. Additional indicators 850, 852, 854 may describe available actions with respect to the movie. The first indicator 850 may describe available actions including navigating to a movie theater. Second indicator 852 may describe the available actions including viewing show times for a movie at a movie theater. The third indicator 854 may describe available actions including purchasing movie tickets (e.g., for movie theaters listed for the movie portion viewed in the user interface 806). The mobile computing device 800 may display the bookmark icon 858, for example, as described above with reference to fig. 7B.
Fig. 9A depicts an example mobile computing device 900 in a first state in which a video 902 and an indicator 904 are displayed in a user interface, in accordance with aspects of the present disclosure. The mobile computing device 900 may input the model input into the recognition model. The model input may include one or more frames of the video 902 and/or a portion of the audio from the video 902, e.g., as described above with reference to fig. 2A. The recognition model may output a model output that describes one or more semantic entities. For example, semantic entities may include words spoken in a video or words from a song played in a video. In some implementations, the recognition model may be configured to recognize the title of a song played in the video.
FIG. 9B depicts the example mobile computing device 900 of FIG. 9A in a second state in which available actions associated with the indicator 904 of FIG. 9A are being performed. More specifically, the music playing application may play songs identified in the video. The panel 952 may be displayed and an application performing the requested action (e.g., playing a song) may display information about the action (e.g., title of the song, album art of the album on which the song appeared) and/or provide a control 953 for the user to control the requested action.
Fig. 10A depicts an example mobile computing device 1000 in which text is displayed in a user interface 1002 of the mobile computing device 1000, in accordance with aspects of the present disclosure. More specifically, an article commenting on a movie is displayed in the user interface 1002.
Fig. 10B depicts the example mobile computing device 1000 of fig. 10A in a second state, where the movie title 1020 is highlighted in the text. According to aspects of the present disclosure, indicators 1022, 1024, 1026 are displayed that describe available actions with respect to the movie title 1020. More specifically, the mobile computing device 1000 may be configured to display the indicators 1022, 1024, 1026 in response to the user highlighting the movie title 1020. In some implementations, the mobile computing device 1000 may be configured to enter model inputs into the recognition model in response to the user highlighting the movie title 1020, e.g., as described with reference to fig. 2A.
The mobile computing device 1000 may be configured to input model inputs into a recognition model that include some or all of the text displayed in the user interface 1002. For example, the model input may include a movie title 1020 and/or additional text and/or images displayed in the user interface 1002. The recognition model may output a model output that describes one or more semantic entities described by the model input. For example, the semantic entities may include a title of a movie and/or a type or category of semantic input (e.g., the model output may describe or identify that the model input includes a movie title).
The mobile computing device 1000 may be configured to provide data describing semantic entities to one or more computer applications. In this example, the data may include a title of the movie and/or a type or category of semantic entity (e.g., movie title). The mobile computing device 1000 may provide data to one or more computer applications, including a movie ticket purchasing application, a movie review application, and/or a search application. The mobile computing device 1000 may provide data according to a predefined API or other protocol.
The mobile computing device 1000 may be configured to receive application output from the computing applications in response to providing data describing the semantic entities to the computer applications, respectively. The computer application may format, transmit, and/or alert the computing system with respect to application output according to rules or protocols (e.g., as specified by a predefined API). The application output received from each computer application may describe one or more available actions of the corresponding computer application with respect to one or more semantic entities. In this example, the application output may include output from a movie ticket purchasing application, a movie review application, and/or a search application. The application output may describe available actions that may be performed with each computer application with respect to the movie.
The mobile computing device 1000 may display the indicators 1022, 1024, 1026 in the user interface 1002. The indicators 1022, 1024, 1026 may describe available actions that may include purchasing movie tickets using a movie ticket purchasing application, viewing movie reviews using a movie review application, and/or performing a web search using a search application. The indicators 1022, 1024, 1026 may be displayed in a panel 1028 of the user interface. Bookmark icon 1030 may be displayed in panel 1028 that is configured to save indicators 1022, 1024, 1026 for use, e.g., as described above with reference to fig. 7B.
Fig. 10C depicts the example mobile computing device 1000 of fig. 10A in a third state in which additional indicators 1044, 1046 are displayed that describe further available actions with respect to movie titles, in accordance with aspects of the present disclosure. The mobile computing device 1000 may be configured to display the additional indicators 1044, 1046 in response to detecting user input directed to one or more of the indicators 1022, 1024, 1026 of fig. 10B. Additional indicators 1044, 1046 may be displayed in a panel 1048 (e.g., a half panel), which may be displayed in a lower area of the user interface 1002. Additional indicators 1044, 1046 may describe available actions (e.g., at a particular time and/or at a particular theater) to purchase movie tickets.
Fig. 11A depicts the example mobile computing device 1100 in a first state, in which text 1102 has been highlighted, and indicators 1104, 1106, 1108 are displayed in a user interface 1110 of the mobile computing device 1100 (e.g., in a panel or tile 1112 displayed in the user interface 1110), according to aspects of the present disclosure. In this example, the user highlights the product name "Oculus Rift". One indicator 1104 depicts the available actions from the shopping application Amazon. Another indicator 1106 describes the actions available from the search application. Yet another indicator 1108 describes an available action that includes viewing a web page from Wikipedia that includes additional information about Oculus Rift. Bookmark icon 1114 may be displayed in panel 1112, which is configured to save indicators 1104, 1106, 1108 for use, e.g., as described above with reference to FIG. 7B.
FIG. 11B depicts the example mobile computing device 1100 of FIG. 11A in a second state in which information regarding the selected action is displayed, in accordance with aspects of the present disclosure. More specifically, in response to user input directed to the indicator 1104 describing available actions from the shopping application, the mobile computing device 1100 may display an indicator 1152 that provides additional information about the selected available actions. In this example, indicator 1152 may include information about highlighted product 1102, such as purchase price and shipping options. The mobile computing device 1100 may be configured to purchase a product in response to a user input action directed to the indicator 1152.
Fig. 12A depicts an example mobile computing device 1200 in which text is displayed in a user interface 1202 of the mobile computing device 1200, in accordance with aspects of the present disclosure. More specifically, the article may be displayed in the user interface 1200.
The mobile computing device 1200 may be configured to input model input to a recognition model that includes some or all of the text of an article displayed in the user interface 1202, e.g., as described above with reference to fig. 2A. The recognition model may output a model output that describes one or more semantic entities described by the model input. For example, semantic entities may include names of products, technologies, historical characters, and/or other entities of interest mentioned in the article. In this example, the model output may include the name of the product, such as Oculus Rift and HTC Vibe. The model output may include the name of the technology, such as Virtual Reality (VR).
The mobile computing device 1200 may be configured to provide data describing semantic entities to one or more computer applications. In this example, the data may include names of products and/or technologies mentioned in the article. Mobile computing device 1200 can provide data to one or more computer applications, including, for example, a shopping application and/or other suitable computer applications. The mobile computing device 1200 may provide data according to a predefined API or other protocol.
The mobile computing device 1200 may be configured to receive application output from the computing applications in response to providing data describing the semantic entities to the computer applications, respectively. The computer application may format, transmit, and/or alert the computing system with respect to application output according to rules or protocols (e.g., as specified by a predefined API). The application output received from each computer application may describe one or more available actions of the corresponding computer application with respect to one or more semantic entities. In this example, the application output may include output from a shopping application.
Fig. 12B depicts the example mobile computing device 1200 of fig. 12A in a second state, in which indicators 1222, 1224, 1226 are displayed, describing available actions with respect to portions of text, in accordance with aspects of the present disclosure. More specifically, in this example, the indicators 1222, 1224, 1226 may include respective semantic entities that are displayed with an outline, highlighted, or otherwise changed appearance (e.g., font, size, color, etc.). The indicators 122, 1224, 1226 may alert the user that an action is available with respect to the semantic entity.
Fig. 12C depicts the example mobile computing device 1200 of fig. 12A in a third state, in which additional information is displayed about the selected indicator of fig. 12A and additional indicators describing further available actions are displayed, in accordance with aspects of the present disclosure. More specifically, in response to a user touch action directed to indicator 1224 including the text "Oculus rise," mobile computing device 1200 may be configured to display additional information 1242 about the available actions described by indicator 1224. In this example, the additional information 1242 can include a price, shipping options, etc., associated with purchasing the Oculus Rift system with the shopping application.
The mobile computing device 1200 may also display additional indicators 1244, 1246 that describe additional available actions. In this example, one additional indicator 1244 describes performing a web search on the semantic entity "accumus rise". Another additional indicator 1246 describes an available action that includes viewing a web page (e.g., on Wikipedia) that provides additional information about the semantic entity.
Fig. 13A depicts an example mobile computing device 1300 that is processing audio during a phone call, in accordance with aspects of the present disclosure. The mobile computing device 1300 may be configured to input a model into a machine-learned model (e.g., the recognition model described above with reference to fig. 2A) that includes some or all of the audio processed during a phone call. Importantly, the user may be provided with controls that allow the user to make selections as to whether and when the systems, programs, or features described herein may enable the collection of such information.
Fig. 13B depicts the example mobile computing device 1300 of fig. 13A in a second state, in which an indicator 1302 is displayed in a user interface 1304 that describes available actions with respect to semantic entities identified or recognized in the audio of a phone call, according to aspects of the present disclosure. For example, if the user mentions "dinner is eaten at Tullulah's at 7 pm," the mobile computing device 1300 may provide one or more indicators associated with the restaurant mentioned in the phone call eating dinner.
The mobile computing device 1300 may be configured to provide data describing semantic entities to one or more computer applications. In this example, the data may include one or more semantic entities described by the model output and identified in the audio of the telephone call. The mobile computing device 1300 may provide data to several computer applications, including a calendar application, a restaurant reservation application, and/or additional applications (e.g., a search application, a navigation application, an application for viewing movie times, and/or an application for purchasing movie tickets). The mobile computing device 1300 may provide data according to a predefined API or other protocol.
Importantly, the user may be provided with controls that allow the user to make selections as to whether and when the systems, programs, or features described herein may enable the collection of user information as indicated above. Additionally, in some implementations, when potentially sensitive or personal information is transmitted from the device 1300 (e.g., to a server computing system for processing), the potentially sensitive or personal information may remain on the device and/or a notification or signal may be provided to the user.
The mobile computing device 1300 may be configured to receive application output from the computing applications in response to providing data describing the semantic entities to the computer applications, respectively. The computer application may format, transmit, and/or alert the computing system with respect to application output according to rules or protocols (e.g., as specified by a predefined API). The application output received from each computer application may describe one or more available actions of the corresponding computer application with respect to one or more semantic entities. The application output may describe available actions that may be performed. In this example, the application output can include output from a calendar application, a restaurant reservation application, and/or other computer applications.
The mobile computing device 1300 may be configured to display the indicator 1302 in the user interface 1304. In this example, the indicator 1302 may describe an available action that includes creating a calendar event corresponding to an event mentioned in the audio of the phone session.
FIG. 13C depicts the example mobile computing device 1300 of FIG. 13A in a third state, where information 1306 is displayed regarding the selected available action. More specifically, in response to a user input action with respect to the indicator 1302, the mobile computing device 1300 may be configured to display information 1306 about available actions. In this example, information 1306 may include suggested calendar events that the user may confirm (e.g., by performing a user input for "add event" 1307).
According to aspects of the present disclosure, the mobile computing device 1300 may display additional indicators 1308, 1310, 1312 that describe further available actions with respect to semantic entities detected from the audio of a phone call. In this example, the additional indicators 1308, 1310, 1312 may describe available actions including navigating to a restaurant mentioned in the phone call, making a reservation at the restaurant mentioned in the phone call, viewing the business hours of the restaurant, etc. using the respective computer application.
Fig. 14A depicts the example mobile computing device 1400 in a camera mode, where images from a camera of the mobile computing device 1400 are displayed in a user interface 1402 of the mobile computing device 1400, in accordance with aspects of the present disclosure. The mobile computing device 1400 may identify one or more entities depicted in the user interface 1402 (e.g., using the machine-learned model described above with reference to fig. 2A). The mobile computing device 1400 may be configured to provide data describing one or more semantic entities to one or more computer applications and/or receive application output from a computer application, e.g., as described above with reference to fig. 4-13C.
FIG. 14B depicts the example mobile computing device 1400 of FIG. 14A in a second state, in which additional information 1422 is displayed about the entity depicted in the user interface of FIG. 14A. For example, the mobile computing device 1400 may be configured to provide information 1422 regarding available actions in response to user input with respect to the indicator 1404 of fig. 14A. In this example, the information 1422 may include the name of the landmark, a description of the landmark, and/or the location of the landmark on the map. The information may be provided by a navigation application. The mobile computing device 1400 may be configured to provide additional indicators 1424, 1426, 1428 that describe additional actions that may be performed by the navigation application. In this example, the additional indicators 1424, 1426, 1428 may describe additional actions that may be performed with the navigation application. The mobile computing device 1400 may also be configured to provide additional indicators 1430, 1432, 1423 that describe available actions that may be performed by other applications, such as a search application and/or a subscription application. Information 1422 and/or indicators 1424, 1426, 1428, 1430, 1432, 1434 may be displayed within a panel 1436 (e.g., a half-shelf) in user interface 1402.
Fig. 14C depicts the example mobile computing device 1400 of fig. 14A in a third state in which indicators describing further available actions with respect to landmarks depicted in images from the camera are displayed, in accordance with aspects of the present disclosure. More specifically, in some implementations, the mobile computing device 1400 may be configured to display a larger panel or shelf 1438 in response to user input actions to those shown in fig. 14B requesting additional information and/or available actions. For example, the user may swipe up (swipe) or drag panel 1436 of FIG. 14A to request that larger panel 1438 be displayed. Larger panel 1438 may include additional information 1450, 1460 (e.g., from other computer applications) and/or additional indicators 1452, 1456, 1458, 1464 (e.g., from other computer applications) describing additional available actions. For example, the additional indicators 1464 may include the purchase of tickets to travel or other events at landmarks depicted in the user interface 1402 of fig. 14A and 14B.
Fig. 15A depicts an example mobile computing device 1500 in which text is displayed in a user interface 1502 of the mobile computing device, in accordance with aspects of the present disclosure. The text may include articles in a language. The mobile computing device 1500 may be configured to display an indicator 1504 that describes available actions including translating some or all of the text into another language.
FIG. 15B depicts the example mobile computing device 1500 of FIG. 15A in a second state in which indicators describing available actions with respect to portions of text are displayed, in accordance with aspects of the present disclosure. The mobile computing device 1500 may be configured to provide indicators 1506, 1508, 1510 that describe available actions including translating some or all of the text of an article. For example, the mobile computing device 1500 may be configured to provide the indicators 1506, 1508, 1510 in response to the user highlighting a portion of text.
FIG. 15C depicts the example mobile computing device 1500 of FIG. 15A in a third state, wherein the requested action is performed. For example, the translation may be displayed in a panel or shelf 1542.
Fig. 16A depicts the example mobile computing device 1600 in a first state in which the environmental audio of spanish spoken language is detected and an indicator 1602 describing available actions including translating spanish spoken language into english is displayed in a lock screen 1604 of the mobile computing device 1600, according to aspects of the present disclosure. The mobile computing device 1600 may detect the audio with the microphone and input a model input that includes a recording of the audio into the recognition machine learned model, e.g., as described above with reference to fig. 2A.
Fig. 16B depicts the example mobile computing device 1600 of fig. 16A in a second state, in which indicators 1622, 1624, 1626 are displayed that describe available actions with respect to translations and/or entities mentioned in audio. For example, one indicator 1622 may provide a translation of spanish spoken language from a translation application. Another indicator 1624 may describe an available action from the navigation application that includes navigating to a location mentioned in spanish spoken language. The third indicator 1626 may describe an available action from the reservation application that includes making a reservation at a restaurant mentioned in spanish spoken language, e.g., at or near the time mentioned in spanish spoken language.
FIG. 16C depicts the example mobile computing device 1600 of FIG. 16A in a third state, where additional information 1642, 1644 and/or indicators 1646, 1648 are displayed from the translation application. For example, mobile computing device 1600 may be configured to display additional information 1642, 1644 and/or indicators 1646, 1648 in response to a user input action with respect to indicator 1622 of fig. 16B.
Fig. 17A depicts an example mobile computing device 1700 in which an indicator 1702 is displayed in a lock screen 1704 of the mobile computing device 1700 to show available actions based on calendar data, which may include upcoming flights, in accordance with aspects of the present disclosure. The mobile computing device 1700 may be configured to enter some or all of the calendar data stored by the computing system into a model that identifies machine learning, e.g., as described above with reference to fig. 2A. The mobile computing device 1700 may be configured to receive a model output that describes one or more semantic entities described by a model input. For example, the semantic entity may include information about upcoming flights that the user plans to take.
The mobile computing device 1700 may be configured to provide data describing semantic entities to one or more computer applications. In this example, the data may include an airport code (e.g., "LAX") for the destination airport, a date of the upcoming flight, and/or a time of the upcoming flight. Mobile computing device 1700 may provide data to one or more computer applications, including, for example, an airline application, a lodging application, an entertainment application, and/or a weather application. The mobile computing device 1700 may provide data according to a predefined API or other protocol.
The mobile computing device 1700 may be configured to receive application output from the computing applications in response to providing data describing the semantic entities to the computer applications, respectively. The computer application may format, transmit, and/or alert the computing system with respect to the application output according to rules or protocols (e.g., specified by a predefined API). The application output received from each computer application may describe one or more available actions of the corresponding computer application with respect to one or more semantic entities. In this example, the application output may include output from an airline application, a lodging application, an entertainment application, and/or a weather application. The application output may describe available actions that may be performed with each computer application with respect to the upcoming flight. Mobile computing device 1700 may display indicator 1702 in user interface 1704. The indicator 1702 may alert the user that an action from one or more computer applications is available with respect to an upcoming flight.
Fig. 17B depicts the example mobile computing device 1700 of fig. 17A in a second state in which indicators 1742, 1744, 1746, 1748 are displayed that describe further available actions with respect to calendar data, in accordance with aspects of the present disclosure. For example, one of the indicators 1742 may be displayed that describes the available actions including viewing information and/or documents associated with the upcoming flight (e.g., flight time, boarding pass, etc.). Another one of the indicators 1744 may describe an available action that includes viewing or booking an accommodation in the destination city of the upcoming flight. Another one of the indicators 1746 may describe an available action including downloading entertainment content (e.g., a movie, podcast, etc.) for viewing during the flight. Another of the indicators 1748 may provide weather forecast information about the destination city and/or a date on which the user will be in the destination city.
Fig. 18A depicts a user interface 1800 of an example mobile computing device, with an indicator 1802 displayed in a lock screen 1804 displayed in the user interface 1800. According to aspects of the present disclosure, the indicator 1802 may describe available actions based on the location of the mobile computing device. More specifically, the mobile computing device may use the location of the mobile computing device to identify nearby restaurants or other points of interest, e.g., based on past user interactions or learned preferences. If the user has previously indicated an interest in a particular restaurant or point of interest (e.g., by text message, by viewing articles about the restaurant, by ordering food from the restaurant, having a meal at the restaurant, etc.), the mobile computing device may recognize the restaurant or point of interest when the mobile computing device (e.g., an artificial intelligence system) is nearby.
FIG. 18B depicts the user interface 1800 of FIG. 18A, wherein indicators 1852, 1854 are displayed that describe the available actions for the restaurant described with respect to the indicator 1802 of FIG. 18A. For example, the mobile computing device may be configured to display the indicators 1852, 1854 in response to detecting a user input action with respect to the indicator 1802 of fig. 18A. One indicator 1852 can describe available actions including ordering take-away food from a restaurant. Another indicator 1854 can describe an available action that includes viewing social media about a restaurant.
In some implementations, the indicator may be an available action including a wheelchair-unobstructed entrance and/or exit to navigate to a restaurant or point of interest. For example, the indicator may inform the user that the navigation application is available to navigate the user to the entrance/exit along a suitable unobstructed route (e.g., a route suitable for a wheelchair).
Additional disclosure
The technology discussed herein relates to servers, databases, software applications, and other computer-based systems, and the actions taken by and information sent to and from such systems. The inherent flexibility of computer-based systems allows for a wide variety of possible configurations, combinations, and divisions of tasks and functions between components. For example, the processes discussed herein may be implemented using a single device or component or multiple devices or components operating in combination. Databases and applications may be implemented on a single system or may be distributed across multiple systems. The distributed components may operate sequentially or in parallel.
While the present subject matter has been described in detail with respect to various specific example embodiments thereof, each example is provided by way of illustration, and not limitation, of the present disclosure. Alterations, permutations, and equivalents of such embodiments may readily occur to those skilled in the art upon a reading of the foregoing description. Accordingly, the present disclosure does not preclude inclusion of modifications, variations and/or additions to the present subject matter that would be readily apparent to one of ordinary skill in the art. For instance, features illustrated or described as part of one embodiment, can be used with another embodiment to yield a still further embodiment. Accordingly, it is intended that the present disclosure encompass such alterations, modifications, and equivalents.
Claims (20)
1. A computing system, comprising:
at least one processor;
a machine-learned model configured to receive a model input comprising context data, and in response to receipt of the model input, output a model output describing one or more semantic entities referenced by the context data;
one or more computer applications; and
at least one tangible, non-transitory computer-readable medium storing instructions that, when executed by the at least one processor, cause the at least one processor to perform operations comprising:
inputting the model input into the machine-learned model;
receiving a model output describing one or more semantic entities referenced by the context data as an output of a machine-learned model;
providing data describing the one or more semantic entities to one or more computer applications;
receiving one or more application outputs from the one or more computing applications, respectively, in response to providing data describing the one or more semantic entities to the one or more computer applications, wherein the application outputs received from each computer application describe one or more available actions of the corresponding computer application with respect to the one or more semantic entities; and
providing at least one indicator to a user of the computing system, wherein the at least one indicator describes at least one of the one or more available actions of the corresponding computer application with respect to the one or more semantic entities.
2. The computing system of claim 1, wherein the context data comprises at least one of: information displayed in a user interface, audio played by the computing system, or ambient audio detected by the computing system.
3. The computing system of any preceding claim, wherein the context data comprises at least one of calendar data or a location of a mobile computing device of the computing system.
4. The computing system of any preceding claim, wherein the computing system comprises an artificial intelligence system that includes a machine-learned model and performs the operations, wherein the artificial intelligence system is separate and distinct from, but capable of communicating with, the one or more computer applications.
5. The computing system of any preceding claim, wherein the artificial intelligence system provides data describing the one or more semantic entities to the one or more computer applications via a predefined application programming interface and receives the one or more application outputs from one or more computing applications, respectively.
6. The computing system of any preceding claim, wherein the at least one indicator comprises:
a graphical indicator presented in the user interface; or
An audio indicator played to the user.
7. The computing system of any preceding claim, wherein the operation is performed proactively, without user input requesting performance of the operation.
8. The computing system of any preceding claim, wherein at least the operation of providing the at least one indicator is performed reactively in response to user input.
9. The computing system of any preceding claim, wherein providing the at least one indicator to a user of the computing system comprises displaying the at least one indicator in at least one of an operating system level navigation bar in a user interface or a lock screen in the user interface.
10. The computing system of any preceding claim, wherein the operations further comprise selecting the one or more computer applications from a plurality of applications operable on the computing system to provide data describing the one or more semantic entities based on a comparison between the model output and respective information about the plurality of applications.
11. The computing system of any preceding claim, further comprising selecting at least one or more available actions described by the at least one indicator from the one or more available actions described by the application output to provide to a user based on at least one of a relevance to the one or more semantic entities, a location of a mobile computing device of the computing system, past user interactions, a type of one or more semantic entities, or a type of one or more available actions.
12. The computing system of any preceding claim, further comprising a ranking machine learned model configured to receive input describing one or more available actions described by the output received from each computer application, and in response to receipt of the input, output a ranking output describing a ranking of the one or more available actions, and wherein the operations further comprise:
inputting input describing the one or more available actions into a model of ranking machine learning; and
a ranking output is received that describes a ranking of respective outputs as an output of the ranking machine learned model.
13. The computing system of any preceding claim, wherein the operations further comprise:
receiving stylized output from the one or more computing applications, the stylized output describing aesthetic features associated with displaying the at least one indicator in a user interface; and
displaying the at least one indicator in the user interface based on the stylized output.
14. A computer-implemented method for selecting and providing available actions to a user from one or more computer applications, the method comprising:
inputting, by one or more computing devices, a model input comprising context data into a machine-learned model, the machine-learned model configured to receive the model input, and in response to receipt of the model input, output a model output describing one or more semantic entities referenced by the context data;
receiving, by the one or more computing devices, the model output describing the one or more semantic entities referenced by the context data as an output of the machine-learned model;
providing, by the one or more computing devices, data describing the one or more semantic entities to the one or more computer applications;
receiving, by the one or more computing devices, one or more application outputs from the one or more computing applications, respectively, in response to providing the data describing the one or more semantic entities to the one or more computer applications, wherein the application outputs received from each computer application describe one or more available actions of the corresponding computer application with respect to the one or more semantic entities; and
providing, by the one or more computing devices, at least one indicator to a user of the computing system, wherein the at least one indicator describes at least one of the one or more available actions of the corresponding computer application with respect to the one or more semantic entities.
15. The method of claim 14, wherein at least the operation of providing the at least one indicator is performed reactively in response to user input.
16. The method of any of claims 14 to 15, wherein at least the operation of providing the at least one indicator is performed reactively in response to a user input.
17. The method of any of claims 14 to 16, wherein providing the at least one indicator to the user of the computing system comprises displaying the at least one indicator in at least one of an operating system level navigation bar in a user interface or a lock screen in the user interface.
18. The method of any of claims 14 to 17, further comprising selecting the one or more computer applications from a plurality of applications operable on the computing system to provide data describing the one or more semantic entities based on a comparison between the model output and corresponding information about the plurality of applications.
19. The method of any of claims 14 to 18, further comprising selecting at least one or more available actions described by the at least one indicator from the one or more available actions described by the application output to provide to a user based on at least one of relevance to the one or more semantic entities, past user interactions, a type of one or more semantic entities, or a type of one or more available actions.
20. The method of any of claims 14 to 19, further comprising:
inputting inputs describing the one or more available actions described by the output received from each computer application into a ranking machine learned model configured to receive the inputs, and in response to receipt of the inputs, outputting a ranking output describing a ranking of the one or more available actions; and
a ranking output describing a ranking of the respective outputs is received as an output of a ranking machine learned model.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201862776586P | 2018-12-07 | 2018-12-07 | |
US62/776,586 | 2018-12-07 | ||
PCT/US2019/013029 WO2020117290A1 (en) | 2018-12-07 | 2019-01-10 | System and method for selecting and providing available actions from one or more computer applications to a user |
Publications (2)
Publication Number | Publication Date |
---|---|
CN113168354A true CN113168354A (en) | 2021-07-23 |
CN113168354B CN113168354B (en) | 2024-04-16 |
Family
ID=65366000
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201980081101.7A Active CN113168354B (en) | 2018-12-07 | 2019-01-10 | System and method for selecting and providing available actions to a user from one or more computer applications |
Country Status (6)
Country | Link |
---|---|
US (3) | US11553063B2 (en) |
EP (1) | EP3871092A1 (en) |
JP (1) | JP7134357B2 (en) |
KR (1) | KR20210082250A (en) |
CN (1) | CN113168354B (en) |
WO (1) | WO2020117290A1 (en) |
Families Citing this family (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20220164078A1 (en) * | 2020-11-20 | 2022-05-26 | Microsoft Technology Licensing, Llc | Dynamic and selective presentation of interaction zones by a computer program based on user interaction with content from other computer programs |
JP7414868B2 (en) | 2022-02-18 | 2024-01-16 | Ｌｉｎｅヤフー株式会社 | Information processing device, information processing method, and information processing program |
Citations (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6865528B1 (en) * | 2000-06-01 | 2005-03-08 | Microsoft Corporation | Use of a unified language model |
US20050197823A1 (en) * | 2004-02-03 | 2005-09-08 | Horst Werner | Context modeler and method for modeling a context representation |
CN102792320A (en) * | 2010-01-18 | 2012-11-21 | 苹果公司 | Intelligent automated assistant |
US20130091463A1 (en) * | 2011-10-05 | 2013-04-11 | Paul Nordstrom | Semantic selection and purpose facilitation |
US20130275164A1 (en) * | 2010-01-18 | 2013-10-17 | Apple Inc. | Intelligent Automated Assistant |
CN104781815A (en) * | 2012-12-20 | 2015-07-15 | 英特尔公司 | Method and apparatus for optimization analysis of bonding positions on structure |
CN105068661A (en) * | 2015-09-07 | 2015-11-18 | 百度在线网络技术（北京）有限公司 | Man-machine interaction method and system based on artificial intelligence |
CN105718035A (en) * | 2014-12-04 | 2016-06-29 | 深迪半导体(上海)有限公司 | Human-machine interaction based security control methods and apparatuses for electronic device |
US20160350304A1 (en) * | 2015-05-27 | 2016-12-01 | Google Inc. | Providing suggested voice-based action queries |
CN107924483A (en) * | 2015-08-31 | 2018-04-17 | 微软技术许可有限责任公司 | The general generation and application for assuming arranged model |
US20180314755A1 (en) * | 2015-06-11 | 2018-11-01 | Nuance Communications LLC | Systems and methods for learning semantic patterns from textual data |
Family Cites Families (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9519726B2 (en) * | 2011-06-16 | 2016-12-13 | Amit Kumar | Surfacing applications based on browsing activity |
US20140188956A1 (en) * | 2012-12-28 | 2014-07-03 | Microsoft Corporation | Personalized real-time recommendation system |
KR102202574B1 (en) * | 2013-01-31 | 2021-01-14 | 삼성전자주식회사 | User Interface Displaying Method for Device and Device Thereof |
US20160283055A1 (en) * | 2013-12-20 | 2016-09-29 | Intel Corporation | Customized contextual user interface information displays |
US9965559B2 (en) | 2014-08-21 | 2018-05-08 | Google Llc | Providing automatic actions for mobile onscreen content |
US10686738B2 (en) * | 2015-07-24 | 2020-06-16 | Facebook, Inc. | Providing personal assistant service via messaging |
US11049147B2 (en) * | 2016-09-09 | 2021-06-29 | Sony Corporation | System and method for providing recommendation on an electronic device based on emotional state detection |
US10893011B2 (en) * | 2016-09-13 | 2021-01-12 | Gluru Limited | Semantic interface definition language for action discovery in cloud services and smart devices |
US10540055B2 (en) * | 2017-02-08 | 2020-01-21 | Google Llc | Generating interactive content items based on content displayed on a computing device |
US11263241B2 (en) * | 2018-10-19 | 2022-03-01 | Oracle International Corporation | Systems and methods for predicting actionable tasks using contextual models |
US11900046B2 (en) * | 2020-08-07 | 2024-02-13 | Microsoft Technology Licensing, Llc | Intelligent feature identification and presentation |
-
2019
- 2019-01-10 US US17/311,506 patent/US11553063B2/en active Active
- 2019-01-10 CN CN201980081101.7A patent/CN113168354B/en active Active
- 2019-01-10 WO PCT/US2019/013029 patent/WO2020117290A1/en unknown
- 2019-01-10 EP EP19704690.7A patent/EP3871092A1/en active Pending
- 2019-01-10 JP JP2021532058A patent/JP7134357B2/en active Active
- 2019-01-10 KR KR1020217016763A patent/KR20210082250A/en active IP Right Grant
-
2022
- 2022-12-15 US US18/082,205 patent/US11831738B2/en active Active
-
2023
- 2023-10-24 US US18/493,509 patent/US20240056512A1/en active Pending
Patent Citations (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6865528B1 (en) * | 2000-06-01 | 2005-03-08 | Microsoft Corporation | Use of a unified language model |
US20050197823A1 (en) * | 2004-02-03 | 2005-09-08 | Horst Werner | Context modeler and method for modeling a context representation |
CN102792320A (en) * | 2010-01-18 | 2012-11-21 | 苹果公司 | Intelligent automated assistant |
US20130275164A1 (en) * | 2010-01-18 | 2013-10-17 | Apple Inc. | Intelligent Automated Assistant |
US20130091463A1 (en) * | 2011-10-05 | 2013-04-11 | Paul Nordstrom | Semantic selection and purpose facilitation |
CN104781815A (en) * | 2012-12-20 | 2015-07-15 | 英特尔公司 | Method and apparatus for optimization analysis of bonding positions on structure |
CN105718035A (en) * | 2014-12-04 | 2016-06-29 | 深迪半导体(上海)有限公司 | Human-machine interaction based security control methods and apparatuses for electronic device |
US20160350304A1 (en) * | 2015-05-27 | 2016-12-01 | Google Inc. | Providing suggested voice-based action queries |
US20180314755A1 (en) * | 2015-06-11 | 2018-11-01 | Nuance Communications LLC | Systems and methods for learning semantic patterns from textual data |
CN107924483A (en) * | 2015-08-31 | 2018-04-17 | 微软技术许可有限责任公司 | The general generation and application for assuming arranged model |
CN105068661A (en) * | 2015-09-07 | 2015-11-18 | 百度在线网络技术（北京）有限公司 | Man-machine interaction method and system based on artificial intelligence |
Non-Patent Citations (2)
Title |
---|
MA WEI-BING等: "Semantic Web services description based on command and control interaction user context", 《2014 IEEE 7TH JOINT INTERNATIONAL INFORMATION TECHNOLOGY AND ARTIFICIAL INTELLIGENCE CONFERENCE》, 23 March 2015 (2015-03-23), pages 541 - 544 * |
贺巧艳: "基于多模态信息视频语义检索技术研究", 《中国优秀硕士学位论文全文数据库 信息科技辑》, 15 March 2016 (2016-03-15), pages 138 - 6087 * |
Also Published As
Publication number | Publication date |
---|---|
US11553063B2 (en) | 2023-01-10 |
JP7134357B2 (en) | 2022-09-09 |
EP3871092A1 (en) | 2021-09-01 |
US20230110421A1 (en) | 2023-04-13 |
KR20210082250A (en) | 2021-07-02 |
WO2020117290A1 (en) | 2020-06-11 |
US20220021749A1 (en) | 2022-01-20 |
CN113168354B (en) | 2024-04-16 |
US11831738B2 (en) | 2023-11-28 |
US20240056512A1 (en) | 2024-02-15 |
JP2022511518A (en) | 2022-01-31 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11809886B2 (en) | Intelligent automated assistant in a messaging environment | |
US11076039B2 (en) | Accelerated task performance | |
US10803244B2 (en) | Determining phrase objects based on received user input context information | |
US20190171339A1 (en) | Method, system, and apparatus for executing an action related to user selection | |
US10229167B2 (en) | Ranking data items based on received input and user context information | |
CN105830150B (en) | User experience based on intention | |
US8930393B1 (en) | Referent based search suggestions | |
WO2017058292A1 (en) | Proactive assistant with memory assistance | |
US11831738B2 (en) | System and method for selecting and providing available actions from one or more computer applications to a user | |
US10013152B2 (en) | Content selection disambiguation | |
CN114041145A (en) | System and method for generating and providing suggested actions | |
CA2842031A1 (en) | Method, system, and apparatus for executing an action related to user selection |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |