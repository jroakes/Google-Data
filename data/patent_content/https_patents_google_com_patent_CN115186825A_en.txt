CN115186825A - Full attention with sparse computational cost - Google Patents
Full attention with sparse computational cost Download PDFInfo
- Publication number
- CN115186825A CN115186825A CN202210800059.3A CN202210800059A CN115186825A CN 115186825 A CN115186825 A CN 115186825A CN 202210800059 A CN202210800059 A CN 202210800059A CN 115186825 A CN115186825 A CN 115186825A
- Authority
- CN
- China
- Prior art keywords
- attention
- sequence
- locations
- structured
- input data
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/205—Parsing
- G06F40/216—Parsing using statistical methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/279—Recognition of textual entities
- G06F40/284—Lexical analysis, e.g. tokenisation or collocates
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/30—Semantic analysis
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/40—Processing or translation of natural language
- G06F40/42—Data-driven translation
- G06F40/44—Statistical methods, e.g. probability models
Abstract
The present disclosure relates to a full attention machine learning model architecture with sparse computational cost that provides full attention capability in each attention head while maintaining low computational and memory complexity. In particular, according to one aspect of the present disclosure, the example attention model provided herein can treat the self-attention mechanism as a conditional expectation for embedding at each location and approximate the conditional distribution with structured factorization. Each location can pay attention to all other locations via direct attention or by indirect attention to a group representation, which in turn is a conditional expectation of embedding from the corresponding local region.
Description
RELATED APPLICATIONS
This application claims priority and benefit of U.S. provisional patent application No. 63/220,063, filed 7/9/2021. U.S. provisional patent application No. 63/220,063 is hereby incorporated by reference in its entirety.
Technical Field
The present disclosure relates generally to machine learning. More particularly, the present disclosure relates to a machine learning attention model that provides a full attention mechanism with sparse computational cost.
Background
The Transformer is a powerful neural network architecture that has demonstrated the state-of-the-art performance in machine translation and many other Natural Language Processing (NLP) tasks via pre-training using either uni-directional or bi-directional language modeling. It and its variants have achieved excellent results in other fields as well, like image recognition, code understanding, speech recognition, protein, music and image generative modeling.
The core component of Transformer and other attention-based models is the attention mechanism, which computes the dependency between all pairs of positions in a sequence. However, for sequences of length L, the pair-wise attention expressiveness comes at a secondary cost in terms of both time and memory consumption
Recently, there have been several attempts to expand attention to long sequences. One popular class of methods sparsifies the attention matrix with different sparse patterns including local window, local + stride, log-sparse, axial pattern, or learnable pattern by hashing or clustering. Sparse attention enjoys a sub-quadratic cost, but is lossy in capturing all pair relationships. In general, sparse attention requires more layers to achieve full autoregressive or bi-directional dependence (or receptive field) for each location in a long sequence.
Alternatively, another study has sought to achieve scalability with explicit low rank assumptions on the attention matrix or by using explicit eigenmaps of some kernels. However, these explicit low-dimensional approximations can be too limiting for potential full rank attention matrices that use exponential kernels of virtually infinite dimensions. As one example, performers were in the first work trying to approximate conventional full rank attention with random feature skills. However, such stochastic feature-based approaches require more basis to better approximate the exponential kernel and have been empirically found to produce cross-over results in some sequence modeling tasks such as density estimation.
Thus, attention-based models such as the Transformer provide a class of expression architectures that are extremely efficient for sequence modeling. However, a key limitation of the Transformer is its quadratic memory and temporal complexity relative to the sequence length in the attention layer
Disclosure of Invention
Aspects and advantages of embodiments of the present disclosure will be set forth in part in the description which follows or can be learned from the description or may be learned by practice of the embodiments.
One general aspect includes a computing system for performing an attentiveness mechanism with reduced computing requirements. The computing system also includes one or more processors. The system also includes one or more non-transitory computer-readable media collectively storing a machine-learned attention model configured to receive and process model inputs to generate model outputs, wherein the machine-learned attention model may include one or more attention layers, wherein at least one of the attention layers may include one or more attention heads, and wherein at least one of the attention heads is configured to: receiving a sequence of input data elements; and applying a structured attention pattern to the sequence of input data elements to generate a sequence of output data elements. For each input data element in the sequence of input data elements, the structured attention pattern can specify one or more locations that are directly desired and one or more location groups that are locally desired. For each of the one or more groups of locations having a local expectation, at least one of the heads of attention is configured to: determining a single group probability for a group of locations; and determining a separate local expectation for each location in the set of locations. Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.
Implementations may include one or more of the following features. A computing system, wherein the structured attention pattern may comprise a full attention pattern having a support (support) covering an entirety of the sequence of input data elements. At least one of the attention heads may be configured to reuse the separate local expectation for each position in the group of positions when applying the structured attention pattern for two or more different input data elements in the sequence of input data elements. The input data element sequence may comprise an input embedding sequence. The structured attention pattern can specify a plurality of position groups that are locally desired. The machine-learned attention model may include a plurality of attention layers, wherein each of the plurality of attention layers may include a plurality of attention heads, and wherein each of the plurality of attention heads is configured to apply a structured attention pattern. The structured attention pattern may include a partition tree having two or more hierarchical partition levels. At least one of the attention heads may be configured to: for each of one or more location groups having a local expectation, normalizing (normaize) the individual local expectations of the location groups; and normalizing individual group probabilities for the directly expected location or locations and the group of locations or locations. The structured attention mode may include a combiner-fixed attention mode. The structured attention mode may include a combiner-logsparse attention mode. The model input may include natural language data. The model input may include image data, audio data, protein data, or computer readable code data. The structured attention pattern may specify a plurality of location groups that are locally desired. The structured attention mode may include a combiner-axial (combiner-axial) attention mode. The structured attention pattern may include a machine learning factorization plan specifying one or more locations that are directly desired and one or more location groups that are locally desired. Implementations of the described techniques may include hardware, methods or processes, or computer software on a computer-accessible medium.
Another general aspect includes a computer-implemented method for performing an attention mechanism with reduced computational requirements. The computer-implemented method includes receiving a sequence of input data elements. The method further includes applying a structured attention pattern to each input data element in the sequence of input data elements to generate a sequence of output data elements, wherein applying the structured attention pattern to each input data element may include: determining one or more locations that are directly desired and one or more groups of locations that are locally desired; determining a direct expectation for each of the one or more locations of direct expectation; and for each of the one or more location groups having a local expectation: determining a single group probability for a group of locations; and determining a separate local expectation for each location in the set of locations. Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.
Implementations may include one or more of the following features. A computer-implemented method in which a structured attention pattern has an ensemble of coverage input data element sequences. For at least one of the one or more position groups having a local expectation, determining a separate local expectation for each position in the position group may comprise reusing the separate local expectation for each position in the position group previously calculated for a different input data element in the sequence of input data elements. The input data element sequence may comprise an input embedding sequence. Implementations of the described techniques may include hardware, methods or processes, or computer software on a computer-accessible medium. Other aspects of the disclosure relate to various systems, apparatuses, non-transitory computer-readable media, user interfaces, and electronic devices.
These and other features, aspects, and advantages of various embodiments of the present disclosure will become better understood with reference to the following description and appended claims. The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate exemplary embodiments of the disclosure and, together with the description, serve to explain relevant principles.
Drawings
A detailed discussion of embodiments directed to one of ordinary skill in the art is set forth in the specification, which makes reference to the appended drawings, in which:
fig. 1 depicts an example machine learning attention model according to an example embodiment of the present disclosure.
2A-F depict example attention patterns according to example embodiments of the present disclosure.
Fig. 3A depicts a block diagram of an example computing system, according to an example embodiment of the present disclosure.
Fig. 3B depicts a block diagram of an example computing device, according to an example embodiment of the present disclosure.
Fig. 3C depicts a block diagram of an example computing device, according to an example embodiment of the present disclosure.
Reference numerals repeated across multiple figures are intended to identify the same features in various implementations.
Detailed Description
SUMMARY
In general, the present disclosure relates to a machine learning model architecture that provides full attention capability in each of the heads of attention while maintaining low computational and memory complexity.In particular, according to one aspect of the present disclosure, the example attention model provided herein can treat the self-attention mechanism as a conditional expectation for embedding at each location and approximate the conditional distribution with structured factorization. Each location can pay attention to all other locations via direct attention or by indirect attention to a group representation, which in turn is a conditional expectation of embedding from the corresponding local region. The present disclosure also provides specific example attention patterns for full attention that roughly correspond to certain sparse patterns used in existing sparse transformers and result in the same secondary cost
The systems and methods described herein (an example implementation of which can be referred to as a "Combiner") are a direct substitute for the attention layer in existing converters and can be readily implemented in a common framework. Example experimental evaluation of both the autoregressive and bi-directional sequence tasks contained in U.S. provisional patent application No. 63/220,063 demonstrates the effectiveness of this approach to produce state-of-the-art results over several image and text modeling tasks.
More specifically, the present disclosure provides an improved attention mechanism that can be used as a direct substitute for the vanilla secondary attention mechanism with secondary computation and memory costs. Unlike methods that employ sparse or low rank approximations, the proposed method can still achieve full attention capability within each head of multi-head attention. In particular, in some implementations, the standard attention calculated at each location can be considered as a conditional expectation of value embedding at all feasible locations given the current location.
Based on such an understanding, the proposed attention mechanism explicitly approximates the conditional distribution by a structured decomposition of the probability space. In particular, given a location x, the probability of noting the location y can be computed either directly via a query vector of x and a key vector of y, or indirectly by a local group-based approach in which x first notes a key vector representing a group of locations that contains y and then multiplies the probability of choosing y within the group. An example implementation of this method can be referred to as Combiner because the conditional distribution in attention becomes a combination between several local and direct attentions. This structured factorization enables the proposed attention mechanism to take existing sparse attention modes and convert them into corresponding design choices for probability factorization to achieve full attention.
Example implementations of the present disclosure can achieve full attention with the same asymptotic complexity as the sparse variant. The proposed attention mechanism can be easily implemented in most existing deep learning frameworks without the need for specialized hardware implementations and is GPU/TPU friendly. In fact, both fixed and learnable sparse attention patterns from many existing Transformer variants can be enhanced with such structured factorization at the same order of time or memory cost.
The example experiment contained in U.S. provisional patent application No. 63/220,063 validated Combiner on both the autoregressive and bi-directional sequence modeling tasks in a variety of fields including text and images. Experiments have shown that Combiner can achieve better confusion and accuracy when using the same Transformer architecture, while being much faster in runtime, and achieve state-of-the-art performance on density estimation on standard datasets CIFAR-10 (2.77 bits/dim) and ImageNet-64 (3.42 bits/dim) and Long-Range Arena.
The systems and methods of the present disclosure provide a number of technical effects and benefits. As one example, the systems and methods of the present disclosure can enable full attention to be performed over long sequences with reduced computational cost, resulting in savings of computational resources, such as reduced memory usage, reduced processor usage, and the like. The ability to perform full attention at reduced computational cost also provides better performance (e.g., accuracy) from machine learning models in situations where large input lengths previously prevented the use of full attention due to computational cost. Thus, the system and method of the present disclosure both improves the performance of the model and the computer itself, while also enabling conservation of computing resources.
Referring now to the drawings, example embodiments of the disclosure will be discussed in further detail.
Example attention model
Fig. 1 depicts an example machine learning attention model 12 according to an example embodiment of the present disclosure. The example model 12 illustrated in fig. 1 is simplified for purposes of illustration and is also provided as an example only. Other different architectures or arrangements of layers can be used without departing from the scope of the present disclosure.
The machine learning attention model 12 can be configured to receive and process model inputs 14 to generate model outputs 16. Model input 14 may be any form of data, including raw text or natural language data, text or natural language embedding, audio data, image data, sensor data, protein data, and/or other forms of data, such as various data sequences.
The machine learning attention model 12 can include one or more attention layers (illustrated as example attention layers 18, 20, and 22). Some or all of the attention layers can include one or more attention heads. For example, the attention layer 20 is shown to include four heads, including the head 24. Any number of layers and/or heads can be used.
Some of all of the attention heads (e.g., head 24) can be configured to receive a sequence of input data elements 26 and apply a structured attention pattern to the sequence of input data elements to generate a sequence of output data elements 28.
According to one aspect of the disclosure, for each input data element in the sequence of input data elements, the structured attention pattern can specify one or more locations that are directly desired and one or more sets of locations that are locally desired. Each of the sets of locations can contain any number of locations. The groups may be the same size (number of positions) or different sizes (number of positions).
The attention head 24 can apply the structured attention pattern as follows: for each of the one or more locations that are directly desired, the attention head 24 is able to determine the direct desired. For each of the one or more groups of locations having a local expectation, attention head 24 is able to: determining a single group probability for a group of locations; and determining a separate local expectation for each location in the set of locations. An individual group probability for a group can be determined for the group as a whole or for a representative member of the group.
As an example of this approach, fig. 2A-F depict example attention patterns in accordance with example embodiments of the present disclosure. In particular, FIGS. 2A-C illustrate existing sparse attention patterns with less than a full input set of branches. In contrast, fig. 2D-F illustrate example structured attention patterns that utilize a subset of the universe of input sets to provide full attention in accordance with the present disclosure.
Referring to fig. 2D as an example, the structured attention pattern 200 includes directly desired locations (e.g., locations 202, 204, and 206). Schema 200 also includes location groups (e.g., groups 208 and 210). For example, group 208 contains four locations including, for example, locations 212 and 214.
Wherein
Thus, in some implementations, for a given input x i Applying the structured attention schema 200 can include calculatinglocation 212, local expectation for location 214, etc.). Final attention can then be provided as shown in the above expression.
In some implementations, the local expectation may not necessarily depend on x, e.g., as shown in the expression above i Can thus be reused for a number of different input elements, thereby reducing the number of calculations that need to be performed.
Attention expected as a condition
This section re-looks at the formulation of the standard Transformer from the perspective of conditional expectations, which motivates the derivation of Combiner.
Without loss of generality, the present disclosure uses a single sequence in a self-attentive scenario for ease of description. Given L embedded sequences X = [ X ] 1 ，x 2 ，...，x L ]Wherein
and from each head A h The attention vector of (X) is stitched and projected:
where H is the total number of headers per transform layer. The present disclosure describes how to approximate full attention within each head of multi-head attention. For ease of notation, we drop the head index h whenever possible, and use the lower case letter x i 、q i 、k i 、
For position i ∈ [ L ], attention formula (1) can be considered as conditional expectation of a row in V. Specifically, since softmax outputs a probability distribution, we can rewrite (1) to be
Where p (j | i) represents the conditional probability of the lemma at a given position i at position j and is spread over the support Ω i Is divided into
Full attention expected via structured conditions
The complexity of p (j | i) is A (x) i ) The bottleneck of the calculation of (c). Typically, in existing sparse transformers, the subset of p (j | i) is thinned out to reduce computational and memory complexity, e.g., for LM
Conditional expected local factorization
One of the main ideas described herein is to use a hierarchical structure for conditional probability modeling in equation (3), which provides an opportunity for reducing computational complexity while maintaining the same support. Specifically, we introduce an argument variable
Wherein r is j Indicating the index of the branch to which j belongs. The final equation results from
In which we are dividing
Equivalently, we can also rewrite the structured attention (7) as
Wherein
The requirement for secondary cost. The benefit of this formula can be seen immediately from the fact that the local expectation in (7) is independent of position i. The full dependence being via a multiplier
2. Order to
3. (7) Order of total number of locally desired unique calculations across all locations
then, anyone can see the overall computation and memory costs at the full attention set
Review (further hierarchical decomposition): for simple introduction of one with a support of p (· | i)Local decomposition of individual layer partitions. In fact, such partial decompositions can be further stacked, which introduces a partitioning tree. In particular, we can use disjoint subsets
Parameterized conditional probability
While we have obtained a possible way to accelerate the standard transform via a combination of direct expectation and local expectation, for the probability term in (7), i.e., from the direct expectation
The probability terms are normalized. Capable of normalizing each local expectation within a local span
Example tradeoffs
Combiner achieves full attention at a reduced cost without making explicit sparseness or low rank assumptions throughout the attention matrix. This efficiency gain is not free, however. This section discusses the simplified limitations made by Combiner and provides a simple strain method.
Structured attention approximation.
We obtained a local decomposition under the condition independent assumption (5). Thus, the local expectation in (7) is independent of position i, which indicates having
A hybrid model is used to improve expressiveness.
One way to further improve the expressiveness of local factorization is to use a hybrid model. This idea achieves a high rank softmax layer in language modeling. Let omega be omega i The support of (i.e.,
Example instantiation
This section illustrates several example partial factorization schemes that meet the requirements described herein. As shown, combiner is able to convert several sparse transformers to full attention with the same order of computation and memory consumption. Anyone can also design other factorization modes that can be easily instantiated in Combiner.
Combiner-fixation
Sparse Transformer is capable of being implemented with Sparse attention
Here it is shown how the fixed pattern is converted into a factorisation plan and a full attention variant named Combiner-fixed is instantiated (fig. 2D).
In fixed sparse attention, the support is
where each local expectation is performed in each span of size s, and there are a total of L div s spans across all locations. For each position i e L](s + (L divs)) in (7); local expectation has (L div s) terms. The overall complexity is
Combiner-Logsparse
Logspreary transformers are proposed in the development of the memory bottleneck Neural Information Processing system (NeuroIPS) for enhancing locality and breaking time series predictions by Shiyang Li, xiaooyong Jin, yao Xuan, xioyou Zhou, wenhu Chen, yu-Xiang Wang and Xifeng Yan.
Logsparse transducer can be theoretically implemented
To take advantage of this scheme in the Combiner framework, we can define
Combiner-axial
Axial transformers are described in Jonathan Ho, nal Kalchbrenner, dirk Weissenborn and Tim Salimans.Axilian Attention in multidimensional transformations.arXiv preprint arXiv 1912.12180,2019 (axial attention in Jonathan Ho, nal Kalchbrenner, dirk Weissenborn and Tim Salimans.multidimensional transformations. ArXiv preprint arXiv 1912.12180, 2019).
The axial Transformer builds attention along each axis of the input data. Without loss of generality, we focus on the 2D case where the input sequence is reshaped into a matrix of size n × m = L. Specifically, position i in the original sequence will be at row i = (i-1) div m +1 and col i = (i-1) mod m + 1. We show how to achieve Combiner-axial direction simply by factoring the 2D matrix to achieve full attention.
Axially of sparseness having
In all cases above, if we reshape the sequence to have
Combiner-learning (Combiner-Learnable)
Another example implementation is also able to learn the factoring plan ω from the data. We illustrate this with a Routing Transformer and provide a means for achieving full attention in a Routing Transformer that follows the Combiner principle.
For a particular layer, assume we have disjoint regions of learning (or clusters in a routing Transformer)
Note that n is for all positions i = n (number of clusters learned). The above factorization can only be applied to MLM. LM requires the following definitions:
generally, when
Example apparatus and System
Fig. 3A depicts a block diagram of an example computing system 100 capable of performing full attention at reduced computational cost, according to an example embodiment of the present disclosure. The system 100 includes a user computing device 102, a server computing system 130, and a training computing system 150 communicatively coupled through a network 180.
The user computing device 102 may be any type of computing device, such as, for example, a personal computing device (e.g., a laptop or desktop), a mobile computing device (e.g., a smartphone or tablet), a gaming console or controller, a wearable computing device, an embedded computing device, or any other type of computing device.
The user computing device 102 includes one or more processors 112 and memory 114. The one or more processors 112 may be any suitable processing device (e.g., processor cores, microprocessors, ASICs, FPGAs, controllers, microcontrollers, etc.) and may be one processor or a plurality of processors operatively connected. The memory 114 can include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, disks, and the like, and combinations thereof. The memory 114 can store data 116 and instructions 118 that are executed by the processor 112 to cause the user computing device 102 to perform operations.
In some implementations, the user computing device 102 can store or include one or more machine learning models 120. For example, the machine learning model 120 may be or can otherwise include various machine learning models, such as a neural network (e.g., a deep neural network) or other types of machine learning models, including non-linear models and/or linear models. The neural network can include a feed-forward neural network, a recurrent neural network (e.g., a long-short term memory recurrent neural network), a convolutional neural network, or other form of neural network. Some example machine learning models can utilize attention mechanisms such as self-attention. For example, some example machine learning models can include a multi-headed self-attention model (e.g., a transform model). An example machine learning model 120 is discussed with reference to fig. 1-2F.
In some implementations, the one or more machine learning models 120 can be received from the server computing system 130 over the network 180, stored in the user computing device memory 114, and then used or otherwise implemented by the one or more processors 112. In some implementations, the user computing device 102 can implement multiple parallel instances of a single machine learning model 120 (e.g., to perform parallel processing across multiple instances of input).
Additionally or alternatively, one or more machine learning models 140 can be included in the server computing system 130 or otherwise stored and implemented by the server computing system 130, the server computing system 130 communicating with the user computing device 102 according to a client-server relationship. For example, the machine learning model 140 can be implemented by the server computing system 140 as part of a web service. Thus, one or more models 120 can be stored and implemented at the user computing device 102 and/or one or more models 140 can be stored and implemented at the server computing system 130.
The user computing device 102 can also include one or more user input components 122 that receive user input. For example, the user input component 122 may be a touch-sensitive component (e.g., a touch-sensitive display screen or touchpad) that is sensitive to touch by a user input object (e.g., a finger or stylus). The touch sensitive component can be used to implement a virtual keyboard. Other example user input components include a microphone, a conventional keyboard, or other device through which a user can provide user input.
The server computing system 130 includes one or more processors 132 and memory 134. The one or more processors 132 may be any suitable processing device (e.g., processor cores, microprocessors, ASICs, FPGAs, controllers, microcontrollers, etc.) and may be one processor or a plurality of processors operatively connected. The memory 134 can include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, disks, and the like, and combinations thereof. The memory 134 is capable of storing data 136 and instructions 138 that are executed by the processor 132 to cause the server computing system 130 to perform operations.
In some implementations, the server computing system 130 includes or is otherwise implemented by one or more server computing devices. Where the server computing system 130 includes multiple server computing devices, such server computing devices can operate according to a sequential computing architecture, a parallel computing architecture, or some combination thereof.
As described above, the server computing system 130 can store or otherwise include one or more machine learning models 140. For example, the model 140 may be or can otherwise include various machine learning models. Example machine learning models include neural networks or other multi-layered nonlinear models. Example neural networks include feed-forward neural networks, deep neural networks, recurrent neural networks, and convolutional neural networks. Some example machine learning models can utilize attention mechanisms such as self-attention. For example, some example machine learning models can include a multi-headed self-attention model (e.g., a transform model). An example model 140 is discussed with reference to fig. 1-2F.
The user computing device 102 and/or the server computing system 130 can train the models 120 and/or 140 via interaction with a training computing system 150 communicatively coupled through a network 180. Training computing system 150 can be separate from server computing system 130 or can be part of server computing system 130.
The training computing system 150 can include a model trainer 160 that trains the machine learning models 120 and/or 140 stored at the user computing device 102 and/or the server computing system 130 using various training or learning techniques, such as, for example, back propagation of errors. For example, the loss function can be propagated back through the model to update one or more parameters of the model (e.g., based on the gradient of the loss function). Various loss functions can be used, such as mean square error, likelihood loss, cross entropy loss, hinge loss, and/or various other loss functions. The gradient descent technique can be used to iteratively update the parameters through a number of training iterations.
In some implementations, performing back-propagation of the error can include performing truncated back-propagation over time. The model trainer 160 can perform a number of generalization techniques (e.g., weight decay, tentative regression, etc.) to improve the generalization capability of the trained model.
In particular, the model trainer 160 can train the machine learning models 120 and/or 140 based on a set of training data 162. In some implementations, the training examples can be provided by the user computing device 102 if the user has provided consent. Thus, in such implementations, the model 120 provided to the user computing device 102 can be trained by the training computing system 150 on user-specific data received from the user computing device 102. In some cases, this process can be referred to as a personalization model.
The model trainer 160 includes computer logic that is utilized to provide the desired functionality. The model trainer 160 can be implemented in hardware, firmware, and/or software that controls a general purpose processor. For example, in some implementations, model trainer 160 includes program files stored on a storage device, loaded into memory, and executed by one or more processors. In other implementations, the model trainer 160 includes one or more sets of computer executable instructions stored in a tangible computer readable storage medium such as RAM, a hard disk, or an optical or magnetic medium.
The network 180 may be any type of communication network, such as a local area network (e.g., an intranet), a wide area network (e.g., the internet), or some combination thereof and can include any number of wired or wireless links. In general, communications over network 180 can be carried via any type of wired and/or wireless connection using a wide variety of communication protocols (e.g., TCP/IP, HTTP, SMTP, FTP), encodings or formats (e.g., HTML, XML), and/or protection schemes (e.g., VPN, secure HTTP, SSL).
The machine learning models described in this specification can be used in a variety of tasks, applications, and/or use cases.
In some implementations, the input to the machine learning model of the present disclosure can be image data. The machine learning model can process the image data to generate an output. As an example, the machine learning model can process the image data to generate an image recognition output (e.g., identification of the image data, potential embedding of the image data, an encoded representation of the image data, a hash of the image data, etc.). As another example, the machine learning model can process image data to generate an image segment output. As another example, the machine learning model can process image data to generate an image classification output. As another example, the machine learning model can process the image data to generate an image data modification output (e.g., alteration of the image data, etc.). As another example, the machine learning model can process the image data to generate an encoded image data output (e.g., an encoded and/or compressed representation of the image data, etc.). As another example, the machine learning model can process the image data to generate an augmented image data output. As another example, the machine learning model can process image data to generate a prediction output.
In some implementations, the input to the machine learning model of the present disclosure can be text or natural language data. The machine learning model can process text or natural language data to generate an output. As an example, the machine learning model can process natural language data to generate a language coded output. As another example, the machine learning model can process text or natural language data to generate potential text-embedded output. As another example, a machine learning model can process text or natural language data to generate translation output. As another example, the machine learning model can process text or natural language data to generate a classification output. As another example, the machine learning model can process text or natural language data to generate a text segment output. As another example, the machine learning model can process text or natural language data to generate a semantic intent output. As another example, the machine learning model can process text or natural language data to generate an augmented text or natural language output (e.g., text or natural language data of a higher quality than the input text or natural language, etc.). As another example, a machine learning model can process textual or natural language data to generate a prediction output.
In some implementations, the input to the machine learning model of the present disclosure can be speech data. The machine learning model can process the speech data to generate an output. As an example, a machine learning model can process speech data to generate a speech recognition output. As another example, the machine learning model can process speech data to generate a speech translation output. As another example, the machine learning model can process speech data to generate a potential embedded output. As another example, the machine learning model can process speech data to generate an encoded speech output (e.g., an encoded and/or compressed representation of the speech data, etc.). As another example, the machine learning model can process speech data to generate an augmented speech output (e.g., speech data of higher quality than the input speech data, etc.). As another example, the machine learning model can process speech data to generate a textual representation output (e.g., a textual representation of the input speech data, etc.). As another example, a machine learning model can process speech data to generate a prediction output.
In some implementations, the input to the machine learning model of the present disclosure can be potentially encoded data (e.g., a potential spatial representation of the input, etc.). The machine learning model is capable of processing the latent coded data to generate an output. As an example, the machine learning model can process the latent encoding data to generate the recognition output. As another example, the machine learning model can process the potentially encoded data to generate a reconstructed output. As another example, the machine learning model can process the potential encoding data to generate a search output. As another example, the machine learning model can process the potential encoding data to generate a re-clustering output. As another example, the machine learning model can process the potentially encoded data to generate a prediction output.
In some implementations, the input to the machine learning model of the present disclosure can be statistical data. The statistical data may be, represent, or otherwise include data calculated and/or computed from some other data source. The machine learning model can process the statistical data to generate an output. As an example, the machine learning model can process the statistics to generate recognition output. As another example, the machine learning model can process the statistics to generate a prediction output. As another example, the machine learning model can process the statistics to generate a classification output. As another example, the machine learning model can process the statistics to generate a segmented output. As another example, the machine learning model can process the statistics to generate a visual output. As another example, the machine learning model can process the statistics to generate a diagnostic output.
In some implementations, the input to the machine learning model of the present disclosure can be sensor data. The machine learning model can process the sensor data to generate an output. As an example, the machine learning model can process sensor data to generate a recognition output. As another example, the machine learning model can process sensor data to generate a predicted output. As another example, the machine learning model can process sensor data to generate classification outputs. As another example, the machine learning model can process sensor data to generate a segmented output. As another example, the machine learning model can process sensor data to generate a visual output. As another example, the machine learning model can process sensor data to generate a diagnostic output. As another example, the machine learning model can process sensor data to generate a detection output.
In some cases, the machine learning model can be configured to perform tasks that include encoding input data for reliable and/or efficient transmission or storage (and/or corresponding decoding). For example, the task may be an audio compression task. The input may include audio data and the output may include compressed audio data. In another example, the input includes visual data (e.g., one or more images or video), the output includes compressed visual data, and the task is a visual data compression task. In another example, a task may include generating an embedding for input data (e.g., input audio or video data).
In some cases, the input includes visual data and the task is a computer vision task. In some cases, pixel data comprising one or more images is input and the task is an image processing task. For example, the image processing task may be image classification, where the output is a set of scores, each score corresponding to a different object class and representing a likelihood that one or more images depict an object belonging to that object class. The image processing task may be object detection, where the image processing output identifies one or more regions in one or more images, and for each region, a likelihood that the region depicts an object of interest. As another example, the image processing task may be image segmentation, wherein the image processing output defines, for each pixel in the one or more images, a respective likelihood for each category in a predetermined set of categories. For example, the set of categories may be foreground and background. As another example, the set of categories may be object classes. As another example, the image processing task may be depth estimation, where the image processing output defines a respective depth value for each pixel in one or more images. As another example, the image processing task may be motion estimation, where the network input comprises a plurality of images, and the image processing output defines for each pixel of one of the input images the motion of a scene depicted at the pixel between the images in the network input.
In some cases, the input includes audio data representing a spoken utterance and the task is a speech recognition task. The output may include a textual output mapped to the spoken utterance. In some cases, the task includes encrypting or decrypting the input data. In some cases, the tasks include microprocessor performance tasks, such as branch prediction or memory address translation.
FIG. 3A illustrates one example computing system that can be used to implement the present disclosure. Other computing systems can also be used. For example, in some implementations, the user computing device 102 can include a model trainer 160 and a training data set 162. In such implementations, the model 120 can be both trained and used locally at the user computing device 102. In some such implementations, the user computing device 102 can implement the model trainer 160 to personalize the model 120 based on user-specific data.
Fig. 3B depicts a block diagram of an example computing device 10, performed in accordance with an example embodiment of the present disclosure. Computing device 10 may be a user computing device or a server computing device.
As illustrated in fig. 3B, each application is capable of communicating with many other components of the computing device, such as, for example, one or more sensors, a context manager, a device status component, and/or additional components. In some implementations, each application can use an API (e.g., a public API) to communicate with each device component. In some implementations, the APIs used by each application are specific to that application.
Fig. 3C depicts a block diagram of an example computing device 50, performed in accordance with an example embodiment of the present disclosure. Computing device 50 may be a user computing device or a server computing device.
The central smart inlay includes a number of machine learning models. For example, as illustrated in fig. 3C, a respective machine learning model can be provided for each application and managed by the central intelligence layer. In other implementations, two or more applications can share a single machine learning model. For example, in some implementations, the central smart inlay can provide a single model for all applications. In some implementations, the central smart inlay is included within the operating system of the computing device 50 or is otherwise implemented by the operating system of the computing device 50.
The central intelligent layer can communicate with the central equipment data layer. The central device data layer may be a centralized repository for data of the computing device 50. As illustrated in fig. 3C, the central device data layer can communicate with many other components of the computing device, such as, for example, one or more sensors, a context manager, a device state component, and/or additional components. In some implementations, the central device data layer can communicate with each device component using an API (e.g., a proprietary API).
Additional disclosure
The techniques discussed herein make reference to servers, databases, software applications, and other computer-based systems, as well as actions taken and information sent to and from such systems. The inherent flexibility of computer-based systems allows for a wide variety of possible configurations, combinations, and divisions between and among components of accomplishing tasks and functionality. For example, the processes discussed herein can be implemented using a single device or component or multiple devices or components working in conjunction. Databases and applications can be implemented on a single system or distributed across multiple systems. The distributed components can work sequentially or in parallel.
While the present subject matter has been described in detail with respect to various specific example embodiments thereof, each example is provided by way of illustration, and not limitation, of the present disclosure. Alterations, modifications, and equivalents of such embodiments may occur to others skilled in the art upon the attainment of an understanding of the foregoing. Accordingly, the subject disclosure does not preclude inclusion of such modifications, variations and/or additions to the present subject matter as would be readily apparent to one of ordinary skill in the art. For instance, features illustrated or described as part of one embodiment, can be used with another embodiment to yield a still further embodiment. Accordingly, the present disclosure is intended to cover such alternatives, modifications, and equivalents.
Claims (20)
1. A computing system for performing an attention mechanism with reduced computing requirements, the computing system comprising:
one or more processors; and
one or more non-transitory computer-readable media collectively storing a machine-learned attention model configured to receive and process model inputs to generate a model output, wherein the machine-learned attention model comprises one or more attention layers, wherein at least one of the one or more attention layers comprises one or more attention heads, and wherein at least one of the one or more attention heads is configured to:
receiving a sequence of input data elements; and is
Applying a structured attention pattern to the sequence of input data elements to generate a sequence of output data elements;
wherein, for each input data element in the sequence of input data elements, the structured attention pattern specifies one or more locations that are directly desired and one or more groups of locations that are locally desired; and
wherein, for each of the one or more locally desired groups of locations, the at least one of the one or more attention heads is configured to:
determining a single group probability for the group of locations; and is
A local expectation is determined for each location in the set of locations.
2. The computing system of claim 1, wherein the structured attention pattern comprises a full attention pattern having a set of ensembles covering the sequence of input data elements.
3. The computing system of claim 1, wherein the at least one of the one or more attention heads is configured to reuse a local expectation for each location in the set of locations when applying the structured attention pattern for two or more different input data elements in the sequence of input data elements.
4. The computing system of claim 1, wherein the sequence of input data elements comprises an input embedding sequence.
5. The computing system of claim 1 wherein the structured attention pattern specifies a plurality of location groups that are locally desired.
6. The computing system of claim 1, wherein the machine learning attention model comprises a plurality of attention layers, wherein each of the plurality of attention layers comprises a plurality of attention heads, and wherein each of the plurality of attention heads is configured to apply the structured attention pattern.
7. The computing system of claim 1 wherein the structured attention pattern comprises a partition tree having two or more hierarchical partition levels.
8. The computing system of claim 1, wherein the at least one of the one or more attention heads is configured to:
normalizing, for each of the one or more location groups of local expectations, the individual local expectations of that location group; and is
Normalizing the individual group probabilities for the one or more locations and the one or more location groups that are directly expected.
9. The computing system of claim 1, wherein the structured attention mode comprises a combiner-fixed attention mode.
10. The computing system of claim 1, wherein the structured attention mode comprises a combiner-logsparse attention mode.
11. The computing system of claim 1, wherein the structured attention mode comprises a combiner-axial attention mode.
12. The computing system of claim 1, wherein the structured attention pattern comprises a machine learning factorization plan specifying the one or more locations that are directly desired and the one or more groups of locations that are locally desired.
13. The computing system of any of claims 1-12, wherein the model input includes natural language data.
14. The computing system of any of claims 1-12, wherein the model input comprises image data, audio data, protein data, or computer-readable code data.
15. A computer-implemented method for performing an attention mechanism with reduced computational requirements, the method comprising:
receiving a sequence of input data elements; and
applying a structured attention pattern to each input data element in the sequence of input data elements to generate a sequence of output data elements, wherein applying the structured attention pattern to each input data element comprises:
determining one or more locations that are directly desired and one or more groups of locations that are locally desired; and
determining a direct expectation for each of the one or more locations of direct expectation; and
for each of the one or more locally desired location groups:
determining a single group probability for the group of locations; and
a local expectation is determined for each location in the set of locations.
16. The computer-implemented method of claim 15, wherein the structured attention pattern has a set that covers an entirety of the sequence of input data elements.
17. The computer-implemented method of claim 15, wherein, for at least one of the one or more location groups that is locally desired, determining the local desire for each location in the location group comprises: reusing the local expectation for each position in the set of positions previously calculated for a different input data element in the sequence of input data elements.
18. The computer-implemented method of claim 15, wherein the sequence of input data elements comprises an input embedding sequence.
19. The computer-implemented method of any of claims 15-18, wherein the structured attention pattern specifies a plurality of location groups that are locally desired.
20. One or more non-transitory computer-readable media collectively storing a machine-learned attention model, wherein:
the machine-learned attention model is configured to receive and process model inputs to generate a model output, the machine-learned attention model includes one or more attention layers, at least one of the one or more attention layers includes one or more attention heads, and at least one of the one or more attention heads is configured to:
receiving a sequence of input data elements; and
applying a structured attention pattern to the sequence of input data elements to generate a sequence of output data elements;
wherein, for each input data element in the sequence of input data elements, the structured attention pattern specifies one or more locations that are directly desired and one or more groups of locations that are locally desired; and
wherein, for each of the one or more locally desired position groups, the at least one of the one or more attention heads is configured to:
determining a single group probability for the group of locations; and is
A local expectation is determined for each location in the set of locations.
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202163220063P | 2021-07-09 | 2021-07-09 | |
US63/220,063 | 2021-07-09 |
Publications (1)
Publication Number | Publication Date |
---|---|
CN115186825A true CN115186825A (en) | 2022-10-14 |
Family
ID=83518068
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202210800059.3A Pending CN115186825A (en) | 2021-07-09 | 2022-07-08 | Full attention with sparse computational cost |
Country Status (2)
Country | Link |
---|---|
US (1) | US20230022151A1 (en) |
CN (1) | CN115186825A (en) |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2024087185A1 (en) * | 2022-10-28 | 2024-05-02 | Intel Corporation | Memory access adaptive self-attention mechanism for transformer model |
-
2022
- 2022-07-08 CN CN202210800059.3A patent/CN115186825A/en active Pending
- 2022-07-08 US US17/860,691 patent/US20230022151A1/en active Pending
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2024087185A1 (en) * | 2022-10-28 | 2024-05-02 | Intel Corporation | Memory access adaptive self-attention mechanism for transformer model |
Also Published As
Publication number | Publication date |
---|---|
US20230022151A1 (en) | 2023-01-26 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
Choudhary et al. | A comprehensive survey on model compression and acceleration | |
CN110796190B (en) | Exponential modeling with deep learning features | |
Song et al. | Unified binary generative adversarial network for image retrieval and compression | |
US20230359865A1 (en) | Modeling Dependencies with Global Self-Attention Neural Networks | |
WO2021159714A1 (en) | Data processing method and related device | |
CN111755078A (en) | Drug molecule attribute determination method, device and storage medium | |
CN110084281A (en) | Image generating method, the compression method of neural network and relevant apparatus, equipment | |
US20200311548A1 (en) | Learning compressible features | |
US20230017072A1 (en) | Systems And Methods For Improved Video Understanding | |
US20240046067A1 (en) | Data processing method and related device | |
CN114021696A (en) | Conditional axial transform layer for high fidelity image transformation | |
CN114926770A (en) | Video motion recognition method, device, equipment and computer readable storage medium | |
US20200372295A1 (en) | Minimum-Example/Maximum-Batch Entropy-Based Clustering with Neural Networks | |
Yang et al. | Attentional gated Res2Net for multivariate time series classification | |
CN115186825A (en) | Full attention with sparse computational cost | |
Yi et al. | Elanet: effective lightweight attention-guided network for real-time semantic segmentation | |
US11948090B2 (en) | Method and apparatus for video coding | |
US11531863B1 (en) | Systems and methods for localization and classification of content in a data set | |
CN116740078A (en) | Image segmentation processing method, device, equipment and medium | |
US20220245428A1 (en) | Machine-Learned Attention Models Featuring Omnidirectional Processing | |
Chung et al. | Filter pruning by image channel reduction in pre-trained convolutional neural networks | |
US20230229886A1 (en) | Modeling of Long-Range Interactions with Reduced Feature Materialization via Lambda Functions | |
US20220245917A1 (en) | Systems and methods for nearest-neighbor prediction based machine learned models | |
US11755883B2 (en) | Systems and methods for machine-learned models having convolution and attention | |
CN116561425B (en) | Web service recommendation method based on domain interaction self-attention factor decomposition machine |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |