CN112352276A - Coordinated execution of a series of requested actions to be performed via an automated assistant - Google Patents
Coordinated execution of a series of requested actions to be performed via an automated assistant Download PDFInfo
- Publication number
- CN112352276A CN112352276A CN201980043375.7A CN201980043375A CN112352276A CN 112352276 A CN112352276 A CN 112352276A CN 201980043375 A CN201980043375 A CN 201980043375A CN 112352276 A CN112352276 A CN 112352276A
- Authority
- CN
- China
- Prior art keywords
- actions
- action
- user
- automated assistant
- order
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
- 230000009471 action Effects 0.000 title claims abstract description 667
- 230000003993 interaction Effects 0.000 claims abstract description 41
- 238000012545 processing Methods 0.000 claims abstract description 29
- 238000000034 method Methods 0.000 claims description 97
- 230000004044 response Effects 0.000 claims description 31
- 230000002123 temporal effect Effects 0.000 claims description 27
- 230000000977 initiatory effect Effects 0.000 claims description 25
- 230000000153 supplemental effect Effects 0.000 claims description 5
- 238000003062 neural network model Methods 0.000 claims description 4
- 238000004590 computer program Methods 0.000 claims 1
- 238000010801 machine learning Methods 0.000 abstract description 64
- 230000015654 memory Effects 0.000 abstract description 9
- 239000002699 waste material Substances 0.000 abstract description 5
- 238000012549 training Methods 0.000 description 40
- 239000003795 chemical substances by application Substances 0.000 description 12
- 230000008859 change Effects 0.000 description 8
- 230000008569 process Effects 0.000 description 8
- 239000000463 material Substances 0.000 description 5
- 230000003111 delayed effect Effects 0.000 description 3
- 235000013305 food Nutrition 0.000 description 3
- 230000006870 function Effects 0.000 description 3
- 230000004048 modification Effects 0.000 description 3
- 238000012986 modification Methods 0.000 description 3
- 230000002093 peripheral effect Effects 0.000 description 3
- 230000002085 persistent effect Effects 0.000 description 3
- 230000000007 visual effect Effects 0.000 description 3
- 244000025272 Persea americana Species 0.000 description 2
- 235000008673 Persea americana Nutrition 0.000 description 2
- 238000004891 communication Methods 0.000 description 2
- 238000010586 diagram Methods 0.000 description 2
- 238000013507 mapping Methods 0.000 description 2
- 230000007246 mechanism Effects 0.000 description 2
- 241000196324 Embryophyta Species 0.000 description 1
- 241000282412 Homo Species 0.000 description 1
- 238000013528 artificial neural network Methods 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 238000012790 confirmation Methods 0.000 description 1
- 230000000694 effects Effects 0.000 description 1
- 230000007613 environmental effect Effects 0.000 description 1
- 230000002452 interceptive effect Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 230000000116 mitigating effect Effects 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 238000012913 prioritisation Methods 0.000 description 1
- 238000009877 rendering Methods 0.000 description 1
- 230000004043 responsiveness Effects 0.000 description 1
- 238000013179 statistical model Methods 0.000 description 1
- 230000001755 vocal effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/02—Feature extraction for speech recognition; Selection of recognition unit
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/28—Constructional details of speech recognition systems
- G10L15/34—Adaptation of a single recogniser for parallel processing, e.g. by use of multiple processors or cloud computing
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/223—Execution procedure of a spoken command
Abstract
Embodiments are set forth herein for creating a sequence of performance of actions requested by a user via a spoken utterance to an automated assistant. The order of execution of the requested actions may be based on how each requested action is able to or predicted to affect other requested actions. In some implementations, the order of execution of the series of actions can be determined based on the output of a machine learning model, such as a model that has been trained from supervised learning. The particular order of execution may be selected to mitigate waste of processing, memory, and network resources, at least relative to other possible orders of execution. Certain execution sequences may be adapted over time using interaction data characterizing past executions of the automated assistant, thereby allowing the automated assistant to learn from past interactions with one or more users.
Description
Background
Humans may participate in human-computer conversations using what is referred to herein as an "automated assistant" (also referred to as a "digital agent," "chat bot," "interactive personal assistant," "intelligent personal assistant," "assistant application," "session agent," etc.). For example, a human being (which may be referred to as a "user" when they interact with the automatic assistant) may provide commands and/or requests to the automatic assistant using spoken natural language input (i.e., utterances) that may be converted to text and then processed in some cases and/or by providing textual (e.g., typed) natural language input. The automated assistant responds to the request by providing a responsive user interface output, which may include an audible and/or visual user interface output.
In many cases, the automated assistant can perform actions exclusively according to the order set forth by the user in specifying the requested action. This can prove problematic, particularly when the user provides a single natural language input that includes requests for multiple different actions to be performed. For example, the user may be able to set forth a request for a series of actions to be performed in an order that the user may not have fully considered before the request is spoken. As a result, certain actions that might otherwise be performed relatively quickly may be inadvertently delayed, thereby introducing latency and other computational inefficiencies into certain performance aspects of the automated assistant.
As one example, if a user requests to provide a media stream and perform a web search, the automated assistant may thus prioritize providing the media stream, thereby delaying execution of the web query — which in many cases can otherwise be performed relatively quickly. If the user forgets about the web query while continuous media playback is being provided, the user may then reissue the same request, thereby wasting computing resources on processing duplicate requests. Further, when an automated assistant relies exclusively on the user to indicate the order of a sequence of actions to be performed, certain processes such as downloading, caching, and/or rendering data may not be arranged sequentially to perform in an optimal manner.
Disclosure of Invention
Described herein are implementations related to systems, methods, and apparatuses for creating an execution order for a plurality of requested automated assistant actions. In particular, a user can request actions via a single spoken utterance, and the order of execution of the actions can be arranged according to how each requested action can or is predicted to affect another requested action. For example, the spoken utterance can include a request for a plurality of actions to perform, and at least one action of the plurality of actions can correspond to a dialog-initiated query. If the dialog-initiated query is to be prioritized before other actions, execution of the other actions will be deferred until the dialog between the user and the automated assistant is completed. Thus, to mitigate latency between the request and execution of an action, a dialog initiation query can be assigned as the last action to complete in a plurality of requested actions. The user is thus able to submit commands to the device or system, which are then interpreted for more efficient performance. That is, since the order of execution can be determined by a system, method, or device, the user does not need to coordinate the most efficient form of command input with the most efficient form of command execution. Thus improving the ease and efficiency of interaction.
As an example, a user can provide a user interface such as "Assistant, set an alarm for tomorrow and an also couled you tell me the weather for tomorrow? (assistant, set up tomorrow's alarm and can you also tell me tomorrow. The automated assistant can receive the spoken utterance and determine that the spoken utterance includes a first request to set an alarm and a second request to provide a weather report. Further, the automatic assistant can determine that setting the alarm requires the user to provide supplemental information and, therefore, further dialog needs to be processed. For example, the automatic assistant may have to generate a natural language output that includes a request for the time of the alarm clock, and the automatic assistant may have to process subsequent input from the user that identifies the time of the alarm clock. Additionally, the automated assistant can determine that providing a weather report requires submitting a request over the internet, receiving weather data in response, and converting the weather data into a natural language output for the user. Thus, based at least on the first request requiring further information from the user and the second request not requiring any further dialog with the user, the automated assistant is able to command performance of the action by fulfilling the second request prior to fulfilling the first request. In other words, at least based on the setting of the alarm clock requiring further information from the user, the user will receive a weather report before participating in the dialog session for setting the alarm clock.
In some implementations, the requested actions performed by the user can be scheduled according to whether one or more of the requested actions result in a persistent output or are predicted to result in a persistent output relative to other requested actions. In other words, actions to be performed upon a user request can be arranged according to whether one or more of the requested actions would result in an output from a particular modality that would interfere with the performance of one or more of the other requested actions. For example, a user can provide a spoken utterance such as "Assistant, play my rounding playlist and tell me the weather". In response to the automated assistant receiving the spoken utterance, the automated assistant can identify a first requested action to play a morning playlist and a second requested action to provide a weather report to the user. Further, the automated assistant can determine that providing the weather report will result in providing an audible weather report for a first time period, and playing the morning playlist will result in providing audible music for a second time period. Additionally, the automated assistant can determine that the second time period is greater than the first time period, and thus, the automated assistant can command performance of the action such that the second requested action is performed before the first requested action. In other words, because providing a weather report to a user may take less time than providing a morning playlist, the automated assistant can prioritize the weather report over the music of the morning playlist. In this way, the prioritization of actions can be based on whether the estimated amount of execution time of one particular action exceeds the estimated amount of execution time of another particular action.
In some implementations, when a user provides a spoken utterance that includes a request to playback a plurality of media items, the order of actions for playback of the media items can be arranged according to an order specified in the spoken utterance. For example, when the user provides a spoken utterance such as "Assistant, play the sound 'For Onnece in My Life' and 'I Was Made to Love Her' (Assistant, play songs 'For Onnece in My Life' and 'I Was Made to Love Her')". In response, the automated assistant can play each of the two songs in the order specified by the user, regardless of whether the order was intentionally specified by the user. However, in some implementations, the user can explicitly specify that one or more actions are to be performed after playback of one or more media items. For example, the user can provide spoken utterances such as "Assistant, play the song 'superstation' and the n-read me to set up the security alarm". In response, because the user specified a particular condition for performance of the action, the automatic assistant is able to receive this spoken utterance and cause playback of the specified media item, and then provide the user with a reminder that their secure alarm is to be set.
In some implementations, when the user provides a spoken utterance that includes a first action to schedule a particular task or event and a second action that depends at least in part on a result of completion of the first action, the automated assistant can delay the second action until completion of the first action. For example, the user can provide spoken utterances such as "Assistant, book me a ticket for a movie friends, and the book a table at a restaurant after the movie" spoken utterances. In response to receiving the spoken utterance, the automated assistant can determine parameters necessary to complete each action. For example, to order a movie ticket, the automated assistant needs to determine the time of play of the movie on the specified date. In addition, in order to book seats at a restaurant, the automated assistant needs a parameter for booking the time to book the restaurant. However, to mitigate the amount of conversation exchanged, and to conserve computing and/or network resources, the automated assistant can determine an estimated appointment time for the restaurant based on: the time of the movie, the length of the movie, the distance of the movie theater from the restaurant, and/or any other factors that may affect when the reservation restaurant should be booked.
In some implementations, the user can provide a spoken utterance that includes a command to the automated assistant to open a particular application and use the application to perform a particular action. However, the user may provide the spoken utterance in a manner that is agnostic with respect to the application that is to perform the action. For example, the user can provide a spoken utterance such as "Assistant open my auction application and tell all me how much money the box of avocado seeds" the Assistant opens my auction application and tells me how much money the box of avocado seeds is. In response, the automated assistant can determine that actions to open the auction application and provide prices for certain items are associated with each other. In some implementations, the automated assistant is able to quantify the correlation between two requests. When the quantified correlation satisfies a particular threshold, the automated assistant can attempt to perform another action requested by the user using the application identified by the user. In this way, not only will the execution order be determined by the automated assistant — assuming that the application will need to be opened before performing the action, but also ambiguities about the application for performing the action can be resolved by the automated assistant.
In some implementations, the reordering of the performance of requested actions can be learned over time. For example, in some implementations, feedback from the user can be used to learn whether the automated assistant accurately or inaccurately sequenced the performance of certain requested actions. For example, when the user provides a spoken utterance that includes a request to perform a plurality of actions and the automated assistant begins performing an action that the user does not intend to override other actions, the user can provide an indication that performance of the action has been incorrectly initiated. The user can interrupt the performance of the action by providing input to the display panel for interrupting the action being performed by the automated assistant and/or provide a spoken utterance, such as "stop," for stopping the ongoing performance of the action. Thereafter, such feedback or input can be relied upon to correct the ordering of the actions when the user requests them again.
In some implementations, historical interaction data characterizing interactions between a user and an automated assistant can be analyzed to determine a particular action that caused a request for additional input from the user. In this manner, the automated assistant is able to identify actions that typically result in further interaction and actions that do not result in further interaction. Thereafter, when the user provides a spoken utterance requesting that certain actions be performed, the automated assistant can prioritize or de-prioritize those particular actions that resulted in additional input being requested from the user.
In some implementations, historical interaction data characterizing interactions between various users and their respective automated assistants can be used to determine a ranking of actions to perform in response to spoken utterances. For example, a user can provide a spoken utterance such as "Assistant, play my rounding play ist and ball me the weather". In response, the automated assistant can access historical interaction data characterizing the previous instance when those particular actions are requested by one or more users, or otherwise be trained based on the historical interaction data characterizing the previous instance. In particular, the automated assistant is able to determine that the user often requests a weather report before requesting music in the morning. Thus, in response to receiving a spoken utterance from the user, the automated assistant can reorder the requested actions such that a weather report is provided before the playlist is played in the morning. In some implementations, the historical interaction data can indicate or otherwise characterize instances in which the user requested two actions in a single spoken utterance and/or at separate times (but optionally within a threshold time of each other), but still request that the two actions be performed more frequently in a particular order. The automated assistant is thus able to identify the order of execution that is most frequently performed for these actions, and re-order the execution of any requested actions accordingly.
In some implementations, when the user provides spoken speech including a request for multiple actions to be performed and one action corresponds to the request for the personal message, the personal message can be delayed until any other action is completed. For example, when a user provides a spoken utterance such as "Talk to you tomorrow, and please set an alarm for 8 a. m. tomorrow (tomorrow talking to you tomorrow, please set an alarm at 8 am tomorrow)," the automatic assistant can receive this spoken utterance and determine that the spoken utterance includes a request for a personal message and a request for an alarm to be configured. In response, the automated assistant can override providing the personal message by setting an alarm clock. Thus, when the automatic assistant responds to the spoken utterance, the automatic assistant can cause a natural language output to be provided, such as "Ok, I set the alarm. In this manner, actions having higher utility can be prioritized over actions that may not otherwise affect the user's schedule, the user's environment, the device accessible to the user, and/or any other features associated with the user.
In some implementations, a method implemented by one or more processors is presented that includes operations such as receiving, from a user, audio data characterizing a spoken utterance, wherein the spoken utterance includes a request for a plurality of actions to be performed via an automatic assistant, and receiving the spoken utterance at an automatic assistant interface of a computing device. The operations can further include: based on audio data characterizing the spoken utterance, each action of a plurality of actions requested by the user to be performed via the automatic assistant is identified, wherein the request for the plurality of actions to be performed is presented in the spoken utterance according to a first action order. The operations can further include: based on identifying each of the plurality of actions, determining an execution characteristic of each of the plurality of actions, wherein a particular execution characteristic of the actions of the plurality of actions affects a temporal aspect of execution of the plurality of actions when the plurality of actions are executed by the one or more computing devices according to the first order of actions, and wherein determining the execution characteristic of each of the plurality of actions comprises accessing data generated at the computing device and/or a separate computing device based on past executions of one or more of the plurality of actions. The operations can further include: determining a second order of actions for performing the plurality of actions based on the particular performance characteristics of the actions of the plurality of actions, wherein the second order of actions, when performed by the one or more computing devices, causes the one or more computing devices to exhibit different temporal aspects of performance of the plurality of actions. The operations can further include: based on determining the second order of actions, the automated assistant is caused to initiate performance of one or more of the plurality of actions in accordance with the second order of actions.
In some implementations, determining the second order of actions includes: output data from a trained neural network model that has been trained using historical interaction data characterizing at least one or more previous interactions between a user and an automated assistant is processed. In some implementations, the historical interaction data further characterizes a plurality of interactions involving other users that have previously interacted with the automated assistant to facilitate causing the automated assistant to perform various sequences of actions. In some implementations, the historical interaction data further characterizes feedback provided by the user to the automated assistant to affect the order of execution of previously requested actions. In some implementations, the particular performance characteristic of the action of the plurality of actions characterizes the action as a dialog initiation action, and a supplemental dialog session will occur between the user and the automated assistant for the user to identify a value of a parameter to be assigned to the action.
In some implementations, the temporal aspect of the performance of the plurality of actions according to the first order of actions includes an estimated performance time of one or more actions of the at least plurality of actions, and the method further includes: determining that the supplemental dialog session is predicted to extend the estimated execution time of the one or more actions when the plurality of actions are performed according to the first action sequence. In some implementations, another action of the plurality of actions includes providing continuous media playback, and the second sequence of actions prioritizes the conversation initiation action over other actions including providing continuous media playback. In some implementations, causing the automated assistant to initialize performance of at least one action of the plurality of actions according to the second sequence of actions includes: generating a natural language output that provides an indication to a user that the at least one of the plurality of actions has been initiated according to the second action sequence.
In other implementations, a method implemented by one or more processors is presented that includes operations such as processing audio data characterizing a spoken utterance from a user requesting an automated assistant to perform a plurality of actions, wherein the plurality of actions are characterized in the spoken utterance by the user according to a first sequence of actions. The method can further comprise: based on processing the audio data, an action classification for each of a plurality of actions requested by the user is determined, wherein a particular action classification for a particular action of the plurality of actions includes a conversation-initiated action performed in accordance with at least one parameter. The method can further include determining whether the user specified a value of at least one parameter in the spoken utterance. The method can further include, when a value of the at least one parameter is not specified in the spoken utterance: generating a second action sequence for the plurality of actions, wherein the second action sequence causes the conversation-initiating action to have a reduced priority relative to another action of the plurality of actions based on a value of the at least one parameter not specified in the spoken utterance.
The method can further include, when at least one parameter is specified in the spoken utterance: generating a third sequence of actions for the plurality of actions, wherein the third sequence of actions causes the conversation-initiating action to have a priority that is unaffected by the user specifying the value of the at least one parameter in the spoken utterance. The method can further include determining the action classification includes, for each action of the plurality of actions, determining whether the action corresponds to continuous media playback, and the method further includes: when the requested action of the plurality of actions comprises a continuous media playback action: the second or third order of actions is generated to prioritize the requested action such that the requested action is performed later in time relative to other actions of the plurality of actions. The method can further comprise: determining whether a user explicitly specifies a time condition for performing at least one of a plurality of actions; and when the user has explicitly specified a time condition for performing at least one of the plurality of actions: the second or third sequence of actions is generated to respect a temporal condition for performing at least one of the plurality of actions. The method can further include wherein, when the particular action is at least one action affected by a temporal condition explicitly requested by the user, the automated assistant is configured to override the second order of actions or the third order of actions according to the temporal condition.
In still other implementations, a method implemented by one or more processors is presented that includes operations such as determining that a user has provided a spoken utterance that includes a request for an automatic assistant to perform a plurality of actions including a first type of action and a second type of action, wherein the user can access the automatic assistant via an automatic assistant interface of a computing device. The method can further comprise: in response to the user providing the spoken utterance, an estimated delay for the first type of action is generated when the second type of action takes precedence over the first type of action during performance of the plurality of actions. The method can further comprise: based on the estimated delay, it is determined whether the estimated delay for the first type of action satisfies a threshold, wherein the first type of action is performed in preference to the second type of action when the estimated delay for the first type of action satisfies the threshold. The method can further comprise: based on whether the estimated delay satisfies the threshold, a preferred execution order for the plurality of actions requested by the user is generated. The method can further include causing the automated assistant to initiate execution of the plurality of actions according to the preferred execution order.
In some implementations, the method can further include determining an action classification for each of a plurality of actions requested by the user, wherein the automated assistant is configured to prioritize at least one particular classification of actions over at least one other classification of actions. In some implementations, the first type of action includes a conversation initiation action and the second type of action includes a media playback action. In some implementations, the media playback action is configured to be performed, at least in part, at a separate computing device, and the method further includes: when the dialog initiating action takes precedence over the media playback action: the conversation-initiating action is initiated at the computing device concurrently with initiating a separate device to the application for performing the media playback action. In some implementations, the method can further include, when the media playback action takes precedence over the conversation initiation action: causing the automated assistant to provide natural language output corresponding to the conversation to facilitate completion of the conversation-initiated action, and when the conversation-initiated action is completed: causing the automated assistant to initiate performance of the media playback action at the computing device or a separate computing device.
In some implementations, the dialog initiation action, when executed, includes initializing a dialog session between the user and the automated assistant for the user to identify values to assign to the parameters to facilitate completion of the dialog initiation action. In some implementations, the media playback action, when executed, includes initiating playback of media accessible via the one or more files, and the estimated delay is based on a total number of file lengths of the one or more files. In some implementations, the media playback action, when executed, includes initiating playback of media accessible via one or more network sources, and the estimated delay is based on time data accessible via the one or more network sources.
Other embodiments may include a non-transitory computer-readable storage medium storing instructions executable by one or more processors (e.g., a Central Processing Unit (CPU), a Graphics Processing Unit (GPU)) and/or a Tensor Processing Unit (TPU) to perform a method such as one or more of the methods described above and/or elsewhere herein. Still other embodiments may include one or more computers and/or one or more robotic systems comprising one or more processors operable to execute stored instructions to perform methods such as one or more of the methods described above and/or elsewhere herein.
It should be understood that all combinations of the foregoing concepts and additional concepts described in greater detail herein are considered a part of the subject matter disclosed herein. For example, all combinations of claimed subject matter appearing at the end of this disclosure are considered part of the subject matter disclosed herein.
Drawings
FIG. 1 illustrates a view of a user invoking an automatic assistant using a spoken utterance in order to cause the automatic assistant to perform a series of actions.
FIG. 2 illustrates a view of a user invoking an automated assistant to perform a number of different actions and having the automated assistant rearrange the order of the actions.
FIG. 3 illustrates a system for modifying an order of execution of a set of requested actions performed via an automated assistant in order to eliminate waste of processing resources and network resources.
Fig. 4 illustrates a method of determining a sequence of actions to be performed by an automated assistant based at least on processing using a trained machine learning model.
FIG. 5 illustrates a method for ranking a sequence of actions according to a classification of the actions.
FIG. 6 is a block diagram of an example computer system.
Detailed Description
FIG. 1 illustrates a view 100 of a user 108 invoking an automatic assistant using a spoken utterance 102 to cause the automatic assistant to perform a series of actions. In particular, the view 100 provides an illustration of how an automated assistant can modify the order of a requested series of actions in order to provide a more efficient utilization of computing resources. For example, a user 108 may be located within an area 106 of their home, which may include a client device 124 and another device 110. Each of the client device 124 and the other devices 110 may provide access to an automated assistant. For example, the client device 124 may provide access to the client automated assistant 126 via one or more assistant interfaces 128. The assistant interface 128 may be one or more devices or a group of devices capable of receiving input from the user 108 and/or providing output to the user 108. For example, the assistant interface 128 may include one or more microphones and one or more audio speakers. Alternatively or additionally, the assistant interface 128 may include one or more touch display panels and/or one or more cameras. Input and output may be generated at the client device 124 and/or the remote computing device 118 (such as the server device 120) for the client automated assistant 126. The server device 120 may include a server automated assistant 122, which may provide support for input and output processing as discussed herein.
When the user 108 provides a spoken utterance 102 that includes a spoken utterance for an automatic assistant (i.e., the client automatic assistant 126 and/or the server automatic assistant 122) to perform a plurality of different actions, the client automatic assistant 126 can receive the spoken utterance 102 and determine an order of the plurality of different actions. For example, the spoken utterance 102 may be "Assistant, play my evening playlist, set an alarm, and tell me weather tomorrow". The spoken utterance 102 can describe the requested actions in a first order that proposes to play a night playlist, then set an alarm, and then provide a weather report. However, in response to receiving the spoken utterance 102, the automated assistant may determine that a different order should be more efficient or appropriate for the user 108 than another execution order.
The client automated assistant 126 and/or the client device 124 may convert the spoken utterance 102 into audio data, which may be transmitted from the client device 124 to the server device 120 via a network 130 (e.g., the internet). The server device 120 and/or the server automated assistant 122 may convert the audio data to text data and then parse the text data to identify an action requested by the user 108, as discussed herein. In some implementations, the identifier of the action may be processed using a trained machine learning model to generate an output indicating a second order of execution of the actions. In other implementations, a classification of the requested action may be identified, and a second execution order of the actions may be generated based on the classification of the action requested by the user.
Additionally or alternatively, context data associated with the user 108, the region 106, and/or any other feature that may be associated with the user 108 may also be processed when determining the second order of execution of the actions. For example, the context data may indicate that user 108 is located within region 106, which includes client device 124 (such as independent speaker device 112) and another device 110 (such as a touch display device). Thus, in response to receiving the spoken utterance 102 and accessing the context data, the automated assistant can determine an order of actions to be performed by the client device 124 and an order of actions to be performed by the other devices 110. For example, the automated assistant may cause the client device 124 to be assigned actions to set an alarm and play a night playlist. Further, the automated assistant may cause another device 110 to be assigned an action that provides a weather report tomorrow. In particular, the automatic assistant may cause a weather report to be displayed on a display panel of another device 110, thereby mitigating disruption of the conversation session that may occur between the user 108 and the automatic assistant for setting an alarm.
In some implementations, the identifier of the action requested in the spoken utterance 102 can be processed using a machine learning model as well as other data associated with the action. For example, the other data may include the type of device that is receiving the spoken utterance 102, the time of day the spoken utterance 102 was received, an identifier of the user 108 that provided the spoken utterance 102, a total or estimated amount of execution time for each of the requested actions, and/or any other data that may be suitable for characterizing the requested actions. Based on the processed one or more inputs, the machine learning model may be used to generate an output characterizing a sequence for performing the requested action. The output generated using the machine learning model may be shared with the client device 124 and/or other devices 110.
In response to receiving the execution order generated by the server device 120 and/or generated in response to receiving the spoken utterance 102, the client device 124 can initiate execution of the action. For example, the client automated assistant 126 may provide the natural language output 104 to facilitate completing the action. As indicated in fig. 1, requests for weather reports may be prioritized over setting an alarm clock, and setting of the alarm clock may be prioritized over playing an evening playlist. Thus, the foregoing sequence of actions may cause the natural language output 104 to initially incorporate the weather report. Specifically, The natural language output 104 may include a weather report, such as "The weather for tomorrow is sunny, with a high of 75 degrees and a low of 66 degrees (sunny weather, 75 degrees high, 66 degrees low) tomorrow)". After completing the first action (e.g., weather report), the customer automated assistant 126 may initiate performance of a second action (e.g., set an alarm for the next day). When performing the second action, the client automated assistant 126 may initiate a dialog session with the user 108 by providing a natural language output, such as "What time would you like to set your alarm for tomorrow. And in response, the user 108 can provide a spoken utterance 102, such as "7:00 a.m., please set at 7:00 am".
In response, the client automated assistant 126 can provide confirmation of the spoken utterance 102 by providing a natural language output 104, such as "Ok". Thereafter, and in response to completing the second action, the client automated assistant 126 may initiate execution of a third action in a second order of actions. In particular, the client automated assistant 126 may initiate playback of an evening playlist, which may be a list of songs set by the user 108 to play preferentially during the evening. Thus, while the user 108 provides a request for actions to be performed according to the first execution order, the automated assistant may rearrange the execution order to ensure that the execution time of each action is minimized. For example, if a night playlist should be executed first, setting an alarm and providing a weather report will have been delayed until the end of the playlist is reached. Further, if the playback of the playlist is to continue until the next day, a request for weather of "tomorrow" will be performed on the next day, thereby changing the target date of the weather report to the next day thereafter. Changing the target date wastes computational resources, especially if the user must provide another spoken utterance to again specify the exact date on which the weather report should be addressed.
FIG. 2 illustrates a view 200 in which a user 208 invokes an automated assistant to perform a number of different actions and cause the automated assistant to rearrange the order of the actions. For example, the user 208 may provide a spoken utterance 202 such as "Assistant, play my show, set an alert, and tell me the weather for tomorrow". A request to "play my show" may correspond to a continuous media playback action that may involve one or more subtasks, such as downloading and caching a portion of the media, and then initiating playback of the downloaded media. Further, the request to "set an alarm" may include one or more subtasks, such as generating a natural language output, processing a subsequent natural language input from the user, and setting an alarm based on the content of the natural language input from the user.
In various implementations, to save computational resources and eliminate latency when performing multiple different actions, the order of actions and corresponding subtasks may be determined using a machine learning model and/or one or more modules capable of determining the order of execution of a particular action. In some implementations, machine learning models can be trained using supervised learning. In some of those implementations, a positive training example is utilized that includes data identifying the automated assistant action in the request and, optionally, one or more context values as training example inputs. The data identifying the automated assistant action may include, for example, an intent of the action and optional parameters. For example, for the act of streaming "show X (program X)" on a smart television, the intent may be "streaming media," and the parameters may include the type of media (e.g., television program type), the duration of the media (e.g., 30 minutes), and/or the target device for receiving the stream. The one or more context values may include, for example, a device via which the request is received, a time of day, a day of week, etc. The training example input may optionally have a fixed dimension dictated by a maximum number of actions for which the machine learning model is trained, and may include a "null" value in the training example that is less than the maximum number of actions. For example, the maximum number of actions may be five, and for a training example input that includes three actions in the request, data identifying each of the three actions may be included in the training example input, and null data may be provided for the remaining two actions.
A positive training example may further include data identifying a particular order of actions identified in the training example input as the training example output. The particular order is a preferred order, which may be determined as a preferred order using various techniques. For example, the particular order of "Action 3, Action1, Action 2" output for a training example of a positive training example may be based on a human flag (e.g., an explicit flag to the "Action 3, Action1, Action 2" order), a past occurrence of the user explicitly specifying a preferred order in a single request (e.g., "Assistant, first Performance Action3, the n Action1, and the N Action 2 (Assistant, performs Action3 first, then Action1, and then Action 2")), and/or a past occurrence of the user specifying a preferred order in a separate request that is temporally proximate to each other (e.g., "Assistant, Performance Action 3", a low thread side minor "Assistant, form Action1 (Assistant, perform Action 3"), then within one minute "Assistant, form 1 (Assistant, perform Action 1", and then within one minute "Assistant 2" (Assistant, perform action 2) "). Additionally, for example, a particular order of "action 3, action1, action 2" of the training example output for a positive training example may be based on past occurrences of actions being performed in a particular order that is more computationally efficient than performing actions in any alternative particular order. For example, a particular order of selecting "action 3, action1, action 2" may be completed based on the average of 33.5 seconds past occurrences of a particular command, while all past occurrences of an alternative particular command take more than 34.0 seconds per average to complete. The average completion time for a particular order is shorter, perhaps due to, for example, a prompt that causes one of the actions to be presented at a time that results in a faster response by the user to the prompt, actions 1 and/or actions 2 that are associated with a lagging and/or relatively slower server and that can be preemptively taken/performed in a particular order (because they are not the first), and so forth.
The training example output may optionally have a fixed dimension that is determined by or based on the maximum number of actions for which the machine learning model is being trained, and may include "null" values that have less than the maximum number of actions in the training example. For example, the maximum number of actions may be five, and for a training example input that includes three actions in the request, data identifying the order of the three actions may be included in the training example output, and null data may be provided for the remaining two actions. As one particular example, the training example output may be a vector of twenty-five values, with each five sequence groups indicating placement of one of the respective actions in a particular order. For example, assuming that an order is provided, in the training example input "action 1, action 2, action 3" and the particular order "action 3, action1, action 2", the following training example outputs may indicate the particular order: [0,1,0,0,0, |0,0,1,0,0, |1,0,0,0,0, | null, … null, | null, … null ]. In the foregoing example, each "|" indicates a break between spoken placements corresponding to each action, the first "1" indicates that "action 1" should be the second (because it is the second placement of five options for "action 1"), the second "1" indicates that "action 2" should be the 3 rd (because it is the third placement of five options for "action 2"), the third "1" indicates that "action 3" should be the 1 st (because it is the first placement of five options for "action 3"), and an "empty" indicates that there are no fourth and fifth actions in the training example input.
The machine learning model may be trained using supervised training examples. After training, the machine learning model may be used to predict a particular order in which to perform the requested plurality of actions based on the identifiers of those actions and, optionally, based on context values. The use of such a machine learning model may provide for rapid resolution of a particular order at runtime and may summarize combinations of actions and/or context values that may not occur in the training data. Thus, such machine learning models are robust to newly added actions and/or newly presented combinations of actions. Moreover, such machine learning models, once trained, consume less space than large mappings between various actions and preferred orders, and are more computationally efficient than searching for such large mappings.
In some implementations, training data for a machine learning model, such as a supervised learning model, can include input data identifying actions that can be performed via an automated assistant. Additionally, the training data for the supervised learning model may include data characterizing a total time for performing each of those actions identified in the input data. Alternatively or additionally, the training data for the supervised learning model may comprise data characterizing the latency time and/or estimated latency time of each of those actions requested by the user. Alternatively or additionally, the training data for the supervised learning model may include data characterizing a total or estimated total amount of time for all actions in the requested set of actions to be performed. In this manner, the functions and/or models generated from supervised learning can effectively demonstrate correlations between requested actions performed via the automated assistant and one or more temporal aspects of each action or all actions.
An action performed via the automated assistant may have one or more performance characteristics, and the performance characteristics of the action may affect one or more temporal aspects of the action and/or actions (e.g., a series of actions performed by a user via the automated assistant). For example, an execution characteristic may characterize one or more operations to be performed during execution of an action. For example, a request for the automatic assistant to perform an action to set an alarm may cause the automatic assistant to perform an operation to initiate a dialog session with the user, and the dialog session may be an execution characteristic of the action to set the alarm. In particular, a dialog session may be initiated to provide the user with an opportunity to specify the time at which the alarm is to sound. Thus, because the action has the performance characteristic of initiating a dialog session, which takes some amount of time to complete, the temporal aspect of the action (e.g., the total time to complete the action) can be affected. Various temporal aspects of the action may be affected, such as, but not limited to, a total estimated time for performing the action, an estimated time for providing input to the user to complete the action, an estimated amount of latency between the request and the initiation of the action, and/or an amount of latency between initiating the action via the request to the third party and receiving a response from the third party.
A temporal aspect of an action and/or series of actions may include one or more time-related properties exhibited by one or more computing devices before, during, and/or after performance of the action and/or series of actions by the one or more computing devices. Additionally or alternatively, a temporal aspect of an action and/or series of actions may be any quantifiable value that characterizes the time of a particular operation and/or action and/or portion of a series of actions. For example, a request for an automated Assistant to perform a series of actions such as "Assistant, turn on my lights and play my podcast" may have a quantifiable amount of latency between actually turning on the lights and playing the podcast, and/or a quantifiable amount of time required to complete the action. Further, the amount of delay and/or any other temporal aspect that may be quantified may be different depending on the order in which the actions are performed. For example, the latency between the user providing the request and the automated assistant turning on the lights may be greater when "play my podcasts" is initialized before the act of initializing "turning on the lights," at least relative to the order of the acts of initializing "turning on the lights" before the act of initializing "play my podcasts" is initialized.
In some implementations, the trained machine learning model can be used to estimate a temporal aspect of the sequence of requested actions. For example, identifiers of the first, second, and third actions may be provided as inputs to the machine learning model, as well as an indication of the order in which the actions are performed. In response to receiving the input, the machine learning model may provide an output indicating one or more temporal aspects of the execution sequence provided for the action. An application or module responsible for providing an efficient order of execution of actions may rearrange the sequence of actions and provide a different order of actions as input to a machine learning model. In response to receiving input corresponding to a different order of actions, the machine learning model may provide another indication of one or more other temporal aspects of the different order of the sequence of actions. This process of estimating temporal aspects of various sequences of actions may be repeated for all or at least a plurality of changes in the order of execution of the actions. Respective outputs from the machine learning model for each respective change in execution order may be compared in order to determine an execution order that provides the most efficient use of computing resources and/or mitigates any negative features in respective temporal aspects. For example, when the output of the machine learning model indicates a total expected amount of latency during execution of an action, a change in the sequence of actions corresponding to the lowest expected amount of latency may be selected and provided to the automated assistant for execution. Alternatively or additionally, when the output of the machine learning model indicates a total estimated execution time of the sequence of actions, a change in the sequence of actions corresponding to a shortest total estimated execution time of the sequence of actions may be selected and provided to the automated assistant for execution.
For example, as illustrated in FIG. 2, the user 208 may provide a spoken utterance 202 such as "Assistant, play my show, set an alarm, and tell me the weather for tomorrow". The spoken utterance 202 can be received by a client device 214, such as a separate speaker device 212. The client device 214 may convert the spoken utterance 202 into audio data and transmit it to a remote computing device 220, such as a server device 224, over a network 228 (e.g., the internet). The server device 224 and/or the server automated assistant 226 can process the audio data to identify any actions required by the user 208 via the spoken utterance 202. An identifier of the action (optionally provided with other data) may be provided as an input to the machine learning model in order to identify the order of execution of the action. The machine learning models may be stored or otherwise accessible at server device 224 and/or client device 214.
In response to receiving the input, the machine learning model may be used to generate an output indicative of one or more execution sequences of the actions. In some implementations, the machine learning model can be used to provide a single execution order that the automated assistant can rely on in order to initiate execution of the requested action. Alternatively, the machine learning model may be used to provide multiple execution orders, and the automated assistant may select an execution order that satisfies certain criteria. For example, the machine learning model may be used to provide output indicative of one or more execution orders of actions and one or more respective attributes for each respective execution order. For example, the execution order provided based on the machine learning model may be provided with a total amount of execution time, a total amount of latency, a total amount of memory usage, a total amount of CPU usage, and/or any other indicator that may be associated with computational efficiency. The automated assistant may then select a particular order that satisfies particular criteria, such as a minimum amount of execution time, a minimum amount of latency, a minimum amount of memory usage, a minimum amount of CPU usage, and/or any other criteria or combination thereof.
When the automated assistant has selected an execution order based on a machine learning model or other operations discussed herein, the automated assistant may initiate the execution of one or more actions. For example, when the automated assistant has selected an execution sequence that first provides a weather report, then sets an alarm, and then plays the program, the automated assistant may initiate the operation of retrieving the weather report via the network 228. When a weather report has been retrieved, the client device 214 may perform operations to provide the natural language output 204 corresponding to the weather report. For example, a natural language output corresponding to a weather report may be "The weather for tomorrow is sunny, with a high of 75 degrees and a low of 66 degrees".
In some implementations, the automated assistant can cause the weather report to be provided by a third party agent, such as a third party weather application or a module accessible via another computing device and/or website. A third party may refer to an entity that provides an application, service, device, and/or any other product and may be different from an entity that provides an automated assistant and/or operating system for client device 214. After the third party agent has provided the weather report to the automated assistant, the third party agent may provide an indication that one or more tasks assigned to the third party agent have been completed. In response to receiving the indication from the third party agent, the automated assistant may initiate execution of the actions in an order of execution of the actions selected by the automated assistant.
In some implementations, the selected order of execution may correspond to or indicate that the automated assistant should complete the requested actions rather than initializing the order of the requested actions. For example, although the order of actions requested by user 208 in FIG. 2 is completed according to a first order, the subtasks associated with each action may be completed in a different second order. For example, the machine learning model may indicate an order of completion, and may also provide an output indicating an order of subtasks to be performed. Thus, in some implementations, the automated assistant may initialize a first action prioritized in order of execution of the actions (e.g., providing a weather report) and, concurrently or shortly thereafter, initialize performance of a subtask of a third action (e.g., downloading and/or caching program data) in order of execution of the actions. For example, the order of execution of these actions may identify the weather report as the highest priority action to complete, but a sub-task or operation (such as downloading a program) may also be designated as a higher priority than most other sub-tasks or operations (e.g., providing a natural language output that is requested when an alarm should be set, and confirming the setting of the alarm), even though the program playback action is last relative to the priority for completing the action. In other words, the automated assistant may initialize certain subtasks or operations that may mitigate the overall latency and/or total estimated time for completion of a series of requested operations. Additionally or alternatively, the automated assistant can delegate one or more subtasks or operations to one or more devices that are not the target device for the spoken utterance 202. For example, the subtask of downloading the program may be performed by another device 210 as a background action 230 while the automated assistant retrieves the weather report over the network 228 and/or provides the weather report via the client device 214.
Fig. 3 illustrates a system 300 for modifying an order of execution of a requested set of actions performed via an automated assistant 304 in order to eliminate waste of processing resources and network resources. Automated assistant 304 may operate as part of an assistant application provided on one or more computing devices, such as computing device 318 and/or server device 302. A user may interact with the automated assistant 304 via an assistant interface, which may be a microphone, a camera, a touch screen display, a user interface, and/or any other device capable of providing an interface between a user and an application. For example, a user may initialize the automated assistant 304 by providing verbal, textual, or graphical input to the assistant interface to cause the automated assistant 304 to perform functions (e.g., provide data, control peripherals, access agents, generate input and/or output, etc.). Computing device 318 may include a display device, which may be a display panel including a touch interface for receiving touch inputs and/or gestures to allow a user to control applications of computing device 318 via the touch interface. In some implementations, the computing device 318 may lack a display device to provide audible user interface output, rather than graphical user interface output. Further, the computing device 318 may provide a user interface, such as a microphone, for receiving spoken natural language input from the user. In some implementations, the computing device 318 may include a touch interface and may not have a camera, but may optionally include one or more other sensors.
In various implementations, all or less than all aspects of automated assistant 304 may be implemented on computing device 318. In some of those implementations, aspects of the automatic assistant 304 are implemented via the client automatic assistant 322 of the computing device 318 and interfaced with the server device 302 that implements other aspects of the automatic assistant 304. The server device 302 can optionally serve multiple users and their associated assistant applications via multiple threads. In implementations in which all or less than all aspects of the automated assistant 304 are implemented via the client automated assistant 322 at the computing device 318, the client automated assistant 322 may be an application separate from the operating system of the computing device 318 (e.g., installed "on top" of the operating system) -or may be implemented directly alternatively by the operating system of the computing device 318 (e.g., an application considered to be the operating system, but integrated with the operating system).
In some implementations, the automated assistant 304 and/or the client automated assistant 322 can include an input processing engine 306 that can employ a number of different modules to process input and/or output for the computing device 318 and/or the server. For example, the input processing engine 306 can include a speech processing module 308, and the speech processing module 308 can process audio data received at the assistant interface 320 to identify text embodied in the audio data. The audio data may be transmitted from, for example, the computing device 318 to the server device 302 in order to conserve computing resources at the computing device 318.
The process for converting audio data to text may include a speech recognition algorithm that may employ a neural network, a word2vec algorithm, and/or a statistical model to identify sets of audio data corresponding to words or phrases. Text converted from the audio data may be parsed by data parsing module 310 and may be provided to the automated assistant as text data, which may be used to generate and/or identify command phrases from the user. In some implementations, the output data provided by the data parsing module 310 may be provided to the parameter module 312 to determine whether the user provides input corresponding to a particular action and/or routine that can be performed by the automatic assistant 304 and/or an application or agent that may be accessed by the automatic assistant 304. For example, assistant data 316 can be stored as client data 332 on server device 302 and/or computing device 318, and can include data defining one or more actions that can be performed by automated assistant 304 and/or client automated assistant 322, as well as parameters necessary to perform the actions. The user may specify one or more values to assign to one or more parameters of actions performed by the automatic assistant 304 and/or by third party agents accessible via the automatic assistant 304 under the direction of the automatic assistant 304. The third party agent may be provided by another party different from the other party that has provided automated assistant 304.
In some implementations, the system 300 can include the server device 302 and/or the computing device 318 that include one or more features for reordering a sequence of requested actions by a user in a spoken utterance. A spoken utterance, such as a user requesting a series of actions to be performed by the automatic assistant, may be received at the automatic assistant interface 320 and converted into audio data. The audio data may be processed by the input processing engine 306, which may be provided at the server device 302 and/or the computing device 318. The audio data may be converted to text and/or otherwise processed to identify each action requested by the user. In some implementations, audio data based on the spoken utterance can be provided as input to a neural network model, which can provide an output indicative of the action requested by the user and/or the sequence of actions.
When an action has been identified, data characterizing the action may be shared with or otherwise available at the computing device 318 and may be processed by the action classification engine 324. The action classification engine may receive data characterizing an action and determine a classification of the action based on the data. For example, a request to play a television series or song may correspond to a category referred to as a continuous playback action. Alternatively or additionally, a request for information from the internet, such as a weather report or economic news, may correspond to a category referred to as a request for information action. Additionally, or alternatively, the request to change the settings of a particular device may correspond to a classification referred to as a device settings classification. In some embodiments, one or more different actions may be classified according to one or more different classifications, respectively, and are not limited to those disclosed herein.
In some implementations, the action classification engine 324 can determine a classification for each action requested by the user via the spoken utterance. In some implementations, the action classification may be based on one or more determined performance characteristics of the particular action. For example, actions that require further dialog between the user and the automated assistant may be classified as dialog initiating actions for use with at least a dialog session as an execution characteristic. Additionally or alternatively, the action requiring the determination and modification of the settings of the local device may be classified as a request to change device settings based at least on the action having an execution characteristic that causes the automated assistant to communicate a request to modify device settings to the local device. Each classification may be communicated to an action sequence engine 326, which action sequence engine 326 may receive data characterizing the classification of the action requested by the user and generate a sequence of actions based at least on the data. For example, when a user requests a first action corresponding to continuous playback of music and a second action corresponding to a request for information from the internet, the action classification engine 324 may determine that the request from the user includes a continuous playback action and a request for action on the information. These classifications may be provided to an action sequence engine 326, which action sequence engine 326 may generate a sequence that prioritizes a second action that provides information from the internet over an action that provides continuous playback of music.
In some implementations, the action order engine 326 can generate the order of the requested action sets based on the action order model 332. The action sequence model 332 may be one or more neural network models trained based on historical user interaction data 336 and/or historical community interaction data 328 with prior permissions from respective users. For example, the historical user interaction data 336 may include data characterizing interactions between the user and the automated assistant 304. Such interactions may include those in which a user provides a request for a number of actions to be performed and then provides feedback to automated assistant 304, and/or does not provide feedback to automated assistant 304 subsequently. The feedback may include a subsequent spoken utterance in which the user commands automated assistant 304 to rearrange the order of performance of the plurality of actions. For example, the user may have previously provided a spoken utterance, such as "Assistant, plant organism noise and set an alarm for tomorrow". In response, the automatic assistant 304 may have initiated playback of the ambient noise prior to initiating the dialog for setting the alarm clock, which may not be preferred by the user. Thus, the user may have provided feedback to the automatic assistant 304 in the form of spoken utterances such as "No, set the alarm first and the n play the ambient noise" (No, set the alarm first and then play the ambient noise) ". Such feedback may be used to train the order of action model 332 so that subsequent similar requests may be modified according to user preferences learned under the user's permission over time.
The training may be based on client data 334, which may characterize the operation of computing device 318, the context of computing device 318, and/or any other information that may be associated with computing device 318. The customer data 334 may characterize the state of one or more applications 340 of the computing device 318. In this manner, the action sequence model 332 may learn the preferences of the user based on feedback from the user and any other information that may have been relevant when the user provided the feedback. In some implementations, the order of actions generated by the action order model 332 may be based on the context of the computing device 318. For example, GPS data provided by the computing device 318 may indicate that the user has just arrived home, while other data may indicate that the user subsequently provided a spoken utterance, such as "assistance, play my lounge playlist and set my alarm". In response, instead of initiating a dialog session between the user and the automated assistant to set a wake alarm, the automated assistant can initiate playback of the lobby music playlist and turn on the home security alarm in the background at the user's home. This resulting sequence of actions can be different if it is determined that the user has at least arrived home for at least a threshold amount of time (as can be determined by the action sequence model 332), and can be based on a time window in which the user typically lies in bed to sleep. For example, if it is determined that the user is at home and in their room within the time window that the user normally lies in bed to sleep, and the user provides the spoken utterance, "Assistant, play my loud speaker and set my alarm," the automated Assistant may initiate a dialog box that sets a wake alarm, and then, after completing the settings to wake the alarm, initiate playback of the restroom music playlist.
In some implementations, the one or more action sequence models 332 may be trained from historical user interaction data 336 (which may correspond to a user of the computing device 318) and from historical community interaction data 328 (which may correspond to one or more other users of the automated assistant). The action sequence model 332 may be trained and configured such that the action sequence model 332 may receive as input a classification of actions requested by a user according to a first sequence and provide as output a second sequence of actions requested by the user. In some implementations, the historical community interaction data 328 may characterize interactions between users and their respective automated assistants, and may specifically identify those interactions in which the user requests different classifications of actions to be performed. Alternatively or additionally, the historical community interaction data 328 may also characterize the conditional statements that the user makes when requesting to perform certain action classifications. In this way, the action order model 332 may identify trends in user preferences for the order of execution of a particular category of actions, at least as compared to other orders of execution for the particular category of actions.
For example, training of the action sequence model 332 may reflect the preferences of most users to have a request for an information action completed before a continuous playback action. The preferences of most users, who also include the conditional statement "and then," may be identified by processing the historical community interaction data 328 and determining a majority of the time that the user requests a request for information manipulation and continuous playback action in the same spoken utterance. For example, most users, or at least a number of users, may have provided spoken utterances, such as "Assistant, all me the weather for today and the n play my mourning piaylist. "because multiple users provide these two different action classifications to a conditional statement, the conditional statement may affect the training of the action order model 332. As a result, when the user of the computing device 318 provides similar spoken utterances requesting each of the two different categories of actions to be performed, the automated assistant 304 may use the action order model 332 to ensure that the order of performance of the actions is performed according to the user's preferences corresponding to the historical community interaction data 328. However, if the user provides a conditional statement in the spoken utterance, the conditional statement may employ rules for ordering the execution of actions over the order of actions determined from the action order model 332.
In some implementations, the order of actions may be determined based on the number of available devices accessible to the automated assistant 304. For example, the action sequence model 332 may receive as input an identifier of a computing device that: connected to the local network by the computing device 318, accessible via the client automated assistant 322, and/or otherwise associated with the computing device 318. Because some actions requested by the user may be performed on one or more different computer devices, automated assistant 304 may delegate particular actions to particular computer devices with particular actions based on which computing devices are available. For example, a user may have a smart thermostat, a smart light bulb, and an automatic assistant device in their home. Further, the user may provide a spoken utterance that includes a request to perform a plurality of actions, such as "Assistant, turn up the thermal, turn down the lights, play my night time playlist, and order a food delivery".
Audio data corresponding to the spoken utterance may be processed to determine a classification of the above-described action that the user has requested. The identifiers for the categories and the identifiers of the intelligent thermostat, the intelligent light bulb, and the assistant device may be provided as input to the action sequence model 332. Action sequence model 332 may provide as output the determined sequence of actions to be followed by automated assistant 304 and/or one or more sequences of actions to be followed by each particular device in the household. For example, the automated assistant 304 may delegate the change in light to a smart light bulb, delegate the modification of the house temperature to a thermostat, and provide the execution sequence of the remaining actions to the automated assistant device. In particular, the sequence of actions of the automated assistant device may prioritize order takeaway over playing a night play list.
The order of actions of the automated assistant devices may be based on historical user interaction data 336, historical community interaction data 328, and/or categorical preferences of the automated assistant 304. For example, automated assistant 304 may prioritize a conversation-initiating action (such as ordering take-away) over a continuous playback action (such as playing a night-time playlist). As a result, while the automated assistant 304 has the order of actions set for the automated assistant device, other requested actions will be performed simultaneously on other respective devices. In other words, the first action of the sequence of actions set for the automated assistant device is performed simultaneously with the action set for the smart light bulb and the action set for the thermostat. In this manner, automated assistant 304 and/or action command model 332 may generate a sequence of actions for multiple different devices to perform simultaneously.
Fig. 4 illustrates a method 400 for determining a sequence of actions performed by an automated assistant based at least on a trained machine learning model. Method 400 may be performed by one or more computing devices, applications, and/or any other apparatus or module capable of interacting with an automated assistant. The method 400 may include an operation 402 of determining whether a spoken utterance has been detected. The spoken utterance may be provided by a user to an automated assistant interface of a computing device, and the spoken utterance may be converted into data that is transmittable between the devices. For example, when the automated assistant interface is used as a microphone, the spoken utterance may be converted to audio data, which is then transmitted to a server device for further processing. In some implementations, determining whether to provide the spoken utterance can include determining whether the user intends to invoke an automated assistant to provide a response or otherwise perform a particular action. Operation 402 may be determined periodically, and when a spoken utterance is detected, the method 400 may proceed to operation 404.
Operation 404 may include identifying an action requested by the user via the spoken utterance. The user can cause the spoken utterance to embody one or more requests for one or more actions to be performed by the automated assistant. When processing data corresponding to a spoken utterance, the data may be converted to text, parsed, and further processed to determine any actions that the user requests to perform. In some implementations, data generated based on the spoken utterance can be processed using a machine learning model to generate output data and then identify one or more actions requested by the user via the spoken utterance. The machine learning model may be trained using data that has been generated since a user interacted with the automated assistant and/or data generated by a provider of the automated assistant.
The method 400 may further include an optional operation 406 of accessing contextual data associated with the spoken utterance. The context data may be based on operation of one or more computing devices associated with the user, an automated assistant, and/or any other device accessible to the user. For example, the context data may identify an operational state of one or more devices accessible to the automated assistant. Alternatively or additionally, the context data may characterize environmental characteristics associated with the user's environment, such as when and where the user provided the spoken utterance. Alternatively or additionally, the context data may characterize a schedule of the user, occupancy of a location where the user is located, a time of day during which the user provided the spoken utterance, one or more previous spoken utterances provided by the user or another person, scheduled actions performed by one or more devices when the user provided the spoken utterance, and/or any other information that may be associated with the context of the user.
The method 400 may further include an operation 408 of determining whether an explicit order of actions is requested in the spoken utterance. A user providing an explicit statement indicating the order of actions to be performed may request an explicit order of operations. For example, within a spoken utterance provided by a user, the user may request that a first action be performed, and then perform a second action. The conditional statement "and then" may be interpreted as an explicit request for contingency for performance of a second action based on completion of the first action and/or at least initialization of the first action. If the user has not provided an explicit order of the requested actions, the method 400 may proceed to operation 410.
The method 400 may further include an operation 412 of determining an order of execution of the actions based on the output generated using the trained machine learning model. The execution order determined using the trained machine learning model may be generated to reduce latency and thus eliminate waste of computing resources such as memory and processing bandwidth. Further, by reducing latency between receiving the spoken utterance and performing the action, network bandwidth may be saved as actions that depend on network responsiveness may be prioritized according to availability of network resources. For example, based on training of the machine learning model, the machine learning model may be trained to rank certain actions that exhibit a maximum network latency below those actions that have been performed in the past with the lowest amount of network latency. Such training of the machine learning model may be based on data characterizing interactions between a user providing a spoken utterance and the automated assistant. Additionally or alternatively, the training of the machine learning model may be based on data characterizing interactions between one or more other users and their own respective automated assistants. In this way, the trained machine learning model may indicate whether certain actions that rely on third party providers or third party agents have resulted in more latency or less latency when other users have requested such actions to be performed.
In some implementations, the method 400 may include an optional operation 414 of training the machine learning model according to or otherwise based on an explicit order provided by the user and/or optionally any contextual data used to determine the order. In other words, when the user has provided a definite sequence of actions, the definite sequence can be characterized as data used in further training the machine learning model.
The method 400 may also include an operation 416 of causing actions to be performed according to the determined order. For example, when the first action indicated in the sequence is a request for information, the automated assistant may initiate a web query. Alternatively, when the first action in the determined order includes a change to a device setting, the automated assistant may cause a third party device to adjust the operation setting. In some implementations, the method 400 may proceed from operation 408 and/or operation 414 to operation 416 when the user explicitly provides the order. In this manner, the automated assistant can rely on the explicit order when the user has provided the explicit order, and can rely on the separately determined order when the user has not provided the explicit order of the requested actions based on the output generated using the machine learning model.
In some implementations, the method 400 includes an operation 418 of determining whether feedback has been received regarding the order of execution of the actions. The feedback may include one or more inputs provided by the user before, during, and/or after performing the action. For example, after the automated Assistant has completed performing all actions requested by the user according to a particular sequence, the user may provide a spoken utterance, such as "Assistant, next time set my alarm first". The aforementioned spoken utterance may be processed by an automated assistant, determined to be fed back and embodied in training data, which may be used to train a machine learning model. In some implementations, the user can provide feedback during the performance of the action. For example, a user may provide a spoken utterance, such as "Assistant, wait to play my music I had finished ordering food" and so on until I finishes ordering. "in response, the automated assistant can modify any ongoing and/or pending actions in accordance with the feedback provided by the user. Further, the feedback is characterized by data that may be provided as training data to further train the machine learning model in order to further adapt the machine learning model to provide a more appropriate order for the requested sequence of actions. When the user provides feedback, the method 400 may proceed to operation 420 where the machine learning model is trained based on the feedback. When no feedback is received from the user, at least with respect to performance of the action, the method 400 may return to operation 402 to detect whether the automated assistant has received any other spoken utterance.
Fig. 5 illustrates a method 500 for arranging a requested sequence of actions for execution by an automated assistant according to a classification of actions. Method 500 may be performed by one or more computing devices, applications, and/or any other apparatus or module capable of interacting with an automated assistant. The method 500 may include an operation 502 of determining whether a spoken utterance has been detected. The spoken utterance may be detected at a computing device that provides access to the automatic assistant and/or a server device in communication with the computing device from which the automatic assistant is accessible. Spoken utterances may be provided by the user to an automated Assistant interface (such as a microphone) and may include natural languages such as "Assistant, order food delivery, turn down the lights, and tell I what TV is today".
In response to determining that the spoken utterance has been detected, the method 500 may proceed to operation 504 of identifying one or more actions requested by the user via the spoken utterance. In particular, audio data embodying and/or otherwise characterizing the spoken utterance may be processed to determine natural language content of the spoken utterance. The natural language content may be parsed and further processed to identify one or more actions requested by the user via the spoken utterance. For example, when processing audio data corresponding to the aforementioned spoken utterance, a takeaway action, a light setting action, and an action requesting a television schedule may be identified.
In some implementations, the method 500 can include an optional operation 506 of determining whether a conditional statement is included in the spoken utterance. A conditional statement may be a phrase or word provided by a user indicating that one or more requested actions are conditional on other circumstances occurring. For example, a user may provide a phrase describing temporal contingency of an action, such as contingency upon completion of another action. Alternatively or in addition, the user may provide words or phrases that describe conditions that may be satisfied based on the context of the user, the state of one or more devices, application data associated with the user, and/or any other data that may be relevant to the action being performed by the automated assistant.
In some implementations, the method 500 can proceed to operation 508 when the spoken utterance includes or does not include one or more conditional statements. Operation 508 may include determining a classification for each of the identified actions. The classification may be determined based on an output of a text processing module, which may provide text data reflecting a natural language context of the spoken utterance. Alternatively or additionally, audio data or other data based on the spoken utterance may be provided to the trained machine learning model, and the output of the trained machine learning model may be used to determine each classification of each action requested via the spoken utterance. When a classification has been identified, the method 500 may proceed to operation 510.
The method 500 further includes an operation 514 of causing actions to be performed according to the determined order. The determined order may be explicitly provided by the user, at least when it is determined from operation 512 that the user has provided a conditional statement. Alternatively or additionally, the order of execution of the identified actions may be based on the output of one or more machine learning models, contextual data characterizing the context of the user, and/or any other data that may be based on action priorities in a series of actions.
In some implementations, the method 500 may optionally include an operation 516 of determining whether feedback regarding the execution order has been received. As discussed herein, the feedback may be based on another spoken utterance provided by the user before, during, and/or after performing the identified action. The feedback may, for example, characterize a user's preference for a particular action category to have a higher priority than another action category. When it is determined that feedback has been received before, during, and/or after performing the identified action, the method 500 may proceed to operation 518 of training the machine learning model based on the feedback. In other words, feedback may be received from a user and processed to generate training data that may be provided to the machine learning model to update the machine learning model according to the preferences of one or more users. In this way, computing resources may be saved because the user does not have to repeatedly provide the same feedback. Rather, preferences relating to execution order may be learned over time such that feedback from the user is required to provide fewer timeouts. Certain computing resources, such as network bandwidth and memory, may be saved when the computing device processes spoken utterances less frequently. Regardless of whether the user has provided feedback, the method 500 may return to operation 502 where it is determined whether a spoken utterance has been detected. In this manner, the method 500 allows for continuous learning of user preferences while also eliminating wasteful processing and unnecessary network transmissions that may occur when a user must repeatedly correct requests there or otherwise provide feedback to the automated assistant regarding their previous requests.
FIG. 6 is a block diagram of an example computer system 610. Computer system 610 typically includes at least one processor 614 which communicates with a number of peripheral devices via a bus subsystem 612. These peripheral devices may include a storage subsystem 624, including, for example, memory 625 and a file storage subsystem 626, user interface output devices 620, user interface input devices 622, and network interface subsystem 616. The input and output devices allow a user to interact with computer system 610. Network interface subsystem 616 provides an interface to external networks and couples to corresponding interface devices in other computer systems.
The user interface input devices 622 may include a keyboard, a pointing device such as a mouse, trackball, touchpad, or tablet, a scanner, a touch screen incorporated into a display, an audio input device such as a speech recognition system, a microphone, and/or other types of input devices. In general, use of the term "input device" is intended to include all possible types of devices and ways to input information to computer system 610 or onto a communication network.
User interface output device 620 may include a display subsystem, a printer, a facsimile machine, or a non-visual display such as an audio output device. The display subsystem may include a Cathode Ray Tube (CRT), a flat panel device such as a Liquid Crystal Display (LCD), a projection device, or some other mechanism for creating a visible image. The display subsystem may also provide non-visual displays, for example, via an audio output device. In general, use of the term "output device" is intended to include all possible types of devices and ways to output information from computer system 610 to a user or to another machine or computer system.
These software modules are typically executed by processor 614 alone or in combination with other processors. Memory 625 used in storage subsystem 624 can include a number of memories including a main Random Access Memory (RAM)630 for storing instructions and data during program execution and a Read Only Memory (ROM)632 in which fixed instructions are stored. File storage subsystem 626 is capable of providing persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive along with associated removable media, a CD-ROM drive, an optical drive, or removable media cartridges. Modules implementing the functionality of certain implementations may be stored by file storage subsystem 626 in storage subsystem 624, or in other machines accessible by processor 614.
Where the systems described herein collect personal information about a user (or are often referred to herein as "participants") or may utilize personal information, the user may be provided with an opportunity to control whether programs or features collect user information (e.g., information about the user's social network, social actions or activities, profession, the user's preferences, or the user's current geographic location) or whether and/or how to receive content from a content server that may be more relevant to the user. In addition, certain data may be processed in one or more ways before it is stored or used, so that personally identifiable information is removed. For example, the identity of the user may be processed such that personally identifiable information cannot be determined for the user, or the geographic location of the user may be generalized (such as to a city, zip code, or state level) if geographic location information is obtained such that a particular geographic location of the user cannot be determined. Thus, the user may control how information is collected and/or used with respect to the user.
While several implementations have been described and illustrated herein, one or more of various other means and/or structures for performing the function and/or obtaining the result and/or the advantages described herein may be utilized and each of such variations and/or modifications is deemed to be within the scope of the implementations described herein. More generally, all parameters, dimensions, materials, and configurations described herein are intended to be exemplary, and the actual parameters, dimensions, materials, and/or configurations will depend upon the specific application or applications for which these teachings are used. Those skilled in the art will recognize, or be able to ascertain using no more than routine experimentation, many equivalents to the specific implementations described herein. It is, therefore, to be understood that the foregoing implementations are presented by way of example only and that, within the scope of the appended claims and equivalents thereto, implementations may be practiced otherwise than as specifically described and claimed. Implementations of the present disclosure are directed to each individual feature, system, article, material, kit, and/or method described herein. In addition, any combination of two or more such features, systems, articles, materials, kits, and/or methods, where such features, systems, articles, materials, kits, and/or methods are not mutually inconsistent, is included within the scope of the present disclosure.
Claims (24)
1. A method implemented by one or more processors, the method comprising:
receiving, from a user, audio data characterizing a spoken utterance, wherein the spoken utterance includes a request for a plurality of actions to be performed via an automatic assistant, and the spoken utterance is received at an automatic assistant interface of a computing device;
based on the audio data characterizing the spoken utterance, identifying each action of the plurality of actions requested by the user to perform via the automatic assistant, wherein requests for the plurality of actions to perform are presented in the spoken utterance according to a first action order;
based on identifying each of the plurality of actions, determining an execution characteristic for each of the plurality of actions,
wherein the particular performance characteristic of an action of the plurality of actions affects a temporal aspect of performance of the plurality of actions when the plurality of actions are performed by one or more computing devices according to the first order of actions, and
wherein determining the performance characteristics of each of the plurality of actions comprises accessing data generated at the computing device and/or a separate computing device based on past performance of one or more of the plurality of actions;
determining a second order of actions for performing the plurality of actions based on the particular performance characteristics of the actions of the plurality of actions, wherein the second order of actions, when performed by the one or more computing devices, causes the one or more computing devices to demonstrate different temporal aspects of performance of the plurality of actions; and
based on determining the second order of actions, causing the automated assistant to initialize performance of one or more actions of the plurality of actions in accordance with the second order of actions.
2. The method of claim 1, wherein determining a second order of actions comprises:
processing output data from a trained neural network model that has been trained using historical interaction data characterizing at least one or more previous interactions between the user and the automated assistant.
3. The method of claim 2, wherein the historical interaction data further characterizes a plurality of interactions involving other users that have previously interacted with the automated assistant to facilitate causing the automated assistant to perform various sequences of actions.
4. The method of claim 2, wherein the historical interaction data further characterizes feedback provided by the user to the automated assistant in order to influence an order of execution of previously requested actions.
5. The method of any of the preceding claims, wherein a particular performance characteristic of an action of the plurality of actions characterizes the action as a dialog initiating action, and wherein a supplemental dialog session is to occur between the user and the automated assistant for the user to identify values of parameters to be assigned to the action.
6. The method of claim 5, wherein the temporal aspect of the performance of the plurality of actions according to the first order of actions comprises at least an estimated performance time of one or more actions of the plurality of actions, and wherein the method further comprises:
determining that the supplemental dialog session is predicted to extend the estimated execution time of the one or more actions when the plurality of actions are performed according to the first action sequence.
7. The method according to any one of claims 5 and 6,
wherein another of the plurality of actions comprises providing continuous media playback, and
wherein the second order of actions prioritizes the conversation initiation action over other actions including providing the continuous media playback.
8. The method of claim 5, wherein causing the automated assistant to initialize execution of at least one action of the plurality of actions in accordance with the second sequence of actions comprises:
generating a natural language output that provides an indication to the user that the at least one of the plurality of actions has been initialized according to the second action order.
9. A method implemented by one or more processors, the method comprising:
processing audio data characterizing a spoken utterance from a user requesting an automated assistant to perform a plurality of actions, wherein the plurality of actions are characterized by the user in the spoken utterance according to a first sequence of actions;
based on processing the audio data, determining an action classification for each of the plurality of actions requested by the user, wherein a particular action classification for a particular action of the plurality of actions includes a dialog-initiating action performed in accordance with at least one parameter;
determining whether the user specified a value of the at least one parameter in the spoken utterance; and
when a value of the at least one parameter is not specified in the spoken utterance:
generating a second sequence of actions for the plurality of actions, wherein the second sequence of actions causes the dialog-initiating action to have a reduced priority relative to another action of the plurality of actions based on the value for which at least one parameter is not specified in the spoken utterance.
10. The method of claim 9, further comprising:
when the at least one parameter is specified in the spoken utterance:
generating a third sequence of actions for the plurality of actions, wherein the third sequence of actions causes the conversation-initiated action to have a priority that is unaffected by the user's specification of the value of the at least one parameter in the spoken utterance.
11. The method of claim 10, wherein determining the action classification comprises determining, for each action of the plurality of actions, whether the action corresponds to continuous media playback, and the method further comprises:
when the requested action of the plurality of actions comprises a continuous media playback action:
generating the second or third order of actions to prioritize the requested action such that the requested action is performed later in time relative to other actions of the plurality of actions.
12. The method according to any one of claims 9-11, further comprising:
determining whether the user explicitly specifies a temporal condition for performing at least one of the plurality of actions; and
when the user has explicitly specified a temporal condition for performing at least one of the plurality of actions:
generating the second or third order of actions to comply with the temporal condition for performing at least one action of the plurality of actions.
13. The method of claim 12, wherein the automated assistant is configured to override the second order of actions or the third order of actions according to the temporal condition when the particular action is the at least one action explicitly requested by the user that is affected by the temporal condition.
14. A method implemented by one or more processors, the method comprising:
determining that a user has provided a spoken utterance, the spoken utterance including a request for an automatic assistant to perform a plurality of actions including a first type of action and a second type of action, wherein the automatic assistant is accessible to the user via an automatic assistant interface of a computing device;
in response to the user providing the spoken utterance,
generating an estimated delay for the first type of action when the second type of action takes precedence over the first type of action during execution of the plurality of actions;
determining, based on the estimated delay, whether the estimated delay for the first type of action satisfies a threshold,
wherein the first type of action is prioritized for execution over the second type of action when the estimated delay for the first type of action satisfies the threshold;
generating a preferred execution order for the plurality of actions requested by the user based on whether the estimated delay satisfies the threshold; and
causing the automated assistant to initialize execution of the plurality of actions according to the preferred execution order.
15. The method of claim 14, further comprising:
determining an action classification for each of the plurality of actions requested by the user, wherein the automated assistant is configured to prioritize at least one particular classification of actions over at least one other classification of actions.
16. The method of any of claims 14 and 15, wherein the first type of action comprises a dialog initiating action and the second type of action comprises a media playback action.
17. The method of claim 16, wherein the media playback action is configured to be performed at least in part at a separate computing device, and further comprising:
when the conversation initiation action takes precedence over the media playback action:
causing the conversation-initiating action to be initiated at the computing device concurrently with causing the separate device to initialize an application for performing the media playback action.
18. The method of claim 17, further comprising:
when the media playback action takes precedence over the conversation-initiating action:
cause the automated assistant to provide natural language output corresponding to a conversation to facilitate completion of the conversation-initiated action, and
when the dialog initiating action is complete:
causing the automated assistant to initiate performance of the media playback action at the computing device or the separate computing device.
19. The method of any of claims 16-18, wherein the conversation-initiating action, when executed, comprises initializing a conversation session between the user and the automated assistant for the user to identify values to assign to parameters to facilitate completion of the conversation-initiating action.
20. The method of any of claims 16-18, wherein the media playback action, when executed, comprises initiating playback of media accessible via one or more files, and the estimated delay is based on a total number of file lengths of the one or more files.
21. The method of any of claims 16-18, wherein the media playback action, when executed, comprises initiating playback of media accessible via one or more network sources, and the estimated delay is based on time data accessible via the one or more network sources.
22. A computer program product comprising instructions which, when executed by one or more processors, cause the one or more processors to carry out the method according to any one of the preceding claims.
23. A computer-readable storage medium comprising instructions that, when executed by one or more processors, cause the one or more processors to perform the method of any one of claims 1-21.
24. A system comprising one or more processors configured to perform the method of any one of claims 1-21.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201862770516P | 2018-11-21 | 2018-11-21 | |
US62/770,516 | 2018-11-21 | ||
PCT/US2019/017039 WO2020106315A1 (en) | 2018-11-21 | 2019-02-07 | Orchestrating execution of a series of actions requested to be performed via an automated assistant |
Publications (2)
Publication Number | Publication Date |
---|---|
CN112352276A true CN112352276A (en) | 2021-02-09 |
CN112352276B CN112352276B (en) | 2024-04-09 |
Family
ID=65494654
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201980043375.7A Active CN112352276B (en) | 2018-11-21 | 2019-02-07 | Coordinated execution of a series of requested actions to be performed via an automated assistant |
Country Status (6)
Country | Link |
---|---|
US (3) | US11031007B2 (en) |
EP (2) | EP3944233A3 (en) |
JP (1) | JP7195343B2 (en) |
KR (1) | KR102477072B1 (en) |
CN (1) | CN112352276B (en) |
WO (1) | WO2020106315A1 (en) |
Families Citing this family (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2020106315A1 (en) | 2018-11-21 | 2020-05-28 | Google Llc | Orchestrating execution of a series of actions requested to be performed via an automated assistant |
US11238868B2 (en) * | 2019-05-06 | 2022-02-01 | Google Llc | Initializing non-assistant background actions, via an automated assistant, while accessing a non-assistant application |
US11568246B2 (en) * | 2019-05-09 | 2023-01-31 | Sri International | Synthetic training examples from advice for training autonomous agents |
US11200898B2 (en) * | 2019-05-31 | 2021-12-14 | Google Llc | Dynamically assigning multi-modality circumstantial data to assistant action requests for correlating with subsequent requests |
US11842025B2 (en) * | 2019-08-06 | 2023-12-12 | Sony Group Corporation | Information processing device and information processing method |
US11769013B2 (en) * | 2019-11-11 | 2023-09-26 | Salesforce, Inc. | Machine learning based tenant-specific chatbots for performing actions in a multi-tenant system |
US11763090B2 (en) | 2019-11-11 | 2023-09-19 | Salesforce, Inc. | Predicting user intent for online system actions through natural language inference-based machine learning model |
US11763809B1 (en) * | 2020-12-07 | 2023-09-19 | Amazon Technologies, Inc. | Access to multiple virtual assistants |
US20220246144A1 (en) * | 2021-01-29 | 2022-08-04 | Salesforce.Com, Inc. | Intent disambiguation within a virtual agent platform |
JP7334988B2 (en) | 2021-03-11 | 2023-08-29 | 株式会社フジキカイ | Box coupling device |
DE102021006023B3 (en) * | 2021-12-07 | 2022-12-22 | Mercedes-Benz Group AG | Method for operating a speech dialogue system and speech dialogue system |
Citations (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN1985222A (en) * | 2004-04-19 | 2007-06-20 | 西门子能量及自动化公司 | System and method to query for machine conditions |
CN104325973A (en) * | 2013-07-22 | 2015-02-04 | 通用电气公司 | System And Method For Monitoring Braking Effort |
CN104584096A (en) * | 2012-09-10 | 2015-04-29 | 苹果公司 | Context-sensitive handling of interruptions by intelligent digital assistants |
CN106471570A (en) * | 2014-05-30 | 2017-03-01 | 苹果公司 | Order single language input method more |
KR20170030297A (en) * | 2015-09-09 | 2017-03-17 | 삼성전자주식회사 | System, Apparatus and Method For Processing Natural Language, and Computer Readable Recording Medium |
CN107004412A (en) * | 2014-11-28 | 2017-08-01 | 微软技术许可有限责任公司 | Equipment arbitration for audiomonitor |
US9721570B1 (en) * | 2013-12-17 | 2017-08-01 | Amazon Technologies, Inc. | Outcome-oriented dialogs on a speech recognition platform |
US20170372703A1 (en) * | 2016-06-27 | 2017-12-28 | Google Inc. | Asynchronous processing of user requests |
CN108268235A (en) * | 2016-12-30 | 2018-07-10 | 谷歌有限责任公司 | Proactive notification is perceived for the dialogue of speech interface equipment |
US20180261203A1 (en) * | 2017-03-09 | 2018-09-13 | Capital One Services, Llc | Systems and methods for providing automated natural language dialogue with customers |
Family Cites Families (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20090125380A1 (en) * | 2001-11-14 | 2009-05-14 | Retaildna, Llc | System and method for location based suggestive selling |
JP3788793B2 (en) * | 2003-04-25 | 2006-06-21 | 日本電信電話株式会社 | Voice dialogue control method, voice dialogue control device, voice dialogue control program |
JP4412504B2 (en) * | 2007-04-17 | 2010-02-10 | 本田技研工業株式会社 | Speech recognition apparatus, speech recognition method, and speech recognition program |
US10276170B2 (en) | 2010-01-18 | 2019-04-30 | Apple Inc. | Intelligent automated assistant |
JP6359327B2 (en) * | 2014-04-25 | 2018-07-18 | シャープ株式会社 | Information processing apparatus and control program |
US10431218B2 (en) * | 2016-02-15 | 2019-10-01 | EVA Automation, Inc. | Integration and probabilistic control of electronic devices |
US10950230B2 (en) * | 2016-10-28 | 2021-03-16 | Panasonic Intellectual Property Corporation Of America | Information processing device and information processing method |
US10311872B2 (en) * | 2017-07-25 | 2019-06-04 | Google Llc | Utterance classifier |
US10248379B2 (en) * | 2017-07-27 | 2019-04-02 | Motorola Solutions, Inc. | Automatic and selective context-based gating of a speech-output function of an electronic digital assistant |
US10896675B1 (en) * | 2018-06-29 | 2021-01-19 | X Development Llc | Multi-tiered command processing |
WO2020106315A1 (en) | 2018-11-21 | 2020-05-28 | Google Llc | Orchestrating execution of a series of actions requested to be performed via an automated assistant |
-
2019
- 2019-02-07 WO PCT/US2019/017039 patent/WO2020106315A1/en unknown
- 2019-02-07 US US16/343,285 patent/US11031007B2/en active Active
- 2019-02-07 JP JP2020569786A patent/JP7195343B2/en active Active
- 2019-02-07 EP EP21192015.2A patent/EP3944233A3/en active Pending
- 2019-02-07 KR KR1020207035897A patent/KR102477072B1/en active IP Right Grant
- 2019-02-07 EP EP19706214.4A patent/EP3679572B1/en active Active
- 2019-02-07 CN CN201980043375.7A patent/CN112352276B/en active Active
-
2021
- 2021-06-04 US US17/339,114 patent/US11769502B2/en active Active
-
2023
- 2023-08-07 US US18/231,112 patent/US20230377572A1/en active Pending
Patent Citations (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN1985222A (en) * | 2004-04-19 | 2007-06-20 | 西门子能量及自动化公司 | System and method to query for machine conditions |
CN104584096A (en) * | 2012-09-10 | 2015-04-29 | 苹果公司 | Context-sensitive handling of interruptions by intelligent digital assistants |
CN104325973A (en) * | 2013-07-22 | 2015-02-04 | 通用电气公司 | System And Method For Monitoring Braking Effort |
US9721570B1 (en) * | 2013-12-17 | 2017-08-01 | Amazon Technologies, Inc. | Outcome-oriented dialogs on a speech recognition platform |
CN106471570A (en) * | 2014-05-30 | 2017-03-01 | 苹果公司 | Order single language input method more |
CN107004412A (en) * | 2014-11-28 | 2017-08-01 | 微软技术许可有限责任公司 | Equipment arbitration for audiomonitor |
KR20170030297A (en) * | 2015-09-09 | 2017-03-17 | 삼성전자주식회사 | System, Apparatus and Method For Processing Natural Language, and Computer Readable Recording Medium |
US20170372703A1 (en) * | 2016-06-27 | 2017-12-28 | Google Inc. | Asynchronous processing of user requests |
CN108268235A (en) * | 2016-12-30 | 2018-07-10 | 谷歌有限责任公司 | Proactive notification is perceived for the dialogue of speech interface equipment |
US20180261203A1 (en) * | 2017-03-09 | 2018-09-13 | Capital One Services, Llc | Systems and methods for providing automated natural language dialogue with customers |
Also Published As
Publication number | Publication date |
---|---|
KR102477072B1 (en) | 2022-12-13 |
KR20210010523A (en) | 2021-01-27 |
JP7195343B2 (en) | 2022-12-23 |
EP3944233A2 (en) | 2022-01-26 |
US11769502B2 (en) | 2023-09-26 |
CN112352276B (en) | 2024-04-09 |
JP2021533399A (en) | 2021-12-02 |
EP3679572B1 (en) | 2021-08-25 |
EP3944233A3 (en) | 2022-05-11 |
US11031007B2 (en) | 2021-06-08 |
EP3679572A1 (en) | 2020-07-15 |
US20230377572A1 (en) | 2023-11-23 |
WO2020106315A1 (en) | 2020-05-28 |
US20200302924A1 (en) | 2020-09-24 |
US20210295841A1 (en) | 2021-09-23 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11769502B2 (en) | Orchestrating execution of a series of actions requested to be performed via an automated assistant | |
US20230297214A1 (en) | Providing composite graphical assistant interfaces for controlling various connected devices | |
EP3788620B1 (en) | Supplementing voice inputs to an automated assistant according to selected suggestions | |
AU2019432912B2 (en) | Dynamically adapting assistant responses | |
US11664028B2 (en) | Performing subtask(s) for a predicted action in response to a separate user interaction with an automated assistant prior to performance of the predicted action | |
US11900944B2 (en) | Initializing non-assistant background actions, via an automated assistant, while accessing a non-assistant application | |
US11848013B2 (en) | Automated assistant invocation of second interactive module using supplemental data provided by first interactive module | |
WO2020226667A1 (en) | Performing subtask(s) for a predicted action in response to a separate user interaction with an automated assistant prior to performance of the predicted action | |
US20240020091A1 (en) | Assistant adaptation of graphical user interface to guide interaction with user in fulfilling user request | |
US20240038246A1 (en) | Non-wake word invocation of an automated assistant from certain utterances related to display content | |
WO2024015114A1 (en) | Assistant adaptation of graphical user interface to guide interaction with user in fulfilling user request |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |