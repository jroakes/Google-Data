CN102549603B - Relevance-based image selection - Google Patents
Relevance-based image selection Download PDFInfo
- Publication number
- CN102549603B CN102549603B CN201080042760.9A CN201080042760A CN102549603B CN 102549603 B CN102549603 B CN 102549603B CN 201080042760 A CN201080042760 A CN 201080042760A CN 102549603 B CN102549603 B CN 102549603B
- Authority
- CN
- China
- Prior art keywords
- video
- keyword
- frame
- training
- module
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/70—Information retrieval; Database structures therefor; File system structures therefor of video data
- G06F16/78—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/7867—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using information manually generated, e.g. tags, keywords, comments, title and artist information, manually generated time, location and usage information, user ratings
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/70—Information retrieval; Database structures therefor; File system structures therefor of video data
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/70—Information retrieval; Database structures therefor; File system structures therefor of video data
- G06F16/73—Querying
- G06F16/738—Presentation of query results
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/70—Information retrieval; Database structures therefor; File system structures therefor of video data
- G06F16/74—Browsing; Visualisation therefor
- G06F16/743—Browsing; Visualisation therefor a collection of video files or sequences
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/70—Information retrieval; Database structures therefor; File system structures therefor of video data
- G06F16/78—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/70—Information retrieval; Database structures therefor; File system structures therefor of video data
- G06F16/78—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/783—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/70—Information retrieval; Database structures therefor; File system structures therefor of video data
- G06F16/78—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/783—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
- G06F16/7844—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content using original textual content or text extracted from visual content or transcript of audio data
Abstract
A system, computer readable storage medium, and computer-implemented method presents video search results responsive to a user keyword query. The video hosting system uses a machine learning process to learn a feature-keyword model associating features of media content from a labeled training dataset with keywords descriptive of their content. The system uses the learned model to provide video search results relevant to a keyword query based on features found in the videos. Furthermore, the system determines and presents one or more thumbnail images representative of the video using the learned model.
Description
Technical field
Present invention relates in general to the mark video relevant to search terms or its part.Specifically, the audio-visual content that embodiments of the invention relate to based on video selects one or more representational thumbnail image.
Background technology
The user of media trustship website browses or searches for the media content of trustship by input keyword or search terms with the text meta-data of query specification media content usually.Can such as can comprise the title of media file or the description summary of media content by search metadata.Such text meta-data often cannot represent the whole content of video, very long and especially true when having several scenes at video.In other words, if video has a large amount of scene and plurality of kinds of contents, then in these scenes certain some may not be described in text meta-data, therefore this video can not in response to probably describe this type of scene keyword search and returned.Therefore, traditional search engines often cannot return media content maximally related with the search of user.
The Second Problem of Conventional media trustship website is: due to the media content substantial amounts of trustship, so search inquiry may return hundreds of or even thousands of media files in response to user's inquiry.Thus, user's which Search Results that may be difficult to evaluate in hundreds of or thousands of Search Results is the most relevant.In order to assisted user, to assess which Search Results the most relevant, and each Search Results can present by website together with thumbnail.Traditionally, the thumbnail image being used for representing video is the predetermined frame (such as, the first frame, intermediate frame or last frame) from video file.But the thumbnail selected in this way does not often represent the actual content of video, because there is not the relation between the ordinal position of thumbnail and the content of video.In addition, thumbnail may be not relevant to the search inquiry of user.Therefore, user's which Search Results that may be difficult to evaluate in hundreds of or thousands of Search Results is the most relevant.
Thus, need for find and present media research result will allow user easily assess improving one's methods of their correlativity.
Summary of the invention
System, computer-readable recording medium and a computer-implemented method, for finding in response to user's keyword query and present video search result.Video mandatory system receives keyword search query from user, and selects the video with the content relevant to keyword query.Video mandatory system uses video index to select the content of frame representatively video from video, and this video index stores the frame of multiple video with the keyword relevance score between the keyword be associated with frame.Video mandatory system presents selected frame as the thumbnail for video.
In an aspect, a kind of computer system, uses the model of the machine learning of the relation between the feature of frame of video and the keyword of describing video contents to generate and can search for video index.Video mandatory system receives and marks training dataset, and this data set comprises one or more keyword of the content of media item (such as, image or audio clips) collection and description media item.Video mandatory system extracts the feature of the content of characterizing media project.The model of training machine study is with the correlativity of study between specific features and the keyword describing content.Then the model based on the characteristic sum machine learning of video generates video index, and the frame of video in video database is mapped to keyword by this video index.
Advantageously, video mandatory system based on video actual content instead of only depend on text meta-data to find and present Search Results.Therefore, video mandatory system allows user can evaluate the correlativity of the video in search result set better.
The feature and advantage described in content of the present invention and following embodiment are not exhaustive.Those of ordinary skill in the art will know many supplementary features and advantage according to its accompanying drawing, instructions and claim.
Accompanying drawing explanation
Fig. 1 is the high level block diagram of the video mandatory system 100 according to an embodiment.
Fig. 2 be a diagram that the high level block diagram of the study engine 140 according to an embodiment.
Fig. 3 be a diagram that the process flow diagram of the step performed by study engine 140 for generating learning characteristic-keyword models according to an embodiment.
Fig. 4 be a diagram that the process flow diagram of the step performed by study engine 140 for generating feature data set 255 according to an embodiment.
Fig. 5 be a diagram that the process flow diagram of the step performed by study engine 140 for generating feature-keyword matrix according to an embodiment.
Fig. 6 be a diagram that the block diagram of the concrete view of the imagery annotation engine 160 according to an embodiment.
Fig. 7 be a diagram that according to an embodiment for finding and presenting the process flow diagram of the step performed by video mandatory system 100 of video search result.
Fig. 8 be a diagram that according to an embodiment for based on video metadata being the process flow diagram of the step performed by video mandatory system 100 that video selects thumbnail.
Fig. 9 be a diagram that according to an embodiment for based on the keyword in user search queries being the process flow diagram of the step performed by video mandatory system 100 that video selects thumbnail.
Figure 10 is the process flow diagram of the step performed by imagery annotation engine 160 for identifying concrete event in video or scene based on user's keyword query according to an embodiment.
Accompanying drawing only describes the preferred embodiments of the present invention for illustrated object.Those skilled in the art according to hereafter discuss will easily recognize can use here shown in structure and method alternate embodiment and do not depart from the principle of the invention described herein.
Embodiment
System architecture
Fig. 1 illustrates an embodiment of video mandatory system 100.Video mandatory system 100 finds in response to user's keyword query and presents video search result set.Video mandatory system 100 based on video actual audio-visual content instead of only depend on the text meta-data associated with video and present Search Results.Each Search Results with represent video audio-visual content thumbnail together with present, the correlativity of this thumbnail assisted user evaluation result.
In one embodiment, video mandatory system 100 comprises front-end server 110, video search engine 120, video note engine 130, study engine 140, video database 175, video note index 185 and feature-keyword models 195.The user that video mandatory system 100 represents any permission client device 150 is via the system of search and/or browser interface accessing video content.Video source can be uploaded from the video of user, the search or crawl (crawl) etc. of system or other video website or database or its any combination.Such as in one embodiment, video mandatory system 100 can be arranged to and allow user's upload contents.In another embodiment, video mandatory system 100 can be arranged to by means of only other source of crawl or search for such source and to carry out off-line (in order to build video database) from such source or obtain video at query time.
Various parts (or claim module, such as front-end server 110, video search server 120, video explain engine 130, study engine 140, video database 175, video note index 185 and feature-keyword models 195, each be implemented as the part of the server class computer systems with one or more computing machine, this computing machine comprises CPU, storer, network interface, peripheral interface and other well-known components.Parts itself preferably operation system (such as LINUX), there is general high-performance CPU, 1G or more multi-memory and 100G or more polydisc reservoir.Certainly can use the computing machine of other type, and be expected to, along with developing more powerful computing machine in the future, to be configured them according to instruction here.In this embodiment, module stores goes up in computer readable storage devices (such as, hard disk), is loaded in storer and one or more processor comprised by the part as system 100 execution.Alternatively, hardware or software module can be stored in the other places in system 100.When being configured for execution various operation described herein, as understood by a person skilled in the art, multi-purpose computer becomes concrete computing machine, because the specific function of such Computer Storage and data configure it in the mode different from its this capabilities that such as its underlying operating system and hardware logic can provide.YOUTUBE for implementing the suitable video mandatory system 100 of this system
tMwebsite; Other video mandatory system is also known and can be suitable for operating according to instruction disclosed herein.One embodiment of the present of invention are represented and other embodiment can comprise other parts by understanding the name parts of video mandatory system 100 described herein.In addition, other embodiment can not containing parts described herein and/or the function described that distributes among module by different way.In addition, the function incorporating multiple parts into can be incorporated in single parts.
Fig. 1 also illustrates three client devices 150 that can be coupled to video mandatory system 100 by network 160 communicatedly.Client device 150 can be to support the communication facilities with any type of the communication interface of system 100.Suitable equipment can include but not limited to personal computer, mobile computer (such as, notebook), personal digital assistant (PDA), smart phone, mobile phone and game console and equipment, there is the equipment of checking (such as, Set Top Box, televisor and receiver) of network function.Figure 1 illustrates only three clients 150 in case make description simplify and clear.In fact, thousands of or millions of clients 150 can be connected to video mandatory system 100 via network 160.
Network 160 can be wired or wireless network.The example of network 160 comprises the Internet, in-house network, WiFi network, WiMAX network, mobile telephone network or its combination.Those skilled in the art will recognize that, other embodiment can have the module different from module described herein and can differently distributed function among module.Communication means between client device and system 100 is not limited to any particular user interface or procotol, but in an exemplary embodiment, user comes with video mandatory system 100 mutual via the conventional web browser of the utilization standard the Internet protocols of client device 150.
Client 150 is mutual to search the video content stored in video database 175 via front-end server 110 and video mandatory system 100.Front-end server 110 provides the control and element that allow user's inputted search inquiry (such as, keyword).In response to inquiry, front-end server 110 provides search result set associated with the query.In one embodiment, Search Results comprises the lists of links of the associated video content pointed in video database 175.Link can present by front-end server 110 together with the information that such as thumbnail image, title and/or text snippet etc. and video content are associated.Front-end server 110 also provides and allows user to select video for the control checked in client 150 and element from Search Results.
Video search engine 120 processes the user's inquiry received via front-end server 110, and generation comprises, and points to the results set of the link of video associated with the query or video section in video database 175, and is a mode for this function.Video search engine 120 can also perform function of search, such as, score to search result rank and/or to Search Results according to the correlativity of Search Results.In one embodiment, video search engine 120 uses various text query technology to find associated video based on the text meta-data associated with video.In another embodiment, video search engine 120 based on video or video section actual audio-visual content instead of depend on text meta-data and search video or video section.Such as, if user's typing search inquiry " car match ", then video search engine 120 can find and return from moving picture car competition field scape, although this scene may be only the brief portions be not described in text meta-data in film.Hereafter more specifically describe for using video search engine with the process of locating the special scenes of video based on the audio-visual content of video with reference to Figure 10.
In one embodiment, video search engine 120 also selects thumbnail image or thumbnail image collection to show together with each Search Results fetched.Each thumbnail image comprises the audio-visual content representing video and the picture frame inquired about in response to user, and the correlativity of assisted user determination Search Results.Below with reference to Fig. 8-Fig. 9, the method for selecting one or more representational thumbnail image is more specifically described.
Video is explained engine 130 and is utilized the keyword relevant to the frame of the video from video database 175 or the audio-visual content of scene to explain frame or scene, and these are explained and is stored into video note index 185, and be a kind of means for performing this function.In one embodiment, video explains sampling section (such as, frame of video or short audio editing) the generating feature vector of engine 130 according to the video from video database 175.Video is explained the proper vector that learning characteristic-keyword models 195 is applied to extraction by engine 130 then and is obtained diversity to generate keyword.The relative intensity that learn associate of each keyword score representative between keyword with one or more feature.Therefore, score can be understood as the relative possibility of the content describing keyword descriptor frame.In one embodiment, video explain engine 130 also according to the keyword score of the frame of each video to frame rank, this contributes at query time video score and rank.Video is explained engine 130 and the keyword score being used for every frame is stored into video note engine 185.Video search engine 120 can use these keyword scores to determine to inquire about with user maximally related video or video section and to determine the thumbnail image of representing video content.Below with reference to Fig. 6, video annotation engine 130 is more specifically described.
Study engine 140 uses machine learning to carry out training characteristics-keyword models 195, and the feature of image or short audio editing is associated with the keyword of the visual or audio content describing them by this model, and is a kind of means for performing this function.Study engine 140 processes the media item of the mark destination aggregation (mda) with representing training image, the audio frequency of video and/or audio editing (" media item ") and/or one or more keyword mark of content viewable.Such as, the image of the dolphin of ocean went swimming can be labeled in the such as keyword such as " dolphin ", " swimming ", " ocean ".Study engine 140 extracts feature set from marking training data (image, video or audio frequency), and the feature that analysis is extracted is to determine the statistical correlation between specific features and the keyword of mark.Such as, in one embodiment, the matrix of study engine 140 weight generation, frequency values or discriminant function, to indicate in the relative intensity being used to mark the relevance between the keyword of media item and the feature derived according to the content of media item.Learn engine 140 by the relational storage of the derivation between keyword and feature to feature-keyword models 195.Below with reference to Fig. 2, study engine 140 is more specifically described.
Fig. 2 be a diagram that the block diagram of the concrete view of the study engine 140 according to an embodiment.In the embodiment shown, study engine comprises and a little leads to (click-through) module 210, characteristic extracting module 220, keyword learning module 240, Associate learning module 230, marks training dataset 245, characteristic data set 255 and keyword data collection 265.Those skilled in the art will recognize that, other embodiment can have the module different from module described herein and can differently distributed function among module.In addition, the function incorporating various module into can be performed by multiple engine.
Point logical module 210 be provided for obtaining marked training dataset 245 robotization mechanism and be a kind of means for performing this function.The logical module 210 of point follows the tracks of the user search queries on video mandatory system 100 or one or more foreign medium search website.When performing search inquiry as user and select media item from Search Results, the logical module 210 of point stores just (positive) relevance between keyword in user's inquiry and user-selected media item.The logical module 210 of point also can be stored in keyword and not select negative (negative) relevance between Search Results.Such as, user searches " dolphin " and receives image result set.In fact user may comprise the image of dolphin from the image that list is selected and therefore be provided for the good label of image.Based on study just and/or negative customers, the logical module 210 of point determines one or more keyword being appended hereto each image.Such as, in one embodiment, logical module 210 (such as 5 users searching " dolphin " from after results set selects identical image) after the positive implicative observing the threshold number between image and keyword stores the keyword being used for media item.Therefore, the logical module 210 of point can based on monitoring user search and the gained user action when selecting Search Results, the relation statistically between Identifying Keywords and image.This mode utilize individual consumer to what content in the common procedure of their search behavior for given keyword as the valuable understanding of associated picture.In certain embodiments, keyword identification module 240 can use natural language technology (such as getting stem and filtration) with pre-service search inquiry data so that mark and extraction keyword.The media item of mark and keyword associated with it are stored into and mark training dataset 245 by the logical module 210 of point.
In an alternative embodiment, mark training dataset 245 and can replace the training data stored from external source 291 (such as, having marked deposit image or audio clips database).In one embodiment, keyword is extracted from the metadata associated with image or audio clips (such as filename, title or text snippet).Mark training dataset 245 and can also store the data (such as, using from the logical module 210 of point and the data from the derivation of one or more external data base 291) obtained from the combination in source discussed above.
Characteristic extracting module 220 extracts feature set from marking training dataset 245, and is a kind of means for performing this function.Feature carrys out the different aspect of characterizing media by this way, that is, the image of analogical object will have similar feature and the audio clips of similar sound will have similar feature.In order to from image zooming-out feature, characteristic extracting module 220 can practical writing adjustment method, edge detection algorithm or colour code algorithm to extract characteristics of image.For audio clips, characteristic extracting module 220 can to the various conversion of applications of sound waves (such as, generating sonograph), application bandpass filter or autocorrelative set, then application vector quantization algorithm to extract audio frequency characteristics.
In one embodiment, training image is segmented into " burst (patch) " and extracts the feature being used for each burst by characteristic extracting module 220.Burst can have height and width range (such as 64 × 64 pixels).Burst can be overlapping or not overlapping.Characteristic extracting module 220 by unsupervised learning algorithm application in characteristic, to identify the character subset of most bursts of most Efficient Characterization image.Such as, characteristic extracting module 220 can apply clustering algorithm (such as K mean cluster) to identify feature clustering that is similar or that jointly occur in the picture or group each other.Therefore, such as characteristic extracting module 220 can identify 10,000 most representative feature mode and the burst be associated.
Similarly, training audio clips is segmented into briefly " sound " and extracts the feature for sound by characteristic extracting module 220.The same with training image, characteristic extracting module 220 applies unsupervised learning characterizes training audio clips most effectively audio frequency characteristics subset with mark.
Keyword identification module 240 carrys out the keyword set of identified frequent appearance based on marking training dataset 245, and is a kind of means for performing this function.Such as, in one embodiment, keyword identification module 240 is determined to mark the concentrated N number of the most frequently used keyword (such as N=20,000) of training data.Keyword identification module 220 stores and frequently occurs keyword set in keyword data collection 265.
Associate learning module 230 determines the statistical correlation between the feature in characteristic data set 255 and the keyword in keyword data collection 265, and is a kind of means for performing this function.Such as, in one embodiment, Associate learning module 230 represents relevance with the form of feature-keyword matrix.Feature-keyword matrix comprises the matrix with m capable and n row, wherein m capable in often row correspond to the different characteristic vector from characteristic data set 255 and the often row in n row and correspond to different keywords in keyword data collection 265 (such as, m=10,000 and n=20,000).In one embodiment, each entry of feature-keyword matrix comprises weight or score, the relative intensity of the correlativity between the keyword that this weight or score indicative character and training data are concentrated.The entry that such as matrix data is concentrated can indicate the image marked with keyword " dolphin " by the relative possibility of instruction specific features vector Y.The feature of study-keyword matrix is stored into learning characteristic-keyword models 195 by Associate learning module 230.In other alternative, different correlation function and expression (such as by keyword and visual and/or that audio frequency characteristics is relevant nonlinear function) can be used.
Fig. 3 be a diagram that the process flow diagram of an embodiment of the method for generating feature-keyword models 195.First, matrix study engine 140 is such as from external source 291 or receive (302) from the logical module 210 of point described above and marked training dataset 245.Keyword learning module 240 is determined that (304) have marked and is the most frequently occurred keyword (such as front 20,000 keyword) in training data 245.Characteristic extracting module 220 generates (306) feature for training data 245 then, and by representational characteristic storage to characteristic data set 255.Associate learning module 230 generates feature-keyword matrix that keyword is mapped to feature by (308), and feature-keyword models 195 is stored in this mapping.
Fig. 4 illustrates and generates an example embodiment of the process of (306) feature for basis standard exercise image 245.In this example embodiment, characteristic extracting module 220 is by determining that the color histogram representing the color data associated with image slices generates (402) color characteristic.Color histogram for given burst stores the number of pixels of each color in this burst.
Characteristic extracting module 220 also generates (404) textural characteristics.In one embodiment, characteristic extracting module 220 uses local binary pattern (LBP) to represent edge in each burst and data texturing.LBP for pixel represents the relative pixel intensity value of neighborhood pixels.Such as, the LBP for given pixel can be 8 codes (being 8 neighborhood pixels in the circle of 1 pixel corresponding to radius), and 1 instruction neighborhood pixels has higher-strength value, and 0 instruction neighborhood pixels has comparatively low intensity value.Characteristic extracting module determines the histogram of each burst then, and this histogram stores the LBP value counting in given burst.
Cluster is applied (406) in color characteristic and textural characteristics by characteristic extracting module 220.Such as, in one embodiment, K mean cluster is applied to color histogram by characteristic extracting module 220, to identify the multiple clusters (such as 20) representing burst best.For each cluster, determine the cluster barycenter (proper vector) of the dominant color representing this cluster, create the dominant color feature set being used for all bursts thus.Characteristic extracting module 220 pairs of independent clusters of LBP histogram, to identify Texture similarity (i.e. textural characteristics) subset characterizing burst texture best, also mark is used for the leading texture feature set of burst thus.
Characteristic extracting module 220 generates (408) proper vector for each feature then.In one embodiment, for texture and the color histogram united of burst, to form the single proper vector for this burst.The proper vector that unsupervised learning algorithm (such as cluster) is applied to for burst by characteristic extracting module 220 is amassed, to generate the proper vector subset (such as 10,000 most representative proper vector) that (410) represent most of burst.Proper vector subset is stored into property data base 255 by characteristic extracting module 220.
For audio training data, characteristic extracting module 220 can generate audio feature vector by calculating Mel frequency cepstrum coefficient (MFCC).These coefficients carry out the short-term power spectrum of representative voice based on the linear conversion in advance of log power spectrum in non-linear frequency scale.Audio feature vector is stored in characteristic data set 255 then, and similarly can be processed with image feature vector.In another embodiment, characteristic extracting module 220 generates audio feature vector by using stabilization sense of hearing image (SAI).In another embodiment, one or more bandpass filter is applied to voice data, and based within sound channel and between correlativity derive feature.In another embodiment, sonograph is used as audio frequency characteristics.
Fig. 5 illustrates for the instantiation procedure from characteristic data set 255 and keyword data collection 265 learning characteristic keyword matrix iteratively.In one embodiment, Associate learning module 230 carrys out initialization (502) feature-keyword matrix by filling entry with initial weight.Such as, in one embodiment, initial weight is all arranged to zero.For the given keyword K from keyword data collection 265, the positive training program p+ of Associate learning module 230 Stochastic choice (504) (namely by the training program that keyword K marks) and Stochastic choice bears training program p-(namely by the training program that keyword K marks).Characteristic extracting module 220 determines (506) proper vector for positive training program and negative training program as described above.Associate learning engine 230 generates (508) keyword score for each training program in positive training program and negative training program by using feature-keyword matrix that proper vector (is such as passed through proper vector and feature-keyword matrix multiple to be produced keyword vectorial) from Feature Space Transformation to keyword space.Associate learning module 230 determines (510) difference between keyword score then.If difference is greater than predefine threshold value (namely positive and negative training program is correctly sorted), then matrix constant (512).Otherwise matrix entries is set up, and (514) make difference be greater than threshold value.Associate learning module 230 determines whether (516) meet stopping criterion then.If do not meet stopping criterion, then matrix study performs another iteration 520 to improve matrix further by new positive and negative training program.If meet stop condition, then learning process stops (518).
In one embodiment, when the positive and negative training previously selected for sliding window on the logarithmic mean of correct sequence exceeded predefine threshold value time, stopping criterion is met.Alternatively, can by by the matrix application of study in confirming that separately data set measures the performance of the matrix of study, and stopping criterion is met when performance exceedes predefine threshold value.
In an alternative embodiment, in order to score is compatible between keyword, in each iteration of learning process, calculate for different keyword instead of same keyword K and compare keyword score.Therefore, in this embodiment, select positive training program p+ as with the first keyword K
1the training program of mark, and select negative training program p-as with different keyword K
2the training program of mark.In this embodiment, Associate learning module 230 generate be used for each training program/keyword to (namely just to negative to) keyword score.Associate learning module 230 then with the Method compare keyword score identical with above-described mode, although keyword score is relevant from different keyword.
In alternative embodiments, Associate learning module 230 learns dissimilar feature-keyword models 195 (such as, generation model or discrimination model).Such as, in an alternative embodiment, Associate learning module 230 derives and can be applied to feature set, to obtain and the discriminant function of one or more keyword of those feature associations (i.e. sorter).In this embodiment, clustering algorithm is applied to the feature of the particular type be associated with image slices or audio section or all features by Associate learning module 230.Associate learning module 230 generates the sorter of each keyword be used in keyword data collection 265.Sorter comprises the set of discriminant function (such as, lineoid) and weight or other value, and wherein weight or value specific characteristic are at the discriminating power distinguishing media item class and another media item class.The sorter of study is stored into learning characteristic-keyword models 195 by Associate learning module 230.
In certain embodiments, characteristic extracting module 220 and Associate learning module 230 generate the feature set being used for new training data 245 iteratively, and re-training sorter is until the convergence of this sorter.When adding new training characteristics collection and substantially not changing the discriminant function and weight that associate with feature set, sorter is restrained.In a specific embodiment, Online SVM algorithm is used to, based on the eigenwert be associated with new training data 245, recalculate lineoid function iteratively until lineoid function convergence.In other embodiments, Associate learning module 230 re-training sorter termly.In certain embodiments, no matter when Associate learning module 230 in continuous foundation, such as, adds query data (such as from the logical data of new point), re-training sorter to marking training dataset 245.
In any previous embodiment, the model of the relation of gained feature-keyword matrix representative between (as being applied to image/audio file) keyword and the proper vector derived according to image/audio file.This model can be understood as expresses bottom physical relation in the common appearance of keyword and physical characteristics (such as, color, texture, the frequency information) aspect of representative image/audio file.
Fig. 6 illustrates the concrete view that video explains engine 130.In one embodiment, video note engine 130 comprises video sampling module 610, multipletexture fetch modules 620 and thumbnail selection module 630.Those skilled in the art will recognize that, other embodiment can have the module different from module described herein and can differently distributed function among module.In addition, the function incorporating various module into can be performed by multiple engine.
Video sampling module 610 is from the video sampling video content frames video database 175.In one embodiment, video sampling module 610 is from the individual video sampling video content video database 175.Sampling module 610 can according to fixed cycle speed (such as every 10 seconds 1 frames), depend on internal factor (length of such as video) speed or based on the speed of external factor (popularity of such as video (such as by according to than based on checking that frequency that the less welcome video of number of times is higher is to how welcome video sampling)) to video sampling.Alternatively, video sampling module 610 use scenes segmentation with based on scene boundary to frame sampling.Such as video sampling module 610 can from least one frame of every frame sampling to ensure that the frame of sampling represents the whole content of video.In another alternative embodiment, whole scene of video sampling module 610 sample video instead of individual frame.
Characteristic extracting module 620 use with above about learning the identical method of characteristic extracting module 220 that engine 140 describes.Characteristic extracting module 620 generates the proper vector being used for each sample frame or scene.Such as, as described above, each proper vector can comprise 10,000 entry, and each entry represents the specific features obtained by vector quantization.
Frame explains the keyword relevance score that module 630 generates each sample frame being used for video.Frame is explained module 630 and learning characteristic-keyword models 195 is applied to the proper vector for sample frame, to determine the keyword relevance score of this frame.Such as, frame explain module 630 feature-keyword matrix can be used to perform matrix multiplication, with by eigenvector transform to keyword space.Frame is explained module 630 and is therefore generated the keyword relevance score vector (" keyword score vector ") being used for every frame, the possibility that each keyword relevance score designated frame wherein in keyword score vector is relevant to the keyword of the frequent use keyword set in keyword data collection 265.Frame explains the keyword score vector that mark that the mark (such as, the skew of frame in the video of frame as its part) of module 630 and frame and video explain the video in index 185 stores this frame explicitly.Therefore, each sample frame is come based on the proper vector derived according to frame and is described each keyword and be associated with the keyword vector scores of the relation between frame.In addition, as described above, therefore one or more sample frame is associated each video in database with (may be used for thumbnail), and these sample frame are associated with keyword.
In an alternative embodiment, video is explained engine 130 and is generated the keyword score being used for frame set (such as scene) instead of each personal sampling frame.Such as, keyword score can be stored for the concrete scene of video.For audio frequency characteristics, keyword score can store explicitly with the frame set (as example, the speech from concrete individual) of crossing over concrete audio clips.
Operation and use
When user inputs the search inquiry of one or more word, search engine 120 accessing video explains index 185 to find and to present associated video results set (such as searching by performing in index 185).In one embodiment, the keyword for the input inquiry word to selected Keywords matching that search engine 120 uses video to explain in index 185 must assign to find the video relevant with search inquiry, and to the associated video rank in results set.Video search engine 120 also can be provided for the Relevance scores of the instruction of each Search Results and the perceived relevance of search inquiry.To explain except video except the keyword score in index 185 or alternatively, search engine 120 also can access comprise the text meta-data associated with video conventional index to find Search Results, to search result rank and score.
Fig. 7 be a diagram that for finding and presenting the process flow diagram of the main process performed by video mandatory system 100 of video search result.Front-end server 110 is from the search inquiry comprising one or more query term with reception (702).Search engine 120 determines that (704) meet the results set of keyword search query; The searching algorithm of any type and indexed results can be used to select this results set.Results set comprises the link pointing to one or more video, and this video has the content relevant to query term.
Search engine 120 selects (706) to represent a frame (or multiple frame) of the content of video based on keyword score from each video results set then.For each Search Results, front-end server 110 presents the set of the frame selected by (708) as one or more representational thumbnail.
Fig. 8 and Fig. 9 illustrates two different embodiments that can be used for assigning to select (906) frame based on keyword.In the embodiment in fig. 8, video search engine 120 selects to represent the thumbnail of video based on the text meta-data be stored explicitly with the video in video database 175.Video search engine 120 selects (802) video to select for thumbnail from video database.Video search engine 120 is then from meta-data extraction (804) keyword be stored explicitly with the video video database 175.Metadata can such as comprise author or other users video title or the text snippet of video are provided.Video search engine 120 then accessing video explains index 185, and use the keyword extracted to select (806) to represent one or more representational frame (such as, by selecting to have for the keyword extracted frame or the frame collection of the highest rank keyword score) of video.Front-end server 110 shows frame selected by (808) then as the thumbnail for the video in Search Results.This embodiment advantageously ensures that selected thumbnail incites somebody to action in fact representing video content.Such as consider that title is the video of " dolphin swimming ", this video comprises some scenes of swimming dolphin, but other scene is only spacious ocean.In fact selection is described one or more frame of dolphin instead of is selected arbitrarily thumbnail frame (such as the first frame or center frame) by video search engine 120.Therefore, user can evaluate the correlativity of Search Results and inquiry better.
Fig. 9 be a diagram that for selecting the process flow diagram of second embodiment of process of thumbnail for presenting together with the video in search result set.In this embodiment, thumbnail selected by one or more depends on the keyword provided in user search queries.First, search engine 120 identifies (902) video search result set based on user search queries.Search engine 120 extracts (904) keyword from the search inquiry of user, uses for when selecting the representational thumbnail frame being used for each Search Results.For each video in results set, video search engine 120 then accessing video explains index 185, and uses the keyword extracted to select one or more representational frame of (906) video (such as by selecting to have for the keyword extracted one or more frame of the highest rank keyword score).Front-end server 110 shows frame selected by (908) then as the thumbnail for the video in Search Results.
This embodiment advantageously ensures that video thumbnails is in fact relevant with the search inquiry of user.Such as suppose user's typing inquiry " dog on slide plate ".Title for the animal of acrobatics " play " but video comprise the associated scenario of being acted the leading role by the dog on slide plate and also comprise other scenes some without dog or slide plate.The method of Fig. 9 advantageously ensures the scene (dog namely on slide plate) that the thumbnail representative of consumer presented is searched.Therefore, user easily can evaluate the correlativity of Search Results and keyword query.
Another feature of video mandatory system 100 allows user to use video to explain index 185 to search concrete scene in video or event.Such as in long action movie, user may want to use query term (such as " car match " or " fight ") to search fight scene or car competition field scape.Video mandatory system 100 only fetches one or more concrete scene (instead of whole video) associated with the query then.Figure 10 illustrates an example embodiment of the process for finding the scene relevant to keyword query or event.Search engine 120 receives (1002) search inquiry from user and identifies (1004) keyword from search string.Use keyword, search engine 120 identifies video and explains index 185 (such as, by performing locating function) to fetch multiple frames 1006 (such as, first 10) for the keyword extracted with the highest keyword score.Search engine is determined then (1008) for the border of the associated scenario in video.Such as, search engine 120 can use scenes cutting techniques to find the border of the scene comprising height correlation frame.Alternatively, search engine 120 can analyze the keyword score of frame around to determine border.Such as, search engine 120 can return such video clipping, and all sample frame in this video clipping have the keyword more than threshold value.Search engine 120 must assign to select (1010) for the thumbnail image of each video in results set based on keyword.Front-end server 110 shows the rank set of the video of (1012) selected thumbnail representative then.
Another feature of video mandatory system 100 be have the ability based on video explain index 185 select can before video selected by replaying user, period or the set of " about video " that shows afterwards.In this embodiment, the keyword that associates with the playback of selected video from title or other metadata of video mandatory system 100.Video mandatory system 100 uses the keyword extracted to inquire about video and explains the index 185 searching video relevant to keyword; This mark may be similar to user-selected video instead of in its metadata, only have other video of same keyword in its real image/audio content.Video mandatory system 100 is then selected the thumbnail that is used for about video as described above and present thumbnail in " relevant video " part of user interface display.This embodiment is advantageously based on other video that the content of playback video provides possibility interesting to user.
Another feature of video mandatory system 100 be have the ability based on use video explain index 185 find and present can before video selected by playback, period or the advertisement that shows afterwards.In one embodiment, video mandatory system 100 fetches the keyword (searching explaining in index 185 to perform by using present frame index) associated with frame of video in real time when user checks video.Video mandatory system 100 then can use the keyword fetched to carry out queries ad database and find the advertisement relevant to keyword.Video mandatory system 100 can show the advertisement relevant with present frame when video is paid a return visit then in real time.
Above-described embodiment advantageously allows media host to provide and the maximally related video content item of the search inquiry of user and representational thumbnail image.By the relevance of study between text query and non-text media content, the system that video mandatory system more only depends on text meta-data provides the Search Results of improvement.
Specifically describe the present invention about priority number object embodiment.It will be appreciated by those skilled in the art that and can also realize the present invention in other embodiments.First, the concrete name of parts, the capitalization of term, attribute, data structure or any other programming or configuration aspects nonessential or important, and the mechanism implementing the present invention or its feature can have different names, form or agreement.In addition can as described via combination or the complete implementation system in hardware cell of hardware and software.Concrete division between function various system unit described here be only citing and nonessential; The function that individual system parts perform can replace and performed by multiple parts, and the function that multiple parts perform can replace and performed by single parts.The concrete function of media hosted service such as can be provided in a multiple or module.
Above-described some parts represents at the algorithm of the operation to information and symbol and presents feature of the present invention.These arthmetic statements and represent it is that those skilled in the art are used for the means of the work essence the most effectively passing on them to others skilled in the art.Implemented by computer program although but these operations are interpreted as in function or described in logic.Also these arrangements of operations have been confirmed to be called that module or code devices are easily and without loss of generality sometimes in addition.
But should remember that all these will associate with suitable physical quantity with term similar and be only the facility mark being applied to these quantity.Unless as according to this discussion clearly another have specifically express, understand in instructions full text, utilize the discussion of such as " process " or " calculatings " or " computing " or the term such as " determination " or " display " to refer to the action of computer system and process or similar electronic computing equipment, this computer system or electronic computing device are handled and are converted as in computer system memory or register or the storage of the information other, the data transmitting or be expressed as in display device physics (electronics) quantity.
Some feature of the present invention comprises the process steps and instruction that describe with the form of algorithm here.All such process steps, instruction or algorithm are performed by computing equipment, and these computing equipments comprise the processing unit (such as microprocessor, microcontroller, dedicated logic circuit etc.) of a certain form and storer (RAM, ROM etc.) and are such as suitable for receiving or providing the input-output apparatus of data.
The present invention also relates to a kind of device for performing operation here.This device can be specifically configured to expect object, or it can comprise the multi-purpose computer that the computer program be stored in computing machine activates selectively or reconfigures, in this case, multi-purpose computer is equivalent to the concrete computing machine being exclusively used in and performing function described herein and operation on 26S Proteasome Structure and Function.Embodying the computer program that computing machine can perform data (such as program code and data) is stored in tangible computer readable storage medium storing program for executing, and this storage medium is such as but is not limited to the dish (comprising floppy disk, CD, CD-ROM, photomagneto disk) of any type, ROM (read-only memory) (ROM), random access memory (RAM), EPROM, EEPROM, magnetic card or optical card, special IC (ASIC) or is suitable for the medium of any type of lasting store electrons coded order.(they are originally as by change physical medium, (physical arrangement and/or the character of such as change or change medium are (such as electric to should also be noted that such computer program, optics, machinery, magnetic, chemical property)) be stored in the data in such medium and exist) not abstract idea or concept or expression and to replace be the artificial product of physics that physical process produces itself, physical medium becomes another state (such as electric charge changes or magnetic polarity changes) to store computer program lastingly in media as well from a state transformation by these physical processes.In addition, the computing machine mentioned in the description can comprise single processor or can be following framework, and these frameworks use the computing power of multiple CPU design in the hope of increasing.
The language that finally should be noted that to use in the description is main to be selected for object that is readable and that instruct and may not yet be selected to define or limit subject matter content.Thus disclosure of the present invention is intended to illustrate and limit the scope of the invention.
Claims (46)
1., for creating the computer-implemented method can searching for video index, described method is performed by computer system and comprises:
Receive and mark training dataset, the described training dataset that marked comprises one group of training video and describes one or more keyword of audio-visual content of each frame in multiple frames of each described training video;
Extract one or more features of the described audio-visual content of each frame in the described multiple frame characterizing each described training video;
Relevance between the most representative proper vector of following storage and the most found keyword:
Generate correlation function set, each correlation function represents the strength of association between one of one of described most representative proper vector and the most found described keyword; And
Generate the feature-keyword matrix of described most representative maps feature vectors to the most found described keyword;
Use described correlation function set and described feature-keyword matrix come training machine study model, with learn the described audio-visual content of each frame in described multiple frame of each described training video of extracted sign described feature and describe each frame in described multiple frame of each described training video described audio-visual content described one or more keyword between correlativity; And
Based on the feature of frame and the model of described machine learning of the multiple index videos in video database, generate described one or more keyword is mapped to the described multiple index videos in described video database described frame described in can search for video index.
2. method according to claim 1, wherein said audio-visual content comprises the external appearance characteristic of the picture frame of described training video.
3. method according to claim 1, wherein said audio-visual content comprises the audio content of the voice data in described training video.
4. method according to claim 1, the described feature wherein extracting the described audio-visual content characterizing described training video comprises:
Each frame in described multiple frame of each described training video is divided into multiple burst, and wherein each burst is overlapping at least in part;
Generate the proper vector being used for each described burst; And
Application clustering algorithm is to have marked the multiple most representative proper vector in training data described in determining.
5. method according to claim 1, wherein generates described feature-keyword matrix and comprises:
Fill entry by utilizing initial weight and carry out feature described in initialization-keyword matrix;
Select the positive training video be associated with the first keyword and the negative training video do not associated with the second keyword;
Extract the feature of described positive training video and negative training video to obtain positive proper vector and negative proper vector;
Use described feature-keyword matrix to described positive proper vector application conversion to obtain the first keyword score for described positive training video;
Use described feature-keyword matrix to described negative proper vector application conversion to obtain the second keyword score for described negative training video;
The described keyword score determining described positive training video does not at least exceed a threshold value than the described keyword score for described negative training video; And
At least do not exceed described threshold value than the described keyword score for described negative training video in response to the described keyword score for described positive training video, regulate the described weight in described feature-keyword matrix.
6. method according to claim 1, wherein generates described video index and comprises:
The frame of the video in described video database is sampled;
Calculate the first eigenvector of the first sample frame of described video, described first eigenvector represents the content of described first sample frame;
By the models applying of described machine learning in described first eigenvector to generate the keyword relevance score between described first sample frame and the keyword of selection; And
Described keyword relevance score and described first sample frame are stored in described video index explicitly.
7. method according to claim 1, wherein generates described video index and comprises:
The scene of the video in described video database is sampled;
Calculate the first eigenvector of the first sampling scene of described video, described first eigenvector represents the content of described first sampling scene;
By the models applying of described machine learning in described first eigenvector with generate described first sampling scene and the keyword of selection between keyword relevance score; And
Scene of described keyword relevance score and described first being sampled is stored in described video index explicitly.
8., for presenting a computer-implemented method for video search result, described method is performed by computer system and comprises:
Select video;
Using video to explain index selects frame as the representative of the content of described video from described video, described video explains the keyword relevance score between the same keyword be associated with each frame in described multiple frame of each video in described multiple video of each frame in multiple frames of each video in the multiple video of index stores, described keyword relevance score represent the audio-visual content of the frame of the described video of sign extracted feature and describe described frame described audio-visual content keyword between the relative intensity of correlativity, the described relative intensity of described correlativity based on machine learning model and determine, the model of described machine learning has used correlation function set and feature-keyword matrix and has trained, strength of association between the most representative proper vector of wherein said correlation function set representative and the most found keyword, and described feature-keyword matrix is by described most representative maps feature vectors extremely the most found described keyword, and
The thumbnail of described frame as described video of selection is provided.
9. method according to claim 8, wherein select described frame to comprise as the representative of the content of described video from described video:
Representative is selected to expect the keyword of video content;
Access described video and explain index to determine the keyword relevance score between the frame of described video and the keyword of selection; And
Explain index according to described video and select that there is the frame with the keyword relevance score of the highest rank of the keyword of described selection.
10. method according to claim 9, the title wherein selecting the described keyword representing described expectation video content to comprise to use described video is as the keyword of described selection.
11. methods according to claim 9, wherein select the described keyword representing described expectation video content to comprise the keyword using keyword query as described selection.
12. methods according to claim 8, wherein select described video to comprise:
Keyword query is received from user; And
The video with the content relevant to described keyword query is selected from video database.
13. methods according to claim 12, wherein select the described video with the content relevant to described keyword query to comprise:
Determine that there is the frame of video with the high keyword relevance score of the keyword from described keyword query;
Determine the scene boundary of the scene relevant to described keyword query, the described scene of video comprises the described frame with described high keyword relevance score; And
Select described scene as the video of described selection.
14. methods according to claim 13, also comprise:
Based on the described keyword relevance score between the keyword in the frame of the video in results set and described keyword query, among the multiple videos in described results set, rank is carried out to the video of described selection.
15. methods according to claim 13, also comprise:
Based on the described keyword relevance score between the keyword in the frame of described video and described keyword query, present the Relevance scores of the video of described selection.
16. 1 kinds of computer-implemented devices can searching for video index for creating, described device is performed by computer system and comprises:
For receiving the module marking training dataset, the described training dataset that marked comprises one group of training video and describes one or more keyword of audio-visual content of each frame in multiple frames of each described training video;
For extracting the module of one or more features of the described audio-visual content of each frame in the described multiple frame characterizing each described training video;
For storing the module of the relevance between most representative proper vector and the most found keyword, comprising:
For generating the module of correlation function set, each correlation function represents the strength of association between one of one of described most representative proper vector and the most found described keyword; And
For generating the feature-keyword matrix norm block of described most representative maps feature vectors to the most found described keyword;
For the model using described correlation function set and described feature-keyword matrix to carry out training machine study, with learn the described audio-visual content of each frame in described multiple frame of each described training video of extracted sign described feature and describe each frame in described multiple frame of each described training video described audio-visual content described one or more keyword between the module of correlativity; And
For based on the feature of frame of the multiple index videos in video database and the model of described machine learning, generate described one or more keyword is mapped to the described multiple index videos in described video database described frame described in can search for the module of video index.
17. devices according to claim 16, wherein said audio-visual content comprises the external appearance characteristic of the picture frame of described training video.
18. devices according to claim 16, wherein said audio-visual content comprises the audio content of the voice data in described training video.
19. devices according to claim 16, the module wherein for the described feature extracting the described audio-visual content characterizing described training video comprises:
For each frame in described multiple frame of each described training video being divided into the module of multiple burst, wherein each burst is overlapping at least in part;
For generating the module of the proper vector for each described burst; And
For applying clustering algorithm to have marked the module of the multiple most representative proper vector in training data described in determining.
20. devices according to claim 16, wherein comprise for generating described feature-keyword matrix norm block:
Feature described in initialization-keyword matrix norm block is carried out for the entry by utilizing initial weight to fill described feature-keyword matrix;
For the module of the negative training video of selecting the positive training video that is associated with the first keyword and do not associate with the second keyword;
For extracting the feature of described positive training video and negative training video to obtain the module of positive proper vector and negative proper vector;
For using described feature-keyword matrix to described positive proper vector application conversion to obtain the module for the first keyword score of described positive training video;
For using described feature-keyword matrix to described negative proper vector application conversion to obtain the module for the second keyword score of described negative training video;
Described keyword score for determining described positive training video does not at least exceed the module of a threshold value than the described keyword score for described negative training video; And
For at least not exceeding described threshold value than the described keyword score for described negative training video in response to the described keyword score for described positive training video, regulate the module of the described weight in described feature-keyword matrix.
21. devices according to claim 16, the module wherein for generating described video index comprises:
For the module of sampling to the frame of the video in described video database;
For calculating the module of the first eigenvector of the first sample frame of described video, described first eigenvector represents the content of described first sample frame;
For by the models applying of described machine learning in described first eigenvector to generate the module of the keyword relevance score between described first sample frame and the keyword of selection; And
For described keyword relevance score and described first sample frame being stored in explicitly the module in described video index.
22. devices according to claim 16, the module wherein for generating described video index comprises:
For the module of sampling to the scene of the video in described video database;
For calculating the module of the first eigenvector of the first sampling scene of described video, described first eigenvector represents the content of described first sampling scene;
For by the models applying of described machine learning in described first eigenvector to generate the module of keyword relevance score between described first sampling scene and the keyword of selection; And
The module in described video index is stored in explicitly for scene of described keyword relevance score and described first being sampled.
23. 1 kinds for presenting the computer-implemented device of video index result, described device is performed by computer system and comprises:
For selecting the module of video;
Explain index for using video selects frame as the module of the representative of the content of described video from described video, described video explains the keyword relevance score between the same keyword be associated with each frame in described multiple frame of each video in described multiple video of each frame in multiple frames of each video in the multiple video of index stores, described keyword relevance score represent the audio-visual content of the frame of the described video of sign extracted feature and describe described frame described audio-visual content keyword between the relative intensity of correlativity, the described relative intensity of described correlativity based on machine learning model and determine, the model of described machine learning has used correlation function set and feature-keyword matrix and has trained, strength of association between the most representative proper vector of wherein said correlation function set representative and the most found keyword, and described feature-keyword matrix is by described most representative maps feature vectors extremely the most found described keyword, and
For providing the frame of selection as the module of the thumbnail of described video.
24. devices according to claim 23, wherein for selecting described frame to comprise as the module of the representative of the content of described video from described video:
For selecting the module representing the keyword expecting video content;
Index is explained to determine the module of the keyword relevance score between the frame of described video and the keyword of selection for accessing described video; And
Select that there is the module with the frame of the highest rank keyword relevance score of the keyword of described selection for explaining index according to described video.
25. devices according to claim 24, wherein for selecting the module of the described keyword representing described expectation video content to comprise for using the title of described video as the module of the keyword of described selection.
26. devices according to claim 24, wherein for selecting the module of the described keyword representing described expectation video content to comprise for using keyword query as the module of the keyword of described selection.
27. devices according to claim 23, wherein for selecting the module of described video to comprise:
For receiving the module of keyword query from user; And
For selecting the module with the video of the content relevant to described keyword query from video database.
28. devices according to claim 27, wherein for selecting the module of the described video with the content relevant to described keyword query to comprise:
For determining that there is the module with the frame of video of the high keyword relevance score of the keyword from described keyword query;
For determining the module of the scene boundary of the scene relevant to described keyword query, the described scene of video comprises the described frame with described high keyword relevance score; And
For selecting described scene as the module of the video of described selection.
29. devices according to claim 27, also comprise:
For based on the described keyword relevance score between the keyword in the frame of the video in results set and described keyword query, among the multiple videos in described results set, the video of described selection is carried out to the module of rank.
30. devices according to claim 27, also comprise:
For based on the described keyword relevance score between the keyword in the frame of described video and described keyword query, present the module of the Relevance scores of the video of described selection.
31. 1 kinds of video mandatory systems, for finding and presenting the video relevant to keyword query, described system comprises:
Front-end server, be configured for and receive keyword query from user and present results set, described results set comprises the thumbnail image of the associated video with the audio-visual content relevant to described keyword query and the described audio-visual content representing described associated video;
Video explains index, store the mapping between each frame in multiple frames of keyword and multiple index video, described mapping is exported according to the model of machine learning, the model of described machine learning is by relevant to the keyword of the described audio-visual content of the described frame of the described training video of description for the feature of the described audio-visual content of each frame in multiple frames of each training video in the multiple training video of sign, the model of described machine learning uses correlation function set and feature-keyword matrix and trains, strength of association between the most representative proper vector of wherein said correlation function set representative and the most found keyword, and described feature-keyword matrix is by described most representative maps feature vectors extremely the most found described keyword, and
Video search engine, be configured for the described video of access and explain index, to determine the described associated video with the described audio-visual content relevant to described keyword, and determine the described thumbnail image of the described audio-visual content representing described associated video.
32. systems according to claim 31, also comprise:
Video database, store described multiple index videos that described video search engine can be searched for, wherein explain in index at described video and the frame of each index video in described multiple index video is indexed, described frame to be mapped to the keyword describing its audio-visual content.
33. systems according to claim 31, also comprise:
Video explains engine, is configured for the described mapping between the frame of each index video in the described multiple index videos using the learning characteristic-keyword models obtained by machine learning to determine in video database and the keyword describing its audio-visual content.
34. systems according to claim 33, wherein said video is explained engine and is comprised:
Video sampling module, is configured for and samples to the frame of the given video from described video database;
Characteristic extracting module, is configured for the proper vector generating and represent each described sample frame of described given video; And
Frame explains module, be configured for and described learning characteristic-keyword models is applied to described proper vector, to determine the keyword score of each described sample frame of described given video, described keyword score and relevant sample frame are indexed to described video explicitly and are explained in index.
35. systems according to claim 31, also comprise:
Study engine, be configured for the model from marking the study of training dataset Learning machine, the described training dataset that marked comprises one group of training video and describes one or more keyword of audio-visual content of frame of described training video, and the model representation of described machine learning characterizes the correlativity between the feature of the audio-visual content of the frame of multiple training video and the described keyword describing described audio-visual content.
36. systems according to claim 35, wherein said study engine comprises:
Characteristic extracting module, is configured for generating feature data set, and described characteristic data set comprises for the described multiple most representative proper vector having marked training dataset;
Keyword learning module, is configured for and generates keyword data collection, has marked multiple that training data concentrates and the most often occur keyword described in described keyword data collection comprises; And
Associate learning module, is suitable for generating keyword-characteristic model, the relevance between the described keyword that the described proper vector of described keyword-characteristic model mappings characteristics data centralization and described keyword data are concentrated.
37. systems according to claim 36, wherein said study engine also comprises:
The logical module of point, be configured for by following the tracks of user search queries on media research website and automatic acquisition for the described label having marked training data, and by observing Search Results and the unselected Search Results of described user that user selects and the label learnt for training video.
38. 1 kinds for presenting the method for advertisement, described method is performed by computing machine and comprises:
The video player based on web is used to play the video of selection;
At the present frame of the playback monitoring video of the video of described selection;
Use the described present frame of video to visit video and explain index to determine one or more keyword associated with described present frame, described video explains the keyword that the frame of the video of described selection is mapped to the audio-visual content describing described frame by index, and wherein said video is explained index and generated as follows:
Receive and mark training dataset, the described training dataset that marked comprises one group of training video and describes one or more keyword of audio-visual content of each frame in multiple frames of each described training video,
Extract one or more features of the described audio-visual content of each frame in the described multiple frame characterizing each described training video,
Relevance between the most representative proper vector of following storage and the most found keyword:
Generate correlation function set, each correlation function represents the strength of association between one of one of described most representative proper vector and the most found described keyword, and
Generate the feature-keyword matrix of described most representative maps feature vectors to the most found described keyword,
Described correlation function set and described feature-keyword matrix is used to carry out the model of training machine study, with learn the described audio-visual content of each frame in described multiple frame of each described training video of sign of extracting described feature and describe each frame in described multiple frame of each described training video described audio-visual content described one or more keyword between correlativity, and
Based on the feature of frame and the model of described machine learning of the multiple index videos in video database, the described video generating the described frame described one or more keyword being mapped to the described multiple index videos in described video database explains index;
One or more keyword described is used to visit advertising database to select the advertisement associated with one or more keyword described; And
There is provided described advertisement for playing at the playback of described present frame.
39. according to method according to claim 38, and wherein said video is explained index, according to the model of machine learning, the frame of the video of described selection is mapped to one or more keyword.
40. 1 kinds of computer-implemented methods can searching for video index for creating, comprising:
The search inquiry comprising one or more keyword is received at the computing equipment place comprising one or more processor;
Generate as follows can search for video index at described computing equipment place:
Receive and mark training dataset, the described training dataset that marked comprises one group of training video and describes one or more keyword of audio-visual content of each frame in multiple frames of each described training video,
Extract one or more features of the described audio-visual content of each frame in the described multiple frame characterizing each described training video,
Relevance between the most representative proper vector of following storage and the most found keyword:
Generate correlation function set, each correlation function represents the strength of association between one of one of described most representative proper vector and the most found described keyword, and
Generate the feature-keyword matrix of described most representative maps feature vectors to the most found described keyword,
Described correlation function set and described feature-keyword matrix is used to carry out the model of training machine study, with learn the described audio-visual content of each frame in described multiple frame of each described training video of sign of extracting described feature and describe each frame in described multiple frame of each described training video described audio-visual content described one or more keyword between correlativity, and
Based on the feature of frame and the model of described machine learning of the multiple index videos in video database, generate described one or more keyword is mapped to the described multiple index videos in described video database described frame described in can search for video index;
At described computing equipment place based on described search inquiry with describedly search for video index and select one or more video from video database, each video in described one or more video comprises multiple frame and representative thumbnails image, each particular frame is associated with one or more particular keywords, and wherein said selection comprises one of described one or more particular keywords of the particular video frequency in each Keywords matching of described search inquiry to described video database further to obtain described one or more video;
The described representative thumbnails image of each video in described one or more video is exported at described computing equipment place;
Receive at described computing equipment place the selection of one of the described representative thumbnails image of described one or more video to obtain the video of selection; And
Export the video of described selection in response to described selection at described computing equipment place.
41. computer-implemented methods according to claim 40, are also included in described computing equipment place and analyze each video in described video database to determine described one or more keyword of each frame of each video.
42. computer-implemented methods according to claim 41, each video wherein analyzed in described video database is also included in described computing equipment place and extracts one or more feature from each frame of each video.
43. computer-implemented methods according to claim 42, each video wherein analyzed in described video database is also included in described computing equipment place and uses the one or more of each frame of each video of model analysis of machine learning to extract feature.
44. computer-implemented methods according to claim 43, also being included in described computing equipment place uses one or more machine learning techniques and multiple training video to generate the model of described machine learning, and wherein said training pattern is relevant to the sample frame of described multiple training video by the feature of described multiple training video.
45. computer-implemented methods according to claim 40, wherein said search inquiry and described selection each be received via the user of network from another computing equipment at described computing equipment place, and the video of wherein said selection is exported to the described user of another computing equipment described via described network by described computing equipment.
46. computer-implemented methods according to claim 45, wherein said computing equipment is the server being configured to store described video database.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US12/546,436 | 2009-08-24 | ||
US12/546,436 US20110047163A1 (en) | 2009-08-24 | 2009-08-24 | Relevance-Based Image Selection |
PCT/US2010/045909 WO2011025701A1 (en) | 2009-08-24 | 2010-08-18 | Relevance-based image selection |
Publications (2)
Publication Number | Publication Date |
---|---|
CN102549603A CN102549603A (en) | 2012-07-04 |
CN102549603B true CN102549603B (en) | 2015-05-06 |
Family
ID=43606147
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201080042760.9A Active CN102549603B (en) | 2009-08-24 | 2010-08-18 | Relevance-based image selection |
Country Status (6)
Country | Link |
---|---|
US (5) | US20110047163A1 (en) |
EP (2) | EP2471026B1 (en) |
CN (1) | CN102549603B (en) |
AU (3) | AU2010286797A1 (en) |
CA (1) | CA2771593C (en) |
WO (1) | WO2011025701A1 (en) |
Families Citing this family (139)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20200257596A1 (en) | 2005-12-19 | 2020-08-13 | Commvault Systems, Inc. | Systems and methods of unified reconstruction in storage systems |
WO2009144698A1 (en) * | 2008-05-26 | 2009-12-03 | Kenshoo Ltd. | A system for finding website invitation cueing keywords and for attribute-based generation of invitation-cueing instructions |
US8111923B2 (en) * | 2008-08-14 | 2012-02-07 | Xerox Corporation | System and method for object class localization and semantic class based image segmentation |
US9477667B2 (en) * | 2010-01-14 | 2016-10-25 | Mobdub, Llc | Crowdsourced multi-media data relationships |
JP5805665B2 (en) | 2010-01-22 | 2015-11-04 | トムソン ライセンシングＴｈｏｍｓｏｎ Ｌｉｃｅｎｓｉｎｇ | Data pruning for video compression using Example-based super-resolution |
KR101789845B1 (en) | 2010-01-22 | 2017-11-20 | 톰슨 라이센싱 | Methods and apparatus for sampling-based super resolution video encoding and decoding |
US20110218994A1 (en) * | 2010-03-05 | 2011-09-08 | International Business Machines Corporation | Keyword automation of video content |
US20110225133A1 (en) * | 2010-03-09 | 2011-09-15 | Microsoft Corporation | Metadata-aware search engine |
CN102193946A (en) * | 2010-03-18 | 2011-09-21 | 株式会社理光 | Method and system for adding tags into media file |
US8910046B2 (en) | 2010-07-15 | 2014-12-09 | Apple Inc. | Media-editing application with anchored timeline |
US20130170564A1 (en) | 2010-09-10 | 2013-07-04 | Thomson Licensing | Encoding of a picture in a video sequence by example-based data pruning using intra-frame patch similarity |
US9544598B2 (en) | 2010-09-10 | 2017-01-10 | Thomson Licensing | Methods and apparatus for pruning decision optimization in example-based data pruning compression |
KR101838320B1 (en) * | 2010-09-10 | 2018-03-13 | 톰슨 라이센싱 | Video decoding using example - based data pruning |
US8971651B2 (en) | 2010-11-08 | 2015-03-03 | Sony Corporation | Videolens media engine |
US8923607B1 (en) * | 2010-12-08 | 2014-12-30 | Google Inc. | Learning sports highlights using event detection |
US20130334300A1 (en) * | 2011-01-03 | 2013-12-19 | Curt Evans | Text-synchronized media utilization and manipulation based on an embedded barcode |
US8954477B2 (en) | 2011-01-28 | 2015-02-10 | Apple Inc. | Data structures for a media-editing application |
US8838680B1 (en) | 2011-02-08 | 2014-09-16 | Google Inc. | Buffer objects for web-based configurable pipeline media processing |
US11747972B2 (en) | 2011-02-16 | 2023-09-05 | Apple Inc. | Media-editing application with novel editing tools |
US9997196B2 (en) | 2011-02-16 | 2018-06-12 | Apple Inc. | Retiming media presentations |
FR2973134B1 (en) * | 2011-03-23 | 2015-09-11 | Xilopix | METHOD FOR REFINING THE RESULTS OF A SEARCH IN A DATABASE |
US8938393B2 (en) | 2011-06-28 | 2015-01-20 | Sony Corporation | Extended videolens media engine for audio recognition |
US8879835B2 (en) * | 2011-08-26 | 2014-11-04 | Adobe Systems Incorporated | Fast adaptive edge-aware matting |
US20130073961A1 (en) * | 2011-09-20 | 2013-03-21 | Giovanni Agnoli | Media Editing Application for Assigning Roles to Media Content |
US9536564B2 (en) | 2011-09-20 | 2017-01-03 | Apple Inc. | Role-facilitated editing operations |
US20130073960A1 (en) | 2011-09-20 | 2013-03-21 | Aaron M. Eppolito | Audio meters and parameter controls |
US9075825B2 (en) * | 2011-09-26 | 2015-07-07 | The University Of Kansas | System and methods of integrating visual features with textual features for image searching |
US9098533B2 (en) * | 2011-10-03 | 2015-08-04 | Microsoft Technology Licensing, Llc | Voice directed context sensitive visual search |
US8649613B1 (en) * | 2011-11-03 | 2014-02-11 | Google Inc. | Multiple-instance-learning-based video classification |
CN102542066B (en) * | 2011-11-11 | 2014-04-09 | 冉阳 | Video clustering method, ordering method, video searching method and corresponding devices |
EP3557442A1 (en) * | 2011-12-28 | 2019-10-23 | INTEL Corporation | Real-time natural language processing of datastreams |
US9846696B2 (en) * | 2012-02-29 | 2017-12-19 | Telefonaktiebolaget Lm Ericsson (Publ) | Apparatus and methods for indexing multimedia content |
US9146993B1 (en) * | 2012-03-16 | 2015-09-29 | Google, Inc. | Content keyword identification |
US9633015B2 (en) | 2012-07-26 | 2017-04-25 | Telefonaktiebolaget Lm Ericsson (Publ) | Apparatus and methods for user generated content indexing |
US9292552B2 (en) * | 2012-07-26 | 2016-03-22 | Telefonaktiebolaget L M Ericsson (Publ) | Apparatus, methods, and computer program products for adaptive multimedia content indexing |
US8935246B2 (en) * | 2012-08-08 | 2015-01-13 | Google Inc. | Identifying textual terms in response to a visual query |
CN103714063B (en) * | 2012-09-28 | 2017-08-04 | 国际商业机器公司 | Data analysing method and its system |
US9172740B1 (en) | 2013-01-15 | 2015-10-27 | Google Inc. | Adjustable buffer remote access |
US9311692B1 (en) | 2013-01-25 | 2016-04-12 | Google Inc. | Scalable buffer remote access |
US9225979B1 (en) | 2013-01-30 | 2015-12-29 | Google Inc. | Remote access encoding |
US10594763B2 (en) | 2013-03-15 | 2020-03-17 | adRise, Inc. | Platform-independent content generation for thin client applications |
US10887421B2 (en) | 2013-03-15 | 2021-01-05 | Tubi, Inc. | Relevant secondary-device content generation based on associated internet protocol addressing |
US10356461B2 (en) | 2013-03-15 | 2019-07-16 | adRise, Inc. | Adaptive multi-device content generation based on associated internet protocol addressing |
WO2014185834A1 (en) | 2013-05-14 | 2014-11-20 | Telefonaktiebolaget L M Ericsson (Publ) | Search engine for textual content and non-textual content |
US9521189B2 (en) * | 2013-08-21 | 2016-12-13 | Google Inc. | Providing contextual data for selected link units |
US10311038B2 (en) | 2013-08-29 | 2019-06-04 | Telefonaktiebolaget Lm Ericsson (Publ) | Methods, computer program, computer program product and indexing systems for indexing or updating index |
EP3039811B1 (en) | 2013-08-29 | 2021-05-05 | Telefonaktiebolaget LM Ericsson (publ) | Method, content owner device, computer program, and computer program product for distributing content items to authorized users |
US10108617B2 (en) * | 2013-10-30 | 2018-10-23 | Texas Instruments Incorporated | Using audio cues to improve object retrieval in video |
CN104995639B (en) | 2013-10-30 | 2018-11-06 | 宇龙计算机通信科技(深圳)有限公司 | terminal and video file management method |
US9189834B2 (en) | 2013-11-14 | 2015-11-17 | Adobe Systems Incorporated | Adaptive denoising with internal and external patches |
US9286540B2 (en) * | 2013-11-20 | 2016-03-15 | Adobe Systems Incorporated | Fast dense patch search and quantization |
US9635108B2 (en) | 2014-01-25 | 2017-04-25 | Q Technologies Inc. | Systems and methods for content sharing using uniquely generated idenifiers |
US9728230B2 (en) * | 2014-02-20 | 2017-08-08 | International Business Machines Corporation | Techniques to bias video thumbnail selection using frequently viewed segments |
WO2015127385A1 (en) * | 2014-02-24 | 2015-08-27 | Lyve Minds, Inc. | Automatic generation of compilation videos |
US9779775B2 (en) | 2014-02-24 | 2017-10-03 | Lyve Minds, Inc. | Automatic generation of compilation videos from an original video based on metadata associated with the original video |
US9767540B2 (en) | 2014-05-16 | 2017-09-19 | Adobe Systems Incorporated | Patch partitions and image processing |
EP3192273A4 (en) * | 2014-09-08 | 2018-05-23 | Google LLC | Selecting and presenting representative frames for video previews |
US10318575B2 (en) * | 2014-11-14 | 2019-06-11 | Zorroa Corporation | Systems and methods of building and using an image catalog |
US10074102B2 (en) * | 2014-11-26 | 2018-09-11 | Adobe Systems Incorporated | Providing alternate words to aid in drafting effective social media posts |
US9847101B2 (en) * | 2014-12-19 | 2017-12-19 | Oracle International Corporation | Video storytelling based on conditions determined from a business object |
US9842390B2 (en) * | 2015-02-06 | 2017-12-12 | International Business Machines Corporation | Automatic ground truth generation for medical image collections |
US10095786B2 (en) * | 2015-04-09 | 2018-10-09 | Oath Inc. | Topical based media content summarization system and method |
CN104881798A (en) * | 2015-06-05 | 2015-09-02 | 北京京东尚科信息技术有限公司 | Device and method for personalized search based on commodity image features |
US20160378863A1 (en) * | 2015-06-24 | 2016-12-29 | Google Inc. | Selecting representative video frames for videos |
US10062015B2 (en) | 2015-06-25 | 2018-08-28 | The Nielsen Company (Us), Llc | Methods and apparatus for identifying objects depicted in a video using extracted video frames in combination with a reverse image search engine |
US10242033B2 (en) * | 2015-07-07 | 2019-03-26 | Adobe Inc. | Extrapolative search techniques |
US10140880B2 (en) * | 2015-07-10 | 2018-11-27 | Fujitsu Limited | Ranking of segments of learning materials |
EP3326083A4 (en) * | 2015-07-23 | 2019-02-06 | Wizr | Video processing |
US9779304B2 (en) * | 2015-08-11 | 2017-10-03 | Google Inc. | Feature-based video annotation |
US9858967B1 (en) * | 2015-09-09 | 2018-01-02 | A9.Com, Inc. | Section identification in video content |
CN106708876B (en) * | 2015-11-16 | 2020-04-21 | 任子行网络技术股份有限公司 | Similar video retrieval method and system based on Lucene |
CN105488183B (en) * | 2015-12-01 | 2018-12-04 | 北京邮电大学世纪学院 | The method and apparatus for excavating rock cave mural painting spatial and temporal association in rock cave mural painting group |
US10592750B1 (en) * | 2015-12-21 | 2020-03-17 | Amazon Technlogies, Inc. | Video rule engine |
US10381022B1 (en) * | 2015-12-23 | 2019-08-13 | Google Llc | Audio classifier |
US10678853B2 (en) * | 2015-12-30 | 2020-06-09 | International Business Machines Corporation | Aligning visual content to search term queries |
KR20170098079A (en) * | 2016-02-19 | 2017-08-29 | 삼성전자주식회사 | Electronic device method for video recording in electronic device |
US10891019B2 (en) * | 2016-02-29 | 2021-01-12 | Huawei Technologies Co., Ltd. | Dynamic thumbnail selection for search results |
CN105787087B (en) * | 2016-03-14 | 2019-09-17 | 腾讯科技（深圳）有限公司 | Costar the matching process and device worked together in video |
US9858340B1 (en) | 2016-04-11 | 2018-01-02 | Digital Reasoning Systems, Inc. | Systems and methods for queryable graph representations of videos |
US10289642B2 (en) * | 2016-06-06 | 2019-05-14 | Baidu Usa Llc | Method and system for matching images with content using whitelists and blacklists in response to a search query |
US10008218B2 (en) | 2016-08-03 | 2018-06-26 | Dolby Laboratories Licensing Corporation | Blind bandwidth extension using K-means and a support vector machine |
US10467257B2 (en) | 2016-08-09 | 2019-11-05 | Zorroa Corporation | Hierarchical search folders for a document repository |
US10311112B2 (en) | 2016-08-09 | 2019-06-04 | Zorroa Corporation | Linearized search of visual media |
US10664514B2 (en) | 2016-09-06 | 2020-05-26 | Zorroa Corporation | Media search processing using partial schemas |
US10645142B2 (en) * | 2016-09-20 | 2020-05-05 | Facebook, Inc. | Video keyframes display on online social networks |
US10606887B2 (en) * | 2016-09-23 | 2020-03-31 | Adobe Inc. | Providing relevant video scenes in response to a video search query |
US11580589B2 (en) | 2016-10-11 | 2023-02-14 | Ebay Inc. | System, method, and medium to select a product title |
CN106776890B (en) * | 2016-11-29 | 2021-06-29 | 北京小米移动软件有限公司 | Method and device for adjusting video playing progress |
US10685047B1 (en) | 2016-12-08 | 2020-06-16 | Townsend Street Labs, Inc. | Request processing system |
WO2018111242A1 (en) * | 2016-12-13 | 2018-06-21 | Google Llc | Compensation pulses for qubit readout |
US10430661B2 (en) * | 2016-12-20 | 2019-10-01 | Adobe Inc. | Generating a compact video feature representation in a digital medium environment |
US10606814B2 (en) * | 2017-01-18 | 2020-03-31 | Microsoft Technology Licensing, Llc | Computer-aided tracking of physical entities |
US10216766B2 (en) * | 2017-03-20 | 2019-02-26 | Adobe Inc. | Large-scale image tagging using image-to-topic embedding |
CN107025275B (en) * | 2017-03-21 | 2019-11-15 | 腾讯科技（深圳）有限公司 | Video searching method and device |
US10268897B2 (en) | 2017-03-24 | 2019-04-23 | International Business Machines Corporation | Determining most representative still image of a video for specific user |
US10540444B2 (en) * | 2017-06-20 | 2020-01-21 | The Boeing Company | Text mining a dataset of electronic documents to discover terms of interest |
CN107609461A (en) * | 2017-07-19 | 2018-01-19 | 阿里巴巴集团控股有限公司 | The training method of model, the determination method, apparatus of data similarity and equipment |
US10970334B2 (en) * | 2017-07-24 | 2021-04-06 | International Business Machines Corporation | Navigating video scenes using cognitive insights |
CN109598527A (en) * | 2017-09-30 | 2019-04-09 | 北京国双科技有限公司 | Analysis of advertising results method and device |
US10740394B2 (en) * | 2018-01-18 | 2020-08-11 | Oath Inc. | Machine-in-the-loop, image-to-video computer vision bootstrapping |
US10372991B1 (en) * | 2018-04-03 | 2019-08-06 | Google Llc | Systems and methods that leverage deep learning to selectively store audiovisual content |
US10965985B2 (en) | 2018-05-21 | 2021-03-30 | Hisense Visual Technology Co., Ltd. | Display apparatus with intelligent user interface |
US20190354608A1 (en) * | 2018-05-21 | 2019-11-21 | Qingdao Hisense Electronics Co., Ltd. | Display apparatus with intelligent user interface |
US11507619B2 (en) | 2018-05-21 | 2022-11-22 | Hisense Visual Technology Co., Ltd. | Display apparatus with intelligent user interface |
CN110795597A (en) * | 2018-07-17 | 2020-02-14 | 上海智臻智能网络科技股份有限公司 | Video keyword determination method, video retrieval method, video keyword determination device, video retrieval device, storage medium and terminal |
CN109089133B (en) * | 2018-08-07 | 2020-08-11 | 北京市商汤科技开发有限公司 | Video processing method and device, electronic equipment and storage medium |
US11386284B2 (en) * | 2018-09-12 | 2022-07-12 | Avigilon Corporation | System and method for improving speed of similarity based searches |
WO2020060538A1 (en) * | 2018-09-18 | 2020-03-26 | Google Llc | Methods and systems for processing imagery |
WO2020065839A1 (en) * | 2018-09-27 | 2020-04-02 | 株式会社オプティム | Object situation assessment system, object situation assessment method, and program |
CN109376145B (en) * | 2018-11-19 | 2022-05-06 | 深圳Tcl新技术有限公司 | Method and device for establishing movie and television dialogue database and storage medium |
WO2020106451A1 (en) * | 2018-11-20 | 2020-05-28 | Google Llc | Methods, systems, and media for modifying search results based on search query risk |
US11250039B1 (en) * | 2018-12-06 | 2022-02-15 | A9.Com, Inc. | Extreme multi-label classification |
US11803556B1 (en) * | 2018-12-10 | 2023-10-31 | Townsend Street Labs, Inc. | System for handling workplace queries using online learning to rank |
CN109933688A (en) * | 2019-02-13 | 2019-06-25 | 北京百度网讯科技有限公司 | Determine the method, apparatus, equipment and computer storage medium of video labeling information |
CN111800671B (en) * | 2019-04-08 | 2022-08-12 | 百度时代网络技术（北京）有限公司 | Method and apparatus for aligning paragraphs and video |
CN110110140A (en) * | 2019-04-19 | 2019-08-09 | 天津大学 | Video summarization method based on attention expansion coding and decoding network |
CN110362694A (en) * | 2019-07-05 | 2019-10-22 | 武汉莱博信息技术有限公司 | Data in literature search method, equipment and readable storage medium storing program for executing based on artificial intelligence |
CN110381368A (en) * | 2019-07-11 | 2019-10-25 | 北京字节跳动网络技术有限公司 | Video cover generation method, device and electronic equipment |
US11531707B1 (en) | 2019-09-26 | 2022-12-20 | Okta, Inc. | Personalized search based on account attributes |
US11500927B2 (en) | 2019-10-03 | 2022-11-15 | Adobe Inc. | Adaptive search results for multimedia search queries |
CA3144489A1 (en) * | 2020-02-27 | 2021-09-02 | Rovi Guides, Inc. | Systems and methods for generating dynamic annotations |
US11606613B2 (en) | 2020-02-27 | 2023-03-14 | Rovi Guides, Inc. | Systems and methods for generating dynamic annotations |
US11128910B1 (en) | 2020-02-27 | 2021-09-21 | Rovi Guides, Inc. | Systems and methods for generating dynamic annotations |
US11243995B2 (en) | 2020-02-28 | 2022-02-08 | Lomotif Private Limited | Method for atomically tracking and storing video segments in multi-segment audio-video compositions |
CN111432282B (en) * | 2020-04-01 | 2022-01-04 | 腾讯科技（深圳）有限公司 | Video recommendation method and device |
WO2021251953A1 (en) * | 2020-06-09 | 2021-12-16 | Google Llc | Generation of interactive audio tracks from visual content |
US11645733B2 (en) | 2020-06-16 | 2023-05-09 | Bank Of America Corporation | System and method for providing artificial intelligence architectures to people with disabilities |
US11829413B1 (en) * | 2020-09-23 | 2023-11-28 | Amazon Technologies, Inc. | Temporal localization of mature content in long-form videos using only video-level labels |
US20220114361A1 (en) * | 2020-10-14 | 2022-04-14 | Adobe Inc. | Multi-word concept tagging for images using short text decoder |
CN112399262B (en) * | 2020-10-30 | 2024-02-06 | 深圳Tcl新技术有限公司 | Video searching method, television and storage medium |
CN112559800B (en) * | 2020-12-17 | 2023-11-14 | 北京百度网讯科技有限公司 | Method, apparatus, electronic device, medium and product for processing video |
CN112733779B (en) * | 2021-01-19 | 2023-04-07 | 三星电子（中国）研发中心 | Video poster display method and system based on artificial intelligence |
US11532111B1 (en) * | 2021-06-10 | 2022-12-20 | Amazon Technologies, Inc. | Systems and methods for generating comic books from video and images |
US11962817B2 (en) | 2021-06-21 | 2024-04-16 | Tubi, Inc. | Machine learning techniques for advanced frequency management |
CN113378781B (en) * | 2021-06-30 | 2022-08-05 | 北京百度网讯科技有限公司 | Training method and device of video feature extraction model and electronic equipment |
CN117643061A (en) * | 2021-07-23 | 2024-03-01 | 聚好看科技股份有限公司 | Display equipment and media asset content recommendation method |
CN113901263B (en) * | 2021-09-30 | 2022-08-19 | 宿迁硅基智能科技有限公司 | Label generation method and device for video material |
CN116150428A (en) * | 2021-11-16 | 2023-05-23 | 腾讯科技（深圳）有限公司 | Video tag acquisition method and device, electronic equipment and storage medium |
US11893032B2 (en) * | 2022-01-11 | 2024-02-06 | International Business Machines Corporation | Measuring relevance of datasets to a data science model |
Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6574378B1 (en) * | 1999-01-22 | 2003-06-03 | Kent Ridge Digital Labs | Method and apparatus for indexing and retrieving images using visual keywords |
CN101071439A (en) * | 2007-05-24 | 2007-11-14 | 北京交通大学 | Interactive video searching method based on multi-view angle |
Family Cites Families (33)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7263659B2 (en) * | 1998-09-09 | 2007-08-28 | Ricoh Company, Ltd. | Paper-based interface for multimedia information |
WO2000040011A1 (en) * | 1998-12-28 | 2000-07-06 | Sony Corporation | Method for editing video information and editing device |
KR20010108159A (en) * | 1999-01-29 | 2001-12-07 | 다니구찌 이찌로오, 기타오카 다카시 | Method of image feature encoding and method of image search |
US20040125877A1 (en) * | 2000-07-17 | 2004-07-01 | Shin-Fu Chang | Method and system for indexing and content-based adaptive streaming of digital video content |
US20020159642A1 (en) * | 2001-03-14 | 2002-10-31 | Whitney Paul D. | Feature selection and feature set construction |
US20030126136A1 (en) * | 2001-06-22 | 2003-07-03 | Nosa Omoigui | System and method for knowledge retrieval, management, delivery and presentation |
JP4062908B2 (en) * | 2001-11-21 | 2008-03-19 | 株式会社日立製作所 | Server device and image display device |
US6865226B2 (en) * | 2001-12-05 | 2005-03-08 | Mitsubishi Electric Research Laboratories, Inc. | Structural analysis of videos with hidden markov models and dynamic programming |
US20030196206A1 (en) * | 2002-04-15 | 2003-10-16 | Shusman Chad W. | Method and apparatus for internet-based interactive programming |
US8682097B2 (en) * | 2006-02-14 | 2014-03-25 | DigitalOptics Corporation Europe Limited | Digital image enhancement with reference images |
US20080193016A1 (en) * | 2004-02-06 | 2008-08-14 | Agency For Science, Technology And Research | Automatic Video Event Detection and Indexing |
US8156427B2 (en) * | 2005-08-23 | 2012-04-10 | Ricoh Co. Ltd. | User interface for mixed media reality |
US7639387B2 (en) * | 2005-08-23 | 2009-12-29 | Ricoh Co., Ltd. | Authoring tools using a mixed media environment |
US20060179051A1 (en) * | 2005-02-09 | 2006-08-10 | Battelle Memorial Institute | Methods and apparatus for steering the analyses of collections of documents |
US8572088B2 (en) * | 2005-10-21 | 2013-10-29 | Microsoft Corporation | Automated rich presentation of a semantic topic |
US7680853B2 (en) * | 2006-04-10 | 2010-03-16 | Microsoft Corporation | Clickable snippets in audio/video search results |
US20070255755A1 (en) * | 2006-05-01 | 2007-11-01 | Yahoo! Inc. | Video search engine using joint categorization of video clips and queries based on multiple modalities |
WO2008018064A2 (en) * | 2006-08-07 | 2008-02-14 | Yeda Research And Development Co. Ltd. | Data similarity and importance using local and global evidence scores |
US20080120291A1 (en) * | 2006-11-20 | 2008-05-22 | Rexee, Inc. | Computer Program Implementing A Weight-Based Search |
US7840076B2 (en) * | 2006-11-22 | 2010-11-23 | Intel Corporation | Methods and apparatus for retrieving images from a large collection of images |
US20080154889A1 (en) * | 2006-12-22 | 2008-06-26 | Pfeiffer Silvia | Video searching engine and methods |
KR100856027B1 (en) * | 2007-01-09 | 2008-09-03 | 주식회사 태그스토리 | System for providing copyright-verified video data and method thereof |
US8806320B1 (en) * | 2008-07-28 | 2014-08-12 | Cut2It, Inc. | System and method for dynamic and automatic synchronization and manipulation of real-time and on-line streaming media |
US20090263014A1 (en) * | 2008-04-17 | 2009-10-22 | Yahoo! Inc. | Content fingerprinting for video and/or image |
US20090327236A1 (en) * | 2008-06-27 | 2009-12-31 | Microsoft Corporation | Visual query suggestions |
US9390169B2 (en) * | 2008-06-28 | 2016-07-12 | Apple Inc. | Annotation of movies |
US20100191689A1 (en) * | 2009-01-27 | 2010-07-29 | Google Inc. | Video content analysis for automatic demographics recognition of users and videos |
US8559720B2 (en) * | 2009-03-30 | 2013-10-15 | Thomson Licensing S.A. | Using a video processing and text extraction method to identify video segments of interest |
US9916538B2 (en) * | 2012-09-15 | 2018-03-13 | Z Advanced Computing, Inc. | Method and system for feature detection |
US8873813B2 (en) * | 2012-09-17 | 2014-10-28 | Z Advanced Computing, Inc. | Application of Z-webs and Z-factors to analytics, search engine, learning, recognition, natural language, and other utilities |
US8983192B2 (en) * | 2011-11-04 | 2015-03-17 | Google Inc. | High-confidence labeling of video volumes in a video sharing service |
US9070046B2 (en) * | 2012-10-17 | 2015-06-30 | Microsoft Technology Licensing, Llc | Learning-based image webpage index selection |
US9779304B2 (en) * | 2015-08-11 | 2017-10-03 | Google Inc. | Feature-based video annotation |
-
2009
- 2009-08-24 US US12/546,436 patent/US20110047163A1/en not_active Abandoned
-
2010
- 2010-08-18 EP EP10812505.5A patent/EP2471026B1/en active Active
- 2010-08-18 CN CN201080042760.9A patent/CN102549603B/en active Active
- 2010-08-18 EP EP18161198.9A patent/EP3352104A1/en active Pending
- 2010-08-18 WO PCT/US2010/045909 patent/WO2011025701A1/en active Application Filing
- 2010-08-18 AU AU2010286797A patent/AU2010286797A1/en not_active Abandoned
- 2010-08-18 CA CA2771593A patent/CA2771593C/en active Active
-
2015
- 2015-04-15 US US14/687,116 patent/US10614124B2/en active Active
-
2016
- 2016-04-04 AU AU2016202074A patent/AU2016202074B2/en active Active
-
2018
- 2018-03-06 AU AU2018201624A patent/AU2018201624B2/en active Active
- 2018-08-10 US US16/100,414 patent/US11017025B2/en active Active
-
2021
- 2021-05-24 US US17/328,442 patent/US11693902B2/en active Active
-
2023
- 2023-05-22 US US18/321,225 patent/US20230306057A1/en active Pending
Patent Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6574378B1 (en) * | 1999-01-22 | 2003-06-03 | Kent Ridge Digital Labs | Method and apparatus for indexing and retrieving images using visual keywords |
CN101071439A (en) * | 2007-05-24 | 2007-11-14 | 北京交通大学 | Interactive video searching method based on multi-view angle |
Also Published As
Publication number | Publication date |
---|---|
AU2016202074B2 (en) | 2017-12-07 |
CN102549603A (en) | 2012-07-04 |
AU2016202074A1 (en) | 2016-04-28 |
EP3352104A1 (en) | 2018-07-25 |
US20110047163A1 (en) | 2011-02-24 |
EP2471026A1 (en) | 2012-07-04 |
US11017025B2 (en) | 2021-05-25 |
WO2011025701A1 (en) | 2011-03-03 |
CA2771593C (en) | 2018-10-30 |
US20230306057A1 (en) | 2023-09-28 |
US20210349944A1 (en) | 2021-11-11 |
CA2771593A1 (en) | 2011-03-03 |
EP2471026A4 (en) | 2014-03-12 |
US20150220543A1 (en) | 2015-08-06 |
EP2471026B1 (en) | 2018-04-11 |
AU2018201624B2 (en) | 2019-11-21 |
US10614124B2 (en) | 2020-04-07 |
US11693902B2 (en) | 2023-07-04 |
AU2018201624A1 (en) | 2018-03-29 |
AU2010286797A1 (en) | 2012-03-15 |
US20180349391A1 (en) | 2018-12-06 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN102549603B (en) | Relevance-based image selection | |
US11949964B2 (en) | Generating action tags for digital videos | |
CN106560810B (en) | Searching using specific attributes found in images | |
US8396286B1 (en) | Learning concepts for video annotation | |
AU2011326430B2 (en) | Learning tags for video annotation using latent subtags | |
WO2017070656A1 (en) | Video content retrieval system | |
CN114342353A (en) | Method and system for video segmentation | |
Roy et al. | Deep metric and hash-code learning for content-based retrieval of remote sensing images | |
CN113704507B (en) | Data processing method, computer device and readable storage medium | |
Ulges et al. | A system that learns to tag videos by watching youtube | |
Chen et al. | Name-face association with web facial image supervision | |
CN116955707A (en) | Content tag determination method, device, equipment, medium and program product | |
Liu et al. | On the automatic online collection of training data for visual event modeling | |
Jadhav et al. | Marking Celebrity Faces Utilizing Annotation by Mining Weakly Labeled Facial Images | |
Agarkar et al. | Marking Human Labeled Training Facial Images Searching and Utilizing Annotations as a Part of Features for Videos | |
CN114880572A (en) | Intelligent news client recommendation system | |
Feng et al. | BUPT & ORANGELABS (OrangeBJ) at TRECVID 2014: Instance Search. | |
Thangarasu et al. | Diversity in image retrieval based on inferring user image search goals |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
C06 | Publication | ||
PB01 | Publication | ||
C10 | Entry into substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
C14 | Grant of patent or utility model | ||
GR01 | Patent grant | ||
CP01 | Change in the name or title of a patent holder | ||
CP01 | Change in the name or title of a patent holder |
Address after: American CaliforniaPatentee after: Google limited liability companyAddress before: American CaliforniaPatentee before: Google Inc. |