KR20200011425A - Hand skeleton learning, lifting, and noise removal from 2D images - Google Patents
Hand skeleton learning, lifting, and noise removal from 2D images Download PDFInfo
- Publication number
- KR20200011425A KR20200011425A KR1020197034132A KR20197034132A KR20200011425A KR 20200011425 A KR20200011425 A KR 20200011425A KR 1020197034132 A KR1020197034132 A KR 1020197034132A KR 20197034132 A KR20197034132 A KR 20197034132A KR 20200011425 A KR20200011425 A KR 20200011425A
- Authority
- KR
- South Korea
- Prior art keywords
- thumb
- keypoints
- palm
- hand
- fingers
- Prior art date
Links
- 210000003811 finger Anatomy 0.000 claims abstract description 150
- 210000003813 thumb Anatomy 0.000 claims abstract description 89
- 210000000707 wrist Anatomy 0.000 claims abstract description 24
- 210000000245 forearm Anatomy 0.000 claims abstract description 6
- 238000000034 method Methods 0.000 claims description 59
- 238000012549 training Methods 0.000 claims description 26
- 230000006870 function Effects 0.000 claims description 8
- 210000001503 joint Anatomy 0.000 abstract 1
- 239000013598 vector Substances 0.000 description 15
- 238000012545 processing Methods 0.000 description 9
- 230000008901 benefit Effects 0.000 description 8
- 238000010586 diagram Methods 0.000 description 5
- 230000003993 interaction Effects 0.000 description 4
- 230000008569 process Effects 0.000 description 4
- 230000003190 augmentative effect Effects 0.000 description 3
- 238000006073 displacement reaction Methods 0.000 description 3
- 210000004247 hand Anatomy 0.000 description 3
- 238000012986 modification Methods 0.000 description 3
- 230000004048 modification Effects 0.000 description 3
- 230000003287 optical effect Effects 0.000 description 3
- 230000000694 effects Effects 0.000 description 2
- 238000003384 imaging method Methods 0.000 description 2
- 238000012804 iterative process Methods 0.000 description 2
- 241000699670 Mus sp. Species 0.000 description 1
- 230000002730 additional effect Effects 0.000 description 1
- 238000004891 communication Methods 0.000 description 1
- 238000010276 construction Methods 0.000 description 1
- 238000013461 design Methods 0.000 description 1
- 238000001514 detection method Methods 0.000 description 1
- 239000011521 glass Substances 0.000 description 1
- 210000004932 little finger Anatomy 0.000 description 1
- 238000012544 monitoring process Methods 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 238000013519 translation Methods 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/017—Gesture based interaction, e.g. based on a set of recognized hand gestures
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/28—Determining representative reference patterns, e.g. by averaging or distorting; Generating dictionaries
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/03—Arrangements for converting the position or the displacement of a member into a coded form
- G06F3/0304—Detection arrangements using opto-electronic means
-
- G06K9/00362—
-
- G06K9/6255—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T5/00—Image enhancement or restoration
- G06T5/001—Image restoration
- G06T5/002—Denoising; Smoothing
-
- G06T5/70—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/70—Determining position or orientation of objects or cameras
- G06T7/73—Determining position or orientation of objects or cameras using feature-based methods
- G06T7/75—Determining position or orientation of objects or cameras using feature-based methods involving models
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V40/00—Recognition of biometric, human-related or animal-related patterns in image or video data
- G06V40/10—Human or animal bodies, e.g. vehicle occupants or pedestrians; Body parts, e.g. hands
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V40/00—Recognition of biometric, human-related or animal-related patterns in image or video data
- G06V40/10—Human or animal bodies, e.g. vehicle occupants or pedestrians; Body parts, e.g. hands
- G06V40/107—Static hand or arm
-
- G06K2209/055—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/20—Special algorithmic details
- G06T2207/20081—Training; Learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/30—Subject of image; Context of image processing
- G06T2207/30196—Human being; Person
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/60—Analysis of geometric attributes
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V2201/00—Indexing scheme relating to image or video recognition or understanding
- G06V2201/03—Recognition of patterns in medical or anatomical images
- G06V2201/033—Recognition of patterns in medical or anatomical images of skeletal patterns
Abstract
프로세서는 카메라(215)에 의해 캡처된 2 차원 이미지(100)에서 손의 키포인트(115, 120, 125, 130)를 식별한다. 키포인트의 위치의 함수로서 손의 잠재적인 포즈를 나타내는 룩업 테이블(LUT)(230)에 액세스하도록 키포인트의 위치를 사용하여 손의 3 차원 포즈가 결정된다. 일부 실시예에서, 키포인트는 손가락들과 엄지의 끝들의 위치들, 손가락들과 엄지의 마디뼈들을 연결하는 관절들, 손가락들과 엄지가 손바닥에 부착하는 지점을 나타내는 손바닥 너클들, 손이 팔뚝에 부착되는 지점을 나타내는 손목 위치를 포함한다. LUT의 일부 실시예는 대응하는 손가락 포즈 평면에서 손가락들과 엄지의 2D 좌표들을 나타낸다. The processor identifies keypoints 115, 120, 125, 130 of the hand in the two-dimensional image 100 captured by camera 215. The three-dimensional pose of the hand is determined using the location of the keypoint to access a lookup table (LUT) 230 representing the potential pose of the hand as a function of the location of the keypoint. In some embodiments, the keypoint is located at the ends of the fingers and thumb, joints connecting the knuckles of the fingers and thumb, palm knuckles indicating the point where the fingers and thumb attach to the palm, and the hand at the forearm. Wrist position indicating the point of attachment. Some embodiments of the LUT represent the 2D coordinates of the fingers and thumb in the corresponding finger pose plane.
Description
본 발명은 이미지 프로세싱에 관한 발명이다. The present invention relates to image processing.
3D 공간에서 인체 부위, 특히 사람의 손의 위치는 수많은 어플리케이션에서 유용한 도구이다. 가상 현실(VR), 증강 현실(AR) 또는 혼합 현실(MR) 어플리케이션은 사용자의 손에 대한 표현을 사용하여 한 손으로는 메뉴를 그리고 다른 한 손으로는 메뉴의 요소들을 선택함으로써, 가상 객체와의 상호 작용을 촉진하고, 가상 메모리에서 아이템을 선택하고, 사용자의 가상 손에 객체를 배치하고, 사용자 인터페이스를 제공할 수 있다. 제스처 상호 작용은 구글 홈(Google Home) 또는 네스트(Nest)와 같은 자동화된 가정 비서(automated household assistants)와의 상호작용의 추가적인 양식(modality)을 부가한다. 보안 또는 모니터링 시스템은 사람의 손 또는 기타 신체 부위에 대한 3D 표현을 사용하여 이상 상황을 감지하고 신호를 보낼 수 있다. 일반적으로, 인간의 손 또는 다른 신체 부위의 위치에 대한 3D 표현은 음성 통신, 터치 스크린, 키보드, 컴퓨터 마우스 등과 같은 기존의 양식 대신에 또는 이에 부가하여 사용되는 추가적인 상호작용 양식 또는 검출 양식을 제공한다. The location of the human body, especially the human hand, in 3D space is a useful tool in many applications. Virtual reality (VR), augmented reality (AR), or mixed reality (MR) applications use a representation of a user's hand to select a menu with one hand and elements of the menu with the other, May facilitate interaction of the user, select items in virtual memory, place objects in the user's virtual hand, and provide a user interface. Gesture interaction adds an additional modality of interaction with automated household assistants such as Google Home or Nest. Security or monitoring systems can use 3D representations of human hands or other body parts to detect and signal anomalies. In general, the 3D representation of the location of a human hand or other body part provides an additional form of interaction or detection that is used in place of or in addition to existing forms such as voice communications, touch screens, keyboards, computer mice, and the like. .
그러나 컴퓨터 시스템이 항상 3D 이미징 디바이스를 구현하는 것은 아니다. 예를 들어, 스마트 폰, 태블릿 컴퓨터 및 헤드 마운트 디바이스(HMD)와 같은 디바이스는 일반적으로 2 차원(2D) 카메라와 같은 경량의 이미징 디바이스를 구현한다.However, computer systems do not always implement 3D imaging devices. For example, devices such as smart phones, tablet computers, and head mounted devices (HMD) generally implement lightweight imaging devices such as two-dimensional (2D) cameras.
첨부된 도면들을 참조함으로써, 본 발명이 더 잘 이해될 수 있으며, 본 발명의 수많은 특징 및 장점들이 당업자에게 명백해질 것이다. 상이한 도면들에서 동일한 참조 부호의 사용은 유사하거나 동일한 요소들을 나타낸다.
도 1은 일부 실시예에 따른 손의 2 차원(2D) 이미지이다.
도 2는 일부 실시예에 따라 손의 2D 이미지를 획득하고 2D 이미지에 기초하여 손의 3D 포즈를 생성하도록 구성되는 프로세싱 시스템의 블록도이다.
도 3은 일부 실시예에 따른 손의 골격 모델의 일부를 나타내는 손바닥 삼각형 및 엄지 삼각형을 도시한다.
도 4는 일부 실시예에 따른 대응하는 손가락 포즈 평면에서의 손가락 포즈를 도시한다.
도 5는 일부 실시예에 따른 손가락 포즈 평면에서 손가락의 골격 모델을 도시한다.
도 6은 일부 실시예에 따른 손바닥 너클의 상대 위치 및 손가락의 끝(tip)에 기초하여 손가락 포즈 평면에서 손가락의 2D 좌표를 찾는데 사용되는 룩업 테이블(LUT)을 도시한다.
도 7은 일부 실시예에 따라 도 6에 도시된 원으로 표시된 손가락 끝 및 손바닥 너클의 상대적인 위치를 갖는 손가락의 2D 좌표를 도시한다.
도 8은 일부 실시예에 따라 손가락 끝과 손바닥 너클의 상대적인 위치를 손가락의 2D 좌표에 매핑하는 LUT를 구성하는 방법의 흐름도이다.
도 9는 일부 실시예에 따른 손의 2D 이미지로부터 손의 3D 포즈를 리프팅하는 방법의 흐름도이다.
도 10은 일부 실시예에 따른 손의 2D 이미지로부터 열거된 3D 키포인트들을 반복적으로 디-디노이징하는 것의 예시이다.
도 11은 일부 실시예에 따른 손의 2D 이미지로부터 추출된 키포인트를 디노이징하는 방법의 흐름도이다. By referring to the accompanying drawings, the present invention may be better understood, and numerous features and advantages of the present invention will become apparent to those skilled in the art. The use of the same reference numerals in different drawings represents similar or identical elements.
1 is a two dimensional (2D) image of a hand in accordance with some embodiments.
2 is a block diagram of a processing system configured to acquire a 2D image of a hand and generate a 3D pose of a hand based on the 2D image in accordance with some embodiments.
3 illustrates a palm triangle and a thumb triangle representing a portion of a skeletal model of a hand in accordance with some embodiments.
4 illustrates a finger pose in a corresponding finger pose plane in accordance with some embodiments.
5 illustrates a skeletal model of a finger in the finger pose plane in accordance with some embodiments.
FIG. 6 illustrates a lookup table (LUT) used to find a 2D coordinate of a finger in the finger pose plane based on the relative position of the palm knuckle and the tip of the finger in accordance with some embodiments.
FIG. 7 illustrates 2D coordinates of a finger with the relative positions of the fingertip and palm knuckles indicated by the circles shown in FIG. 6, in accordance with some embodiments.
8 is a flowchart of a method of configuring a LUT that maps the relative positions of a fingertip and a palm knuckle to 2D coordinates of a finger in accordance with some embodiments.
9 is a flowchart of a method of lifting a 3D pose of a hand from a 2D image of a hand in accordance with some embodiments.
10 is an illustration of repeatedly de-dinozing 3D keypoints listed from a 2D image of a hand in accordance with some embodiments.
11 is a flowchart of a method for denoising keypoints extracted from a 2D image of a hand in accordance with some embodiments.
손의 3D 표현(본원에서 골격 모델(skeleton model)이라고 함)은 키포인트들의 위치들의 함수로서 손의 잠재적인 3D 포즈들을 나타내는 룩업 테이블들에 액세스하도록, 2D 이미지에서 손의 키포인트(keypoints)들을 식별하고 그리고 키포인트들의 위치들을 이용하여 손의 3D 포즈 및 위치를 판별함으로써, 손의 2D 이미지로부터 실시간으로 생성된다. 일부 실시 양상들에서, 키포인트들은 손가락들 및 엄지의 끝(tip, 이하, "끝" 또는 "팁" 이라 함)의 위치, 손가락들 및 엄지의 마디뼈(phalanxes)를 연결하는 관절들(joints), 각 손가락 및 엄지가 손바닥에 부착하는 지점을 나타내는 손바닥 너클(palm knuckles), 및 손이 사용자의 팔뚝에 부착되는 지점을 나타내는 손목의 위치를 포함한다. 룩업 테이블은, 손가락 또는 엄지의 해당 손바닥 너클에 대한 손가락(또는 엄지) 끝의 위치에 대한 함수로서 대응 손가락 포즈 평면(finger pose plane)에서 각 손가락(또는 엄지)의 2D 좌표를 나타내는 손가락 포즈 룩업 테이블을 포함한다. 손가락들 및 엄지의 마디뼈들(phalanxes)의 길이들은 손에 대한 트레이닝 이미지들의 세트로부터 결정된다. 마디뼈들을 연결하는 관절들의 움직임 범위에 대한 해부학적 제한 및 상기 길이들에 기초하여 손가락 포즈 룩업 테이블들이 생성된다. 손바닥은 손바닥 삼각형과 엄지 삼각형으로 표현되며, 이들은 해당 정점들의 세트들로 정의된다. 손바닥 삼각형은 손목 위치에 정점(vertex)이 있으며, 이는 손가락들의 손바닥 너클들을 포함하는 삼각형의 반대쪽에 있다. 엄지 삼각형은 손목 위치에 있는 정점, 엄지의 손바닥 너클에 있는 정점, 집게 손가락(index finger)의 손바닥 너클에 있는 정점을 갖는다. 손바닥 삼각형과 엄지 삼각형을 정의하는 파라미터들은 또한, 트레이닝 이미지들의 세트로부터 결정된다. The 3D representation of the hand (also referred to herein as the skeleton model) identifies keypoints of the hand in the 2D image to access lookup tables that represent potential 3D poses of the hand as a function of the locations of the keypoints. And using the positions of the keypoints to determine the 3D pose and position of the hand, which is generated in real time from the 2D image of the hand. In some embodiments, the keypoints are the joints that connect the fingers and the position of the tip of the thumb (hereinafter referred to as “tip” or “tip”), the fingers and the phalanxes of the thumb. Palm knuckles, which indicate where each finger and thumb are attached to the palm, and wrist positions, which indicate where the hand is attached to the user's forearm. The lookup table is a finger pose lookup table that represents the 2D coordinates of each finger (or thumb) in the corresponding finger pose plane as a function of the position of the tip of the finger (or thumb) relative to the corresponding palm knuckle of the finger or thumb. It includes. The lengths of the phalanxes of the fingers and thumb are determined from a set of training images for the hand. Finger pose lookup tables are generated based on the anatomical limits and ranges of motion of the joints connecting the knuckles. Palms are represented by palm triangles and thumb triangles, which are defined as sets of corresponding vertices. The palm triangle has a vertex at the wrist position, which is opposite the triangle containing the palm knuckles of the fingers. The thumb triangle has a vertex at the wrist position, a vertex at the palm knuckle of the thumb, and a vertex at the palm knuckle of the index finger. Parameters defining the palm and thumb triangles are also determined from the set of training images.
동작에서, 손의 골격 모델은 손가락 포즈 룩업 테이블로부터 결정된 2D 좌표들 및 손바닥 삼각형과 엄지 삼각형의 방위들을 이용하여 손의 2D 이미지로부터 결정된다. 엄지를 포함한 손가락들은 해당 포즈 평면들 내에서 움직임이 해부학적으로 제한되는바, 이들은 손바닥 삼각형과 엄지 삼각형에 대해서 각각 고정된 방향을 갖는다. 예를 들어, 2D 이미지에서 집게 손가락은 집게 손가락의 손바닥 너클을 집게 손가락의 손가락 끝과 연결하는 손가락 포즈 평면에 놓이도록 제한된다. 손가락 포즈 평면에서 손가락의 2D 좌표는 손가락 끝과 손바닥 너클의 상대적인 위치를 이용하여, 해당 손가락 포즈 룩업 테이블에 액세스함으로써 결정된다. 그런 다음 손바닥 삼각형의 방향에 따라 2D 좌표를 회전시킴으로써 손가락의 3D 포즈가 결정된다. 엄지 손가락의 3D 포즈는 엄지 삼각형의 방향에 기초하여 엄지 손가락의 2D 포즈(손가락 포즈 룩업 테이블에서 결정됨)를 회전시킴으로써 결정된다. 일부 실시예에서, 손의 3D 포즈는 위에서 논의된 바와 같이 키포인트들의 초기 추정치들에 기초하여 2D 이미지로부터 손의 3D 골격 모델을 결정함으로써 2D 이미지에서 키포인트들의 노이즈 값(noisy values)을 사용하여 결정된다. 골격 모델에 의해서 표시되는 키포인트들의 3D 위치들은, 오리지널 2D 키포인트들을 2D 이미지와 관련된 소실점(vanishing pointss)에 연결하는 라인을 따르는 이미지 평면 내의 키포인트들의 3D 위치들의 투영들에 기초하여 수정된다. 그런 다음 키포인트들의 수정된 3D 위치들을 이용하여 위에서 설명한 바와 같이 골격 모델을 수정한다. 이러한 프로세스는 수렴하도록 반복된다. In operation, the skeletal model of the hand is determined from the 2D image of the hand using the 2D coordinates determined from the finger pose lookup table and the orientations of the palm triangle and thumb triangle. Fingers, including the thumb, are anatomically restricted in their pose planes, which have a fixed direction with respect to the palm and thumb triangles, respectively. For example, in a 2D image the index finger is constrained to lie in the finger pose plane that connects the palm knuckle of the index finger with the finger tip of the index finger. The 2D coordinates of the finger in the finger pose plane are determined by accessing the corresponding finger pose lookup table using the relative position of the fingertip and the palm knuckle. The 3D pose of the finger is then determined by rotating the 2D coordinates along the direction of the palm triangle. The 3D pose of the thumb is determined by rotating the 2D pose of the thumb (as determined in the finger pose lookup table) based on the direction of the thumb triangle. In some embodiments, the 3D pose of the hand is determined using the noise values of the keypoints in the 2D image by determining a 3D skeletal model of the hand from the 2D image based on the initial estimates of the keypoints as discussed above. . The 3D positions of the keypoints represented by the skeletal model are modified based on projections of the 3D positions of the keypoints in the image plane along a line connecting the original 2D keypoints to vanishing points associated with the 2D image. Then modify the skeletal model as described above using the modified 3D positions of the keypoints. This process is repeated to converge.
본 명세서에 개시된 기술의 일부 실시예는 상이한 데이터 세트들에 대해 검증되었고, 비교를 하기 전에 결과들이 그라운드 트루 데이터(Ground truth data)에 정렬되지 않는 경우 80 % 이상으로 정확하게 식별된 키포인트들(및 일부 경우에는 98 % 이상으로)을 달성한다. 비교 전에 결과들을 그라운드 트루 데이터에 정렬하면, 정확한 키포인트들의 백분율이 향상된다. Some embodiments of the techniques disclosed herein have been validated on different data sets, and keypoints (and some accurately identified) that are more than 80% accurate if results are not aligned with ground truth data prior to comparison. In the case of 98% or more). Sorting the results into ground true data before the comparison improves the percentage of correct keypoints.
도 1은 일부 실시예에 따른 손(105)의 2 차원(2D) 이미지(100)이다. 손(105)은 골격 모델(110)로 표현되며, 골격 모델(110)은 손(105)의 손가락, 엄지 손가락 및 손바닥을 상호연결된 키포인트들의 세트로서 모델링한다. 예시된 실시예에서, 키포인트들은, 손가락들과 엄지의 끝들(115)(명확성을 위해 하나만 참조 번호로 표시됨), 손가락들과 엄지의 마디뼈들(125)을 연결하는 관절들(120)(명확성을 위해 하나만 참조 번호로 표시됨), 각각의 손가락과 엄지 손가락이 손바닥에 부착되는 지점을 나타내는 손바닥 너클들(130)(명확성을 위해 하나만 참조 번호로 표시됨), 및 손이 사용자의 팔뚝에 부착되는 지점을 나타내는 손목 위치(135)를 포함한다.1 is a two-dimensional (2D)
도 2는 일부 실시예에 따라 손(205)의 2D 이미지를 획득하고, 2D 이미지에 기초하여 손의 3D 포즈를 생성하도록 구성된 프로세싱 시스템(200)의 블록도이다. 2D 이미지로부터 손(205)의 3D 포즈를 생성하는 것은, 2D 이미지로부터 손(205)의 3D 포즈를 "리프팅(lifting)"하는 것이라 지칭된다. 예시된 실시예에서, 손(205)의 3D 포즈는 도 1에 도시된 골격 모델(110)과 같은 골격 모델(210)에 의해 표현된다. 명확성을 위해, 다음의 논의는 신체 부분의 일례로서 손(205)을 사용한다. 그러나, 본 명세서에서 논의된 기술의 일부 실시예는 대응하는 2D 이미지로부터 다른 신체 부분의 3D 포즈를 리프팅하는데 동일하게 적용된다. 예를 들어, 프로세싱 시스템(200)은 대응 신체 부위의 2D 이미지로부터 발, 팔, 다리, 머리, 다른 신체 부위 또는 이들의 조합의 3D 포즈를 리프팅할 수 있다. 2 is a block diagram of a
프로세싱 시스템(200)은 카메라(215)와 같은 이미지 획득 디바이스를 포함한다. 카메라(215)를 구현하기 위해 사용되는 이미지 획득 디바이스의 일례들은, 가상 현실 또는 증강 현실 어플리케이션을 수행하기 위해 모바일 전화 또는 태블릿 컴퓨터에 구현된 카메라와 같은 RGB(Red-Green-Blue) 카메라, 하나 이상의 깊이 센서를 사용한 깊이 추정 기능이 있는 RGB 카메라, 스테레오 어안 카메라를 사용하여 6°의 자유도를 제공하는 올인원 가상 현실 디바이스와 같은 그레이 스케일 카메라, 야간 촬상기(nighttime imager) 또는 깊이 센서 상의 촬상기 등을 포함할 수 있다. 일부 실시예들에서, 카메라(215)는 작은 폼 팩터로 구현되고 소량의 전력을 소비하는 경량 RGB 카메라이다. 따라서, 경량 RGB 카메라는 증강 현실 안경에서 구현하는데 유용하다. 카메라(215)의 일부 실시예는 장면 내의 움직임을 나타내는 이미지들의 시퀀스를 캡처하는 비디오 카메라로서 구현된다.The
카메라(215)는 손(205)의 2D 이미지를 획득하고 메모리(220)에 2D 이미지를 나타내는 정보를 저장한다. 프로세서(225)는 메모리(220)로부터 2D 이미지를 나타내는 정보에 액세스하고 2D 이미지에 대한 트레이닝, 리프팅 및 노이즈 제거(denoising)를 포함하는 동작을 수행할 수 있다. 트레이닝 단계는 손(205)의 트레이닝 이미지들을 이용하여 하나 이상의 룩업 테이블들(LUTs)(230)을 생성하는 것을 포함할 수 있다. 예를 들어, 엄지 손가락을 포함한 손가락들의 마디뼈들의 길이들이 손(205)의 트레이닝 이미지들의 세트로부터 결정된다. 상기 길이들 및 마디뼈들에 연결되는 관절의 운동 범위에 대한 해부학적 제한에 기초하여 룩업 테이블(LUT)(230)가 생성되며, 그리고 메모리(220)에 저장된다. 또한, 손바닥 삼각형 및 엄지 삼각형을 정의하는 정점들과 같은 파라미터들이 트레이닝 이미지 세트로부터 결정되어 메모리(220)에 저장된다. The
리프팅 단계에서, 프로세서(225)는 2D 이미지에서 손(205)의 키포인트를 식별함으로써 손의 2D 이미지로부터 실시간으로 골격 모델(210)을 생성한다. 프로세서는 LUT(230)로부터 손가락들의 2D 좌표에 액세스하도록 키포인트들의 위치들을 사용하여 손(205)의 3D 포즈 및 위치를 결정하며, LUT(230)는 각 손가락들의 2D 좌표들을 손가락 끝과 손바닥 너클의 상대적인 위치에 대한 함수로서 저장한다. 프로세서(225)는 손바닥 삼각형의 방향에 기초하여 2D 좌표를 회전시킴으로써 손가락들의 3D 포즈를 결정한다. 프로세서(225)는 엄지 삼각형의 방향에 기초하여 엄지 손가락의 2D 포즈(손가락 포즈 룩업 테이블로부터 결정됨)를 회전시킴으로써 엄지 손가락의 3D 포즈를 결정한다. In the lifting phase,
프로세서(225)의 일부 실시예는 손(205)의 2D 이미지로부터 추출된 키포인트들의 잡음이 있는(noisy) 값들에 대한 노이즈 제거(denoising)를 수행하도록 구성된다. 노이즈 제거 단계는 반복 프로세스이다. 초기에, 프로세서(225)는 잡음이 있는(noisy) 키포인트들의 초기 추정에 기초하여 2D 이미지로부터 손의 3D 골격 모델을 결정함으로써 2D 이미지에서 손의 3D 포즈를 결정한다. 다음으로, 프로세서(225)는 잡음이 있는 오리지널 키포인트를 2D 이미지와 관련된 소실점(vanishing point)에 연결하는 라인을 따른 이미지 평면으로의 키포인트의 3D 위치의 투영에 기초하여, 골격 모델에 의해 표시되는 키포인트의 3D 위치를 수정한다. 소실점은 카메라(215)를 특징짓는 파라미터에 기초하여 결정된다. 프로세서(225)는 골격 모델에 의해 표시되는 키포인트의 수정된 3D 위치에 기초하여 잡음이 있는 키포인트의 값을 업데이트하고, 이러한 프로세스는 잡음이 있는 키포인트가 대응하는 수렴 기준을 충족할 때까지 반복된다.Some embodiments of the
도 3은 일부 실시예에 따라 손의 골격 모델의 일부를 나타내는 손바닥 삼각형(300) 및 엄지 삼각형(305)을 도시한다. 손바닥 삼각형(300) 및 엄지 삼각형(305)은 도 1에 도시된 골격 모델(110) 및도 2에 도시된 골격 모델(210)의 일부를 나타낸다.3 illustrates
손바닥 삼각형(300)은 손의 손목 위치(310) 및 손바닥 너클들(311, 312, 313, 314)(집합적으로 "손바닥 너클(311-314)"로 지칭됨)에서의 정점들에 의해 정의된다. 손바닥 삼각형(300)을 포함하는 평면은 단위 벡터(315, 316)에 의해 정의되며, 이는 각각 파라미터 uI , uL 로 표시된다. 손목 위치(310)로부터 검지의 손바닥 너클(311)까지의 거리(320)는 파라미터 I 로 표시되고, 손목 위치(310)로부터 새끼 손가락의 손목 너클(314)까지의 거리(325)는 파라미터 L 로 표시된다. 따라서, 손목의 위치(310)에 대한 손바닥 너클(311)의 위치는, 방향 uI 및 크기 I 를 갖는 벡터에 의해 주어진다. 손목의 위치(310)에 대한 손바닥 너클(314)의 위치는, 방향 uL 및 크기 L 를 갖는 벡터에 의해 주어진다.
가운데 손가락의 손바닥 너클(312)의 위치는 다음과 같이 정의된다:The position of the
여기서, λm 은 가운데 손가락과 관련된 파라미터이다. 약지(ring finger)의 손바닥 너클(313)의 위치는 다음과 같이 정의된다:Is the parameter associated with the middle finger. The position of the
여기서, λr 은 약지와 관련된 파라미터이다. 손바닥 삼각형(300)을 정의하는 파라미터들의 값들은 손이 일련의 트레이닝 포즈로 유지되는 동안 손의 2D 이미지를 사용하여 학습된다. Here, lambda r is a parameter related to ring finger. The values of the parameters defining
엄지 삼각형(305)은 손목 위치(310), 검지의 손바닥 너클(311) 및 엄지의 손바닥 너클(330)에 있는 정점들에 의해서 정의된다. 엄지 삼각형(305)을 포함하는 평면은 단위 벡터(315, 335)에 의해 정의되며, 이는 파라미터 uI , uT 로 각각 표시된다. 본 명세서에서 논의된 바와 같이, 손목 위치(310)로부터 검지의 손바닥 너클(311)까지의 거리(320)는 파라미터 I에 의해 표현된다. 손목 위치(310)로부터 엄지의 손바닥 너클(330)까지의 거리(340)는 파라미터 T에 의해 표현된다. 따라서, 손목의 위치(310)에 대한 손바닥 너클(330)의 위치는 방향 uT 및 크기 T를 갖는 벡터에 의해 주어진다. 엄지 삼각형(305)은 엄지 삼각형(305)이 압축가능하고 제로 영역을 가질 수 있다는 점에서 손바닥 삼각형(300)과 다르다. 엄지 삼각형(305)을 정의하는 파라미터의 값은 손이 일련의 트레이닝 포즈로 유지되는 동안 손의 2D 이미지를 사용하여 학습된다. The
도 4는 일부 실시예에 따라 해당 손가락 포즈 평면(405)에서의 손가락 포즈(400)를 도시한다. 손가락 포즈 평면(405)은 평면(410)에 대하여 대략적으로 고정된 방향을 유지하도록 해부학적으로 제한된다. 따라서, 손가락의 움직임은 대략적으로 해당 손가락 포즈 평면(405) 내에 놓이도록 제한을 받는다. 도 4에 도시된 손가락 포즈 평면(405)은 검지, 중지, 약지, 새끼 손가락 또는 엄지의 운동 평면의 소정 실시예들을 나타낸다. 만일, 손가락 포즈 평면(405)이 검지, 중지, 약지 또는 새끼 손가락의 운동 평면을 나타낸다면, 평면(410)은 도 3에서 도시된 손바닥 삼각형(300)과 같은 손바닥 삼각형을 포함하는 평면이다. 만일, 손가락 포즈 평면(405)이 엄지의 운동 평면을 나타낸다면, 평면(410)은 도 3에 도시된 엄지 삼각형(305)과 같은 손의 엄지 삼각형을 포함하는 평면이다. 4 illustrates a
손가락은 손가락의 골격 모델(415)로 표현된다. 골격 모델(415)은 손가락의 손바닥 너클(425)에 대한 손가락 끝(420)의 위치에 의해 특징지워질 수 있다. 후술하는 바와 같이, 손바닥 너클(425)에 대한 손가락 끝(420)의 상대적 위치는 손가락 포즈 평면(405)에서 손가락의 골격 모델(415)의 위치를 정의하는 2D 좌표를 결정한다.The finger is represented by a
평면(410)의 방향은 벡터(430)에 의해 결정되며, 벡터(430)는 평면(410)에 수직인(normal) 벡터로 정의된다. 벡터(430)에 의해 정의된 방향은 손의 2D 이미지에서 손바닥 삼각형(또는 엄지 삼각형)의 치수들과, 도 3을 참조하여 위에서 논의된 치수들과 같은 손바닥 삼각형(또는 엄지 삼각형)의 트레이닝된 표현의 치수들을 비교함으로써 결정된다. 손가락 포즈 평면(405)의 방향은 벡터(435)에 의해 결정되며, 벡터(435)는 손가락 포즈 평면(405)에서 벡터(430)에 수직인 벡터로 정의된다. 2D 이미지에서 손가락의 3D 포즈는, 벡터들(430, 435)에 의해 결정된 방향들에 기초하여 손가락의 골격 모델(415)을 회전시킴으로써 생성된다. The direction of
도 5는 일부 실시예에 따른 손가락 포즈 평면에서 손가락의 골격 모델(500)을 도시한다. 골격 모델(500)은 도 4에 도시된 골격 모델(415)의 일부 실시예를 나타낸다. 골격 모델(500)은 또한 도 1에 도시된 골격 모델(110) 및 도 2에 도시된 골격 모델(210)의 일부 실시예의 일부를 나타낸다. 골격 모델(500) 손바닥 너클(501), 제 1 관절(joint) 너클(502), 제 2 관절 너클(503) 및 손가락 끝(504)을 포함한다. 골격 모델(500)은 손바닥 너클(501)과 제 1 관절 너클(502) 사이의 마디뼈(510)의 길이, 제 1 관절 너클(502)과 제 2 관절 너클(503) 사이의 마디뼈(515)의 길이, 및 제 2 관절 너클(503)과 손가락 끝(504) 사이의 마디뼈(520)의 길이에 의해 특징지워진다. 5 illustrates a
마디뼈들(510, 515, 520)의 길이 값들은 트레이닝 포즈들의 세트에서 유지되는 손의 트레이닝 이미지들의 세트로부터 학습된다. 일부 실시예에서, 마디뼈들(510, 515, 520)의 길이 값들은 트레이닝 이미지들의 세트로부터 손바닥 너클들, 관절 너클들 및 손가락 끝들에 대응하는 키포인트들을 추출함으로써 학습된다. 키포인트들은 특이점에 대해 필터링되는바, 이는 평균(median) 또는 평균 절대 편차(median absolute deviation)와 같은 기법을 사용하여 특이점 키포인트(outlier keypoints)를 찾아내어 폐기하기 위한 것이다. 다음으로, 2차 프로그래밍(quadratic programming)을 포함하는 기법이 사용되어, 트레이닝 이미지들의 세트 내의 키포인트들의 변위들(displacements)에 상기 길이 값들을 끼워맞춘다(fit). The length values of the
손바닥 너클(501)에 대한 손가락 끝(504)의 위치는 손바닥 너클(501), 제 1 관절 너클(502) 및 제 2 관절 너클(503)에서의 일련의 각도들에 의해 결정된다. 제 1 각도(525)는 마디뼈(510)과 손바닥 삼각형(또는 엄지 삼각형)의 평면(점선 530으로 표시된 바와 같은) 사이의 각도를 나타낸다. 제 2 각도(535)는 마디뼈(510)와 마디뼈(515) 사이의 각도를 나타낸다. 제 3 각도(540)는 마디뼈(515)와 마디뼈(520) 사이의 각도를 나타낸다. 상기 각도들(525, 535, 540)의 범위는 한정된 값들의 세트만으로 해부학적으로 제한되며, 이는 서로 다른 손들에 대하여 사소한 변동이 있을 뿐 실질적으로 동일하다. 예를 들어, 상기 각도들(525, 535, 540)의 범위는 0°~ 90°사이로 제한된다. The position of the
도 6은 일부 실시예에 따른 손바닥 너클의 상대적인 위치 및 손가락의 끝에 기초하여 손가락 포즈 평면에서 손가락의 2D 좌표를 찾는데 사용되는 LUT(600)를 도시한다. LUT(600)의 수직 축은 수직 방향으로 손바닥 너클에 대한 손가락 끝의 변위를 나타낸다. LUT의 수평 축은 수평 방향으로 손바닥 너클에 대한 손가락 끝의 변위를 나타낸다. 폐쇄 곡선(605)은 손바닥 너클에 대한 손가락 끝의 가능한 위치의 외부 경계를 나타낸다. 따라서, 폐쇄 곡선(605)은 손가락의 마디뼈들의 길이 및 해당 관절의 운동 범위의 한계로 인한 마디뼈들 사이의 상대적인 각도에 대한 해부학적 제약에 기초하여 결정된다. 폐쇄 곡선(605) 내의 위치들은 손가락 끝과 손바닥 너클의 가능한 상대 위치들을 나타낸다. 6 illustrates a
특정 손에 대한 LUT(600)는 기결정된 포즈들의 세트에서 손의 트레이닝 이미지들의 세트를 사용하여 결정된다. 상이한 손들에서 마디뼈들의 길이가 달라지는 것을 설명하기 위해서, 트레이닝 이미지들의 세트는 폐쇄 곡선(605)에 의해 정의된 경계 부근의 위치들을 포함하도록 정의된다. 폐쇄 곡선(605) 내의 위치들의 대부분은 손가락의 2D 좌표를 고유하게 결정한다. 하지만, 폐쇄 곡선(605)의 일부 실시예는 폐쇄 곡선(605) 내의 단일 포인트를 손가락의 2D 좌표들의 2 이상의 세트에 매핑하는 퇴화 사례들(degenerate cases)의 작은 세트를 포함한다. 2D 좌표들의 상이한 세트들 사이의 퇴화성(degeneracy)는 다른 정보, 가령, 손가락의 이전 위치, 깊이 정보, 음영 또는 조명 정보 등을 사용하여 파기(break)될 수 있다. The
일부 실시예에서, LUT(600)의 정보는 2 이상의 상이한 포즈들이 손가락의 투영된 2D 좌표들의 동일하거나 유사한 세트를 언제 초래하는지를 결정하는데 사용된다(예컨대, 하나의 3D 포즈에 대해 LUT(600)로부터 도출된 하나 이상의 키포인트들이 다른 하나의 3D 포즈에 대해 LUT(600)로부터 도출된 하나 이상의 키포인트들과 동일하거나 유사하다). 다음으로, 동일하거나 유사한 투영된 2D 좌표들을 갖는 상이한 포즈들을 식별하기 위하여 신호가 생성될 수 있다. 또한, LUT(600)는 새로운 데이터 수집없이 예컨대, 손의 2D 라벨들을 3D 포즈들로 변환하는데 이용될 수 있다. 일부 실시예에서, 투영된 2D 좌표들의 동일하거나 유사한 세트를 야기할 수 있는 상이한 포즈들에 대해서 신뢰도 점수가 도출될 수 있다. 예를 들어, 현재 포즈로부터 동일하거나 유사한 2D 좌표를 갖는 가장 먼 포즈까지의 거리가 이용되어, 신뢰도 점수가 생성되는바, 가령 상기 거리가 0 이라면(또는 임계 거리보다 작다면) 높은 신뢰도 점수가 생성되고, 상기 거리가 임계 거리보다 멀다면 낮은 신뢰도 점수가 생성된다. 일부 실시예에서, 상이한 포즈들을 생성하는 키포인트들 혹은 2D 좌표들에 대한 신뢰도 점수들에 기초하여, 상이한 포즈들이 차별화될 수 있다(disambiguated). 예를 들어, 경우에 따라 사람에 대한 이미지가 이용되어, 2D 라벨들로부터의 3D 포즈의 3D 리프트가 올바른지가 체크 또는 확인될 수 있다. 이미지는 서로 다른 가능한 솔루션들 중에서 선택함으로써 보다 정확한 데이터를 생성하는데 사용될 수도 있다. In some embodiments, the information of the
도 7은 일부 실시예에 따라 도 6에서 원형(1, 2, 3, 4 및 5)으로 표시된 손가락 끝 및 손바닥 너클의 상대적인 위치를 갖는 손가락들의 2D 좌표들을 예시한다. 원형(1)은 손가락 끝과 손바닥 너클 사이의 상대적인 위치를 나타내며, 이는 골격 모델(705)에 도시된 바와 같이, 연장된 손가락에 대응한다. 원형(2)은 손가락 끝과 손바닥 너클 사이에 상대 위치를 나타내며, 이는 골격 모델(710)에 도시된 바와 같이, 손가락의 제 2 관절에 대해 90°로 구부러진 손가락 끝에 대응한다. 원형(3)은 손가락 끝과 손바닥 너클 사이에 상대 위치를 나타내며, 이는 골격 모델(715)에 도시된 바와 같이, 손바닥 너클과 제 1 관절을 연결하는 수평으로 연장된 마디뼈 아래에서 말려있는 손가락 끝에 대응한다. 원형(4)은 손가락 끝과 손바닥 너클 사이에 상대 위치를 나타내며, 이는 골격 모델(720)에 도시된 바와 같이, 손바닥 너클과 제 1 관절을 연결하는 수직으로 연장된 마디뼈 옆에서 말려있는 손가락 끝에 대응한다. 원형(5)은 손가락 끝과 손바닥 너클 사이에 상대 위치를 나타내며, 이는 골격 모델(725)에 도시된 바와 같이, 아래 방향으로 수직으로 연장된 손가락에 대응한다. FIG. 7 illustrates the 2D coordinates of the fingers with the relative positions of the fingertip and palm knuckles shown in
도 8은 일부 실시예에 따라 손가락 끝과 손바닥 너클의 상대적인 위치들을 손가락의 2D 좌표들에 매핑하는 LUT를 구성하는 방법(800)의 흐름도이다. 방법(800)은 도 2에 도시된 LUT(230) 및 도 6에 도시된 LUT(600)의 일부 실시예를 트레이닝시키기 위해 사용된다. 따라서, 방법(800)은 도 2에 도시된 프로세서(225)의 일부 실시예에 의해 수행된다. 8 is a flowchart of a
블록(805)에서, 포즈들의 트레이닝 세트에 위치된 손의 2D 이미지들이 캡처된다. 예를 들어, 2D 이미지들은 도 2에 도시된 카메라(215)에 의해 캡처될 수 있다. 2D 이미지들은 도 2에 도시된 메모리(220)와 같은 메모리에 저장된다. At
블록(810)에서, 프로세서는 손의 2D 이미지들에서 키포인트들을 식별한다. 본 명세서에서 논의된 바와 같이, 키포인트들은 손가락 끝들의 위치, 손가락들의 마디뼈들을 연결하는 관절들, 각각의 손가락이 손바닥에 부착하는 지점을 나타내는 손바닥 너클(palm knuckles), 및 손이 사용자의 팔뚝에 부착되는 지점을 나타내는 손목의 위치를 포함한다. 2D 이미지에서 키포인트들을 식별하는 기술은 당업계에 공지되어 있으며, 명확성을 위해 본 명세서에서는 더 이상 논의되지 않는다. At
블록(815)에서, 프로세서는 키포인트들에 기초하여, 예를 들어, 앞서 논의된 2차 프로그래밍을 이용하여 손가락들의 마디뼈들의 길이를 결정한다. At
블록(820)에서, 프로세서는 마디뼈들의 길이들 및 손가락 끝과 손바닥 너클의 상대적인 위치에 대한 다른 해부학적 제한에 기초하여 LUT를 구성한다. 프로세서는 도 2에 도시된 메모리(220)와 같은 메모리에 LUT를 저장한다. In
도 9는 일부 실시예에 따라 손의 2D 이미지로부터 손의 3D 포즈를 리프팅하는 방법(900)의 흐름도이다. 방법(900)은 도 2에 도시된 프로세싱 시스템(200)의 일부 실시예에서 구현된다. 도시된 실시예에서, 손가락 끝들의 상대적인 위치를 해당 손바닥 너클들에 매핑하는 LUT가, 도 8에 도시된 방법(800)의 일부 실시예에 따라 예를 들어, 손에 대해 생성되었다. 따라서, 손의 골격 모델을 나타내는 다른 파라미터들도 또한 결정되었는바, 가령 마디뼈들의 길이들, 손의 손바닥 삼각형을 정의하는 파라미터들, 및 손의 엄지 삼각형을 정의하는 파라미터들이 결정된다. 9 is a flowchart of a
블록(905)에서, 프로세서는 손의 2D 이미지에서 키포인트들을 식별한다. 다음으로, 프로세서는 키포인트들에 기초하여 3D 공간에서 손의 변환(translation)을 추정한다. 프로세서의 일부 실시예는 손의 골격 모델을 정의하는 파라미터들과, 2D 이미지에서의 대응 파라미터들의 상대적인 값들을 비교함으로써 변환을 추정한다. 예를 들어, 프로세서는 골격 모델에서의 손가락 마디뼈들의 길이들과 2D 이미지에서의 대응 마디뼈들의 길이들을 비교하여, 원근 투영(perspective projection)을 설명하고 손의 2D 이미지를 투영 해제(de-project)할 수 있다. At
블록(915)에서, 프로세서는 손바닥 삼각형 및 엄지 삼각형의 방향을 학습한다. 프로세서의 일부 실시예는 손바닥 삼각형 및 엄지 삼각형을 정의하는 파라미터를 2D 이미지의 일부분과 비교함으로써 손바닥 삼각형 및 엄지 삼각형의 방향을 학습한다. 손바닥 삼각형 및 엄지 삼각형의 방향은 대응하는 벡터들에 의해 특징지워지며, 이들 벡터들은 손바닥 삼각형 및 엄지 삼각형의 평면들에 수직인 방향으로 놓이도록 정의된다. At
블록(920)에서, 프로세서는 손가락들에 대한 손가락 포즈 평면들의 방향들을 학습한다. 손가락 포즈 평면들의 방향들은 대응하는 벡터들에 의해 특징지워지며, 이들 벡터들은 대응손바닥 삼각형 및 엄지 삼각형을 정의하는 벡터에 수직이며, 대응하는 손가락 포즈 평면에 놓인다. At
블록(925)에서, 프로세서는 LUT, 그리고 손가락 끝들 및 대응하는 손바닥 너클들의 상대적인 위치들에 기초하여 손가락들의 2D 손가락 좌표들을 결정한다. At
블록(930)에서, 프로세서는 손의 3D 포즈를 나타내는 3D 골격 모델을 생성한다. 3D 골격 모델을 생성하기 위해 프로세서는 손바닥 삼각형과 엄지 삼각형의 방향들에 각각 기초하여 손가락들과 엄지 손가락의 2D 좌표들을 회전시킨다. 3D 골격 모델은 손바닥 삼각형, 손바닥 삼각형의 방향, 엄지 삼각형, 엄지 삼각형의 방향 및 손가락들과 엄지의 회전된 2D 손가락 좌표들을 결합하여 결정된다. At
도 10은 일부 실시예에 따라 손의 2D 이미지로부터 열거된 3D 키포인트들에 대한 반복적인 디-디노이징(iteratively de-denoising)의 예시도(1000)이다. 상기 예시도(1000)에 도시된 반복 프로세스는 도 2에 도시된 프로세싱 시스템(200)의 일부 실시예에서 구현된다. 예시도(1000)는 예컨대 도 2에 도시된 카메라(215)와 같은 카메라의 이미지 평면(1005)을 도시한다. 카메라에 의해 캡처된 이미지들은 이미지 평면(1005) 상에 투영된다. 카메라의 특성들은 또한, 소실점을 결정하며, 소실점은 3D 공간에서 평행 라인들의 2D 투영들이 수렴된 것처럼 보이는, 이미지 평면(1005) 상의 추상적인 지점이다. 10 is an exemplary diagram 1000 of iterative de-denoising for 3D keypoints listed from a 2D image of a hand, in accordance with some embodiments. The iterative process shown in the example diagram 1000 is implemented in some embodiments of the
초기에, 키포인트(1015)가 2D 이미지로부터 추출된다. 도시된 실시예에서, 2D 이미지는 노이즈 이미지이고, 키포인트(1015)의 초기 추정치는 반드시 손 이미지의 정확한 위치에 있지는 않다. 손의 3D 골격 모델은 잡음이 있는 키포인트(1015) 및 2D 이미지로부터 추출된 다른 잠재적으로 잡음이 있는 키포인트(도 10에 도시되지 않음)에 기초하여 2D 이미지로부터 리프팅된다. 예를 들어, 손의 3D 골격 모델은 도 8에 도시된 방법(800) 및 도 9에 도시된 방법(900)의 일부 실시예에 따라 리프팅된다. 3D 키포인트(1020)를 결정하는데 손의 3D 골격 모델이 이용되며, 3D 키포인트(1020)는 키포인트(1015)와 손에서 동일한 위치에 대응한다. Initially,
본 명세서에서 골격-호환 키포인트(skeleton-compliant keypoint)로 지칭되는 3D 키포인트(1020)는 초기 키포인트(1015)의 원근 투영(perspective projection)과 반드시 일치하는 것은 아닌데 왜냐하면, 스켈레톤 호환 키포인트(1020)가 초기 키포인트(1015)와 소실점(1010) 사이의 라인(1025) 상에 반드시 위치하는 것은 아니기 때문이다. 따라서, 수정된 3D 키포인트(1030)는 골격-호환 키포인트(1020)를 라인(1025) 상에 투영시킴으로써 결정된다. 다음으로, 수정된 3D 키포인트(1030)(이는 본 명세서에서 카메라-호환 키포인트라 지칭됨)와 동일하게 초기 키포인트(1015)의 값을 세팅하여 초기 키포인트(1015)의 값을 업데이트함으로써 상기 프로세스가 반복된다. 상기 프로세스는, 키포인트(및 2D 이미지에서 잡음이 있는 임의의 다른 키포인트들)에 대한 수렴 기준을 충족할 때까지 반복된다. The
도 11은 일부 실시예에 따라 손의 2D 이미지로부터 추출된 키포인트를 노이즈 제거하는(denoising) 방법(1100)의 흐름도이다. 방법(1100)은 도 2에 도시된 프로세싱 시스템(200)의 일부 실시예에서 수행된다.11 is a flowchart of a
블록 1105에서, 프로세서는 2D 이미지로부터 추출된 잡음이 있는 키포인트에 기초하여 손의 3D 골격 모델을 생성한다. 일부 실시예에서, 3D 골격 모델은 도 8에 도시된 방법(800) 및 도 9에 도시된 방법(900)의 일부 실시예에 따라 생성된다.At
블록 1110에서, 프로세서는 손의 3D 골격 모델과 호환되는 제 1 세트의 3D 키포인트들을 식별한다. 예를 들어, 제 1 세트의 3D 키포인트들은 손의 3D 골격 모델에 의해 정의된 손가락과 엄지의 끝, 손가락과 엄지의 관절, 손가락과 엄지의 손바닥 너클 및 손목 위치에 대응하는 키포인트를 나타낸다. 일부 실시예에서, 제 1 세트의 3D 키포인트는 도 10에 도시된 골격-호환 키포인트(1020)를 포함한다. At
블록 1115에서, 프로세서는 제 1 3D 키포인트 및 이미지와 관련된 소실점에 기초하여 제 2 3D 키포인트를 식별한다. 본 명세서에서 논의된 바와 같이, 소실점은 2D 이미지를 획득한 카메라의 특성들에 기초하여 결정된다. 일부 실시예에서, 제 2 세트의 3D 키포인트들은 도 10에 도시된 카메라-호환 키포인트(1030)를 포함한다. At
블록 1120에서, 프로세서는 제 2 세트의 3D 키포인트에 기초하여 2D 이미지로부터 추출된 잡음이 있는 키포인트를 수정한다. 예를 들어, 잡음이 있는 키포인트들의 값들은 제 2 세트의 3D 키포인트들의 해당 값들과 동일해지도록 업데이트된다. At
결정 블록(1125)에서, 프로세서는 잡음이 있는 키포인트의 값이 수렴되었는지를 결정한다(예를 들어, 잡음이 있는 키포인트에 대한 수렴 기준에 기초하여). 그렇지 않다면, 방법(1100)은 블록(1105)으로 진행하고, 잡음이 있는 키포인트의 수정된 값에 기초하여 업데이트된 3D 골격 모델이 생성된다. 값들이 수렴되었다라고 프로세서가 결정하면, 방법은 종료 블록(1130)으로 진행하여 종료된다. At
일부 실시예에서, 전술한 설명의 특정 양상들은 소프트웨어를 실행하는 프로세싱 시스템의 하나 이상의 프로세서에 의해 구현된다. 소프트웨어는 비일시적 컴퓨터 판독가능 저장 매체에 저장되거나 그렇지 않으면 유형적으로 구현된 하나 이상의 실행 가능 명령 세트를 포함한다. 소프트웨어는 하나 이상의 프로세서에 의해 실행될 때 하나 이상의 프로세서를 조작하여 전술한 설명의 하나 이상의 양상들을 수행하는 명령 및 특정 데이터를 포함할 수 있다. 비일시적 컴퓨터 판독가능 저장 매체는 예를 들어 자기 또는 광 디스크 저장 디바이스, 플래시 메모리와 같은 솔리드 스테이트 저장 디바이스, 캐시, 랜덤 액세스 메모리(RAM) 또는 다른 비휘발성 메모리 디바이스 등등을 포함할 수 있다. 비일시적 컴퓨터 판독가능 저장 매체에 저장된 실행 가능 명령은 소스 코드, 어셈블리 언어 코드, 객체 코드, 또는 하나 이상의 프로세서에 의해 해석되거나 달리 실행가능한 다른 명령 포맷일 수 있다.In some embodiments, certain aspects of the foregoing description are implemented by one or more processors in a processing system that executes software. The software includes one or more executable instructions sets stored on or otherwise tangibly embodied in a non-transitory computer readable storage medium. The software may include instructions and specific data when executed by one or more processors to manipulate one or more processors to perform one or more aspects of the foregoing description. Non-transitory computer readable storage media may include, for example, magnetic or optical disk storage devices, solid state storage devices such as flash memory, caches, random access memory (RAM) or other nonvolatile memory devices, and the like. Executable instructions stored on a non-transitory computer readable storage medium may be source code, assembly language code, object code, or other instruction format interpreted or otherwise executable by one or more processors.
컴퓨터 판독 가능 저장 매체는 컴퓨터 시스템에 명령 및/또는 데이터를 제공하기 위해 사용 중에 컴퓨터 시스템에 의해 액세스 가능한 임의의 저장 매체, 또는 저장 매체의 조합을 포함할 수 있다. 이러한 저장 매체는 광학 매체(예를 들어, 콤팩트 디스크(CD), 디지털 다목적 디스크(DVD), 블루 레이 디스크), 자기 매체(예를 들어, 플로피 디스크, 자기 테이프 또는 자기 하드 드라이브), 휘발성 메모리(예를 들어, 랜덤 액세스 메모리(RAM) 또는 캐시), 비휘발성 메모리(예를 들어, ROM(read-only memory) 또는 플래시 메모리), 또는 MEMS(microelectromechanical systems) 기반 저장 매체를 포함할 수 있지만 이에 제한되지는 않는다. 컴퓨터 판독가능 저장 매체는 컴퓨팅 시스템 내에 내장될 수 있으며(예를 들어, 시스템 ROM 또는 RAM), 컴퓨팅 시스템에 고정식으로 부착될 수 있으며(예를 들어, 자기 하드 드라이브), 컴퓨팅 시스템에 착탈식으로 부착될 수 있으며(예를 들어, 광학 디스크 또는 USB-기반의 플래시 메모리), 또는 유/무선 네트워크를 통해 컴퓨팅 시스템에 접속될 수 있다(예를 들어, 네트워크 액세스가능 스토리지(NAS)). Computer-readable storage media can include any storage medium or combination of storage media that can be accessed by a computer system during use to provide instructions and / or data to a computer system. Such storage media may include optical media (eg, compact discs (CDs), digital general purpose discs (DVDs), Blu-ray discs), magnetic media (eg, floppy disks, magnetic tapes, or magnetic hard drives), volatile memory ( For example, it may include, but is not limited to, random access memory (RAM) or cache, non-volatile memory (eg, read-only memory or flash memory), or microelectromechanical systems (MEMS) based storage media. It doesn't work. Computer-readable storage media may be embedded within a computing system (eg, system ROM or RAM), may be fixedly attached to the computing system (eg, a magnetic hard drive), and may be removable from the computing system. (Eg, optical disk or USB-based flash memory) or may be connected to a computing system via a wired / wireless network (eg, network accessible storage (NAS)).
전술한 일반적인 설명 부분에서 상술된 모든 행동들 또는 요소들이 요구되는 것은 아니며, 특정 활동 또는 디바이스의 일부가 요구되지 않을 수 있으며, 설명된 것들 이외의 하나 이상의 추가 활동이 수행될 수 있거나, 또는 요소들이 추가될 수 있음에 유의해야 한다. 또한, 활동들이 나열된 순서가 반드시 수행되는 순서는 아니다. 또한, 개념들은 특정 실시예들을 참조하여 설명되었다. 그러나, 당업자는 이하의 청구 범위에 기재된 본 개시의 범위를 벗어나지 않고 다양한 수정 및 변경이 이루어질 수 있음을 이해할 것이다. 따라서, 명세서 및 도면은 제한적인 의미보다는 예시적인 의미로 간주되어야하고, 그러한 모든 수정은 본 개시의 범위 내에 포함되는 것으로 의도된다. Not all of the actions or elements described in the general description above are required, some of the specific activities or devices may not be required, and one or more additional activities other than those described may be performed, or Note that it may be added. Also, the order in which the activities are listed is not necessarily the order in which they are performed. Also, the concepts have been described with reference to specific embodiments. However, one of ordinary skill in the art appreciates that various modifications and changes can be made without departing from the scope of the present disclosure as set forth in the claims below. Accordingly, the specification and figures are to be regarded in an illustrative rather than a restrictive sense, and all such modifications are intended to be included within the scope of present disclosure.
본 발명의 이점들, 다른 장점들 및 문제에 대한 해결책들이 특정 실시예와 관련하여 설명되었다. 하지만, 상기 이점들, 장점들, 문제에 대한 해결책들 및 임의의 이점, 장점, 해결책을 야기하거나 더욱 자명하게 만들 수 있는 임의의 피처(들)은 임의의 청구항 또는 모든 청구항들에서 중요하고, 필수적이고, 또는 본질적으로 피처로서 간주되어서는 안된다. 또한, 본 발명의 주제는 본 명세서의 교시의 이점을 갖는 당업자에게 명백하지만 상이한 방식으로 수정 및 실시될 수 있기 때문에, 위에서 개시된 특정 실시예는 단지 일례일 뿐이다. 이하의 청구 범위에 기재된 것 이외의 본 명세서에 도시된 구성 또는 설계의 세부 사항에 대한 제한은 없다. 따라서, 상기 개시된 특정 실시예는 변경되거나 수정될 수 있으며, 이러한 모든 변형은 개시된 주제의 범위 내에서 고려된다는 것이 명백하다. 따라서, 본 명세서에서 요구되는 보호는 아래의 청구 범위에 기술된 바와 같다. Advantages, other advantages, and solutions to problems of the present invention have been described with reference to specific embodiments. However, the above advantages, advantages, solutions to problems, and any feature (s) that can cause or become more apparent to any advantage, advantage, solution, are important and essential in any or all claims. It should not be considered as a feature or feature in nature. In addition, the specific embodiments disclosed above are merely examples as the subject matter of the present invention is apparent to those skilled in the art having the benefit of the teachings herein, and may be modified and practiced in different ways. There is no limitation on the details of construction or design shown herein other than as described in the claims below. Accordingly, it is apparent that the specific embodiments disclosed above may be changed or modified, and all such modifications are considered within the scope of the disclosed subject matter. Accordingly, the protection required herein is as described in the claims below.
Claims (30)
프로세서에서, 카메라에 의해 캡처된 2차원(2D) 이미지에서 신체 부분의 키포인트들을 식별하는 단계; 및
상기 프로세서에서, 상기 키포인트들의 위치들의 함수로서 상기 신체 부분의 잠재적인 3D 포즈를 나타내는 룩업 테이블(LUT)들에 액세스하도록, 상기 키포인트들의 위치들을 사용하여 상기 신체 부분의 3차원(3D) 포즈(pose)를 결정하는 단계
를 포함하는 것을 특징으로 하는 방법. As a method,
At the processor, identifying keypoints of the body part in a two-dimensional (2D) image captured by the camera; And
In the processor, a three-dimensional (3D) pose of the body part using the locations of the keypoints is used to access lookup tables (LUTs) that represent a potential 3D pose of the body part as a function of the positions of the keypoints. Steps to determine
Method comprising a.
상기 신체 부분은 손, 발, 팔, 다리, 및 머리 중 적어도 하나를 포함하는 것을 특징으로 하는 방법. The method of claim 1,
The body portion comprises at least one of a hand, foot, arm, leg, and head.
프로세서에서, 카메라에 의해 캡처된 2차원(2D) 이미지에서 손의 키포인트들을 식별하는 단계; 및
상기 프로세서에서, 상기 키포인트들의 위치들의 함수로서 상기 손의 잠재적인 3D 포즈들을 나타내는 룩업 테이블(LUT)들에 액세스하도록, 상기 키포인트들의 위치들을 사용하여 상기 손의 3차원(3D) 포즈(pose)를 결정하는 단계
를 포함하는 것을 특징으로 하는 방법. The method according to claim 1 or 2,
At the processor, identifying keypoints of the hand in the two-dimensional (2D) image captured by the camera; And
In the processor, use the positions of the keypoints to create a three-dimensional (3D) pose of the hand to access lookup tables (LUTs) that represent potential 3D poses of the hand as a function of the positions of the keypoints. Decision Step
Method comprising a.
상기 키포인트들은 손가락들과 엄지의 끝들의 위치들, 손가락들과 엄지의 마디뼈들(phalanxes)을 연결하는 관절들(joints), 손가락들과 엄지가 손바닥에 부착되는 지점을 나타내는 손바닥 너클들(palm knuckles), 및 손이 팔뚝에 부착되는 지점을 나타내는 손목 위치를 포함하는 것을 특징으로 하는 방법. The method of claim 3,
The keypoints are palm knuckles indicating the positions of the tips of the fingers and thumb, the joints connecting the phalanxes of the fingers and thumb, and the point where the fingers and thumb are attached to the palm. knuckles), and a wrist position representing the point at which the hand is attached to the forearm.
상기 룩업 테이블(LUT)들은, 손가락들 또는 엄지의 대응 손바닥 너클들에 대한 손가락들 또는 엄지의 끝들(tips)의 위치들의 함수로서 대응 손가락 포즈 평면들에서의 손가락들 및 엄지의 2D 좌표들을 나타내는 손가락 포즈 룩업 테이블들을 포함하는 것을 특징으로 하는 방법. The method of claim 4, wherein
The lookup tables (LUTs) show fingers 2D coordinates of the thumb and fingers in the corresponding finger pose planes as a function of the positions of the fingers or the tips of the thumb relative to the fingers or the corresponding palm knuckles of the thumb. And pose lookup tables.
복수의 3D 트레이닝 포즈들에 있는 손에 대한 트레이닝 이미지들의 세트에 기초하여 상기 손가락 포즈 룩업 테이블(LUT)들을 생성하는 단계를 더 포함하는 것을 특징으로 하는 방법. The method of claim 5,
Generating the finger pose lookup tables (LUTs) based on a set of training images for a hand in a plurality of 3D training poses.
상기 손가락 포즈 룩업 테이블(LUT)들을 생성하는 단계는,
상기 트레이닝 이미지들의 세트로부터 손가락들 및 엄지의 마디뼈들의 길이들을 결정하는 단계를 포함하는 것을 특징으로 하는 방법. The method of claim 6,
Generating the finger pose lookup tables (LUTs),
Determining the lengths of the knuckles of the fingers and thumb from the set of training images.
상기 손가락 포즈 룩업 테이블(LUT)들을 생성하는 단계는,
상기 마디뼈들의 길이들 그리고 상기 마디뼈들을 연결하는 관절들의 운동 범위에 대한 해부학적 제한에 기초하여, 상기 손가락 포즈 룩업 테이블(LUT)들을 생성하는 단계를 포함하는 것을 특징으로 하는 방법. The method of claim 7, wherein
Generating the finger pose lookup tables (LUTs),
Generating finger pose lookup tables (LUTs) based on anatomical limitations on the length of the knuckles and the range of motion of the joints connecting the knuckles.
상기 손가락 포즈 룩업 테이블들에 기초하여 결정된 유사한 키포인트들을 갖는 2개 이상의 잠재적인 3D 포즈들을 식별하는 단계를 더 포함하는 것을 특징으로 하는 방법. The method according to claim 6 to 8,
Identifying at least two potential 3D poses having similar keypoints determined based on the finger pose lookup tables.
상기 트레이닝 이미지들에 기초하여, 상기 손목 위치에서 정점을 갖는 손바닥 삼각형(palm triangle)을 정의하는 파라미터들을 결정하는 단계를 더 포함하며, 상기 정점은 손가락들의 손바닥 너클들을 포함하는 손바닥 삼각형의 하나의 변(side)과 마주보는 것을 특징으로 하는 방법. The method according to claim 6 to 9,
Based on the training images, determining parameters defining a palm triangle with a vertex at the wrist position, wherein the vertex is one side of a palm triangle comprising palm knuckles of fingers. a method characterized by facing the side.
상기 트레이닝 이미지들에 기초하여, 상기 손목 위치, 상기 엄지의 손바닥 너클, 및 검지의 손바닥 너클에서 정점들을 갖는 엄지 삼각형(thumb triangle)을 정의하는 파라미터들을 결정하는 단계를 더 포함하는 것을 특징으로 하는 방법. The method according to claim 6 to 10,
Based on the training images, determining parameters defining a thumb triangle having vertices in the wrist position, the palm knuckle of the thumb, and the palm knuckle of the index finger. .
상기 손의 3D 포즈를 결정하는 단계는,
손가락들 및 엄지의 대응 손가락 끝들(tips) 및 손바닥 너클들의 상대적인 위치들에 기초하여, 상기 손가락 포즈 룩업 테이블들로부터 손가락들 및 엄지의 2D 좌표들을 결정하는 단계를 포함하는 것을 특징으로 하는 방법. The method according to any one of claims 3 to 11, wherein
Determining the 3D pose of the hand,
Determining 2D coordinates of the fingers and thumb from the finger pose lookup tables based on the corresponding finger tips of the fingers and thumb and the relative positions of the palm knuckles.
상기 손의 3D 포즈를 결정하는 단계는,
상기 2D 이미지로부터 손바닥 삼각형 및 엄지 삼각형의 방향들을 결정하는 단계를 포함하는 것을 특징으로 하는 방법. The method of claim 11, wherein
Determining the 3D pose of the hand,
Determining directions of the palm triangle and thumb triangle from the 2D image.
상기 손의 3D 포즈를 결정하는 단계는,
상기 손바닥 삼각형 및 엄지 삼각형의 방향들에 각각 기초하여, 손가락들 및 엄지의 2D 좌표들을 회전시키는 단계를 포함하는 것을 특징으로 하는 방법. The method of claim 11,
Determining the 3D pose of the hand,
Rotating 2D coordinates of the fingers and thumb based on directions of the palm and thumb triangles, respectively.
상기 키포인트들을 식별하는 단계는 상기 손의 2D 이미지에서 잡음이 있는(noisy) 키포인트들의 3D 위치들을 식별하는 단계를 포함하고, 그리고 상기 손의 3D 포즈를 결정하는 단계는 상기 잡음이 있는(noisy) 키포인트들에 기초하여 손의 3D 포즈를 나타내는 골격 모델(skeleton model)을 생성하는 단계를 포함하는 것을 특징으로 하는 방법. The method according to any one of claims 3 to 12,
Identifying the keypoints includes identifying 3D positions of noisy keypoints in the 2D image of the hand, and determining the 3D pose of the hand comprises the noisy keypoint Generating a skeleton model representing the 3D pose of the hand based on the field.
상기 골격 모델에 기초하여 골격-호환(skeleton-compliant) 키포인트들을 생성하는 단계;
상기 2D 이미지와 연관된 소실점(vanishing point)과 잡음이 있는 해당 키포인트를 연결하는 라인에 기초하여 상기 골격-호환 키포인트들을 수정하는 단계;
수정된 골격-호환 키포인트들과 동일해지도록 상기 잡음이 있는 키포인트들을 세팅하는 단계; 및
상기 잡음이 있는 키포인트들이 수렴 기준을 충족할 때까지 반복하는 단계
를 더 포함하는 것을 특징으로 하는 방법. The method of claim 15,
Generating skeleton-compliant keypoints based on the skeletal model;
Modifying the skeletal-compatible keypoints based on a line connecting the vanishing point associated with the 2D image with the noisy corresponding keypoint;
Setting the noisy keypoints to be identical to modified skeleton-compatible keypoints; And
Repeating until the noisy keypoints meet a convergence criterion
Method further comprising a.
손의 2차원(2D) 이미지들을 획득하도록 구성된 카메라; 및
상기 2D 이미지에서 손의 키포인트들을 식별하고 그리고 상기 키포인트들의 위치들의 함수로서 손의 잠재적인 3D 포즈들을 나타내는 룩업 테이블(LUT)들에 액세스하도록, 상기 키포인트들의 위치들을 사용하여 상기 손의 3차원(3D) 포즈(pose)를 결정하는 프로세서
를 포함하는 것을 특징으로 하는 장치. As a device,
A camera configured to acquire two-dimensional (2D) images of a hand; And
Three-dimensional (3D) of the hand using the locations of the keypoints to identify keypoints of the hand in the 2D image and to access lookup tables (LUTs) that represent potential 3D poses of the hand as a function of the locations of the keypoints. Processor to determine pose
Apparatus comprising a.
상기 키포인트들은 손가락들과 엄지의 끝들의 위치들, 손가락들과 엄지의 마디뼈들을 연결하는 관절들, 손가락들과 엄지가 손바닥에 부착되는 지점을 나타내는 손바닥 너클들, 및 손이 팔뚝에 부착되는 지점을 나타내는 손목 위치를 포함하는 것을 특징으로 하는 장치. The method of claim 17,
The keypoints are the positions of the ends of the fingers and thumb, the joints connecting the knuckles of the fingers and thumb, the palm knuckles indicating where the fingers and thumb are attached to the palm, and the point at which the hand is attached to the forearm. And a wrist position indicative of a wrist position.
상기 룩업 테이블(LUT)들은, 손가락들 또는 엄지의 대응 손바닥 너클들에 대한 손가락들 또는 엄지의 끝들(tips)의 위치들의 함수로서 대응 손가락 포즈 평면들에서의 손가락들 및 엄지의 2D 좌표들을 나타내는 손가락 포즈 룩업 테이블들을 포함하는 것을 특징으로 하는 장치. The method of claim 18,
The lookup tables (LUTs) show fingers 2D coordinates of the thumb and fingers in the corresponding finger pose planes as a function of the positions of the fingers or the tips of the thumb relative to the fingers or the corresponding palm knuckles of the thumb. And pose lookup tables.
상기 프로세서는,
복수의 3D 트레이닝 포즈들에 있는 손에 대한 트레이닝 이미지들의 세트에 기초하여 상기 손가락 포즈 룩업 테이블(LUT)들을 생성하고; 그리고
상기 손가락 포즈 룩업 테이블(LUT)들을 메모리에 저장하는 것을 특징으로 하는 장치. The method of claim 19,
The processor,
Generate the finger pose lookup tables (LUTs) based on a set of training images for a hand in a plurality of 3D training poses; And
And store the finger pose lookup tables (LUTs) in a memory.
상기 프로세서는, 상기 손가락 포즈 룩업 테이블들에 기초하여 결정된 유사한 키포인트들을 갖는 2개 이상의 잠재적인 3D 포즈들을 식별하는 것을 특징으로 하는 장치. The method of claim 19 or 20,
And the processor identifies two or more potential 3D poses with similar keypoints determined based on the finger pose lookup tables.
상기 프로세서는, 상기 트레이닝 이미지들의 세트로부터 손가락들 및 엄지의 마디뼈들의 길이들을 결정하는 것을 특징으로 하는 장치. The method of claim 19,
And the processor determines lengths of the knuckles of fingers and thumb from the set of training images.
상기 프로세서는, 상기 마디뼈들의 길이들 그리고 상기 마디뼈들을 연결하는 관절들의 운동 범위에 대한 해부학적 제한에 기초하여, 상기 손가락 포즈 룩업 테이블(LUT)들을 생성하는 것을 특징으로 하는 장치. The method of claim 22,
And the processor generates the finger pose lookup tables (LUTs) based on anatomical limitations on the lengths of the knuckles and the range of motion of the joints connecting the knuckles.
상기 프로세서는, 상기 트레이닝 이미지들에 기초하여, 상기 손목 위치에서 정점을 갖는 손바닥 삼각형(palm triangle)을 정의하는 파라미터들을 결정하며, 상기 정점은 손가락들의 손바닥 너클들을 포함하는 손바닥 삼각형의 하나의 변(side)과 마주보는 것을 특징으로 하는 장치. The method according to any one of claims 20 to 23, wherein
The processor determines, based on the training images, parameters defining a palm triangle with a vertex at the wrist position, the vertex being one side of the palm triangle comprising palm knuckles of fingers. device facing the side).
상기 프로세서는, 상기 트레이닝 이미지들에 기초하여, 상기 손목 위치, 상기 엄지의 손바닥 너클, 및 검지의 손바닥 너클에서 정점들을 갖는 엄지 삼각형(thumb triangle)을 정의하는 파라미터들을 결정하는 것을 특징으로 하는 장치. The method according to any one of claims 20 to 24,
And the processor determines, based on the training images, parameters defining a thumb triangle having vertices at the wrist position, the palm knuckle of the thumb, and the palm knuckle of the index finger.
상기 프로세서는, 손가락들 및 엄지의 대응 손가락 끝들(tips) 및 손바닥 너클들의 상대적인 위치들에 기초하여, 상기 손가락 포즈 룩업 테이블들로부터 손가락들 및 엄지의 2D 좌표들을 결정하는 것을 특징으로 하는 장치. The method according to any one of claims 20 to 25,
And the processor determines 2D coordinates of the fingers and thumb from the finger pose lookup tables based on the corresponding finger tips of the fingers and thumb and the relative positions of the palm knuckles.
상기 프로세서는, 상기 2D 이미지로부터 손바닥 삼각형 및 엄지 삼각형의 방향들을 결정하는 것을 특징으로 하는 장치. The method of claim 26, wherein
And the processor determines directions of the palm triangle and thumb triangle from the 2D image.
상기 프로세서는, 상기 손바닥 삼각형 및 엄지 삼각형의 방향들에 각각 기초하여, 손가락들 및 엄지의 2D 좌표들을 회전시키는 것을 특징으로 하는 장치. 28. The method of claim 27, wherein quoting 26 or 25,
And the processor rotates 2D coordinates of the fingers and thumb based on directions of the palm triangle and thumb triangle, respectively.
상기 프로세서는 상기 손의 2D 이미지에서 잡음이 있는 키포인트들의 3D 위치들을 식별하며, 상기 프로세서는 상기 잡음이 있는 키포인트들에 기초하여 손의 3D 포즈를 나타내는 골격 모델을 생성하는 것을 특징으로 하는 장치. The method according to any one of claims 17 to 28,
The processor identifies 3D locations of noisy keypoints in the 2D image of the hand, and wherein the processor generates a skeletal model representing a 3D pose of the hand based on the noisy keypoints.
상기 프로세서는,
상기 골격 모델에 기초하여 골격-호환 키포인트들을 생성하고;
상기 2D 이미지와 연관된 소실점(vanishing point)과 잡음이 있는 해당 키포인트를 연결하는 라인에 기초하여 상기 골격-호환 키포인트들을 수정하고;
수정된 골격-호환 키포인트들과 동일해지도록 상기 잡음이 있는 키포인트들을 세팅하고; 그리고
상기 잡음이 있는 키포인트들이 수렴 기준을 충족할 때까지 반복하는 것을 특징으로 하는 장치. The method of claim 29,
The processor,
Generate skeleton-compatible keypoints based on the skeletal model;
Modify the skeletal-compatible keypoints based on a line connecting the vanishing point associated with the 2D image with the noisy corresponding keypoint;
Set the noisy keypoints to be identical to modified skeleton-compatible keypoints; And
And repeat until the noisy keypoints meet a convergence criterion.
Applications Claiming Priority (5)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201762598306P | 2017-12-13 | 2017-12-13 | |
US62/598,306 | 2017-12-13 | ||
US16/112,264 US11544871B2 (en) | 2017-12-13 | 2018-08-24 | Hand skeleton learning, lifting, and denoising from 2D images |
US16/112,264 | 2018-08-24 | ||
PCT/US2018/055809 WO2019118058A1 (en) | 2017-12-13 | 2018-10-15 | Hand skeleton learning, lifting, and denoising from 2d images |
Publications (2)
Publication Number | Publication Date |
---|---|
KR20200011425A true KR20200011425A (en) | 2020-02-03 |
KR102329781B1 KR102329781B1 (en) | 2021-11-22 |
Family
ID=66697067
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
KR1020197034132A KR102329781B1 (en) | 2017-12-13 | 2018-10-15 | Learning the hand skeleton from 2D images, lifting and denoising |
Country Status (5)
Country | Link |
---|---|
US (2) | US11544871B2 (en) |
EP (1) | EP3724810A1 (en) |
KR (1) | KR102329781B1 (en) |
CN (1) | CN111492367B (en) |
WO (1) | WO2019118058A1 (en) |
Cited By (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
KR20210112848A (en) | 2020-03-06 | 2021-09-15 | 경북대학교 산학협력단 | Method for generating rotated hand bone 2d projection image from 1 hand bone 2d projection image, recording medium and device for performing the method |
KR102367584B1 (en) * | 2021-11-04 | 2022-02-25 | 주식회사 티지 | Automatic video surveillance system using skeleton video analysis technique |
WO2022065763A1 (en) * | 2020-09-22 | 2022-03-31 | Samsung Electronics Co., Ltd. | Display apparatus and method for controlling thereof |
Families Citing this family (19)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN109934065B (en) * | 2017-12-18 | 2021-11-09 | 虹软科技股份有限公司 | Method and device for gesture recognition |
US11113887B2 (en) * | 2018-01-08 | 2021-09-07 | Verizon Patent And Licensing Inc | Generating three-dimensional content from two-dimensional images |
WO2019178114A1 (en) * | 2018-03-13 | 2019-09-19 | Magic Leap, Inc. | Gesture recognition system and method of using same |
US20190362128A1 (en) * | 2018-05-23 | 2019-11-28 | Wen-Kuei Liu | Knuckle-print identification system |
US10902638B2 (en) * | 2018-09-28 | 2021-01-26 | Wipro Limited | Method and system for detecting pose of a subject in real-time |
KR102203933B1 (en) * | 2018-11-26 | 2021-01-15 | 재단법인 실감교류인체감응솔루션연구단 | Method and apparatus for motion capture interface using multiple fingers |
US10867441B2 (en) * | 2019-02-15 | 2020-12-15 | Microsoft Technology Licensing, Llc | Method and apparatus for prefetching data items to a cache |
KR20220062338A (en) * | 2019-09-09 | 2022-05-16 | 스냅 인코포레이티드 | Hand pose estimation from stereo cameras |
US11288841B2 (en) * | 2019-10-17 | 2022-03-29 | Shanghai United Imaging Intelligence Co., Ltd. | Systems and methods for patient positioning |
CN112767300A (en) * | 2019-10-18 | 2021-05-07 | 宏达国际电子股份有限公司 | Method for automatically generating labeling data of hand and method for calculating skeleton length |
CN112686084A (en) * | 2019-10-18 | 2021-04-20 | 宏达国际电子股份有限公司 | Image annotation system |
CN110942007B (en) * | 2019-11-21 | 2024-03-05 | 北京达佳互联信息技术有限公司 | Method and device for determining hand skeleton parameters, electronic equipment and storage medium |
CN110991319B (en) * | 2019-11-29 | 2021-10-19 | 广州市百果园信息技术有限公司 | Hand key point detection method, gesture recognition method and related device |
CN112083800B (en) * | 2020-07-24 | 2024-04-30 | 青岛小鸟看看科技有限公司 | Gesture recognition method and system based on adaptive finger joint rule filtering |
US11301041B1 (en) | 2020-09-30 | 2022-04-12 | International Business Machines Corporation | Hand tremor accessible using augmented reality user interface |
CN112861606A (en) * | 2020-12-24 | 2021-05-28 | 北京航空航天大学 | Virtual reality hand motion recognition and training method based on skeleton animation tracking |
CN113384291A (en) * | 2021-06-11 | 2021-09-14 | 北京华医共享医疗科技有限公司 | Medical ultrasonic detection method and system |
CN113807323B (en) * | 2021-11-01 | 2022-12-09 | 北京大学 | Accurate hand function evaluation system and method based on image recognition |
CN115830635A (en) * | 2022-12-09 | 2023-03-21 | 南通大学 | PVC glove identification method based on key point detection and target identification |
Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20020024593A1 (en) * | 1999-12-06 | 2002-02-28 | Jean-Yves Bouguet | 3D scanning using shadows |
US20090082701A1 (en) * | 2007-03-07 | 2009-03-26 | Motek Bv | Method for real time interactive visualization of muscle forces and joint torques in the human body |
US20140039861A1 (en) * | 2012-08-06 | 2014-02-06 | CELSYS, Inc. | Object correcting apparatus and method and computer-readable recording medium |
US8872899B2 (en) * | 2004-07-30 | 2014-10-28 | Extreme Reality Ltd. | Method circuit and system for human to machine interfacing by hand gestures |
US20170193289A1 (en) * | 2015-12-31 | 2017-07-06 | Microsoft Technology Licensing, Llc | Transform lightweight skeleton and using inverse kinematics to produce articulate skeleton |
Family Cites Families (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6738065B1 (en) * | 1999-08-10 | 2004-05-18 | Oshri Even-Zohar | Customizable animation system |
US8600166B2 (en) * | 2009-11-06 | 2013-12-03 | Sony Corporation | Real time hand tracking, pose classification and interface control |
US9936128B2 (en) * | 2015-05-20 | 2018-04-03 | Google Llc | Automatic detection of panoramic gestures |
-
2018
- 2018-08-24 US US16/112,264 patent/US11544871B2/en active Active
- 2018-10-15 WO PCT/US2018/055809 patent/WO2019118058A1/en unknown
- 2018-10-15 EP EP18796327.7A patent/EP3724810A1/en active Pending
- 2018-10-15 KR KR1020197034132A patent/KR102329781B1/en active IP Right Grant
- 2018-10-15 CN CN201880033400.9A patent/CN111492367B/en active Active
-
2022
- 2022-12-12 US US18/079,762 patent/US20230108253A1/en active Pending
Patent Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20020024593A1 (en) * | 1999-12-06 | 2002-02-28 | Jean-Yves Bouguet | 3D scanning using shadows |
US8872899B2 (en) * | 2004-07-30 | 2014-10-28 | Extreme Reality Ltd. | Method circuit and system for human to machine interfacing by hand gestures |
US20090082701A1 (en) * | 2007-03-07 | 2009-03-26 | Motek Bv | Method for real time interactive visualization of muscle forces and joint torques in the human body |
US20140039861A1 (en) * | 2012-08-06 | 2014-02-06 | CELSYS, Inc. | Object correcting apparatus and method and computer-readable recording medium |
US20170193289A1 (en) * | 2015-12-31 | 2017-07-06 | Microsoft Technology Licensing, Llc | Transform lightweight skeleton and using inverse kinematics to produce articulate skeleton |
Cited By (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
KR20210112848A (en) | 2020-03-06 | 2021-09-15 | 경북대학교 산학협력단 | Method for generating rotated hand bone 2d projection image from 1 hand bone 2d projection image, recording medium and device for performing the method |
WO2022065763A1 (en) * | 2020-09-22 | 2022-03-31 | Samsung Electronics Co., Ltd. | Display apparatus and method for controlling thereof |
KR102367584B1 (en) * | 2021-11-04 | 2022-02-25 | 주식회사 티지 | Automatic video surveillance system using skeleton video analysis technique |
Also Published As
Publication number | Publication date |
---|---|
US11544871B2 (en) | 2023-01-03 |
EP3724810A1 (en) | 2020-10-21 |
US20230108253A1 (en) | 2023-04-06 |
CN111492367A (en) | 2020-08-04 |
CN111492367B (en) | 2024-03-05 |
KR102329781B1 (en) | 2021-11-22 |
WO2019118058A1 (en) | 2019-06-20 |
US20190180473A1 (en) | 2019-06-13 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
KR102329781B1 (en) | Learning the hand skeleton from 2D images, lifting and denoising | |
JP6644833B2 (en) | System and method for rendering augmented reality content with albedo model | |
Yao et al. | Contour model-based hand-gesture recognition using the Kinect sensor | |
CN107833271B (en) | Skeleton redirection method and device based on Kinect | |
CN111694429A (en) | Virtual object driving method and device, electronic equipment and readable storage | |
CN104937635B (en) | More hypothesis target tracking devices based on model | |
Wang et al. | Real-time hand-tracking with a color glove | |
JP6011102B2 (en) | Object posture estimation method | |
JP5887775B2 (en) | Human computer interaction system, hand-to-hand pointing point positioning method, and finger gesture determination method | |
US20200193638A1 (en) | Hand tracking based on articulated distance field | |
JP5525407B2 (en) | Behavior model learning device, three-dimensional posture estimation device, behavior model learning method, three-dimensional posture estimation method, and program | |
WO2016132371A1 (en) | Gesture recognition using multi-sensory data | |
US20170193289A1 (en) | Transform lightweight skeleton and using inverse kinematics to produce articulate skeleton | |
JP2019096113A (en) | Processing device, method and program relating to keypoint data | |
US20220283647A1 (en) | User input and virtual touch pad in augmented reality for use in surgical settings | |
JP4938748B2 (en) | Image recognition apparatus and program | |
Stommel et al. | Model-free detection, encoding, retrieval, and visualization of human poses from kinect data | |
Haggag et al. | An adaptable system for rgb-d based human body detection and pose estimation: Incorporating attached props | |
JP6052533B2 (en) | Feature amount extraction apparatus and feature amount extraction method | |
Schröder et al. | Design and evaluation of reduced marker layouts for hand motion capture | |
JP2021144359A (en) | Learning apparatus, estimation apparatus, learning method, and program | |
Liang et al. | Hand pose estimation by combining fingertip tracking and articulated ICP | |
CN113496168B (en) | Sign language data acquisition method, device and storage medium | |
KR102540560B1 (en) | Hierarchical estimation method for hand poses using random decision forests, recording medium and device for performing the method | |
Zanuttigh et al. | Human Pose Estimation and Tracking |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
A201 | Request for examination | ||
E902 | Notification of reason for refusal | ||
E701 | Decision to grant or registration of patent right | ||
GRNT | Written decision to grant |