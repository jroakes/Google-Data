DE202017105835U1 - Neural machine translation systems - Google Patents
Neural machine translation systems Download PDFInfo
- Publication number
- DE202017105835U1 DE202017105835U1 DE202017105835.2U DE202017105835U DE202017105835U1 DE 202017105835 U1 DE202017105835 U1 DE 202017105835U1 DE 202017105835 U DE202017105835 U DE 202017105835U DE 202017105835 U1 DE202017105835 U1 DE 202017105835U1
- Authority
- DE
- Germany
- Prior art keywords
- input
- sequence
- output
- neural
- lstm
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 230000001537 neural effect Effects 0.000 title claims abstract description 117
- 238000013519 translation Methods 0.000 title claims abstract description 84
- 238000000034 method Methods 0.000 claims abstract description 68
- 230000008569 process Effects 0.000 claims abstract description 52
- 238000012545 processing Methods 0.000 claims abstract description 35
- 230000006403 short-term memory Effects 0.000 claims abstract description 3
- 238000004590 computer program Methods 0.000 claims description 22
- 238000013528 artificial neural network Methods 0.000 claims description 10
- 238000004364 calculation method Methods 0.000 claims description 5
- 230000002441 reversible effect Effects 0.000 abstract description 5
- 230000014616 translation Effects 0.000 description 56
- 238000004891 communication Methods 0.000 description 6
- 230000006870 function Effects 0.000 description 6
- 238000010801 machine learning Methods 0.000 description 5
- 230000015654 memory Effects 0.000 description 5
- 238000012549 training Methods 0.000 description 5
- 230000002457 bidirectional effect Effects 0.000 description 3
- 238000010606 normalization Methods 0.000 description 3
- 230000000306 recurrent effect Effects 0.000 description 3
- 230000009286 beneficial effect Effects 0.000 description 2
- 230000003993 interaction Effects 0.000 description 2
- 230000007246 mechanism Effects 0.000 description 2
- 230000003287 optical effect Effects 0.000 description 2
- 230000001737 promoting effect Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 230000026676 system process Effects 0.000 description 2
- BUHVIAUBTBOHAG-FOYDDCNASA-N (2r,3r,4s,5r)-2-[6-[[2-(3,5-dimethoxyphenyl)-2-(2-methylphenyl)ethyl]amino]purin-9-yl]-5-(hydroxymethyl)oxolane-3,4-diol Chemical compound COC1=CC(OC)=CC(C(CNC=2C=3N=CN(C=3N=CN=2)[C@H]2[C@@H]([C@H](O)[C@@H](CO)O2)O)C=2C(=CC=CC=2)C)=C1 BUHVIAUBTBOHAG-FOYDDCNASA-N 0.000 description 1
- 230000002411 adverse Effects 0.000 description 1
- 238000013459 approach Methods 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 150000001875 compounds Chemical class 0.000 description 1
- 238000013500 data storage Methods 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000007726 management method Methods 0.000 description 1
- 239000011159 matrix material Substances 0.000 description 1
- 230000001902 propagating effect Effects 0.000 description 1
- 230000004044 response Effects 0.000 description 1
- 238000005070 sampling Methods 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 238000000926 separation method Methods 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 230000009466 transformation Effects 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/40—Processing or translation of natural language
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/40—Processing or translation of natural language
- G06F40/58—Use of machine translation, e.g. for multi-lingual retrieval, for server-side translation for client devices or for real-time translation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/40—Processing or translation of natural language
- G06F40/42—Data-driven translation
- G06F40/44—Statistical methods, e.g. probability models
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/40—Processing or translation of natural language
- G06F40/55—Rule-based translation
- G06F40/56—Natural language generation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
- G06N3/0442—Recurrent networks, e.g. Hopfield networks characterised by memory or gating, e.g. long short-term memory [LSTM] or gated recurrent units [GRU]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
- G06N3/0455—Auto-encoder networks; Encoder-decoder networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
Abstract
Neuronales Maschinenübersetzungssystem, das durch einen oder mehrere Computer implementiert ist, wobei das neuronale Maschinenübersetzungssystem zum Empfangen einer Eingabesequenz von Eingabe-Token, die eine erste Sequenz von Wörtern in einer ersten natürlichen Sprache darstellt, und zum Generieren einer Ausgabesequenz von Ausgabe-Token, die eine zweite Sequenz von Wörtern darstellt, die eine Übersetzung der ersten Sequenz in eine zweite natürliche Sprache ist, konfiguriert ist, wobei das neuronale Maschinenübersetzungssystem umfasst: ein neuronales Codierer-Netzwerk, umfassend: eine Eingabevorwärts long short-term memory (LSTM) Schicht, die zum Verarbeiten von jedem Eingabe-Token in der Eingabesequenz in einer Vorwärtsreihenfolge zum Generieren einer jeweiligen Vorwärtsdarstellung jedes Eingabe-Tokens konfiguriert ist, eine Eingaberückwärts-LSTM-Schicht, die zum Verarbeiten von jedem Eingabe-Token in der Eingabesequenz in einer Rückwärtsreihenfolge zum Generieren einer jeweiligen Rückwärtsdarstellung von jedem Eingabe-Token konfiguriert ist, eine Kombinationsschicht, die konfiguriert ist, um für jedes Eingabe-Token in der Eingabesequenz die Vorwärtsdarstellung des Eingabe-Tokens und die Rückwärtsdarstellung des Eingabe-Tokens zum Generieren einer kombinierten Darstellung des Eingabe-Tokens zu kombinieren, und eine Vielzahl verborgener LSTM-Schichten, die zum Generieren einer jeweiligen codierten Darstellung von jedem der Eingabe-Token in der Eingabesequenz konfiguriert sind; und ein Decodierer-Subsystem, das zum Empfangen der jeweiligen codierten Darstellung von jedem der Eingabe-Token in der Eingabesequenz und zum Verarbeiten der codierten Darstellungen konfiguriert ist, um die Ausgabesequenz zu generieren.A neural machine translation system implemented by one or more computers, the neural machine translation system for receiving an input sequence of input tokens representing a first sequence of words in a first natural language and for generating an output sequence of output tokens comprising a representing a second sequence of words representing a translation of the first sequence into a second natural language, the neural engine translation system comprising: a neural encoder network comprising: an input forward long short-term memory (LSTM) layer associated with Processing each input token in the input sequence is configured in a forward order to generate a respective forward representation of each input token, an input backward LSTM layer adapted to process each input token in the input sequence in a reverse order to generate a respective backward representation of each input token is configured, a combination layer configured to combine, for each input token in the input sequence, the forward representation of the input token and the backward representation of the input token to generate a combined representation of the input token and a plurality of hidden LSTM layers configured to generate a respective coded representation of each of the input tokens in the input sequence; and a decoder subsystem configured to receive the respective coded representation of each of the input tokens in the input sequence and to process the coded representations to generate the output sequence.
Description
HINTERGRUND BACKGROUND
Diese Spezifikation betrifft Übersetzen von Text unter Verwendung neuronaler Netzwerke. This specification relates to translating text using neural networks.
Maschinenlernmodelle empfangen eine Eingabe und generieren eine Ausgabe, z. B. eine vorhergesagte Ausgabe, basierend auf der empfangenen Eingabe. Einige Maschinenlernmodelle sind parametrische Modelle und generieren die Ausgabe basierend auf der empfangenen Eingabe und auf Werten der Parameter des Modells. Machine learning models receive an input and generate an output, e.g. A predicted output based on the received input. Some machine learning models are parametric models and generate the output based on the received input and on values of the parameters of the model.
Einige Maschinenlernmodelle sind tiefe Modelle, die mehrere Schichten von Modellen verwenden, um eine Ausgabe für eine empfangene Eingabe zu generieren. Beispielsweise ist ein tiefes neuronales Netzwerk ein tiefes Maschinenlernmodell, das eine Ausgabeschicht und eine oder mehrere verborgene Schichten beinhaltet, wovon jede eine nicht lineare Transformation auf eine empfangene Eingabe anwendet, um eine Ausgabe zu generieren. Some machine learning models are deep models that use multiple layers of models to generate output for a received input. For example, a deep neural network is a deep machine learning model that includes an output layer and one or more hidden layers, each of which applies a non-linear transformation to a received input to generate an output.
Einige neuronale Netzwerke sind wiederkehrende neuronale Netzwerke. Ein wiederkehrendes neuronales Netzwerk ist ein neuronales Netzwerk, das eine Eingabesequenz empfängt und eine Ausgabesequenz von der Eingabesequenz generiert. Insbesondere verwendet ein wiederkehrendes neuronales Netzwerk einige oder alle der internen Status des Netzwerks nach dem Verarbeiten einer vorherigen Eingabe in der Eingabesequenz durch Erzeugen einer Ausgabe von der aktuellen Eingabe in der Eingabesequenz. Some neural networks are recurrent neural networks. A recurrent neural network is a neural network that receives an input sequence and generates an output sequence from the input sequence. In particular, a recurrent neural network uses some or all of the internal status of the network after processing a previous input in the input sequence by generating an output from the current input in the input sequence.
KURZDARSTELLUNG SUMMARY
Diese Spezifikation beschreibt ein als Computerprogramme auf einem oder mehreren Computern an einem oder mehreren Standorten implementiertes System, das Text unter Verwendung neuronaler Netzwerke von einer Quellsprache in eine Zielsprache übersetzt. This specification describes a system implemented as computer programs on one or more computers at one or more sites that translates text from a source language to a target language using neural networks.
In einigen Aspekten des beschriebenen Gegenstandes ist ein durch einen oder mehrere Computer implementiertes neuronales Maschinenübersetzungssystem zum Empfangen einer Eingabesequenz von Eingabe-Token konfiguriert, die eine erste Sequenz von Wörtern in einer ersten natürlichen Sprache darstellen, und zum Generieren einer Ausgabesequenz von Ausgabe-Token, die eine zweite Sequenz von Wörtern darstellt, die eine Übersetzung der ersten Sequenz in eine zweite natürliche Sprache ist, und umfasst: ein neuronales Codierer-Netzwerk und ein Decodierer-Subsystem. Das neuronale Codierer-Netzwerk umfasst wiederum: eine Eingabevorwärts long-short-term memory (LSTM) Schicht (auf Deutsch: Lang-Kurzzeitspeicherschicht), die zum Verarbeiten von jedem Eingabe-Token in der Eingabesequenz in einer Vorwärtsreihenfolge zum Generieren einer jeweiligen Vorwärtsdarstellung jedes Eingabe-Tokens konfiguriert ist, eine Eingaberückwärts-LSTM-Schicht, die zum Verarbeiten von jedem Eingabe-Token in der Eingabesequenz in einer Rückwärtsreihenfolge zum Generieren einer jeweiligen Rückwärtsdarstellung von jedem Eingabe-Token konfiguriert ist, eine Kombinationsschicht, die konfiguriert ist, um für jedes Eingabe-Token in der Eingabesequenz die Vorwärtsdarstellung und die Rückwärtsdarstellung des Eingabe-Tokens zum Generieren einer kombinierten Darstellung des Eingabe-Tokens zu kombinieren, und eine Vielzahl verborgener LSTM-Schichten, die zum Generieren einer jeweiligen codierten Darstellung von jedem der Eingabe-Token in der Eingabesequenz konfiguriert sind. Das Decodierer-Subsystem ist zum Empfangen der jeweiligen codierten Darstellung von jedem der Eingabe-Token in der Eingabesequenz, und zum Verarbeiten der codierten Darstellungen konfiguriert, um die Ausgabesequenz zu generieren. In some aspects of the described subject matter, a computer implemented neural machine translation system is configured to receive an input sequence of input tokens representing a first sequence of words in a first natural language and to generate an output sequence of output tokens represents a second sequence of words that is a translation of the first sequence into a second natural language and includes: a neural encoder network and a decoder subsystem. The neural encoder network, in turn, comprises: an input long-short-term memory (LSTM) layer, for processing each input token in the input sequence in a forward order to generate a respective forward representation of each input An input backward LSTM layer configured to process each input token in the input sequence in a backward order to generate a respective backward representation of each input token, a combination layer configured to be for each input To combine in the input sequence the forward and backward representations of the input token to generate a combined representation of the input token, and a plurality of hidden LSTM layers used to generate a respective coded representation of each of the input tokens in the input sequence configured sin d. The decoder subsystem is configured to receive the respective coded representation of each of the input tokens in the input sequence, and to process the coded representations to generate the output sequence.
In einigen dieser Aspekte werden die verborgenen LSTM-Schichten nacheinander in einem Stapel angeordnet, wobei die Vielzahl verborgener LSTM-Schichten mindestens sieben LSTM-Schichten umfasst. In einigen dieser Aspekte werden mindestens zwei der Vielzahl verborgener LSTM-Schichten mit einer Restverbindung verbunden. In some of these aspects, the hidden LSTM layers are sequentially stacked, with the plurality of hidden LSTM layers comprising at least seven LSTM layers. In some of these aspects, at least two of the plurality of hidden LSTM layers are connected to a remainder link.
In einigen dieser Aspekte umfasst das Decodierer-Subsystem: ein neuronales Decodierer-Netzwerk, wobei das neuronale Decodierer-Netzwerk umfasst: eine Vielzahl von LSTM-Schichten, die in einem Stapel aufeinander angeordnet, und für jede von einer Vielzahl von Positionen in der Ausgabesequenz konfiguriert sind zum: Empfangen eines Aufmerksamkeitskontextvektors und des Ausgabe-Tokens an einer vorangegangenen Position in der Ausgabesequenz, und Verarbeiten des Aufmerksamkeitskontextvektors und des Tokens an der vorangegangenen Position in der Ausgabesequenz zum Generieren einer LSTM-Ausgabe für die Position, und eine Softmax-Ausgabeschicht, wobei die Softmax-Ausgabeschicht zum Empfangen der LSTM-Ausgabe für die Position für jede der Vielzahl von Positionen, und zum Generieren einer jeweiligen Punktzahl für jedes Ausgabe-Token in einem Vokabular von Ausgabe-Token konfiguriert ist. In some of these aspects, the decoder subsystem comprises: a neural decoder network, the neural decoder network comprising: a plurality of LSTM layers arranged in a stack, and configured for each of a plurality of positions in the output sequence are to receive an attention context vector and the output token at a previous position in the output sequence, and process the attention context vector and the token at the previous position in the output sequence to generate an LSTM output for the position, and a softmax output layer the Softmax output layer is configured to receive the LSTM output for the position for each of the plurality of positions, and to generate a respective score for each output token in a vocabulary of output tokens.
In einigen dieser Aspekte umfasst die Vielzahl von LSTM-Schichten in dem Decodierer mindestens acht LSTM-Schichten. In einigen dieser Aspekte umfasst die Vielzahl von LSTM-Schichten in dem Decodierer mindestens acht LSTM-Schichten. In einigen dieser Aspekte sind mindestens zwei der Vielzahl in dem Decodierer mit einer Restverbindung verbunden. In einigen dieser Aspekte ist eine erste LSTM-Schicht in dem Stapel für jede der Vielzahl von Positionen konfiguriert zum: Empfangen einer ersten Schichteingabe, die den Aufmerksamkeitskontextvektor umfasst, einer Schichtausgabe, die durch eine LSTM-Schicht direkt unter der ersten LSTM-Schicht in dem Stapel generiert wird, und einer Schichteingabe in die LSTM-Schicht direkt unter der ersten LSTM-Schicht in dem Stapel; und Verarbeiten der ersten Schichteingabe gemäß einem aktuellen verborgenen Status der ersten LSTM-Schicht zum Generieren einer ersten Schichtausgabe und zum Aktualisieren des aktuellen verborgenen Status. In some of these aspects, the plurality of LSTM layers in the decoder includes at least eight LSTM layers. In some of these aspects, the plurality of LSTM layers in the decoder includes at least eight LSTM layers. In some of these aspects, at least two of the plurality in the decoder are connected to a remainder link. In some of these aspects is a first LSTM layer in the stack for each of the plurality of positions configured to: receive a first layer input comprising the attention context vector, a layer output generated by an LSTM layer directly under the first LSTM layer in the stack, and layer input into the LSTM layer directly under the first LSTM layer in the stack; and processing the first layer input according to a current hidden state of the first LSTM layer to generate a first layer output and to update the current hidden state.
In einigen dieser Aspekte umfasst das Decodierer-Subsystem ferner: ein Aufmerksamkeits-Subsystem, wobei das Aufmerksamkeits-Subsystem für jede der Vielzahl von Positionen konfiguriert ist zum: Empfangen einer Aufmerksamkeitseingabe, die eine unterste Schichtausgabe umfasst, die durch eine unterste LSTM-Schicht in dem Stapel von LSTM-Schichten in dem neuronalen Decodierer-Netzwerk für die vorangegangene Position generiert wird; und Verarbeiten der Aufmerksamkeitseingabe zum Generieren des Aufmerksamkeitskontextvektors für den Zeitschritt. In some of these aspects, the decoder subsystem further includes: an attention subsystem, wherein the attention subsystem for each of the plurality of locations is configured to: receive an attention input comprising a lowest layer output that is defined by a lowermost LSTM layer in the one Stack of LSTM layers in the neural decoder network for the previous position is generated; and processing the attention input to generate the attention context vector for the time step.
In einigen dieser Aspekte umfasst Verarbeiten der untersten Schichtausgabe und der jeweiligen codierten Darstellung von jedem der Eingabe-Token in der Eingabesequenz zum Generieren des Aufmerksamkeitskontextvektors für den Zeitschritt für jeden der Vielzahl von Zeitschritten: Verarbeiten der untersten Schichtausgabe und der codierten Darstellungen durch eine oder mehrere Netzwerkschichten zum Generieren einer jeweiligen Punktzahl für jedes der Eingabe-Token; Generieren eines jeweiligen Gewichts für jedes der Eingabe-Token von den jeweiligen Punktzahlen; und Bestimmen einer gewichteten Summe der codierten Darstellung gemäß den Gewichten. In some of these aspects, processing the lowest layer output and the respective coded representation of each of the input tokens in the input sequence to generate the attention context vector for the time step for each of the plurality of time steps comprises: processing the lowest layer output and the coded representations through one or more network layers for generating a respective score for each of the input tokens; Generating a respective weight for each of the input tokens from the respective scores; and determining a weighted sum of the coded representation in accordance with the weights.
In einigen dieser Aspekte sind die Eingabe- und Ausgabe-Token Wortteile. In some of these aspects, the input and output tokens are word parts.
In einigen dieser Aspekte sind das neuronale Codierer-Netzwerk und das Decodierer-Subsystem zum Ausführen quantifizierter Berechnungen während des Erzeugens der Ausgabesequenz konfiguriert. In some of these aspects, the neural encoder network and the decoder subsystem are configured to perform quantified calculations during generation of the output sequence.
Andere Aspekte beinhalten ein oder mehrere Computerspeichermedien, die mit Anweisungen codiert sind, die, wenn sie durch einen oder mehrere Computer ausgeführt werden, den einen oder die mehreren Computer zum Implementieren des neuronalen Maschinenübersetzungssystems von einem beliebigen der oben genannten Aspekte veranlassen. Other aspects include one or more computer storage media encoded with instructions that, when executed by one or more computers, cause the one or more computers to implement the neural machine translation system of any of the above aspects.
Einige andere Aspekte beinhalten Computerprogrammprodukte, die Anweisungen enthalten, die, wenn sie durch einen Prozessor oder mehrere Prozessoren ausgeführt werden, den einen Prozessor oder die mehreren Prozessoren veranlassen, ein Verfahren zum Erzeugen einer Ausgabesequenz von Ausgabe-Token durchzuführen, die eine zweite Sequenz von Wörtern darstellt, die eine Übersetzung der ersten Sequenz von Wörtern in eine zweite natürliche Sprache ist, die die folgenden Aktionen beinhaltet: Erhalten einer Vielzahl von Kandidatenausgabesequenzen von einem neuronalen Maschinenübersetzungssystem; Aufrechterhalten höchstens einer zuvor festgelegten Anzahl der Kandidatenausgabesequenzen in einem Strahl zur weiteren Berücksichtigung durch das neuronale Maschinenübersetzungssystem, umfassend: Zuweisen einer jeweiligen Punktzahl für jede Kandidatenausgabesequenz; Einstufen der Kandidatenausgabesequenzen basierend auf den jeweiligen Punktzahlen; Entfernen aller Kandidatenausgabesequenzen von dem Strahl außer der zuvor festgelegten Anzahl von am höchsten eingestuften Kandidatenausgabesequenzen gemäß der Rangfolge; Bestimmen, ob eine beliebige der Kandidatenausgabesequenzen in dem Strahl als eine finalisierte Kandidatenausgabesequenz durch das neuronale Maschinenübersetzungssystem identifiziert wurde; und wenn mindestens eine der Kandidatenausgabesequenzen als eine finalisierte Kandidatenausgabesequenz identifiziert wurde: Entfernen aller Kandidatenausgabesequenzen von dem Strahl, die eine Punktzahl aufweisen, die mehr als ein Schwellenwert ist, der unter der Punktzahl für eine am höchsten eingestufte finalisierte Kandidatenausgabesequenz liegt. Some other aspects include computer program products that contain instructions that, when executed by one or more processors, cause the one or more processors to perform a method of generating an output sequence of output tokens that include a second sequence of words representing a translation of the first sequence of words into a second natural language, comprising the following actions: obtaining a plurality of candidate output sequences from a neural machine translation system; Maintaining at most a predetermined number of the candidate output sequences in a beam for further consideration by the neural machine translation system, comprising: assigning a respective score for each candidate output sequence; Ranking the candidate output sequences based on the respective scores; Removing all candidate output sequences from the beam other than the predetermined number of highest ranked candidate output sequences according to the ranking; Determining whether any one of the candidate output sequences in the beam has been identified as a finalized candidate output sequence by the neural machine translation system; and if at least one of the candidate output sequences has been identified as a finalized candidate output sequence: removing all candidate output sequences from the beam having a score that is more than a threshold below the score for a highest ranked finalized candidate output sequence.
In einigen dieser Aspekte umfasst Bestimmen, ob beliebige der Kandidatenausgabesequenzen als eine finalisierte Kandidatenausgabesequenz durch das neuronale Maschinenübersetzungssystem identifiziert wurden: Bestimmen, ob eine beliebige Kandidatenausgabesequenz mit einem zuvor festgelegten Satzende-Ausgabe-Token endet. In some of these aspects, determining whether any of the candidate output sequences have been identified as a finalized candidate output sequence by the neural machine translation system comprises determining whether any candidate output sequence ends with a predetermined end-of-sentence output token.
In einigen dieser Aspekte umfasst Zuweisen einer jeweiligen Punktzahl zu jeder Kandidatenausgabesequenz: Bestimmen einer Log-Likelihood (auf Deutsch: Protokollwahrscheinlichkeit) der Kandidatenausgabesequenz gemäß den durch das neuronale Maschinenübersetzungssystem generierten Ausgabepunktzahlen; und Normalisieren der Log-Likelihood basierend auf einer Anzahl von Ausgabe-Token in der Kandidatenausgabesequenz zum Generieren einer längennormalisierten Log-Likelihood. In some of these aspects, assigning a respective score to each candidate output sequence comprises: determining a log likelihood of the candidate output sequence in accordance with the output score generated by the neural machine translation system; and normalizing the log likelihood based on a number of output tokens in the candidate output sequence to generate a length normalized log likelihood.
In einigen dieser Aspekte umfasst Zuweisen einer jeweiligen Punktzahl zu jeder Kandidatenausgabesequenz ferner: Modifizieren der längennormalisierten Log-Likelihood unter Verwendung einer Abdeckungsstrafe, um Kandidatenausgabesequenzen zu begünstigen, die die Wörter in der ersten Sequenz vollständig abdecken. In some of these aspects, assigning a respective score to each candidate output sequence further comprises: modifying the length normalized log likelihood using a coverage penalty to favor candidate output sequences that completely cover the words in the first sequence.
In einigen dieser Aspekte, wenn alle der Kandidatenausgabesequenzen in dem Strahl als eine finalisierte Kandidatenausgabesequenz durch das neuronale Maschinenübersetzungssystem identifiziert wurden, beinhalten die Aktionen ferner Auswählen einer Kandidatenausgabesequenz mit der höchsten Punktzahl als die Ausgabesequenz. In some of these aspects, when all of the candidate output sequences in the beam have been identified as a finalized candidate output sequence by the neural machine translation system, the actions further include selecting a candidate output sequence having the highest score as the output sequence.
Andere Aspekte beinhalten ein oder mehrere Computerspeichermedien, die mit Anweisungen codiert sind, die, wenn sie durch einen oder mehrere Computer ausgeführt werden, den einen oder die mehreren Computer zum Ausführen der Vorgänge nach einem beliebigen der oben genannten Verfahren veranlassen. Other aspects include one or more computer storage media encoded with instructions that, when executed by one or more computers, cause the one or more computers to perform the operations according to any of the above-referenced methods.
Andere Aspekte beinhalten ein System, das einen oder mehrere Computer und ein oder mehrere Speichergeräte, die Anweisungen speichern, die, wenn sie durch einen oder mehrere Computer ausgeführt werden, den einen oder die mehreren Computer nach einem beliebigen der oben genannten Verfahren veranlassen, die Vorgänge auszuführen. Other aspects include a system that includes one or more computers and one or more storage devices that store instructions that, when executed by one or more computers, cause the one or more computers to perform the operations in accordance with any one of the above methods perform.
Bestimmte Ausführungsformen des in dieser Spezifikation beschriebenen Gegenstands können so implementiert werden, dass sie einen oder mehrere der folgenden Vorteile verwirklichen. Certain embodiments of the subject matter described in this specification may be implemented to achieve one or more of the following advantages.
Durch Verwenden einer bidirektionalen LSTM-Schicht als die unterste Schicht des Codierers kann der Kontext für die Wörter in der Eingabesequenz effektiv erfasst werden, was die Übersetzungsqualität verbessert. Da die anderen Codiererschichten unidirektional sind, kann Parallelisierung während Berechnungen immer noch erreicht werden, wodurch Trainings- und in manchen Fällen Schlussfolgerungszeiten reduziert werden. By using a bidirectional LSTM layer as the lowest layer of the encoder, the context for the words in the input sequence can be effectively detected, which improves the translation quality. Because the other encoder layers are unidirectional, parallelization can still be achieved during computations, thereby reducing training and, in some cases, inference times.
Durch Verwenden tiefer Stapel von LSTM-Schichten im Codierer und im Decodierer, z. B. 8 oder mehr gesamte LSTM-Schichten in jedem von dem Decodierer und dem Codierer, sind der Codierer und Decodierer tief genug, um feine Unregelmäßigkeiten in der Quell- und Zielsprache zu erfassen, was die Genauigkeit von durch das System generierten Übersetzungen verbessert. By using deep stacks of LSTM layers in the encoder and in the decoder, e.g. B. 8 or more entire LSTM layers in each of the decoder and encoder, the encoder and decoder are deep enough to detect fine irregularities in the source and target languages, which improves the accuracy of translations generated by the system.
Durch Verbinden der untersten Schicht des Decodierers mit der obersten Schicht des Codierers durch das Aufmerksamkeits-Subsystem kann Parallelismus erhöht, und Trainingszeit kann dadurch verringert werden, wobei immer noch ein effektives Aufmerksamkeitsschema verwendet wird, das es dem neuronalen Decodierer-Netzwerk ermöglicht, die Quellsequenz genau zu übersetzen. Das heißt, weil das Aufmerksamkeits-Subsystem Eingaben von der untersten LSTM-Schicht in dem Decodierer, und nicht von beliebigen der anderen LSTM-Schichten in dem Decodierer und anders als bei herkömmlichen Aufmerksamkeitsschemen empfängt, kann die durch das Aufmerksamkeits-Subsystem ausgeführte Berechnung parallel zu anderen Berechnungen effektiv ausgeführt werden, die durch die anderen LSTM-Schichten ausgeführt werden, ohne die Qualität der Übersetzung negativ zu beeinträchtigen. By connecting the lowermost layer of the decoder to the top layer of the encoder by the attention subsystem, parallelism can be increased, and training time can thereby be reduced, still using an effective attention scheme that allows the neural decoder network to accurately match the source sequence to translate. That is, because the attention subsystem receives inputs from the lowest LSTM layer in the decoder, rather than any of the other LSTM layers in the decoder and unlike conventional attention schemes, the computation performed by the attention subsystem may be parallel to other calculations performed by the other LSTM layers without adversely affecting the quality of the translation.
Durch Verwenden von Arithmetik mit geringer Präzision, d. h. quantifizierter Berechnung, während Schlussfolgerungsberechnungen, wie in dieser Spezifikation beschrieben, kann die endgültige Übersetzungszeit beschleunigt werden, während hohe Übersetzungsqualität aufrechterhalten wird. By using low precision arithmetic, i. H. quantified computation, while inference computations as described in this specification can speed up the final compile time while maintaining high translation quality.
Durch Decodieren unter Verwendung der beschriebenen Strahlensuchtechniken kann die Übersetzungszeit bei Schlussfolgerung verringert werden, während eine hohe Übersetzungsqualität relativ zu herkömmlichen Strahlensuchtechniken aufrechterhalten wird. By decoding using the described beam search techniques, the compile time can be reduced in conclusion while maintaining high translation quality relative to conventional beam search techniques.
Die Details von einer oder mehreren Ausführungsformen des in dieser Patentschrift beschriebenen Gegenstandes sind in den begleitenden Zeichnungen und der nachfolgenden Beschreibung dargelegt. Weitere Merkmale, Aspekte und Vorteile des Gegenstands werden anhand der Beschreibung, der Zeichnungen und der Patentansprüche offensichtlich. The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other features, aspects and advantages of the subject matter will become apparent from the description, the drawings and the claims.
KURZE BESCHREIBUNG DER ZEICHNUNGEN BRIEF DESCRIPTION OF THE DRAWINGS
In den unterschiedlichen Zeichnungen werden gleiche Bezugszeichen und Bezeichnungen für gleiche Elemente verwendet. In the various drawings, like reference numerals and designations are used for like elements.
AUSFÜHRLICHE BESCHREIBUNG DETAILED DESCRIPTION
Das neuronale Maschinenübersetzungssystem
Das neuronale Maschinenübersetzungssystem beinhaltet ein neuronales Codierer-Netzwerk
Zum Generieren einer Eingabe in das neuronale Codierer-Netzwerk
Insbesondere sind die Worteinheiten in einigen dieser Implementierungen Wortteile und das System
Das neuronale Codierer-Netzwerk
Im Allgemeinen beinhaltet das neuronale Codierer-Netzwerk
Das neuronale Decodierer-Netzwerk
Im Allgemeinen beinhaltet das Ausgabevokabular ein jeweiliges Token für jede von einer Gruppe von Worteinheiten in der natürlichen Zielsprache. Zusätzlich zu den Worteinheiten beinhaltet das Vokabular auch ein oder mehrere speziell bezeichnete Token, z. B. ein Satzende-Token. In einigen Implementierungen sind die Worteinheiten Wörter in der natürlichen Zielsprache. In einigen anderen Implementierungen sind Worteinheiten Subworteinheiten, z. B. Zeichen, gemischte Wörter/Zeichen, Wortteile usw. für Wörter in der natürlichen Zielsprache. In einigen Fällen, wenn die Worteinheiten Wortteile sind, wird der Wortteil von der Quell- und Zielsprache gemeinsam genutzt, um zu ermöglichen, dass Strings von der Quellsequenz zur Zielsequenz kopiert werden. In general, the output vocabulary includes a respective token for each of a group of word units in the natural target language. In addition to the word units, the vocabulary also includes one or more specially designated tokens, e.g. For example, a sentence end token. In some implementations, the word units are words in the natural target language. In some other implementations, word units are subword units, e.g. Characters, mixed words / characters, parts of words, etc. for words in the natural target language. In some cases, when the word units are word parts, the word part is shared by the source and target languages to allow strings to be copied from the source sequence to the target sequence.
Im Allgemeinen ist das neuronale Decodierer-Netzwerk
Insbesondere beinhaltet das neuronale Decodierer-Netzwerk
Der Betrieb des neuronalen Codierer-Netzwerks wird nachfolgend unter Bezugnahme auf
Das Aufmerksamkeits-Subsystem
Zum Generieren der Ausgabe an einer gegebenen Position in der Ausgabesequenz
Das System
In einigen Implementierungen generiert das System
Sobald die Ausgabesequenz
In einigen Implementierungen sind das neuronale Codierer-Netzwerk
Das System empfängt eine Eingabesequenz von Token (Schritt
Das System verarbeitet die Eingabesequenz von Token unter Verwendung eines neuronalen Codierer-Netzwerks, um eine jeweilige codierte Darstellung für jedes der Token in der Eingabesequenz zu generieren (Schritt
Das System verarbeitet die codierten Darstellungen unter Verwendung eines Decodierer-Subsystems zum Generieren einer Ausgabesequenz von Ausgabe-Token, die eine Sequenz von Wörtern darstellt, die eine Übersetzung der ersten Sequenz von Wörtern in eine andere natürliche Sprache ist (Schritt
Das neuronale Codierer-Netzwerk verarbeitet die Eingabesequenz durch eine Eingabevorwärts-LSTM-Schicht (Schritt
Das neuronale Codierer-Netzwerk verarbeitet die Eingabesequenz durch eine Eingaberückwärts-LSTM-Schicht (Schritt
Das neuronale Codierer-Netzwerk verarbeitet die Vorwärts- und Rückwärtsdarstellungen des Eingabe-Tokens durch eine Kombinationsschicht (Schritt
Das neuronale Codierer-Netzwerk verarbeitet die kombinierten Darstellungen der Eingabe-Token durch einen Stapel mehrerer verborgener LSTM-Schichten (Schritt
Daher ist die unterste Schicht des neuronalen Codierer-Netzwerks eine bidirektionale LSTM-Schicht, d. h. die Kombination der Eingabevorwärts-, Eingaberückwärts- und Kombinationsschicht verarbeitet effektiv die Eingabesequenz in der Vorwärts- und Rückwärtsrichtung, da die unterste Schicht des Codierers, der Kontext für die Wörter in der Eingabesequenz effektiv erfasst werden kann, was die Übersetzungsqualität verbessert. Da die anderen Codiererschichten, d. h. die verborgenen LSTM-Schichten in dem Stapel, unidirektional sind, kann Parallelisierung während Berechnungen immer noch erreicht werden, wodurch Trainings- und in einigen Fällen Schlussfolgerungszeiten reduziert werden. Therefore, the lowest layer of the neural encoder network is a bidirectional LSTM layer, i. H. the combination of the input forward, input backward, and combination layers effectively processes the input sequence in the forward and reverse directions because the lowest layer of the encoder, the context for the words in the input sequence can be effectively detected, which improves the translation quality. Since the other encoder layers, i. H. while the hidden LSTM layers in the stack are unidirectional, parallelization can still be achieved during computations, thereby reducing training and, in some cases, inference times.
Im Allgemeinen sind viele verborgene LSTM-Schichten in dem Stapel, d. h. sieben oder mehr, vorhanden, um dem neuronalen Codierer-Netzwerk zu ermöglichen, feine Unregelmäßigkeiten in der Quellsprache zu erfassen. In general, many hidden LSTM layers in the stack, i. H. seven or more are present to allow the neural encoder network to detect subtle irregularities in the source language.
Zum Verbessern der Leistung des neuronalen Codierer-Netzwerks durch Fördern von Gradientenstrom zwischen den verborgenen Schichten während des Trainings, beinhaltet das neuronale Codierer-Netzwerk in einigen Implementierungen Restverbindungen zwischen einigen oder allen der verborgenen Schichten in dem Stapel. Beispielsweise beginnen in einigen Implementierungen Restverbindungen von dem Schichtdrittel vom Boden des Codierers, d. h. von der zweiten Schicht vom Boden des Stapels verborgener Schichten. In order to improve the performance of the neural encoder network by promoting gradient current between the hidden layers during training, in some implementations the neural encoder network includes residual connections between some or all of the hidden layers in the stack. For example, in some implementations, remainder links from the third layer start from the bottom of the encoder, i. H. from the second layer from the bottom of the stack of hidden layers.
Wenn eine Restverbindung zwischen einer LSTM-Schicht A und einer LSTM-Schicht B direkt über der Schicht A in einem Stapel von LSTM-Schichten enthalten ist, empfängt Schicht B als Eingabe die Ausgabe von Schicht A und die Eingabe von Schicht A. Schicht B fügt dann pro Element die Ausgabe von Schicht A und die Eingabe von Schicht A hinzu, und verarbeitet dann die Summe als die neue Eingabe in die Schicht. When a residual connection between an LSTM layer A and an LSTM layer B is contained directly above the layer A in a stack of LSTM layers, layer B receives as input the output of layer A and the input of layer A. Layer B then adds the output of layer A and the input of layer A per element, and then processes the sum as the new input into the layer.
Das neuronale Decodierer-Netzwerk erhält einen Aufmerksamkeitskontextvektor für die gegebene Position und das Ausgabe-Token an der vorangegangenen Position, d. h. an der Position, die der gegebenen Position in der Ausgabesequenz unmittelbar vorangeht (Schritt
Das neuronale Decodierer-Netzwerk verarbeitet den Aufmerksamkeitskontextvektor und das Ausgabe-Token durch einen Stapel von LSTM-Schichten, um eine LSTM-Ausgabe für die Position zu generieren (Schritt
Die erste LSTM-Schicht in dem Stapel, d. h. die unterste LSTM-Schicht, ist zum Empfangen des Aufmerksamkeitskontextvektors und des Ausgabe-Tokens, und zum Verarbeiten des Ausgabe-Tokens konfiguriert, um eine unterste Schichtausgabe zu generieren. Jede andere LSTM-Schicht ist zum Empfangen einer Eingabe, die den Aufmerksamkeitskontextvektor und die Schichtausgabe beinhaltet, die durch die LSTM-Schicht direkt vor der Schicht in den Stapel generiert wird, und zum Verarbeiten einer Schichtausgabe für die Eingabe konfiguriert. The first LSTM layer in the stack, i. H. the lowest LSTM layer is configured to receive the attention context vector and the output token, and to process the output token to generate a lowest layer output. Each other LSTM layer is configured to receive an input including the attention context vector and the layer output generated by the LSTM layer just before the layer into the stack, and configured to process a layer output for input.
Wie bei dem neuronalen Codierer-Netzwerk beinhaltet das neuronale Codierer-Netzwerk in einigen Implementierungen Restverbindungen zwischen einigen oder allen der Schichten in dem Stapel zum Verbessern der Leistung des neuronalen Codierer-Netzwerks durch Fördern von Gradientenstrom zwischen den Schichten während des Trainings. Beispielsweise beginnen in einigen Implementierungen Restverbindungen von dem Schichtdrittel von dem Boden des Decodierers. As with the neural encoder network, in some implementations the neural encoder network includes residual connections between some or all of the layers in the stack to improve the performance of the neural encoder network by promoting gradient current between the layers during training. For example, in some implementations, remainder links from the third layer start from the bottom of the decoder.
Das neuronale Decodierer-Netzwerk verarbeitet die LSTM-Ausgabe durch eine Softmax-Ausgabeschicht, um eine jeweilige Punktzahl, z. B. eine jeweilige Wahrscheinlichkeit, für jedes Ausgabe-Token in dem Vokabular von Ausgabe-Token zu generieren (Schritt
Das System kann dann ein Ausgabe-Token für die Position unter Verwendung der Punktzahlen auswählen oder kann, wie nachfolgend beschrieben, die Punktzahlen zum Aktualisieren des aufrechterhaltenen Strahls von Kandidatenausgabesequenzen verwenden. The system may then select an output token for the position using the scores, or may use the scores to update the maintained beam of candidate output sequences, as described below.
Das Aufmerksamkeits-Subsystem empfängt die durch die unterste Schicht in dem Stapel von LSTM-Schichten in dem neuronalen Decodierer-Netzwerk generierte LSTM-Ausgabe für die vorangegangene Position in der Ausgabesequenz (Schritt
Für die erste Position in der Ausgabesequenz kann die von dem Aufmerksamkeits-Subsystem empfangene LSTM-Ausgabe eine Platzhalterausgabe sein, oder kann z. B. die codierte Darstellung von einem der Token, z. B. das erste Token oder das letzte Token, in der Eingabesequenz sein. For the first position in the output sequence, the LSTM output received from the attention subsystem may be a wildcard output, or may be e.g. B. the coded representation of one of the tokens, z. The first token or the last token in the input sequence.
Das Aufmerksamkeits-Subsystem generiert ein jeweiliges Aufmerksamkeitsgewicht für jede der codierten Darstellungen von der untersten LSTM-Ausgabe, und die codierten Darstellungen für die Eingabe-Token (Schritt
Das Aufmerksamkeits-Subsystem generiert den Aufmerksamkeitskontextvektor für die Ausgabeposition durch Bestimmen einer gewichteten Summe der codierten Darstellungen gemäß der entsprechenden Aufmerksamkeitsgewichte (Schritt
Das System kann wiederholt den Prozess
Das System erhält eine Vielzahl von Kandidatenausgabesequenzen von einem neuronalen Maschinenübersetzungssystem, z. B. dem neuronalen Maschinenübersetzungssystem
In einigen Implementierungen verwirft das System für jede Kandidatensequenz, die sich in dem Strahl befand, jedoch nicht finalisiert wurde, die Erweiterungen der Kandidatensequenz, die eine lokale Punktzahl aufweisen, die mehr als ein Schwellenwert unter einer höchsten lokalen Punktzahl ist. In some implementations, for each candidate sequence that was in the beam but has not been finalized, the system discards the extensions of the candidate sequence that have a local score that is more than a threshold below a highest local score.
Das System hält dann höchstens eine zuvor festgelegte Anzahl der Kandidatenausgabesequenzen in dem Strahl zur weiteren Berücksichtigung durch das neuronale Maschinenübersetzungssystem, d. h. zur weiteren Verarbeitung durch das neuronale Maschinenübersetzungssystem, aufrecht. The system then holds at most a predetermined number of candidate output sequences in the beam for further consideration by the neural machine translation system, i. H. for further processing by the neural machine translation system.
Insbesondere weist das System zum Bestimmen, welche Kandidatenausgabesequenz in dem Strahl aufrechtzuerhalten ist, jeder der Kandidatenausgabesequenzen eine jeweilige Punktzahl zu (Schritt
Insbesondere bestimmt das System die anfängliche Punktzahl für eine gegebene Sequenz basierend auf einer Log-Likelihood der Kandidatenausgabesequenz gemäß den durch das neuronale Maschinenübersetzungssystem generierten Ausgabepunktzahlen. Das heißt dass, wie oben beschrieben, bei jeder Position in einer Ausgabesequenz ein neuronales Maschinenübersetzungssystem eine jeweilige Punktzahl für jedes der mehreren möglichen Token in der Ausgabesequenz generiert. Zum Bestimmen der anfänglichen Punktzahl für eine Kandidatenausgabesequenz bestimmt das System eine Summe der Logarithmen der Punktzahlen, die für jedes der Token in der Sequenz durch das neuronale Maschinenübersetzungssystem an der Position generiert wurden, an der das Token positioniert ist. Da die durch das Übersetzungssystem generierten Punktzahlen im Allgemeinen Wahrscheinlichkeiten sind, wird die anfängliche Punktzahl typischerweise negativ sein, d. h. weil der Logarithmus einer Zahl zwischen Null und Eins eine negative Zahl ist. In particular, the system determines the initial score for a given sequence based on a log likelihood of the candidate output sequence according to the output score generated by the neural engine translation system. That is, as described above, at each position in an output sequence, a neural machine translation system generates a respective score for each of the multiple possible tokens in the output sequence. To determine the initial score for a candidate output sequence, the system determines a sum of the logarithms of the scores generated for each of the tokens in the sequence by the neural machine translation system at the position where the token is located. Because of the translation system Generated scores are generally probabilities, the initial score will typically be negative, that is, because the logarithm of a number between zero and one is a negative number.
In einigen Implementierungen verwirft das System vor dem Berechnen beliebiger anfänglicher Punktzahlen und für jede Kandidatensequenz der vorherigen Iteration des Prozesses jede beliebige Erweiterung der Kandidatensequenz, bei der die Punktzahl, die dem neuen Token durch das neuronale Maschinenübersetzungssystem zugewiesen war, um mehr als einen Schwellenwert niedriger als die höchste Punktzahl war, die einer beliebigen Erweiterung der Kandidatensequenz zugewiesen wurde. In some implementations, before computing any initial scores and for each candidate sequence of the previous iteration of the process, the system discards any candidate sequence extension in which the score assigned to the new token by the neural machine translation system is more than one threshold lower than was the highest score assigned to any extension of the candidate sequence.
In einigen Implementierungen, bei denen das System eine Längennormalisierungsstrafe anwendet, normalisiert das System dann die anfängliche Punktzahl für die Kandidatensequenz basierend auf einer Anzahl von Ausgabe-Token, die sich in der Ausgabesequenz befinden. Im Allgemeinen wendet das System die Strafe durch Teilen der anfänglichen Punktzahl durch eine Strafbedingung an, die umso größer ist, je mehr Ausgabe-Token sich in der Ausgabesequenz befinden. Verwenden einer solchen Strafe ermöglicht dem System, effektiv Kandidatensequenzen unterschiedlicher Längen während des Decodierens zu vergleichen, d. h. ohne kürzere Ergebnisse gegenüber längeren übermäßig zu begünstigen. In einigen Implementierungen ist die Strafbedingung die Länge der Sequenz, d. h. die Anzahl von Token, die sich in der Sequenz befinden. In einigen anderen Implementierungen ist die Strafbedingung die Längeα, wobei α eine Konstante zwischen Null und Eins ist, z. B. ein Wert zwischen 0,6 und 0,7, und Länge die Länge der Sequenz ist, d. h. die Anzahl von Token in der Sequenz. In weiteren Implementierungen kann die Strafe lp erfüllen: In some implementations where the system applies a length normalization penalty, the system then normalizes the initial score for the candidate sequence based on a number of issue tokens that are in the issue sequence. In general, the system applies the penalty by dividing the initial score by a penalty condition, which is the greater the more issue tokens are in the issue sequence. Using such a penalty allows the system to effectively compare candidate sequences of different lengths during decoding, ie, without unduly favoring shorter results over longer ones. In some implementations, the penalty condition is the length of the sequence, that is, the number of tokens that are in the sequence. In some other implementations, the penalty condition is the length α , where α is a constant between zero and one, e.g. A value between 0.6 and 0.7, and length is the length of the sequence, ie the number of tokens in the sequence. In further implementations, the punishment lp may be met:
In Implementierungen, bei denen das System eine Abdeckungsstrafe anwendet, fügt das System dann eine Strafbedingung zu der normalisierten anfänglichen Punktzahl (oder wenn Längennormalisierung nicht verwendet wird, die anfängliche Punktzahl) hinzu. Im Allgemeinen begünstigt die Abdeckungsstrafenbedingung Kandidatenausgabesequenzen, die die Wörter in der ersten Sequenz vollständig abdecken, wie durch den Aufmerksamkeitsmechanismus bestimmt, der durch das neuronale Maschinenübersetzungssystem verwendet wird. Beispielsweise kann die Abdeckungsstrafenbedingung cp erfüllen:
Das System stuft die Kandidatenausgabesequenzen basierend auf den endgültigen Punktzahlen ein, d. h. von der Kandidatenausgabesequenz, die die höchste endgültige Punktzahl aufweist, zu der Sequenz, die die niedrigste Punktzahl aufweist (Schritt
Das System entfernt von dem Strahl alle Kandidatenausgabesequenzen, außer der zuvor festgelegten Anzahl von am höchsten eingestuften Kandidatenausgabesequenzen, gemäß der Rangfolge (Schritt
Das System bestimmt dann, ob irgendwelche der verbleibenden Kandidatenausgabesequenzen in dem Strahl durch das neuronale Maschinenübersetzungssystem als finalisiert identifiziert wurden (Schritt
Wenn mindestens eine Kandidatenausgabesequenz finalisiert wurde, entfernt das System von dem Strahl alle Kandidatenausgabesequenzen, die eine Punktzahl aufweisen, die mehr als ein Schwellenwert ist, der unter der Punktzahl für die am höchsten eingestufte finalisierte Kandidatenausgabesequenz liegt (Schritt
Wenn keine der Kandidatenausgabesequenzen finalisiert wurde, dann stutzt das System den Strahl nicht weiter, und stellt die Kandidatenausgabesequenzen in dem Strahl dem neuronalen Maschinenübersetzungssystem zur weiteren Verarbeitung bereit (Schritt
In dieser Spezifikation wird der Begriff „konfiguriert“ in Verbindung mit Systemen und Computerprogrammkomponenten verwendet. Ein zur Ausführung bestimmter Operationen oder Aktionen konfiguriertes System aus einem oder mehreren Computern bedeutet, dass auf diesem System Software, Firmware, Hardware oder eine Kombination derselben installiert ist, die im Betrieb das System veranlassen, Operationen oder Aktionen auszuführen. Ein oder mehrere zur Ausführung bestimmter Operationen oder Aktionen konfigurierte Computerprogramme bedeutet, dass dieses eine oder die mehreren Programme Anweisungen beinhalten, die wenn sie durch die Datenverarbeitungsvorrichtung ausgeführt werden, die Vorrichtung veranlassen, die Operationen oder Aktionen durchzuführen. In this specification, the term "configured" is used in connection with systems and computer program components. A system of one or more computers configured to perform certain operations or actions means that software, firmware, hardware, or a combination thereof is installed on the system that causes the system to perform operations or actions during operation. One or more computer programs configured to perform certain operations or actions means that this one or more programs include instructions that, when executed by the computing device, cause the device to perform the operations or actions.
Die in dieser Spezifikation beschriebenen Ausführungsformen des Gegenstands sowie Funktionsvorgänge können in digitalen elektronischen Schaltungen, in physisch enthaltener Computersoftware oder -firmware, in Computerhardware, einschließlich der in dieser Spezifikation offenbarten Strukturen und ihrer strukturellen Entsprechungen oder in Kombinationen von einer oder mehrerer derselben, implementiert werden. The article embodiments and operations described in this specification may be implemented in digital electronic circuits, in physically contained computer software or firmware, in computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of the same.
Die in dieser Beschreibung beschriebenen Ausführungsformen des Gegenstandes können als ein oder mehrere Computerprogramme implementiert werden, d. h. als ein oder mehrere Module mit Computerprogrammanweisungen, die auf einem materiellen, nicht flüchtgen Programmträger kodiert sind, um dann von einem Datenverarbeitungsgerät ausgeführt zu werden bzw. den Betrieb desselben zu steuern. Alternativ oder ergänzend dazu können die Programmbefehle auf einem künstlich erzeugten, sich ausbreitenden Signal, wie beispielsweise einem maschinenerzeugten elektrischen, optischen oder elektromagnetischen Signal codiert werden, das erzeugt wird, um Informationen zur Übertragung an ein geeignetes Empfängergerät zu codieren, die dann von einem Datenverarbeitungsgerät ausgeführt werden. Bei dem Computer-Speichermedium kann es sich um ein maschinenlesbares Speichergerät, ein maschinenlesbares Speichersubstrat, einen frei adressierbaren oder seriellen Zugriffsspeicher oder eine Kombination aus einem oder mehreren derselben handeln. The embodiments of the subject matter described in this specification may be implemented as one or more computer programs, i. H. as one or more modules with computer program instructions encoded on a tangible, non-volatile program support for being executed by a data processing device or controlling its operation. Alternatively or additionally, the program instructions may be encoded on an artificially generated propagating signal, such as a machine-generated electrical, optical or electromagnetic signal generated to encode information for transmission to a suitable receiver device, which is then executed by a computing device become. The computer storage medium may be a machine-readable storage device, a machine-readable storage substrate, a freely-addressable or serial-access storage, or a combination of one or more of them.
Der Begriff „Datenverarbeitungsvorrichtung“ umfasst alle Arten von Vorrichtungen, Geräten und Maschinen zum Verarbeiten von Daten, einschließlich beispielsweise eines programmierbaren Prozessors, eines Rechners oder mehrerer Prozessoren oder Rechner. Die Vorrichtung kann eine Spezial-Logikschaltung, wie z. B. einen FPGA (feldprogrammierbarer Universalschaltkreis) oder einen ASIC (anwendungsspezifischen integrierten Schaltkreis) beinhalten. Die Vorrichtung kann neben der Hardware auch einen Code beinhalten, der eine Ausführungsumgebung für das betreffende Computerprogramm, z. B. einen Code, der Prozessor-Firmware, einen Protokollstapel, ein Datenbankverwaltungssystem, ein Betriebssystem, oder eine Kombination einer oder mehrerer der genannten, erstellt. The term "data processing device" encompasses all types of devices, devices and machines for processing data including, for example, a programmable processor, a computer or multiple processors or computers. The device can be a special logic circuit, such. As an FPGA (field programmable universal circuit) or an ASIC (application specific integrated circuit) include. The device may include, in addition to the hardware, a code that defines an execution environment for the particular computer program, e.g. For example, a code, the processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of the named, created.
Ein Computerprogramm (auf das sich auch bezogen, oder das als Programm, Software, Softwareanwendung, Modul, Softwaremodul, Script oder Code beschrieben wird) kann in einer beliebigen Programmiersprachenform geschrieben sein, darunter auch in kompilierten oder interpretierten Sprachen, deklarativen oder verfahrensorientierten Sprachen, zudem kann das Programm in jeder beliebigen Form eingesetzt werden, darunter auch als unabhängiges Programm oder als Modul, Komponente, Subroutine oder eine andere Einheit, die zur Verwendung in einer Rechenumgebung geeignet ist. Ein Computerprogramm kann, muss jedoch nicht einer Datei in einem Dateisystem entsprechen. Ein Programm kann in einem Teil einer Datei gespeichert sein, die andere Programme oder Daten enthält, wie z. B. eine oder mehrere Scripts, die in Markup-Sprache in einem Dokument, in einer einzelnen Datei speziell für das betreffende Programm oder in mehreren koordinierten Dateien, z. B. Dateien gespeichert sind, die ein oder mehrere Module, Unterprogramme oder Teile von Code speichern. Ein Computerprogramm kann auf einem Computer oder auf mehreren Computern bereitgestellt und ausgeführt werden, die sich an einem Standort oder an mehreren Standorten verteilt befinden und über ein Kommunikationsnetzwerk miteinander verbunden sind. A computer program (also referred to as program, software, software application, module, software module, script or code) may be written in any programming language form, including compiled or interpreted languages, declarative or procedural languages The program may be used in any form, including as an independent program or as a module, component, subroutine, or other entity suitable for use in a computing environment. A computer program may or may not be a file in a file system. A program may be stored in a portion of a file that contains other programs or data, such as: For example, one or more scripts that are in markup language in a document, in a single file specific to that program, or in multiple coordinated files, such as a file. For example, files that store one or more modules, subprograms, or pieces of code are stored. A computer program may be deployed and executed on one or more computers distributed in one or more locations and interconnected via a communications network.
Die in dieser Spezifikation beschriebenen Prozesse und Logikabläufe können von einem oder mehreren programmierbaren Prozessoren durchgeführt werden, die ein oder mehrere Computerprogramme ausführen, um Funktionen durch das Verarbeiten von Eingabedaten und das Erzeugen von Ausgaben auszuführen. Die Prozesse und Logikabläufe können zudem durch eine vorhabensgebundene Logikschaltung, wie z. B. einen FPGA (Universalschaltkreis) oder eine ASIC (anwendungsspezifische integrierte Schaltung) ausgeführt, und das Gerät in Form derselben implementiert werden. The processes and logic operations described in this specification may be performed by one or more programmable processors executing one or more computer programs to perform functions by processing input data and generating outputs. The processes and logic processes can also be controlled by a project-bound logic circuit, such. As an FPGA (Universal Circuit) or an ASIC (application specific integrated circuit) are executed, and the device can be implemented in the same form.
Computer, die zur Ausführung eines Datenverarbeitungsprogramms geeignet sind, können beispielsweise allgemeine oder spezielle Mikroprozessoren oder beides oder eine andere Art von zentraler Verarbeitungseinheit beinhalten. Im Allgemeinen nimmt eine zentrale Recheneinheit Befehle und Daten von einem Nur-Lese-Speicher oder einem Direktzugriffsspeicher oder von beiden entgegen. Die wesentlichen Elemente eines Computers sind eine zentrale Recheneinheit zum Durchführen bzw. Ausführen von Befehlen und ein oder mehrere Speichergeräte zum Speichern von Befehlen und Daten. Im Allgemeinen beinhaltet ein Computer außerdem ein oder mehrere Massenspeichergeräte zum Speichern von Daten, wie z. B. Magnet-, magneto-optische oder optische Disketten, um Daten zu empfangen und/oder zu senden, oder derselbe ist operativ an ein solches Speichergerät gekoppelt. Ein Computer muss jedoch nicht über diese Geräte verfügen. Darüber hinaus kann ein Computer in einem anderen Gerät, unter anderem z. B. in einem Mobiltelefon, einem persönlichen digitalen Assistenten (PDA), einem mobilen Audio- oder Videoplayer, einer Spielekonsole, einem globalen Positionsbestimmungssystem (GPS) oder einem tragbaren Speichergerät, z. B. in einem USB-Stick, integriert sein. Computers that are suitable for running a data processing program can For example, general or specific microprocessors, or both, or another type of central processing unit. In general, a central processing unit receives instructions and data from a read-only memory or a random access memory, or both. The essential elements of a computer are a central processing unit for executing instructions and one or more storage devices for storing instructions and data. In general, a computer also includes one or more mass storage devices for storing data, such as data storage devices. Magnetic, magneto-optical or optical disks to receive and / or transmit data, or is operatively coupled to such a storage device. However, a computer does not need to have these devices. In addition, a computer in another device, including, for. In a mobile phone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a global positioning system (GPS) or a portable storage device, e.g. B. in a USB stick, be integrated.
Computerlesbare Medien, die zum Speichern von Computerprogrammanweisungen und Daten geeignet sind, beinhalten alle Formen von nicht flüchtigem Speicher, Medien und Speichergeräten, darunter auch beispielsweise Halbleiterspeichergeräte, wie z. B. EPROM, EEPROM und Flash-Speichergeräte, magnetische Platten, wie z. B. interne Festplattenlaufwerke oder Wechselplatten, magneto-optische Platten sowie CD-ROMs und DVD-ROMs. Der Prozessor und der Speicher können durch eine Spezial-Logikschaltung ergänzt oder in dieselbe integriert werden. Computer-readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media, and storage devices, including, for example, semiconductor memory devices such as memory cards. As EPROM, EEPROM and flash memory devices, magnetic disks such. Internal hard disk drives or removable disks, magneto-optical disks, as well as CD-ROMs and DVD-ROMs. The processor and memory may be supplemented or integrated into a special logic circuit.
Um die Interaktion mit einem Benutzer zu ermöglichen, können in dieser Spezifikation beschriebene Ausführungsformen des Gegenstandes auf einem Computer mit einem Anzeigegerät, wie z. B. einem CRT-(Kathodenstrahlröhren) oder LCD-(Flüssigkristallanzeigen)-Monitor, mit welchem dem Benutzer Informationen angezeigt werden, sowie einer Tastatur und einem Zeigegerät, z. B. einer Maus oder einer Steuerkugel (Trackball), mit denen der Benutzer Eingaben an den Computer vornehmen kann, implementiert werden. Darüber hinaus können andere Geräte verwendet werden, um die Interaktion mit einem Benutzer zu ermöglichen; zum Beispiel kann es sich bei der Rückmeldung an den Benutzer um jegliche Art von sensorischer Rückmeldung, wie z. B. visuelle, akustische oder taktile Rückmeldungen, handeln; zudem können die Eingaben des Benutzers in beliebiger Form, darunter auch akustisch, sprachlich oder taktil, empfangen werden. Außerdem kann ein Computer durch das Senden von Dokumenten an und das Empfangen von Dokumenten von einem Gerät, das vom Benutzer verwendet wird, mit einem Benutzer interagieren, beispielsweise durch das Senden von Webseiten an einen Webbrowser auf dem Client-Gerät des Benutzers in Reaktion auf die vom Webbrowser empfangenen Anfragen. In order to facilitate interaction with a user, embodiments of the subject matter described in this specification may be stored on a computer having a display device, such as a display device. A CRT (Cathode Ray Tube) or LCD (Liquid Crystal Display) monitor, which displays information to the user, as well as a keyboard and pointing device, e.g. As a mouse or a control ball (trackball), with which the user can make inputs to the computer can be implemented. In addition, other devices may be used to facilitate interaction with a user; For example, the feedback to the user may be any kind of sensory feedback, such as: Visual, audible or tactile feedback; In addition, the inputs of the user can be received in any form, including acoustically, verbally or tactilely. In addition, by sending documents to and receiving documents from a device used by the user, a computer may interact with a user, for example, by sending web pages to a web browser on the user's client device in response to the user requests received from the web browser.
Die in dieser Spezifikation beschriebenen Ausführungsformen des Gegenstandes können in einem Datenverarbeitungssystem implementiert werden, das eine Backend-Komponente wie z. B. einen Datenserver oder eine Middleware-Komponente, wie z. B. einen Anwendungsserver oder eine Frontend-Komponente, wie z. B. einen Client-Computer mit einer grafischen Benutzeroberfläche, oder eine beliebige Kombination einer oder mehrerer der besagten Backend-, Middleware- oder Frontend-Komponenten oder einen Webbrowser beinhaltet, durch den ein Benutzer mit einer in dieser Spezifikation beschriebenen Implementierung des Gegenstandes interagieren kann. Die Komponenten des Systems können durch eine beliebige Form oder ein beliebiges Medium digitaler Datenkommunikation, wie z. B. ein Kommunikationsnetzwerk, miteinander verbunden sein. So beinhalten beispielsweise Kommunikationsnetzwerke ein lokales Netzwerk („LAN“), ein Großraumnetzwerk („WAN“), z. B. das Internet. The embodiments of the subject matter described in this specification may be implemented in a data processing system that includes a backend component such as a backend component. As a data server or a middleware component such. B. an application server or a front-end component such. A client computer having a graphical user interface, or any combination of one or more of said backend, middleware or frontend components or a web browser through which a user may interact with an implementation of the article described in this specification. The components of the system may be replaced by any form or medium of digital data communication, such as digital data communication. As a communication network, be connected to each other. For example, communication networks include a local area network ("LAN"), a wide area network ("WAN"), e.g. For example, the Internet.
Das Computersystem kann Clients und Server beinhalten. Ein Client und ein Server befinden sich im Allgemeinen entfernt voneinander und interagieren typischerweise über ein Kommunikationsnetzwerk. Die Beziehung zwischen Client und Server entsteht aufgrund von Computerprogrammen, die auf den jeweiligen Computern laufen und die eine Client-Server-Beziehung zueinander aufweisen. The computer system may include clients and servers. A client and a server are generally remote and typically interact over a communications network. The relationship between client and server is due to computer programs that run on the respective computers and that have a client-server relationship with each other.
Zwar enthält diese Spezifikation viele spezifische Implementierungsdetails, jedoch sollten diese nicht als Beschränkungen des Umfangs oder des Anspruchs ausgelegt werden, sondern vielmehr als Beschreibungen spezifischer Merkmale bestimmter Ausführungsformen bestimmter Erfindungen. Bestimmte Merkmale, die in dieser Spezifikation im Kontext der unterschiedlichen Ausführungsformen beschrieben werden, können auch in Kombination in einer einzelnen Ausführungsform implementiert werden. Andererseits können verschiedene Merkmale, die im Kontext einer einzelnen Ausführungsform beschrieben werden, in mehreren Ausführungsformen oder in einer geeigneten Teilkombination implementiert werden. Außerdem können, auch wenn die Merkmale vorstehend ggf. als in bestimmten Kombinationen wirkend beschrieben und zunächst auch als solche beansprucht werden, in einigen Fällen ein oder mehrere Merkmale einer beanspruchten Kombination aus der Kombination herausgenommen und die beanspruchte Kombination auf eine Teilkombination oder eine Variante einer Teilkombination gerichtet werden. While this specification contains many specific implementation details, these should not be construed as limitations on the scope or on the claims, but rather as descriptions of specific features of particular embodiments of particular inventions. Certain features described in this specification in the context of the various embodiments may also be implemented in combination in a single embodiment. On the other hand, various features described in the context of a single embodiment may be implemented in multiple embodiments or in an appropriate sub-combination. In addition, although the features may be described above and claimed as such in certain combinations, in some cases one or more features of a claimed combination may be removed from the combination and the claimed combination may be a partial combination or a variant of a partial combination be directed.
Gleichermaßen soll, obwohl die Operationen in den Zeichnungen in einer bestimmten Reihenfolge dargestellt sind, dies nicht so verstanden werden, dass die besagten Operationen in der dargestellten Reihenfolge oder in fortlaufender Reihenfolge durchgeführt werden müssen bzw. alle veranschaulichten Operationen durchgeführt werden müssen, um die erwünschten Ergebnisse zu erzielen. Unter bestimmten Umständen können Multitasking und Parallelverarbeitung von Vorteil sein. Darüber hinaus sollte die Trennung verschiedener Systemkomponenten in den vorstehend beschriebenen Ausführungsformen nicht als in allen Ausführungsformen erforderlich aufgefasst werden, und es versteht sich, dass die beschriebenen Programmkomponenten und Systeme im Allgemeinen zusammen in ein einziges Softwareprodukt integriert oder in mehrere Softwareprodukte aufgeteilt werden können. Likewise, although the operations in the drawings are illustrated in a particular order, they are not to be understood as having to perform said operations in the illustrated order or in sequential order, or all illustrated operations must be performed to achieve the desired results to achieve. Under certain circumstances, multitasking and parallel processing can be beneficial. Moreover, the separation of various system components in the embodiments described above should not be construed as required in all embodiments, and it is understood that the described program components and systems generally can be integrated together into a single software product or divided into multiple software products.
Es wurden bestimmte Ausführungsformen des Gegenstands beschrieben. Weitere Ausführungsformen liegen innerhalb des Schutzumfangs der folgenden Ansprüche. Die in den Ansprüchen ausgeführten Vorgänge können beispielsweise in einer anderen Reihenfolge ausgeführt werden und dennoch wünschenswerte Ergebnisse erzielen. Die in den beigefügten Figuren dargestellten Verfahren erfordern beispielsweise nicht notwendigerweise die gezeigte Reihenfolge oder sequenzielle Reihenfolge, um erwünschte Ergebnisse zu erzielen. In bestimmten Implementierungen können Multitasking und eine Parallelverarbeitung vorteilhaft sein. Certain embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example, the operations set forth in the claims may be performed in a different order and still achieve desirable results. For example, the methods illustrated in the attached figures do not necessarily require the order or sequential order shown to achieve desired results. In certain implementations, multitasking and parallel processing may be beneficial.
ZITATE ENTHALTEN IN DER BESCHREIBUNG QUOTES INCLUDE IN THE DESCRIPTION
Diese Liste der vom Anmelder aufgeführten Dokumente wurde automatisiert erzeugt und ist ausschließlich zur besseren Information des Lesers aufgenommen. Die Liste ist nicht Bestandteil der deutschen Patent- bzw. Gebrauchsmusteranmeldung. Das DPMA übernimmt keinerlei Haftung für etwaige Fehler oder Auslassungen.This list of the documents listed by the applicant has been generated automatically and is included solely for the better information of the reader. The list is not part of the German patent or utility model application. The DPMA assumes no liability for any errors or omissions.
Zitierte Nicht-PatentliteraturCited non-patent literature
- Schuster, M., und Nakajima, K. Japanese and Korean voice search (Japanische und Koreanische Sprachsuche) beschrieben. 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (Internationale Konferenz über Akustik-, Sprach- und Signalverarbeitung) (2012) [0040] Schuster, M., and Nakajima, K. Japanese and Korean Voice Search. 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (International Conference on Acoustics, Speech and Signal Processing) (2012) [0040]
- Sennrich, R., Haddow, B., und Birch, A. Neural machine translation of rare words with subword units (Neuronale Maschinenübersetzung von seltenen Wörtern mit Subworteinheiten). In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Verfahren des 54. Jahrestreffens des Verbandes für Computerlinguistik) (2016) [0040] Sennrich, R., Haddow, B., and Birch, A. Neural machine translation of rare words with subword units (neural machine translation of rare words with sub-word units). In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Procedure of the 54th Annual Meeting of the Association for Computational Linguistics) (2016) [0040]
Claims (20)
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201662399990P | 2016-09-26 | 2016-09-26 | |
US62/399,990 | 2016-09-26 |
Publications (1)
Publication Number | Publication Date |
---|---|
DE202017105835U1 true DE202017105835U1 (en) | 2018-01-02 |
Family
ID=60084077
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
DE202017105835.2U Active DE202017105835U1 (en) | 2016-09-26 | 2017-09-26 | Neural machine translation systems |
DE102017122276.6A Pending DE102017122276A1 (en) | 2016-09-26 | 2017-09-26 | NEURONAL MACHINE TRANSLATION SYSTEMS |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
DE102017122276.6A Pending DE102017122276A1 (en) | 2016-09-26 | 2017-09-26 | NEURONAL MACHINE TRANSLATION SYSTEMS |
Country Status (10)
Country | Link |
---|---|
US (2) | US11113480B2 (en) |
EP (1) | EP3516591A1 (en) |
JP (1) | JP6870076B2 (en) |
KR (1) | KR102323548B1 (en) |
CN (2) | CN113553862A (en) |
DE (2) | DE202017105835U1 (en) |
GB (1) | GB2556674A (en) |
IE (1) | IE20170201A1 (en) |
SG (1) | SG10201707936TA (en) |
WO (1) | WO2018058046A1 (en) |
Cited By (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN108629743A (en) * | 2018-04-04 | 2018-10-09 | 腾讯科技（深圳）有限公司 | Processing method, device, storage medium and the electronic device of image |
CN108984539A (en) * | 2018-07-17 | 2018-12-11 | 苏州大学 | The neural machine translation method of translation information based on simulation future time instance |
CN109033094A (en) * | 2018-07-18 | 2018-12-18 | 五邑大学 | A kind of writing in classical Chinese writings in the vernacular inter-translation method and system based on sequence to series neural network model |
CN111401078A (en) * | 2020-03-17 | 2020-07-10 | 江苏省舜禹信息技术有限公司 | Running method, device, equipment and medium of neural network text translation model |
CN112364119A (en) * | 2020-12-01 | 2021-02-12 | 国家海洋信息中心 | Ocean buoy track prediction method based on LSTM coding and decoding model |
CN113095092A (en) * | 2021-04-19 | 2021-07-09 | 南京大学 | Method for improving translation quality of non-autoregressive neural machine through modeling synergistic relationship |
US11726750B1 (en) * | 2021-11-17 | 2023-08-15 | Outsystems—Software Em Rede, S.A. | Constrained decoding and ranking of language models for code generation |
Families Citing this family (43)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11113480B2 (en) * | 2016-09-26 | 2021-09-07 | Google Llc | Neural machine translation systems |
WO2018066083A1 (en) * | 2016-10-04 | 2018-04-12 | 富士通株式会社 | Learning program, information processing device and learning method |
US10558750B2 (en) * | 2016-11-18 | 2020-02-11 | Salesforce.Com, Inc. | Spatial attention model for image captioning |
CN110476206B (en) * | 2017-03-29 | 2021-02-02 | 谷歌有限责任公司 | System for converting text into voice and storage medium thereof |
US10776581B2 (en) * | 2018-02-09 | 2020-09-15 | Salesforce.Com, Inc. | Multitask learning as question answering |
CN108388549B (en) * | 2018-02-26 | 2021-02-19 | 腾讯科技（深圳）有限公司 | Information conversion method, information conversion device, storage medium and electronic device |
CN108563640A (en) * | 2018-04-24 | 2018-09-21 | 中译语通科技股份有限公司 | A kind of multilingual pair of neural network machine interpretation method and system |
CN108932232A (en) * | 2018-05-07 | 2018-12-04 | 内蒙古工业大学 | A kind of illiteracy Chinese inter-translation method based on LSTM neural network |
CN108920468B (en) * | 2018-05-07 | 2019-05-14 | 内蒙古工业大学 | A kind of bilingual kind of inter-translation method of illiteracy Chinese based on intensified learning |
CN110489761B (en) | 2018-05-15 | 2021-02-02 | 科大讯飞股份有限公司 | Chapter-level text translation method and device |
CN111727444A (en) | 2018-05-18 | 2020-09-29 | 谷歌有限责任公司 | Universal converter |
US11574122B2 (en) * | 2018-08-23 | 2023-02-07 | Shenzhen Keya Medical Technology Corporation | Method and system for joint named entity recognition and relation extraction using convolutional neural network |
CN109271646B (en) * | 2018-09-04 | 2022-07-08 | 腾讯科技（深圳）有限公司 | Text translation method and device, readable storage medium and computer equipment |
US10672382B2 (en) * | 2018-10-15 | 2020-06-02 | Tencent America LLC | Input-feeding architecture for attention based end-to-end speech recognition |
US10891951B2 (en) | 2018-10-17 | 2021-01-12 | Ford Global Technologies, Llc | Vehicle language processing |
CN109634578B (en) * | 2018-10-19 | 2021-04-02 | 北京大学 | Program generation method based on text description |
CN109543199B (en) * | 2018-11-28 | 2022-06-10 | 腾讯科技（深圳）有限公司 | Text translation method and related device |
CN109598334B (en) * | 2018-12-03 | 2021-06-29 | 中国信息安全测评中心 | Sample generation method and device |
US10963652B2 (en) * | 2018-12-11 | 2021-03-30 | Salesforce.Com, Inc. | Structured text translation |
US11822897B2 (en) | 2018-12-11 | 2023-11-21 | Salesforce.Com, Inc. | Systems and methods for structured text translation with tag alignment |
CN109558605B (en) * | 2018-12-17 | 2022-06-10 | 北京百度网讯科技有限公司 | Method and device for translating sentences |
CN111368558B (en) * | 2018-12-25 | 2024-01-02 | Tcl科技集团股份有限公司 | Neural network-based real-time translation method, intelligent terminal and storage medium |
CN111476039B (en) * | 2019-01-04 | 2023-06-30 | 深圳永德利科技股份有限公司 | Instant language translation method of intelligent vehicle-mounted system and related products |
WO2020154538A1 (en) * | 2019-01-23 | 2020-07-30 | Google Llc | Generating neural network outputs using insertion operations |
WO2020225942A1 (en) * | 2019-05-07 | 2020-11-12 | 株式会社Ｎｔｔドコモ | Internal state modifying device |
JP7170594B2 (en) * | 2019-06-25 | 2022-11-14 | Kddi株式会社 | A program, apparatus and method for constructing a learning model that integrates different media data generated chronologically for the same event |
US11250322B2 (en) * | 2019-07-15 | 2022-02-15 | Sony Interactive Entertainment LLC | Self-healing machine learning system for transformed data |
CN110489766B (en) * | 2019-07-25 | 2020-07-10 | 昆明理工大学 | Chinese-lower resource neural machine translation method based on coding induction-decoding deduction |
KR102286999B1 (en) * | 2019-08-05 | 2021-08-09 | 강원대학교산학협력단 | An apparatus extracting a relation among multiple entities by using a dual pointer network and a method thereof |
CN110442880B (en) * | 2019-08-06 | 2022-09-30 | 上海海事大学 | Translation method, device and storage medium for machine translation |
CN112765998A (en) * | 2019-11-01 | 2021-05-07 | 华为技术有限公司 | Machine translation method, machine translation model training method, device and storage medium |
CN110941966A (en) | 2019-12-10 | 2020-03-31 | 北京小米移动软件有限公司 | Training method, device and system of machine translation model |
CN111178092B (en) * | 2019-12-20 | 2023-04-07 | 沈阳雅译网络技术有限公司 | Translation model optimization method for dynamically adjusting length punishment and translation length |
CN111325000B (en) * | 2020-01-23 | 2021-01-26 | 北京百度网讯科技有限公司 | Language generation method and device and electronic equipment |
KR20210097588A (en) | 2020-01-30 | 2021-08-09 | 삼성전자주식회사 | Electronic device and operating method for obtaining a sentence corresponding to context information |
CN111523329A (en) * | 2020-04-10 | 2020-08-11 | 昆明理工大学 | Neural network machine translation method based on encoder-converter-decoder framework |
US11868737B2 (en) * | 2020-04-24 | 2024-01-09 | Direct Cursus Technology L.L.C | Method and server for processing text sequence for machine processing task |
DE102020114046A1 (en) | 2020-05-26 | 2021-12-02 | Thomas Eißfeller | Neural machine translation method, neural machine translation system, learning method, learning system and program |
US11875131B2 (en) | 2020-09-16 | 2024-01-16 | International Business Machines Corporation | Zero-shot cross-lingual transfer learning |
US20220129645A1 (en) * | 2020-10-27 | 2022-04-28 | Samsung Electronics Co., Ltd. | Electronic device and method for controlling same |
US20220207244A1 (en) * | 2020-12-30 | 2022-06-30 | Yandex Europe Ag | Method and server for training a machine learning algorithm for executing translation |
CN112836485B (en) * | 2021-01-25 | 2023-09-19 | 中山大学 | Similar medical record prediction method based on neural machine translation |
WO2023219752A1 (en) * | 2022-05-13 | 2023-11-16 | Alexander Waibel | Face-translator: end-to-end system for speech-translated lip-synchronized and voice preserving video generation |
Family Cites Families (15)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2006083690A2 (en) * | 2005-02-01 | 2006-08-10 | Embedded Technologies, Llc | Language engine coordination and switching |
ES2435093B1 (en) * | 2012-06-15 | 2014-12-16 | Centro De Investigaciones Energéticas, Medioambientales Y Tecnológicas (Ciemat) | NEURONAL NETWORK TRAINING PROCEDURE FOR CENTELLEATING DETECTORS. |
US10181098B2 (en) * | 2014-06-06 | 2019-01-15 | Google Llc | Generating representations of input sequences using neural networks |
WO2016065327A1 (en) * | 2014-10-24 | 2016-04-28 | Google Inc. | Neural machine translation systems with rare word processing |
KR102305584B1 (en) | 2015-01-19 | 2021-09-27 | 삼성전자주식회사 | Method and apparatus for training language model, method and apparatus for recognizing language |
JP6313730B2 (en) * | 2015-04-10 | 2018-04-18 | タタ コンサルタンシー サービシズ リミテッドＴＡＴＡ Ｃｏｎｓｕｌｔａｎｃｙ Ｓｅｒｖｉｃｅｓ Ｌｉｍｉｔｅｄ | Anomaly detection system and method |
CN105068998B (en) * | 2015-07-29 | 2017-12-15 | 百度在线网络技术（北京）有限公司 | Interpretation method and device based on neural network model |
US10606846B2 (en) * | 2015-10-16 | 2020-03-31 | Baidu Usa Llc | Systems and methods for human inspired simple question answering (HISQA) |
JP6876061B2 (en) * | 2016-01-26 | 2021-05-26 | コーニンクレッカ フィリップス エヌ ヴェＫｏｎｉｎｋｌｉｊｋｅ Ｐｈｉｌｉｐｓ Ｎ．Ｖ． | Systems and methods for neural clinical paraphrase generation |
US11113480B2 (en) * | 2016-09-26 | 2021-09-07 | Google Llc | Neural machine translation systems |
WO2018094296A1 (en) * | 2016-11-18 | 2018-05-24 | Salesforce.Com, Inc. | Sentinel long short-term memory |
US10733380B2 (en) * | 2017-05-15 | 2020-08-04 | Thomson Reuters Enterprise Center Gmbh | Neural paraphrase generator |
WO2019118864A1 (en) * | 2017-12-15 | 2019-06-20 | Google Llc | Training and/or using an encoder model to determine responsive action(s) for natural language input |
US10585988B2 (en) * | 2018-06-08 | 2020-03-10 | Microsoft Technology Licensing, Llc | Graph representations for identifying a next word |
US20200272695A1 (en) * | 2019-02-25 | 2020-08-27 | Disney Enterprises, Inc. | Techniques for performing contextual phrase grounding |
-
2017
- 2017-09-25 US US16/336,870 patent/US11113480B2/en active Active
- 2017-09-25 EP EP17784461.0A patent/EP3516591A1/en active Pending
- 2017-09-25 KR KR1020197008608A patent/KR102323548B1/en active IP Right Grant
- 2017-09-25 WO PCT/US2017/053267 patent/WO2018058046A1/en active Application Filing
- 2017-09-25 JP JP2019516134A patent/JP6870076B2/en active Active
- 2017-09-26 IE IE20170201A patent/IE20170201A1/en unknown
- 2017-09-26 CN CN202110688995.5A patent/CN113553862A/en active Pending
- 2017-09-26 SG SG10201707936TA patent/SG10201707936TA/en unknown
- 2017-09-26 CN CN201710882491.0A patent/CN107870902B/en active Active
- 2017-09-26 DE DE202017105835.2U patent/DE202017105835U1/en active Active
- 2017-09-26 DE DE102017122276.6A patent/DE102017122276A1/en active Pending
- 2017-09-26 GB GB1715516.9A patent/GB2556674A/en not_active Withdrawn
-
2021
- 2021-08-27 US US17/459,111 patent/US20210390271A1/en not_active Abandoned
Non-Patent Citations (2)
Title |
---|
Schuster, M., und Nakajima, K. Japanese and Korean voice search (Japanische und Koreanische Sprachsuche) beschrieben. 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (Internationale Konferenz über Akustik-, Sprach- und Signalverarbeitung) (2012) |
Sennrich, R., Haddow, B., und Birch, A. Neural machine translation of rare words with subword units (Neuronale Maschinenübersetzung von seltenen Wörtern mit Subworteinheiten). In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Verfahren des 54. Jahrestreffens des Verbandes für Computerlinguistik) (2016) |
Cited By (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN108629743A (en) * | 2018-04-04 | 2018-10-09 | 腾讯科技（深圳）有限公司 | Processing method, device, storage medium and the electronic device of image |
CN108984539A (en) * | 2018-07-17 | 2018-12-11 | 苏州大学 | The neural machine translation method of translation information based on simulation future time instance |
CN108984539B (en) * | 2018-07-17 | 2022-05-17 | 苏州大学 | Neural machine translation method based on translation information simulating future moment |
CN109033094A (en) * | 2018-07-18 | 2018-12-18 | 五邑大学 | A kind of writing in classical Chinese writings in the vernacular inter-translation method and system based on sequence to series neural network model |
CN111401078A (en) * | 2020-03-17 | 2020-07-10 | 江苏省舜禹信息技术有限公司 | Running method, device, equipment and medium of neural network text translation model |
CN112364119A (en) * | 2020-12-01 | 2021-02-12 | 国家海洋信息中心 | Ocean buoy track prediction method based on LSTM coding and decoding model |
CN113095092A (en) * | 2021-04-19 | 2021-07-09 | 南京大学 | Method for improving translation quality of non-autoregressive neural machine through modeling synergistic relationship |
US11726750B1 (en) * | 2021-11-17 | 2023-08-15 | Outsystems—Software Em Rede, S.A. | Constrained decoding and ranking of language models for code generation |
Also Published As
Publication number | Publication date |
---|---|
KR102323548B1 (en) | 2021-11-08 |
JP2019537096A (en) | 2019-12-19 |
CN107870902A (en) | 2018-04-03 |
GB2556674A (en) | 2018-06-06 |
CN107870902B (en) | 2021-07-09 |
WO2018058046A1 (en) | 2018-03-29 |
EP3516591A1 (en) | 2019-07-31 |
US20200034435A1 (en) | 2020-01-30 |
SG10201707936TA (en) | 2018-04-27 |
US11113480B2 (en) | 2021-09-07 |
CN113553862A (en) | 2021-10-26 |
GB201715516D0 (en) | 2017-11-08 |
IE20170201A1 (en) | 2018-04-04 |
DE102017122276A1 (en) | 2018-03-29 |
US20210390271A1 (en) | 2021-12-16 |
JP6870076B2 (en) | 2021-05-12 |
KR20190039817A (en) | 2019-04-15 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
DE202017105835U1 (en) | Neural machine translation systems | |
DE102016125831B4 (en) | Speech Recognition System and Method | |
DE202017106363U1 (en) | Neural answer-on-demand networks | |
DE102020205786B4 (en) | SPEECH RECOGNITION USING NLU (NATURAL LANGUAGE UNDERSTANDING) RELATED KNOWLEDGE OF DEEP FORWARD NEURAL NETWORKS | |
DE102018009243A1 (en) | Abstracting long documents using deep-learning structured documents | |
DE102017125256A1 (en) | Search for a neural architecture | |
DE112017004397T5 (en) | System and method for classifying hybrid speech recognition results with neural networks | |
DE102019004300A1 (en) | USE OF A DYNAMIC STORAGE NETWORK TO TRACK DIGITAL DIALOG STATES AND GENERATE ANSWERS | |
DE112019001533T5 (en) | EXTENSION OF TRAINING DATA FOR THE CLASSIFICATION OF NATURAL LANGUAGE | |
DE102019000294A1 (en) | Create company-specific knowledge graphs | |
DE102016125838A1 (en) | COMPRESSED RECURRENTS NEURONAL NETWORK MODELS | |
DE102016125918A1 (en) | Compressed recurrent neural network models | |
DE112018005244T5 (en) | STRUCTURING INCENSE NODES BY OVERLAYING A BASIC KNOWLEDGE | |
DE102014113870A1 (en) | Identify and display relationships between candidate responses | |
DE102017124264A1 (en) | Determine phonetic relationships | |
DE112020003538T5 (en) | CROSS-MODAL RECOVERY WITH WORD OVERLAP BASED CLUSTERS | |
DE112018005272T5 (en) | SEARCHING MULTI-LANGUAGE DOCUMENTS BASED ON AN EXTRACTION OF THE DOCUMENT STRUCTURE | |
DE112020002886T5 (en) | CONTEXTUAL DATA MINING | |
DE112020003909T5 (en) | PROCEDURE FOR MULTIMODAL RETRIEVING RECOVERY AND CLUSTERS USING A DEEP CCA AND ACTIVE PAIRWISE QUERIES | |
DE102021004562A1 (en) | Modification of scene graphs based on natural language commands | |
DE112018004140T5 (en) | ABSTRACTION AND TRANSFERABILITY TO A DETECTION OF INTENT | |
DE112020001853T5 (en) | MODIFICATION OF TRAINING DATA TO TRAIN A MODEL | |
DE112017007361T5 (en) | SUPPORTING INTERACTIVE TEXT MINING PROCESS WITH DIALOG IN NATURAL LANGUAGE | |
DE102022003003A1 (en) | Automatic photo editing using spoken instructions | |
DE112021005925T5 (en) | DOMAIN GENERALIZED SCOPE OVER METALLER TO DEEP FACE RECOGNITION |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
R207 | Utility model specification | ||
R081 | Change of applicant/patentee |
Owner name: GOOGLE LLC (N.D.GES.D. STAATES DELAWARE), MOUN, USFree format text: FORMER OWNER: GOOGLE INC., MOUNTAIN VIEW, CALIF., US |
|
R082 | Change of representative |
Representative=s name: MAIKOWSKI & NINNEMANN PATENTANWAELTE PARTNERSC, DE |
|
R079 | Amendment of ipc main class |
Free format text: PREVIOUS MAIN CLASS: G06F0017280000Ipc: G06F0040400000 |
|
R079 | Amendment of ipc main class |
Free format text: PREVIOUS MAIN CLASS: G06F0040400000Ipc: G06F0040440000 |
|
R150 | Utility model maintained after payment of first maintenance fee after three years | ||
R151 | Utility model maintained after payment of second maintenance fee after six years |