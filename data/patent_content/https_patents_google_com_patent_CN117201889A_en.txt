CN117201889A - Automatic speech translation dubbing of pre-recorded video - Google Patents
Automatic speech translation dubbing of pre-recorded video Download PDFInfo
- Publication number
- CN117201889A CN117201889A CN202311273416.6A CN202311273416A CN117201889A CN 117201889 A CN117201889 A CN 117201889A CN 202311273416 A CN202311273416 A CN 202311273416A CN 117201889 A CN117201889 A CN 117201889A
- Authority
- CN
- China
- Prior art keywords
- strings
- caption
- translated
- video
- string
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000013519 translation Methods 0.000 title claims abstract description 80
- 238000012545 processing Methods 0.000 claims abstract description 216
- 238000000034 method Methods 0.000 claims abstract description 68
- 239000012634 fragment Substances 0.000 claims description 172
- 238000013507 mapping Methods 0.000 claims description 16
- 230000015654 memory Effects 0.000 claims description 14
- 230000014616 translation Effects 0.000 description 67
- 230000000694 effects Effects 0.000 description 13
- 230000009471 action Effects 0.000 description 8
- 230000008569 process Effects 0.000 description 7
- 230000011218 segmentation Effects 0.000 description 6
- 238000010586 diagram Methods 0.000 description 5
- 238000013518 transcription Methods 0.000 description 5
- 230000035897 transcription Effects 0.000 description 5
- 235000010523 Cicer arietinum Nutrition 0.000 description 4
- 244000045195 Cicer arietinum Species 0.000 description 4
- 230000002411 adverse Effects 0.000 description 4
- 238000004590 computer program Methods 0.000 description 4
- 229910003460 diamond Inorganic materials 0.000 description 4
- 239000010432 diamond Substances 0.000 description 4
- 238000004519 manufacturing process Methods 0.000 description 4
- 230000007704 transition Effects 0.000 description 4
- 238000013500 data storage Methods 0.000 description 3
- 210000003127 knee Anatomy 0.000 description 3
- 230000003287 optical effect Effects 0.000 description 3
- 235000014347 soups Nutrition 0.000 description 3
- 230000009286 beneficial effect Effects 0.000 description 2
- 230000001413 cellular effect Effects 0.000 description 2
- 238000004891 communication Methods 0.000 description 2
- 238000013461 design Methods 0.000 description 2
- 230000006870 function Effects 0.000 description 2
- 238000010606 normalization Methods 0.000 description 2
- 235000015067 sauces Nutrition 0.000 description 2
- 206010011878 Deafness Diseases 0.000 description 1
- 230000008901 benefit Effects 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 239000000872 buffer Substances 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 230000007812 deficiency Effects 0.000 description 1
- 230000002996 emotional effect Effects 0.000 description 1
- 230000037406 food intake Effects 0.000 description 1
- 230000007274 generation of a signal involved in cell-cell signaling Effects 0.000 description 1
- 230000008676 import Effects 0.000 description 1
- 230000007774 longterm Effects 0.000 description 1
- 238000007726 management method Methods 0.000 description 1
- 239000003550 marker Substances 0.000 description 1
- 239000011159 matrix material Substances 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000035772 mutation Effects 0.000 description 1
- 230000008439 repair process Effects 0.000 description 1
- 230000003068 static effect Effects 0.000 description 1
- 239000000126 substance Substances 0.000 description 1
- 230000000153 supplemental effect Effects 0.000 description 1
- 230000009466 transformation Effects 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/43—Processing of content or additional data, e.g. demultiplexing additional data from a digital video stream; Elementary client operations, e.g. monitoring of home network or synchronising decoder's clock; Client middleware
- H04N21/4302—Content synchronisation processes, e.g. decoder synchronisation
- H04N21/4307—Synchronising the rendering of multiple content streams or additional data on devices, e.g. synchronisation of audio on a mobile phone with the video output on the TV screen
- H04N21/43072—Synchronising the rendering of multiple content streams or additional data on devices, e.g. synchronisation of audio on a mobile phone with the video output on the TV screen of multiple content streams on the same device
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/47—End-user applications
- H04N21/488—Data services, e.g. news ticker
- H04N21/4884—Data services, e.g. news ticker for displaying subtitles
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/30—Semantic analysis
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/40—Processing or translation of natural language
- G06F40/58—Use of machine translation, e.g. for multi-lingual retrieval, for server-side translation for client devices or for real-time translation
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/26—Speech to text systems
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/20—Servers specifically adapted for the distribution of content, e.g. VOD servers; Operations thereof
- H04N21/23—Processing of content or additional data; Elementary server operations; Server middleware
- H04N21/233—Processing of audio elementary streams
- H04N21/2335—Processing of audio elementary streams involving reformatting operations of audio signals, e.g. by converting from one coding standard to another
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/20—Servers specifically adapted for the distribution of content, e.g. VOD servers; Operations thereof
- H04N21/23—Processing of content or additional data; Elementary server operations; Server middleware
- H04N21/234—Processing of video elementary streams, e.g. splicing of video streams, manipulating MPEG-4 scene graphs
- H04N21/2343—Processing of video elementary streams, e.g. splicing of video streams, manipulating MPEG-4 scene graphs involving reformatting operations of video signals for distribution or compliance with end-user requests or end-user device requirements
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/20—Servers specifically adapted for the distribution of content, e.g. VOD servers; Operations thereof
- H04N21/23—Processing of content or additional data; Elementary server operations; Server middleware
- H04N21/234—Processing of video elementary streams, e.g. splicing of video streams, manipulating MPEG-4 scene graphs
- H04N21/2343—Processing of video elementary streams, e.g. splicing of video streams, manipulating MPEG-4 scene graphs involving reformatting operations of video signals for distribution or compliance with end-user requests or end-user device requirements
- H04N21/234336—Processing of video elementary streams, e.g. splicing of video streams, manipulating MPEG-4 scene graphs involving reformatting operations of video signals for distribution or compliance with end-user requests or end-user device requirements by media transcoding, e.g. video is transformed into a slideshow of still pictures or audio is converted into text
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/20—Servers specifically adapted for the distribution of content, e.g. VOD servers; Operations thereof
- H04N21/23—Processing of content or additional data; Elementary server operations; Server middleware
- H04N21/235—Processing of additional data, e.g. scrambling of additional data or processing content descriptors
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/43—Processing of content or additional data, e.g. demultiplexing additional data from a digital video stream; Elementary client operations, e.g. monitoring of home network or synchronising decoder's clock; Client middleware
- H04N21/4302—Content synchronisation processes, e.g. decoder synchronisation
- H04N21/4307—Synchronising the rendering of multiple content streams or additional data on devices, e.g. synchronisation of audio on a mobile phone with the video output on the TV screen
- H04N21/43074—Synchronising the rendering of multiple content streams or additional data on devices, e.g. synchronisation of audio on a mobile phone with the video output on the TV screen of additional data with content streams on the same device, e.g. of EPG data or interactive icon with a TV program
Abstract
The application discloses automatic speech translation dubbing of prerecorded video, and provides a method for aligning translation of original subtitle data with an audio part of the video. The method includes identifying, by a processing device, original subtitle data for a video including a plurality of subtitle strings. The processing device identifies speech recognition data that includes a plurality of generated strings and associated timing information for each generated string. The processing device maps the plurality of caption strings to the plurality of generated strings using an assigned value indicating semantic similarity between the strings. The processing device assigns timing information to individual caption strings based on the mapped timing information of the individual generated strings. The processing device aligns the translation of the original subtitle data with the audio portion of the video using the assigned timing information of the individual subtitle strings.
Description
Description of the division
The application belongs to a divisional application of Chinese patent application 201880090248.8 with the application date of 2018, 02 and 26.
Technical Field
The present application relates to automatic speech translation dubbing of pre-recorded video.
Background
Translating an utterance in a video from an originally recorded language to another language may involve labor intensive work of dubbing the translated audio portion of speech onto the original video. In general, speech dubbing refers to combining other or supplemental recordings (dubbed utterances) with the originally recorded utterance to create the final soundtrack of the video. However, the dubbed utterance may be different from the original recorded utterance and may not be aligned with the start time and end time of the original recorded utterance. As a result, the translated audio may appear unsynchronized and may not be attractive to viewers.
Disclosure of Invention
The following is a simplified summary of the disclosure in order to provide a basic understanding of some aspects of the disclosure. This summary is not an extensive overview of the disclosure. It is intended to neither identify key or critical elements of the disclosure nor delineate any scope of the particular embodiments of the disclosure or any scope of the claims. Its sole purpose is to present some concepts of the disclosure in a simplified form as a prelude to the more detailed description that is presented later.
Embodiments of the present disclosure may include a method of identifying, by a processing device, original subtitle data for a video, wherein the original subtitle data includes a plurality of subtitle strings associated with an audio portion of the video. The processing device may identify speech recognition data generated for the audio portion of the video, wherein the speech recognition data includes a plurality of generated strings and associated timing information for each generated string. The processing device may map the plurality of generated strings using an assigned value indicating semantic similarity between individual ones of the plurality of caption strings and individual ones of the plurality of generated strings. The method may further assign timing information to individual caption strings based on the mapped timing information of the individual generated strings. The processing device may use the timing information of the assigned individual subtitle strings to align the translation of the original subtitle data with the audio portion of the video.
Embodiments of the present disclosure may include a method for generating a translated audio utterance of a translation of raw subtitle data. The translated audio utterance may be a machine-generated audio utterance that contains a set of translated audio utterance fragments. The method may further include overlaying translated audio utterance segments in the set of audio utterance segments over video segments of a video that correspond to a time interval of each of the translated audio utterance segments. The method may further include generating a second video that contains a video portion of the video and a translated audio portion that includes the overlaid translated audio speech segment.
Drawings
Aspects and embodiments of the present disclosure will be understood more fully from the detailed description given below and from the accompanying drawings of various aspects and embodiments of the disclosure, which, however, should not be taken to limit the disclosure to specific aspects or embodiments, but are for explanation and understanding only.
Fig. 1 illustrates an example system architecture in accordance with some aspects of the present disclosure.
Fig. 2 depicts a flowchart of a method for assigning timing information to raw subtitle data and using the assigned timing information to align a translation of the raw subtitle data with an audio portion of a video content item, in accordance with some aspects of the present disclosure.
Fig. 3A illustrates an example method of assigning values to each unique caption string and unique generated string in accordance with some aspects of the present disclosure.
Fig. 3B illustrates an example method for determining a subtitle string and generating a matching sequence of the strings using an assignment value according to some aspects of the present disclosure.
Fig. 4A illustrates an example method for aligning translation language subtitle data with an audio portion of a video content item using timing information of assigned individual subtitle strings according to some aspects of the present disclosure.
Fig. 4B illustrates an example method for aligning machine-generated translated caption data with an audio portion of a video using timing information of an assigned individual caption string, according to some aspects of the present disclosure.
Fig. 5A is an illustrative example of assignment values for a plurality of caption strings and a plurality of generated strings in accordance with some aspects of the present disclosure.
Fig. 5B is an illustrative example of a sequence matching pair generated from a caption string sequence and a generated string sequence in accordance with some aspects of the present disclosure.
Fig. 5C illustrates an example table of individual caption strings with assigned timing information according to some aspects of the present disclosure.
Fig. 6A illustrates an example method for overlaying a translated audio utterance on a video and generating a second video that includes an audio portion of the translated utterance, in accordance with some aspects of the disclosure.
Fig. 6B illustrates an example method for adjusting overlay parameters to match audio and video durations in a video content item with translated utterances, in accordance with some aspects of the disclosure.
FIG. 7 depicts a block diagram of an example computing system operating in accordance with one or more aspects of the present disclosure.
Detailed Description
Aspects of the present disclosure relate to aligning translated caption data to an audio portion of a video using timing information of individual caption strings from original caption data. The translation subtitle data may be used to overlay dubbing translations of the utterances onto the video to generate translated versions of the video. The original video and the translated version of the video may be stored within a content sharing platform for the media content item. A user of a client device may query and retrieve translated versions of video for playback on other respective client devices. The client device may include, but is not limited to, a personal computer, a tablet, a smart phone, or any other computing device.
The video stored on the content sharing platform may have audio that includes utterances recorded in an original language that is different from the desired language of the user viewing the video. For example, the video on the content sharing platform may include utterances recorded in english; however, the user may only understand japanese and thus want to watch video in japanese. Many professionally produced videos may include different audio tracks that specify utterances in different languages. Producers of these professionally produced videos may use dubbing actors to generate foreign language tracks for viewers speaking foreign languages. However, generating alternative language tracks using dubbing actors is a time consuming and expressive process. The cost may further increase with the number of different foreign language versions desired for a particular video.
Another alternative to generating a foreign language version of the video using dubbing actors is to provide the video for foreign language dialect. The foreign language dialect may then be overlaid on the video to display the foreign language translations of the spoken words within the video. However, a major disadvantage of simply providing foreign language dialogs is that viewers are required to read dialogs while playing the video. This extra effort by the viewer on this part may distract the user from the content displayed within the video. For example, if a viewer is watching an "entry guide" video regarding auto repair, it would be beneficial for the viewer to note the steps shown within the video rather than the foreign language dialect provided.
Techniques may be used to use computer-generated utterances that take foreign language dialects as input and generate utterances to be overlaid on the original video. However, synchronizing the duration of an automatically generated foreign language utterance with the original utterance can be problematic and may result in the foreign language utterance starting or ending too early or too late. Synchronization problems of automatically generating foreign language utterances may occur when timing information from the spoken utterance is used to determine the time interval of the overlaid foreign language utterance. For example, timing information (subtitle timing) associated with the dialog may relate to when the dialog is displayed on the video. Typically, the dialog is displayed before the speaker in the video speaks, and remains displayed after the speaker finishes speaking. The reason for this is to ensure that the viewer has enough time to read the text within the displayed text. When a dialog is converted into an utterance for foreign language dubbing, the duration of the converted utterance may not match the duration of the original utterance in the video. As a result, synchronizing foreign language utterances based on subtitle timing (as is conventionally done) may result in inaccurate utterance coverage, which may result in undesirable video with inaccurate utterance timing.
Aspects of the present disclosure address the above-mentioned and other deficiencies by using speech recognition data associated with an original language and caption data associated with the original language to help determine how to match a translated speech to an audio portion of a video. The speech recognition data may include automatically generated speech data in the form of generated strings that include timing information for each generated string. The generated string may represent words and/or phrases from the dialog originally spoken in the video content item. For example, the automatically generated utterance data may be a transcription from an initially spoken dialog of the video, where each word has associated timing information describing a word start time and an end time. Generating utterance data may include errors, such as misidentified words within an utterance. To ameliorate this error, raw subtitle data, including a subtitle string representing the word originally spoken in the video, may be used in conjunction with generating speech data. The original subtitle data may include a transcription of an originally spoken dialog provided from subtitle text for subtitle a video content item.
The caption string within the original caption data may be mapped to the generated string in the speech recognition data so that timing information is assigned from the generated string to the caption string of the original caption data. Mapping the caption string to the generated string may be performed using values assigned to the individual caption string and the individual generated string, the values indicating semantic similarity between the individual caption string and the individual generated string. By so doing, the accuracy of the utterance from the original subtitle data and the accuracy of the timing information from the speech recognition data may be combined to align the original subtitle data with the audio portion of the video.
Once the caption string within the original caption data is mapped to the generated string in the speech recognition data and timing information from the generated string is assigned to the caption string of the original caption data, the original caption data and the assigned timing information may be used to align the translated caption data (for languages other than the original caption data) with the audio portion of the video. The translated caption data may include a translated string associated with the audio portion of the video.
As part of the alignment process, caption sentence fragments may be generated from the caption string of the original caption data based on punctuation in the caption string. In addition, a translation sentence fragment may be generated from the translated character string of the translated caption data based on the punctuation marks in the translated character string. The caption sentence fragments may be mapped to the corresponding translation sentence fragments based on timing associated with the original caption data and the translation caption data. Mapping the sentence fragments of the original subtitle data to the corresponding sentence fragments of the translated data, rather than mapping individual strings of the original subtitle data to corresponding individual strings of the translated subtitle data, saves processing resources because the mapping between individual strings would involve a deeper understood meaning of the strings in both the original subtitle data and the translated subtitle data in order to determine a one-to-one correspondence between the original strings and the translated strings. Determining the correspondence of the original subtitle data to the translated subtitle data at the character string level would consume a significant amount of operations to ensure proper alignment of the subtitle data at the character string level.
For each resulting subtitle sentence fragment, assigned timing information for individual subtitle strings in the subtitle sentence fragment may be used to estimate a time interval covering the subtitle sentence fragment. The time interval may be assigned to an individual translated sentence fragment based on an estimated time interval of the mapped individual caption sentence fragment. Translated sentences may be generated from successive translated sentence fragments based on punctuation. Each translated sentence may have timing information defined by an assigned time interval of a translated sentence fragment included in the respective translated sentence. Using the timing information for each translation sentence, the translation sentence can be aligned with the audio portion of the video.
In some implementations, the foreign language dubbing translates the original subtitle data using the translated audio utterance as a machine-generated utterance. The translated audio utterances include translated audio utterance segments that may be overlaid on video segments of the video that correspond to time intervals of each translated audio utterance segment. The resulting video may contain the original video portion and a translated audio portion comprising the overlaid translated audio speech segment.
Aspects of the present disclosure provide for precise alignment of translated audio portions generated from translated caption utterances with the duration and timing of original audio utterances in video. As a result, dubbed audio is smoothly integrated onto video, thereby improving the user's viewing experience and increasing the user's interest in video dubbed in foreign languages.
Fig. 1 illustrates an example system architecture 100 according to one embodiment of this disclosure. The system architecture 100 includes client devices 110A-110Z, a network 105, a data store 106, a content sharing platform 120, a server 130, and speech recognition and generation services 150. In one embodiment, the network 105 may include a public network (e.g., the internet), a private network (e.g., a Local Area Network (LAN) or Wide Area Network (WAN)), a wired network (e.g., an ethernet network), a wireless network (e.g., an 802.11 network or Wi-Fi network), a cellular network (e.g., a Long Term Evolution (LTE) network), a router, hub, switch, server computer, and/or combinations thereof. In one implementation, the data store 106 can be a memory (e.g., random access memory), a cache, a drive (e.g., hard drive), a flash drive, a database system, or other type of component or device capable of storing data. Data store 106 can also include multiple storage components (e.g., multiple drives or multiple databases) that can also span multiple computing devices (e.g., multiple server computers).
Client devices 110A-110Z may each include a computing device such as a Personal Computer (PC), laptop, mobile phone, smart phone, tablet, networked television, netbook computer, or the like. In some implementations, the client devices 110A-110Z may also be referred to as "user devices. Each client device includes a media viewer 111. In one implementation, the media viewer 111 may be an application that allows a user to view content, such as video. For example, the media viewer 111 may be a web browser that can access, retrieve, render, and/or navigate content (e.g., web pages such as hypertext markup language (HTML) pages, digital media items or content items, etc.) served by a web server. The media viewer 111 may render, display, and/or present content (e.g., web pages, media viewers) to a user. The media viewer 111 may also display an embedded media player embedded in a web page (e.g., a web page that may provide information about products sold by an online merchant) (e.g.,a player or HTML5 player). In another example, the media viewer 111 may be a stand-alone application that allows a user to view digital media content items, such as digital video.
The media viewer 111 may be provided to the client devices 110A-110Z by the server 130 and/or the content sharing platform 120. For example, the media viewer 111 may be an embedded media player embedded in a web page provided by the content sharing platform 120. In another example, the media viewer 111 may be an application in communication with the server 130.
The functions described in one embodiment as being performed by the content sharing platform 120 may also be performed on the client devices 110A-110Z in other embodiments, if appropriate. Furthermore, functionality attributed to a particular component may be performed by different or multiple components operating together. The content sharing platform 120 may also be accessed as a service provided to other systems or devices through an appropriate application programming interface and is thus not limited to use in a website.
In one implementation, the content sharing platform 120 may be one or more computing devices (such as a rack-mounted server, router computer, server computer, personal computer, mainframe computer, laptop computer, tablet computer, networked television, desktop computer, etc.), data storage (e.g., hard disk, memory, database), a network, software components, and/or hardware components that may be used to provide a user with access to and/or to media items (also referred to as content items). For example, the content sharing platform 120 may allow users to consume, upload, search, approve ("like"), dislike, and/or comment on media items. The content sharing platform 120 may also include a website (e.g., web page) that may be used to provide users with access to media items.
In embodiments of the present disclosure, a "user" may be represented as a single individual. However, other embodiments of the present disclosure include "users," which are entities controlled by a set of users and/or an automation source. For example, a set of individual users that are joined into a community in a social network may be considered a "user. In another example, the automated consumer may be an automated ingestion pipeline of the content sharing platform 120, such as a theme channel.
The content sharing platform 120 may include multiple channels (e.g., channels a through Z). A channel may be data content available from a common source or data content having a common topic, theme or theme. The data content may be digital content selected by the user, digital content made available by the user, digital content uploaded by the user, digital content selected by a content provider, digital content selected by a broadcaster, and so forth. For example, channel X may include videos Y and Z. The channel may be associated with an owner, which is a user that may perform an action on the channel. Different activities may be associated with a channel based on actions of the owner, such as the owner making digital content available on the channel, the owner selecting (e.g., enjoying) digital content associated with another channel, the owner commenting on digital content associated with another channel, etc. The activity associated with the channel may be collected into an activity feed for the channel. Users other than the owners of the channels may subscribe to one or more channels of interest to them. The concept of "subscription" may also be referred to as "like", "follow", "add as friends", etc.
Once the user subscribes to the channel, the user may be presented with information from the active feeds of the channel. If the user subscribes to multiple channels, the activity feeds for each channel to which the user subscribes may be combined into an aggregate activity feed. Information from the aggregated activity feed may be presented to the user. Channels may have their own feeds. For example, when navigating to a home page of a channel on a content sharing platform, feeds generated by the channel may be displayed on the channel home page. The user may have an aggregated feed, which is a feed that includes a subset of content items from at least all channels to which the user subscribes. The syndication feed may also include content items from channels to which the user is not subscribed. For example, the content sharing platform 120 or other social network may insert recommended content items into an aggregated feed of users, or may insert content items associated with related connections of users in an aggregated feed.
Each channel may include one or more media content items 121. Examples of media content items 121 may include, but are not limited to, digital videos, digital movies, digital photos, digital music, website content, social media updates, electronic books (e-books), electronic magazines, digital newspapers, digital audio books, electronic periodicals, web blogs, really Simple Syndication (RSS) feeds, electronic comics, software applications, and the like. In some implementations, the media content item 121 is also referred to as a video content item.
The media content item 121 may be consumed via the internet and/or via a mobile device application. As used herein, "media," "media item," "online media item," "digital media item," "content item," and "video content item" may include electronic files that may be executed or loaded using software, firmware, or hardware configured to present the digital media item to an entity. In one implementation, the content sharing platform 120 may use the data store 106 to store media content items 121.
In one implementation, the speech recognition and generation service 150 may be one or more computing devices (e.g., rack-mounted servers, server computers, etc.) configured to generate speech recognition data by analyzing audio from video content items to identify spoken language in the video. The speech recognition and generation service 150 may implement Automatic Speech Recognition (ASR), speech-to-text (STT), or any other speech recognition algorithm configured to recognize spoken dialog in audio. The speech recognition data may include a plurality of generated strings, each generated string representing one or more spoken words from the audio portion. Each generated string may include an audio timing that identifies a start and end time of the generated string within the audio portion. For example, if the speech recognition and generation service 150 analyzes a video containing audio of a spoken utterance having "Hello world", the speech recognition and generation service 150 may implement ASR to generate speech recognition data containing strings generated from "Hello world" as "Hello" and "world". The generated string may have associated timing information indicating that "hello" in the video starts at 0:05 seconds and ends at 0:07 seconds. Similarly, a "world" string may have timing information indicating that "world" starts at 0:07 seconds and ends at 0:08 seconds. The speech recognition and generation service 150 can be configured to recognize a plurality of different spoken languages within the video content item.
In one embodiment, the speech recognition and generation service 150 may be configured to generate computerized audio for speaking a dialog using the subtitle data. For example, speech recognition and generation service 150 may receive transcribed caption data for a conversation between two different people. The speech recognition and generation service 150 may associate two different computer-generated voices with the subtitle data and then generate audio representing the conversation using the subtitle data as input. The speech recognition and generation service 150 may be configured to input subtitle data in different languages and generate an audio portion for the speech in the language of the input subtitle data. For example, the speech recognition and generation service 150 may receive a japanese sentence and may synthesize the sentence into audio representing a computer voice speaking the japanese sentence.
In one implementation, the server 130 may be one or more computing devices (e.g., rack-mounted servers, server computers, etc.). The server 130 may be included in the content sharing platform 120 or may be part of a different system. Server 130 may host subtitle alignment component 140 and audio/video duration matching component 145. The caption alignment component 140 may be configured to align the caption data with the speech recognition data based on semantic similarity of character strings within the caption data and the speech recognition data. For example, the caption alignment component 140 may receive a request to align raw caption data representing a caption transcript of audio from a particular video content item with speech recognition data representing an automatically recognized utterance from the particular video content item. The subtitle alignment component 140 may determine a matching string sequence using semantic similarity within strings from the original subtitle data and the speech recognition data and generate mapping information for the matching string sequence. For example, if the caption alignment component 140 determines that the phrase "this is really cool (which is very cool) from the original caption data matches the same phrase" this is really cool "in the speech recognition data, the caption alignment component 140 may generate mapping information that maps the caption strings that make up" this is really cool "to the corresponding strings generated from the speech recognition data. By so doing, the caption alignment component 140 may assign timing information associated with the individual generated strings to the corresponding individual caption strings.
In one embodiment, the caption alignment component 140 may align caption strings from the original caption data with the translated caption data. The translated subtitle data may represent a translated transcription of a spoken dialog in an audio portion of the video content item. The subtitle alignment component 140 may use translation resources, such as a dictionary, language database, or other translation service, to determine how to align raw subtitle data in one language with translated subtitle data in another language. For example, the translation service may provide translation of words, phrases, and/or sentences in one language to another language, and the caption alignment component 140 may identify those words, phrases, and/or sentences in the original caption data and their translated versions in the translated caption data to perform the alignment.
In one implementation, the audio/video matching component 145 may be configured to determine a duration difference between the translated audio portion of the utterance from the video and the original audio portion of the utterance. The audio/video matching component 145 can be configured to adjust the rate of the translated audio portion, the rate of the video portion in the video content item, and/or both, in order to align the duration of the translated audio portion of the utterance with the original audio portion of the utterance for dubbing purposes. By adjusting the audio rate and/or video rate of the translated portion of the utterance, the audio/video matching component 145 can seamlessly overlay the translated audio portion over a corresponding video portion in the original video content item such that the translated audio portion is aligned with the original utterance.
Various methods related to aspects of the present disclosure will now be discussed in more detail. In general, the methods may be performed by processing logic that may comprise hardware (e.g., processing devices, circuitry, dedicated logic, programmable logic, microcode, etc.), software (e.g., instructions run on a processing device), or a combination thereof. In some implementations, the method may be performed by a component within the server 130. In alternative implementations, some methods may also be performed using the speech recognition and generation service 150 and/or the media viewer 111 of fig. 1.
For simplicity of explanation, the methodologies of the present disclosure are depicted and described as a series of acts. However, acts in accordance with the present disclosure may occur in various orders and/or concurrently, and with other acts not presented and described herein. Moreover, not all illustrated acts may be required to implement a methodology in accordance with the disclosed subject matter. In addition, those skilled in the art will understand and appreciate that a methodology could alternatively be represented as a series of interrelated states via a state diagram or events. Additionally, it should be appreciated that the methodologies disclosed herein are capable of being stored on an article of manufacture to facilitate transporting and transferring such methodologies to computing devices. The term "article of manufacture" as used herein is intended to encompass a computer program accessible from any computer-readable device or storage media.
Fig. 2 depicts a flowchart of aspects of a method 200 for assigning timing information to caption strings of original caption data and using the assigned timing information to align a translation of the original caption data with an audio portion of a video content item.
At block 205, processing logic may identify raw subtitle data for a video content item. The original subtitle data includes a plurality of subtitle strings associated with an audio portion of the video content item. In an embodiment, processing logic may retrieve raw subtitle data from a data store 106 that may store video content items and their associated raw subtitle data. The original subtitle data may represent a transcription of an audio portion (e.g., utterance) of the video content item. The raw caption data may be formatted to include sentences or clips representing spoken words from speakers in the video and associated caption timing information indicating when the caption is to be displayed in the video. For example, if the first person in the video speaks "Hello world-! "then the original subtitle data may include the subtitle data to be" Hello world-! "sentence expressed as two caption character strings, one caption character string expressed as" Hello ", and the other caption character string expressed as" world-! ". The subtitle string may represent a word, phrase, or character set. The subtitle timing information in the original subtitle data may indicate when the sentence "Hello world-! ". The caption timing information is not necessarily identical to the word "Hello world-! "time alignment. Instead, the caption timing information may be configured to display the caption such that the viewer has enough time to read the caption. For example, the caption timing information may display caption 1 or 1/2 seconds before the first person in the video begins speaking. The start and end times from the subtitle timing information may vary based on the size of the subtitle. Thus, the caption timing information may not be an accurate indicator of when the speaker begins speaking in the video.
At block 210, processing logic may identify speech recognition data generated for an audio portion of a video content item. In one implementation, processing logic may request speech recognition data from speech recognition and generation service 150. The speech recognition and generation service 150 may generate upon request, or may have previously generated speech recognition data for the video content item, and may have stored the speech recognition data in the data store 106. The speech recognition data may represent automatically recognized speech or other audio from the video content item. The speech recognition data may include a plurality of generated strings, wherein each of the individually generated strings represents a word, phrase, or character set spoken by a speaker within the video content item. Each generated string within the speech recognition data may be associated with timing information that indicates a particular time at which the generated string was spoken within the video. Using the above example, the phrase "Hello world-! The timing information of "may include the timing when the word" hello "is spoken, and the timing when the word" world "is spoken. The timing information may include specific start and end times of each generated string or may include specific start time and duration information of each generated string.
At block 215, processing logic may map the plurality of caption strings from the original caption data to the plurality of generated strings from the speech recognition data using an assignment value indicating semantic similarity between the individual caption strings and the individual generated caption strings. In one embodiment, processing logic may assign an integer Identifier (ID) to each unique string within the plurality of caption strings and the plurality of generated strings. The integer ID may be used to determine semantic similarity between character strings based on a word edit distance value that represents the number of instructions required to transform one word into another. Instructions to transform one word into another may include instructions such as letter replacement, letter addition, letter removal, and adjacent letter switching. In other embodiments, identifiers other than integer IDs may be assigned to each unique string of the plurality of caption strings and the plurality of generated strings.
Some aspects of mapping a plurality of caption strings to a plurality of generation strings by assigning values to each unique caption string and generation string and determining a matching sequence of caption strings and generation strings are discussed in more detail below in connection with fig. 3A and 3B.
Referring to fig. 2, at block 220, processing logic assigns timing information to individual caption strings based on the mapped timing information of the individual generated strings. Fig. 5C illustrates an example table of individual subtitle strings having assigned timing information. The table 570 contains start and end timings of referring to audio portions associated with the video content items, duration timings of each individual subtitle string, and confidence information based on the speech recognition data. Confidence information refers to a confidence value that may be assigned to each caption string based on how accurately the caption string matches the corresponding generated string. For example, if the match is an exact match based on an integer ID value, the confidence value may be set to 1. Alternatively, if the match is a partial match or a match inferred from adjacent caption strings, the confidence value may be set to a proportionally lower value.
As shown in fig. 5C, the string 575 refers to the word "pea" and has not previously been mapped to a corresponding generated string. As a result, the timing information is set to zero. Processing logic may infer timing information for strings 575 based on neighboring strings. For example, since the actually spoken dialog may include the word "chickpea" instead of the strings "chip" and "pea", the end time of the preceding string "chip" may be applied to the end time of the string 575 "pea".
Referring to fig. 2, at block 225, processing logic aligns a translation of the original subtitle data with an audio portion of the video content item using the assigned timing information of the individual subtitle strings of the original subtitle data and the detected speech segmentation time interval. Translation of the raw caption data may include, but is not limited to, translating language captions and machine translated conversations generated from speech recognition and generation service 150. The translation language subtitle may include a text translation generated from crowd-sourced users or professional translators. The machine-translated dialog generated by the speech recognition and generation service 150 may be generated by providing the speech recognition and generation service with the original subtitle data for translation or portions of the original subtitle data for translation, such as separate sentences or sentence fragments.
Fig. 3A illustrates a process of assigning a value to each unique subtitle string and generating a string. At block 305, processing logic may normalize the caption string in the original caption data and the generated string in the speech recognition data by removing minor character differences to effectively match the corresponding caption string and the generated string. The caption strings in the original caption data may be formatted to assist the viewer of the video in understanding the spoken dialog as well as other aspects of the video. For example, the raw caption data may include additional non-spoken text or specific characters for indicating who is speaking, such as "Mary: hi, it's a great day," where "Mary" indicates the speaker of the phrase. Other non-spoken text may include an indication of sound effects within the video, such as "music playing" or "BANG," each of which may represent sound in the video. Sound effects subtitles are useful to deaf dumb persons and poorly hearing viewers who might not otherwise be aware of other sounds in the video. For the purpose of aligning spoken dialog, non-spoken text may be removed in the normalization process.
In an embodiment, processing logic may implement regular expression matching to identify and remove prefix tags from caption strings and generated strings. For example, the subtitle string set may include "> > Fred: how are you). Processing logic may identify and remove the non-spoken text "> > Fred from the subtitle string set: ", thereby obtaining" How are you ". In an embodiment, processing logic may implement conventional expression matching to remove annotations indicative of the audio description. For example, the caption string "[ music ]" may be removed because it is not part of the dialog.
In an embodiment, the caption string and the generated string may represent languages other than english. Other languages, such as hindi or japanese, use different sentence punctuation. Processing logic may be configured to normalize punctuation into a single type of punctuation. Punctuation marks for different languages may be normalized to english, for example, to accurately align caption string sequences with the generated strings and to determine sentences and sentence fragment boundaries. For example, a caption string representing a hindi dialog may contain "|" for statement breaking. Processing logic may recognize these sentence breaks and replace the hindi sentence breaks with english periods. The processing logic may be configured to use any language as the normalized punctuation language, or may be configured to assign specific characters to represent sentence punctuation.
In one embodiment, processing logic may be configured to infer sentence punctuation based on patterns with caption strings and generated strings. For example, the original subtitle data provided by a crowd-sourced (crawl-sourced) subtitle may lack appropriate punctuation marks. Processing logic is configured to infer sentence punctuation, such as periods, based on additional spacing between words or other queues of capitalized words in the middle of a word sequence. These queues may indicate the start of a new statement. Processing logic may be configured to insert periods where additional space or capitalization of subsequent subtitles occurs.
In one embodiment, processing logic may be configured to normalize the plurality of caption strings and strings in the plurality of generated strings by converting letter cases, removing preceding or following punctuation marks, separating hyphenated words, and/or separating word abbreviations. For example, the generated string from speech recognition and generation service 150 may contain unnecessary hyphenated words, or words may be displayed in abbreviated form when they should not use abbreviated form, which may be removed by processing logic.
After normalizing each sequence of caption strings and generated strings, an integer ID may be assigned to each unique string. The integer ID represents a value within an integer edit distance space within which the integer ID is arranged with respect to a relative word edit distance between associated words. The word edit distance value represents the number of instructions required to transform one word into another. For example, the word edit distance values of the words "mean" and "men" will be equal to 1, because only one transformation instruction of "a" needs to be removed from "mean" to transform "mean" into "men". By representing each character string in the integer editing distance space, the character strings can be arranged in a single-dimensional space. The single dimensional spatial assignment of strings using integer IDs may be based on determining a unique string set within both sequences of caption strings and generating strings.
At block 310, processing logic determines that the set of strings from the subtitle string and the generated string includes a unique set of subtitle strings, a unique set of generated strings, and a common set of strings. The unique set of caption strings may represent strings found only in the sequence of caption strings. The unique set of generated strings may represent strings found only within the sequence of generated strings. The common string set represents strings found by the subtitle string sequence and the generated string sequence.
In an embodiment, the strings within the sequence of caption strings represent a transcription of the spoken dialog from the video, and may represent the spoken dialog more accurately than the generated strings from the speech recognition data. As described above, the speech recognition data mainly contains accurate timing information of each generated character string, but may contain minute errors that may have occurred when recognizing a spoken dialog. As a result, processing logic may treat the unique set of caption strings as strings that are more likely to represent spoken dialog within the video content item.
At block 315, processing logic may assign an integer ID to each string within the set of unique caption strings. In an embodiment, each string in the set of unique caption strings is treated as a seed string within the integer edit distance space. The seed string is a string that is determined to represent a word or phrase that may be spoken in the video content item. The integer IDs assigned to each unique caption string are spaced apart from each other such that the integer edit distance value between the unique caption strings exceeds the edit distance value threshold for matching strings with similar semantics. For example, if the unique set of caption strings includes { light, men, moles, told }, processing logic may assign integer IDs to { light=4352, men=350, moles=1402, told=7701 }. The values of the assigned integer IDs are spaced far enough apart that the number of editing instructions required to change one unique caption string to another is large enough to ensure that the unique caption strings do not erroneously match each other or ignore string differences from one another.
In an embodiment, the spacing between integer ID values of the unique caption strings allows for clustering of unique generated strings that may have small character differences between the generated strings and the caption strings. At block 320, processing logic may assign an integer ID value to at least one unique generation string subset of the set of unique generation strings. Assigning the integer ID value to the at least one subset of uniquely generated strings may be based on determining which uniquely generated strings from the set of uniquely generated strings are within a threshold of the integer edit value. An integer edit value within the integer edit value threshold may indicate that the generated string is sufficiently close to the subtitle string and may be recognized as a variant of the subtitle string. For example, at block 315, the generation string "man" may be identified as sufficiently close to the caption string "men" to which the integer ID value 350 has been assigned. In this case, processing logic may calculate that the integer edit distance value between "men" and "man" is 1, and thus processing logic may assign an integer ID value 351 to the generated string, the integer ID value 351 being one greater than the integer ID value (350) of "men". In an embodiment, the integer edit value threshold used to determine the proximity between two strings may be based on the size of the caption string and the type of edit instruction required to make an exact match. For example, if a caption string is shorter, such as "men", the edit distance value threshold of "men" may be smaller than the edit distance value threshold of a caption string, such as "uniportant", where a longer string may allow a larger edit distance value threshold when two strings are determined to be relatively close.
The operations performed in block 320 may include assigning integer ID values to the generated string subsets. A subset of generated strings within the unique set of generated strings may be assigned integer ID values by treating the subset of generated strings as strings having similarity to the already assigned integer IDs; thus, assigning integer ID values to the subset of generated strings is based on a clustering technique that assigns integer ID values based on already assigned integer ID values of similar strings. At decision block 325, processing logic determines whether there are additional uniquely generated strings within the unique string set that require integer ID value assignments. If there are additional uniquely generated strings to assign integer ID values, processing logic proceeds to block 330 to assign integer ID values to the additional uniquely generated strings. If there are no additional uniquely generated strings to assign integer ID values, processing logic proceeds to block 340 to assign integer ID values to strings within the common string set.
At block 330, processing logic may assign integer ID values to one or more of the remaining unique generation strings in the set of unique generation strings. In an embodiment, one or more remaining uniquely generated strings are treated as seed strings within an integer edit distance space and the integer ID values of the one or more remaining uniquely generated strings are spaced apart such that the integer edit distance values between the remaining uniquely generated strings exceed an edit distance value threshold to match other strings to which the integer ID values have been assigned. For example, the remaining unique generated strings within the set of unique generated strings may include { importance, bright, importantly }. Processing logic may select "import" and "bright" and assign integer ID values 21154 and 25002, respectively.
At decision block 335, processing logic determines whether there are remaining uniquely generated strings within the uniquely generated string set that have not been assigned an integer ID value. If there are remaining unique generated strings in the set of unique generated strings that require assignment of integer ID values, processing logic proceeds to block 320 to assign integer ID values to the remaining unique generated strings using a clustering technique that determines which unique generated strings are within a threshold of integer edit values that indicates that the generated strings are sufficiently close to the assigned strings of integer ID values. For example, the uniquely generated string "important" may be assigned an integer ID value 21151, as "important" may be transformed into the string "importance" (integer ID 21154). Processing logic may iteratively repeat blocks 320 through 335 until all remaining uniquely generated strings are assigned integer ID values.
If at decision block 335, processing logic determines that the unique generation string has been assigned an integer ID value, processing logic proceeds to block 340. At block 340, processing logic assigns integer ID values to strings in the common string set. The common string set includes strings that have been identified as being in both the subtitle string sequence and the generated string sequence. In an embodiment, processing logic may assign integer ID values to at least one subset of strings in the common string set using the clustering technique described in block 320. If there are remaining common strings for which integer ID values need to be assigned, processing logic may select one or more of the remaining common strings and treat the selected common string as a seed string by assigning an integer ID value sufficiently spaced from the already assigned string. Processing logic may then iteratively assign integer ID values to the remaining common strings using clustering and seed assignment techniques similar to those described in blocks 320 and 330, respectively.
In an embodiment, each of the plurality of caption strings and the plurality of generated strings is assigned an integer ID value. Fig. 5A is an illustrative example of integer ID value assignments for a plurality of caption strings and a plurality of generated strings. Table 505 illustrates a plurality of caption strings, a plurality of generated strings, a unique set of caption strings, a unique set of generated strings, and a common set of strings. Table 505 illustrates an integer ID value assignment for each unique string within a plurality of caption strings and a plurality of generated strings. Column 515 displays an integer ID value assigned to each character string, column 520 displays each unique character string from the plurality of caption character strings and the plurality of generated character strings, and column 525 displays an assignment method used to assign an integer ID value to each character string.
Fig. 3B illustrates a process for determining a subtitle string and generating a matching sequence of the strings using the assigned integer ID value. At block 350, processing logic identifies a subtitle string sequence and generates a string sequence for matching. In an embodiment, processing logic uses a plurality of caption strings in the order identified in the original caption data. For example, a sequence of subtitle strings represents the strings in the order of how they are spoken in the video content item. Processing logic uses the plurality of generated strings in the order identified in the speech recognition data.
At block 360, processing logic determines the longest sequence matching pair from the subtitle string sequence and the generated string sequence. In an embodiment, an integer subsequence matching algorithm uses integer ID values associated with the strings in the subtitle string sequence and the generated string sequence to align precise strings with closely matching strings. The closely matching strings may be based on an edit distance value threshold. If the integer subsequence matching algorithm results in a longer sequence matching pair, the integer subsequence matching algorithm may allow a certain number of mismatches within the sequence matching pair. Fig. 5B illustrates a sequence matching pair generated from a subtitle string sequence and a generated string sequence. The match pair 530 may be determined by processing logic at block 360 because it is the longest identified sequence match pair.
In an embodiment, processing logic may allow for mismatches and differences between caption string sequences and generated string sequences. For example, mismatch 532 illustrates a mismatch between "of/and", and mismatch 534 illustrates a mismatch between "to/on". These mismatches may be due to errors in the caption string or the generated string sequence based on miswritten or incorrectly recognized words. Processing logic may be configured to allow for mismatches on short strings because the edit distance value threshold may not be applied when the maximum number of character mutations has occurred. For example, if the transform instruction on the original string transforms more than 40% of the original string, then the edit distance value threshold may be applied, as this may result in an unnecessary match of disparate strings.
At decision diamond 365, processing logic determines whether there is a sequence of caption strings that still need to be matched and the remainder of the generated string sequence. Using the previous example, only a first portion of the subtitle string sequence is matched and a string sequence is generated; thus, processing logic may be configured to iteratively match the remaining sequence portions. Processing logic proceeds to block 370 to select a subtitle string sequence and a subsequence of the generated string sequence for matching. However, if at decision diamond 365 processing logic determines that all portions of the caption string sequence and the generated string sequence have matched, processing logic proceeds to block 375 to generate a mapping between individual caption strings in the caption string sequence and individual generated string sequences in the caption string sequence.
At block 370, processing logic selects a sub-sequence from the subtitle string sequence and the generated string sequence for matching. Processing logic proceeds to 360 to determine the longest sequence matching pair from the selected sequences. Blocks 360-370 are repeated until all sequence matching pairs have been determined.
In an embodiment, processing logic may partially match the sequence of caption strings and the sequence of generated string sequences even if the partially matched sequences contain unmatched strings. The matching portion of the partially matched sequence may be used as an anchor for determining the matched region even if the string is not matched using integer ID values. Other techniques may be used to match sub-sequences in the partially matched sequence.
In an embodiment, if the subsequence of strings includes only the generated string, because the subtitle string is used to determine the spoken dialog, processing logic may ignore the generated string. In another embodiment, if the subtitle string and the subsequence of the generated string have a one-to-one correspondence between the number of strings, the processing logic may assume that the corresponding strings match. For example, the mismatch 552 illustrates the subtitle string "com on (refuel)" and the generated string "c mon". These corresponding strings may be paired together into matches of "com/c" and "on/mon".
In an embodiment, processing logic may calculate a string similarity matrix to determine matches for non-matching subsequences. For example, the mismatch 565 includes a caption string "served with chutney (hot and sour sauce)" and a generated string "so with the chuck knee". Processing logic may determine an exact match of the string "with" between the two subsequences. Processing logic may iteratively determine matches for subsequences generated from the match "with" including "served/so" and "chutney/the chuck knee". Processing logic may determine a match from the subsequence "served/so" because it is a one-to-one correspondence. For the remaining subsequences, processing logic determines the similarity between "chutney" and "chuck" based on the matched three characters "chu". Similarly, "knees" and "chutneys" contain phonetic symbol matched pairs. Because there are no corresponding pairs in the subtitle string subsequence, processing logic may ignore "the" from the generated string subsequence.
At block 375, processing logic may generate a mapping between the individual caption strings and the generated string using the matched sequence pairs.
Fig. 4A illustrates an example method for aligning translated language subtitle data with an audio portion of a video content item using assigned timing information of individual subtitle strings of original subtitle data. In an embodiment, blocks 405-430 represent detailed actions that may be performed within block 225 of FIG. 2.
At block 405, processing logic identifies translation language subtitle data for a video content item. The translated caption data may comprise a plurality of translated strings associated with the audio portion of the video content item. Processing logic may be configured to normalize the translated string in the translated language subtitle data to remove non-spoken text or specific characters from the translated string. Normalization may include identifying statement breaks and other punctuations in the translation string and replacing the translation punctuations with standardized punctuations, such as english, used by processing logic.
At block 410, processing logic generates a set of caption sentence fragments from a plurality of caption strings of the original caption data and generates a set of translation sentence fragments from the set of translation strings. In an embodiment, processing logic may identify the sentence fragments using punctuation as sentence fragment boundary markers in the plurality of caption strings and the plurality of translation strings. For example, commas and sentence periods may be used as boundary markers for identifying sentence fragments within a plurality of caption strings and a plurality of translation strings. Processing logic may generate a set of caption sentence fragments containing the identified sentence fragments of the caption string and a set of translated sentence fragments containing the identified sentence fragments of the translated string.
At block 415, processing logic maps the caption sentence fragments in the caption sentence fragment set to the translation sentence fragments in the translation sentence fragment set. In an embodiment, processing logic may pair statement fragments from the set of caption statement fragments and the set of translation statement fragments using overlapping timing information. The sentence fragments in the set of caption sentence fragments may use caption timing from the original caption data to pair the sentence fragments with the corresponding translated sentence fragments. Timing information from the translated language caption data may be used to identify overlapping pairs of translated sentence fragments and caption sentence fragments.
At block 420, processing logic may estimate a caption sentence fragment time interval for a caption sentence fragment set. In an embodiment, processing logic may use timing information (assigned at block 220) for individual caption strings to estimate a sentence fragment time interval for a caption sentence fragment set. For example, processing logic may assign a start time of a sentence fragment as a time associated with a first string in the sentence fragment. The end time of the sentence fragment may be assigned a time associated with the last string in the sentence fragment.
In some cases, the timing information of individual caption strings may be inaccurate due to timing errors that may be associated with the generated string, or due to mismatches that may occur when mapping caption strings to the generated string. In an embodiment, processing logic may aggregate timing information by string length to calculate a median string duration for each string length to generate summary statistics for different string lengths. For example, summary statistics are aggregated for strings of five characters in length. The calculated median duration of the strings of five characters may then be used to estimate timing information for strings that match at a lower confidence level. A median duration is calculated for each string length of the plurality of caption strings.
Processing logic may use the assigned caption string timing information and the estimated string timing information based on the median duration value to estimate the start and end times of each sentence fragment based on the confidence value for each string. If the sentence fragment contains caption strings that match a high confidence value, processing logic may use assigned timing information associated with the start time of the first caption string in the sentence fragment and the end time of the last string of the sentence fragment to calculate the start and end times of the entire sentence fragment.
If a sentence fragment contains a set of caption strings with a high confidence value near the middle of the sentence fragment and another set of caption strings with a lower confidence value at the beginning and end of the sentence, processing logic may use the set of caption strings with the high confidence value as anchors to determine timing information for the sentence fragment. For the duration of the preceding string and the trailing string, processing logic may use estimated timing information from the calculated summary statistics. Processing logic may subtract the median duration timing value of each low confidence caption string preceding the anchored caption string in the sentence fragment to determine a start time of the sentence fragment. For example, if the sentence fragment contains a caption string "fried served with chutney (deep-fried with hot-spicy sauce)", with a confidence value of { freed=0.72, served=0.18, with=1.00, chutney=0.34 }, processing logic may identify the caption string "with" as a high confidence caption string and anchor the string with start and end times. The start time and end time of "with" are [2:12.709,2:12.979], respectively. The median time of the strings "fred" and "served" are based on the string median time of 5 characters and 6 characters, respectively. Processing logic may subtract the string median time of 5 characters and 6 characters from the start time of the anchor word "with" to estimate the start time of the sentence fragment. The end time of the sentence fragment may similarly be calculated by adding a median duration corresponding to each trailing low confidence caption string until the end of the sentence fragment is reached. For example, the end time may be estimated by adding the median duration of the 7-character string "chutney" to the end time of the anchor subtitle string "with".
If the sentence fragments do not contain any caption strings with high confidence scores, processing logic may approximate the sentence fragment duration using summary statistics for each caption string in the sentence fragments. In an embodiment, processing logic aggregates median duration values for each caption string in the sentence fragments and then uses the estimated timing to determine the midpoint of the sentence fragments. After the midpoint is determined, a start time and an end time may be estimated.
At block 425, processing logic assigns an estimated time interval to an individual translation statement fragment in the translation statement fragment set based on the estimated time interval of the mapped individual caption statement fragment of the caption statement fragment set. In an embodiment, processing logic assigns estimated start/end timings from the set of caption sentence fragments to corresponding translated sentence fragments in the set of translated sentence fragments.
At block 430, processing logic matches the set of speech segments with the set of translated sentence segments. Speech segmentation refers to the time interval during which speech is active in a video content item and only one speaker is speaking. For example, an individual speech segment may represent the duration that one of the persons is speaking. If there are multiple people speaking one after the other within the video, each person's utterance will be represented as a separate utterance segmentation. The speech segments may be provided by the content sharing platform 120 or any other external or internal speech segment service platform configured to analyze the audio and video portions of the video content item to provide speaker information and associated speech segments of subtitle data associated with the video content item. The speech segment set may include timing information identifying a start/end time of each speech segment.
In an implementation, processing logic may request a set of speech segments of a video from content sharing platform 120. Processing logic may compare timing information from the set of speech segments to estimated timing information assigned to the set of translated sentence fragments. If the timing from the individual speech segment overlaps with the estimated timing information of the translated sentence fragment, processing logic may consider the individual speech segment and the corresponding translated sentence fragment as matching. Processing logic may use timing information from the matching utterance sections to adjust timing information of individual translation sentence fragments in the set of translation sentence fragments.
In an embodiment, if the timing information of the translated sentence fragment overlaps with the plurality of speech segments, the processing logic may merge the plurality of speech segments into a single speech segment for the purpose of matching the translated sentence fragment with the corresponding speech segment. Matching the translated sentence fragments to multiple speech segments may occur if the speech segments are over-segmented by the speech segment service platform. For example, the speech segmentation service platform may include multiple speech segments that refer to consecutive utterances that may be over-segmented by a single speaker. During matching, processing logic may group the over-segmented speech segments into single speech segments that match the single translation sentence segments.
In an embodiment, if the translated sentence fragment sufficiently overlaps with an utterance fragment and slightly overlaps with another utterance fragment, the processing logic may be configured to ignore the slightly matched utterance fragment and match the translated sentence fragment with the utterance fragment sufficiently overlapping the translated sentence fragment. Full overlap may be defined as a percentage of timing overlap that exceeds a configured threshold. For example, if 80% of the translated sentence fragments overlap with the speech segments, the translated sentence fragments and the speech segments may be considered to overlap sufficiently. In other embodiments, the sufficient overlap may be based on a minimum amount of time or any other measurable metric.
In an embodiment, if the translated sentence fragment does not sufficiently overlap with a particular speech segment, processing logic may match the translated sentence fragment with the speech segment based on the speech segment, the speech segment being closest based on the timing information.
After matching the translated sentence fragments with the corresponding speech segments, processing logic may use timing information from the matched speech segments to adjust timing information for individual translated sentence fragments in the set of translated sentence fragments. For each speech segment, the matching translated sentence fragment may be scaled in time to match the timing information of the matching speech segment. For example, if two translated sentence fragments match a single speech segment and the first translated sentence fragment is twice the duration of the second translated sentence fragment, processing logic may adjust the start time of the first translated sentence fragment to align with the start time of the speech segment and adjust the end time of the first translated sentence fragment to end at approximately 2/3 of the duration of the speech segment. Processing logic may adjust the start time of the second translation sentence fragment to be approximately 2/3 of the duration of the speech segment and the end time of the second translation sentence fragment to be the end time of the speech segment.
At block 435, processing logic may associate speaker information with each translation sentence fragment in the set of translation sentence fragments using speaker Identifier (ID) information from the speech segment. In an implementation, the provided utterance sections can include metadata information that includes speaker ID information associated with each utterance section. For example, the speaker ID information may indicate which person within the video content item speaks the conversation within the speech segment. The speaker ID information may include demographic information related to the gender, age, screen location of the speaker, or any other relevant information related to the identity of the speaker. For example, the provided speech segments from content sharing platform 120 may use techniques such as face tracking, visual speech classification (i.e., looking at each face in the video over time to automatically infer whether it is speaking) and audio speech recognition to determine speaker ID information for each speaker in the video. In an embodiment, processing logic may assign a unique synthesized speech to each speaker ID based on the speaker ID information.
At block 440, processing logic may combine consecutive translated sentence fragments from the set of translated sentence fragments to generate the set of translated sentences. In an embodiment, processing logic may use the associated speaker ID, punctuation, and timing information of the continuous translation sentence fragments to combine the continuous translation sentence fragments. For example, if consecutive translated sentence fragments have the same associated speaker ID, processing logic may combine the consecutive translated sentence fragments to generate the translated sentence. In another example, if successive translation statement fragments are identified as partial statements, processing logic may combine the partial statements based on punctuation in the translation statement fragments to generate the translation statement. In yet another example, if consecutive translated sentence fragments are close in time with a short gap or no gap between the end time of one translated sentence fragment and the start time of another translated sentence fragment, processing logic may combine the consecutive translated sentence fragments to generate a single translated sentence. Processing logic generates a set of translated sentences including timing information and speaker ID information from the translated sentence fragments. In an embodiment, processing logic may align the set of translation statements with the audio portion of the video.
Fig. 4B illustrates an example method of aligning machine-generated translated caption data with an audio portion of a video using timing information of an assigned individual caption string. In an embodiment, blocks 450-475 represent detailed actions that may be performed within block 225 of FIG. 2.
At block 450, processing logic generates sentence fragments from a plurality of caption strings. In an embodiment, processing logic may identify the sentence fragments using punctuation as a sentence fragment boundary marker in the plurality of caption strings. Processing logic may generate a set of caption sentence fragments containing sentence fragments of the caption string.
At block 455, processing logic estimates a sentence fragment time interval for the set of caption sentence fragments. In an embodiment, processing logic may use the assigned timing information for the individual caption strings to estimate the sentence fragment time intervals of the caption sentence fragment set using summary statistics as depicted in block 415.
At block 460, processing logic may match a set of speech segments provided by the content sharing platform 120 or any other external or internal speech segment service platform with a set of caption sentence segments. In an implementation, processing logic may request a set of speech segments for a video from the content sharing platform 120. Processing logic may compare timing information from the speech segment set with estimated timing information assigned to the subtitle sentence segment set. If the timing from the individual utterance sections overlaps with the estimated timing information of the caption sentence fragments, processing logic may consider the individual utterance sections and the corresponding caption sentence fragments as matching. Processing logic may use timing information from the matched utterance sections to adjust timing information of individual caption sentence fragments in the set of caption sentence fragments.
In an embodiment, if the timing information of the caption sentence fragments overlaps with the plurality of utterance fragments, the processing logic may merge the plurality of utterance fragments into a single utterance fragment to match the caption sentence fragments with the corresponding utterance fragments. Matching the caption sentence fragments to multiple speech segments may occur if the speech segments are over-segmented by the speech segment service platform. For example, the speech segmentation service platform may include multiple speech segments that refer to consecutive utterances made by a single speaker that may have been over-segmented. During matching, processing logic may combine the over-segmented speech segments into a single speech segment that matches the single caption sentence segment.
In an embodiment, if the caption sentence segment substantially overlaps with the utterance segment and slightly overlaps with another utterance segment, the processing logic may be configured to ignore the slightly matched utterance segment and match the caption sentence segment with the utterance segment substantially overlapping the caption sentence segment. In an embodiment, if the caption sentence segment does not sufficiently overlap with a particular utterance segment, processing logic may match the caption sentence segment with the utterance segment based on the utterance segment that is closest based on the timing information.
After matching the caption sentence fragments to the corresponding utterance segments, processing logic may use timing information from the matched utterance segments to adjust timing information for individual caption sentence fragments in the caption sentence fragment set. For each speech segment, the matching caption sentence segments may be proportionally time-adjusted to match the timing information of the matching speech segment. For example, if two caption sentence fragments match a single speech segment and the first caption sentence fragment is twice the duration of the second caption sentence fragment, processing logic may adjust the start time of the first caption sentence fragment to align with the start time of the speech segment and adjust the end time of the first caption sentence fragment to end at approximately 2/3 of the duration of the speech segment. Processing logic may adjust the start time of the second caption sentence segment to be approximately 2/3 of the duration of the speech segment and the end time of the second caption sentence segment to be the end time of the speech segment.
At block 465, processing logic may associate speaker information with each caption sentence segment in the set of caption sentence segments using speaker ID information from the speech segment. As discussed previously, the provided speech segments may include metadata information including speaker ID information associated with each speech segment. The speaker ID information may include demographic information related to the gender, age, screen location of the speaker, or any other relevant information related to the speaker's identity.
At block 470, processing logic may combine successive caption sentence fragments from the caption sentence fragment set to generate a caption sentence set. In an embodiment, processing logic may combine the continuous caption sentence fragments using their associated speaker IDs, punctuation, and timing information. For example, if the continuous caption sentence fragments have the same associated speaker ID, processing logic may combine the continuous caption sentence fragments to generate the caption sentence. In another example, if a continuous caption sentence fragment is identified as a partial sentence, processing logic may combine the partial sentences based on punctuation in the caption sentence fragment to generate the caption sentence. In yet another example, if consecutive caption sentence fragments are close in time with a short gap or no gap between the end time of one caption sentence fragment and the start time of another caption sentence fragment, processing logic may combine the consecutive caption sentence fragments to generate a single caption sentence. Processing logic generates a set of caption statements from the caption statement fragments that includes time information and speaker ID information.
At block 475, processing logic generates a set of translated sentences using the machine translation and the set of caption sentences. In an embodiment, processing logic may send a request to speech recognition and generation service 150 to generate a set of translated sentences from the set of caption sentences. The generated machine translation may be represented as a set of translated sentences, and processing logic may associate timing information and speaker ID information with each individual translated sentence from the corresponding caption sentence. In an embodiment, processing logic may align the set of translation statements with the audio portion of the video.
In an implementation, processing logic may transform a set of translation statements into translated audio utterances that may be superimposed on a video to create a translated version of an original video content item. The translated audio utterance may be generated using an automatic speech synthesizer configured to synthesize a translated sentence into the translated audio utterance. In other examples, the translated audio utterance may be generated using a translated voice performance from a dubbing actor. If higher quality speech and/or emotional utterances produce more desirable translated audio utterances for coverage, it may be beneficial to use dubbing actors to generate the translated audio utterances. FIG. 6A illustrates an example method for overlaying a translated audio utterance onto a video and generating a second video that includes an audio portion of the translated utterance. Example methods may be described using translated audio utterances from a speech synthesizer, translated audio utterances produced by a dubbing actor, or a combination of both. At block 605, processing logic may receive a set of translated audio utterances, wherein each translated audio utterance in the set of translated audio utterances corresponds to a translated sentence in a set of translated sentences.
In an implementation, processing logic may request an audio utterance corresponding to the set of translation sentences from the speech recognition and generation service 150 or any other translation and speech generation service. An audio utterance, referred to as a translated audio utterance, may be received as a set of translated audio utterance segments, where each translated audio utterance segment corresponds to a translated sentence in the set of translated sentences. In an implementation, each audio utterance section in the set of audio utterance sections can include a machine-generated audio utterance that matches a corresponding speaker ID attribute of each translation sentence. For example, if the translation sentence has a speaker ID and associated speaker attributes indicating that the speaker is female, the received corresponding audio utterance section may be a machine-generated audio utterance that matches female speech.
In an embodiment, speech recognition and generation service 150 may be configured to select synthesized speech for a speaker based on associated speaker ID information. The speech recognition and generation service 150 may rank the different speakers based on the duration of each speaker speaking and then assign the matched synthesized speech first to the speaker with the most speaking time. After assigning synthesized speech to the speaker with the most talk time, the remaining speakers may be assigned synthesized speech based on the associated speaker ID information and the remaining available synthesized speech.
At block 610, for each translated audio utterance in the set of translated audio utterances, processing logic may overlay the translated audio utterance onto the video using timing information associated with the corresponding translated sentence and duration information associated with the translated audio utterance. If the duration of the translated audio utterance matches the duration of the corresponding translated sentence, the translated audio utterance may be seamlessly overlaid on the video, replacing the original audio portion of the corresponding translated sentence. For example, if the corresponding translation sentence is "I like chickpea soup (i like chickpea soup)" and the timing information of the translation sentence indicates that the translation sentence is 2.2 seconds long, the translation audio utterance of "I like chickpea soup" can be seamlessly overlaid to the video if the translation audio utterance is also 2.2 seconds long.
If the duration of the translated audio utterance is longer or shorter, then overlaying the translated audio utterance without modification may end up in a seemingly unpleasant appearance. For example, if the translated audio utterance is shorter than the duration of the translated sentence, the audio utterance will end, but the video will show that the speaker's lips are still moving. Conversely, if the translated audio utterance is longer than the duration of the translated sentence, the audio utterance will still be playing after the speaker's lips have stopped moving. In both cases, viewers of the translated video may be prevented from viewing the video due to incorrectly dubbed translated audio utterances.
In an implementation, processing logic may reduce the audio volume of the original audio portion corresponding to the original audio utterance and override the translated audio utterance. In another embodiment, processing logic may digitally wipe speech from the original audio portion while retaining other sounds such as music or background noise and overlay the corresponding translated audio utterance. In yet another embodiment, processing logic may replace the original audio portion with the translated audio utterance.
Fig. 6B illustrates an example method for adjusting overlay parameters to match audio and video durations in a video content item with translated utterances. At block 620, processing logic selects a set of translated audio utterances to overlay on the video. At decision block 625, processing logic determines whether the duration of the selected translated audio utterance matches the duration of the associated video segment. If the duration of the translated audio utterance matches the duration of the associated video segment, no adjustments to the audio or video need be made to overlay the translated audio utterance, and processing logic may proceed to block 630 to perform the overlay. If the duration of the translated audio utterance and the duration of the associated video segment do not match, the audio and/or video of the translated audio utterance may be adjusted to generate seamless coverage of the translated audio utterance and video.
At block 635, processing logic may adjust the overlay parameters so that the duration of the translated speech segment matches a corresponding video segment in the video. The overlay parameters may include adjusting an audio rate of the translated audio utterance and a video rate and/or duration of a segment of video corresponding to the translated audio utterance.
In an implementation, processing logic may adjust the translated audio utterance rate faster or slower in order to match the translated audio utterance to the duration of the original audio utterance in the video. For example, if the translated audio utterance is 2.5 seconds long and the original audio utterance is 2.2 seconds long, processing logic may accelerate the rate of translating the audio utterance by about 10 seconds in order to adjust the duration of translating the audio utterance to be 2.2 seconds long. In another example, if the translated audio utterance is 1.95 seconds long and the original audio utterance is 2.2 seconds long, processing logic may slow down the rate of translating the audio utterance by about 10 seconds in order to adjust the duration of translating the audio utterance to 2.2 seconds long.
If the adjustment parameter is within the desired speed adjustment range, an adjustment to the audio speech rate of the translated audio speech is preferred. If the rate of translating the audio utterance is too fast, the translated utterance sections may play too fast to be understood by a viewer. If the rate of translating the audio utterance slows too much, the translated utterance segmentation may play too slowly and the translated audio utterance may sound distorted or blurry. Processing logic may implement configured minimum and maximum playback speed thresholds, where the minimum speed threshold is the lowest rate at which translated audio utterances may be played without adverse effects and the maximum speed threshold is the fastest rate at which translated audio utterances may be played without adverse effects experienced by a viewer. In an embodiment, the configured minimum and maximum playback speed thresholds may be language specific. The language specific configured min/max playback speed threshold may be based on the intelligibility of each particular language after the utterance is adjusted. For example, an english min/max playback speed threshold may allow for greater speed adjustment than a chinese min/max playback speed threshold.
In an implementation, processing logic may determine a plurality of translated audio utterances that occur proximate in time for the purpose of reducing the amount of translation audio speed rate slowdown. Short pauses can be recognized between multiple translated audio utterances. Processing logic may adjust and override the plurality of translated audio utterances as a group by: the start times of subsequent translated audio utterances within the group are adjusted so as to add additional pauses between the plurality of translated audio utterances. Adding additional pauses between the plurality of translated audio utterances allows the processing logic to extend the duration of the plurality of translated audio utterances without having to significantly slow down the speed rate of each of the plurality of translated audio utterances.
In an implementation, at block 635, processing logic may adjust a translation audio speed rate of the translated audio utterance within the configured minimum and maximum playback speed thresholds. Processing logic may return to decision block 630. If at decision block 630, processing logic determines that the translated audio utterance matches the duration of the original audio utterance in the video, processing logic may proceed to 630 to overlay the translated audio utterance on a corresponding video portion of the video. If the duration of the translated audio utterance does not match the duration of the original audio utterance in the video after adjusting the translated audio speed rate within the configured minimum and maximum playback speed thresholds, processing logic may return to block 635 to make further adjustments to the overlay parameters. Processing logic may adjust a video rate from a corresponding video portion of the video in order to match a duration of the translated audio utterance to an adjusted duration of the video portion corresponding to the original audio utterance.
In an embodiment, processing logic may adjust the video rate of the corresponding video portion corresponding to the original audio utterance by adjusting the video portion playback rate faster or slower in order to match the translated audio utterance duration to the duration of the corresponding video portion. Processing logic may implement a configured min/max video rate adjustment threshold, where the min adjustment threshold is the lowest rate at which video portions may be played without adverse effects and the max adjustment threshold is the fastest rate at which video portions may be played without adverse effects experienced by a viewer. The configured min/max video rate adjustment threshold may be a smaller adjustment window than the configured min/max playback speed threshold for adjusting the audio rate, as adjustments to video may be more noticeable to a viewer than adjustments to the audio rate.
In an embodiment, processing logic may adjust the video rate of the corresponding video portion by copying selected frames in the video portion. Copying the selected frames may increase the playback duration of the video portion in order to match the duration of the longer interpreted audio utterance. The selected frames may be duplicated at regular intervals to extend the duration of the video portion.
In an embodiment, processing logic may remove selected video frames in the video portion in order to compress the duration of the video portion to match the shorter translated audio utterance. For example, processing logic may remove selected frames at regular intervals over the duration of the video portion in order to cause the video portion to play faster.
In an embodiment, adjusting the video rate by copying or removing selected frames may be based on a min/max video adjustment threshold that specifies the number of frames that may be added or removed during a regular interval. The min/max video adjustment threshold may be configured based on the type of video within the video portion. For example, if the video portion includes static frames (such as a display of a slide show), the min/max video adjustment threshold may allow for the removal or duplication of the added video frames, as the viewer is less likely to notice the adjustment to the video portion. If the video portion includes fast moving scenes, such as a sequence of actions, the min/max video adjustment threshold may allow a limited number of changes to the video frames, as the adjustment to fast moving video is more noticeable to the viewer.
In an implementation, processing logic may adjust audio that translates the audio utterance, a video portion corresponding to the original audio utterance, or a combination of audio and video adjustments.
Referring to block 635, processing logic may adjust overlay parameters, such as an audio rate at which the audio utterance is translated or a video rate of the corresponding video portion. After adjusting the overlay parameters, processing logic proceeds to decision block 625 to determine whether additional adjustments are necessary to match the duration of the translated audio utterance and the original audio utterance. If additional adjustments are necessary, processing logic returns to block 635 to make additional adjustments to the audio rate at which the audio utterance is translated or the video rate of the corresponding video portion. If additional adjustments are not necessary, processing logic proceeds to 630 to overlay the translated audio utterance onto a corresponding video portion of the video.
At block 630, processing logic may overlay the translated audio utterance onto a corresponding video portion of the video to generate a translated audio track for the viewer. In an embodiment, processing logic may maintain a non-speaking audio portion of the original audio portion when generating the translated audio track. This allows the viewer to still hear audio related to other non-talking sounds in order to preserve the overall video experience. However, if the audio switches between translating the audio utterance and the original audio, the viewer may experience abrupt changes in the audio sounds, which may be undesirable.
In an embodiment, when there is a short gap between two translated audio utterances, processing logic may reduce the impact of abrupt changes in audio by inserting silence between the two translated audio utterances. For example, if there is a 1/2 second gap between two translated audio utterances, it may be undesirable to play 1/2 second of the original audio portion. Instead, processing logic may insert silence between the two translated audio utterances because the gap between the two translated audio utterances is small.
In an embodiment, if the original audio includes continuous sound to sound immediately before or after the translation of the audio utterance, processing logic may add a short period of silence immediately before and after the translation of the audio utterance. For example, if the original audio includes a continuous sound of traffic noise, hearing the ending interpreted audio utterance and immediately thereafter starting the continuous sound of traffic noise may be uncomfortable. Processing logic may add mute buffers before and after to allow a viewer to understand the interpreted audio utterance before introducing original audio such as continuous sound of traffic noise.
In an embodiment, processing logic may implement a fade-in and fade-out technique to transition between original audio and translated speech segments to prevent abrupt transitions. For example, processing logic may fade out the original audio before playing the translated audio utterance in order to produce a smooth transition to the translated audio utterance. Similarly, after playing the translated audio utterance, processing logic may fade in the original audio in order to produce a smooth transition from the translated audio utterance and the original audio.
After overlaying the translated audio utterance onto the corresponding video portion, processing logic proceeds to decision diamond 640. At decision diamond 640, processing logic determines whether to overlay additional translated audio utterances on the video. If additional translated audio utterances are to be overlaid on the video, processing logic proceeds to 620 to select another translated audio utterance. If all of the translated audio utterances have been overlaid on the video, processing logic proceeds to 615 to generate a second video.
Referring to fig. 6A, at block 615 processing logic generates a second video that includes the translated audio portion overlaid on the audio track and the video portion of the original video. In one embodiment, if the video rate has been adjusted, processing logic may generate a separate second video comprising the adjusted video and the overlaid audio portion.
In an embodiment, if the audio rate is adjusted only during the overlay process, processing logic may merge the overlaid audio portion into an additional audio track in the original video. However, if video rate adjustment has been used to adjust the video rate, processing logic may add additional metadata and translated audio tracks related to the overlay portion to the original video in order to incorporate the translated audio into the original video. The additional metadata may specify when the media player needs to speed up or slow down the video rate, including playback instructions for a client media player such as the media viewer 111, to allow the original video to merge the translated audio without generating a separate second video.
FIG. 7 depicts a block diagram of an example computing system operating in accordance with one or more aspects of the present disclosure. In various illustrative examples, computer system 700 may correspond to any of the computing devices within system architecture 100 of fig. 1. In one embodiment, computer system 700 may be each of servers 130A-130Z. In another implementation, computer system 700 may be each of client devices 130A-130Z.
In some implementations, computer system 700 may be connected to other computer systems (e.g., via a network such as a Local Area Network (LAN), intranet, extranet, or the internet). The computer system 700 may operate in a server or client computer role in a client-server environment, or as a peer computer in a peer-to-peer or distributed network environment. Computer system 700 may be provided by a Personal Computer (PC), tablet PC, set-top box (STB), personal Digital Assistant (PDA), cellular telephone, web appliance, server, network router, switch or bridge, or any device capable of executing a set of instructions (sequential or otherwise) that specify actions to be taken by that device. In addition, the term "computer" shall include any collection of computers that individually or jointly execute a set (or multiple sets) of instructions to perform any one or more of the methodologies discussed herein.
In another aspect, the computer system 700 may include a processing device 702, a volatile memory 704 (e.g., random Access Memory (RAM)), a non-volatile memory 706 (e.g., read Only Memory (ROM) or Electrically Erasable Programmable ROM (EEPROM)), and a data storage device 716 that may communicate with each other via a bus 708.
The processing device 702 may be provided by one or more processors such as: a general purpose processor such as, for example, a Complex Instruction Set Computing (CISC) microprocessor, a Reduced Instruction Set Computing (RISC) microprocessor, a Very Long Instruction Word (VLIW) microprocessor, a microprocessor implementing other types of instruction sets, or a microprocessor implementing a combination of various types of instruction sets, or a special purpose processor such as, for example, an Application Specific Integrated Circuit (ASIC), a Field Programmable Gate Array (FPGA), a Digital Signal Processor (DSP), or a network processor.
Computer system 700 may also include a network interface device 722. The computer system 700 may also include a video display unit 710 (e.g., LCD), an alphanumeric input device 712 (e.g., keyboard), a cursor control device 714 (e.g., mouse), and a signal generation device 720.
The data storage 716 may include a non-transitory computer-readable storage medium 724 on which instructions 726 encoding any one or more of the methods or functions described herein may be stored, including instructions implementing the cloud-based content management platform 125 (125A-125Z) and/or the user interface 134 (134A-134Z) of fig. 1 for implementing the methods described herein.
The instructions 726 may also reside, completely or partially, within the volatile memory 704 and/or within the processing device 702 during execution thereof by the computer system 700, and thus the volatile memory 704 and the processing device 702 may also constitute machine-readable storage media.
While the computer-readable storage medium 724 is shown in an illustrative example to be a single medium, the term "computer-readable storage medium" should be taken to include a single medium or multiple media (e.g., a centralized or distributed database, and/or associated caches and servers) that store the one or more sets of executable instructions. The term "computer-readable storage medium" shall also include any tangible medium that is capable of storing or encoding a set of instructions for execution by a computer to cause the computer to perform any one or more of the methodologies described herein. The term "computer-readable storage medium" shall include, but not be limited to, solid-state memories, optical media, and magnetic media.
In the preceding description, numerous details are set forth. However, it will be apparent to one of ordinary skill in the art having the benefit of the present disclosure that the present disclosure may be practiced without these specific details. In some instances, well-known structures and devices are shown in block diagram form, rather than in detail, in order to avoid obscuring the present disclosure.
Some portions of the detailed description have been presented in terms of algorithms and symbolic representations of operations on data bits within a computer memory. These algorithmic descriptions and representations are the means used by those skilled in the data processing arts to most effectively convey the substance of their work to others skilled in the art. An algorithm is here, and generally, considered to be a self-consistent sequence of steps leading to a desired result. The steps are those requiring physical manipulations of physical quantities. Usually, though not necessarily, these quantities take the form of electrical or magnetic signals capable of being stored, transferred, combined, compared, and otherwise manipulated. It has proven convenient at times, principally for reasons of common usage, to refer to these signals as bits, values, elements, symbols, characters, terms, numbers, or the like.
It should be borne in mind, however, that all of these and similar terms are to be associated with the appropriate physical quantities and are merely convenient labels applied to these quantities. Unless specifically stated otherwise as apparent from the following discussions, it is appreciated that throughout the specification discussions utilizing terms such as "receiving," "displaying," "moving," "adjusting," "replacing," "determining," "playing," or the like, refer to the action and processes of a computer system, or similar electronic computing device, that manipulates and transforms data represented as physical (e.g., electronic) quantities within the computer system's registers and memories into other data similarly represented as physical quantities within the computer system memories or registers or other such information storage, transmission or display devices.
For simplicity of explanation, the methodologies are depicted and described herein as a series of acts. However, acts in accordance with the present disclosure may occur in various orders and/or concurrently, and additional acts are not presented and described herein. Moreover, not all illustrated acts may be required to implement a methodology in accordance with the disclosed subject matter. Furthermore, it should be understood and apparent to those skilled in the art that the methods may alternatively be represented as a series of interrelated states via a state diagram or events. Additionally, it should be apparent that the methods disclosed in this specification can be stored on an article of manufacture to facilitate transporting and transferring such methods to computing devices. The term "article of manufacture" as used herein is intended to encompass a computer program accessible from any computer-readable device or storage media.
Certain embodiments of the present disclosure also relate to an apparatus for performing the operations herein. The apparatus can be configured for the intended purpose or it can comprise a general-purpose computer selectively activated or reconfigured by a computer program stored in the computer. Such a computer program can be stored in a computer readable storage medium, such as, but not limited to, any type of disk including floppy disks, optical disks, CD-ROMs, and magnetic-optical disks, read-only memories (ROMs), random Access Memories (RAMs), EPROMs, EEPROMs, magnetic or optical cards, or any type of media suitable for storing electronic instructions.
Reference throughout this specification to "one embodiment" or "an embodiment" means that a particular feature, structure, or characteristic described in connection with the embodiment is included in at least one embodiment. Thus, appearances of the phrases "in one embodiment" or "in an embodiment" in various places throughout this specification are not necessarily all referring to the same embodiment. Furthermore, the term "or" is intended to mean an inclusive "or" rather than an exclusive "or". Furthermore, the word "example" or "exemplary" is used herein to mean serving as an example, instance, or illustration. Any aspect or design described herein as "exemplary" is not necessarily to be construed as preferred or advantageous over other aspects or designs. Rather, use of the word "example" or "exemplary" is intended to present concepts in a concrete fashion.
It is to be understood that the above description is intended to be illustrative, and not restrictive. Many other embodiments will be apparent to those of skill in the art upon reading and understanding the above description. The scope of the disclosure should, therefore, be determined with reference to the appended claims, along with the full scope of equivalents to which such claims are entitled.
In addition to the above description, the user may be provided with controls that allow the user to make a selection as to whether and when the system, program, or feature described herein may enable user information (e.g., information about the user's social network, social actions or activities, profession, user preferences, or the user's current location) to be collected and whether the user has been sent content or communications from a server. In addition, certain data may be processed in one or more ways before it is stored or used so that personally identifiable information is removed. For example, the identity of the user may be processed such that personally available information cannot be determined for the user, or the geographic location of the user may be generalized (such as to a city, zip code, or state level) where location information is obtained such that a particular location of the user cannot be determined. Thus, the user may have control over what information is collected about the user, how the information is used, and what information is provided to the user.
Claims (14)
1. A method, comprising:
identifying, by a processing device, original subtitle data for a video having audio including an utterance recorded in an original language, wherein the original subtitle data includes a plurality of subtitle strings in the original language and associated with an audio portion of the video;
Identifying, by the processing device, translated language caption data for the video, wherein the translated language caption data includes a plurality of translated strings associated with the audio portion of the video;
generating, by the processing device, a set of caption sentence segments from the plurality of caption strings and a set of translation sentence segments from the plurality of translation strings;
mapping, by the processing device, caption sentence fragments in the set of caption sentence fragments to corresponding translation sentence fragments in the set of translation sentence fragments based on timing associated with the original caption data and the translation language caption data;
estimating, by the processing device, a time interval of individual caption sentence fragments in the set of caption sentence fragments using timing information corresponding to the individual caption character strings;
assigning, by the processing device, time intervals to individual translation sentence fragments of the set of translation sentence fragments based on the estimated time intervals of the individual caption sentence fragments;
generating, by the processing device, a set of translated terms using successive translated terms segments in the set of translated terms segments; and
The set of translated sentences is aligned with the audio portion of the video by the processing device using the assigned time interval from the individual translated sentence fragments of the corresponding translated sentences.
2. The method of claim 1, wherein the set of caption sentence fragments is generated from the plurality of caption strings using punctuation marks in the plurality of caption strings.
3. The method of claim 1, further comprising:
speech recognition data generated for the audio portion of the video is identified, the speech recognition data including a plurality of generated strings and associated timing information for each generated string.
4. A method according to claim 3, further comprising: determining the timing associated with the original subtitle data by mapping the plurality of subtitle strings to the plurality of generated strings, and assigning timing information to individual subtitle strings of the original subtitle data based on timing information of mapped individual generated strings, wherein the timing associated with the original subtitle data corresponds to the timing information assigned to the individual subtitle strings of the original subtitle data.
5. The method of claim 4, wherein the plurality of caption strings are mapped to the plurality of generated strings using an assigned value indicating semantic similarity between the individual caption string and an individual generated string of the plurality of generated strings.
6. The method of claim 4, further comprising: the plurality of caption strings and the plurality of generated strings are normalized by removing non-spoken strings before mapping the plurality of caption strings to the plurality of generated strings.
7. The method of claim 5, wherein mapping the plurality of caption strings to the plurality of generated strings using the assigned value indicative of semantic similarity between the individual caption string of the plurality of caption strings and the individual generated string of the plurality of generated strings comprises:
assigning an integer identifier value to each unique string of the individual subtitle strings of the plurality of subtitle strings and each unique string of the individual generation strings of the plurality of generation strings;
wherein a distance between a first integer identifier value associated with a first string and a second integer identifier value associated with a second string represents a character difference between the first string and the second string.
8. The method of claim 5, wherein mapping the plurality of caption strings to the plurality of generated strings using the assignment value comprises:
determining a matching pair of a sequence of caption strings in the plurality of caption strings and a sequence of generation strings in the plurality of generation strings using the assigned value of each individual caption string in the plurality of caption strings and each individual caption string in the plurality of generation strings; and
mapping the individual ones of the plurality of caption strings to the individual ones of the plurality of generated strings based on the matching pairs of the caption string sequence and the generated string sequence.
9. The method of claim 1, wherein generating the set of translated sentences using successive translated sentence fragments in the set of translated sentence fragments comprises:
generating a set of caption statements using successive caption statement fragments of the set of caption statement fragments based on the punctuation; and
the set of translated sentences is generated based on the set of caption sentences using machine translation.
10. The method of claim 1, further comprising:
Identifying speech segment data of the video, wherein the speech segment data comprises a plurality of speech segments having speech timing information associated with the audio portion of the video; and
the time interval of the individual caption sentence segments in the set of caption sentence segments is updated based on utterance timing information associated with the plurality of utterance segments.
11. The method of claim 1, further comprising:
obtaining a translated audio utterance, the translated audio utterance being a machine-generated utterance for translation of the original subtitle data, wherein the translated audio utterance includes a set of translated audio utterance segments;
overlaying translated audio utterance segments of the set of translated audio utterance segments onto video segments of the video, the video segments corresponding to time intervals of each of the translated audio utterance segments; and
a second video is generated that includes a video portion of the video and a translated audio portion that includes the overlaid translated audio speech segments.
12. The method of claim 11, wherein overlaying the translated audio utterance segments of the set of translated audio utterance segments onto the video segments of the video comprises:
For each translated audio utterance segment in the set of translated audio utterance segments:
determining whether timing information of the translated audio utterance sections matches timing information of corresponding video sections;
when determining that the timing information of the translated audio utterance section does not match the timing information of the corresponding video section, adjusting a playback speed rate of the translated audio utterance section to match the timing information of the translated audio utterance section with the timing information of the corresponding video section; and
overlaying the translated audio utterance segments onto the corresponding video segments.
13. The method of claim 11, wherein overlaying the translated audio utterance segments of the set of translated audio utterance segments onto the video segments of the video comprises:
for each translated audio utterance segment in the set of translated audio utterance segments:
determining whether timing information of the translated audio utterance sections matches timing information of corresponding video sections;
when determining that the timing information of the translated audio utterance section does not match the timing information of the corresponding video section, adjusting a playback speed rate of the corresponding video section to match the timing information of the corresponding video section with the timing information of the translated audio utterance section; and
Overlaying the translated audio utterance segments onto the corresponding video segments.
14. A system, comprising:
a memory; and
a processing device operably coupled with the memory to perform the method of any of claims 1 to 13.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
CN202311273416.6A CN117201889A (en) | 2018-02-26 | 2018-02-26 | Automatic speech translation dubbing of pre-recorded video |
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2018/019779 WO2019164535A1 (en) | 2018-02-26 | 2018-02-26 | Automated voice translation dubbing for prerecorded videos |
CN202311273416.6A CN117201889A (en) | 2018-02-26 | 2018-02-26 | Automatic speech translation dubbing of pre-recorded video |
CN201880090248.8A CN111758264A (en) | 2018-02-26 | 2018-02-26 | Automatic speech translation dubbing for prerecorded video |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201880090248.8A Division CN111758264A (en) | 2018-02-26 | 2018-02-26 | Automatic speech translation dubbing for prerecorded video |
Publications (1)
Publication Number | Publication Date |
---|---|
CN117201889A true CN117201889A (en) | 2023-12-08 |
Family
ID=61622713
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202311273416.6A Pending CN117201889A (en) | 2018-02-26 | 2018-02-26 | Automatic speech translation dubbing of pre-recorded video |
CN201880090248.8A Pending CN111758264A (en) | 2018-02-26 | 2018-02-26 | Automatic speech translation dubbing for prerecorded video |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201880090248.8A Pending CN111758264A (en) | 2018-02-26 | 2018-02-26 | Automatic speech translation dubbing for prerecorded video |
Country Status (5)
Country | Link |
---|---|
US (2) | US11582527B2 (en) |
EP (1) | EP3759935A1 (en) |
KR (2) | KR102598824B1 (en) |
CN (2) | CN117201889A (en) |
WO (1) | WO2019164535A1 (en) |
Families Citing this family (19)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN108401192B (en) * | 2018-04-25 | 2022-02-22 | 腾讯科技（深圳）有限公司 | Video stream processing method and device, computer equipment and storage medium |
KR20200054354A (en) | 2018-11-02 | 2020-05-20 | 삼성전자주식회사 | Electronic apparatus and controlling method thereof |
WO2020181133A1 (en) * | 2019-03-06 | 2020-09-10 | Syncwords Llc | System and method for simultaneous multilingual dubbing of video-audio programs |
JP7208531B2 (en) * | 2019-05-31 | 2023-01-19 | 日本電信電話株式会社 | Synchronization control device, synchronization control method and synchronization control program |
CN111741231B (en) * | 2020-07-23 | 2022-02-22 | 北京字节跳动网络技术有限公司 | Video dubbing method, device, equipment and storage medium |
CN112562719B (en) * | 2020-11-30 | 2024-03-19 | 清华珠三角研究院 | Method, system, device and storage medium for matching synthesized voice with original video |
US11729476B2 (en) * | 2021-02-08 | 2023-08-15 | Sony Group Corporation | Reproduction control of scene description |
CN112954434B (en) * | 2021-02-26 | 2023-07-04 | 北京奇艺世纪科技有限公司 | Subtitle processing method, system, electronic device and storage medium |
KR102440890B1 (en) * | 2021-03-05 | 2022-09-06 | 주식회사 한글과컴퓨터 | Video automatic dubbing apparatus that automatically dubs the video dubbed with the voice of the first language to the voice of the second language and operating method thereof |
CN113207044A (en) * | 2021-04-29 | 2021-08-03 | 北京有竹居网络技术有限公司 | Video processing method and device, electronic equipment and storage medium |
US20230026467A1 (en) * | 2021-07-21 | 2023-01-26 | Salah M. Werfelli | Systems and methods for automated audio transcription, translation, and transfer for online meeting |
CN113569700A (en) * | 2021-07-23 | 2021-10-29 | 杭州菲助科技有限公司 | Method and system for generating dubbing materials through foreign language videos |
CN113490058A (en) * | 2021-08-20 | 2021-10-08 | 云知声（上海）智能科技有限公司 | Intelligent subtitle matching system applied to later stage of movie and television |
TWI783718B (en) * | 2021-10-07 | 2022-11-11 | 瑞昱半導體股份有限公司 | Display control integrated circuit applicable to performing real-time video content text detection and speech automatic generation in display device |
CN114501159B (en) * | 2022-01-24 | 2023-12-22 | 传神联合（北京）信息技术有限公司 | Subtitle editing method and device, electronic equipment and storage medium |
KR102546559B1 (en) * | 2022-03-14 | 2023-06-26 | 주식회사 엘젠 | translation and dubbing system for video contents |
CN117201876A (en) * | 2022-05-31 | 2023-12-08 | 北京字跳网络技术有限公司 | Subtitle generation method, subtitle generation device, electronic device, storage medium, and program |
CN115942043A (en) * | 2023-03-03 | 2023-04-07 | 南京爱照飞打影像科技有限公司 | Video clipping method and device based on AI voice recognition |
CN116471436A (en) * | 2023-04-12 | 2023-07-21 | 央视国际网络有限公司 | Information processing method and device, storage medium and electronic equipment |
Family Cites Families (31)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7047191B2 (en) * | 2000-03-06 | 2006-05-16 | Rochester Institute Of Technology | Method and system for providing automated captioning for AV signals |
US6505153B1 (en) * | 2000-05-22 | 2003-01-07 | Compaq Information Technologies Group, L.P. | Efficient method for producing off-line closed captions |
US7013273B2 (en) * | 2001-03-29 | 2006-03-14 | Matsushita Electric Industrial Co., Ltd. | Speech recognition based captioning system |
JP2007213176A (en) * | 2006-02-08 | 2007-08-23 | Sony Corp | Information processing device, method, and program |
JP2008065653A (en) | 2006-09-08 | 2008-03-21 | Matsushita Electric Ind Co Ltd | Video translation device |
JP4271224B2 (en) | 2006-09-27 | 2009-06-03 | 株式会社東芝 | Speech translation apparatus, speech translation method, speech translation program and system |
US8843368B2 (en) * | 2009-08-17 | 2014-09-23 | At&T Intellectual Property I, L.P. | Systems, computer-implemented methods, and tangible computer-readable storage media for transcription alignment |
WO2012177160A1 (en) * | 2011-06-22 | 2012-12-27 | General Instrument Corporation | Method and apparatus for processing and displaying multiple captions superimposed on video images |
WO2013043984A1 (en) * | 2011-09-23 | 2013-03-28 | Documentation Services Group, Inc. | Systems and methods for extracting and processing intelligent structured data from media files |
CN103150329A (en) | 2013-01-06 | 2013-06-12 | 清华大学 | Word alignment method and device of bitext |
JP6150405B2 (en) * | 2013-01-15 | 2017-06-21 | ヴィキ, インク.Viki, Inc. | System and method for captioning media |
CN104038804B (en) * | 2013-03-05 | 2017-09-29 | 三星电子（中国）研发中心 | Captioning synchronization apparatus and method based on speech recognition |
US9804729B2 (en) * | 2013-03-15 | 2017-10-31 | International Business Machines Corporation | Presenting key differences between related content from different mediums |
KR102061044B1 (en) * | 2013-04-30 | 2020-01-02 | 삼성전자 주식회사 | Method and system for translating sign language and descriptive video service |
KR20150021258A (en) * | 2013-08-20 | 2015-03-02 | 삼성전자주식회사 | Display apparatus and control method thereof |
CN103646645B (en) | 2013-12-13 | 2016-03-02 | 南京丰泰通信技术股份有限公司 | A kind of method exported based on voice translation text |
US9635219B2 (en) * | 2014-02-19 | 2017-04-25 | Nexidia Inc. | Supplementary media validation system |
US9462230B1 (en) * | 2014-03-31 | 2016-10-04 | Amazon Technologies | Catch-up video buffering |
US9614969B2 (en) * | 2014-05-27 | 2017-04-04 | Microsoft Technology Licensing, Llc | In-call translation |
CN105338394B (en) * | 2014-06-19 | 2018-11-30 | 阿里巴巴集团控股有限公司 | The processing method and system of caption data |
CN104252861B (en) | 2014-09-11 | 2018-04-13 | 百度在线网络技术（北京）有限公司 | Video speech conversion method, device and server |
FR3025926B1 (en) * | 2014-09-17 | 2018-11-02 | France Brevets | METHOD FOR CONTROLLING THE DISPLAY SPEED OF SUBTITLES |
CN104244081B (en) | 2014-09-26 | 2018-10-16 | 可牛网络技术（北京）有限公司 | The providing method and device of video |
WO2017152935A1 (en) | 2016-03-07 | 2017-09-14 | Arcelik Anonim Sirketi | Image display device with synchronous audio and subtitle content generation function |
US10037313B2 (en) * | 2016-03-24 | 2018-07-31 | Google Llc | Automatic smoothed captioning of non-speech sounds from audio |
CN105848005A (en) | 2016-03-28 | 2016-08-10 | 乐视控股（北京）有限公司 | Video subtitle display method and video subtitle display device |
CN106791913A (en) | 2016-12-30 | 2017-05-31 | 深圳市九洲电器有限公司 | Digital television program simultaneous interpretation output intent and system |
US10652592B2 (en) * | 2017-07-02 | 2020-05-12 | Comigo Ltd. | Named entity disambiguation for providing TV content enrichment |
US10425696B2 (en) * | 2017-07-11 | 2019-09-24 | Sony Corporation | User placement of closed captioning |
CN108040277B (en) * | 2017-12-04 | 2020-08-25 | 海信视像科技股份有限公司 | Subtitle switching method and device for multi-language subtitles obtained after decoding |
US11245950B1 (en) * | 2019-04-24 | 2022-02-08 | Amazon Technologies, Inc. | Lyrics synchronization |
-
2018
- 2018-02-26 KR KR1020227045220A patent/KR102598824B1/en active IP Right Grant
- 2018-02-26 WO PCT/US2018/019779 patent/WO2019164535A1/en unknown
- 2018-02-26 CN CN202311273416.6A patent/CN117201889A/en active Pending
- 2018-02-26 US US16/975,696 patent/US11582527B2/en active Active
- 2018-02-26 KR KR1020207027403A patent/KR102481871B1/en not_active Application Discontinuation
- 2018-02-26 CN CN201880090248.8A patent/CN111758264A/en active Pending
- 2018-02-26 EP EP18710624.0A patent/EP3759935A1/en active Pending
-
2023
- 2023-02-13 US US18/109,243 patent/US20230199264A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
KR20200118894A (en) | 2020-10-16 |
KR102481871B1 (en) | 2022-12-28 |
US20230199264A1 (en) | 2023-06-22 |
US11582527B2 (en) | 2023-02-14 |
KR102598824B1 (en) | 2023-11-06 |
CN111758264A (en) | 2020-10-09 |
EP3759935A1 (en) | 2021-01-06 |
US20200404386A1 (en) | 2020-12-24 |
WO2019164535A1 (en) | 2019-08-29 |
KR20230005430A (en) | 2023-01-09 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US20230199264A1 (en) | Automated voice translation dubbing for prerecorded video | |
US9786283B2 (en) | Transcription of speech | |
US8812311B2 (en) | Character-based automated shot summarization | |
JP6150405B2 (en) | System and method for captioning media | |
US20090100454A1 (en) | Character-based automated media summarization | |
US20200126583A1 (en) | Discovering highlights in transcribed source material for rapid multimedia production | |
US9645985B2 (en) | Systems and methods for customizing text in media content | |
CN110750996B (en) | Method and device for generating multimedia information and readable storage medium | |
US20150371679A1 (en) | Semi-automatic generation of multimedia content | |
US20230107968A1 (en) | Systems and methods for replaying a content item | |
CN117769739A (en) | System and method for assisted translation and lip matching of dubbing | |
US20240134597A1 (en) | Transcript question search for text-based video editing | |
US11770590B1 (en) | Providing subtitle for video content in spoken language | |
US20240135973A1 (en) | Video segment selection and editing using transcript interactions | |
US20240134909A1 (en) | Visual and text search interface for text-based video editing | |
US20240127820A1 (en) | Music-aware speaker diarization for transcripts and text-based video editing | |
US20240127857A1 (en) | Face-aware speaker diarization for transcripts and text-based video editing | |
US20240127855A1 (en) | Speaker thumbnail selection and speaker visualization in diarized transcripts for text-based video | |
US20240126994A1 (en) | Transcript paragraph segmentation and visualization of transcript paragraphs | |
US11922931B2 (en) | Systems and methods for phonetic-based natural language understanding | |
US20240144931A1 (en) | Systems and methods for gpt guided neural punctuation for conversational speech | |
US20240087557A1 (en) | Generating dubbed audio from a video-based source | |
CN117909545A (en) | Video clip selection and editing using transcript interactions |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |