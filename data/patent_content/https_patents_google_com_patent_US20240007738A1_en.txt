US20240007738A1 - Method and Device for Image Frame Selection - Google Patents
Method and Device for Image Frame Selection Download PDFInfo
- Publication number
- US20240007738A1 US20240007738A1 US18/367,533 US202318367533A US2024007738A1 US 20240007738 A1 US20240007738 A1 US 20240007738A1 US 202318367533 A US202318367533 A US 202318367533A US 2024007738 A1 US2024007738 A1 US 2024007738A1
- Authority
- US
- United States
- Prior art keywords
- frame
- frames
- computer
- capture
- animation
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000000034 method Methods 0.000 title claims abstract description 63
- 230000006870 function Effects 0.000 claims description 27
- 238000010801 machine learning Methods 0.000 claims description 23
- 230000002123 temporal effect Effects 0.000 claims description 12
- 238000013500 data storage Methods 0.000 claims description 9
- 239000003550 marker Substances 0.000 claims description 7
- 238000005070 sampling Methods 0.000 claims description 3
- 238000004891 communication Methods 0.000 description 14
- 230000009471 action Effects 0.000 description 9
- 238000013459 approach Methods 0.000 description 6
- 230000005540 biological transmission Effects 0.000 description 6
- 230000003993 interaction Effects 0.000 description 6
- 230000004044 response Effects 0.000 description 6
- 238000004422 calculation algorithm Methods 0.000 description 5
- 238000010586 diagram Methods 0.000 description 5
- 238000012545 processing Methods 0.000 description 5
- 238000004519 manufacturing process Methods 0.000 description 4
- 230000003287 optical effect Effects 0.000 description 4
- 238000012549 training Methods 0.000 description 4
- 230000008859 change Effects 0.000 description 3
- 230000001815 facial effect Effects 0.000 description 3
- 230000001133 acceleration Effects 0.000 description 2
- 238000013478 data encryption standard Methods 0.000 description 2
- 230000009466 transformation Effects 0.000 description 2
- 241001465754 Metazoa Species 0.000 description 1
- 238000012937 correction Methods 0.000 description 1
- 125000004122 cyclic group Chemical group 0.000 description 1
- 238000006073 displacement reaction Methods 0.000 description 1
- 230000000694 effects Effects 0.000 description 1
- 238000005538 encapsulation Methods 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 230000007613 environmental effect Effects 0.000 description 1
- 238000003384 imaging method Methods 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 230000007774 longterm Effects 0.000 description 1
- 230000007246 mechanism Effects 0.000 description 1
- 230000002085 persistent effect Effects 0.000 description 1
- 230000002441 reversible effect Effects 0.000 description 1
- 238000013515 script Methods 0.000 description 1
- 238000012163 sequencing technique Methods 0.000 description 1
- 239000000779 smoke Substances 0.000 description 1
- 238000000844 transformation Methods 0.000 description 1
- 238000002604 ultrasonography Methods 0.000 description 1
- 238000012795 verification Methods 0.000 description 1
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/64—Computer-aided capture of images, e.g. transfer from script file into camera, check of taken image quality, advice or proposal for image composition or decision on when to take image
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/62—Control of parameters via user interfaces
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/63—Control of cameras or camera modules by using electronic viewfinders
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N5/00—Details of television systems
- H04N5/222—Studio circuitry; Studio devices; Studio equipment
- H04N5/262—Studio circuits, e.g. for mixing, switching-over, change of character of image, other special effects ; Cameras specially adapted for the electronic generation of special effects
Definitions
- image capture devices such as still and/or video cameras.
- the image capture devices can capture images, such as images that include people, animals, landscapes, and/or objects.
- Some image capture devices and/or computing devices can obtain a plurality of images in rapid succession. For example, some image capture devices can obtain burst image data or create a micro video in response to receiving a selection of an option to capture an image. After the plurality of images has been captured, the images can be saved, displayed, transmitted, printed to paper, and/or otherwise utilized.
- a method in one aspect, includes receiving, from an image capture device, a plurality of frames including a capture frame. The method further includes determining a computer-selected frame of the plurality of frames. The method also includes receiving, by way of a user interface, a selection of an option to view the capture frame. The method additionally includes, responsive to receiving the selection, providing, by way of the user interface, an animation between the capture frame and the computer-selected frame. The animation includes an interpolation of one or more frames captured between the capture frame and the computer-selected frame.
- a non-transitory computer readable medium has stored therein instructions executable by one or more processors to cause the one or more processors to perform functions.
- the functions include receiving, from an image capture device, a plurality of frames including a capture frame.
- the functions further include determining a computer-selected frame of the plurality of frames.
- the functions also include receiving, by way of a user interface, a selection of an option to view the capture frame.
- the functions additionally include, responsive to receiving the selection, providing, by way of the user interface, an animation between the capture frame and the computer-selected frame.
- the animation includes an interpolation of one or more frames captured between the capture frame and the computer-selected frame.
- a computing device in another aspect, includes at least one processor, an image capture device, and a non-transitory computer readable medium.
- the non-transitory computer readable medium has stored therein instructions executable by the at least one processor to cause the at least one processor to perform functions.
- the functions include receiving, from an image capture device, a plurality of frames including a capture frame.
- the functions further include determining a computer-selected frame of the plurality of frames.
- the functions also include receiving, by way of a user interface, a selection of an option to view the capture frame.
- the functions additionally include, responsive to receiving the selection, providing, by way of the user interface, an animation between the capture frame and the computer-selected frame.
- the animation includes an interpolation of one or more frames captured between the capture frame and the computer-selected frame.
- a system in another aspect, includes means for receiving a plurality of frames including a capture frame.
- the system further includes means for determining a computer-selected frame of the plurality of frames.
- the system also includes means for receiving a selection of an option to view the capture frame.
- the system additionally includes means for responsive to receiving the selection, providing an animation between the capture frame and the computer-selected frame.
- the animation includes an interpolation of one or more frames captured between the capture frame and the computer-selected frame.
- an article of manufacture includes one or more computer readable media having computer-readable instructions stored thereon that, when executed by one or more processors of a computing device, cause the computing device to carry out functions.
- the functions include receiving, from an image capture device, a plurality of frames including a capture frame.
- the functions further include determining a computer-selected frame of the plurality of frames.
- the functions also include receiving, by way of a user interface, a selection of an option to view the capture frame.
- the functions additionally include, responsive to receiving the selection, providing, by way of the user interface, an animation between the capture frame and the computer-selected frame.
- the animation includes an interpolation of one or more frames captured between the capture frame and the computer-selected frame.
- FIG. 3 shows a computing device providing a capture frame, in accordance with an example embodiment.
- FIG. 4 depicts an animation between a capture frame and a computer-selected frame, in accordance with an example embodiment.
- FIG. 5 depicts an animation between a first computer-selected frame and a second computer-selected frame, in accordance with an example embodiment.
- FIG. 6 depicts interpolation characteristics of an animation, in accordance with an example embodiment.
- FIG. 7 is a flowchart of a method, in accordance with an example embodiment.
- Computing devices may permit a user to select an option to capture an image by way of a user interface.
- the computing device may capture a plurality of frames, which can allow for high-dynamic range imaging, creation of a micro video, image denoising, or other operations.
- these frames one in particular might correspond to a user selection.
- one frame might be captured at substantially the same time that an image capture option is selected by way of a user interface.
- Such a frame may be referred to as a capture frame.
- a user might determine that the capture frame is unsuitable for editing, sharing, or other options.
- the capture frame might be blurry, poorly exposed, or mistimed. Capturing a plurality of frames may resolve these issues by allowing a user to select another frame that the user feels is more suitable for editing, sharing, or otherwise.
- a computing device can further assist the user in finding a more suitable frame by providing one or more computer-selected frames. These computer-selected frames can be determined using a machine learning model that is trained using images from the computing device or from images captured by many computing devices.
- the user might have difficulty determining the relationship between the capture frame and the computer-selected frame.
- the user might have difficulty determining the temporal relationship and the differences between the capture frame and the computer-selected frame.
- Providing an animation between a capture frame and a computer-selected frame may involve receiving a plurality of frames from an image capture device.
- the plurality of frames might be captured in a burst mode of the image capture device, as part of a micro video associated with an application installed on a computing device, or as part of an automated procedure for generating a high-quality image.
- the plurality of frames may include a capture frame that is indicative of which frame a user intended to capture.
- the capture frame can be captured at substantially the same time that an image capture option is selected by a user, or a time at which the image capture option is first selected.
- a computer-selected frame may be determined based on the plurality of frames. For instance, a computing device can receive the frames from the image capture device, send the frames to a server with a request for a computer-selected frame, and receive one or more computer-selected frames from the server.
- the computer-selected frame can be determined based on image quality (e.g. lighting, clarity, noise level, etc.), on characteristics of the scene (e.g. whether subjects are smiling, whether subjects are looking at the image capture device, whether a perceived action in the scene is complete, etc.), or other factors.
- the server or computing device may determine the computer-selected frame using a machine learning model that is trained based on captured frames and on subsequent frame selection decisions of users.
- the computing device may determine the computer-selected frame without use of a machine learning model. For example, the computing device may determine the computer-selected frame based on predetermined criteria for determining image quality or based on past image selections of a user. Other ways of determining the computer-selected image are possible.
- a user may select an option to view the capture frame. For instance, the user may open a virtual photograph gallery, or select a representation of the capture frame in order to edit or share the capture frame using a user interface of the computing device.
- the computing device may provide an option to navigate away from the computer-selected frame using the user interface.
- the computing device may provide a timeline along which the user can scroll to view a different frame.
- the user may interact with the same marker used to denote a temporal relationship between the capture frame and the computer-selected frame during the animation.
- providing the timeline during the animation may also serve to show a user how to interact with the timeline to navigate to other frames.
- the computing device may also provide multiple computer-selected frames, and allow the user to navigate to each computer-selected frame in turn.
- a machine learning model used to determine the computer-selected frames may be trained based on which computer-selected frame is selected or based on a comparison of a computer-selected frame to a frame ultimately selected by a user.
- a user may be provided with controls allowing the user to make an election as to both if and when systems, programs, or features described herein may enable collection of user information (e.g., information about a user's social network, social actions, or activities, profession, a user's preferences, or a user's current location), and if the user is sent content or communications from a server.
- user information e.g., information about a user's social network, social actions, or activities, profession, a user's preferences, or a user's current location
- certain data may be treated in one or more ways before it is stored or used, so that personally identifiable information is removed.
- a user's identity may be treated so that no personally identifiable information can be determined for the user, or a user's geographic location may be generalized where location information is obtained (such as to a city, ZIP code, or state level), so that a particular location of a user cannot be determined.
- location information such as to a city, ZIP code, or state level
- the user may have control over what information is collected about the user, how that information is used, and what information is provided to the user.
- FIG. 1 depicts a distributed computing architecture 100 , in accordance with an example embodiment.
- Distributed computing architecture 100 can include server devices 108 , 110 configured to communicate, via network 106 , with programmable devices 104 a , 104 b , 104 c , 104 d , 104 e .
- Network 106 may correspond to a LAN, a wide area network (WAN), a corporate intranet, the public Internet, or any other type of network configured to provide a communications path between networked computing devices.
- Network 106 may also correspond to a combination of one or more LANs, WANs, corporate intranets, and/or the public Internet.
- FIG. 1 only shows five programmable devices, distributed application architectures may serve tens, hundreds, or thousands of programmable devices.
- programmable devices 104 a , 104 b , 104 c , 104 d , 104 e may be any sort of computing device, such as an ordinary laptop computer, desktop computer, wearable computing device, mobile computing device, head-mountable device, network terminal, wireless communication device (e.g., a smart phone or cell phone), and so on.
- programmable devices 104 a , 104 b , 104 c can be directly connected to network 106 .
- Server devices 108 , 110 can be configured to perform one or more services, as requested by programmable devices 104 a - 104 e .
- server device 108 and/or 110 can provide content to programmable devices 104 a - 104 e .
- the content can include, but is not limited to, web pages, hypertext, scripts, binary data such as compiled software, images, audio, and/or video.
- the content can include compressed and/or uncompressed content.
- the content can be encrypted and/or unencrypted. Other types of content are possible as well.
- server device 108 and/or 110 can provide programmable devices 104 a - 104 e with access to software for database, search, computation, graphical, audio, video, World Wide Web/Internet utilization, and/or other functions.
- server devices Many other examples are possible as well.
- FIG. 2 is a functional block diagram of an example computing device 200 , in accordance with an example embodiment.
- computing device 200 shown in FIG. 2 can be configured to perform at least one function of distributed computing architecture 100 , programmable devices 104 a , 104 b , 104 c , 104 d , 104 e , network 106 , and/or server devices 108 , 110 , computing devices 300 , 400 , and 500 , and/or at least one function related to method 700 .
- Computing device 200 may include user interface module 201 , network communications interface module 202 , one or more processors 203 , data storage 204 , and one or more sensors 220 , all of which may be linked together via a system bus, network, or other connection mechanism 1805 .
- User interface module 201 can be operable to send data to and/or receive data from external user input/output devices.
- user interface module 201 can be configured to send and/or receive data to and/or from user input devices such as a touch screen, a computer mouse, a keyboard, a keypad, a touch pad, a track ball, a joystick, a camera, a voice recognition module, and/or other similar devices.
- User interface module 201 can also be configured to provide output to user display devices, such as one or more cathode ray tubes (CRT), liquid crystal displays, light emitting diodes (LEDs), displays using digital light processing (DLP) technology, printers, light bulbs, and/or other similar devices, either now known or later developed.
- CTR cathode ray tubes
- LEDs light emitting diodes
- DLP digital light processing
- User interface module 201 can also be configured to generate audible outputs, such as a speaker, speaker jack, audio output port, audio output device, earphones, and/or other similar devices.
- User interface module 201 can further be configured with one or more haptic devices that can generate haptic outputs, such as vibrations and/or other outputs detectable by touch and/or physical contact with computing device 200 .
- user interface module 201 can be used to provide a graphical user interface for utilizing computing device 200 .
- Network communications interface module 202 can include one or more wireless interfaces 207 and/or one or more wireline interfaces 208 that are configurable to communicate via a network.
- Wireless interfaces 207 can include one or more wireless transmitters, receivers, and/or transceivers, such as a BluetoothTM transceiver, a Zigbee® transceiver, a Wi-FiTM transceiver, a WiMAXTM transceiver, and/or other similar type of wireless transceiver configurable to communicate via a wireless network.
- Wireline interfaces 208 can include one or more wireline transmitters, receivers, and/or transceivers, such as an Ethernet transceiver, a Universal Serial Bus (USB) transceiver, or similar transceiver configurable to communicate via a twisted pair wire, a coaxial cable, a fiber-optic link, or a similar physical connection to a wireline network.
- wireline transmitters such as an Ethernet transceiver, a Universal Serial Bus (USB) transceiver, or similar transceiver configurable to communicate via a twisted pair wire, a coaxial cable, a fiber-optic link, or a similar physical connection to a wireline network.
- USB Universal Serial Bus
- network communications interface module 202 can be configured to provide reliable, secured, and/or authenticated communications.
- information for ensuring reliable communications i.e., guaranteed message delivery
- a message header and/or footer e.g., packet/message sequencing information, encapsulation headers and/or footers, size/time information, and transmission verification information such as cyclic redundancy check (CRC) and/or parity check values.
- CRC cyclic redundancy check
- Communications can be made secure (e.g., be encoded or encrypted) and/or decrypted/decoded using one or more cryptographic protocols and/or algorithms, such as, but not limited to, Data Encryption Standard (DES), Advanced Encryption Standard (AES), an Rivest-Shamir-Adelman (RSA) algorithm, a Diffie-Hellman algorithm, a secure sockets protocol such as Secure Sockets Layer (SSL) or Transport Layer Security (TLS), and/or Digital Signature Algorithm (DSA).
- DES Data Encryption Standard
- AES Advanced Encryption Standard
- RSA Rivest-Shamir-Adelman
- SSL Secure Sockets Layer
- TLS Transport Layer Security
- DSA Digital Signature Algorithm
- Other cryptographic protocols and/or algorithms can be used as well or in addition to those listed herein to secure (and then decrypt/decode) communications.
- One or more processors 203 can include one or more general purpose processors, and/or one or more special purpose processors (e.g., digital signal processors, graphics processing units, application specific integrated circuits, etc.). One or more processors 203 can be configured to execute computer-readable program instructions 206 that are contained in data storage 204 and/or other instructions as described herein.
- processors 203 can include one or more general purpose processors, and/or one or more special purpose processors (e.g., digital signal processors, graphics processing units, application specific integrated circuits, etc.).
- One or more processors 203 can be configured to execute computer-readable program instructions 206 that are contained in data storage 204 and/or other instructions as described herein.
- Data storage 204 can include one or more computer-readable storage media that can be read and/or accessed by at least one of one or more processors 203 .
- the one or more computer-readable storage media can include volatile and/or non-volatile storage components, such as optical, magnetic, organic or other memory or disc storage, which can be integrated in whole or in part with at least one of one or more processors 203 .
- data storage 204 can be implemented using a single physical device (e.g., one optical, magnetic, organic or other memory or disc storage unit), while in other embodiments, data storage 204 can be implemented using two or more physical devices.
- Data storage 204 can include computer-readable program instructions 206 and perhaps additional data. In some embodiments, data storage 204 can additionally include storage required to perform at least part of the herein-described methods, scenarios, and techniques and/or at least part of the functionality of the herein-described devices and networks.
- computing device 200 can include one or more sensors 220 .
- Sensors 220 can be configured to measure conditions in an environment of computing device 200 and provide data about that environment.
- sensors 220 can include one or more of: (i) an identification sensor to identify other objects and/or devices, such as, but not limited to, a Radio Frequency Identification (RFID) reader, proximity sensor, one-dimensional barcode reader, two-dimensional barcode (e.g., Quick Response (QR) code) reader, and a laser tracker, where the identification sensors can be configured to read identifiers, such as RFID tags, barcodes, QR codes, and/or other devices and/or object configured to be read and provide at least identifying information; (ii) sensors to measure locations and/or movements of computing device 200 , such as, but not limited to, a tilt sensor, a gyroscope, an accelerometer, a Doppler sensor, a Global Positioning System (GPS) device, a sonar sensor, a radar device, a laser-disp
- GPS
- the sensors 220 can also include one or more image capture devices 222 .
- the one or more image capture devices 222 may be configured to capture a plurality of images in rapid succession.
- computing device 200 may receive a selection of an option to capture an image by way of user interface module 201 , and may responsively capture a plurality of images using one or more image capture devices 222 .
- the plurality of images may be referred to as a plurality of frames, and may take the form of burst image data, a micro video, or another collection of images.
- FIG. 3 shows a computing device 300 that is providing a capture frame.
- Computing device 300 includes a user interface 302 .
- User interface 302 provides an image 304 , and an option 306 to see another frame.
- Computing device 300 may be configured in much the same fashion described above with respect to computing device 200 , and may operate similarly to programmable devices 104 a , 104 b , 104 c , 104 d , and 104 e in a network context. Though computing device 300 is depicted as a mobile computing device, the forthcoming description can apply to other types of devices as well.
- Computing device 300 may provide image 304 responsive to an image capture device capturing a plurality of images. For example, computing device 300 may receive a selection of an option to take a photograph by way of an interaction with user interface 302 .
- Image 304 may be an image that is captured at substantially the same time that the selection is received. Accordingly, image 304 may be referred as a capture frame of a plurality of frames.
- computing device 300 may provide, by way of user interface 302 , a representation of image 304 , as well as options related to image 304 .
- options to share, edit, or delete image 304 are presented, though other options are possible.
- user interface 302 provides an option 306 to “see another moment.”
- Interacting with option 306 may prompt computing device 300 to determine a computer-selected frame.
- the computer-selected frame might be determined by computing device 300 automatically each time a plurality of images are captured in rapid succession by an image capture device associated with computing device 300 .
- Computing device 300 the server, or another device may weigh characteristics of each image in the plurality of images to determine a computer-selected frame.
- the computer-selected frame may be determined using a machine learning model that is trained based on images received from computing device 300 and other computing devices.
- computing device 300 may determine the computer-selected frame based on predetermined criteria for measuring quality of a frame. For example, computing device 300 can assign a score to each frame in the plurality of frames based on the criteria, and determine the computer-selected frame by comparing respective scores each frame. In some examples, the criteria can be weighted based on preferences selected by a user. In other examples, computing device 300 , a server, or another computing device may track selections of frames to determine the user preferences. Accordingly, in some embodiments determining the computer-selected frame may be performed based on one or more prior user actions.
- computing device 300 may determine a plurality of computer-selected frames, and may rank them. For example, computing device 300 may determine a primary computer-selected frame that is determined most likely to be suitable for selection, a secondary computer-selected frame that is next most likely to be suitable for selection, and so on. After receiving selection of option 306 to view a computer-selected frame, computing device may present, by way of user interface 302 , a representation of the capture frame and the one or more computer-selected frames.
- FIG. 4 depicts an animation between a capture frame 404 and a computer-selected frame 424 , in accordance with an example embodiment.
- Computing device 400 includes user interface 402 .
- User interface 402 is used to provide an animation between capture frame 404 and computer-selected frame 424 .
- the present example depicts a first time during the animation at which capture frame 404 is shown, a second time at which computer-selected frame 424 is shown, and a representation of an animation provided between the first time and the second time.
- Timeline 406 may include a plurality of snapshots of the plurality of images that show a thumbnail view of the captured scene at a time that corresponds to the snapshot.
- Timeline 406 may further include one or more indicators denoting snapshots that correspond to significant frames, such as the capture frame or a computer-selected frame.
- indicator 408 is associated with snapshot 410 , which provides a thumbnail view of capture frame 404 .
- indicator 412 is associated with snapshot 414 , which provides a thumbnail view of a computer-selected frame.
- the snapshots provided in timeline 406 are can be representative of frames used in the animation.
- the animation involves displaying frames captured between capture frame 404 and computer-selected frame 424 .
- Such frames can be referred to as interpolation frames.
- the interpolation frames can include each frame captured between capture frame 404 and computer-selected frame 424 .
- the interpolation frames can be sampled such that only a portion of captured frames are used in the animation.
- additional frames may be generated based on the captured frames.
- FIG. 4 depicts four interpolation frames 430 used to animate from capture frame 404 to computer-selected frame 424 .
- four interpolation frames 430 are depicted in FIG. 4 to correspond to the snapshots in timeline 406 , it should be understood that several interpolation frames can correspond to a single representative snapshot.
- the animation may help to illustrate differences between capture frame 404 and computer-selected frame 424 .
- the animation might show that capture frame 404 is blurred and that computer-selected frame 424 is clearer.
- the animation may show that subjects of the portrait are not smiling in capture frame 404 and that all subjects are smiling in computer-selected frame 424 .
- the animation may show that capture frame 404 depicts a less interesting portion of the action and that computer-selected frame 424 depicts a more interesting portion of the action.
- Other types of scenes and animations are possible.
- timeline 406 shows that a computer-selected frame was captured after capture frame 404 , which in turn indicates that the computer-selected frame was captured after a selection of an option to capture an image.
- Timeline 406 also depicts frames that are captured prior to capture frame 404 .
- an application associated with an image capture device that captured capture frame 404 may cause the image capture device to continuously capture images while awaiting a selection to capture an image.
- the plurality of frames can include frames captured prior to the capture frame and frames captured after the capture frame. In other examples the plurality of frames may only include frames captured after the capture frame.
- a marker on timeline 406 is adjusted to indicate a temporal relationship between the capture frame and the computer-selected frame. For example, different snapshots of timeline 406 can be highlighted to serve as a marker that denotes the passage of time. In other examples, a bar may pass across timeline 406 to serve as the marker. Other types of markers can be used with a timeline to depict a temporal relationship between different frames.
- an option may be provided to interact with the computer-selected frame, or to view another frame of the plurality of frames. For instance, to share or edit computer-selected frame 424 , a user may simply touch the portion of user interface 402 that depicts computer-selected frame 424 . A user can also interact with timeline 406 to navigate to another frame. In other examples, more than one computer-selected frame may be provided, and user interface 402 may provide a selectable option to view additional computer-selected frames. Other options to interact with computer-selected frame 424 or view another frame are possible as well.
- FIG. 5 depicts an animation between a first computer-selected frame 504 and a second computer-selected frame 524 , in accordance with an example embodiment.
- Computing device 500 includes user interface 502 .
- User interface 502 provides first computer-selected frame 504 .
- User interface 502 further includes a timeline 506 .
- Timeline 506 includes indicators 508 and 512 which respectively correspond to a snapshot 510 of a capture frame and a snapshot 514 of first computer-selected frame 504 .
- the present example depicts a first time during the animation at which first computer-selected frame 504 is shown, a second time at which computer-selected frame 524 is shown, and a representation of an animation provided between the first time and the second time.
- FIG. 5 depicts user interface 502 after an animation from a capture frame to first computer-selected frame 504 has already occurred.
- User interface 502 may include an option to perform an action associated with first computer-selected frame 504 . For instance, an option may be presented to share or edit first computer-selected frame 504 . Further, an option to navigate to another frame, such as a second computer-selected frame, may be presented by way of user interface 502 .
- FIG. 5 also shows an interaction 520 that causes user interface 502 to scroll through the plurality of frames towards second computer-selected frame 524 .
- Interaction 520 might be, for example, a swipe across a screen of computing device 500 .
- Second computer-selected frame 524 may be associated with a snap radius 522 that causes user interface 502 to show second computer-selected frame 524 once a threshold level of proximity is reached between a current frame shown by user interface 502 and second computer-selected frame 524 . For example, a user may scroll across timeline 506 until snap radius 522 is reached.
- computing timeline 506 may cause user interface 502 to move timeline 506 to snapshot 518 , and provide second computer-selected frame 524 .
- Computing device 500 may animate between frames as a user scrolls through timeline 506 .
- FIG. 5 shows an animation between first computer-selected frame 504 and second computer-selected frame 524 being provided in response to interaction 520 , such an animation may occur automatically.
- the computer-selected frames may be ranked.
- first computer-selected frame 504 might be a primary computer-selected frame and second computer-selected frame 524 might be a secondary computer-selected frame.
- Computing device 500 may provide an animation that depicts the differences between a capture frame and the primary frame, between the primary frame and the secondary frame, and so on.
- the ranked computer-selected frames might be determined using a machine learning model that is trained based on images received from computing device 500 and other computing devices. In such examples, the machine learning model can be trained based on which of the computer-selected frames, if any, is selected, and/or based on a comparison of the computer-selected frames.
- FIG. 6 depicts interpolation characteristics of an animation, in accordance with an example embodiment.
- FIG. 6 includes exemplary graphs 600 and 602 .
- Graph 600 depicts frames traversed in the animation versus time of the animation and graph 602 depicts interpolation distance between animation frames versus time of the animation.
- Interpolating between two frames of the plurality of frames can involve displaying each frame captured between the capture frame and the computer-selected frame.
- interpolating between the frames can involve sampling select frames within the plurality, and in some instances may involve generating additional frames for the animation based on characteristics of the captured frames in the plurality.
- the animation time will remain the same for all animations, regardless of the number of frames that separate a pair of frames that begin and end the animation.
- graph 600 shows that capture frame 404 and computer-selected frame 424 are separated by fewer frames than those that separate first computer-selected frame 504 and second computer-selected frame 524 .
- graph 600 depicts their animation time as being substantially identical.
- Interpolating between frames may be non-linear. For example, different frames may be displayed for different periods of time such that an animation appears to accelerate or decelerate as the animation approaches an ending frame.
- interpolating frames includes sampling frames captured between a capture frame and a computer-selected frame
- non-linear interpolation involves changing the number of frames skipped between sampled frames used in the animation may change as the animation approaches the last frame.
- this change in time periods for displaying different frames, or change to numbers of skipped frames may differ between animations.
- graph 602 shows that the animation between capture frame 404 and computer-selected frame 424 progresses more slowly than the animation between first computer-selected frame 504 and second computer-selected frame 524 . Because both depicted animations take the same time to conclude, and because the animation between first computer-selected frame 504 and second computer-selected frame 524 traverses more captured frames than that between capture frame 404 and computer-selected frame 424 , the former shows a greater animation speed.
- Graph 602 also shows that the depicted animations accelerate toward the last frame. In other words, as the animation progresses, the animation approaches the last frame more quickly. This acceleration is more pronounced where more frames separate the first frame and the last frame in an animation. For example, first computer-selected frame 504 accelerates more quickly towards second computer-selected frame 524 than capture frame 404 accelerates towards computer-selected frame 424 within the respective animations depicted in FIGS. 5 and 4 .
- graphs 600 and 602 depict a particular way of interpolating frames for an animation
- interpolation can take different times depending on the number of traversed frames in a given animation, can include uniform accelerations across all animations, or can be linear rather than non-linear.
- These different ways of interpolating frames may vary based on context as well. For example, an animation between a capture frame and a computer-selected frame have the characteristics depicted in graphs 600 and 602 , while animating between frames as a user scrolls through a timeline, such as timeline 406 or 506 , may depend on how quickly the user scrolls. Other context-based changes in interpolation are possible as well.
- FIG. 7 is a flowchart of a method, in accordance with an example embodiment.
- Method 700 can be a computer-implemented method.
- method 700 can be executed by a computing device, such as computing device 200 , 300 , 400 , or 500 .
- method 700 may further include receiving, by way of the user interface, a selection of the computer-selected frame, and training the machine learning model based on the received selection.
- Method 700 may further include block 730 .
- Block 730 can be performed to receive, by way of a user interface, a selection of an option to view the capture frame.
- the option to view the capture frame can include an option to edit the capture frame.
- Method 700 may further include block 740 .
- Block 740 can be performed to, responsive to receiving the selection, provide, by way of the user interface, an animation between the capture frame and the computer-selected frame.
- the animation may include an interpolation of one or more frames captured between the capture frame and the computer-selected frame.
- Providing the animation can further include providing a timeline of the plurality of frames. Concurrently with the interpolation of one or more frames, a marker on the timeline can be adjusted to indicate a temporal relationship between the capture frame and the computer-selected frame.
- the interpolation of the one or more frames can be non-linear.
- the non-linear interpolation might be performed such that the animation accelerates from the capture frame to the computer-selected frame.
- Method 700 may additionally include, after providing the animation, provide, by way of the user interface, an option to navigate from the computer-selected frame to select a frame from the plurality of frames.
- Method 700 can further include, responsive to receiving the selection of the option to view the capture frame, providing, by way of the user interface, a still image of the capture frame.
- Method 700 may also include providing, with the still image of the capture frame, an option to view the computer-selected frame and receiving, by way of the user interface, a selection of the option to view the computer-selected frame.
- Providing the animation can be performed responsive to receiving the selection of the option to view the computer-selected frame.
- method 700 may include determining a plurality of computer-selected frames.
- the computer-selected frame can be a primary computer-selected frame, and method 700 can further include further determining a secondary computer-selected frame.
- method 700 can include providing, by way of the user interface, an option to navigate from the primary computer-selected frame to select the secondary computer-selected frame.
- the primary computer-selected frame and the secondary computer-selected frame can be determined using a machine learning model.
- method 700 may further include receiving, by way of the user interface, a selection of the option to navigate from the primary computer-selected frame to select the secondary computer-selected frame, and training the machine learning model based on the received selection.
- method 700 can further include receiving, by way of the user interface, a selection of the option to navigate from the primary computer-selected frame to select the secondary computer-selected frame, and receiving, by way of the user interface, a selection of the secondary computer-selected frame.
- Further method 700 may include, responsive to receiving the selection of the secondary computer-selected frame, training the machine learning model based on a comparison of the secondary computer-selected frame and the primary computer-selected frame.
- method 700 may further include, receiving, by way of the user interface, a selection of the option to navigate from the computer-selected frame to select a frame from the plurality of frames, and receiving, by way of the user interface, a selection of a selected frame from the plurality of frames.
- method 700 can include, responsive to receiving the selection, providing, by way of the user interface, an option to perform an action related to the selected frame. Further method 700 may include training the machine learning model based on a comparison of the selected frame and the computer-selected frame.
- a computing device can be provided, where the computing device includes: one or more processors; and one or more computer readable media.
- the one or more computer readable media can have computer-readable instructions stored thereon that, when executed by the one or more processors, cause the computing device to carry out functions that include method 700 .
- a computing device can be provided, where the computing device includes means for carrying out method 700 .
- an article of manufacture can be provided.
- the article of manufacture can include one or more computer readable media having computer-readable instructions stored thereon that, when executed by one or more processors of a computing device, cause the computing device to carry out functions that include method 700 .
- the one or more computer readable media can include one or more non-transitory computer readable media.
- each block and/or communication may represent a processing of information and/or a transmission of information in accordance with example embodiments.
- Alternative embodiments are included within the scope of these example embodiments.
- functions described as blocks, transmissions, communications, requests, responses, and/or messages may be executed out of order from that shown or discussed, including substantially concurrent or in reverse order, depending on the functionality involved.
- more or fewer blocks and/or functions may be used with any of the ladder diagrams, scenarios, and flow charts discussed herein, and these ladder diagrams, scenarios, and flow charts may be combined with one another, in part or in whole.
- a block that represents a processing of information may correspond to circuitry that can be configured to perform the specific logical functions of a herein-described method or technique.
- a block that represents a processing of information may correspond to a module, a segment, or a portion of program code (including related data).
- the program code may include one or more instructions executable by a processor for implementing specific logical functions or actions in the method or technique.
- the program code and/or related data may be stored on any type of computer readable medium such as a storage device including a disk or hard drive or other storage medium.
- the computer readable medium may also include non-transitory computer readable media such as non-transitory computer-readable media that stores data for short periods of time like register memory, processor cache, and random access memory (RAM).
- the computer readable media may also include non-transitory computer readable media that stores program code and/or data for longer periods of time, such as secondary or persistent long term storage, like read only memory (ROM), optical or magnetic disks, compact-disc read only memory (CD-ROM), for example.
- the computer readable media may also be any other volatile or non-volatile storage systems.
- a computer readable medium may be considered a computer readable storage medium, for example, or a tangible storage device.
- a block that represents one or more information transmissions may correspond to information transmissions between software and/or hardware modules in the same physical device. However, other information transmissions may be between software modules and/or hardware modules in different physical devices.
Abstract
Method and device for image frame selection are provided. A computing device can receive, from an image capture device, a plurality of frames including a capture frame. The computing device can determine a computer-selected frame of the plurality of frames. The computing device can receive, by way of a user interface, a selection of an option to view the capture frame. The computing device can, responsive to receiving the selection, provide, by way of the user interface, an animation between the capture frame and the computer-selected frame. The animation includes an interpolation of one or more frames captured between the capture frame and the computer-selected frame.
Description
- Many modern computing devices, including mobile phones, personal computers, and tablets, include image capture devices, such as still and/or video cameras. The image capture devices can capture images, such as images that include people, animals, landscapes, and/or objects.
- Some image capture devices and/or computing devices can obtain a plurality of images in rapid succession. For example, some image capture devices can obtain burst image data or create a micro video in response to receiving a selection of an option to capture an image. After the plurality of images has been captured, the images can be saved, displayed, transmitted, printed to paper, and/or otherwise utilized.
- In one aspect, a method is provided. The method includes receiving, from an image capture device, a plurality of frames including a capture frame. The method further includes determining a computer-selected frame of the plurality of frames. The method also includes receiving, by way of a user interface, a selection of an option to view the capture frame. The method additionally includes, responsive to receiving the selection, providing, by way of the user interface, an animation between the capture frame and the computer-selected frame. The animation includes an interpolation of one or more frames captured between the capture frame and the computer-selected frame.
- In another aspect, a non-transitory computer readable medium is provided. The non-transitory computer readable medium has stored therein instructions executable by one or more processors to cause the one or more processors to perform functions. The functions include receiving, from an image capture device, a plurality of frames including a capture frame. The functions further include determining a computer-selected frame of the plurality of frames. The functions also include receiving, by way of a user interface, a selection of an option to view the capture frame. The functions additionally include, responsive to receiving the selection, providing, by way of the user interface, an animation between the capture frame and the computer-selected frame. The animation includes an interpolation of one or more frames captured between the capture frame and the computer-selected frame.
- In another aspect, a computing device is provided. The computing device includes at least one processor, an image capture device, and a non-transitory computer readable medium. The non-transitory computer readable medium has stored therein instructions executable by the at least one processor to cause the at least one processor to perform functions. The functions include receiving, from an image capture device, a plurality of frames including a capture frame. The functions further include determining a computer-selected frame of the plurality of frames. The functions also include receiving, by way of a user interface, a selection of an option to view the capture frame. The functions additionally include, responsive to receiving the selection, providing, by way of the user interface, an animation between the capture frame and the computer-selected frame. The animation includes an interpolation of one or more frames captured between the capture frame and the computer-selected frame.
- In another aspect, a system is provided. The system includes means for receiving a plurality of frames including a capture frame. The system further includes means for determining a computer-selected frame of the plurality of frames. The system also includes means for receiving a selection of an option to view the capture frame. The system additionally includes means for responsive to receiving the selection, providing an animation between the capture frame and the computer-selected frame. The animation includes an interpolation of one or more frames captured between the capture frame and the computer-selected frame.
- In another aspect, an article of manufacture is provided. The article of manufacture includes one or more computer readable media having computer-readable instructions stored thereon that, when executed by one or more processors of a computing device, cause the computing device to carry out functions. The functions include receiving, from an image capture device, a plurality of frames including a capture frame. The functions further include determining a computer-selected frame of the plurality of frames. The functions also include receiving, by way of a user interface, a selection of an option to view the capture frame. The functions additionally include, responsive to receiving the selection, providing, by way of the user interface, an animation between the capture frame and the computer-selected frame. The animation includes an interpolation of one or more frames captured between the capture frame and the computer-selected frame.
- The foregoing summary is illustrative only and is not intended to be in any way limiting. In addition to the illustrative aspects, embodiments, and features described above, further aspects, embodiments, and features will become apparent by reference to the figures and the following detailed description and the accompanying drawings.
-
FIG. 1 depicts a distributed computing architecture, in accordance with an example embodiment. -
FIG. 2 is a functional block diagram of an example computing device, in accordance with an example embodiment. -
FIG. 3 shows a computing device providing a capture frame, in accordance with an example embodiment. -
FIG. 4 depicts an animation between a capture frame and a computer-selected frame, in accordance with an example embodiment. -
FIG. 5 depicts an animation between a first computer-selected frame and a second computer-selected frame, in accordance with an example embodiment. -
FIG. 6 depicts interpolation characteristics of an animation, in accordance with an example embodiment. -
FIG. 7 is a flowchart of a method, in accordance with an example embodiment. - Computing devices may permit a user to select an option to capture an image by way of a user interface. Upon receiving such a selection, the computing device may capture a plurality of frames, which can allow for high-dynamic range imaging, creation of a micro video, image denoising, or other operations. Of these frames, one in particular might correspond to a user selection. For instance, one frame might be captured at substantially the same time that an image capture option is selected by way of a user interface. Such a frame may be referred to as a capture frame.
- In some instances, a user might determine that the capture frame is unsuitable for editing, sharing, or other options. For example, the capture frame might be blurry, poorly exposed, or mistimed. Capturing a plurality of frames may resolve these issues by allowing a user to select another frame that the user feels is more suitable for editing, sharing, or otherwise. A computing device can further assist the user in finding a more suitable frame by providing one or more computer-selected frames. These computer-selected frames can be determined using a machine learning model that is trained using images from the computing device or from images captured by many computing devices.
- Where computer-selected frames are provided to a user, the user might have difficulty determining the relationship between the capture frame and the computer-selected frame. In particular, the user might have difficulty determining the temporal relationship and the differences between the capture frame and the computer-selected frame.
- The detailed description provided herein describes methods and devices for providing an animation between a captured frame and a computer-selected frame that may help to remedy the issues described above. In particular, such an animation allows a user to understand the temporal relationship and the differences between a capture frame and a computer-selected frame in order to allow the user to select a suitable frame for editing, sharing, or other operations.
- Providing an animation between a capture frame and a computer-selected frame may involve receiving a plurality of frames from an image capture device. The plurality of frames might be captured in a burst mode of the image capture device, as part of a micro video associated with an application installed on a computing device, or as part of an automated procedure for generating a high-quality image. The plurality of frames may include a capture frame that is indicative of which frame a user intended to capture. The capture frame can be captured at substantially the same time that an image capture option is selected by a user, or a time at which the image capture option is first selected.
- A computer-selected frame may be determined based on the plurality of frames. For instance, a computing device can receive the frames from the image capture device, send the frames to a server with a request for a computer-selected frame, and receive one or more computer-selected frames from the server. The computer-selected frame can be determined based on image quality (e.g. lighting, clarity, noise level, etc.), on characteristics of the scene (e.g. whether subjects are smiling, whether subjects are looking at the image capture device, whether a perceived action in the scene is complete, etc.), or other factors. In some examples, the server or computing device may determine the computer-selected frame using a machine learning model that is trained based on captured frames and on subsequent frame selection decisions of users. In other examples, the computing device may determine the computer-selected frame without use of a machine learning model. For example, the computing device may determine the computer-selected frame based on predetermined criteria for determining image quality or based on past image selections of a user. Other ways of determining the computer-selected image are possible.
- After the plurality of images are captured, a user may select an option to view the capture frame. For instance, the user may open a virtual photograph gallery, or select a representation of the capture frame in order to edit or share the capture frame using a user interface of the computing device.
- In response to receiving a selection of the option to view the capture frame, the computing device can provide an animation between the capture frame and the computer-selected frame by way of the user interface. In some examples, the computing device can provide the animation automatically. In alternative examples, providing the animation can be user-initiated. The animation can include an interpolation of frames captured between the capture frame and the computer-selected frame. In some instances, the interpolation may be non-linear. For example, the animation may decelerate from the capture frame to the computer-selected frame.
- The computing device may also provide a timeline along with the animation that has a marker that scrolls as the animation plays out. In this fashion, the user can perceive a temporal relationship between the capture frame and the computer-selected frame.
- After providing the animation, the computing device may provide an option to navigate away from the computer-selected frame using the user interface. For example, the computing device may provide a timeline along which the user can scroll to view a different frame. The user may interact with the same marker used to denote a temporal relationship between the capture frame and the computer-selected frame during the animation. Thus, providing the timeline during the animation may also serve to show a user how to interact with the timeline to navigate to other frames. The computing device may also provide multiple computer-selected frames, and allow the user to navigate to each computer-selected frame in turn. In such examples, a machine learning model used to determine the computer-selected frames may be trained based on which computer-selected frame is selected or based on a comparison of a computer-selected frame to a frame ultimately selected by a user.
- With respect to embodiments that include determining a computer-selected frame using a machine learning model, interactions by the computing device with cloud-based servers, or otherwise involve sharing captured images with other computing devices, a user may be provided with controls allowing the user to make an election as to both if and when systems, programs, or features described herein may enable collection of user information (e.g., information about a user's social network, social actions, or activities, profession, a user's preferences, or a user's current location), and if the user is sent content or communications from a server. In addition, certain data may be treated in one or more ways before it is stored or used, so that personally identifiable information is removed. For example, a user's identity may be treated so that no personally identifiable information can be determined for the user, or a user's geographic location may be generalized where location information is obtained (such as to a city, ZIP code, or state level), so that a particular location of a user cannot be determined. Thus, the user may have control over what information is collected about the user, how that information is used, and what information is provided to the user.
-
FIG. 1 depicts a distributedcomputing architecture 100, in accordance with an example embodiment. Distributedcomputing architecture 100 can includeserver devices network 106, withprogrammable devices Network 106 may correspond to a LAN, a wide area network (WAN), a corporate intranet, the public Internet, or any other type of network configured to provide a communications path between networked computing devices.Network 106 may also correspond to a combination of one or more LANs, WANs, corporate intranets, and/or the public Internet. - Although
FIG. 1 only shows five programmable devices, distributed application architectures may serve tens, hundreds, or thousands of programmable devices. Moreover,programmable devices programmable devices network 106. In other embodiments, such as indicated withprogrammable device 104 d, programmable devices can be indirectly connected to network 106 via an associated computing device, such asprogrammable device 104 c. In this example,programmable device 104 c can act as an associated computing device to pass electronic communications betweenprogrammable device 104 d andnetwork 106. In yet other embodiments, such as shown inprogrammable device 104 e, a computing device can be part of and/or inside a vehicle; e.g., a car, a truck, a bus, a boat or ship, an airplane, etc. In still other embodiments not shown inFIG. 1 , a programmable device can be both directly and indirectly connected tonetwork 106. -
Server devices server device 108 and/or 110 can provide content to programmable devices 104 a-104 e. The content can include, but is not limited to, web pages, hypertext, scripts, binary data such as compiled software, images, audio, and/or video. The content can include compressed and/or uncompressed content. The content can be encrypted and/or unencrypted. Other types of content are possible as well. - As another example,
server device 108 and/or 110 can provide programmable devices 104 a-104 e with access to software for database, search, computation, graphical, audio, video, World Wide Web/Internet utilization, and/or other functions. Many other examples of server devices are possible as well. -
FIG. 2 is a functional block diagram of anexample computing device 200, in accordance with an example embodiment. In particular,computing device 200 shown inFIG. 2 can be configured to perform at least one function of distributedcomputing architecture 100,programmable devices network 106, and/orserver devices computing devices method 700. -
Computing device 200 may include user interface module 201, networkcommunications interface module 202, one ormore processors 203,data storage 204, and one ormore sensors 220, all of which may be linked together via a system bus, network, or other connection mechanism 1805. - User interface module 201 can be operable to send data to and/or receive data from external user input/output devices. For example, user interface module 201 can be configured to send and/or receive data to and/or from user input devices such as a touch screen, a computer mouse, a keyboard, a keypad, a touch pad, a track ball, a joystick, a camera, a voice recognition module, and/or other similar devices. User interface module 201 can also be configured to provide output to user display devices, such as one or more cathode ray tubes (CRT), liquid crystal displays, light emitting diodes (LEDs), displays using digital light processing (DLP) technology, printers, light bulbs, and/or other similar devices, either now known or later developed. User interface module 201 can also be configured to generate audible outputs, such as a speaker, speaker jack, audio output port, audio output device, earphones, and/or other similar devices. User interface module 201 can further be configured with one or more haptic devices that can generate haptic outputs, such as vibrations and/or other outputs detectable by touch and/or physical contact with
computing device 200. In some embodiments, user interface module 201 can be used to provide a graphical user interface for utilizingcomputing device 200. - Network
communications interface module 202 can include one or morewireless interfaces 207 and/or one ormore wireline interfaces 208 that are configurable to communicate via a network. Wireless interfaces 207 can include one or more wireless transmitters, receivers, and/or transceivers, such as a Bluetooth™ transceiver, a Zigbee® transceiver, a Wi-Fi™ transceiver, a WiMAX™ transceiver, and/or other similar type of wireless transceiver configurable to communicate via a wireless network. Wireline interfaces 208 can include one or more wireline transmitters, receivers, and/or transceivers, such as an Ethernet transceiver, a Universal Serial Bus (USB) transceiver, or similar transceiver configurable to communicate via a twisted pair wire, a coaxial cable, a fiber-optic link, or a similar physical connection to a wireline network. - In some embodiments, network
communications interface module 202 can be configured to provide reliable, secured, and/or authenticated communications. For each communication described herein, information for ensuring reliable communications (i.e., guaranteed message delivery) can be provided, perhaps as part of a message header and/or footer (e.g., packet/message sequencing information, encapsulation headers and/or footers, size/time information, and transmission verification information such as cyclic redundancy check (CRC) and/or parity check values). Communications can be made secure (e.g., be encoded or encrypted) and/or decrypted/decoded using one or more cryptographic protocols and/or algorithms, such as, but not limited to, Data Encryption Standard (DES), Advanced Encryption Standard (AES), an Rivest-Shamir-Adelman (RSA) algorithm, a Diffie-Hellman algorithm, a secure sockets protocol such as Secure Sockets Layer (SSL) or Transport Layer Security (TLS), and/or Digital Signature Algorithm (DSA). Other cryptographic protocols and/or algorithms can be used as well or in addition to those listed herein to secure (and then decrypt/decode) communications. - One or
more processors 203 can include one or more general purpose processors, and/or one or more special purpose processors (e.g., digital signal processors, graphics processing units, application specific integrated circuits, etc.). One ormore processors 203 can be configured to execute computer-readable program instructions 206 that are contained indata storage 204 and/or other instructions as described herein. -
Data storage 204 can include one or more computer-readable storage media that can be read and/or accessed by at least one of one ormore processors 203. The one or more computer-readable storage media can include volatile and/or non-volatile storage components, such as optical, magnetic, organic or other memory or disc storage, which can be integrated in whole or in part with at least one of one ormore processors 203. In some embodiments,data storage 204 can be implemented using a single physical device (e.g., one optical, magnetic, organic or other memory or disc storage unit), while in other embodiments,data storage 204 can be implemented using two or more physical devices. -
Data storage 204 can include computer-readable program instructions 206 and perhaps additional data. In some embodiments,data storage 204 can additionally include storage required to perform at least part of the herein-described methods, scenarios, and techniques and/or at least part of the functionality of the herein-described devices and networks. - In some embodiments,
computing device 200 can include one ormore sensors 220.Sensors 220 can be configured to measure conditions in an environment ofcomputing device 200 and provide data about that environment. For example, sensors 220 can include one or more of: (i) an identification sensor to identify other objects and/or devices, such as, but not limited to, a Radio Frequency Identification (RFID) reader, proximity sensor, one-dimensional barcode reader, two-dimensional barcode (e.g., Quick Response (QR) code) reader, and a laser tracker, where the identification sensors can be configured to read identifiers, such as RFID tags, barcodes, QR codes, and/or other devices and/or object configured to be read and provide at least identifying information; (ii) sensors to measure locations and/or movements of computing device 200, such as, but not limited to, a tilt sensor, a gyroscope, an accelerometer, a Doppler sensor, a Global Positioning System (GPS) device, a sonar sensor, a radar device, a laser-displacement sensor, and a compass; (iii) an environmental sensor to obtain data indicative of an environment of computing device 200, such as, but not limited to, an infrared sensor, an optical sensor, a light sensor, a camera, a biosensor, a capacitive sensor, a touch sensor, a temperature sensor, a wireless sensor, a radio sensor, a movement sensor, a microphone, a sound sensor, an ultrasound sensor, and/or a smoke sensor; and (iv) a force sensor to measure one or more forces (e.g., inertial forces and/or G-forces) acting about computing device 200, such as, but not limited to one or more sensors that measure: forces in one or more dimensions, torque, ground force, friction, and/or a zero moment point (ZMP) sensor that identifies ZMPs and/or locations of the ZMPs. Many other examples ofsensors 220 are possible as well. - The
sensors 220 can also include one or moreimage capture devices 222. The one or moreimage capture devices 222 may be configured to capture a plurality of images in rapid succession. For instance,computing device 200 may receive a selection of an option to capture an image by way of user interface module 201, and may responsively capture a plurality of images using one or moreimage capture devices 222. The plurality of images may be referred to as a plurality of frames, and may take the form of burst image data, a micro video, or another collection of images. -
FIG. 3 shows acomputing device 300 that is providing a capture frame.Computing device 300 includes auser interface 302.User interface 302 provides animage 304, and anoption 306 to see another frame. -
Computing device 300 may be configured in much the same fashion described above with respect tocomputing device 200, and may operate similarly toprogrammable devices device 300 is depicted as a mobile computing device, the forthcoming description can apply to other types of devices as well. -
Computing device 300 may provideimage 304 responsive to an image capture device capturing a plurality of images. For example,computing device 300 may receive a selection of an option to take a photograph by way of an interaction withuser interface 302.Image 304 may be an image that is captured at substantially the same time that the selection is received. Accordingly,image 304 may be referred as a capture frame of a plurality of frames. - Subsequent to capturing
image 304,computing device 300 may provide, by way ofuser interface 302, a representation ofimage 304, as well as options related toimage 304. In the present example, options to share, edit, or deleteimage 304 are presented, though other options are possible. In addition,user interface 302 provides anoption 306 to “see another moment.” - Interacting with
option 306 may promptcomputing device 300 to determine a computer-selected frame. In other examples, the computer-selected frame might be determined by computingdevice 300 automatically each time a plurality of images are captured in rapid succession by an image capture device associated withcomputing device 300. - Determining a computer-selected frame may be performed by one or more processors executing program instructions stored on
computing device 300. In other examples, determining the computer-selected frame can include sending a request for a computer-selected frame to a server, such asserver 106, along with a plurality of frames. The server can be a cloud-based server that compiles images received fromcomputing device 300 and/or images received from other computing devices. -
Computing device 300, the server, or another device may weigh characteristics of each image in the plurality of images to determine a computer-selected frame. In other examples, the computer-selected frame may be determined using a machine learning model that is trained based on images received fromcomputing device 300 and other computing devices. - In some examples,
computing device 300 may determine the computer-selected frame based on predetermined criteria for measuring quality of a frame. For example,computing device 300 can assign a score to each frame in the plurality of frames based on the criteria, and determine the computer-selected frame by comparing respective scores each frame. In some examples, the criteria can be weighted based on preferences selected by a user. In other examples,computing device 300, a server, or another computing device may track selections of frames to determine the user preferences. Accordingly, in some embodiments determining the computer-selected frame may be performed based on one or more prior user actions. - In some examples,
computing device 300 may determine a plurality of computer-selected frames, and may rank them. For example,computing device 300 may determine a primary computer-selected frame that is determined most likely to be suitable for selection, a secondary computer-selected frame that is next most likely to be suitable for selection, and so on. After receiving selection ofoption 306 to view a computer-selected frame, computing device may present, by way ofuser interface 302, a representation of the capture frame and the one or more computer-selected frames. -
FIG. 4 depicts an animation between acapture frame 404 and a computer-selectedframe 424, in accordance with an example embodiment.Computing device 400 includesuser interface 402.User interface 402 is used to provide an animation betweencapture frame 404 and computer-selectedframe 424. The present example depicts a first time during the animation at whichcapture frame 404 is shown, a second time at which computer-selectedframe 424 is shown, and a representation of an animation provided between the first time and the second time. -
User interface 402 also provides atimeline 406 that can be used to establish a temporal relationship betweencapture frame 404 and computer-selectedframe 424.Timeline 406 may include a plurality of snapshots of the plurality of images that show a thumbnail view of the captured scene at a time that corresponds to the snapshot.Timeline 406 may further include one or more indicators denoting snapshots that correspond to significant frames, such as the capture frame or a computer-selected frame. In the present example,indicator 408 is associated withsnapshot 410, which provides a thumbnail view ofcapture frame 404. Additionally,indicator 412 is associated withsnapshot 414, which provides a thumbnail view of a computer-selected frame. - The snapshots provided in
timeline 406 are can be representative of frames used in the animation. The animation involves displaying frames captured betweencapture frame 404 and computer-selectedframe 424. Such frames can be referred to as interpolation frames. In some examples, the interpolation frames can include each frame captured betweencapture frame 404 and computer-selectedframe 424. In other examples, the interpolation frames can be sampled such that only a portion of captured frames are used in the animation. In still other examples, additional frames may be generated based on the captured frames. - In the present example, four snapshots
separate snapshot 410 andsnapshot 414. Accordingly,FIG. 4 depicts fourinterpolation frames 430 used to animate fromcapture frame 404 to computer-selectedframe 424. Though fourinterpolation frames 430 are depicted inFIG. 4 to correspond to the snapshots intimeline 406, it should be understood that several interpolation frames can correspond to a single representative snapshot. - The animation may help to illustrate differences between
capture frame 404 and computer-selectedframe 424. For instance, where the captured scene is landscape, the animation might show thatcapture frame 404 is blurred and that computer-selectedframe 424 is clearer. Where the scene includes a portrait, the animation may show that subjects of the portrait are not smiling incapture frame 404 and that all subjects are smiling in computer-selectedframe 424. Where the scene includes an action, the animation may show thatcapture frame 404 depicts a less interesting portion of the action and that computer-selectedframe 424 depicts a more interesting portion of the action. Other types of scenes and animations are possible. - In
FIG. 4 ,timeline 406 shows that a computer-selected frame was captured aftercapture frame 404, which in turn indicates that the computer-selected frame was captured after a selection of an option to capture an image.Timeline 406 also depicts frames that are captured prior to captureframe 404. In such examples, an application associated with an image capture device that capturedcapture frame 404 may cause the image capture device to continuously capture images while awaiting a selection to capture an image. In this fashion, the plurality of frames can include frames captured prior to the capture frame and frames captured after the capture frame. In other examples the plurality of frames may only include frames captured after the capture frame. - In some examples, concurrently with the animation, a marker on
timeline 406 is adjusted to indicate a temporal relationship between the capture frame and the computer-selected frame. For example, different snapshots oftimeline 406 can be highlighted to serve as a marker that denotes the passage of time. In other examples, a bar may pass acrosstimeline 406 to serve as the marker. Other types of markers can be used with a timeline to depict a temporal relationship between different frames. - Once
user interface 402 provides the animation, which ends with computer-selectedframe 424, an option may be provided to interact with the computer-selected frame, or to view another frame of the plurality of frames. For instance, to share or edit computer-selectedframe 424, a user may simply touch the portion ofuser interface 402 that depicts computer-selectedframe 424. A user can also interact withtimeline 406 to navigate to another frame. In other examples, more than one computer-selected frame may be provided, anduser interface 402 may provide a selectable option to view additional computer-selected frames. Other options to interact with computer-selectedframe 424 or view another frame are possible as well. - In examples where a machine learning model is used to determine computer-selected frames, a selection of an option to navigate from computer-selected
frame 424 to another frame, or a selection of an option to interact with computer-selectedframe 424, can be used to train the machine learning model. For instance, selecting computer-selectedframe 424 may indicate that the machine learning model computer-selected a suitable frame, while selecting an option to navigate to another frame, and ultimately selecting another frame, may indicate that the machine learning model did not suggest a suitable frame. -
FIG. 5 depicts an animation between a first computer-selectedframe 504 and a second computer-selectedframe 524, in accordance with an example embodiment.Computing device 500 includesuser interface 502.User interface 502 provides first computer-selectedframe 504.User interface 502 further includes atimeline 506.Timeline 506 includesindicators snapshot 510 of a capture frame and asnapshot 514 of first computer-selectedframe 504. The present example depicts a first time during the animation at which first computer-selectedframe 504 is shown, a second time at which computer-selectedframe 524 is shown, and a representation of an animation provided between the first time and the second time. -
FIG. 5 depictsuser interface 502 after an animation from a capture frame to first computer-selectedframe 504 has already occurred.User interface 502 may include an option to perform an action associated with first computer-selectedframe 504. For instance, an option may be presented to share or edit first computer-selectedframe 504. Further, an option to navigate to another frame, such as a second computer-selected frame, may be presented by way ofuser interface 502. - Similar to the animation depicted in
FIG. 4 ,FIG. 5 depicts an animation interpolating between first computer-selectedframe 504 and second computer-selectedframe 524. In particular,interpolation frame 530 correspond to frames captured between first computer-selectedframe 504 and second computer-selectedframe 524. -
FIG. 5 also shows aninteraction 520 that causesuser interface 502 to scroll through the plurality of frames towards second computer-selectedframe 524.Interaction 520, might be, for example, a swipe across a screen ofcomputing device 500. Second computer-selectedframe 524 may be associated with asnap radius 522 that causesuser interface 502 to show second computer-selectedframe 524 once a threshold level of proximity is reached between a current frame shown byuser interface 502 and second computer-selectedframe 524. For example, a user may scroll acrosstimeline 506 untilsnap radius 522 is reached. In response to receiving an indication thatinteraction 520 has reachedsnap radius 522,computing timeline 506 may causeuser interface 502 to movetimeline 506 tosnapshot 518, and provide second computer-selectedframe 524.Computing device 500 may animate between frames as a user scrolls throughtimeline 506. - Though
FIG. 5 shows an animation between first computer-selectedframe 504 and second computer-selectedframe 524 being provided in response tointeraction 520, such an animation may occur automatically. For example, the computer-selected frames may be ranked. In such examples, first computer-selectedframe 504 might be a primary computer-selected frame and second computer-selectedframe 524 might be a secondary computer-selected frame.Computing device 500 may provide an animation that depicts the differences between a capture frame and the primary frame, between the primary frame and the secondary frame, and so on. The ranked computer-selected frames might be determined using a machine learning model that is trained based on images received fromcomputing device 500 and other computing devices. In such examples, the machine learning model can be trained based on which of the computer-selected frames, if any, is selected, and/or based on a comparison of the computer-selected frames. -
FIG. 6 depicts interpolation characteristics of an animation, in accordance with an example embodiment.FIG. 6 includesexemplary graphs Graph 600 depicts frames traversed in the animation versus time of the animation andgraph 602 depicts interpolation distance between animation frames versus time of the animation. - Interpolating between two frames of the plurality of frames, such as a capture frame and a computer-selected frame, can involve displaying each frame captured between the capture frame and the computer-selected frame. In some examples, interpolating between the frames can involve sampling select frames within the plurality, and in some instances may involve generating additional frames for the animation based on characteristics of the captured frames in the plurality.
- In some embodiments, the animation time will remain the same for all animations, regardless of the number of frames that separate a pair of frames that begin and end the animation. Using the scenarios presented in
FIG. 4 andFIG. 5 as an example,graph 600 shows thatcapture frame 404 and computer-selectedframe 424 are separated by fewer frames than those that separate first computer-selectedframe 504 and second computer-selectedframe 524. In other words, there are fewer frames traversed in the animation depicted inFIG. 4 than there are in the animation depicted inFIG. 5 . However,graph 600 depicts their animation time as being substantially identical. - Interpolating between frames may be non-linear. For example, different frames may be displayed for different periods of time such that an animation appears to accelerate or decelerate as the animation approaches an ending frame. In examples where interpolating frames includes sampling frames captured between a capture frame and a computer-selected frame, non-linear interpolation involves changing the number of frames skipped between sampled frames used in the animation may change as the animation approaches the last frame.
- In some examples, this change in time periods for displaying different frames, or change to numbers of skipped frames, may differ between animations. Using the scenarios presented in
FIGS. 4 and 5 as an example,graph 602 shows that the animation betweencapture frame 404 and computer-selectedframe 424 progresses more slowly than the animation between first computer-selectedframe 504 and second computer-selectedframe 524. Because both depicted animations take the same time to conclude, and because the animation between first computer-selectedframe 504 and second computer-selectedframe 524 traverses more captured frames than that betweencapture frame 404 and computer-selectedframe 424, the former shows a greater animation speed. -
Graph 602 also shows that the depicted animations accelerate toward the last frame. In other words, as the animation progresses, the animation approaches the last frame more quickly. This acceleration is more pronounced where more frames separate the first frame and the last frame in an animation. For example, first computer-selectedframe 504 accelerates more quickly towards second computer-selectedframe 524 thancapture frame 404 accelerates towards computer-selectedframe 424 within the respective animations depicted inFIGS. 5 and 4 . - Though
graphs graphs timeline -
FIG. 7 is a flowchart of a method, in accordance with an example embodiment.Method 700 can be a computer-implemented method. For example,method 700 can be executed by a computing device, such ascomputing device -
FIG. 7 shows thatmethod 700 can begin atblock 710. Block 710 can be performed to receive, from an image capture device, a plurality of frames including a capture frame. For example,method 700 may further include receiving, at a capture selection time, by way of a user interface, a selection of an image capture option. The capture frame can be associated with the capture selection time. In such examples, the plurality of frames can include one or more frames captured prior to the capture selection time and one or more frames captured after the capture selection time. -
Method 700 may further includeblock 720. Block 720 can be performed to determine a computer-selected frame of the plurality of frames. For instance, the computer-selected frame may be determined using a machine learning model. The machine learning model can be trained based on images captured from the image capture device, or from a plurality of image capture devices. Determining the computer-selected frame can include, responsive to receiving the plurality of frames, sending, to a server, the plurality of frames and a request to receive one or more computer-selected frames from the server. Determining the computer-selected frame may also include receiving an indication of the computer-selected frame from the server. - In some examples where
block 720 is performed to determine a computer-selected frame using a machine learning model,method 700 may further include receiving, by way of the user interface, a selection of the computer-selected frame, and training the machine learning model based on the received selection. -
Method 700 may further include block 730. Block 730 can be performed to receive, by way of a user interface, a selection of an option to view the capture frame. The option to view the capture frame can include an option to edit the capture frame. -
Method 700 may further include block 740. Block 740 can be performed to, responsive to receiving the selection, provide, by way of the user interface, an animation between the capture frame and the computer-selected frame. The animation may include an interpolation of one or more frames captured between the capture frame and the computer-selected frame. - Providing the animation can further include providing a timeline of the plurality of frames. Concurrently with the interpolation of one or more frames, a marker on the timeline can be adjusted to indicate a temporal relationship between the capture frame and the computer-selected frame.
- The interpolation of the one or more frames can be non-linear. For example, the non-linear interpolation might be performed such that the animation accelerates from the capture frame to the computer-selected frame.
-
Method 700 may additionally include, after providing the animation, provide, by way of the user interface, an option to navigate from the computer-selected frame to select a frame from the plurality of frames. -
Method 700 can further include, responsive to receiving the selection of the option to view the capture frame, providing, by way of the user interface, a still image of the capture frame.Method 700 may also include providing, with the still image of the capture frame, an option to view the computer-selected frame and receiving, by way of the user interface, a selection of the option to view the computer-selected frame. Providing the animation can be performed responsive to receiving the selection of the option to view the computer-selected frame. - In some examples,
method 700 may include determining a plurality of computer-selected frames. In such examples, the computer-selected frame can be a primary computer-selected frame, andmethod 700 can further include further determining a secondary computer-selected frame. In such examples, after providing the animation,method 700 can include providing, by way of the user interface, an option to navigate from the primary computer-selected frame to select the secondary computer-selected frame. The primary computer-selected frame and the secondary computer-selected frame can be determined using a machine learning model. - In examples where
method 700 includes determining a plurality of computer-selected frames using a machine learning model,method 700 may further include receiving, by way of the user interface, a selection of the option to navigate from the primary computer-selected frame to select the secondary computer-selected frame, and training the machine learning model based on the received selection. In other such examples,method 700 can further include receiving, by way of the user interface, a selection of the option to navigate from the primary computer-selected frame to select the secondary computer-selected frame, and receiving, by way of the user interface, a selection of the secondary computer-selected frame.Further method 700 may include, responsive to receiving the selection of the secondary computer-selected frame, training the machine learning model based on a comparison of the secondary computer-selected frame and the primary computer-selected frame. - In examples where the computer-selected frame is determined using a machine learning model,
method 700 may further include, receiving, by way of the user interface, a selection of the option to navigate from the computer-selected frame to select a frame from the plurality of frames, and receiving, by way of the user interface, a selection of a selected frame from the plurality of frames. In addition,method 700 can include, responsive to receiving the selection, providing, by way of the user interface, an option to perform an action related to the selected frame.Further method 700 may include training the machine learning model based on a comparison of the selected frame and the computer-selected frame. - In some examples, a computing device can be provided, where the computing device includes: one or more processors; and one or more computer readable media. The one or more computer readable media can have computer-readable instructions stored thereon that, when executed by the one or more processors, cause the computing device to carry out functions that include
method 700. - In other examples, a computing device can be provided, where the computing device includes means for carrying out
method 700. - In even other examples, an article of manufacture can be provided. The article of manufacture can include one or more computer readable media having computer-readable instructions stored thereon that, when executed by one or more processors of a computing device, cause the computing device to carry out functions that include
method 700. In some of these examples, the one or more computer readable media can include one or more non-transitory computer readable media. - The above detailed description describes various features and functions of the disclosed systems, devices, and methods with reference to the accompanying figures. In the figures, similar symbols typically identify similar components, unless context dictates otherwise. The illustrative embodiments described in the detailed description, figures, and claims are not meant to be limiting. Other embodiments can be utilized, and other changes can be made, without departing from the spirit or scope of the subject matter presented herein. It will be readily understood that the aspects of the present disclosure, as generally described herein, and illustrated in the figures, can be arranged, substituted, combined, separated, and designed in a wide variety of different configurations, all of which are explicitly contemplated herein.
- With respect to any or all of the ladder diagrams, scenarios, and flow charts in the figures and as discussed herein, each block and/or communication may represent a processing of information and/or a transmission of information in accordance with example embodiments. Alternative embodiments are included within the scope of these example embodiments. In these alternative embodiments, for example, functions described as blocks, transmissions, communications, requests, responses, and/or messages may be executed out of order from that shown or discussed, including substantially concurrent or in reverse order, depending on the functionality involved. Further, more or fewer blocks and/or functions may be used with any of the ladder diagrams, scenarios, and flow charts discussed herein, and these ladder diagrams, scenarios, and flow charts may be combined with one another, in part or in whole.
- A block that represents a processing of information may correspond to circuitry that can be configured to perform the specific logical functions of a herein-described method or technique. Alternatively or additionally, a block that represents a processing of information may correspond to a module, a segment, or a portion of program code (including related data). The program code may include one or more instructions executable by a processor for implementing specific logical functions or actions in the method or technique. The program code and/or related data may be stored on any type of computer readable medium such as a storage device including a disk or hard drive or other storage medium.
- The computer readable medium may also include non-transitory computer readable media such as non-transitory computer-readable media that stores data for short periods of time like register memory, processor cache, and random access memory (RAM). The computer readable media may also include non-transitory computer readable media that stores program code and/or data for longer periods of time, such as secondary or persistent long term storage, like read only memory (ROM), optical or magnetic disks, compact-disc read only memory (CD-ROM), for example. The computer readable media may also be any other volatile or non-volatile storage systems. A computer readable medium may be considered a computer readable storage medium, for example, or a tangible storage device.
- Moreover, a block that represents one or more information transmissions may correspond to information transmissions between software and/or hardware modules in the same physical device. However, other information transmissions may be between software modules and/or hardware modules in different physical devices.
- Variations of the above referenced approach will be apparent to the skilled person. For example, while the above description provides particular disclosure of corrections to distortion of faces in an image, the approach may also be applied to other regions or objects of interest. As such, where the adjective “facial” is referred to in the above disclosure (such as in the phrases “facial regions” or “facial transformation”), the skilled person will appreciate that alternative approaches may be adopted in which such an adjective is not required. Similarly, references to “face-related costs”, “face-related transformations” or other “face-related” features may be more generally considered as “object-related” or “region-related” in alternative implementations.
- While various aspects and embodiments have been disclosed herein, other aspects and embodiments will be apparent to those skilled in the art. The various aspects and embodiments disclosed herein are for provided for explanatory purposes and are not intended to be limiting, with the true scope being indicated by the following claims.
Claims (21)
1-20. (canceled)
21. A computer-implemented method comprising:
receiving, by a user interface, a selection of an option to view a capture frame;
responsive to receiving the selection, receiving, from an image capture device, a plurality of frames corresponding to a time of capture of the capture frame; and
providing, by the user interface, an animation comprising an interpolation of the plurality of frames and the capture frame, wherein the animation is indicative of a temporal relationship between the plurality of frames and the capture frame.
22. The method of claim 1, wherein the image capture device is configured to capture the plurality of frames prior to, and after, a user indication to capture the capture frame.
23. The method of claim 1, wherein the image capture device is configured to capture the plurality of frames prior to a user indication to capture the capture frame.
24. The method of claim 1, further comprising:
providing, by the user interface, a computer-selected frame, wherein the computer-selected frame is selected from the plurality of frames; and
providing, by the user interface, an option to navigate to the computer-selected frame and the capture frame.
25. The method of claim 4, wherein the computer-selected frame is determined using a machine learning model.
26. The method of claim 4, wherein the computer-selected frame is associated with a snap radius, and further comprising:
determining whether a proximity between a currently displayed frame and the computer-selected frame is within a proximity threshold; and
providing, by the user interface, the computer-selected frame upon a determination that the proximity between the currently displayed frame and the computer-selected frame is within the proximity threshold.
27. The method of claim 4, further comprising:
capturing, by the image capture device, the plurality of frames; and
responsive to the capturing, automatically determining the computer-selected frame.
28. The method of claim 7, further comprising:
associating each frame of the plurality of frames with a respective image quality score, and
wherein the automatically determining of the computer-selected frame is based on a comparison of the respective image quality scores.
29. The method of claim 8, wherein the respective image quality scores are weighted based on one or more user-selected preferences.
30. The method of claim 1, wherein the providing of the animation further comprises:
sampling the plurality of frames to select a subplurality of the plurality of frames for the interpolation.
31. The method of claim 1, wherein the providing of the animation comprises providing a timeline associated with the plurality of frames, and further comprises:
adjusting a marker on the timeline concurrently with the interpolation of the plurality of frames, wherein the adjusting indicates the temporal relationship between the frames and the capture frame.
32. The method of claim 1, wherein the providing of the animation is responsive to a user indication to provide the animation.
33. The method of claim 1, wherein the interpolation of the one or more frames is non-linear.
34. The method of claim 1, further comprising:
receiving, by the user interface, a user selection of a frame from the plurality of frames.
35. The method of claim 1, further comprising, after providing the animation, providing, by the user interface, an option to navigate the plurality of frames.
36. The method of claim 1, further comprising:
providing, by the user interface, one or more options to share, edit, or delete a frame of the plurality of frames.
37. The method of claim 1, wherein the plurality of frames are captured in a burst mode of the image capture device.
38. The method of claim 1, wherein the plurality of frames are captured as part of a micro video associated with an application installed on a computing device.
39. A computing device, comprising:
one or more processors; and
data storage, wherein the data storage has stored thereon computer-executable instructions that, when executed by the one or more processors, cause the computing device to carry out functions comprising:
receiving, by a user interface, a selection of an option to view a capture frame;
responsive to receiving the selection, receiving, from an image capture device, a plurality of frames corresponding to a time of capture of the capture frame; and
providing, by the user interface, an animation comprising an interpolation of the plurality of frames and the capture frame, wherein the animation is indicative of a temporal relationship between the plurality of frames and the capture frame.
40. A non-transitory computer readable medium having program instructions stored thereon that are executable by a processor to perform functions comprising:
receiving, by a user interface, a selection of an option to view a capture frame;
responsive to receiving the selection, receiving, from an image capture device, a plurality of frames corresponding to a time of capture of the capture frame; and
providing, by the user interface, an animation comprising an interpolation of the plurality of frames and the capture frame, wherein the animation is indicative of a temporal relationship between the plurality of frames and the capture frame.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US18/367,533 US20240007738A1 (en) | 2018-09-24 | 2023-09-13 | Method and Device for Image Frame Selection |
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2018/052501 WO2020068044A1 (en) | 2018-09-24 | 2018-09-24 | Method and device for image frame selection |
US202117272839A | 2021-03-02 | 2021-03-02 | |
US18/367,533 US20240007738A1 (en) | 2018-09-24 | 2023-09-13 | Method and Device for Image Frame Selection |
Related Parent Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
PCT/US2018/052501 Continuation WO2020068044A1 (en) | 2018-09-24 | 2018-09-24 | Method and device for image frame selection |
US17/272,839 Continuation US11785334B2 (en) | 2018-09-24 | 2018-09-24 | Method and device for image frame selection |
Publications (1)
Publication Number | Publication Date |
---|---|
US20240007738A1 true US20240007738A1 (en) | 2024-01-04 |
Family
ID=63915355
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US17/272,839 Active 2039-01-07 US11785334B2 (en) | 2018-09-24 | 2018-09-24 | Method and device for image frame selection |
US18/367,533 Pending US20240007738A1 (en) | 2018-09-24 | 2023-09-13 | Method and Device for Image Frame Selection |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US17/272,839 Active 2039-01-07 US11785334B2 (en) | 2018-09-24 | 2018-09-24 | Method and device for image frame selection |
Country Status (2)
Country | Link |
---|---|
US (2) | US11785334B2 (en) |
WO (1) | WO2020068044A1 (en) |
Family Cites Families (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8830341B2 (en) * | 2008-05-22 | 2014-09-09 | Nvidia Corporation | Selection of an optimum image in burst mode in a digital camera |
US8413054B2 (en) * | 2009-04-13 | 2013-04-02 | Cisco Technology, Inc. | Graphical user interface for still image capture from video footage |
KR102362997B1 (en) * | 2014-02-12 | 2022-02-16 | 소니그룹주식회사 | A method for presentation of images |
US10334158B2 (en) * | 2014-11-03 | 2019-06-25 | Robert John Gove | Autonomous media capturing |
KR20170013083A (en) * | 2015-07-27 | 2017-02-06 | 엘지전자 주식회사 | Mobile terminal and method for controlling the same |
KR102349428B1 (en) * | 2015-08-12 | 2022-01-10 | 삼성전자주식회사 | Method for processing image and electronic device supporting the same |
JP6652039B2 (en) * | 2016-12-05 | 2020-02-19 | カシオ計算機株式会社 | Imaging device, imaging method, and program |
KR20180095331A (en) * | 2017-02-17 | 2018-08-27 | 엘지전자 주식회사 | Mobile terminal and method for controlling the same |
US11706507B2 (en) * | 2020-12-30 | 2023-07-18 | Waymo Llc | Systems, apparatus, and methods for generating enhanced images |
-
2018
- 2018-09-24 US US17/272,839 patent/US11785334B2/en active Active
- 2018-09-24 WO PCT/US2018/052501 patent/WO2020068044A1/en active Application Filing
-
2023
- 2023-09-13 US US18/367,533 patent/US20240007738A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
WO2020068044A1 (en) | 2020-04-02 |
US20210289132A1 (en) | 2021-09-16 |
US11785334B2 (en) | 2023-10-10 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US10803664B2 (en) | Redundant tracking system | |
KR102606601B1 (en) | Surface aware lens | |
KR20210103525A (en) | virtual surface modification | |
CN115956255B (en) | 3D reconstruction using wide angle imaging device | |
US11120601B2 (en) | Animated expressive icon | |
CN112740279A (en) | Collaborative achievement interface | |
US11687150B2 (en) | Occlusion detection system | |
CN112639876A (en) | Moving image depth prediction | |
US20210192198A1 (en) | Image landmark detection | |
CN114080824A (en) | Real-time augmented reality dressing | |
CN115335820A (en) | Location mapping for large-scale augmented reality | |
CN113966525A (en) | Augmented reality system based on position | |
TW201618038A (en) | Method and device for providing image | |
US20240007738A1 (en) | Method and Device for Image Frame Selection | |
US20230289560A1 (en) | Machine learning techniques to predict content actions | |
US11983307B2 (en) | Occlusion detection system | |
CN117597690A (en) | Hybrid search system for customizable media |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:JOHNSTON, CHORONG;OBERBECK, JOHN;SANDRIKOVA, MARIIA;SIGNING DATES FROM 20181128 TO 20181129;REEL/FRAME:065177/0780 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |