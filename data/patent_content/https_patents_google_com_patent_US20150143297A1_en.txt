US20150143297A1 - Input detection for a head mounted device - Google Patents
Input detection for a head mounted device Download PDFInfo
- Publication number
- US20150143297A1 US20150143297A1 US13/302,345 US201113302345A US2015143297A1 US 20150143297 A1 US20150143297 A1 US 20150143297A1 US 201113302345 A US201113302345 A US 201113302345A US 2015143297 A1 US2015143297 A1 US 2015143297A1
- Authority
- US
- United States
- Prior art keywords
- movement
- user
- hmd
- user interface
- head
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Abandoned
Links
- 238000001514 detection method Methods 0.000 title 1
- 230000033001 locomotion Effects 0.000 claims abstract description 174
- 238000000034 method Methods 0.000 claims abstract description 21
- 238000013500 data storage Methods 0.000 claims abstract description 5
- 230000001133 acceleration Effects 0.000 claims description 16
- 230000006870 function Effects 0.000 claims description 11
- 230000004886 head movement Effects 0.000 abstract description 22
- 210000003128 head Anatomy 0.000 description 28
- 230000004044 response Effects 0.000 description 11
- 230000002093 peripheral effect Effects 0.000 description 9
- 230000009471 action Effects 0.000 description 7
- 238000004891 communication Methods 0.000 description 7
- 238000007620 mathematical function Methods 0.000 description 7
- 230000001351 cycling effect Effects 0.000 description 6
- 238000005516 engineering process Methods 0.000 description 6
- 230000003287 optical effect Effects 0.000 description 6
- 238000010586 diagram Methods 0.000 description 5
- 239000000463 material Substances 0.000 description 4
- 230000008859 change Effects 0.000 description 3
- 239000011521 glass Substances 0.000 description 3
- 230000003993 interaction Effects 0.000 description 3
- 238000004091 panning Methods 0.000 description 3
- 230000000007 visual effect Effects 0.000 description 3
- 230000003190 augmentative effect Effects 0.000 description 2
- 239000011248 coating agent Substances 0.000 description 2
- 238000000576 coating method Methods 0.000 description 2
- 210000005069 ears Anatomy 0.000 description 2
- 239000011159 matrix material Substances 0.000 description 2
- 239000007787 solid Substances 0.000 description 2
- 230000003068 static effect Effects 0.000 description 2
- 230000001413 cellular effect Effects 0.000 description 1
- 230000026058 directional locomotion Effects 0.000 description 1
- 230000005057 finger movement Effects 0.000 description 1
- 230000009191 jumping Effects 0.000 description 1
- 238000012886 linear function Methods 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 239000002184 metal Substances 0.000 description 1
- 230000008569 process Effects 0.000 description 1
- 238000012887 quadratic function Methods 0.000 description 1
- 210000001525 retina Anatomy 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 238000010897 surface acoustic wave method Methods 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0484—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range
- G06F3/0485—Scrolling or panning
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/017—Head mounted
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F1/00—Details not covered by groups G06F3/00 - G06F13/00 and G06F21/00
- G06F1/16—Constructional details or arrangements
- G06F1/1613—Constructional details or arrangements for portable computers
- G06F1/163—Wearable computers, e.g. on a belt
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/011—Arrangements for interaction with the human body, e.g. for user immersion in virtual reality
- G06F3/012—Head tracking input arrangements
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0481—Interaction techniques based on graphical user interfaces [GUI] based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance
- G06F3/0482—Interaction with lists of selectable items, e.g. menus
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/0101—Head-up displays characterised by optical features
- G02B2027/014—Head-up displays characterised by optical features comprising information/image processing systems
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/017—Head mounted
- G02B2027/0178—Eyeglass type
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/0179—Display position adjusting means not related to the information to be displayed
- G02B2027/0187—Display position adjusting means not related to the information to be displayed slaved to motion of at least a part of the body of the user, e.g. head, eye
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2203/00—Indexing scheme relating to G06F3/00 - G06F3/048
- G06F2203/033—Indexing scheme relating to G06F3/033
- G06F2203/0339—Touch strips, e.g. orthogonal touch strips to control cursor movement or scrolling; single touch strip to adjust parameter or to implement a row of soft keys
Definitions
- heads-up displays are typically positioned near the user's eyes to allow the user to view displayed images or information within the user's field of view.
- a computer processing system may be used to generate the images on the display.
- heads-up displays have a variety of applications, such as aviation information systems, vehicle navigation systems, and video games.
- a head-mounted display can be incorporated into a pair of glasses that the user can wear.
- the display may include a user interface.
- the user interface may have various components. Some components may be graphics and other components may be words. One component of the user interface may be a list of items. Additionally, there may be various ways the user could interact with the user interface of the heads up display.
- the wearable heads-up display may include a processor, a display element configured to receive display information from the processor and to display the display information.
- a user of the heads-up display may interact with items in the display with movements of his or her head. In this manner, a user may be able to interact with the heads-up display without any touching of the hardware. For example, a user may interact with a heads-up display with a head movement.
- the user interface may display a list of items, such as a list of program, a list of emails, and a list of input characters.
- a user may turn his or her head in a direction, left, right, up, or down.
- the user interface shown on the display may update responsively.
- the head movement may be mapped to a scrolling or other movement action of an element within the user interface.
- the element may scroll in response to the head movement. For example, a head turn to the left may cause the items in the list on the user interface to scroll to the right.
- the speed of the scrolling may be a mathematical function based on the speed of the head movement.
- the speed of the head movement is mapped via a non-linear mathematical function to the speed of the scrolling.
- other parameters of the head movement may be used to calculate the scrolling speed.
- the parameter may be the acceleration of the heads-up display, the position of the heads up display, or any other parameter.
- FIG. 1A illustrates an example system for receiving, transmitting, and displaying data.
- FIG. 1B illustrates an alternate view of the system illustrated in FIG. 1A .
- FIG. 2A illustrates an example system for receiving, transmitting, and displaying data.
- FIG. 2B illustrates an example system for receiving, transmitting, and displaying data.
- FIG. 3 shows a simplified block diagram of an example computer network infrastructure.
- FIG. 4 shows a simplified block diagram depicting example components of an example computing system.
- FIG. 5A shows aspects of an example user-interface.
- FIG. 5B shows aspects of an example user-interface after receiving movement data corresponding to an upward movement.
- FIG. 5C shows aspects of an example user-interface after selection of a selected content object.
- FIG. 5D shows aspects of an example user-interface after receiving input data corresponding to a user input.
- FIG. 6A shows an example head movement for interacting with the user interface.
- FIG. 6B shows an example scrolling user interface element.
- FIG. 6C shows an example scrolling user interface element.
- FIG. 7A shows an example text entry system.
- FIG. 7B shows an example scrolling selection in a text entry system.
- FIG. 8 shows an example method for interacting with the user interface.
- the wearable computing device include a heads-up display, which could be configured as a pair of glasses.
- the user interface may appear within the field of view of a person wearing the heads up display.
- the person may move his or her head. For example, in order to move through a list of items, a person may turn his or her head to the left or right.
- the user interface may scroll through a list of items based on the head movement of the person wearing the wearable computing device.
- FIG. 1A illustrates an example system 100 for receiving, transmitting, and displaying data.
- the system 100 is shown in the form of a wearable computing device. While FIG. 1A illustrates a head-mounted device 102 as an example of a wearable computing device, other types of wearable computing devices could additionally or alternatively be used.
- the head-mounted device 102 has frame elements including lens-frames 104 , 106 and a center frame support 108 , lens elements 110 , 112 , and extending side-arms 114 , 116 .
- the center frame support 108 and the extending side-arms 114 , 116 are configured to secure the head-mounted device 102 to a user's face via a user's nose and ears, respectively.
- Each of the frame elements 104 , 106 , and 108 and the extending side-arms 114 , 116 may be formed of a solid structure of plastic and/or metal, or may be formed of a hollow structure of similar material so as to allow wiring and component interconnects to be internally routed through the head-mounted device 102 . Other materials may be possible as well.
- each of the lens elements 110 , 112 may be formed of any material that can suitably display a projected image or graphic.
- Each of the lens elements 110 , 112 may also be sufficiently transparent to allow a user to see through the lens element. Combining these two features of the lens elements may facilitate an augmented reality or heads-up display where the projected image or graphic is superimposed over a real-world view as perceived by the user through the lens elements 110 , 112 .
- the extending side-arms 114 , 116 may each be projections that extend away from the lens-frames 104 , 106 , respectively, and may be positioned behind a user's ears to secure the head-mounted device 102 to the user.
- the extending side-arms 114 , 116 may further secure the head-mounted device 102 to the user by extending around a rear portion of the user's head.
- the system 100 may connect to or be affixed within a head-mounted helmet structure. Other possibilities exist as well.
- the system 100 may also include an on-board computing system 118 , a video camera 120 , a sensor 122 , and a finger-operable touch pad 124 .
- the on-board computing system 118 is shown to be positioned on the extending side-arm 114 of the head-mounted device 102 ; however, the on-board computing system 118 may be provided on other parts of the head-mounted device 102 or may be positioned remote from the head-mounted device 102 (e.g., the on-board computing system 118 could be connected by wires or wirelessly connected to the head-mounted device 102 ).
- the on-board computing system 118 may include a processor and memory, for example.
- the on-board computing system 118 may be configured to receive and analyze data from the video camera 120 , the sensor 122 , and the finger-operable touch pad 124 (and possibly from other sensory devices, user-interfaces, or both) and generate images for output by the lens elements 110 and 112 .
- the on-board computing system 118 may additionally include a speaker or a microphone for user input (not shown). An example computing system is further described below in connection with FIG. 4 .
- the video camera 120 is shown positioned on the extending side-arm 114 of the head-mounted device 102 ; however, the video camera 120 may be provided on other parts of the head-mounted device 102 .
- the video camera 120 may be configured to capture images at various resolutions or at different frame rates. Video cameras with a small form-factor, such as those used in cell phones or webcams, for example, may be incorporated into an example embodiment of the system 100 .
- FIG. 1A illustrates one video camera 120
- more video cameras may be used, and each may be configured to capture the same view, or to capture different views.
- the video camera 120 may be forward facing to capture at least a portion of the real-world view perceived by the user. This forward facing image captured by the video camera 120 may then be used to generate an augmented reality where computer generated images appear to interact with the real-world view perceived by the user.
- the sensor 122 is shown on the extending side-arm 116 of the head-mounted device 102 ; however, the sensor 122 may be positioned on other parts of the head-mounted device 102 .
- the sensor 122 may include one or more of a gyroscope or an accelerometer, for example. Other sensing devices may be included within, or in addition to, the sensor 122 or other sensing functions may be performed by the sensor 122 .
- the finger-operable touch pad 124 is shown on the extending side-arm 114 of the head-mounted device 102 . However, the finger-operable touch pad 124 may be positioned on other parts of the head-mounted device 102 . Also, more than one finger-operable touch pad may be present on the head-mounted device 102 .
- the finger-operable touch pad 124 may be used by a user to input commands.
- the finger-operable touch pad 124 may sense at least one of a position and a movement of a finger via capacitive sensing, resistance sensing, or a surface acoustic wave process, among other possibilities.
- the finger-operable touch pad 124 may be capable of sensing finger movement in a direction parallel or planar to the pad surface, in a direction normal to the pad surface, or both, and may also be capable of sensing a level of pressure applied to the pad surface.
- the finger-operable touch pad 124 may be formed of one or more translucent or transparent insulating layers and one or more translucent or transparent conducting layers. Edges of the finger-operable touch pad 124 may be formed to have a raised, indented, or roughened surface, so as to provide tactile feedback to a user when the user's finger reaches the edge, or other area, of the finger-operable touch pad 124 . If more than one finger-operable touch pad is present, each finger-operable touch pad may be operated independently, and may provide a different function.
- FIG. 1B illustrates an alternate view of the system 100 illustrated in FIG. 1A .
- the lens elements 110 , 112 may act as display elements.
- the head-mounted device 102 may include a first projector 128 coupled to an inside surface of the extending side-arm 116 and configured to project a display 130 onto an inside surface of the lens element 112 .
- a second projector 132 may be coupled to an inside surface of the extending side-arm 114 and configured to project a display 134 onto an inside surface of the lens element 110 .
- the lens elements 110 , 112 may act as a combiner in a light projection system and may include a coating that reflects the light projected onto them from the projectors 128 , 132 .
- a reflective coating may be omitted (e.g., when the projectors 128 , 132 are scanning laser devices).
- the lens elements 110 , 112 themselves may include: a transparent or semi-transparent matrix display, such as an electroluminescent display or a liquid crystal display, one or more waveguides for delivering an image to the user's eyes, or other optical elements capable of delivering an in focus near-to-eye image to the user.
- a corresponding display driver may be disposed within the frame elements 104 , 106 for driving such a matrix display.
- a laser or light emitting diode (LED) source and scanning system could be used to draw a raster display directly onto the retina of one or more of the user's eyes. Other possibilities exist as well.
- FIG. 2A illustrates an example system 200 for receiving, transmitting, and displaying data.
- the system 200 is shown in the form of a wearable computing device 202 .
- the wearable computing device 202 may include frame elements and side-arms such as those described with respect to FIGS. 1A and 1B .
- the wearable computing device 202 may additionally include an on-board computing system 204 and a video camera 206 , such as those described with respect to FIGS. 1A and 1B .
- the video camera 206 is shown mounted on a frame of the wearable computing device 202 ; however, the video camera 206 may be mounted at other positions as well.
- the wearable computing device 202 may include a single display 208 which may be coupled to the device.
- the display 208 may be formed on one of the lens elements of the wearable computing device 202 , such as a lens element described with respect to FIGS. 1A and 1B , and may be configured to overlay computer-generated graphics in the user's view of the physical world.
- the display 208 is shown to be provided in a center of a lens of the wearable computing device 202 , however, the display 208 may be provided in other positions.
- the display 208 is controllable via the computing system 204 that is coupled to the display 208 via an optical waveguide 210 .
- FIG. 2B illustrates an example system 220 for receiving, transmitting, and displaying data.
- the system 220 is shown in the form of a wearable computing device 222 .
- the wearable computing device 222 may include side-arms 223 , a center frame support 224 , and a bridge portion with nosepiece 225 .
- the center frame support 224 connects the side-arms 223 .
- the wearable computing device 222 does not include lens-frames containing lens elements.
- the wearable computing device 222 may additionally include an on-board computing system 226 and a video camera 228 , such as those described with respect to FIGS. 1A and 1B .
- the wearable computing device 222 may include a single lens element 230 that may be coupled to one of the side-arms 223 or the center frame support 224 .
- the lens element 230 may include a display such as the display described with reference to FIGS. 1A and 1B , and may be configured to overlay computer-generated graphics upon the user's view of the physical world.
- the single lens element 230 may be coupled to a side of the extending side-arm 223 .
- the single lens element 230 may be positioned in front of or proximate to a user's eye when the wearable computing device 222 is worn by a user.
- the single lens element 230 may be positioned below the center frame support 224 , as shown in FIG. 2B .
- FIG. 3 shows a simplified block diagram of an example computer network infrastructure.
- a device 310 communicates using a communication link 320 (e.g., a wired or wireless connection) to a remote device 330 .
- the device 310 may be any type of device that can receive data and display information corresponding to or associated with the data.
- the device 310 may be a heads-up display system, such as the head-mounted device 102 , 200 , or 220 described with reference to FIGS. 1A-2B .
- the device 310 may include a display system 312 comprising a processor 314 and a display 316 .
- the display 316 may be, for example, an optical see-through display, an optical see-around display, or a video see-through display.
- the processor 314 may receive data from the remote device 330 , and configure the data for display on the display 316 .
- the processor 314 may be any type of processor, such as a micro-processor or a digital signal processor, for example.
- the device 310 may further include on-board data storage, such as memory 318 coupled to the processor 314 .
- the memory 318 may store software that can be accessed and executed by the processor 314 , for example.
- the remote device 330 may be any type of computing device or transmitter including a laptop computer, a mobile telephone, or tablet computing device, etc., that is configured to transmit data to the device 310 .
- the remote device 330 and the device 310 may contain hardware to enable the communication link 320 , such as processors, transmitters, receivers, antennas, etc.
- the communication link 320 is illustrated as a wireless connection; however, wired connections may also be used.
- the communication link 320 may be a wired serial bus such as a universal serial bus or a parallel bus, among other connections.
- the communication link 320 may also be a wireless connection using, e.g., Bluetooth® radio technology, communication protocols described in IEEE 802.11 (including any IEEE 802.11 revisions), Cellular technology (such as GSM, CDMA, UMTS, EV-DO, WiMAX, or LTE), or Zigbee® technology, among other possibilities. Either of such a wired and/or wireless connection may be a proprietary connection as well.
- the remote device 330 may be accessible via the Internet and may include a computing cluster associated with a particular web service (e.g., social-networking, photo sharing, address book, etc.).
- an example wearable computing device may include, or may otherwise be communicatively coupled to, a computing system, such as computing system 118 or computing system 204 .
- FIG. 4 shows a simplified block diagram depicting example components of an example computing system 400 .
- One or both of the device 310 and the remote device 330 may take the form of computing system 400 .
- Computing system 400 may include at least one processor 402 and system memory 404 .
- computing system 400 may include a system bus 406 that communicatively connects processor 402 and system memory 404 , as well as other components of computing system 400 .
- processor 402 can be any type of processor including, but not limited to, a microprocessor ( ⁇ P), a microcontroller ( ⁇ C), a digital signal processor (DSP), or any combination thereof.
- system memory 404 can be of any type of memory now known or later developed including but not limited to volatile memory (such as RAM), non-volatile memory (such as ROM, flash memory, etc.) or any combination thereof.
- An example computing system 400 may include various other components as well.
- computing system 400 includes an A/V processing unit 408 for controlling graphical display 410 and speaker 412 (via A/V port 414 ), one or more communication interfaces 416 for connecting to other computing devices 418 , and a power supply 420 .
- Graphical display 410 may be arranged to provide a visual depiction of various input regions provided by user-interface module 422 .
- user-interface module 422 may be configured to provide a user-interface, such as the example user-interface described below in connection with FIGS. 5A-D , and graphical display 410 may be configured to provide a visual depiction of the user-interface.
- User-interface module 422 may be further configured to receive data from and transmit data to (or be otherwise compatible with) one or more user-interface devices 428 .
- computing system 400 may also include one or more data storage devices 424 , which can be removable storage devices, non-removable storage devices, or a combination thereof.
- removable storage devices and non-removable storage devices include magnetic disk devices such as flexible disk drives and hard-disk drives (HDD), optical disk drives such as compact disk (CD) drives or digital versatile disk (DVD) drives, solid state drives (SSD), and/or any other storage device now known or later developed.
- Computer storage media can include volatile and nonvolatile, removable and non-removable media implemented in any method or technology for storage of information, such as computer readable instructions, data structures, program modules, or other data.
- computer storage media may take the form of RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium now known or later developed that can be used to store the desired information and which can be accessed by computing system 400 .
- computing system 400 may include program instructions 426 that are stored in system memory 404 (and/or possibly in another data-storage medium) and executable by processor 402 to facilitate the various functions described herein including, but not limited to, those functions described with respect to FIG. 8 .
- program instructions 426 are stored in system memory 404 (and/or possibly in another data-storage medium) and executable by processor 402 to facilitate the various functions described herein including, but not limited to, those functions described with respect to FIG. 8 .
- system memory 404 and/or possibly in another data-storage medium
- FIGS. 5A-D show aspects of an example user-interface 500 .
- the user-interface 500 may be displayed by, for example, a wearable computing device as described above for FIGS. 1A-2B .
- FIG. 5A An example state of the user-interface 500 is shown in FIG. 5A .
- the example state shown in FIG. 5A may correspond to a first position of the wearable computing device. That is, the user-interface 500 may be displayed as shown in FIG. 5A when the wearable computing device is in the first position.
- the first position of the wearable computing device may correspond to a position of the wearable computing device when a wearer of the wearable computing device is looking in a direction that is generally parallel to the ground (e.g., a position that does not correspond to the wearer looking up or looking down). Other examples are possible as well.
- the user-interface 500 includes a view region 502 .
- An example boundary of the view region 502 is shown by a dotted frame. While the view region 502 is shown to have a landscape shape (in which the view region 502 is wider than it is tall), in other embodiments the view region 502 may have a portrait or square shape, or may have a non-rectangular shape, such as a circular or elliptical shape. The view region 502 may have other shapes as well.
- the view region 502 may be, for example, the viewable area between (or encompassing) the upper, lower, left, and right boundaries of a display on the wearable computing device. As shown, when the wearable computing device is in the first position, the view region 502 is substantially empty (e.g., completely empty) of user-interface elements, such that the user's view of their real-world environment is generally uncluttered, and objects in the user's environment are not obscured.
- the view region 502 may correspond to a field of view of a wearer of the wearable computing device, and an area outside the view region 502 may correspond to an area outside the field of view of the wearer.
- the view region 502 may correspond to a non-peripheral portion of a field of view of a wearer of the wearable computing device, and an area outside the view region 502 may correspond to a peripheral portion of the field of view of the wearer.
- the user-interface 500 may be larger than or substantially the same as a field of view of a wearer of the wearable computing device, and the field of view of the wearer may be larger than or substantially the same size as the view region 502 .
- the view region 502 may take other forms as well.
- the portions of the user-interface 500 outside of the view region 502 may be outside of or in a peripheral portion of a field of view of a wearer of the wearable computing device.
- a menu 504 may be outside of or in a peripheral portion of the field of view of the user in the user-interface 500 . While the menu 504 is shown to be not visible in the view region 502 , in some embodiments the menu 504 may be partially visible in the view region 502 .
- the wearable computing device may be configured to receive movement data corresponding to, for example, an upward movement of the wearable computing device to a position above the first position.
- the wearable computing device may, in response to receiving the movement data corresponding to the upward movement, cause one or both of the view region 502 and the menu 504 to move such that the menu 504 becomes more visible in the view region 502 .
- the wearable computing device may cause the view region 502 to move upward and may cause the menu 504 to move downward.
- the view region 502 and the menu 504 may move the same amount, or may move different amounts.
- the menu 504 may move further than the view region 502 .
- the wearable computing device may cause only the menu 504 to move.
- Other examples are possible as well, and are discussed in section 4 below.
- the upward movement may encompass any movement having any combination of moving, tilting, rotating, shifting, sliding, or other movement that results in a generally upward movement. Further, in some embodiments “upward” may refer to an upward movement in the reference frame of a wearer of the wearable computing device. Other reference frames are possible as well. In embodiments where the wearable computing device is a head-mounted device, the upward movement of the wearable computing device may also be an upward movement of a wearer's head such as, for example, the user looking upward.
- the movement data corresponding to the upward movement may take several forms.
- the movement data may be (or may be derived from) data received from one or more movement sensors, accelerometers, and/or gyroscopes configured to detect the upward movement, such as the sensor 122 described above in connection with FIG. 1A .
- the movement data may comprise a binary indication corresponding to the upward movement.
- the movement data may comprise an indication corresponding to the upward movement as well as an extent of the upward movement.
- the movement data may take other forms as well.
- FIG. 5B shows aspects of an example user-interface after receiving movement data corresponding to an upward movement.
- the user-interface 500 includes the view region 502 and the menu 504 .
- the wearable computing device may move one or both of the view region 502 and the menu 504 such that the menu 504 becomes more visible in the view region 502 .
- the menu 504 is fully visible in the view region 502 . In other embodiments, however, only a portion of the menu 504 may be visible in the view region 502 . In some embodiments, the extent to which the menu 504 is visible in the view region 502 may be based at least in part on an extent of the upward movement.
- the view region 502 may be moved in response to receiving data corresponding to an upward movement.
- the view region 502 may be moved in an upward scrolling or panning motion.
- the view region 502 may appear to a wearer of the wearable computing device as if mapped onto the inside of a static sphere centered at the wearable computing device, and movement of the view region 502 may map onto movement of the real-world environment relative to the wearable computing device.
- a speed, acceleration, and/or magnitude of the upward scrolling may be based at least in part on a speed, acceleration, and/or magnitude of the upward movement.
- the view region 502 may be moved by, for example, jumping between fields of view.
- the view region 502 may be moved only when the upward movement exceeds a threshold speed, acceleration, and/or magnitude. In response to receiving data corresponding to an upward movement that exceeds such a threshold or thresholds, the view region 502 may pan, scroll, slide, or jump to a new field of view. The view region 502 may be moved in other manners as well.
- the wearable computing device could be configured to receive data corresponding to other directional movement (e.g., downward, leftward, rightward, etc.) as well, and that the view region 502 may be moved in response to receiving such data in a manner similar to that described above in connection with upward movement.
- other directional movement e.g., downward, leftward, rightward, etc.
- the menu 504 includes a number of content objects 506 .
- the content objects 506 may be arranged in a ring (or partial ring) around and above the head of a wearer of the wearable computing device.
- the content objects 506 may be arranged in a dome-shape above the wearer's head. The ring or dome may be centered above the wearable computing device and/or the wearer's head.
- the content objects 506 may be arranged in other ways as well.
- the number of content objects 506 in the menu 504 may be fixed or may be variable. In embodiments where the number is variable, the content objects 506 may vary in size according to the number of content objects 506 in the menu 504 . In embodiments where the content objects 506 extend circularly around a wearer's head, like a ring (or partial ring), only some of the content objects 506 may be visible at a particular moment. In order to view other content objects 504 , a wearer of the wearable computing device may interact with the wearable computing device to, for example, rotate the content objects 506 along a path (e.g., clockwise or counterclockwise) around the wearer's head.
- a path e.g., clockwise or counterclockwise
- the wearable computing device may be configured to receive data indicating such an interaction through, for example, a touch pad, such as finger-operable touch pad 124 .
- the wearable computing device may be configured to receive such data through other input devices as well.
- the content objects 506 may take several forms.
- the content objects 506 may include one or more of people, contacts, groups of people and/or contacts, calendar items, lists, notifications, alarms, reminders, status updates, incoming messages, recorded media, audio recordings, video recordings, photographs, digital collages, previously-saved states, webpages, and applications, as well as tools, such as a still camera, a video camera, and an audio recorder.
- Content objects 506 may take other forms as well.
- the tools may be located in a particular region of the menu 504 , such as the center. In some embodiments, the tools may remain in the center of the menu 504 , even if the other content objects 506 rotate, as described above. Tool content objects may be located in other regions of the menu 504 as well.
- the particular content objects 506 that are included in menu 504 may be fixed or variable.
- the content objects 506 may be preselected by a wearer of the wearable computing device.
- the content objects 506 for each content region may be automatically assembled by the wearable computing device from one or more physical or digital contexts including, for example, people, places, and/or objects surrounding the wearable computing device, address books, calendars, social-networking web services or applications, photo sharing web services or applications, search histories, and/or other contexts.
- some content objects 506 may be fixed, while the content objects 506 may be variable.
- the content objects 506 may be selected in other manners as well.
- an order or configuration in which the content objects 506 are displayed may be fixed or variable.
- the content objects 506 may be pre-ordered by a wearer of the wearable computing device.
- the content objects 506 may be automatically ordered based on, for example, how often each content object 506 is used (on the wearable computing device only or in other contexts as well), how recently each content object 506 was used (on the wearable computing device only or in other contexts as well), an explicit or implicit importance or priority ranking of the content objects 506 , and/or other criteria.
- the wearable computing device may be further configured to receive from the wearer a selection of a content object 506 from the menu 504 .
- the user-interface 500 may include a cursor 508 , shown in FIG. 5B as a reticle, which may be used to navigate to and select content objects 506 from the menu 504 .
- the cursor 508 may be controlled by a wearer of the wearable computing device through one or more predetermined movements. Accordingly, the wearable computing device may be further configured to receive selection data corresponding to the one or more predetermined movements.
- the selection data may take several forms.
- the selection data may be (or may be derived from) data received from one or more movement sensors, accelerometers, gyroscopes, and/or detectors configured to detect the one or more predetermined movements.
- the one or more movement sensors may be included in the wearable computing device, like the sensor 122 , or may be included in a peripheral device communicatively coupled to the wearable computing device.
- the selection data may be (or may be derived from) data received from a touch pad, such as the finger-operable touch pad 124 described above in connection with FIG. 1A , or other input device included in or coupled to the wearable computing device and configured to detect one or more predetermined movements.
- the selection data may take the form of a binary indication corresponding to the predetermined movement. In other embodiments, the selection data may indicate the extent, the direction, the velocity, and/or the acceleration associated with the predetermined movement. The selection data may take other forms as well.
- the predetermined movements may take several forms.
- the predetermined movements may be certain movements or sequence of movements of the wearable computing device or peripheral device.
- the predetermined movements may include one or more predetermined movements defined as no or substantially no movement, such as no or substantially no movement for a predetermined period of time.
- one or more predetermined movements may involve a predetermined movement of the wearer's head (which is assumed to move the wearable computing device in a corresponding manner).
- the predetermined movements may involve a predetermined movement of a peripheral device communicatively coupled to the wearable computing device.
- the peripheral device may similarly be wearable by a wearer of the wearable computing device, such that the movement of the peripheral device may follow a movement of the wearer, such as, for example, a movement of the wearer's hand.
- one or more predetermined movements may be, for example, a movement across a finger-operable touch pad or other input device. Other predetermined movements are possible as well.
- a wearer of the wearable computing device has navigated the cursor 508 to the content object 506 using one or more predetermined movements.
- the wearer may perform an additional predetermined movement, such as holding the cursor 508 over the content object 506 for a predetermined period of time.
- the wearer may select the content object 506 in other manners as well.
- the wearable computing device may cause the content object 506 to be displayed in the view region 502 as a selected content object.
- FIG. 5C shows aspects of an example user-interface after selection of a selected content object, in accordance with an embodiment.
- the content object 506 is displayed in the view region 502 as a selected content object 510 .
- the selected content object 510 is displayed larger and in more detail in the view region 502 than in the menu 504 .
- the selected content object 510 could be displayed in the view region 502 smaller than or the same size as, and in less detail than or the same detail as, the menu 504 .
- additional content e.g., actions to be applied to, with, or based on the selected content object 510 , information related to the selected content object 510 , and/or modifiable options, preferences, or parameters for the selected content object 510 , etc.
- additional content may be showed adjacent to or nearby the selected content object 510 in the view region 502 .
- a wearer of the wearable computing device may interact with the selected content object 510 .
- the selected content object 510 may wish to read one of the emails in the email inbox.
- the wearer may interact with the selected content object in other ways as well (e.g., the wearer may locate additional information related to the selected content object 510 , modify, augment, and/or delete the selected content object 510 , etc.).
- the wearable computing device may be further configured to receive input data corresponding to one or more predetermined movements indicating interactions with the user-interface 500 .
- the input data may take any of the forms described above in connection with the selection data.
- FIG. 5D shows aspects of an example user-interface after receiving input data corresponding to a user input, in accordance with an embodiment.
- a wearer of the wearable computing device has navigated the cursor 508 to a particular subject line in the email inbox and selected the subject line.
- the email 512 is displayed in the view region, so that the wearer may read the email 512 .
- the wearer may interact with the user-interface 500 in other manners as well, depending on, for example, the selected content object.
- a wearer of the wearable computing device tilts the device to a certain degree and the device responds by applying a visual manipulation (sometimes referred to as a manipulative action) to the selected content object.
- FIG. 6A shows an example head movement for interacting with the user interface.
- a user 604 of the wearable electronic device 602 may move his or her head with a sideways motion 606 .
- the user 604 may wear the wearable electronic device 602 like a pair of glasses.
- Various means may be used for a user to interact with a user interface of the wearable electronic device 602 . It may be desirable in some embodiments to provide an input to the user interface without having to touch the wearable electronic device 602 .
- the methods presented herein may be used responsive to the presentation of a user interface of a wearable electronic device 602 .
- a first motion of the wearable electronic device 602 may bring a user interface 504 within the view region 502 .
- the methods presented here may allow a user to interact with the user interface 504 .
- a user may wish to pan across a user interface shown in the wearable electronic device 602 .
- a user may want to see items of a menu where some of the items are located outside his or her field of view. Therefore, a user 604 may turn his head toward the right, as indicated by motion 606 , in order to pan the user interface to the right.
- FIG. 6B shows aspects of an example user-interface after receiving movement data corresponding to a rightward movement.
- a movement in the right direction is merely an example.
- the view region 652 and menu 654 may move with respect to any movement.
- the user-interface 650 includes the view region 652 and the menu 654 .
- a portion of the menu 654 may be located outside the view region 652 .
- the wearable computing device may move one or both of the view region 652 and the menu 654 such that the menu 654 becomes more visible in the view region 652 .
- the menu 654 may move to the left, as indicated by motion 660 , in response to the user 604 (of FIG. 6A ) moving his or her head to the right (e.g., motion 606 ).
- the menu 654 may appear to have a portion located outside of the view region 652 .
- the upward movement may bring the user-interface 650 into the view region 652 .
- a second movement possibly in either a left- or right-ward direction, may move the user-interface 650 within the view region 652 .
- the second movement possibly in either a left- or right-ward direction, may selected a particular content objects 656 within the user interface menu 654 .
- a user may be able to scroll through a list of particular content objects 656 by either the left- or right-ward movement.
- the menu 654 is not fully visible in the view region 652 . In other embodiments, however, the entire menu 654 may be visible in the view region 652 (as discussed with respect to FIG. 5B ). In some embodiments, the extent to which the menu 654 is visible in the view region 652 may be based at least in part on an extent of the rightward movement.
- the view region 652 may be moved in response to receiving data corresponding to a rightward movement.
- the view region 652 may be moved in a rightward scrolling or panning motion.
- the view region 652 may appear to a wearer of the wearable computing device as if mapped onto the inside of a static sphere centered at the wearable computing device, and movement of the view region 652 may map onto movement of the real-world environment relative to the wearable computing device.
- a speed, acceleration, and/or magnitude of the scrolling may be based at least in part on a speed, acceleration, and/or magnitude of the rightward movement.
- the view region 502 may be moved only when the right-ward movement exceeds a threshold speed, acceleration, and/or magnitude. In response to receiving data corresponding to an right-ward movement that exceeds such a threshold or thresholds, the view region 502 may pan, scroll, slide, or jump to a new field of view.
- the user interface has scrolled to the left in response to head movement.
- a portion of the menu 654 has left the view region 652 off the left hand side.
- the selected object has responsively updated to select a particular content object 656 b .
- the manipulative action is scrolling the selected content object.
- Scrolling may be used, for example, when the selected content object is a text document, spreadsheet, list of items (e.g., a list of contacts in an address book), etc.
- scrolling right on a list of contacts may move through each individual contact, moving some contacts off the display at the left of the view region and bringing some new contacts into display at the right of the view region.
- scrolling a list of items e.g., a list of emails in an email inbox, letters of a character input system
- Other examples of scrolling are possible as well.
- the manipulative action is cycling the display to a new content object. Cycling may be used, for example, when the selected content object is associated with a group of other content objects. For example, in embodiments in which the selected content object is one image in a photo album, cycling may change the displayed image to another image from that photo album. In embodiments in which the selected content object is a web page, cycling may change the displayed webpage to a previously viewed webpage. Alternatively, cycling may change the selected content object to another content object from the menu 504 . Other examples of cycling are possible as well, as are other examples of manipulative actions such as panning and other object movements.
- FIGS. 7A and 7B show an example of a text entry system for use with the methods and systems disclosed herein.
- FIGS. 7A and 7B show a character input system 700 .
- the character input system may be used on the heads up display.
- the character input system comprises a text entry box 702 and a set of available characters 710 .
- Within the text entry box 702 is a cursor 704 indicating current position in the text input box. Additionally, previously entered characters 706 are also shown in the text entry box 702 .
- a user may select a specific character with the cursor 712 .
- the letter “G” is selected.
- the user may turn his or her head to the left.
- the cursor 712 may responsively move to a new character, the letter “e” as shown in FIG. 7B .
- the head motion described herein may allow a repositioning of the cursor 712 to facilitate text entry.
- There may be a mathematical function that maps a user's head movement to a speed of scrolling through characters of the set of available characters 710 .
- a mathematical function may relate a head movement to a scroll speed.
- the movement may be mapped based on the magnitude of the movement (e.g. the number of degrees the user turned his or her head).
- the movement may be mapped based on the movement speed (e.g. the number of degrees per second the user turned his or her head).
- the movement may be mapped based on additional movement parameters (e.g. the acceleration of the head movement or the impulse of the head movement).
- the mathematical function may have a linear relationship between the movement speed and the scroll speed.
- the user interface menu 654 may scrolled based on a fixed ratio speed multiplier. The ratio may be greater, less than, or equal to one.
- a head movement of 5 degrees per second may translated to the user interface menu 654 scrolling at 10 degrees per second (assuming the user interface menu 654 may be considered a projection on to an imaginary sphere).
- the user interface menu 654 scrolling may be increased to 20 degrees per second due to the linear relationship.
- the mathematical function may have a non-linear relationship between the movement speed and the scroll speed.
- the user interface menu 654 may scrolled based on a quadratic speed multiplier.
- a head movement of 2 degrees per second may translated to the user interface menu 654 scrolling at 4 degrees per second (assuming the user interface menu 654 may be considered a projection on to an imaginary sphere).
- the user interface menu 654 scrolling may be increased to 25 degrees per second due to the quadratic relationship.
- the scrolling speed may also have a maximum speed as well.
- the scrolling rate my increase based on the mathematical formula, until a threshold speed it met. When the threshold is met, the scrolling speed does not increase further.
- the relationship between the head movement and the user interface scroll speed may be based on any mathematical relationship, including an exponential expression, a Sigmoid expression, an inverse exponential function or a polynomial function. Different use cases may be optimized with different functions.
- the user interface may scroll as if it had momentum and resistance (like friction).
- a quick head movement could start the user interface scrolling a specific speed, the speed being determined by a mathematical function based on the speed or distance of the motion.
- the user interface scrolling may slow down based on a virtual friction coefficient. The faster the scrolling starts, the longer it may take to stop.
- the user may also be able to make a motion in the opposite direction to stop or slow down the user interface scrolling.
- the head movement may set an initial movement speed of the items in the user interface, but the movement of the items will slow based on the virtual resistance.
- the position sensor may include a filter on the position information.
- the filter may stop some movements from updating the user interface. For example, very small magnitude movements or very slow movements may be ignored as undesirable. Very small magnitude movements or very slow movements may be unintentional and reduce the user experience of the wearable electronic device. Additionally, in some embodiments very fast movements may be filtered as well. For example, the user of the head mounted display may be running and with each step have a quick head acceleration. It may be desirable to filter out the quick jarring acceleration caused by each step.
- FIG. 8 is a flow diagram of one embodiment of the method for interacting with the user interface of the wearable electronic device presented herein. Some examples of method 800 may be performed by the example systems shown in FIGS. 1-4 in combination with the various user interfaces in FIGS. 5A , 5 B, 5 C, 5 D, 6 B, 6 C, 7 A, and 7 B. Although the blocks of FIG. 8 are illustrated in a sequential order, these blocks may also be performed in parallel, and/or in a different order than those described herein. Also, the various blocks may be combined into fewer blocks, divided into additional blocks, and/or eliminated based upon the desired implementation.
- Method 800 may begin at block 801 , where the wearable electronic device monitors position information.
- the wearable electronic device may use various sensors, such as a gyroscope or an accelerometer, to monitor its position.
- a GPS sensor may also be coupled to allow the wearable electronic device to know its absolute position.
- relative position information provided by a gyroscope or an accelerometer may be sufficient.
- block 801 may also store position information to a memory.
- the memory may hold location information for a period of time for use in other portions of the methods discussed herein. The user may also be able to clear the memory whenever he or she wishes.
- the storage may be configured to store the position information for a finite amount of time, such as 2 seconds, before deleting the information.
- the wearable electronic device may calculate a direction of movement for the wearable electronic device.
- a processor may receive the movement information that was monitored at block 801 .
- a processor may calculate the direction of movement on the fly based on data provided by sensors in the wearable electronic device.
- the processor may read position information from memory and calculate the direction of movement.
- the wearable electronic device may calculate a movement parameter for the wearable electronic device.
- the processor may receive the movement information that was monitored at block 801 or the direction of movement calculated at block 802 .
- a processor may calculate the direction of movement on the fly based on data provided by sensors in the wearable electronic device.
- the processor may read position information from memory and calculate the direction of movement.
- the processor may calculate the movement speed based on the calculated direction of movement.
- the movement parameter may be either the movement speed, movement acceleration or movement impulse.
- the wearable electronic device may scroll an element in the user interface based at least on the calculated direction.
- a user interface menu 654 may be scrolled within the view region 502 .
- the user interface menu 654 may be scrolled in the opposite direction of the calculated direction. For example, if a user of the head mounted display moved his or her head to the left, the user interface may be scrolled to the right. Thus, it would appear like the user was able to scroll through the interface.
- the speed of the scrolling may be based on the calculated movement parameter.
- the movement parameter may be the movement speed. Therefore, the user interface menu 654 may scroll in the opposite direction of the movement based on the movement speed.
- the scrolling speed may have a non-linear relationship with the movement speed.
- different movement parameters may be used to calculate the scrolling speed.
- the movement acceleration or movement impulse could also be used to calculate a scrolling speed of the interface menu 654 .
Abstract
Methods and devices for providing a user-interface are disclosed. In one aspect, a head-mounted-device system includes a processor data storage comprising user-interface logic executable by the at least one processor to receive data corresponding to first position of a head-mounted display (HMD) and responsively cause the HMD to display a user-interface comprising a view region, at least one content region located above the view region, and a history region located below the view region. The user-interface logic is further executable to receive data corresponding to an left or right movement of the HMD and responsively cause the HMD to move the field of view such that the at least one content region becomes more visible, for example, scrolling an item in a user interface. The scrolling may have a non-linear relationship with the head movement speed.
Description
- Unless otherwise indicated herein, the materials described in this section are not prior art to the claims in this application and are not admitted to be prior art by inclusion in this section.
- Various technologies can be utilized to display information to a user of a system. Some systems for displaying information may utilize “heads-up” displays. A heads-up display is typically positioned near the user's eyes to allow the user to view displayed images or information within the user's field of view. To generate the images on the display, a computer processing system may be used. Such heads-up displays have a variety of applications, such as aviation information systems, vehicle navigation systems, and video games.
- One type of heads-up display is a head-mounted display. A head-mounted display can be incorporated into a pair of glasses that the user can wear. The display may include a user interface. The user interface may have various components. Some components may be graphics and other components may be words. One component of the user interface may be a list of items. Additionally, there may be various ways the user could interact with the user interface of the heads up display.
- Disclosed herein are improved methods and devices for controlling and interfacing with a wearable heads-up display. In an exemplary embodiment, the wearable heads-up display may include a processor, a display element configured to receive display information from the processor and to display the display information. A user of the heads-up display may interact with items in the display with movements of his or her head. In this manner, a user may be able to interact with the heads-up display without any touching of the hardware. For example, a user may interact with a heads-up display with a head movement.
- In a further example, the user interface may display a list of items, such as a list of program, a list of emails, and a list of input characters. In order to select one of the items displayed on the heads up display, a user may turn his or her head in a direction, left, right, up, or down. When a user moves his or head in a direction, the user interface shown on the display may update responsively. In one embodiment, the head movement may be mapped to a scrolling or other movement action of an element within the user interface. The element may scroll in response to the head movement. For example, a head turn to the left may cause the items in the list on the user interface to scroll to the right. The speed of the scrolling may be a mathematical function based on the speed of the head movement. In one embodiment, the speed of the head movement is mapped via a non-linear mathematical function to the speed of the scrolling. In some additional embodiments, other parameters of the head movement may be used to calculate the scrolling speed. For example, the parameter may be the acceleration of the heads-up display, the position of the heads up display, or any other parameter.
- The foregoing summary is illustrative only and is not intended to be in any way limiting. In addition to the illustrative aspects, embodiments, and features described above, further aspects, embodiments, and features will become apparent by reference to the figures and the following detailed description.
-
FIG. 1A illustrates an example system for receiving, transmitting, and displaying data. -
FIG. 1B illustrates an alternate view of the system illustrated inFIG. 1A . -
FIG. 2A illustrates an example system for receiving, transmitting, and displaying data. -
FIG. 2B illustrates an example system for receiving, transmitting, and displaying data. -
FIG. 3 shows a simplified block diagram of an example computer network infrastructure. -
FIG. 4 shows a simplified block diagram depicting example components of an example computing system. -
FIG. 5A shows aspects of an example user-interface. -
FIG. 5B shows aspects of an example user-interface after receiving movement data corresponding to an upward movement. -
FIG. 5C shows aspects of an example user-interface after selection of a selected content object. -
FIG. 5D shows aspects of an example user-interface after receiving input data corresponding to a user input. -
FIG. 6A shows an example head movement for interacting with the user interface. -
FIG. 6B shows an example scrolling user interface element. -
FIG. 6C shows an example scrolling user interface element. -
FIG. 7A shows an example text entry system. -
FIG. 7B shows an example scrolling selection in a text entry system. -
FIG. 8 shows an example method for interacting with the user interface. - In the following detailed description, reference is made to the accompanying figures, which form a part thereof. In the figures, similar symbols typically identify similar components, unless context dictates otherwise. The illustrative embodiments described in the detailed description, figures, and claims are not meant to be limiting. Other embodiments may be utilized, and other changes may be made, without departing from the spirit or scope of the subject matter presented herein. It will be readily understood that the aspects of the present disclosure, as generally described herein, and illustrated in the figures, can be arranged, substituted, combined, separated, and designed in a wide variety of different configurations, all of which are contemplated herein.
- The methods and systems disclosed herein generally relate to controlling a user interface present in a wearable computing device. In some example embodiments, the wearable computing device include a heads-up display, which could be configured as a pair of glasses. The user interface may appear within the field of view of a person wearing the heads up display. To operate the user interface, the person may move his or her head. For example, in order to move through a list of items, a person may turn his or her head to the left or right. The user interface may scroll through a list of items based on the head movement of the person wearing the wearable computing device.
-
FIG. 1A illustrates anexample system 100 for receiving, transmitting, and displaying data. Thesystem 100 is shown in the form of a wearable computing device. WhileFIG. 1A illustrates a head-mounteddevice 102 as an example of a wearable computing device, other types of wearable computing devices could additionally or alternatively be used. As illustrated inFIG. 1A , the head-mounteddevice 102 has frame elements including lens-frames center frame support 108,lens elements arms center frame support 108 and the extending side-arms device 102 to a user's face via a user's nose and ears, respectively. - Each of the
frame elements arms device 102. Other materials may be possible as well. - One or more of each of the
lens elements lens elements lens elements - The extending side-
arms frames device 102 to the user. The extending side-arms device 102 to the user by extending around a rear portion of the user's head. Additionally or alternatively, for example, thesystem 100 may connect to or be affixed within a head-mounted helmet structure. Other possibilities exist as well. - The
system 100 may also include an on-board computing system 118, avideo camera 120, asensor 122, and a finger-operable touch pad 124. The on-board computing system 118 is shown to be positioned on the extending side-arm 114 of the head-mounteddevice 102; however, the on-board computing system 118 may be provided on other parts of the head-mounteddevice 102 or may be positioned remote from the head-mounted device 102 (e.g., the on-board computing system 118 could be connected by wires or wirelessly connected to the head-mounted device 102). The on-board computing system 118 may include a processor and memory, for example. The on-board computing system 118 may be configured to receive and analyze data from thevideo camera 120, thesensor 122, and the finger-operable touch pad 124 (and possibly from other sensory devices, user-interfaces, or both) and generate images for output by thelens elements board computing system 118 may additionally include a speaker or a microphone for user input (not shown). An example computing system is further described below in connection withFIG. 4 . - The
video camera 120 is shown positioned on the extending side-arm 114 of the head-mounteddevice 102; however, thevideo camera 120 may be provided on other parts of the head-mounteddevice 102. Thevideo camera 120 may be configured to capture images at various resolutions or at different frame rates. Video cameras with a small form-factor, such as those used in cell phones or webcams, for example, may be incorporated into an example embodiment of thesystem 100. - Further, although
FIG. 1A illustrates onevideo camera 120, more video cameras may be used, and each may be configured to capture the same view, or to capture different views. For example, thevideo camera 120 may be forward facing to capture at least a portion of the real-world view perceived by the user. This forward facing image captured by thevideo camera 120 may then be used to generate an augmented reality where computer generated images appear to interact with the real-world view perceived by the user. - The
sensor 122 is shown on the extending side-arm 116 of the head-mounteddevice 102; however, thesensor 122 may be positioned on other parts of the head-mounteddevice 102. Thesensor 122 may include one or more of a gyroscope or an accelerometer, for example. Other sensing devices may be included within, or in addition to, thesensor 122 or other sensing functions may be performed by thesensor 122. - The finger-
operable touch pad 124 is shown on the extending side-arm 114 of the head-mounteddevice 102. However, the finger-operable touch pad 124 may be positioned on other parts of the head-mounteddevice 102. Also, more than one finger-operable touch pad may be present on the head-mounteddevice 102. The finger-operable touch pad 124 may be used by a user to input commands. The finger-operable touch pad 124 may sense at least one of a position and a movement of a finger via capacitive sensing, resistance sensing, or a surface acoustic wave process, among other possibilities. The finger-operable touch pad 124 may be capable of sensing finger movement in a direction parallel or planar to the pad surface, in a direction normal to the pad surface, or both, and may also be capable of sensing a level of pressure applied to the pad surface. The finger-operable touch pad 124 may be formed of one or more translucent or transparent insulating layers and one or more translucent or transparent conducting layers. Edges of the finger-operable touch pad 124 may be formed to have a raised, indented, or roughened surface, so as to provide tactile feedback to a user when the user's finger reaches the edge, or other area, of the finger-operable touch pad 124. If more than one finger-operable touch pad is present, each finger-operable touch pad may be operated independently, and may provide a different function. -
FIG. 1B illustrates an alternate view of thesystem 100 illustrated inFIG. 1A . As shown inFIG. 1B , thelens elements device 102 may include afirst projector 128 coupled to an inside surface of the extending side-arm 116 and configured to project adisplay 130 onto an inside surface of thelens element 112. Additionally or alternatively, asecond projector 132 may be coupled to an inside surface of the extending side-arm 114 and configured to project adisplay 134 onto an inside surface of thelens element 110. - The
lens elements projectors projectors - In alternative embodiments, other types of display elements may also be used. For example, the
lens elements frame elements -
FIG. 2A illustrates anexample system 200 for receiving, transmitting, and displaying data. Thesystem 200 is shown in the form of awearable computing device 202. Thewearable computing device 202 may include frame elements and side-arms such as those described with respect toFIGS. 1A and 1B . Thewearable computing device 202 may additionally include an on-board computing system 204 and avideo camera 206, such as those described with respect toFIGS. 1A and 1B . Thevideo camera 206 is shown mounted on a frame of thewearable computing device 202; however, thevideo camera 206 may be mounted at other positions as well. - As shown in
FIG. 2A , thewearable computing device 202 may include asingle display 208 which may be coupled to the device. Thedisplay 208 may be formed on one of the lens elements of thewearable computing device 202, such as a lens element described with respect toFIGS. 1A and 1B , and may be configured to overlay computer-generated graphics in the user's view of the physical world. Thedisplay 208 is shown to be provided in a center of a lens of thewearable computing device 202, however, thedisplay 208 may be provided in other positions. Thedisplay 208 is controllable via thecomputing system 204 that is coupled to thedisplay 208 via an optical waveguide 210. -
FIG. 2B illustrates anexample system 220 for receiving, transmitting, and displaying data. Thesystem 220 is shown in the form of awearable computing device 222. Thewearable computing device 222 may include side-arms 223, acenter frame support 224, and a bridge portion withnosepiece 225. In the example shown inFIG. 2B , thecenter frame support 224 connects the side-arms 223. Thewearable computing device 222 does not include lens-frames containing lens elements. Thewearable computing device 222 may additionally include an on-board computing system 226 and avideo camera 228, such as those described with respect toFIGS. 1A and 1B . - The
wearable computing device 222 may include asingle lens element 230 that may be coupled to one of the side-arms 223 or thecenter frame support 224. Thelens element 230 may include a display such as the display described with reference toFIGS. 1A and 1B , and may be configured to overlay computer-generated graphics upon the user's view of the physical world. In one example, thesingle lens element 230 may be coupled to a side of the extending side-arm 223. Thesingle lens element 230 may be positioned in front of or proximate to a user's eye when thewearable computing device 222 is worn by a user. For example, thesingle lens element 230 may be positioned below thecenter frame support 224, as shown inFIG. 2B . -
FIG. 3 shows a simplified block diagram of an example computer network infrastructure. Insystem 300, adevice 310 communicates using a communication link 320 (e.g., a wired or wireless connection) to aremote device 330. Thedevice 310 may be any type of device that can receive data and display information corresponding to or associated with the data. For example, thedevice 310 may be a heads-up display system, such as the head-mounteddevice FIGS. 1A-2B . - Thus, the
device 310 may include adisplay system 312 comprising aprocessor 314 and adisplay 316. Thedisplay 316 may be, for example, an optical see-through display, an optical see-around display, or a video see-through display. Theprocessor 314 may receive data from theremote device 330, and configure the data for display on thedisplay 316. Theprocessor 314 may be any type of processor, such as a micro-processor or a digital signal processor, for example. - The
device 310 may further include on-board data storage, such asmemory 318 coupled to theprocessor 314. Thememory 318 may store software that can be accessed and executed by theprocessor 314, for example. - The
remote device 330 may be any type of computing device or transmitter including a laptop computer, a mobile telephone, or tablet computing device, etc., that is configured to transmit data to thedevice 310. Theremote device 330 and thedevice 310 may contain hardware to enable thecommunication link 320, such as processors, transmitters, receivers, antennas, etc. - In
FIG. 3 , thecommunication link 320 is illustrated as a wireless connection; however, wired connections may also be used. For example, thecommunication link 320 may be a wired serial bus such as a universal serial bus or a parallel bus, among other connections. Thecommunication link 320 may also be a wireless connection using, e.g., Bluetooth® radio technology, communication protocols described in IEEE 802.11 (including any IEEE 802.11 revisions), Cellular technology (such as GSM, CDMA, UMTS, EV-DO, WiMAX, or LTE), or Zigbee® technology, among other possibilities. Either of such a wired and/or wireless connection may be a proprietary connection as well. Theremote device 330 may be accessible via the Internet and may include a computing cluster associated with a particular web service (e.g., social-networking, photo sharing, address book, etc.). - As described above in connection with
FIGS. 1A-2B , an example wearable computing device may include, or may otherwise be communicatively coupled to, a computing system, such ascomputing system 118 orcomputing system 204.FIG. 4 shows a simplified block diagram depicting example components of anexample computing system 400. One or both of thedevice 310 and theremote device 330 may take the form ofcomputing system 400. -
Computing system 400 may include at least oneprocessor 402 andsystem memory 404. In an example embodiment,computing system 400 may include a system bus 406 that communicatively connectsprocessor 402 andsystem memory 404, as well as other components ofcomputing system 400. Depending on the desired configuration,processor 402 can be any type of processor including, but not limited to, a microprocessor (μP), a microcontroller (μC), a digital signal processor (DSP), or any combination thereof. Furthermore,system memory 404 can be of any type of memory now known or later developed including but not limited to volatile memory (such as RAM), non-volatile memory (such as ROM, flash memory, etc.) or any combination thereof. - An
example computing system 400 may include various other components as well. For example,computing system 400 includes an A/V processing unit 408 for controllinggraphical display 410 and speaker 412 (via A/V port 414), one ormore communication interfaces 416 for connecting toother computing devices 418, and apower supply 420.Graphical display 410 may be arranged to provide a visual depiction of various input regions provided by user-interface module 422. For example, user-interface module 422 may be configured to provide a user-interface, such as the example user-interface described below in connection withFIGS. 5A-D , andgraphical display 410 may be configured to provide a visual depiction of the user-interface. User-interface module 422 may be further configured to receive data from and transmit data to (or be otherwise compatible with) one or more user-interface devices 428. - Furthermore,
computing system 400 may also include one or moredata storage devices 424, which can be removable storage devices, non-removable storage devices, or a combination thereof. Examples of removable storage devices and non-removable storage devices include magnetic disk devices such as flexible disk drives and hard-disk drives (HDD), optical disk drives such as compact disk (CD) drives or digital versatile disk (DVD) drives, solid state drives (SSD), and/or any other storage device now known or later developed. Computer storage media can include volatile and nonvolatile, removable and non-removable media implemented in any method or technology for storage of information, such as computer readable instructions, data structures, program modules, or other data. For example, computer storage media may take the form of RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium now known or later developed that can be used to store the desired information and which can be accessed by computingsystem 400. - According to an example embodiment,
computing system 400 may include program instructions 426 that are stored in system memory 404 (and/or possibly in another data-storage medium) and executable byprocessor 402 to facilitate the various functions described herein including, but not limited to, those functions described with respect toFIG. 8 . Although various components ofcomputing system 400 are shown as distributed components, it should be understood that any of such components may be physically integrated and/or distributed according to the desired configuration of the computing system. -
FIGS. 5A-D show aspects of an example user-interface 500. The user-interface 500 may be displayed by, for example, a wearable computing device as described above forFIGS. 1A-2B . - An example state of the user-
interface 500 is shown inFIG. 5A . The example state shown inFIG. 5A may correspond to a first position of the wearable computing device. That is, the user-interface 500 may be displayed as shown inFIG. 5A when the wearable computing device is in the first position. In some embodiments, the first position of the wearable computing device may correspond to a position of the wearable computing device when a wearer of the wearable computing device is looking in a direction that is generally parallel to the ground (e.g., a position that does not correspond to the wearer looking up or looking down). Other examples are possible as well. - As shown, the user-
interface 500 includes aview region 502. An example boundary of theview region 502 is shown by a dotted frame. While theview region 502 is shown to have a landscape shape (in which theview region 502 is wider than it is tall), in other embodiments theview region 502 may have a portrait or square shape, or may have a non-rectangular shape, such as a circular or elliptical shape. Theview region 502 may have other shapes as well. - The
view region 502 may be, for example, the viewable area between (or encompassing) the upper, lower, left, and right boundaries of a display on the wearable computing device. As shown, when the wearable computing device is in the first position, theview region 502 is substantially empty (e.g., completely empty) of user-interface elements, such that the user's view of their real-world environment is generally uncluttered, and objects in the user's environment are not obscured. - In some embodiments, the
view region 502 may correspond to a field of view of a wearer of the wearable computing device, and an area outside theview region 502 may correspond to an area outside the field of view of the wearer. In other embodiments, theview region 502 may correspond to a non-peripheral portion of a field of view of a wearer of the wearable computing device, and an area outside theview region 502 may correspond to a peripheral portion of the field of view of the wearer. In still other embodiments, the user-interface 500 may be larger than or substantially the same as a field of view of a wearer of the wearable computing device, and the field of view of the wearer may be larger than or substantially the same size as theview region 502. Theview region 502 may take other forms as well. - Accordingly, the portions of the user-
interface 500 outside of theview region 502 may be outside of or in a peripheral portion of a field of view of a wearer of the wearable computing device. For example, as shown, amenu 504 may be outside of or in a peripheral portion of the field of view of the user in the user-interface 500. While themenu 504 is shown to be not visible in theview region 502, in some embodiments themenu 504 may be partially visible in theview region 502. - In some embodiments, the wearable computing device may be configured to receive movement data corresponding to, for example, an upward movement of the wearable computing device to a position above the first position. In these embodiments, the wearable computing device may, in response to receiving the movement data corresponding to the upward movement, cause one or both of the
view region 502 and themenu 504 to move such that themenu 504 becomes more visible in theview region 502. For example, the wearable computing device may cause theview region 502 to move upward and may cause themenu 504 to move downward. Theview region 502 and themenu 504 may move the same amount, or may move different amounts. In one embodiment, themenu 504 may move further than theview region 502. As another example, the wearable computing device may cause only themenu 504 to move. Other examples are possible as well, and are discussed insection 4 below. - While the term “upward” is used, it is to be understood that the upward movement may encompass any movement having any combination of moving, tilting, rotating, shifting, sliding, or other movement that results in a generally upward movement. Further, in some embodiments “upward” may refer to an upward movement in the reference frame of a wearer of the wearable computing device. Other reference frames are possible as well. In embodiments where the wearable computing device is a head-mounted device, the upward movement of the wearable computing device may also be an upward movement of a wearer's head such as, for example, the user looking upward.
- The movement data corresponding to the upward movement may take several forms. For example, the movement data may be (or may be derived from) data received from one or more movement sensors, accelerometers, and/or gyroscopes configured to detect the upward movement, such as the
sensor 122 described above in connection withFIG. 1A . In some embodiments, the movement data may comprise a binary indication corresponding to the upward movement. In other embodiments, the movement data may comprise an indication corresponding to the upward movement as well as an extent of the upward movement. The movement data may take other forms as well. -
FIG. 5B shows aspects of an example user-interface after receiving movement data corresponding to an upward movement. As shown, the user-interface 500 includes theview region 502 and themenu 504. - As noted above, in response to receiving the movement data corresponding to an upward movement of the wearable computing device, the wearable computing device may move one or both of the
view region 502 and themenu 504 such that themenu 504 becomes more visible in theview region 502. - As shown, the
menu 504 is fully visible in theview region 502. In other embodiments, however, only a portion of themenu 504 may be visible in theview region 502. In some embodiments, the extent to which themenu 504 is visible in theview region 502 may be based at least in part on an extent of the upward movement. - Thus, the
view region 502 may be moved in response to receiving data corresponding to an upward movement. In some embodiments, theview region 502 may be moved in an upward scrolling or panning motion. For instance, theview region 502 may appear to a wearer of the wearable computing device as if mapped onto the inside of a static sphere centered at the wearable computing device, and movement of theview region 502 may map onto movement of the real-world environment relative to the wearable computing device. A speed, acceleration, and/or magnitude of the upward scrolling may be based at least in part on a speed, acceleration, and/or magnitude of the upward movement. In other embodiments, theview region 502 may be moved by, for example, jumping between fields of view. In still other embodiments, theview region 502 may be moved only when the upward movement exceeds a threshold speed, acceleration, and/or magnitude. In response to receiving data corresponding to an upward movement that exceeds such a threshold or thresholds, theview region 502 may pan, scroll, slide, or jump to a new field of view. Theview region 502 may be moved in other manners as well. - While the foregoing description focused on upward movement, it is to be understood that the wearable computing device could be configured to receive data corresponding to other directional movement (e.g., downward, leftward, rightward, etc.) as well, and that the
view region 502 may be moved in response to receiving such data in a manner similar to that described above in connection with upward movement. - As shown, the
menu 504 includes a number of content objects 506. In some embodiments, the content objects 506 may be arranged in a ring (or partial ring) around and above the head of a wearer of the wearable computing device. In other embodiments, the content objects 506 may be arranged in a dome-shape above the wearer's head. The ring or dome may be centered above the wearable computing device and/or the wearer's head. In other embodiments, the content objects 506 may be arranged in other ways as well. - The number of
content objects 506 in themenu 504 may be fixed or may be variable. In embodiments where the number is variable, the content objects 506 may vary in size according to the number ofcontent objects 506 in themenu 504. In embodiments where the content objects 506 extend circularly around a wearer's head, like a ring (or partial ring), only some of the content objects 506 may be visible at a particular moment. In order to view other content objects 504, a wearer of the wearable computing device may interact with the wearable computing device to, for example, rotate the content objects 506 along a path (e.g., clockwise or counterclockwise) around the wearer's head. To this end, the wearable computing device may be configured to receive data indicating such an interaction through, for example, a touch pad, such as finger-operable touch pad 124. Alternatively or additionally, the wearable computing device may be configured to receive such data through other input devices as well. - Depending on the application of the wearable computing device, the content objects 506 may take several forms. For example, the content objects 506 may include one or more of people, contacts, groups of people and/or contacts, calendar items, lists, notifications, alarms, reminders, status updates, incoming messages, recorded media, audio recordings, video recordings, photographs, digital collages, previously-saved states, webpages, and applications, as well as tools, such as a still camera, a video camera, and an audio recorder. Content objects 506 may take other forms as well.
- In embodiments where the content objects 506 include tools, the tools may be located in a particular region of the
menu 504, such as the center. In some embodiments, the tools may remain in the center of themenu 504, even if the other content objects 506 rotate, as described above. Tool content objects may be located in other regions of themenu 504 as well. - The particular content objects 506 that are included in
menu 504 may be fixed or variable. For example, the content objects 506 may be preselected by a wearer of the wearable computing device. In another embodiment, the content objects 506 for each content region may be automatically assembled by the wearable computing device from one or more physical or digital contexts including, for example, people, places, and/or objects surrounding the wearable computing device, address books, calendars, social-networking web services or applications, photo sharing web services or applications, search histories, and/or other contexts. Further, some content objects 506 may be fixed, while the content objects 506 may be variable. The content objects 506 may be selected in other manners as well. - Similarly, an order or configuration in which the content objects 506 are displayed may be fixed or variable. In one embodiment, the content objects 506 may be pre-ordered by a wearer of the wearable computing device. In another embodiment, the content objects 506 may be automatically ordered based on, for example, how often each
content object 506 is used (on the wearable computing device only or in other contexts as well), how recently eachcontent object 506 was used (on the wearable computing device only or in other contexts as well), an explicit or implicit importance or priority ranking of the content objects 506, and/or other criteria. - In some embodiments, the wearable computing device may be further configured to receive from the wearer a selection of a
content object 506 from themenu 504. To this end, the user-interface 500 may include acursor 508, shown inFIG. 5B as a reticle, which may be used to navigate to and select content objects 506 from themenu 504. In some embodiments, thecursor 508 may be controlled by a wearer of the wearable computing device through one or more predetermined movements. Accordingly, the wearable computing device may be further configured to receive selection data corresponding to the one or more predetermined movements. - The selection data may take several forms. For example, the selection data may be (or may be derived from) data received from one or more movement sensors, accelerometers, gyroscopes, and/or detectors configured to detect the one or more predetermined movements. The one or more movement sensors may be included in the wearable computing device, like the
sensor 122, or may be included in a peripheral device communicatively coupled to the wearable computing device. As another example, the selection data may be (or may be derived from) data received from a touch pad, such as the finger-operable touch pad 124 described above in connection withFIG. 1A , or other input device included in or coupled to the wearable computing device and configured to detect one or more predetermined movements. In some embodiments, the selection data may take the form of a binary indication corresponding to the predetermined movement. In other embodiments, the selection data may indicate the extent, the direction, the velocity, and/or the acceleration associated with the predetermined movement. The selection data may take other forms as well. - The predetermined movements may take several forms. In some embodiments, the predetermined movements may be certain movements or sequence of movements of the wearable computing device or peripheral device. In some embodiments, the predetermined movements may include one or more predetermined movements defined as no or substantially no movement, such as no or substantially no movement for a predetermined period of time. In embodiments where the wearable computing device is a head-mounted device, one or more predetermined movements may involve a predetermined movement of the wearer's head (which is assumed to move the wearable computing device in a corresponding manner). Alternatively or additionally, the predetermined movements may involve a predetermined movement of a peripheral device communicatively coupled to the wearable computing device. The peripheral device may similarly be wearable by a wearer of the wearable computing device, such that the movement of the peripheral device may follow a movement of the wearer, such as, for example, a movement of the wearer's hand. Still alternatively or additionally, one or more predetermined movements may be, for example, a movement across a finger-operable touch pad or other input device. Other predetermined movements are possible as well.
- As shown, a wearer of the wearable computing device has navigated the
cursor 508 to thecontent object 506 using one or more predetermined movements. In order to select thecontent object 506, the wearer may perform an additional predetermined movement, such as holding thecursor 508 over thecontent object 506 for a predetermined period of time. The wearer may select thecontent object 506 in other manners as well. - Once a
content object 506 is selected, the wearable computing device may cause thecontent object 506 to be displayed in theview region 502 as a selected content object.FIG. 5C shows aspects of an example user-interface after selection of a selected content object, in accordance with an embodiment. - As indicated by the dotted arrow, the
content object 506 is displayed in theview region 502 as a selectedcontent object 510. As shown, the selectedcontent object 510 is displayed larger and in more detail in theview region 502 than in themenu 504. In other embodiments, however, the selectedcontent object 510 could be displayed in theview region 502 smaller than or the same size as, and in less detail than or the same detail as, themenu 504. In some embodiments, additional content (e.g., actions to be applied to, with, or based on the selectedcontent object 510, information related to the selectedcontent object 510, and/or modifiable options, preferences, or parameters for the selectedcontent object 510, etc.) may be showed adjacent to or nearby the selectedcontent object 510 in theview region 502. - Once the selected
content object 510 is displayed in theview region 502, a wearer of the wearable computing device may interact with the selectedcontent object 510. For example, as the selectedcontent object 510 is shown as an email inbox, the wearer may wish to read one of the emails in the email inbox. Depending on the selected content object, the wearer may interact with the selected content object in other ways as well (e.g., the wearer may locate additional information related to the selectedcontent object 510, modify, augment, and/or delete the selectedcontent object 510, etc.). To this end, the wearable computing device may be further configured to receive input data corresponding to one or more predetermined movements indicating interactions with the user-interface 500. The input data may take any of the forms described above in connection with the selection data. -
FIG. 5D shows aspects of an example user-interface after receiving input data corresponding to a user input, in accordance with an embodiment. As shown, a wearer of the wearable computing device has navigated thecursor 508 to a particular subject line in the email inbox and selected the subject line. As a result, theemail 512 is displayed in the view region, so that the wearer may read theemail 512. The wearer may interact with the user-interface 500 in other manners as well, depending on, for example, the selected content object. - In accordance with another embodiment, a wearer of the wearable computing device tilts the device to a certain degree and the device responds by applying a visual manipulation (sometimes referred to as a manipulative action) to the selected content object.
FIG. 6A shows an example head movement for interacting with the user interface. Auser 604 of the wearableelectronic device 602 may move his or her head with asideways motion 606. Theuser 604 may wear the wearableelectronic device 602 like a pair of glasses. Various means may be used for a user to interact with a user interface of the wearableelectronic device 602. It may be desirable in some embodiments to provide an input to the user interface without having to touch the wearableelectronic device 602. In some embodiments, the methods presented herein may be used responsive to the presentation of a user interface of a wearableelectronic device 602. For example, a first motion of the wearableelectronic device 602 may bring auser interface 504 within theview region 502. Once theuser interface 504 is within theview region 502, the methods presented here may allow a user to interact with theuser interface 504. - In one embodiment, a user may wish to pan across a user interface shown in the wearable
electronic device 602. For example, a user may want to see items of a menu where some of the items are located outside his or her field of view. Therefore, auser 604 may turn his head toward the right, as indicated bymotion 606, in order to pan the user interface to the right. -
FIG. 6B shows aspects of an example user-interface after receiving movement data corresponding to a rightward movement. A movement in the right direction is merely an example. Theview region 652 andmenu 654 may move with respect to any movement. As shown, the user-interface 650 includes theview region 652 and themenu 654. A portion of themenu 654 may be located outside theview region 652. - As noted above, in response to receiving the movement data corresponding to a rightward movement of the wearable computing device, the wearable computing device may move one or both of the
view region 652 and themenu 654 such that themenu 654 becomes more visible in theview region 652. Themenu 654 may move to the left, as indicated bymotion 660, in response to the user 604 (ofFIG. 6A ) moving his or her head to the right (e.g., motion 606). Themenu 654 may appear to have a portion located outside of theview region 652. - In some embodiments, the upward movement, as disclosed in section 3 above, may bring the user-
interface 650 into theview region 652. Once the user-interface 650 is in theview region 652, a second movement, possibly in either a left- or right-ward direction, may move the user-interface 650 within theview region 652. Additionally, the second movement, possibly in either a left- or right-ward direction, may selected a particular content objects 656 within theuser interface menu 654. A user may be able to scroll through a list of particular content objects 656 by either the left- or right-ward movement. - As shown, the
menu 654 is not fully visible in theview region 652. In other embodiments, however, theentire menu 654 may be visible in the view region 652 (as discussed with respect toFIG. 5B ). In some embodiments, the extent to which themenu 654 is visible in theview region 652 may be based at least in part on an extent of the rightward movement. - Thus, the
view region 652 may be moved in response to receiving data corresponding to a rightward movement. In some embodiments, theview region 652 may be moved in a rightward scrolling or panning motion. For instance, theview region 652 may appear to a wearer of the wearable computing device as if mapped onto the inside of a static sphere centered at the wearable computing device, and movement of theview region 652 may map onto movement of the real-world environment relative to the wearable computing device. - A speed, acceleration, and/or magnitude of the scrolling may be based at least in part on a speed, acceleration, and/or magnitude of the rightward movement. In still other embodiments, the
view region 502 may be moved only when the right-ward movement exceeds a threshold speed, acceleration, and/or magnitude. In response to receiving data corresponding to an right-ward movement that exceeds such a threshold or thresholds, theview region 502 may pan, scroll, slide, or jump to a new field of view. - As shown in
FIG. 6C , the user interface has scrolled to the left in response to head movement. A portion of themenu 654 has left theview region 652 off the left hand side. The selected object has responsively updated to select aparticular content object 656 b. Once the user has selected a content object, the user can the apply a manipulative action the selected content object. - In at least one embodiment, the manipulative action is scrolling the selected content object. Scrolling may be used, for example, when the selected content object is a text document, spreadsheet, list of items (e.g., a list of contacts in an address book), etc. For example, scrolling right on a list of contacts may move through each individual contact, moving some contacts off the display at the left of the view region and bringing some new contacts into display at the right of the view region. Similarly, scrolling a list of items (e.g., a list of emails in an email inbox, letters of a character input system) may bring some new items into display from one direction while moving some items off the display in the other direction. Other examples of scrolling are possible as well.
- In at least one embodiment, the manipulative action is cycling the display to a new content object. Cycling may be used, for example, when the selected content object is associated with a group of other content objects. For example, in embodiments in which the selected content object is one image in a photo album, cycling may change the displayed image to another image from that photo album. In embodiments in which the selected content object is a web page, cycling may change the displayed webpage to a previously viewed webpage. Alternatively, cycling may change the selected content object to another content object from the
menu 504. Other examples of cycling are possible as well, as are other examples of manipulative actions such as panning and other object movements. -
FIGS. 7A and 7B show an example of a text entry system for use with the methods and systems disclosed herein.FIGS. 7A and 7B show acharacter input system 700. The character input system may be used on the heads up display. The character input system comprises atext entry box 702 and a set ofavailable characters 710. Within thetext entry box 702 is acursor 704 indicating current position in the text input box. Additionally, previously enteredcharacters 706 are also shown in thetext entry box 702. - To input characters a user may select a specific character with the
cursor 712. InFIG. 7A the letter “G” is selected. To move to another character the user may turn his or her head to the left. Thecursor 712 may responsively move to a new character, the letter “e” as shown inFIG. 7B . The head motion described herein may allow a repositioning of thecursor 712 to facilitate text entry. There may be a mathematical function that maps a user's head movement to a speed of scrolling through characters of the set ofavailable characters 710. - Thus, a mathematical function may relate a head movement to a scroll speed. The movement may be mapped based on the magnitude of the movement (e.g. the number of degrees the user turned his or her head). In other embodiments, the movement may be mapped based on the movement speed (e.g. the number of degrees per second the user turned his or her head). In still further embodiments, the movement may be mapped based on additional movement parameters (e.g. the acceleration of the head movement or the impulse of the head movement). For example, the mathematical function may have a linear relationship between the movement speed and the scroll speed. For example, the
user interface menu 654 may scrolled based on a fixed ratio speed multiplier. The ratio may be greater, less than, or equal to one. Thus, in one embodiment, a head movement of 5 degrees per second may translated to theuser interface menu 654 scrolling at 10 degrees per second (assuming theuser interface menu 654 may be considered a projection on to an imaginary sphere). Thus, if the head movement was increased to 10 degrees per second, theuser interface menu 654 scrolling may be increased to 20 degrees per second due to the linear relationship. - In additional embodiments, the mathematical function may have a non-linear relationship between the movement speed and the scroll speed. For example, the
user interface menu 654 may scrolled based on a quadratic speed multiplier. In one embodiment, a head movement of 2 degrees per second may translated to theuser interface menu 654 scrolling at 4 degrees per second (assuming theuser interface menu 654 may be considered a projection on to an imaginary sphere). Thus, if the head movement was increased to 5 degrees per second, theuser interface menu 654 scrolling may be increased to 25 degrees per second due to the quadratic relationship. - The scrolling speed may also have a maximum speed as well. Thus, the scrolling rate my increase based on the mathematical formula, until a threshold speed it met. When the threshold is met, the scrolling speed does not increase further.
- Although only linear and quadratic functions were discussed, the relationship between the head movement and the user interface scroll speed may be based on any mathematical relationship, including an exponential expression, a Sigmoid expression, an inverse exponential function or a polynomial function. Different use cases may be optimized with different functions.
- In some additional embodiments, the user interface may scroll as if it had momentum and resistance (like friction). For example, a quick head movement could start the user interface scrolling a specific speed, the speed being determined by a mathematical function based on the speed or distance of the motion. The user interface scrolling may slow down based on a virtual friction coefficient. The faster the scrolling starts, the longer it may take to stop. The user may also be able to make a motion in the opposite direction to stop or slow down the user interface scrolling. The head movement may set an initial movement speed of the items in the user interface, but the movement of the items will slow based on the virtual resistance.
- Additionally, the position sensor may include a filter on the position information. The filter may stop some movements from updating the user interface. For example, very small magnitude movements or very slow movements may be ignored as undesirable. Very small magnitude movements or very slow movements may be unintentional and reduce the user experience of the wearable electronic device. Additionally, in some embodiments very fast movements may be filtered as well. For example, the user of the head mounted display may be running and with each step have a quick head acceleration. It may be desirable to filter out the quick jarring acceleration caused by each step.
-
FIG. 8 is a flow diagram of one embodiment of the method for interacting with the user interface of the wearable electronic device presented herein. Some examples ofmethod 800 may be performed by the example systems shown inFIGS. 1-4 in combination with the various user interfaces inFIGS. 5A , 5B, 5C, 5D, 6B, 6C, 7A, and 7B. Although the blocks ofFIG. 8 are illustrated in a sequential order, these blocks may also be performed in parallel, and/or in a different order than those described herein. Also, the various blocks may be combined into fewer blocks, divided into additional blocks, and/or eliminated based upon the desired implementation. -
Method 800 may begin atblock 801, where the wearable electronic device monitors position information. The wearable electronic device may use various sensors, such as a gyroscope or an accelerometer, to monitor its position. In some embodiments, a GPS sensor may also be coupled to allow the wearable electronic device to know its absolute position. However, for most embodiments disclosed, relative position information provided by a gyroscope or an accelerometer may be sufficient. In some embodiments, block 801 may also store position information to a memory. The memory may hold location information for a period of time for use in other portions of the methods discussed herein. The user may also be able to clear the memory whenever he or she wishes. Additionally, the storage may be configured to store the position information for a finite amount of time, such as 2 seconds, before deleting the information. - At
block 802, the wearable electronic device may calculate a direction of movement for the wearable electronic device. A processor may receive the movement information that was monitored atblock 801. In some embodiments, a processor may calculate the direction of movement on the fly based on data provided by sensors in the wearable electronic device. In other embodiments, the processor may read position information from memory and calculate the direction of movement. - At
block 803, the wearable electronic device may calculate a movement parameter for the wearable electronic device. The processor may receive the movement information that was monitored atblock 801 or the direction of movement calculated atblock 802. In some embodiments, a processor may calculate the direction of movement on the fly based on data provided by sensors in the wearable electronic device. In other embodiments, the processor may read position information from memory and calculate the direction of movement. In still further embodiments, the processor may calculate the movement speed based on the calculated direction of movement. The movement parameter may be either the movement speed, movement acceleration or movement impulse. - At
block 804, the wearable electronic device may scroll an element in the user interface based at least on the calculated direction. In some embodiments, auser interface menu 654 may be scrolled within theview region 502. Theuser interface menu 654 may be scrolled in the opposite direction of the calculated direction. For example, if a user of the head mounted display moved his or her head to the left, the user interface may be scrolled to the right. Thus, it would appear like the user was able to scroll through the interface. The speed of the scrolling may be based on the calculated movement parameter. - For example, the movement parameter may be the movement speed. Therefore, the
user interface menu 654 may scroll in the opposite direction of the movement based on the movement speed. In one embodiment, the scrolling speed may have a non-linear relationship with the movement speed. In other embodiments, different movement parameters may be used to calculate the scrolling speed. For example, the movement acceleration or movement impulse could also be used to calculate a scrolling speed of theinterface menu 654. - While various aspects and embodiments have been disclosed herein, other aspects and embodiments will be apparent to those skilled in the art. The various aspects and embodiments disclosed herein are for purposes of illustration and are not intended to be limiting, with the true scope and spirit being indicated by the following claims.
Claims (24)
1. A method of entering text for head mounted display (HMD) comprising:
(a) receiving data that indicates a position of the HMD;
(b) calculating a direction of movement for the HMD;
(c) calculating a movement parameter for the HMD; and
(d) scrolling a text-entry menu within the user interface based on the calculated movement direction and the calculated movement parameter, wherein the scrolling has an associated speed, and wherein the associated speed has a non-linear functional relationship to the calculated movement parameter.
2. The method of claim 1 , wherein the non-linear functional relationship is a quadratic relationship.
3. The method of claim 2 , wherein the non-linear functional relationship is a polynomial function.
4. The method of claim 1 , wherein the text-entry menu within the user interface comprises a scrollable list of items.
5. The method of claim 1 , wherein the movement parameter is one of either movement speed, movement acceleration or movement impulse.
6. The method of claim 1 , wherein the direction of movement corresponds to a rotation around an axis.
7. The method of claim 6 , wherein the axis is defined by a head position of a wearer of the HMD.
8. The method of claim 1 , wherein (a), (b), (c), and (d) are performed responsive to a first motion, wherein the first motion causes the user interface to be displayed.
9. A system comprising:
at least one processor; and
data storage comprising user-interface logic executable by the at least one processor to:
(a) receive data that indicates a position of a head mounted display (HMD);
(b) calculate a direction of movement for the HMD;
(c) calculate a movement parameter for the HMD; and
(d) scroll a text-entry menu within the user interface based on the calculated movement direction and the calculated movement parameter, wherein the scrolling has an associated speed, and wherein the associated speed has a non-linear functional relationship to the calculated movement parameter.
10. The system of claim 9 , wherein the movement parameter is one of either movement speed, movement acceleration or movement impulse.
11. The system of claim 9 , wherein the non-linear functional relationship is a quadratic relationship.
12. The system of claim 11 , wherein the non-linear functional relationship is a polynomial function.
13. The system of claim 9 , wherein the element text-entry menu within the user interface comprises a scrollable list of items.
14. The system of claim 9 , wherein the direction of movement corresponds to a rotation around an axis.
15. The system of claim 14 , wherein the axis is defined by a head position of a wearer of the HMD.
16. The system of claim 9 , further comprising user-interface logic executable by the at least one processor to perform (a), (b), (c), and (d) responsive to a first motion, wherein the first motion causes the user interface to be displayed.
17. A non-transitory computer readable medium having stored therein instructions executable by a computing device to cause the computing device to perform functions comprising:
(a) receiving data that indicates a position of a head mounted display (HMD);
(b) calculating a direction of movement for the HMD;
(c) calculating a movement speed for the HMD; and
(d) scrolling a text-entry menu within the user interface based on the calculated movement direction and the calculated movement speed, wherein the scrolling has an associated speed, and wherein the associated speed has a non-linear functional relationship to the calculated movement speed.
18. The computer readable medium of claim 17 , wherein the movement parameter is one of either movement speed, movement acceleration or movement impulse.
19. The computer readable medium of claim 17 , wherein the non-linear functional relationship is a quadratic relationship.
20. The computer readable medium of claim 19 , wherein the non-linear functional relationship is a polynomial function.
21. The computer readable medium of claim 17 , wherein the text-entry menu within the user interface comprises a scrollable list of items
22. The computer readable medium of claim 17 , wherein the direction of movement corresponds to a rotation around an axis.
23. The computer readable medium of claim 22 , wherein the axis is defined by a head position of a wearer of the HMD.
24. The computer readable medium of claim 17 , further comprising user-interface logic executable by the at least one processor to perform (a), (b), (c), and (d) responsive to a first motion, wherein the first motion causes the user interface to be displayed.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/302,345 US20150143297A1 (en) | 2011-11-22 | 2011-11-22 | Input detection for a head mounted device |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/302,345 US20150143297A1 (en) | 2011-11-22 | 2011-11-22 | Input detection for a head mounted device |
Publications (1)
Publication Number | Publication Date |
---|---|
US20150143297A1 true US20150143297A1 (en) | 2015-05-21 |
Family
ID=53174590
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/302,345 Abandoned US20150143297A1 (en) | 2011-11-22 | 2011-11-22 | Input detection for a head mounted device |
Country Status (1)
Country | Link |
---|---|
US (1) | US20150143297A1 (en) |
Cited By (73)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20140059472A1 (en) * | 2012-08-24 | 2014-02-27 | Recon Instruments Inc. | Methods and systems for controlling electronic devices |
US20150277122A1 (en) * | 2014-03-28 | 2015-10-01 | Osterhout Group, Inc. | Sensor dependent content position in head worn computing |
US20150346813A1 (en) * | 2014-06-03 | 2015-12-03 | Aaron Michael Vargas | Hands free image viewing on head mounted display |
US20160027414A1 (en) * | 2014-07-22 | 2016-01-28 | Osterhout Group, Inc. | External user interface for head worn computing |
US9377625B2 (en) | 2014-01-21 | 2016-06-28 | Osterhout Group, Inc. | Optical configurations for head worn computing |
US9401540B2 (en) | 2014-02-11 | 2016-07-26 | Osterhout Group, Inc. | Spatial location presentation in head worn computing |
US20160249856A1 (en) * | 2015-02-27 | 2016-09-01 | Quentin S. Miller | Enhanced motion tracking using a transportable inertial sensor |
US9436006B2 (en) | 2014-01-21 | 2016-09-06 | Osterhout Group, Inc. | See-through computer display systems |
US9494800B2 (en) | 2014-01-21 | 2016-11-15 | Osterhout Group, Inc. | See-through computer display systems |
US9523856B2 (en) | 2014-01-21 | 2016-12-20 | Osterhout Group, Inc. | See-through computer display systems |
US9529192B2 (en) | 2014-01-21 | 2016-12-27 | Osterhout Group, Inc. | Eye imaging in head worn computing |
US9529195B2 (en) | 2014-01-21 | 2016-12-27 | Osterhout Group, Inc. | See-through computer display systems |
US9547465B2 (en) | 2014-02-14 | 2017-01-17 | Osterhout Group, Inc. | Object shadowing in head worn computing |
US9575321B2 (en) | 2014-06-09 | 2017-02-21 | Osterhout Group, Inc. | Content presentation in head worn computing |
US20170092002A1 (en) * | 2015-09-30 | 2017-03-30 | Daqri, Llc | User interface for augmented reality system |
US9615742B2 (en) | 2014-01-21 | 2017-04-11 | Osterhout Group, Inc. | Eye imaging in head worn computing |
US9651787B2 (en) | 2014-04-25 | 2017-05-16 | Osterhout Group, Inc. | Speaker assembly for headworn computer |
US9651784B2 (en) | 2014-01-21 | 2017-05-16 | Osterhout Group, Inc. | See-through computer display systems |
US9672210B2 (en) | 2014-04-25 | 2017-06-06 | Osterhout Group, Inc. | Language translation with head-worn computing |
US9671613B2 (en) | 2014-09-26 | 2017-06-06 | Osterhout Group, Inc. | See-through computer display systems |
US9684172B2 (en) | 2014-12-03 | 2017-06-20 | Osterhout Group, Inc. | Head worn computer display systems |
USD792400S1 (en) | 2014-12-31 | 2017-07-18 | Osterhout Group, Inc. | Computer glasses |
CN106970734A (en) * | 2016-01-13 | 2017-07-21 | 阿里巴巴集团控股有限公司 | A kind of task start method and apparatus of display device |
US9715112B2 (en) | 2014-01-21 | 2017-07-25 | Osterhout Group, Inc. | Suppression of stray light in head worn computing |
US9720234B2 (en) | 2014-01-21 | 2017-08-01 | Osterhout Group, Inc. | See-through computer display systems |
USD794637S1 (en) | 2015-01-05 | 2017-08-15 | Osterhout Group, Inc. | Air mouse |
US9740280B2 (en) | 2014-01-21 | 2017-08-22 | Osterhout Group, Inc. | Eye imaging in head worn computing |
US9746686B2 (en) | 2014-05-19 | 2017-08-29 | Osterhout Group, Inc. | Content position calibration in head worn computing |
US9753288B2 (en) | 2014-01-21 | 2017-09-05 | Osterhout Group, Inc. | See-through computer display systems |
US9766463B2 (en) | 2014-01-21 | 2017-09-19 | Osterhout Group, Inc. | See-through computer display systems |
US9779554B2 (en) | 2015-04-10 | 2017-10-03 | Sony Interactive Entertainment Inc. | Filtering and parental control methods for restricting visual activity on a head mounted display |
US9784973B2 (en) | 2014-02-11 | 2017-10-10 | Osterhout Group, Inc. | Micro doppler presentations in head worn computing |
US9811152B2 (en) | 2014-01-21 | 2017-11-07 | Osterhout Group, Inc. | Eye imaging in head worn computing |
US9810906B2 (en) | 2014-06-17 | 2017-11-07 | Osterhout Group, Inc. | External user interface for head worn computing |
US9829707B2 (en) | 2014-08-12 | 2017-11-28 | Osterhout Group, Inc. | Measuring content brightness in head worn computing |
US9836122B2 (en) | 2014-01-21 | 2017-12-05 | Osterhout Group, Inc. | Eye glint imaging in see-through computer display systems |
US9841599B2 (en) | 2014-06-05 | 2017-12-12 | Osterhout Group, Inc. | Optical configurations for head-worn see-through displays |
US9852545B2 (en) | 2014-02-11 | 2017-12-26 | Osterhout Group, Inc. | Spatial location presentation in head worn computing |
US20180033204A1 (en) * | 2016-07-26 | 2018-02-01 | Rouslan Lyubomirov DIMITROV | System and method for displaying computer-based content in a virtual or augmented environment |
US9939646B2 (en) | 2014-01-24 | 2018-04-10 | Osterhout Group, Inc. | Stray light suppression for head worn computing |
US9939934B2 (en) | 2014-01-17 | 2018-04-10 | Osterhout Group, Inc. | External user interface for head worn computing |
US9952664B2 (en) | 2014-01-21 | 2018-04-24 | Osterhout Group, Inc. | Eye imaging in head worn computing |
US9965681B2 (en) | 2008-12-16 | 2018-05-08 | Osterhout Group, Inc. | Eye imaging in head worn computing |
US20180190388A1 (en) * | 2015-06-15 | 2018-07-05 | University Of Maryland, Baltimore | Method and Apparatus to Provide a Virtual Workstation With Enhanced Navigational Efficiency |
US10062182B2 (en) | 2015-02-17 | 2018-08-28 | Osterhout Group, Inc. | See-through computer display systems |
US20180267615A1 (en) * | 2017-03-20 | 2018-09-20 | Daqri, Llc | Gesture-based graphical keyboard for computing devices |
US10139966B2 (en) | 2015-07-22 | 2018-11-27 | Osterhout Group, Inc. | External user interface for head worn computing |
US10152141B1 (en) | 2017-08-18 | 2018-12-11 | Osterhout Group, Inc. | Controller movement tracking with light emitters |
US10191279B2 (en) | 2014-03-17 | 2019-01-29 | Osterhout Group, Inc. | Eye imaging in head worn computing |
US10254856B2 (en) | 2014-01-17 | 2019-04-09 | Osterhout Group, Inc. | External user interface for head worn computing |
US10444018B2 (en) | 2015-02-27 | 2019-10-15 | Microsoft Technology Licensing, Llc | Computer-implemented method to test the sensitivity of a sensor for detecting movement of a tracking device within an established frame of reference of a moving platform |
US10466491B2 (en) | 2016-06-01 | 2019-11-05 | Mentor Acquisition One, Llc | Modular systems for head-worn computers |
US10558050B2 (en) | 2014-01-24 | 2020-02-11 | Mentor Acquisition One, Llc | Haptic systems for head-worn computers |
US10558420B2 (en) | 2014-02-11 | 2020-02-11 | Mentor Acquisition One, Llc | Spatial location presentation in head worn computing |
US10649220B2 (en) | 2014-06-09 | 2020-05-12 | Mentor Acquisition One, Llc | Content presentation in head worn computing |
WO2020098605A1 (en) * | 2018-11-16 | 2020-05-22 | 北京字节跳动网络技术有限公司 | Method and apparatus for displaying menu, device, and storage medium |
US10663740B2 (en) | 2014-06-09 | 2020-05-26 | Mentor Acquisition One, Llc | Content presentation in head worn computing |
US10684478B2 (en) | 2016-05-09 | 2020-06-16 | Mentor Acquisition One, Llc | User interface systems for head-worn computers |
US10684687B2 (en) | 2014-12-03 | 2020-06-16 | Mentor Acquisition One, Llc | See-through computer display systems |
US10824253B2 (en) | 2016-05-09 | 2020-11-03 | Mentor Acquisition One, Llc | User interface systems for head-worn computers |
US10853589B2 (en) | 2014-04-25 | 2020-12-01 | Mentor Acquisition One, Llc | Language translation with head-worn computing |
US10867445B1 (en) * | 2016-11-16 | 2020-12-15 | Amazon Technologies, Inc. | Content segmentation and navigation |
US10942701B2 (en) * | 2016-10-31 | 2021-03-09 | Bragi GmbH | Input and edit functions utilizing accelerometer based earpiece movement system and method |
US11003246B2 (en) | 2015-07-22 | 2021-05-11 | Mentor Acquisition One, Llc | External user interface for head worn computing |
US11103122B2 (en) | 2014-07-15 | 2021-08-31 | Mentor Acquisition One, Llc | Content presentation in head worn computing |
US11104272B2 (en) | 2014-03-28 | 2021-08-31 | Mentor Acquisition One, Llc | System for assisted operator safety using an HMD |
US11227294B2 (en) | 2014-04-03 | 2022-01-18 | Mentor Acquisition One, Llc | Sight information collection in head worn computing |
US11269182B2 (en) | 2014-07-15 | 2022-03-08 | Mentor Acquisition One, Llc | Content presentation in head worn computing |
US11314082B2 (en) * | 2017-09-26 | 2022-04-26 | Sony Interactive Entertainment Inc. | Motion signal generation |
US11487110B2 (en) | 2014-01-21 | 2022-11-01 | Mentor Acquisition One, Llc | Eye imaging in head worn computing |
US11669163B2 (en) | 2014-01-21 | 2023-06-06 | Mentor Acquisition One, Llc | Eye glint imaging in see-through computer display systems |
US11737666B2 (en) | 2014-01-21 | 2023-08-29 | Mentor Acquisition One, Llc | Eye imaging in head worn computing |
US11892644B2 (en) | 2014-01-21 | 2024-02-06 | Mentor Acquisition One, Llc | See-through computer display systems |
Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20100026629A1 (en) * | 2000-12-19 | 2010-02-04 | Cisco Technology, Inc. | Device, Methods, and User Interface for Providing Optimized Entry of Alphanumeric Text |
-
2011
- 2011-11-22 US US13/302,345 patent/US20150143297A1/en not_active Abandoned
Patent Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20100026629A1 (en) * | 2000-12-19 | 2010-02-04 | Cisco Technology, Inc. | Device, Methods, and User Interface for Providing Optimized Entry of Alphanumeric Text |
Cited By (162)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9965681B2 (en) | 2008-12-16 | 2018-05-08 | Osterhout Group, Inc. | Eye imaging in head worn computing |
US20140059472A1 (en) * | 2012-08-24 | 2014-02-27 | Recon Instruments Inc. | Methods and systems for controlling electronic devices |
US11169623B2 (en) | 2014-01-17 | 2021-11-09 | Mentor Acquisition One, Llc | External user interface for head worn computing |
US11507208B2 (en) | 2014-01-17 | 2022-11-22 | Mentor Acquisition One, Llc | External user interface for head worn computing |
US9939934B2 (en) | 2014-01-17 | 2018-04-10 | Osterhout Group, Inc. | External user interface for head worn computing |
US11782529B2 (en) | 2014-01-17 | 2023-10-10 | Mentor Acquisition One, Llc | External user interface for head worn computing |
US10254856B2 (en) | 2014-01-17 | 2019-04-09 | Osterhout Group, Inc. | External user interface for head worn computing |
US11231817B2 (en) | 2014-01-17 | 2022-01-25 | Mentor Acquisition One, Llc | External user interface for head worn computing |
US9720227B2 (en) | 2014-01-21 | 2017-08-01 | Osterhout Group, Inc. | See-through computer display systems |
US9529195B2 (en) | 2014-01-21 | 2016-12-27 | Osterhout Group, Inc. | See-through computer display systems |
US9494800B2 (en) | 2014-01-21 | 2016-11-15 | Osterhout Group, Inc. | See-through computer display systems |
US9746676B2 (en) | 2014-01-21 | 2017-08-29 | Osterhout Group, Inc. | See-through computer display systems |
US9529192B2 (en) | 2014-01-21 | 2016-12-27 | Osterhout Group, Inc. | Eye imaging in head worn computing |
US10698223B2 (en) | 2014-01-21 | 2020-06-30 | Mentor Acquisition One, Llc | See-through computer display systems |
US9529199B2 (en) | 2014-01-21 | 2016-12-27 | Osterhout Group, Inc. | See-through computer display systems |
US10139632B2 (en) | 2014-01-21 | 2018-11-27 | Osterhout Group, Inc. | See-through computer display systems |
US11054902B2 (en) | 2014-01-21 | 2021-07-06 | Mentor Acquisition One, Llc | Eye glint imaging in see-through computer display systems |
US9594246B2 (en) | 2014-01-21 | 2017-03-14 | Osterhout Group, Inc. | See-through computer display systems |
US11126003B2 (en) | 2014-01-21 | 2021-09-21 | Mentor Acquisition One, Llc | See-through computer display systems |
US9615742B2 (en) | 2014-01-21 | 2017-04-11 | Osterhout Group, Inc. | Eye imaging in head worn computing |
US9651783B2 (en) | 2014-01-21 | 2017-05-16 | Osterhout Group, Inc. | See-through computer display systems |
US9651788B2 (en) | 2014-01-21 | 2017-05-16 | Osterhout Group, Inc. | See-through computer display systems |
US11892644B2 (en) | 2014-01-21 | 2024-02-06 | Mentor Acquisition One, Llc | See-through computer display systems |
US9651789B2 (en) | 2014-01-21 | 2017-05-16 | Osterhout Group, Inc. | See-Through computer display systems |
US9651784B2 (en) | 2014-01-21 | 2017-05-16 | Osterhout Group, Inc. | See-through computer display systems |
US9658458B2 (en) | 2014-01-21 | 2017-05-23 | Osterhout Group, Inc. | See-through computer display systems |
US9658457B2 (en) | 2014-01-21 | 2017-05-23 | Osterhout Group, Inc. | See-through computer display systems |
US11487110B2 (en) | 2014-01-21 | 2022-11-01 | Mentor Acquisition One, Llc | Eye imaging in head worn computing |
US11622426B2 (en) | 2014-01-21 | 2023-04-04 | Mentor Acquisition One, Llc | See-through computer display systems |
US9684165B2 (en) | 2014-01-21 | 2017-06-20 | Osterhout Group, Inc. | Eye imaging in head worn computing |
US11619820B2 (en) | 2014-01-21 | 2023-04-04 | Mentor Acquisition One, Llc | See-through computer display systems |
US9684171B2 (en) | 2014-01-21 | 2017-06-20 | Osterhout Group, Inc. | See-through computer display systems |
US11796805B2 (en) | 2014-01-21 | 2023-10-24 | Mentor Acquisition One, Llc | Eye imaging in head worn computing |
US11669163B2 (en) | 2014-01-21 | 2023-06-06 | Mentor Acquisition One, Llc | Eye glint imaging in see-through computer display systems |
US9715112B2 (en) | 2014-01-21 | 2017-07-25 | Osterhout Group, Inc. | Suppression of stray light in head worn computing |
US10001644B2 (en) | 2014-01-21 | 2018-06-19 | Osterhout Group, Inc. | See-through computer display systems |
US9720235B2 (en) | 2014-01-21 | 2017-08-01 | Osterhout Group, Inc. | See-through computer display systems |
US9720234B2 (en) | 2014-01-21 | 2017-08-01 | Osterhout Group, Inc. | See-through computer display systems |
US9377625B2 (en) | 2014-01-21 | 2016-06-28 | Osterhout Group, Inc. | Optical configurations for head worn computing |
US10866420B2 (en) | 2014-01-21 | 2020-12-15 | Mentor Acquisition One, Llc | See-through computer display systems |
US9740012B2 (en) | 2014-01-21 | 2017-08-22 | Osterhout Group, Inc. | See-through computer display systems |
US9740280B2 (en) | 2014-01-21 | 2017-08-22 | Osterhout Group, Inc. | Eye imaging in head worn computing |
US9523856B2 (en) | 2014-01-21 | 2016-12-20 | Osterhout Group, Inc. | See-through computer display systems |
US9753288B2 (en) | 2014-01-21 | 2017-09-05 | Osterhout Group, Inc. | See-through computer display systems |
US9436006B2 (en) | 2014-01-21 | 2016-09-06 | Osterhout Group, Inc. | See-through computer display systems |
US9766463B2 (en) | 2014-01-21 | 2017-09-19 | Osterhout Group, Inc. | See-through computer display systems |
US9772492B2 (en) | 2014-01-21 | 2017-09-26 | Osterhout Group, Inc. | Eye imaging in head worn computing |
US11947126B2 (en) | 2014-01-21 | 2024-04-02 | Mentor Acquisition One, Llc | See-through computer display systems |
US11099380B2 (en) | 2014-01-21 | 2021-08-24 | Mentor Acquisition One, Llc | Eye imaging in head worn computing |
US9811159B2 (en) | 2014-01-21 | 2017-11-07 | Osterhout Group, Inc. | Eye imaging in head worn computing |
US9811152B2 (en) | 2014-01-21 | 2017-11-07 | Osterhout Group, Inc. | Eye imaging in head worn computing |
US11353957B2 (en) | 2014-01-21 | 2022-06-07 | Mentor Acquisition One, Llc | Eye glint imaging in see-through computer display systems |
US9829703B2 (en) | 2014-01-21 | 2017-11-28 | Osterhout Group, Inc. | Eye imaging in head worn computing |
US11737666B2 (en) | 2014-01-21 | 2023-08-29 | Mentor Acquisition One, Llc | Eye imaging in head worn computing |
US9836122B2 (en) | 2014-01-21 | 2017-12-05 | Osterhout Group, Inc. | Eye glint imaging in see-through computer display systems |
US10579140B2 (en) | 2014-01-21 | 2020-03-03 | Mentor Acquisition One, Llc | Eye glint imaging in see-through computer display systems |
US9958674B2 (en) | 2014-01-21 | 2018-05-01 | Osterhout Group, Inc. | Eye imaging in head worn computing |
US9952664B2 (en) | 2014-01-21 | 2018-04-24 | Osterhout Group, Inc. | Eye imaging in head worn computing |
US11103132B2 (en) | 2014-01-21 | 2021-08-31 | Mentor Acquisition One, Llc | Eye imaging in head worn computing |
US9933622B2 (en) | 2014-01-21 | 2018-04-03 | Osterhout Group, Inc. | See-through computer display systems |
US9885868B2 (en) | 2014-01-21 | 2018-02-06 | Osterhout Group, Inc. | Eye imaging in head worn computing |
US9927612B2 (en) | 2014-01-21 | 2018-03-27 | Osterhout Group, Inc. | See-through computer display systems |
US9939646B2 (en) | 2014-01-24 | 2018-04-10 | Osterhout Group, Inc. | Stray light suppression for head worn computing |
US11822090B2 (en) | 2014-01-24 | 2023-11-21 | Mentor Acquisition One, Llc | Haptic systems for head-worn computers |
US10558050B2 (en) | 2014-01-24 | 2020-02-11 | Mentor Acquisition One, Llc | Haptic systems for head-worn computers |
US10558420B2 (en) | 2014-02-11 | 2020-02-11 | Mentor Acquisition One, Llc | Spatial location presentation in head worn computing |
US9843093B2 (en) | 2014-02-11 | 2017-12-12 | Osterhout Group, Inc. | Spatial location presentation in head worn computing |
US9784973B2 (en) | 2014-02-11 | 2017-10-10 | Osterhout Group, Inc. | Micro doppler presentations in head worn computing |
US9841602B2 (en) | 2014-02-11 | 2017-12-12 | Osterhout Group, Inc. | Location indicating avatar in head worn computing |
US9852545B2 (en) | 2014-02-11 | 2017-12-26 | Osterhout Group, Inc. | Spatial location presentation in head worn computing |
US9401540B2 (en) | 2014-02-11 | 2016-07-26 | Osterhout Group, Inc. | Spatial location presentation in head worn computing |
US11599326B2 (en) | 2014-02-11 | 2023-03-07 | Mentor Acquisition One, Llc | Spatial location presentation in head worn computing |
US9547465B2 (en) | 2014-02-14 | 2017-01-17 | Osterhout Group, Inc. | Object shadowing in head worn computing |
US9928019B2 (en) | 2014-02-14 | 2018-03-27 | Osterhout Group, Inc. | Object shadowing in head worn computing |
US10191279B2 (en) | 2014-03-17 | 2019-01-29 | Osterhout Group, Inc. | Eye imaging in head worn computing |
US11104272B2 (en) | 2014-03-28 | 2021-08-31 | Mentor Acquisition One, Llc | System for assisted operator safety using an HMD |
US20150277122A1 (en) * | 2014-03-28 | 2015-10-01 | Osterhout Group, Inc. | Sensor dependent content position in head worn computing |
US9423612B2 (en) | 2014-03-28 | 2016-08-23 | Osterhout Group, Inc. | Sensor dependent content position in head worn computing |
US11227294B2 (en) | 2014-04-03 | 2022-01-18 | Mentor Acquisition One, Llc | Sight information collection in head worn computing |
US9672210B2 (en) | 2014-04-25 | 2017-06-06 | Osterhout Group, Inc. | Language translation with head-worn computing |
US11474360B2 (en) | 2014-04-25 | 2022-10-18 | Mentor Acquisition One, Llc | Speaker assembly for headworn computer |
US11727223B2 (en) | 2014-04-25 | 2023-08-15 | Mentor Acquisition One, Llc | Language translation with head-worn computing |
US10634922B2 (en) | 2014-04-25 | 2020-04-28 | Mentor Acquisition One, Llc | Speaker assembly for headworn computer |
US9651787B2 (en) | 2014-04-25 | 2017-05-16 | Osterhout Group, Inc. | Speaker assembly for headworn computer |
US11880041B2 (en) | 2014-04-25 | 2024-01-23 | Mentor Acquisition One, Llc | Speaker assembly for headworn computer |
US10853589B2 (en) | 2014-04-25 | 2020-12-01 | Mentor Acquisition One, Llc | Language translation with head-worn computing |
US9746686B2 (en) | 2014-05-19 | 2017-08-29 | Osterhout Group, Inc. | Content position calibration in head worn computing |
US20150346813A1 (en) * | 2014-06-03 | 2015-12-03 | Aaron Michael Vargas | Hands free image viewing on head mounted display |
US11402639B2 (en) | 2014-06-05 | 2022-08-02 | Mentor Acquisition One, Llc | Optical configurations for head-worn see-through displays |
US9841599B2 (en) | 2014-06-05 | 2017-12-12 | Osterhout Group, Inc. | Optical configurations for head-worn see-through displays |
US11960089B2 (en) | 2014-06-05 | 2024-04-16 | Mentor Acquisition One, Llc | Optical configurations for head-worn see-through displays |
US10877270B2 (en) | 2014-06-05 | 2020-12-29 | Mentor Acquisition One, Llc | Optical configurations for head-worn see-through displays |
US10976559B2 (en) | 2014-06-09 | 2021-04-13 | Mentor Acquisition One, Llc | Content presentation in head worn computing |
US10139635B2 (en) | 2014-06-09 | 2018-11-27 | Osterhout Group, Inc. | Content presentation in head worn computing |
US11663794B2 (en) | 2014-06-09 | 2023-05-30 | Mentor Acquisition One, Llc | Content presentation in head worn computing |
US11327323B2 (en) | 2014-06-09 | 2022-05-10 | Mentor Acquisition One, Llc | Content presentation in head worn computing |
US11790617B2 (en) | 2014-06-09 | 2023-10-17 | Mentor Acquisition One, Llc | Content presentation in head worn computing |
US9720241B2 (en) | 2014-06-09 | 2017-08-01 | Osterhout Group, Inc. | Content presentation in head worn computing |
US10649220B2 (en) | 2014-06-09 | 2020-05-12 | Mentor Acquisition One, Llc | Content presentation in head worn computing |
US9575321B2 (en) | 2014-06-09 | 2017-02-21 | Osterhout Group, Inc. | Content presentation in head worn computing |
US10663740B2 (en) | 2014-06-09 | 2020-05-26 | Mentor Acquisition One, Llc | Content presentation in head worn computing |
US11022810B2 (en) | 2014-06-09 | 2021-06-01 | Mentor Acquisition One, Llc | Content presentation in head worn computing |
US11360318B2 (en) | 2014-06-09 | 2022-06-14 | Mentor Acquisition One, Llc | Content presentation in head worn computing |
US11887265B2 (en) | 2014-06-09 | 2024-01-30 | Mentor Acquisition One, Llc | Content presentation in head worn computing |
US10698212B2 (en) | 2014-06-17 | 2020-06-30 | Mentor Acquisition One, Llc | External user interface for head worn computing |
US11789267B2 (en) | 2014-06-17 | 2023-10-17 | Mentor Acquisition One, Llc | External user interface for head worn computing |
US11054645B2 (en) | 2014-06-17 | 2021-07-06 | Mentor Acquisition One, Llc | External user interface for head worn computing |
US9810906B2 (en) | 2014-06-17 | 2017-11-07 | Osterhout Group, Inc. | External user interface for head worn computing |
US11294180B2 (en) | 2014-06-17 | 2022-04-05 | Mentor Acquisition One, Llc | External user interface for head worn computing |
US11269182B2 (en) | 2014-07-15 | 2022-03-08 | Mentor Acquisition One, Llc | Content presentation in head worn computing |
US11786105B2 (en) | 2014-07-15 | 2023-10-17 | Mentor Acquisition One, Llc | Content presentation in head worn computing |
US11103122B2 (en) | 2014-07-15 | 2021-08-31 | Mentor Acquisition One, Llc | Content presentation in head worn computing |
US20160027414A1 (en) * | 2014-07-22 | 2016-01-28 | Osterhout Group, Inc. | External user interface for head worn computing |
US20160025974A1 (en) * | 2014-07-22 | 2016-01-28 | Osterhout Group, Inc. | External user interface for head worn computing |
US11630315B2 (en) | 2014-08-12 | 2023-04-18 | Mentor Acquisition One, Llc | Measuring content brightness in head worn computing |
US11360314B2 (en) | 2014-08-12 | 2022-06-14 | Mentor Acquisition One, Llc | Measuring content brightness in head worn computing |
US10908422B2 (en) | 2014-08-12 | 2021-02-02 | Mentor Acquisition One, Llc | Measuring content brightness in head worn computing |
US9829707B2 (en) | 2014-08-12 | 2017-11-28 | Osterhout Group, Inc. | Measuring content brightness in head worn computing |
US9671613B2 (en) | 2014-09-26 | 2017-06-06 | Osterhout Group, Inc. | See-through computer display systems |
US9684172B2 (en) | 2014-12-03 | 2017-06-20 | Osterhout Group, Inc. | Head worn computer display systems |
US10684687B2 (en) | 2014-12-03 | 2020-06-16 | Mentor Acquisition One, Llc | See-through computer display systems |
US11262846B2 (en) | 2014-12-03 | 2022-03-01 | Mentor Acquisition One, Llc | See-through computer display systems |
US11809628B2 (en) | 2014-12-03 | 2023-11-07 | Mentor Acquisition One, Llc | See-through computer display systems |
USD792400S1 (en) | 2014-12-31 | 2017-07-18 | Osterhout Group, Inc. | Computer glasses |
USD794637S1 (en) | 2015-01-05 | 2017-08-15 | Osterhout Group, Inc. | Air mouse |
US10062182B2 (en) | 2015-02-17 | 2018-08-28 | Osterhout Group, Inc. | See-through computer display systems |
US10444018B2 (en) | 2015-02-27 | 2019-10-15 | Microsoft Technology Licensing, Llc | Computer-implemented method to test the sensitivity of a sensor for detecting movement of a tracking device within an established frame of reference of a moving platform |
US10111620B2 (en) * | 2015-02-27 | 2018-10-30 | Microsoft Technology Licensing, Llc | Enhanced motion tracking using transportable inertial sensors to determine that a frame of reference is established |
US20160249856A1 (en) * | 2015-02-27 | 2016-09-01 | Quentin S. Miller | Enhanced motion tracking using a transportable inertial sensor |
US9779554B2 (en) | 2015-04-10 | 2017-10-03 | Sony Interactive Entertainment Inc. | Filtering and parental control methods for restricting visual activity on a head mounted display |
US20180190388A1 (en) * | 2015-06-15 | 2018-07-05 | University Of Maryland, Baltimore | Method and Apparatus to Provide a Virtual Workstation With Enhanced Navigational Efficiency |
US10139966B2 (en) | 2015-07-22 | 2018-11-27 | Osterhout Group, Inc. | External user interface for head worn computing |
US11003246B2 (en) | 2015-07-22 | 2021-05-11 | Mentor Acquisition One, Llc | External user interface for head worn computing |
US11886638B2 (en) | 2015-07-22 | 2024-01-30 | Mentor Acquisition One, Llc | External user interface for head worn computing |
US11209939B2 (en) | 2015-07-22 | 2021-12-28 | Mentor Acquisition One, Llc | External user interface for head worn computing |
US11816296B2 (en) | 2015-07-22 | 2023-11-14 | Mentor Acquisition One, Llc | External user interface for head worn computing |
US20170092002A1 (en) * | 2015-09-30 | 2017-03-30 | Daqri, Llc | User interface for augmented reality system |
CN106970734A (en) * | 2016-01-13 | 2017-07-21 | 阿里巴巴集团控股有限公司 | A kind of task start method and apparatus of display device |
US10684478B2 (en) | 2016-05-09 | 2020-06-16 | Mentor Acquisition One, Llc | User interface systems for head-worn computers |
US11226691B2 (en) | 2016-05-09 | 2022-01-18 | Mentor Acquisition One, Llc | User interface systems for head-worn computers |
US11500212B2 (en) | 2016-05-09 | 2022-11-15 | Mentor Acquisition One, Llc | User interface systems for head-worn computers |
US10824253B2 (en) | 2016-05-09 | 2020-11-03 | Mentor Acquisition One, Llc | User interface systems for head-worn computers |
US11320656B2 (en) | 2016-05-09 | 2022-05-03 | Mentor Acquisition One, Llc | User interface systems for head-worn computers |
US11460708B2 (en) | 2016-06-01 | 2022-10-04 | Mentor Acquisition One, Llc | Modular systems for head-worn computers |
US11754845B2 (en) | 2016-06-01 | 2023-09-12 | Mentor Acquisition One, Llc | Modular systems for head-worn computers |
US11977238B2 (en) | 2016-06-01 | 2024-05-07 | Mentor Acquisition One, Llc | Modular systems for head-worn computers |
US11022808B2 (en) | 2016-06-01 | 2021-06-01 | Mentor Acquisition One, Llc | Modular systems for head-worn computers |
US11586048B2 (en) | 2016-06-01 | 2023-02-21 | Mentor Acquisition One, Llc | Modular systems for head-worn computers |
US10466491B2 (en) | 2016-06-01 | 2019-11-05 | Mentor Acquisition One, Llc | Modular systems for head-worn computers |
US20180033204A1 (en) * | 2016-07-26 | 2018-02-01 | Rouslan Lyubomirov DIMITROV | System and method for displaying computer-based content in a virtual or augmented environment |
US10489978B2 (en) * | 2016-07-26 | 2019-11-26 | Rouslan Lyubomirov DIMITROV | System and method for displaying computer-based content in a virtual or augmented environment |
US10942701B2 (en) * | 2016-10-31 | 2021-03-09 | Bragi GmbH | Input and edit functions utilizing accelerometer based earpiece movement system and method |
US11599333B2 (en) | 2016-10-31 | 2023-03-07 | Bragi GmbH | Input and edit functions utilizing accelerometer based earpiece movement system and method |
US11947874B2 (en) | 2016-10-31 | 2024-04-02 | Bragi GmbH | Input and edit functions utilizing accelerometer based earpiece movement system and method |
US10867445B1 (en) * | 2016-11-16 | 2020-12-15 | Amazon Technologies, Inc. | Content segmentation and navigation |
US20180267615A1 (en) * | 2017-03-20 | 2018-09-20 | Daqri, Llc | Gesture-based graphical keyboard for computing devices |
US11474619B2 (en) | 2017-08-18 | 2022-10-18 | Mentor Acquisition One, Llc | Controller movement tracking with light emitters |
US10152141B1 (en) | 2017-08-18 | 2018-12-11 | Osterhout Group, Inc. | Controller movement tracking with light emitters |
US11947735B2 (en) | 2017-08-18 | 2024-04-02 | Mentor Acquisition One, Llc | Controller movement tracking with light emitters |
US11079858B2 (en) | 2017-08-18 | 2021-08-03 | Mentor Acquisition One, Llc | Controller movement tracking with light emitters |
US11314082B2 (en) * | 2017-09-26 | 2022-04-26 | Sony Interactive Entertainment Inc. | Motion signal generation |
WO2020098605A1 (en) * | 2018-11-16 | 2020-05-22 | 北京字节跳动网络技术有限公司 | Method and apparatus for displaying menu, device, and storage medium |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US20150143297A1 (en) | Input detection for a head mounted device | |
US20190011982A1 (en) | Graphical Interface Having Adjustable Borders | |
US8866852B2 (en) | Method and system for input detection | |
US8643951B1 (en) | Graphical menu and interaction therewith through a viewing window | |
US9035878B1 (en) | Input system | |
US9552676B2 (en) | Wearable computer with nearby object response | |
US9058054B2 (en) | Image capture apparatus | |
US20130246967A1 (en) | Head-Tracked User Interaction with Graphical Interface | |
US10330940B1 (en) | Content display methods | |
US10379346B2 (en) | Methods and devices for rendering interactions between virtual and physical objects on a substantially transparent display | |
US20130117707A1 (en) | Velocity-Based Triggering | |
US20160011724A1 (en) | Hands-Free Selection Using a Ring-Based User-Interface | |
US8745058B1 (en) | Dynamic data item searching | |
US20150199081A1 (en) | Re-centering a user interface | |
US10114466B2 (en) | Methods and systems for hands-free browsing in a wearable computing device | |
US8922481B1 (en) | Content annotation | |
US9448687B1 (en) | Zoomable/translatable browser interface for a head mounted device | |
US20130007672A1 (en) | Methods and Systems for Correlating Head Movement with Items Displayed on a User Interface | |
US8799810B1 (en) | Stability region for a user interface | |
US20130021269A1 (en) | Dynamic Control of an Active Input Region of a User Interface | |
US11200869B1 (en) | Wearable display system for portable computing devices | |
US20150185971A1 (en) | Ring-Based User-Interface | |
US20150193098A1 (en) | Yes or No User-Interface | |
US9153043B1 (en) | Systems and methods for providing a user interface in a field of view of a media item | |
US9547406B1 (en) | Velocity-based triggering |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:WHEELER, AARON;THRUN, SEBASTIAN;REEL/FRAME:027270/0727Effective date: 20111122 |
|
STCB | Information on status: application discontinuation |
Free format text: ABANDONED -- FAILURE TO RESPOND TO AN OFFICE ACTION |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044142/0357Effective date: 20170929 |