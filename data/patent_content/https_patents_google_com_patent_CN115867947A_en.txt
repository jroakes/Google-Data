CN115867947A - Transmitter network for determining robot actions - Google Patents
Transmitter network for determining robot actions Download PDFInfo
- Publication number
- CN115867947A CN115867947A CN202080102599.3A CN202080102599A CN115867947A CN 115867947 A CN115867947 A CN 115867947A CN 202080102599 A CN202080102599 A CN 202080102599A CN 115867947 A CN115867947 A CN 115867947A
- Authority
- CN
- China
- Prior art keywords
- action
- embedding
- observation data
- model
- data
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- B—PERFORMING OPERATIONS; TRANSPORTING
- B25—HAND TOOLS; PORTABLE POWER-DRIVEN TOOLS; MANIPULATORS
- B25J—MANIPULATORS; CHAMBERS PROVIDED WITH MANIPULATION DEVICES
- B25J9/00—Programme-controlled manipulators
- B25J9/16—Programme controls
- B25J9/1628—Programme controls characterised by the control loop
- B25J9/163—Programme controls characterised by the control loop learning, adaptive, model based, rule based expert control
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/60—Type of objects
- G06V20/64—Three-dimensional objects
-
- B—PERFORMING OPERATIONS; TRANSPORTING
- B25—HAND TOOLS; PORTABLE POWER-DRIVEN TOOLS; MANIPULATORS
- B25J—MANIPULATORS; CHAMBERS PROVIDED WITH MANIPULATION DEVICES
- B25J19/00—Accessories fitted to manipulators, e.g. for monitoring, for viewing; Safety devices combined with or specially adapted for use in connection with manipulators
- B25J19/02—Sensing devices
- B25J19/021—Optical sensing devices
- B25J19/022—Optical sensing devices using lasers
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/24—Classification techniques
- G06F18/241—Classification techniques relating to the classification model, e.g. parametric or non-parametric approaches
- G06F18/2413—Classification techniques relating to the classification model, e.g. parametric or non-parametric approaches based on distances to training or reference patterns
- G06F18/24133—Distances to prototypes
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/24—Classification techniques
- G06F18/241—Classification techniques relating to the classification model, e.g. parametric or non-parametric approaches
- G06F18/2415—Classification techniques relating to the classification model, e.g. parametric or non-parametric approaches based on parametric or probabilistic models, e.g. based on likelihood ratio or false acceptance rate versus a false rejection rate
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/10—Terrestrial scenes
-
- B—PERFORMING OPERATIONS; TRANSPORTING
- B25—HAND TOOLS; PORTABLE POWER-DRIVEN TOOLS; MANIPULATORS
- B25J—MANIPULATORS; CHAMBERS PROVIDED WITH MANIPULATION DEVICES
- B25J9/00—Programme-controlled manipulators
- B25J9/16—Programme controls
- B25J9/1602—Programme controls characterised by the control system, structure, architecture
- B25J9/161—Hardware, e.g. neural networks, fuzzy logic, interfaces, processor
-
- B—PERFORMING OPERATIONS; TRANSPORTING
- B25—HAND TOOLS; PORTABLE POWER-DRIVEN TOOLS; MANIPULATORS
- B25J—MANIPULATORS; CHAMBERS PROVIDED WITH MANIPULATION DEVICES
- B25J9/00—Programme-controlled manipulators
- B25J9/16—Programme controls
- B25J9/1694—Programme controls characterised by use of sensors other than normal servo-feedback from position, speed or acceleration sensors, perception control, multi-sensor controlled systems, sensor fusion
- B25J9/1697—Vision controlled systems
-
- G—PHYSICS
- G05—CONTROLLING; REGULATING
- G05B—CONTROL OR REGULATING SYSTEMS IN GENERAL; FUNCTIONAL ELEMENTS OF SUCH SYSTEMS; MONITORING OR TESTING ARRANGEMENTS FOR SUCH SYSTEMS OR ELEMENTS
- G05B2219/00—Program-control systems
- G05B2219/30—Nc systems
- G05B2219/40—Robotics, robotics mapping to robotics vision
- G05B2219/40532—Ann for vision processing
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V2201/00—Indexing scheme relating to image or video recognition or understanding
- G06V2201/12—Acquisition of 3D measurements of objects
Abstract
A conveyor network that determines robot motion based on sensor feedback may provide efficient autonomous movement for the robot. The transmitter network can take advantage of spatial symmetry and does not require the assumption of object metrics to provide accurate object manipulation instructions. The machine learning model of the transmitter network may also allow various tasks to be learned with fewer training examples than other machine learning models. A machine learning model of the conveyor network may ingest the observation data as input and may output an action in response to the processed observation data.
Description
Technical Field
The present disclosure relates generally to robotic operations. More particularly, the present disclosure relates to determining robot actions using machine learning.
Background
The field of robotics includes the design, construction, operation and application of robots, as well as computer systems for their control, sensory feedback and information processing. In many cases, a robot's steering or control plan can be formulated to induce a series of spatial displacements: wherein the space being moved may contain one or more objects or end effectors (e.g., a controlled robot).
Machine learning techniques have proven particularly advantageous for robotic manipulation or other forms of robot control planning. For example, machine learning techniques, such as end-to-end models that map directly from pixels to robot actions, have the ability to learn complex manipulation techniques. However, it is well known that existing models that map directly from pixels to robot motion require a large amount of data and are therefore difficult to apply in situations where little or no training data is available or where obtaining training data is difficult or expensive.
As an alternative approach, some machine learning approaches for robotic manipulation integrate object-centric assumptions about objects within the scene (e.g., object keypoints, embeddings, or dense descriptors). This object-centric approach has been shown to improve sampling efficiency. However, these object representations often burden data collection and still have difficulty resolving difficult scenes with unseen classes of objects, occluded objects, highly deformable objects, or heaps of small objects. The large data collection required can slow down processing speed and can become a computational burden. Furthermore, the robustness of the model may be diminished due to the rigidification of the characterization constraints.
Disclosure of Invention
Aspects and advantages of embodiments of the present disclosure will be set forth in part in the description which follows, or may be learned by practice of the embodiments.
One example aspect of the present disclosure relates to a computer-implemented method for generating actions of a robot. A computer-implemented method for generating actions of a robot may include: observation data is obtained, wherein the observation data includes data describing an environment. The computer-implemented method may include: a first action of the robot is determined based at least in part on the observation data, and the observation data is processed with a first embedding model to generate a first feature embedding. The computer-implemented method may include: the observation data is processed with a second embedding model to generate a second feature embedding, wherein the second embedding model is conditioned on the first action. The computer-implemented method may include: a second action of the robot is determined based at least in part on the comparison of the first feature embedding to the second feature embedding.
Another example aspect of the present disclosure is directed to a computer system. The computer system may include one or more processors and one or more non-transitory computer-readable media that collectively store instructions that, when executed by the one or more processors, cause the computing system to perform operations. The operations may include: observation data is obtained, wherein the observation data includes data describing an environment. The operations may include: a first action of the robot is determined and the observation data is processed with a first embedding model to generate a first feature embedding. In some embodiments, the operations may include: the observation data is processed with a second embedding model to generate a second feature embedding, wherein the second embedding model is conditioned on the first action. The operations may include: a second action of the robot is determined based at least in part on the comparison of the first feature embedding to the second feature embedding. The operations may include: an evaluation penalty function that compares one or both of the first action and the second action to one or both of a first ground true (ground true) action and a second ground true (ground true) action; and modifying one or more values of one or more parameters of one or both of the first and second embedding models based at least in part on the loss function.
Another example aspect of the present disclosure relates to a robotic device. The robotic device may include one or more non-transitory computer-readable media collectively storing instructions that, when executed by one or more computing devices, cause the one or more computing devices to perform operations. In some embodiments, the operations comprise: observation data is obtained, wherein the observation data includes data describing an environment. The operations may include: a first action of the robotic device is determined and the observation data is processed with a first embedding model to generate a first feature embedding. The operations may include: the observation data is processed with a second embedding model to generate a second feature embedding, wherein the second embedding model is conditioned on the first action. The operations may include: a second action of the robotic device is determined based at least in part on the comparison of the first feature embedding to the second feature embedding. The operations may further include: the first action and the second action are performed by the robotic device.
Other aspects of the disclosure relate to various systems, apparatuses, non-transitory computer-readable media, user interfaces, and electronic devices.
These and other features, aspects, and advantages of various embodiments of the present disclosure will become better understood with reference to the following description and appended claims. The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate exemplary embodiments of the disclosure and together with the description, serve to explain the relevant principles.
Drawings
A detailed discussion of embodiments directed to one of ordinary skill in the art is set forth in the specification with reference to the drawings, in which:
fig. 1A depicts a block diagram of an example computing system, according to an example embodiment of the present disclosure.
Fig. 1B depicts a block diagram of an example computing device, according to an example embodiment of the present disclosure.
Fig. 1C depicts a block diagram of an example computing system, according to an example embodiment of the present disclosure.
Fig. 2 depicts a block diagram of an example network process for determining actions of a robot, according to an example embodiment of the present disclosure.
Fig. 3 depicts a block diagram of an example network process for training a system to determine actions of a robot, according to an example embodiment of the present disclosure.
Fig. 4 depicts a block diagram of an example network process for determining actions of a robot, according to an example embodiment of the present disclosure.
Fig. 5 depicts a flowchart of an example method for determining an action of a robot, according to an example embodiment of the present disclosure.
Fig. 6 depicts a flowchart of an example method for determining an action of a robot, according to an example embodiment of the present disclosure.
Fig. 7 depicts a flowchart of an example method for training a system to determine an action of a robot, according to an example embodiment of the present disclosure.
Reference numerals repeated among the figures are intended to identify identical features in the various embodiments.
Detailed Description
SUMMARY
In general, this disclosure describes systems and methods for determining actions of a robot based on observations of an environment, such as images of the environment. In particular, the systems and methods presented herein may utilize an end-to-end machine learning conveyor network that may learn to infer robot actions from observed data, such as visual inputs. In particular, the system may utilize depth feature template matching to infer spatial displacements from visual input, which may parameterize robot motion. The proposed model avoids the assumption of relying on object metrics (objects), e.g. typical poses, models or key points, exploits spatial symmetry and has a sampling efficiency several orders of magnitude higher than the alternatives of the example benchmarks when learning a vision-based manipulation task: assembling the complete equipment from the cones of the stacked blocks to the objects which are not seen; from manipulating the deformable tether to pushing the stack of small objects with closed loop feedback. The proposed system and method can represent a complex multi-modal policy distribution and can be generalized to multi-step sequential tasks as well as 6DoF tasks, such as 6DoF pick and place. Experiments on various simulation tasks show that the proposed model learns faster and more generalized than various end-to-end baselines, including strategies that use ground truth object poses.
Accordingly, aspects of the present disclosure relate to an end-to-end model architecture (which may be referred to as a "transport network," for example) that preserves the spatial structure of vision-based manipulation without an object-centric assumption. The proposed model and associated system provide improved learning efficiency over existing pixel-based methods, but do not bring any of the limitations or burdens of explicit object representation.
In particular, manipulation often involves rearranging things, which can be considered as performing a series of spatial displacements: wherein the space being moved (e.g., conveyed) may include an object or an end effector. The proposed method can express the visual representation of the manipulation as estimating these displacements. The transmitter network can directly optimize this by learning to focus on local regions and predicting their target spatial displacement via depth feature template matching — then parameterize the robot action to do the steering. This expression enables high-level perceptual reasoning, which relates to which visual cues are important, and how they should be rearranged in the scene, the distribution of which can be learned from the presentation.
The example transmitter network may also preserve the 3D spatial structure of the visual input. For example, previous end-to-end models often use a convolution architecture with the original image, where valuable spatial information may be lost due to perspective distortion. An exemplary embodiment of the proposed method projects visual data onto a spatially consistent representation using 3D information, with which the equivalence of inductive biases present in data symmetry can be better exploited for more efficient learning.
Experimentally, the proposed exemplary embodiment of the conveyor network exhibits excellent sampling efficiency over several table top manipulation tasks, wherein these tasks involve changing the state of the robot's environment in a purposeful way: assembling the complete equipment from the cone of the accumulation block to the invisible object; from manipulating the deformable tether to pushing the stack of small objects with closed loop feedback. The transmitter network is also excellent in modeling multi-modal spatial motion distributions and generalizes the rotation and translation of objects according to the construction. They do not require any a priori knowledge of the object to be manipulated, but operate based on information contained in partial visual data (e.g., RGB-D data) from several presentations, and can generalize to new objects and configurations, and for some tasks, to one-time learning from a single presentation.
Thus, example aspects of the present disclosure leverage the role of the spatial structure and the new perspective of its capabilities to improve end-to-end learning of vision-based manipulations. An example model architecture is provided that learns to focus on local regions and predict their spatial displacement while preserving the spatial structure of visual input.
More specifically, example systems and methods for performing robotic control may first obtain observation data. The observation data may include data describing the environment. The observation data may be image data (e.g., RGB data, etc.). Additionally or alternatively, the observation data may be light detection and ranging (LiDAR) point cloud data.
As described above, some example embodiments may pre-process observations to ensure spatial consistency. For example, the robotic control system may de-project the raw viewing data into a three-dimensional space and then render the de-projected data into an orthographic representation of the environment to generate spatially consistent viewing data.
The robot control system may process the observation data to determine a first action of the robot. For example, determining the first action may include: a first probability distribution of the first action is generated based on the observed data by using a first machine-learned action value model (e.g., that is part of a larger network of transmitters). The determination of the first action may also involve selecting a maximum of the first probability distribution as the first action. The first action may be a change in pose of the robot and/or contact with an object. The first probability distribution can include one or more points that indicate respective probabilities that the one or more points can be a starting location of the object manipulation.
The robotic control system may also process the observation data using a first embedding model to generate a first feature embedding. For example, the first embedded model may be part of a larger transmitter network. The system may also process the observation data with a second embedding model to generate a second feature embedding (again, the second embedding model may be part of a larger transmitter network).
In some embodiments, the second embedding model is conditioned on the first action. In particular, in some embodiments, processing the observation data using the second embedding model may include: the observed data is rotated and/or cropped (e.g., in a local area around the location associated with the first action) to generate a plurality of rotationally cropped data samples. In such an embodiment, processing the observation data using the second embedding model may include: and processing the plurality of the data samples subjected to the rotary cutting by using a second embedding model to respectively generate a plurality of second feature embedding. In some embodiments, each of the first and second embedding models may be or include a full convolution neural network.
The robot control system may then determine a second action of the robot. In some implementations, the determination of the second action may depend in part on a comparison of the first feature embedding and the second feature embedding (or multiple).
As one example, the comparison may be or include a convolution of two feature insertions. The convolution may include convolving each of the second feature embeddings over the first feature embeddings to generate respective second probability distributions.
As another example, in a more complex implementation, rather than simply convolving the first feature embedding and the second feature embedding (or multiple), the determination of the second action may include: the first feature embedding and the second feature embedding (or multiple) are processed with a second machine-learned value action model (e.g., which may also be part of a larger transmitter network). The second machine-learned value action model may output one or more second probability distributions based on the feature embedding (or multiple).
The determination of the second action may comprise selecting a maximum of the second probability distribution(s) as the second action. In some cases, the second action may be a second pose of the robot and/or releasing contact with the object. Thus, the second probability distribution (or distributions) can include one or more points that indicate respective probabilities that the one or more points can be stop positions for object manipulation.
In some embodiments, the robotic control system may also obtain a set of target image data, process the target image data with a third embedding model to generate a third feature embedding, and determine the second action and/or the first action based on the feature embedding from all three embedding models.
Additional aspects of the present disclosure may include a training step for training the transmitter network (e.g., its constituent models) for action determination. The training step may include evaluating a loss function that compares the first and second actions to first and second ground truth actions. In response to the evaluating step, the values of the parameters of the embedded model and/or the action value model(s) may be modified based on a loss function (e.g., in an end-to-end manner).
In some embodiments, evaluating the loss function may include: a first one-hot (one hot) ground truth pixel map is generated based on the first ground truth action and a second one-hot ground truth pixel map is generated based on the second ground truth action. The first one-hot ground truth pixel map may include a first plurality of pixels having respective binary values indicating ground truth locations of the first action in the environment. Further, the second one-hot ground truth pixel map may include a second plurality of pixels having respective binary values indicating ground truth locations of the second action in the environment. The first and second actions (e.g. their respective positions) determined by the transmitter network may then be compared with the first and second unique ground value pixel maps, respectively.
In some embodiments, the loss function may be or include a cross-entropy loss. The cross-entropy loss may compare the first and second actions with the first and second one-hot reference true value pixel maps, respectively. In particular, the comparison may be evaluated on a pixel-by-pixel basis.
Thus, the systems and methods provided herein can be implemented as a simple end-to-end model architecture that preserves the spatial structure of vision-based manipulation without object-centric assumptions. The vision-based manipulation may include performing a series of spatial displacements. Manipulation may move an object or an end effector. The system of the network may involve a system of visual feedback to determine the spatial displacement and the order of the spatial displacement. The system may learn local regions of interest and predict target spatial displacements by depth feature template matching. Depth feature template matching may use the observation data to determine a first pose and a second pose based on the processed observation data. Depth feature template matching may then enable determination of robot motion for manipulation.
The system and method of the present disclosure provide a number of technical effects and advantages. As one example, the systems and methods may provide more efficient motion determination for robots, thus, increasing productivity of the robots. Further, the system and method may reduce the burden of data collection and may reduce the computational energy required for the computation. Because the system and method can make action determinations without requiring object keypoints, embedding, or dense descriptors, the system or method can reduce the required ingestion data. This reduction may reduce the computational energy required by the system.
Another technical advantage of the disclosed systems and methods is the ability to provide better performance with only a small number of training examples or demonstrations. The systems and methods may be trained with fewer examples than previous models, which allows the systems and methods to learn faster than other models. Faster learning may result in saving computing resources. Moreover, training the model on fewer training examples may actually result in less use of computing resources (e.g., processor memory, etc.), and thus by enabling the model to learn using fewer training examples, the systems and methods may conserve computing resources.
Further, the systems and methods may be used iteratively to accomplish a series of tasks. The learned model can effectively determine the actions to adjust the multimodal task. The ability to effectively handle multi-modal tasks can further increase the productivity of the robot.
Referring now to the drawings, example embodiments of the disclosure will be discussed in further detail.
Example apparatus and System
Fig. 1A depicts a block diagram of an example computing system 100 that performs action determination based on sensor feedback in accordance with an example embodiment of the present disclosure. The system 100 includes a user computing device 102, a server computing system 130, and a training computing system 150 that are communicatively coupled over a network 180.
The user computing device 102 may be any type of computing device, such as, for example, a personal computing device (e.g., a notebook computer or a desktop computer), a mobile computing device (e.g., a smartphone or a tablet computer), a gaming machine or controller, a wearable computing device, an embedded computing device, or any other type of computing device.
The user computing device 102 includes one or more processors 112 and memory 114. The one or more processors 112 may be any suitable processing device (e.g., processor core, microprocessor, ASIC, FPGA, controller, microcontroller, etc.) and may be one processor or multiple processors operatively connected. The memory 114 may include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, disks, etc., and combinations thereof. The memory 114 may store data 116 and instructions 118, wherein the instructions 118 are executed by the processor 112 to cause the user computing device 102 to perform operations.
In some implementations, the user computing device 102 may store or include one or more action determination models 120. For example, the motion determination model 120 may be or may otherwise include various machine learning models, such as a neural network (e.g., a deep neural network) or other types of machine learning models, including non-linear models and/or linear models. The neural network may include a feed-forward neural network, a recurrent neural network (e.g., a long-short term memory recurrent neural network), a convolutional neural network, or other form of neural network. An example action determination model 120 will be discussed with reference to fig. 2, 4, 5, and 6.
In some implementations, the one or more action determination models 120 may be received from the server computing system 130 via the network 180, stored in the memory 114 of the user computing device, and then used or otherwise implemented by the one or more processors 112. In some implementations, the user computing device 102 can implement multiple parallel instances of a single action determination model 120.
More specifically, the motion determination model may take sensor data and output the motion of the robot. The motion determination model may be used to determine the motion of the robot including, but not limited to, pick and place motions, push motions, drag motions, and sequence motions.
Additionally or alternatively, one or more action determination models 140 may be included in the server computing system 130 in communication with the user computing device 102 according to a client-server relationship, or otherwise stored in and implemented by the server computing system 130. For example, the action determination model 140 may be implemented by the server computing system 140 as part of a web service (e.g., a robotic manipulation service). Thus, one or more models 120 may be stored and implemented at the user computing device 102, and/or one or more models 140 may be stored and implemented at the server computing system 130.
The user computing device 102 may also include one or more user input components 122 that receive user input. For example, the user input component 122 may be a component (e.g., a touch-sensitive display screen or a touch pad) that is sensitive to touch by a user input object (e.g., a finger or stylus). Touch sensitive components may be used to implement a virtual keyboard. Other example user input components include a microphone, a conventional keyboard, or other means by which a user may provide user input.
The server computing system 130 includes one or more processors 132 and memory 134. The one or more processors 132 may be any suitable processing device (e.g., processor core, microprocessor, ASIC, FPGA, controller, microcontroller, etc.) and may be one processor or multiple processors operatively connected. The memory 134 may include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, disks, etc., and combinations thereof. The memory 134 may store data 136 and instructions 138, wherein the instructions 138 are executed by the processor 132 to cause the server computing system 130 to perform operations.
In some implementations, the server computing system 130 includes or is otherwise implemented by one or more server computing devices. Where the server computing system 130 includes multiple server computing devices, such server computing devices may operate according to a sequential computing architecture, a parallel computing architecture, or some combination thereof.
As described above, the server computing system 130 may store or otherwise include one or more machine-learned action determination models 140. For example, the model 140 may be or may otherwise include various machine learning models. Example machine learning models include neural networks or other multi-layer non-linear models. Example neural networks include feed-forward neural networks, deep neural networks, recurrent neural networks, and convolutional neural networks. Example models 140 will be discussed with reference to fig. 2, 4, 5, and 6.
In some embodiments, performing back-propagation of the error may include performing truncated back-propagation over time. Model trainer 160 may perform several generalization techniques (e.g., weight decay, drop, etc.) to improve the generalization capability of the trained model.
In particular, the model trainer 160 may train the motion determination models 120 and/or 140 based on a set of training data 162. Training data 162 may include, for example, a set of ground truth actions. The training method may include parameterization based on a comparison of the determined action with a ground truth action. The ground truth data may be a one-hot ground truth pixel map.
In some implementations, the training examples may be provided by the user computing device 102 if the user has agreed. Thus, in such an implementation, the model 120 provided to the user computing device 102 may be trained by the training computing system 150 on user-specific data received from the user computing device 102. In some cases, this process may be referred to as personalizing the model.
The model trainer 160 includes computer logic for providing the desired functionality. Model trainer 160 may be implemented in hardware, firmware, and/or software that controls a general purpose processor. For example, in some embodiments, model trainer 160 includes a program file stored on a storage device, which is loaded into memory and executed by one or more processors. In other embodiments, model trainer 160 includes one or more sets of computer-executable instructions stored in a tangible computer-readable storage medium, such as a RAM hard disk or an optical or magnetic medium.
FIG. 1A illustrates one example computing system that can be used to implement the present disclosure. Other computing systems may also be used. For example, in some implementations, the user computing device 102 may include a model trainer 160 and a training data set 162. In such an implementation, the model 120 may be trained and used locally at the user computing device 102. In some such implementations, the user computing device 102 may implement a model trainer 160 to personalize the model 120 based on user-specific data.
Fig. 1B depicts a block diagram of an example computing device 10, performed in accordance with an example embodiment of the present disclosure. Computing device 10 may be a user computing device or a server computing device.
As shown in fig. 1B, each application may communicate with several other components of the computing device, such as, for example, one or more sensors, a context manager, a device state component, and/or additional components. In some implementations, each application can communicate with each device component using an API (e.g., a public API). In some embodiments, the APIs used by each application are specific to that application.
Fig. 1C depicts a block diagram of an example computing device 50, performed in accordance with an example embodiment of the present disclosure. Computing device 50 may be a user computing device or a server computing device.
The central smart tier includes a number of machine learning models. For example, as shown in fig. 1C, a respective machine learning model (e.g., model) may be provided for each application and managed by the central intelligence layer. In other embodiments, two or more applications may share a single machine learning model. For example, in some embodiments, the central smart inlay may provide a single model (e.g., a single model) for all applications. In some embodiments, the central smart inlay is included in or otherwise implemented by the operating system of the computing device 50.
The central smart inlay may communicate with a central device data plane. The central device data layer may be a centralized data store of computing devices 50. As shown in fig. 1C, the central device data layer may communicate with several other components of the computing device, such as, for example, one or more sensors, a context manager, a device status component, and/or additional components. In some implementations, the central device data layer can communicate with each device component using an API (e.g., a proprietary API).
Example model implementation
Spatial inaccuracies may occur by data ingestion generating perceptual distortion. The transmitter network system may preserve the three-dimensional spatial structure of the visual input. The system may project visual data to a spatially consistent representation using three-dimensional information. A spatially consistent representation may allow for exploiting the equivalence of inductive biases present in data symmetry for more efficient learning.
The system may be used to learn various tasks. One such task may be a pick and place action (e.g., the example in fig. 2). From visual observation o t Learning the pick-and-place action a by a robot t Can be expressed as
in this representation, the first pose, or pick-up pose, of the robot relies on visual observation o t . Once the first pose is determined, the second pose, or placement pose, of the robot may be based on the visual observation o t And a determined first pose
A system explained by learning a process of taking and placing actions may be understood as: (i) There is a distribution 208 of successful pick gestures from which samples can be takengestures 220 from which samples ≧ are taken>
The system can recover the distribution of successful picks and successful placements from the visual observation 206 without assuming object metrics. Avoiding object-centric representations may allow the system the ability to handle unseen objects, deformable objects, and noncount heaps of small objects.
The equation can be generalized to other motion primitives besides pick and place motion, which can also be parameterized by two end-effector poses (i.e., initial and final motion poses). Although the above equation focuses on two-pose primitives, the same method may be generally applicable to longer sequences of pose changes
In addition, visual observation o t 206 may be a projection of a scene (e.g., in the form of an RGB-D image) defined on a regular grid of pixels { (u, v) } at time step t of the sequence rearrangement task. Through camera to robot calibration, the system can be calibrated to t Each pixel in (a) corresponds to a pick-up action at that location:distribution 208 of successful picks for a pixel can be multi-modal in nature. Multimodal distributions may be more prevalent when there are multiple instances of the same object in the scene, or when there is symmetry in the shape of the object. The system may use various types of models. In some embodiments, the system may use a full convolution network to ≦ the action value function associated with pickup success>
the selected neural network may provide equivalence such that if the object to be picked up in the scene is translated, the pick pose is also translated. Equivalence may be characterized as
The system or method may convert the RGB-D image into a spatially consistent form by de-projecting to a three-dimensional point cloud. The system may then render the three-dimensional point cloud into an orthographic projection, where each pixel (u, v) represents a fixed window of three-dimensional space. The system may use the spatially consistent representation for the transfer operation.
The spatially consistent visual representation may enable the system to perform visual spatial transfer. The transfer may include a partial crop from
For example,
wherein the content of the first and second substances,
The system or method may be applied to the cross-correlation from the relative transformation Δ τ as long as it produces the desired imaginary superposition
In the case of a two-dimensional planar application,
The system may be applied to three-dimensional rigid transformations. Transfer functions can be used to extend the system to three-dimensional interactions. The system can be extended to tasks requiring three-dimensional placement using a multi-stage approach. First, the three degrees of freedom of the two-dimensional rigid transformation can be solved, yielding an estimate
the hybrid discrete/continuous approach of the system can provide an accurate representation of complex multi-modal distributions in a two-dimensional rigid transformed space of the image, while the remaining degrees of freedom out of the image plane can be single-modal once conditioned on multi-modal in-plane distributions. Furthermore, once the approximate two-dimensional rigid transformation arrangement is superimposed via the transfer operation, the method may provide an attention mechanism to assist the model in accurate continuous regression.
Possibility of systemAn example of an application of (a) is a sequential challenge of an intra-river tower object pick-and-place sequence (e.g., the example embodiment in fig. 3). The challenge of the river tower may require the actor to move three trays in sequence from the first tower to the third tower without placing a large tray on a small tray. The discs may be of different colours. Completing a task may require a proper sequencing of seven pick and place actions. The system may adjust the actions of the sequential tasks based on visual contextual cues (e.g., the next pick-and-place depends on the positioning of the observed object in the scene or due to the absence of occlusions). The system or method may be stateless. The system or method may learn the ranking behavior through visual feedback. The system may increase the field of view of the embedded model to cover most of the visual observation o t . The system can utilize more memory to process non-markov tasks.
The system or method may be used for tasks other than pick and place actions. The system may be applied to other two-pose primitive tasks, including but not limited to: push and drag. For example, the system may learn to rearrange the deformable cords in order so that the system determines a set of actions that can allow a robot or other actor to connect two endpoints of a partially trihedron. Another example application may include using a shovel-shaped end effector to push a pile of small objects in order into a desired target set. The system may use visual feedback to learn other closed-loop behaviors to accomplish these tasks. The system may be adapted for rigid displacement and non-rigid placement, such as these examples. The system may be used to tap an object to initiate movement of the object to a given location.
The system may be used to determine the start and end positions of a slicing or cutting motion. These example motion primitives may also be combined to learn to perform more complex tasks.
The system may be configured with various network architecture configurations.
Example embodiments of the viewing space of the system may include a visual observation o t It may be an elevational top view of a 0.5 x 1m tabletop workspace generated by fusing 480 x 640RGB-D images captured with a calibration camera using known intrinsic and extrinsic factors. Overhead view image o t May have a pixel resolution of 160 x 320. The pixels may represent 3.125 x 3.125mm vertical columns of 3D space in the workspace. Image o t Information of both color (RGB) and scalar height from base (H) may be included. Access to color and height values may enable the depth model to learn features that are rich in both visual texture and geometry. It will be appreciated that other sizes of workspaces, RGB-D images, and overhead images may be used instead.
An example embodiment in which a first pose determination feature of the system is used as a pick model may include a single feed-forward fully-convolutional neural network that may look visual
An example embodiment in which a first pose determination feature of the system is used as a placement model may use a dual-stream feed-forward full convolution neural network, which may observe vision
The system may be trained using ground truth data and various loss functions. For example, during training, can be selected fromData set
The system or method may be applied or taught using various sets of transformations. The system may utilize lie groups including, but not limited to: SE (2), SO (2) and SE (3).
Fig. 2 depicts an illustration of an example action determination model 200 according to an example embodiment of the present disclosure. In some implementations, the motion determination model 200 is trained to receive a set of input data 206 describing the observation region and, as a result of receiving the input data 206, provide output data 208 and 220 that includes a first pose and a second pose. Thus, in some embodiments, the action-determining model 200 may include an object manipulation model operable to move an object from an initial point to a determined end point.
FIG. 2 depicts an example application of a system or method of action determination. The depicted action may involve a pick-and-place action, a push action, a drag action, and the like. The environment 202 may include a robot with an end effector, an object, and an endpoint/target point of the object. The object may be any shape, but in this embodiment, the shape of the object is an uppercase "L". The objects and endpoints may be located in an observation scene inside the environment. The observed scene may be observed by one or more sensors. Further, the one or more sensors may be cameras, liDAR sensors, or other forms of measurement sensors. In this embodiment, the sensor ingests data across the scene to generate observation data 206. The observation data may be input into the system. The system may have a target for picking and placing objects with the endpoints 204 set.
In the example shown, the observation data 206 is processed by an action model to generate a distribution 208 of successful first actions. Based on the distribution 208 of successful first actions, the system determines a first pose of the robot.
The observation data 206 is then processed by the first embedding model 210. In this embodiment, the first embedding model 210 outputs one or more first feature embeddings 214.
In this embodiment, the observation data 206 may also be cropped and rotated to produce a plurality of cropped and rotated data sets. The plurality of cropped and rotated data sets may be processed by the second embedding model 212 to generate a plurality of second feature embeddings 216.
The one or more first feature embeddings 214 and the plurality of second feature embeddings 216 may be compiled to provide a plurality of second possible poses 218. In the example shown, the system uses depth feature template matching to determine the success of each possible second pose. The system generates 220 a distribution of successful second poses. The system then selects a second pose from the distribution 220 of successful second poses.
The motion determination model 200 may then be used to guide the robot of the environment 202 to complete a first pose and a second pose, wherein the first pose is a pose in which an end effector of the robot is in contact with the object, and the second pose is a pose in which the object is placed in an end position by means of the end effector of the robot.
Fig. 3 depicts an illustration of an example action determination model 300 according to an example embodiment of the present disclosure. The motion determination model 300 is similar to the motion determination model 200 of FIG. 2, except that the motion determination model 300 also includes a set of ground truth data for evaluating and modifying parameters of the motion determination model. FIG. 3 depicts an example embodiment of a training system. The depicted embodiment may involve seven steps for completing the target task.
The motion determination model 300 may be taught to accomplish the seven-step task of the "estuary tower" sequence. The system may have been taught a specific order or specific rules that allow the tasks to be completed. The motion determination model 300 may include an environment 302 having robots and an observation scenario in which a group of "river towers" is disposed. One or more sensors may generate observation data 306 based on the scene. The action determination model may then be used to process the observation data 306 to complete the target task 304.
In the example shown, the first step of the action determining model involves obtaining observation data 306. Once the observation data 306 is obtained, the observation data 306 is processed by the action model to generate a distribution 308 of possible first actions that are successful. The system then selects argmax for the model to determine the first action 312.
In this embodiment, observation data 306 and first action 312 are then processed by the first and second embedding models to generate first feature embedding 316 and second feature embedding. First feature embedding 316 may be associated with a plurality of possible translations. The second feature embedding may be associated with a plurality of possible rotations. Then, the first feature embedding and the second feature embedding are convolved, and then the convolved feature embedding 320 is used for depth feature template matching. Based on the depth feature template matching, the system may determine a distribution of successful possible second actions. The system may select one of the possible second actions as the second action of the robot. The action-determining model 300 may be run iteratively to complete the entire sequence 314.
The motion determination model 300 may be trained using the ground truth data 314. The ground truth data may include the starting image 310, the end point images 318, and each placed image in between. For the seven-step "river tower" depicted in the illustrated example, the ground truth data 314 includes eight images. Each step may be processed to generate a map of unique reference true values pixels for each step. To complete the first step of "river inner tower", a one hot pixel map is generated for the first action of ground truth and the second action of ground truth (i.e., the pick action and the place action). The generated first and second true-value pixel maps may then be compared to the determined first and second actions to evaluate the action determination model. In response to the evaluation, parameters of the action model, the first embedding model, or the second embedding model may be modified. The training process may be run iteratively to provide more accurate results.
In some embodiments, the action determination model 300 may use ground truth data 314 to help complete a task. The ground truth data 314 may be processed by a third embedding model to generate a third feature embedding. Third feature embedding may be used to provide more accurate results for depth template matching.
FIG. 4 depicts an example application of a system or method of action determination. The depicted embodiment may involve using a multi-modal strategy to accomplish different actions of different shapes.
The motion determination model 400 may be used to accomplish several tasks in the same environment 402, involving different objects located at different locations in the environment 402. In this embodiment, the observation region 404 includes five objects having four different shapes. The observation region 404 also includes five destinations of four different shapes corresponding to the object. The action determination model 400 may be used to fetch and drop each object to a corresponding destination. The action-determining model may be run iteratively to complete the task.
The action determination model may include obtaining observation data 406, which may be processed by the first action model to determine a first action 408 on one of the objects. The second action may be different for each respective object at each respective location on each object due to the shape of the object and the shape of the destination. For example, if the first action is a pick-up action on an edge of the circular object 410, the action determination model may produce a distribution of possible second actions, where the distribution is a circular distribution in the circular destination 414. Another example may involve the first action being a pick-up action on a leg of one of the "E" shaped objects 412. The action determination model may generate a distribution consisting of two points, where each point is located in one of the "E" shaped destinations 416.
Further, the motion determination model may be used to determine a first motion (e.g., a pick motion or a contact motion) and a second motion (e.g., a place motion or a release motion) for each respective object 420.
In addition, FIG. 4 shows a success distribution 418 for the rotational placement of each shape. The symmetry of each object helps to distribute the results. For example, a circle selected at the center placed at a circular destination may have a uniform distribution from 0 to 2 π. However, when the selection point is the center, the square and clover shapes may have four separate maxima, the magnitudes of which are nearly uniform. When the destinations are the same shape, a shape lacking any form of symmetry may only produce a single maximum of statistical significance.
Similarly, changing the selection point or the translation point may change the successful distributions 422 and 424.
Example method
Fig. 5 depicts a flowchart of an example method performed in accordance with an example embodiment of the present disclosure. Although fig. 5 depicts the steps as being performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particular illustrated order or arrangement. The various steps of the method 500 may be omitted, rearranged, combined, and/or adapted in various ways without departing from the scope of the present disclosure.
At 502, a computing system may obtain observation data. The observation data may be image data, liDAR point cloud data, or another form of data. Further, the data may be obtained from one or more sensors located around the observed scene. The observation data may include data about the environment. Further, the environment may include one or more objects to take an action.
At 504, the computing system may determine a first action. The determination of the first action may include processing the observation data with a first action model. The first motion model may use pixel mapping to generate a distribution of possible first motions that are successful. The system may use the distribution to select the first action. In some embodiments, the first action may be argmax of the model function.
In 506, the computing system may process the observation data to determine a first feature embedding. The observation data may be processed by a first embedding model to generate a first feature embedding. In some embodiments, the first feature embedding may be a plurality of possible robot movement translations. Further, the first embedded model may be a full convolution neural network.
In 508, the computing system may process the observation data to generate a second feature embedding based at least in part on the first action. The observation data may be processed by a second embedding model to generate a second feature embedding. The second feature embedding may describe a plurality of possible robot movement rotations. In some embodiments, the second embedded model may be a full convolution neural network.
In some implementations, the first and second embedding models can process the observation data for the first action.
In 510, the computing system may determine a second action. In some implementations, the second action can be determined based at least in part on a convolution of the first feature embedding and the second feature embedding. The determining may include depth feature template matching. The convolution of the first feature embedding and the second feature embedding may be used to generate a distribution of possible second actions, wherein the second action may be selected from the distribution.
Fig. 6 depicts a flowchart of an example method performed in accordance with an example embodiment of the present disclosure. Although fig. 6 depicts the steps as being performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particular illustrated order or arrangement. The various steps of the method 600 may be omitted, rearranged, combined, and/or adjusted in various ways without departing from the scope of the present disclosure.
In 602, the computing system may obtain observation data and then process the observation data to generate a distribution of successful initial actions. The observation data may be processed by the action model and the observation data may include information about the environment in which the robot is working. In some implementations, the motion model can process data on a pixel-by-pixel basis.
In 604, the computing system may determine a first action. The determining may include selecting the first action from a distribution of successful initial actions. The selection may be based on whether the point is a local maximum. Further, the first action may be a pick-up action, wherein the action causes an end effector of the robot to contact the item and pick up the item.
In 606, the computing system may process the observation data to generate a first feature embedding. The processing may be done by the first embedding model, and in some embodiments, the first embedding model may be a full convolution neural network.
In 608, the computing system may process the observation data to generate a second feature embedding. The processing may be done by the second embedding model, and in some embodiments, the second embedding model may be a full convolution neural network.
In 610, the computing system may determine a second action based at least in part on a comparison of the first feature embedding and the second feature embedding. The determination of the second action may also depend on the first action. Further, the second action may be determined by depth feature template matching using data generated by a comparison of the first feature embedding and the second feature embedding. In some embodiments, the second action may be a placing action, wherein the action releases the item contacted by the first action. In some embodiments, the second action may place the item in a desired location. In some implementations, the machine learning model can be used to generate the second action based on the first feature embedding and the second feature embedding.
Fig. 7 depicts a flowchart of an example training method performed in accordance with an example embodiment of the present disclosure. Although fig. 7 depicts the steps as being performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particular illustrated order or arrangement. The various steps of the method 700 may be omitted, rearranged, combined, and/or adapted in various ways without departing from the scope of the present disclosure.
At 702, a computing system may obtain observation data. The observation data may be a spatially consistent representation to allow the use of equivalence principles. The observation data may include data of the observation environment. The environment may include one or more items for robotic manipulation.
In 704, the computing system may determine a first action. The determination of the first action may be based on processing the observed data by the action model to generate a distribution of successful initial actions. The distribution of successful pick actions may include one or more maxima, including argmax for a function of the action model. The first action may be determined by selecting a maximum or even argmax of a function of the action model.
In 706, the computing system may process the observation data to generate a first feature embedding. The processing may be accomplished by a first embedding model, wherein the first embedding model processes the observation data to generate a first feature embedding. The first feature embedding may include a possible translation of the item for the first action. The first embedding model may generate a plurality of first feature embeddings having different possible translations.
At 708, the computing system may process the observation data to generate a second feature embedding. The processing may be accomplished by a second embedding model, wherein the second embedding model processes the observation data to generate a second feature embedding. The second feature embedding may include a possible rotation of the item for the second action. The second embedding model may generate a plurality of second feature embeddings having different possible rotations. The processed observation data may be cropped and rotated observation data. The cropped and rotated view data may be generated by cropping view data that is pinned to a set of pixels of interest. The cropped data may then be copied and rotated to generate a plurality of cropped and rotated observation data to be processed.
In 710, the computing system may determine a second action, for example, based at least in part on a convolution of the first feature embedding and the second feature embedding or based at least in part on a more complex learning model process. The determination of the second action may be determined by performing a convolution of the first feature embedding and the second feature embedding and performing a depth template matching to determine which second actions will succeed. The second action may be selected from the determined successful second actions.
At 712, the computing system may evaluate the first action and the second action with a loss function. The evaluating may include comparing the first action and the second action to a ground truth action using a loss function. The ground truth actions may be processed to generate a one-hot ground truth pixel map for comparison to the determined actions. For three-dimensional manipulation, the loss function may be a Huber loss.
In 714, the computing system may modify the values of the parameters of the embedded model. The modification of the action model, the first embedded model, or the second embedded model may be done in response to an evaluation of the first action and the second action. The evaluation and modification may be used to train the motion determination model. The model may be trained by an iterative training technique.
Additional disclosure
The techniques discussed herein make reference to servers, databases, software applications, and other computer-based systems, and the actions taken and information sent to and by such systems. The inherent flexibility of computer-based systems allows for a large number of possible configurations, combinations, and divisions of tasks and functions between components. For example, the processes discussed herein may be implemented using a single device or component or multiple devices or components operating in conjunction. The database and applications may be implemented on a single system or distributed across multiple systems. The distributed components may operate sequentially or in parallel.
While the present subject matter has been described in detail with respect to various specific example embodiments thereof, each example is provided by way of explanation, not limitation of the disclosure. Those skilled in the art, having the benefit of the foregoing description, may effect numerous modifications, changes, and equivalents to the described embodiments. Accordingly, the present subject matter disclosure does not preclude inclusion of such modifications, variations and/or additions to the present subject matter as would be readily apparent to one of ordinary skill in the art. For instance, features illustrated or described as part of one embodiment, can be used with another embodiment to yield a still further embodiment. Accordingly, the present disclosure is intended to cover such modifications, alterations, and equivalents.
Claims (20)
1. A computer-implemented method for generating actions of a robot, comprising:
obtaining observation data, wherein the observation data includes data describing an environment;
determining a first action of the robot based at least in part on the observation data;
processing the observation data with a first embedding model to generate a first feature embedding;
processing the observation data with a second embedding model to generate a second feature embedding, wherein the second embedding model is conditioned on the first action; and
a second action of the robot is determined based at least in part on the comparison of the first feature embedding to the second feature embedding.
2. The computer-implemented method of any of the preceding claims, further comprising: the robot is controlled to perform a first action and a second action.
3. The computer-implemented method of any of the preceding claims, wherein the first action comprises a robot contacting an object.
4. The computer-implemented method of any of the preceding claims, wherein the observation data comprises image data.
5. The computer-implemented method of any of the preceding claims, wherein the observation data comprises light detection and ranging (LiDAR) point cloud data.
6. The computer-implemented method of any of the preceding claims, wherein the first action comprises a change in a pose of the robot.
7. The computer-implemented method of any of the preceding claims, wherein each of the first and second embedding models comprises a full convolution neural network.
8. The computer-implemented method of any of the preceding claims, wherein determining a first action of a robot comprises:
generating a first probability distribution of the first action based on the observation data and using a first machine learned action value model; and
the action having the maximum value in the first probability distribution is selected as the first action.
9. The computer-implemented method of claim 8, wherein the first probability distribution includes one or more points indicating respective probabilities that the one or more points are a starting location of the object manipulation.
10. The computer-implemented method of any of the preceding claims, wherein determining the second action of the robot based at least in part on the comparison of the first feature embedding to the second feature embedding comprises:
convolving the second feature embedding over the first feature embedding to generate a second probability distribution; and
the action having the maximum value in the second probability distribution is selected as the second action.
11. The computer-implemented method of any of the preceding claims, wherein processing the observation data with a second embedding model to generate a second feature embedding comprises:
rotating and cropping the observation data to generate a plurality of rotationally cropped data samples; and
the plurality of rotation-clipped data samples are processed using a second embedding model to generate a plurality of second feature embeddings, respectively.
12. The computer-implemented method of any of the preceding claims, wherein obtaining observation data comprises:
obtaining original observation data;
de-projecting the original observation data into a three-dimensional space; and
rendering the de-projected data into an elevational representation of the environment, wherein the elevational representation of the environment is used as the viewing data.
13. The computer-implemented method of any of the preceding claims, wherein the observation data, the first feature embedding, and the second feature embedding comprise three-dimensional data.
14. The computer-implemented method of any of the preceding claims, wherein determining a second action of the robot comprises: the first feature embedding and the second feature embedding are processed using a second machine-learned value action model.
15. The computer-implemented method of any of the preceding claims, further comprising:
obtaining a set of target image data; and
processing the set of target image data with a third embedding model to generate a third feature embedding;
wherein determining the second action of the robot comprises: a second action is determined based at least in part on the first feature embedding, the second feature embedding, and the third feature embedding.
16. A computer system, comprising:
one or more processors; and
one or more non-transitory computer-readable media collectively storing instructions that, when executed by one or more processors, cause a computing system to perform operations comprising:
obtaining observation data, wherein the observation data includes data describing an environment;
determining a first action of the robot;
processing the observation data with a first embedding model to generate a first feature embedding;
processing the observation data with a second embedding model to generate a second feature embedding, wherein the second embedding model is conditioned on the first action;
determining a second action of the robot based at least in part on the comparison of the first feature embedding to the second feature embedding;
evaluating a loss function that compares one or both of the first and second actions to one or both of the first and second ground truth actions; and
modifying one or more values of one or more parameters of one or both of the first and second embedding models based at least in part on the loss function.
17. The computer system of claim 16, wherein evaluating a loss function comprises:
generating a first one-hot ground truth pixel map based on the first ground truth actions and a second one-hot ground truth pixel map based on the second ground truth actions, wherein the first one-hot ground truth pixel map comprises a first plurality of pixels having respective binary values indicative of ground truth locations of the first actions in the environment, and wherein the second one-hot ground truth pixel map comprises a second plurality of pixels having respective binary values indicative of ground truth locations of the second actions in the environment; and
the first action is compared to a first one-hot reference true value pixel map and the second action is compared to a second one-hot reference true value pixel map.
18. The computer system of claim 17, wherein the penalty function comprises a cross-entropy penalty, and wherein comparing the first action to the first one-hot reference true-value pixel map and the second action to the second one-hot reference true-value pixel map comprises: the cross-entropy loss is evaluated on a pixel-by-pixel basis.
19. A robotic device comprising one or more non-transitory computer-readable media collectively storing instructions that, when executed by one or more computing devices, cause the one or more computing devices to perform operations comprising:
obtaining observation data, wherein the observation data includes data describing an environment;
determining a first action of the robotic device;
processing the observation data with a first embedding model to generate a first feature embedding;
processing the observation data with a second embedding model to generate a second feature embedding, wherein the second embedding model is conditioned on the first action;
determining a second action of the robotic device based at least in part on the comparison of the first feature embedding to the second feature embedding; and
the first action and the second action are performed by the robotic device.
20. The robotic device of claim 19, wherein the observation data includes spatially consistent data.
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2020/055726 WO2022081157A1 (en) | 2020-10-15 | 2020-10-15 | Transporter network for determining robot actions |
Publications (1)
Publication Number | Publication Date |
---|---|
CN115867947A true CN115867947A (en) | 2023-03-28 |
Family
ID=73172817
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202080102599.3A Pending CN115867947A (en) | 2020-10-15 | 2020-10-15 | Transmitter network for determining robot actions |
Country Status (4)
Country | Link |
---|---|
US (1) | US20230256597A1 (en) |
EP (1) | EP4150526A1 (en) |
CN (1) | CN115867947A (en) |
WO (1) | WO2022081157A1 (en) |
Family Cites Families (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
EP3867021B1 (en) * | 2018-11-28 | 2023-08-23 | Google LLC | Robot navigation using a high-level policy model and a trained low-level policy model |
-
2020
- 2020-10-15 US US18/011,561 patent/US20230256597A1/en active Pending
- 2020-10-15 EP EP20803705.1A patent/EP4150526A1/en active Pending
- 2020-10-15 WO PCT/US2020/055726 patent/WO2022081157A1/en unknown
- 2020-10-15 CN CN202080102599.3A patent/CN115867947A/en active Pending
Also Published As
Publication number | Publication date |
---|---|
US20230256597A1 (en) | 2023-08-17 |
EP4150526A1 (en) | 2023-03-22 |
WO2022081157A1 (en) | 2022-04-21 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11325252B2 (en) | Action prediction networks for robotic grasping | |
US11941719B2 (en) | Learning robotic tasks using one or more neural networks | |
Qin et al. | Dexmv: Imitation learning for dexterous manipulation from human videos | |
Fujita et al. | What are the important technologies for bin picking? Technology analysis of robots in competitions based on a set of performance metrics | |
WO2018221614A1 (en) | Learning device, learning method, learning model, estimation device, and grip system | |
CN110730970A (en) | Policy controller using image embedding to optimize robotic agents | |
EP4309151A1 (en) | Keypoint-based sampling for pose estimation | |
Xu et al. | GraspCNN: Real-time grasp detection using a new oriented diameter circle representation | |
Zeng | Learning visual affordances for robotic manipulation | |
Matsumoto et al. | End-to-end learning of object grasp poses in the Amazon Robotics Challenge | |
Park et al. | Development of robotic bin picking platform with cluttered objects using human guidance and convolutional neural network (CNN) | |
Ghazaei et al. | Dealing with ambiguity in robotic grasping via multiple predictions | |
US20220402125A1 (en) | System and method for determining a grasping hand model | |
Zhuang et al. | Instance segmentation based 6D pose estimation of industrial objects using point clouds for robotic bin-picking | |
Abid et al. | Dynamic hand gesture recognition for human-robot and inter-robot communication | |
Tobin | Real-World Robotic Perception and Control Using Synthetic Data | |
Zhang et al. | Digital twin-enabled grasp outcomes assessment for unknown objects using visual-tactile fusion perception | |
Nagata et al. | Modeling object arrangement patterns and picking arranged objects | |
JP2019164836A (en) | Learning device, learning method, learning model, detection device, and holding system | |
CN115867947A (en) | Transmitter network for determining robot actions | |
Peng et al. | A pushing-grasping collaborative method based on deep Q-network algorithm in dual viewpoints | |
Song et al. | Tactile–visual fusion based robotic grasp detection method with a reproducible sensor | |
Schwan et al. | Visual Movement Prediction for Stable Grasp Point Detection | |
Zhu et al. | On robot grasp learning using equivariant models | |
CN110363793B (en) | Object tracking method and device |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |