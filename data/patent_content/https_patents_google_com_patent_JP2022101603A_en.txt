JP2022101603A - Efficient image analysis using environment sensor data - Google Patents
Efficient image analysis using environment sensor data Download PDFInfo
- Publication number
- JP2022101603A JP2022101603A JP2022065610A JP2022065610A JP2022101603A JP 2022101603 A JP2022101603 A JP 2022101603A JP 2022065610 A JP2022065610 A JP 2022065610A JP 2022065610 A JP2022065610 A JP 2022065610A JP 2022101603 A JP2022101603 A JP 2022101603A
- Authority
- JP
- Japan
- Prior art keywords
- image
- objects
- data
- particular image
- class
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
- 238000010191 image analysis Methods 0.000 title abstract description 18
- 238000012545 processing Methods 0.000 claims abstract description 108
- 238000000034 method Methods 0.000 claims abstract description 60
- 238000004458 analytical method Methods 0.000 claims abstract description 52
- 238000004891 communication Methods 0.000 claims abstract description 11
- 230000005055 memory storage Effects 0.000 claims abstract description 8
- 230000008569 process Effects 0.000 claims description 33
- 230000004044 response Effects 0.000 claims description 14
- 238000012015 optical character recognition Methods 0.000 claims description 6
- 230000003993 interaction Effects 0.000 claims description 5
- 230000006399 behavior Effects 0.000 claims description 3
- 239000002131 composite material Substances 0.000 claims description 3
- 230000000977 initiatory effect Effects 0.000 claims description 3
- 230000033001 locomotion Effects 0.000 abstract description 65
- 230000000007 visual effect Effects 0.000 description 32
- 238000004590 computer program Methods 0.000 description 15
- 230000000875 corresponding effect Effects 0.000 description 10
- 230000009471 action Effects 0.000 description 9
- 230000003287 optical effect Effects 0.000 description 9
- 230000007613 environmental effect Effects 0.000 description 4
- 238000010801 machine learning Methods 0.000 description 4
- 238000005259 measurement Methods 0.000 description 4
- 230000008901 benefit Effects 0.000 description 3
- 230000002596 correlated effect Effects 0.000 description 3
- 230000001133 acceleration Effects 0.000 description 2
- 238000003491 array Methods 0.000 description 2
- 238000004364 calculation method Methods 0.000 description 2
- 238000010586 diagram Methods 0.000 description 2
- 238000003708 edge detection Methods 0.000 description 2
- 238000007726 management method Methods 0.000 description 2
- 238000003909 pattern recognition Methods 0.000 description 2
- 230000009467 reduction Effects 0.000 description 2
- 230000000717 retained effect Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 230000005540 biological transmission Effects 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 150000001875 compounds Chemical class 0.000 description 1
- 238000010205 computational analysis Methods 0.000 description 1
- 239000000470 constituent Substances 0.000 description 1
- 230000001276 controlling effect Effects 0.000 description 1
- 238000013527 convolutional neural network Methods 0.000 description 1
- 238000013500 data storage Methods 0.000 description 1
- 230000003247 decreasing effect Effects 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 230000006870 function Effects 0.000 description 1
- 230000017525 heat dissipation Effects 0.000 description 1
- 230000010354 integration Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000006855 networking Effects 0.000 description 1
- 238000010422 painting Methods 0.000 description 1
- 230000000737 periodic effect Effects 0.000 description 1
- 238000003672 processing method Methods 0.000 description 1
- 230000001902 propagating effect Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V30/00—Character recognition; Recognising digital ink; Document-oriented image-based pattern recognition
- G06V30/40—Document-oriented image-based pattern recognition
- G06V30/41—Analysis of document content
- G06V30/413—Classification of content, e.g. text, photographs or tables
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/10—Image acquisition
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/24—Classification techniques
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/285—Selection of pattern recognition techniques, e.g. of classifiers in a multi-classifier system
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/03—Arrangements for converting the position or the displacement of a member into a coded form
- G06F3/033—Pointing devices displaced or positioned by the user, e.g. mice, trackballs, pens or joysticks; Accessories therefor
- G06F3/0346—Pointing devices displaced or positioned by the user, e.g. mice, trackballs, pens or joysticks; Accessories therefor with detection of the device orientation or free movement in a 3D space, e.g. 3D mice, 6-DOF [six degrees of freedom] pointers using gyroscopes, accelerometers or tilt-sensors
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/98—Detection or correction of errors, e.g. by rescanning the pattern or by human intervention; Evaluation of the quality of the acquired patterns
- G06V10/993—Evaluation of the quality of the acquired pattern
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/20—Scenes; Scene-specific elements in augmented reality scenes
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V30/00—Character recognition; Recognising digital ink; Document-oriented image-based pattern recognition
- G06V30/10—Character recognition
- G06V30/19—Recognition using electronic means
- G06V30/191—Design or setup of recognition systems or techniques; Extraction of features in feature space; Clustering techniques; Blind source separation
- G06V30/19113—Selection of pattern recognition techniques, e.g. of classifiers in a multi-classifier system
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/62—Control of parameters via user interfaces
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/68—Control of cameras or camera modules for stable pick-up of the scene, e.g. compensating for camera body vibrations
- H04N23/681—Motion detection
- H04N23/6812—Motion detection based on additional sensors, e.g. acceleration sensors
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N5/00—Details of television systems
- H04N5/44—Receiver circuitry for the reception of television signals according to analogue transmission standards
- H04N5/445—Receiver circuitry for the reception of television signals according to analogue transmission standards for displaying additional information
- H04N5/44504—Circuit details of the additional information generator, e.g. details of the character or graphics signal generator, overlay mixing circuits
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N5/00—Details of television systems
- H04N5/76—Television signal recording
- H04N5/765—Interface circuits between an apparatus for recording and another apparatus
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N5/00—Details of television systems
- H04N5/76—Television signal recording
- H04N5/765—Interface circuits between an apparatus for recording and another apparatus
- H04N5/77—Interface circuits between an apparatus for recording and another apparatus between a recording apparatus and a television camera
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0484—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range
- G06F3/04842—Selection of displayed objects or displayed text elements
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V2201/00—Indexing scheme relating to image or video recognition or understanding
- G06V2201/10—Recognition assisted with metadata
Abstract
Description
本発明は、環境センサデータを用いる効率的な画像解析に関する。 The present invention relates to efficient image analysis using environment sensor data.
コンピュータビジョン技法は、コンピュータが画像を解析し、画像から情報を抽出することを可能にする。たとえば、画像からテキストを検出および抽出するために光学的文字認識(OCR)技法が使用され得る。同様に、画像内に示されるオブジェクトを検出するためにエッジ検出技法が使用され得る。 Computer vision techniques allow a computer to analyze an image and extract information from the image. For example, optical character recognition (OCR) techniques can be used to detect and extract text from images. Similarly, edge detection techniques can be used to detect the objects shown in the image.
本明細書は、電子デバイスが画像内のオブジェクトを認識し、かつ/または画像から情報を抽出し、情報または画像に関する他のコンテンツを提示することを可能にすると共に、電子デバイスの計算能力または電力の消費を低減する、効率的な画像解析に関する技術を説明する。 The present specification allows an electronic device to recognize an object in an image and / or extract information from the image to present the information or other content about the image, as well as the computational power or power of the electronic device. We will explain the technology related to efficient image analysis that reduces the consumption of information.
一般には、本明細書において説明される主題の1つの革新的な態様が、画像ピクセルデータを出力するように構成された画像センサと、画像ピクセルデータを一時的に記憶するように構成された画像バッファと、環境センサと、画像処理装置とを含むデバイスで実施され得る。デバイスはまた、環境センサによって出力された信号に基づいて、画像ピクセルデータの予想画像品質を決定し、予想画像品質に従って画像ピクセルデータを画像処理装置に選択的に提供するように構成されたコントローラをも含み得る。この態様の他の実装は、コンピュータ記憶デバイス上に符号化された、方法のアクションを実施するように構成された、対応する装置、方法、システム、およびコンピュータプログラムを含む。 In general, one innovative aspect of the subject matter described herein is an image sensor configured to output image pixel data and an image configured to temporarily store the image pixel data. It can be performed on a device that includes a buffer, an environment sensor, and an image processing device. The device also has a controller configured to determine the expected image quality of the image pixel data based on the signal output by the environment sensor and selectively provide the image pixel data to the image processor according to the expected image quality. Can also be included. Other implementations of this embodiment include corresponding devices, methods, systems, and computer programs encoded on computer storage devices and configured to perform method actions.
任意選択で、これらおよび他の実装はそれぞれ、以下の特徴のうちの1つまたは複数を含み得る。いくつかの態様では、画像ピクセルデータの予想画像品質がプリセット最小品質しきい値未満であるとコントローラが判定した場合、画像ピクセルデータは画像処理装置に提供されない。画像ピクセルデータの予想画像品質がプリセット最小品質しきい値以上であるとコントローラが判定した場合、画像ピクセルデータは画像処理装置に提供される。 Optionally, these and other implementations may each include one or more of the following features: In some embodiments, the image pixel data is not provided to the image processor if the controller determines that the expected image quality of the image pixel data is less than the preset minimum quality threshold. If the controller determines that the expected image quality of the image pixel data is greater than or equal to the preset minimum quality threshold, the image pixel data is provided to the image processor.
いくつかの態様は、選択されたフレームバッファを含む。コントローラは、画像ピクセルデータの予想画像品質に従って、画像バッファから、選択されたフレームバッファに画像ピクセルデータをコピーし、または画像バッファに対する選択されたフレームポインタを更新し、選択されたフレームバッファ内に記憶された画像ピクセルデータを画像処理装置に提供するように構成され得る。 Some embodiments include a selected frame buffer. The controller copies the image pixel data from the image buffer to the selected frame buffer or updates the selected frame pointer to the image buffer and stores it in the selected frame buffer according to the expected image quality of the image pixel data. The resulting image pixel data may be configured to be provided to the image processing apparatus.
いくつかの態様では、コントローラは、画像バッファ内の画像ピクセルデータの予想画像品質を、選択されたフレームバッファの画像ピクセルデータの予想画像品質と比較し、画像バッファ内の画像ピクセルデータの予想画像品質が、選択されたフレームバッファの予想画像品質以上である場合、画像バッファから、選択されたフレームバッファに画像ピクセルデータをコピーし、または画像バッファに対する選択されたフレームポインタを更新するように構成される。 In some embodiments, the controller compares the expected image quality of the image pixel data in the image buffer with the expected image quality of the image pixel data in the selected frame buffer and the expected image quality of the image pixel data in the image buffer. Is configured to copy image pixel data from the image buffer to the selected frame buffer or update the selected frame pointer to the image buffer if is greater than or equal to the expected image quality of the selected frame buffer. ..
いくつかの態様では、環境センサは慣性センサを含み、コントローラは、感知された装置の運動に基づいて予想画像品質を決定するように構成される。いくつかの態様では、コントローラは、環境センサによって出力された信号と、画像センサによって出力された信号とに基づいて予想画像品質を決定するように構成される。いくつかの態様では、画像センサによって出力された信号は、画像センサによって出力された画像ピクセルデータに関する輝度情報、フォーカス情報、またはヒストグラムデータのうちの少なくとも1つを含む。 In some embodiments, the environment sensor comprises an inertial sensor and the controller is configured to determine the expected image quality based on the perceived motion of the device. In some embodiments, the controller is configured to determine the expected image quality based on the signal output by the environment sensor and the signal output by the image sensor. In some embodiments, the signal output by the image sensor comprises at least one of brightness information, focus information, or histogram data for the image pixel data output by the image sensor.
一般には、本明細書において説明される主題の別の態様は、画像ピクセルデータを生成すること、画像バッファ内に画像ピクセルデータを一時的に記憶すること、環境信号に基づいて画像ピクセルデータの予想画像品質を決定すること、および予想画像品質に従って画像ピクセルデータを選択的に画像処理することを含む方法で実施され得る。この態様の他の実装は、コンピュータ記憶デバイス上に符号化された、方法のアクションを実施するように構成された、対応する装置、システム、およびコンピュータプログラムを含む。 In general, another aspect of the subject described herein is to generate image pixel data, to temporarily store image pixel data in an image buffer, to predict image pixel data based on environmental signals. It can be performed by methods that include determining the image quality and selectively image processing the image pixel data according to the expected image quality. Other implementations of this embodiment include corresponding devices, systems, and computer programs encoded on a computer storage device and configured to perform the action of the method.
任意選択で、これらおよび他の実装はそれぞれ、以下の特徴のうちの1つまたは複数を含み得る。いくつかの態様では、画像ピクセルデータの予想画像品質がプリセット最小品質しきい値未満であると判定された場合、画像ピクセルデータは画像処理のために提供されない。 Optionally, these and other implementations may each include one or more of the following features: In some embodiments, if the expected image quality of the image pixel data is determined to be less than the preset minimum quality threshold, the image pixel data is not provided for image processing.
いくつかの態様は、画像ピクセルデータの予想画像品質に従って、画像バッファから、選択されたフレームバッファに画像ピクセルデータをコピーすること、または画像バッファに対するポインタを更新することを含む。選択されたフレームバッファ内に記憶された画像ピクセルデータは、画像処理のために提供され得る。 Some embodiments include copying the image pixel data from the image buffer to the selected frame buffer, or updating the pointer to the image buffer, according to the expected image quality of the image pixel data. The image pixel data stored in the selected frame buffer may be provided for image processing.
いくつかの態様では、環境信号は慣性信号を含み、予想画像品質が、感知された運動に基づいて決定される。予想画像品質は、環境信号と、画像ピクセルデータと共に生成された画像情報信号とに基づいて決定され得る。画像情報信号は、生成された画像ピクセルデータに関する輝度情報、フォーカス情報、またはヒストグラムデータのうちの少なくとも1つを含み得る。 In some embodiments, the environmental signal includes an inertial signal and the expected image quality is determined based on the sensed motion. The expected image quality can be determined based on the environmental signal and the image information signal generated with the image pixel data. The image information signal may include at least one of brightness information, focus information, or histogram data for the generated image pixel data.
一般には、本明細書において説明される主題の別の態様は、それぞれのオブジェクトクラスに対応する1つまたは複数の画像認識モジュールを含む画像処理装置で実施され得る。各画像認識モジュールは、それぞれのオブジェクトクラス内の1つまたは複数のオブジェクトを識別するように構成され得る。画像処理装置はまた、入力画像を受け取り、入力画像がオブジェクトクラスのうちの1つを識別する画像特徴を含むかどうかを判定し、識別されたオブジェクトクラスに対応する画像認識モジュールによる処理のための入力画像を提供するように構成された粗認識モジュールをも含み得る。この態様の他の実装は、コンピュータ記憶デバイス上に符号化された、方法のアクションを実施するように構成された、対応する装置、システム、およびコンピュータプログラムを含む。 In general, another aspect of the subject matter described herein may be implemented in an image processing apparatus that includes one or more image recognition modules corresponding to each object class. Each image recognition module can be configured to identify one or more objects within its respective object class. The image processor also receives the input image, determines if the input image contains image features that identify one of the object classes, and for processing by the image recognition module corresponding to the identified object class. It may also include a coarse recognition module configured to provide the input image. Other implementations of this embodiment include corresponding devices, systems, and computer programs encoded on a computer storage device and configured to perform the action of the method.
任意選択で、これらおよび他の実装はそれぞれ、以下の特徴のうちの1つまたは複数を含み得る。いくつかの態様では、粗認識モジュールは、オブジェクトの位置および/または方位を示す情報を画像認識モジュールに提供するように構成される。いくつかの態様では、画像認識モジュールは、テキストオブジェクトクラス、ランドマークオブジェクトクラス、バーコードオブジェクトクラス、メディアオブジェクトクラス、およびアートワークオブジェクトクラスのうちの1つまたは複数に対応する。 Optionally, these and other implementations may each include one or more of the following features: In some embodiments, the coarse recognition module is configured to provide the image recognition module with information indicating the position and / or orientation of the object. In some embodiments, the image recognition module corresponds to one or more of a text object class, a landmark object class, a bar code object class, a media object class, and an artwork object class.
いくつかの態様では、1つまたは複数の画像認識モジュールは、それぞれのオブジェクトクラス内の識別されたオブジェクトを、関連する信頼スコアと共に出力するように構成される。1つまたは複数の画像認識モジュールは、画像認識モジュールの前の出力に基づいて、識別されたオブジェクトについての出力信頼スコアを調節するように構成され得る。 In some embodiments, one or more image recognition modules are configured to output the identified objects in their respective object classes, along with their associated confidence scores. One or more image recognition modules may be configured to adjust the output confidence score for the identified object based on the previous output of the image recognition module.
いくつかの態様では、出力信頼スコアは、識別されたオブジェクトまたは識別されたオブジェクト内のテキストと、以前に識別されたオブジェクトとの間の編集距離、以前に識別されたオブジェクトの位置、および以前に識別されたオブジェクトに関連する信頼スコアのうちの少なくとも1つに基づいて調節され得る。いくつかの態様は通信ユニットを含む。粗認識モジュールは、1つまたは複数の画像認識モジュールから遠隔に配置され得る。 In some embodiments, the output trust score is the edit distance between the identified object or the text within the identified object and the previously identified object, the location of the previously identified object, and previously. It can be adjusted based on at least one of the confidence scores associated with the identified object. Some embodiments include a communication unit. The coarse recognition module may be located remotely from one or more image recognition modules.
いくつかの態様では、粗認識モジュールは、画像認識モジュールによる処理のための入力画像縮小バージョンを提供するように構成される。入力画像の縮小バージョンは、入力画像の低解像度バージョン、トリミングバージョン、またはベクトル表現のうちの1つであり得る。 In some embodiments, the coarse recognition module is configured to provide an input image reduction version for processing by the image recognition module. The reduced version of the input image can be one of a low resolution version, a cropped version, or a vector representation of the input image.
一般には、本明細書において説明される主題の別の態様は、入力画像を受け取ること、入力画像が1つまたは複数のオブジェクトクラスのうちの1つを識別する画像特徴を含むかどうかを判定すること、および入力画像を画像処理して、識別されたオブジェクトクラス内の1つまたは複数のオブジェクトを識別することを含む画像処理方法で実施され得る。この態様の他の実装は、コンピュータ記憶デバイス上に符号化された、方法のアクションを実施するように構成された、対応するシステム、装置、およびコンピュータプログラムを含む。 In general, another aspect of the subject described herein is to receive an input image and determine if the input image contains image features that identify one or more object classes. It can be performed in an image processing method that involves image processing the input image to identify one or more objects in the identified object class. Other implementations of this embodiment include corresponding systems, devices, and computer programs encoded on computer storage devices and configured to perform method actions.
任意選択で、これらおよび他の実装はそれぞれ、以下の特徴のうちの1つまたは複数を含み得る。いくつかの態様は、オブジェクトの位置および/または方位を示す情報を決定することを含む。入力画像を画像処理して、1つまたは複数のオブジェクトを識別することは、オブジェクトの位置および/または方位を示す、前記決定された情報に基づき得る。オブジェクトクラスは、テキストオブジェクトクラス、ランドマークオブジェクトクラス、バーコードオブジェクトクラス、メディアオブジェクトクラス、およびアートワークオブジェクトクラスのうちの1つまたは複数を含み得る。 Optionally, these and other implementations may each include one or more of the following features: Some embodiments include determining information indicating the position and / or orientation of an object. Image processing of the input image to identify one or more objects may be based on the determined information indicating the position and / or orientation of the objects. The object class can include one or more of a text object class, a landmark object class, a bar code object class, a media object class, and an artwork object class.
いくつかの態様は、識別されたオブジェクトを、関連する信頼スコアと共に出力すること、および前の出力に基づいて、識別されたオブジェクトについての出力信頼スコアを調節することを含む。いくつかの態様では、出力信頼スコアは、識別されたオブジェクトと以前に識別されたオブジェクトとの間の編集距離、以前に識別されたオブジェクトの位置、および以前に識別されたオブジェクトに関連する信頼スコアのうちの少なくとも1つに基づいて調節される。 Some embodiments include outputting the identified object with the associated confidence score, and adjusting the output confidence score for the identified object based on the previous output. In some embodiments, the output trust score is the edit distance between the identified object and the previously identified object, the location of the previously identified object, and the confidence score associated with the previously identified object. Adjusted based on at least one of.
いくつかの態様は、画像処理についての入力画像の縮小バージョンを生成して、1つまたは複数のオブジェクトを識別することを含む。入力画像の縮小バージョンは、入力画像の低解像度バージョン、トリミングバージョン、またはベクトル表現のうちの1つであり得る。 Some embodiments include generating a reduced version of the input image for image processing to identify one or more objects. The reduced version of the input image can be one of a low resolution version, a cropped version, or a vector representation of the input image.
一般には、本明細書において説明される主題の別の態様は、画像を取り込むように構成されたカメラと、カメラの運動を検出するように構成された1つまたは複数の環境センサと、データ処理装置と、データ処理装置とデータ通信しているメモリ記憶装置とを含む画像処理システムで実施され得る。メモリ記憶装置は、データ処理装置によって実行可能であり、そのような実行時に、モバイルデバイスカメラによって取り込まれた多数の画像のそれぞれについて、カメラが画像を取り込んだ時のカメラの運動を示すデータにアクセスすることを含む動作をデータ処理装置に実施させる命令を記憶し得る。データ処理装置は、各画像についてのカメラの運動を示すデータに基づいて、解析のための特定の画像を画像から選択し得る。データ処理装置は、特定の画像を解析して、特定の画像内に示される1つまたは複数のオブジェクトを認識し、1つまたは複数の認識されたオブジェクトに関するコンテンツを提示し得る。この態様の他の実装は、コンピュータ記憶デバイス上に符号化された、方法のアクションを実施するように構成された、対応する方法、装置、およびコンピュータプログラムを含む。 In general, another aspect of the subject described herein is a camera configured to capture an image, one or more environment sensors configured to detect camera motion, and data processing. It can be implemented in an image processing system that includes a device and a memory storage device that is in data communication with the data processing device. The memory storage device can be executed by a data processing device, and at such an execution, for each of the large number of images captured by the mobile device camera, access data indicating the movement of the camera when the camera captures the image. It is possible to store an instruction to cause a data processing apparatus to perform an operation including the operation. The data processor may select a particular image for analysis from the images based on the data showing the motion of the camera for each image. A data processor may analyze a particular image to recognize one or more objects shown within the particular image and present content about the one or more recognized objects. Other implementations of this embodiment include corresponding methods, devices, and computer programs encoded on computer storage devices and configured to perform method actions.
任意選択で、これらおよび他の実装はそれぞれ、以下の特徴のうちの1つまたは複数を含み得る。いくつかの態様では、動作は、粗分類器を使用して特定の画像を解析して、特定の画像内に示されるオブジェクトの1つまたは複数のクラスの存在を検出することを含み得る。特定の画像内のオブジェクトの1つまたは複数のクラスの存在を検出したことに応答して、特定の画像は、特定の画像内に示される1つまたは複数のオブジェクトを認識するように解析され得る。 Optionally, these and other implementations may each include one or more of the following features: In some embodiments, the action may include analyzing a particular image using a coarse classifier to detect the presence of one or more classes of objects shown within the particular image. In response to detecting the presence of one or more classes of objects in a particular image, the particular image may be parsed to recognize one or more objects shown in the particular image. ..
いくつかの態様では、画像を解析して、画像によって示される1つまたは複数のオブジェクトを認識することは、1つまたは複数のコンピュータビジョン技法を使用して画像を解析することを含む。モバイルデバイスの運動を示すデータは、(i)モバイルデバイスの加速度計センサから受け取った慣性加速度測定値、または(ii)モバイルデバイスのジャイロスコープから受け取った方位データの少なくとも一方を含み得る。 In some embodiments, analyzing an image to recognize one or more objects represented by the image involves analyzing the image using one or more computer vision techniques. The data indicating the motion of the mobile device may include at least one of (i) the inertial acceleration measurement received from the accelerometer sensor of the mobile device or (ii) the orientation data received from the gyroscope of the mobile device.
いくつかの態様では、解析のための特定の画像を選択することは、特定の画像が、カメラが多数の画像内のそれぞれの他の画像を取り込む時のそれぞれの運動の量に比べて、カメラが特定の画像を取り込んだ時に少なくともある量の運動を有することに基づいて、特定の画像を選択することを含み得る。各画像についてのモバイルデバイスの運動を示すデータは、画像が取り込まれた時のモバイルデバイスの回転運動を記述するデータを含み得る。いくつかの態様では、解析のための特定の画像を選択することは、特定の画像が取り込まれた時のモバイルデバイスの回転運動がしきい量の回転運動未満であることに基づいて、特定の画像を選択することを含み得る。 In some embodiments, selecting a particular image for analysis is a camera relative to the amount of movement when a particular image captures each other image within a large number of images. May include selecting a particular image based on having at least a certain amount of motion when capturing the particular image. The data indicating the motion of the mobile device for each image may include data describing the rotational motion of the mobile device when the image is captured. In some embodiments, selecting a particular image for analysis is based on the fact that the rotational movement of the mobile device when the particular image is captured is less than a threshold amount of rotational movement. It may include selecting an image.
いくつかの態様では、動作は、粗分類器による前の画像の解析の完了後に、粗分類器によって解析すべき画像を求める要求を受け取ることを含み得る。解析のための特定の画像を選択することは、前の画像が解析のために粗分類器に送られた後、特定の画像が、カメラによって取り込まれた画像のセット内の他の画像と比べて少なくともある量の運動を有することに基づいて、特定の画像を選択することを含み得る。 In some embodiments, the operation may include receiving a request for an image to be analyzed by the coarse classifier after the analysis of the previous image by the coarse classifier is complete. Selecting a specific image for analysis means that after the previous image has been sent to the coarse classifier for analysis, the specific image will be compared to the other images in the set of images captured by the camera. It may include selecting a particular image based on having at least a certain amount of motion.
いくつかの態様では、粗分類器を使用して特定の画像を解析することは、処理レートに基づいて周期的に粗分類器による解析を開始することを含み得る。動作は、カメラアプリケーションとのユーザ対話に基づいて処理レートを調節することを含み得る。動作は、前の時間枠にわたって、粗分類器によって解析された1つまたは複数の画像内で、オブジェクトの1つまたは複数のクラスの存在が検出されたかどうかに基づいて、処理レートを調節することを含み得る。前の時間枠にわたって、少なくとも1つの画像内でオブジェクトの1つまたは複数のクラスの存在を検出したことに応答して、単位時間当たりにより多くの画像が粗分類器によって解析されるように、処理レートが増加され得る。前の時間枠にわたって、少なくとも1つの画像内でオブジェクトの1つまたは複数のクラスの存在を検出しなかったことに応答して、単位時間当たりにより少ない画像が粗分類器によって解析されるように、処理レートが低減され得る。 In some embodiments, analyzing a particular image using a coarse classifier may include initiating periodic analysis by the coarse classifier based on the processing rate. The operation may include adjusting the processing rate based on user interaction with the camera application. The behavior is to adjust the processing rate based on whether the presence of one or more classes of objects is detected in one or more images analyzed by the coarse classifier over the previous time frame. May include. Processed so that more images per unit time are analyzed by the coarse classifier in response to detecting the presence of one or more classes of objects in at least one image over the previous time frame. The rate can be increased. To allow the coarse classifier to analyze fewer images per unit time in response to not detecting the presence of one or more classes of objects in at least one image over the previous time frame. The processing rate can be reduced.
いくつかの態様では、1つまたは複数の識別されたオブジェクトに関するコンテンツを提示することは、1つまたは複数の識別されたオブジェクトに関するリソースへのリンクを含む結果を含む結果ページを提示することを含み得る。1つまたは複数の識別されたオブジェクトに関するコンテンツを提示することは、それぞれの識別されたオブジェクトについて、オブジェクトを覆うオーバーレイ内のオブジェクトに関するコンテンツを提示することを含み得る。1つまたは複数の識別されたオブジェクトに関するコンテンツを提示することは、ユーザによって選択されたとき、1つまたは複数の識別されたオブジェクトに関連する特定のアクションを開始する、選択可能なユーザインターフェース要素を提示することを含み得る。1つまたは複数の識別されたオブジェクトに関するコンテンツを提示することは、クラスのオブジェクトの存在が粗分類器によって検出された各クラスについて、クラスに関するコンテンツアイテムを選択すること、およびクラスのオブジェクトの存在が粗分類器によって検出された各クラスについてコンテンツアイテムを提示することを含み得る。 In some embodiments, presenting content about one or more identified objects involves presenting a results page that contains results that include links to resources for one or more identified objects. obtain. Presenting content about one or more identified objects may include presenting content about the objects in the overlay that covers the objects for each identified object. Presenting content about one or more identified objects is a selectable user interface element that, when selected by the user, initiates a specific action associated with one or more identified objects. May include presenting. Presenting content about one or more identified objects means selecting a content item about the class for each class for which the presence of objects in the class is detected by the coarse classifier, and the presence of objects in the class. It may include presenting a content item for each class detected by the coarse classifier.
いくつかの態様では、モバイルデバイスカメラの運動を示すデータに基づいて、解析のための特定の画像を選択することは、画像の視覚特性を示すデータとは無関係の特定の画像を選択することを含み得る。モバイルデバイスカメラの運動を示すデータに基づいて、解析のための特定の画像を選択することは、各画像の視覚特性を示すデータと共に、各画像についてのモバイルデバイスカメラの運動を示すデータに基づいて、特定の画像を選択することを含み得る。各画像の視覚特性を示すデータは、(i)輝度データ、(ii)フォーカスデータ、または(iii)ヒストグラムデータのうちの少なくとも1つを含み得る。 In some embodiments, selecting a specific image for analysis based on data demonstrating the motion of a mobile device camera may select a specific image unrelated to the data demonstrating the visual characteristics of the image. Can include. Selecting a specific image for analysis based on data showing the motion of the mobile device camera is based on data showing the motion of the mobile device camera for each image, as well as data showing the visual characteristics of each image. , May include selecting a particular image. The data showing the visual characteristics of each image may include at least one of (i) luminance data, (ii) focus data, or (iii) histogram data.
いくつかの態様では、解析のための特定の画像を選択することは、各画像について、画像についてのモバイルデバイスカメラの運動を示すデータに基づいて、画像の予想画像品質を決定すること、および最高の予想画像品質を有する画像を特定の画像として選択することを含み得る。解析のための特定の画像を選択することは、第1の画像について、第1の画像についてのモバイルデバイスカメラの運動を示すデータに基づいて、第1の画像の第1の予想画像品質を決定することを含み得る。第1の画像についての画像データは、第1の画像の第1の予想画像品質に基づいて、選択されたフレームバッファ内に記憶され得る。第2の画像の第2の予想画像品質は、第2の画像についてのモバイルデバイスカメラの運動を示すデータに基づいて決定され得る。選択されたフレームバッファにおいて、第2の予想画像品質が第1の予想画像品質よりも高いことに基づいて、第1の画像についての画像データを第2の画像についての画像データで置き換えるように決定が行われ得る。 In some embodiments, selecting a particular image for analysis determines, for each image, the expected image quality of the image, based on data showing the movement of the mobile device camera for the image, and best. It may include selecting an image with the expected image quality of the above as a particular image. Choosing a specific image for analysis determines the first expected image quality of the first image, based on the data showing the movement of the mobile device camera for the first image for the first image. May include doing. The image data for the first image may be stored in the selected frame buffer based on the first expected image quality of the first image. The second expected image quality of the second image can be determined based on data showing the movement of the mobile device camera for the second image. Determined to replace the image data for the first image with the image data for the second image based on the fact that the second expected image quality is higher than the first expected image quality in the selected framebuffer. Can be done.
いくつかの態様は、粗分類器を使用して第2の画像を解析して、特定の画像内で示されるオブジェクトの1つまたは複数のクラスの存在を検出すること、および粗分類器を使用して特定の画像内に示されるオブジェクトの1つまたは複数のクラスの存在を検出することができないことに応答して、特定の画像内に示される1つまたは複数のオブジェクトを認識するために第2の画像を解析しないように決定することを含み得る。 In some embodiments, a coarse classifier is used to analyze a second image to detect the presence of one or more classes of objects shown within a particular image, and the coarse classifier is used. To recognize one or more objects shown in a particular image in response to the inability to detect the presence of one or more classes of objects shown in a particular image. It may include deciding not to analyze the image in 2.
本明細書において説明される主題は、以下の利点のうちの1つまたは複数を実現するように特定の実施形態で実装され得る。画像を取り込むカメラの運動を示すデータに基づいて、解析のための画像を選択することによって、各画像の視覚特性の解析に基づいて画像を選択することよりも少ないコンピューティングリソースおよび少ない電力を使用して、より高い品質の画像が、より詳細な解析のために迅速に選択され得る。後続の解析のために画像のストリームの単一の最良の画像(またはしきい数未満の画像)のみを記憶することによって、消費されるメモリ量が削減され、他のアプリケーションのためにメモリ空間が解放される。これらの特徴はまた、より高い品質画像がコンピュータビジョン解析において使用されることを保証し、その結果、より正確なビジョン解析結果が得られ、画像内で認識されたオブジェクトに基づいて、より関連のあるコンテンツが提供される。 The subject matter described herein may be implemented in a particular embodiment to achieve one or more of the following advantages: By selecting images for analysis based on data showing the motion of the camera that captures the images, it uses less computing resources and less power than selecting images based on the analysis of the visual characteristics of each image. Higher quality images can then be quickly selected for more detailed analysis. By storing only a single best image (or less than a threshold) of a stream of images for subsequent analysis, the amount of memory consumed is reduced and memory space is freed up for other applications. To be released. These features also ensure that higher quality images are used in computer vision analysis, resulting in more accurate vision analysis results and more relevant based on the objects recognized in the image. Some content is provided.
粗分類器を使用して特定のクラスオブジェクトの存在を識別し、クラスのうちの1つのオブジェクトを示すと分類された画像に対するさらなるコンピュータビジョン解析を実施するだけによって、コンピュータビジョン解析技法を使用して解析される画像数が削減される。さらに、マルチクラス粗分類器が、粗分類器によって検出されたタイプに合致する微細分類器に追加の処理を限定し得る。後のステージのコンピュータビジョン解析は、粗い分類および画像選択よりも計算集約的であり得るので、これにより、コンピューティングリソースに対して課される要求が大いに削減され、他のアプリケーションによる使用のために計算パワーおよび電力が温存される。さらに、これにより、解析のためにキュー内に配置される画像数が削減されるので、画像が解析される速度が改善され得る。本明細書において説明される画像選択技法および画像解析技法はまた、アプリケーションの主な挙動(たとえば、カメラで写真を撮ること)を損なうことなく、既存のアプリケーションへの視覚解析のモードレス統合を可能にする。 Using computer vision analysis techniques by simply using a coarse classifier to identify the presence of a particular class object and performing further computer vision analysis on the image classified as showing one of the objects in the class. The number of images analyzed is reduced. In addition, the multiclass coarse classifier may limit additional processing to the fine classifier that matches the type detected by the coarse classifier. Later stages of computer vision analysis can be more computationally intensive than coarse classification and image selection, which greatly reduces the demand placed on computing resources for use by other applications. Computational power and power are preserved. In addition, this reduces the number of images placed in the queue for analysis, which can improve the speed at which the images are analyzed. The image selection and image analysis techniques described herein also enable modeless integration of visual analysis into existing applications without compromising the main behavior of the application (eg, taking a picture with a camera). do.
前述の主題の様々な特徴および利点が、以下で図に関連して説明される。追加の特徴および利点は、本明細書および特許請求の範囲において説明される主題から明らかである。 The various features and advantages of the aforementioned subjects are described below in connection with the figures. Additional features and advantages are evident from the subject matter described herein and in the claims.
様々な図面内の同様の参照番号および名称は同様の要素を示す。 Similar reference numbers and names in various drawings indicate similar elements.
一般には、本明細書において説明されるシステムおよび技法は、画像を解析するために使用されるコンピューティングリソースおよび電力の消費を削減し得ると共に、高品質であると予想される画像を選択的に解析することによって、画像解析の精度をも改善する。たとえば、画像が解析され、画像内に示されるオブジェクトが認識され、オブジェクトについての追加の情報を提供する目的で、画像内に示されるオブジェクトについての情報が抽出され得る。モバイルデバイス、たとえばスマートフォンまたはタブレットコンピューティングデバイスのユーザは、モバイルデバイスのカメラを使用してオブジェクトの画像を取り込み得る。たとえば、画像が取り込まれた時のカメラの運動を示すデータに基づいて決定された画像の予想品質に基づいて、画像のうちの1つまたは複数が解析のために選択され得る。選択された画像が解析され、モバイルデバイスに提示するための追加のコンテンツが提供され得る。たとえば、コンテンツが、モバイルデバイスのユーザインターフェース内の画像と共に表示され得、または解析の結果が後の使用のために記憶され得る。 In general, the systems and techniques described herein can reduce the consumption of computing resources and power used to analyze images, while selectively selecting images that are expected to be of high quality. By analyzing, the accuracy of image analysis is also improved. For example, the image may be parsed, the objects shown in the image recognized, and information about the objects shown in the image may be extracted for the purpose of providing additional information about the objects. Users of mobile devices, such as smartphones or tablet computing devices, may use the camera of the mobile device to capture an image of an object. For example, one or more of the images may be selected for analysis based on the expected quality of the image, which is determined based on the data showing the motion of the camera when the image is captured. The selected image may be parsed to provide additional content for presentation to mobile devices. For example, the content may be displayed with an image in the user interface of the mobile device, or the results of the analysis may be stored for later use.
モバイルデバイスは通常、デスクトップコンピュータおよびサーバよりも低い計算機能力およびデータ記憶容量を有する。さらに、モバイルデバイスは通常、限られた電池電力および熱放散能力を有する。したがって、コンピューティングリソースおよび電池電力の利用を削減する方式で画像解析を実施するための技法が、他のアプリケーションおよび/またはプロセスのために限定された計算機能力および電力を温存すること、およびモバイルデバイスによって生成された熱量を削減することによって、モバイルデバイスの機能および性能を改善し得る。 Mobile devices typically have lower computing power and data storage capacity than desktop computers and servers. In addition, mobile devices typically have limited battery power and heat dissipation capacity. Therefore, techniques for performing image analysis in a manner that reduces the use of computing resources and battery power preserves limited computing power and power for other applications and / or processes, and mobile devices. By reducing the amount of heat generated by the mobile device, the functionality and performance of the mobile device can be improved.
いくつかの実装では、画像解析が、ビデオストリームなどの画像のシーケンスのうちの画像に対して実施される。たとえば、ユーザが、画像内に示されるオブジェクトに関するコンテンツを提供するカメラモードを活動化し、カメラを様々なオブジェクトに向け得る。別の例では、カメラアプリケーションがアクティブであるとき、カメラアプリケーションが一般にこの特徴をアクティブにし得る。これらの例では、画像のシーケンス内の画像のいくつか(たとえば、すべて未満)が解析され、画像内に示されるオブジェクトが認識され得る。本明細書において説明される技法は、1つまたは複数の低電力消費ステージを使用して、さらなる解析のための画像を選択し、解析される画像数を削減し、解析プロセスによって消費される計算機能力および電力の量を削減し得る。 In some implementations, image analysis is performed on the image in a sequence of images such as a video stream. For example, a user may activate a camera mode that provides content about an object shown in an image and point the camera at various objects. In another example, when the camera application is active, the camera application may generally activate this feature. In these examples, some (eg, less than all) of the images in the sequence of images can be analyzed and the objects shown in the images can be recognized. The techniques described herein use one or more low power consumption stages to select images for further analysis, reduce the number of images analyzed, and the computer consumed by the analysis process. The amount of capacity and power can be reduced.
いくつかの実装では、低電力消費ステージが画像の予想画像品質を決定し得る。たとえば、予想画像品質が、デバイスの環境センサに基づいて決定され得る。予想画像品質の決定は、示されるオブジェクトを解析するためのさらなる解析よりも、必要とする処理リソースが少なくなり得、したがって消費される電力が少なくなる。このようにして、低い画像品質を有する画像を解析するための不必要なリソースの消費が回避され得る。 In some implementations, the low power consumption stage may determine the expected image quality of the image. For example, the expected image quality can be determined based on the device's environmental sensors. Determining the expected image quality may require less processing resources and therefore consume less power than further analysis to analyze the indicated object. In this way, unnecessary resource consumption for analyzing images with low image quality can be avoided.
いくつかの実装では、低電力消費ステージが、画像がオブジェクトクラスを識別する画像特徴を含むかどうかを決定し得る。そのような画像特徴を含む画像が、識別されたクラスに対応する画像認識モジュールによるさらなる解析のために提供され得る。そのような画像特徴の決定は、示されるオブジェクトを認識するためのさらなる処理よりも、必要とする処理リソースが少なくなり得、したがって消費される電力が少なくなる。このようにして、どんなオブジェクトも含まない画像を解析するための不必要なリソースの消費が回避され得る。さらに、識別されたクラスに対応する画像認識モジュールによるさらなる解析のための画像を提供することによって、画像認識モジュールがより効率的にされ得、システムの処理要件におけるさらなる削減が実現され得る。 In some implementations, the low power consumption stage may determine whether the image contains image features that identify the object class. Images containing such image features may be provided for further analysis by the image recognition module corresponding to the identified class. Determining such image features may require less processing resources and therefore consume less power than further processing to recognize the indicated object. In this way, unnecessary resource consumption for parsing an image that does not contain any objects can be avoided. In addition, by providing images for further analysis by the image recognition module corresponding to the identified class, the image recognition module can be made more efficient and further reductions in the processing requirements of the system can be realized.
図1は、例示的モバイルデバイス110が画像を解析し、画像内に示される1つまたは複数のオブジェクトに関するコンテンツを提示する環境100のブロック図である。モバイルデバイス110は、スマートフォン、タブレットコンピュータ、ラップトップコンピュータ、ウェアラブルデバイス、または他の適切なタイプのモバイルデバイスであり得る。モバイルデバイス110が図1に示され、本明細書において説明されるが、モバイルデバイス110の構成要素が、他のタイプの電子デバイス(たとえば、デスクトップコンピュータ)内に含まれ得、モバイルデバイス110によって実施される技法が、他の電子デバイスによって実施され得る。
FIG. 1 is a block diagram of an
モバイルデバイス110はカメラ112およびカメラアプリケーション114を含む。カメラ112は静止画像(たとえば、デジタル写真)およびビデオを取り込む。カメラアプリケーション114は、特定のプラットフォームまたは特定のデバイス上での使用のために開発されたネイティブアプリケーションであり得る。カメラアプリケーション114は、ユーザがカメラ112を制御し、カメラ112によって取り込まれた画像およびビデオを閲覧することを可能にする。カメラ112は、画像センサによって取り込まれた画像データ(たとえば、画像ピクセルデータ)を出力するように構成される画像センサを含み得る。画像の各ピクセルについてのピクセルデータは、ピクセルの1つまたは複数の視覚特性、たとえばピクセルのカラーを指定し得る。
The
カメラアプリケーション114はまた、取り込まれた画像に対して画像選択技法および/または画像解析技法を実施し、画像内で認識されたオブジェクトに関するコンテンツを提供し得る。いくつかの実装では、カメラアプリケーション114は、ユーザがアイコンを選択したことに応答して、画像を取り込み、画像を解析する。たとえば、ユーザは、オブジェクトにカメラ112を向け、オブジェクトの画像を取り込むためのアイコンを選択し、オブジェクトに関するコンテンツを受け取り得る。
The
いくつかの実装では、カメラアプリケーション114は、たとえば常時オン状態で、カメラ112によって取り込まれている画像のストリームに対して、画像解析プロセスを自動的に実施する。たとえば、カメラアプリケーション114は、ビデオストリームから画像を選択し、選択した画像を解析し、画像内に示されるオブジェクトを認識し、認識したオブジェクトに関するコンテンツを提供し得る。特定の例では、カメラアプリケーション114は、ユーザがオブジェクトにカメラ112を向けている間にカメラ112によって取り込まれた画像のシーケンスから画像を選択し得る。別の例では、カメラアプリケーション114は、カメラ112を使用して、カメラ112(またはモバイルデバイス110)のビューファインダ内に見えるシーンの画像を取り込み得る。
In some implementations, the
以下の説明は、画像に対して画像選択技法および画像解析技法を実施するカメラアプリケーション114に関するものであるが、技法(またはその一部)は、別のアプリケーション(たとえば、カメラにアクセスし、かつ/またはカメラを制御し、カメラによって取り込まれた画像に関するコンテンツを提示し得るカメラ第1アプリケーション)、モバイルデバイスのハードウェア回路、コントローラ、あるいは別の適切なハードウェアおよび/またはソフトウェア構成要素によって実施され得る。
The following description relates to a
カメラアプリケーション114は、1つまたは複数の低電力消費ステージを使用して、オブジェクト認識のために解析される画像を選択し得る。1つまたは複数のステージは、後のステージにおいて解析される画像が十分な品質であり、ユーザの関心を引き得る1つまたは複数のオブジェクトを示すことを保証し得る。このことにより、モバイルデバイス110が、オブジェクトが認識され得ない画像、またはコンテンツが識別され得ない画像に対して計算資源および電池電力を浪費しないことが可能となる。
The
カメラアプリケーション114は、画像が取り込まれた時のカメラ112の運動を示すデータ(「運動データ」)に基づいて、さらなる処理のために画像ストリームから画像を選択する画像セレクタ122を含む。画像が取り込まれた時の運動データは、画像が取り込まれた後の指定の時間枠までの、画像が取り込まれる前の指定の時間枠を開始するタイムウィンドウの間のカメラ112についての運動データを含み得る。たとえば、画像を取り込んでいる間にカメラ112が移動している場合、画像は、カメラ112が静止している場合よりも低い品質である(たとえば、ぼやける)可能性が高い。画像の視覚特性の代わりに画像についての運動データを使用することによって、選択が、より迅速に、かつより少ないコンピューティングリソースを使用することによって実施され得る。たとえば、カメラアプリケーション114は、画像のどんな視覚特性も検出および評価することなく、画像についての運動データを使用するだけで、複数の画像から画像を選択し得る。以下で説明されるように、カメラアプリケーション114はまた、画像の視覚特性を画像についての運動データと組み合わせて使用し、さらなる処理のための画像を選択し得る。
The
カメラアプリケーション114は、モバイルデバイス110の慣性測定ユニット(IMU)113または別のタイプの環境センサから、画像についての運動データを取得し、または受け取り得る。たとえば、カメラアプリケーション114がカメラ112に画像を取り込ませるとき、カメラアプリケーション114は画像についての運動データを取得し得る。IMUは、1つまたは複数の加速度計、1つまたは複数のジャイロスコープ、および/または1つまたは複数の磁力計を含み得る電子デバイスである。IMUデータは、ゲーム回転ベクトルの形式であり得る。いくつかの実装では、モバイルデバイス110は、別々の加速度計、ジャイロスコープ、および/または磁力計を含み得る。画像についての運動データは、IMU113から受け取った慣性加速度測定値、および/またはIMU113もしくは別々のジャイロスコープから受け取った方位データを含み得る。
The
いくつかの実装では、画像についての運動データは、ジャイロスコープから受け取った方位データのみを含む。たとえば、ジャイロスコープからの方位データは、画像が取り込まれた時に生じた方位変化を決定するために使用され得る。 In some implementations, the motion data for the image contains only the orientation data received from the gyroscope. For example, directional data from a gyroscope can be used to determine the directional changes that occur when an image is captured.
カメラアプリケーション114は、画像についての運動データ(たとえば、IMU113から受け取ったデータ)を使用して、画像についての品質スコアを決定し得る。画像についての品質スコアは、画像の予想画像品質を表し得る。画像についての品質スコアは、運動データを使用して決定され得る、画像が取り込まれた時のカメラ112の回転運動の測定値に基づき(たとえば、それに反比例し)得る。画像についての品質スコアは、画像が取り込まれた時のカメラ112の方位変化に基づき得る。たとえば、画像が取り込まれる前、画像が取り込まれる間、および画像が取り込まれた後のモバイルデバイス110の方位を指定する方位データが使用され、画像が取り込まれた時にカメラ112の方位が変化していたかどうかが判定され得る。
The
カメラアプリケーション114は、画像についての品質スコアに基づいて、複数の画像から、さらなる処理のための1つまたは複数の画像を選択し得る。たとえば、カメラアプリケーション114は、最高の品質スコア(たとえば、最小の量の回転運動または最小の量の運動)を有する画像を、さらなる処理のために選択し得る。カメラアプリケーション114はまた、画像の新しさを使用して、さらなる処理のためにどれを使用するかを決定し得る。次いで、カメラアプリケーション114は、たとえば以下で説明される粗分類器123による、さらなる処理のために、選択された画像についての画像データを送り得る。次いで、カメラアプリケーション114は、追加の画像を取り込み、追加の画像から、さらなる処理のための別の画像を選択し得る。この例では、粗分類器123は、別の画像を要求し、または別の画像の準備ができていることをカメラアプリケーション114に通知し得る。それに応答して、カメラアプリケーション114は、選択された画像を粗分類器123に送り得る。
The
粗分類器123が前の画像の処理を終了し、あるいは別の画像を解析する準備ができるのを待機する間、カメラアプリケーション114は、継続的に画像を解析し、最良の画像を記憶し得る。たとえば、カメラアプリケーション114は、前の画像についての画像データが粗分類器123に送られた以降に取り込まれた画像のセットの中の、関連する最高の品質スコアを有する最高品質の画像についての画像データを記憶し得る。待機している間、カメラアプリケーション114は、別の画像を受け取り得る。カメラアプリケーション114は、新たに受け取った画像についての品質スコア(たとえば、回転運動の尺度)を決定し得る。カメラアプリケーション114は、記憶された最良の画像についての品質スコアを、新たに受け取った画像についての品質スコアと比較し得る。新たに受け取った画像についての品質スコアが、記憶された最良の画像についての品質スコアよりも高い場合、カメラアプリケーション114は、最高品質の画像についての記憶された画像データを、新たに受け取った画像についての画像データで置き換える。このようにして、カメラアプリケーション114は、後続の解析を実施すべき時まで、待機している間に受け取った最高品質の画像についての画像データを記憶している。最高品質の画像についての画像データだけを記憶することによって、画像データを記憶するために使用されるメモリ量が、複数の画像についての画像データを記憶することに比べて削減される。
While waiting for the
カメラアプリケーション114は、バッファを使用して画像データを記憶し得る。たとえば、カメラアプリケーション114は、画像バッファ内の新たに受け取った画像についての画像データを記憶し得る。カメラアプリケーション114はまた、選択された画像バッファ内の最高品質の画像についての画像データを記憶し得る。新たに受け取った画像が、選択された画像バッファ内に画像データが記憶される画像よりも高い品質スコアを有する場合、カメラアプリケーション114は、選択された画像バッファ内の画像データを、新たに受け取った画像についての画像データで置き換え得る。別の例では、新たに受け取った画像が、選択された画像バッファ内に画像データが記憶される画像よりも高い品質スコアを有する場合、カメラアプリケーション114は、選択されたフレームポインタを、画像バッファを指し示すように更新し得る。カメラアプリケーション114は、たとえば、所定の時間間隔の後、または粗分類器123が別の画像を処理する準備ができたとき、選択された画像バッファ内に記憶された画像データを粗分類器123に提供し得る。
The
いくつかの実装では、カメラアプリケーション114は、各画像が取り込まれた時と共に、各画像が取り込まれた時のカメラ112の運動(たとえば、回転運動)を示すデータに基づいて、複数の画像から、さらなる解析のための画像を選択する。たとえば、より古い画像についての品質スコアよりもわずかに低い品質スコア(たとえば、しきい量以内)を有する、より最近に受け取った画像が好ましいことがあり、したがって選択され得る。
In some implementations, the
複数の画像から画像を選択するために3段階の方策が使用され得る。この例では、画像が第1のしきい値未満の品質スコアを有する場合、画像の品質が画像内に示されるオブジェクトを検出または認識するのに十分ではないので、画像は全く使用されないことがある。そのような低品質画像を後続のステージに送らないことによって、普通なら使用されるはずの計算機能力および電力が回避される。1つまたは複数の画像が(第1のしきい値よりも高い)第2のしきい値よりも高い品質スコアを有する場合、カメラアプリケーション114は、第2のしきい値よりも高い品質スコアを有する最新の画像を選択し得る。複数の画像のうち、第2のしきい値よりも高い品質スコアを有する画像がなく、1つまたは複数の画像が2つのしきい値の間の品質スコアを有する場合、カメラアプリケーション114は、最高の品質スコアを有する1つまたは複数の画像のうちの1つを選択し得る。
A three-step strategy can be used to select an image from multiple images. In this example, if the image has a quality score below the first threshold, the image may not be used at all because the quality of the image is not sufficient to detect or recognize the objects shown in the image. .. By not sending such low quality images to subsequent stages, the computational power and power that would otherwise be used is avoided. If one or more images have a quality score higher than the second threshold (higher than the first threshold), the
いくつかの実装では、カメラアプリケーション114は、画像が取り込まれた時のカメラ114の運動を示すデータと、画像の視覚特性とに基づいて、さらなる処理のための画像を選択し得る。カメラ114の運動を示すデータを使用することは、視覚特性だけを使用することと比べて、画像の品質を決定するのに必要とされる視覚特性の数および/または視覚特性のタイプを削減し得る。
In some implementations, the
いくつかの実装では、カメラアプリケーション114は、画像の光特徴に基づいて、さらなる解析のための画像を選択し得る。光特徴は、視覚入力に基づいて計算されるが、ビジョン解析を使用して画像を検査するよりも計算集約的でない方式で計算されるものである。たとえば、画像についての品質スコアは、カメラ114の運動を示すデータと、画像の光特徴の組合せに基づき得る。光特徴は、画像についてのメタデータ内で指定され得る。光特徴は、輝度情報、フォーカス情報、画像内の高レベルコントラストを含む特性を示すヒストグラム、および/または(たとえば、前の画像内のオブジェクトの位置と、この現画像内の同一のオブジェクトの位置とに基づく)画像内のオブジェクトの運動を含み得る。光データと同様に、メタデータ内で既に指定されている画像の他の特徴が使用され得る。このデータはさらなる画像解析を必要としないからである。
In some implementations,
いくつかの実装では、画像選択プロセスは、ハードウェア、たとえばモバイルデバイス110のプロセッサとは別々のハードウェア回路またはコントローラ内で実施される。このようにして、モバイルデバイスのプロセッサは、どんなデータも処理する必要がなく、画像選択プロセスについてのどんな命令も実行する必要がなく、その結果、プロセッサに対して課される要求がさらに低くなる。たとえば、ハードウェア回路は、IMU、ジャイロスコープ、または他のセンサから運動データを受け取り、データを使用して、画像が粗分類器123に送られるべき十分な品質(たとえば、運動または回転ジッタのしきい量未満)を有するかどうかを判定する。画像がハードウェア回路によって十分な品質を有すると検出される場合、ハードウェア回路はプロセッサをウェイクアップし、画像に関する粗分類をプロセッサに実施させる。
In some implementations, the image selection process is performed in hardware, eg, a hardware circuit or controller separate from the processor of the
粗分類器123は複数の粗分類器を含み得、画像内に示されるオブジェクトの1つまたは複数のクラスの存在を検出する。粗分類器123は、画像がオブジェクトのクラスを示す1つまたは複数の特徴を含むか否かに基づいて、オブジェクトのクラスの存在を検出し得る。粗分類器123は、低い計算解析を実施して、そのオブジェクトのクラス内のオブジェクトの存在を検出するための軽量モデルを含み得る。たとえば、粗分類器123は、オブジェクトの各クラスについて、画像内に示される視覚特徴の限定されたセットを検出し、画像がオブジェクトのクラス内に含まれるオブジェクトを示すかどうかを判定し得る。特定の例では、粗分類器123は、画像が以下のクラスのうちの1つまたは複数に分類されるオブジェクトを示すかどうかを検出し得る。テキスト、バーコード、ランドマーク、メディアオブジェクト(たとえば、アルバムカバー、映画ポスタなど)、またはアートワークオブジェクト(たとえば、絵画、彫刻など)。バーコードについて、粗分類器123は、画像が相異なる幅を有する平行線を含むかどうかを判定し得る。
The
いくつかの実装では、粗分類器123は、トレーニングされた機械学習モデル(たとえば、畳込みニューラルネットワーク)を使用して、画像の視覚特徴に基づいて画像を分類する。たとえば、機械学習モデルは、それぞれのクラスでラベル付けされるラベル付き画像を使用してトレーニングされ得る。機械学習モデルは、オブジェクトのクラスの特定のセットのうちの0個以上に画像を分類するようにトレーニングされ得る。機械学習モデルは、画像の視覚特徴に関するデータを入力として受け取り、オブジェクトのクラスの特定のセット内のオブジェクトのクラスのうちの0個以上への分類を出力し得る。
In some implementations, the
粗分類器123は、オブジェクトのクラスが画像内で検出されたかどうかを指定するデータを出力し得る。粗分類器123はまた、オブジェクトのクラスの存在が画像内で検出されたことの信頼度を示す信頼値、および/または実際のオブジェクト、たとえばエッフェル塔が画像内に示されることの信頼度を示す信頼値を出力し得る。
The
いくつかの実装では、カメラアプリケーション114は複数の粗分類器を含む。この例では、各粗分類器は、オブジェクトの特定のクラスの存在を検出し、特定のクラスについての信頼スコアを出力し得る。複数の粗分類器のそれぞれは、それぞれの他の粗分類器とはオブジェクトの異なるクラスの存在を検出し得る。
In some implementations,
いくつかの実装では、粗分類器123は、複数の粗分類器の信頼スコアを、たとえば同時に決定し得る複合粗分類器123である。たとえば、複合粗分類器は、画像について、オブジェクトの複数の異なるクラスについての信頼スコアを同時に決定し得る。複合粗分類器は、各分類器に共通のコア部分と、オブジェクトのクラスについてのクラスごとの確率をそれぞれ決定するいくつかのモジュールとを含み得る。これにより、画像内に示されるオブジェクトの1つまたは複数のクラスの存在を検出するためにカメラアプリケーション114によって実施される全計算が、たとえば冗長な計算を削減することによって削減され得る。
In some implementations, the
粗分類器123はまた、画像内で検出されたオブジェクトについてのデータを指定する注釈を出力し、データを1つまたは複数のビジョンアナライザ134に提供し得る。たとえば、粗分類器123が画像内のテキストの存在を検出した場合、粗分類器123は、テキストがどこに配置されるか、およびテキストの向きを指定する注釈を提供し得る。粗分類器123はまた、検出されたテキストのタイプ、たとえば電話番号、住所などを指定する注釈を提供し得る。そのようなデータは、テキストアナライザ126によって後で実際のテキストを認識するためにOCRを実施する計算コストおよび時間を節約し得る。同様に、粗分類器123が画像内のバーコードを検出した場合、粗分類器123は、バーコードアナライザ128によってバーコードを読み取る/復号化する計算コストおよび時間を節約するために、バーコードの位置およびタイプを指定する注釈を提供し得る。
The
前述のように、粗分類が前の画像について完了したときなどの適切な時に、画像が粗分類のために選択され得る。いくつかの実装では、カメラアプリケーション114は、処理レートに基づいて、画像に関する粗分類を周期的に実施し得る。たとえば、カメラアプリケーション114は、毎秒(または何らかの他の時間枠)ごとに画像を選択し(または選択されたフレームバッファから、記憶された最良の画像を取り出し)、画像に関する粗分類を実施し得る。
As mentioned above, an image may be selected for the coarse classification at the appropriate time, such as when the coarse classification is completed for the previous image. In some implementations,
いくつかの実装では、カメラアプリケーション114は、オブジェクトの1つまたは複数のクラスの存在が粗分類器123によって1つまたは複数の以前に解析された画像内で検出されたかどうかに基づいて、処理レートを動的かつ自動的に調節し得る。たとえば、粗分類器123が検出するように構成されるオブジェクトのクラスのうちの1つの中に分類されるオブジェクトの画像をカメラ112が取り込んでいない場合、カメラアプリケーション114は、画像が粗分類器123によって解析されているレートを低減し得る。この例では、オブジェクトのクラスの存在が前の画像(またはしきい数の前の画像)内で検出されなかった場合、カメラアプリケーション114は、処理レートを低減して、各粗分類間の時間枠を増大させ得る。これは、関連するオブジェクトを示さない画像が取り込まれているとき、粗分類を実施する際に消費される計算機能力および電力の量を削減し得る。たとえば、この処理レート調節の結果、画像を分類するのに使用されるCPUサイクルが少なくなり得る。
In some implementations, the
同様に、オブジェクトの少なくとも1つのクラスの存在が前の画像内で検出された場合(または少なくともしきい数の前の画像)、カメラアプリケーション114は、処理レートを増大して、各粗分類間の時間枠を削減し得る。このようにして、ユーザがカメラ112を使用して、関連するオブジェクトの画像を取り込んでいるとき、カメラアプリケーション114は、有用な、関連するコンテンツを識別し、ユーザに提示し得る。この増大された処理レートは一時的であり得る。たとえば、指定の時間枠の後、または画像または少なくともしきい数の画像内のオブジェクトのクラスの存在を検出しなかったことに応答して、処理レートの増大が通常の処理レート(または低減された処理レート)に低減され得る。
Similarly, if the presence of at least one class of objects is detected in the previous image (or at least a threshold number of previous images), the
カメラアプリケーション114は、ユーザがカメラアプリケーション114を開き、または画像解析およびコンテンツ提示が実施されるカメラアプリケーションのモードを選択してからの時間量に基づいて、粗分類についての処理レートを調節し得る。たとえば、ユーザは、カメラアプリケーション114を開き、または解析およびコンテンツ提示モードに入った直後に、画像を取り込み、画像内に示されるオブジェクトに関するコンテンツを閲覧することによりアクティブであり得る。この例では、カメラアプリケーション114は、カメラアプリケーション114が開かれ、または解析およびコンテンツ提示モードに入ったとき、より高速な処理レートを使用し得、次いで、指定の時間量が経過した後、またはオブジェクトのクラスのうちの少なくとも1つの中のオブジェクトを検出することなく、粗分類器123が少なくともしきい数の画像を評価した後、より低速な処理レートを使用し得る。
The
カメラアプリケーション114は、カメラアプリケーション114とのユーザ対話に基づいて、粗分類についての処理レートを調節し得る。たとえば、ユーザが、解析された画像に基づいて提供される結果または他のコンテンツと対話した場合、カメラアプリケーション114は、粗分類のための処理レートを増大し得、または初期高速処理レートを維持し得る。カメラアプリケーション114は、カメラアプリケーション114との現セッションでのユーザ対話の頻度、および/またはユーザの履歴ユーザセッションを含む複数のユーザセッションでのユーザ対話の頻度に基づいて処理レートを調節し得る。ユーザセッションは、カメラアプリケーション114を開き、閉じること、および/または画像解析およびコンテンツ提示モードの開始および終了によって定義され得る。
The
いくつかの実装では、カメラアプリケーション114またはモバイルデバイス110自体は、前の画像が1つまたは複数のクラスのオブジェクトを示すものとして検出されたかどうかに基づいて、粗分類についての処理レートを決定および調節する電源管理システムを含む。画像が粗分類器によって分類されていないとき、モバイルデバイス110のプロセッサ(または画像を解析するのに使用されるコントローラ)は、電力の消費を抑えるためにスリープし得る(たとえば、スリープモードに入り、命令を実行しない)。
In some implementations, the
粗分類器123が画像内のオブジェクトのクラスの存在を検出した場合、粗分類器123は、画像についての画像データを適切なオブジェクト認識器134に提供し得る。オブジェクト認識器134は、画像内のテキストを認識する(たとえば、文字、語などを認識する)テキスト認識器126、画像内のバーコード(たとえば、QRコードを含む)を認識する(たとえば、復号化する)バーコード認識器128、および画像内のランドマークを認識する(たとえば、実際のランドマークを識別する)ランドマーク認識器130を含み得る。カメラアプリケーション114は、アナライザ126、128、および130に加えて、またはそれらの代わりに、他のタイプのオブジェクトのためのアナライザを含み得る。たとえば、カメラアプリケーション114は、メディアカバー(たとえば、アルバムカバー)アナライザ、アートワークアナライザ、および/または他の適切なアナライザを含み得る。いくつかの実装では、カメラアプリケーション114は、オブジェクトの複数の異なるクラス、たとえばテキスト、バーコード、ランドマーク、メディアカバー、アートワークなどを認識する単一のオブジェクト認識器を含む。
If the
粗分類器123が画像内のテキストの存在を検出した場合、粗分類器123は、画像についての画像データをテキスト認識器126に提供し得る。粗分類器123が画像内のバーコードの存在を検出した場合、粗分類器123は、画像についての画像データをバーコード認識器128に提供し得る。粗分類器123が画像内のランドマークの存在を検出した場合、粗分類器123は、画像についての画像データをランドマーク認識器128に提供し得る。
If the
いくつかの実装では、画像がしきい値を満たす(たとえば、しきい値以上の)信頼値を有するオブジェクトの各クラスについて、粗分類器123は、画像についての画像データを各オブジェクト認識器134に提供する。たとえば、粗分類器123は、画像がテキストを示すことの信頼値を決定し得、信頼値がしきい値を満たす場合、粗分類器123は、画像をテキスト認識器126に提供し得る。
In some implementations, for each class of objects whose image has a confidence value that meets or exceeds the threshold (for example, above or below the threshold), the
各オブジェクト認識器134は、粗分類器123よりも詳細な画像解析を実施して、粗分類器123から受け取った画像内に示されるオブジェクトを認識し得る。たとえば、オブジェクト認識器134は、エッジ検出、パターン認識、および他のコンピュータビジョン技法を使用して、画像内に示されるオブジェクトを認識し、画像から情報を抽出し得る。
Each
テキスト認識器126は、光学的文字認識(OCR)を使用して、画像内に示されるテキストを認識し得る。バーコード認識器128は、画像内に示されるバーコードを読み取り/復号化し、バーコードによって表されるオブジェクト(たとえば、製品)についての情報を取得し得る。ランドマーク認識器130は、ランドマークの画像データおよびパターン認識を使用して、画像内に示されるランドマークを認識し得る。いくつかの実装では、カメラアプリケーション114は、オブジェクトを認識し、画像内に示されるバーコードを読み取る際に使用するための、特定のオブジェクトについての画像データおよびバーコードデータを含むオブジェクト索引115を記憶する。オブジェクトについての画像データは、オブジェクトの画像を認識するためにコンピュータビジョン解析において使用され得る、オブジェクトの視覚特徴を指定するデータを含み得る。
The text recognizer 126 may use Optical Character Recognition (OCR) to recognize the text shown in the image. The barcode recognizer 128 may read / decode the barcode shown in the image to obtain information about the object represented by the barcode (eg, the product). The
カメラアプリケーション114は、オブジェクトの少なくともいくつかのクラスについてのフレーム間処理を実施し得る。たとえば、カメラアプリケーション114は、テキスト認識器126がその中のテキストを認識した画像に対するフレーム間処理127を実施し得、カメラアプリケーション114は、ランドマーク認識器130がその中のランドマークを認識した画像に対するフレーム間処理131を実施し得る。一般には、フレーム間処理は、現画像内のオブジェクトを検出する際に、前の画像内で認識されたオブジェクトに関するデータを活用する。
The
テキストについてのフレーム間処理127は、2つの画像内のテキストの同一の行(または同一の部分)の間の相関を確立して、同一のテキストが検出されているかどうかを判定し得る。次いで、カメラアプリケーション114は、最良のバージョンのテキスト、たとえばテキスト認識器126がより高い品質であると判定したテキストを保ち得、これにより、テキスト認識の精度が改善され得る。テキストの2つの部分が同一であるかどうかを判定するために、カメラアプリケーション114は、フレーム間処理127を使用して、2つ(またはそれ以上)の画像内のテキストの2つの部分の間の距離、および2つの画像内のテキスト間の編集距離を評価し得る。カメラアプリケーション114は、フレーム間処理127を使用して、2つの画像内のテキストの部分の位置に基づいて、最適な追跡を使用してテキストを追跡することによって、距離を直接的に決定し、かつ/または2つの画像の取込みの間のモバイルデバイス110の運動に基づいてテキストがどこにあるかを予測し得る。編集距離は、2つの画像内のテキストの部分の間で異なる文字数または文字の割合を示し得る。距離および編集距離がどちらもしきい値未満である場合、2つ以上の画像が相関され得、最高の信頼値および/または最高の品質スコアを有する画像が、後に使用のために保持され得る。
Interframe processing 127 for text may establish a correlation between the same line (or the same part) of text in two images to determine if the same text has been detected. The
いくつかの実装では、テキストの部分が、複数の画像から保持され得る。たとえば、カメラアプリケーション114は、フレーム間処理127を使用して、行ごとに、文字ごとに、または語ごとにテキストを相関させ得る。カメラアプリケーション114は、テキストの相関された行のどれが、最高の信頼度および/または最高の品質を有するかを識別し、最高の信頼度および/または最高の品質のテキストの行を有する画像のその部分をテキストアナライザ126のために保持し得る。同様に、カメラアプリケーション114は、相関された語(または文字)のどれが、最高の信頼度および/または最高の品質を有するかを判定し、最高の信頼度および/または最高の品質の語(または文字)を有する画像のその部分を保持し得る。したがって、たとえば、ある時間枠の間、カメラが同一の文書または他のテキストソースに向いているために、複数の画像が同一のテキストを有するとき、複数の画像部分内で認識されたテキストが、結果識別器132に提供され、テキストの各部分の最高品質バージョンが提供され得る。
In some implementations, parts of the text can be preserved from multiple images. For example,
ランドマーク(および他のタイプのオブジェクト)についてのフレーム間処理131は、画像が特定のランドマーク(またはオブジェクト)を示すかどうかを判定する際に、特定のランドマーク(または他のオブジェクト)が複数の画像内で検出されたことの信頼度を示す(ランドマーク認識器130によって決定された)信頼値を評価し得る。いくつかの実装では、フレーム間処理131は、画像が結果識別器132のためにランドマークを示すと分類する前に、シーケンス内のどれほどの画像が特定のランドマークを示すと識別されなければならないかを決定するための複数のしきい値を使用する。たとえば、画像がランドマークを示すことを示す信頼値が第1のしきい値よりも大きい場合、カメラアプリケーション114は、画像が特定のランドマークを示すと分類する前に、第1の数の画像が特定のランドマークを示すと分類されなければならないと決定し得る。信頼値が第1のしきい値未満であるが、第2のしきい値よりも大きい場合、カメラアプリケーション114は、画像が特定のランドマークを示すと判定するために、第2の数の画像が特定のランドマークを示すと分類されなければならないことを必要とし得る。ただし第2の数は第1の数よりも大きい。
Interframe processing 131 for landmarks (and other types of objects) has multiple specific landmarks (or other objects) in determining whether an image shows a particular landmark (or object). A confidence value (determined by the landmark recognizer 130) indicating the confidence of what was detected in the image can be evaluated. In some implementations, interframe processing 131 must be identified as how many images in the sequence represent a particular landmark before the image is classified as showing a landmark for the
結果識別器132は、モバイルデバイス110に(たとえば、カメラアプリケーション114内で)提示するためのコンテンツを識別し得る。たとえば、画像がバーコードを含む場合、結果識別器132は、バーコードによって表されるオブジェクトを識別し、オブジェクトに関するコンテンツ(たとえば、画像、タイトルなど)を提示する。画像がランドマークを含む場合、結果識別器132は、ランドマークに関するコンテンツ、たとえばランドマークの写真、ランドマークまでの地図などを識別し、ユーザインターフェース116内で提示し得る。
The
結果識別器132は、コンテンツデータストア116内の、認識されたオブジェクトについてのコンテンツを識別し得る。コンテンツデータストア116は、オブジェクトのセットのそれぞれについてのコンテンツを含み得る。コンテンツデータストア116内のコンテンツが、たとえばカメラアプリケーション114によって、モバイルデバイス110上にロードされ得る。コンテンツは、モバイルデバイス110によって提示され得るテキスト、画像、ビデオ、および/または他の適切なコンテンツを含み得る。
The
オブジェクトについての識別されたコンテンツは、画像と共にユーザインターフェース116内に提示され得る。たとえば、コンテンツは、カメラアプリケーション114内の画像が示されるビューファインダ内に提示され得る。この例では、コンテンツは、オブジェクト、テキスト、またはバーコードが検出されたカメラのリアルタイムビューを覆うオーバーレイ内に提示され得る。
The identified content about the object may be presented in the
粗分類器123が画像内に示されるオブジェクトの複数のクラスを検出した場合、結果識別器132は、各クラスについての結果を識別し、各クラスについての少なくとも1つの結果を提示し得る。たとえば、カメラアプリケーション114は、画像内で検出されたオブジェクトの各クラスに対応する結果を含む結果ページを提示し得る。
If the
いくつかの実装では、カメラアプリケーション114は、検出されたオブジェクトに関するコンテンツをユーザがそれを用いて要求することのできるアイコンを提示し得る。たとえば、画像が電話番号を含む場合、カメラアプリケーション114は、ユーザによって対話される(たとえば、選択される)場合、電話番号への呼出しをモバイルデバイス110に開始させるアイコンを提示し得る。バーコードが検出される場合、カメラアプリケーション114は、ショッピングアプリケーションを立ち上げて、バーコードによって表される製品を購入するためのアイコン、検索アプリケーションまたはウェブブラウザを使用して製品の検索を開始するためのアイコン、および/または、たとえばソーシャルネットワーキングアプリケーションを使用して製品を共有するためのアイコンを提示し得る。ランドマークが検出された場合、カメラアプリケーション114は、マップアプリケーションを立ち上げてマップをランドマークに提示するためのアイコン、ランドマークの検索を開始するためのアイコン、および/またはランドマークの画像を閲覧するためのアイコンを提示し得る。アイコンは、たとえばカメラアプリケーション114のビューファインダ内に、画像と共に提示され得る。
In some implementations, the
いくつかの実装では、カメラアプリケーション114は、どのオブジェクトが検出されているかをユーザが見ることができるように、カメラアプリケーション114のユーザインターフェース116内で、検出されているオブジェクトを強調表示し得る。たとえば、ユーザがカメラ112をオブジェクトに向けている間、粗分類器123は、オブジェクトのクラスの存在を検出し得、画像が、解析のためにオブジェクト認識器134のうちの1つまたは複数に送られ得る。画像が1つまたは複数のオブジェクト認識器134によって解析されている間、カメラアプリケーション114は、認識されているオブジェクトを強調表示し得る。
In some implementations, the
いくつかの実装では、カメラアプリケーション114は、オブジェクト索引115および/またはコンテンツデータストア116内のコンテンツをユーザ向けにカスタマイズする。たとえば、カメラアプリケーション114が、ユーザが特定の場所に移動しようとしていることを示すデータを受け取った場合、カメラアプリケーション114は、特定の場所に位置するオブジェクト、ランドマークなどについての画像データおよび/またはバーコードデータを含めるようにオブジェクト索引115を更新し得る。カメラアプリケーション114はまた、特定の場所に位置するオブジェクト、ランドマークなどに関するコンテンツを含めるようにコンテンツデータストア116を更新し得る。
In some implementations, the
いくつかの実装では、画像解析および結果識別のいくつかの要素が、視覚解析サーバ150に配置される。たとえば、視覚解析サーバ150は、データ通信ネットワーク140、たとえばローカルエリアネットワーク(「LAN」)および広域ネットワーク(「WAN」)、たとえばインターネットを介してカメラアプリケーション114から受け取った画像内に示されるオブジェクトを認識する視覚アナライザ152を含み得る。視覚解析サーバ150はまた、ビジョンアナライザ152によって認識されたオブジェクトに関する結果または他のコンテンツを識別し、結果またはコンテンツをモバイルデバイス110に提供するプロセッサ154をも含み得る。この例では、画像セレクタ122および粗分類器123は、モバイルデバイス110上に、たとえばカメラアプリケーション114の部分として実装され得る。
In some implementations, some elements of image analysis and result identification are placed on the visual analysis server 150. For example, the visual analysis server 150 recognizes an object shown in an image received from a data communication network 140, such as a local area network (“LAN”) and a wide area network (“WAN”), eg, a
カメラアプリケーション114は、粗分類器123が画像についてのオブジェクトのクラスまたは特徴データの存在を検出した画像の部分を視覚解析サーバ154に提供し得る。たとえば、粗分類器123が画像内のランドマークの存在を検出した場合、カメラアプリケーション114は、画像全体を提供することなく、ランドマークを含む画像の部分を視覚解析サーバ150に提供し得る。
The
カメラアプリケーション実装と同様に、(たとえば、しきい値を満たさない品質スコアを有するために)画像が画像セレクタ122によって選択されない場合、カメラアプリケーション114は、画像(または画像の部分)を視覚解析サーバ150に送らないことを決定し得る。同様に、粗分類器123が画像内のオブジェクトのクラスの存在を検出しない場合、カメラアプリケーション114は、画像(または画像の部分)を視覚解析サーバ150に送らないことを決定し得る。
Similar to the camera application implementation, if the image is not selected by the image selector 122 (for example, because it has a quality score that does not meet the threshold), the
画像選択および画像解析のステージが順番に実施されるものとして示されているが、あるステージは並列に実施され得る。たとえば、複数の画像が粗分類器によって同時に分類され得る。同様に、画像が複数の粗分類器によって並列に解析され得る。 Although the stages of image selection and image analysis are shown to be performed in sequence, some stages can be performed in parallel. For example, multiple images can be classified simultaneously by a coarse classifier. Similarly, images can be analyzed in parallel by multiple coarse classifiers.
いくつかの実装では、画像セレクタ122が、モバイルデバイス110のコントローラまたはコントローラの部分として実装される。この例では、粗分類器123および/またはオブジェクト認識器134が、たとえばモバイルデバイス110または視覚解析サーバ150の画像処理装置上で実装され得る。
In some implementations, the
図2は、画像を解析し、画像内に示される1つまたは複数のオブジェクトに関するコンテンツを提示するための例示的プロセス200を示す流れ図である。プロセス200の動作は、たとえば、図1のモバイルデバイス110などの、1つまたは複数のデータ処理装置を含むシステムによって実装され得る。プロセス200はまた、コンピュータ記憶媒体上に記憶された命令によって実装され得、データ処理装置を含むシステムによる命令の実行が、プロセス200の動作をデータ処理装置に実施させる。
FIG. 2 is a flow chart illustrating an
システムは、モバイルデバイスカメラによって取り込まれた画像について、カメラが画像を取り込んだ時のカメラの運動を示すデータにアクセスする(202)。たとえば、システムは画像のストリーム(たとえば、ビデオストリーム)を受け取り得る。システムはまた、各画像について、カメラがその上に設置されるデバイスの運動を示すデータを取得し得る。たとえば、各画像についての運動データは、ジャイロスコープ、加速度計、IMUなどの1つまたは複数の環境センサから受け取られ得る。各画像について、運動データは、画像が取り込まれた時のデバイスの運動を示し得る。たとえば、システムは、カメラが画像を取り込んでいる時にセンサに運動データを要求し得る。 For images captured by mobile device cameras, the system accesses data that indicates the movement of the camera when the camera captures the image (202). For example, the system may receive a stream of images (eg, a video stream). The system may also obtain data for each image that indicates the motion of the device on which the camera is mounted. For example, motion data for each image can be received from one or more environment sensors such as gyroscopes, accelerometers, IMUs, and so on. For each image, the motion data may indicate the motion of the device when the image was captured. For example, the system may request motion data from the sensor when the camera is capturing an image.
別の例では、システムは、画像が取り込まれた時を含むタイムウィンドウを環境センサに提供し得る。それに応答して、環境センサは、タイムウィンドウの間に環境センサによって検出された運動データを提供し得る。次いで、システムは、データに基づいて、画像が取り込まれた時にカメラがどれほど移動していたかを決定し得る。 In another example, the system may provide the environment sensor with a time window that includes when the image was captured. In response, the environment sensor may provide motion data detected by the environment sensor during the time window. The system can then determine how much the camera was moving when the image was captured, based on the data.
システムは、画像についての運動データに基づいて、解析のための特定の画像を画像から選択する(204)。システムは、デバイスの運動が最小であった画像、または回転運動が最小であった画像を選択し得る。前述のように、画像はまた、他の画像が取り込まれた時と比べた、画像が取り込まれた時に基づいて、かつ/または画像の光特徴に基づいて選択され得る。選択は、画像の視覚特性とは無関係に、または画像の視覚特性と組み合わせて行われ得る。 The system selects a specific image from the image for analysis based on the motion data about the image (204). The system may select an image with minimal motion of the device, or an image with minimal rotational motion. As mentioned above, an image can also be selected based on when the image was captured and / or based on the optical characteristics of the image compared to when other images were captured. The selection may be made independently of the visual characteristics of the image or in combination with the visual characteristics of the image.
システムは、1つまたは複数の粗分類器を使用して特定の画像を解析して、特定の画像内に示されるオブジェクトの1つまたは複数のクラスの存在を検出する(206)。各粗分類器は、オブジェクトのそれぞれのクラス、たとえばテキスト、ランドマーク、アートワーク、メディアカバー、バーコードなどの存在を検出するように構成され得る。各粗分類器は、特定の画像がオブジェクトのそのクラスを示すかどうかを指定するデータと、粗分類器がその決定の際に有する信頼度を示す信頼値とを出力し得る。各粗分類器はまた、位置、向きなどの、検出されたオブジェクトの特性を記述するデータを含む注釈を出力し得る。 The system analyzes a particular image using one or more coarse classifiers to detect the presence of one or more classes of objects shown within the particular image (206). Each coarse classifier can be configured to detect the presence of each class of objects, such as text, landmarks, artwork, media covers, barcodes, and so on. Each coarse classifier may output data that specifies whether a particular image represents that class of object and a confidence value that indicates the confidence that the coarse classifier has in making that decision. Each coarse classifier may also output annotations containing data describing the characteristics of the detected object, such as position and orientation.
画像内のオブジェクトの少なくとも1つのクラスの存在を検出したことに応答して、システムは、画像を解析して、画像内に示される1つまたは複数のオブジェクトを認識する(208)。たとえば、システムは、1つまたは複数のコンピュータビジョン技法を使用して、画像内に示されるオブジェクトを認識し得る。使用される技法は、粗分類器によって画像内で検出されたオブジェクトのクラスに基づき得る。たとえば、画像がバーコードを有すると分類される場合、システムは、バーコード認識器を使用してバーコードを読み取り、バーコードによって参照される製品を識別し得る。 In response to detecting the presence of at least one class of objects in the image, the system analyzes the image to recognize one or more objects shown in the image (208). For example, the system may use one or more computer vision techniques to recognize the objects shown in the image. The technique used may be based on the class of objects found in the image by the coarse classifier. For example, if an image is classified as having a barcode, the system may use a barcode recognizer to read the barcode and identify the product referenced by the barcode.
システムは、1つまたは複数の識別されたオブジェクトに関するコンテンツを提示する(212)。たとえば、システムは、画像を覆う1つまたは複数のオーバーレイ内に、または結果ページ上にコンテンツを提示し得る。システムは、ユーザによって対話されるとき、画像内で認識されたオブジェクトに関するコンテンツをシステムに提示させるアイコンを提示し得る。たとえば、アイコンは、認識されたオブジェクトの検索を開始すること、認識されたランドマークまでのマップを表示すること、認識されたオブジェクトの他の画像を提示することなどのためのアイコンを含み得る。 The system presents content about one or more identified objects (212). For example, the system may present content within one or more overlays covering an image, or on a result page. The system may present an icon that causes the system to present content about an object recognized in the image when interacted with by the user. For example, an icon may include an icon for initiating a search for a recognized object, displaying a map to a recognized landmark, presenting another image of a recognized object, and so on.
図3は、解析のための画像を選択するための例示的プロセス300を示す流れ図である。プロセス300の動作は、たとえば、図1のモバイルデバイス110などの1つまたは複数のデータ処理装置を含むシステムによって実装され得る。プロセス300はまた、コンピュータ記憶媒体上に記憶された命令によって実装され得、データ処理装置を含むシステムによる命令の実行が、データ処理装置にプロセス300の動作を実施させる。
FIG. 3 is a flow chart illustrating an
システムは、モバイルデバイスカメラによって取り込まれた画像についての運動データを受け取る(302)。前述のように、運動データは、モバイルデバイスカメラによって画像が取り込まれた時のモバイルデバイスカメラの運動を示し得る。モバイルデバイスカメラは、画像をシーケンスで、たとえばビデオストリームとして取り込み得る。いくつかの実装では、モバイルデバイスカメラは、毎秒20～70フレームの範囲のレートで画像を取り込む。 The system receives motion data for images captured by mobile device cameras (302). As mentioned above, the motion data may indicate the motion of the mobile device camera when the image is captured by the mobile device camera. Mobile device cameras can capture images in sequence, for example as a video stream. In some implementations, mobile device cameras capture images at rates in the range of 20-70 frames per second.
システムは、運動データに基づいて画像の予想品質を決定する(304)。たとえば、画像の予想品質は、画像が取り込まれた時のモバイルデバイスカメラの運動の量に基づき得る。この例では、ぼやけるために運動が画像の品質を低下させるので、運動が多いと品質スコアが低下する。 The system determines the expected quality of the image based on the motion data (304). For example, the expected quality of an image can be based on the amount of motion of the mobile device camera when the image is captured. In this example, more exercise reduces the quality score, as exercise reduces the quality of the image due to blurring.
システムは、画像の予想品質が、画像データが記憶されている最高品質の画像の予想品質よりも高いかどうかを判定する(306)。たとえば、システムは、単一の画像のみについての画像データを記憶し得、この単一の画像は、前の画像が粗分類器に送られた以降に取り込まれた画像の(予想品質に基づく)最高品質の画像であり得る。新しい画像が受け取られるごとに、システムは、新たに受け取った画像が、画像データが現在記憶されている最高品質の画像よりも高い予想品質を有するかどうかを判定し得る。 The system determines if the expected quality of the image is higher than the expected quality of the highest quality image in which the image data is stored (306). For example, the system may store image data for only a single image, which is the image captured since the previous image was sent to the coarse classifier (based on expected quality). It can be the highest quality image. Each time a new image is received, the system may determine if the newly received image has a higher expected quality than the highest quality image currently stored in the image data.
新たに受け取った画像が、前に識別された最高品質の画像よりも高い予想品質を有さない場合、システムは、最高品質の画像についての前に記憶された画像データを引き続き記憶し、別の画像を受け取るのを待機する。新たに受け取った画像が最高品質の画像よりも高い予想品質を有する場合、システムは、最高品質の画像についての画像データを、新たに受け取った画像についての画像データで置き換える(308)。 If the newly received image does not have a higher expected quality than the previously identified top quality image, the system will continue to remember the previously stored image data for the top quality image and another. Wait to receive the image. If the newly received image has a higher expected quality than the highest quality image, the system replaces the image data for the highest quality image with the image data for the newly received image (308).
いくつかの実装では、システムは、最高品質の画像について記憶した画像データに基づいて、予想画像品質についての置換しきい値を設定する。この例では、新たに受け取った画像の予想画像品質が置換しきい値を超える場合、システムは、最高品質の画像についての画像データを、新たに受け取ったデータについての画像データに置き換える。システムは、経時的に置換しきい値を低減し得る。 In some implementations, the system sets replacement thresholds for expected image quality based on the image data stored for the highest quality images. In this example, if the expected image quality of the newly received image exceeds the replacement threshold, the system replaces the image data for the highest quality image with the image data for the newly received data. The system may reduce the replacement threshold over time.
システムは、最高品質の画像についての画像データを粗分類器に提供するかどうかを決定する(310)。いくつかの実装では、システムは、所定の時間枠に基づいて、粗分類器に画像データを提供する。いくつかの実装では、システムは、粗分類器から要求を受け取ったことに応答して、粗分類器に画像データを提供する。 The system determines whether to provide the coarse classifier with image data for the highest quality images (310). In some implementations, the system provides image data to the coarse classifier based on a given time frame. In some implementations, the system provides image data to the coarse classifier in response to receiving a request from the coarse classifier.
システムが粗分類器に画像データを提供しないと決定した場合、システムは、粗分類器に画像データを提供することを決定するまで、引き続き画像を受け取り、最高品質の画像についての画像データを置き換えるかどうかを決定する。このようにして、システムは、最高品質の画像についての画像データをのみを記憶する。 If the system decides not to provide the image data to the coarse classifier, does the system continue to receive the image and replace the image data for the highest quality image until it decides to provide the image data to the coarse classifier? Decide whether or not. In this way, the system stores only image data for the highest quality images.
システムが粗分類器に画像データを提供することを決定した場合、システムは、分類のために粗分類器に画像データを提供する(312)。 If the system decides to provide image data to the coarse classifier, the system will provide the image data to the coarse classifier for classification (312).
図4は、画像を解析するための処理レートを調節するための例示的プロセス400を示す流れ図である。プロセス400の動作は、たとえば、図1のモバイルデバイス110などの、1つまたは複数のデータ処理装置を含むシステムによって実装され得る。プロセス400はまた、コンピュータ記憶媒体上に記憶された命令によって実装され得、データ処理装置を含むシステムによる命令の実行が、プロセス400の動作をデータ処理装置に実施させる。
FIG. 4 is a flow chart showing an
システムは、画像についての画像データを受け取る(402)。画像データは、画像の各ピクセルについてのカラー値を含む、画像についての画像ピクセルデータを含み得る。 The system receives image data about the image (402). The image data may include image pixel data for the image, including color values for each pixel of the image.
システムは、画像が1つまたは複数のクラスに分類されるオブジェクトを含むかどうかを判定する(404)。たとえば、システムは、前述のように、粗分類器を使用して、画像が1つまたは複数のクラスに分類されるオブジェクトを含むかどうかを決定し得る。 The system determines if an image contains objects that fall into one or more classes (404). For example, the system may use a coarse classifier, as described above, to determine whether an image contains objects that fall into one or more classes.
画像が1つまたは複数のクラスに分類されるオブジェクトを含まないとシステムが判定した場合、システムは、画像を解析するための処理レートを低減して、画像が1つまたは複数のクラスのオブジェクトを含むかどうかを判定する(406)。たとえば、システムは、粗分類器を使用して、処理レートに基づいて周期的に画像を解析し得る。1つまたは複数の画像がクラスのうちの1つの中のオブジェクトを含まないとシステムが判定した場合、システムは、処理レートを低減して、画像が解析される頻度を削減し得る。これにより、システムは、注目のオブジェクトを含まない画像が取り込まれているとき、コンピュータリソース使用量および電力消費を削減することが可能となる。システムは、処理レートを低減する前に、所定の数(たとえば、5、10、または別の適切な数)の画像がクラスのうちの1つの上のオブジェクトを含まなくなるまで待機し得る。 If the system determines that the image does not contain objects that fall into one or more classes, the system reduces the processing rate for parsing the image to object in one or more classes of images. Determine if it is included (406). For example, the system may use a coarse classifier to periodically analyze the image based on the processing rate. If the system determines that one or more images do not contain objects in one of the classes, the system may reduce the processing rate to reduce the frequency with which the images are parsed. This allows the system to reduce computer resource usage and power consumption when images that do not contain objects of interest are captured. The system may wait until a given number (eg, 5, 10, or another suitable number) of images no longer contains an object above one of the classes before reducing the processing rate.
画像がクラスのうちの少なくとも1つに分類されるオブジェクトを含むとシステムが判定した場合、システムは処理レートを増大させる(408)。たとえば、システムは、画像が注目のオブジェクトを含むときにシステムが使用するアクティブ処理レートに処理レートを増大させ得る。処理レートが既にアクティブ処理レートであった場合、システムは処理レートを未変更のままにし得る。 If the system determines that the image contains objects that fall into at least one of the classes, the system increases the processing rate (408). For example, the system may increase the processing rate to the active processing rate used by the system when the image contains objects of interest. If the processing rate was already the active processing rate, the system may leave the processing rate unchanged.
システムは、クラスのうちの少なくとも1つのオブジェクトを含む画像を、さらなる解析のために送る(410)。たとえば、システムは、少なくとも1つのクラスのオブジェクトを認識するオブジェクト認識器に画像を送り得る。 The system sends an image containing at least one object of the class for further analysis (410). For example, the system may send an image to an object recognizer that recognizes at least one class of objects.
本明細書において説明される主題および機能的動作の実施形態は、本明細書において開示された構造およびその構造的均等物を含む、デジタル電子回路、有形に実施されたコンピュータソフトウェアまたはファームウェア、コンピュータハードウェア、あるいはそれらのうちの1つまたは複数の組合せとして実装され得る。本明細書において説明される主題の実施形態は、1つまたは複数のコンピュータプログラムとして、すなわちデータ処理装置による実行のための、またはデータ処理装置の動作を制御するための、有形の非一時的プログラムキャリア上に符号化されたコンピュータプログラム命令の1つまたは複数のモジュールとして実装され得る。代替または追加として、プログラム命令は、データ処理装置による実行のために適切な受信機装置への伝送のために情報を符号化するように生成される、人工的に生成された伝播信号、たとえば機械で生成された電気信号、光信号、または電磁信号上に符号化され得る。コンピュータ記憶媒体は、機械可読記憶デバイス、機械可読記憶基板、ランダムもしくはシリアルアクセスメモリデバイス、またはそれらのうちの1つまたは複数の組合せであり得る。 Embodiments of the subject matter and functional operation described herein are digital electronic circuits, tangibly implemented computer software or firmware, computer hardware, including the structures disclosed herein and their structural equivalents. It can be implemented as wear, or a combination of one or more of them. The embodiments of the subject described herein are tangible, non-temporary programs as one or more computer programs, i.e., for execution by a data processor or for controlling the operation of the data processor. It can be implemented as one or more modules of computer program instructions encoded on the carrier. Alternatively or additionally, the program instruction is an artificially generated propagating signal, eg, a machine, that is generated to encode information for transmission to a receiver device suitable for execution by the data processing device. Can be encoded on an electrical, optical, or electromagnetic signal generated in. The computer storage medium can be a machine-readable storage device, a machine-readable storage board, a random or serial access memory device, or a combination thereof.
「データ処理装置」という用語は、例としてプログラム可能プロセッサ、コンピュータ、または複数のプロセッサもしくはコンピュータを含む、データを処理するためのすべての種類の装置、デバイス、および機械を包含する。装置は、専用論理回路、たとえばFPGA(フィールドプログラマブルゲートアレイ)またはASIC(特定用途向け集積回路)を含み得る。装置はまた、ハードウェアに加えて、当該のコンピュータプログラムのための実行環境を作成するコード、たとえばプロセッサファームウェア、プロトコルスタック、データベース管理システム、オペレーティングシステム、あるいはそれらのうちの1つまたは複数の組合せを構成するコードをも含み得る。 The term "data processor" includes all types of devices, devices, and machines for processing data, including, for example, programmable processors, computers, or multiple processors or computers. The device may include dedicated logic circuits, such as FPGAs (Field Programmable Gate Arrays) or ASICs (Application Specific Integrated Circuits). In addition to the hardware, the device also contains code that creates an execution environment for the computer program in question, such as processor firmware, protocol stacks, database management systems, operating systems, or a combination of one or more of them. It can also include constituent code.
コンピュータプログラム(プログラム、ソフトウェア、ソフトウェアアプリケーション、モジュール、ソフトウェアモジュール、スクリプト、またはコードとも呼ばれる)は、コンパイル型言語またはインタプリタ型言語、宣言型言語または手続き型言語を含む任意の形態のプログラミング言語で書かれ得、スタンドアロンプログラムとして、またはモジュール、コンポーネント、サブルーチン、もしくはコンピューティング環境内での使用に適した他のユニットとしての形態を含む任意の形態として配置され得る。コンピュータプログラムは、必須ではないがファイルシステム内のファイルに対応し得る。プログラムは、他のプログラムまたはデータを保持するファイルの一部、たとえばマークアップ言語文書内に記憶された1つまたは複数のスクリプト内に、当該のプログラム専用の単一のファイル内に、または複数の協調ファイル、たとえば1つまたは複数のモジュール、サブプログラム、もしくはコードの部分を記憶するファイル内に記憶され得る。コンピュータプログラムは、1つのコンピュータ上で、または1つの場所に配置され、もしくは複数の場所にわたって分散され、通信ネットワークによって相互接続される複数のコンピュータ上で実行されるように配置され得る。 Computer programs (also called programs, software, software applications, modules, software modules, scripts, or code) are written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages. It can be deployed as any form, including as a stand-alone program, or as a module, component, subroutine, or other unit suitable for use within a computing environment. Computer programs can accommodate files in the file system, although they are not required. A program is part of a file that holds other programs or data, such as in one or more scripts stored in a markup language document, in a single file dedicated to that program, or in multiples. It can be stored in a collaborative file, such as a file that stores one or more modules, subprograms, or parts of code. Computer programs may be located on one computer, in one location, or distributed across multiple locations and run on multiple computers interconnected by communication networks.
本明細書において説明されるプロセスおよび論理フローは、入力データに対して演算し、出力を生成することによって機能を実施するように1つまたは複数のコンピュータプログラムを実行する1つまたは複数のプログラム可能コンピュータによって実施され得る。プロセスおよび論理フローはまた、専用論理回路、たとえばFPGA(フィールドプログラマブルゲートアレイ)、ASIC(特定用途向け集積回路)、またはGPGPU(汎用グラフィックス処理装置)によって実施され得、装置はまた、それらとして実装され得る。 The processes and logical flows described herein are programmable one or more to execute one or more computer programs to perform functions by computing on input data and producing output. Can be done by computer. Processes and logic flows can also be implemented by dedicated logic circuits such as FPGAs (Field Programmable Gate Arrays), ASICs (Application Specific Integrated Circuits), or GPGPUs (General Purpose Graphics Processing Equipment), and the equipment is also implemented as them. Can be done.
コンピュータプログラムの実行に適したコンピュータは、例として、汎用もしくは専用マイクロプロセッサまたはその両方、あるいは任意の他の種類の中央演算処理装置に基づき得る。一般には、中央演算処理装置は、読取り専用メモリもしくはランダムアクセスメモリまたはその両方から命令およびデータを受け取る。コンピュータの不可欠な要素は、命令を実施または実行するための中央演算処理装置と、命令およびデータを記憶するための1つまたは複数のメモリデバイスである。一般には、コンピュータはまた、データを記憶するための1つまたは複数の大容量記憶デバイス、たとえば磁気ディスク、光磁気ディスク、または光ディスクをも含み、あるいはそれらからデータを受け取り、もしくはそれらにデータを転送し、またはその両方を行うように動作可能に結合される。しかしながら、コンピュータはそのようなデバイスを有する必要はない。さらに、コンピュータは、ほんのいくつかの例を挙げれば、別のデバイス、たとえば携帯電話、携帯情報端末(PDA)、モバイルオーディオもしくはビデオプレーヤ、ゲームコンソール、全地球測位システム(GPS)受信機、またはポータブル記憶デバイス、たとえばユニバーサルシリアルバス(USB)フラッシュドライブ内に組み込まれ得る。 A computer suitable for executing a computer program may be based, for example, on a general purpose and / or dedicated microprocessor, or any other type of central processing unit. In general, central processing units receive instructions and data from read-only memory and / or random access memory. Essential elements of a computer are a central processing unit for executing or executing instructions and one or more memory devices for storing instructions and data. In general, a computer also includes, or receives data from, or transfers data to one or more mass storage devices for storing data, such as magnetic disks, magneto-optical disks, or optical disks. And / or are operably combined to do both. However, the computer does not have to have such a device. In addition, the computer may be another device, such as a mobile phone, personal digital assistant (PDA), mobile audio or video player, game console, Global Positioning System (GPS) receiver, or portable, to name just a few. It can be embedded in a storage device, such as a universal serial bus (USB) flash drive.
コンピュータプログラム命令およびデータを記憶するのに適したコンピュータ可読媒体は、例として、半導体メモリデバイス、たとえばEPROM、EEPROM、およびフラッシュメモリデバイス、磁気ディスク、たとえば内部ハードディスクまたは取外し可能ディスク、光磁気ディスク、ならびにCD-ROMおよびDVD-ROMディスクを含む、すべての形態の不揮発性メモリ、媒体、およびメモリデバイスを含む。プロセッサおよびメモリは、専用論理回路によって補足され、またはその中に組み込まれ得る。 Computer-readable media suitable for storing computer program instructions and data include, for example, semiconductor memory devices such as EPROM, EEPROM, and flash memory devices, magnetic disks such as internal hard disks or removable disks, optomagnetic disks, and Includes all forms of non-volatile memory, media, and memory devices, including CD-ROMs and DVD-ROM discs. Processors and memory can be supplemented or incorporated into dedicated logic circuits.
ユーザとの対話を実現するために、本明細書において説明される主題の実施形態は、ユーザに情報を表示するためのディスプレイデバイス、たとえばCRT(陰極線管)もしくはLCD(液晶ディスプレイ)モニタと、ユーザがそれによってコンピュータに入力を与えることのできるキーボードおよびポインティングデバイス、たとえばマウスもしくはトラックボールとを有するコンピュータ上に実装され得る。他の種類のデバイスもユーザとの対話を実現するために使用され得、たとえば、ユーザに提供されるフィードバックは、任意の形態の感覚フィードバック、たとえば視覚フィードバック、音声フィードバック、または触覚フィードバックであり得、ユーザからの入力は、音響、音声、または触覚入力を含む任意の形態として受け取られ得る。さらに、コンピュータは、ユーザによって使用されるデバイスに文書を送り、デバイスから文書を受け取ることによって、たとえば、ユーザのクライアントデバイス上のウェブブラウザから受け取った要求に応答して、ウェブブラウザにウェブページを送ることによって、ユーザと対話し得る。 In order to achieve user interaction, embodiments of the subject described herein include a display device for displaying information to the user, such as a CRT (cathode tube) or LCD (liquid crystal display) monitor, and the user. Can be implemented on a computer with a keyboard and pointing device that can thereby give input to the computer, such as a mouse or trackball. Other types of devices can also be used to provide user interaction, for example, the feedback provided to the user can be any form of sensory feedback, such as visual feedback, audio feedback, or tactile feedback. Input from the user can be received in any form, including acoustic, voice, or tactile input. In addition, the computer sends a document to the device used by the user, and by receiving the document from the device, for example, in response to a request received from a web browser on the user's client device, sends a web page to the web browser. By doing so, you can interact with the user.
本明細書において説明される主題の実施形態は、バックエンド構成要素をたとえばデータサーバとして含み、またはミドルウェア構成要素、たとえばアプリケーションサーバを含み、またはフロントエンド構成要素、たとえばユーザが本明細書において説明される主題の実装とそれを通じて対話することができるグラフィカルユーザインターフェースもしくはウェブブラウザを有するクライアントコンピュータを含み、あるいは1つまたは複数のそのようなバックエンド、ミドルウェア、またはフロントエンド構成要素の任意の組合せを含むコンピューティングシステムとして実装され得る。システムの構成要素は、任意の形態または媒体のデジタルデータ通信、たとえば通信ネットワークによって相互接続され得る。通信ネットワークの例には、ローカルエリアネットワーク(「LAN」)および広域ネットワーク(「WAN」)、たとえばインターネットが含まれる。 Embodiments of the subject described herein include back-end components, such as data servers, or middleware components, such as application servers, or front-end components, such as the user described herein. Includes an implementation of a subject and a client computer with a graphical user interface or web browser through which it can interact, or includes any combination of one or more such backends, middleware, or frontend components. It can be implemented as a computing system. The components of the system may be interconnected by any form or medium of digital data communication, such as a communication network. Examples of communication networks include local area networks (“LAN”) and wide area networks (“WAN”), such as the Internet.
コンピューティングシステムはクライアントおよびサーバを含み得る。クライアントとサーバは、一般には互いに離れており、通常は通信ネットワークを通じて対話する。クライアントとサーバの関係は、それぞれのコンピュータ上で実行中の、互いにクライアント-サーバ関係を有するコンピュータプログラムによって生じる。 Computing systems can include clients and servers. Clients and servers are generally separated from each other and usually interact through a communication network. The client-server relationship arises from computer programs running on each computer that have a client-server relationship with each other.
本明細書は多くの特定の実装詳細を含むが、これらは、何らかの発明の範囲、または特許請求され得るものの範囲に関する限定と解釈されるべきではなく、むしろ特定の発明の特定の実施形態に特有のものであり得る特徴の説明と解釈されるべきである。本明細書において別々の実施形態の状況において説明されるいくつかの特徴はまた、単一の実施形態において組み合わせて実装され得る。逆に、単一の実施形態の状況において説明される様々な特徴はまた、複数の実施形態として別々に、または任意の適切な部分組合せとして実装され得る。さらに、特徴が、いくつかの組合せとして働くものとして上記で説明され、さらにはそのように最初に特許請求され得るが、いくつかのケースでは、特許請求される組合せからの1つまたは複数の特徴が組合せから削除され得、特許請求される組合せは、部分組合せまたは部分組合せの変形を対象とし得る。 Although this specification contains many specific implementation details, these should not be construed as limitations relating to the scope of any invention, or claims, but rather specific to a particular embodiment of a particular invention. It should be interpreted as an explanation of the characteristics that can be. Some of the features described herein in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, the various features described in the context of a single embodiment can also be implemented separately as multiple embodiments or as any suitable subcombination. Further, the features are described above as acting as some combination, and even as such they may be claimed first, but in some cases one or more features from the claimed combination. May be removed from the combination and the claimed combination may be subject to a partial combination or a modification of the partial combination.
同様に、図面では動作が特定の順序で示されるが、このことは、望ましい結果を達成するために、そのような動作が図示される特定の順序で、または順次的に実施されること、またはすべての図示される動作が実施されることを必要とすると理解されるべきではない。いくつかの環境では、マルチタスキングおよび並列処理が有利であり得る。さらに、前述の実施形態での様々なシステムモジュールおよび構成要素の分離が、すべての実施形態においてそのような分離を必要とすると理解されるべきではなく、記載のプログラム構成要素およびシステムは一般に、単一のソフトウェア製品として共に一体化され、または複数のソフトウェア製品としてパッケージ化され得ることを理解されたい。 Similarly, the drawings show the operations in a particular order, which means that such operations are performed in the specific order shown, or sequentially, in order to achieve the desired result. It should not be understood that all illustrated actions need to be performed. In some environments, multitasking and parallel processing can be advantageous. Moreover, the separation of the various system modules and components in the aforementioned embodiments should not be understood as requiring such separation in all embodiments, and the program components and systems described are generally simply simple. It should be understood that they can be integrated together as one software product or packaged as multiple software products.
主題の特定の実施形態が説明された。他の実施形態は以下の特許請求の範囲内にある。たとえば、請求項に記載のアクションは、異なる順序で実施され、それでもなお望ましい結果を達成し得る。一例として、添付の図に示されるプロセスは、望ましい結果を達成するために、図示される特定の順序、または順次的順序を必ずしも必要とするわけではない。いくつかの実装では、マルチタスキングおよび並列処理が有利であり得る。 Specific embodiments of the subject were described. Other embodiments are within the scope of the following claims. For example, the actions described in the claims may be performed in a different order and still achieve the desired result. As an example, the process shown in the attached figure does not necessarily require the specific order or sequential order shown to achieve the desired result. In some implementations, multitasking and parallelism may be advantageous.
100 環境
110 モバイルデバイス
112 カメラ
113 慣性測定ユニット(IMU)
114 カメラアプリケーション
115 オブジェクト索引
116 ユーザインターフェース
116 コンテンツデータストア
122 画像セレクタ
123 粗分類器
126 テキストアナライザ、テキスト認識器
127 フレーム間処理
128 バーコードアナライザ、バーコード認識器
130 ランドマーク認識器
131 フレーム間処理
132 結果識別器
140 データ通信ネットワーク
150 視覚解析サーバ
152 視覚アナライザ
154 結果プロセッサ
100 environment
110 mobile device
112 camera
113 Inertial Measurement Unit (IMU)
114 Camera application
115 Object index
116 User interface
116 Content data store
122 Image selector
123 Coarse classifier
126 Text analyzer, text recognizer
127 Inter-frame processing
128 Bar code analyzer, bar code recognizer
130 Landmark recognizer
131 Inter-frame processing
132 Result classifier
140 data communication network
150 Visual analysis server
152 Visual Analyzer
154 result processor
Claims (20)
データ処理装置と、
前記データ処理装置とデータ通信しているメモリ記憶装置とを備え、前記メモリ記憶装置が、前記データ処理装置によって実行可能であり、そのような実行時に、前記画像の特定の画像について、
粗分類器を使用して前記特定の画像を解析して、前記特定の画像が1つまたは複数の特定のクラスのオブジェクトのうちの少なくとも1つのオブジェクトを示すかどうかを判定すること、
前記特定の画像が前記1つまたは複数の特定のクラスのオブジェクトのうちの少なくとも1つのオブジェクトを示すとき、
オブジェクト認識処理を使用して前記特定の画像を解析して、前記特定の画像に示される1つまたは複数のオブジェクトを認識すること、
前記1つまたは複数の認識されたオブジェクトに関するコンテンツを提示すること、および
前記特定の画像が前記1つまたは複数の特定のクラスのオブジェクトのうちの少なくとも1つのオブジェクトを示さないとき、前記オブジェクト認識処理を使用して前記特定の画像を解析しないと決定すること
を含む動作を前記データ処理装置に実施させる命令を記憶するメモリ記憶装置とを備えるシステム。 With a camera configured to capture images,
Data processing equipment and
The data processing device and the memory storage device for data communication are provided, and the memory storage device can be executed by the data processing device, and at the time of such execution, with respect to a specific image of the image.
Analyzing the particular image using a coarse classifier to determine if the particular image represents at least one object of one or more specific classes of objects.
When the particular image shows at least one of the objects of the one or more particular classes.
Analyzing the particular image using object recognition processing to recognize one or more objects shown in the particular image,
The object recognition process is to present content about the one or more recognized objects, and when the particular image does not show at least one object of the one or more specific class of objects. A system comprising a memory storage device that stores instructions for causing the data processing device to perform an operation including determining not to analyze the particular image using.
粗分類器を使用してコンピューティングシステムによって特定の画像を解析して、前記特定の画像が1つまたは複数の特定のクラスのオブジェクトのうちの少なくとも1つのオブジェクトを示すかどうかを判定するステップと、
前記特定の画像が前記1つまたは複数の特定のクラスのオブジェクトのうちの少なくとも1つのオブジェクトを示すとき、
オブジェクト認識処理を使用して前記特定の画像を解析して、前記特定の画像に示される1つまたは複数のオブジェクトを、前記コンピューティングシステムによって認識するステップと、
前記1つまたは複数の認識されたオブジェクトに関するコンテンツを、前記コンピューティングシステムによって提示するステップと、
前記特定の画像が前記1つまたは複数の特定のクラスのオブジェクトのうちの少なくとも1つのオブジェクトを示さないとき、前記オブジェクト認識処理を使用して前記特定の画像を解析しないと、前記コンピューティングシステムによって決定するステップとを含む、コンピュータ実装方法。 A computer implementation method for efficiently providing content in response to objects contained in an image.
A step of analyzing a particular image by a computing system using a coarse classifier to determine if the particular image represents at least one of one or more objects of a particular class. ,
When the particular image shows at least one of the objects of the one or more particular classes.
A step of analyzing the particular image using object recognition processing to recognize one or more objects shown in the particular image by the computing system.
A step of presenting content about the one or more recognized objects by the computing system.
When the particular image does not represent at least one of the objects of the one or more particular classes, the computing system must analyze the particular image using the object recognition process. Computer implementation methods, including deciding steps.
粗分類器を使用して特定の画像を解析して、前記特定の画像が1つまたは複数の特定のクラスのオブジェクトのうちの少なくとも1つのオブジェクトを示すかどうかを判定するステップと、
前記特定の画像が前記1つまたは複数の特定のクラスのオブジェクトのうちの少なくとも1つのオブジェクトを示すとき、
オブジェクト認識処理を使用して前記特定の画像を解析して、前記特定の画像に示される1つまたは複数のオブジェクトを認識するステップと、
前記1つまたは複数の認識されたオブジェクトに関するコンテンツを提示するステップと、
前記特定の画像が前記1つまたは複数の特定のクラスのオブジェクトのうちの少なくとも1つのオブジェクトを示さないとき、前記オブジェクト認識処理を使用して前記特定の画像を解析しないと決定するステップとを含む、メモリ記憶装置。 A memory storage device that stores instructions that can be executed by a data processing device, and at the time of such execution, the data processing device is made to execute an operation, and the operation is performed.
A step of analyzing a particular image using a coarse classifier to determine if the particular image represents at least one of one or more objects of a particular class.
When the particular image shows at least one of the objects of the one or more particular classes.
A step of analyzing the particular image using object recognition processing to recognize one or more objects shown in the particular image.
The step of presenting content about the one or more recognized objects,
Includes the step of deciding not to analyze the particular image using the object recognition process when the particular image does not represent at least one of the objects of the one or more particular classes. , Memory storage device.
Applications Claiming Priority (4)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201762508873P | 2017-05-19 | 2017-05-19 | |
US62/508,873 | 2017-05-19 | ||
PCT/US2018/017634 WO2018212809A1 (en) | 2017-05-19 | 2018-02-09 | Efficient image analysis using environment sensor data |
JP2019554333A JP2020513127A (en) | 2017-05-19 | 2018-02-09 | Efficient image analysis using environmental sensor data |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2019554333A Division JP2020513127A (en) | 2017-05-19 | 2018-02-09 | Efficient image analysis using environmental sensor data |
Publications (2)
Publication Number | Publication Date |
---|---|
JP2022101603A true JP2022101603A (en) | 2022-07-06 |
JP7407856B2 JP7407856B2 (en) | 2024-01-04 |
Family
ID=61274359
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2019554333A Pending JP2020513127A (en) | 2017-05-19 | 2018-02-09 | Efficient image analysis using environmental sensor data |
JP2022065610A Active JP7407856B2 (en) | 2017-05-19 | 2022-04-12 | Efficient image analysis using environmental sensor data |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2019554333A Pending JP2020513127A (en) | 2017-05-19 | 2018-02-09 | Efficient image analysis using environmental sensor data |
Country Status (6)
Country | Link |
---|---|
US (3) | US10621435B2 (en) |
EP (1) | EP3577597A1 (en) |
JP (2) | JP2020513127A (en) |
KR (1) | KR102361570B1 (en) |
CN (2) | CN113888761A (en) |
WO (1) | WO2018212809A1 (en) |
Families Citing this family (23)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11876941B1 (en) | 2016-06-20 | 2024-01-16 | Pipbin, Inc. | Clickable augmented reality content manager, system, and network |
US11785161B1 (en) | 2016-06-20 | 2023-10-10 | Pipbin, Inc. | System for user accessibility of tagged curated augmented reality content |
KR102399947B1 (en) * | 2016-07-05 | 2022-05-20 | 서울바이오시스 주식회사 | Skin check apparatus including ultraviolet light emitting device and skin check system |
EP3428834B1 (en) * | 2017-07-12 | 2019-06-12 | Sick AG | Optoelectronic code reader and method for reading optical codes |
US10846561B1 (en) | 2020-04-01 | 2020-11-24 | Scandit Ag | Recognition and selection of discrete patterns within a scene or image |
JP2019186791A (en) * | 2018-04-12 | 2019-10-24 | シャープ株式会社 | Imaging apparatus, control method of the imaging apparatus, and control program |
US10725629B2 (en) * | 2018-06-25 | 2020-07-28 | Google Llc | Identifying and controlling smart devices |
WO2020176064A1 (en) * | 2018-12-31 | 2020-09-03 | Didi Research America, Llc | Method and system of annotation densification for semantic segmentation |
US11157762B2 (en) | 2019-06-18 | 2021-10-26 | At&T Intellectual Property I, L.P. | Surrogate metadata aggregation for dynamic content assembly |
JP7345387B2 (en) * | 2019-12-26 | 2023-09-15 | Kddi株式会社 | Tactile sensation presentation system, local terminal and server device of the tactile sensation presentation system, tactile sensation presentation method, and tactile sensation presentation program |
JP7452620B2 (en) | 2020-02-26 | 2024-03-19 | 日本電気株式会社 | Image processing device, image processing method, and program |
CH717252A2 (en) * | 2020-03-23 | 2021-09-30 | 4Art Holding Ag | Process for the recognition of surfaces. |
US11514665B2 (en) | 2020-04-01 | 2022-11-29 | Scandit Ag | Mapping optical-code images to an overview image |
US11403477B1 (en) | 2020-05-15 | 2022-08-02 | Scandit Ag | Image exposure performance improvements for recognition of optical patterns |
US11295430B2 (en) | 2020-05-20 | 2022-04-05 | Bank Of America Corporation | Image analysis architecture employing logical operations |
US11379697B2 (en) | 2020-05-20 | 2022-07-05 | Bank Of America Corporation | Field programmable gate array architecture for image analysis |
CN112016440B (en) * | 2020-08-26 | 2024-02-20 | 杭州云栖智慧视通科技有限公司 | Target pushing method based on multi-target tracking |
US11470282B2 (en) * | 2020-12-09 | 2022-10-11 | Waymo Llc | Systems, apparatus, and methods for transmitting image data |
US11495036B1 (en) | 2021-01-29 | 2022-11-08 | Scandit Ag | Segmenting images for optical character recognition |
KR20220121105A (en) * | 2021-02-24 | 2022-08-31 | 삼성전자주식회사 | A method for tracking a location of an object in a tarrget area and an electronic device performing the same |
US11880738B1 (en) | 2021-08-17 | 2024-01-23 | Scandit Ag | Visual odometry for optical pattern scanning in a real scene |
US11557133B1 (en) | 2022-04-22 | 2023-01-17 | Verkada Inc. | Automatic license plate recognition |
US11978267B2 (en) * | 2022-04-22 | 2024-05-07 | Verkada Inc. | Automatic multi-plate recognition |
Family Cites Families (17)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9310892B2 (en) * | 2000-11-06 | 2016-04-12 | Nant Holdings Ip, Llc | Object information derived from object images |
US7315631B1 (en) * | 2006-08-11 | 2008-01-01 | Fotonation Vision Limited | Real-time face tracking in a digital image acquisition device |
US20090066799A1 (en) * | 2007-09-11 | 2009-03-12 | Omnivision Technologies, Inc. | Image sensor apparatus and method for embedding image stabilization data into image data |
US9024972B1 (en) * | 2009-04-01 | 2015-05-05 | Microsoft Technology Licensing, Llc | Augmented reality computing with inertial sensors |
US9619891B2 (en) * | 2010-08-26 | 2017-04-11 | Blast Motion Inc. | Event analysis and tagging system |
JP5434965B2 (en) * | 2011-06-03 | 2014-03-05 | カシオ計算機株式会社 | Movie generation method, movie generation device, and program |
US20140126822A1 (en) * | 2011-03-11 | 2014-05-08 | The University Of Sydney | Image Processing |
US9852135B1 (en) * | 2011-11-29 | 2017-12-26 | Amazon Technologies, Inc. | Context-aware caching |
EP2763077B1 (en) * | 2013-01-30 | 2023-11-15 | Nokia Technologies Oy | Method and apparatus for sensor aided extraction of spatio-temporal features |
JP5750558B2 (en) * | 2013-02-14 | 2015-07-22 | 富士フイルム株式会社 | Imaging apparatus and focus control method |
US9402018B2 (en) * | 2013-12-17 | 2016-07-26 | Amazon Technologies, Inc. | Distributing processing for imaging processing |
US8965117B1 (en) * | 2013-12-17 | 2015-02-24 | Amazon Technologies, Inc. | Image pre-processing for reducing consumption of resources |
US9626577B1 (en) * | 2014-09-15 | 2017-04-18 | Amazon Technologies, Inc. | Image selection and recognition processing from a video feed |
US9750420B1 (en) * | 2014-12-10 | 2017-09-05 | Amazon Technologies, Inc. | Facial feature selection for heart rate detection |
US10062412B2 (en) * | 2015-06-05 | 2018-08-28 | Apple Inc. | Hierarchical segmentation and quality measurement for video editing |
US9628116B2 (en) * | 2015-07-14 | 2017-04-18 | At&T Intellectual Property I, L.P. | Apparatus and methods for transmitting wireless signals |
US20170148488A1 (en) * | 2015-11-20 | 2017-05-25 | Mediatek Inc. | Video data processing system and associated method for analyzing and summarizing recorded video data |
-
2018
- 2018-02-09 EP EP18707199.8A patent/EP3577597A1/en active Pending
- 2018-02-09 CN CN202111170687.XA patent/CN113888761A/en active Pending
- 2018-02-09 CN CN201880023254.1A patent/CN110506276B/en active Active
- 2018-02-09 JP JP2019554333A patent/JP2020513127A/en active Pending
- 2018-02-09 KR KR1020197028726A patent/KR102361570B1/en active IP Right Grant
- 2018-02-09 US US15/754,909 patent/US10621435B2/en active Active
- 2018-02-09 WO PCT/US2018/017634 patent/WO2018212809A1/en unknown
-
2020
- 2020-03-27 US US16/832,182 patent/US11704923B2/en active Active
-
2022
- 2022-04-12 JP JP2022065610A patent/JP7407856B2/en active Active
-
2023
- 2023-06-02 US US18/328,445 patent/US20230316793A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
WO2018212809A1 (en) | 2018-11-22 |
JP7407856B2 (en) | 2024-01-04 |
US20200226382A1 (en) | 2020-07-16 |
US11704923B2 (en) | 2023-07-18 |
KR20190126347A (en) | 2019-11-11 |
KR102361570B1 (en) | 2022-02-11 |
CN113888761A (en) | 2022-01-04 |
JP2020513127A (en) | 2020-04-30 |
CN110506276A (en) | 2019-11-26 |
US20180373934A1 (en) | 2018-12-27 |
US10621435B2 (en) | 2020-04-14 |
US20230316793A1 (en) | 2023-10-05 |
CN110506276B (en) | 2021-10-15 |
EP3577597A1 (en) | 2019-12-11 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
JP7407856B2 (en) | Efficient image analysis using environmental sensor data | |
US20230063920A1 (en) | Content navigation with automated curation | |
CN109964236B (en) | Neural network for detecting objects in images | |
KR102355267B1 (en) | Content collection navigation and autoforwarding | |
CN112740709B (en) | Computer-implemented method, computing device, and computer-readable medium for performing gating for video analytics | |
US11789582B2 (en) | Content collection navigation queue | |
EP3815042B1 (en) | Image display with selective depiction of motion | |
US10839007B1 (en) | Generating a probability of music | |
KR102494642B1 (en) | Select an input mode for your virtual assistant | |
US9729792B2 (en) | Dynamic image selection | |
US9691180B2 (en) | Determination of augmented reality information | |
KR102467015B1 (en) | Explore media collections using opt-out interstitial | |
WO2014074959A1 (en) | Real-time face detection using pixel pairs | |
JP2023529380A (en) | Machine learning-based image compression settings that reflect user preferences | |
US20230066331A1 (en) | Method and system for automatically capturing and processing an image of a user | |
CN110929093B (en) | Method, apparatus, device and medium for search control |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
A621 | Written request for application examination |
Free format text: JAPANESE INTERMEDIATE CODE: A621Effective date: 20220415 |
|
A131 | Notification of reasons for refusal |
Free format text: JAPANESE INTERMEDIATE CODE: A131Effective date: 20230605 |
|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20230824 |
|
TRDD | Decision of grant or rejection written | ||
A01 | Written decision to grant a patent or to grant a registration (utility model) |
Free format text: JAPANESE INTERMEDIATE CODE: A01Effective date: 20231120 |
|
A61 | First payment of annual fees (during grant procedure) |
Free format text: JAPANESE INTERMEDIATE CODE: A61Effective date: 20231219 |
|
R150 | Certificate of patent or registration of utility model |
Ref document number: 7407856Country of ref document: JPFree format text: JAPANESE INTERMEDIATE CODE: R150 |