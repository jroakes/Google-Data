Embodiment
Embodiment comprises for the method and apparatus of realizing that distributed multi-modal is used.These embodiment can promote to use standard agreement and conventional content to write technology, and can overcome the intrinsic stationary problem of some conventional distributed computing technology.In addition, these embodiment can come together to use in conjunction with the device of " thin-client " type.
As used herein, term " multi-mode " expression is suitable for realizing the multiple user interfaces pattern.Term " distributed multi-modal application ", " multi-mode application " and " Distributed Application " can be exchanged use (for example to be illustrated in client terminal device, client terminal device 102, the software application of multi-mode user interface is provided Fig. 1), and it (for example is included in different platform, client terminal device 102, application server 104 and/or voice server 106, the different assemblies of carrying out in the independent operating time environment Fig. 1), wherein different platform by network (for example, network 108 Fig. 1) interconnects.
In one embodiment, distributed multi-modal is used and to be suitable for that (for example, system 100 carries out in system Fig. 1) having the client-server system system.As used herein, term " client " or " client terminal device " are used interchangeably to represent to be suitable for carrying out with distributed multi-modal to use processing entities, electronic installation or the application (for example, client browser and/or client side application assembly) that is associated.In addition, client terminal device is suitable for using the one or more services that are associated and provided by one or more remote servers via access to netwoks and distributed multi-modal.
Processing entities, electronic installation or the application of other service of the request of sending at the service of the client of one or more network connections or in response to client or server carried out in term " server " expression.Term " application server " and " AS " expression is suitable for initiating the foundation of the data that are associated with the distributed multi-modal utility cession and control link and to the server of controlling synchronously between various " views " that are associated with utility cession.Term " schema server " expression is suitable for carrying out the server of the server side application assembly that is associated with one or more user interface modes.Term " voice server " and " VS " expression are specially adapted for carrying out the schema server of the server side application assembly that is associated with speech pattern.Though following description comprises comprising the detailed description of voice server with the system of support voice pattern, but institute should be understood that, in other embodiments, can in system, comprise in addition or replacedly support the schema server of other type of other type user interface pattern (for example, gesture mode, pen mode etc.).
To run through the specific example that this description uses distributed multi-modal to use, in order to promote the understanding of exemplary embodiment.The illustrated examples that it being understood that is not considered to the scope of embodiment only is confined to this example.In the exemplary embodiment, distributed multi-modal is used and is suitable for supporting visual pattern and speech pattern.Client terminal device comprises " allowing multimodal " browser and/or client side application assembly, wherein any or its two be suitable for making an explanation to making client terminal device present the machine code that vision shows, vision shows and comprises that client terminal device can receive at least one display element at its input data by visual pattern and speech pattern.In one embodiment, machine code is included in the multi-mode page and/or by the multi-mode page and quotes, when being explained, machine code makes client terminal device present a plurality of user interactions demonstrations (the data input area that for example, is used for city, state, street number and street name).
As used herein, by understood by one of ordinary skill in the art, term " explanation " expression is explained and/or is carried out as those terms.As used herein, term " machine code " expressive notation (for example, with standard generalized markup language (SGML), extend markup language (XML), HTML(Hypertext Markup Language), eXtensible HTML (XHTML), dynamic HTML (DHTML), VoiceXML, voice application language mark (SALT), the mark that scalable vector graphics (SVG) and/or various other SGML are write), script (for example, the JavaScript code), the code of compiling (for example, the C/C++ of compiling, Java, Lisp, Fortran, Pascal etc.) and/or out of Memory, it can be by computer processor (for example, client terminal device 102, application server 104 and/or voice server 106, Fig. 1) execution easily or explanation.
Referring again to exemplary multi-mode discussed herein uses, the user of client terminal device can current to using " focusing " display element thereon (for example, the element selected or cursor glimmers therein) input information, and/or can change focus (for example, by selecting another element).In one embodiment, the user can use visual pattern or speech pattern that display element is imported information as required.For example, when the user used the keyboard input information of client terminal device, this action was corresponding to use visual pattern input data, and information will present in vision view focusing data input area thereon as text.In case showing, submission information (for example, by pressing " input " or moving cursor), vision just can be updated to reflect different focuses.
Replacedly, the user can use speech pattern input information (for example, the user can tell the city title).When it took place, in one embodiment, client terminal device sent the voice data of this speech of expression to voice server by the voice data path.In one embodiment, voice server is suitable for carrying out voice recognition unit, it assesses the speech data that receives according to the corresponding voice dialogues of the machine code of explaining with client terminal device (for example, the multi-mode page or the code that is associated with the client side application assembly).Voice recognition unit can generate speech recognition result (or wrong), and in one embodiment, voice server can pass to client terminal device with it via application server.According to each embodiment, client terminal device then can present speech recognition result in respective data area as text, and the state attempted of voice view and vision can be updated and synchronously.When the state synchronized of voice view and vision view, the machine code part that the voice dialogues part that voice server is explained is explained corresponding to client terminal device.
As will be described in detail, embodiment comprises for the ability of using any pattern input information according to the user vision view and voice view is carried out synchronous method.In each embodiment, even the state of vision view is kept on the client terminal device, and the state of voice view is kept on the voice server, and this also can realize all the time.Though in new ways, can use standard agreement and the conventional content technology of writing to realize synchronously, the research and development that promote thus and encourage various distributed multi-modals to use.
Fig. 1 is the simplification diagram according to the system 100 that is suitable for realizing that distributed multi-modal is used of exemplary embodiment.System 100 comprises a plurality of system entitys, and in one embodiment, it comprises at least one client terminal device 102, application server 104 (AS) and voice server 106 (VS).Will describe in detail as following, between these system entitys, set up various data and control path, and system entity follow various communication protocols, realize in the system 100 that distributed multi-modals use in order to be supported in.As used herein, term " multi-mode utility cession " can be defined as the example that distributed multi-modal is used.In one embodiment, the multi-mode utility cession comprise that client terminal device 102 carries out to machine code (for example, the machine code that is associated with the group of client side application assembly 114 and/or one or more relevant multi-mode pages 115) explanation (for example, carry out and/or explain), and the explanation to the group of one or more corresponding voice dialogues carried out of voice server 106.
Though only illustrate a client terminal device 102, system 100 can comprise a plurality of similar or dissimilar client terminal devices.For example, client terminal device 102 can form the part of selected electronic installation from device group, provide some examples, device group (for example comprises cell phone, radio device, pager, PDA(Personal Digital Assistant), personal navigation apparatus (PND), mobile computer system, vehicle or aircraft computer system), computing machine (for example, laptop computer, notebook or desk-top computer) and realize on computers pass through internetwork agreement to transmit speech sound (VoIP) phone, but be not limited thereto.
Client terminal device 102 comprises processing subsystem and data storage subsystem, and can be portable or fixed electronic device.For any device, client terminal device 102 can comprise the battery subsystem that is suitable for providing energy under the situation that can not use line powering to it.Client terminal device 102 can be suitable for using various wired and/or wireless communication technologys to communicate by one or more networks 108.Therefore, client terminal device 102 can comprise in the wired and radio network interface any one or its two.Client terminal device 102 (for example can also comprise various user interface input medias, keyboard, button, dial (of a telephone), touch-screen, microphone, indicating device are (for example, mouse or touch pad) and input pen) and various user interface output unit (for example, display screen, loudspeaker, audio output jack, headphone, ear muff, mechanical shock device).In one embodiment, the user's interface device that is associated with visual pattern comprises selected any one or a plurality of device from the group of the device that comprises visual displays, touch-screen, keyboard, button, dial (of a telephone), indicating device and input pen.In one embodiment, the user's interface device that is associated with speech pattern comprises selected any one or a plurality of device from the group of the device that comprises microphone, loudspeaker, audio output jack, headphone, ear muff.
In one embodiment, client terminal device 102 can be suitable for carrying out one or more examples of client middleware 110, client browser 112 and/or client side application assembly 114.As used herein, term " middleware " is illustrated in the computer software that interface is provided between the last component software that moves of different entities (for example, client, server or other processing entities) and/or the application.In one embodiment, client middleware 110 is suitable for across a network 108 provides interface between client browser 112 and/or client side application assembly 114 and one or more server (for example, application server 104 and voice server 106).
Client browser 112 is suitable in conjunction with the machine code on the client side application assembly 114 access customer end device 102 together (for example, the multi-mode page 115), and further machine code is made an explanation.In a particular embodiment, client browser 112 is suitable for visiting at least one multi-mode page 115, and the machine code in the multi-mode page 115 (for example, mark, script and out of Memory) is made an explanation.As used herein, term " the multi-mode page " refers to be illustrated in the information set of at least one user interactions display element that visual performance on the client terminal device 102 and user can select its input information and/or indication by various modes (for example, speech pattern and visual pattern) arbitrarily.For example, the multi-mode page 115 can comprise the information set of webpage, document, file, list, tabulation or other type, but is not limited thereto.When being explained, the multi-mode page 115 can be so that client terminal device presents one or more user interactions display elements.As used herein, except other, " user interactions display element " for example can comprise input text area territory, selective elements (for example, button or check box) and/or interactive text, but is not limited thereto.For example, together with one or more user interactions display elements, the multi-mode page can also comprise out of Memory and/or element, such as text message, image (for example, static state or animated image), audio-frequency information, video, hyperlink, metadata and script.
In one embodiment, the multi-mode page 115 comprises mark, and it makes client browser 112 and/or client side application assembly 114 (or other syntactic analysis software) carry out one or more embeddings or quote script (for example, JavaScript code).Script can be embedded in the multi-mode page 115, perhaps script can be quoted in the multi-mode page 115, and client browser 112 and/or client side application assembly 114 can be from external source (for example, server) or from local storage (for example, the high-speed cache from the client terminal device 102) accesses script.In optional embodiment, script can be embedded in or quote at client side application assembly 114 rather than the multi-mode page 115.In one embodiment, will describe in detail in conjunction with Fig. 2-6 as follow-up, script can be adapted so that client terminal device 102 sends asynchronous request for a variety of causes to application server 104.In the following description, in many cases, when client terminal device 102 was considered to transmit control message to application server 104, this can be interpreted as client terminal device 102 and send asynchronous request to application server 104.Similarly, when application server 104 was considered to transmit control message to client terminal device 102, this can be interpreted as the asynchronous request that application server 104 sends before the client terminal device 102 and send response.In one embodiment, asynchronous request is issued as asynchronous HTTP(Hypertext Transport Protocol) request, and response is http response.
In one embodiment, can use AJAX (asynchronous JavaScript and extend markup language (XML)) technology to develop client side application assembly 114 and/or the multi-mode page 115, and it can comprise and (for example is suitable for sending asynchronous request thus, asynchronous HTTP request) and handle XML tag and JavaScript at those request responding (for example, http response).In other embodiments, can use programming, script and/or the SGML of other type to develop client side application assembly 114 and/or the multi-mode page 115.
As used herein, term " SGML " expression is to (for example being embedded in the information set, the multi-mode page 115 or other machine code) in information (for example, mark or other machine code) set of defined syntactic rule, and except other, its when element is presented on the electronic installation to syntactic analysis software (for example, client browser 112 and/or client side application assembly 114) structure, layout and/or the further feature (for example, text or other content) of indicated number element.For example, the mark that is associated with SGML can comprise expressive notation, process mark and/or descriptive markup (being also referred to as " semanteme " mark), but is not limited thereto.The SGML that can use in each embodiment comprises SGML, XML, HTML, XHTML, DHTML, VoiceXML, SALT, SVG and various other SGML, but is not limited thereto.
Client browser 112 comprises software program, it (for example is suitable for carrying out the multi-mode page, the multi-mode page 115) syntactic analysis of Nei machine code (for example, mark) and/or so that client terminal device 102 can present text, image, video, music and/or in machine code and/or client side application assembly 114 mode and the client side application assembly 114 of expression or the out of Memory quoted carry out alternately.In each embodiment, client browser 112 can comprise HTML/XHTML browser, wireless application protocol (wap) browser, customized application and/or can (provide several examples by the commercial browser that obtains, for example, Internet Explorer, Mozilla Firefox, Safari, Opera and Netscape), but be not limited thereto.In a particular embodiment, client browser 112 is the XHTML browser, and it supports JavaScript to carry out and HTTP network service, and it also has the audio frequency processing power.Client browser 112 is " allowing multimodal ", and in one embodiment, this expression client browser 112 is suitable for explaining with multi-mode uses the machine code that is associated.
Client side application assembly 114 comprises computer software application.In one embodiment, the language (for example, SGML, XML, HTML, XHTML, DHTML or other SGML) supported with browser of client side application assembly is encoded and is depended on client browser 112 and make application component to carry out.In other embodiments, client side application assembly 114 can not rely on client browser 112 and makes application component to carry out.Can develop the client side application assembly 114 of number of different types, provide several examples, for example comprise with map application, navigation application and search and use the application component that is associated.
Client terminal device 102 can communicate by one or more networks 108 and application server 104 and voice server 106.For example, network 108 can comprise packet switching network and/or circuit-switched network, and more specifically, can comprise one or more in internet, PAN (Personal Area Network) (PAN), Local Area Network, wide area network (WAN), broadband LAN (WLAN), cellular phone network, radio net, satellite communication network, the public switch telephone network (PSTN) and/or be adapted so that between each system entity, to carry out the network of various other types of message exchange arbitrarily.Network 108 can be suitable for using various wired or wireless communication agreements exchange message between system entity arbitrarily.
Application server 104 (AS) is suitable for carrying out the various services for client terminal device 102.In one embodiment, will describe in more detail as following, application server 104 is suitable for carrying out one or more examples of AS middleware 116 and AS service 118.In one embodiment, AS middleware 116 be suitable for respectively server to server connect 140 and voice server 106 and the client terminal device 102 at network 108 two ends interface is provided.
In one embodiment, AS service 118 comprises that session communication sets up software, and it is suitable for initiating various data between client terminal device 102, application server 104 and the voice server 106 and control path 120,122,124 foundation together with the multi-mode utility cession.For example, the data that are associated with the multi-mode utility cession and control path can comprise AS/ client control path 120, AS/VS control path 122 and VS/ client voice data path 124.In one embodiment, client terminal device 102 and application server 104 can be via AS/ client control path 120 exchange messages, and client terminal device 102 and voice server 106 can be via VS/ client voice data path 124 exchange messages, and wherein at least a portion in AS/ client control path 120 and VS/ client voice data path 124 is based upon on one or more networks 108.Application server 104 and voice server 106 can be controlled path 122 exchange messages via AS/VS, the at least a portion in AS/VS control path 122 is based upon server to server and connects on 140, server to server connects 140 can comprise direct, hard-wired link, and perhaps it can be realized by one or more wired or wireless networks or other media entity.
In one embodiment, the control message that exchanges together with the multi-mode utility cession between client terminal device 102 and application server 104 uses the communication protocol that is applicable to network 108 in network 108 exchanges.Therefore, network 108 is as the control path between client terminal device 102 and the application server 104.Path 120 is controlled by the consistent AS/ of being called client here in this control path, even defined routed path can not be to be exclusively used in session by network 108, and each message of exchange between client terminal device 102 and application server 104 can be different with message by the physics route of network 108.Therefore, as used herein, " AS/ client control path " can be defined as representing that message can be along its any one or more path by network 108 (perhaps some other communication media) that exchanges between the IP address that is associated with client terminal device 102 and/or port and the IP address that is associated with application server 104 and/or port.Similarly, " AS/VS controls the path " can be defined as representing the IP address and/or any one or more path between port and the IP address that is associated with voice server 106 and/or the port (it can traverse network or not traverse network) that are associated with application server 104.In addition, " VS/ client voice data path " can be defined as representing that voice data can be along its any one or more path by network 108 (perhaps some other communication media) that exchanges between the IP address that is associated with voice server 106 and/or port and the IP address that is associated with client terminal device 102 and/or port.In one embodiment, AS/ client control path 120 and VS/ client voice data path 124 differ from one another, and reason is to be different from client ip address and/or the port that distributes for VS/ client voice data path 124 at the client ip address that distributes for AS/ client control path 120 during the session and/or port during session.In addition, the application server IP address and/or the port that distribute for AS/ client control path 120 are different from voice server IP address and/or the port that distributes for VS/ client voice data path 124, and this situation is because application server 104 and voice server 106 differ from one another.
As mentioned before, the machine code that the multi-mode page that presents with client terminal device 102 and/or the client side application assembly of carrying out at client terminal device 102 are associated (for example is suitable for sending asynchronous request, asynchronous HTTP request), asynchronous request is received by AS/ client control path 120 by application server 104.AS service 118 is suitable for carrying out processing and/or other task according to request, and suitably returns response (for example, http response) by AS/ client control path to client terminal device 102.In addition, by the control message via AS/ client control path 120 and AS/VS control path 122 transmission, AS service 118 is suitable for controlling synchronously between the state of the voice view of preserving on the state of the vision view of preserving on the client terminal device 102 and the voice server 106.In essence, comprising synchronously to client terminal device 102 and voice server 106 of vision view and voice view provides information, information make the machine code on the client terminal device 102 explanation can with the explanation synchronised of the machine code of corresponding voice dialogues 136 on the voice server 106.From another point of view, comprising synchronously to client terminal device 102 and voice server 106 of vision view and voice view provides information, information make the vision view on the client terminal device 102 executing state can with voice server 106 on the executing state synchronised of voice view.Below will describe in more detail for carrying out this synchronous method.
Voice server 106 is schema server, and it is particularly suited for carrying out speech together with the distributed multi-modal utility cession and handles relevant task.As mentioned before, in other embodiments, the schema server of other type can be integrated in the system.Voice server 106 is suitable for carrying out one or more examples of VS middleware 130, VS browser/dialogue interpreter 132 (below be called VS browser 132) and voice recognition unit 134.Because it forms the part of the server side of Distributed Application, so voice recognition unit 134 can be considered to the application component of VS side.In other embodiments, can realize the VS side application component of other type.
VS middleware 130 be suitable for respectively server to server connect 140 and/or the VS browser 132 at network 108 two ends and other server (for example, application server 104) and/or client terminal device 102 between interface is provided.VS browser 132 is to make voice server 106 to visit under the background of multi-mode utility cession and (for example explain voice dialogues, voice dialogues 136), together with explaining that voice dialogues sends and receive voice data, (for example send asynchronous request, asynchronous HTTP request), and the software program of reception and processing response (for example, http response).In each embodiment, VS browser 106 can comprise customization or can the commercial browser that obtains or be suitable for explaining provide the mark that is associated with speech pattern other application program (for example, VoiceXML, SALT and/or various other SGML), but be not limited thereto.
As mentioning just now, voice dialogues 136 can be visited and explain to VS browser 132.As used herein, " voice dialogues " can be interpreted as representing the information set that is associated with the set of sound recognition result and/or audio prompt if possible.According to each embodiment, voice dialogues can comprise the machine code that is suitable for collecting and/or providing in one or more stages voice data.For example, voice dialogues can comprise the information set of webpage, document, file, tabulation or other type, but is not limited thereto.For example, voice dialogues can also comprise out of Memory and/or element, such as text message, metadata, machine code and script.Though comprising by voice server 106, following described embodiment (for example visited, download) voice dialogues, but institute should be understood that, in other embodiments, voice dialogues may be implemented as the machine code that is present on the voice server 106, and it does not need to be downloaded or to visit.
In one embodiment, voice dialogues 136 comprises machine code, and wherein machine code can comprise compiled code, mark and/or the out of Memory that voice server 106 can easily be explained.In one embodiment, voice dialogues 136 can comprise mark, and it makes VS browser 132 visit and/or the script (for example, JavaScript code) of carrying out one or more embeddings or quoting.Script can be embedded within the voice dialogues 136, and perhaps VS browser 132 can be visited from external source (for example, server) or from the script of local storage (for example, from the high-speed cache on the voice server 106).Except other, script can comprise and is suitable for calling voice recognition unit 134 to attempt determining speech recognition result based on the voice data that is received, recover or generate audio prompt and/or send the script of asynchronous request (for example, for the asynchronous HTTP request of voice dialogues or the request of other type).In one embodiment, voice dialogues 136 can use the AJAX technology to develop, and can comprise mark (for example, VoiceXML and/or SALT mark) and the JavaScript that is suitable for sending asynchronous request thus.In other embodiments, speech recognition 136 can use programming language, script and/or the SGML of other type to develop.
Voice recognition unit 134 is the software applications that can be called by VS browser 132, and it (for example is suitable for receiving voice data, to compress, not compress, to encode, not encode or codec format), use voice data to carry out the speech recognition algorithm and (for example determine speech recognition result in order to attempt, and return speech recognition result or the result is determined in indication the indication of the speech of identifying).As used herein, the digitized representations (being generally " audio frequency ") of term " voice data " expression speech or other sub-audible sound, wherein digitized audio represent to comprise compression, do not compress, encode, encode and/or codec format the speech of sampling or other sub-audible sound.Voice data can also be included in form or comprise pointer among other data structure of voice data, and wherein form can be employed server 102, voice server 106 and/or client terminal device 102 and visits.Voice recognition unit 134 can be together with one or more speeches storehouse 138 or other speech recognition resource (for example, grammer, n-gram sequence, statistical language model or other speech recognition resource) carry out together, wherein the speech recognition resource can be based on the particular voice dialogue of explaining 136 and accessed.
In one embodiment, voice server 106 communicates with application server 104 via AS/VS control path 122.In addition, voice server 106 and client terminal device 102 can be via VS/ client voice data path 124 direct exchange audio data.In one embodiment, can use grouping-data communication protocol by VS/ client voice data path flow transmitting audio data.Replacedly, (push-to-talk, PTT) communication means is by VS/ client voice data path 124 exchange audio data can to use circuit switching or a key intercommunication.Though can (for example realize other agreement in other embodiments, transmission control protocol (TCP) etc.), but use in one embodiment, a kind of RTP/RTCP Real-time Transport Control Protocol (RTP/RTCP) of version to transmit voice data by VS/ client voice data path 124.
Application server 104 and voice server 106 differ from one another, and difference is application server 104 and voice server 106 execution different disposal, and influence the control message of the performance of those processing by 122 exchanges of AS/VS control path.In addition, AS/ client control path 120 between client terminal device 102 and the application server 104 is different from the VS/ client voice data path 124 between client terminal device 102 and the voice server 106, difference is that at least client terminal device uses different addresses (for example, different IP address) application server 104 and voice server 106 to carry out addressing.In addition, client terminal device 102 can use and be used for the next and application server 104 exchange control message of the communication protocol different with the communication protocol of voice server 106 exchange audio data.In one embodiment, application server 104 can realize on the different physically hardware that with voice server 106 hardware can arrange jointly or not arrange jointly.In another embodiment, as described in detail below, though still exchange various control message by AS/VS control path 122 between two servers, application server 104 and voice server 106 can be realized at least part of shared hardware.
As mentioned before, the embodiment of theme of the present invention is suitable for the vision view and the voice view that are associated with the distributed multi-modal utility cession are carried out synchronously.The state of " vision view " by client terminal device 102 together with the multi-mode page (for example, the multi-mode page 115) explanation is preserved together, the state of " voice view " is then preserved together with the voice dialogues that is associated with the multi-mode page (for example, voice dialogues 136) together by voice server 106.Term " vision view " and " vision viewstate " can be defined as browser (for example, client browser 112) current explanation or the multi-mode page that wait for to explain in the machine code of included or visit in the client side application assembly of the machine code of included or visit and/or current execution.For example, current vision viewstate can be corresponding to showing the machine code that current focusing single page elements thereon is associated with vision, single page elements vision show can be used as be marked by highlighted, cursor glimmers there or the page elements that is provided with some other focuses indications is there represented.Term " voice view " and " voice view state " can be defined as browser (for example, the VS browser 132) current execution or etc. the machine code of included or visit in the pending voice dialogues.For example, the current speech viewstate can be corresponding to making voice server wait for receive as the machine code to the voice data of the input that is presented on the display element on the client terminal device 102.
To be described in conjunction with Fig. 2-6 couple of method embodiment that be used for to initiate and carry out the multi-mode utility cession now.To under the background of the example that the distributed multi-modal of particular type is used, be described these embodiment, in order to promote explanation and the understanding of each embodiment.Exemplary application is map application, and it comprises and is adapted so that the explanation of element represented in the multi-mode page by client browser is presented on the client side application assembly on the client terminal device.The client side application assembly (for example, map application) and client browser be " allowing multimodal ", it means that they all are suitable for supporting to use various modes to identify and/or provide the user interface of user interactions, and client browser is particularly suited for exchanging messages with one or more remote servers under the support of Distributed Application session and understanding message.In optional embodiment, client browser can be suitable for explaining the multi-mode page and need not from the client side application assembly initiation (for example, when the user directly initiate the example of client browser and make client browser download and when explaining the multi-mode page).In another optional embodiment, the client side application assembly can be carried out some or all of client browser tasks, and/or the client side application assembly can comprise some or all of the machine codes that will be included in the multi-mode page.Therefore, can utilize or need not the multi-mode page and carry out embodiment.Though it will be understood that to fall within the scope of theme of the present invention, will not be discussed in detail these embodiment at this.
This example will comprise the client terminal device that shows the multi-mode page with the form of data-entry form, wherein data-entry form (for example comprises a plurality of data input areas of filling, city, state, street number, street address), the user can use in visual pattern or the speech pattern any or its two to the address information of wherein importing the target location.In case submitted the information of importing to, system just can provide cartographic information, and cartographic information makes client terminal device can show the map that comprises the target location.It being understood that also can use (except other, for example searching for and navigation application) together with the distributed multi-modal of other type use this embodiment together.
In given example, various modes comprises visual pattern and speech pattern.On client terminal device, visual pattern can use display screen and realize for the hardware and software that is associated that generates the vision demonstration.Visual pattern also can use keyboard and/or one or more other user interface input media to realize at client terminal device, the user interface input media makes that the user can be by showing with vision and/or the selecting alternately of other user interface data device, input information and/or take other action (for example, changing focus).Speech pattern can be used microphone, loudspeaker and be suitable for receiving human speech and its hardware and software that is associated that carries out digitizing and/or output audio information is realized.
In given example, the user can be undertaken mutual by watching vision demonstration and use keyboard or other input media visual pattern (for example, vision demonstration) next to the zone input information (perhaps selecting) that vision shows and user interface.In addition, the user can be undertaken alternately by the speech pattern of listening to audio prompting and/or speech and user interface, in order to the information that is associated with its page elements that is provided with vision demonstration focus is provided.It being understood that is in optional embodiment, and distributed multi-modal is used the set of modes can be suitable for allowing more than visual pattern and speech pattern.
To illustrate the bulk treatment of be used for initiating and carrying out the multi-mode utility cession according to embodiment at Fig. 2 described below.Fig. 2 is intended to the scope of subject matter is limited, but will be provided for understanding the whole background of describing in conjunction with the more detailed processing of Fig. 3,5 and 6 diagrams and description.
Fig. 2 is the process flow diagram that is used for initiating and carrying out the method for multi-mode utility cession according to exemplary embodiment.This method is in frame 202 beginnings, and wherein client terminal device receives the indication that should initiate the multi-mode utility cession.In each embodiment, the multi-mode utility cession can be initiated by client side application assembly (for example, the part of the client-side of map application) and/or the example of client browser.For example, the user of client terminal device can initiate to be suitable for to present the example of the client side application assembly that vision shows by himself or together with client browser, and vision shows and comprises that at least one can use visual pattern or speech pattern to the display element of its input data.Replacedly, the user can initiate the example of client browser, and can indicate the user to wish browser access or the download multi-mode page to client browser, and when being explained, the multi-mode page makes client terminal device present the vision demonstration.For example, the user can provide the user to import to navigate to the multi-mode page.
In case client terminal device receives the indication that should initiate the multi-mode utility cession, as describing in more detail below with reference to Fig. 3, by initiating the multi-mode utility cessions at frame 204 alternately between client terminal device, application server and the conversation server.Initiate the multi-mode utility cession be included in set up between client terminal device and the voice server voice data path (for example, VS/ client voice data path 124, Fig. 1), will be by its exchange audio data during the multi-mode utility cession.In addition, in an embodiment, initiating the multi-mode utility cession by the control of first between client terminal device and application server path (for example comprises, AS/ client control path 120, Fig. 1) exchange message between client terminal device and application server, and (for example, AS/VS controls path 122, Fig. 1) exchange message between application server and voice server by the control of second between application server and voice server path.
As describing in more detail in conjunction with Fig. 5 and 6, during the multi-mode utility cession, will be by voice data path exchange audio data between client terminal device and voice server, and will be by first and second control path exchange of control information between client terminal device, application server and voice server.Control information makes can being kept all the time during whole session synchronously of state time of vision view and voice view.
In response to event exchange audio data and control information between each system entity that voice server and client terminal device generate, event is respectively referred to as " event that VS generates " and " event that client generates ".The event that the event that VS generates and client generate can influence the state of voice view and/or vision view.The event that the event that VS generates and client generate can take place in any time of carrying out the multi-mode utility cession.As will describing in more detail in conjunction with Fig. 5, when the event that VS generates takes place, carry out the operational processes of the event that VS generate at frame 206.In addition, as will describing in more detail in conjunction with Fig. 6, when the event that client generates takes place, carry out the operational processes of the event that clients generate at frame 208.Though frame 206 and 208 is illustrated as and occurs in sequence, these are handled and also can walk abreast or take place with reverse order, and any or two processing can repeatedly take place when carrying out the multi-mode utility cession.As shown in Figure 2, use to handle until the multi-mode described as frame 210 and stop, the processing that the event that the event that VS is generated and client generate is operated all continues execution.
Fig. 3 is that the method that is used for carrying out the initiation of multi-mode utility cession according to exemplary embodiment (for example, handles 204, process flow diagram Fig. 2).Described as the processing 202 in conjunction with Fig. 2 before, in one embodiment, can when receiving the indication that should initiate the multi-mode utility cession, client terminal device initiate the multi-mode utility cession.For example, when opening, the user is suitable for explaining or initiates the multi-mode utility cession when carrying out the client side application assembly of machine code or browser that machine code makes client terminal device present can use visual pattern, speech pattern or its two one or more display elements to its input data.In one embodiment, except other action, can be when the user opens the client side application assembly be suitable for explaining the multi-mode page or browser and/or the user initiate the multi-mode utility cession when choosing the hyperlink of the multi-mode page.
In frame 302, client terminal device by AS/ client control path (for example, AS/ client control path 120, Fig. 1) send one or more control message to application server, control message comprises the client audio path information, wherein the client audio path information is specified the voice data path of wishing to set up with voice server with client terminal device (for example, VS/ client data path 124, Fig. 1) relevant descriptive characteristics.For example, the client audio path information can comprise (multiple) audio format, (multiple) bit rate and/or (multiple) audio types that description client side application assembly and/or client browser are supported, the information of required (multiple) audio coding decoding, and/or download to the client terminal device IP address of voice data (being called " downlink voice data " here) of client terminal device and the client-side audio frequency address information the port numbers such as being used for during the multi-mode utility cession by the voice data path, but be not limited thereto.In another embodiment, before sending the client audio path information to application server, client terminal device can send to use to application server by AS/ client control path initiates message.For example, use initiation message and can comprise application identities symbol, client certificate and/or out of Memory.
In frame 304, application server receives the client audio path information that client terminal device sends, and initiates voice data path (for example, VS/ client voice data path 124, foundation Fig. 1) between client terminal device and the voice server.In one embodiment, this comprises and sets up AS/VS control path with voice server by application server (for example, AS/VS controls path 122, Fig. 1).Application server then sends one or more control message by AS/VS control path to voice server, and control message comprises some or all of client audio path information.
In frame 306, the client audio path information that voice server receives and the storage application server sends.Voice server then sends one or more control message by AS/VS control path to application server, and control message comprises the VS audio path information.In one embodiment, the VS audio path information is specified the additional descriptive characteristics relevant with VS/ client voice data path.For example, the VS audio path information can comprise (multiple) audio format, (multiple) bit rate and/or (multiple) audio types that voice server is supported, required (multiple) audio coding decoding, and/or such as IP address and the frequency of the VS sidetone the port numbers address information that will be used for during the multi-mode utility cession, uploading to by VS/ client voice data path the voice data (being called " uplink audio data " here) of voice server, but be not limited thereto.
In frame 308, application server receives the VS audio path information that voice server sends.Application server then sends one or more control message by AS/ client control path to client terminal device, and control message comprises some or all of VS audio path information.In frame 310, the VS audio path information that client terminal device receives and the storage application server sends.At this, client terminal device and voice server are all understood IP address and the port that should send voice data each other.In other words, what address voice server is known with sends the downlink voice data, and what address transmission uplink audio data client terminal device is known with.Therefore, at this, between voice server and client terminal device, set up two-way voice data path.In optional embodiment, can between voice server and client terminal device, set up the one-way audio data routing.For example, can set up the voice data path that wherein only sends uplink audio data from client terminal device to voice server, in this case, not provide the client audio path information to voice server.In another embodiment, can set up key intercommunication (PTT) channel, it makes client terminal device or voice server to send voice data by VS/ client voice data path at any given time.
In frame 312, client terminal device transmits control message to application server by AS/ client control path, and control message comprises the multi-mode application message.In one embodiment, the multi-mode application message comprises the information of the multi-mode application that the sign client terminal device will be prepared to carry out.In another embodiment, the multi-mode application message comprises for (for example using at least one multi-mode page of being associated with the multi-mode that client terminal device prepare to be carried out, the multi-mode page 115, Fig. 1) and/or at least one voice dialogues (for example, voice dialogues 136, quoting Fig. 1) (for example, URL(uniform resource locator) (URL)).
In frame 314, application server receives the multi-mode application message.Application server then can be identified at least one multi-mode page and quoting (for example, URL) at least one voice dialogues according to the multi-mode application message.In one embodiment, application server then can be controlled the path by AS/VS and send quote (for example, (a plurality of) URL) that is used for voice dialogues to voice server.Voice server then can be downloaded or visit with (a plurality of) and be quoted corresponding (a plurality of) voice dialogues.In optional embodiment, application server can be quoted based on (a plurality of) and obtain (a plurality of) voice dialogues, and (a plurality of) voice dialogues can be sent to voice server by AS/VS control path.In addition, application server can be obtained (a plurality of) multi-mode page and send it to client terminal device by AS/ client control path.
In frame 316, client terminal device receives the multi-mode page and initiates explanation to it, and presents visual indicia included in the multi-mode page on client terminal device.For example, client browser or client side application assembly can make an explanation to visual indicia included in the multi-mode page, and can show one or more page elements in the display device that is associated with client terminal device according to visual indicia.
Fig. 4 is the example according to the visual indicia that presents at client terminal device 400 of exemplary embodiment.In illustrated example, part visual indicia presents with four multi-mode display elements 401,402,403,404 form, wherein each includes text label (for example, " city ", " state ", " street number " and " street name ") and data input area.
Refer again to Fig. 3, in frame 318, client terminal device transmits control message by AS/ client control path, its indication client terminal device has been initiated the explanation (for example, client terminal device has been initiated the execution that multi-mode is used) to the multi-mode page and/or the execution of other machine code of having initiated to be associated with multimodal session.At this, the vision view is original state.Original state at the vision view, client terminal device can wait for and receive the sound signal corresponding with speech and with its digitizing, and by VS/ client voice data path to the voice server transmission uplink audio data corresponding with digitized sound signal.In addition or replacedly, original state at the vision view, client terminal device can be waited for from application server and receive extra control message, and/or wait for by the downlink voice data of VS/ client voice data path from voice server, and the audio output device that use is associated with client terminal device (for example, loudspeaker, earphone or headphone) presents the downlink voice data.
In frame 320, the application server receiving control message, control message indication client terminal device has been initiated the explanation to the multi-mode page or other machine code of being associated with the multi-mode utility cession, and application server transmits control message to voice server by AS/VS control path, and its order voice server begins to explain voice dialogues.In frame 322, voice server then begins to explain voice dialogues.At this, voice view is in the original state with the original state synchronised of vision view.In the original state of voice view, voice server can be waited for reception from the extra control message of application server, and/or can wait for receiving and pass through VS/ client voice data path from the uplink audio data of client terminal device.
Then finish the processing of initialization and beginning multi-mode utility cession.At this, the vision view is in first state, wherein client terminal device arrived its to multimodal session (for example, the multi-mode page and/or client side application assembly) point that makes an explanation of the machine code that is associated, it treats that at these user imports via visual pattern or speech pattern, and perhaps it is at the downlink voice data of this wait from voice server.In addition, voice view is in first state, wherein voice server arrived its to multimodal session (for example, voice dialogues) point that the machine code that is associated makes an explanation, it is waited for from the control input of application server or its at this and waits for up-link audio frequency from client terminal device at this.
Fig. 3 illustrates the specific embodiment for the method for carrying out the initiation of multi-mode utility cession.As here (for example utilizing, in Fig. 5 and 6) described other method, illustrated processing sequence can with illustrated in and describe different, in other embodiments, some processing and/or message can be incorporated in together, and/or some processing and/or message can be got rid of together.For example, optional embodiment in the method that is used for the initiation of execution multi-mode utility cession, the step 302 of Fig. 3 and 312 can be grouped together (for example, client terminal device can send audio path information to voice server together with the multi-mode application message).In other optional embodiment, handle 304 all or part of can being excluded, such as when client terminal device and/or voice server have loaded the machine code that is associated with the multi-mode utility cession (for example, make unnecessary download machine code).In another optional embodiment, client terminal device and voice server can direct communication (for example to set up VS/ client voice data path, client terminal device can be initiated to the calling line of voice server, and can utilize Session ID to set up VS/ client voice data path, Session ID is sent to application server).
As discussing in conjunction with the frame 206 and 208 of Fig. 2 before, after initiating the multi-mode utility cession, at least two types event can take place, it can influence the state of voice view and/or vision view.These event types comprise the event that event that VS generates and client generate.Fig. 5 and 6 illustrates the embodiment of the operational processes of the operational processes of the event that VS generates and the event that client generates respectively.
Fig. 5 (for example, handles 206, the process flow diagram of method Fig. 2) according to the operational processes that be used for to carry out the event that VS generates of exemplary embodiment.Term " event that VS generates " can be defined in event on the voice server, and it can permit the state variation of voice view.For example, except other, the event that various VS generate can comprise speech recognition event, overtime event and download or visit other voice dialogue, but be not limited thereto.
In one embodiment, method is in frame 502 beginnings, and wherein client terminal device receives the signal that can represent user pronunciation (for example, voiceband user) by its user interface.Client terminal device then can carry out digitizing and handle to generate the uplink audio data of expression signal signal, and can be by the voice data path (for example, VS/ client voice data path 124 Fig. 1) sends to voice server with uplink audio data.Voiceband user can be corresponding to single display element (for example, speech can be corresponding to the pronunciation of display element 402 " Illinois (Illinois) ", Fig. 4), perhaps corresponding to more than one display element (for example, speech can be corresponding to the pronunciation in display element 401 and 402 " Chicago; Illinois ", Fig. 4).In each embodiment, can set up the voice data path by packet switch or circuit-switched network.Setting up by packet switching network among the embodiment in voice data path, client terminal device carries out packetizing to voice data, and in the one or more message that are addressed to voice server IP address that client terminal device receives and port during the multi-mode utility cession initiate to be handled (for example, the processing of Fig. 3), send voice data.
In frame 504, voice server receiving uplink voice data.In response to the reception of uplink audio data, voice server can be carried out speech recognition processes, and wherein voice server attempts to identify the represented speech of uplink audio data (for example, pronunciation).In one embodiment, this comprises that (for example, voice recognition unit 134 Fig. 1) is handled uplink audio data by voice recognition unit.In one embodiment, voice recognition unit can be visited and the voice view effective speech recognition resource that is associated of the state of (then-current) at that time, wherein the speech recognition resource can comprise with the voice view effective data that are associated of the pronunciation set that is associated of state at that time.For example, refer again to Fig. 4, data " Chicago " have been filled out in the data input area of first display element 401 (for example, " city " display element).Cursor is displayed on second display element 402, and (for example, " state " display element in) the data input area, it indicates the current focus of vision view, the perhaps state of vision view.When vision view and voice view were synchronous, the state of voice view will be corresponding to waiting for the voice server that receives the voice data corresponding with the user pronunciation of state title.Owing to there is the finite aggregate of state title, so can be in the robustness of voice recognition unit visit lifting voice recognition unit during with speech recognition resource that pronunciation set corresponding to the finite aggregate of state title is associated.Except other variable, the speech recognition resource can be suitable for utilizing the various articulation type of state title and/or the data of expression masculinity and femininity voice characteristics to identify speech.
In frame 506, determine whether speech is identified.When voice server can't be associated uplink audio data with recognizable talk (for example, but voice data and do not correspond to any single state title in the acceptance error limit), voice server can determine that speech is not identified.Under these circumstances, in frame 508, voice server can be controlled the path by AS/VS and transmit control message to application server, and the speech recognition mistake has taken place in its indication.In frame 510, application server then can send the control message of misdirection by AS/ client control path to client terminal device.In frame 511, in case receive wrong indication, client terminal device just can present error message, and it indicates user's input speech not to be identified to the user.In each embodiment, client terminal device can present error message as audio prompt and/or in the message that vision show to show.Client terminal device then can be pointed out the user to re-enter data or can be taked some other action (or not taking action), and method can finish.
Refer again to frame 506, when voice server has been determined uplink data corresponding to recognizable talk (for example, voice data is corresponding to the state title of identifying), in frame 512, voice server (for example can generate the identification event, the event that VS generates), and can control the path by AS/VS and transmit control message to application server, it is to application server notice identification event.In one embodiment, the identification event comprises indication (for example, binary numeral) and the speech recognition result that speech is identified, and speech recognition result can comprise the indication (for example, identifying the indication of state title) of the speech of identifying.In optional embodiment, the identification event can also comprise one or more other speech recognition result, and may comprise the strength of association indication (for example, each result is the indication of correct result's possibility) of each speech recognition result.In each embodiment, the indication of the speech of identifying can comprise the text representation of the speech of identifying, the index in the table (for example, listing the table of state title) that may recognition result, the perhaps designator of other type.In frame 514, application server then can transmit control message by AS/ client control path, and it comprises from the control message that voice server receives included or from its information that draws (for example, the message of indication identification event).For example, control message can also comprise indication and the speech recognition result (for example, the indication of the speech of identifying) that speech is identified.
Though in one embodiment, the reception of voice data and identification can trigger voice server and produce the event that VS generates, and in other embodiments, other condition also can trigger voice server and produce the event that VS generates.For example, when voice view was in its state of waiting for the reception voice data, voice server produced the event that VS generates in the time of can receiving voice data in predetermined time out period, but is not limited thereto.Under these circumstances, VS to the AS control message that sends in frame 512 can be notified the event of other type of application server, and the AS that sends in frame 514 is to client control message and then can notify the event of other type of client terminal device.Client terminal device can be taked action according to event type.Though the event that the VS of other type generates is not discussed in detail at this, the scope of embodiment is intended to comprise the event that the VS of other type generates.
In frame 514, in case application server has sent control message to client terminal device, in frame 516, client terminal device is with regard to receiving control message, and the processing speech recognition result.In addition, client terminal device according to speech recognition result more new vision show, and transmit control message to indicate renewal to finish and/or client terminal device prepares to receive another message of the event that indication VS generates by AS/ client control path to application server.For example, if speech recognition result is identified some forms of the text representation of speech and is transmitted to be different from, then client terminal device can be determined text representation (for example, client terminal device can be searched text representation when speech recognition result has identified the clauses and subclauses in the table).Refer again to Fig. 4, for example, if speech recognition result is treated to text " Illinois ", then client terminal device can show " Illinois " in the data input area of second display element 402.When recognition result corresponding to more than one display element the time, client terminal device can show that data (for example in more than one corresponding data input area, when recognition result is indicated the identification speech in " Chicago " and " Illinois ", if " Chicago " is not shown, then client terminal device can show " Chicago " in display element 401, and shows " Illinois " in display element 402).In addition, client terminal device can come the focus on automatic (for example, need not further user interactions) change vision view by the data input area that cursor is moved to the 3rd display element 403.Changing under the background of focus, client terminal device can be by finishing it to the explanation of the mark that is associated with second display element 402, and by initiating the explanation of the mark that is associated with the 3rd display element 403 is carried out the state variation of vision view.
Refer again to Fig. 5, in frame 518, client terminal device has determined whether to take place the state conversion of vision view.For example, as described in the paragraph before, when can being changed in the focus of vision view, state transformation takes place.When state transformation not taking place, method can finish.When state transformation taking place, in frame 520, client terminal device sends the control message that state transformation has taken place in indication by AS/ client control path to application server.In optional embodiment, information in the control message that sends in frame 520 can combine with the information in the control message that sends in frame 516, and/or (in other words application server can infer arbitrary information project based on the reception of out of Memory project, when application server receives when upgrading the indication finish, it can infer state transformation has taken place that vice versa).
In frame 522, application server receives the information that state transitions has taken place in indication, and transmits control message to voice server by AS/VS control path, and it comprises and will make voice server initiate the information of the state transformation of voice view.For example, in one embodiment, the information of control in the message can be that the voice server that makes from application server carries out the instruction of voice view state transformation.In another embodiment, the information in the control message can be the indication that the conversion of vision viewstate has taken place, and voice server can correspondingly determine whether to carry out the conversion of voice view state.
In frame 524, voice server is from the application server receiving control message, and in response, carries out the conversion of voice view state.In addition, voice server can be controlled the path via AS/VS and transmit control message to application server, and the state conversion has taken place in its indication.For example, refer again to Fig. 4, voice server can finish its to the explanation of waiting for the machine code that the voice data relevant with second display element 402 is associated, and can initiate to wait for and machine code that the voice data of the 3rd display element 403 is associated and/or with the explanation of the machine code that is associated to client terminal device transmission downlink voice data (for example, audio prompt) by VS/ client voice data path.
When the current state of having determined voice view in frame 526 comprises to client terminal device transmission audio prompt, then in frame 528, voice server sends suitable audio prompt as the downlink voice data by VS/ client voice data path to client terminal device.For example, when focus has been changed to that display element 403 of ground, the machine code that is associated with the 3rd display element 403 can be so that voice server sends audio prompt by VS/ client voice data path to client terminal device, and it comprises the voice data corresponding with " please say street number " or some similar audio prompts.In case receive voice data, client terminal device just can be such as presenting audio prompt by the audio output device output at client terminal device corresponding to the earcon of pointing out.Then, the method for Fig. 5 can finish.
The embodiment of method shown in Figure 5 is relevant with the processing of carrying out when the event that VS generates takes place, and it can influence in vision view or the voice view the two state of any or its.As previously discussed, client terminal device also can generate event, and it can influence the state of vision view and/or voice view.Under these circumstances, in one embodiment, the operational processes of the event that the execution client generates.
Fig. 6 (for example, handles 208, the process flow diagram of method Fig. 2) according to the operational processes that be used for to carry out the event that client generates of exemplary embodiment.Term " event that client generates " can be defined in client and give birth to event, and its state that can permit the vision view changes.For example, except other, the event that various clients generate can comprise that the focus of vision view changes, and uses key in data and download or visits another multi-mode page, but be not limited thereto.
In one embodiment, method is in frame 602 beginnings, and wherein client terminal device receives user's input by its visual pattern, and user's input can be permitted the state variation of vision view and/or the renewal that vision shows.For example, the user can use the rolling of client terminal device or indicating mechanism to select and the current focusing of vision view different display elements and/or data input area thereon.As another example, the user for example can use keyboard to make text be imported among the current focusing of the vision view data input area thereon, and such as can be by (for example pressing " input " key, " input " key 406 is Fig. 4) or by providing some other to indicate designation data to finish the insertion of data area.As another example again, the user is such as arriving another multi-mode page by user interface navigation by indication, perhaps by submitting in the current page included information to make client-side information request or visit another multi-mode page, information is such as by (for example pressing " submission " key, " submission " key 408 is Fig. 4) or by providing some to indicate to submit to.
When client terminal device receives can permit user that vision display update and/or vision viewstate change and import the time, in frame 604, client terminal device can generate the event that client generates, and can transmit control message the event notification applications server that it generates with regard to client to application server by AS/ client control path.In one embodiment, the event that generates of client comprises the indication (for example, text input, focus change, multi-mode page changes) of event type and the indication of event details.For example, still referring to Fig. 4, when client terminal device had received for the input of " Illinois " text of data input area 402, control message can be indicated the event of text input type, and can comprise the indication of institute's input text " Illinois ".In each embodiment, the indication of institute's input text can comprise the text representation of the data of importing, the index (for example, showing the table of state title) in the table of effective clauses and subclauses or the designator of some other type.As another example, when client terminal device has received the user that focus is changed to data input area 404 when importing, control message can comprise that event that focus changes type (for example, current focus from the data input area 403 is changed into the different focuses on the data input area 404), and can comprise the information of identification data input area 404.
In frame 606, application server is from the client terminal device receiving control message, and as described below control message handled.In frame 608, application server generates response control message and sends it to client terminal device by AS/ client control path, response control message can comprise make client terminal device more new vision show and/or the vision view be converted to the information of another state.For example, during the event indication text input that generates when client (for example, the user has imported " Illinois " and has selected " input " at keyboard), if control message can change the state of vision view so that corresponding with it under the situation that next alphabetic data input area is arranged by the commands client device, and/or can come more new vision demonstration by cursor being moved to next alphabetic data input area (for example, carrying out focus changes) by the commands client device.In one embodiment, if the event that client generates indication client has been imported text in last data input area, then control message and can commands client take some other actions (for example, initiate the mapping of the information of importing or be transformed into the new page).The event indication focus that generates when client changes and when not having the text input, the state that control message can the commands client device changes the vision view be with corresponding to user-selected display element (it needn't be next sequential element), and/or can the commands client device by cursor being moved to corresponding to showing with new vision more in the data input area of user-selected display element.
In frame 610, client terminal device is from the application server receiving control message, and correspondingly responds.For example, client terminal device can by moving cursor more new vision show that and/or state that can the transforming visual view is to show the current display element that should focus on thereon corresponding to vision.For example, still referring to Fig. 4, when application server to client terminal device indicate its more new vision show and during the state of transforming visual view that client terminal device can show by new vision more and the state of transforming visual view responds according to order.For example, if client terminal device is ordered the focus with the vision view to change to display element 403, then client terminal device can be by to the explanation of the machine code that is associated with second display element 402 (for example finishing it, wait for that the user is via the input to second display element 402 of visual pattern or speech pattern, and/or wait is from the audio prompt of voice server), and by to the explanation of the machine code that is associated with the 3rd display element 403 (for example initiating, wait for the user via the input to the 3rd display element 403 of visual pattern or speech pattern, and/or wait for the audio prompt from voice server) carry out the state conversion of vision view.
With frame 608 and 610 parallel or before or after it, in frame 612, application server can be controlled the path by AS/VS and transmit control message to voice server, and it is by being included in the event included or that indicate client to generate from the information that it draws from the control message that client terminal device receives of application server in the frame 606.For example, as previously described, control message can also comprise the indication of type of the event that client generates and the indication of event details.
In frame 614, voice server is from the application server receiving control message, and for example by one or more numerical value suitably being set, identifying audio prompt, carrying out the voice view state and change and/or download or visit another voice dialogues message is handled.For example, still referring to Fig. 4, when application server has indicated client terminal device to receive the focus that the text of data input area 402 input " Illinois " and vision are shown when having changed into data and importing 403 to voice server, the indication that voice server can be stored institute's input text has been used for (for example quoting future, in definite speech recognition resource so that when during subsequent voice identification is handled, using), and the state of voice view can be changed into and vision view synchronised.For example, if the focus of vision view changes, then voice server can be by to the explanation of the machine code that is associated with second display element 402 (for example finishing it, wait is about the voice data of second display element 402 and/or generate audio prompt at second display element 402), and the state that carries out the vision view with the explanation of the machine code that has been associated at its display element (for example, the 3rd display element 403) that changes focus is changed by initiating.With the machine code that is associated at its display element that changes focus can comprise the machine code that is associated with the voice data of waiting for this display element and/or with send the machine code that the downlink voice data is associated by VS/ client voice data path to client terminal device, this can comprise the audio prompt that is associated with this display element.
In frame 616, when the current state of having determined voice view comprises to client terminal device transmission audio prompt, then in frame 618, voice server sends suitable audio prompt as the downlink voice data by VS/ client voice data path to client terminal device.For example, when focus has been changed to the 3rd display element 403, the machine code that is associated with the 3rd display element 403 can be so that voice server sends audio prompt by VS/ client voice data path to client terminal device, and it comprises corresponding to " please say street number " corresponding voice data or some similar audio prompts.In case receive voice data, client terminal device just can be such as presenting audio prompt by the audio output device output at client terminal device corresponding to the earcon of pointing out.In one embodiment, client terminal device does not present prompting and has finished frame 610 until client terminal device.Then, the method for Fig. 6 can finish.
Now, above embodiment for the method and apparatus of realizing the distributed multi-modal application is described.Above detailed description in essence only for exemplary be not to be intended to application and the use of theme of the present invention or theme of the present invention are restricted to described embodiment.In addition, be not be intended to the background technology by before or describe in detail in given any theoretical limited range.
Embodiment by the performed method of application server comprises: by the application server between application server and the voice server/voice server control path, receive the step of based on the uplink audio data that sends to voice server from client terminal device by the voice data path between client terminal device and the voice server speech having been carried out the indication of identification from voice server.Uplink audio data is represented the simulated audio signal that the speech pattern by client terminal device receives, and voice server is different from application server.Method also comprises: by the application server between application server and the client terminal device/client control path, send the step of message to client terminal device, message comprise the recognition result of speech and make client terminal device more new vision show to reflect recognition result.
Another embodiment of the method that client terminal device is performed comprises: based on the explanation that makes client terminal device present the machine code of vision demonstration is presented the step that vision shows, wherein the vision demonstration comprises and can receive at least one display element of input data by visual pattern and speech pattern to it by client terminal device.Method also comprises: receive speech by speech pattern, speech is carried out digitizing with the generation uplink audio data corresponding with one or more display elements of at least one display element, and uplink audio data is sent to the step of voice server by the voice data path between client terminal device and the voice server.Method also comprises: receive the step of speech recognition result from application server by the application server between application server and the client terminal device/client control path, wherein speech recognition result is based on the voice server of uplink audio data having been carried out speech recognition processes, and the voice data path is different from application server/client control path, and voice server is different from application server.Method also comprises: according to speech recognition result one or more display elements of showing of new vision more.
System embodiment comprises: client terminal device, voice server and application server.Client terminal device is suitable for showing and can receives at least one display element of input data to it by visual pattern and speech pattern, and when the input data that receive by speech pattern as speech, send the uplink audio data of expression speech to voice server by the voice data path between client terminal device and the voice server.Voice server is suitable for determining based on uplink audio data whether speech is identified, and when speech is identified, send the indication that speech is identified by the application server between application server and the voice server/voice server control path to application server.Application server is suitable for receiving the indication that speech is identified, and based on indication, send speech recognition result by the application server between application server and the client terminal device/client control path to client terminal device, wherein application server/client control path is different from the voice data path.
Any text sequence in the claim and do not mean that treatment step must carry out with time or logical order according to such sequence is unless limited especially by the language of claim.Treatment step can exchange and/or can executed in parallel with random order, and can't deviate from the scope of theme of the present invention.In addition, the above information that is described as be in the various different messages that exchange between the system unit that it being understood that can be combined in the single message, and/or the information in the particular message can be divided in a plurality of message.In addition, message can be sent with the order different with order described above by system unit.In addition, the word such as " being connected to " or " being coupled to " of the relation between the different parts and do not mean that must carry out direct physical connection between these parts be used for to be described.For example, two parts can be connected to each other by one or more additional components with physics, electricity, any alternate manner of logical OR, and can't deviate from the scope of theme of the present invention.
Those skilled in the art will be appreciated that, can using arbitrarily, various different technologies and technical ability come expression information and signal.For example, can run through data, instruction, order, information, signal, bit, symbol and the chip that above description quotes can be represented by voltage, electric current, electromagnetic wave, magnetic field or particle, light field or particle or its combination in any.
Those skilled in the art should further recognize, may be implemented as the two combination of electronic hardware, computer software or its in conjunction with the described various illustrative logical blocks of embodiment disclosed herein, module, circuit and algorithm steps.For the clear interchangeability of describing hardware and software, various illustrated components, frame, module, circuit and step have been carried out general description above in its function aspects.These functions whether are implemented as hardware or software depends on application-specific and the design that puts on the total system.The technician can be at every kind of described function of application-specific accomplished in various ways, but the result of these embodiments should not be interpreted as causing deviating from the scope of theme of the present invention.
Its combination in any that can utilize general processor, digital signal processor (DSP), special IC (ASIC), field programmable gate array (FPGA) or other programmable logic device (PLD), discrete gate or transistor logic, discrete hardware components or be designed to realize function as described herein in conjunction with the described various illustrative logical blocks of embodiment disclosed herein and module realizes or carries out.General processor can be microprocessor, but replacedly, processor can make conventional processors, controller, microcontroller or state machine arbitrarily.Processor also may be implemented as the combination of calculation element, such as combination or other such configuration arbitrarily of the combination of the combination of DSP and microprocessor, a plurality of microprocessors, one or more microprocessors of being combined with the DSP nuclear phase.
Can be directly realize with the performed one or more software modules of hardware, processor or with the two combination in conjunction with the step of the described method of embodiment disclosed herein or algorithm.Software module may reside in the storage medium of random access storage device, flash memory, ROM (read-only memory) (ROM), erasable programmable ROM (EPROM), electric EPROM, register, hard disk, portable disk, compact-disc ROM (CD-ROM) or other form arbitrarily known in the art.Exemplary storage medium is coupled to processor so that processor can write information from read information and to it.In interchangeable mode, storage medium can be integrated in the processor.Processor and storage medium may reside among the ASIC.ASIC may reside in the user terminal.In interchangeable mode, processor and storage medium can be used as discrete component and are present in the user terminal.
Though in above detailed description, at least one exemplary embodiment is described, should recognizes to have multiple version.What should also realize that is, one or more exemplary embodiments are example only, and is not to be intended to by any way scope, application or the configuration of theme of the present invention are limited.On the contrary, above detailed description will be provided for realizing the conventional scheme of the exemplary embodiment of theme of the present invention for those skilled in the art, and institute should be understood that described component function in the exemplary embodiment and the aspect is set carries out various variations and can not deviate from scope as the given theme of the present invention of claims and legal equivalents body thereof.