CN117716344A - Determining available memory on a mobile platform - Google Patents
Determining available memory on a mobile platform Download PDFInfo
- Publication number
- CN117716344A CN117716344A CN202280049113.3A CN202280049113A CN117716344A CN 117716344 A CN117716344 A CN 117716344A CN 202280049113 A CN202280049113 A CN 202280049113A CN 117716344 A CN117716344 A CN 117716344A
- Authority
- CN
- China
- Prior art keywords
- memory
- application
- metrics
- computing device
- processors
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000000034 method Methods 0.000 claims description 232
- 230000008569 process Effects 0.000 claims description 120
- 238000012549 training Methods 0.000 claims description 61
- 238000013528 artificial neural network Methods 0.000 claims description 40
- 238000003860 storage Methods 0.000 claims description 33
- 230000004044 response Effects 0.000 claims description 21
- 238000012360 testing method Methods 0.000 claims description 9
- 238000012544 monitoring process Methods 0.000 claims description 6
- 238000010801 machine learning Methods 0.000 description 140
- 230000006870 function Effects 0.000 description 44
- 238000012545 processing Methods 0.000 description 25
- 239000010410 layer Substances 0.000 description 21
- 238000004891 communication Methods 0.000 description 18
- 241000287219 Serinus canaria Species 0.000 description 14
- 238000005457 optimization Methods 0.000 description 12
- 230000000306 recurrent effect Effects 0.000 description 11
- 238000004458 analytical method Methods 0.000 description 10
- 230000009471 action Effects 0.000 description 9
- 238000010586 diagram Methods 0.000 description 9
- 238000003066 decision tree Methods 0.000 description 8
- 238000001514 detection method Methods 0.000 description 8
- 238000005516 engineering process Methods 0.000 description 8
- 230000010354 integration Effects 0.000 description 8
- 230000002787 reinforcement Effects 0.000 description 8
- 238000009662 stress testing Methods 0.000 description 7
- 230000003287 optical effect Effects 0.000 description 6
- 230000001965 increasing effect Effects 0.000 description 5
- 238000000513 principal component analysis Methods 0.000 description 5
- 238000007637 random forest analysis Methods 0.000 description 5
- 230000009467 reduction Effects 0.000 description 5
- 238000013527 convolutional neural network Methods 0.000 description 4
- 238000009826 distribution Methods 0.000 description 4
- 238000012417 linear regression Methods 0.000 description 4
- 238000007781 pre-processing Methods 0.000 description 4
- 239000013598 vector Substances 0.000 description 4
- 230000003044 adaptive effect Effects 0.000 description 3
- 238000013145 classification model Methods 0.000 description 3
- 238000004138 cluster model Methods 0.000 description 3
- 238000004590 computer program Methods 0.000 description 3
- JHIVVAPYMSGYDF-UHFFFAOYSA-N cyclohexanone Chemical compound O=C1CCCCC1 JHIVVAPYMSGYDF-UHFFFAOYSA-N 0.000 description 3
- 238000013500 data storage Methods 0.000 description 3
- 230000000694 effects Effects 0.000 description 3
- 238000013507 mapping Methods 0.000 description 3
- 230000009466 transformation Effects 0.000 description 3
- 230000000007 visual effect Effects 0.000 description 3
- 229920001621 AMOLED Polymers 0.000 description 2
- 238000012935 Averaging Methods 0.000 description 2
- 230000002776 aggregation Effects 0.000 description 2
- 238000004220 aggregation Methods 0.000 description 2
- 230000003321 amplification Effects 0.000 description 2
- 230000008901 benefit Effects 0.000 description 2
- 230000001413 cellular effect Effects 0.000 description 2
- 230000008859 change Effects 0.000 description 2
- 239000003795 chemical substances by application Substances 0.000 description 2
- 238000000354 decomposition reaction Methods 0.000 description 2
- 230000007812 deficiency Effects 0.000 description 2
- 238000011143 downstream manufacturing Methods 0.000 description 2
- 238000000605 extraction Methods 0.000 description 2
- 239000000835 fiber Substances 0.000 description 2
- 238000007477 logistic regression Methods 0.000 description 2
- 238000003058 natural language processing Methods 0.000 description 2
- 230000001537 neural effect Effects 0.000 description 2
- 238000003199 nucleic acid amplification method Methods 0.000 description 2
- 238000010238 partial least squares regression Methods 0.000 description 2
- 201000003042 peeling skin syndrome Diseases 0.000 description 2
- 229920001467 poly(styrenesulfonates) Polymers 0.000 description 2
- 238000012628 principal component regression Methods 0.000 description 2
- 238000005070 sampling Methods 0.000 description 2
- 230000003068 static effect Effects 0.000 description 2
- 241000288113 Gallirallus australis Species 0.000 description 1
- 235000014749 Mentha crispa Nutrition 0.000 description 1
- 244000078639 Mentha spicata Species 0.000 description 1
- 230000006978 adaptation Effects 0.000 description 1
- 238000003491 array Methods 0.000 description 1
- 238000013473 artificial intelligence Methods 0.000 description 1
- 230000006399 behavior Effects 0.000 description 1
- 238000003339 best practice Methods 0.000 description 1
- 239000000872 buffer Substances 0.000 description 1
- 238000004422 calculation algorithm Methods 0.000 description 1
- 238000002790 cross-validation Methods 0.000 description 1
- 125000004122 cyclic group Chemical group 0.000 description 1
- 230000003247 decreasing effect Effects 0.000 description 1
- 238000013135 deep learning Methods 0.000 description 1
- 238000003708 edge detection Methods 0.000 description 1
- 230000005611 electricity Effects 0.000 description 1
- 238000013213 extrapolation Methods 0.000 description 1
- 230000002068 genetic effect Effects 0.000 description 1
- 238000009499 grossing Methods 0.000 description 1
- 230000001939 inductive effect Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 230000007774 longterm Effects 0.000 description 1
- 238000004519 manufacturing process Methods 0.000 description 1
- 238000013178 mathematical model Methods 0.000 description 1
- 239000011159 matrix material Substances 0.000 description 1
- 230000003446 memory effect Effects 0.000 description 1
- 210000005036 nerve Anatomy 0.000 description 1
- 210000002569 neuron Anatomy 0.000 description 1
- 238000013450 outlier detection Methods 0.000 description 1
- 239000002245 particle Substances 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 238000013139 quantization Methods 0.000 description 1
- 238000009877 rendering Methods 0.000 description 1
- 239000002356 single layer Substances 0.000 description 1
- 238000012706 support-vector machine Methods 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 238000013526 transfer learning Methods 0.000 description 1
- 238000000844 transformation Methods 0.000 description 1
Abstract
An application of a plurality of applications executing at one or more processors of a computing device may determine a plurality of memory metrics of the computing device. The application may determine, based at least in part on the plurality of memory metrics, information indicative of a predicted amount of secure memory available for allocation by an application of the plurality of applications. The application may adjust one or more characteristics of the application executing at the one or more processors based at least in part on the information indicative of the predicted amount of secure memory available to be allocated by the application to adjust the amount of memory allocated by the application.
Description
RELATED APPLICATIONS
This application is PCT with U.S. patent priority to U.S. patent application Ser. No.17/649,120, filed on day 27 at 1 month 2022, which U.S. patent application Ser. No.17/649,120 claims the benefit of U.S. provisional patent application Ser. No.63/227,094, filed on day 29 at 7 month 2021, each of which is incorporated herein by reference in its entirety.
Background
Applications and processes may execute at computing devices with limited amounts of memory. When the applications and processes execute at the computing device, the applications and processes may allocate memory to be used by the applications and processes, thereby reducing the amount of memory available at the computing device. When the available memory at the computing device is too low, the computing device may terminate an application or process executing at the computing device to free up memory used by the terminated application or process to increase the amount of available memory at the computing device.
Disclosure of Invention
In general, the present disclosure relates to techniques for enabling an application executing at a computing device to predict an amount of secure memory that the application can allocate without being terminated by the computing device when the computing device experiences a low memory condition. The mobile computing device may execute multiple processes (e.g., multiple applications) simultaneously. When the processes execute, each process may utilize the memory of the mobile computing device by allocating and deallocating memory as needed. As processes allocate more and more memory for mobile computing devices, the mobile computing devices may reach a low memory state in which the mobile computing devices lack available memory for use (e.g., allocation) by processes executing at the mobile computing devices.
When the mobile computing device reaches such a low memory state or condition, the mobile computing device may free up available memory by throttling or terminating one or more lower priority processes, requesting the processes to free up non-critical cache resources, and so forth. However, a process executing at a computing device may not be able to receive or otherwise determine information regarding whether the process is at risk of being terminated by the mobile computing device due to the mobile computing device reaching a low memory state or condition. For example, a process may not be able to determine whether the process is able to continue to allocate additional memory without being at risk of being terminated by the mobile computing device, or whether the process should free additional memory to prevent the mobile computing device from reaching a low memory state.
In some examples, a process may access a system level memory metric (metric) to determine whether the process is able to continue to allocate additional memory without being at risk of being terminated by the mobile computing device (e.g., the mobile computing device will not enter a low memory state) or whether the process should release additional memory to prevent the mobile computing device from reaching a low memory state. However, the value of the system level memory metric may not necessarily provide an accurate indication of the real-time memory usage status of the computing device. For example, values of some system-level memory metrics, such as memory metrics that indicate the amount of memory available in a computing device, may behave incorrectly and/or may provide incorrect values, may take too long to access and/or read, may not be affected by all types of memory allocation, and/or may lag behind activities occurring at the computing device.
In accordance with aspects of the disclosure, an application executing at a mobile computing device may access system-level memory metrics as the application executes and may use one or more neural networks trained via machine learning to predict memory usage information of the application based on the memory metrics. Such memory usage information may include information regarding the amount of secure memory that can be allocated without being at risk of being terminated by the computing device, whether the application is at risk of being terminated by the computing device, whether the application should cease significant memory allocation, whether the application should release memory, and so forth.
One or more neural networks may be trained using training data collected from stress tests conducted across a variety of different computing devices. For example, stress testing can be performed on a computing device by an application that performs different types of memory (e.g., system memory graphics) allocations on the computing device until the application crashes or is terminated, and data associated with such different memory allocations is included as training data. Hundreds of such stress tests can be conducted on each of the computing devices spanning tens, hundreds, or thousands of different computing devices, and data from those stress tests can be included in the training data. By training using training data collected from stress tests performed across a variety of different computing devices, one or more neural networks may not be specific to any one type or model of computing device, but may instead be used by a variety of different types and/or models of computing devices to accurately predict memory usage information that can be used by an application to make memory usage decisions.
In some aspects, the technology described herein relates to a method comprising: determining, by one of a plurality of applications executing at one or more processors of a computing device, a plurality of memory metrics of the computing device; determining, by an application executing at one or more processors, information indicative of a predicted amount of secure memory available for allocation by the application based at least in part on a plurality of memory metrics; and adjusting, by an application executing at the one or more processors, one or more characteristics of the application executing at the one or more processors to adjust the amount of memory allocated by the application based at least in part on the information indicative of the predicted amount of secure memory available to be allocated by the application.
In some aspects, the technology described herein relates to a computing device comprising: a memory; and at least one processor communicatively coupled to the memory and configured to: executing a plurality of applications; determining a plurality of memory metrics for a memory; determining, based at least in part on the plurality of memory metrics, information indicative of a predicted amount of secure memory available for allocation by an application from the plurality of applications; and adjusting one or more characteristics of an application executing at the one or more processors to adjust the amount of memory allocated by the application based at least in part on the information indicative of the predicted amount of secure memory available to be allocated by the application.
In some aspects, the technology described herein relates to a computer-readable storage medium having instructions stored thereon that, when executed, cause one or more processors of a computing device to: determining a plurality of memory metrics for a memory; determining information indicative of a predicted amount of secure memory available for allocation by the application based at least in part on the plurality of memory metrics; and adjusting one or more characteristics of an application executing at the one or more processors to adjust the amount of memory allocated by the application based at least in part on the information indicative of the predicted amount of secure memory available to be allocated by the application.
In some aspects, the technology described herein relates to a computer-readable storage medium comprising: means for determining a plurality of memory metrics for a computing device; means for determining information indicative of a predicted amount of secure memory available for allocation by the application based at least in part on the plurality of memory metrics; means for adjusting one or more characteristics of an application executing at one or more processors to adjust an amount of memory allocated by the application based at least in part on information indicative of a predicted amount of secure memory available to be allocated by the application.
The details of one or more examples are set forth in the accompanying drawings and the description below. Other features, objects, and advantages of the disclosure will be apparent from the description and drawings, and from the claims.
Drawings
Fig. 1 is a block diagram illustrating an example mobile computing device in accordance with one or more aspects of the present disclosure.
Fig. 2A-2E are conceptual diagrams illustrating aspects of an example machine learning model according to example embodiments of the disclosure.
Fig. 3 is a flowchart illustrating example operations of a mobile computing device in accordance with one or more aspects of the present disclosure.
Detailed Description
Fig. 1 is a block diagram illustrating an example computing device 100 in accordance with one or more aspects of the present disclosure. Fig. 1 illustrates only one particular example of a computing device 100, and many other examples of computing devices 100 may be used in other instances and may include a subset of the components included in the example computing device 100 or may include additional components not shown in fig. 1.
In the example of fig. 1, computing device 100 includes one or more processors 102, a user interface device 104, one or more input devices 106, one or more output devices 108, one or more communication units 110, random access memory 112, and one or more storage devices 114. Communication channel 116 may interconnect each of components 102, 104, 106, 108, 110, 112, and/or 114 for inter-component communication (physically, communicatively, and/or operatively). In some examples, communication channel 116 may include a system bus, a network connection, an interprocess communication data structure, or any other method for transferring data between hardware and/or software.
One or more input devices 106 of the computing device 100 may receive input, such as input from a user. Examples of inputs are touch/haptic, presence-sensitive and audio inputs. Examples of one or more input devices 106 include a presence-sensitive screen, a touch screen, a mouse, a keyboard, a touch pad, a voice response system, a video camera, a microphone, or any other type of device for detecting input from a person or machine.
One or more output devices 108 of computing device 100 may generate output. Examples of outputs are tactile, audio and visual outputs. Examples of one or more output devices 108 include a presence-sensitive screen, a touch screen, a sound card, a video graphics adapter card, a speaker, a Liquid Crystal Display (LCD), an Organic Light Emitting Diode (OLED) display, a micro-light emitting diode (micro-led) display, an Active Matrix Organic Light Emitting Diode (AMOLED) display, a haptic device, or any other type of device for generating output to a person or machine.
The one or more communication units 110 of the computing device 100 may communicate with external devices via one or more networks by transmitting and/or receiving network signals over the one or more networks (e.g., one or more wired and/or wireless networks). For example, computing device 100 may use one or more communication units 110 to transmit and/or receive radio signals over a radio network, such as a cellular radio network. Also, one or more communication units 110 may transmit and/or receive satellite signals over a satellite network, such as a Global Positioning System (GPS) network. Examples of one or more communication units 110 include a network interface card (e.g., such as an ethernet card), an optical transceiver, a radio frequency transceiver, a GPS receiver, or any other type of device capable of transmitting and/or receiving information. Other examples of communication unit 110 may include a short wave radio, a cellular data radio, a wireless ethernet network radio, and a Universal Serial Bus (USB) controller.
The user interface device 104 may be a display device that displays information. In some examples, the user interface device 104 may use haptic, audio, or visual stimuli as described above with reference to the one or more output devices 108 to provide output to a user. For example, the user interface device 104 may provide a display or video output as described with reference to the one or more output devices 108.
The user interface device 104 may also provide input capabilities such as those described above with reference to the one or more input devices 106. For example, the user interface device 104 may be a presence-sensitive screen that may receive tactile user input from a user of the computing device 100. When the user interface device 104 is a presence-sensitive screen, the user interface device 104 may include a presence-sensitive input component that may detect objects at and/or near the screen of the user interface device 104. As one example range, the user interface device 104 may detect objects such as fingers or a stylus within two inches or less of the screen of the user interface device 104. The user interface device 104 may determine the location (e.g., (x, y) coordinates) of the user interface device 104 of the detected object. In another example scope, the user interface device 104 may detect objects in a range of six inches or less from the user interface device 104, and other ranges are possible. The user interface device 104 may use capacitive, inductive, radar-based, and/or optical recognition techniques to determine the location of the user interface device 104 selected by the user's finger. In some examples, the user interface device 104 also provides output to the user using touch, presence-sensitive, audio, or video stimuli, as described with respect to the user interface device 104. The user interface device 104 may be any type of output device that provides visual output, such as described with respect to one or more output devices 108.
While shown as an internal component of computing device 100, user interface device 104 may also represent an external component that shares a data path with computing device 100 to transmit and/or receive inputs and outputs. For example, in one example, the user interface device 104 represents a built-in component of the computing device 100 that is located within an external envelope (e.g., a screen on a mobile phone) of the computing device 100 and that is physically connected to the external envelope of the computing device 100. In another example, the user interface device 104 represents an external component of the computing device 100 (e.g., a monitor and/or projector sharing a wired and/or wireless data path with a tablet computer) that is located outside of the enclosure of the computing device 100 and physically separate from the enclosure of the computing device 100.
The user interface device 104 of the computing device 100 may detect two-dimensional and/or three-dimensional gestures as input from a user of the computing device 100. For example, the sensor of the user interface device 104 may detect user movements (e.g., moving a hand, arm, pen, stylus) that are within a threshold distance of the sensor of the user interface device 104. The user interface device 104 may determine a two-dimensional or three-dimensional vector representation of movement and associate the vector representation with a gesture input having multiple dimensions (e.g., waving, pinching, clapping, stroke gesture (pen stroke)). In other words, the user interface device 104 is able to detect multi-dimensional gestures without requiring the user to gesture at or near a screen or surface where the user interface device 104 outputs information for display. Instead, the user interface device 104 is capable of detecting multi-dimensional gestures performed at or near a sensor, which may or may not be located near a screen or surface where the user interface device 104 outputs information for display.
Random Access Memory (RAM) 112 within computing device 100 may store information for processing during operation of computing device 100 (e.g., during execution of one or more of applications 120, applications 122A-122N, canary application 124, operating system 126, memory usage prediction module 128, low memory termination module 130, and memory usage prediction model 132). In some examples, RAM 112 includes temporary memory, meaning that the primary purpose of RAM 112 is not long-term storage. RAM 112 on computing device 100 may be configured to store information for a short period of time as volatile memory and, thus, not retain stored content if powered down. Examples of volatile memory include Random Access Memory (RAM), dynamic Random Access Memory (DRAM), static Random Access Memory (SRAM), and other forms of volatile memory known in the art.
In some examples, storage device 114 includes one or more computer-readable storage media. The storage device 114 may be configured to store larger amounts of information than volatile memory. The storage device 114 may be further configured to store information for long periods of time as a non-volatile memory space and to retain information after power on/off cycles. Examples of non-volatile memory include magnetic hard disk, optical disk, floppy disk, flash memory, or forms of electrically programmable memory (EPROM) or Electrically Erasable Programmable (EEPROM) memory. In the example of FIG. 1, storage 114 may store program instructions and/or data associated with applications 120, applications 122A-122N, canary applications 124, operating system 126, memory usage prediction module 128, low memory termination module 130, and memory usage prediction model 132.
In some examples, the storage device 114 or one or more components included in the storage device 114 may be stored on one or more remote computing devices external to the computing device 100 (e.g., on one or more external servers). In some examples, one or more remote computing devices may store and/or execute application 120, applications 122A-122N, canary application 124, operating system 126, memory usage prediction module 128, low memory termination module 130, and/or memory usage prediction model 132. In these examples, one or more remote computing devices may perform functionality similar to that described herein with reference to processor 102.
The one or more processors 102 may implement functionality and/or execute instructions within the computing device 100. One or more processors 102 may receive and execute instructions stored by storage device 114 that execute the functionality of applications 120, applications 122A-122N, canary applications 124, operating system 126, memory usage prediction module 128, low memory termination module 130, and memory usage prediction model 132. These instructions, executed by processor 102, may cause computing device 100 to store information within storage device 114 and/or RAM 112 during program execution. Processor 102 may execute instructions of application 120, applications 122A-122N, canary application 124, operating system 126, memory usage prediction module 128, low memory termination module 130, and memory usage prediction model 132 to perform one or more operations. That is, applications 120, applications 122A-122N, canary applications 124, operating system 126, memory usage prediction module 128, low memory termination module 130, and memory usage prediction model 132 may be operated by processor 102 to perform the various functions described herein.
In some alternative examples, computing device 100 may include only or otherwise include processor 102 and memory 112. In these examples, one or more input devices 106, user interface devices 104, one or more communication units 110, one or more output devices 108, and storage devices 114 may be external to computing device 100, but communicatively coupled with computing device 100 (e.g., via communication channel 116).
An operating system 126 may execute at one or more processors 102 to cause computing device 100 to perform various functions to manage hardware resources of computing device 100 and provide various general-purpose services (common services) for other computer programs.
The low memory termination module 130 may be executed at the one or more processors 102 to monitor the usage of the RAM 112 by processes executing at the one or more processors 102, such as the application 120. For example, low memory termination module 130 may be an operating system level daemon (daemon) of operating system 126. In some examples, low memory termination module 130 may execute to determine whether the use of RAM 112 (e.g., the amount of memory allocated) exceeds a low memory termination threshold. If low memory termination module 130 determines that the use of RAM 112 exceeds a low memory termination threshold, computing device 100 may enter a low memory state. In some examples, low memory termination module 130 may execute to determine whether the amount of free memory in RAM 112 is less than a free memory termination threshold. If low memory termination module 130 determines that the amount of free memory (e.g., unallocated memory) in RAM 112 is below the free memory termination threshold, computing device 100 may enter a low memory state. When computing device 100 enters a low memory state, low memory termination module 130 may terminate (i.e., terminate (kill)) one or more processes executing at one or more processors 102, such as by terminating one or more applications 120.
In some examples, low memory termination module 130 may determine a process to terminate from among processes currently executing at one or more processors 102 in response to use of RAM 112 exceeding a low memory termination threshold and/or an amount of free memory in RAM 112 being below an idle memory termination threshold (and thus causing computing device 100 to enter a low memory state), such as based on a priority level associated with the process executing at one or more processors 102. For example, when low memory termination module 130 determines that the use of RAM 112 exceeds a low memory termination threshold, then low memory termination module 130 may terminate the process associated with the lowest priority level of the processes executing at one or more processors 102.
In some examples, each process executing at one or more processors 102 is associated with a memory overflow score referred to as a oom _adj score, wherein the memory overflow score associated with a process may indicate a priority level associated with the process, and wherein a higher oom _adj score may indicate a lower priority. When low memory termination module 130 determines that computing device 100 has entered a low memory state, low memory termination module 130 may terminate the process associated with the highest memory overflow score among the processes executing at one or more processors 102.
Applications 120 and applications 122A-122N ("applications 122") may include one or more different various applications. Examples of applications 120 and 122 include an email application, a camera application, a map or navigation application, a calendar application, a messaging application, a social media application, a travel application, a gaming application, a stock application, a weather application, and the like.
Application 120 and application 122 may execute at one or more processors 102. When the applications 120 and 122 execute at the one or more processors 102, the applications 120 may determine one or more memory metrics of the computing device and may determine information indicative of an amount of secure memory available for allocation by the applications 120 based at least in part on the one or more memory metrics based on the memory metrics of the computing device. The application 120 may adjust one or more characteristics of the application 120 executing at the one or more processors 102 based on information indicating an amount of secure memory available for allocation by the application 120.
In the example of fig. 1, the application 120 includes a memory usage prediction module 128. In some examples, the memory usage prediction module 128 may be a library included in an application package (application package) of the application 120 or otherwise bundled in the application 120 instead of the operating system 126. That is, in examples where computing device 100 downloads application 120 from an external system, the application package of application 120 downloaded by computing device 100 may bundle or otherwise include a copy of memory usage prediction module 128. The memory usage prediction module 128 may execute to predict an amount of memory of the RAM 112 that the application 120 is able to safely allocate without causing the computing device 100 to enter a low memory state and thus be at risk of being terminated by the low memory termination module 130 based on memory usage of the application 120 and other applications (e.g., the application 122) and/or processes executing at the one or more processors 102. Additionally, the memory usage prediction module 128 may also execute to predict whether the application 120 is at risk of being terminated by the low memory termination module 130 due to the computing device 100 being in a low memory state based on memory usage of the application 120 and other applications (e.g., the application 122) and/or processes executing at the one or more processors 102. For purposes of this disclosure, memory of RAM 112 may refer to native heap (native heap) memory, such as memory allocated by malloc, as well as graphics memory capable of being allocated by application 120.
When the application 120 is executed at the one or more processors 102, the memory usage prediction module 128 may execute as a separate process at the one or more processors 102 to determine one or more memory metrics of the computing device 100. Memory usage prediction module 128 may execute at one or more processors 102 to determine one or more memory metrics in real-time as application 120 executes at one or more processors 102. When the memory metrics of computing device 100 change during execution of application 120, memory usage prediction module 128 may be able to determine the most current memory metrics of computing device 100.
Memory metrics of computing device 100 may include information associated with the use of RAM 112 by processes executing at one or more processors 102 of computing device 100, such as the use of RAM 112 by applications 120, applications 122, operating system 126, any other processes executing at one or more processors 102. In some examples, the memory metrics of computing device 100 may include: one or more memory metrics indicating how much RAM 112 memory can also be reallocated before the low memory termination module 130 terminates a process executing at the one or more processors 102, one or more memory metrics indicating how much RAM 112 memory is allocated to a process executing at the one or more processors 102, one or more metrics indicating the total amount of RAM 112 memory that can be allocated before the low memory termination module 130 terminates a process executing at the one or more processors 102, and/or one or more metrics indicating RAM 112 experiences memory pressure.
The computing device 100 may track memory metrics at the operating system level or at the kernel level through the computing device 100. As such, the memory metrics of computing device 100 may include one or more kernel-level memory metrics and/or one or more operating system-level memory metrics, and memory usage prediction module 128 may use any combination of the kernel-level memory metrics and the system-level memory metrics as input to make the one or more predictions. The one or more kernel-level memory metrics may include information derived by the kernel of the operating system 126 and stored in the internal data structures of the kernel. In the example where the kernel is a Linux kernel, the values of the internal data structures are disclosed by one or more files of the proc file system that serve as an interface to the internal data structures in the kernel. The kernel may continuously write the latest values of the kernel's internal data structures to one or more files of the proc file system and the memory usage prediction module 128 may access (e.g., read) the one or more files of the proc file system on the one or more storage devices 114 to determine the latest values of the one or more kernel-level memory metrics.
In some examples, the kernel-level memory metrics may include information regarding the distribution and utilization of memory of RAM 112 by applications and processes executing at one or more processors 102. In the example of a Linux kernel, the directory/proc/meminfo includes the following kernel-level memory metrics:
·Active
·Active(anon)
·Active(file)
·AnonPages
·Bounce
·Buffers
·Cached
·CmaTotal
·CommitLimit
·Committed_AS
·Dirty
·Inactive
·Inactive(anon)
·Inactive(file)
·KernelStack
·Mapped
·MemAvailable
·MemFree
·MemTotal
·Mlocked
·NFS_Unstable
·PageTables
·SReclaimable
·SUnreclaim
·Shmem
·Slab
·SwapCached
·SwapFree
·SwapTotal
·Unevictable
·VmallocChunk
·VmallocTotal
·VmallocUsed
·Writeback
·WritebackTmp
In some examples, the kernel-level memory metrics may include per-process memory metrics. In the example of a Linux kernel, the directory/proc contains a subdirectory for each process executing at one or more processors 102. For example, the directory/proc/< pid > may be a subdirectory of a process < pid > executing at one or more processors 102. For a process, the kernel-level memory metric may include a value indicating a priority level of the process, such as a memory overflow score associated with the process. In the example of a Linux kernel, for process < pid >, the file/proc/< pid >/oom _score includes an associated memory overflow score for process < pid > indicating a priority level for the process, where processes associated with lower memory overflow scores may have a higher priority level than processes associated with higher memory overflow scores.
In some examples, the kernel-level memory metrics may include per-process memory metrics that include state information for the process. In the kernel example of Linux, the file/proc/< pid >/status includes the following state information for process < pid >:
·VmData
·VmExe
·VmHWM
·VmLck
·VmLib
·VmPMD
·VmPTE
·VmPeak
·VmPin
·VmRSS
·VmSize
·VmStk
·VmSwap
the one or more operating system level memory metrics may be information returned from operating system 126 by invoking one or more functions of an Application Programming Interface (API) of operating system 126. Memory usage prediction module 128 may call such functions of the API of operating system 126 to receive the latest value of one or more system-level memory metrics.
In some examples, memory usage prediction module 128 may call functions provided by operating system 126 to receive information about available memory. In the example of an Android operating system, the memory usage prediction module 128 may call an actigymonitor. The function may return values for the following parameters:
availmem—the available memory on the system;
lowmemory—set to true when the system considers itself to be in a low memory condition;
threshold—threshold of availMem where the system considers memory low and starts to terminate background services and other non-unrelated processes; and
total mem—total memory accessible by the kernel.
In some examples, memory usage prediction module 128 may call functions provided by operating system 126 to receive information regarding memory usage of one or more processes executing at one or more processors 102. In the example of an Android operating system, the memory usage prediction module 128 may call an actigymonitor. The function may return a value of the parameter memryinfo, which may be information about the memory usage of the application 120.
In some examples, memory usage prediction module 128 may also call one or more debug (debug) functions provided by operating system 126 to receive information regarding memory usage of one or more processes executing at one or more processors 102. In the example of an Android operating system, the memory usage prediction module 128 may call a debug.
When the application 120 is executing on one or more processors 102 and uses memory in the RAM 112, the memory usage prediction module 128 may perform the techniques described herein to retrieve, access, or otherwise determine a plurality of memory metrics, including one or more kernel-level metrics and/or one or more operating system-level metrics, in real-time. In some examples, memory usage prediction module 128 may poll one or more memory metrics to retrieve, access, or otherwise determine values of one or more memory metrics. In some examples, the memory usage prediction module 128 may generate a background process that periodically, such as based on a timer, retrieves, accesses, or otherwise determines values of one or more memory metrics and pushes the one or more memory metric values to the memory usage prediction module 128.
The application 120, when executed at one or more processors 102, may use the memory usage prediction module 128 to determine information indicative of the predicted amount of secure memory available in the RAM 112 for allocation by the application 120. When the application 120 is launched, the memory usage prediction module 128 associated with the application 120 may also be launched and executed as a separate process at the one or more processors 102 to determine information indicative of memory available for allocation by the application 120 based on a plurality of memory metrics. In some examples, determining memory information indicative of a predicted amount of secure memory for allocation by the application 120 may include predicting the amount of secure memory available for allocation by the application 120 based on a plurality of memory metrics. As the memory usage prediction module 128 executes at the one or more processors 102, the memory usage prediction module 128 may retrieve and/or determine values of a plurality of memory metrics in real-time and predict an amount of secure memory available for allocation by the application 120 based on the plurality of memory metrics.
As described above, when the low memory termination module 130 determines that the use of the RAM 112 (e.g., the amount of memory allocated by a process executing at one or more processors 102) exceeds a low memory termination threshold, the low memory termination module 130 may enter a low memory state and may terminate one or more processes executing at one or more processors 102. For example, the low memory termination module 130 may terminate a process associated with a lowest priority level of one or more processes executing at the one or more processors 102. Thus, in some examples, the application 120 may be at risk of being terminated by the low memory termination module 130 when the amount of memory allocated by the processes executing at the one or more processors 102 of the RAM 112 exceeds a low memory termination threshold.
As such, the amount of secure memory for allocation by the application 120 may be the total amount of memory of the RAM 112 that the application 120 is able to allocate during execution of the application 120 without causing the computing device 100 to enter a low memory state. In some examples, the amount of secure memory for allocation by the application 120 may be an additional amount of memory for the RAM 112 that can be securely allocated by the application 120 without causing the computing device 100 to enter a low memory state. That is, assuming that the application 120 may have allocated a certain amount of memory in the RAM 112 before the memory usage prediction model 128 predicts the amount of secure memory for allocation by the application 120, the resulting output of the memory usage prediction module 128 may thus indicate that there is an amount of memory that is secure for allocation by the application 120 in addition to the amount of memory that the application 120 has allocated. The amount of additional memory in RAM 112 that is secure for allocation by application 120 may be referred to herein as a headroom (headroom).
The memory usage prediction module 128 may predict the amount of secure memory available for allocation by the application 120 based on a plurality of memory metrics, which may be a subset (i.e., less than all) of the memory metrics listed above, rather than relying on the value of a single memory metric, as such a single memory metric may not necessarily be related to actual memory usage and availability of RAM 112, and may not reflect accurate real-time memory usage of RAM 112. For example, a single memory metric may sometimes have unexpected behavior on some systems, may take a long time to read, may not be affected by all types of memory allocation, may lag behind real-time memory activity, and/or may not otherwise accurately reflect the actual memory usage of RAM 112.
In an example of an availMem memory metric, a significant portion of the available memory of the system indicated as available by the availMem memory metric may actually be required for computing device 100 to operate in an optimal manner and thus be practically unavailable for allocation by application 120. In another example, while the MemAvailable memory metric in the directory/proc/meminfo may indicate an estimated amount of memory available to start a new application, such memory metric may not exist on some computing devices and may not increase the estimated amount of memory available to start a new application appropriately when memory is released.
In some examples, determining information indicative of the predicted amount of secure memory for allocation by the application 120 may include predicting whether the application 120 is at risk of being terminated by the low memory termination module 130 based on a plurality of memory metrics. When computing device 100 is in a low memory state, application 120 may be at risk of being terminated by low memory termination module 130. As such, predicting whether the application 120 is at risk of being terminated by the low memory termination module 130 based on the plurality of memory metrics may include predicting whether the computing device 100 is in a low memory state based on the plurality of memory metrics. If memory usage prediction module 128 predicts that computing device 100 is in a low memory state based on a plurality of memory metrics, memory usage prediction module 128 may determine that application 120 is at risk of being terminated by low memory termination module 130.
The memory usage prediction module 128 may use any suitable technique to predict the amount of secure memory available for allocation by the application 120 based on a plurality of memory metrics and predict whether the application 120 is at risk of being terminated by the low memory termination module 130 based on the plurality of memory metrics. In some examples, using memory metrics to predict the amount of secure memory available for allocation by application 120 and to predict whether application 120 is at risk of being terminated by low memory termination module 130 may include any combination of the following memory metrics:
parameters from the/proc/meminfo file, including parameters Active, active (anon), active (file), anonPages, memAvailable, memFree, vmData, vmRSS, commitLimit, highTotal, lowTotal, and MemTotal;
parameters from the/proc/< pid >/status file for: applications and processes executing on the computing device and/or applications or processes that terminate due to low memory or memory overflow conditions, including the parameters VmRSS and VmSize, wherein VmRSS may indicate for a process the size of the memory portion used by the process, and wherein VmSize may indicate the overall program size of the process;
parameters returned by calling the function actigitymanager. Getmemorinfo (), including parameters totalMem, threshold, availMem and lowMemory;
Memory overflow score from/proc/< pid >/oom _score file for: applications and processes executing on the computing device and/or applications or processes that terminate due to low memory or memory overflow conditions;
parameters returned by calling function debug. Getnative HeapoAlcalionSize (); and
parameters returned by calling the function actiglymanager.
In some examples, the memory usage prediction module 128 may predict whether the application 120 is at risk of being terminated by the low memory termination module 130 by monitoring canary applications 124 executing at the one or more processors 102 to determine whether the applications 124 have been terminated by the low memory termination module 130. In response to determining that canary application 124 has been terminated by low memory termination module 130, memory usage prediction module 128 may make a prediction that application 120 is at risk of being terminated by low memory termination module 130, and may send an alert to application 120 indicating that application 120 is at risk of being terminated by low memory termination module 130 based on making the prediction that application 120 is at risk of being terminated by low memory termination module 130.
In some examples, memory usage prediction module 128 may predict the amount of secure memory available for allocation by application 120 and may predict whether application 120 is at risk of being terminated by low memory termination module 130 by monitoring the values of the plurality of memory metrics. In some examples, the plurality of memory metrics may include any combination of the memory metrics listed above. In some examples, the plurality of memory metrics may be memory metrics that indicate how much memory of RAM 112 is being used. Examples of such memory metrics may include the VmRSS parameter from the file/proc/< pid >/status indicating the amount of physical memory used by process < pid >, the VmSize parameter from the file/proc/< pid >/status indicating the overall program size of process < pid >, the debug.total-value-PSSs parameter from the debug.memory info.getmemostat () function indicating the scaled memory use by calling the mapping from/proc/meminfo, the parameter from the debug.getnative heat allocation size () parameter indicating the size of the allocated native heap, the PSS parameter from the debugs class indicating the scaled memory size used by the process, etc.
When applications and processes executing at the processor 102 are terminated, such as by the low memory termination module 130, the memory usage prediction module 128 may monitor the values of the plurality of memory metrics to determine the highest value reached by each of the plurality of memory metrics at a point in time immediately prior to the termination of the application or process. Such a highest value reached by each of the plurality of metrics may be used as a respective upper threshold for each of the plurality of memory metrics.
In some examples, memory usage prediction module 128 may use an upper threshold for each of a plurality of memory metrics, which may be preloaded on computing device 100, such as during manufacturing, may be received from an external system (e.g., cloud), may be downloaded during a software update, and so forth. In some examples, the external system may periodically determine an upper threshold for each of the plurality of memory metrics specific to the particular model and configuration of computing device 100, and computing device 100 may periodically receive updated upper thresholds for each of the plurality of memory metrics specific to the particular model and configuration of computing device 100 from the external system.
In some examples, memory usage prediction module 128 may use machine learning to predict the upper threshold for each of the plurality of memory metrics. For example, the memory usage prediction module 128 may include a memory usage prediction model 132 that includes one or more neural networks trained to generate an output as an upper threshold for each of a plurality of memory metrics at a point in time immediately prior to termination of one or more applications or processes input to the memory usage prediction module 128 based on the highest value reached by each of the plurality of memory metrics.
In general, the one or more neural networks implemented by the memory usage prediction model 132 may include a plurality of interconnected nodes, and each node may apply one or more functions to a set of input values corresponding to one or more features and provide one or more corresponding output values that are upper threshold values for each of a plurality of memory metrics. In some examples, one or more neural networks of the memory usage prediction model 132 may be trained off-device and then downloaded to or installed at the computing device 100. In some examples, one or more neural networks of the memory usage prediction model 132 may be trained on the device by the memory usage prediction model 132 to more accurately determine the upper threshold for each of the plurality of memory metrics. For example, the one or more neural networks may include one or more learnable parameters or "weights" applied to the features. The memory usage prediction model 132 may adjust these learnable parameters during training to improve the accuracy with which the one or more neural networks determine the upper threshold for each of the plurality of memory metrics.
The memory usage prediction module 128 may determine whether the application 120 is at risk of being terminated by the low memory termination module 130 by determining whether any of the plurality of memory metrics has a current value that exceeds an upper threshold of the memory metrics. In some examples, if the memory usage prediction module 128 determines that the current value of at least one of the plurality of metrics exceeds the upper threshold of the metric, the memory usage prediction module 128 may predict that the application 120 is at risk of being terminated by the low memory termination module 130 and may send a warning to the application 120 indicating that the application 120 is at risk of being terminated by the low memory termination module 130. In some examples, if the memory usage prediction module 128 determines that the current value of each of the plurality of metrics exceeds the respective upper threshold of the metric, the memory usage prediction module 128 may predict that the application 120 is at risk of being terminated by the low memory termination module 130 and may send a warning to the application 120 indicating that the application 120 is at risk of being terminated by the low memory termination module 130.
In some examples, memory usage prediction module 128 may predict the amount of secure memory available for allocation by application 120 as a difference between one or more of the current values of the plurality of memory metrics and one or more of the upper threshold values of the respective plurality of memory metrics. For example, the memory usage prediction module 128 may predict the amount of secure memory available for allocation by the application 120 as the difference between the current value of the availMem memory metric and the determined upper threshold of the availMem memory metric. In another example, the memory usage prediction module 128 may determine a difference between a current value of each of the plurality of memory metrics and a respective upper threshold value of each of the plurality of memory metrics that indicates an amount of memory of the RAM 112 being used, and may predict the secure memory amount available for allocation by the application 120 as a smallest of the differences between the current value of each of the plurality of memory metrics and the respective upper threshold value of each of the plurality of memory metrics.
In some examples, memory usage prediction module 128 may determine whether significant allocation of memory by application 120 should cease and/or determine whether memory allocated by application 120 should be released based on predicting the amount of secure memory for allocation by application 120 and/or whether application 120 is at risk of being terminated. Examples of significant allocation of memory may be allocation of memory in excess of a specified amount of memory, such as allocation of memory in excess of 100 megabytes, allocation of memory in excess of 50 megabytes, and so forth.
In some examples, if the memory usage prediction module 128 determines that the predicted amount of secure memory for allocation by the application 120 is below a specified threshold, such as 100 megabytes of memory, 50 megabytes of memory, etc., or if the predicted amount of secure memory for allocation by the application 120 as a percentage of the memory size of the RAM 112 is below a specified threshold (e.g., 3%, 5%, 7%, etc.), the memory usage prediction module 128 may determine that significant allocation of memory by the application 120 should cease. In some examples, if the memory usage prediction module 128 predicts that the application 120 is at risk of being terminated, the memory usage prediction module 128 may determine that memory allocated by the application 120 should be released.
In some examples, memory usage prediction module 128 may also use machine learning to predict the amount of secure memory available for allocation by application 120 without causing computing device 100 to enter a low memory state. For example, the memory usage prediction module 128 may include a memory usage prediction model 132 that includes one or more neural networks trained to determine, based on values of a plurality of memory metrics input to the memory usage prediction module 128, to generate an output indicative of an amount of secure memory available for allocation by the application 120. In some examples, memory usage prediction module 128 may also use machine learning to determine whether a portion of application 120 is predicted to be at risk of being terminated by low memory termination module 130. For example, the memory usage prediction module 128 may include a memory usage prediction model 132 that includes one or more neural networks trained to determine to generate an output indicative of whether the application 120 is at risk of being terminated based on values of one or more memory metrics input to the memory usage prediction model 132.
One or more neural networks of memory usage prediction module 128 may be trained using training data generated by executing multiple applications on various computing devices for stress testing. The different computing devices may include computing devices manufactured by different manufacturers, different models of computing devices manufactured by the same manufacturers, computing devices including different processors, computing devices having different amounts of memory that can be allocated by processes executing at the computing devices, computing devices running different versions of an operating system, computing devices executing different combinations of applications and/or processes, computing devices having different versions of firmware, and so forth.
Pressure testing may be performed on each computing device by executing multiple applications and/or processes on each computing device that may continue to allocate memory until the computing device terminates the application and/or process, such as due to a memory overflow or a memory deficiency of the computing device. The applications and/or processes may allocate different types of memory (e.g., native heap memory and/or graphics memory) as the applications and/or processes execute on the computing device during stress testing, and may monitor and collect values of memory metrics of the computing device as the applications and/or processes execute. Similarly, when an application and/or process is terminated by a low memory termination module, for example at a computing device, the value of the memory metric when the application and/or process is terminated may also be collected. Such pressure tests may be run thousands of times (e.g., over 2000 times).
Thus, the training data may include: the value of the memory metric collected by the computing device when the application and/or process is executing during a period of time when no application or process is terminated due to a low memory or memory overflow condition. The training data may also include: the value of the memory metric collected by the computing device when the application and/or process is executed at a point in time when the application and/or process is terminated due to a low memory or memory overflow condition. Examples of memory metrics may include any combination of the memory metrics listed above. In addition, the training data may also include other memory metrics, such as information associated with canary applications executing at the computing device. Similarly, the inputs to the memory usage prediction model 132 may be the values of the memory metrics listed above, as well as any other suitable values, such as information associated with canary applications 124 executing at one or more processors 102.
In some examples, the memory usage prediction module 128 may predict whether the application 120 is at risk of being terminated by the low memory termination module 130 by monitoring the canary application 124 executing at the one or more processors 102 to determine whether the canary application 124 has been terminated by the low memory termination module 130. In response to determining that canary application 124 has been terminated by low memory termination module 130, memory usage prediction module 128 may make a prediction that application 120 is at risk of being terminated by low memory termination module 130, and based on making the prediction that application 120 is at risk of being terminated by low memory termination module 130, may send an alert to application 120 indicating that application 120 is at risk of being terminated by low memory termination module 130.
In some examples, when executed by memory usage prediction module 128, memory usage prediction module 128 may provide an API including one or more functions that application 120 may call to receive information from memory usage prediction module 128 associated with the predicted amount of secure memory determined by memory usage prediction module 128 to be available for allocation by application 120. When the application 120 invokes one or more functions of the API to receive information associated with the predicted amount of secure memory available for allocation by the application 120, the memory usage prediction module 128 may use the real-time values of the plurality of memory metrics to determine the predicted amount of secure memory available for allocation by the application 120 and may send information to the application 120 indicating the predicted amount of secure memory available for allocation by the application 120.
In some examples, the API provided by the memory usage prediction module 128 may include one or more callback functions that may provide the application 120 with information indicating the predicted amount of secure memory available for allocation by the application 120. For example, the API may include a callback function that may alert the application 120 when the memory usage prediction module 128 predicts that the application 120 is at risk of being terminated by the low memory termination module 130. The memory usage prediction module 128 may periodically determine whether the application 120 is at risk of being terminated by the low memory termination module 130 based on the techniques described in this disclosure. Thus, in response to determining that the application 120 is at risk of being terminated by the low memory termination module 130, the memory usage prediction module 128 may send an alert to the application 120 indicating that the application 120 is at risk of being terminated by the low memory termination module 130.
Similarly, the API provided by the memory usage prediction module 128 may include one or more callback functions that send an indication to the application 120 of whether significant allocation of memory by the application 120 should cease and/or whether memory allocated by the application 120 should be released. The memory usage prediction module 128 may periodically determine whether significant allocation of memory by the application 120 should cease based on the techniques described in this disclosure. Thus, in response to determining that the application 120 should cease significant allocation of memory, the memory usage prediction module 128 may send an indication to the application 120 that the application 120 should cease significant allocation of memory.
Memory usage prediction module 128 may periodically determine whether memory allocated by application 120 should be freed based on the techniques described in this disclosure. Thus, in response to determining that the memory allocated by the application 120 should be released, the memory usage prediction module 128 may send an indication to the application 120 that the memory allocated by the application 120 should be released. The memory usage prediction module 128 is also capable of determining that significant allocation of memory by the application 120 should cease and that memory allocated by the application 120 should be freed before the operating system 126 signals to the application 120 that the application 120 is about to consume memory.
At startup, the application 120 may communicate with the memory usage prediction module 128 to receive information from the memory usage prediction module 128 indicating the predicted amount of secure memory available for allocation by the application 120. For example, the application 120 may invoke one or more functions of the API provided by the memory usage prediction module 128, and the memory usage prediction module 128 may use real-time values of a plurality of memory metrics to determine a predicted amount of secure memory available for allocation by the application 120, and may send information to the application 120 indicating the predicted amount of secure memory available for allocation by the application 120.
In response to receiving the information indicating the predicted amount of secure memory available for allocation by the application 120, the application 120 may adjust one or more characteristics of the application 120 based on the predicted amount of secure memory available for allocation by the application 120. For example, based on the predicted amount of secure memory available for allocation by the application 120, the application 120 may be able to estimate memory occupancy of assets, graphics quality, screen resolution, texture resolution, etc., and may select assets such as audio, particle effects, or shadows to be loaded into memory, adjust the graphics quality of the application 120, adjust the screen resolution used by the application 120, and/or adjust the texture resolution of graphics output by the application 120, etc.
As the application 120 continues to execute at the one or more processors 102, the application 120 may continue to periodically communicate with the memory usage prediction module 128 to receive, from the memory usage prediction module 128, up-to-date information indicating a predicted amount of secure memory available for allocation by the application 120, and may adjust one or more characteristics of the application 120 based on the predicted amount of secure memory available for allocation by the application 120. In examples where the application 120 is a gaming application, the application 120 may call one or more functions of the API of the memory usage prediction module 128 to receive information from the memory usage prediction module 128 indicating the predicted amount of secure memory available for allocation by the application 120 prior to rendering each graphics frame when the application 120 is in a gaming state.
Accordingly, the application 120 may adjust the quality of graphics output by the application 120 based on the predicted amount of secure memory available to be allocated by the application 120, such as increasing the amount of memory of the RAM 112 allocated by the application 120 or decreasing the amount of memory of the RAM 112 allocated by the application 120. For example, if the application 120 determines that the amount of secure memory available for allocation by the application 120 as predicted by the memory usage prediction module 128 is sufficient for the application 120 to improve the quality of graphics output by the application 120, the application 120 may select a higher quality asset to load into memory, improve the graphics quality of the application 120, improve the screen resolution used by the application 120, improve the texture resolution of graphics output by the application 120, and so forth. In another example, if the application 120 determines that the amount of secure memory available for allocation by the application 120 as predicted by the memory usage prediction module 128 is insufficient for the application 120 to maintain the current quality of graphics output by the application 120, such as by determining that the amount of secure memory available for allocation by the application 120 as predicted by the memory usage prediction module 128 is less than a specified threshold (e.g., less than a certain amount of memory), the application 120 may reduce the quality of graphics output by the application 120, such as by selecting lower quality assets to load into memory, reducing the graphics quality of the application 120, reducing screen resolution used by the application 120, reducing texture resolution of graphics output by the application 120, and so forth.
In some examples, the application 120 may adjust the amount of memory adjusted by the application 120 by adjusting the number of concurrent threads used by the application 120, such as the number of concurrent threads used to decompress data. For example, the application 120 may reduce the amount of concurrent threads used by the application 120, such as by using a single thread, to reduce the amount of memory allocated by the application 120, or may increase the amount of concurrent threads used by the application 120 to increase the amount of memory allocated by the application 120.
In some examples, when application 120 executes, memory usage prediction module 128 may determine that application 120 is at risk of being terminated based on a plurality of memory metrics, such as by predicting that computing device 100 is currently in a low memory condition based on a plurality of memory metrics. In response to determining that the application 120 is at risk of being terminated, the memory usage prediction module 128 may send an indication to the application 120 that the application 120 is at risk of being terminated, such as via one or more callback functions. The application 120 may take one or more actions to reduce memory usage in response to receiving an indication that the application 120 is at risk of being terminated, such as by reducing the quality of graphics output by the application 120, reducing the number of concurrent threads used by the application 120, as described above, and the like.
In some examples, when the application 120 executes, the memory usage prediction module 128 may determine that significant allocation of memory by the application 120 should cease based on a plurality of memory metrics. In response to determining that the significant allocation of memory by the application 120 should cease, the memory usage prediction module 128 may send an indication to the application 120 to cease the significant allocation of memory by the application 120, such as via one or more callback functions. As described above, in response to receiving an indication to stop the application 120 from significantly allocating memory, the application 120 may refrain from significantly increasing the use of memory by the application 120, such as by refraining from significantly increasing the quality of graphics output by the application 120, and so forth.
In some examples, memory usage prediction module 128 may determine that memory allocated by application 120 should be freed based on a plurality of memory metrics when application 120 executes. In response to determining that the memory allocated by the application 120 should be released, the memory usage prediction module 128 may send an indication to the application 120 to release the memory allocated by the application 120, such as via one or more callback functions. In response to receiving an indication to release memory allocated by the application 120, the application 120 may take one or more actions to reduce use of memory, such as by reducing the quality of graphics output by the application 120, reducing the number of concurrent threads used by the application 120, as described above, and the like.
Fig. 2A-2E are conceptual diagrams illustrating aspects of an example machine learning model according to example embodiments of the disclosure. Fig. 2A-2E are described below in the context of the memory usage prediction model 132 of fig. 1. For example, in some instances, machine learning model 200 may be an example of memory usage prediction model 132, as referenced below.
FIG. 2A depicts a conceptual diagram of an example machine learning model, according to an example embodiment of the disclosure. As shown in fig. 2A, in some implementations, the machine learning model 200 is trained to receive one or more types of input data and, in response, to provide one or more types of output data. Thus, FIG. 2A illustrates a machine learning model 200 that performs inference.
The input data may include one or more characteristics associated with the instance or example, such as the values of one or more memory metrics described above. In some implementations, one or more features associated with an instance (instance) or sample (example) can be organized into feature vectors. In some implementations, the output data can include one or more predictions. Predictions can also be referred to as inferences. Thus, given features associated with a particular instance, machine learning model 200 is able to output predictions for such instance based on the features. For example, the machine learning model 200 may output a prediction of whether the application is at risk of being terminated, such as by the low memory termination module 130 of fig. 1, or the machine learning model 200 may output a predicted amount of secure memory available for allocation by the application.
The machine learning model 200 can be or include one or more of a variety of different types of machine learning models. Specifically, in some implementations, machine learning model 200 is capable of performing classification, regression, clustering, anomaly detection, recommendation generation, and/or other tasks.
In some implementations, the machine learning model 200 can perform various types of classification based on input data. For example, the machine learning model 200 can perform binary classification or multi-class classification. In binary classification, outputting the data can include classifying the input data into one of two different classes. In multi-class classification, outputting the data can include classifying the input data into one (or more) of more than two classes. The classification can be single-labeled or multi-labeled. The machine learning model 200 is capable of discrete class classification, wherein input data is simply classified into one or more classes or categories.
In some implementations, the machine learning model 200 is capable of classification, wherein the machine learning model 200 provides, for each of one or more classes, a numerical value describing: the machine learning model 200 should classify the input data into a degree of belief in the corresponding class. In some instances, the numerical values provided by the machine learning model 200 can be referred to as "confidence scores" that indicate respective confidence associated with classifying the input into respective classes. In some implementations, the confidence score can be compared to one or more thresholds to present discrete category predictions. In some implementations, only a certain number of classes (e.g., one) with relatively large confidence scores can be selected to present discrete class predictions.
The machine learning model 200 may output a probabilistic classification. For example, the machine learning model 200 may predict probability distributions over a set of classes given sample inputs. Thus, rather than only outputting the most likely class to which the sample input should belong, the machine learning model 200 is able to output, for each class, the probability that the sample input belongs to that class. In some implementations, the sum of probability distributions over all possible classes can be one. In some implementations, a Softmax function or other type of function or layer can be used to compress a set of real values respectively associated with the possible classes to a set of real values within a range (0, 1), the set of real values summing to one.
In some examples, the probability provided by the probability distribution can be compared to one or more thresholds to present discrete category predictions. In some implementations, only a certain number of classes (e.g., one) with a relative maximum prediction probability can be selected to present the discrete class prediction.
In the case where the machine learning model 200 is classified, the machine learning model 200 may be trained using supervised learning techniques. For example, the machine learning model 200 may be trained on a training data set that includes training examples labeled as belonging (or not belonging) to one or more classes. Further details regarding supervised training techniques are provided in the following description of fig. 2B-2E.
In some implementations, the machine learning model 200 can perform regression to provide output data in the form of continuous values. The consecutive values can correspond to any number of different metrics or numerical representations, including, for example, monetary values, scores, or other numerical representations. As an example, the machine learning model 200 can perform linear regression, polynomial regression, or nonlinear regression. As an example, the machine learning model 200 can perform simple regression or multiple regression. As described above, in some embodiments, a Softmax function or other function or layer can be used to compress a set of real values respectively associated with multiple possible classes to a set of real values within a range (0, 1) that is one in sum.
The machine learning model 200 may perform various types of clustering. For example, the machine learning model 200 can identify one or more previously defined clusters to which the input data most likely corresponds. The machine learning model 200 may identify one or more clusters within the input data. That is, in instances where the input data includes multiple objects, documents, or other entities, the machine learning model 200 is capable of classifying the multiple entities included in the input data into multiple clusters. In some embodiments where machine learning model 200 is clustered, machine learning model 200 can be trained using unsupervised learning techniques.
The machine learning model 200 may perform anomaly detection or outlier detection. For example, the machine learning model 200 can identify input data that does not conform to an expected pattern or other characteristic (e.g., as previously observed from previous input data). As an example, anomaly detection can be used for fraud detection or system fault detection.
In some implementations, the machine learning model 200 can provide the output data in the form of one or more recommendations. For example, the machine learning model 200 can be included in a recommendation system or engine. As an example, given input data describing previous results for certain entities (e.g., a score, ranking, or rating indicating an amount of success or happiness), machine learning model 200 can output suggestions or recommendations of one or more additional entities expected to have a desired result (e.g., elicit a score, ranking, or rating indicating success or happiness) based on the previous results. As one example, given input data describing the context of a computing device, such as computing device 100 of fig. 1, a recommendation system can output suggestions or recommendations for applications that a user may like or wish to download to computing device 100.
In some cases, machine learning model 200 may act as an agent within an environment. For example, machine learning model 200 can be trained using reinforcement learning, which is discussed in further detail below.
In some implementations, machine learning model 200 can be a parametric model, while in other implementations, machine learning model 200 can be a non-parametric model. In some implementations, the machine learning model 200 can be a linear model, while in other implementations, the machine learning model 200 can be a nonlinear model.
As described above, the machine learning model 200 can be or include one or more of a variety of different types of machine learning models. Examples of such different types of machine learning models are provided below for illustration. One or more of the example models described below can be used (e.g., combined) to provide output data in response to input data. Additional models can be used in addition to the example models provided below.
In some implementations, the machine learning model 200 can be or include one or more classifier models, such as, for example, a linear classification model; a secondary classification model, etc. The machine learning model 200 may be or include one or more regression models, such as, for example, a simple linear regression model; a multiple linear regression model; a logistic regression model; stepwise regression model; a multi-element adaptive regression spline; a locally estimated scatter plot smoothing model, etc.
In some examples, machine learning model 200 can be or include one or more decision tree-based models, such as, for example, classification and/or regression trees; iterative dichotomy 2 decision tree; c4.5 decision tree; the chi-square automatically and interactively detects a decision tree; decision tree stakes; conditional decision trees, etc.
Machine learning model 200 may be or include one or more core machines. In some implementations, the machine learning model 200 can be or include one or more support vector machines. Machine learning model 200 may be or include one or more example-based learning models, such as, for example, a learning vector quantization model; a self-organizing map model; a locally weighted learning model, etc. In some implementations, the machine learning model 200 can be or include one or more nearest neighbor models, such as, for example, a k-nearest neighbor classification model; k-nearest neighbor regression models, etc. Machine learning model 200 can be or include one or more bayesian models, such as, for example, a naive bayesian model; a gaussian naive bayes model; a polynomial na iotave bayes model; averaging a correlation estimate; a bayesian network; a bayesian belief network; hidden markov models, etc.
In some implementations, the machine learning model 200 can be or include one or more artificial neural networks (also simply referred to as neural networks). A neural network can include a set of connected nodes, which can also be referred to as neurons or perceptrons. The neural network can be organized into one or more layers. A neural network comprising multiple layers can be referred to as a "deep" network. The depth network can include an input layer, an output layer, and one or more hidden layers located between the input layer and the output layer. The nodes of the neural network can be connected or not fully connected.
The machine learning model 200 can be or include one or more feedforward neural networks. In a feed forward network, the connections between nodes do not form loops. For example, each connection can connect a node of an earlier tier to a node of a later tier.
In some examples, the machine learning model 200 can be or include one or more recurrent neural networks. In some examples, at least some nodes of the recurrent neural network are capable of forming loops. The recurrent neural network can be particularly useful for processing input data that is continuous in nature. In particular, in some examples, the recurrent neural network is capable of passing or retaining information from a previous portion of the input data sequence to a subsequent portion of the input data sequence using a recurrent or directed recurrent node connection.
In some examples, the sequential input data can include time series data (e.g., images of sensor data captured at (versus) times or at different times). For example, the recurrent neural network can analyze sensor data over time to detect or predict slip direction, perform handwriting recognition, and the like. The sequential input data may include words in sentences (e.g., for natural language processing, speech detection or processing, etc.); notes in a musical composition; continuous actions taken by the user (e.g., detecting or predicting sequential application usage); sequential object states, etc.
Example recurrent neural networks include Long Short Term (LSTM) recurrent neural networks; a gate control circulation unit; a bi-directional recurrent neural network; a continuous time cyclic neural network; a nerve history compressor; an echo state network; an elman network; a jordan network; a recurrent neural network; a hopfield network; a complete cycle network; sequence-to-sequence configuration, etc.
In some implementations, the machine learning model 200 can be or include one or more convolutional neural networks. In some examples, the convolutional neural network can include one or more convolutional layers that convolve the input data with a learned filter.
Filters can also be referred to as cores. Convolutional neural networks can be particularly useful for vision problems, such as when the input data includes images such as still images or video. However, convolutional neural networks can also be applied to natural language processing.
In some examples, machine learning model 200 can be or include one or more generation networks, such as, for example, a generation countermeasure network. The generation network can be used to generate new data, such as new images or other content.
The machine learning model 200 may be or include an automatic encoder. In some examples, the purpose of an auto-encoder is to learn a representation of a set of data (e.g., low-dimensional encoding), typically for the purpose of dimension reduction. For example, in some instances, an automatic encoder can seek to encode input data and provide output data that reconstruct the input data from the encoding. Recently, the automatic encoder concept has been more widely used for learning a model of data generation. In some examples, the auto encoder can include additional losses beyond reconstructing the input data.
The machine learning model 200 may be or include one or more other forms of artificial neural networks, such as, for example, deep boltzmann machines; a deep belief network; stacked automatic encoders, etc. Any of the neural networks described herein can be combined (e.g., stacked) to form a more complex network.
One or more neural networks can be used to provide the embedding based on the input data. For example, the embedding can be a representation of knowledge from the input data abstraction to one or more learning dimensions. In some instances, embedding can be a useful source of identifying related entities. In some instances, the embeddings can be extracted from the output of the network, while in other instances, the embeddings can be extracted from any hidden node or layer of the network (e.g., near the final but not the final layer of the network). Embedding can be useful for making auto-suggestion next video, product suggestion, entity or object recognition, and the like. In some instances, inputs useful for downstream models are embedded. For example, embedding can be used to summarize input data (e.g., search queries) of a downstream model or processing system.
The machine learning model 200 may include one or more cluster models, such as, for example, a k-means cluster model; k-median cluster model; the maximization model is expected; hierarchical clustering models, and the like.
In some implementations, the machine learning model 200 is capable of performing one or more dimension reduction techniques, such as principal component analysis, for example; analyzing the principal components of the core; graph-based kernel principal component analysis; principal component regression; partial least squares regression; sammon mapping; multidimensional scaling; projection pursuit; linear discriminant analysis; mixing and discriminant analysis; secondary discriminant analysis; generalized discriminant analysis; flexible discriminant analysis; automatic coding, etc.
In some implementations, the machine learning model 200 can perform or be subjected to one or more reinforcement learning techniques, such as a markov decision process; dynamic planning is carried out; q function or Q learning; a value function method; a deep Q network; a microcomputer; asynchronous dominant actor-commentator; decision-making policy gradients, etc.
In some implementations, the machine learning model 200 can be an autoregressive model. In some instances, the autoregressive model can specify that the output data depends linearly on its own previous values and random terms. In some examples, the autoregressive model can take the form of a random differential equation. One example of an autoregressive model is WaveNet, which is a model of the generation of raw audio.
In some implementations, the machine learning model 200 can include or form part of a multimodal integration. As an example, self-service (bootstrapping) aggregation can be performed, which can also be referred to as "bagging". In self-service aggregation, the training data set is divided into subsets (e.g., by having alternative random sampling) and multiple models are trained on the subsets, respectively. In inference, the respective outputs of the multiple models can be combined (e.g., by averaging, voting, or other techniques) and used as integrated outputs.
One example integration is a random forest, which can also be referred to as a random decision forest. Random forests are an integrated learning method for classification, regression and other tasks. Random forests are generated by generating multiple decision trees during training. In some examples, the average predictions (regression) of the class or individual trees as class (classification) patterns can be used as output of the forest at the time of inference. Random decision forests can correct the tendency of decision trees to overfit the training set.
Another example of an integration technique is stacking (stacking), which in some instances can be referred to as stack generalization. Stacking includes training a combiner model to mix or otherwise combine predictions of several other machine learning models. Thus, multiple machine learning models (e.g., of the same or different types) can be trained based on the training data. In addition, the combiner model can be trained to take predictions from other machine learning models as input and in response produce final inferences or predictions. In some examples, a single-layer logistic regression model can be used as the combiner model.
Another example integration technique is boosting. Boosting can include building an integration step by iteratively training weak models and then adding to the final strong model. For example, in some instances, each new model can be trained to emphasize training examples of prior model misunderstandings (e.g., misclassifications). For example, the weight associated with each such misinterpreted example can be increased. One common implementation of Boosting is AdaBoost, which can also be referred to as adaptive Boosting. Other example boosting techniques include LPBoost, totalBoost, brownBoost, xgboost, madaBoost, logitBoost, gradient boosting, and the like. Furthermore, any of the models described above (e.g., regression models and artificial neural networks) can be combined to form an integration. As an example, the integration can include a top level machine learning model or heuristic function to combine and/or weight the outputs that form the integrated model.
In some implementations, multiple machine learning models (e.g., forming an integrated model) can be linked and co-trained (e.g., by sequentially back-propagating errors in the model integration). However, in some embodiments, only a subset (e.g., one) of the jointly trained models is used for inference.
In some implementations, the machine learning model 200 can be used to pre-process input data for subsequent input into another model. For example, the machine learning model 200 can be capable of dimension reduction techniques and embedding (e.g., matrix decomposition, principal component analysis, singular value decomposition, word2vec/GLOVE, and/or related methods); clustering; and even categorize and regress against downstream consumption. Many of which have been discussed above and will be discussed further below.
As described above, the machine learning model 200 can be trained or otherwise configured to receive input data and provide output data in response. The input data can include different types, forms, or variants of the input data. As an example, in various embodiments, the input data can include features describing the content (or a portion of the content) originally selected by the user, e.g., the content of the document or image selected by the user, links to the user selections, links within the user selections related to other files available on the device or cloud, user-selected metadata, etc. Additionally, in the case of user permissions, the input data includes user usage context obtained from the application itself or other sources. Examples of usage contexts include the breadth of sharing (public sharing, or sharing with a large group, or private sharing, or sharing by a particular person), sharing contexts, and so forth. When allowed by the user, the additional input data can include the status of the device, e.g., the location of the device, the applications running on the device, etc.
In some implementations, the machine learning model 200 is capable of receiving and using input data in raw form. In some embodiments, the raw input data can be pre-processed. Thus, the machine learning model 200 is able to receive and use pre-processed input data in addition to or in place of the original input data.
In some implementations, preprocessing the input data can include extracting one or more additional features from the original input data. For example, feature extraction techniques can be applied to the input data to generate one or more new additional features. Example feature extraction techniques include edge detection; detecting corner points; spot detection; detecting the ridge; the scale-invariant feature transformation; detecting motion; optical flow; hough transform, and the like.
In some implementations, the extracted features can include or can be derived from transformations from the input data to other domains and/or dimensions. As an example, the extracted features can include or be derived from a transformation of the input data into the frequency domain. For example, the input data can be wavelet transformed and/or fast fourier transformed to generate additional features.
In some implementations, the extracted features can include statistics calculated from the input data or some portion or dimension of the input data. Example statistics include a mode, average, maximum, minimum, or other metric of the input data or portion thereof.
In some embodiments, as described above, the input data can be continuous in nature. In some instances, sequential input data can be generated by sampling or otherwise segmenting an input data stream. As one example, frames can be extracted from a video. In some embodiments, sequential data can be made non-sequential by summarizing.
As another example preprocessing technique, a portion of the input data can be interpolated (inpute). For example, additional synthetic input data can be generated by interpolation and/or extrapolation.
As another example preprocessing technique, some or all of the input data can be scaled, normalized, generalized, and/or regularized. Example regularization techniques include ridge regression; a Least Absolute Shrinkage and Selection Operator (LASSO); an elastic net; minimum angle regression; cross-validation; regularization of L1; l2 regularization, etc. As one example, some or all of the input data can be normalized by subtracting the average of the eigenvalues across a given dimension from each individual eigenvalue and then dividing by the standard deviation or other metric.
As another example preprocessing technique, some or all of the input data can be quantized or discretized. In some cases, qualitative features or variables included in the input data can be converted into quantitative features or variables. For example, one-hot encoding can be performed.
In some examples, dimension reduction techniques can be applied to the input data prior to input to the machine learning model 200. Several examples of dimension reduction techniques are provided above, including, for example, principal component analysis; analyzing the principal components of the core; graph-based kernel principal component analysis; principal component regression; partial least squares regression; sammon mapping; multidimensional scaling; projection pursuit; linear discriminant analysis; mixing and discriminant analysis; secondary discriminant analysis; generalized discriminant analysis; flexible discriminant analysis; automatic coding, etc.
In some implementations, during training, input data can be intentionally deformed in a variety of ways to improve model robustness, generalization, or other quality. Example techniques to distort the input data include adding noise; changing color, shading (shade) or hue; amplifying; dividing; amplification (amplification), and the like.
In response to receipt of the input data, machine learning model 200 can provide output data. The output data can include different types, forms, or variants of the output data. As an example, in various embodiments, the output data can include content stored locally on the user device or in the cloud, which can be relatedly shared with the initial content selection.
As discussed above, in some embodiments, the output data can include various types of classification data (e.g., binary classification, multi-class classification, single-label, multi-label, discrete classification, regression classification, probability classification, etc.) or can include various types of regression data (e.g., linear regression, polynomial regression, nonlinear regression, simple regression, multiple regression, etc.). In other examples, the output data can include cluster data, anomaly detection data, recommendation data, or any other form of output data described above.
In some implementations, the output data can affect downstream processes or decisions. As one example, in some implementations, the output data can be interpreted and/or acted upon by a rule-based regulator.
The present disclosure provides systems and methods that include or otherwise leverage one or more machine learning models to predict whether an application executing at a computing device is at risk of being terminated and/or available for secure amounts of memory allocated by the application executing at the computing device based on real-time values of one or more memory metrics of the computing device. Any of the different types or forms of input data described above can be combined with any of the different types or forms of machine learning models described above to provide any of the different types or forms of output data described above.
The systems and methods of the present disclosure can be implemented by or otherwise performed on one or more computing devices. Example computing devices include user computing devices (e.g., laptop computers, desktop computers, and mobile computing devices such as tablet computers, smart phones, wearable computing devices, etc.); embedded computing devices (e.g., devices embedded within a carrier, camera, image sensor, industrial machine, satellite, game console or controller or household appliance such as a refrigerator, thermostat, electricity meter, home energy manager, smart home assistant, etc.); server computing devices (e.g., database servers, parameter servers, file servers, mail servers, print servers, web servers, game servers, application servers, etc.); dedicated, specialized model processing or training equipment; a virtual computing device; other computing devices or computing infrastructures; or a combination thereof.
Fig. 2B illustrates a conceptual diagram of computing device 210, which computing device 210 is an example of computing device 100 of fig. 1. Computing device 210 includes processing component 202, memory component 204, and machine learning model 200. Computing device 210 may store and implement machine learning model 200 locally (i.e., on the device). Thus, in some implementations, the machine learning model 200 can be stored at and/or implemented locally by an embedded device or a user computing device such as a mobile device. Output data obtained by a local implementation of the machine learning model 200 at an embedded device or a user computing device can be used to improve performance of the embedded device or the user computing device (e.g., an application implemented by the embedded device or the user computing device).
FIG. 2C illustrates a conceptual diagram of an example client computing device capable of communicating with an example server computing system that includes a machine learning model over a network. Fig. 2C includes a client device 210A in communication with a server device 260 over a network 230. Client device 210A is an example of computing device 100 of fig. 1 and server device 260 is an example of a computing system that trains memory usage prediction model 132 of fig. 1 and transmits trained memory usage prediction model 132 to computing device 100. The server device 260 stores and implements a machine learning model 200, which machine learning model 200 may be an example of the memory usage prediction model 132 of fig. 1. In some instances, the output data obtained by the machine learning model 200 at the server device 260 can be used to improve other server tasks or can be used by other non-user devices to improve services performed by or for such other non-user devices. For example, the output data can improve other downstream processes performed by the server device 260 for the user's computing device or embedded computing device. In other examples, output data obtained by an implementation of the machine learning model 200 at the server device 260 can be sent to and used by a user computing device, an embedded computing device, or some other client device such as the client device 210A. For example, the server device 260 can be considered machine learning as a service.
In yet another embodiment, different respective portions of machine learning model 200 can be stored at and/or implemented by some combination of user computing devices, embedded computing devices, server computing devices, and the like. In other words, portions of machine learning model 200 may be distributed in whole or in part among client device 210A and server device 260.
Devices 210A and 260 may use one or more machine learning platforms, frameworks, and/or libraries, such as, for example, tensorFlow, caffe/Caffe2, theano, torch/PyTorch, MXnet, CNTK, etc., to perform graphics processing techniques or other machine learning techniques. Devices 210A and 260 may be distributed at different physical locations and connected via one or more networks including network 230. If configured as a distributed computing device, devices 210A and 260 may operate in accordance with a sequential computing architecture, a parallel computing architecture, or a combination thereof. In one example, the distributed computing devices can be controlled or self-service through the use of a parameter server.
In some implementations, multiple instances of machine learning model 200 can be parallelized to provide increased processing throughput. For example, multiple instances of machine learning model 200 can be parallelized on a single processing device or computing device or parallelized across multiple processing devices or computing devices.
Each computing device implementing machine learning model 200 or other aspects of the disclosure can include a plurality of hardware components that implement the performance of the techniques described herein. For example, each computing device can include one or more memory devices that store some or all of the machine learning model 200. For example, machine learning model 200 can be a structured numerical representation stored in memory. The one or more memory devices can also include instructions for implementing the machine learning model 200 or performing other operations. Example memory devices include RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, and the like, as well as combinations thereof.
Each computing device can also include one or more processing devices that implement some or all of the machine learning model 200 and/or perform other related operations. Example processing devices include one or more of the following: a Central Processing Unit (CPU); a Vision Processing Unit (VPU); a Graphics Processing Unit (GPU); tensor Processing Unit (TPU); a Neural Processing Unit (NPU); a neural processing engine; CPU, VPU, GPU, TPU, NPU or other processing device; an Application Specific Integrated Circuit (ASIC); a Field Programmable Gate Array (FPGA); a coprocessor; a controller; or a combination of the above processing devices. The processing device can be embedded within other hardware components, such as, for example, an image sensor, an accelerometer, etc.
Hardware components (e.g., memory devices and/or processing devices) can be distributed across multiple physically distributed computing devices and/or virtual distributed computing systems.
FIG. 2D illustrates a conceptual diagram of an example computing device in communication with an example training computing system that includes a model trainer. Fig. 2D includes a client device 210B in communication with a training device 270 over a network 230. Client device 210B is an example of computing device 100 of fig. 1. The machine learning model 200 described herein can be trained at a training computing system, such as the training device 270, and then provided for storage and/or implementation at one or more computing devices, such as the client device 210B. For example, model trainer 272 is executed locally at training device 270. However, in some examples, training device 270 including model trainer 272 can be included in or separate from client device 210B or any other computing device implementing machine learning model 200.
In some implementations, the machine learning model 200 may be trained in an offline manner or in an online manner. In offline training (also known as batch learning), the machine learning model 200 is trained on the entire static training dataset. In online learning, the machine learning model 200 is continuously trained (or retrained) as new training data becomes available (e.g., as the model is used to make inferences).
Model trainer 272 may perform a focused training of machine learning model 200 (e.g., based on a centrally stored dataset). In other implementations, decentralized training techniques, such as distributed training, joint learning, etc., can be used to train, update, or personalize the machine learning model 200.
The machine learning model 200 described herein can be trained according to one or more of a variety of different training types or techniques. For example, in some implementations, the machine learning model 200 can be trained by the model trainer 272 using supervised learning, where the machine learning model 200 is trained on a training dataset comprising instances or samples with labels. Tags can be provided by expert manual applications, by crowd-sourced generation, or by other techniques (e.g., by a physics-based or complex mathematical model). In some implementations, the training examples can be provided by the user computing device if the user has provided consent. In some embodiments, this process may be referred to as personalizing the model.
FIG. 2E shows a conceptual diagram of a training process 290, the training process 290 being an example training process in which a machine learning model 200 is trained on training data 291 that includes example input data 292 with labels 293. Training process 290 is an example training process; other training procedures may also be used.
When a user permits training using such data, training data 291 used by training process 290 can include historical values of memory metrics of the computing device over time. In some examples, training data 219 may include data generated by executing multiple applications on various computing devices to conduct stress testing. The different computing devices may include computing devices manufactured by different manufacturers, computing devices of different models manufactured by the same manufacturers, computing devices comprising different processors, computing devices having different amounts of memory (e.g., RAM) capable of being allocated by processors executing at the computing devices, computing devices running different versions of an operating system, computing devices executing different combinations of applications and/or processes, computing devices having different versions of firmware, and so forth.
Pressure testing may be performed on each computing device by executing multiple applications and/or processes on each computing device that may continue to allocate memory until the computing device terminates the application and/or process, such as due to a computing device memory overflow or a memory deficiency. The applications and/or processes may allocate different types of memory (e.g., native heap memory and/or graphics memory) as the applications and/or processes execute on the computing device during stress testing, and may monitor and collect values of memory metrics of the computing device as the applications and/or processes execute. Similarly, when an application and/or process is terminated by a low memory termination module, for example at a computing device, the value of the memory metric when the application and/or process is terminated may also be collected. Such pressure tests may be run thousands of times (e.g., over 2000 times).
Thus, the training data may include values of memory metrics collected by the computing device when an application and/or process is executing during a period of time when no application or process is terminated due to a low memory or memory overflow condition. The training data may also include values of memory metrics collected by the computing device when the application and/or process is executed at a point in time when the application and/or process is terminated due to a low memory or memory overflow condition. Examples of memory metrics may include any combination of the memory metrics listed above. In some implementations, training data 291 can include examples of input data 292 that have been assigned labels 293 corresponding to output data 294.
In some implementations, the machine learning model 200 can be trained by optimizing an objective function, such as the objective function 295. For example, in some implementations, the objective function 295 may be or include a loss function that compares (e.g., determines the difference between) output data generated by the model from the training data and a tag (e.g., a true value tag) associated with the training data. For example, the loss function can evaluate the sum of squared differences or average between the output data and the labels. In some examples, the objective function 295 may be or include a cost function describing the cost of a particular result or output data. Other examples of objective functions 295 can include interval-based (margin) techniques such as, for example, triplet loss or maximum interval training.
One or more of a variety of optimization techniques can be performed to optimize the objective function 295. For example, the optimization technique can minimize or maximize the objective function 295. Example optimization techniques include Hessian-based techniques and gradient-based techniques, such as coordinate descent, for example; gradient descent (e.g., random gradient descent); secondary gradient methods, and the like. Other optimization techniques include black box optimization techniques and heuristic techniques.
In some implementations, the back propagation of errors can be used in conjunction with optimization techniques (e.g., gradient-based techniques) to train the machine learning model 200 (e.g., when the machine learning model is a multi-layer model such as an artificial neural network). For example, an iterative loop of propagation and model parameter (e.g., weight) updates can be performed to train the machine learning model 200. Example back propagation techniques include truncated back propagation over time, levenberg-Marquardt back propagation, and the like.
In some implementations, the machine learning model 200 described herein can be trained using unsupervised learning techniques. Unsupervised learning can include inferring functions describing hidden structures from unlabeled data. For example, the classification or category may not be included in the data. Unsupervised learning techniques can be used to generate machine learning models that can be clustered, anomaly detected, learn latent variable models, or other tasks.
The machine learning model 200 can be trained using semi-supervised techniques that combine aspects of supervised learning and unsupervised learning. The machine learning model 200 can be trained or otherwise generated by evolutionary techniques or genetic algorithms. In some implementations, the machine learning model 200 described herein can be trained using reinforcement learning. In reinforcement learning, an agent (e.g., a model) may take an action in an environment and learn to maximize rewards caused by such action and/or minimize penalties caused by such action. Reinforcement learning differs from supervised learning in that reinforcement learning does not exhibit the correct input/output pair, nor does reinforcement learning explicitly correct sub-optimal actions.
In some implementations, one or more generalization techniques can be performed during training to improve generalization of the machine learning model 200. Generalization techniques can help reduce overfitting of machine learning model 200 to training data. Example generalization techniques include a backoff (dropout) technique; weight decay techniques; normalizing in batches; stopping in advance; selecting a subset; stepwise selection, etc.
In some implementations, the machine learning model 200 described herein can include or otherwise be affected by a plurality of hyper-parameters, such as, for example, a learning rate, a number of layers, a number of nodes in each layer, a number of leaves in a tree, a number of clusters, and so forth. Hyper-parameters can affect model performance. The hyper-parameters can be selected manually or can also be selected automatically by applying techniques such as grid searching; black box optimization techniques (e.g., bayesian optimization, random search, etc.); gradient-based optimization, etc. Example techniques and/or tools for performing automatic hyper-parametric optimization include Hyperopt; automatic WEKA; spearmint; metric Optimization Engine (MOE), etc.
In some implementations, various techniques can be used to optimize and/or adapt the learning rate when training the model. Example techniques and/or tools for performing learning rate optimization or adaptation include adagard; adaptive moment estimation (ADAM); adadelta; RMSprop, etc.
In some implementations, the transfer learning technique can be used to provide an initial model from which to begin training the machine learning model 200 described herein.
In some implementations, the machine learning model 200 described herein can be included in different portions of computer readable code on a computing device. In one example, the machine learning model 200 can be included in and used by a particular application or program (e.g., exclusively). Thus, in one example, a computing device can include multiple applications, and one or more of such applications can contain its own respective machine learning libraries and machine learning models.
In another example, the machine learning model 200 described herein can be included in an operating system of a computing device (e.g., in a central intelligent layer of the operating system) and can be invoked or otherwise used by one or more applications that interact with the operating system. In some implementations, each application can communicate with the central intelligence layer (and the model stored therein) using an Application Programming Interface (API) (e.g., a common public API across all applications).
In some implementations, the central intelligence layer can communicate with the central device data layer. The central device data layer can be a centralized data store for the computing devices. The central device data layer can communicate with a plurality of other components of the computing device, such as, for example, one or more sensors, a context manager, a device status component, and/or an add-on component. In some implementations, the central device data layer can communicate with each device component using an API (e.g., a proprietary API).
The technology discussed herein relates to servers, databases, software applications, and other computer-based systems, as well as actions taken and information sent to and from such systems. The flexibility inherent in computer-based systems allows for a wide variety of possible configurations, combinations, and divisions of tasks and functions among components. For example, the processes discussed herein can be implemented using a single device or component or multiple devices or components working in combination.
Databases and applications can be implemented on a single system or distributed across multiple systems. Distributed components can operate sequentially or in parallel.
Furthermore, the machine learning techniques described herein may be easily interchanged and combined. While certain example techniques have been described, many other techniques exist and can be used in connection with aspects of the disclosure.
The present disclosure has provided a brief overview of an example machine learning model and associated techniques. Regarding additional details, the reader should see the following references: machine learning probability perspective (Murphy); machine learning rules: ML engineering best practices (zinkavich); deep learning goodfullow); reinforcement learning: introduction (Sutton); and artificial intelligence: modern methods (Norvig).
In addition to the above description, a control may be provided to the user that allows the user to make a collection of information about whether and when the systems, programs, or features described herein may enable the user (e.g., information about the user's social network, social actions or activities, profession, user preferences, or the user's current location), and whether to send content or communications from the server to the user. In addition, some data may be processed in one or more ways prior to storage or use to remove personal identification information. For example, the identity of a user may be processed such that personal identity information of the user cannot be determined, or the geographic location of the user may be summarized when location information is obtained (such as to a city, zip code, or state level) such that a specific location of the user cannot be determined. Thus, the user can control what information is collected about the user, how that information is used, and what information is provided to the user.
Fig. 3 is a flowchart illustrating example operations of a process 300 by a mobile computing device, such as computing device 100 (fig. 1), in accordance with one or more aspects of the present disclosure. For purposes of illustration only, the operations of FIG. 3 are described with reference to computing device 100 shown in FIG. 1.
As shown in fig. 3, the process 300 includes: a plurality of memory metrics of the computing device 100 are determined by an application 120 of a plurality of applications executing at one or more processors 102 of the computing device 100 (302). The process 300 further includes: information indicating a predicted amount of secure memory available for allocation by the application 120 is determined based at least in part on the plurality of memory metrics by the application 120 executing at the one or more processors 102 (304). The process 300 further includes: one or more characteristics of the application 120 executing at the one or more processors 102 are adjusted by the application 120 executing at the one or more processors 102 to adjust the amount of memory allocated by the application 120 based at least in part on the information indicative of the prediction of the amount of memory available to be allocated by the application 120 (306).
The present disclosure includes the following examples.
Example 1: a method comprising determining, by an application of a plurality of applications executing at one or more processors of a computing device, a plurality of memory metrics of the computing device; determining, by an application executing at one or more processors, information indicative of a predicted amount of secure memory available for allocation by the application based at least in part on a plurality of memory metrics; and adjusting, by an application executing at the one or more processors, one or more characteristics of the application executing at the one or more processors to adjust the amount of memory allocated by the application based at least in part on the information indicative of the predicted amount of secure memory available to be allocated by the application.
Example 2: the method of example 1, wherein the information indicative of the predicted amount of secure memory for allocation by the application comprises the predicted amount of secure memory available for allocation by the application without causing the computing device to enter a low memory state.
Example 3: the method of example 2, wherein adjusting one or more characteristics of the application executing at the one or more processors to adjust the amount of memory allocated by the application further comprises: determining, by the application, that a predicted amount of secure memory available for allocation by the application is below a specified threshold; and responsive to determining that the predicted amount of secure memory available for allocation by the application is below a specified threshold, adjusting, by the application, one or more characteristics of the application executing at the one or more processors to reduce the amount of memory allocated by the application.
Example 4: the method of example 1, wherein the information indicating the predicted amount of secure memory for allocation by the application comprises an indication that the computing device is in a low memory state, and wherein the application is at risk of being terminated when the computing device is in the low memory state.
Example 5: the method of example 1, wherein determining information indicative of the predicted amount of secure memory available for allocation by the application further comprises: information indicative of a predicted amount of secure memory available for allocation by an application is determined based at least in part on a plurality of memory metrics by a library executing at one or more processors as a process separate from the application.
Example 6: the method of example 5, wherein determining information indicative of the predicted amount of secure memory available for allocation by the application further comprises: a plurality of memory metrics are input into a memory usage prediction model comprising one or more neural networks by a library executing at one or more processors to generate an output of information indicative of a predicted amount of secure memory available for allocation by an application.
Example 7: the method of example 6, wherein the memory usage prediction model is trained using training data generated by: performing stress testing on a plurality of different computing devices and monitoring values of a plurality of memory metrics of the plurality of different computing devices at a point in time when the application is terminated by the plurality of different computing devices.
Example 8: the method of example 5, wherein determining information indicative of the predicted amount of secure memory available for allocation by the application further comprises: determining, by a library executing at one or more processors, an upper threshold for a plurality of memory metrics; and comparing, by a library executing at the one or more processors, the values of the plurality of memory metrics to an upper threshold value for each of the plurality of memory metrics to determine information indicative of a predicted amount of secure memory available for allocation by the application.
Example 9: the method of example 8, wherein determining the upper threshold for each of the plurality of memory metrics further comprises: determined by one or more processors: immediately before one or more applications executing at the one or more processors are terminated because the computing device is in a low memory state, each of the plurality of memory metrics reaches a highest value; and determining, by the one or more processors, an upper threshold of the plurality of memory metrics based at least in part on the highest value reached by each of the plurality of memory metrics.
Example 10: the method of example 9, wherein determining the upper threshold of the plurality of memory metrics based at least in part on the highest value reached by each of the plurality of memory metrics further comprises: the highest value reached by each of the plurality of memory metrics is input into one or more neural networks by one or more processors to determine an upper threshold of the plurality of memory metrics.
Example 11: a computing device includes a memory and at least one processor communicatively coupled to the memory and configured to: executing a plurality of applications; determining a plurality of memory metrics for a memory; determining information indicative of a predicted amount of secure memory available for allocation by the application based at least in part on the plurality of memory metrics; and adjusting one or more characteristics of an application executing at the at least one processor to adjust the amount of memory allocated by the application based at least in part on the information indicative of the predicted amount of secure memory available to be allocated by the application.
Example 12: the computing device of example 11, wherein the information indicating the predicted amount of secure memory for allocation by the application comprises the predicted amount of secure memory available for allocation by the application without causing the computing device to enter a low memory state.
Example 13: the computing device of example 12, wherein to adjust one or more characteristics of the application executing at the at least one processor to adjust an amount of memory allocated by the application, the at least one processor is further configured to: determining, by the application, that a predicted amount of secure memory available for allocation by the application is below a specified threshold; and in response to determining that the predicted amount of secure memory available for allocation by the application is below the specified threshold, adjusting, by the application, one or more characteristics of the application executing at the at least one processor to reduce the amount of memory allocated by the application.
Example 14: the computing device of example 11, wherein the information indicating the predicted amount of secure memory for allocation by the application comprises an indication that the computing device is in a low memory state.
Example 15: the computing device of example 11, wherein to determine information indicative of a predicted amount of secure memory available for allocation by the application, the at least one processor is further configured to: a library, which is a separate process from the application, is executed to determine information indicative of a predicted amount of secure memory available for allocation by the application based at least in part on the plurality of memory metrics.
Example 16: the computing device of example 15, wherein to execute the library to determine information indicative of a predicted amount of secure memory available for allocation by the application, the at least one processor is further configured to: the library is executed to input a plurality of memory metrics into a memory usage prediction model comprising one or more neural networks to generate an output of information indicative of a predicted amount of secure memory available for allocation by the application.
Example 17; the computing device of example 16, wherein the memory usage prediction model is trained using training data generated by: performing stress testing on a plurality of different computing devices and monitoring values of a plurality of memory metrics of the plurality of different computing devices at a point in time when the application is terminated by the plurality of different computing devices.
Example 18: the computing device of example 15, wherein to execute the library to determine information indicative of a predicted amount of secure memory available for allocation by the application, the at least one processor is further configured to: executing the library to determine upper threshold values for the plurality of memory metrics; and executing the library to compare the values of the plurality of memory metrics to an upper threshold value for each of the plurality of memory metrics to determine information indicative of a predicted amount of secure memory available for allocation by the application.
Example 19: the computing device of example 18, wherein to execute the library to determine the upper threshold for each of the plurality of memory metrics, the at least one processor is further configured to: determining a highest value reached by each of the plurality of memory metrics by one or more applications executing at the at least one processor immediately before being terminated because the computing device is in a low memory state; and determining an upper threshold of the plurality of memory metrics based at least in part on the highest value reached by each of the plurality of memory metrics.
Example 20: a computer-readable storage medium having instructions stored thereon that, when executed, cause one or more processors of a computing device to: determining a plurality of memory metrics for a memory of a computing device; determining information indicative of a predicted amount of secure memory available for allocation by the application based at least in part on the plurality of memory metrics; and adjusting one or more characteristics of an application executing at the one or more processors to adjust the amount of memory allocated by the application based at least in part on the information indicative of the predicted amount of secure memory available to be allocated by the application.
Example 21: a method comprising determining, by an application of a plurality of applications executing at one or more processors of a computing device, one or more memory metrics of the computing device; determining, by an application executing at one or more processors, information indicative of a prediction of an amount of secure memory available for allocation by the application based at least in part on one or more memory metrics; and adjusting, by an application executing at the one or more processors, one or more characteristics of the application executing at the one or more processors based at least in part on information indicative of a predicted amount of secure memory available for allocation by the application.
Example 22: the method of example 21, wherein the information indicating the amount of secure memory available for allocation by the application comprises the amount of memory available for allocation by the application without causing the computing device to enter a low memory state.
Example 23: the method of any of examples 21 and 22, wherein the information indicating the predicted amount of secure memory available for allocation by the application includes an indication that the computing device is in a low memory condition.
Example 24: the method of any of examples 21-23, wherein the information indicating a prediction of the amount of secure memory available for allocation by the application includes an indication that significant allocation of memory by the application should cease.
Example 25: the method of any of examples 21-24, wherein the information indicating the predicted amount of secure memory available for allocation by the application includes an indication that memory allocated by the application should be released.
Example 26: the method of any of examples 21-25, wherein the one or more memory metrics include at least one of one or more kernel-level memory statistics or one or more operating system-level memory statistics.
Example 27: the method of example 26, wherein determining one or more memory metrics further comprises: one or more operating system level memory metrics are received by one or more methods of an operating system level Application Programming Interface (API) invoked by an application executing at one or more processors.
Example 28: the method of any of examples 26 and 27, wherein the one or more metrics include a value indicative of a priority level associated with the application.
Example 29: the method of example 5, wherein the one or more memory metrics include information associated with whether a canary application executing at the one or more processors has been terminated.
Example 30: the method of any of examples 21-29, wherein determining information indicative of a prediction of an amount of secure memory available for allocation by an application further comprises: information indicative of a prediction of an amount of secure memory available for allocation by an application is determined by a library included in the application, the library executing at one or more processors as a separate process from the application, based at least in part on one or more memory metrics.
Example 31: the method of example 30, wherein determining a prediction of the amount of secure memory available for allocation by the application further comprises: one or more memory metrics are input by a library executing at one or more processors into a memory usage prediction model comprising one or more neural networks to generate an output indicative of information indicative of predictions of the amount of secure memory available for allocation by an application.
Example 32: the method of any of examples 21-31, wherein determining one or more memory metrics further comprises: one or more memory metrics of a computing device are determined in real-time by an application executing at one or more processors as the application executes at the one or more processors.
Example 33: the method of any of examples 21-32, wherein adjusting one or more characteristics of an application executing at one or more processors further comprises: the quality of graphics output by the application for display at the display device is adjusted by the application executing at the one or more processors.
Example 34: a computing device includes a memory and at least one processor communicatively coupled to the memory and configured to: executing a plurality of applications; determining, by one of a plurality of applications, one or more memory metrics of a memory; determining, by the application, information indicative of a prediction of an amount of secure memory available for allocation by the application based at least in part on the one or more memory metrics; and adjusting, by the application, one or more characteristics of the application executing at the one or more processors based at least in part on the information indicative of the prediction of the amount of secure memory available for allocation by the application.
Example 35: the computing device of example 34, wherein the at least one processor is configured to perform any of the methods of examples 22-33.
Example 36 is a computer-readable storage medium having instructions stored thereon that, when executed, cause one or more processors of a computing device to: determining, by an application of a plurality of applications executing at one or more processors of a computing device, one or more memory metrics of the computing device; determining, by an application executing at one or more processors, information indicative of a prediction of an amount of secure memory available for allocation by the application based at least in part on the one or more memory metrics; and adjusting, by an application executing at the one or more processors, one or more characteristics of the application executing at the one or more processors based at least in part on the information indicative of the prediction of the amount of secure memory available for allocation by the application.
Example 37: the computer-readable storage medium of example 16, wherein the instructions further cause the one or more processors to perform any of the methods of examples 22-33.
In one or more examples, the described functionality may be implemented in hardware, software, firmware, or any combination thereof. If implemented in software, the functions may be stored on or transmitted over as one or more instructions or code on a computer-readable medium, and executed by a hardware-based processing unit. The computer-readable medium may include a computer-readable storage medium corresponding to a tangible medium, such as a data storage medium, or include any medium that facilitates transfer of a computer program from one place to another, e.g., according to a communication protocol. In this manner, a computer-readable medium may generally correspond to (1) a tangible computer-readable storage medium-which is non-transitory-or (2) a communication medium such as a signal or carrier wave. Data storage media may be any available media that can be accessed by one or more computers or one or more processors to retrieve instructions, code and/or data structures for implementations of the technology described in this disclosure. The computer program product may include a computer-readable medium.
By way of example, and not limitation, such computer-readable storage media can comprise RAM, ROM, EEPROM, CD-ROM or other optical disk storage, magnetic disk storage or other magnetic storage devices, flash memory, or any other storage medium that can be used to store desired program code in the form of instructions or data structures and that can be accessed by a computer. Furthermore, any connection may be properly termed a computer-readable medium. For example, if the instructions are transmitted from a website, server, or other remote source using a coaxial cable, fiber optic cable, twisted pair, digital Subscriber Line (DSL), or wireless technologies such as infrared, radio, and microwave, then the coaxial cable, fiber optic cable, twisted pair, DSL, or wireless technologies such as infrared, radio, and microwave are included in the definition of medium. However, it should be understood that computer-readable storage media and data storage media do not include connections, carrier waves, signals, or other transitory media, but are instead directed to non-transitory, tangible storage media. Disk (Disk) and disc (Disk), as used herein, includes Compact Disc (CD), laser disc, optical disc, digital Versatile Disc (DVD), floppy Disk and blu-ray disc where disks usually reproduce data magnetically, while discs reproduce data optically with lasers. Combinations of the above should also be included within the scope of computer-readable media.
The instructions may be executed by one or more processors, such as one or more Digital Signal Processors (DSPs), general purpose microprocessors, application Specific Integrated Circuits (ASICs), field programmable logic arrays (FPGAs), or other equivalent integrated or discrete logic circuitry. Thus, the term "processor" as used herein may refer to any of the foregoing structure or any other structure suitable for implementation of the techniques described herein. Additionally, in some aspects, the functionality described herein may be provided within dedicated hardware and/or software modules. Furthermore, the techniques may be implemented entirely in one or more circuits or logic elements.
The techniques of this disclosure may be implemented in a variety of devices or apparatuses including a wireless handset, an Integrated Circuit (IC), or a set of ICs (e.g., a chipset). Various components, modules, or units are described in this disclosure to emphasize functional aspects of devices configured to perform the disclosed techniques but do not necessarily require realization by different hardware units. Rather, as noted above, the various units may be combined in hardware units or provided by a collection of intraoperative hardware units, including one or more processors as described above and appropriate software and/or firmware.
It should be appreciated that, according to an embodiment, certain acts or events of any of the methods described herein can be performed in a different order, may be added, combined, or omitted entirely (e.g., not all of the described acts or events are necessary for the practice of the method). Further, in some embodiments, acts or events may be performed concurrently, e.g., by multi-threaded processing, interrupt processing, or multiple processors, rather than sequentially.
In some examples, the computer-readable storage medium includes a non-transitory medium. The term "non-transitory" indicates that the storage medium is not embodied in a carrier wave or propagated signal. In some examples, a non-transitory storage medium may store data (e.g., in RAM or cache) that can change over time.
Various examples have been described. These and other examples are within the scope of the following claims.
Claims (12)
1. A method, comprising:
determining, by one of a plurality of applications executing at one or more processors of a computing device, a plurality of memory metrics of the computing device;
determining, by the application executing at the one or more processors, information indicative of a predicted amount of secure memory available for allocation by the application based at least in part on the plurality of memory metrics; and
One or more characteristics of the application executing at the one or more processors are adjusted to adjust the amount of memory allocated by the application based at least in part on the information indicative of the predicted amount of secure memory available for allocation by the application.
2. The method of claim 1, wherein the information indicating the predicted amount of secure memory available for allocation by the application comprises the predicted amount of secure memory available for allocation by the application without causing the computing device to enter a low memory state.
3. The method of any of claims 1 and 2, wherein adjusting the one or more characteristics of the application executing at the one or more processors to adjust the amount of memory allocated by the application further comprises:
determining, by the application, that the predicted amount of secure memory available for allocation by the application is below a specified threshold; and
in response to determining that the predicted amount of secure memory available for allocation by the application is below the specified threshold, the one or more characteristics of the application executing at the one or more processors are adjusted by the application to reduce the amount of memory allocated by the application.
4. The method of claim 1, wherein the information indicating the predicted amount of secure memory for allocation by the application comprises an indication that the computing device is in a low memory state, and wherein the application is at risk of being terminated when the computing device is in the low memory state.
5. The method of any of claims 1-4, wherein determining the information indicative of the predicted amount of secure memory available for allocation by the application further comprises:
the information indicative of the predicted amount of secure memory available for allocation by the application is determined based at least in part on the plurality of memory metrics by a library executing at the one or more processors as a separate process from the application.
6. The method of claim 5, wherein determining the information indicative of the predicted amount of secure memory available for allocation by the application further comprises:
the plurality of memory metrics are input by the library executing at the one or more processors into a memory usage prediction model comprising one or more neural networks to generate an output of the information indicative of the predicted amount of secure memory available for allocation by the application.
7. The method of claim 6, wherein the memory usage prediction model is trained using training data generated by: performing a stress test on a plurality of different computing devices and monitoring values of the plurality of memory metrics of the plurality of different computing devices at a point in time when an application is terminated by the plurality of different computing devices.
8. The method of claim 5, wherein determining the information indicative of the predicted amount of secure memory available for allocation by the application further comprises:
determining, by the library executing at the one or more processors, an upper threshold for the plurality of memory metrics; and
the values of the plurality of memory metrics are compared by the library executing at the one or more processors to an upper threshold value of each of the plurality of memory metrics to determine the information indicative of the predicted amount of secure memory available for allocation by the application.
9. The method of claim 8, wherein determining an upper threshold for each memory metric of the plurality of memory metrics further comprises:
Determining, by the one or more processors: immediately before one or more applications executing at the one or more processors are terminated because the computing device is in a low memory state, each memory metric of the plurality of memory metrics reaches a highest value; and
an upper threshold for the plurality of memory metrics is determined by the one or more processors based at least in part on a highest value reached by each memory metric of the plurality of memory metrics.
10. The method of claim 9, wherein determining an upper threshold for the plurality of memory metrics based at least in part on a highest value reached by each memory metric of the plurality of memory metrics further comprises:
the highest value reached by each memory metric of the plurality of memory metrics is input into one or more neural networks by the one or more processors to determine an upper threshold of the plurality of memory metrics.
11. A computing device, comprising:
a memory storing instructions; and
one or more processors executing the instructions to perform the method of any of claims 1 to 10.
12. A computer-readable storage medium having instructions stored thereon that, when executed, cause one or more processors of a computing device to perform the method of any of claims 1-10.
Applications Claiming Priority (4)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US63/227,094 | 2021-07-29 | ||
US17/649,120 | 2022-01-27 | ||
US17/649,120 US20230036737A1 (en) | 2021-07-29 | 2022-01-27 | Determining available memory on a mobile platform |
PCT/US2022/072375 WO2023009905A1 (en) | 2021-07-29 | 2022-05-17 | Determining available memory on a mobile platform |
Publications (1)
Publication Number | Publication Date |
---|---|
CN117716344A true CN117716344A (en) | 2024-03-15 |
Family
ID=90155697
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202280049113.3A Pending CN117716344A (en) | 2021-07-29 | 2022-05-17 | Determining available memory on a mobile platform |
Country Status (1)
Country | Link |
---|---|
CN (1) | CN117716344A (en) |
-
2022
- 2022-05-17 CN CN202280049113.3A patent/CN117716344A/en active Pending
Similar Documents
Publication | Publication Date | Title |
---|---|---|
Angra et al. | Machine learning and its applications: A review | |
US11238211B2 (en) | Automatic hyperlinking of documents | |
Lin et al. | Particle swarm optimization for parameter determination and feature selection of support vector machines | |
WO2020005240A1 (en) | Adapting a sequence model for use in predicting future device interactions with a computing system | |
JP2023514282A (en) | Automated data analysis method, related system and apparatus for non-tabular data | |
AghaeiRad et al. | Improve credit scoring using transfer of learned knowledge from self-organizing map | |
CN112384938A (en) | Text prediction based on recipient's electronic messages | |
KR20190029083A (en) | Apparatus and Method for learning a neural network | |
US20220249906A1 (en) | On-device activity recognition | |
Nair et al. | Covariate shift: A review and analysis on classifiers | |
US20200234158A1 (en) | Determining feature impact within machine learning models using prototypes across analytical spaces | |
Shi et al. | FS-MGKC: Feature selection based on structural manifold learning with multi-granularity knowledge coordination | |
US11816185B1 (en) | Multi-view image analysis using neural networks | |
US20230122684A1 (en) | Systems and methods for automatically sourcing corpora of training and testing data samples for training and testing a machine learning model | |
US20230036737A1 (en) | Determining available memory on a mobile platform | |
CN117716344A (en) | Determining available memory on a mobile platform | |
Camastra et al. | Clustering methods | |
EP4338060A1 (en) | Determining available memory on a mobile platform | |
Fisch et al. | Towards automation of knowledge understanding: An approach for probabilistic generative classifiers | |
US20240152440A1 (en) | Game performance prediction across a device ecosystem | |
Chander et al. | Enhanced pelican optimization algorithm with ensemble-based anomaly detection in industrial internet of things environment | |
Iyigun et al. | Semi-supervised probabilistic distance clustering and the uncertainty of classification | |
Ji et al. | Fast and General Incomplete Multi-view Adaptive Clustering | |
Gu et al. | Active learning by extreme learning machine with considering exploration and exploitation simultaneously | |
Panda et al. | Ensemble methods for improving classifier performance |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |