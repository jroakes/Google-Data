CN114008594A - Scheduling operations on a computational graph - Google Patents
Scheduling operations on a computational graph Download PDFInfo
- Publication number
- CN114008594A CN114008594A CN202080044760.6A CN202080044760A CN114008594A CN 114008594 A CN114008594 A CN 114008594A CN 202080044760 A CN202080044760 A CN 202080044760A CN 114008594 A CN114008594 A CN 114008594A
- Authority
- CN
- China
- Prior art keywords
- node
- computational graph
- schedule
- nodes
- graph
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/46—Multiprogramming arrangements
- G06F9/48—Program initiating; Program switching, e.g. by interrupt
- G06F9/4806—Task transfer initiation or dispatching
- G06F9/4843—Task transfer initiation or dispatching by program, e.g. task dispatcher, supervisor, operating system
- G06F9/4881—Scheduling strategies for dispatcher, e.g. round robin, multi-level priority queues
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/901—Indexing; Data structures therefor; Storage structures
- G06F16/9024—Graphs; Linked lists
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/10—Interfaces, programming languages or software development kits, e.g. for simulating neural networks
- G06N3/105—Shells for specifying net layout
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
Abstract
Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for scheduling operations represented on a computational graph. One of the methods includes: receiving, by a computational graph system, a request to generate a schedule for processing a computational graph, obtaining data representing the computational graph, and generating a separator of the computational graph; and generating a schedule to perform the operations represented in the computational graph, wherein generating the schedule comprises: initializing scheduling by utilizing a zero node; for each node in the separator: determining whether a node has any predecessor nodes in the computational graph, adding the predecessor nodes to the schedule when the node has any predecessor nodes, and adding the node to the schedule, and adding each node in each sub-graph that is neither in the separator nor a predecessor of any node in the separator on the computational graph to the schedule.
Description
Cross Reference to Related Applications
This application claims priority from U.S. patent application No.62/875,433 filed on 7/17/2019, the entire contents of which are incorporated herein by reference.
Technical Field
This description relates to optimizing peak memory usage when executing a computational graph.
Background
Neural networks are machine learning models that employ one or more layers of nonlinear units to predict the output of received inputs. In addition to the output layer, some neural networks include one or more hidden layers. The output of each hidden layer is used as an input to the next layer in the network, i.e. the next hidden layer or output layer. Each layer of the network generates an output from the received input in accordance with the current values of the respective set of parameters.
To allow neural networks to perform better for various tasks, such as speech, and visual processing tasks, neural networks are designed with larger sized layers, larger numbers of layers, and/or increasingly complex and intricate data correlations. However, large and complex neural networks are difficult to train due to the computational cost and the high memory capacity required. To facilitate computational speed, specialized hardware such as GPUs and AI accelerators are typically employed in training complex neural network models. In more and more recent neural networks, when training these large models, the peak memory capacity of the hardware does not reach the peak demand for memory, thus turning memory capacity into a bottleneck for training.
Disclosure of Invention
This specification generally describes techniques for scheduling operations of a computational graph for execution by one or more devices to optimize peak memory usage.
According to one aspect, a method of scheduling operations of a computational graph includes: the method includes receiving, by a computational graph system, a request to generate a schedule for processing a computational graph, obtaining data representing the computational graph, generating a separator of the computational graph, and generating the schedule to perform an operation represented in the computational graph. The schedule defines a sequence of operations represented by the computational graph that are executed according to an ordering. The computational graph includes a plurality of nodes and directed edges, where each node represents a respective operation. Each directed edge from a respective first node to a respective second node represents a second operation represented by the respective second node requiring as input at least one output generated by performing the first operation represented by the respective first node. The separator satisfies the property of removing nodes in the separator from the computational graph and connecting other nodes to edges of the separator such that remaining nodes and edges of the computational graph form a plurality of connected components. Generating the schedule includes: (1) initializing scheduling by utilizing a zero node; (2) for each node in the separator, determining whether the node has any predecessor nodes in the computational graph, adding a predecessor node to the schedule when the node has any predecessor nodes, and adding the node in the schedule, and (3) adding each node in each sub-graph that is neither in the separator nor a predecessor of any node in the separator on the computational graph to the schedule.
The subject matter described in this specification can be implemented in particular embodiments to realize one or more of the following advantages.
The described system can be used to optimize memory usage when computing directed or undirected acyclic graphs, i.e., when performing operations represented by the graphs on one or more computing devices. For example, the described techniques can be used to optimize memory usage when computing graphs for training large and complex neural networks. To optimize memory usage when computing a graph, the system generates a schedule that specifies a sequence of operations represented in the graph in execution order. The system-generated schedule is efficient and reduces peak memory usage of the computational graph by utilizing rematerialization, such that only necessary operational inputs and outputs are maintained in memory at any given time during execution. Some embodiments can be used to meet peak memory requirements when computing directed or undirected acyclic graphs to a given memory size, such as memory available on particular hardware.
When generating the schedule, the system simply rearranges the order in which each operation is performed, deciding which operation to keep in memory, and if so, when and for how long to keep the operation. Because none of the operations represented in the graph have been modified, this technique of scheduling operations avoids the risk of reducing the accuracy of the final output of the computational graph when using other techniques that reduce memory usage, such as reusing memory regions and communication between the CPU and GPU memories.
Further, the schedule can be generated in a faster clock time than conventional schedule generation techniques. The system can generate a schedule based on graph characteristics of the computational graph, including a tree width of a tree decomposition output of the computational graph. The system can perform computational graphs according to the schedule while utilizing other graph characteristics to further reduce memory costs.
The details of one or more embodiments of the subject matter of this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Drawings
FIG. 1 illustrates an example schedule of an example directed acyclic computational graph.
FIG. 2 is a schematic diagram of an example operational scheduling system.
FIG. 3A is an example of a directed acyclic computational graph.
FIG. 3B is an exemplary non-directional counterpart of the computational graph of FIG. 3A.
FIG. 3C is an example tree decomposition of the non-directed counterpart in FIG. 3B.
FIG. 4 is a flow diagram of an example process for scheduling operations of a computational graph based on tree decomposition.
FIG. 5 is a flow diagram of an example process for operating a scheduling system to schedule operations of a computational graph.
FIG. 6 illustrates an example process of generating a schedule to perform operations in a computational graph based on path decomposition.
FIG. 7 illustrates an example process of modifying a computational graph to define a long spine (long spine).
FIG. 8 illustrates an example process for scheduling operations of a computational graph based on path decomposition.
Detailed Description
This specification describes a system implemented as a computer program on one or more computers in one or more locations that schedules operations for processing an input computation graph to reduce peak memory requirements for performing the operations. To reduce peak memory usage, the system can receive an input computation graph representing an operation and an input dependency of the operation on other operations in the input computation graph. The system can then generate a schedule representing the sequence of operations for execution. The system can generate schedules by identifying where intermediate inputs between operations can be rematerialized to reduce or completely eliminate the need to store the intermediate inputs in memory until they are needed.
A directed acyclic computational graph can represent operations for processing inputs through a neural network. The neural network can be configured to receive any kind of digital data input as a network input and generate any kind of network output based on the network input, i.e., any kind of score, classification, or regression output. For example, in image classification, if the input to the neural network is an image or a feature that has been extracted from an image, the output generated by the neural network for a given image may be a score for each of a set of object classes, where each score represents an estimated likelihood that the image contains an image of an object belonging to that class. Neural networks process inputs by performing a number of operations to generate network outputs. Each operation can receive zero or more inputs and can generate an output. Examples of operations that can be represented in a graph representing a neural network include activation functions, e.g., taking intermediate variables based on inputs from a previous layer and generating a non-linear mapping for outputs of a next layer, forward gradient propagation operations, e.g., operations related to calculating and storing intermediate variables of each layer and outputs for the neural network in order from a first input layer to an output layer, backward gradient propagation operations, e.g., operations related to calculating and storing gradients of a loss function with respect to parameters of each layer in the neural network from the output layer to the first input layer, and optimization operations, e.g., operations used by an optimizer, such as (random) gradient descent, momentum, RMSProp, and Adam, to assist in updating parameters of the neural network based on the calculated gradients of the loss function. The input and output for any given operation can be, for example, scalar values, class values, vectors, matrices, or higher order tensors.
FIG. 1 illustrates scheduling of an example directed acyclic computational graph 100. As shown in FIG. 1, the plurality of operations can be represented as a directed acyclic computational graph 100 having a plurality of nodes (103, 105, 107, 109, and 111) and edges (135, 157, 179, 191, 131, and 159). Each node of the computational graph represents a respective operation of a plurality of operations. Each node consumes a set of inputs from its incoming edge(s), performs its corresponding operation on the inputs, and outputs the operation results to any node connected to that node by an outgoing edge. For example, node 105 takes as input the output operated on by node 103 entering edge 135, performs its corresponding operation, and sends the output via two exiting edges 157 and 159 as inputs to nodes 107 and 109, respectively. Typically, both the inputs and outputs of the computation graph should be stored in memory throughout the computation, and the order of the computation nodes, i.e., the schedule, will determine the peak memory usage. As shown in FIG. 1, when storing the output of each node to occupy one memory cell, computing the nodes in the order of 103, 105, 107, 109, 111 would require four memory cells, while computing them in the order of 103, 105, 107, 109, 103, 111 would require only three memory cells. The latter order here involves rematerializing the output of node 103, i.e. regenerating the output of node 103 before it is needed as input to another node, rather than always keeping it in memory. The combination of ordering the execution of operations and possibly rematerializing some intermediate nodes is referred to herein as scheduling.
FIG. 2 illustrates an example operating scheduling system 200 that includes a tree decomposition engine 215 and a scheduling engine 220.
The operational scheduling system 200 is implemented as a computer program on one or more computers in one or more locations that implement the systems, components, and techniques described below.
The operation scheduling system 200 can employ as input a computational graph 205, e.g., a directed acyclic graph. The computational graph 205 can be directed acyclic.
The tree decomposition engine 215 takes input into the computational graph 205 and performs a tree decomposition process to generate (1) multiple sets of tree decomposition outputs 230, each of the multiple sets including one or more nodes of the computational graph 205, and (2) directed paths connecting each of the multiple sets. In short, the set of nodes in (1) can be referred to as a "packet" and the directed path in (2) can be referred to as a "tree". It is noted here that the computation graph can have a number of different possible tree decompositions by applying one or more decomposition processes to each computation graph.
The schedule engine 220 takes as input the raw computation graph 205 and the tree decomposition output 230 and outputs the schedule 210 for the computation graph 205.
More specifically, the directed acyclic computational graph may be represented by G ═ (V, E), where V is the set of nodes that includes all the nodes of the computational graph G, and E is the set of edges that includes all the directed edges of the computational graph G. For any two nodes u and V in the set of nodes V, a directed edge represents the data dependency of (u, V) from u to V. The data dependency from u to v means that the operation represented by node u generates an output that is input to the operation represented by node v. Therefore, the node u operation must be performed before the node v operation is performed.
In addition, one or some of the nodes are designated as a final node f set. When the computation of the computation graph is finished, the nodes in f need to be held in memory, for example, because the output of the nodes in f is the output of the machine learning model represented by the computation graph. In this specification we assume that these imaginary nodes are out of zero, i.e. the output of each node in the set of nodes f will not be used by any other node in the same graph.
The schedule for computing the graph is a sequence of nodes belonging to V, which has the following characteristics: (1) the final set of nodes f is represented in the schedule, and (2) each node in the schedule only occurs after all its predecessors have been added to the schedule at least once. For a directed edge (u, v), the predecessor of node v is node u connected by the outgoing path from u to v in the computational graph. The second requirement prevents the schedule from including a node in the schedule before scheduling all other nodes in the computational graph that have data dependencies with the node. If a node's predecessor nodes have its own predecessor or predecessors, those predecessor nodes appear before the predecessor node in the schedule, and so on, until a node without predecessors is reached, e.g., the input node that receives input to the computational graph.
In addition, a node can appear in the schedule more than once. As described below, the system can generate a schedule that can add the same node to the schedule more than once, thereby executing the computational graph in exchange for additional computational time in exchange for reduced memory requirements.
Scheduling naturally implies time and memory boundaries for calculating the graph G. The length of a node includes the time required to perform the corresponding operation of that node, and the length of the schedule is the sum of the lengths of each node in the schedule of the computational graph G. Therefore, the lower limit of the length of the schedule is the time required to perform each operation of the computation graph G at a time.
The scheduled peak memory usage is defined by the maximum memory required at any given time step when executing the graph according to the schedule. The maximum memory required at a time step is defined by summing the magnitude of the tensor outputs of all the immediate predecessors of the node at that time step. Thus, the set of tensors that need to be kept in memory at a time step depends on the current node, each predecessor of the current node, and all predecessors of each predecessor of the current node.
The computational graph G can be decomposed into (1) multiple sets, each set including one or more nodes (packets) of the computational graph and (2) a path (tree) connecting each of the multiple sets. Each package is a subset of the nodes in the computational graph, and the tree is a tree structure that connects all the packages. A tree is a non-directed acyclic computational graph in which any two nodes are connectable by exactly one edge, or equivalently, each node has at most one nearest predecessor. This decomposition, called tree decomposition, satisfies the following three characteristics: (1) each node in the original computational graph G is in at least one of the packets, (2) for each directed edge (u, v) in the original computational graph, two nodes of the edge appear together in at least one packet, and (3) for each node in the original computational graph, packets containing the same node form a connected subgraph of the tree. In other words, a node can be assigned to more than one packet, and all packets containing a given node must form a connected subgraph of the tree. Each connected subgraph can be viewed as a subtree.
As shown in FIG. 3A, the directed acyclic computation graph 301 has nodes A through J, and the edge 313 is directed because node I is the predecessor of node J (311). In some implementations, prior to performing tree decomposition, the system transforms the directed computational graph into a non-directed computational graph by removing the direction of edges in the computational graph 301. As shown in fig. 3B, each edge of the non-directed computation graph 303 based on the computation graph 301 has no direction. FIG. 3C illustrates an example tree decomposition output 305 of the non-directed computational graph 303, where the tree decomposition output is (1) a plurality of packets and (2) a tree that connects the packets. The tree split output 305 satisfies the 3 characteristics of tree splitting:
(1) each node in the original non-directed computational graph 303 is in at least one packet. For example, node D is in packages 331, 333, and 335. For another example, node B is in packets 333, 335, and 337.
(2) For each edge in computational graph 303, the two end nodes of the edge are assigned to the same packet at least once in the tree decomposition. For example, edge 323 in computational graph 303 has two end nodes, A and B, which are now together in package 335 in tree split output 305. As another example, edge 325 in computational graph 303 connects two end nodes C and D, so in tree split output 305, both nodes C and D appear in both packet 333 and packet 341.
(3) For any node assigned to more than one packet, all packets containing that node can form a sub-tree. For example, the packets 333, 335, and 337 contain node B at the same time, and the packets 333, 335, and 337 and the edges connecting the packets can together form a connected component subgraph of the tree decomposition output 305, as highlighted by the dashed lasso in FIG. 3C.
The tree width (tree width) tw (G) of the computation graph G is the minimum width of all tree split outputs, and the width of the tree split outputs is defined as the maximum packet size minus 1, where the packet size of a given packet is based on the number of nodes in the packet. For example, the width of tree split output 305 is 1. Assume that there are only two possible tree decomposition outputs from the non-directed computational graph 303, one with width 2 and the other with width 5. Then, of the three tree decomposition outputs, the minimum width is 1, i.e., the tree width tw (g) of the computation graph 303 is 1.
In some embodiments, the system maintains a width threshold, i.e., a predetermined threshold, such that the width of any tree decomposition output of the computation graph G is less than the width threshold. In other words, the system does not select any tree split outputs unless the tree has a width less than the width of the tree.
The separator is a collection of nodes: when removed from the computational graph with the nodes in the separator and edges linked to each node in the separator removed, the separator separates the remaining nodes and edges of the computational graph, i.e., nodes not in the separator and edges that connect only nodes not in the separator, into a plurality of connected component subgraphs that allow the remaining nodes and edges to form the computational graph. Each connected component subgraph comprises a plurality of nodes connected to each other by edges, but no connected component subgraph is connected to any other connected component subgraph.
The divider for tree splitting is a package: when removed from the tree structure in such a way that nodes in the separator and edges connecting nodes from other packets to nodes in the separator are removed, a selected packet from all packets in the tree decomposition of the plurality of connected sub-trees is formed from the remaining packets and edges.
The balanced divider for tree splitting is a divider where the remaining packets and edges form a plurality of connected sub-trees when removed from the tree structure as described above, where the size of a sub-tree is at most half the size of the original tree of the tree splitting.
In some implementations, the system can generate partitions that connect component subgraphs, where each component subgraph is much smaller in size than the original graph.
For example, the system can do this by generating a balanced separator for the tree decomposition of the computation graph and then removing the balanced separator from the tree decomposition.
The system can then efficiently generate a schedule for computing the graph by recursively generating a schedule for each connected component subgraph produced by the partition. The recursion may be repeated until a threshold condition is met. The threshold condition may be, for example, that a peak memory requirement for executing the directed or undirected acyclic graph falls below a threshold corresponding to available memory for executing the directed or undirected acyclic graph on particular hardware.
In some embodiments, this processing can be performed in parallel for each connected component subgraph. Because of the nature of partitioning, each connected component subgraph is independent of each other, i.e., not connected together by any edge, so the system can generate schedules for each component subgraph independently without data dependency problems.
The system can then add each schedule obtained for each sub-graph to the effective final schedule.
A system or other device configured to execute the computational graph can then execute the computational graph according to the final schedule.
FIG. 4 illustrates a flow diagram of an example process 400 for scheduling operations of a computational graph based on tree decomposition. For convenience, process 400 will be described as being performed by a system of one or more computers located at one or more locations. For example, a suitably programmed operation scheduling system, such as operation scheduling system 200 of FIG. 2, can perform process 400.
To schedule operations using process 400, the system takes as input (a) an acyclic computational graph G and (b) a tree decomposition having (1) a plurality of packets and (2) a tree obtained from the computational graph G by a tree decomposition process. In some embodiments, the system takes as input (a) a subset of nodes of the computation graph G as a set of constraints, (b) a tree decomposition based on the set of constraints, (c) a subset of nodes as a set of constraints of the computation set, and (d) the computation graph G. That is, a device that performs the operations of the computational graph according to the schedule can use the annotations for any given node in the schedule to determine whether to store the output of the operation represented by the node in memory or to discard it once it has been consumed as input by another operation. In some implementations, adding annotations to a node can be performed by defining a scalar variable associated with the node that describes whether the output of the node is saved in memory.
As previously described, the system finds the balanced divider (410) of the computational graph G. The balanced separator enables the remaining packets and edges in the tree decomposition to form a set of connected component subgraphs (subtrees) after removal, where their size is at most half the size of the original tree of the tree decomposition.
The system then determines a connected component subgraph set after removing the balanced separator and all nodes associated with the separator from all packets (420). Packets containing any identical node from the computational graph G can form a connected component subgraph using the corresponding tree edges of the tree decomposition. Because of the way the separator is identified, if a node is in two or more connected component subgraphs, that node must also be in a balanced separator. Thus, it is guaranteed that the remaining nodes and edges can form a connected component subgraph set after the balanced separator is removed.
Next, the system initializes a dispatch list (430). Typically, the schedule list is initialized to an empty set.
Thereafter, the system finds all predecessors for each node in the separator and adds the node, optionally with a corresponding annotation, to the schedule (440). For example, the system can iterate over the nodes in the separator according to a random order or according to a fixed order when generating the schedule. Specifically, for a given node in the separator, the system finds all direct predecessors of that node and adds the direct predecessor with an annotation to the schedule, then the system finds all predecessors of that direct predecessor and adds all predecessors with an annotation to the schedule, and finally the system adds the node with an annotation to the schedule until all predecessors of that node are found and added to the schedule.
Further, the system finds all final nodes in each connected component subgraph and adds them to the schedule (450). All final nodes are nodes that are not in the separator and are not predecessors of any other nodes. Each final node will also optionally be added to the schedule along with the annotations.
Finally, the system optimizes the schedule length by freeing up unneeded balanced divider nodes (460). The unneeded balanced divider node is the node that was not added to the schedule but was originally included in the balanced divider by step 410.
In some embodiments, steps 440 and 450 can be invoked recursively based on a divide and conquer strategy. For example, when step 440 is first performed, some internal parameters, such as the limit set and the calculation set, are updated. The system then calls from step 410 again by taking the updated internal parameters as input and performs the same process. When the system recursively invokes the process, the updated internal parameters can affect the scope of the search predecessors and/or the tracking schedule. The system recursively stops invoking the process when certain criteria are met. For example, the criterion can be an undefined input parameter of the last recursive call. As another example, the criteria may include that a peak memory requirement for performing the scheduling falls below a threshold, such as available memory for performing the scheduling on particular hardware. Similarly, for step 450, the system recursively invokes and performs the same process again from step 410. Recursive calls ensure that the size of the tree decomposition is reduced at each recursive call, thereby ensuring efficient scheduling, and that each operation is performed at least once.
FIG. 5 further illustrates a flow diagram of an example process 500 for operating the scheduling system to schedule operations of the computational graph. Process 500 will be described as being performed by a system of one or more computers located at one or more locations. For example, a suitably programmed operation scheduling system, such as operation scheduling system 200 of FIG. 2, can perform process 500.
An operation scheduling system receives a request to generate a schedule for processing a computational graph (510), obtains data representing the computational graph (520), generates a divider of the computational graph (530), and then generates a schedule to perform an operation represented in the computational graph (540). To generate the schedule, the system first initializes the schedule of operations to be performed in the computational graph (540a), then, for each node in the separator, determines whether the node has any predecessor nodes, if any, adds the predecessor nodes to the schedule, and adds the node to the schedule (540b), and finally adds each node in each connected component subgraph that is neither in the separator nor a predecessor to any node in the separator on the computational graph to the schedule (540 c). Generating the divider in step 530 is described in more detail above in step 410 of fig. 4. Adding the predecessor of nodes from the separator to the schedule in step 540b is described in more detail above in step 440 of fig. 4. Adding nodes that are neither in the separator nor predecessors to the schedule is described in more detail above in step 450 of fig. 4.
In some implementations, the system can instead use path decomposition, i.e., instead of tree decomposition, to generate the schedule for computing the graph. In these embodiments, the computational graph defines a directed path that traverses each node in the computational graph.
A path in computational graph G is a sequence of different nodes in the computational graph such that each pair of consecutive nodes in the sequence has an edge from the computational graph connecting them. When each pair of nodes in a sequence is connected by a directed edge, the path is referred to as a directed path, where the edge walks from an earlier node in the sequence to a node immediately after the node in the sequence. In this specification, a directed path traversing each node in a computational graph is also referred to as a long spine (long spine) of the computational graph. In some implementations, if the computational graph does not include long ridges, the system can modify the computational graph, for example, by adding nodes and edges, such that the computation defines long ridges.
For directed acyclic computational graphs defining long ridges, the system can generate a schedule for executing operations based on the long ridges of the computational graph. For directed acyclic computational graphs that do not define long ridges, the system can generate a schedule to perform operations in the graph by first generating a path decomposition for the computational graph. A path decomposition is a particular tree decomposition of a computational graph that specifies (1) multiple sets, each set comprising one or more nodes (multiple packets) of the computational graph and (2) a directed path (tree) through each of the multiple sets, but with the additional requirement that the tree define a path that sequentially traverses each of the multiple packets. The plurality of node sets of the path decomposition are ordered according to the path defined by the path decomposition. For example, a plurality of packets is defined as a set X of packets in a path decomposition, e.g., X ═ X1，X2，X3In which X1，X2，X3Pressing bag X1In the path at packet X2(it is in packet X)3Before) represents the corresponding packet, or set of nodes in multiple sets in the path decomposition.
In some embodiments where the computational graph does not define long ridges, the system can modify the computational graph to define long ridges so that a path decomposition can still be formed from the computational graph. In one embodiment, long ridges can be added to the computation graph while controlling the path width of its path decomposition. Note here that the definition of the path width pw (G) of the computation graph G is the minimum width of the arbitrary path decomposition of the computation graph G, similar to the definition of the tree width tw (G).
FIG. 6 illustrates an example process 600 for generating a schedule to perform operations in a computational graph based on path decomposition. The process 600 will be described as being performed by a system of one or more computers located at one or more locations. For example, a suitably programmed operational scheduling system, such as operational scheduling system 200 of FIG. 2, can perform this process 600.
First, a scheduling operating system receives a request to generate a schedule for processing a computational graph (610), the schedule defining a sequence of nodes in the computational graph, and each node representing an operation.
The system then obtains data representing a computational graph having a plurality of nodes and directed edges (620). Each directed edge connects a previous node to a successive node having the output of the previous node as an input.
Next, the system determines whether the computational graph has defined long ridges that traverse each node in the computational graph (630).
If the computational graph has defined long ridges, the system generates a path decomposition (640) for the computational graph, wherein the path decomposition includes (1) a plurality of node sets (packets) and (2) a directed path through each of the plurality of node sets.
If the computational graph has not defined long ridges, the system modifies the computational graph to add new long ridges that traverse each node in the computational graph (680). Adding long ridges to the computation graph while controlling the path width of the path decomposition of the computation graph will be described in more detail below. Once the long ridges are added to the computational graph, the system generates a path decomposition for the computational graph as described above (640).
Next, the system initializes a schedule to perform the operations in the computational graph (650). For example, the initial schedule can be an empty set with zero nodes.
Thereafter, the system identifies a particular set of nodes, i.e., packets, from all the sets of nodes in the path decomposition, i.e., from all the packets (660).
For each node in a particular set of nodes and in an order according to the long spine in the computational graph, the system determines whether the node has any predecessors, adds each predecessor of the node to the schedule, and finally adds the node to the schedule (670). Adding these nodes to the schedule is described in more detail below.
As described above, a long ridge can be added to the computation graph while controlling the width of its path decomposition. In some embodiments, the addition of a long ridge adds only a factor of 2 to the path width of the computational graph.
Fig. 7 illustrates an example process 700 of modifying a computational graph to define a long ridge to the computational graph while maintaining a low path width of the computational graph. The process 700 will be described as being performed by a system of one or more computers located at one or more locations. For example, a suitably programmed operation scheduling system, such as operation scheduling system 200 of FIG. 2, can perform this process 700.
The system first adds node u to the computational graph, connects it to all other nodes, and adds it to all node sets (all packets) in the path decomposition (710), and then the system sets node u as the current final node of the directed path in the path decomposition (720). In some embodiments, the system can be recursively invoked to obtain a current end node that does not have a neighbor. Note here that the nodes in the computational graph are processed in topological order, so any node is not processed before its neighbors have been processed. The neighbors of a node are nodes that are connected to the node in topological order. For example, there must be at least one such node v's neighbor node r in the path decomposition, so that when node r has been processed, node v has no unexplored neighbor nodes, and therefore v should then be processed.
Next, the system adds a path from node u to connect with the immediate next node (730). The path should cover all nodes processed in a recursive manner. The system then updates the path decomposition of the computational graph to process the new path (740), and finally, the system removes node u from the computational graph and from all packets in the updated path decomposition of the graph (750).
When generating a schedule for a graph, the system can rely on structural characteristics of the path decomposition of the computation graph with long ridges. For example, any directed path (e.g., long spine) that passes between two node sets (two packets) of a path decomposition must also pass between any intermediate node sets (intermediate packets). For another example, each last node of a packet in the path decomposition of the computational graph can be ordered based on (1) the final node of the directed path (e.g., long spine) and (2) the packet containing the final node.
As another example, given a computational graph with a path split and one of its subgraphs, removing all nodes from each packet of the computational graph that are not in the subgraph results in the remainder of the path split being an efficient path split for the subgraph. By removing nodes, edges connecting the nodes are also removed. Thus, the remaining nodes and edges of the original path decomposition form an effective path decomposition.
As another example, the schedule of subgraphs of the path-decomposed computation graph can be interleaved, and the interleaved schedule of computation graphs can be merged, flattened, or compressed together to form the schedule for the computation graph.
The above structural features allow the divide and conquer strategy to recursively schedule the operations to the right and left of the path decomposition. The recursion may be repeated until one or more criteria are met. The criteria may include that a peak memory requirement for performing the scheduling falls below a threshold, such as available memory for performing the scheduling on particular hardware.
FIG. 8 illustrates an example process 800 for scheduling operations of computational graphs based on path decomposition. The process 800 will be described as being performed by a system of one or more computers located at one or more locations. For example, a suitably programmed operation scheduling system, such as operation scheduling system 200 of FIG. 2, can perform this process 800.
The system in fig. 8 first divides the path decomposition of the computation graph into a left path decomposition and a right path decomposition (810). The computational graph has defined long ridges that sequentially traverse each node in the computational graph. The partitioning of the path decomposition is based on the particular nodes in the computational graph. Nodes ordered after a particular node in the ridge are removed from each packet in the path decomposition. For example, a set of a plurality of node sets (e.g., a set of all packets) obtained from path decomposition is defined as X, where X ═ (X ═ X1，X2，...，Xb). U is defined as a specific node. Then, the system removes any nodes in X that follow node u in order according to the long spine to result in a new path decomposition of X '═ X'1，X’2，...，X’b). The set of nodes (packet) containing a particular node u separates the new path decomposition X' into a left path decomposition and a right decomposition. For example, if a particular node u is in packet Xi'in, then the new path decomposition X' is divided into a left path decomposition Xl＝(X’i-1，X’i-2，...，X’1) And right decomposition Xr＝(X’i+1，X’i+2，...，X’b)。
The system then determines whether there are any predecessors of a particular node u in the multiple node sets of the left path decomposition according to the order of the long ridges by taking the left path decomposition as input, and returns an interleaved schedule annotating the predecessors annotated to all of the particular nodes u (820). Note here that steps 810 and 820 can be recursively invoked to further divide the left path decomposition into smaller sizes, reflecting divide and conquer techniques.
Similarly, the system determines whether there are any predecessors of the particular node u in the multiple node sets of the right path decomposition according to the order of the long ridges by taking the right path decomposition as input, and returns an interleaved schedule that annotates all predecessors to the particular node u (830). Again, steps 810 and 830 can be recursively invoked. In some embodiments, step 820 and step 830 can share the same system structure without distinguishing between left path decomposition or right path decomposition. For example, steps 820 and 830 can be the same system that recursively calls back to step 810.
Finally, the system outputs a schedule for computing the graph by combining the interleaved schedules from both the left path decomposition and the right path decomposition (840). To this end, the system can take merge, flatten, and/or compress operations to integrate the interleaved schedule into the schedule based on the structural characteristics of the path decomposition.
As an explanation of the integrated interlace scheduling, σ is first defined as the scheduling, and u is defined as the node. Then defineCascade operation
next, the flattening operation is defined as
Order to
furthermore, consistency (X ', L) ═ consistency (X ', merge (L)), and scatter (L) ═ scatter (merge (L) — scatter (consistency (X ', L)).
This specification uses the term "configured" in connection with system and computer program components. A system of one or more computers to be configured to perform particular operations or actions means that the system has installed thereon software, firmware, hardware, or a combination of software, firmware, hardware that in operation causes the system to perform the operations or actions. By one or more computer programs to be configured to perform particular operations or actions is meant that the one or more programs include instructions which, when executed by a data processing apparatus, cause the apparatus to perform the operations or actions.
Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, tangibly embodied computer software or firmware, computer hardware including the structures disclosed in this specification and their structural equivalents, or combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible, non-transitory storage medium for execution by, or to control the operation of, data processing apparatus. The computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access storage device, or a combination of one or more of them. Alternatively or in addition, the program instructions can be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by the data processing apparatus.
The term "data processing apparatus" refers to data processing hardware and encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. An apparatus can also be, or further comprise, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). The apparatus can optionally include, in addition to hardware, code that creates an execution environment for the computer program, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
A computer program, which may also be referred to or described as a program, software application, app, module, software module, script, or code, can be written in any form of programming language, including compiled or interpreted languages, or declarative or processing languages; and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data, such as one or more scripts stored in a markup language document; in a single file dedicated to the program or in multiple consolidated files, e.g., files storing one or more modules, sub programs, or portions of code. A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a data communication network.
In this specification, the term "database" is used broadly to refer to any set of data: the data need not be structured in any particular way, or at all, and it can be stored on a storage device in one or more locations. Thus, for example, an index database can include multiple data sets, each of which can be organized and accessed differently.
Similarly, the term "engine" is used broadly in this specification to refer to a software-based system, subsystem, or process that is programmed to perform one or more particular functions. Typically, the engine will be implemented as one or more software modules or components installed on one or more computers in one or more locations. In some cases, one or more computers will be dedicated to a particular engine; in other cases, multiple engines can be installed and run on the same computer or multiple computers.
The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, and in combination with, special purpose logic circuitry, e.g., an FPGA or an ASIC.
A computer suitable for executing a computer program can be based on a general purpose microprocessor or a special purpose microprocessor or both, or any other kind of central processing unit. Generally, a central processing unit will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a central processing unit for executing or carrying out instructions and one or more memory devices for storing instructions and data. The central processing unit and the memory can be supplemented by, or incorporated in, special purpose logic circuitry. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, a computer need not have such devices. Further, the computer can be embedded in another device, e.g., a mobile telephone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game controller, a Global Positioning System (GPS) receiver, or a portable storage device, e.g., a Universal Serial Bus (USB) flash drive, etc.
Computer-readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and storage devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD ROM and DVD-ROM disks.
To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices can also be used to provide for interaction with the user; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input. In addition, the computer is able to interact with the user by sending documents to and receiving documents from the device used by the user; for example, by sending a web page to a web browser on the user's device in response to receiving a request from the web browser. In addition, computers can interact with a user by sending a text message or other form of message to a personal device, e.g., a smartphone that is running a messaging application, and then receiving a response message from the user.
The data processing apparatus for implementing the machine learning model can also include, for example, a dedicated hardware accelerator unit for processing common and computationally intensive portions of machine learning training or production, i.e., reasoning, workload.
The machine learning model can be implemented and deployed using a machine learning framework, such as a TensorFlow framework, a Microsoft Cognitive Toolkit framework, an Apache Singa framework, or an Apache MXNet framework.
Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server; or include middleware components, such as application servers; or include a front-end component, e.g., a client computer having a graphical user interface, a web browser, or an app through which a user can interact with an embodiment of the subject matter described in this specification; or any combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a Local Area Network (LAN) and a Wide Area Network (WAN), e.g., the internet.
The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, the server transmits data, such as HTML pages, to the user device, for example, for the purpose of displaying data to and receiving user input from a user interacting with the device as a client. Data generated at the user device, e.g., the results of the user interaction, can be received at the server from the device.
While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any invention or of what may be claimed, but rather as descriptions of features that may be specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, while operations are depicted in the drawings and described in the claims in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be beneficial. Moreover, the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
Specific embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some cases, multitasking and parallel processing may be beneficial.
Claims (13)
1. A method, comprising:
receiving, by a computational graph system, a request to generate a schedule for processing a computational graph, wherein the schedule defines a sequence of operations represented by the computational graph that are performed according to an ordering;
obtaining data representing the computational graph, the computational graph comprising a plurality of nodes and directed edges, wherein each node represents a respective operation,
wherein each directed edge from a respective first node to a respective second node represents a second operation represented by the respective second node requiring as input at least one output generated by performing a first operation represented by the respective first node;
generating a separator of the computational graph, wherein the separator satisfies a characteristic of removing nodes in the separator and edges connecting other nodes to the separator from the computational graph such that remaining nodes and edges of the computational graph form a plurality of connected components; and
generating the schedule to perform the operations represented in the computational graph, wherein generating the schedule comprises:
initializing the schedule with a zero node;
for each node in the separator:
determining whether the node has any predecessor nodes in the computational graph,
adding the predecessor node to the schedule when the node has any predecessor node, an
Adding the node in the schedule, an
Adding each node in each sub-graph that is neither in the separator nor a predecessor of any node in the separator on the computational graph to the schedule.
2. The method of any preceding claim, further comprising processing the computational graph according to the schedule.
3. The method of any preceding claim, wherein each operation represented in the computational graph is an operation associated with processing an input of a neural network.
4. The method of any of the preceding claims, wherein generating the separator comprises generating a tree decomposition for the computation graph.
5. The method of any of the preceding claims, wherein the divider is a balanced divider of the tree decomposition, and wherein generating the tree decomposition for the computation graph comprises generating the tree decomposition to be less than a predetermined width.
6. The method of any preceding claim, wherein the schedule comprises annotations, each annotation corresponding to a respective node in the schedule and indicating whether an output generated by performing a respective operation corresponding to the respective node is saved in memory.
7. A method, comprising:
receiving, by a computational graph system, a request to generate a schedule for processing a computational graph, wherein the schedule defines a sequence of operations represented by the computational graph that are performed according to an ordering;
obtaining data representing the computational graph, the computational graph including a plurality of nodes and directed edges,
wherein each node represents a respective operation, an
Wherein each directed edge from a respective first node to a respective second node represents a second operation represented by the respective second node requiring as input at least one output generated by performing a first operation represented by the respective first node;
determining whether the computational graph includes a first directed path through the computational graph that traverses each node in the computational graph, including through a particular node;
in response to determining that the computational graph includes the first directed path:
generating a path decomposition defining (i) a plurality of sets, each set of the plurality of sets including one or more nodes of the computational graph, and (ii) a second directed path through each set of the plurality of sets,
the schedule is initialized with a zero node,
identifying a particular set of the plurality of sets of the path decomposition that includes the particular node,
for each node in the particular set and in an order according to the first directed path:
determining whether the node has any predecessor nodes in the computational graph
Adding the predecessor node to the schedule when the node has any predecessor nodes in the computational graph, an
Adding the node to the schedule after the predecessor node.
8. The method of claim 7, further comprising:
determining that the computational graph does not include the first directed path;
modifying the computational graph in response to determining that the computational graph does not include the first directed path, wherein the computational graph defines a third directed path through each node of the computational graph, including through the particular node, as a result of modifying the computational graph; and
generating the schedule in response to the request, including:
the schedule is initialized with a zero node,
generating the path decomposition defining the plurality of sets,
identifying the particular set of the plurality of sets that includes the particular node,
for each node in the particular set and in an order according to the third directed path, adding the node to the schedule:
determining whether the node has any predecessor nodes in the computational graph,
adding the predecessor node to the schedule when the node has any predecessor nodes in the computational graph, an
Adding the node to the schedule after the predecessor node.
9. The method of any of claims 7 or 8, further comprising:
after generating the path decomposition for the computation graph:
dividing the path decomposition of the computational graph into a left path decomposition and a right path decomposition, wherein the left path decomposition includes each of the plurality of sets ordered according to the second directed path before the particular set, and wherein the right path decomposition includes each of the plurality of sets ordered according to the second directed path after the particular set; and
wherein determining that one or more nodes of the computational graph are predecessors of the nodes comprises performing operations comprising:
determining that one or more nodes of the computational graph are predecessors of the nodes and are included in the left path decomposition, and in response, adding each of the one or more nodes in the left path decomposition to the schedule, an
Determining that one or more nodes of the computational graph are predecessors of the nodes and are included in the right path decomposition, and in response, adding each of the one or more nodes in the right path decomposition to the schedule.
10. The method of any of the preceding claims 7-9, further comprising: after generating the path split, removing all nodes in the path split ordered after the particular node in the first directed path.
11. The method of any of the preceding claims, wherein generating the schedule comprises optimizing the divider size.
12. A system comprising one or more computers and one or more storage devices storing instructions that, when executed by the one or more computers, cause the one or more computers to perform operations of the method of any of claims 1-11.
13. One or more computer storage media storing instructions that, when executed by one or more computers, cause the one or more computers to perform operations of the method of any of claims 1-11.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201962875433P | 2019-07-17 | 2019-07-17 | |
US62/875,433 | 2019-07-17 | ||
PCT/US2020/042646 WO2021011914A1 (en) | 2019-07-17 | 2020-07-17 | Scheduling operations on a computation graph |
Publications (1)
Publication Number | Publication Date |
---|---|
CN114008594A true CN114008594A (en) | 2022-02-01 |
Family
ID=72193568
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202080044760.6A Pending CN114008594A (en) | 2019-07-17 | 2020-07-17 | Scheduling operations on a computational graph |
Country Status (4)
Country | Link |
---|---|
US (3) | US10963301B2 (en) |
EP (1) | EP3970012A1 (en) |
CN (1) | CN114008594A (en) |
WO (1) | WO2021011914A1 (en) |
Families Citing this family (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN114008594A (en) * | 2019-07-17 | 2022-02-01 | 谷歌有限责任公司 | Scheduling operations on a computational graph |
US11526761B2 (en) * | 2019-08-24 | 2022-12-13 | Microsoft Technology Licensing, Llc | Neural network training with decreased memory consumption and processor utilization |
US11521062B2 (en) * | 2019-12-05 | 2022-12-06 | International Business Machines Corporation | Neural network training using a data flow graph and dynamic memory management |
CN114003306B (en) * | 2021-10-27 | 2024-03-15 | 上海商汤科技开发有限公司 | Video memory optimization method, device, equipment and storage medium |
CN114510338B (en) * | 2022-04-19 | 2022-09-06 | 浙江大华技术股份有限公司 | Task scheduling method, task scheduling device and computer readable storage medium |
US20240104341A1 (en) * | 2022-09-27 | 2024-03-28 | Zhejiang Lab | Memory optimization method and apparatus for neural network compilation |
CN116166405B (en) * | 2023-04-21 | 2023-08-01 | 北京燧原智能科技有限公司 | Neural network task scheduling strategy determination method and device in heterogeneous scene |
Family Cites Families (21)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6044222A (en) * | 1997-06-23 | 2000-03-28 | International Business Machines Corporation | System, method, and program product for loop instruction scheduling hardware lookahead |
US7102641B2 (en) * | 2004-03-29 | 2006-09-05 | International Business Machines Corporation | Method, data processing system, and computer program product for determining inversion edges for a cyclic compound directed graph |
JP4379894B2 (en) * | 2006-11-28 | 2009-12-09 | 株式会社エスグランツ | Method and program for splitting / joining coupled node tree |
US9218209B2 (en) * | 2008-12-31 | 2015-12-22 | Pivotal Software, Inc. | Apparatus and method for parallel processing of a query |
US8332862B2 (en) * | 2009-09-16 | 2012-12-11 | Microsoft Corporation | Scheduling ready tasks by generating network flow graph using information receive from root task having affinities between ready task and computers for execution |
US9715560B2 (en) * | 2012-04-24 | 2017-07-25 | International Business Machines Corporation | Optimizing sparse schema-less data in data stores |
US9563470B2 (en) * | 2013-12-23 | 2017-02-07 | International Business Machines Corporation | Backfill scheduling for embarrassingly parallel jobs |
US10152557B2 (en) * | 2014-01-31 | 2018-12-11 | Google Llc | Efficient similarity ranking for bipartite graphs |
US9652286B2 (en) * | 2014-03-21 | 2017-05-16 | Oracle International Corporation | Runtime handling of task dependencies using dependence graphs |
US10291696B2 (en) * | 2014-04-28 | 2019-05-14 | Arizona Board Of Regents On Behalf Of Arizona State University | Peer-to-peer architecture for processing big data |
KR102433254B1 (en) * | 2015-10-28 | 2022-08-18 | 구글 엘엘씨 | Processing computational graphs |
EP4202782A1 (en) | 2015-11-09 | 2023-06-28 | Google LLC | Training neural networks represented as computational graphs |
CN114385350A (en) * | 2016-11-30 | 2022-04-22 | 华为技术有限公司 | Method, device and system for processing graph data |
US10599482B2 (en) * | 2017-08-24 | 2020-03-24 | Google Llc | Method for intra-subgraph optimization in tuple graph programs |
CN109960570B (en) * | 2017-12-14 | 2021-09-03 | 北京图森智途科技有限公司 | Multi-module scheduling method, device and system |
US20190317812A1 (en) * | 2018-04-16 | 2019-10-17 | State Street Corporation | Guaranteed quality of service in cloud computing environments |
GB2580284B (en) * | 2018-08-13 | 2021-01-06 | Metaswitch Networks Ltd | Generating packet processing graphs |
US10474497B1 (en) * | 2018-11-14 | 2019-11-12 | Capital One Services, Llc | Computing node job assignment using multiple schedulers |
US20200249998A1 (en) * | 2019-02-01 | 2020-08-06 | Alibaba Group Holding Limited | Scheduling computation graph heterogeneous computer system |
CN114008594A (en) * | 2019-07-17 | 2022-02-01 | 谷歌有限责任公司 | Scheduling operations on a computational graph |
US11709059B2 (en) * | 2019-12-23 | 2023-07-25 | Waymo Llc | Asynchronous execution graphs for autonomous vehicles |
-
2020
- 2020-07-17 CN CN202080044760.6A patent/CN114008594A/en active Pending
- 2020-07-17 EP EP20760981.9A patent/EP3970012A1/en active Pending
- 2020-07-17 WO PCT/US2020/042646 patent/WO2021011914A1/en unknown
- 2020-07-17 US US16/932,581 patent/US10963301B2/en active Active
-
2021
- 2021-03-26 US US17/214,699 patent/US11755367B2/en active Active
-
2023
- 2023-07-18 US US18/223,495 patent/US20240126596A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
US11755367B2 (en) | 2023-09-12 |
US20210019184A1 (en) | 2021-01-21 |
WO2021011914A1 (en) | 2021-01-21 |
EP3970012A1 (en) | 2022-03-23 |
US10963301B2 (en) | 2021-03-30 |
US20240126596A1 (en) | 2024-04-18 |
US20210216367A1 (en) | 2021-07-15 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN114008594A (en) | Scheduling operations on a computational graph | |
US20200293838A1 (en) | Scheduling computation graphs using neural networks | |
Zhang et al. | Poseidon: An efficient communication architecture for distributed deep learning on {GPU} clusters | |
US20220391665A1 (en) | Method for splitting neural network model by using multi-core processor, and related product | |
US10970628B2 (en) | Training neural networks represented as computational graphs | |
WO2018099085A1 (en) | Neural network model training method and device, and chip | |
US11074107B1 (en) | Data processing system and method for managing AI solutions development lifecycle | |
EP4036803A1 (en) | Neural network model processing method and apparatus, computer device, and storage medium | |
US8959138B2 (en) | Distributed data scalable adaptive map-reduce framework | |
CN111242321A (en) | Data processing method and related product | |
CN114756383A (en) | Distributed computing method, system, device and storage medium | |
US20160378809A1 (en) | Massive time series correlation similarity computation | |
Han et al. | Signal processing and networking for big data applications | |
WO2022068663A1 (en) | Memory allocation method, related device, and computer readable storage medium | |
CN113994350A (en) | Generating parallel computing schemes for neural networks | |
CN109669772A (en) | Calculate the parallel execution method and apparatus of figure | |
CN114610474B (en) | Multi-strategy job scheduling method and system under heterogeneous supercomputing environment | |
EP4198771A1 (en) | Data processing method and apparatus, computer readable medium, and electronic device | |
CN113822173A (en) | Pedestrian attribute recognition training acceleration method based on node merging and path prediction | |
CN111133458A (en) | Enhancing neural networks | |
US20110209007A1 (en) | Composition model for cloud-hosted serving applications | |
Yu et al. | A sum-of-ratios multi-dimensional-knapsack decomposition for DNN resource scheduling | |
CN115481730A (en) | Attention mechanism model training method and device, terminal and storage medium | |
Bengre et al. | A learning-based scheduler for high volume processing in data warehouse using graph neural networks | |
CN117201319B (en) | Micro-service deployment method and system based on edge calculation |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |