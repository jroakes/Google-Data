CN117083668A - Reducing streaming ASR model delay using self-alignment - Google Patents
Reducing streaming ASR model delay using self-alignment Download PDFInfo
- Publication number
- CN117083668A CN117083668A CN202180096433.XA CN202180096433A CN117083668A CN 117083668 A CN117083668 A CN 117083668A CN 202180096433 A CN202180096433 A CN 202180096433A CN 117083668 A CN117083668 A CN 117083668A
- Authority
- CN
- China
- Prior art keywords
- speech recognition
- transducer
- model
- recognition model
- layer
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000009826 distribution Methods 0.000 claims abstract description 11
- 238000000034 method Methods 0.000 claims description 35
- 238000012549 training Methods 0.000 claims description 24
- 238000012545 processing Methods 0.000 claims description 17
- 238000013518 transcription Methods 0.000 claims description 17
- 230000035897 transcription Effects 0.000 claims description 17
- 230000000873 masking effect Effects 0.000 claims description 16
- 238000010606 normalization Methods 0.000 claims description 12
- 239000013598 vector Substances 0.000 claims description 10
- 230000007704 transition Effects 0.000 claims description 7
- 230000008859 change Effects 0.000 claims description 6
- 230000001537 neural effect Effects 0.000 claims description 5
- 230000000306 recurrent effect Effects 0.000 claims description 5
- 230000015654 memory Effects 0.000 description 39
- 238000010586 diagram Methods 0.000 description 11
- 238000013528 artificial neural network Methods 0.000 description 10
- 238000004891 communication Methods 0.000 description 9
- 238000004590 computer program Methods 0.000 description 8
- 230000001934 delay Effects 0.000 description 8
- 230000003287 optical effect Effects 0.000 description 6
- 230000036961 partial effect Effects 0.000 description 6
- 230000008569 process Effects 0.000 description 6
- 238000003058 natural language processing Methods 0.000 description 5
- 230000004044 response Effects 0.000 description 4
- 230000006870 function Effects 0.000 description 3
- 238000005457 optimization Methods 0.000 description 3
- 238000013459 approach Methods 0.000 description 2
- 230000008901 benefit Effects 0.000 description 2
- 230000001143 conditioned effect Effects 0.000 description 2
- 230000003993 interaction Effects 0.000 description 2
- 230000007246 mechanism Effects 0.000 description 2
- 230000006855 networking Effects 0.000 description 2
- 230000009467 reduction Effects 0.000 description 2
- 239000004065 semiconductor Substances 0.000 description 2
- 239000007787 solid Substances 0.000 description 2
- 230000003068 static effect Effects 0.000 description 2
- 230000009471 action Effects 0.000 description 1
- 230000004913 activation Effects 0.000 description 1
- 230000002411 adverse Effects 0.000 description 1
- 230000015556 catabolic process Effects 0.000 description 1
- 238000013527 convolutional neural network Methods 0.000 description 1
- 238000003066 decision tree Methods 0.000 description 1
- 238000006731 degradation reaction Methods 0.000 description 1
- 238000011161 development Methods 0.000 description 1
- 230000000694 effects Effects 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 230000002452 interceptive effect Effects 0.000 description 1
- 230000000670 limiting effect Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000012423 maintenance Methods 0.000 description 1
- 238000013507 mapping Methods 0.000 description 1
- 239000008267 milk Substances 0.000 description 1
- 210000004080 milk Anatomy 0.000 description 1
- 235000013336 milk Nutrition 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000002829 reductive effect Effects 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 230000005236 sound signal Effects 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/26—Speech to text systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/06—Creation of reference templates; Training of speech recognition systems, e.g. adaptation to the characteristics of the speaker's voice
- G10L15/063—Training
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/16—Speech classification or search using artificial neural networks
Abstract
A streaming speech recognition model (200) includes an audio encoder (210) configured to receive a sequence of acoustic frames (110) and generate a high-order feature representation (202) for a respective acoustic frame in the sequence of acoustic frames. The streaming speech recognition model also includes a tag encoder (220) configured to receive the sequence of non-blank symbols (242) output by the final softmax layer (240) and generate a dense representation (222). The streaming speech recognition model further includes a joint network (230) configured to receive the high-order feature representation generated by the audio encoder and the dense representation generated by the tag encoder and to generate a probability distribution over possible speech recognition hypotheses (232). Here, a self-alignment is used to train the streaming speech recognition model to reduce the prediction delay by encouraging an alignment path of 1 frame to the left from the reference forced alignment frame.
Description
Technical Field
The present disclosure relates to reducing streaming Automatic Speech Recognition (ASR) model delay using self-alignment.
Background
Automatic Speech Recognition (ASR), a process that takes audio input and transcribes it into text, has greatly become an important technology for use in mobile devices and other devices. Typically, ASR attempts to provide accurate transcription of what a person speaks by taking an audio input (e.g., a speech utterance) and transcribing the audio input into text. Based on the continued development of deep neural networks, modern ASR models continue to increase in both accuracy (e.g., low Word Error Rate (WER)) and latency (e.g., delay between user speech and transcription). When ASR systems are used today, it is necessary that the ASR system decodes the speech in a streaming manner, which corresponds to real-time or even faster than real-time, but which is also accurate. However, the streaming end-to-end model that optimizes sequence likelihood without any delay constraints suffers from high delays between the audio input and the predicted text, as these models learn to improve their predictions by using more future contexts.
Disclosure of Invention
One aspect of the present disclosure provides a streaming speech recognition model comprising an audio encoder configured to: receiving as input a sequence of acoustic frames; and at each of a plurality of time steps, generating a high-order feature representation for a corresponding acoustic frame in the sequence of acoustic frames. The streaming speech recognition model further includes a tag encoder configured to receive as input a sequence of non-blank symbols output by the final softmax layer; and generating a dense representation at each of the plurality of time steps. The streaming speech recognition model further includes a federated network configured to: receiving as input the high-order feature representation generated by the audio encoder at each of the plurality of time steps and the dense representation generated by the tag encoder at each of the plurality of time steps; and at each of the plurality of time steps, generating a probability distribution over possible speech recognition hypotheses at the corresponding time step. Here, the streaming speech recognition model is trained using self-alignment to reduce the prediction delay by encouraging an alignment path of 1 frame to the left of the reference forced alignment frame at each time step for each training batch.
Implementations of the disclosure may include one or more of the following optional features. In some implementations, the streaming speech recognition model can include a transducer-transducer model. In some embodiments, the audio encoder may include a stack of transducer layers, wherein each transducer layer includes: normalizing the layer; masking multi-headed attention layer with relative position coding; residual connection; stacking/de-stacking layers; and a feed-forward layer. Here, the stacking/de-stacking layer may be configured to change the frame rate of the corresponding transducer layer to adjust the processing time of the transducer-transducer model during training and inference. In some examples, the tag encoder includes a stack of transformer layers, wherein each transformer layer includes: normalizing the layer; masking multi-headed attention layer with relative position coding; residual connection; stacking/de-stacking layers; and a feed-forward layer.
Alternatively, the tag encoder may include a binary (bigram) embedded look-up decoder model. In some examples, the streaming speech recognition model includes one of: a recurrent neural transducer (RNN-T) model; a transducer-transducer model; a convolutional network-Transducer (ConvNet-Transducer) model; or a Conformer-transducer model. Training the streaming speech recognition model using self-alignment to reduce prediction delay may include constraining alignment of the decoded pictures using self-alignment without using any external aligner model. In some embodiments, the streaming speech recognition model is executed on a user device or server. In some examples, each acoustic frame in the sequence of acoustic frames includes a dimensional feature vector.
Another aspect of the present disclosure provides a computer-implemented method that, when executed on data processing hardware, causes the data processing hardware to perform operations for training a streaming speech recognition model using self-alignment to reduce prediction delay. The operations include receiving a sequence of acoustic frames corresponding to an utterance as input to the streaming speech recognition model. The streaming speech recognition model is configured to learn an alignment probability between the sequence of acoustic frames and the output sequence of tag tokens. The operations also include generating a speech recognition result for the utterance as an output from the streaming speech recognition model. The speech recognition result includes an output sequence of tag tokens using a decoding graph. The operations also include generating a speech recognition model penalty based on the speech recognition result and a true transcription of the utterance. The operations further include obtaining a reference forced alignment path including a reference forced alignment frame from the decoded picture; and identifying from the decoded picture 1 frame to the left from each reference forced alignment frame in the reference forced alignment path. The operations further include summing tag transition probabilities based on the identified frames to the left from each forced alignment frame in the reference forced alignment path; and updating the streaming speech recognition model based on the summation of the tag transition probabilities and the speech recognition model loss.
Implementations of the disclosure may include one or more of the following optional features. In some implementations, the operations further include generating, by an audio encoder of the streaming speech recognition model, a high-order feature representation for a corresponding acoustic frame in the sequence of acoustic frames at each of a plurality of time steps; receiving as input to a tag encoder of the streaming speech recognition model a sequence of non-blank symbols output by a final softmax layer; generating, by the tag encoder, a dense representation at each of the plurality of time steps; receiving the high-order feature representation generated by the audio encoder at each of the plurality of time steps and the dense representation generated by the tag encoder at each of the plurality of time steps as inputs to a federated network of the streaming speech recognition model; and generating, by the federated network, a probability distribution over possible speech recognition hypotheses at each of the plurality of time steps. In some examples, the tag encoder includes a stack of transformer layers, wherein each transformer layer includes: normalizing the layer; masking multi-headed attention layer with relative position coding; residual connection; stacking/de-stacking layers; and a feed-forward layer. The tag encoder may include a binary embedded look-up decoder model.
In some implementations, the streaming speech recognition model includes a transducer-transducer model. The audio encoder may comprise a stack of transducer layers, wherein each transducer layer comprises: normalizing the layer; masking multi-headed attention layer with relative position coding; residual connection; stacking/de-stacking layers; and a feed-forward layer. Here, the stacking/de-stacking layer may be configured to change the frame rate of the corresponding transducer layer to adjust the processing time of the transducer-transducer model during training and inference.
In some embodiments, the streaming speech recognition model comprises one of: a recurrent neural transducer (RNN-T) model; a transducer-transducer model; a convolutional network-Transducer (ConvNet-Transducer) model; or a Conformer-transducer model. The streaming speech recognition model may be executed on a user device or a server. In some examples, the operations further include training the streaming speech recognition model using self-alignment to reduce prediction delay without using any external aligner model to constrain alignment of the decoded pictures
The details of one or more embodiments of the disclosure are set forth in the accompanying drawings and the description below. Other aspects, features, and advantages will be apparent from the description and drawings, and from the claims.
Drawings
FIG. 1 is a schematic diagram of a speech environment implementing a transducer model to perform streaming speech recognition.
FIG. 2 is a schematic diagram of an example transducer model architecture.
Fig. 3 is a diagram depicting an example decoding diagram of a self-aligned path and a forced aligned path.
Fig. 4 is a schematic diagram of an example transformer architecture.
FIG. 5 is a flow diagram of an example arrangement of operations of a method for reducing streaming ASR model delay with self-alignment.
FIG. 6 is a schematic diagram of an example computing device that may be used to implement the systems and methods described herein.
Like reference symbols in the various drawings indicate like elements.
Detailed Description
Automatic Speech Recognition (ASR) systems focus on providing not only quality/accuracy (e.g., low Word Error Rate (WER)), but also low latency (e.g., short delay between user speech and transcription occurrence). Recently, end-to-end (E2E) ASR models have gained popularity in achieving the most advanced performance in terms of accuracy and latency. In contrast to conventional hybrid ASR systems that include separate acoustic, pronunciation, and language models, the E2E model applies a sequence-to-sequence approach to jointly learn acoustic and language modeling in a single neural network that is trained end-to-end based on training data (e.g., speech transcription pairs). Here, the E2E model refers to a model whose architecture is entirely constructed of a neural network. The complete neural network works without external and/or manually designed components (e.g., finite state transducers, lexicons, or text normalization modules). Additionally, when training E2E models, these models typically do not require bootstrapping (bootstrapping) from a decision tree or time alignment from a separate system.
When ASR systems are in use today, it may be desirable for the ASR system to decode the utterance in a streaming manner that corresponds to displaying a description of the utterance in real-time or even faster than real-time as the user speaks. To illustrate, when the ASR system is displayed on a user computing device (e.g., such as a mobile phone) that experiences direct user interactivity, an application (e.g., a digital assistant application) executing on the user device and using the ASR system may require speech recognition to be streaming such that words, word blocks, and/or individual characters appear on the screen as soon as they are spoken. Additionally, the user of the user device may also have a low tolerance to time delays. For example, when a user speaks a query requesting the digital assistant to retrieve details of an upcoming appointment from a calendar application, the user desires the digital assistant to provide a response conveying the retrieved details as soon as possible. Because of this low tolerance, ASR systems strive to run on user devices in a manner that minimizes the effects from delays and inaccuracies that may adversely affect the user's experience.
One form of sequence-to-sequence model, known as a recurrent neural network transducer (RNN-T), does not employ an attention mechanism, and unlike other sequence-to-sequence models that typically require processing of an entire sequence (e.g., an audio waveform) to produce an output (e.g., a sentence), RNN-T continuously processes input samples and streams output symbols, a feature that is particularly attractive for real-time communications. For example, speech recognition using RNN-T can output characters one after the other when speaking. Here, RNN-T uses a feedback loop that feeds back the symbol predicted by the model to itself to predict the next symbol. Because decoding RNN-T involves beam searching through a single neural network rather than a large decoder graph, RNN-T can scale to a fraction of the size of the server-based speech recognition model. With the reduction in size, RNN-T may be fully deployed on the device and able to run offline (i.e., without network connection); thus avoiding unreliability problems with respect to the communication network. While suitable for providing streaming transcription capability that typically identifies session queries (e.g., "set timer", "remind me to purchase milk", etc.) and time-sensitive applications, RNN-T models that utilize long-short-term memory (LSTM) to provide sequence encoders have limited look-ahead (look-ahead) audio context capability, thus still falling behind large advanced conventional models (e.g., server-based models with separate AM, PM, and LM) and attention-based sequence-to-sequence models (e.g., listen-attention-spelling (LAS)) in terms of quality (e.g., speech recognition accuracy typically measured by Word Error Rate (WER)).
Recently, transducer-transducer (T-T) and Conformer-transducer (C-T) model architectures have been introduced to further improve the RNN-T model architecture by replacing the LSTM layer at the audio encoder and/or prediction network with a corresponding one of the transducer or Conformer layers. In general, the T-T and C-T model architectures are able to access future audio frames (e.g., right context) when computing self-attention in their respective transformer or Conformer layers. Thus, the T-T and C-T model architectures may utilize future right contexts to operate in a non-streaming transcription mode to improve speech recognition performance when latency constraints are relaxed. That is, there is a duration of the predicted delay that is proportional to the amount of future audio frames that are accessed. However, similar to RNN-T, T-T and C-T model architectures may also operate in a streaming transcription mode, where self-attention depends only on past acoustic frames (e.g., left context).
Streaming speech recognition models, such as transducer models (e.g., RNN-T, T-T and C-T), optimize sequence likelihood without any delay constraints and therefore suffer from high delays between audio input and predicted text, as these models learn to improve their predictions by using more future contexts. A recent approach to reducing predicted delays includes a constrained alignment technique that penalizes word boundaries by masking alignment paths that exceed a predetermined threshold delay based on audio alignment information obtained from an external alignment model. While this technique is effective in reducing the latency of the streaming end-to-end model, a high precision external alignment model is required to minimize WER degradation, which further complicates the model training step. Other techniques of blind reduction of delay by selecting the most efficient direction in the RNN-T decoding diagram typically select a direction that is not optimal for all audio inputs due to lack of alignment information, which may further degrade the delay-WER tradeoff.
To alleviate the drawbacks associated with using an external alignment model or simply blindly reducing delay by selecting the most efficient direction from the decoded graph, embodiments herein are directed to reducing predictive delay in a streaming speech recognition model by using self-alignment. Notably, self-alignment does not require the use of any external alignment model or blind optimization delay, but rather utilizes reference forced alignment learned from a trained speech recognition model to select the best low-latency direction to reduce delay. The reference forced alignment may include Viterbi (Viterbi) forced alignment. That is, self-alignment will always locate the path of the 1 frame to the left of the viterbi forced alignment at each time step in the decoded picture. Self-alignment has advantages over existing schemes for constraining delay. First, the training complexity of self-alignment is much lower than the teacher-aided solution, because self-alignment does not require an external alignment model. Second, self-alignment minimally affects ASR training by constraining only the most likely alignment paths. Instead, other schemes affect many alignment paths by masking them or changing the weights on their tag transition probabilities. Since the delay constraint regularization term always conflicts with the primary ASR penalty, minimal intervention on the primary penalty will be important to optimize the delay and performance tradeoff. Self-alignment regularizes a single path by pushing it only to the left.
Fig. 1 is an example of a speech environment 100. In the speech environment 100, the manner in which the user 104 interacts with a computing device, such as the user device 10, may be through voice input. The user device 10 (also commonly referred to as device 10) is configured to capture sound (e.g., streaming audio data) from one or more users 104 within the speech environment 100. Here, the streaming audio data may refer to the spoken utterance 106 of the user 104, which spoken utterance 106 is used as an audible query, command of the device 10, or audible communication captured by the device 10. The voice-enabled system of device 10 may respond to a query or command by answering the query and/or having the command executed/fulfilled by one or more downstream applications.
User device 10 may correspond to any computing device associated with user 104 and capable of receiving audio data. Some examples of user devices 10 include, but are not limited to, mobile devices (e.g., mobile phones, tablet computers, laptops, etc.), computers, wearable devices (e.g., smartwatches), smart appliances, internet of things (IoT) devices, vehicle infotainment systems, smart displays, smart speakers, etc. The user device 10 includes data processing hardware 12 and memory hardware 14 in communication with the data processing hardware 12 and storing instructions that, when executed by the data processing hardware 12, cause the data processing hardware 12 to perform one or more operations. The user device 10 further comprises an audio system 16, the audio system 16 having an audio capturing device (e.g. microphone) 16, 16a for capturing and converting spoken utterances 106 within the speech environment 100 into electrical signals and a speech output device (e.g. speaker) 16, 16b for delivering audible audio signals (e.g. as output audio data from the device 10). Although in the illustrated example the user device 10 implements a single audio capture device 16a, the user device 10 may implement an array of audio capture devices 16a without departing from the scope of the disclosure, whereby one or more capture devices 16a in the array may not physically reside on the user device 10, but instead communicate with the audio system 16.
In the speech environment 100, the transducer model 200 of the Automated Speech Recognition (ASR) system 118 resides on the user device 10 of the user 104 and/or on a remote computing device 60 (e.g., one or more remote servers of a distributed system executing in a cloud computing environment) in communication with the user device 10 via the network 40. The user device 10 and/or the remote computing device 60 also includes an audio subsystem 108, the audio subsystem 108 being configured to receive the utterance 106 spoken by the user 104 and captured by the audio capture device 16a, and to convert the utterance 106 into a corresponding digital format associated with the input acoustic frame 110 that can be processed by the ASR system 118. In the illustrated example, the user speaks the respective utterance 106 and the audio subsystem 108 converts the utterance 106 into corresponding audio data (e.g., acoustic frames) 110 for input to the ASR system 118. Thereafter, the transducer model 200 receives as input the audio data 110 corresponding to the utterance 106 and generates/predicts as output a corresponding transcription 120 (e.g., recognition result/hypothesis) of the utterance 106. The transducer model 200 provides streaming speech recognition results without access to the look-ahead audio, and thus provides streaming transcription capabilities in real-time as the user 104 is speaking the speech 106. For example, the digital assistant application 50 executing on the user device 10 may require that the speech recognition be streaming such that words, word blocks, and/or individual characters appear on the screen as soon as they are spoken.
The user device 10 and/or the remote computing device 60 further execute a user interface generator 107, the user interface generator 107 being configured to present a representation of the transcription 120 of the utterance 106 to the user 104 of the user device 10. As described in more detail below, the user interface generator 107 may stream the partial speech recognition result 120a during time 1 and then display the final speech recognition result 120b during time 2. In some configurations, the transcription 120 output from the ASR system 118 is processed, for example, by a Natural Language Understanding (NLU) module executing on the user device 10 or the remote computing device 60, to execute the user command/query specified by the utterance 106. Additionally or alternatively, a text-to-speech system (not shown), such as executing on any combination of user device 10 or remote computing device 60, may convert the transcription into synthesized speech for audible output by user device 10 and/or another device.
In the illustrated example, the user 104 interacts with a program or application 50 (e.g., a digital assistant application 50) of the user device 10 using the ASR system 118. For example, fig. 1 depicts user 104 in communication with digital assistant application 50, and digital assistant application 50 displays digital assistant interface 18 on a screen of user device 10 to depict a session between user 10 and digital assistant application 50. In this example, user 104 asks digital assistant application 50"What time is the concert tonight? (is the concert in the evening. The question from the user 104 is the spoken utterance 106 captured by the audio capturing device 16a and processed by the audio system 16 of the user device 10. In this example, the audio system 16 receives the spoken utterance 106 and converts it into an acoustic frame 110 for input to the ASR system 118.
Continuing with the example, when an acoustic frame 110 corresponding to the utterance 106 as spoken by the user 104 is received, the transducer model 200 encodes the acoustic frame 110 and then decodes the encoded acoustic frame 110 into partial speech recognition results 120a. During time 1, user interface generator 107 presents a representation of the partial speech recognition result 120a of utterance 106 to user 104 of user device 10 via digital assistant interface 18 in a streaming manner such that words, word blocks, and/or individual characters appear on the screen as soon as they are spoken. In some examples, the first look-ahead audio context is equal to 0.
During time 2, user interface generator 107 presents a representation of final speech recognition result 120b of utterance 106 to user 104 of user device 10 via digital assistant interface 18. Once the user has finished speaking, the final speech recognition result 120b may simply be a partial speech recognition result 120a. Alternatively, the ASR system 118 may include another speech recognition to re-score portions of the speech recognition results and/or use an external language model. Alternatively, once the user has finished speaking, the same transducer model 200 may process the audio again, but instead utilize the right look-ahead audio context to generate the final speech recognition result 120b. The present disclosure does not relate to how the final speech recognition result 120b is obtained, but focuses on limiting the delay in the streaming partial speech recognition result 120a output by the transducer model 200.
In the example shown in fig. 1, digital assistant application 50 may use natural language processing to respond to questions posed by user 10. Natural language processing generally refers to the process of interpreting a written language (e.g., partial speech recognition results 120a and/or final speech recognition results 120 b) and determining whether the written language suggests any actions. In this example, digital assistant application 50 uses natural language processing to identify that the problem from user 10 relates to the user's calendar, and more particularly to a concert on the user's calendar. By identifying these details using natural language processing, the automated assistant returns a response 19 to the user's query, where the response 19 states "Venue doors open at 6:30PM and concert starts at 8pm (stadium opens the door at 6:30pm, and concert starts at 8 pm)". In some configurations, natural language processing occurs on a remote server 60 in communication with the data processing hardware 12 of the user device 10.
Referring to fig. 2, the transducer model 200 may provide end-to-end (E2E) speech recognition by integrating acoustic, pronunciation, and language models into a single neural network, and does not require a lexicon or a separate text normalization component. Various structures and optimization mechanisms can provide increased accuracy and reduced model training time. In the example shown, the transducer model 200 includes a transducer-transducer (T-T) model architecture that complies with latency constraints associated with interactive applications. The T-T model 200 provides a small computational footprint compared to conventional ASR architectures and utilizes less memory requirements, making the T-T model architecture suitable for performing speech recognition entirely on the user device 10 (e.g., without requiring communication with the remote server 60). The T-T model 200 includes an audio encoder 210, a tag encoder 220, and a joint network 230. An audio encoder 210, substantially similar to an Acoustic Model (AM) in a conventional ASR system, includes a neural network having a plurality of transducer layers. For example, the audio encoder 210 reads a d-dimensional feature vector sequence (e.g., the acoustic frame 110 (fig. 1)) x= (x) 1 ,x 2 ,···,x T ) Wherein, the method comprises the steps of, wherein,and at each time step a higher order feature representation 202 is generated. The higher order feature representation 202 is denoted ah 1 ,...,ah T . An example transducer-transducer model architecture is described in U.S. application No.17/210,465 filed 3/23 at 2021, the contents of which are incorporated herein by reference in their entirety.
Similarly, the tag encoder 220 may also include a neural network or look-up table embedded model of the transformer layer, which, like the Language Model (LM), will so far output the sequence y of non-blank symbols 242 by the final Softmax layer 240 0 ,...,y ui-1 Is processed to encode a dense representation 222 (e.g., denoted Ih u ). At tag encoder 220 packageIn embodiments of neural networks including transformer layers, each transformer layer may include a normalization layer, a masking multi-headed attention layer with relative position coding, a residual connection, a feed forward layer, and a discard layer. In these embodiments, the tag encoder 220 may include two transformer layers. In embodiments where the tag encoder 220 includes a look-up table embedding model with binary tag contexts, the embedding model is configured to learn a weight vector for d dimensions of each possible binary tag context, where d is the dimension of the output of the audio encoder 210 and the tag encoder 220. In some examples, the total number of parameters embedded in the model is N 2 X d, where N is the vocabulary size for the tag. Here, the learned weight vector is then used as an embedding of the binary tag context in the T-T model 200 to produce a fast tag encoder 220 runtime.
Finally, with the T-T model architecture, the representations generated by the audio encoder 210 and the tag encoder 220 are used by the joint network 230 using dense layer J u,t To be combined. The joint network 230 then predicts the alignment distribution (e.g., alignment probability 232) on the next output symbol as follows.
Pr(z u,t |x,t,y 1 ,…,y u-1 ) (1)
Where x is the audio input, y is the real tag sequence, and z is the alignment belonging to y. In other words, the federated network 230 generates a probability distribution 232 over possible speech recognition hypotheses at each output step (e.g., time step). Here, a "possible speech recognition hypothesis" corresponds to a set of output labels (also referred to as "phonetic units"), each representing a grapheme (e.g., symbol/character) or a word block in a particular natural language. For example, when the natural language is english, the output set of labels may include twenty-seven (27) symbols, e.g., one label for each of the 26 letters in the english alphabet, and one label specifying a space. Thus, the federated network 230 may output a set of values that indicate the likelihood of occurrence of each output label in the predetermined set of output labels. The set of values can be vectors (e.g., one-hot vectors) and can indicate probability distribution over the set of output labels . In some cases, the output labels are graphemes (e.g., single characters and possibly punctuation and other symbols), but the set of output labels is not so limited. For example, the output tag set can include word blocks and/or whole words in addition to or instead of graphemes. The output profile of the federated network 230 can include a posterior probability value for each of the different output tags. Thus, if there are 100 different output labels representing different graphemes or other symbols, then the output z of the union network 230 u,t 100 different probability values can be included, one for each output tag. The probability distribution can then be used to select scores and assign the scores to candidate positive word elements (e.g., graphemes, word blocks, and/or words) during a beam search (e.g., by Softmax layer 240) to determine the transcription 120.
The Softmax layer 240 may employ any technique to select the output label/symbol in the distribution with the highest probability as the next output symbol 242 predicted by the T-T model 200 at the corresponding output step. Thus, the set of output symbols 242 predicted by the T-T model 200 may also be collectively referred to as the output sequence of tag tokens 242. In this way, the T-T model 200 does not make a condition-independent assumption, but rather the predictions for each symbol are conditioned not only on acoustics, but also on the tag sequences output so far.
To determine the logarithmic conditional probability of y given the audio input x, all alignment distributions corresponding to y are summed as follows.
Wherein the mapping K removes the blank symbols in z. The logarithmic total alignment probability of equation 2 includes a target loss function, which can be effectively calculated using the forward-backward algorithm as follows.
Pr(y|x)＝α(T,U) (3)
α(t,u)＝α(t-1,u-1)Pr(φ|t-1,u)+α(t,u-1)Pr(y u |t,u-1) (4)
Wherein Pr (phi|t-1, u) and Pr (y) u |t,U-1) are blank probability and tag probability, respectively, and T and U are audio sequence length and tag sequence length.
Although fig. 2 depicts a transducer model 200 including a T-T model architecture, the transducer model 200 may also include an RNN-T model architecture, a convolutional neural network-transducer (CNN-transducer) model architecture, a convolutional network-transducer (ConvNet-transducer) model, or a Conformer-transducer model architecture without departing from the scope of the present disclosure. An exemplary CNN-transducer model architecture is described in detail in Contextnet: improving Convolutional Neural Networks for Automatic Speech Recognition with Global Context, https:// arxiv. Org/abs/2005.03191, the contents of which are incorporated herein by reference in their entirety. An exemplary Conformer-transducer model architecture is described in detail in Conformer: convolume-augmented transformer for speech recognition, "https:// arxiv. Org/abs/2005.08100, the contents of which are incorporated herein by reference in their entirety.
The transducer model 200 is trained on a training dataset corresponding to audio data of the spoken utterance paired with the corresponding transcription. Training the transducer model 200 may occur on the remote server 60 and the trained transducer model 200 may be pushed to the user device 10. The transducer model 200 is trained with cross entropy loss based on viterbi forced alignment. The alignment delay includes a delay between the input audio frame and the streamed decoded output tag. Since the conventional model iteratively trains the alignment model with the realigned tags, the conventional model is able to learn accurate alignment after multiple iterations. Accessing the T-T or C-T model of the future frame when self-attention is calculated at the respective transformer or Conformer layer may include alignment delays consistent with conventional models. However, its self-attention depends only on the transducer model in the streaming mode of the past frame experiencing excessive alignment delays.
Embodiments herein are directed to reducing predicted delays in a streaming transducer model 200 by using self-alignment. Notably, self-alignment does not require the use of any external alignment model or blind optimization delay, but rather utilizes reference forced alignment learned from a trained speech recognition model to select the best low-latency direction to reduce delay. The reference forced alignment may include viterbi forced alignment. That is, self-alignment will always locate the path of the viterbi at each time step forced alignment 1 frame to the left in the decoded picture.
Fig. 3 shows a diagram of a decoding diagram 300 for a transducer model 200 having a T-T model architecture for the output sequence of tag token 242 (fig. 2) 'I like it'. The x-axis represents the corresponding acoustic frame at each time step, and the y-axis represents the output tag token 242 (fig. 2). The non-bold solid circles and arrows represent tokens that are not included in any alignment path described below. Constrained alignment path 310 (e.g., represented by the bold circles and bold arrows shown in fig. 3) includes a word boundary threshold equal to 2. Forced alignment path 320 (also referred to as reference forced alignment path 320) (e.g., represented by the dotted circles and dotted arrows shown in fig. 3) is learned from trained transducer model 200, and left side alignment path 330 (e.g., represented by the dotted circles) includes 1 frame to the left of each frame of forced alignment path 320. During training of the transducer model 200, for each training batch, self-alignment encourages a left-side alignment path 330 (e.g., represented by the dashed circle and dashed arrow as shown in fig. 3) by continually pushing the forced alignment path 320 of the model in a left-side direction. The training loss can be expressed as follows.
Where λ is the weighting factor of the left alignment likelihood, t u Is the left aligned frame index at the u-th tag/token.
Fig. 4 shows an example transformer layer 400 among multiple transformer layers of the audio encoder 210. Here, during each time step, the initial transducer layer 400 receives as input a corresponding acoustic frame 110 and generates a corresponding output representation/embedding 450 that is received as input by the next transducer layer 400. That is, each transformer layer 400 after the initial transformer layer 400 may receive an input insert 450 thatThe input 450 corresponds to an output representation/embedding generated as output by the immediately preceding converter layer 400. The final transducer layer 400 (e.g., the last transducer layer in the final stack 320) generates a signal for the corresponding acoustic frame 110 at each of a plurality of time steps (see fig. 2, e.g., by ah t Shown) higher order feature representation 202.
The input to the tag encoder 220 (fig. 2) may include a vector (e.g., a one-hot vector) indicating the sequence y of non-blank symbols that was output so far by the final Softmax layer 240 0 ,...,y ui-1 . Thus, when the tag encoder 220 includes a transformer layer, the initial transformer layer may receive the input embedment 111 by passing the single hot vector through a lookup table.
Each transformer layer 400 of the audio encoder 210 includes a normalization layer 404, a masking multi-headed attention layer 406 with relative position coding, a residual connection 408, a stack/unstacking layer 410, and a feedforward layer 412. Masking the multi-headed attention layer 406 with relative position coding provides a flexible way to control the amount (i.e., duration) of the prospective audio context used by the T-T model 200. Specifically, after normalization layer 404 normalizes acoustic frame 110 and/or input embedding 111, masking multi-headed attention layer 406 projects the input to all head values. Thereafter, the masking multi-headed layer 406 may mask the attention score to the left of the current acoustic frame 110 to produce an output conditioned only on the previous acoustic frame 110. The weighted averages of all the heads are then concatenated and passed to dense layer 2 416, with residual connection 414 added to the normalized inputs and outputs of dense layer 416 to form the final output of multi-headed attention layer 406 with relative position coding. Residual connection 408 is added to the output of normalization layer 404 by adder 430 and is provided as an input to a respective one of masking multi-headed attention layer 406 or feedforward layer 412. The stacking/de-stacking layer 410 can be used to vary the frame rate of each transducer layer 400 to speed training and inference.
The feed forward layer 412 applies the normalization layer 404 followed by the dense layer 1 420, the rectifying linear layer (ReLu) 418, and the dense layer 2 416.ReLu 418 is used as an activation of the output of dense layer 1 420. As with the multi-headed attention layer 406 with relative position coding, the residual connection 414 from the output of the normalization layer 404 is added to the output of the dense layer 2 416 by adder 430.
FIG. 5 includes a flow chart of an example arrangement of operations of a method 500 for training a streaming speech recognition model to reduce prediction delay using self-alignment. At operation 502, the method includes receiving a sequence of acoustic frames 110 corresponding to an utterance 106 as input to a streaming speech recognition model (e.g., transducer model) 200. The streaming speech recognition model 200 is configured to learn alignment probabilities 232 between the sequence of acoustic frames 110 and the output sequence of tag tokens 242. At operation 504, the method 500 includes generating, using the decoding graph 300, the speech recognition result 120 for the utterance 106 that includes the output sequence of tag tokens 242 as output from the streaming speech recognition model 200. At operation 506, the method 500 includes generating speech recognition model loss based on the speech recognition result 120 and the true transcription of the utterance 106.
At operation 508, the method 500 includes obtaining a reference forced alignment path 320 from the decoding graph 300. At operation 510, method 500 includes identifying 1 frame from decoding graph 300 to the left from each reference forced alignment frame in reference forced alignment path 320. At operation 512, the method 500 includes summing tag transition probabilities based on the identified frames to the left from each forced alignment frame. At operation 514, the method 500 includes updating the streaming speech recognition model 200 based on the summation of tag transition probabilities and the speech recognition model penalty.
FIG. 6 is a schematic diagram of an example computing device 600 that may be used to implement the systems and methods described in this document. Computing device 600 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers. The components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the invention described and/or claimed in this document.
Computing device 600 includes a processor 610, memory 620, storage 630, high-speed interface/controller 640 connected to memory 620 and high-speed expansion ports 650, and low-speed interface/controller 660 connected to low-speed bus 670 and storage 630. Each of the components 610, 620, 630, 640, 650, and 660 are interconnected using various buses, and may be mounted on a common motherboard or in other manners as appropriate. The processor 610 is capable of processing instructions for execution within the computing device 600, including instructions stored in the memory 620 or on the storage device 630, to display graphical information for a Graphical User Interface (GUI) on an external input/output device, such as a display 680 coupled to the high-speed interface 640. In other embodiments, multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory. Moreover, multiple computing devices 600 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a set of blade servers, or a multiprocessor system).
Memory 620 stores information non-transitory within computing device 600. Memory 620 may be a computer-readable medium, a volatile memory unit, or a non-volatile memory unit. Non-transitory memory 620 may be a physical device for storing programs (e.g., sequences of instructions) or data (e.g., program state information) for use by computing device 600 on a temporary or permanent basis. Examples of non-volatile memory include, but are not limited to, flash memory and read-only memory (ROM)/programmable read-only memory (PROM)/erasable programmable read-only memory (EPROM)/electrically erasable programmable read-only memory (EEPROM) (e.g., typically for firmware such as a boot program). Examples of volatile memory include, but are not limited to, random Access Memory (RAM), dynamic Random Access Memory (DRAM), static Random Access Memory (SRAM), phase Change Memory (PCM), and optical disks or tapes.
The storage device 630 is capable of providing mass storage for the computing device 600. In some implementations, the storage device 630 is a computer-readable medium. In various different implementations, the storage device 630 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations. In additional implementations, the computer program product is tangibly embodied in an information carrier. The computer program product contains instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer-readable medium or a machine-readable medium, such as the memory 620, the storage device 630, or memory on the processor 610.
The high-speed controller 640 manages bandwidth-intensive operations for the computing device 600, while the low-speed controller 660 manages lower bandwidth-intensive operations. This allocation of responsibilities is merely exemplary. In some implementations, the high-speed controller 640 is coupled to a memory 620, a display 680 (e.g., via a graphics processor or accelerator), and a high-speed expansion port 650, which high-speed expansion port 650 may accept various expansion cards (not shown). In some implementations, a low-speed controller 660 is coupled to the storage device 630 and the low-speed expansion port 690. The low-speed expansion port 690, which may include various communication ports (e.g., USB, bluetooth, ethernet, wireless ethernet), may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device (such as a switch or router), for example, through a network adapter.
As shown, computing device 600 may be implemented in a number of different forms. For example, it may be implemented as a standard server 600a or multiple times in a group of such servers 600a, as a laptop computer 600b, or as part of a rack server system 600 c.
Various implementations of the systems and techniques described here can be implemented in digital electronic and/or optical circuits, integrated circuits, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof. These various embodiments may include embodiments in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
A software application (i.e., a software resource) may refer to computer software that causes a computing device to perform tasks. In some examples, a software application may be referred to as an "application," app, "or" program. Example applications include, but are not limited to, system diagnostic applications, system management applications, system maintenance applications, word processing applications, spreadsheet applications, messaging applications, media streaming applications, social networking applications, and gaming applications.
Non-transitory memory may be a physical device for storing programs (e.g., sequences of instructions) or data (e.g., program state information) for use by a computing device on a temporary or permanent basis. The non-transitory memory may be volatile and/or non-volatile addressable semiconductor memory. Examples of non-volatile memory include, but are not limited to, flash memory and read-only memory (ROM)/programmable read-only memory (PROM)/erasable programmable read-only memory (EPROM)/electrically erasable programmable read-only memory (EEPROM) (e.g., typically for firmware such as a boot program). Examples of volatile memory include, but are not limited to, random Access Memory (RAM), dynamic Random Access Memory (DRAM), static Random Access Memory (SRAM), phase Change Memory (PCM), and optical disks or tapes.
These computer programs (also known as programs, software applications or code) include machine instructions for a programmable processor, and can be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. The terms "machine-readable medium" and "computer-readable medium" as used herein refer to any computer program product, non-transitory computer-readable medium, apparatus and/or device (e.g., magnetic discs, optical disks, memory, programmable Logic Devices (PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term "machine-readable signal" refers to any signal used to provide machine instructions and/or data to a programmable processor.
The processes and logic flows described in this specification can be performed by one or more programmable processors (also referred to as data processing hardware) executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, and special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Typically, a processor will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a processor for executing instructions and one or more memory devices for storing instructions and data. Typically, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, a computer need not have such a device. Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices (e.g., EPROM, EEPROM, and flash memory devices); magnetic disks (e.g., internal hard disks or removable disks); magneto-optical disk; CD ROM and DVD-ROM discs. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
To provide for interaction with a user, one or more aspects of the present disclosure may be implemented on a computer having: a display device for displaying information to a user, such as a CRT (cathode ray tube), LCD (liquid crystal display) monitor or touch screen; and optionally a keyboard and pointing device (e.g., a mouse or trackball) by which a user can provide input to the computer. Other kinds of devices can also be used to provide for interaction with a user; for example, feedback provided to the user can be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic input, speech input, or tactile input. In addition, the computer can send and receive documents to and from the device used by the user; for example, by sending a web page to a web browser on a user's client device in response to a request received from the web browser.
Many embodiments have been described. Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the disclosure. Accordingly, other implementations are within the scope of the following claims.
Claims (20)
1. A streaming speech recognition model (200), comprising:
an audio encoder (210), the audio encoder configured to:
receiving as input a sequence of acoustic frames (110); and
generating, at each of a plurality of time steps, a high-order feature representation (202) for a corresponding acoustic frame (110) in the sequence of acoustic frames (110);
a tag encoder (220) configured to:
receiving as input a sequence of non-blank symbols (242) output by the final softmax layer (240); and
generating a dense representation (222) at each of the plurality of time steps; and
a federated network (230), the federated network configured to:
receiving as inputs the high-order feature representation (202) generated by the audio encoder (210) at each of the plurality of time steps and the dense representation (222) generated by the tag encoder (220) at each of the plurality of time steps; and
at each of the plurality of time steps, generating a probability distribution over possible speech recognition hypotheses at the corresponding time step (232),
wherein the streaming speech recognition model (200) is trained using self-alignment to reduce the prediction delay by encouraging an alignment path of 1 frame to the left of the reference forced alignment frame at each time step for each training batch.
2. The speech recognition model (200) according to claim 1, wherein the streaming speech recognition model (200) comprises a transducer-transducer model.
3. The speech recognition model (200) of claim 2, wherein the audio encoder (210) comprises a stack of transducer layers (400), each transducer layer (400) comprising:
a normalization layer (404);
a masking multi-headed attention layer (406) having relative position coding;
residual connection (408);
stacking/de-stacking layers (410); and
a feed-forward layer (412).
4. The speech recognition model (200) of claim 3, wherein the stacking/de-stacking layer (410) is configured to change a frame rate of a corresponding transducer layer (400) to adjust a processing time of the transducer-transducer model during training and inference.
5. The speech recognition model (200) according to any one of claims 2-4, wherein the tag encoder (220) comprises a stack of transformer layers (400), each transformer layer (400) comprising:
a normalization layer (404);
a masking multi-headed attention layer (406) having relative position coding;
residual connection (408);
stacking/de-stacking layers (410); and
a feed-forward layer (412).
6. The speech recognition model (200) of any of claims 1-5, wherein the tag encoder (220) comprises a binary embedded look-up decoder model.
7. The speech recognition model (200) according to any one of claims 1-6, wherein the streaming speech recognition model (200) comprises one of:
a recurrent neural transducer (RNN-T) model;
a transducer-transducer model;
a convolutional network-transducer (ConvNet-transducer) model; or alternatively
Conformer-transducer model.
8. The speech recognition model (200) of any of claims 1-7, wherein training the streaming speech recognition model (200) using self-alignment to reduce prediction delay comprises: the alignment of the decoding graph (300) is constrained using self-alignment without using any external aligner model.
9. The speech recognition model (200) according to any one of claims 1-8, wherein the streaming speech recognition model (200) is executed on a user device (10) or a server (60).
10. The speech recognition model (200) according to any one of claims 1-9, wherein each acoustic frame (110) in the sequence of acoustic frames (110) comprises a dimensional feature vector.
11. A computer-implemented method (500) that, when executed on data processing hardware (12), causes the data processing hardware (12) to perform operations for training a streaming speech recognition model (200) using self-alignment to reduce prediction delay, the operations comprising:
Receiving as input to the streaming speech recognition model (200) a sequence of acoustic frames (110) corresponding to an utterance (106), the streaming speech recognition model (200) being configured to learn an alignment probability between the sequence of acoustic frames (110) and an output sequence of tag tokens (242);
generating a speech recognition result (120) for the utterance (106) using a decoding graph (300) as output from the streaming speech recognition model (200), the speech recognition result (120) comprising an output sequence of tag tokens (242);
generating a speech recognition model penalty based on the actual transcription of the utterance (106) and the speech recognition result (120);
obtaining a reference forced alignment path (320) comprising a reference forced alignment frame from the decoding graph (300);
identifying 1 frame from the decoding graph (300) to the left from each reference forced alignment frame in the reference forced alignment path (320);
summing tag transition probabilities based on the identified frames to the left from each forced alignment frame in the reference forced alignment path (320); and
the streaming speech recognition model (200) is updated based on the summation of the tag transition probabilities and the speech recognition model loss.
12. The computer-implemented method (500) of claim 11, wherein the operations further comprise:
generating, by an audio encoder (210) of the streaming speech recognition model (200), a high-order feature representation (202) for a corresponding acoustic frame (110) in the sequence of acoustic frames (110) at each of a plurality of time steps;
receiving as input to a tag encoder (220) of the streaming speech recognition model (200) a sequence of non-blank symbols (242) output by a final softmax layer (240);
generating, by the tag encoder (220), a dense representation (222) at each of the plurality of time steps;
receiving as input to a federated network (230) of the streaming speech recognition model (200) the high-order feature representation (202) generated by the audio encoder (210) at each of the plurality of time steps and the dense representation (222) generated by the tag encoder (220) at each of the plurality of time steps; and
a probability distribution (232) over possible speech recognition hypotheses at each of the plurality of time steps is generated by the federated network (230) at the corresponding time step.
13. The computer-implemented method (500) of claim 12, wherein the tag encoder (220) comprises a stack of transformer layers (400), each transformer layer (400) comprising:
A normalization layer (404);
a masking multi-headed attention layer (406) having relative position coding;
residual connection (408);
stacking/de-stacking layers (410); and
a feed-forward layer (412).
14. The computer-implemented method (500) of claim 12 or 13, wherein the tag encoder (220) comprises a binary embedded look-up decoder model.
15. The computer-implemented method (500) of any of claims 11-14, wherein the streaming speech recognition model (200) comprises a transducer-transducer model.
16. The computer-implemented method (500) of claim 15, wherein the audio encoder (210) comprises a stack of transducer layers (400), each transducer layer (400) comprising:
a normalization layer (404);
a masking multi-headed attention layer (406) having relative position coding;
residual connection (408);
stacking/de-stacking layers (410); and
a feed-forward layer (412).
17. The computer-implemented method (500) of claim 16, wherein the stacking/de-stacking layer (410) is configured to change a frame rate of a corresponding transducer layer (400) to adjust a processing time of the transducer-transducer model during training and inference.
18. The computer-implemented method (500) of any of claims 11-17, wherein the streaming speech recognition model (200) comprises one of:
a recurrent neural transducer (RNN-T) model;
a transducer-transducer model;
a convolutional network-transducer (ConvNet-transducer) model; or alternatively
Conformer-transducer model.
19. The computer-implemented method (500) of any of claims 11-18, wherein the streaming speech recognition model (200) is executed on a user device (10) or a server (60).
20. The computer-implemented method (500) of any of claims 11-19, wherein the operations further comprise: the streaming speech recognition model (200) is trained using self-alignment to reduce prediction delay without using any external aligner model to constrain alignment of the decoded pictures (300).
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202163166943P | 2021-03-26 | 2021-03-26 | |
US63/166,943 | 2021-03-26 | ||
PCT/US2021/063465 WO2022203735A1 (en) | 2021-03-26 | 2021-12-15 | Reducing streaming asr model delay with self alignment |
Publications (1)
Publication Number | Publication Date |
---|---|
CN117083668A true CN117083668A (en) | 2023-11-17 |
Family
ID=80168120
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202180096433.XA Pending CN117083668A (en) | 2021-03-26 | 2021-12-15 | Reducing streaming ASR model delay using self-alignment |
Country Status (6)
Country | Link |
---|---|
US (1) | US20220310097A1 (en) |
EP (1) | EP4295356A1 (en) |
JP (1) | JP2024512606A (en) |
KR (1) | KR20230156425A (en) |
CN (1) | CN117083668A (en) |
WO (1) | WO2022203735A1 (en) |
Families Citing this family (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN114627874A (en) * | 2021-06-15 | 2022-06-14 | 宿迁硅基智能科技有限公司 | Text alignment method, storage medium and electronic device |
Family Cites Families (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
KR101217525B1 (en) * | 2008-12-22 | 2013-01-18 | 한국전자통신연구원 | Viterbi decoder and method for recognizing voice |
WO2014025682A2 (en) * | 2012-08-07 | 2014-02-13 | Interactive Intelligence, Inc. | Method and system for acoustic data selection for training the parameters of an acoustic model |
-
2021
- 2021-12-15 KR KR1020237035507A patent/KR20230156425A/en active Search and Examination
- 2021-12-15 US US17/644,377 patent/US20220310097A1/en active Pending
- 2021-12-15 CN CN202180096433.XA patent/CN117083668A/en active Pending
- 2021-12-15 JP JP2023558844A patent/JP2024512606A/en active Pending
- 2021-12-15 EP EP21852085.6A patent/EP4295356A1/en active Pending
- 2021-12-15 WO PCT/US2021/063465 patent/WO2022203735A1/en active Application Filing
Also Published As
Publication number | Publication date |
---|---|
EP4295356A1 (en) | 2023-12-27 |
US20220310097A1 (en) | 2022-09-29 |
KR20230156425A (en) | 2023-11-14 |
WO2022203735A1 (en) | 2022-09-29 |
JP2024512606A (en) | 2024-03-19 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11741947B2 (en) | Transformer transducer: one model unifying streaming and non-streaming speech recognition | |
US11929060B2 (en) | Consistency prediction on streaming sequence models | |
US20220122622A1 (en) | Cascaded Encoders for Simplified Streaming and Non-Streaming ASR | |
JP7351018B2 (en) | Proper noun recognition in end-to-end speech recognition | |
US20230104228A1 (en) | Joint Unsupervised and Supervised Training for Multilingual ASR | |
CN117378004A (en) | Supervised and unsupervised training with loss of alignment of sequences | |
US11715458B2 (en) | Efficient streaming non-recurrent on-device end-to-end model | |
CN117063228A (en) | Mixed model attention for flexible streaming and non-streaming automatic speech recognition | |
CN117099157A (en) | Multitasking learning for end-to-end automatic speech recognition confidence and erasure estimation | |
US20230352006A1 (en) | Tied and reduced rnn-t | |
US20220310097A1 (en) | Reducing Streaming ASR Model Delay With Self Alignment | |
US20230107695A1 (en) | Fusion of Acoustic and Text Representations in RNN-T | |
US20230298569A1 (en) | 4-bit Conformer with Accurate Quantization Training for Speech Recognition | |
US20240135923A1 (en) | Universal Monolingual Output Layer for Multilingual Speech Recognition | |
US20220310081A1 (en) | Multilingual Re-Scoring Models for Automatic Speech Recognition | |
US20230306958A1 (en) | Streaming End-to-end Multilingual Speech Recognition with Joint Language Identification | |
US20230326461A1 (en) | Unified Cascaded Encoder ASR model for Dynamic Model Sizes | |
US20220310061A1 (en) | Regularizing Word Segmentation | |
US20230298570A1 (en) | Rare Word Recognition with LM-aware MWER Training |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |