JP7486464B2 - A platform for registering and processing visual coding - Google Patents
A platform for registering and processing visual coding Download PDFInfo
- Publication number
- JP7486464B2 JP7486464B2 JP2021152253A JP2021152253A JP7486464B2 JP 7486464 B2 JP7486464 B2 JP 7486464B2 JP 2021152253 A JP2021152253 A JP 2021152253A JP 2021152253 A JP2021152253 A JP 2021152253A JP 7486464 B2 JP7486464 B2 JP 7486464B2
- Authority
- JP
- Japan
- Prior art keywords
- machine
- readable visual
- image data
- readable
- encoding
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 230000000007 visual effect Effects 0.000 title claims description 361
- 238000012545 processing Methods 0.000 title claims description 55
- 238000000034 method Methods 0.000 claims description 108
- 230000008569 process Effects 0.000 claims description 65
- 230000009471 action Effects 0.000 claims description 18
- 238000012795 verification Methods 0.000 claims description 15
- 230000004044 response Effects 0.000 claims description 13
- 238000003860 storage Methods 0.000 claims description 9
- 238000010200 validation analysis Methods 0.000 claims description 9
- 239000002131 composite material Substances 0.000 claims description 5
- 230000000977 initiatory effect Effects 0.000 claims description 4
- 230000026676 system process Effects 0.000 claims description 4
- 238000003384 imaging method Methods 0.000 description 31
- 238000013507 mapping Methods 0.000 description 16
- 230000015654 memory Effects 0.000 description 16
- 238000010586 diagram Methods 0.000 description 13
- 238000013528 artificial neural network Methods 0.000 description 12
- 238000012549 training Methods 0.000 description 11
- 238000009877 rendering Methods 0.000 description 10
- 230000000295 complement effect Effects 0.000 description 6
- 238000004891 communication Methods 0.000 description 5
- 238000012986 modification Methods 0.000 description 5
- 230000004048 modification Effects 0.000 description 5
- 230000002093 peripheral effect Effects 0.000 description 5
- 238000000701 chemical imaging Methods 0.000 description 4
- 230000006870 function Effects 0.000 description 4
- 238000007781 pre-processing Methods 0.000 description 4
- 230000011218 segmentation Effects 0.000 description 4
- 238000004458 analytical method Methods 0.000 description 3
- 230000001815 facial effect Effects 0.000 description 3
- 239000000463 material Substances 0.000 description 3
- 230000000306 recurrent effect Effects 0.000 description 3
- 230000003595 spectral effect Effects 0.000 description 3
- 230000003190 augmentative effect Effects 0.000 description 2
- 238000013527 convolutional neural network Methods 0.000 description 2
- 238000012937 correction Methods 0.000 description 2
- 238000009826 distribution Methods 0.000 description 2
- 230000000694 effects Effects 0.000 description 2
- 238000005516 engineering process Methods 0.000 description 2
- 230000007613 environmental effect Effects 0.000 description 2
- 238000005259 measurement Methods 0.000 description 2
- 238000012015 optical character recognition Methods 0.000 description 2
- 230000003287 optical effect Effects 0.000 description 2
- 238000004806 packaging method and process Methods 0.000 description 2
- 230000008447 perception Effects 0.000 description 2
- 238000007639 printing Methods 0.000 description 2
- 238000001228 spectrum Methods 0.000 description 2
- 238000013519 translation Methods 0.000 description 2
- 238000012800 visualization Methods 0.000 description 2
- 230000001133 acceleration Effects 0.000 description 1
- 230000006978 adaptation Effects 0.000 description 1
- 238000007792 addition Methods 0.000 description 1
- 230000004075 alteration Effects 0.000 description 1
- 230000004888 barrier function Effects 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 238000006243 chemical reaction Methods 0.000 description 1
- 239000003086 colorant Substances 0.000 description 1
- 230000001143 conditioned effect Effects 0.000 description 1
- 238000013461 design Methods 0.000 description 1
- 238000001514 detection method Methods 0.000 description 1
- 239000000284 extract Substances 0.000 description 1
- 239000011521 glass Substances 0.000 description 1
- 238000003709 image segmentation Methods 0.000 description 1
- 238000010348 incorporation Methods 0.000 description 1
- 238000010801 machine learning Methods 0.000 description 1
- 238000004519 manufacturing process Methods 0.000 description 1
- 230000006403 short-term memory Effects 0.000 description 1
- 230000000153 supplemental effect Effects 0.000 description 1
- 230000002123 temporal effect Effects 0.000 description 1
- 230000001960 triggered effect Effects 0.000 description 1
Images
Description
本開示は概して、視覚符号化に関する。より詳細には、本開示は、視覚符号化の登録および/または処理のためのプラットフォームに関する。 The present disclosure relates generally to visual coding. More particularly, the present disclosure relates to a platform for registering and/or processing visual coding.
視覚パターンは、視覚符号化の形で情報を符号化するために使用されている。たとえば、バーコードは、一般に、店舗内で製品に関する情報を通信するために使用される。そのようなパターンで符号化された情報の検索は、一般に、(たとえば、バーコードのバーを検出し処理するために)パターンの一定の特徴を見分けるのに十分な鋭敏さで撮像することを必要とする。いくつかの実際の適用例では、パターンを解釈するために十分な詳細でそのパターンの画像を取得することは、多くの課題を提示する。加えて、視覚パターンは、常に、ユーザによって解釈可能であるとは限らないので、ユーザは、不正な視覚パターンに関連する悪意のある活動を受けやすい可能性がある。 Visual patterns have been used to encode information in the form of visual coding. For example, bar codes are commonly used in stores to communicate information about products. Retrieval of information encoded in such patterns generally requires imaging with sufficient acuity to discern certain features of the pattern (e.g., to detect and process the bars of the bar code). In some practical applications, obtaining an image of the pattern with sufficient detail to interpret the pattern presents many challenges. In addition, visual patterns are not always interpretable by users, making users susceptible to malicious activity associated with fraudulent visual patterns.
本開示の実施形態の態様および利点が、以下の記述において部分的に説明され、または記述から学ぶことができ、または実施形態の実践を通して知ることができる。 Aspects and advantages of embodiments of the present disclosure are set forth in part in the description that follows, or may be learned from the description, or may be learned by practice of the embodiments.
本開示の1つの例示的態様は、機械可読視覚符号化を処理するためのコンピュータ実装方法を対象とする。この方法は、1つまたは複数のコンピューティングデバイスを備えたコンピューティングシステムによって、機械可読視覚符号化を含むシーンを記述する画像データを取得するステップを含む。この方法は、コンピューティングシステムによって、機械可読視覚符号化を認識するように構成された第1の認識システムを用いて画像データを処理するステップを含む。この方法は、コンピューティングシステムによって、機械可読視覚符号化を囲むシーンの周囲部分を認識するように構成された第2の異なる認識システムを用いて画像データを処理するステップを含む。この方法は、コンピューティングシステムによって、画像データに基づいて第1の認識システムによって生成された1つまたは複数の第1の出力に少なくとも部分的に基づいて、かつ画像データに基づいて第2の認識システムによって生成された1つまたは複数の第2の出力に少なくとも部分的に基づいて、機械可読視覚符号化に関連する、記憶された基準を識別するステップを含む。この方法は、コンピューティングシステムが、記憶された基準の識別に応じて、1つまたは複数のアクションを実行するステップを含む。 One exemplary aspect of the present disclosure is directed to a computer-implemented method for processing a machine-readable visual encoding. The method includes obtaining, by a computing system having one or more computing devices, image data describing a scene including the machine-readable visual encoding. The method includes processing, by the computing system, the image data with a first recognition system configured to recognize the machine-readable visual encoding. The method includes processing, by the computing system, the image data with a second, different recognition system configured to recognize a surrounding portion of the scene surrounding the machine-readable visual encoding. The method includes identifying, by the computing system, stored criteria associated with the machine-readable visual encoding based at least in part on one or more first outputs generated by the first recognition system based on the image data and based at least in part on one or more second outputs generated by the second recognition system based on the image data. The method includes performing, by the computing system, one or more actions in response to identifying the stored criteria.
本開示の別の例示的態様は、機械可読視覚符号化の認識に応じてアクションを実行するためのコンピューティングシステムを対象とする。このコンピューティングシステムは、1つまたは複数のプロセッサと、命令をまとめて記憶する1つまたは複数の非一時的コンピュータ可読媒体とを含み、これらの命令は、1つまたは複数のプロセッサによって実行されると、コンピューティングシステムに動作を実行させる。これらの動作は、機械可読視覚符号化を含むシーンを記述する画像データを取得することを含む。これらの動作は、認識サーバシステムによって処理するために画像データを認識サーバシステムに送信することであって、認識サーバシステムが、機械可読視覚符号化を認識するように構成された第1の認識システムと、機械可読視覚符号化を囲むシーンの周囲部分を認識するように構成された第2の異なる認識システムとを備える、送信することを含む。これらの動作は、記憶された基準と画像データとの間の関連付けに少なくとも部分的に基づいて、1つまたは複数のアクションを実行することであって、関連付けが、第1の認識システムによって生成された1つまたは複数の第1の出力を使用して、かつ第2の認識システムによって生成された1つまたは複数の第2の出力を使用して、認識サーバシステムによって判定されている、実行することを含む。 Another exemplary aspect of the present disclosure is directed to a computing system for performing actions in response to recognition of a machine-readable visual encoding. The computing system includes one or more processors and one or more non-transitory computer-readable media collectively storing instructions that, when executed by the one or more processors, cause the computing system to perform operations. The operations include obtaining image data describing a scene including the machine-readable visual encoding. The operations include transmitting the image data to a recognition server system for processing by the recognition server system, the recognition server system comprising a first recognition system configured to recognize the machine-readable visual encoding and a second, different recognition system configured to recognize a surrounding portion of the scene surrounding the machine-readable visual encoding. The operations include performing one or more actions based at least in part on an association between the stored criteria and the image data, the association being determined by the recognition server system using one or more first outputs generated by the first recognition system and using one or more second outputs generated by the second recognition system.
本開示の別の例示的態様は、命令をまとめて記憶する1つまたは複数の非一時的コンピュータ可読媒体を対象とし、これらの命令は、1つまたは複数のプロセッサによって実行されると、1つまたは複数のプロセッサに動作を実行させる。これらの動作は、機械可読視覚符号化を含むシーンを記述する画像データを受信することであって、画像データが、機械可読視覚符号化のコンテキストをさらに記述する、受信することを含む。これらの動作は、画像データを記憶された基準に関連付けることを含む。関連付けることは、第1の認識システムを用いて、機械可読視覚符号化と記憶された基準との間の第1の類似性を判定することと、第2の認識システムを用いて、コンテキストと記憶された基準との間の第2の類似性を判定することとを含む。これらの動作は、前記関連付けに基づいて、1つまたは複数の動作を開始することを含む。 Another exemplary aspect of the present disclosure is directed to one or more non-transitory computer-readable media collectively storing instructions that, when executed by one or more processors, cause the one or more processors to perform operations. The operations include receiving image data describing a scene including a machine-readable visual encoding, the image data further describing a context of the machine-readable visual encoding. The operations include associating the image data with a stored reference. The associating includes determining, with a first recognition system, a first similarity between the machine-readable visual encoding and the stored reference, and determining, with a second recognition system, a second similarity between the context and the stored reference. The operations include initiating one or more operations based on the association.
本開示の他の態様は、様々なシステム、装置、非一時的コンピュータ可読媒体、ユーザインターフェース、および電子デバイスを対象とする。 Other aspects of the present disclosure are directed to various systems, apparatus, non-transitory computer-readable media, user interfaces, and electronic devices.
本開示の様々な実施形態のこれらおよび他の特徴、態様、および利点は、以下の説明および添付の特許請求の範囲を参照してより良く理解されよう。添付の図面は、本明細書に組み込まれるとともにその一部を成し、本開示の例示的実施形態を示し、説明とともに、関連原理を説明するのに役立つ。 These and other features, aspects, and advantages of various embodiments of the present disclosure will become better understood with reference to the following description and appended claims. The accompanying drawings, which are incorporated in and form a part of this specification, illustrate exemplary embodiments of the present disclosure and, together with the description, serve to explain relevant principles.
当業者を対象とする、実施形態の詳細な考察が本明細書において説明され、本明細書は、添付の図面を参照する。 A detailed discussion of embodiments, directed to those skilled in the art, is described herein, and this specification refers to the accompanying drawings.
複数の図面にわたって繰り返される参照番号は、様々な実装形態において同じ特徴を識別することを意図している。 Reference numbers repeated across multiple drawings are intended to identify the same features in various implementations.
概要
概して、本開示は、コンテキスト情報に鑑みた視覚符号化の処理を対象とする。コンテキスト情報は、いくつかの実施形態では、視覚符号化の認識、識別、および/または処理を支援するために使用され得る。本開示の態様によるコンテキスト情報の使用は、有利には、視覚符号化を認識および処理するためのロバストなプラットフォームを提供し得る。
In general, the present disclosure is directed to processing visual coding in light of contextual information. The contextual information may be used in some embodiments to aid in the recognition, identification, and/or processing of visual coding. The use of contextual information according to aspects of the present disclosure may advantageously provide a robust platform for recognizing and processing visual coding.
本開示の1つの例示的態様は、(たとえば、実世界におけるアクションポイントの呼出しに対応し得る)視覚符号化を処理または認識するために複数のデータ信号および/または複数の処理システムを使用するコンピューティングシステムおよび方法を対象とする。それぞれ、異なるデータ信号上でかつ/または異なる処理技法で動作する、複数のシステムの使用は、システムが幅広い異なる距離範囲において視覚符号化を効果的に認識することを可能にし得る。たとえば、広い表面上またはそうでなければ特定の環境またはシーンの中に配置された視覚符号化の場合、ユーザは、視覚符号化が視界の外にある、または完全にまたは部分的に覆い隠されている状況を含めて、視覚符号化から様々な距離において、たとえば、クローズアップビューから数メートルまたは何メートルも離れた状態まで、像をキャプチャし得る。一般的な視覚符号化処理システムは、画像距離におけるこの分散を処理することができない。この課題を解決するために、本開示のいくつかの例示的実装形態は、視覚符号化を処理するために(たとえば、拡張現実経験を生じさせるかまたはロック解除するために)協働的に(たとえば、並列にかつ/または直列に)動作する複数の異なる認識器を活用する。一例として、いくつかの例示的システムは、3つの異なる認識システム、すなわち、視覚符号化を直接的に処理する近距離視覚符号化リーダ、符号化の周囲エリア内に含まれることが知られている意味論的エンティティ(semantic entities)、オブジェクト、および/または他のコンテンツを認識する画像認識ベースシステム、および視覚的または空間的特徴に基づいてロケーションを認識することが可能な視覚測位システムを採用し得る。他の例示的システムは、時刻、ロケーション(たとえば、GPSシステムによって提供される)など、様々な他の形のコンテキストデータに基づいて動作し得る。いくつかの例示的システムは、視覚符号化が最初に検出されるとき動作するように上記のシステムをトリガする低電力デジタル信号プロセッサを特色とすることもある。これらのシステムは各々、異なる距離において最高性能を提供し得る。たとえば、近距離リーダは、近距離(たとえば、10センチメートル)において比較的最高性能を提供し得、画像認識ベースシステムは、中距離(たとえば、10メートル)において比較的最高性能を提供し得、視覚測位システムは、より長距離において比較的最高性能を提供し得る。様々な論理アルゴリズムまたはトリアージ(triaging)アルゴリズムは、複数のシステムから協働動作を提供するために使用され得る。一例として、最も効率的なシステムが最初に動作し得、そのようなシステムが符号化を認識することができない場合、次に最も効率的なシステムがトリガされ得る。別の例として、複数のシステムは、並行して動作することができ、その結果は、組み合わされてよい(たとえば、投票(voting)/信頼性または第1認識(first-to-recognize)パラダイムによって)。複数の異なる認識システムをこのように使用することは、改善された精度および効率性を備えた視覚符号化処理を可能にし得る。たとえば、視覚測位システムの使用は、コンピューティングシステムが、複数の異なるロケーションに配置される同じ視覚符号化(たとえば、広く分散された映画ポスター上に存在する視覚符号化)同士の間の曖昧さを除去することを可能にする。 One exemplary aspect of the present disclosure is directed to computing systems and methods that use multiple data signals and/or multiple processing systems to process or recognize visual coding (e.g., that may correspond to the invocation of an action point in the real world). The use of multiple systems, each operating on different data signals and/or with different processing techniques, may enable the system to effectively recognize the visual coding at a wide range of different distances. For example, for visual coding located on a large surface or otherwise within a particular environment or scene, a user may capture images at various distances from the visual coding, e.g., from a close-up view to several or many meters away, including situations where the visual coding is out of view or fully or partially obscured. Typical visual coding processing systems cannot handle this variance in image distance. To solve this problem, some exemplary implementations of the present disclosure leverage multiple different recognizers operating cooperatively (e.g., in parallel and/or serially) to process the visual coding (e.g., to generate or unlock an augmented reality experience). As an example, some exemplary systems may employ three different recognition systems: a near-range visual coding reader that processes the visual coding directly; an image recognition-based system that recognizes semantic entities, objects, and/or other content known to be contained within the surrounding area of the coding; and a visual positioning system capable of recognizing location based on visual or spatial features. Other exemplary systems may operate based on various other forms of contextual data, such as time of day, location (e.g., provided by a GPS system), etc. Some exemplary systems may also feature a low-power digital signal processor that triggers the above systems to operate when the visual coding is first detected. Each of these systems may provide best performance at different distances. For example, a near-range reader may provide relatively best performance at close ranges (e.g., 10 centimeters), an image recognition-based system may provide relatively best performance at medium ranges (e.g., 10 meters), and a visual positioning system may provide relatively best performance at longer ranges. Various logic or triaging algorithms may be used to provide cooperative operation from multiple systems. As an example, the most efficient system may operate first, and if such system fails to recognize the encoding, the next most efficient system may be triggered. As another example, multiple systems may operate in parallel, and their results may be combined (e.g., by voting/confidence or first-to-recognize paradigms). Using multiple different recognition systems in this way may enable visual encoding processing with improved accuracy and efficiency. For example, the use of a visual positioning system may enable a computing system to remove ambiguity between the same visual encoding that is located in multiple different locations (e.g., visual encoding present on widely distributed movie posters).
本開示の別の例示的態様は、コンテキストデータを使用したロケーションベースの支払不正保護を対象とする。具体的には、視覚符号化の1つの例示的適用例は、支払を可能にするためである。しかしながら、いくつかの視覚符号化(たとえば、QRコード(登録商標))は、公衆プロトコルに従い、誰もが人間の閲覧者にとって同一に見えるコードを生成することを可能にする。したがって、悪意のある当事者は、似たような視覚符号化を印刷し、それを既存の商用符号化に対して配置させることが可能である。悪意のある符号化は、符号化を走査するユーザを不正支払ポータルに再ルーティングし得る。この課題を解決するために、本開示の例示的実装形態は、コンテキスト情報を使用して、視覚符号化を検証し、かつ/またはユーザが不正符号化に対して符号化ベースの支払を行うことを遅らせるかまたは阻止し得る。具体的には、上記で説明したように、コンピューティングシステムは、その各々が、異なる信号上でかつ/または異なる処理技法を使用して動作する、複数の異なる処理システムを含み、かつこれらを使用することができる。したがって、本開示の例示的実装形態は、たとえば、符号化を囲む視覚特徴ポイント、周囲の意味論的エンティティデータ、GPSデータなどのロケーションデータ、環境雑音(たとえば、高速雑音、支払端末雑音)、および/または他のコンテキスト情報を含めて、視覚符号化をそのような異なる信号または情報のうちのいくつかまたはすべてに関連付けることを可能にする。その後、視覚符号化が走査されるときはいつでも、これらの追加データポイントは評価され、視覚符号化に関連付けられ得る。これは、不正符号化を検出するために後で使用され得るテクスチャの各視覚符号化に対する検証を追加する。具体的には、一例として、視覚符号化が最初に走査され、コンテキストデータがそのような視覚符号化に対して確認できないとき、システムは、先に進む前にユーザに警告を提示し得る。別の例として、新しい視覚符号化が走査され、既存のコーディングにマッチするコンテキスト情報を有する場合、新しい視覚符号化は阻止され得るか、または追加の検証ルーチンの対象となり得る。したがって、上記で与えられた例を続けると、特定のロケーションまたは事業において、認証され登録された視覚符号化に対して、コンテキスト「フィンガープリント」が経時的に生成され得る。その場合、悪意のある当事者が既存のコーディングをカバーまたは置換することを探求する場合、不正符号化に対するコンテキスト「フィンガープリント」は、既存の符号化にマッチすることになり、1つまたは複数の不正防止技法をトリガする。 Another exemplary aspect of the present disclosure is directed to location-based payment fraud protection using contextual data. Specifically, one exemplary application of visual coding is to enable payments. However, some visual codings (e.g., QR codes) follow public protocols, allowing anyone to generate codes that appear identical to a human viewer. Thus, a malicious party could print a similar visual coding and have it placed against an existing commercial coding. The malicious coding could reroute a user who scans the coding to a fraudulent payment portal. To solve this problem, an exemplary implementation of the present disclosure could use contextual information to validate the visual coding and/or delay or prevent a user from making a coding-based payment against the fraudulent coding. Specifically, as described above, a computing system can include and use multiple different processing systems, each of which operates on different signals and/or using different processing techniques. Thus, exemplary implementations of the present disclosure allow for visual coding to be associated with some or all of such different signals or information, including, for example, visual feature points surrounding the coding, surrounding semantic entity data, location data such as GPS data, environmental noise (e.g., high speed noise, payment terminal noise), and/or other contextual information. These additional data points can then be evaluated and associated with the visual coding whenever the visual coding is scanned. This adds validation to each visual coding of texture that can later be used to detect fraudulent coding. Specifically, as an example, when a visual coding is first scanned and the contextual data cannot be verified for such visual coding, the system may present a warning to the user before proceeding. As another example, if a new visual coding is scanned and has contextual information that matches an existing coding, the new visual coding may be blocked or subject to additional validation routines. Thus, continuing with the example given above, a context "fingerprint" may be generated over time for authenticated and registered visual codings at a particular location or business. In that case, if a malicious party seeks to cover or replace the existing coding, the context "fingerprint" for the tampered coding will match the existing coding, triggering one or more anti-tamper techniques.
本開示の別の例示的態様は、コンテキストデータを使用して改善されたオブジェクト(たとえば、製品)認識を可能にする技法を対象とする。具体的には、バーコードの使用は、長年かけて実証済みの技術である。しかしながら、結果を見出すためにその技術が使用するインデックスは、しばしば、不正または不完全である。標準化バーコードの場合ですら、カバレージは80%程度であることが多い。この課題を解決するために、本開示の例示的実装形態は、ユーザが視覚符号化の走査を試行するとき、オブジェクト(たとえば、返品結果を返す)を認識するための補足経路として視覚符号化を囲む視覚情報(たとえば、ならびに他のコンテキスト信号)を使用し得る。たとえば、視覚符号化(たとえば、バーコード)を含む製品パッケージングの画像を考慮されたい。視覚符号化が関連するインデックス内に効果的に登録されていない場合、一般的なシステムは、誤った結果を返すことがあるか、または何の結果も返さないことがある。しかしながら、本明細書で説明する例示的システムは、オブジェクトを識別するために他の特徴を使用し得る。一例として、パッケージング上に存在する製品のテキストまたは像を認識することができ、テキストまたは像は、次いで、製品または一般的なウェブクエリとして探索され得る。別の例では、視覚符号化または周囲シーンの複数の画像は、製品のより完全な理解を形成し、インデックスを更新するように「セッション」において共に接合され得る。具体的には、ユーザが、インデックス内には存在しない視覚符号化を走査したが、コンテキスト信号が関連するオブジェクト/製品を認識するために使用されている場合、関連するオブジェクト/製品および視覚符号化は、インデックスに追加され得る。したがって、符号化をインデックスのオブジェクトにマッピングすることは、匿名化されたアグリゲートユーザ関与に基づいて、経時的に発展し得るか、または補足され得る。提案するシステムはまた、視覚符号化に対する複数の潜在的な解決策同士の間の曖昧さを除去するために動作し得る。たとえば、時として、適合しない製品符号化は、異なる製品に対して同じIDを含むことになる。したがって、いくつかの例示的実装形態は、視覚符号化を囲む視覚特徴をキャプチャし、たとえば、競合するIDマップを区別するために、その視覚特徴を二次的識別情報として使用し得る。 Another exemplary aspect of the present disclosure is directed to techniques that enable improved object (e.g., product) recognition using contextual data. In particular, the use of barcodes is a time-tested technology. However, the indexes that the technology uses to find results are often incorrect or incomplete. Even for standardized barcodes, coverage is often around 80%. To solve this problem, exemplary implementations of the present disclosure may use the visual information (e.g., as well as other contextual signals) surrounding the visual coding as a supplemental path to recognize the object (e.g., return a return result) when a user attempts to scan the visual coding. For example, consider an image of a product packaging that includes a visual coding (e.g., a barcode). If the visual coding is not effectively registered in the associated index, a typical system may return erroneous results or no results at all. However, the exemplary systems described herein may use other features to identify the object. As an example, text or images of the product present on the packaging may be recognized, and the text or images may then be searched for as a product or a general web query. In another example, multiple images of the visual coding or surrounding scene may be spliced together in a "session" to form a more complete understanding of the product and update the index. Specifically, if a user scans a visual coding that is not present in the index, but context signals are used to recognize the related object/product, the related object/product and visual coding may be added to the index. Thus, the mapping of codings to objects in the index may evolve or be supplemented over time based on anonymized aggregate user engagement. The proposed system may also operate to remove ambiguity between multiple potential solutions to the visual coding. For example, sometimes incompatible product codings will contain the same ID for different products. Thus, some example implementations may capture visual features surrounding the visual coding and use them as secondary identification information, for example, to distinguish between competing ID maps.
提案するシステムおよび方法は、いくつかの異なる適用例または使用事例において使用され得る。一例として、提案する視覚符号化プラットフォームは、セキュアなまたは他のアクセス制限されたロケーションへのパッケージ配送を認証および/または実現するために使用され得る。一例として、像(たとえば、カメラ対応ドアベルによってキャプチャされる)を使用して、高価な配送を認証し、非常に短いロック解除/再ロックプロトコルを、スマートロックを含むドアと調整させることが可能である。この時間中、パッケージ配送員は、パッケージの手動の受け取りではなく、セキュアなロケーションの内部に製品を配置し得る。したがって、一例では、各配送員は、自らアカウントを任意の配送ロック解除にアタッチする一時的に生成されたコードを有し得る。各パッケージは、視覚符号化プラットフォームを製品購入に関連付けることができる一意のコードを有してもよい。配送員/受取人によって設定された値基準が満たされる場合、配送員がパッケージコードをドアベルカメラに提示するとき、ドアはロック解除され得る。ドアは、短い時間期間、ドアが閉じられるまで、または配送員がそのバッジを走査するまで、のうちの1つにわたって、ロック解除された状態に留まり得る。したがって、視覚符号化は、オブジェクト自体の単なる識別を超えたデータを表し得る。たとえば、視覚符号化の処理は、完全に自動化された(すなわち、何のユーザ介入もない)パッケージ配送のためのセキュリティ検証のための(たとえば、複数のクラウドベースのまたはIoTデバイスによって実行される)詳細プロセスをトリガし得る。 The proposed system and method may be used in several different applications or use cases. As an example, the proposed visual coding platform may be used to authenticate and/or facilitate package delivery to a secure or other access-restricted location. As an example, an image (e.g., captured by a camera-enabled doorbell) may be used to authenticate a high-value delivery and coordinate a very short unlock/relock protocol with a door that includes a smart lock. During this time, the package delivery person may place the product inside the secure location rather than manual receipt of the package. Thus, in one example, each delivery person may have a temporarily generated code that attaches their account to any delivery unlock. Each package may have a unique code that can associate the visual coding platform with a product purchase. If value criteria set by the delivery person/recipient are met, the door may be unlocked when the delivery person presents the package code to the doorbell camera. The door may remain unlocked for one of a short period of time, until the door is closed or until the delivery person scans his/her badge. Thus, the visual coding may represent data beyond the mere identification of the object itself. For example, processing of the visual coding may trigger a detailed process (e.g., performed by multiple cloud-based or IoT devices) for security verification for package delivery that is fully automated (i.e., without any user intervention).
より具体的には、本開示の実施形態によるシステムおよび方法は、シーンの中の1つまたは複数の機械可読視覚符号化を識別し得る。機械可読視覚符号化は、いくつかの実施形態では、1次元(1D)パターン、2次元(2D)パターン、3次元(3D)パターン、4次元(4D)パターン、および1Dおよび2Dパターンの、写真、スケッチ、図面、ロゴなどを含む他の視覚情報との組合せを含めて、それらの組合せを含み得る。機械可読視覚符号化の1つの例示的実施形態は、QRコード(登録商標)である。シーンは、その中の機械可読視覚符号化に視覚的および/または空間的コンテキストを提供し、それらが展開される1つまたは複数の機械可読視覚符号化の図を含み得る。たとえば、いくつかの実施形態では、シーンは、機械可読視覚符号化の図のみを含むことがあるが、いくつかの実施形態では、シーンは、機械可読視覚符号化、および機械可読視覚符号化が添付される任意のオブジェクト、人物、または構造、ならびにシーンのキャプチャされた画像近くのかつ/またはそうでなければその画像内で可視の、他のオブジェクト、人物、または構造を含むこともある。 More specifically, systems and methods according to embodiments of the present disclosure may identify one or more machine-readable visual encodings in a scene. The machine-readable visual encodings may, in some embodiments, include one-dimensional (1D) patterns, two-dimensional (2D) patterns, three-dimensional (3D) patterns, four-dimensional (4D) patterns, and combinations thereof, including combinations of 1D and 2D patterns with other visual information, including photographs, sketches, drawings, logos, and the like. One exemplary embodiment of a machine-readable visual encoding is a QR code. A scene may include a view of one or more machine-readable visual encodings that provide visual and/or spatial context for the machine-readable visual encodings therein and in which they are deployed. For example, in some embodiments, a scene may include only a view of the machine-readable visual encoding, while in some embodiments, a scene may also include the machine-readable visual encodings and any objects, people, or structures to which the machine-readable visual encodings are attached, as well as other objects, people, or structures near and/or otherwise visible within the captured image of the scene.
本開示のシステムおよび方法は、シーンを記述する画像データの処理に基づいて、シーンの中の1つまたは複数の機械可読視覚符号化を識別し得る。いくつかの例では、画像データは、写真または他のスペクトル撮像データ、メタデータ、および/またはそれらの符号化または他の表現を含み得る。たとえば、写真または他のスペクトル撮像データは、デバイス(たとえば、ユーザデバイス)の1つまたは複数のセンサによってキャプチャされ得る。写真または他のスペクトル撮像データは、デバイス(たとえば、モバイルフォンの1つまたは複数のカメラ)上の1つまたは複数の撮像センサから取得され、いくつかの例示的実施形態では、ビットマップ画像データとして記憶され得る。いくつかの実施形態では、写真または他のスペクトル撮像データは、デバイス上の複数の撮像センサの各々からの1つまたは複数の暴露から取得され得る。撮像センサは、人間の目に可視のおよび/または不可視の波長(たとえば、赤外線)を含むスペクトル情報をキャプチャし得る。メタデータは、画像データに対するコンテキストを提供することができ、いくつかの例では、地理的データ(たとえば、GPSロケーション、道路および/または事業所など、近くのマッピングされた要素に関する位置、機械可読視覚符号化など、撮像特徴に対するデバイスのポーズなど)、時間データ、テレメトリデータ(たとえば、画像データに関連するデバイスの配向、速度、加速度、高度など)、1つまたは複数のサービスのユーザアカウント(たとえば、画像データに関連するデバイスに対応するアカウント)に関連するアカウント情報、および/または前処理データ(たとえば、画像データを生成するデバイスによってなど、画像データの前処理によって生成されるデータ)を含み得る。一実施形態では、前処理データは、1つまたは複数の撮像センサによってキャプチャされた撮像データの特徴の深度マッピングを含み得る。 The systems and methods of the present disclosure may identify one or more machine-readable visual encodings in a scene based on processing of image data describing the scene. In some examples, the image data may include a photograph or other spectral imaging data, metadata, and/or encodings or other representations thereof. For example, the photograph or other spectral imaging data may be captured by one or more sensors of a device (e.g., a user device). The photograph or other spectral imaging data may be obtained from one or more imaging sensors on the device (e.g., one or more cameras of a mobile phone) and, in some exemplary embodiments, stored as bitmap image data. In some embodiments, the photograph or other spectral imaging data may be obtained from one or more exposures from each of multiple imaging sensors on the device. The imaging sensors may capture spectral information including wavelengths visible and/or invisible to the human eye (e.g., infrared). The metadata can provide context for the image data and, in some examples, may include geographic data (e.g., GPS location, position relative to nearby mapped features such as roads and/or businesses, machine-readable visual encoding, etc., pose of the device relative to the imaging features, etc.), time data, telemetry data (e.g., orientation, speed, acceleration, altitude, etc. of a device associated with the image data), account information associated with a user account for one or more services (e.g., an account corresponding to a device associated with the image data), and/or pre-processing data (e.g., data generated by pre-processing of the image data, such as by a device generating the image data). In one embodiment, the pre-processing data may include a depth mapping of features of the imaging data captured by one or more imaging sensors.
画像データの例は、1つまたは複数の撮像センサによってキャプチャされる撮像データの符号化または他の表現を含んでもよい。たとえば、いくつかの実施形態では、画像データは、ビットマップ撮像データに基づいて生成されたハッシュ値を含み得る。このようにして、ビットマップ撮像データの表現は、ビットマップ撮像データ自体(または、その1つまたは複数の部分)の代わりにまたはそれに加えて、画像データ内で使用され得る。 Examples of image data may include encodings or other representations of imaging data captured by one or more imaging sensors. For example, in some embodiments, the image data may include a hash value generated based on the bitmapped imaging data. In this manner, a representation of the bitmapped imaging data may be used in the image data in place of or in addition to the bitmapped imaging data itself (or one or more portions thereof).
画像データは、1つまたは複数の認識システムによって処理され得る。たとえば、画像データの1つまたは複数の部分(および/または画像データ全体)は、複数の認識システムによってそれぞれ処理され得る。認識システムは、汎用画像認識モデルおよび/または一定の認識タスク用に構成された1つまたは複数のモデルを備え得る。たとえば、認識システムは、いくつかの実施形態では、顔認識システム、オブジェクト認識システム、ランドマーク認識システム、深層マッピングシステム、機械可読視覚符号化認識システム、光学文字認識システム、意味解析システムなどを含み得る。 The image data may be processed by one or more recognition systems. For example, one or more portions of the image data (and/or the entire image data) may be processed by multiple recognition systems, respectively. The recognition systems may comprise a general-purpose image recognition model and/or one or more models configured for certain recognition tasks. For example, the recognition systems, in some embodiments, may include a facial recognition system, an object recognition system, a landmark recognition system, a deep mapping system, a machine-readable visual coding recognition system, an optical character recognition system, a semantic analysis system, etc.
いくつかの実施形態では、機械可読視覚符号化を含むシーンを記述する画像データは、機械可読視覚符号化を認識するように構成された符号化認識システムによって処理され得る。いくつかの実施形態では、機械可読視覚符号化を記述する画像データの一部分(たとえば、画像のセグメント)のみが符号化認識システムによって処理され得るが、符号化認識システムは、画像データを全部または部分的に処理するように構成されてもよい。たとえば、いくつかの実施形態では、前処理システムは、機械可読視覚符号化の存在を認識し、機械可読視覚符号化に関する画像データの一部分を抽出する。抽出された部分は、次いで、符号化認識システムによる識別のために符号化認識システムによって処理され得る。追加または代替として、画像データの全体が符号化認識システムによって直接的に処理され得る。 In some embodiments, image data describing a scene including a machine-readable visual encoding may be processed by a coding recognition system configured to recognize the machine-readable visual encoding. In some embodiments, only a portion of the image data describing the machine-readable visual encoding (e.g., a segment of an image) may be processed by the coding recognition system, although the coding recognition system may be configured to process the image data in whole or in part. For example, in some embodiments, a pre-processing system recognizes the presence of a machine-readable visual encoding and extracts a portion of the image data related to the machine-readable visual encoding. The extracted portion may then be processed by the coding recognition system for identification by the coding recognition system. Additionally or alternatively, the entirety of the image data may be processed directly by the coding recognition system.
いくつかの例では、画像データは、1つまたは複数の追加の認識システムによってさらに処理され得る。たとえば、画像データは、機械可読視覚符号化を認識するように構成された符号化認識システムによって、かつ符号化認識システムとは異なる別の認識システムによって、処理され得る。一実施形態では、他の認識システムは、機械可読視覚符号化に関連するコンテキストの様々な特徴(aspects)を認識するように構成され得る。たとえば、他の認識システムは、画像データによって記述されるシーンの1つまたは複数の部分(たとえば、シーンの中に含まれる1つまたは複数の機械可読視覚符号化を囲むシーンの周囲部分)を認識するように構成された認識システムを含み得る。いくつかの実施形態では、他の認識システムは、画像データ(たとえば、赤外線暴露)内で伝えられるスペクトル情報を認識するように構成された認識システムを含み得る。いくつかの実施形態では、他の認識システムは、画像データ内に含まれたメタデータを処理するように構成された認識システムを含み得る。 In some examples, the image data may be further processed by one or more additional recognition systems. For example, the image data may be processed by a coding recognition system configured to recognize the machine-readable visual coding and by another recognition system different from the coding recognition system. In one embodiment, the other recognition system may be configured to recognize various aspects of a context associated with the machine-readable visual coding. For example, the other recognition system may include a recognition system configured to recognize one or more portions of a scene described by the image data (e.g., peripheral portions of a scene surrounding one or more machine-readable visual codings contained within the scene). In some embodiments, the other recognition system may include a recognition system configured to recognize spectral information conveyed within the image data (e.g., infrared exposure). In some embodiments, the other recognition system may include a recognition system configured to process metadata contained within the image data.
いくつかの実施形態では、画像データの1つまたは複数の特徴を認識することは、それらの特徴を1つまたは複数の記憶された基準に関連付けることを含み得る。いくつかの実施形態では、記憶された基準は、1つまたは複数の機械可読視覚符号化(および/またはそれに関連する1つまたは複数のエンティティ)に対応して登録されたデータのセットであり得る。たとえば、記憶された基準は、1つまたは複数の機械可読視覚符号化に関連するデータ(たとえば、1つまたは複数の機械可読視覚符号化の実装に関連する、メタデータ、近くのオブジェクト、人物、または構造など)を登録するために1つまたは複数の機械可読視覚符号化が生成されるときに作成され得る。いくつかの実施形態では、記憶された基準を記述するデータは、1つまたは複数の機械可読視覚符号化を含むシーンを記述する画像データと同時に1つまたは複数の認識システムによって受信され得る。たとえば、記憶された基準は、機械可読視覚符号化を含むシーンを記述する画像データがキャプチャおよび/または受信される時間においてまたはその近くで(たとえば、同時に)更新される、機械可読視覚符号化が位置するコンテキスト(たとえば、ロケーション、時間、環境情報、近くのデバイス、ネットワークなど)を記述するデータを含み得る。 In some embodiments, recognizing one or more features of the image data may include associating those features with one or more stored criteria. In some embodiments, the stored criteria may be a set of data registered corresponding to one or more machine-readable visual encodings (and/or one or more entities associated therewith). For example, the stored criteria may be created when one or more machine-readable visual encodings are generated to register data associated with the one or more machine-readable visual encodings (e.g., metadata, nearby objects, people, or structures, etc., associated with the implementation of the one or more machine-readable visual encodings). In some embodiments, the data describing the stored criteria may be received by one or more recognition systems contemporaneously with the image data describing the scene including the one or more machine-readable visual encodings. For example, the stored criteria may include data describing the context in which the machine-readable visual encodings are located (e.g., location, time, environmental information, nearby devices, networks, etc.), which is updated at or near (e.g., contemporaneously) the time that the image data describing the scene including the machine-readable visual encodings is captured and/or received.
いくつかの実施形態では、記憶された基準のセットは、機械可読視覚符号化を生成するための同じ所定のアルゴリズムおよび/または規格に関連付けられ得る。たとえば、機械可読視覚符号化は、所定のアルゴリズムおよび/または規格に従って、1つまたは複数のデータ項目(たとえば、命令および/または情報)を視覚パターンに符号化することによって生成され得る。このようにして、符号化認識システムは、機械可読視覚符号化が記憶された基準のセットの記憶された基準のうちの少なくとも1つに関連付けられることを認識し、視覚パターンを復号して、1つまたは複数のデータ項目を検索するために、所定のアルゴリズムおよび/または規格に従って、機械可読視覚符号化を処理し得る。 In some embodiments, the sets of stored criteria may be associated with the same predefined algorithm and/or standard for generating the machine-readable visual coding. For example, the machine-readable visual coding may be generated by encoding one or more data items (e.g., instructions and/or information) into a visual pattern according to a predefined algorithm and/or standard. In this manner, the coding recognition system may process the machine-readable visual coding according to the predefined algorithm and/or standard to recognize that the machine-readable visual coding is associated with at least one of the stored criteria of the set of stored criteria, decode the visual pattern, and retrieve the one or more data items.
記憶された基準は、1つまたは複数の機械可読視覚符号化を記述する情報および/または機械可読視覚符号化のうちの1つまたは複数に関連するコンテキストを記述する情報を含んでもよい。たとえば、記憶された基準は、1つまたは複数のデータ項目が符号化されない視覚特徴(たとえば、設計および/または美的特徴、形状、サイズ、配向、色など)など、機械可読視覚符号化のコンテキスト特徴を記述するデータを含み得る。このようにして、1つまたは複数の機械可読視覚符号化は、1つまたは複数の機械可読視覚符号化の符号化されていないコンテキスト特徴に少なくとも部分的に基づいて、1つまたは複数の記憶された基準(または、随意に、単一の記憶された基準の1つまたは複数の副基準)にそれぞれ関連付けられ得る。一例では、2つの機械可読視覚符号化が同じデータ項目(たとえば、動作を実行するための命令)を符号化し得る。しかしながら、機械可読視覚符号化を記述する画像データは、機械可読視覚符号化のうちの1つが、たとえば、それらの機械可読視覚符号化のうちの別の機械可読視覚符号化とは異なる形状で描かれることを示すことがあり、符号化認識システムは、それにより、機械可読視覚符号化を区別し、それぞれ、機械可読視覚符号化を異なる記憶された基準および/または副基準に関連付けることができる。 The stored criteria may include information describing one or more machine-readable visual encodings and/or information describing a context associated with one or more of the machine-readable visual encodings. For example, the stored criteria may include data describing contextual features of the machine-readable visual encodings, such as visual features (e.g., design and/or aesthetic features, shape, size, orientation, color, etc.) in which one or more data items are not encoded. In this manner, one or more machine-readable visual encodings may be associated with one or more stored criteria (or, optionally, one or more sub-criteria of a single stored criteria) respectively based at least in part on the unencoded contextual features of the one or more machine-readable visual encodings. In one example, two machine-readable visual encodings may encode the same data item (e.g., instructions for performing an operation). However, image data describing the machine-readable visual codings may indicate that one of the machine-readable visual codings is, for example, depicted with a different shape than another of the machine-readable visual codings, and the coding recognition system may thereby distinguish between the machine-readable visual codings and associate each with a different stored criterion and/or sub-criterion.
たとえば、記憶された基準は、その中に1つまたは複数の機械可読視覚符号化を見出すことができるシーンおよび/または1つまたは複数の機械可読視覚符号化に関連するメタデータなど、1つまたは複数の機械可読視覚符号化に関連する他のコンテキストを記述するデータを含み得る。このようにして、コンテキスト情報を記述する画像データの部分は、1つまたは複数の記憶された基準に関連するとして認識されることも可能である。たとえば、1つまたは複数の画像認識システムは、それにより示される1つまたは複数の人物、オブジェクト、および/または構造を認識するためのコンテキスト情報を記述する画像データを処理し、随意に、そのようなコンテキスト情報とシーンの中の1つまたは複数の機械可読視覚符号化との間の何らかの関係(たとえば、相対的配置、意味的関連付けなど)が存在するかどうかを判定し得る。いくつかの例では、1つまたは複数の認識システムは、画像データによって含まれるメタデータの観点から、(たとえば、ロケーションまたは他のメタデータを記憶された基準に関連するロケーションまたは他の特徴と比較することによって)画像データを1つまたは複数の記憶された基準に関連付けることができる。 For example, the stored reference may include data describing a scene in which the one or more machine-readable visual encodings may be found and/or other context associated with the one or more machine-readable visual encodings, such as metadata associated with the one or more machine-readable visual encodings. In this manner, portions of the image data describing contextual information may also be recognized as associated with the one or more stored references. For example, one or more image recognition systems may process image data describing contextual information for recognizing one or more people, objects, and/or structures depicted thereby, and optionally determine whether there is any relationship (e.g., relative positioning, semantic association, etc.) between such contextual information and one or more machine-readable visual encodings in the scene. In some examples, the one or more recognition systems may associate the image data with one or more stored references in terms of metadata included by the image data (e.g., by comparing the location or other metadata with a location or other feature associated with the stored reference).
いくつかの実施形態では、機械可読視覚符号化を含む1つまたは複数のシーンを記述する、記憶された基準は、認証および/または登録され得る。たとえば、機械可読視覚符号化の特定の実装に関連するエンティティは、それに関連する、記憶された基準を登録および/または認証することを選択し得る。このようにして、他のティティティ(たとえば、悪意のあるエンティティ、誤ったエンティティ、競合エンティティなど)は同じシーンの記憶された基準を生成、記憶、またはそうでなければ文書化することが禁止され得る。いくつかの実施形態では、そのような禁止は、一定のエンティティが登録された、記憶された基準の認証されたコンテキストデータを不正に偽装すること、またはそうでなければスプーフィングすることができないことを確実にし得る。登録は、シーンの様々な視覚的特性をキャプチャするために、カメラを用いて「掃引」(たとえば、180度または360度)を実行することを含み得る。 In some embodiments, stored standards describing one or more scenes that include machine-readable visual encodings may be authenticated and/or registered. For example, an entity associated with a particular implementation of the machine-readable visual encoding may choose to register and/or authenticate the stored standards associated therewith. In this manner, other entities (e.g., malicious, erroneous, competing entities, etc.) may be prohibited from generating, storing, or otherwise documenting stored standards of the same scene. In some embodiments, such prohibition may ensure that certain entities cannot fraudulently spoof or otherwise spoof the authenticated contextual data of a registered, stored standard. Registration may include performing a "sweep" (e.g., 180 degrees or 360 degrees) with a camera to capture various visual characteristics of the scene.
いくつかの実施形態では、1つまたは複数の機械可読視覚符号化を記述する画像データの部分およびコンテキスト情報を記述する画像データの部分は、それぞれ、1つまたは複数の認識システムによって1つまたは複数の記憶された基準に関連付けられ得る。いくつかの実施形態では、コンテキスト情報を記述する画像データの一部分は、1つまたは複数の機械可読視覚符号化を記述する画像データの一部分とは異なる1つまたは複数の認識システムによって処理され得る(とはいえ、いくつかの実施形態では、同じ認識システムが両方の部分を処理し得る)。それぞれの関連付けは各々、異なる信頼性レベルとの類似性の判定を含み得る。いくつかの実施形態では、1つまたは複数の機械可読視覚符号化を記述する画像データの一部分が記憶された基準の特徴に類似する信頼性レベルは、コンテキスト情報を記述する画像データの一部分が同じ記憶された基準の特徴(または、たとえば、その副基準)に類似する信頼性レベルと比較され得る。 In some embodiments, the portion of the image data describing the one or more machine-readable visual encodings and the portion of the image data describing the contextual information may each be associated with one or more stored criteria by one or more recognition systems. In some embodiments, the portion of the image data describing the contextual information may be processed by one or more recognition systems that are different from the portion of the image data describing the one or more machine-readable visual encodings (although in some embodiments the same recognition system may process both portions). Each respective association may include a determination of similarity with a different confidence level. In some embodiments, the confidence level that the portion of the image data describing the one or more machine-readable visual encodings resembles a feature of the stored criteria may be compared to the confidence level that the portion of the image data describing the contextual information resembles a feature (or, for example, a subcriterion thereof) of the same stored criteria.
いくつかの実施形態では、信頼性レベルのうちの少なくとも1つが所定の信頼性しきい値および/またはターゲット値以上である場合、機械可読視覚符号化の処理は成功した(たとえば、認識された、識別されたなど)と判定される。いくつかの実施形態では、(たとえば、検証のための)処理の完了のために複合信頼性スコアが必要とされ得る。いくつかの実施形態では、コンテキスト情報を記述する画像データの一部分は、1つまたは複数の機械可読視覚符号化を記述する画像データの別の部分が十分高い信頼性スコアで認識されていないとの判定に応じて、フェイルセーフまたは代替オプションとして異なる認識システムによって処理され得る。 In some embodiments, processing of the machine-readable visual encoding is determined to be successful (e.g., recognized, identified, etc.) if at least one of the confidence levels is equal to or exceeds a predetermined confidence threshold and/or target value. In some embodiments, a composite confidence score may be required for completion of processing (e.g., for verification). In some embodiments, a portion of the image data describing the contextual information may be processed by a different recognition system as a fail-safe or fallback option in response to a determination that another portion of the image data describing one or more machine-readable visual encodings has not been recognized with a sufficiently high confidence score.
成功裏の認識時に、認識システムは動作を開始し得る。たとえば、動作は、画像データが機械可読視覚符号化に対応することを検証すること、画像データが(たとえば、適切なロケーション内で)承認されたおよび/または所定のコンテキスト内の機械可読視覚符号化に対応することを検証することなどを含み得る。いくつかの実施形態では、そのような検証は、画像データ(たとえば、キャプチャされた画像)を作り出したデバイスによって受信され得る。いくつかの実施形態では、そのような検証は、機械可読視覚符号化に関連する1つまたは複数のプロバイダシステム(たとえば、第三者が提供するシステム)によって受信され得る。検証は、機械可読視覚符号化において符号化されたデータを処理するために必要とされるセキュリティ証明を含み得る検証インジケータを含み得る。他の動作は、(たとえば、セキュアなデータ交換のためになど)デバイスと別のシステムとの間でセキュアな接続を開始することを含み得る。 Upon successful recognition, the recognition system may initiate an action. For example, the action may include verifying that the image data corresponds to the machine-readable visual encoding, verifying that the image data corresponds to the machine-readable visual encoding in an approved (e.g., in an appropriate location) and/or predetermined context, etc. In some embodiments, such verification may be received by the device that produced the image data (e.g., the captured image). In some embodiments, such verification may be received by one or more provider systems (e.g., systems provided by a third party) associated with the machine-readable visual encoding. The verification may include a verification indicator, which may include security credentials required to process the data encoded in the machine-readable visual encoding. Other actions may include initiating a secure connection between the device and another system (e.g., for secure data exchange, etc.).
本開示の実施形態によるシステムおよび方法は、いくつかの技術的な効果および利点を伝える。たとえば、本明細書で開示するようなコンテキスト情報に鑑みた機械可読視覚符号化の処理は、雑音、データ損失、および測定誤差、ならびに/または不備に対する認識プロセスのロバストネスの改善を提供し得る。いくつかの実施形態では、本開示の実施形態によるシステムおよび方法は、そのような符号化が撮像デバイスの撮像センサによって明確に解決され得ないときですら、機械可読視覚符号化の認識および処理を提供し、撮像デバイスの限界(たとえば、不十分なセンサおよび/または光学分解能)以上に認識能力を改善し得る。したがって、場合によっては、視覚符号化を認識するために複数のタイプまたはモダリティのデータが使用され、それにより、符号化を認識するために処理される必要がある画像の数を低減するため、視覚符号化は、より効率的に認識/処理され得る。このより効率的な処理は、結果として、プロセッサ使用、メモリ使用、帯域幅使用など、コンピューティングリソースの節約をもたらし得る。 The systems and methods according to embodiments of the present disclosure convey several technical effects and advantages. For example, processing of machine-readable visual encodings in light of contextual information as disclosed herein may provide improved robustness of the recognition process to noise, data loss, and measurement errors and/or imperfections. In some embodiments, the systems and methods according to embodiments of the present disclosure provide recognition and processing of machine-readable visual encodings even when such encodings cannot be unambiguously resolved by the imaging sensor of the imaging device, and may improve recognition capabilities beyond the limitations of the imaging device (e.g., insufficient sensor and/or optical resolution). Thus, in some cases, visual encodings may be recognized/processed more efficiently because multiple types or modalities of data are used to recognize the visual encodings, thereby reducing the number of images that need to be processed to recognize the encodings. This more efficient processing may result in savings in computing resources, such as processor usage, memory usage, bandwidth usage, etc.
本開示の態様による改善された認識技法から生じる追加の技術的利点は、より小さなサイズで、かつ様々な実装に組み込むことを助長する視覚構成で、符号化が作り出されることを可能にすることを含む。このようにして、機械可読視覚符号化の実装において、より少ない材料および労力が消費されることになる。実装に対するより低い障壁は、幅広い採用をも可能にし、データ送信コストを低減するためになど(たとえば、視覚パターン内のデータをコンパクトに符号化することによって)データ通信に対する効率をもたらす。 Additional technical advantages arising from improved recognition techniques according to aspects of the present disclosure include enabling encodings to be produced in smaller sizes and visual configurations that are conducive to incorporation into a variety of implementations. In this manner, less material and labor will be consumed in implementing machine-readable visual encodings. The lower barrier to implementation also enables wider adoption and provides efficiencies for data communication (e.g., by compactly encoding data within visual patterns) to reduce data transmission costs, etc.
追加の技術的利点は、所与の機械可読視覚符号化を使用して、大量のデータを通信するための能力を含む。たとえば、いくつかの機械可読視覚符号化は、所与のサイズのデータ(たとえば、印刷エリア、表示エリアなど)を符号化するために所与の数のビジュアル「ビット」を提供する規格に対応するように生成され得る。いくつかの例では、ビジュアル「ビット」のうちの少なくともいくつかは、符号化を処理するための誤差補正および/または整合のために使用され得る。有利には、本開示の実施形態によるシステムおよび方法は、追加のビジュアル「ビット」を費やすことなく、改善された誤差補正、整合、および/またはデータ通信を提供し得、いくつかの実施形態では、コンテキストデータを処理しない符号化認識システムとの互換性を保持し得る。いくつかの実施形態は、場合によっては、同等の機械可読視覚符号化を区別するためにコンテキスト情報を使用することによって改善された解析を提供し、単一の機械可読視覚符号化が、粒度の細かい記録管理のための能力を保持しながら、(たとえば、表示、印刷、配給などにおける規模の経済により)より低い製造コストに向けて複数のコンテキストで展開されることを可能にする。 Additional technical advantages include the ability to communicate large amounts of data using a given machine-readable visual encoding. For example, some machine-readable visual encodings may be generated to correspond to a standard that provides a given number of visual "bits" to encode a given size of data (e.g., a printed area, a display area, etc.). In some examples, at least some of the visual "bits" may be used for error correction and/or alignment to process the encoding. Advantageously, systems and methods according to embodiments of the present disclosure may provide improved error correction, alignment, and/or data communication without expending additional visual "bits," and in some embodiments may retain compatibility with encoding recognition systems that do not process contextual data. Some embodiments may provide improved analysis by using context information to distinguish between equivalent machine-readable visual encodings, allowing a single machine-readable visual encoding to be deployed in multiple contexts (e.g., through economies of scale in display, printing, distribution, etc.) for lower production costs while retaining the ability for fine-grained record-keeping.
追加の技術的利点は、機械可読視覚符号化を処理するための改善されたセキュリティを含む。いくつかの実施形態では、悪意のある当事者は、機械可読視覚符号化を処理する任意のデバイスに対する制御をアサートするために、1つまたは複数の機械可読視覚符号化の変更および/または置換を試行することがある。例示的実施形態は、機械可読視覚符号化の1つまたは複数の特徴および/またはそれらの1つまたは複数のコンテキスト特徴を記憶された基準と比較し、攻撃者の機械可読視覚符号化における不一致をさらすことによって、そのような攻撃の成功(または、合法な機械可読視覚符号化の誤使用または誤配置)を防止し得る。このようにして、実施形態は、機械可読視覚符号化を介して不正を犯す試行を低減することも可能である。いくつかの実施形態では、符号化を変更することによって機械可読視覚符号化を処理するためにデバイスを使用してユーザを騙す試行は低減され得る。同様に、サービスプロバイダ(たとえば、機械学習済み視覚符号化の生成に関連するエンティティ)を騙す試行は、正確かつ真正の機械可読視覚符号化のみがユーザデバイスによって処理されることを確実にすることによって、低減され得る。 Additional technical advantages include improved security for processing machine-readable visual encodings. In some embodiments, a malicious party may attempt to modify and/or replace one or more machine-readable visual encodings in order to assert control over any device that processes the machine-readable visual encodings. Exemplary embodiments may prevent the success of such attacks (or the misuse or misplacement of legitimate machine-readable visual encodings) by comparing one or more features of the machine-readable visual encodings and/or one or more contextual features thereof to a stored reference and exposing discrepancies in the attacker's machine-readable visual encodings. In this manner, embodiments may also reduce attempts to commit fraud via machine-readable visual encodings. In some embodiments, attempts to deceive a user using a device to process the machine-readable visual encoding by modifying the encoding may be reduced. Similarly, attempts to deceive a service provider (e.g., an entity associated with the generation of the machine-learned visual encodings) may be reduced by ensuring that only accurate and authentic machine-readable visual encodings are processed by a user device.
追加の技術的利点は、機械可読視覚符号化の処理によって、またはそれに応じて通信されるデータの改善された制御を含む。たとえば、機械可読視覚符号化の処理は、機械可読視覚符号化に関連する動作を実行するために一定のコンテキスト状態が必要とされるように、そのコンテキストに鑑みて制限され得る。そのような制御は、事後的に、機械可読視覚符号化が生成され、表示され(たとえば、印刷され)、かつ/または配給された後で、行使され、カスタマイゼーションコスト(たとえば、印刷消耗品、個別化された配給コスト、一意の識別専用の符号化および/またはビジュアル「ビット」のサイズなど)を低減しながら、粒度の細かい制御を可能にし得る。 Additional technical advantages include improved control of data communicated by or in response to processing of a machine-readable visual encoding. For example, processing of a machine-readable visual encoding may be limited in light of its context, such that certain contextual conditions are required to perform operations associated with the machine-readable visual encoding. Such control may be exercised after the fact, after the machine-readable visual encoding has been generated, displayed (e.g., printed), and/or distributed, allowing for fine-grained control while reducing customization costs (e.g., printing consumables, individualized distribution costs, size of uniquely identifying dedicated encoding and/or visual "bits," etc.).
ここで図面を参照して、本開示の例示的実施形態についてさらに詳しく論じる。 Exemplary embodiments of the present disclosure will now be discussed in more detail with reference to the drawings.
例示的デバイスおよびシステム
図1から図9Bは、本開示の態様による機械可読視覚符号化の例示的実施形態を示す。特定のジオメトリ、レイアウト、および構成に準拠する例示的な図が本明細書で提供されるが、本明細書で提供する例は、単なる例示のためであることを理解されたい。本明細書で提供する例示的な例の適応形態および修正形態は、本開示の範囲内に留まることを理解されたい。
1 to 9B show exemplary embodiments of machine-readable visual encoding according to aspects of the present disclosure. Although exemplary diagrams conforming to specific geometries, layouts, and configurations are provided herein, it should be understood that the examples provided herein are for illustrative purposes only. It should be understood that adaptations and modifications of the exemplary examples provided herein remain within the scope of the present disclosure.
図1は、字形の形状を含む機械可読視覚符号化の例示的実施形態を示す。図1の機械可読視覚符号化100は、視覚パターン104で満たされた文字「G」の字形102を含む。いくつかの実施形態では、視覚パターン104は、背景に対して形状106を含み、形状106と背景との間の視覚的コントラストは、情報の「ビット」値を示し得る。いくつかの実施形態では、視覚パターン内の形状106の幾何学的関係(たとえば、形状106の相対サイズ、形状106同士の間の距離、充填エリアおよび/または非充填エリアの輪郭など)は、情報を符号化するために使用され得る。たとえば、認識システムは、視覚パターン104の別個の特徴を認識し、機械可読視覚符号化100を記述する画像データをそれに対応する、記憶された基準に関連付けるようにトレーニングされ得る。図1は、個別の形状を成す視覚パターン104を示すが、視覚パターンは、情報を符号化する連続的特徴を含み得ることを理解されたい。たとえば、代替の機械可読視覚符号化200は、別の字形および/または別の視覚パターンを備え得る。たとえば、代替の視覚パターンは、振幅、周波数、位相、線の太さなど、情報符号化特徴を個々に持ち得る波形を含み得る。図1は個々の字形を示すが、2つ以上の字形が(たとえば、語、ロゴなどを形成するために)組み合わされてよいことを理解されたい。このようにして、機械可読視覚符号化は、個別かつ/または連続的視覚特徴を使用して、所望の美的価値に統合され得るか、またはその一部分を形成し得る。 FIG. 1 illustrates an exemplary embodiment of a machine-readable visual coding including glyph shapes. The machine-readable visual coding 100 of FIG. 1 includes a glyph 102 for the letter "G" filled with a visual pattern 104. In some embodiments, the visual pattern 104 includes shapes 106 against a background, and the visual contrast between the shapes 106 and the background may indicate a "bit" value of information. In some embodiments, the geometric relationships of the shapes 106 in the visual pattern (e.g., the relative sizes of the shapes 106, the distance between the shapes 106, the contours of the filled and/or unfilled areas, etc.) may be used to encode information. For example, a recognition system may be trained to recognize distinct features of the visual pattern 104 and associate image data describing the machine-readable visual coding 100 with corresponding stored criteria. Although FIG. 1 illustrates the visual pattern 104 as being in discrete shapes, it should be understood that the visual pattern may include continuous features that encode information. For example, an alternative machine-readable visual coding 200 may comprise alternative glyphs and/or alternative visual patterns. For example, alternative visual patterns may include waveforms that may each have information-encoding characteristics, such as amplitude, frequency, phase, line thickness, etc. Although FIG. 1 shows individual glyphs, it should be understood that two or more glyphs may be combined (e.g., to form words, logos, etc.). In this manner, machine-readable visual encodings may be integrated into or form part of a desired aesthetic using discrete and/or continuous visual features.
図2および図3は、他の視覚情報と組み合わされた視覚パターンを備える機械可読視覚符号化の例示的実施形態を示す。例示的な符号化300を示す図2では、視覚パターン304は、テキストおよび/またはグラフィカル情報310(たとえば、ブランド名、記述子など)を含む中央スペース308を囲む。いくつかの実施形態では、中央スペース308は符号化されたデータを含まないことがあるが、中央スペース308のコンテンツ(たとえば、テキスト情報310)は、視覚パターン304の処理に関するコンテキストを提供し得る。たとえば、機械可読視覚符号化300を記述する画像データは、中央スペース308を記述する情報をコンテキストデータとして含んでよく、意味認識システムは、テキスト情報310を解析して、視覚パターン304の認識を支援および/または相補し得る。図3では、例示的な符号化400は、人物の写真または他のアバター410を含む中央スペース408を囲む視覚パターン404を備える。テキスト情報310を参照しながら論じるように、写真410は、視覚パターン404の認識またはその後続処理を支援および/または相補するためのコンテキストを提供し得る。たとえば、視覚パターン404は、暗号化データをユーザアカウントに通信するためなど、動作を実行するための命令を通信することができ、写真410は、ユーザアカウントに関連するユーザに対応し得る。いくつかの実施形態では、たとえば、異なる宛先は、写真410に関連するアカウントにもはや関連付けられなくなり、不一致が検出可能になるため、悪意のある当事者は、異なる宛先を指定するために視覚パターン404を単に変更することによって、暗号化されたデータストリームを方向変換させることが可能でないことになる。 2 and 3 illustrate exemplary embodiments of machine-readable visual coding comprising a visual pattern combined with other visual information. In FIG. 2, which illustrates an exemplary coding 300, the visual pattern 304 surrounds a central space 308 that includes text and/or graphical information 310 (e.g., a brand name, a descriptor, etc.). In some embodiments, the central space 308 may not include encoded data, but the content of the central space 308 (e.g., the text information 310) may provide context for processing the visual pattern 304. For example, image data describing the machine-readable visual coding 300 may include information describing the central space 308 as context data, and a semantic recognition system may analyze the text information 310 to aid and/or complement the recognition of the visual pattern 304. In FIG. 3, the exemplary coding 400 comprises a visual pattern 404 surrounding a central space 408 that includes a person's photograph or other avatar 410. As discussed with reference to text information 310, photograph 410 may provide context to aid and/or complement recognition of visual pattern 404 or subsequent processing thereof. For example, visual pattern 404 may communicate instructions to perform an action, such as to communicate encrypted data to a user account, and photograph 410 may correspond to a user associated with the user account. In some embodiments, a malicious party will not be able to redirect the encrypted data stream by simply altering visual pattern 404 to designate a different destination, for example, because the different destination will no longer be associated with the account associated with photograph 410 and the mismatch will be detectable.
視覚パターン304および404は、いくつかの構成された円形形状を含むが、データを符号化するために、複数の形状、幾何学的構成、色相、および/または色値を活用する任意の数の視覚パターンが使用され得ることを理解されたい。加えて、いくつかの変形形態がビジュアル「ビット」(たとえば、明/暗を0/1)として符号化し得るが、各変形形態は、随意に、1つを超えるビットを符号化することができ、いくつかの変形形態(たとえば、異なる形状および/またはサイズ)は、所定の値またはデータオブジェクトに対応する。 Although visual patterns 304 and 404 include several configured circular shapes, it should be understood that any number of visual patterns utilizing multiple shapes, geometric configurations, hues, and/or color values may be used to encode data. In addition, while some variations may encode as visual "bits" (e.g., light/dark 0/1), each variation may optionally encode more than one bit, and some variations (e.g., different shapes and/or sizes) correspond to a predetermined value or data object.
図4は、形状506a、506bの視覚パターン504を備える符号化500の1つの例示的実施形態を示す。形状506a、506bは、値(たとえば、グレースケール実施形態では白から黒の範囲、および、随意に、白、ライトブルー、ダークブルーなど、1つまたは複数の所定の値にビニングされている範囲)、配向(たとえば、形状506aなど、正の半径方向、または形状506bなど、負の半径方向)、および/またはロケーション(たとえば、符号化500の象限(quadrants)に基づく)に基づいて、情報を符号化し得る。図5の別の例示的実施形態は、異なるサイズ、値、および位置の複数の形状606の視覚パターン604を備えた例示的な符号化600を示す。図6で実証されるように、例示的な符号化700は、その各々が値、位置、配向、およびサイズに関連付けられ得る、複数の形状706を含む視覚パターン704を備え得るため、視覚パターンは、いずれかの単一形状の変形形態に限定されない。別の例示的な符号化は、同心リングの形に構成された連続形状を有する視覚パターンを採用し得る。形状、値、位置、配向、およびサイズが符号化データに対応し得るように、位置のグループも一定のデータクラスに対応し得る。たとえば、符号化の1つまたは複数のリングは、特定のクラスのデータ(たとえば、識別番号)を符号化するために割り当てられてよく、リングの中の各弧セグメントの長さおよび/または値は、その値を示し得る。 4 illustrates one exemplary embodiment of an encoding 500 comprising a visual pattern 504 of shapes 506a, 506b. The shapes 506a, 506b may encode information based on value (e.g., a range from white to black in a grayscale embodiment, and optionally binned to one or more predefined values, such as white, light blue, dark blue, etc.), orientation (e.g., positive radial direction, such as for shape 506a, or negative radial direction, such as for shape 506b), and/or location (e.g., based on the quadrants of the encoding 500). Another exemplary embodiment of FIG. 5 illustrates an exemplary encoding 600 comprising a visual pattern 604 of multiple shapes 606 of different sizes, values, and positions. As demonstrated in FIG. 6, the exemplary encoding 700 may comprise a visual pattern 704 including multiple shapes 706, each of which may be associated with a value, position, orientation, and size, such that the visual pattern is not limited to variations of any single shape. Another exemplary encoding may employ a visual pattern having continuous shapes arranged in the form of concentric rings. Just as the shape, value, location, orientation, and size may correspond to the encoded data, the group of locations may also correspond to certain data classes. For example, one or more rings of the encoding may be assigned to encode a particular class of data (e.g., an identification number), and the length and/or value of each arc segment in the ring may indicate that value.
前述の特徴は、図7および図8に示すように、構成可能な機械可読視覚符号化を作り出すように組み合わされかつ/または再構成され得る。たとえば、図7の例示的符号化900は、同心リングと放射状指向線セグメントの両方、ならびに分散された円形形状(たとえば、画像内の配向の登録のための)を備える。図8の例示的な符号化1000は、同心リング1006a、形状1006b、および放射状に分散された軌道1006cを備える。図2および図3の中央エリア308および408は、図1から図8の符号化のうちのいずれかで同様に実装され得ることをやはり理解されたい。 The aforementioned features may be combined and/or reconfigured to create configurable machine-readable visual codings, as shown in Figures 7 and 8. For example, the exemplary coding 900 of Figure 7 includes both concentric rings and radially oriented line segments, as well as distributed circular shapes (e.g., for registration of orientation within an image). The exemplary coding 1000 of Figure 8 includes concentric rings 1006a, shapes 1006b, and radially distributed tracks 1006c. It should also be understood that the central areas 308 and 408 of Figures 2 and 3 may be similarly implemented in any of the codings of Figures 1 through 8.
図1から図8に示すように、本開示による機械可読視覚符号化は、データを符号化するために個別かつ/または連続的特徴を使用して、実質的に任意の所望の美的価値に統合され、その一部分を形成し得る。加えて、本開示による機械可読視覚符号化のコンフィギュアビリティは、機械可読視覚符号化の(人間の知覚および/または機械の知覚による)容易な認識クラスを提供し得る。 As shown in Figures 1-8, machine-readable visual encodings according to the present disclosure may be integrated into or form part of virtually any desired aesthetic, using discrete and/or continuous features to encode data. Additionally, the configurability of machine-readable visual encodings according to the present disclosure may provide for easily recognizable classes (by human perception and/or machine perception) of machine-readable visual encodings.
たとえば、図1から図8の各々における差異など、パターン、色などにおいて使用される輪郭、レイアウト、形状の差異は、機械可読視覚符号化を生成するために使用される異なるアルゴリズムおよび/または規格に対応し得る。したがって、上述の差異は、機械可読視覚符号化を生成するために使用される異なるアルゴリズムおよび/または規格に従って機械可読視覚符号化を読み取るために必要とされる認識モデルのタイプを1つまたは複数の認識システムに示し得る。このようにして、本開示の実施形態によるシステムおよび方法は、特別にトレーニングされた認識モデルを展開することによって効率的な処理を提供し得る。しかしながら、いくつかの実施形態では、上述の容易に知覚される構造的差異はまた、一般的な符号化認識モデルが、異なるクラスの符号化を直ちに、かつ正確に認識し、分類の混乱による誤差を最小限に抑えるのを支援するために有利に採用され得る。 For example, differences in contours, layouts, shapes used in patterns, colors, etc., such as the differences in each of Figures 1 through 8, may correspond to different algorithms and/or standards used to generate the machine-readable visual coding. Thus, the above-mentioned differences may indicate to one or more recognition systems the type of recognition model required to read the machine-readable visual coding according to the different algorithms and/or standards used to generate the machine-readable visual coding. In this manner, systems and methods according to embodiments of the present disclosure may provide efficient processing by deploying specially trained recognition models. However, in some embodiments, the easily perceived structural differences described above may also be advantageously employed to help a general coding recognition model immediately and accurately recognize different classes of codings and minimize errors due to classification confusion.
図9Aは、配向登録のためにいくつかの形状1106cが追加で使用される、情報を符号化するために円形形状1106a、1106bを採用する視覚パターン1104を備えた符号化1100の別の例示的実施形態を示す。いくつかの実施形態では、ユーザデバイスは、符号化1100の画像をキャプチャし、符号化1100の処理の進捗インジケータを示唆するように操作される画像のレンダリング1150を表示し得る。たとえば、図9Bに示すように、レンダリング1150は、角度1152が処理中に0度から360度まで掃引するように、処理が進むにつれて、フルカラーの符号化1100の一部分および「グレーアウトされた」一部分を比例的に表示し得る。いくつかの実施形態では、進捗レンダリング1150は、実際のステータス更新によってまたはその推定によって通知され得る。いくつかの実施形態では、レンダリングは、進捗オーバーレイが、符号化1100の画像上に(たとえば、ユーザデバイス上のカメラの視野を表示するオープンアプリ内に)リアルタイムで出現するようにレンダリングされるような拡張現実レンダリングである。進捗レンダリングを表示するために、図1から図8の例示的な符号化のいずれかの他の符号化が使用されてもよいことを理解されたい。そのような進捗レンダリングは、角度のある様式で、線形様式で、または符号化の1つまたは複数の輪郭(たとえば、図1の字形)をトレースする様式で、符号化にわたって掃引し得る。 9A shows another exemplary embodiment of the encoding 1100 with a visual pattern 1104 employing circular shapes 1106a, 1106b to encode information, where some shapes 1106c are additionally used for orientation registration. In some embodiments, the user device may capture an image of the encoding 1100 and display a rendering 1150 of the image that is manipulated to suggest a progress indicator of the processing of the encoding 1100. For example, as shown in FIG. 9B, the rendering 1150 may proportionally display portions of the encoding 1100 in full color and portions that are "grayed out" as the processing progresses, such that the angle 1152 sweeps from 0 degrees to 360 degrees during processing. In some embodiments, the progress rendering 1150 may be informed by actual status updates or by an estimate thereof. In some embodiments, the rendering is an augmented reality rendering, such that a progress overlay is rendered to appear in real time over the image of the encoding 1100 (e.g., in an open app that displays the field of view of the camera on the user device). It should be understood that other encodings of any of the exemplary encodings of FIGs. 1 through 8 may be used to display the progress rendering. Such a progress rendering may sweep across the encoding in an angular fashion, a linear fashion, or a fashion that traces one or more contours of the encoding (e.g., the glyphs in FIG. 1).
図10A～図10Cは、そのコンテキストに鑑みて機械可読視覚符号化を認識するための本開示の態様の一実施形態を実証する。図10Aは、機械可読視覚符号化1204が印刷されるマップ1202の画像1200を示す。機械可読視覚符号化1204に隣接するのは、テキスト材料1206である。図10Aは、本明細書で「印刷」された「マップ」に関して論じられるが、機械可読視覚符号化1204は、画像1200内でキャプチャするのに適した実質的にあらゆる形態のより大型のディスプレイ1202の部分としてスクリーンまたは他のデバイス上に表示され得ることを理解されたい。 10A-10C demonstrate one embodiment of an aspect of the present disclosure for recognizing a machine-readable visual encoding in light of its context. FIG. 10A shows an image 1200 of a map 1202 on which a machine-readable visual encoding 1204 is printed. Adjacent to the machine-readable visual encoding 1204 is textual material 1206. Although FIG. 10A is discussed herein with respect to a "printed" "map," it should be understood that the machine-readable visual encoding 1204 may be displayed on a screen or other device as part of a larger display 1202 in substantially any form suitable for capture within the image 1200.
いくつかの実施形態では、撮像デバイスは、そのコンテンツを直接的に復号するために十分に機械可読視覚符号化1204を分解し得る。いくつかの実施形態では、コンテンツを復号するのに先立って、かつ/またはそのコンテンツによって通信されるコードの実行に先立って、(たとえば、画像1200内で)機械可読視覚符号化1204に関連する追加のコンテキスト情報は、1つまたは複数の認識システムによって処理され得る。たとえば、マップ1202は、マップおよび/またはその上にマッピングされた1つまたは複数のロケーションとしてその識別を認識するように処理され得る。いくつかの実施形態では、テキスト情報1206は、機械可読視覚符号化1204と「駅」との間の関連付けを認識するために(たとえば、OCRおよび/または意味解析を介して)処理され得る。機械可読視覚符号化1204に関連する、記憶された基準は、機械可読視覚符号化1204をマップ、「駅」、および/またはマップ上の1つまたは複数のロケーションに関連付ける情報を含み得る。画像1200内のコンテキスト情報を記憶された基準と比較することによって、機械可読視覚符号化1204の認識および/または検証に関連する信頼性レベルは増大し得る。このようにして、撮像デバイスが、(たとえば、不十分な照明、遅いシャッター速度などにより) 機械可読視覚符号化1204のコンテンツを直接的に復号するのに十分に機械可読視覚符号化1204を分解できないか、または分解しない場合、コンテキスト情報の認識によって取得される増大した信頼性レベルが何らかの欠落情報が記憶された基準から「満たされる」ことを可能にし得る。 In some embodiments, the imaging device may resolve the machine-readable visual coding 1204 sufficiently to directly decode its contents. In some embodiments, prior to decoding the content and/or prior to execution of code communicated by that content, additional contextual information associated with the machine-readable visual coding 1204 (e.g., within the image 1200) may be processed by one or more recognition systems. For example, the map 1202 may be processed to recognize its identification as a map and/or one or more locations mapped thereon. In some embodiments, the textual information 1206 may be processed (e.g., via OCR and/or semantic analysis) to recognize an association between the machine-readable visual coding 1204 and a “station.” Stored criteria associated with the machine-readable visual coding 1204 may include information that associates the machine-readable visual coding 1204 with a map, a “station,” and/or one or more locations on the map. By comparing the contextual information within the image 1200 to the stored criteria, a confidence level associated with the recognition and/or verification of the machine-readable visual coding 1204 may be increased. In this way, if the imaging device cannot or does not resolve the machine-readable visual encoding 1204 sufficiently to directly decode the content of the machine-readable visual encoding 1204 (e.g., due to poor lighting, slow shutter speed, etc.), the increased confidence level obtained by recognizing the contextual information may allow any missing information to be "filled in" from the stored criteria.
図10Bは、より遠い距離からキャプチャされた同じマップ1202の画像1210を示す。いくつかの実施形態では、機械可読視覚符号化1204は、直接的に認識されるべき十分な詳細で分解され得ない。しかしながら、たとえば、マップ1202内の機械可読視覚符号化1204の相対測位、テキストラベル1212a、1212b、追加の機械可読視覚符号化1214の存在、および大規模なマッピング特徴1216a、1216bなど、画像1210内に含まれたコンテキスト情報が認識され得る。これらのコンテキスト特徴の各々、いくつか、またはすべてが認識され、機械可読視覚符号化1204に関連する、記憶された基準内のコンテキストデータと比較され得る。このようにして、撮像デバイスが機械可読視覚符号化1204のコンテンツを直接的に復号するために十分に機械可読視覚符号化1204を分解できないか、または分解しない場合ですら、コンテキスト情報の認識、およびコンテキスト情報と記憶された基準の関連付けは、符号化1204によって符号化されたデータの記憶された基準からの検索を可能にし得る。 10B shows an image 1210 of the same map 1202 captured from a greater distance. In some embodiments, the machine-readable visual coding 1204 may not be resolved with sufficient detail to be directly recognized. However, contextual information contained within the image 1210 may be recognized, such as, for example, the relative positioning of the machine-readable visual coding 1204 within the map 1202, the text labels 1212a, 1212b, the presence of additional machine-readable visual coding 1214, and large-scale mapping features 1216a, 1216b. Each, some, or all of these contextual features may be recognized and compared to contextual data in a stored reference associated with the machine-readable visual coding 1204. In this manner, recognition of the contextual information, and association of the contextual information with the stored reference, may enable retrieval of data encoded by the coding 1204 from the stored reference, even when the imaging device cannot or does not resolve the machine-readable visual coding 1204 sufficiently to directly decode the contents of the machine-readable visual coding 1204.
図10Cは、機械可読視覚符号化1204が印刷されたマップ1202を含むシーンの画像1220を示す。シーンの中の認識可能なコンテキスト特徴の例は、大型ロゴ1222、中にマップ1202が取り付けられるフレーム1224、ベンチ1226a、1226bなどの近くのオブジェクト、マップ1202の後ろの壁板の接合部1228など、対照的なアーキテクチャ特徴、壁と天井との間の境界1230など、部屋の境界、および照明1232などの照明要素を含む。これらのコンテキスト特徴の各々、いくつか、またはすべてが認識され、機械可読視覚符号化1204に関連する、記憶された基準内のコンテキストデータと比較され得る。このようにして、撮像デバイスが機械可読視覚符号化1204をまったく分解できないか、または分解しない(または、その輪郭および/または縁から離れている)場合ですら、コンテキスト情報の認識、およびコンテキスト情報と記憶された基準の関連付けは、符号化1204によって符号化されたデータの記憶された基準からの検索を可能にし得る。 10C shows an image 1220 of a scene including a map 1202 with a machine-readable visual encoding 1204 printed on it. Examples of recognizable contextual features in the scene include a large logo 1222, a frame 1224 in which the map 1202 is mounted, nearby objects such as benches 1226a, 1226b, contrasting architectural features such as a wall panel joint 1228 behind the map 1202, room boundaries such as a boundary 1230 between a wall and a ceiling, and lighting elements such as a light 1232. Each, some, or all of these contextual features may be recognized and compared to contextual data in a stored reference associated with the machine-readable visual encoding 1204. In this way, even when an imaging device cannot or does not resolve the machine-readable visual encoding 1204 at all (or is away from its contours and/or edges), the recognition of the contextual information and the association of the contextual information with the stored reference may enable retrieval of data encoded by the encoding 1204 from the stored reference.
上述の部屋の境界、オブジェクト、アーキテクチャ特徴など、3D空間におけるコンテキスト特徴の認識は、視覚測位システム(VPS)を使用した画像1220の処理を含み得る。たとえば、いくつかの実施形態では、画像1220は、表面、縁、隅角、および/または他の特徴を検出し、特徴のマッピング(たとえば、図10Dに示すようなポイントクラウド)を生成するためにVPSモデルを使用して処理され得る。そのようなマッピングは、シーンを記述する画像データにパッケージングされ、かつ/または1つまたは複数の認識システムによって生成され得、マッピングは、記憶された基準内のマッピングを記述するデータと比較され得る。いくつかの実施形態では、マッピング(たとえば、ポイントクラウドおよび/またはアンカーのセット)は、画像1220から、かつ/またはそれに関連する画像データ(たとえば、LIDARからの空間測定を含む、シーンを記述する収集されたセンサデータなど)から生成され得る。追加または代替として、(たとえば、電磁スペクトルの不可視部分から)スペクトルデータが収集され、マッピングを生成するために使用され得る。いくつかの実施形態では、(たとえば、複数の時点から、デバイス上の複数のセンサからなど)複数の暴露を組み合わせ、(たとえば、三角測量によって)深層マッピングを判定し得る。このようにして、VPSは、機械可読視覚符号化1204の認識、検証、および/または処理のために、画像1220(および/またはその画像を記述する画像データ)を記憶された基準と関連付けるために使用され得る。 Recognition of contextual features in 3D space, such as room boundaries, objects, architectural features, etc., as described above, may include processing of the image 1220 using a visual positioning system (VPS). For example, in some embodiments, the image 1220 may be processed using a VPS model to detect surfaces, edges, corners, and/or other features and generate a mapping of the features (e.g., a point cloud as shown in FIG. 10D). Such a mapping may be packaged in image data describing the scene and/or generated by one or more recognition systems, and the mapping may be compared to data describing the mapping in stored criteria. In some embodiments, the mapping (e.g., a point cloud and/or a set of anchors) may be generated from the image 1220 and/or from image data associated therewith (e.g., collected sensor data describing the scene, including spatial measurements from LIDAR, etc.). Additionally or alternatively, spectral data (e.g., from non-visible portions of the electromagnetic spectrum) may be collected and used to generate the mapping. In some embodiments, multiple exposures (e.g., from multiple time points, from multiple sensors on the device, etc.) may be combined to determine a deep mapping (e.g., by triangulation). In this manner, the VPS may be used to associate the image 1220 (and/or image data describing that image) with stored references for recognition, verification, and/or processing of the machine-readable visual encoding 1204.
図10A～10Dに示した例示的実施形態は情報表示(マップ)に関連するが、本開示のシステムおよび方法は、ポスター、広告、掲示板、刊行物、チラシ、車両、パーソナルモビリティビークル(たとえば、スクーター、バイクなど)、部屋、壁、家具、建物、通り、標識、タグ、ペットの首輪、プリントされた衣類、医療ブレスレットなど、機械可読視覚符号化を備えた幅広い対象を記述する画像データを処理することを理解されたい。たとえば、本開示の態様によるシステムおよび方法のいくつかの実施形態は、ウェブサーバからの情報検索のためのオブジェクト認識(たとえば、製品認識)を含み得る。たとえば、機械可読視覚符号化を備えるオブジェクトを含むシーンを記述する画像データがキャプチャされ得る。画像データは、機械可読視覚符号化を囲むコンテキスト情報(たとえば、画像および/または画像上のテキストなど、オブジェクトの他の特徴)、およびオブジェクトを囲むコンテキスト情報(たとえば、オブジェクトがどこに位置するか、近くのオブジェクトなど)を含んでもよい。いくつかの実施形態では、1つまたは複数の認識システムは、機械可読視覚符号化を処理し得、1つまたは複数の他の認識システムは、コンテキストデータを処理し得る。たとえば、画像認識システムは、オブジェクトのタイプおよび/またはそのオブジェクトを囲むオブジェクトのタイプを認識および/または識別し得る。いくつかの例では、意味認識システムは、シーンの中で認識される1つまたは複数の意味論的エンティティ(たとえば、オブジェクト上のテキストおよび/または画像)を処理して、機械可読視覚符号化の処理を支援し得る。オブジェクトの識別に応じて、そのオブジェクトを記述する情報がウェブサーバ(たとえば、認識システムに関連するウェブサーバ)から検索され、ユーザデバイス(たとえば、画像データをキャプチャしたユーザデバイス)のディスプレイデバイス上にレンダリングされ得る。 10A-10D relate to information displays (maps), it should be understood that the systems and methods of the present disclosure process image data describing a wide range of objects with machine-readable visual encoding, such as posters, advertisements, billboards, publications, flyers, vehicles, personal mobility vehicles (e.g., scooters, bikes, etc.), rooms, walls, furniture, buildings, streets, signs, tags, pet collars, printed clothing, medical bracelets, etc. For example, some embodiments of the systems and methods according to aspects of the present disclosure may include object recognition (e.g., product recognition) for information retrieval from a web server. For example, image data describing a scene including an object with machine-readable visual encoding may be captured. The image data may include contextual information surrounding the machine-readable visual encoding (e.g., the image and/or other features of the object, such as text on the image), and contextual information surrounding the object (e.g., where the object is located, nearby objects, etc.). In some embodiments, one or more recognition systems may process the machine-readable visual encoding, and one or more other recognition systems may process the contextual data. For example, an image recognition system may recognize and/or identify the type of object and/or the types of objects surrounding the object. In some examples, a semantic recognition system may process one or more semantic entities recognized in the scene (e.g., text and/or images on the object) to assist in processing the machine-readable visual encoding. In response to identifying an object, information describing the object may be retrieved from a web server (e.g., a web server associated with the recognition system) and rendered on a display device of a user device (e.g., a user device that captured the image data).
図11Aは、機械可読視覚符号化1304を含むシーンの画像1300の別の例示的実施形態を示す。示されたシーンでは、店頭は外壁上に機械可読視覚符号化1304を表示している。上記で論じたように、シーン内に示される様々な特徴は、機械可読視覚符号化1304の認識および/または検証を支援および/または相補するために、記憶された基準との比較に向けて認識および/または処理され得る。たとえば、ドア枠1310、窓枠1312、窓台1314、およびオーニング1316など、外部アーキテクチャ特徴が認識され得る。加えて、可視である場合、照明特徴1318など、内部アーキテクチャ特徴も認識され得る。
11A illustrates another example embodiment of an
上述の特徴のうちの1つまたは複数は、VPSを使用して認識され得る。たとえば、特徴マップ1320が図11Bに示すように生成され得る。いくつかの例では、シーンの対象となる特徴をマーキングおよび/またはトレースするために、アンカーポイント1322が位置特定され、随意に、セグメント1324に接続され得る。いくつかの実施形態では、特徴マップ1320は記憶された基準の特徴マップとの関連付け(たとえば、類似性)を判定するために、特徴マップと比較され得る。
One or more of the above-mentioned features may be recognized using the VPS. For example, a feature map 1320 may be generated as shown in FIG. 11B. In some examples, anchor points 1322 may be located and optionally connected to
いくつかの実施形態では、特徴マップ1320が生成され、シーンのキャプチャされた1つまたは複数の画像に基づいて、記憶された基準内に記憶され得る。たとえば、図11Cに示す一実施形態では、記憶された基準に対応するユーザアカウントに関連するユーザ1324は、撮像デバイス(たとえば、スマートフォン1326)を使用して、経路1328に沿って、複数のバンテージポイントからシーンの複数の画像をキャプチャし得る。このようにして、複数の角度からキャプチャされた画像データは、記憶された基準内に記憶されることになるシーンを記述するロバストな特徴マップを生成するために処理され得る。文書化されると、特徴マップは、その後、1つまたは複数の認識システムによる他の特徴マップ(たとえば、場合によっては、1つのバンテージポイントからのみ生成された特徴マップ)と比較するために使用され得る。
In some embodiments, a feature map 1320 may be generated and stored within the stored criteria based on one or more captured images of the scene. For example, in one embodiment shown in FIG. 11C, a
記憶された基準は、随意に、シーンを囲むオブジェクトおよび構造に関する追加のコンテキスト情報を含んでもよい。たとえば、図11Dに示すように、ユーザ1324は、撮像デバイス1326を使用して、(たとえば、ユーザ1324が回転することによって)経路1330に沿って複数のバンテージポイントから画像を収集することによって、パノラマ画像データを生成し得る。このようにして、特徴マップは、図11Aの画像1300内に示されるシーンに対してのみではなく、それに加えて、周囲のオブジェクトおよび構造に対しても(望まれる場合)生成され得る。機械可読視覚符号化1304に関連する、記憶された基準は、その機械可読視覚符号化1304に関連するすべての特徴マップを含み、本開示の実施形態による1つまたは複数の認識システムによる使用のためにコンテキストデータのロバストなセットを提供し得る。
Optionally, the stored reference may include additional contextual information regarding objects and structures surrounding the scene. For example, as shown in FIG. 11D, a
いくつかの実施形態では、機械可読視覚符号化1304は、ユーザ1324のユーザアカウントに関連する複数のロケーションのうちの1つにおいて使用されることが望まれることがある。いくつかの実施形態では、記憶された基準は、副基準にそれぞれ関連する複数のロケーションの各々に関連するコンテキストデータを含み得る。
In some embodiments, it may be desired that the machine-readable
いくつかの実施形態では、1つの機械可読視覚符号化に関連するコンテキストは、別の機械可読視覚符号化を含み得る。たとえば、いくつかの実施形態では、1つの機械可読視覚符号化において通信されるデータの処理(たとえば、それにより通信される命令の実行)は、画像データ内の別の機械可読視覚符号化の認識を条件とするように構成され得る。 In some embodiments, the context associated with one machine-readable visual encoding may include another machine-readable visual encoding. For example, in some embodiments, processing of data communicated in one machine-readable visual encoding (e.g., execution of instructions communicated thereby) may be configured to be conditioned on recognition of another machine-readable visual encoding in image data.
たとえば、図12に示す一実施形態では、セキュアなパッケージ配送のためのシステム1400が本開示の態様に従って実装され得る。たとえば、配送員1402には第1の機械可読視覚符号化1403が(たとえば、バッジ上、デバイス上に)提供され得、パッケージ1404には(たとえば、その上に印刷された)第2の機械可読視覚符号化1405が提供され得る。「スマート」ホームシステム1420は、その視野内に第1の機械可読視覚符号化1403と第2の機械可読視覚符号化1405の両方を含むシーンをキャプチャし得るカメラ1422(たとえば、ドアベルカメラ、セキュリティカメラなど)を含み得る。いくつかの実施形態では、第1の機械可読視覚符号化1403と第2の機械可読視覚符号化1405は両方とも、「スマート」ロック1424がパッケージ1404を収納するためのセキュアエリア(たとえば、パッケージロッカー、ガレージエリア、玄関先、玄関口など)へのアクセスを許可し得る前に、所与の信頼性レベルにマッチすることが必要とされ得る。いくつかの実施形態では、追加の「スマート」構成要素1426は、追加のコンテキスト情報(たとえば、配送時間、予想配送時間、予想パッケージ寸法、予想パッケージ数、アクセス許可を妨げる何らかのオーバーライド、撮像の歪みに影響を及ぼし得る天候状態、パッケージの印刷画像を用いたスプーフィングを防止するための3D撮像データなど)を提供し得る。ホームシステム1420は、随意に、画像データを通信するために(たとえば、サーバ1440上での認識タスクを処理するために)、かつ/または1つまたは複数の認識タスクのローカル処理のための基準情報を受信するために(たとえば、機械可読視覚符号化1403の予想パラメータおよび/または配送員1402の識別を検証するための顔認識データを受信するためになど)、ネットワーク1430に接続され得る。いくつかの実施形態では、第1の機械可読視覚符号化1403は、時間窓のみにわたる制限されたアクセスを提供するために生成され得る。たとえば、機械可読視覚符号化1403を記述するデータ(それに関連する何らかの時間窓を含む)は、サーバ1440上の記憶された基準内に記憶され、随意に、ホームシステム1420に送信され得る。このようにして、いくつかの実施形態では、検証された個人が検証されたパッケージを所有するときのみ、かつそれらの個人が指定された時間にアクセスを行うときのみ、アクセスはそれらの個人に許可され得る。
For example, in one embodiment shown in FIG. 12, a
図13Aは、本開示の例示的実施形態による機械可読視覚符号化を認識および/または処理する例示的コンピューティングシステム1500のブロック図を示す。システム1500は、ネットワーク1580を介して通信可能に結合されている、ユーザコンピューティングデバイス1502、サーバコンピューティングシステム1530、およびプロバイダコンピューティングシステム1560を含む。
FIG. 13A illustrates a block diagram of an
ユーザコンピューティングデバイス1502は、たとえば、パーソナルコンピューティングデバイス(たとえば、ラップトップもしくはデスクトップ)、モバイルコンピューティングデバイス(たとえば、スマートフォンもしくはタブレット)、ゲーム機もしくはコントローラ、装着可能コンピューティングデバイス(たとえば、コンピュータ対応眼鏡、ウォッチなど)、埋め込み型コンピューティングデバイス、または任意の他のタイプのコンピューティングデバイスなど、どのタイプのコンピューティングデバイスであってもよい。
The
ユーザコンピューティングデバイス1502は、1つまたは複数のプロセッサ1512およびメモリ1514を含む。1つまたは複数のプロセッサ1512は、どの適切な処理デバイス(たとえば、プロセッサコア、マイクロプロセッサ、ASIC、FPGA、コントローラ、マイクロコントローラなど)であってもよく、1つのプロセッサまたは動作可能に接続されている複数のプロセッサであってよい。メモリ1514は、RAM、SRAM、DRAM、ROM、EEPROM、EPROM、フラッシュメモリデバイス、磁気ディスクなど、およびそれらの組合せのような、1つまたは複数の非一時的コンピュータ可読記憶媒体を含み得る。メモリ1514は、データ1516と、ユーザコンピューティングデバイス1502に動作を実行させるようにプロセッサ1512によって実行される命令1518とを記憶することができる。
The
ユーザコンピューティングデバイス1502は、1つまたは複数のセンサ1520を含んでもよい。たとえば、ユーザコンピューティングデバイス1502は、ユーザ入力を受信するユーザ入力構成要素1521を含み得る。たとえば、ユーザ入力構成要素1521は、ユーザ入力オブジェクト(たとえば、指またはスタイラス)のタッチに敏感な、タッチ感応構成要素(たとえば、タッチ感応表示スクリーンまたはタッチパッド)であってよい。タッチ感応構成要素は、仮想キーボードを実装するのに役立ち得る。他の例示的ユーザ入力構成要素は、マイクロフォン、従来のキーボード、またはユーザがユーザ入力を与えることができる他の手段を含む。ユーザコンピューティングデバイス1502は、1つまたは複数の撮像センサ1522(たとえば、CCD、CMOS、RADAR、LIDARなど)を含み得る。撮像センサ1522は、各々、同じであってもまたは異なってもよく、各々は、1つまたは複数の異なるレンズ構成で構成され得る。1つまたは複数の撮像センサは、ユーザコンピューティングデバイス1502の片側に位置してよく、1つまたは複数の撮像センサは、ユーザコンピューティングデバイス1502の反対側に位置してもよい。撮像センサは、幅広い範囲の可視および/または不可視光スペクトルをキャプチャするセンサを含み得る。ユーザコンピューティングデバイス1502は、ロケーションデータを測定、記録、および/または補間するための1つまたは複数の地理空間センサ1523(たとえば、GPS)を含んでもよい。ユーザコンピューティングデバイス1502は、1つまたは複数の変換センサ1524(たとえば、加速度計など)および1つまたは複数の回転センサ1525(たとえば、インクリノメータ、ジャイロスコープなど)を含んでもよい。いくつかの実施形態では、1つまたは複数の地理空間センサ1523、1つまたは複数の変換センサ1524、および1つまたは複数の回転センサ1525は、デバイス1502の位置および/または配向を判定および記録するために協働し、1つまたは複数の撮像センサ1522と組み合わせて、撮像シーンに対するデバイス1502のポーズを判定し得る。
The
ユーザデバイス1502は、センサ1520のうちのいずれかまたはすべてを使用して収集および/または生成された画像データ1527を含んでもよい。ユーザデバイス1502は、収集された画像データ1527に対して認識タスクを実行するための1つまたは複数の認識モデル1528を含んでもよい。たとえば、認識モデル1528は、ニューラルネットワーク(たとえば、ディープニューラルネットワーク)または非線形モデルおよび/もしくは線形モデルを含む他のタイプの機械学習済みモデルなど、様々な機械学習済みモデルであってよく、またはそうでなければ、それらの機械学習済みモデルを含んでもよい。ニューラルネットワークは、フィードフォワードニューラルネットワーク、回帰型ニューラルネットワーク(たとえば、長短期メモリ回帰型ニューラルネットワーク)、畳み込みニューラルネットワーク、または他の形のニューラルネットワークを含み得る。
The
いくつかの実装形態では、1つまたは複数の認識モデル1528は、ネットワーク1580を介してサーバコンピューティングシステム1530から受信され、ユーザコンピューティングデバイスメモリ1514内に記憶され、次いで、1つまたは複数のプロセッサ1512によって使われ、またはそうでなければ実装され得る。いくつかの実装形態では、ユーザコンピューティングデバイス1502は、単一の認識モデル1528の複数の平行インスタンスを実装し得る(たとえば、機械可読視覚符号化を記述する画像データ1527の部分に対する認識タスクおよびコンテキストデータを記述する画像データ1527の部分に対する認識タスクを実行するためになど、平行認識タスクを実行するために)。
In some implementations, one or
追加または代替として、1つまたは複数の認識モデル1540は、クライアント-サーバ関係に従ってユーザコンピューティングデバイス1502と通信するサーバコンピューティングシステム1530内に含まれ、またはそうでなければ、サーバコンピューティングシステム1530によって記憶され、実装され得る。サーバコンピューティングシステム1530は、1つまたは複数のプロセッサ1532およびメモリ1534を含む。1つまたは複数のプロセッサ1532は、どの適切な処理デバイス(たとえば、プロセッサコア、マイクロプロセッサ、ASIC、FPGA、コントローラ、マイクロコントローラなど)であってもよく、1つのプロセッサまたは動作可能に接続されている複数のプロセッサであってよい。メモリ1534は、RAM、SRAM、DRAM、ROM、EEPROM、EPROM、フラッシュメモリデバイス、磁気ディスクなど、およびそれらの組合せのような、1つまたは複数の非一時的コンピュータ可読記憶媒体を含み得る。メモリ1534は、データ1536と、サーバコンピューティングシステム1530に動作を実行させるようにプロセッサ1532によって実行される命令1538とを記憶することができる。いくつかの実装形態では、サーバコンピューティングシステム1530は、1つまたは複数のサーバコンピューティングデバイスを含むか、またはそうでなければサーバコンピューティングデバイスによって実装される。サーバコンピューティングシステム1530が複数のサーバコンピューティングデバイスを含む事例では、そのようなサーバコンピューティングデバイスは、順次コンピューティングアーキテクチャ、並列コンピューティングアーキテクチャ、またはそれらの何らかの組合せに従って動作することができる。
Additionally or alternatively, the one or
いくつかの例では、認識モデル1540は、ウェブサービス(たとえば、機械可読視覚符号化認識および/または検証サービス)の一部分としてサーバコンピューティングシステム1530によって実装され得る。したがって、1つまたは複数の認識モデル1528が、ユーザコンピューティングデバイス1502において記憶され、実装されてよく、かつ/または1つまたは複数の認識モデル1540が、サーバコンピューティングシステム1530において記憶され、実装されてよい。たとえば、1つまたは複数の認識タスクは、1つまたは複数の認識モデル1528および1つまたは複数の認識モデル1540の間で共有および/または分散され得る。
In some examples, the
いくつかの例では、ユーザコンピューティングデバイス1502は、画像データ1527内で記述される任意の機械可読視覚符号化の認識および/または処理のために、ネットワーク1580を介して画像データ1527をサーバコンピューティングシステム1530に送信する。サーバコンピューティングシステム1530は、画像データ1527を記憶された基準1542に関連付けるために1つまたは複数の認識モデル1540を使用して、画像データ1527を処理し得る。
In some examples, the
いくつかの実施形態では、記憶された基準1542は、画像データ1527内の機械可読視覚符号化を復号する際に支援し得る符号化データ1544に対応し得る(たとえば、使用されることになる1つまたは複数の特定の符号化認識モデルを示すことによって、図1～図11Aで実証した符号化方式のうちの1つまたは複数など、機械可読視覚符号化を生成/解釈するために使用されるアルゴリズムを詳述することによって、など)。いくつかの実施形態では、記憶された基準は、記憶された基準1542全体に関する一般的なコンテキストデータ1546を含み得る。たとえば、記憶された基準1542は、複数の機械可読視覚符号化に関し得る。1つの例示的実施形態では、符号化は各々、特定の種類のオブジェクトまたは構造(たとえば、デバイス、マップ、広告、スクーター、車、壁、建物など)のインスタンス上にインストールされ得る。インスタンスの各々は、符号化の各々を含むシーンが、少なくともいくつかの重複コンテキストを共有することになるように、何らかの量のコンテキスト情報を共有し得る(たとえば、各符号化の画像は、スクーターの少なくとも一部分、特定のロケーションのマップ、特定の広告、本明細書で説明するような機械可読視覚符号化内に組み込まれるロゴまたは写真など、特定のロゴなどを示すことになる)。一般的なコンテキストデータ1546は、符号化間で共有されるコンテキストを記述する情報を含み得る。
In some embodiments, the stored
いくつかの実施形態では、記憶された基準1542は、第1の副基準1548に関連するコンテキストデータ1550、および随意に、第2の副基準1552に関連するコンテキストデータ1554を含み得る。副基準1548、1552は、いくつかの実装形態では、記憶された基準1542に関連する複数の符号化のサブセットに関連するコンテキスト情報を分類するために使用され得る。たとえば、前に説明した実施形態の言語を引き続き使用するために、特定の種類のオブジェクトまたは構造のインスタンスの様々なサブセットにそれぞれ対応する複数の符号化のサブセットは、1つまたは複数の副基準に関連付けられ得る。たとえば、オブジェクトまたは構造の各インスタンス(たとえば、各スクーター、各レストランロケーションなど)は、コンテキストデータ1550、1554内にそれぞれ記憶され得る、それ自体のコンテキスト(たとえば、ロケーション情報、外観など)に関連付けられ得る。このようにして、機械可読視覚符号化は、記憶された基準1542(たとえば、エンティティ、ユーザアカウント、プロジェクト、カテゴリなどに対応する)および副基準1548、1552(たとえば、特定の実装、適用例などに対応する)に関連付けられ得る。
In some embodiments, the stored
関連付けられると、サーバコンピューティングシステム1530は、動作命令1556に従って動作を開始し得る。いくつかの実施形態では、各記憶された基準1542および/または副基準1548、1552は、同じまたは異なる動作命令1556に対応し得る。いくつかの実施形態では、動作命令1556は、画像データ1527によって記述される機械可読視覚符号化を検証することを含む。検証することは、たとえば、検証インジケータをユーザコンピューティングデバイス1502に送信することを含んでよく、それに応じて、ユーザコンピューティングデバイス1502は、追加の動作(たとえば、機械可読視覚符号化において符号化された任意のデータ項目の処理)を実行し得る。たとえば、検証インジケータは、機械可読視覚符号化において符号化されたデータを処理するために必要とされるセキュリティ証明を含み得る。セキュリティ証明は、いくつかの実施形態では、ユーザコンピューティングデバイス1502によって(たとえば、ユーザコンピューティングデバイス1502上に記憶されるかつ/または上で実行されるアプリケーションによって、ユーザコンピューティングデバイス1502上で実行されるブラウザインターフェースを介してウェブサーバによって、など)必要とされ得る。たとえば、図12に示したパッケージのセキュアな配送のためのシステム1400では、「スマート」ホームシステム1420は、いくつかの実施形態では、「スマート」ロック1424がパッケージ1404を収納するためにセキュアエリアへのアクセスを許可する前に、そのようなセキュリティ証明を必要とし得る。図13Aを再度参照すると、いくつかの実施形態では、セキュリティ証明は、プロバイダコンピューティングシステム1560によって提供されるサービスにアクセスするために(たとえば、プロバイダコンピューティングシステム1560とのセキュアな接続を開始するために)プロバイダコンピューティングシステム1560によって必要とされる。
Once associated, the
いくつかの実施形態では、ユーザデバイス1502は、サーバコンピューティングシステムによって検証される機械可読視覚符号化のみを処理するように構成され得る。いくつかの実施形態では、ユーザデバイス1502が、任意の記憶された基準に関連付けられない、または記憶された基準に部分的に関連付けられるが、そのコンテキストが記憶された基準内に記憶されたコンテキストデータ1546、1550、1554と競合する、1つまたは複数の機械可読視覚符号化を撮像するとき、ユーザデバイス1502は、それを示す警告メッセージを表示するように構成され得る。たとえば、ユーザデバイス1502のユーザは、機械可読視覚符号化によって符号化されたデータの処理に進む前に、機械可読視覚符号化の検証の欠如を示すメッセージ(たとえば、ユーザデバイスのディスプレイ上にレンダリングされるメッセージ)を受け入れるおよび/または退けるように催促され得る。
In some embodiments, the
いくつかの実施形態では、検証することは、ネットワーク1580を介して検証インジケータをプロバイダコンピューティングシステム1560に送信することを含み得る。プロバイダコンピューティングシステム1560は、(たとえば、画像データ1527内で記述され得る)1つまたは複数の機械可読視覚符号化に関連付けられ得る。たとえば、1つまたは複数の機械可読視覚符号化は、プロバイダコンピューティングシステム1560に関連するエンティティのサービスと対話するためにデータ(たとえば、情報、命令)を通信するために生成され得る。
In some embodiments, validating may include transmitting the validation indicator over
プロバイダコンピューティングシステム1560は、1つまたは複数のプロセッサ1562およびメモリ1564を含む。1つまたは複数のプロセッサ1562は、どの適切な処理デバイス(たとえば、プロセッサコア、マイクロプロセッサ、ASIC、FPGA、コントローラ、マイクロコントローラなど)であってもよく、1つのプロセッサまたは動作可能に接続されている複数のプロセッサであってよい。メモリ1564は、RAM、SRAM、DRAM、ROM、EEPROM、EPROM、フラッシュメモリデバイス、磁気ディスクなど、およびそれらの組合せのような、1つまたは複数の非一時的コンピュータ可読記憶媒体を含み得る。メモリ1564は、データ1566と、プロバイダコンピューティングシステム1560に動作を実行させるようにプロセッサ1562によって実行される命令1568とを記憶することができる。いくつかの実装形態では、プロバイダコンピューティングシステム1560は、1つまたは複数のサーバコンピューティングデバイスを含むか、またはそうでなければサーバコンピューティングデバイスによって実装される。いくつかの実施形態では、プロバイダコンピューティングシステム1560は、サーバコンピューティングシステム1530を備える。
The
プロバイダコンピューティングシステム1560は、たとえば、ユーザコンピューティングデバイス1502のユーザへのサービスなどの、サービスを提供するためのサービス命令1570を含んでもよい。いくつかの実施形態では、プロバイダコンピューティングシステム1560は、ユーザコンピューティングデバイス1502が検証された機械可読視覚符号化をキャプチャおよび/またはそうでなければ処理したことを示すインジケータに応じて、サービスのプロビジョンを始める。たとえば、プロバイダコンピューティングシステム1560は、画像データ1527が、記憶された基準1542に関連するコンテキストデータまたは機械可読視覚符号化の特定のインスタンスに関連する副基準1548、1552を含むという、サーバコンピューティングシステム1530からのインジケータに基づいて、その特定のインスタンスに関連するサービスを提供し得る。このようにして、たとえば、プロバイダコンピューティングシステム1560は、提供されるサービスを調整またはそうでなければ改善し得る。
The
いくつかの実施形態では、サービス命令1570は、ユーザデバイス1502上でアプリケーションを実行するための実行可能命令を含み得る。いくつかの実施形態では、ユーザデバイス1502のユーザは、アプリケーション(または、アプリケーション内の処理)を実行するための実行可能コードへのアクセスをダウンロードまたはそうでなければ取得するために、本開示の態様による機械可読視覚符号化を走査し得る。
In some embodiments, the
いくつかの実施形態では、コンテキストデータ1572は、プロバイダコンピューティングシステム1560上で生成および/またはその中に記憶され、かつ/またはサーバコンピューティングシステム1530に送られ得る。いくつかの実施形態では、コンテキストデータ1572は、コンテキストデータ1546、1550、1554を更新するために使用され得る。いくつかの実施形態では、コンテキストデータ1572は、画像データ1527の真正性を検証するために使用され得る。たとえば、プロバイダコンピューティングシステム1560は、機械可読視覚符号化の特定のインスタンスに関連するサービスを提供するように構成され得、機械可読視覚符号化の特定のインスタンスは、コンテキストデータ1572に対応し得る。いくつかの実施形態では、コンテキストデータ1572は、画像データ1527が一意の取引を開始するために処理され得ることを確実にするために生成され得る。たとえば、一実施形態では、サーバコンピューティングシステム1530によって検証されるために、画像データ1527によって記述されなければならないコンテキストデータ1572のセットが生成され得る。たとえば、プロバイダコンピューティングシステム1560は、ターゲット機械可読視覚符号化(たとえば、相補型機械可読視覚符号化)の近くでまたはそれとともに表示されることになる視覚パターンまたは1つまたは複数のセンサ1520によって知覚可能である他のコンテキストを生成し得る。そのような例では、サーバコンピューティングシステム1530による検証は、ターゲット機械可読視覚符号化と画像データ1527によって記述されている相補型機械可読視覚符号化の両方を条件とし得る。一例では、相補型機械可読視覚符号化は、図12に示したようなセキュアパッケージ配送のためのシステム1400の一実施形態において使用され得る。
In some embodiments, the
上記で説明した例は、サーバコンピューティングシステム1530によって画像データ1527内で記述される任意の機械可読視覚符号化の認識および/または処理を参照したが、認識タスクは、ユーザコンピューティングデバイス1502とサーバコンピューティングシステム1530との間で分散され得ることを理解されたい。たとえば、ユーザコンピューティングデバイス1502は、1つまたは複数の認識モデル1528を使用して、画像データ1527の1つまたは複数の部分を処理し、画像データ1527の同じかつ/または異なる部分を、1つまたは複数の認識モデル1540を使用してサーバコンピューティングシステム1530によって画像データ1527を処理するためにサーバコンピューティングシステム1530に送ることができる。いくつかの実施形態では、1つまたは複数の認識モデル1528は、画像データ1527によって記述される任意の機械可読視覚符号化を認識するための符号化認識モデルを含み得る。いくつかの実施形態では、符号化認識モデルを使用したユーザコンピューティングデバイス1502による画像データ1527の処理に基づいて、ユーザコンピューティングデバイス1502は、追加の処理のために、ネットワーク1580を介して画像データ1527をサーバコンピューティングシステム1530に通信し得る。
While the examples described above referenced the recognition and/or processing of any machine-readable visual encoding described in the
たとえば、一例では、ユーザコンピューティングデバイス1502は、画像データ1527によって記述される1つまたは複数の機械可読視覚符号化が記述されていることを判定し得る。ユーザコンピューティングデバイス1502は、機械可読視覚符号化を復号するために符号化認識モデルを使用して、画像データ1527を処理し得る。ユーザコンピューティングデバイス1502は、機械可読視覚符号化に関連する(または関連する可能性がある)記憶された基準が存在するかどうかを判定するために、機械可読視覚符号化を記述するデータ(たとえば、画像データ1527および/または符号化自体から復号されたデータ)をサーバコンピューティングデバイス1530に送信し得る。この関連付けは、サーバコンピューティングシステム1530に送られたデータを処理することによってサーバコンピューティングシステム1530によって行われ得、データは、機械可読視覚符号化を記述する(たとえば、画像データ1527および/または符号化自体から復号されたデータ)。機械可読視覚符号化に関連する、記憶された基準1542が存在する場合、サーバコンピューティングシステムは、画像データ1527(たとえば、一般的なコンテキストデータ1546および/または1つまたは複数の副基準1548、1552に固有のコンテキストデータ1550、1554)と比較するために、記憶された基準1542および/またはそれに関連するコンテキストデータをユーザコンピューティングデバイス1502に送信し得る。そのような実施形態では、ユーザコンピューティングデバイス1502によって収集されたコンテキストデータは、デバイス上に留まり得る(たとえば、追加のプライバシー、低レイテンシなどを達成するために)。
For example, in one example, the
いくつかの例では、1つまたは複数の認識モデル1528および1つまたは複数の認識モデル1540は、様々な機械学習済みモデルであってよいか、またはそうでなければそれらを含み得る。例示的機械学習済みモデルは、ニューラルネットワークまたは他のマルチレイヤ非線形モデルを含む。例示的ニューラルネットワークは、フィードフォワードニューラルネットワーク、ディープニューラルネットワーク、回帰型ニューラルネットワーク、および畳み込みニューラルネットワークを含む。
In some examples, the one or
機械学習済みモデルは、たとえば、誤差逆伝搬など、様々なトレーニングまたは学習技法を使用してトレーニングされ得る。たとえば、損失関数は、(たとえば、損失関数の勾配に基づいて)モデルの1つまたは複数のパラメータを更新するために、モデルを通して逆伝搬され得る。平均2乗誤差、尤度損失、交差エントロピー損失、ヒンジ損失、および/または様々な他の損失関数など、様々な損失関数が使用され得る。勾配降下技法は、いくつかのトレーニング反復に対してパラメータを反復的に更新するために使用され得る。いくつかの実装形態では、誤差逆伝播を実行することは、短縮された時通的誤差逆伝播を実行することを含み得る。モデルトレーナーは、トレーニングされているモデルの汎化能力を向上するために、いくつかの汎化技法(たとえば、重み減衰、ドロップアウトなど)を実行することができる。特に、モデルトレーナーは、トレーニングデータのセットに基づいて、モデルをトレーニングすることができる。トレーニングデータは、たとえば、機械可読視覚符号化、それを記述する画像データ、を記述する画像データを含み得る。 The machine-learned model may be trained using various training or learning techniques, such as, for example, backpropagation. For example, a loss function may be backpropagated through the model to update one or more parameters of the model (e.g., based on the gradient of the loss function). Various loss functions may be used, such as mean squared error, likelihood loss, cross entropy loss, hinge loss, and/or various other loss functions. Gradient descent techniques may be used to iteratively update the parameters for several training iterations. In some implementations, performing backpropagation may include performing abbreviated temporal backpropagation. The model trainer may perform several generalization techniques (e.g., weight decay, dropout, etc.) to improve the generalization ability of the model being trained. In particular, the model trainer may train the model based on a set of training data. The training data may include, for example, image data describing a machine-readable visual encoding, image data describing the same, etc.
いくつかの実装形態では、ユーザが承諾を与えた場合、トレーニング例は、ユーザコンピューティングデバイス1502によって提供され得る。したがって、そのような実装形態では、ユーザコンピューティングデバイス1502に提供されるモデル1528は、ユーザコンピューティングデバイス1502から受信されるユーザ固有データに対してトレーニングコンピューティングシステムによってトレーニングされ得る。場合によっては、このプロセスは、モデルの個人化と呼ばれることがある。
In some implementations, if the user provides consent, the training examples may be provided by the
モデルトレーナーは、所望の機能性を提供するのに利用されるコンピュータ論理を含み得る。モデルトレーナーは、ハードウェア、ファームウェア、および/または汎用プロセッサを制御するソフトウェアで実装することができる。たとえば、いくつかの実装形態では、モデルトレーナーは、記憶デバイス上に記憶され、メモリ内にロードされ、1つまたは複数のプロセッサによって実行されるプログラムファイルを含む。他の実装形態では、モデルトレーナーは、RAMハードディスクまたは光学もしくは磁気媒体などの有形コンピュータ可読記憶媒体内に記憶されるコンピュータ実行可能命令の1つまたは複数のセットを含む。 The model trainer may include computer logic utilized to provide the desired functionality. The model trainer may be implemented in hardware, firmware, and/or software that controls a general-purpose processor. For example, in some implementations, the model trainer includes program files stored on a storage device, loaded into memory, and executed by one or more processors. In other implementations, the model trainer includes one or more sets of computer-executable instructions stored in a tangible computer-readable storage medium, such as RAM, a hard disk, or an optical or magnetic medium.
いくつかの実装形態では、(たとえば、ユーザコンピューティングデバイス1502、サーバコンピューティングシステム1530などのうちのいずれか1つの中に含まれる)本開示の機械学習済み認識モデルに対する入力(たとえば、入力、データ、および/またはトレーニング例)は、画像データであり得る。機械学習済みモデルは、画像データを処理して、出力を生成し得る。一例として、機械学習済みモデルは、画像データを処理して、画像認識出力(たとえば、画像データの認識、画像データの潜在性埋込み、画像データの符号化表現、画像データのハッシュなど)を生成し得る。別の例として、機械学習済みモデルは、画像データを処理して、画像セグメンテーション出力を生成し得る。別の例として、機械学習済みモデルは、画像データを処理して、画像分類出力を生成し得る。別の例として、機械学習済みモデルは、画像データを処理して、画像データ変更出力(たとえば、画像データの改変など)を生成し得る。別の例として、機械学習済みモデルは、画像データを処理して、符号化画像データ出力(たとえば、画像データの符号化および/または圧縮された表現など)を生成し得る。別の例として、機械学習済みモデルは、画像データを処理して、アップスケールされた画像データ出力を生成し得る。別の例として、機械学習済みモデルは、画像データを処理して、予測出力を生成し得る。
In some implementations, the input (e.g., input, data, and/or training examples) for the machine-learned recognition model of the present disclosure (e.g., included in any one of the
いくつかの実施形態では、(たとえば、ユーザコンピューティングデバイス1502、サーバコンピューティングシステム1530などのうちのいずれか1つの中に含まれる)本開示の機械学習済み認識モデルに対する入力(たとえば、入力、データ、および/またはトレーニング例)は、潜在性符号化データ(たとえば、入力の潜在性空間表現など)であり得る。機械学習済みモデルは、潜在性符号化データを処理して、出力を生成し得る。一例として、機械学習済みモデルは、潜在性符号化データを処理して、認識出力を生成し得る。別の例として、機械学習済みモデルは、潜在性符号化データを処理して、再構成出力を生成し得る。別の例として、機械学習済みモデルは、潜在性符号化データを処理して、探索出力を生成し得る。別の例として、機械学習済みモデルは、潜在性符号化データを処理して、再クラスタリング出力を生成し得る。別の例として、機械学習済みモデルは、潜在性符号化データを処理して、予測出力を生成し得る。
In some embodiments, the input (e.g., input, data, and/or training examples) to the machine-learned recognition model of the present disclosure (e.g., included in any one of the
いくつかの実装形態では、(たとえば、ユーザコンピューティングデバイス1502、サーバコンピューティングシステム1530などのうちのいずれか1つの中に含まれる)本開示の機械学習済み認識モデルに対する入力(たとえば、入力、データ、および/またはトレーニング例)は、統計データであり得る。機械学習済みモデルは、統計データを処理して、出力を生成し得る。一例として、機械学習済みモデルは、統計データを処理して、認識出力を生成し得る。別の例として、機械学習済みモデルは、統計データを処理して、予測出力を生成し得る。別の例として、機械学習済みモデルは、統計データを処理して、分類出力を生成し得る。別の例として、機械学習済みモデルは、統計データを処理して、セグメンテーション出力を生成し得る。別の例として、機械学習済みモデルは、統計データを処理して、セグメンテーション出力を生成し得る。別の例として、機械学習済みモデルは、統計データを処理して、視覚化出力を生成し得る。別の例として、機械学習済みモデルは、統計データを処理して、診断出力を生成し得る。
In some implementations, the input (e.g., input, data, and/or training examples) to the machine-learned recognition model of the present disclosure (e.g., included in any one of the
いくつかの実施形態では、(たとえば、ユーザコンピューティングデバイス1502、サーバコンピューティングシステム1530などのうちのいずれか1つの中に含まれる)本開示の機械学習済み認識モデルに対する入力(たとえば、入力、データ、および/またはトレーニング例)は、センサデータであり得る。機械学習済みモデルは、センサデータを処理して、出力を生成し得る。一例として、機械学習済みモデルは、センサデータを処理して、認識出力を生成し得る。別の例として、機械学習済みモデルは、センサデータを処理して、予測出力を生成し得る。別の例として、機械学習済みモデルは、センサデータを処理して、分類出力を生成し得る。別の例として、機械学習済みモデルは、センサデータを処理して、セグメンテーション出力を生成し得る。別の例として、機械学習済みモデルは、センサデータを処理して、セグメンテーション出力を生成し得る。別の例として、機械学習済みモデルは、センサデータを処理して、視覚化出力を生成し得る。別の例として、機械学習済みモデルは、センサデータを処理して、診断出力を生成し得る。別の例として、機械学習済みモデルは、センサデータを処理して、検出出力を生成し得る。
In some embodiments, the input (e.g., input, data, and/or training examples) for the machine-learned recognition model of the present disclosure (e.g., included in any one of the
ネットワーク1580は、ローカルエリアネットワーク(たとえば、イントラネット)、ワイドエリアネットワーク(たとえば、インターネット)、またはそれらの何らかの組合せなど、どのタイプの通信ネットワークであってもよく、任意の数のワイヤードまたはワイヤレスリンクを含み得る。概して、ネットワーク1580を介した通信は、幅広い通信プロトコル(たとえば、TCP/IP、HTTP、SMTP、FTP)、符号化もしくはフォーマット(たとえば、HTML、XML)、および/または保護方式(たとえば、VPN、セキュアHTTP、SSL)を使用して、どのタイプのワイヤードおよび/またはワイヤレス接続を介しても搬送することができる。
図13Aは、本開示を実装するために使用され得る1つの例示的コンピューティングシステムを示す。他のコンピューティングシステムが同様に使用されてもよい。たとえば、いくつかの実装形態では、ユーザコンピューティングデバイス1502は、モデルトレーナーおよびトレーニングデータセットを含み得る。そのような実装形態では、モデル1528、1540は、ユーザコンピューティングデバイス1502においてローカルにトレーニングされることと使用されることの両方が可能である。そのような実装形態のいくつかでは、ユーザコンピューティングデバイス1502は、ユーザ固有データに基づいて、モデルトレーナーを実装して、モデル1528を個人化し得る。
FIG. 13A illustrates one exemplary computing system that may be used to implement the present disclosure. Other computing systems may be used as well. For example, in some implementations, the
図13Bは、本開示の例示的実施形態による例示的コンピューティングデバイス1582のブロック図を示す。コンピューティングデバイス1582は、ユーザコンピューティングデバイスまたはサーバコンピューティングデバイスであってよい。
FIG. 13B illustrates a block diagram of an
コンピューティングデバイス1582は、いくつかのアプリケーション(たとえば、アプリケーション1～N)を含む。各アプリケーションは、それ自体の機械学習ライブラリおよび機械学習済みモデルを含む。たとえば、各アプリケーションは、機械学習済みモデルを含み得る。例示的アプリケーションは、テキストメッセージングアプリケーション、eメールアプリケーション、ディクテーションアプリケーション、仮想キーボードアプリケーション、ブラウザアプリケーションなどを含む。
The
図13Bに示すように、各アプリケーションは、コンピューティングデバイスのいくつかの他の構成要素、たとえば、1つもしくは複数のセンサ、コンテキストマネージャ、デバイス状態構成要素、および/または追加構成要素などと通信することができる。いくつかの実装形態では、各アプリケーションは、API(たとえば、パブリックAPI)を使って、各デバイス構成要素と通信することができる。いくつかの実装形態では、各アプリケーションによって使用されるAPIは、そのアプリケーションに固有である。 As shown in FIG. 13B, each application can communicate with several other components of the computing device, such as one or more sensors, a context manager, a device state component, and/or additional components. In some implementations, each application can communicate with each device component using an API (e.g., a public API). In some implementations, the API used by each application is specific to that application.
図13Cは、本開示の例示的実施形態による例示的コンピューティングデバイス1584のブロック図を示す。コンピューティングデバイス1584は、ユーザコンピューティングデバイスまたはサーバコンピューティングデバイスであってよい。
FIG. 13C illustrates a block diagram of an
コンピューティングデバイス1584は、いくつかのアプリケーション(たとえば、アプリケーション1～N)を含む。各アプリケーションは、中央インテリジェンスレイヤと通信する。例示的アプリケーションは、テキストメッセージングアプリケーション、eメールアプリケーション、ディクテーションアプリケーション、仮想キーボードアプリケーション、ブラウザアプリケーションなどを含む。いくつかの実装形態では、各アプリケーションは、API(たとえば、すべてのアプリケーションにわたる共通API)を使って、中央インテリジェンスレイヤ(およびその中に記憶されるモデル)と通信することができる。
The
中央インテリジェンスレイヤは、いくつかの機械学習済みモデルを含む。たとえば、図13Cに示すように、それぞれの機械学習済みモデル(たとえば、モデル)が、各アプリケーションに与えられ、中央インテリジェンスレイヤによって管理され得る。他の実装形態では、2つ以上のアプリケーションが、単一の機械学習済みモデルを共有することができる。たとえば、いくつかの実装形態では、中央インテリジェンスレイヤは、アプリケーションすべてに単一モデル(たとえば、単一モデル)を提供することができる。いくつかの実装形態では、中央インテリジェンスレイヤは、コンピューティングデバイス1584のオペレーティングシステム内に含まれるか、またはそうでなければ、オペレーティングシステムによって実装される。
The central intelligence layer includes several machine-learned models. For example, as shown in FIG. 13C, a respective machine-learned model (e.g., model) may be provided to each application and managed by the central intelligence layer. In other implementations, two or more applications may share a single machine-learned model. For example, in some implementations, the central intelligence layer may provide a single model (e.g., a single model) to all of the applications. In some implementations, the central intelligence layer is included within or otherwise implemented by the operating system of the
中央インテリジェンスレイヤは、中央デバイスデータレイヤと通信することができる。中央デバイスデータレイヤは、コンピューティングデバイス1584向けのデータの集中型リポジトリであってよい。図13Cに示すように、中央デバイスデータレイヤは、コンピューティングデバイスのいくつかの他の構成要素、たとえば、1つまたは複数のセンサ、コンテキストマネージャ、デバイス状態構成要素、および/または追加構成要素などと通信することができる。いくつかの実装形態では、中央デバイスデータレイヤは、API(たとえば、プライベートAPI)を使用して、各デバイス構成要素と通信することができる。
The central intelligence layer can communicate with a central device data layer. The central device data layer can be a centralized repository of data for the
例示的認識システム構成
図13Dは、本開示の例示的実施形態による例示的認識システム1590のブロック図を示す。いくつかの実装形態では、認識システム1590は、1つまたは複数の機械可読視覚符号化を含むシーンを記述する画像データ1527のセットを受信し、画像データ1527の受信の結果として、機械可読視覚符号化のうちの1つまたは複数と1つまたは複数の記憶された基準との間の関連付けを記述する認識データ1592(たとえば、それらの間の関連付けの信頼性レベル)を提供するようにトレーニングされる。いくつかの実装形態では、認識システム1590は、画像データ1527内で記述される1つまたは複数の機械可読視覚符号化を処理する(たとえば、復号する)ように動作可能である符号化認識モデル1540aを含み得る。認識システム1590は、加えて、コンテキスト画像データ1527を認識および/またはそうでなければ処理するための(たとえば、画像データ1527内で記述されるオブジェクト、人物、および/または構造を認識するための、深層マッピングまたは他のVPS技法を実行するための)画像認識モデル1540bを含み得る。画像データ1527は、モデル1540a、1540bの各々に直接的に流れ、かつ/またはそれらを通して順次に進むことができる。
Exemplary Recognition System Configuration FIG. 13D illustrates a block diagram of an
図13Eは、本開示の例示的実施形態による例示的認識システム1594のブロック図を示す。認識システム1594は、画像データ1527内に記憶されたコンテキストデータ(たとえば、画像データの1つまたは複数の画像に関連するメタデータなど、センサデータ)を直接的に処理し得るコンテキスト構成要素1596をさらに含むことを除いて、認識システム1594は、図13Dの認識システム1590と同様である。
13E illustrates a block diagram of an
いくつかの実施形態では、認識データ1592は、認識モデル1540a、1540bの各々に関連する信頼性を記述する複合スコアを含み得る。いくつかの実装形態では、認識データ1592は、和(重み付きまたは重みなし)を含み得る。いくつかの実施形態では、認識データ1592は、認識モデル1540a、1540bの各々に関連する、高い方の信頼性に基づいて、判定され得る。
In some embodiments, the
例示的方法
図14は、本開示の例示的実施形態による例示的方法1600のフローチャート図を示す。図14は、説明および考察のために、具体的順序で実行されるステップを示すが、本開示の方法は、具体的に示す順序または並びには限定されない。方法1600の様々なステップは、本開示の範囲から逸脱することなく、様々に省かれ、並べ替えられ、組み合わされ、かつ/または適応されてよい。
Exemplary Methods Figure 14 illustrates a flow chart diagram of an
1610において、コンピューティングシステムは、機械可読視覚符号化を含むシーンを記述する画像データを取得する。 At 1610, the computing system obtains image data describing the scene including a machine-readable visual encoding.
1620において、コンピューティングシステムは、機械可読視覚符号化を認識するように構成された第1の認識システムを用いて画像データを処理する。 At 1620, the computing system processes the image data using a first recognition system configured to recognize the machine-readable visual encoding.
1630において、コンピューティングシステムは、機械可読視覚符号化を囲むシーンの周囲部分を認識するように構成された第2の異なる認識システムを用いて画像データを処理する。いくつかの実施形態では、画像データは、機械可読視覚符号化に関連し、情報表示および/または広告を含むシーンの周囲部分の中に含まれる、像を含む。いくつかの実施形態では、第2の異なる認識システムは、シーンの周囲部分の視覚特徴を抽出するように構成された視覚測位システムを備える。いくつかの実施形態では、第2の異なる認識システムは、機械可読視覚符号化に関連し、シーンの周囲部分の中で参照される、意味論的エンティティを認識するように構成された意味認識システムを備える。いくつかの実施形態では、第2の認識システムは、画像データに関連するメタデータ(たとえば、ロケーションデータ)を処理する。 At 1630, the computing system processes the image data with a second, different recognition system configured to recognize peripheral portions of the scene surrounding the machine-readable visual encoding. In some embodiments, the image data includes imagery associated with the machine-readable visual encoding and included within the peripheral portions of the scene including information displays and/or advertisements. In some embodiments, the second, different recognition system comprises a visual positioning system configured to extract visual features of the peripheral portions of the scene. In some embodiments, the second, different recognition system comprises a semantic recognition system configured to recognize semantic entities associated with the machine-readable visual encoding and referenced within the peripheral portions of the scene. In some embodiments, the second recognition system processes metadata (e.g., location data) associated with the image data.
1640において、コンピューティングシステムは、画像データに基づいて第1の認識システムによって生成された1つまたは複数の第1の出力に少なくとも部分的に基づいて、かつ画像データに基づいて第2の認識システムによって生成された1つまたは複数の第2の出力に少なくとも部分的に基づいて、機械可読視覚符号化に関連する、記憶された基準を識別する。いくつかの実施形態では、識別は、1つまたは複数の第1の出力および1つまたは複数の第2の出力に基づく複合出力に基づく。いくつかの実施形態では、1つまたは複数の第1の出力のうちの少なくとも1つは、ターゲット値を満たすことに失敗する可能性があり、1つまたは複数の第1の出力のうちの少なくとも1つがターゲット値を満たすことに失敗するとの判定に応じて、コンピューティングシステムは、第2の認識システムによって1つまたは複数の第2の出力を生成し得る。 At 1640, the computing system identifies a stored criterion associated with the machine-readable visual encoding based at least in part on the one or more first outputs generated by the first recognition system based on the image data and based at least in part on the one or more second outputs generated by the second recognition system based on the image data. In some embodiments, the identification is based on a composite output based on the one or more first outputs and the one or more second outputs. In some embodiments, at least one of the one or more first outputs may fail to meet a target value, and in response to a determination that at least one of the one or more first outputs fails to meet the target value, the computing system may generate one or more second outputs by the second recognition system.
1650において、コンピューティングシステムは、記憶された基準の識別に応じて、1つまたは複数のアクションを実行する。いくつかの実施形態では、コンピューティングシステムは、検証インジケータを生成する。いくつかの実施形態では、検証インジケータは、機械可読視覚符号化によって符号化されたデータを処理するために必要とされるセキュリティ証明を提供するように構成される。いくつかの実施形態では、機械可読視覚符号化において符号化されたデータは、セキュアエリアへのアクセスを取得するための要求に関連付けられ、シーンは、2つ以上の機械可読視覚符号化を含み、セキュリティ証明は、セキュアエリアへのアクセスを取得するための要求のサービスを開始するために必要とされる。いくつかの実施形態では、セキュアエリアへのアクセスを取得するための要求は、パッケージのセキュアエリアへの配送のためのパッケージ配送エンティティに関連付けられ、パッケージは、2つ以上の機械可読視覚符号化のうちの少なくとも1つを含む。 At 1650, the computing system performs one or more actions in response to identifying the stored criteria. In some embodiments, the computing system generates a validation indicator. In some embodiments, the validation indicator is configured to provide a security credential required to process the data encoded by the machine-readable visual encoding. In some embodiments, the data encoded in the machine-readable visual encoding is associated with a request to gain access to a secure area, the scene includes two or more machine-readable visual encodings, and the security credential is required to initiate servicing of the request to gain access to the secure area. In some embodiments, the request to gain access to the secure area is associated with a package delivery entity for delivery of a package to the secure area, and the package includes at least one of the two or more machine-readable visual encodings.
追加開示
本明細書において論じた技術は、サーバ、データベース、ソフトウェアアプリケーション、および他のコンピュータベースのシステム、ならびに行われるアクションおよびそのようなシステムとの間で送られる情報を参照する。コンピュータベースのシステムの固有柔軟性により、構成要素の間でのタスクおよび機能性の非常に様々な可能な構成、組合せ、および分割ができるようになる。たとえば、本明細書において論じるプロセスは、組合せで動く、単一のデバイスもしくは構成要素または複数のデバイスもしくは構成要素を使って実装することができる。データベースおよびアプリケーションは、単一のシステム上で実装されるか、または複数のシステムに分散されてよい。分散構成要素は、順次、または並行して動作することができる。
ADDITIONAL DISCLOSURE The techniques discussed herein refer to servers, databases, software applications, and other computer-based systems, as well as actions taken and information sent to and from such systems. The inherent flexibility of computer-based systems allows for a wide variety of possible configurations, combinations, and divisions of tasks and functionality among components. For example, the processes discussed herein can be implemented using a single device or component or multiple devices or components working in combination. Databases and applications may be implemented on a single system or distributed across multiple systems. Distributed components can operate sequentially or in parallel.
本主題を、その様々な具体的な例示的実施形態に関して詳しく記載したが、各例は、本開示の限定ではなく、説明として与えられている。当業者は、上記内容を理解すると、そのような実施形態に対する改変、変形、および等価物を容易に作り出すことができる。したがって、本開示は、当業者には容易に明らかであろうように、本主題へのそのような修正、変形および/または追加を含めることを排除しない。たとえば、一実施形態の一部として示され、または記載される特徴は、またさらなる実施形態をもたらすために、別の実施形態とともに使われてよい。したがって、本開示は、そのような改変、変形、および等価物をカバーすることが意図される。 Although the present subject matter has been described in detail with respect to various specific exemplary embodiments thereof, each example is provided by way of explanation, not limitation, of the present disclosure. Those skilled in the art, upon understanding the above content, can easily create modifications, variations, and equivalents to such embodiments. Thus, the present disclosure does not exclude the inclusion of such modifications, variations, and/or additions to the present subject matter as would be readily apparent to those skilled in the art. For example, features illustrated or described as part of one embodiment may be used with another embodiment to yield yet a further embodiment. Thus, the present disclosure is intended to cover such modifications, variations, and equivalents.
100 機械可読視覚符号化
102 字形
104 視覚パターン
106 形状
200 機械可読視覚符号化
300 符号化、機械可読視覚符号化
304 視覚パターン
308 中央スペース
310 テキストおよび/またはグラフィカル情報
400 符号化
404 視覚パターン
408 中央スペース
410 人物の写真または他方のアバター
500 符号化
504 視覚パターン
506a 形状
506b 形状
600 符号化
604 視覚パターン
606 形状
700 符号化
704 視覚パターン
706 形状
900 符号化
1000 符号化
1006a 同心リング
1006b 形状
1006c 放射状に分散された軌道
1100 符号化
1104 視覚パターン
1106a 円形形状
1106b 円形形状
1106c 形状
1150 レンダリング、進捗レンダリング
1152 角度
1200 画像
1202 マップ、ディスプレイ
1204 機械可読視覚符号化、符号化
1206 テキスト材料、テキスト情報
1202 マップ
1204 機械可読視覚符号化
1210 画像
1212a テキストラベル
1212b テキストラベル
1214 機械可読視覚符号化
1216a マッピング特徴
1216b マッピング特徴
1220 画像
1222 大型ロゴ
1224 フレーム
1226a ベンチ
1226b ベンチ
1228 接合部
1230 境界
1232 照明
1300 画像
1304 機械可読視覚符号化
1310 ドア枠
1312 窓枠
1314 窓台
1316 オーニング
1318 照明特徴
1320 特徴マップ
1322 アンカーポイント
1324 セグメント、ユーザ
1326 スマートフォン、撮像デバイス
1328 経路
1330 経路
1400 システム
1402 配達員、顔認識データ
1403 第1の機械可読視覚符号化
1404 パッケージ
1405 第2の機械可読視覚符号化
1420 「スマート」ホームシステム、ホームシステム
1422 カメラ
1424 「スマート」ロック
1426 「スマート」構成要素
1430 ネットワーク
1440 サーバ
1500 コンピューティングシステム、システム
1502 ユーザコンピューティングデバイス、デバイス、ユーザデバイス
1512 プロセッサ
1514 メモリ、ユーザコンピューティングデバイスメモリ
1516 データ
1518 命令
1520 センサ
1521 ユーザ入力構成要素
1522 撮像センサ
1523 地理空間センサ
1524 変換センサ
1525 回転センサ
1527 画像データ
1528 認識モデル、モデル
1530 サーバコンピューティングシステム
1532 プロセッサ
1534 メモリ
1536 データ
1538 命令
1540 認識モデル、モデル
1540a 符号化認識モデル、モデル、認識モデル
1540b 画像認識モデル、モデル、認識モデル
1542 記憶された基準
1544 符号化データ
1546 コンテキストデータ
1548 第1の副基準、副基準
1550 コンテキストデータ
1552 第2の副基準、副基準
1554 コンテキストデータ
1556 命令、動作命令
1560 プロバイダコンピューティングシステム
1562 プロセッサ
1564 メモリ
1566 データ
1568 命令
1570 命令、サービス命令
1572 コンテキストデータ
1580 ネットワーク
1582 コンピューティングデバイス
1584 コンピューティングデバイス
1590 認識システム
1592 認識データ
1594 認識システム
1596 コンテキスト構成要素
1600 方法
100 Machine-readable visual coding
102 glyphs
104 Visual Patterns
106 Shape
200 Machine-readable visual coding
300 Coding, Machine-Readable Visual Coding
304 Visual Patterns
308 Central Space
310 Textual and/or Graphical Information
400 Encoding
404 Visual Patterns
408 Central Space
410 A person's photo or other avatar
500 Encoding
504 Visual Patterns
506a Shape
506b Shape
600 Encoding
604 Visual Patterns
606 Shape
700 Encoding
704 Visual Patterns
706 Shape
900 Encoding
1000 encoding
1006a Concentric Ring
1006b Shape
1006c Radially distributed orbits
1100 Encoding
1104 Visual Patterns
1106a Circular shape
1106b Circular shape
1106c Shape
1150 Rendering, Progress Rendering
1152 Angle
1200 images
1202 Maps, displays
1204 Machine-readable visual coding, coding
1206 Text materials, text information
1202 Map
1204 Machine-readable visual coding
1210 images
1212a Text Label
1212b Text Label
1214 Machine-readable visual coding
1216a Mapping Features
1216b Mapping Features
1220 images
1222 Large logo
1224 Frames
1226a Bench
1226b Bench
1228 Junction
1230 Boundary
1232 Lighting
1300 images
1304 Machine-readable visual coding
1310 Door frame
1312 Window Frame
1314 Window sill
1316 Awning
1318 Lighting Features
1320 Feature Map
1322 Anchor Point
1324 segments, users
1326 Smartphones, imaging devices
1328 Routes
1330 Route
1400 System
1402 Deliveryman, facial recognition data
1403 First machine-readable visual coding
1404 Package
1405 Second machine-readable visual coding
1420 "Smart" Home Systems, Home Systems
1422 Camera
1424 "Smart" Lock
1426 "Smart" Components
1430 Network
1440 Server
1500 Computing system, system
1502 User Computing Device, Device, User Device
1512 processor
1514 Memory, User Computing Device Memory
1516 Data
1518 Order
1520 Sensor
1521 User Input Components
1522 Image sensor
1523 Geospatial Sensor
1524 Conversion Sensor
1525 Rotation Sensor
1527 Image data
1528 Recognition Model, Model
1530 Server Computing System
1532 processor
1534 Memory
1536 Data
1538 Order
1540 Recognition Model, Model
1540a Coding recognition model, model, recognition model
1540b Image Recognition Model, Model, Recognition Model
1542 Memorized Criteria
1544 encoded data
1546 Context Data
1548 First subcriterion, subcriterion
1550 Context Data
1552 Second subcriterion, subcriterion
1554 Context Data
1556 Commands, action commands
1560 Provider Computing System
1562 Processor
1564 memory
1566 Data
1568 Order
1570 Orders, Service Orders
1572 Context Data
1580 Network
1582 Computing Devices
1584 Computing Devices
1590 Recognition System
1592 Recognition Data
1594 Recognition System
1596 Context Components
1600 Ways
Claims (19)
1つまたは複数のコンピューティングデバイスを備えたコンピューティングシステムによって、機械可読視覚符号化を含むシーンを記述する画像データを取得するステップと、
前記コンピューティングシステムによって、前記機械可読視覚符号化を認識するように構成された第1の認識システムを用いて前記画像データを処理するステップと、
前記コンピューティングシステムによって、前記機械可読視覚符号化を囲む前記シーンの周囲部分の空間的特徴を認識するように構成された第2の異なる認識システムを用いて前記画像データを処理するステップであって、前記空間的特徴は、前記第2の異なる認識システムが備える視覚測位システムによって生成された特徴マップを含む、ステップと、
前記コンピューティングシステムによって、前記画像データに基づいて前記第1の認識システムによって生成された1つまたは複数の第1の出力に少なくとも部分的に基づいて、かつ前記画像データに基づいて前記第2の異なる認識システムによって生成された1つまたは複数の第2の出力に少なくとも部分的に基づいて、前記機械可読視覚符号化に関連する、記憶された基準を識別するステップと、
前記コンピューティングシステムによって、前記記憶された基準の前記識別に応じて、1つまたは複数のアクションを実行するステップと
を含む、コンピュータ実装方法。 1. A computer-implemented method for processing a machine-readable visual encoding, comprising:
acquiring, by a computing system having one or more computing devices, image data describing a scene including a machine-readable visual encoding;
processing, by the computing system, the image data with a first recognition system configured to recognize the machine-readable visual encoding;
processing, by the computing system, the image data with a second, different recognition system configured to recognize spatial features of a surrounding portion of the scene surrounding the machine-readable visual encoding, the spatial features comprising a feature map generated by a visual positioning system included in the second, different recognition system;
identifying, by the computing system, a stored criteria associated with the machine-readable visual encoding based at least in part on one or more first outputs generated by the first recognition system based on the image data and based at least in part on one or more second outputs generated by the second, different recognition system based on the image data ;
and performing, by the computing system, one or more actions in response to the identification of the stored criteria.
前記第2の異なる認識システムが、前記抽出された空間的特徴を、前記記憶された基準に関連する、前に文書化された空間的特徴と比較して、前記シーンの前記周囲部分を認識するように構成される
請求項1に記載のコンピュータ実装方法。 the second, distinct recognition system comprising a visual positioning system configured to extract spatial features of the surrounding portion of the scene in order to recognize a location of the machine-readable visual encoding;
2. The computer-implemented method of claim 1, wherein the second, different recognition system is configured to compare the extracted spatial features to previously documented spatial features associated with the stored criteria to recognize the surrounding portion of the scene.
前記シーンが、2つ以上の機械可読視覚符号化を含み、
前記セキュリティ証明が、前記セキュアエリアへのアクセスを取得するための前記要求のサービスを開始するために必要とされる
請求項10に記載のコンピュータ実装方法。 the data encoded in the machine-readable visual encoding is associated with a request to gain access to a secure area;
the scene comprises two or more machine-readable visual encodings;
The computer-implemented method of claim 10 , wherein the security credentials are required to initiate servicing of the request to gain access to the secure area.
1つまたは複数のプロセッサと、
命令をまとめて記憶する1つまたは複数のコンピュータ可読記憶媒体と
を含み、前記命令が、前記1つまたは複数のプロセッサによって実行されると、前記コンピューティングシステムに、
械可読視覚符号化を含むシーンを記述する画像データを取得することと、
前記機械可読視覚符号化を認識するように構成された第1の認識システムを用いて前記画像データを処理することと、
前記機械可読視覚符号化を囲む前記シーンの周囲部分の空間的特徴を認識するように構成された第2の異なる認識システムを用いて前記画像データを処理することであって、前記空間的特徴は、前記第2の異なる認識システムが備える視覚測位システムによって生成された特徴マップを含む、ことと、
前記画像データに基づいて前記第1の認識システムによって生成された1つまたは複数の第1の出力に少なくとも部分的に基づいて、かつ前記画像データに基づいて前記第2の異なる認識システムによって生成された1つまたは複数の第2の出力に少なくとも部分的に基づいて、前記機械可読視覚符号化に関連する、記憶された基準を識別することと、
前記記憶された基準の前記識別に応じて、1つまたは複数のアクションを実行することと
を含む動作を実行させる、コンピューティングシステム。 1. A computing system for processing a machine-readable visual encoding, comprising:
one or more processors;
and one or more computer-readable storage media collectively storing instructions that, when executed by the one or more processors, cause the computing system to:
obtaining image data describing a scene including a machine-readable visual encoding;
processing the image data with a first recognition system configured to recognize the machine-readable visual encoding;
processing the image data with a second, different recognition system configured to recognize spatial features of a surrounding portion of the scene surrounding the machine-readable visual encoding, the spatial features comprising a feature map generated by a visual positioning system included in the second, different recognition system;
identifying a stored criteria associated with the machine-readable visual encoding based at least in part on one or more first outputs generated by the first recognition system based on the image data and based at least in part on one or more second outputs generated by the second, different recognition system based on the image data;
performing one or more actions in response to said identification of said stored criteria;
A computing system that performs operations including :
機械可読視覚符号化を含むシーンを記述する画像データを受信すること、
前記画像データを記憶された基準に関連付けることであって、前記関連付けることが、
第1の認識システムを用いて、前記機械可読視覚符号化と前記記憶された基準との間の第1の類似性を判定することと、
第2の異なる認識システムを用いて、前記機械可読視覚符号化を囲む前記シーンの周囲部分の空間的特徴を認識するとともに、前記空間的特徴と前記記憶された基準との間の第2の類似性を判定することであって、前記空間的特徴は、前記第2の異なる認識システムが備える視覚測位システムによって生成された特徴マップを含む、こととを含み、
前記関連付けに基づいて、1つまたは複数の動作を開始すること
を含む動作を実行させる
1つまたは複数のコンピュータ可読記憶媒体。 One or more computer-readable storage media collectively storing instructions that, when executed by one or more processors, cause the one or more processors to:
receiving image data describing a scene including a machine-readable visual encoding ;
Correlating the image data to stored criteria , said correlating comprising:
determining a first similarity between the machine-readable visual encoding and the stored reference using a first recognition system;
using a second, different recognition system to recognize spatial features of a surrounding portion of the scene surrounding the machine-readable visual encoding and to determine a second similarity between the spatial features and the stored reference , the spatial features comprising a feature map generated by a visual positioning system included in the second, different recognition system ;
performing an action based on the association, including initiating one or more actions;
One or more computer-readable storage media.
前記第1の類似性および前記第2の類似性の各々に関連する信頼性レベルに基づいて、検証インジケータを生成すること
を含む、請求項18に記載の1つまたは複数のコンピュータ可読記憶媒体。 Initiating the one or more actions based on the association.
20. The one or more computer-readable storage media of claim 18 , comprising generating a verification indicator based on a confidence level associated with each of the first similarity and the second similarity.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
JP2023084795A JP2023101610A (en) | 2020-09-18 | 2023-05-23 | Platform for registering and processing visual encodings |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/025,597 US11429804B2 (en) | 2020-09-18 | 2020-09-18 | Platform for registering and processing visual encodings |
US17/025,597 | 2020-09-18 |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2023084795A Division JP2023101610A (en) | 2020-09-18 | 2023-05-23 | Platform for registering and processing visual encodings |
Publications (2)
Publication Number | Publication Date |
---|---|
JP2022051558A JP2022051558A (en) | 2022-03-31 |
JP7486464B2 true JP7486464B2 (en) | 2024-05-17 |
Family
ID=
Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2019026873A1 (en) | 2017-07-31 | 2019-02-07 | カレンシーポート株式会社 | Mutual authentication system, authentication image, and recording medium |
Patent Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2019026873A1 (en) | 2017-07-31 | 2019-02-07 | カレンシーポート株式会社 | Mutual authentication system, authentication image, and recording medium |
Non-Patent Citations (1)
Title |
---|
ＳＡＮＤＩ，Ｓｔｅｖａｎ ｅｔ ａｌ、Ｓｍａｒｔ ｔａｇｓ ｆｏｒ ｂｒａｎｄ ｐｒｏｔｅｃｔｉｏｎ ａｎｄ ａｎｔｉ－ｃｏｕｎｔｅｒｆｅｉｔｉｎｇ ｉｎ ｗｉｎｅ ｉｎｄｕｓｔｒｙ、２０１８ ２３ｒｄ Ｉｎｔｅｒｎａｔｉｏｎａｌ Ｓｃｉｅｎｔｉｆｉｃ－Ｐｒｏｆｅｓｓｉｏｎａｌ Ｃｏｎｆｅｒｅｎｃｅ ｏｎ Ｉｎｆｏｒｍａｔｉｏｎ Ｔｅｃｈｎｏｌｏｇｙ（ＩＴ）、米国、２０１８．０４．３０発行、ｐｐ．１－５ |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US10679443B2 (en) | System and method for controlling access to a building with facial recognition | |
US9569778B2 (en) | Efficient prevention of fraud | |
US9576194B2 (en) | Method and system for identity and age verification | |
CA2925325C (en) | Document authentication based on expected wear | |
KR102125379B1 (en) | Apparatus and method for verifying identification based on deep learning | |
CN111587445A (en) | Security tag | |
US20140270404A1 (en) | Efficient prevention of fraud | |
US20130153651A1 (en) | Encoding information in illumination patterns | |
US20140270409A1 (en) | Efficient prevention of fraud | |
US20130156330A1 (en) | Encoding information in illumination patterns | |
US20200226407A1 (en) | Delivery of digital content customized using images of objects | |
US11488419B2 (en) | Identity and liveness verification | |
JP2023101610A (en) | Platform for registering and processing visual encodings | |
Pandimurugan et al. | IoT based face recognition for smart applications using machine learning | |
CN112651333B (en) | Silence living body detection method, silence living body detection device, terminal equipment and storage medium | |
JP7486464B2 (en) | A platform for registering and processing visual coding | |
US20240135126A1 (en) | Platform for Registering and Processing Visual Encodings | |
US20230216684A1 (en) | Integrating and detecting visual data security token in displayed data via graphics processing circuitry using a frame buffer | |
CN115880530A (en) | Detection method and system for resisting attack | |
Shichkina et al. | Synthesis of the method of operative image analysis based on metadata and methods of searching for embedded images | |
Hassani et al. | Efficiently mitigating face-swap-attacks: compressed-PRNU verification with sub-zones | |
Ajitha et al. | Face recognition in digital documents with live image | |
CN117746117A (en) | Variant image recognition and model training method thereof and electronic equipment | |
CN116958495A (en) | Digital collection generation method, system, electronic equipment and storage medium | |
GB2533721A (en) | A method, apparatus and system of encoding content and an image |