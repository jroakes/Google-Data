CN117396887A - Pixel-level video prediction with improved performance and efficiency - Google Patents
Pixel-level video prediction with improved performance and efficiency Download PDFInfo
- Publication number
- CN117396887A CN117396887A CN202280035001.2A CN202280035001A CN117396887A CN 117396887 A CN117396887 A CN 117396887A CN 202280035001 A CN202280035001 A CN 202280035001A CN 117396887 A CN117396887 A CN 117396887A
- Authority
- CN
- China
- Prior art keywords
- video
- computing system
- machine
- prediction model
- video prediction
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000000034 method Methods 0.000 claims abstract description 50
- 230000008569 process Effects 0.000 claims abstract description 11
- 238000012549 training Methods 0.000 claims description 82
- 238000010801 machine learning Methods 0.000 claims description 32
- 230000006870 function Effects 0.000 claims description 12
- 230000009471 action Effects 0.000 claims description 11
- 230000005284 excitation Effects 0.000 claims description 7
- 238000001125 extrusion Methods 0.000 claims description 7
- 238000010606 normalization Methods 0.000 claims description 6
- 230000004913 activation Effects 0.000 claims description 5
- 238000012545 processing Methods 0.000 claims description 5
- 238000001514 detection method Methods 0.000 claims description 4
- 230000007787 long-term memory Effects 0.000 claims description 2
- 238000010200 validation analysis Methods 0.000 claims 3
- 239000010410 layer Substances 0.000 description 15
- 238000013528 artificial neural network Methods 0.000 description 14
- 230000015654 memory Effects 0.000 description 14
- 238000012360 testing method Methods 0.000 description 13
- 238000002474 experimental method Methods 0.000 description 10
- 230000033001 locomotion Effects 0.000 description 9
- 230000008901 benefit Effects 0.000 description 8
- 238000010586 diagram Methods 0.000 description 8
- 239000008186 active pharmaceutical agent Substances 0.000 description 7
- 238000009826 distribution Methods 0.000 description 7
- 230000004438 eyesight Effects 0.000 description 6
- 230000001143 conditioned effect Effects 0.000 description 5
- 238000013461 design Methods 0.000 description 5
- 238000003860 storage Methods 0.000 description 5
- 239000003795 chemical substances by application Substances 0.000 description 4
- 238000004891 communication Methods 0.000 description 4
- 230000000694 effects Effects 0.000 description 4
- 238000011156 evaluation Methods 0.000 description 4
- 238000003909 pattern recognition Methods 0.000 description 4
- 230000000306 recurrent effect Effects 0.000 description 4
- 230000002787 reinforcement Effects 0.000 description 4
- 230000000007 visual effect Effects 0.000 description 4
- 238000004458 analytical method Methods 0.000 description 3
- 238000013459 approach Methods 0.000 description 3
- 230000006399 behavior Effects 0.000 description 3
- 230000007246 mechanism Effects 0.000 description 3
- 238000012986 modification Methods 0.000 description 3
- 230000004048 modification Effects 0.000 description 3
- 238000005070 sampling Methods 0.000 description 3
- 241000282412 Homo Species 0.000 description 2
- 238000000137 annealing Methods 0.000 description 2
- 238000013527 convolutional neural network Methods 0.000 description 2
- 238000005259 measurement Methods 0.000 description 2
- 238000005457 optimization Methods 0.000 description 2
- 238000011160 research Methods 0.000 description 2
- 230000002123 temporal effect Effects 0.000 description 2
- 238000004800 variational method Methods 0.000 description 2
- ORILYTVJVMAKLC-UHFFFAOYSA-N Adamantane Natural products C1C(C2)CC3CC1CC2C3 ORILYTVJVMAKLC-UHFFFAOYSA-N 0.000 description 1
- 241000501754 Astronotus ocellatus Species 0.000 description 1
- 241001351225 Sergey Species 0.000 description 1
- 230000006978 adaptation Effects 0.000 description 1
- 238000007792 addition Methods 0.000 description 1
- 230000003416 augmentation Effects 0.000 description 1
- 238000010276 construction Methods 0.000 description 1
- 238000013434 data augmentation Methods 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 230000003760 hair shine Effects 0.000 description 1
- 238000005286 illumination Methods 0.000 description 1
- 230000010365 information processing Effects 0.000 description 1
- 230000003993 interaction Effects 0.000 description 1
- 230000000116 mitigating effect Effects 0.000 description 1
- 230000001537 neural effect Effects 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 238000005096 rolling process Methods 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 230000006403 short-term memory Effects 0.000 description 1
- 238000004088 simulation Methods 0.000 description 1
- 239000002356 single layer Substances 0.000 description 1
- 238000000638 solvent extraction Methods 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000016776 visual perception Effects 0.000 description 1
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/50—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding
- H04N19/59—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding involving spatial sub-sampling or interpolation, e.g. alteration of picture size or resolution
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/42—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals characterised by implementation details or hardware specially adapted for video compression or decompression, e.g. dedicated software implementation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/047—Probabilistic or stochastic networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/102—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or selection affected or controlled by the adaptive coding
- H04N19/117—Filters, e.g. for pre-processing or post-processing
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/169—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding
- H04N19/17—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being an image region, e.g. an object
- H04N19/176—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being an image region, e.g. an object the region being a block, e.g. a macroblock
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/50—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding
- H04N19/503—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding involving temporal prediction
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
Abstract
One aspect provides a machine-learned video prediction model configured to receive and process one or more previous video frames to generate one or more predicted subsequent video frames, wherein the machine-learned video prediction model comprises a convolutional variational automatic encoder, and wherein the convolutional variational automatic encoder comprises an encoder portion comprising one or more encoding units and a decoder portion comprising one or more decoding units.
Description
RELATED APPLICATIONS
The present application claims priority and benefit from U.S. provisional patent application No. 63/194,457 filed 5/28 of 2021. U.S. provisional patent application No. 63/194,457 is incorporated herein by reference in its entirety.
Technical Field
The present disclosure relates generally to machine learning. More particularly, the present disclosure relates to systems and methods for performing pixel-level video prediction with improved performance and efficiency.
Background
Predicting what happens next is one of the key capabilities of intelligent kerbstones and humans, who rely largely on this capability in daily life to make decisions. This capability enables the system to anticipate future events and plan ahead of time to perform temporarily extended tasks. While machine learning literature has studied a wide range of predictive problems, one of the most immediate challenges is predicting the original sensory input. In particular, prediction of future visual inputs (i.e., pixel-level video prediction) conditioned on context of past observations outlines challenges of visual perception, modeling of physical events, and reasoning about uncertain behavior.
Video prediction can be generalized to self-supervision problems, enabling the system to use large amounts of unlabeled data to provide powerful predictive capabilities for autonomous systems and learn rich representations of downstream tasks. Video models have been successfully deployed in applications such as robotics, simulation, and video composition from a single frame.
Despite recent advances in generative models in many areas such as images and text, video prediction is still considered to be extremely challenging. Current state-of-the-art methods are limited to low resolution video (typically 64 x 64 and a maximum of 256 x 256) in very narrow fields, such as single person walking or robotic arms pushing objects in a stationary setting. Even in such limited areas, the quality of the predicted frames tends to drop significantly after less than 10 seconds in the future.
Disclosure of Invention
Aspects and advantages of embodiments of the disclosure will be set forth in part in the description which follows, or may be learned from the description, or may be learned by practice of the embodiments.
One example aspect of the present disclosure is directed to a computing system for video prediction with improved efficiency. The computing system may include one or more processors and one or more non-transitory computer-readable media. The medium may collectively store a machine-learned video prediction model configured to receive and process one or more previous video frames to generate one or more predicted subsequent video frames. The machine-learned video prediction model may include a convolutional variational automatic encoder. The convolutional diversity automatic encoder may comprise an encoder section comprising one or more encoding units and a decoder section comprising one or more decoding units. The media may collectively store instructions that, when executed by one or more processors, cause a computing system to process one or more previous video frames using a machine-learned video prediction model to generate one or more predicted subsequent video frames.
Another example aspect of the present disclosure is directed to a computing system for data flow prediction with improved efficiency. The computing system may include one or more processors and one or more non-transitory computer-readable media. The medium may collectively store a machine-learned data flow prediction model configured to receive and process one or more previous data sets to generate one or more predicted subsequent data sets. The machine-learned data stream prediction model may include a convolutional variational automatic encoder. The convolutional diversity automatic encoder may comprise an encoder section comprising one or more encoding units and a decoder section comprising one or more decoding units. The media may collectively store instructions that, when executed by one or more processors, cause a computing system to process one or more previous data sets using a machine-learned data flow prediction model to generate one or more predicted subsequent data sets. In some implementations, the data stream includes a sensor data stream generated by a sensor. In some embodiments, the sensor comprises a camera, a light detection and ranging sensor, or a radio detection and ranging sensor.
Another example aspect of the present disclosure is directed to a computer-implemented method for video prediction with an improved balance between efficiency and performance. The method includes obtaining one or more previous video frames associated with a video. The method includes accessing a machine-learned video prediction model configured to perform video prediction, wherein the machine-learned video prediction model includes a model architecture selected to overfit a training dataset comprising one or more training videos. The method includes performing a respective enhancement operation on each of the one or more training videos to generate one or more enhancement training videos forming an enhancement data set. The method includes training a machine-learned video prediction model using one or more enhancement training videos of an enhancement data set to perform video prediction.
Other aspects of the disclosure are directed to various systems, apparatuses, non-transitory computer-readable media, user interfaces, and electronic devices.
These and other features, aspects, and advantages of various embodiments of the present disclosure will become better understood with reference to the following description and appended claims. The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate exemplary embodiments of the disclosure and together with the description, serve to explain the principles of interest.
Drawings
A detailed discussion of embodiments directed to one of ordinary skill in the art is set forth in the specification with reference to the accompanying drawings, in which:
fig. 1 depicts a graphical diagram of an example video prediction setting, according to an example embodiment of the present disclosure.
FIG. 2 depicts a block diagram of an example machine learning model, according to an example embodiment of the present disclosure.
Fig. 3-7 depict example experimental results according to example embodiments of the present disclosure.
Fig. 8A depicts a block diagram of an example computing system, according to an example embodiment of the present disclosure.
Fig. 8B depicts a block diagram of an example computing device, according to an example embodiment of the present disclosure.
Fig. 8C depicts a block diagram of an example computing device, according to an example embodiment of the present disclosure.
Repeated reference characters in the drawings are intended to represent like features in the various embodiments.
Detailed Description
SUMMARY
Example aspects of the present disclosure provide improved video prediction through the use of machine learning models that utilize overfitting training datasets. In particular, aspects of the present disclosure recognize that under-fitting is one of the main reasons for low quality predictions from previous video recognition methods. The present disclosure addresses the under-fitting by providing a naive approach that uses the architecture of its parameters more efficiently rather than simply adding additional parameters.
More specifically, example aspects of the present disclosure are directed to a model architecture that may be referred to as "FitVid". FitVid is a model with the same parameter counts as the current most advanced model, which can be significantly overfitted to the video prediction dataset, including benchmarks to which previous work has not been overfitted. FitVid is the first instance in which the video model reports a number of overfits on these benchmarks.
Additional aspects of the present disclosure are directed to techniques for mitigating overfitting of designs using image enhancement techniques, resulting in models that can fit both training sets and generalize well to provided videos. As a result, fitVid achieves the highest level across four challenging video data sets of broad metrics.
In addition, fitVid can utilize a significantly simpler training recipe. Previous work on video predictions, particularly those that utilize variational methods to provide randomness, often requires multiple elaborate design decisions to successfully train: for example, lesson training, learning a priori, and annealing of weights on VAE KL divergence penalties. In contrast to these methods, the proposed architecture and technique actually fits the training data well without any such components, directly trained via optimization of the underlying evidence using minimum hyper-parameters.
The systems and methods of the present disclosure provide a number of technical effects and benefits. The improvements provided by the proposed model architecture may provide improved model prediction accuracy, reduced computational resource consumption (e.g., faster computational speed, fewer computational cycles, reduced processor or memory usage, etc.), and/or other improvements over existing models. In particular, the model presented herein may achieve performance comparable to the most advanced predictive model while having a smaller number of parameters (e.g., this reduces consumption of computer resources).
Although example aspects of the present disclosure are discussed with particular reference to example video prediction tasks, the systems and methods described herein may be applied to any number of different tasks. As one example, the systems and methods described herein may be applied to data stream prediction, where future portions of a data stream may be predicted based on past or observed portions of the data stream. For example, the data stream may be a sensor data stream generated by a sensor. For example, the sensor may be a camera, a LIDAR sensor, a RADAR sensor, or other form of sensor.
Referring now to the drawings, example embodiments of the present disclosure will be discussed in more detail.
Example discussion of video prediction
The problem of pixel-level video prediction can be described as follows: first c frames x of a given video ＜c ＝x 0 ,x 1 ,…,x c-1 The goal is to obtain a target by a method according to p (x c：T |x ＜c ) Samples are taken to predict future frames. Alternatively, the predictive model may be given additional information a t For conditions, e.g. generation in videoAnd managing the action taken by the plan. This is commonly referred to as motion-conditioned video prediction.
By introducing a set of latent variables z to capture the inherent randomness of the problem, the variant video prediction follows a variant auto-encoder formal hierarchy. The latent variable may be fixed for the entire video or vary over time. In both cases, the likelihood model may be factorized intoWhich is parameterized in an autoregressive manner over time; that is, at each time step t, video frame x t And latent variable z t Conditional on past potential samples and frames. By multiplying by a priori, the predictive model can be factorized into
Wherein a priori p (z) =p (z t |x ＜t ,z ＜t ) May be stationary or learning. For reasoning, a marginalized distribution can be calculated over the underlying variable z, which is difficult to handle. To overcome this problem, the approximate posterior (amortized approximate posterior) q (z|x) = pi may be calculated by defining a flat approximation posterior distribution p (z|x) t q(z t |z <t ,x ≤t ) To use variational reasoning. The approximate posterior typically utilizes an inference network q φ (z|x) modeling, reasoning about network q φ (z|x) output conditional gaussian distribution Is a parameter of (a). The network may be trained using re-parameterization techniques according to the following:
here, θ and Φ are parameters of the generative model and the inference network, respectively. To learn these parameters, example embodiments of the present disclosure may optimize the variant lower bound:
wherein D is KL Is the Kullback-Leibler divergence between the approximate posterior and the a priori p (z), where the a priori p (z) is fixed asThe hyper-parameter β represents a tradeoff between minimizing frame prediction error and fitting a priori.
Example video prediction model architecture
Example aspects of the present disclosure provide a FitVid model for random video prediction, a convolved non-hierarchical variational model with a fixed a priori N (0,I). An example architecture of FitVid is shown in fig. 2.
Specifically, in fig. 2, (bn) is batch normalization, (swish) is activation, (s & e) is extrusion and excitation, and (n×n) is a convolution layer with kernel size n×n. In the example architecture shown in fig. 2, the stride is always one, except that there is a stride of two when downsampling. For up-sampling, nearest neighbors may be used. The numbers under each box show the number of filters, while the top number indicates the input size. To model dynamics, a two-layer LSTM may be used. FIG. 2 provides an example architecture. Other architectures may be used to implement aspects of the present disclosure. Some of these example aspects are described in further detail below.
Encoder and decoder
Some example embodiments of the present disclosure use multiple residual coding and decoding units (cells). Each cell may include a convolution layer with batch normalization and swish as activation functions, followed by extrusion and excitation. Batch normalization is described in the batch normalization of Sergey Ioffe and Christian Szegedy: deep web training (Batch normalization: accelerating deep network training by reducing internal covariate shift) is accelerated by reducing internal covariate offset, international conference on machine learning (International conference on machine learning), pages 448-456, PMLR,2015.swish functions are described in Prajit Ramachandran, barret Zoph, and Quoc V Le search activation functions (Searching for activation functions) (arXiv preprinted arXiv:1710.05941, 2017), and the like. Extrusion and excitation is described in the extrusion and excitation networks of JieHu, liShen and gansu (Squeeze-and-excitation networks), pages 7132-7141, 2018 in the conference treatises on IEEE conferences (IEEE conference on computer vision and pattern recognition) for computer vision and pattern recognition.
In some implementations, the encoder portion of the video prediction model may include four encoded blocks, each having two units therein. In some implementations, downsampling is performed after each encoder block using a stride convolution of size three in the spatial dimension.
In some embodiments, the decoder portion may also include four decoding blocks, with two units in each block, and with nearest neighbor upsampling after each block. In some implementations, the number of filters in each encoded block may be doubled from previous, while the number of filters in each decoded block may be halved from the number of previous filters. In some implementations, after fixing to each unit of output from the last context frame, there is a residual skip connection between the encoder and decoder. In some embodiments, the batch normalized statistics are averaged across time.
Dynamics model
In some embodiments, frame h is encoded t For predicting h using two-layer LSTM t+1 . Similarly, q (z t |x ＜t ) Single layer LSTM can also be used for modeling, where h t+1 Gaussian distribution as output conditionIs used for inputting parameters of the system. During training, one can go from q (z t |x <t ) Sampling z, while at inference time can be from a fixed a priori +.>Sampling z. In some embodiments, the input of the model is always a reference real-time image (which is commonly referred to as teacher-force) at the time of training. At the inference time, the predicted image in the previous time step may be used as an input to predict the next frame.
Data enhancement
According to aspects of the present disclosure, fitVid may be substantially overfitted on some of the video prediction data sets. To prevent model overfitting, augmentation may be performed. In contrast, previous work in video prediction does not use enhancement, possibly because the previous most advanced model tends to have been under fitted and therefore does not benefit from it. As one example, video uses randagment, which is described in randagment by Ekin D Cubuk, barret Zoph, jonathon shines, and Quoc V Le: actual automated data enhancement with reduced search space (Randaugment: practical automated data augmentation with a reduced search space) in the meeting discussion of IEEE/CVF meetings (IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops) on computer vision and pattern recognition seminars, pages 702-703, 2020.
Some example embodiments of the present disclosure may randomize the enhancement of each video, but remain constant for frames of a single video. As can be seen from fig. 3, randagment substantially improved the overfitting, but not completely. In some implementations, enhancement can be improved by selecting random cropping of the video (referred to as randchop) before the training time resizes the random cropping of the video to the desired resolution. The combination of randchop and randagment successfully prevents overfitting, resulting in a model that fits both the training set and generalizes well to the video provided.
FitVid does not require anything
Previous work on variational video predictions typically requires a series of additional design decisions to perform effective training. Common design parameters include training using courses, typically by scheduling samples, to mitigate the distribution offset between training and generation time; beta is heuristically adjusted in equation 1 to balance the prediction with the fitting priors by annealing it during the training or learning priors. Each of these design choices introduces superparameters, tuning burdens, and additional work when the model is applied to new tasks. FitVid does not require any of these details: for example, it may simply optimize the data from equation 1 by using an optimization algorithm (e.g., adam)Training.
Example experiment
To evaluate FitVid, tests were performed on four different real world data sets. Using four different metrics, fitVid's performance is compared to the existing most advanced methods using comparable parameter counts. One of the main goals is to demonstrate that FitVid can actually be over-fitted on these datasets and to demonstrate how enhancement can prevent FitVid from over-fitting, resulting in the most advanced predictive performance.
Experimental setup
Data set: to test FitVid, four datasets covering various real life scenarios were used. As one example, a human3.6m dataset is used, which consists of agents (actors) performing various actions in the room to study structured motion prediction. Human3.6M is described in Human3.6m of Catalin Ionescu, dragos Papava, vlad Olarou and Cristian Sminchisescu: in a large scale dataset and prediction method for 3d human sensing in natural environments (human 3.6m: large scale datasets and predictive methods for 3d human sensing in natural environments), IEEE transactions on pattern analysis and machine intelligence (IEEE transactions on pattern analysis and machine intelligence), 36 (7), 2014. As another example, the KITTI data set is used to evaluate the FitVid processing part observability and the ability of dynamic background. KITTI data sets are described in Andreas Geiger, philip Lenz, christoph Stiller and Raquel Urtasun vision and robotics: in the kitti dataset (Vision meets robotics: the kitti dataset), international journal of robotics research (The International Journal of Robotics Research), 32 (11): 1231-1237, 2013. For both data sets, the experiment included running a model to predict 25 frames conditioned on the first five frames at 64 x 64 resolution.
To evaluate FitVid in an action-conditioned setting, another experiment used a RoboNet dataset described in RoboNet of subep Dasari, frederik Ebert, stephen Tian, suraj Nair, bernadette Bucher, karl Schmeckpeper, sidharth Singh, sergey Levine, and Chelsea Finn: large scale multi-robot learning (robot: large-scale multi-robot learning), in CoRL, 2019. This large dataset includes over 1500 ten thousand video frames from 7 different robotic arms pushing objects in different boxes. It encompasses a wide range of conditions including different viewpoints, objects, tables and lighting. Previous video prediction methods have a tendency to severely under fit over the dataset. Unfortunately, roboNet does not provide standard training/test partitioning. Thus, 256 videos were randomly selected for testing. FitVid is trained to predict the next ten frames given two context frames and ten future actions.
Finally, to compare FitVid with a broader prior effort, BAIR robots are used to push datasets, which are widely used references in video prediction literature. The BAIR dataset is described in Frederik Ebert, chelsea Finn, alex X Lee and Sergey Levine's Self-supervised visual planning with time-skipped connections (Self-supervised visual planning with temporal skip connections), conference on robotics learning (Conference on Robot Learning (CoRL)), 2017. The evaluation protocol includes giving only one context frame and no action to predict the next 16 frames. Given the high randomness of the robotic arm motion in BAIR, it is a good benchmark for evaluating the ability of a model to generate different outputs, especially in a no-motion setting.
Measurement: the proposed method and previous model evaluate on four different metrics: structural Similarity Index Metric (SSIM), peak signal-to-noise ratio (PSNR), learned perceived image block similarity (LPIPS), and Frechet Video Distance (FVD). FVD measures overall visual quality and temporal coherence without reference to the baseline real video. PSNR, SSIM and LPIPS measure true pixel-by-pixel similarity to a reference, where LPIPS most accurately represents human perceptual similarity. Taking into account the random nature of the video prediction references, the following random video prediction assessment protocol is used: each video samples 100 future tracks and selects the best track as the final score for PSNR, SSIM and LPIPS. For FVD, all 100 future trajectories are used, with a batch size of 256.
Example results
Comparison: first, fitVid is compared to GHVAE and SVG. GHVAE is described in Bohan Wu, suraj Nair, roberto Martin-Martin, li Fei-Fei and Chelsea Finn's greedy hierarchical variant auto encoder (Greedy hierarchical variational autoencoders for large-scale video prediction) for large scale video prediction, arXiv preprinted arXiv:2103.04174, 2021.SVG is described in Ruben Villegas, arkanath Pathak, harini Kannan, dumtru Erhan, quoc V Le, and Honglak Lee in advanced systems (Advances in Neural Information Processing Systems), pages 81-91, 2019, using high-fidelity video prediction with large random-loop neural networks (High fidelity video prediction with large stochastic recurrent neural networks). Both baselines were chosen because they both studied the overfitting by scaling the model and achieved the most advanced results. However, SVG reported no over-fit even for its maximum model with 298M parameters, while GHVAE (with 599M parameters) reported "some over-fit" on the smaller dataset. At the same time, the two models share a similar architecture as FitVid. GHVAE is a greedy trained hierarchical variational video prediction model. SVG is a large scale variational video prediction model with learning priors and minimal generalized preferences. As previously described, the comparison is for the largest version of SVG (m=3, k=5) with 29800 tens of thousands of parameters in the same quantitative range (ballpark) as FitVid with 30200 tens of thousands of parameters.
Table 1 contains the results of these experiments. From this table, it can be seen that FitVid is superior to both SVG and GHVAE in Robonet and human3.6m in all metrics. FitVid also consistently outperforms SVG in KITTI while improving or closely matching GHVAE's performance, GHVAE has more than twice the parameters. For qualitative results, see fig. 4, 5 and 6.
In particular, table 1 provides an empirical comparison between FitVid (with 302M parameters), GHVAE (with 599M parameters), and SVG (with 298M parameters). To prevent FitVid overfitting, enhancements were used for Human3.6M and KITTI.
Fig. 4 shows the results of FitVid on action-conditioned RoboNet. The model is conditioned on the first two frames and the future motion of the robotic arm is given to predict the next ten frames. These figures show how the predicted movement of the arm closely follows the reference reality given that future actions are known. The model also predicts detailed movements of the pushed object (visible in the left example) and fills the background that was not previously seen (see the object appearing behind the robot arm on the right) with some random objects. Also note the misprediction of the robot finger in the right example.
Fig. 5 shows the results of FitVid on the KITTI dataset. As can be seen in this figure, the model generates future high quality predictions in the dynamic scene. Note how FitVid keeps predicting the movement of shadows on the ground until it leaves the frame in the top example. Thereafter, the model still brings the background closer in each frame, which means going forward.
Fig. 6 shows the results of FitVid on human3.6m. The figure shows the extremely detailed and human-like motion predicted by FitVid conditioned on a given context frame. However, in a closer examination, it can be seen that the human subject in the video is changing from a test subject to a training subject. This is particularly evident from the clothing. This phenomenon indicates that while FitVid can generalize to frames outside the training distribution, it morphs the human subject to a familiar subject from the training set, and then plays the video from memory. In fact, we can find a video in the training set that is similar to the video that was visualized in the last line. The highlighted frame is the frame used to find the closest training video.
Comparison with non-variational methods: to compare the performance of FitVid with more previous methods (including non-variational models), tests were also performed on the BAIR robot push dataset. As can be seen in table 2, fitVid outperforms most previous models in this setup while performing comparable to video converters containing 373M parameters. Video converters are described in Dirk Weissenborn, oscar And Jakob uszkorait, arXiv preprinted arXiv in a scaled autoregressive video model (Scaling autoregressive video models): 1906.02634, 2019.
Table 2:
BAIR | FVD↓ |
SV2P | 262.5 |
potential video converter | 125.8 |
SAVP | 116.4 |
DVD-GAN-FP | 109.8 |
VideoGPT | 103.3 |
TrIVD-GAN-FP | 103.3 |
Video converter | 94.0 |
FitVid (our) | 93.6 |
Example analysis of experimental results
This section analyzes the results from the previous section to analyze the results of the over-fitting and the effect of regularization on the current baseline.
Regarding human3.6m as a video prediction reference: human3.6m is a common benchmark in video prediction literature, which is also used to evaluate FitVid. At first glance, it appears that a model is generating extremely detailed and human-like motions conditioned on a given context pose. However, upon scrutiny, it is observed that the human subject in the predictive video is changing. In fact, fitVid replaces the invisible human subject with a training subject, which is particularly apparent from clothing. In practice, we can find similar video clips for each of the predicted videos from the training data. The frames are not identical but they look particularly similar. The observation indicates that:
1. since the test background frames are new and unseen, the model can be generalized to unseen frames and subjects. FitVid detects humans and continues video from there.
2. The model remembers the movement and appearance of the training subject. The model morphs the test human subject into a training subject and then plays the relevant video from memory.
This means that FitVid fails to generalize to new principals while still generalizing to unseen frames. This may not be surprising given that human3.6m has five training and two test subjects.
However, this observation shows how the current low resolution setting for human3.6m is not applicable to large scale video prediction. Indeed, after this observation, the same behavior is tracked in other video prediction literature, and unfortunately, it seems that this is a common and neglected problem.
Overfitting and regularization: as mentioned elsewhere herein, there is considerable evidence that current video predictive models tend to be under-fitted when trained on large data sets. GHVAE was the current most advanced model with 59900 ten thousand parameters, reporting "some overfitting" over smaller data sets (such as Human3.6M and KITTI). However, experiments showed that although only 30200 ten thousand parameters, a severe and significant overfitting was observed with FitVid. Fig. 7 visualizes the training and evaluation of the LPIPS metrics when FitVid was trained on human3.6m without enhancement. The graph shows that training remains better and the test quality begins to deteriorate after 15K iterations. Similar behavior was also observed on the KITTI. These results clearly show that FitVid overfits on human3.6m and KITTI, indicating that FitVid uses its parameters more effectively. As mentioned elsewhere herein, to address the overfitting, enhancements may be used. As a result, fitVid achieved the most advanced results as reported in table 1.
Overfitting on Robonet: no overfitting was observed on Robonet, which is expected in view of the fact that Robonet is much larger than other benchmarks. Attempting to find a model that can be overfitted on RoboNet, a scaled version of FitVid with 500M parameters was tested-which is still smaller compared to GHVAE with 599M parameters and reported that no overfitting was done on the dataset. The scaled version of FitVid is over-fitted on RoboNet as shown in fig. 7. Note that the experiment did not use this scaled version in the report number of table 1, which was generated using the 302M version. The goal here is to show that scaled versions of FitVid can also use their parameters more effectively than previous models, resulting in overfitting over an even larger dataset (such as RoboNet).
Fig. 7 shows the effect of FitVid over-fitting without enhancement. The figure visualizes training and evaluation metrics on (a) human3.6m, (b) KITTI, and (c) Robonet without enhancement. As can be seen, fitVid overfits on training data other than Robonet in all cases. This is evident from the rise in the evaluation measurement while the training remains reduced. In the case of Robonet, fitVid with 302M parameters was not over-fitted, but a scaled version of the model with 600M parameters was over-fitted, as can be seen in (d). The y-axis is LPIPS. The x-axis is the training iteration. The graph is smoothed with an average rolling window of ten. Shading is the original unsmooth value.
Enhancement of the effect on SVG: in some example experiments, there are differences between the input data used to train the model. FitVid is trained with enhancements, while baseline is trained without any enhancements, which presents a problem: can better performance of FitVid be explained by enhancement alone? In other words, do the previous approach also benefit from enhancement? To answer this question, the SVG is retrained with and without enhancement. As shown in Table 3, SVG performs worse if trained with enhanced data, which supports its lack of knowledge of the original data. Thus, the experiment provides more support for FitVid to truly overfit on these datasets and thus benefit from enhancement.
Table 3 shows SVG with and without enhancement. The table shows that SVG does not benefit from enhancement due to its under-fitting to the original data.
Human3.6M | FVD↓ | PSNR↑ | SSIM↑ | LPIPS↓ |
Without reinforcement | 389.55 | 27.4 | 93.7 | 0.041 |
With reinforcement | 429.25 | 23.0 | 87.1 | 0.094 |
KITTI | FVD↓ | PSNR↑ | SSIM↑ | LPIPS↓ |
Without reinforcement | 1612.62 | 14.8 | 38.7 | 0.330 |
With reinforcement | 2051.67 | 14.4 | 36.0 | 0.333 |
Zero sample (zero-shot) real robot performance: previous work indicated that improved video prediction translates into better performance in downstream tasks. However, in these works, the training and testing distribution is the same, and there is little domain transfer from training to testing. This section investigates whether FitVid can generalize to similar but visually different tasks without training data for the new domain. Thus, an example real robot experiment was performed with a Franka Emika Panda robot arm, where the goal was to push a specific object to a predetermined target position. FitVid trains on RoboNet and plans using the Cross Entropy Method (CEM). As can be seen from table 4, the agent cannot generalize to new domains, achieving worse performance than random agents. Although the robot is performing the same task (i.e., pushing the object into the box using the robotic arm), this may not be surprising in view of the fact that the video in RoboNet has completely different robots and vision.
Next, the experiments tried brought the training and testing fields closer to each other by fine tuning FitVid on the data from GHVAE. This data contained 5000 autonomously collected videos of the Franka Emika Panda robotic arm pushing object, which appeared more similar to our setup than RoboNet, but still contained different illumination, camera angles, and target objects. This time we observed that FitVid was relatively successful in generalizing to the new field, with success in the 56% trial. Finally, we found that adding data to the fine tuning enhances the generalization ability of the improved model to 78% success rate. These results demonstrate that while large distributed offset adaptation (RoboNet) is still difficult, fitVid can accommodate relatively new fields by using data enhancement.
Table 4 shows the zero sample real robot performance. FitVid is used to plan future actions of the real robot to push the object to the target location without training data from our setup. The model was trained on visually distinct data (RoboNet) and data from a closer domain (from GHVAE) with and without enhancement. Although not directly adaptable to new fields from RoboNet, the results indicate that fine tuning and enhancement on similar data improves FitVid performance.
Training data | Success rate |
Baseline (random action) | 28％ |
RoboNet | 17％ |
RoboNet+GHVAE data | 56％ |
Robonet+ enhanced GHVAE data | 78％ |
Thus, the present disclosure proposes a FitVid, a simple and scalable variational video prediction model that can obtain a significantly better fit to the current video prediction dataset even with similar parameter counts as the previous model. Indeed, while previous approaches typically suffer from under-fitting to these datasets, the naive application of FitVid actually resulted in over-fitting. Accordingly, the present disclosure proposes a set of data enhancement techniques for video prediction that prevent overfitting, resulting in the most advanced results across a range of prediction references.
This is the first time the model reports a substantial overfitting on these benchmarks. This is particularly important because under-fitting is often cited as one of the main reasons for low quality prediction of future frames. The present disclosure demonstrates how image enhancement techniques can prevent model overfitting, resulting in high quality images. As a result, fitVid outperforms the current most advanced model across four different video prediction references on four different metrics. The present disclosure also shows how a model that can fit training data appropriately can fool current benchmarks and metrics, leading to undesirable results, which are often ignored in the video prediction literature.
There are many ways in which FitVid can be extended. As mentioned herein, one of the interesting features of the method we propose is that it is simple. It is non-hierarchical, convoluted, without attentive mechanisms, without course learning, and without training schedules. Any of these features can potentially improve the results of FitVid in order to generate even higher quality images. Taking into account the simplicity of FitVid, construction can be easily performed on it. Another interesting direction is to introduce new training perceptual metrics for video prediction and generation to signal when the model generates high quality video by repeating training data.
Example devices and systems
Fig. 8A depicts a block diagram of an example computing system 100, according to an example embodiment of the disclosure. The system 100 includes a user computing device 102, a server computing system 130, and a training computing system 150 communicatively coupled by a network 180.
The user computing device 102 may be any type of computing device, such as, for example, a personal computing device (e.g., a laptop computer or desktop computer), a mobile computing device (e.g., a smart phone or tablet computer), a game console or controller, a wearable computing device, an embedded computing device, or any other type of computing device.
The user computing device 102 includes one or more processors 112 and memory 114. The one or more processors 112 may be any suitable processing device (e.g., processor core, microprocessor, ASIC, FPGA, controller, microcontroller, etc.), and may be one processor or multiple processors operatively connected. Memory 114 may include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, and the like, as well as combinations thereof. The memory 114 may store instructions 118 and data 116 that are executed by the processor 112 to cause the user computing device 102 to perform operations.
In some implementations, the user computing device 102 may store or include one or more machine learning models 120. For example, the machine learning model 120 may be or otherwise include various machine learning models, such as a neural network (e.g., a deep neural network) or other types of machine learning models, including nonlinear models and/or linear models. The neural network may include a feed forward neural network, a recurrent neural network (e.g., a long and short term memory recurrent neural network), a convolutional neural network, or other form of neural network. Some example machine learning models may utilize an attention mechanism such as self-attention. For example, some example machine learning models may include a multi-headed self-attention model (e.g., a transformer model).
In some implementations, one or more machine learning models 120 may be received from the server computing system 130 over the network 180, stored in the user computing device memory 114, and then used or otherwise implemented by the one or more processors 112. In some implementations, the user computing device 102 can implement multiple parallel instances of a single machine learning model 120 (e.g., to perform parallel prediction across multiple inputs).
Additionally or alternatively, one or more machine learning models 140 may be included in the server computing system 130 or otherwise stored and implemented by the server computing system 130, the server computing system 130 in communication with the user computing device 102 according to a client-server relationship. For example, the machine learning model 140 may be implemented by the server computing system 140 as part of a web service (e.g., predictive service). Accordingly, one or more models 120 may be stored and implemented at the user computing device 102 and/or one or more models 140 may be stored and implemented at the server computing system 130.
The user computing device 102 may also include one or more user input components 122 that receive user input. For example, the user input component 122 may be a touch-sensitive component (e.g., a touch-sensitive display screen or touchpad) that is sensitive to touch by a user input object (e.g., a finger or stylus). The touch sensitive component may be used to implement a virtual keyboard. Other example user input components include a microphone, a conventional keyboard, or other means by which a user may provide user input.
The server computing system 130 includes one or more processors 132 and memory 134. The one or more processors 132 may be any suitable processing device (e.g., processor core, microprocessor, ASIC, FPGA, controller, microcontroller, etc.), and may be one processor or multiple processors operatively connected. Memory 134 may include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, and the like, as well as combinations thereof. Memory 134 may store instructions 138 and data 136 that are executed by processor 132 to cause server computing system 130 to perform operations.
In some implementations, the server computing system 130 includes or is otherwise implemented by one or more server computing devices. In instances where the server computing system 130 includes multiple server computing devices, such server computing devices may operate in accordance with a sequential computing architecture, a parallel computing architecture, or some combination thereof.
As described above, the server computing system 130 may store or otherwise include one or more machine learning models 140. For example, model 140 may be or may otherwise include various machine learning models. Example machine learning models include neural networks or other multi-layer nonlinear models. Example neural networks include feed forward neural networks, deep neural networks, recurrent neural networks, and convolutional neural networks. Some example machine learning models may utilize an attention mechanism such as self-attention. For example, some example machine learning models may include a multi-headed self-attention model (e.g., a transformer model).
The user computing device 102 and/or the server computing system 130 may train the model 120 and/or the model 140 via interaction with a training computing system 150 communicatively coupled via a network 180. The training computing system 150 may be separate from the server computing system 130 or may be part of the server computing system 130.
The training computing system 150 includes one or more processors 152 and memory 154. The one or more processors 152 may be any suitable processing device (e.g., processor core, microprocessor, ASIC, FPGA, controller, microcontroller, etc.), and may be one processor or multiple processors operatively connected. The memory 154 may include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, and the like, as well as combinations thereof. Memory 154 may store instructions 158 and data 156 that are executed by processor 152 to cause training computing system 150 to perform operations. In some implementations, the training computing system 150 includes or is otherwise implemented by one or more server computing devices.
Training computing system 150 may include a model trainer 160, model trainer 160 using various training or learning techniques (such as, for example, back propagation of errors) to train machine learning models 120 and/or 140 stored at user computing device 102 and/or server computing system 130. For example, the loss function may be back-propagated through the model(s) to update one or more parameters of the model(s) (e.g., a gradient based on the loss function). Various loss functions may be used, such as mean square error, likelihood loss, cross entropy loss, hinge loss, and/or various other loss functions. Gradient descent techniques may be used to iteratively update parameters over multiple training iterations.
In some implementations, performing back-propagation of the error may include performing truncated back-propagation of the transit time. Model trainer 160 may perform a variety of generalization techniques (e.g., weight decay, discard, etc.) to improve the generalization ability of the model being trained.
In particular, model trainer 160 may train machine learning models 120 and/or 140 based on a set of training data 162. Training data 162 may include, for example, data that has been enhanced according to one or more enhancement operations. Enhancement operations may include modifying training data in some manner, including adding noise, changing spatial dimensions, and/or various other enhancements.
In some implementations, the training examples can be provided by the user computing device 102 if the user has provided consent. Thus, in such embodiments, the model 120 provided to the user computing device 102 may be trained by the training computing system 150 on user-specific data received from the user computing device 102. In some instances, this process may be referred to as personalizing the model.
Model trainer 160 includes computer logic that is used to provide the desired functionality. Model trainer 160 may be implemented in hardware, firmware, and/or software controlling a general purpose processor. For example, in some embodiments, model trainer 160 includes program files stored on a storage device, loaded into memory, and executed by one or more processors. In other implementations, model trainer 160 includes one or more sets of computer-executable instructions stored in a tangible computer-readable storage medium (such as RAM, a hard disk, or an optical or magnetic medium).
The network 180 may be any type of communication network, such as a local area network (e.g., an intranet), a wide area network (e.g., the internet), or some combination thereof, and may include any number of wired or wireless links. In general, communications over network 180 may be carried via any type of wired and/or wireless connection using various communication protocols (e.g., TCP/IP, HTTP, SMTP, FTP), coding or formats (e.g., HTML, XML), and/or protection schemes (e.g., VPN, secure HTTP, SSL).
FIG. 8A illustrates one example computing system that may be used to implement the present disclosure. Other computing systems may also be used. For example, in some implementations, the user computing device 102 may include a model trainer 160 and a training data set 162. In such implementations, the model 120 may be trained and used locally at the user computing device 102. In some such implementations, the user computing device 102 may implement the model trainer 160 to personalize the model 120 based on user-specific data.
Fig. 8B depicts a block diagram of an example computing device 10, performed in accordance with an example embodiment of the present disclosure. Computing device 10 may be a user computing device or a server computing device.
Computing device 10 includes a plurality of applications (e.g., application 1 through application N). Each application contains its own machine learning library and machine learning model(s). For example, each application may include a machine learning model. Example applications include text messaging applications, email applications, dictation applications, virtual keyboard applications, browser applications, and the like.
As shown in fig. 8B, each application may communicate with a number of other components of the computing device, such as, for example, one or more sensors, a context manager, a device state component, and/or additional components. In some implementations, each application can communicate with each device component using an API (e.g., public API). In some implementations, the APIs used by each application are specific to that application.
Fig. 8C depicts a block diagram of an example computing device 50, performed in accordance with an example embodiment of the present disclosure. Computing device 50 may be a user computing device or a server computing device.
Computing device 50 includes a plurality of applications (e.g., application 1 through application N). Each application communicates with a central intelligent layer. Example applications include text messaging applications, email applications, dictation applications, virtual keyboard applications, browser applications, and the like. In some implementations, each application can communicate with the central intelligence layer (and model(s) stored therein) using an API (e.g., a common API across all applications).
The central intelligence layer includes a plurality of machine learning models. For example, as shown in fig. 8C, a respective machine learning model may be provided for each application and managed by a central intelligent agent. In other implementations, two or more applications may share a single machine learning model. For example, in some embodiments, the central intelligence layer may provide a single model for all applications. In some implementations, the central intelligence layer is included within or otherwise implemented by the operating system of computing device 50.
The central intelligence layer may communicate with the central device data layer. The central device data layer may be a centralized repository for data of computing devices 50. As shown in fig. 8C, the central device data layer may communicate with a plurality of other components of the computing device, such as, for example, one or more sensors, a context manager, a device status component, and/or additional components. In some implementations, the central device data layer can communicate with each device component using an API (e.g., a proprietary API).
Additional disclosure
The technology discussed herein refers to servers, databases, software applications, and other computer-based systems, as well as actions taken and information sent to and from such systems. The inherent flexibility of computer-based systems allows for a variety of possible configurations, combinations, and divisions of tasks and functions between and among components. For example, the processes discussed herein may be implemented using a single device or component or multiple devices or components working in combination. The database and applications may be implemented on a single system or distributed across multiple systems. The distributed components may operate sequentially or in parallel.
While the present subject matter has been described in detail with respect to various specific example embodiments thereof, each example is provided by way of explanation and not limitation of the present disclosure. Modifications, variations and equivalents to those embodiments will readily occur to those skilled in the art upon attaining an understanding of the foregoing. Accordingly, the subject disclosure does not preclude inclusion of such modifications, variations and/or additions to the present subject matter as would be readily apparent to one of ordinary skill in the art. For example, features illustrated or described as part of one embodiment can be used with another embodiment to yield still a further embodiment. Accordingly, the present disclosure is intended to cover such alternatives, modifications, and equivalents.
Claims (25)
1. A computing system for video prediction with improved efficiency, the computing system comprising:
one or more processors; and
one or more non-transitory computer-readable media that collectively store:
a machine-learned video prediction model configured to receive and process one or more previous video frames to generate one or more predicted subsequent video frames, wherein the machine-learned video prediction model comprises a convolutional variational automatic encoder, and wherein the convolutional variational automatic encoder comprises an encoder portion comprising one or more encoding units and a decoder portion comprising one or more decoding units; and
Instructions that, when executed by the one or more processors, cause a computing system to process the one or more previous video frames using a machine-learned video prediction model to generate the one or more predicted subsequent video frames.
2. The computing system of any preceding claim, wherein at least one of the one or more encoding units or the one or more decoding units comprises one or more convolutional layers followed by an extrusion and excitation layer, and wherein each convolutional layer is configured to perform batch normalization and apply a swish activation function.
3. The computing system of any preceding claim, wherein the encoder section comprises four encoding blocks with two encoding units in each encoding block, wherein each encoding unit comprises two or more convolutional layers, followed by extrusion and excitation layers.
4. A computing system according to claim 3, wherein the encoder portion performs downsampling after encoding each of the blocks.
5. The computing system of claim 3 or 4, wherein the respective number of filters in each encoded block is doubled relative to a previous encoded block.
6. The computing system of any preceding claim, wherein the decoder portion comprises four encoded blocks with two decoding units in each decoding block, wherein each decoding unit comprises three convolutional layers followed by an extrusion and excitation layer.
7. The computing system of claim 5 wherein the decoder portion performs nearest neighbor upsampling after decoding each of the blocks.
8. The computing system of claim 5 or 6, wherein the respective number of filters in each decoded block is half relative to a previous encoded block.
9. The computing system of any preceding claim, wherein the convolutional diversity automatic encoder further comprises one or more long-term memory (LSTM) layers located between the encoder portion and the decoder portion, wherein a first LSTM layer of the one or more LSTM layers receives as input a set of latent variables, wherein the set of latent variables are predicted during training of the convolutional diversity automatic encoder but sampled from a fixed prior during reasoning of the convolutional diversity automatic encoder.
10. The computing system of any preceding claim, wherein the convolutional variational automatic encoder is configured to receive an action condition input that conditions the convolutional variational automatic encoder to generate one or more predicted subsequent video frames depicting a given action represented by the action condition input.
11. The computing system of any preceding claim, wherein the convolutional variational automatic encoder is non-hierarchical.
12. The computing system of any preceding claim, wherein the machine learning video prediction model has been trained on a training video, and wherein the training video is enhanced prior to use in training the machine learning video prediction model.
13. The computing system of claim 12, wherein consistent enhancement operations are performed on training videos.
14. The computing system of claim 12 or 13, wherein the computing system applies cropping and resizing enhancement operations to the one or more previous video frames.
15. A computing system for data stream prediction with improved efficiency, the computing system comprising:
one or more processors; and
one or more non-transitory computer-readable media that collectively store:
a machine-learned data stream prediction model configured to receive and process one or more previous data sets from a data stream to generate one or more predicted subsequent data sets for the data stream,
wherein the machine-learned data stream prediction model comprises a convolutional variational automatic encoder;
Wherein the convolutional diversity automatic encoder comprises an encoder section comprising one or more encoding units and a decoder section comprising one or more decoding units; and
instructions that, when executed by the one or more processors, cause a computing system to process the one or more previous data sets using a machine-learned data flow prediction model to generate the one or more predicted subsequent data sets.
16. The computing system of claim 15, wherein the data stream comprises a sensor data stream generated by a sensor.
17. The computing system of claim 16, wherein the sensor comprises a camera, a light detection and ranging sensor, or a radio detection and ranging sensor.
18. A computer-implemented method for video prediction with an improved balance between efficiency and performance, the method comprising:
obtaining one or more previous video frames associated with the video;
accessing a machine-learned video prediction model configured to perform video prediction, wherein the machine-learned video prediction model comprises a model architecture selected to overfit a training dataset comprising one or more training videos;
Performing a respective enhancement operation on each of the one or more training videos to generate one or more enhancement training videos forming an enhancement data set; and
the machine-learned video prediction model is trained using the one or more enhanced training videos of the enhanced data set to perform video prediction.
19. The computer-implemented method of claim 18, wherein the machine-learned video prediction model comprises the machine-learned video prediction model of any of claims 1-17.
20. The computer-implemented method of claim 18 or 19, wherein training a machine-learning video prediction model to perform video prediction using the one or more enhanced training videos comprises: for each of the one or more enhanced training videos:
processing one or more previous video frames from the enhanced training video using the machine-learned video prediction model to generate one or more predicted subsequent video frames;
evaluating a loss function that compares the one or more predicted subsequent video frames to one or more actual subsequent video frames included in the enhanced video; and
One or more values of one or more of the machine-learned video prediction models are modified based on the loss function.
21. The computer-implemented method of any of claims 18-20, wherein the machine-learned video prediction model is selected to have a first difference between a first accuracy of the machine-learned video prediction model on a training dataset and a second accuracy of the machine-learned video prediction model on a validation dataset, the first difference being greater than a threshold difference.
22. The computer-implemented method of any of claims 18-21, wherein the machine-learned video prediction model is selected to have a first difference between a first accuracy of the machine-learned video prediction model on a training dataset and a second accuracy of the machine-learned video prediction model on a validation dataset that is greater than a second difference between a third accuracy of a different available video prediction model on the training dataset and a fourth accuracy of the different available video prediction model on the validation dataset.
23. The computer-implemented method of any of claims 18-22, wherein the machine-learned video prediction model is selected to have a ratio of a number of parameters greater than a threshold ratio to a volume of a training dataset.
24. The computer-implemented method of any of claims 18-23, wherein the machine-learned video prediction model is selected to satisfy one or more algorithmic metrics of an overfitting.
25. One or more non-transitory computer-readable media storing instructions that, when executed by one or more processors, cause the one or more processors to perform the method of any of claims 18-24.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202163194457P | 2021-05-28 | 2021-05-28 | |
US63/194,457 | 2021-05-28 | ||
PCT/US2022/031326 WO2022251621A1 (en) | 2021-05-28 | 2022-05-27 | Pixel-level video prediction with improved performance and efficiency |
Publications (1)
Publication Number | Publication Date |
---|---|
CN117396887A true CN117396887A (en) | 2024-01-12 |
Family
ID=82156787
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202280035001.2A Pending CN117396887A (en) | 2021-05-28 | 2022-05-27 | Pixel-level video prediction with improved performance and efficiency |
Country Status (4)
Country | Link |
---|---|
US (1) | US20230239499A1 (en) |
EP (1) | EP4288909A1 (en) |
CN (1) | CN117396887A (en) |
WO (1) | WO2022251621A1 (en) |
-
2022
- 2022-05-27 EP EP22732821.8A patent/EP4288909A1/en active Pending
- 2022-05-27 US US18/011,922 patent/US20230239499A1/en active Pending
- 2022-05-27 WO PCT/US2022/031326 patent/WO2022251621A1/en active Application Filing
- 2022-05-27 CN CN202280035001.2A patent/CN117396887A/en active Pending
Also Published As
Publication number | Publication date |
---|---|
WO2022251621A1 (en) | 2022-12-01 |
US20230239499A1 (en) | 2023-07-27 |
EP4288909A1 (en) | 2023-12-13 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
Petrovich et al. | TEMOS: Generating diverse human motions from textual descriptions | |
Shi et al. | Machine learning for spatiotemporal sequence forecasting: A survey | |
Weerakody et al. | A review of irregular time series data handling with gated recurrent neural networks | |
Gupta et al. | Maskvit: Masked visual pre-training for video prediction | |
Babaeizadeh et al. | Fitvid: Overfitting in pixel-level video prediction | |
US10503978B2 (en) | Spatio-temporal interaction network for learning object interactions | |
Gao et al. | Long short-term memory-based deep recurrent neural networks for target tracking | |
US20210097401A1 (en) | Neural network systems implementing conditional neural processes for efficient learning | |
Brighton et al. | Are rational actor models “rational” outside small worlds | |
CN111539941B (en) | Parkinson's disease leg flexibility task evaluation method and system, storage medium and terminal | |
US10776691B1 (en) | System and method for optimizing indirect encodings in the learning of mappings | |
US20220366257A1 (en) | Small and Fast Video Processing Networks via Neural Architecture Search | |
Hug et al. | On the reliability of LSTM-MDL models for pedestrian trajectory prediction | |
Kukleva et al. | Utilizing temporal information in deep convolutional network for efficient soccer ball detection and tracking | |
Constantin et al. | Image noise detection in global illumination methods based on FRVM | |
Debard et al. | Learning 3d navigation protocols on touch interfaces with cooperative multi-agent reinforcement learning | |
CN117396887A (en) | Pixel-level video prediction with improved performance and efficiency | |
Park et al. | Multi-neural networks object identification | |
Fu et al. | Spatiotemporal representation learning with gan trained lstm-lstm networks | |
Pantrigo et al. | Heuristic particle filter: applying abstraction techniques to the design of visual tracking algorithms | |
Khan et al. | Attention based parameter estimation and states forecasting of COVID-19 pandemic using modified SIQRD Model | |
Zotov | StyleGAN-based machining digital twin for smart manufacturing | |
WO2017159126A1 (en) | Direct inverse reinforcement learning with density ratio estimation | |
Bharadwaj et al. | Video Frame Rate Doubling Using Generative Adversarial Networks | |
Xing et al. | Scene-aware Human Motion Forecasting via Mutual Distance Prediction |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |