EP2633439A1 - Search with joint image-audio queries - Google Patents
Search with joint image-audio queriesInfo
- Publication number
- EP2633439A1 EP2633439A1 EP11784547.9A EP11784547A EP2633439A1 EP 2633439 A1 EP2633439 A1 EP 2633439A1 EP 11784547 A EP11784547 A EP 11784547A EP 2633439 A1 EP2633439 A1 EP 2633439A1
- Authority
- EP
- European Patent Office
- Prior art keywords
- image
- query
- data
- audio
- resource
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Ceased
Links
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/40—Information retrieval; Database structures therefor; File system structures therefor of multimedia data, e.g. slideshows comprising image and additional audio data
- G06F16/43—Querying
- G06F16/435—Filtering based on additional data, e.g. user or group profiles
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/40—Information retrieval; Database structures therefor; File system structures therefor of multimedia data, e.g. slideshows comprising image and additional audio data
- G06F16/43—Querying
- G06F16/432—Query formulation
- G06F16/433—Query formulation using audio data
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/40—Information retrieval; Database structures therefor; File system structures therefor of multimedia data, e.g. slideshows comprising image and additional audio data
- G06F16/43—Querying
- G06F16/432—Query formulation
- G06F16/434—Query formulation using image data, e.g. images, photos, pictures taken by a user
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/40—Information retrieval; Database structures therefor; File system structures therefor of multimedia data, e.g. slideshows comprising image and additional audio data
- G06F16/43—Querying
- G06F16/438—Presentation of query results
Definitions
- This specification relates to processing queries, particularly to queries including both an image and associated audio.
- the Internet provides access to a wide variety of resources, for example, video files, image files, audio files, or Web pages including content for particular subjects, book articles, or consumer products.
- a search system can select one or more resources in response to receiving a search query.
- a search query is data that a user submits to a search engine to satisfy the user's informational needs.
- the search system selects and scores resources based on their relevance to the search query.
- the search results are typically ordered according to the scores, and provided in a search results page.
- a search system can determine the relevance of an image to a text query based on the textual content of the resource in which the image is located and also based on relevance feedback associated with the image.
- Some search systems search image resources by using query images as input.
- a query image is an image, such as a jpeg file, that is used by a search engine as input to a search processing operation.
- Related images can be found by processing other images and identifying images that are similar in visual appearance to the query image.
- the use of query images is becoming much more prevalent with the advent of smart phones that include cameras. For example, using a smart phone, a user can now take a picture of a subject of interest, and submit the picture to a search engine. The search engine then searches image resources using the picture as a query image.
- one innovative aspect of the subject matter described in this specification can be embodied in methods that include the actions of receiving a joint image-audio query from a client device, the joint image-audio query including query image data defining a query image and query audio data defining query audio;
- Other embodiments of this aspect include corresponding systems, apparatus, and computer programs, configured to perform the actions of the methods, encoded on computer storage devices.
- Another aspect of the subject matter described in this specification can be implemented in methods that include the actions of accessing image annotation data describing a plurality of annotation pairs, each annotation pair including image data defining an image and text data associated with the image; accessing resources, each resource defining a resource image for the resource and text data defining resource text for the resource; and training a joint image-audio relevance model on the image annotation data and the resources to generate relevance scores for a plurality of resources, and wherein each relevance score is a measure of the relevance of a corresponding resource to a joint image-audio query that includes query image data defining a query image and query audio data defining query audio.
- Other embodiments of this aspect include corresponding systems, apparatus, and computer programs, configured to perform the actions of the methods, encoded on computer storage devices.
- Adding audio data to an image query can improve relevance of search results on the query. Relevance can be improved both by providing information that can aid the system in extracting the object of interest in an image, and also by providing information that supplements the user's search beyond what can be found in the image. This information can also be added in various other ways.
- a portion of the image can be selected as containing the object of interest by the user drawing a circle on the image using a touch screen.
- the user can also outline the object of interest more closely than a circle or other shape, and can also draw the outline using other input methods.
- the user can add additional information regarding the image using a dropdown menu box.
- the menu box can have different categories of items, such as shopping categories including shoes, shirts, pants, and others similar categories.
- FIG. 1 is a block diagram of an example environment in which a joint image- audio search system provides search services.
- FIGS. 2A and 2B are example images for a joint image-audio query.
- FIG. 3A is a block diagram of an example process for querying a joint image- audio search system.
- FIG. 3B is a block diagram of an example process for training a joint image- audio relevance model.
- FIG. 4 is a flow chart of an example process for training a joint image-audio relevance model.
- FIG. 5 is a flow chart of an example process for ranking resources for a joint image-audio query.
- An application running on a mobile phone allows a user to take a picture of an object and speak into the phone to record the user's speech.
- the audio recording is paired with the image to form a joint image-audio query.
- the mobile device then submits the joint image-audio query to a search system.
- the search system receives the joint image-audio query and determines text data from the speech and generates image feature data from the image.
- the search system uses the text data and the image feature data as inputs into a joint image-audio relevance model, which compares resources to the input data.
- the resources can be any of the resources found on the Internet, including web pages, documents, images, and video.
- each resource can be a document for a product, which includes an image of the product and associated text data of the product.
- the joint image-audio relevance model compares the query image feature data to the image feature data of each resource and the query text data to the corresponding resource text data and computes a relevance score for each resource.
- the system orders the resources according to the relevance scores and presents search results to the user.
- the search results include links to the ordered resources, and may also include additional information about each resource, for example, thumbnails of the resource image or subsets of the resource text.
- the model For the joint image-audio relevance model to determine the relevance of a joint image-audio query to the resources, the model is first trained. Training the model involves using image annotation data, which are annotation pairs. Each annotation pair is an image paired with text data associated with the image. These annotation pairs are used as training inputs to the joint image-audio relevance model, along with training and testing resources for the annotation pairs. The joint image-audio relevance model is trained until the testing resources are ranked in a manner that is deemed to be acceptable, as defined by one or more criteria.
- FIG. 1 is a block diagram of an example environment 100 in which a joint image-audio search system 106 provides search services.
- the example environment 100 includes a network 104, such as the Internet, connecting a user device 102 to a search system 106.
- the user device 102 transmits a joint image-audio query 120 that includes a pairing of image 122 and audio 124 data over the network 104 to the search system 106.
- Example audio 124 is a speech recording.
- the system 106 processes the image 122 and audio 124 data and compares them to a collection of resources 1 16, computing a relevance score for each resource 1 16.
- the system 106 ranks these resources 116 by their relevance scores and sends a list of search results, each of which includes a resource link 130 to a corresponding resource, to the user device 102.
- the user device 102 is an electronic device that is under control of a user and is capable of requesting and receiving resources 1 16 over the network 104.
- Example user devices 102 include personal computers, mobile communication devices, and other devices that can send and receive data over the network.
- a user device 102 typically includes a user application, e.g., a web browser, to facilitate the sending and receiving of data over the network 104.
- the user device 102 may also include a camera and a microphone for acquiring an image 122 and audio 124.
- the user device also includes an application that pairs the audio 124 with the image 122 to form a joint- image audio query.
- the query audio 124 typically includes speech data that provides more information about the image 122 or about the user's search parameters.
- the query image 122 is a picture of a water bottle taken by the user device 102.
- the image may include more than just the water bottle.
- the user specifies that the water bottle is the object of interest in the picture by augmenting the query image 122 with the query audio 124, "water bottle.”
- the user may provide more specific information, for example, by including "red water bottle” as the query audio 124.
- the query audio 124 may also include positional information, for example if there is more than one object in the query image 122, the user may specify by submitting the query audio 124, "red bottle on the right.”
- FIG. 2B contains only the water bottle in the picture.
- the results may include only bottles that have a similar shape and color, and may not include other types of water bottles.
- the system provides additional information to the search system, and the search system uses this additional information to provide search results that are likely to satisfy the user's informational needs.
- the user may also provide parameters by use of audio to restrict the search results.
- the user may be searching a product database to find a water bottle for purchase.
- the user may provide to the search system the image 122 of the water bottle and the query audio 124, "Brand X water bottle under ten dollars," or as another example, "this water bottle in blue.”
- the search system 106 receives the joint image-audio query that includes the image 122 data and the audio 124 data from the user device 102 through the network 104.
- the search system 106 includes an image processing apparatus 110 to generate image feature data from the image 122 data.
- the search system passes the image 122 data to a separate image processing apparatus 1 10 and receives the image feature data from the separate image processing apparatus 110.
- the search system 106 may also include a speech processing apparatus 1 12 to extract text data from the audio 124 data, or it may pass the audio 124 data to a separate speech processing apparatus 112 and receive the text data.
- the search system 106 uses the image feature data and the text data derived from the joint image-audio query as input to a joint image-audio relevance model 108.
- the joint image-audio relevance model 108 receives these two inputs and also receives resources 116.
- the joint image-audio relevance model 108 scores each resource 116 indicating a measure of relevance of the resource 1 16 to the joint image-audio query.
- the search system using the joint image-audio relevance model 108, computes a score for each resource according to the following ranking function: where
- RELi is a relevance score for a resource R .
- S is the audio data 124
- Ri is a given resource in a resource database or cache.
- the function f(S, I, R) is described in more detail with respect to FIG. 3B below.
- a resource 116 is any data that can be provided over a network 104 and is associated with a resource address or indexed in a database.
- a resource database 1 14 comprises a collection of resources 1 16, with each resource 116 including a resource image and resource text.
- a resource database 114 is a product database that includes product documents comprising an image of a product and data describing the product, such as brand name, price, and a textual description.
- the search system 106 determines resource image feature data from the resource image in a manner similar to how it determines query image feature data from the query image.
- the search system 106 also determines resource text data from the resource 1 16.
- the joint image-audio relevance model 108 then compares the query image feature data to the resource image feature data and the query text data to the resource text data of a resource 1 16 and computes a relevance score RELi for the resource 1 16.
- the model 108 provides the relevance scores to the search system 106.
- the search system 106 then orders the resources according to the relevance scores, and provides search results 130, ranked by the relevance scores of the resources, to the user device 102.
- FIG. 3 A is a block diagram of an example process 300 for querying a joint image-audio search system.
- the search system 106 receives the joint image-audio query, comprising image data 302 and audio data 304. This data is received through the network and, in some implementations, the image data 302 is a picture taken of a query object by a user.
- the audio data 304 includes speech recorded by the user containing information about the query object or about the desired query results. These are paired together as the joint image-audio query.
- the audio data 304 includes audio pertaining to speech.
- the speech data 304 is converted to text data 308 using speech recognition algorithms.
- the text 308 is further analyzed using natural language processing techniques to parse the content of the text data 308.
- the image 302 in the joint image-audio query may contain a water bottle, as in FIG. 2A.
- the audio data 304 accompanying this image may simply be, "water bottle.”
- the search system 106 converts this speech 304 into text data 308 and uses the text 308 as a search parameter when comparing with resource text data.
- the search system 106 can determine spatial areas of the image for inclusion or exclusion.
- the audio 304 may contain the speech, "water bottle in the right of the picture.”
- the search system 106 converts this speech 304 into text data 308 and parses the statement.
- the system 106 determines that the right of the picture is an area of interest from the phrase "in the right of the picture,” and thus ignores features and objects recognized in the left of the picture 302 and focuses only on those found on the right.
- the search system 106 can detect sentiments for particular features or characteristics.
- the image 302 in the joint image-audio query may contain a red water bottle, as in FIG. 2B.
- the audio 304 may contain the speech, "only blue water bottles, not red.”
- the search system 106 converts this speech 304 into text data 308 and parses the statement to interpret that the user wants only blue water bottles in the search results, as opposed to the red water bottle in the image 302.
- Image feature value data 306 are value scores that represent visual characteristics of a portion of an image 302.
- the portion of the image can include the entirety of the image 302, or a sub-portion of the image.
- the image features 306 can include color, texture, edges, saturation, and other characteristics.
- Example processes for extracting values of image features 306 from which a feature score can be computed include processes for generating color histograms, texture detection processes (e.g., based on spatial variation in pixel intensities), scale-invariant feature transform, edge detection, corner detection, and geometric blur.
- the joint image-audio relevance model 108 receives the image feature data 306 and text data 308.
- the model 108 also accesses resources 314 in a collection of resources. With each resource 314 accessed, the model 108 generates resource image feature data from the resource image, in a manner similar to the query image 302.
- the model 108 also determines text data from the resource 314, such as text on a web page that includes the image, or text associated with the image according to a database schema (e.g., a database of commercial products).
- the model 108 compares the query image feature data with the resource image feature data, and the query text data with the resource text data and computes a relevance score for that resource 314.
- the model 108 computes relevance scores for each resource in the collection of resources, ranks the resources according to the scores, and returns a ranked list of the resources 312.
- the search system 106 then generates search results that reference the images and resources, and provides the search results to the user.
- this process may be repeated iteratively one or more times. For example, after producing a list of resources ranked by relevancy 312 to the image-audio query 302, 304, the system 106 may use one or more of the highest ranked resource images to run another query. This may produce an improved list of relevance resources. Alternatively or in combination, the system may use resource text data from one or more highest ranked resources in addition to or in place of the original query text data 308. ⁇ 3.0 Training The Joint Image-Audio Relevancy Model
- FIG. 3B is a block diagram of an example process 350 for training a joint image-audio relevance model 108.
- the model is trained using annotation pairs. Similar to a joint image-audio query, an annotation pair has image data 352 and associated audio data 354. The set of annotation pairs can be partitioned into a training set and a testing set.
- image feature data 358 is generated from the annotation image data 352 using similar image processing algorithms as those used on the query image.
- Text data 360 is determined from the annotation audio data 354 using similar speech recognition and natural language processing techniques as those used on the query audio.
- a training model 362 receives as input the image feature data 358 and the text data 360.
- the training model 362 also receives as input a resource 356 with a predetermined relevance to the annotation pair 352, 354. This predetermined relevance may be binary (e.g. relevant/not relevant), or on a relative scale (e.g., highly relevant, somewhat relevant, not relevant), or on a scale with more refined values.
- the model 362 generates resource image feature data from the resource image and determines resource text data from the resource text.
- the training model 362 computes a relevance score. Weights that correspond to the image features and text features are adjusted to produce a score in the correct range of the predetermined relevancy. This process is repeated for different resources and with different training annotation pairs, all with predetermined relevancies.
- the testing set of annotation data may then be used to verify the trained model.
- the trained model may receive as input annotation pairs from the testing set, along with resources that have predetermined relevance to each of the testing pairs.
- the testing pairs and resources would be processed to generate feature data as done with the training pairs.
- the model would then generate relevance scores for each of these sets of inputs. If the relevance scores are within a threshold range of acceptability, then the model is adequately trained. If, however, the model generates relevance scores that are not within the threshold range of acceptability, then the model is not adequately trained and the training process may be repeated with the training set of annotation data, and the assigned weights reevaluated and readjusted.
- each of the qualitative scale values in the predetermined relevance scale can be assigned relevance score ranges. For example, if the relevance scores generated by the model range from 1 to 100, in a binary predetermined relevance scale, the threshold may be set at greater than or equal to 50 for relevant and less than 50 for not relevant. Alternatively, the threshold may be made more stringent by assigning, for example, greater than 75 for relevant and less than 25 for not relevant. This may provide for a more effective image-audio relevance model, but may also require more iterations of training to produce. Alternatively, the threshold of acceptability may be more qualitative. For example, for a given annotation pair, there may be a set of resources, with a predetermined ranking from more relevant to less relevant. The acceptability of the training of the model may be evaluated by seeing how close the trained model comes to providing the correct ranking of the resources for the annotation pair.
- the annotation data may be obtained in a variety of ways. In one
- the annotation data is derived from a product database, the product database having a collection of product documents.
- Each product document has an image of a product and associated text with information regarding the product, such as a description, prices, sellers of the product, and reviews and ratings of both the product and sellers of the product.
- the annotation pair 352, 354 includes the image from a product document and a subset of the text from the same document. This would also allow for a predetermined relevance between the product document and the annotation pair 352, 354 created from that document. Since the annotation pair 352, 354 was created from that product document, the annotation pair must be highly relevant to the product document.
- annotation data is derived from selection data from image search result data.
- Query input text entered by users into an image search system may be used as the annotation text data 354 of an annotation pair.
- the annotation image data 352 for the pair may be chosen from images that are the most popular results from the image search corresponding to the query input.
- the popularity of results may be determined by statistical measures such as click through rate.
- annotation data may be from selection data from product search result data.
- the query input can again be used as the annotation text data 354 for an annotation pair.
- the annotation image 352 may be obtained from the product image of the most popular product documents selected by users for that query input. This would also provide product documents to use as resources with high predetermined relevance.
- the annotation data is derived from selection data from general web search result data.
- Query input text entered by users into a web search system may be used as the annotation text data 354 of an annotation pair.
- the web search system may return general web resources, including websites, images, and product documents. If the user selects a product document as a result of the web search, the product image may be used as the annotation image data 352 for the annotation pair. The product document is then used as the resource with known high relevancy.
- human annotators may be used to provide training data.
- the annotators may take a photograph to provide the annotation image 352, and provide speech or text data for the annotation text data 354 of resources they wish to search for.
- the annotators may then search through a product document or other resource database and find resources that are both related and unrelated to the photograph and speech data they provided. For each resource they find, the annotators can then label it as a good quality match or a poor quality match.
- the annotators may be used to rate the quality of matches determined through an automated procedure. For example, any of the previously discussed procedures may be used to obtain annotation data from a product database, product search selection data, image search selection data, or web search selection data, and human annotators may rate the relevance of each annotation pair with the resource selected by the automated process.
- the mixing parameter c is a value that is adjusted between 0 and 1.
- Another example model implements a relevance function f(S, I, R) that restricts the set of resource items considered to only those with textual descriptions that contain the words in S. Using this restricted set, the model then scores on the relevance of the image feature data.
- W s is a 1 x I ⁇ ( ⁇ , R) I matrix, or a vector of dimension
- Another example model implements a relevance function that is an extension to the approach found in the paper, "Large Scale Image Annotation: Learning to Rank with Joint Word-Image Embeddings," by Jason Weston, Samy Bengio, and Nicolas Usunier (“Weston paper”), incorporated herein by reference.
- the approach in the Weston paper involves training on an "embedding space” representation of arbitrary dimension, where distance between two items in the space denotes their similarity.
- Wsi is an R x
- WR is an R x
- Another example model implements a relevance function that further extends the Weston paper approach.
- FIG. 4 is a flowchart of an example process 400 for training a joint image-audio relevance model 108.
- the process 400 can be implemented in the search system 106 and is used to train a joint image-audio relevance model 108.
- the process 400 accesses image annotation data (402).
- the search system 106 accesses image annotation data from a product database.
- the search system 106 may also access image annotation data from product search selection data.
- the search system 106 accesses image annotation data from image search selection data.
- the search system 106 accesses image annotation data from web search selection data.
- the search system 106 may also access image annotation data from data annotated by human annotators.
- the human annotators may create their own image and speech data to annotate, or may access data to annotate from a product database or another automated process.
- the process 400 accesses resources (404).
- the search system 106 accesses resources comprising product documents from a product database.
- the process 400 trains a joint image-audio relevance model on the image annotation data and resources (406).
- the search system 106 trains a joint image-audio relevance model using the image annotation data from the product database and the resources from the product database.
- the joint-image audio relevance model can, for example, be trained according to any of the training algorithms described in section 3.2 above, or other training algorithms can be used.
- FIG. 5 shows a flowchart of an example process 500 for ranking resources for a joint image-audio query.
- the process 500 can be implemented in the search system 106 and is used to rank resources for a joint image-audio query.
- the process 500 receives a joint image-audio query (502).
- the search system 106 receives a joint image-audio query from a user device through the network.
- the process 500 determines query image feature data (504).
- the search system 106 generates image feature value data from the query image received from the user device.
- the process 500 determines query audio feature data (506).
- the search system 106 processes the audio data to generate text data from audio data comprising speech data.
- the process 500 provides query image feature data and query audio feature data to the joint image-audio relevance model (508).
- the search system 106 provides query image feature data and text data to the joint image-audio relevance model.
- the joint image-audio relevance model is trained to generate relevance scores for a collection of resources.
- the process 500 orders resources according to their relevance scores (510). For example, the search system 106 orders the resources from most relevant to least relevant to the image-audio query.
- the process 500 provides search results indicating the order of the resources
- the search system 106 provides search results comprising a list of resource addresses, ranked from most relevant to least relevant to the user device.
- Implementations of the subject matter and the operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- a computer storage medium can be, or be included in, a computer-readable storage device, a computer-readable storage substrate, a random or serial access memory array or device, or a combination of one or more of them.
- a computer storage medium is not a propagated signal
- a computer storage medium can be a source or destination of computer program instructions encoded in an artificially generated propagated signal.
- the computer storage medium can also be, or be included in, one or more separate physical components or media (for example, multiple CDs, disks, or other storage devices).
- the operations described in this specification can be implemented as operations performed by a data processing apparatus on data stored on one or more computer- readable storage devices or received from other sources.
- data processing apparatus encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a
- the apparatus can include special purpose logic circuitry, for example, an FPGA (field programmable gate array) or an ASIC
- the apparatus can also include, in addition to hardware, code that creates an execution environment for the computer program in question, for example, code that constitutes processor firmware, a protocol stack, a database management system, an operating system, a cross-platform runtime environment, a virtual machine, or a combination of one or more of them.
- the apparatus and execution environment can realize various different computing model infrastructures, such as web services, distributed computing and grid computing infrastructures.
- a computer program (also known as a program, software, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be deployed in any form, including as a stand alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment.
- a computer program may, but need not, correspond to a file in a file system.
- a program can be stored in a portion of a file that holds other programs or data (for example, one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (for example, files that store one or more modules, sub programs, or portions of code).
- a computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- the processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform actions by operating on input data and generating output.
- the processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, for example, an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- FPGA field programmable gate array
- ASIC application specific integrated circuit
- processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer.
- a processor will receive instructions and data from a read only memory or a random access memory or both.
- the essential elements of a computer are a processor for performing actions in accordance with instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, for example, magnetic, magneto optical disks, or optical disks.
- mass storage devices for storing data, for example, magnetic, magneto optical disks, or optical disks.
- a computer need not have such devices.
- a computer can be embedded in another device, for example, a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device (for example, a universal serial bus (USB) flash drive), to name just a few.
- Devices suitable for storing computer program instructions and data include all forms of non volatile memory, media and memory devices, including by way of example semiconductor memory devices, for example, EPROM, EEPROM, and flash memory devices; magnetic disks, for example, internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks.
- the processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- implementations of the subject matter described in this specification can be implemented on a computer having a display device, for example, a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, for example, a mouse or a trackball, by which the user can provide input to the computer.
- a display device for example, a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- keyboard and a pointing device for example, a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, for example, visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.
- a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web
Abstract
Description
Claims
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US12/914,653 US8788434B2 (en) | 2010-10-28 | 2010-10-28 | Search with joint image-audio queries |
PCT/US2011/058362 WO2012058577A1 (en) | 2010-10-28 | 2011-10-28 | Search with joint image-audio queries |
Publications (1)
Publication Number | Publication Date |
---|---|
EP2633439A1 true EP2633439A1 (en) | 2013-09-04 |
Family
ID=44993181
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
EP11784547.9A Ceased EP2633439A1 (en) | 2010-10-28 | 2011-10-28 | Search with joint image-audio queries |
Country Status (6)
Country | Link |
---|---|
US (2) | US8788434B2 (en) |
EP (1) | EP2633439A1 (en) |
CN (1) | CN103329126B (en) |
AU (1) | AU2011320530B2 (en) |
DE (1) | DE212011100024U1 (en) |
WO (1) | WO2012058577A1 (en) |
Families Citing this family (48)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6735253B1 (en) | 1997-05-16 | 2004-05-11 | The Trustees Of Columbia University In The City Of New York | Methods and architecture for indexing and editing compressed video over the world wide web |
WO2006096612A2 (en) | 2005-03-04 | 2006-09-14 | The Trustees Of Columbia University In The City Of New York | System and method for motion estimation and mode decision for low-complexity h.264 decoder |
WO2009126785A2 (en) | 2008-04-10 | 2009-10-15 | The Trustees Of Columbia University In The City Of New York | Systems and methods for image archaeology |
US8463053B1 (en) | 2008-08-08 | 2013-06-11 | The Research Foundation Of State University Of New York | Enhanced max margin learning on multimodal data mining in a multimedia database |
US8737728B2 (en) | 2011-09-30 | 2014-05-27 | Ebay Inc. | Complementary item recommendations using image feature data |
US9009149B2 (en) * | 2011-12-06 | 2015-04-14 | The Trustees Of Columbia University In The City Of New York | Systems and methods for mobile search using Bag of Hash Bits and boundary reranking |
US8799257B1 (en) * | 2012-03-19 | 2014-08-05 | Google Inc. | Searching based on audio and/or visual features of documents |
US9223776B2 (en) * | 2012-03-27 | 2015-12-29 | The Intellectual Group, Inc. | Multimodal natural language query system for processing and analyzing voice and proximity-based queries |
US8897484B1 (en) | 2012-05-18 | 2014-11-25 | Google Inc. | Image theft detector |
US9098584B1 (en) | 2012-07-19 | 2015-08-04 | Google Inc. | Image search privacy protection techniques |
KR101917695B1 (en) * | 2012-08-09 | 2018-11-13 | 엘지전자 주식회사 | Mobile terminal and control method for the mobile terminal |
KR20140127975A (en) * | 2013-04-26 | 2014-11-05 | 삼성전자주식회사 | Information processing apparatus and control method thereof |
CN104346792B (en) * | 2013-07-24 | 2018-07-27 | 腾讯科技（深圳）有限公司 | Image processing method, Photo Viewer and terminal |
US9384213B2 (en) * | 2013-08-14 | 2016-07-05 | Google Inc. | Searching and annotating within images |
US20150228002A1 (en) * | 2014-02-10 | 2015-08-13 | Kelly Berger | Apparatus and method for online search, imaging, modeling, and fulfillment for interior design applications |
BR112016017262B1 (en) * | 2014-05-15 | 2022-09-27 | Huawei Technologies Co., Ltd. | METHOD FOR SEARCHING FOR OBJECTS AND TERMINAL ATTACHED COMMUNICATIVELY TO A SERVER. |
US11314826B2 (en) | 2014-05-23 | 2022-04-26 | Samsung Electronics Co., Ltd. | Method for searching and device thereof |
US9990433B2 (en) | 2014-05-23 | 2018-06-05 | Samsung Electronics Co., Ltd. | Method for searching and device thereof |
CN111046197A (en) * | 2014-05-23 | 2020-04-21 | 三星电子株式会社 | Searching method and device |
US9830391B1 (en) | 2014-06-24 | 2017-11-28 | Google Inc. | Query modification based on non-textual resource context |
US9811592B1 (en) | 2014-06-24 | 2017-11-07 | Google Inc. | Query modification based on textual resource context |
CN107111601B (en) * | 2014-12-18 | 2021-01-01 | 惠普发展公司，有限责任合伙企业 | Identifying resources based on handwritten annotations |
US9904450B2 (en) | 2014-12-19 | 2018-02-27 | At&T Intellectual Property I, L.P. | System and method for creating and sharing plans through multimodal dialog |
US9633019B2 (en) * | 2015-01-05 | 2017-04-25 | International Business Machines Corporation | Augmenting an information request |
CN104598585A (en) * | 2015-01-15 | 2015-05-06 | 百度在线网络技术（北京）有限公司 | Information search method and information search device |
US10963795B2 (en) | 2015-04-28 | 2021-03-30 | International Business Machines Corporation | Determining a risk score using a predictive model and medical model data |
US11003667B1 (en) | 2016-05-27 | 2021-05-11 | Google Llc | Contextual information for a displayed resource |
US10152521B2 (en) | 2016-06-22 | 2018-12-11 | Google Llc | Resource recommendations for a displayed resource |
US10802671B2 (en) | 2016-07-11 | 2020-10-13 | Google Llc | Contextual information for a displayed resource that includes an image |
US11055335B2 (en) * | 2016-07-15 | 2021-07-06 | Google Llc | Contextual based image search results |
US10489459B1 (en) | 2016-07-21 | 2019-11-26 | Google Llc | Query recommendations for a displayed resource |
US10467300B1 (en) | 2016-07-21 | 2019-11-05 | Google Llc | Topical resource recommendations for a displayed resource |
US10051108B2 (en) | 2016-07-21 | 2018-08-14 | Google Llc | Contextual information for a notification |
US10212113B2 (en) | 2016-09-19 | 2019-02-19 | Google Llc | Uniform resource identifier and image sharing for contextual information display |
US10579688B2 (en) * | 2016-10-05 | 2020-03-03 | Facebook, Inc. | Search ranking and recommendations for online social networks based on reconstructed embeddings |
US10452688B2 (en) * | 2016-11-08 | 2019-10-22 | Ebay Inc. | Crowd assisted query system |
US10623569B2 (en) * | 2017-06-08 | 2020-04-14 | Avaya Inc. | Document detection and analysis-based routing |
US10679068B2 (en) | 2017-06-13 | 2020-06-09 | Google Llc | Media contextual information from buffered media data |
US10977303B2 (en) * | 2018-03-21 | 2021-04-13 | International Business Machines Corporation | Image retrieval using interactive natural language dialog |
US11307880B2 (en) | 2018-04-20 | 2022-04-19 | Meta Platforms, Inc. | Assisting users with personalized and contextual communication content |
US11715042B1 (en) | 2018-04-20 | 2023-08-01 | Meta Platforms Technologies, Llc | Interpretability of deep reinforcement learning models in assistant systems |
US10978056B1 (en) * | 2018-04-20 | 2021-04-13 | Facebook, Inc. | Grammaticality classification for natural language generation in assistant systems |
US11886473B2 (en) | 2018-04-20 | 2024-01-30 | Meta Platforms, Inc. | Intent identification for agent matching by assistant systems |
US11676220B2 (en) | 2018-04-20 | 2023-06-13 | Meta Platforms, Inc. | Processing multimodal user input for assistant systems |
US10782986B2 (en) | 2018-04-20 | 2020-09-22 | Facebook, Inc. | Assisting users with personalized and contextual communication content |
US11169668B2 (en) * | 2018-05-16 | 2021-11-09 | Google Llc | Selecting an input mode for a virtual assistant |
US11586927B2 (en) * | 2019-02-01 | 2023-02-21 | Google Llc | Training image and text embedding models |
JP2021068064A (en) * | 2019-10-18 | 2021-04-30 | 富士ゼロックス株式会社 | Query correction system, search system, and program |
Family Cites Families (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
EP0947937B1 (en) * | 1998-04-02 | 2010-11-03 | Canon Kabushiki Kaisha | Image search apparatus and method |
US6243713B1 (en) | 1998-08-24 | 2001-06-05 | Excalibur Technologies Corp. | Multimedia document retrieval by application of multimedia queries to a unified index of multimedia data for a plurality of multimedia data types |
GB0023930D0 (en) * | 2000-09-29 | 2000-11-15 | Canon Kk | Database annotation and retrieval |
US6925475B2 (en) * | 2001-10-12 | 2005-08-02 | Commissariat A L'energie Atomique | Process and apparatus for management of multimedia databases |
GB2399983A (en) * | 2003-03-24 | 2004-09-29 | Canon Kk | Picture storage and retrieval system for telecommunication system |
US20050038814A1 (en) | 2003-08-13 | 2005-02-17 | International Business Machines Corporation | Method, apparatus, and program for cross-linking information sources using multiple modalities |
US7702681B2 (en) | 2005-06-29 | 2010-04-20 | Microsoft Corporation | Query-by-image search and retrieval system |
US7684651B2 (en) * | 2006-08-23 | 2010-03-23 | Microsoft Corporation | Image-based face search |
US8392411B2 (en) * | 2010-05-20 | 2013-03-05 | Google Inc. | Automatic routing of search results |
EP2588972A4 (en) * | 2010-07-01 | 2014-06-11 | Method and apparatus for adapting a context model |
-
2010
- 2010-10-28 US US12/914,653 patent/US8788434B2/en active Active
-
2011
- 2011-10-28 DE DE212011100024U patent/DE212011100024U1/en not_active Expired - Lifetime
- 2011-10-28 WO PCT/US2011/058362 patent/WO2012058577A1/en active Application Filing
- 2011-10-28 AU AU2011320530A patent/AU2011320530B2/en not_active Ceased
- 2011-10-28 CN CN201180061276.5A patent/CN103329126B/en active Active
- 2011-10-28 EP EP11784547.9A patent/EP2633439A1/en not_active Ceased
-
2014
- 2014-07-21 US US14/336,464 patent/US20140330822A1/en not_active Abandoned
Non-Patent Citations (2)
Title |
---|
None * |
See also references of WO2012058577A1 * |
Also Published As
Publication number | Publication date |
---|---|
WO2012058577A1 (en) | 2012-05-03 |
AU2011320530A1 (en) | 2013-05-23 |
CN103329126A (en) | 2013-09-25 |
US20140330822A1 (en) | 2014-11-06 |
US8788434B2 (en) | 2014-07-22 |
CN103329126B (en) | 2018-04-24 |
US20120109858A1 (en) | 2012-05-03 |
AU2011320530B2 (en) | 2016-06-16 |
DE212011100024U1 (en) | 2012-07-10 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US8788434B2 (en) | Search with joint image-audio queries | |
US9372920B2 (en) | Identifying textual terms in response to a visual query | |
CN108416028B (en) | Method, device and server for searching content resources | |
US9396413B2 (en) | Choosing image labels | |
AU2012253364B2 (en) | Dynamic image display area and image display within web search results | |
US9727584B2 (en) | Refining image annotations | |
TWI420331B (en) | System and method for inclusion of interactive elements on a search results page | |
US20110191336A1 (en) | Contextual image search | |
CN108763244B (en) | Searching and annotating within images | |
US20080133505A1 (en) | Search results presented as visually illustrative concepts | |
US10733228B2 (en) | Sketch and style based image retrieval | |
US10268928B2 (en) | Combined structure and style network | |
US9183577B2 (en) | Selection of images to display next to textual content | |
US9218366B1 (en) | Query image model | |
US9507805B1 (en) | Drawing based search queries | |
US9619705B1 (en) | Object identification in visual media | |
US20160357868A1 (en) | Related entities | |
CN116521990A (en) | Method, apparatus, electronic device and computer readable medium for material processing | |
Soysal et al. | Weighted feature fusion for content-based image retrieval |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PUAI | Public reference made under article 153(3) epc to a published international application that has entered the european phase |
Free format text: ORIGINAL CODE: 0009012 |
|
17P | Request for examination filed |
Effective date: 20130516 |
|
AK | Designated contracting states |
Kind code of ref document: A1Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
DAX | Request for extension of the european patent (deleted) | ||
RIN1 | Information on inventor provided before grant (corrected) |
Inventor name: WESTON, JASON E.Inventor name: MAKADIA, AMEESH |
|
17Q | First examination report despatched |
Effective date: 20160309 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: EXAMINATION IS IN PROGRESS |
|
RAP1 | Party data changed (applicant data changed or rights of an application transferred) |
Owner name: GOOGLE LLC |
|
APBK | Appeal reference recorded |
Free format text: ORIGINAL CODE: EPIDOSNREFNE |
|
APBN | Date of receipt of notice of appeal recorded |
Free format text: ORIGINAL CODE: EPIDOSNNOA2E |
|
APBR | Date of receipt of statement of grounds of appeal recorded |
Free format text: ORIGINAL CODE: EPIDOSNNOA3E |
|
APAF | Appeal reference modified |
Free format text: ORIGINAL CODE: EPIDOSCREFNE |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R003 |
|
APBT | Appeal procedure closed |
Free format text: ORIGINAL CODE: EPIDOSNNOA9E |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: THE APPLICATION HAS BEEN REFUSED |
|
18R | Application refused |
Effective date: 20210902 |
|
P01 | Opt-out of the competence of the unified patent court (upc) registered |
Effective date: 20230519 |