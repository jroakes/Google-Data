CN110832477A - Sensor-based semantic object generation - Google Patents
Sensor-based semantic object generation Download PDFInfo
- Publication number
- CN110832477A CN110832477A CN201880044505.4A CN201880044505A CN110832477A CN 110832477 A CN110832477 A CN 110832477A CN 201880044505 A CN201880044505 A CN 201880044505A CN 110832477 A CN110832477 A CN 110832477A
- Authority
- CN
- China
- Prior art keywords
- objects
- semantic
- data
- indications
- semantic objects
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/70—Information retrieval; Database structures therefor; File system structures therefor of video data
- G06F16/78—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/783—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
- G06F16/7837—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content using objects detected or recognised in the video content
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/30—Semantic analysis
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/40—Processing or translation of natural language
- G06F40/42—Data-driven translation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/20—Scenes; Scene-specific elements in augmented reality scenes
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V30/00—Character recognition; Recognising digital ink; Document-oriented image-based pattern recognition
- G06V30/10—Character recognition
- G06V30/26—Techniques for post-processing, e.g. correcting the recognition result
- G06V30/262—Techniques for post-processing, e.g. correcting the recognition result using context analysis, e.g. lexical, syntactic or semantic context
- G06V30/274—Syntactic or semantic context, e.g. balancing
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/21—Design or setup of recognition systems or techniques; Extraction of features in feature space; Blind source separation
- G06F18/214—Generating training patterns; Bootstrap methods, e.g. bagging or boosting
Abstract
Methods, systems, and apparatus are provided for generating semantic objects and outputs based on detection or recognition of a state of an environment including the objects. Status data based in part on sensor output may be received from one or more sensors that detect a status of an environment including an object. Semantic objects are generated based in part on the state data. The semantic object may correspond to an object and include a set of attributes. Based in part on the set of attributes of the semantic object, one or more operating modes associated with the semantic object may be determined. Based in part on the one or more operating modes, an object output associated with the semantic object may be generated. The object output may include one or more visual indications or one or more audio indications.
Description
Technical Field
The present disclosure relates generally to generating semantic objects and outputs based on detection or recognition of a state of an environment including the objects.
Background
The object detection system may capture a variety of information about objects in the environment, including, for example, the appearance of the objects. Associating aspects of a detected object (e.g., the appearance of the object) with another piece of information, such as an identification of the object, is useful in various applications, such as facial recognition, where facial detection and recognition can be used to authorize use of the device based on whether the recognized face corresponds to an authorized user of the device. However, many existing object detection systems require a large amount of user input and interaction, which can be burdensome. In addition, many existing object detection systems provide limited functionality or have insufficient functionality due to cumbersome user interfaces. Therefore, it would be beneficial if there were ways to more efficiently capture, process, and manipulate information associated with the state of an environment.
Disclosure of Invention
Aspects and advantages of embodiments of the present disclosure will be set forth in part in the description which follows, or may be learned from the description, or may be learned by practice of the embodiments.
One example aspect of the present disclosure is directed to a method for generating semantic objects and outputs based on detection or recognition of a state of an environment including the objects. The method may include receiving, by a computing system comprising one or more computing devices, status data based in part on sensor output from one or more sensors that detect a status of an environment comprising one or more objects. The method may also include generating, by the computing system, one or more semantic objects corresponding to the one or more objects based in part on the state data. One or more semantic objects may include a collection of attributes. The method may include determining, by the computing system, one or more operating modes associated with the one or more semantic objects based in part on the set of attributes of the one or more semantic objects. Additionally, the method may include generating, by the computing system, one or more object outputs associated with the one or more semantic objects based in part on the one or more operating modes. The one or more object outputs may include one or more visual indications or one or more audio indications.
Another example aspect of the disclosure is directed to one or more tangible, non-transitory computer-readable media storing computer-readable instructions that, when executed by one or more processors, cause the one or more processors to perform operations. The operations may include receiving status data based in part on sensor output from one or more sensors that detect a status of an environment including one or more objects. The operations may also include generating one or more semantic objects corresponding to the one or more objects based in part on the state data. One or more semantic objects may include a collection of attributes. The operations may include determining one or more operational modes associated with the one or more semantic objects based in part on a set of attributes of the one or more semantic objects. Additionally, the operations may include generating one or more object outputs associated with the one or more semantic objects based in part on the one or more operational modes. The one or more object outputs may include one or more visual indications or one or more audio indications.
Another example aspect of the disclosure is directed to a computing system comprising one or more processors and one or more non-transitory computer-readable media storing instructions that, when executed by the one or more processors, cause the one or more processors to perform operations. The operations may include receiving status data based in part on sensor output from one or more sensors that detect a status of an environment including one or more objects. The operations may also include generating one or more semantic objects corresponding to the one or more objects based in part on the state data. One or more semantic objects may include a collection of attributes. The operations may include determining one or more operational modes associated with the one or more semantic objects based in part on a set of attributes of the one or more semantic objects. Additionally, the operations may include generating one or more object outputs associated with the one or more semantic objects based in part on the one or more operational modes. The one or more object outputs may include one or more visual indications or one or more audio indications.
Other example aspects of the present disclosure are directed to other computer-implemented methods, systems, apparatuses, tangible, non-transitory computer-readable media, user interfaces, memory devices, and electronic devices for generating semantic objects and outputs based on detection or recognition of a state of an environment that includes the objects.
These and other features, aspects, and advantages of various embodiments will become better understood with reference to the following description and appended claims. The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate embodiments of the disclosure and together with the description, serve to explain the principles involved.
Drawings
A detailed description of embodiments directed to those of ordinary skill in the art is set forth in the specification, which makes reference to the appended drawings, in which:
fig. 1 illustrates a diagram of an example system according to an example embodiment of the present disclosure;
fig. 2 illustrates a diagram of an example apparatus according to an example embodiment of the present disclosure;
fig. 3 illustrates an example of sensor-based semantic object generation including image capture according to an example embodiment of the present disclosure;
fig. 4 illustrates an example of sensor-based semantic object generation including audio generation, according to an example embodiment of the present disclosure;
FIG. 5 illustrates an example of sensor-based semantic object generation including text translation, according to an example embodiment of the present disclosure;
FIG. 6 illustrates an example of sensor-based semantic object generation including text recognition in accordance with an example embodiment of the present disclosure;
FIG. 7 illustrates an example of sensor-based semantic object generation including text recognition in accordance with an example embodiment of the present disclosure;
fig. 8 illustrates an example of sensor-based semantic object generation including object recognition, according to an example embodiment of the present disclosure;
fig. 9 illustrates an example of sensor-based semantic object generation including object recognition, according to an example embodiment of the present disclosure;
FIG. 10 illustrates an example of sensor-based semantic object generation including location identification, according to an example embodiment of the present disclosure;
FIG. 11 illustrates an example of sensor-based semantic object generation including location identification, according to an example embodiment of the present disclosure;
FIG. 12 illustrates an example of sensor-based semantic object generation including navigation in accordance with an example embodiment of the present disclosure;
FIG. 13 illustrates an example of a sensor-based semantic object generated interface element including location identification in accordance with an example embodiment of the present disclosure;
FIG. 14 illustrates a flow diagram of sensor-based semantic object generation according to an example embodiment of the present disclosure;
FIG. 15 illustrates a flow diagram of sensor-based semantic object generation according to an example embodiment of the present disclosure;
FIG. 16 illustrates a flow diagram of sensor-based semantic object generation according to an example embodiment of the present disclosure; and
fig. 17 illustrates a flow diagram for sensor-based semantic object generation according to an example embodiment of the present disclosure.
Detailed Description
Example aspects of the present disclosure are directed to detecting, identifying, and/or identifying objects in an environment, generating semantic objects based on the objects (e.g., data structures stored in a storage device and including one or more attributes associated with one or more objects), and generating output (e.g., visual indications and/or audio indications) based on the semantic objects. The disclosed technology may receive state data associated with a state of an environment (e.g., an outdoor area or an indoor area) and objects in the environment (e.g., buildings, people, vehicles, consumer goods, and/or textual material), generate one or more semantic objects corresponding to the one or more objects (e.g., a handbag semantic object for a physical handbag), determine one or more operational modes associated with the one or more semantic objects (i.e., determine how to process the one or more objects), and generate one or more object outputs that may include one or more visual indications (e.g., one or more images that include textual information associated with the one or more objects) or one or more audio indications (e.g., one or more sounds associated with the one or more objects).
Thus, the disclosed techniques may more efficiently identify objects in an environment and perform various functions based on those objects in a manner that is unobtrusive and may in some cases require a minimal level of user input. Additionally, in some embodiments, by generating one or more semantic objects based on a continuous collection of sensor outputs from real world objects, the disclosed techniques can emphasize regions of interest that may not otherwise be attended to. In addition, by determining an operating mode for collecting and processing sensor inputs, the disclosed techniques can conserve computing resources and provide information that is more relevant to the needs of the user.
For example, the disclosed technology may include a computing device carried by a user in an environment that includes a variety of objects (e.g., a city environment). As the user walks through the environment, the user may hold the computing device in their hand. The computing device may include a camera (e.g., a periscopic (peri) camera) located on a portion of the computing device (e.g., a top edge of the computing device) such that when the device is held perpendicular to the user and/or parallel to the ground for a long edge, the camera may capture one or more images without the user needing to aim the camera at objects in the environment. In particular, the camera may be located at a top edge of the computing device such that when the computing device is held in a comfortable position for the user (e.g., holding the long edge of the device perpendicular to the user and/or parallel to the ground), the camera has a field of view that is generally in the same direction as the user's vision (e.g., a field of view in front of the user in the direction that the user is facing).
As the user walks through the environment, an electronic device (e.g., a television) in the store presentation window may capture the user's interests and the user may be near the store presentation window, a camera may capture an image of the electronic device, and a computing device may generate semantic objects associated with the electronic device. Semantic objects associated with an object, such as an electronic device, may include one or more attributes including the type (e.g., television), size (e.g., sixty-five inch screen size), brand (make) (e.g., brand of the television manufacturer), and model (e.g., model number associated with the television) of the object.
Based on the semantic object, the computing device may determine an operating mode to use for the semantic object. The operating mode may indicate a type of processing that the computing device and/or associated computing system is to perform on the semantic object. For example, when text is detected in an object, the computing device may use a text recognition mode. In this example, the computing device may determine that the object is a commodity and may access one or more remote data sources and generate a query based on attributes of semantic objects associated with the object (e.g., perform a search through an internet search engine).
The disclosed technology may then provide an output to the user that includes information about the electronic device itself, as well as, for example, other stores that may purchase the electronic device, product rating (rating) associated with the electronic device, and links to websites that provide more information about the electronic device. In this manner, the computing device may perform semantic enhancement, including sensor-based semantic object generation, to more efficiently process sensor output and provide greater convenience to the user, as the computing device performs tasks that would otherwise be performed by the user.
In some embodiments, the disclosed technology may include a computing system (e.g., a semantic processing system) that may include one or more computing devices (e.g., a device having one or more computer processors and memory that may store one or more instructions) that may exchange (send and/or receive), process, generate, and/or modify data that includes one or more information patterns or structures that may be stored on one or more memory devices (e.g., random access memory) and/or storage devices (e.g., a hard disk drive and/or a solid state drive) and/or one or more signals (e.g., electronic signals). Data and/or one or more signals may be exchanged by a computing system with various other devices including a remote computing device that may provide data associated with or including semantic type data associated with various attributes of an object (e.g., a price of an item of merchandise), and/or one or more sensor devices that may provide sensor output for a geographic area (e.g., a camera image from an internet-accessible camera device) that may be used to determine a state of an environment including one or more objects.
In some embodiments, the semantic processing system may include a display component (e.g., a Liquid Crystal Display (LCD), an Organic Light Emitting Diode (OLED), a plasma display panel, an electronic ink, and/or a cathode ray tube) configured to display one or more images, which may include images of an environment containing one or more objects detected by one or more sensors.
The semantic processing system may receive data, for example, including status data based in part on sensor output from one or more sensors that detect a status of an environment including one or more objects including a physical object (e.g., a building, a book, and/or luggage). The state data may include information associated with the state of the environment and one or more objects in the environment, including the location of the one or more objects, the time of day at which sensor output from the one or more objects was captured, and/or one or more physical characteristics of the objects in the environment (e.g., the size, appearance, and/or one or more sounds produced by the one or more objects).
In some embodiments, the one or more sensors may include one or more light sensors (e.g., one or more cameras); one or more perimeter mirrors comprising one or more cameras having a field of view in excess of one hundred eighty degrees; one or more audio sensors (e.g., one or more microphones); one or more tactile sensors; one or more barometric pressure sensors; one or more gyro sensors; one or more accelerometers including configurations in which the one or more accelerometers can determine acceleration along three axes (e.g., an X-axis, a Y-axis, and a Z-axis); one or more humidity sensors including one or more sensors that can detect a level of moisture in the air; one or more electromagnetic sensors; and/or one or more thermal sensors.
Additionally, the one or more panoramic cameras may be configured or positioned to capture one or more images including one or more objects or portions of one or more objects that are not within the visual plane of the display assembly. The display component of the semantic computing system may include a visual plane that may include a plane that, if an optical sensor, would capture images within less than one hundred eighty degrees of the center of the optical sensor (e.g., images perpendicular to the visual plane would not be captured). For example, if the semantic processing device is in the shape of a rectangular cuboid, the display component (e.g., an LCD screen) may be located on one or both of the two sides of the cuboid having the largest surface area, and the one or more perimetric cameras may be located on one or more of the four sides of the cuboid not having the largest surface area.
Additionally, the semantic processing system may operate continuously such that detection, identification, and/or recognition of an environment including one or more objects in the environment may be performed without interruption without input or instructions from a user. The semantic processing system may also provide an indication of the identified one or more objects, or an indication of an operational mode (e.g., a path finding mode, a translation mode, and/or an object detection mode) as part of an interface (e.g., a graphical user interface including a status bar).
Additionally, in some embodiments, the identification of one or more objects may be performed in a continuous process as a background operation (e.g., on a background thread). Thus, in some embodiments, the semantic processing system may operate continuously in the background to identify objects within the environment based on sensor data indicative of the environment. In some embodiments, such background operations may include operations to identify objects even if the camera application is not being executed by the system (e.g., to operate in the background even if the user does not have a camera operating the system). The user may be provided with control over when the semantic processing system operates to identify objects and when and what type of data is collected for use by the semantic processing system.
One or more sensors may be configured to detect a state (e.g., a physical state) of an environment including one or more properties or characteristics of one or more objects. In addition, the semantic processing system may have access to a chronograph meter (e.g., a local-based chronograph meter or a chronograph meter at a remote location) that may be used to determine a time of day and/or duration of one or more events including local events (e.g., events detectable by one or more sensors) and non-local events (e.g., events occurring at locations not detected by one or more sensors). The one or more properties or characteristics of the environment may include a time of day and/or a geographic location (e.g., a latitude and longitude associated with the environment). The one or more properties or characteristics of the one or more objects may include a size (e.g., height, length, and/or width), a mass, a weight, a volume, a color, and/or a sound associated with the one or more objects.
The semantic processing system may generate one or more semantic objects corresponding to the one or more objects, for example, based in part on the state data and an object recognition model that includes a machine learning model. The semantic processing system may access a machine learning model that has been created using a classification dataset (e.g., access a machine learning model that has been stored locally and/or a machine learning model stored on a remote computing device) that includes classifier data that includes a set of classified features and a set of classified object labels associated with training data that may be based on or associated with a plurality of training objects (e.g., physical objects or simulated objects that serve as training inputs for the machine learning model). The classification dataset may be based in part on input from one or more sensors (e.g., cameras and/or microphones) that have been used to generate visual output and audio output based on visual input and audio input, respectively. For example, a machine learning model may be created using a set of cameras and microphones that capture training data, including video and audio of a city region, including various objects including buildings, streets, vehicles, people, and/or surfaces with text.
In some embodiments, the machine learning model may be based in part on one or more classification techniques including linear regression, logistic regression, random forest classification, reinforced forest classification, gradient reinforcement, neural networks, support vector machines, or decision trees. In addition, the semantic processing system may use various object recognition models or techniques in conjunction with or without a machine learning model to generate and/or process one or more semantic objects. For example, the object recognition technique may receive sensor data associated with one or more sensor outputs and may include one or more genetic algorithms, edge matching, grayscale matching, gradient matching, and/or gesture clustering.
The one or more semantic objects may include a set of attributes (e.g., a set of attributes for each of the one or more semantic objects). For example, the set of attributes associated with one or more semantic objects may include: one or more object identifications including identifications of one or more objects associated with one or more semantic objects (e.g., a designer and style of an article of clothing); one or more object types (e.g., a pair of pants or a men's shirt that may be associated with a clothing type) associated with a type, category, or category of one or more objects associated with one or more semantic objects; an object location (e.g., an address of a building object) including a geographic location associated with one or more objects associated with one or more semantic objects; monetary value (e.g., one or more prices associated with the object); ownership status including the owner of the object (e.g., owner of real estate); and/or a set of physical characteristics (e.g., a size or mass associated with the object).
The semantic processing system may determine one or more operating modes associated with the one or more semantic objects based in part on a set of attributes of the one or more semantic objects. The one or more operating modes may determine the manner in which one or more semantic objects are processed and/or used by the semantic processing system. Thus, the semantic processing system may selectively dedicate computing resources to a subset of possible operations based on one or more attributes of one or more semantic objects (e.g., detection of a poster including text will result in a determination that a text recognition mode will be used to process the one or more semantic objects associated with the poster).
The one or more operating modes may include a text recognition mode associated with recognizing text information in the environment (e.g., recognizing when an object contains text); a location identification pattern associated with identifying one or more locations in the environment (e.g., locating a portal to a store); an object recognition pattern associated with recognizing one or more objects in the environment (e.g., recognizing a piece of merchandise); and/or an event recognition pattern associated with recognizing the occurrence of one or more events in the environment.
The semantic processing system may generate one or more object outputs associated with the one or more semantic objects based in part on the one or more operating modes. The one or more object outputs may include one or more outputs via one or more output devices of the semantic processing system (e.g., one or more display devices, audio devices, and/or tactile output devices). The text recognition mode may produce one or more object outputs that include text-related outputs including translations of the recognized text (e.g., generating english text based on detection and translation of chinese text).
In some embodiments, the one or more object outputs may include one or more visual indications (e.g., one or more visual images produced by a display device of the semantic processing system) and/or one or more audio indications (e.g., one or more sounds produced by an audio output device of the semantic processing system). For example, the one or more object outputs may include translations displayed on a display device, audio indications including an audio version of written text (e.g., text-to-speech), and/or one or more images superimposed on a camera image of the environment.
The semantic processing system may determine object data that matches the one or more semantic objects based in part on a set of attributes of the one or more semantic objects. For example, the semantic processing system may match the set of attributes to the object data based on one or more comparisons between portions of the set of attributes and the object data. The object data may include information associated with: one or more related objects (e.g., semantic objects for rings may be associated with other jewelry); one or more remote data sources (e.g., semantic objects for a book may be associated with a website associated with the author of the book); one or more locations; and/or one or more events.
The semantic processing system may access one or more portions of object data that match one or more semantic objects. For example, the semantic processing system may access one or more portions of object data stored on one or more remote computing devices. In some embodiments, the one or more object outputs may be based in part on one or more portions of object data matching the one or more semantic objects. For example, when the object data includes links to one or more remote computing devices associated with one or more semantic objects, the one or more object outputs may include those links.
The semantic processing system may generate one or more interface elements associated with the one or more objects based in part on the state data or the one or more semantic objects. The one or more interface elements may include one or more images (e.g., graphical user interface elements including pictograms and/or text) responsive to one or more inputs (e.g., the one or more interface elements may initiate or trigger one or more operations based on tactile and/or audio inputs). For example, one or more interface elements may include a status indicator (e.g., a status bar) that may provide a continuous indication of the status of one or more objects. In some embodiments, the identification of the one or more objects may be performed as a continuous process (e.g., continuous identification of the one or more objects) such that the one or more objects may be detected, identified, and/or identified in real-time (e.g., sensor outputs including visual and/or audio sensor outputs associated with the one or more objects), and the one or more interface elements including the status indicator may also be continuously updated (e.g., when the one or more objects are identified in real-time). In addition, one or more interface elements may be used to provide navigation instructions (e.g., text or audio instructions associated with a path to a location) and other information related to one or more objects in the environment.
Thus, in some embodiments, the semantic processing system may operate continuously in the background to identify objects. Upon identifying one or more objects, the semantic processing system may provide a status indicator in a status bar of the user interface. The status indicator may indicate that an object has been identified and, in some embodiments, may further indicate the type of object that has been identified. The status indicator in the status bar may provide a non-invasive visual indication that additional semantic information for the object is available. If interested in receiving additional semantic information, the user may interact with the status indicator (e.g., by clicking or pulling down), and may display the additional information within the user interface (e.g., in the form of additional interface elements).
In response to receiving one or more inputs to one or more interface elements, the semantic processing system may determine one or more remote computing devices that include at least a portion of the object data (e.g., one or more remote computing devices that store some portion of the object data). The one or more object outputs may include one or more remote source indications associated with one or more remote computing devices (e.g., IP addresses associated with the one or more remote computing devices) that include at least a portion of the object data.
The semantic processing system may determine one or more objects including one or more semantic symbols (e.g., one or more phonemes (grams) including one or more letters, one or more logograms (logograms), one or more syllabic characters (syllabic characters), and/or one or more pictograms) based in part on the state data or one or more semantic objects. Based in part on the one or more semantic symbols, the semantic processing system may determine one or more words associated with the one or more semantic symbols (e.g., using dictionary data, certain combinations of the one or more semantic symbols may be associated with the words). In some embodiments, the set of attributes of one or more semantic objects may include one or more words. For example, a semantic object for a poster with text indicating "a concert at a municipality center at 8 pm" may include a poster semantic object that includes a set of attributes including the concert as a value for an event type attribute, 8 pm as a value for an event time attribute, and the municipality center, or geographic coordinates associated with the municipality center as a value for a location attribute.
The semantic processing system may determine a detected language associated with one or more semantic symbols. For example, based in part on a combination of one or more semantic symbols (e.g., a word associated with one or more semantic symbols), the semantic processing system may determine a language associated with the one or more semantic symbols (e.g., a language including english, russian, chinese, and/or french).
When the detected language is not associated with a default language (e.g., a language that a user of the semantic processing system has selected as the language to which the detected language is to be translated when the detected language is not the same as the default language), the semantic processing system may generate an output of the translation based in part on the translation data. The translation data may include one or more semantic symbols of a default language and one or more semantic symbols of a detected language. The semantic processing system may compare one or more semantic symbols of the detected language with one or more semantic symbols of a default language to determine and perform an analysis of the translated detected language.
The output of the translation may include one or more semantic symbols of the default language corresponding to a portion of the one or more semantic symbols of the detected language (e.g., a multilingual thesaurus including a list of one or more words of the default language, each word associated with a respective word of the detected language). In some embodiments, the one or more object outputs may be based in part on the translated output (e.g., the one or more object outputs may include a visual indication or an audio indication of the translation).
The semantic processing system may receive location data that includes information associated with a current location and a destination location of the environment (e.g., a destination location selected by a user of the semantic processing system). Additionally, the semantic processing system may determine a path from the current location to the destination location (e.g., a path between the current location and the destination location that avoids an obstacle between the current location and the destination location) based in part on the location data and the status and location data of the one or more objects within the field of view of the one or more sensors.
In addition, the semantic processing system may generate one or more directions based in part on the one or more semantic objects and the path from the current location to the destination location. Additionally, the semantic processing system may determine one or more semantic objects that may be used as landmarks associated with one or more directions (e.g., a semantic object associated with a lamppost may be used as part of one or more directions "a lamppost in front of you turns left"). In some embodiments, the one or more object outputs may be based in part on one or more directions (e.g., one or more visual indications or one or more audio indications may include directions).
In some embodiments, the semantic processing system may determine one or more relevance values corresponding to one or more semantic objects. The one or more relevance values may be based in part on a degree to which each of the one or more semantic objects is associated with the context data. The context data may include various characteristics associated with the environment, including data associated with a time of day, a current location (e.g., latitude and longitude associated with the environment), one or more scheduled events (e.g., one or more events that will occur within a predetermined time period), one or more user locations, or one or more user preferences (e.g., one or more preferences of a user including food preferences, music preferences, and/or entertainment preferences). In some embodiments, the one or more object outputs may be based in part on one or more relevance values corresponding to the one or more semantic objects.
The semantic processing system may modify one or more visual indications or one or more audio indications based in part on the state data or semantic data. Modifying the one or more visual indications or the one or more audio indications may include: converting the one or more visual indications into one or more modified audio indications (e.g., generating an artificial language based on the text); converting the one or more audio indications into one or more modified visual indications (e.g., generating text based on audio input to a microphone); modifying a size of the one or more visual indications (e.g., increasing a size of text captured by the camera); modifying one or more color characteristics of the one or more visual indications (e.g., generating a highlight around the one or more visual indications); and/or modify the amplitude of (e.g., increase the volume of) one or more audio indications. Such modification of the one or more visual indications and/or the one or more audio indications may be used to enhance the experience of any user, and may be particularly useful for individuals with visual or auditory impairments. For example, the semantic processing system may enhance the size and definition of text that would otherwise be unreadable by individuals with visual defects.
One example aspect of the present disclosure is directed to a mobile device including a display. In some embodiments, the plane of the display may define a first plane of the mobile device. The mobile device may comprise a camera arranged to capture one or more images from a direction parallel to a first plane of the mobile device. The mobile device may include a processor configured to receive an image captured by the camera, identify one or more objects present in the received image, and control an output of the display based on the one or more identified objects in the received image.
In some embodiments, the processor is configured to control the display to output the user interface element in response to the one or more identified objects. The user interface elements may be displayed over one or more user interface elements that have been displayed by the display. In response to the one or more identified objects, the user interface element output may include a bar element displayed at a top of the display when the output of the display has a portrait orientation. In some embodiments, the processor is configured to identify a hazard and the output user interface element includes a warning message. In some embodiments, the processor is further configured to determine a location of the mobile device based on the one or more objects identified in the received image, and to control an output of the display based on the determined location of the mobile device.
In some embodiments, the display is rectangular in shape and the camera is arranged to capture one or more images from a direction parallel to a long axis of the display. The camera may be configured to sequentially capture a plurality of images at preset intervals, and the processor may be configured to receive each of the plurality of images captured by the camera.
In some embodiments, the camera may be configured to capture a plurality of images depending on whether a display of the mobile device is active. The mobile device may include a character recognition unit. The character recognition unit may be configured to receive a text object recognized in the received image from the processor; determining a text string from the received text object; and/or sending the determined text string to a processor. Additionally, the processor may be configured to control an output of the display based on the determined text string.
In some embodiments, the mobile device may include a language unit. The language unit may be configured to receive the text string determined by the character recognition unit from the processor, convert the text string into a translated text string in a second language, and/or send the translated text string to the processor. The processor may be configured to control an output of the display based on the translated text string.
In some embodiments, the mobile device may include an audio output unit. The processor may be configured to control an output of the audio output unit based on one or more identified objects in the received image.
Another example aspect of the present disclosure is directed to a method of operating a mobile device. The method can comprise the following steps: receiving an image captured by a camera of the mobile device, wherein the camera is arranged to capture one or more images from a direction parallel to a first plane of the mobile device as defined by a plane of a display of the mobile device; identifying one or more objects present in the received image; and/or controlling an output of a display of the mobile device based on one or more identified objects in the received image.
In some embodiments, receiving the image may include receiving a plurality of images sequentially captured by the camera at a preset interval. In some embodiments, receiving the plurality of images may include receiving the plurality of images captured by the camera according to whether a display of the mobile device is active. The method may include controlling the display to output a user interface element in response to the one or more identified objects. The user interface elements may be displayed over one or more user interface elements that have been displayed by the display. In some embodiments, the user interface element output in response to one or more identified objects may include a bar element displayed at a top of the display when the output of the display has a portrait orientation. Identifying one or more objects may include identifying a hazard, and the output user interface element may include a warning message.
In some embodiments, the method may include determining a position of the mobile device based on the one or more objects identified in the received image, and controlling an output of the display based on the determined position of the mobile device. The method may include identifying a text object in an image received from a processor; determining a text string from the recognized text object; and/or controlling an output of the display based on the determined text string.
In some embodiments, the method may include converting the determined text string to a translated text string in a second language and controlling an output of a display based on the translated text string. In some embodiments, the method may include controlling output of an audio output unit based on one or more identified objects in the received image.
Another example aspect of the disclosure is directed to a computer readable medium comprising a program which, when executed by a processor, performs a method of operating a mobile device. The method performed by the program may include: receiving an image captured by a camera of the mobile device, wherein the camera is arranged to capture one or more images from a direction parallel to a first plane of the mobile device as defined by a plane of a display of the mobile device; identifying one or more objects present in the received image; and/or controlling an output of a display of the mobile device based on one or more identified objects in the received image.
In some embodiments, receiving the image may include receiving a plurality of images sequentially captured by the camera at a preset interval. In some embodiments, receiving the plurality of images may include receiving the plurality of images captured by the camera according to whether a display of the mobile device is active. In some embodiments, a method performed by a program may include controlling a display to output a user interface element in response to one or more identified objects. The user interface elements may be displayed over one or more user interface elements that have been displayed by the display. In some embodiments, the user interface element output in response to one or more identified objects may include a bar element displayed at a top of the display when the output of the display has a portrait orientation.
In some embodiments, identifying the one or more objects may include identifying a hazard, and outputting the user interface element may include outputting a warning message. In some embodiments, a method performed by a program may include determining a location of a mobile device based on one or more objects identified in a received image, and controlling an output of a display based on the determined location of the mobile device. In some embodiments, a method performed by a program may include identifying a text object in an image received from a processor, determining a text string from the identified text object, and/or controlling an output of a display based on the determined text string.
In some embodiments, a method performed by a program may include converting the determined text string to a translated text string in a second language, and/or controlling an output of a display based on the translated text string. In some embodiments, a method performed by a program may include controlling output of an audio output unit based on one or more identified objects in a received image.
Systems, methods, apparatus, and non-transitory computer readable media in the disclosed technology can provide a variety of technical effects and facilitate identifying an environment based on sensor output from one or more sensors, generating one or more semantic objects based on the sensor output, and performing an entire process of one or more actions based on the one or more semantic objects. The disclosed techniques may reduce or eliminate the need for users to engage in manual interactions that gather information about their environment and objects in the environment. The reduction in manual interaction may result from an automated process that can continuously monitor the state of the environment, determine an optimal operating mode, and generate sensor data indicative in a more efficient manner (e.g., using fewer steps to produce an output). With manual selection still in use, the disclosed techniques may reduce the amount of human intervention by performing commonly used functions, including translation, image recognition, and association of semantic data with external data sources more quickly than without the assistance of the disclosed techniques (e.g., by eliminating one or more steps performed in different functions).
By changing the operating mode based on conditions in the environment, the disclosed technology may maximize the use of computing resources by selectively activating sensors and selectively performing various operations. For example, by determining the mode of operation to use and the particular action or actions to perform (e.g., text translation), the disclosed techniques may avoid excessive resource usage (e.g., battery power and/or network transmissions) that may result from a more haphazard approach that does not include generation and analysis of semantic objects associated with the environment. Additionally, the disclosed techniques may leverage the capabilities of machine learning models including locally stored machine learning models that may be accessed without requiring the use of network resources (e.g., network bandwidth to contact machine learning models stored on a remote computing device).
In this manner, the disclosed techniques can reduce or otherwise improve the efficiency of user interaction with the device. By changing the mode of operation and/or performing one or more actions based on the environment and one or more semantic objects associated with the environment, the disclosed techniques may direct the user to a desired information result or action in a shorter amount of time or with fewer interactive steps without intervention by the user. Thus, particularly in the field of mobile devices, the disclosed techniques may result in a reduction in power consumption requirements associated with light-on time and processor usage, which may be of particular importance in mobile devices. The disclosed techniques may reduce the need for processing time associated with processing user input queries, and processing responses to such queries. By increasing the number of instances that a user may be provided with a desired information result or action, the disclosed techniques may result in significant power consumption and processing resources over time without processing and responding to user input queries. By extension, the disclosed techniques may provide efficiency in network usage across a system of mobile devices implementing the disclosed techniques by reducing the number of instances that queries must be sent to remote computing devices.
The disclosed technology also provides the benefit of being able to be configured with various sensors (e.g., a panoramic camera) positioned in a manner that is more ergonomic for the user (e.g., more ergonomic for the user to hold) and captures a wider field of view of the environment around the user. Based on the normal or natural grip position of the device, sensors such as a panoramic camera may be located on the device in a manner that improves passive collection of sensor data from the environment, such that the sensors may continuously monitor the state of the environment without active gestures or actions by the user of the device. Additionally, the disclosed technology may use semantic objects based on data acquired from local sensors to enrich guidance in a routing application that may be displayed in one or more interface elements (e.g., status bar indicators, including a routing indicator indicating that a routing is being performed and/or an object identification indicator indicating that object identification is being performed). For example, the disclosed technology may use local landmarks or other objects within the field of view of a camera on the device as clues to enhance guidance.
Thus, the disclosed technology provides more efficient sensor-based semantic object generation in various environments, as well as increased benefits of lower resource usage (e.g., improved utilization of battery and network resources), from semantic object-driven methods of collecting and processing the state of the environment.
Reference will now be made in detail to the embodiments, one or more examples of which are illustrated in the drawings. Each example is provided by way of illustration of an embodiment and not limitation of the disclosure. In fact, it will be apparent to those skilled in the art that various modifications and variations can be made in the embodiments without departing from the scope or spirit of the disclosure. For instance, features illustrated or described as part of one embodiment, can be used with another embodiment to yield a still further embodiment. Accordingly, aspects of the present disclosure are intended to cover such modifications and alterations.
Referring now to fig. 1 through 17, example aspects of the disclosure will be disclosed in greater detail. Fig. 1 illustrates a diagram of an example system 100 according to an example embodiment of the present disclosure. The system 100 can include a user device 102, a remote computing device 104, a communication network 106, an object recognition component 110, object data 114 (e.g., data associated with one or more physical objects and/or one or more semantic objects), and a geographic information system 120.
The user device 102 may receive object data (e.g., information associated with one or more objects detected or identified by the user device 102) from the remote computing device 104 via the communication network 106. An object recognition component 110 operable or executing on the user device 102 may interact with the remote computing device 104 via the network 106 to perform one or more operations including detection and/or recognition of one or more objects; generation of one or more semantic objects; and/or generation of one or more outputs (e.g., physical outputs including visual, audio, and/or tactile indications). In some embodiments, the object recognition component 110 can include a machine learning model that can be used to detect and/or recognize objects and can also be used for the generation of one or more semantic objects. The network 106 may include any type of communication network, such as a local area network (e.g., an intranet), a wide area network (e.g., the internet), a cellular network, or some combination thereof. The network 106 may also include direct connections. In general, communications may be carried via network 106 using any type of wired and/or wireless connection, using a variety of communication protocols (e.g., TCP/IP, HTTP, SMTP, FTP), encodings or formats (e.g., HTML or XML), and/or protection schemes (e.g., VPN, secure HTTP, or SSL).
User device 102 may include one or more computing devices, including a tablet computing device, a wearable device (e.g., a smart watch or smart band), a laptop computing device, a desktop computing device, a mobile computing device (e.g., a smartphone), and/or a display device with one or more processors.
The object recognition component 110 can be implemented on the user device 102. The object recognition component 110 can enable object detection and/or recognition of one or more objects. Additionally, the object recognition component 110 can facilitate generation of one or more semantic objects based upon one or more sensory outputs from one or more sensors (not shown). The sensory output may be associated with one or more images or sounds associated with one or more objects in the environment. The object recognition component 110 may operate or execute locally on the user device 102 through a web application accessed via a web browser implemented on the user device 102, or through a combination of local execution or operation on the user device 102 and remote execution or operation on a remote computing device, which may include the remote computing device 104 or the geographic information system 120.
The object recognition component 110 may be configured to generate, process, or modify data that may be used by a user, including image data (e.g., image files), audio data (e.g., sound files), and/or navigation data (e.g., locations of points of interest associated with the image data).
In some embodiments, the remote computing device 104 may include one or more computing devices including a server (e.g., a web server). The one or more computing devices may include one or more processors and one or more memory devices. The one or more memory devices may store computer readable instructions to, for example, implement one or more applications associated with object data 114. In some embodiments, the object data 114 may be associated with a geographic information system 120, for example.
The geographic information system 120 may be associated with or include data indexed according to geographic coordinates (e.g., latitude and longitude) of constituent elements (e.g., location) of the data. The data associated with the geographic information system 120 may include map data, image data, geo imagery, and/or data associated with various waypoints (e.g., addresses or geographic coordinates). The object data 114 as determined or generated by the remote computing device 104 may include data associated with one or more objects and/or states or characteristics of one or more semantic objects including, for example, an object identifier (e.g., a location name and/or name of the object), a price of the object, a location of the object, and/or ownership of the object.
FIG. 2 illustrates an example computing device 200 that may be configured to generate semantic objects and outputs based on detection or recognition of a state of an environment that includes the objects, according to an example embodiment of the present disclosure. Computing device 200 may include one or more systems (e.g., one or more computing systems) or one or more portions of a device (e.g., one or more computing devices) including user device 102 and/or remote computing device 104 shown in fig. 1. As shown, computing device 200 includes memory 204; an object recognition component 212 that can include one or more instructions that can be stored on the memory 204; one or more processors 220 configured to execute one or more instructions stored in memory 204; a network interface 222 that may support network communications; one or more large scale storage devices 224 (e.g., hard disk drives or solid state drives); one or more output devices 226 (e.g., one or more display devices); an array sensor 228 (e.g., one or more optical and/or audio sensors); one or more input devices 230 (e.g., one or more touch-detecting surfaces); and/or one or more interconnects 232 (e.g., a bus for communicating one or more signals or data between computing components in a computing device). The one or more processors 220 may include any processing device that, for example, may process and/or exchange (send or receive) one or more signals or data associated with a computing device.
For example, the one or more processors 220 may include single or multi-core devices, including microprocessors, microcontrollers, integrated circuits, and/or logic devices. Memory 204 and storage memory 224 are illustrated separately, however, components 204 and 224 may be regions within the same memory module. Computing device 200 may include one or more additional processors, memory devices, network interfaces that may be provided separately or on the same chip or board. Components 204 and 224 may include one or more computer-readable media, including but not limited to non-transitory computer-readable media, RAM, ROM, hard drives, flash drives, and/or other storage devices.
The memory 204 may store a set of instructions for an application, including an operating system associated with various software applications or data. The memory 204 may be used to operate a variety of applications including mobile operating systems developed specifically for mobile devices. Thus, memory 204 may perform functions that allow a software application to access data including wireless network parameters (e.g., identification of a wireless network, quality of service), and invoke various services including telephony, location determination (e.g., via Global Positioning Service (GPS) or WLAN), and/or wireless network data call origination services. In other implementations, the memory 204 may be used for a general purpose operating system that may be used to operate or run on both mobile and stationary devices, such as, for example, smart phones and desktop computers. In some embodiments, the object recognition component 212 can include a machine learning model that can be utilized to detect and/or recognize objects. Additionally, the object recognition component can be employed in the generation of one or more semantic objects.
The array sensor 228 may include one or more sensors that may detect a change in a state of an environment including one or more objects. For example, the array sensor 228 may include one or more light sensors, motion sensors, thermal sensors, audio sensors, tactile sensors, pressure sensors, humidity sensors, and/or electromagnetic sensors. The one or more input devices 230 may include one or more devices for entering input into the computing device 200, including one or more touch-sensitive surfaces (e.g., resistive and/or capacitive touch screens), keyboards, mouse devices, microphones, and/or pen devices. The one or more output devices 226 may include one or more devices that may provide physical output including visual output, audio output, and/or tactile output. For example, the one or more output devices 226 may include one or more display components (e.g., an LCD monitor, an OLED monitor, and/or an indicator light), one or more audio components (e.g., a speaker), and/or one or more tactile output devices that may produce motion including vibration.
The software applications that may be operated or run by the computing device 200 may include the object recognition component 110 shown in fig. 1. Additionally, software applications that may be operated or run by the computing device 200 may include native applications or web-based applications.
In some implementations, the user device may be associated with or include a positioning system (not shown). The positioning system may include one or more devices or circuitry for determining the location of the device. For example, the positioning device may determine the actual or relative position by using a satellite navigation positioning system (e.g., GPS system, galileo positioning system, global navigation satellite system (GLONASS), beidou satellite navigation and positioning system), inertial navigation system, dead reckoning system, based on IP addresses, by using triangulation and/or proximity to cellular towers or Wi-Fi hotspots, beacons, etc., and/or other suitable techniques for determining position. The positioning system may determine a user location of the user device. The user location may be provided to the remote computing device 104 for use by the object data provider to determine travel data associated with the user device 102.
The one or more interconnects 232 may include one or more interconnects or buses that may be used to exchange (e.g., send and/or receive) one or more signals (e.g., electronic signals) and/or data between components of the computing device 200, including the memory 204, the object recognition component 212, the one or more processors 220, the network interface 222, the one or more mass storage devices 224, the one or more output devices 226, the array sensor 228, and/or the one or more input devices 230. The one or more interconnects 232 may be arranged or configured in different ways, including, for example, in parallel or in series. Additionally, the one or more interconnects 232 may include one or more internal buses that connect the internal components of the computing device 200; and one or more external buses for connecting internal components of the computing device 200 to one or more external devices. For example, the one or more interconnects 232 may include different interfaces including Industry Standard Architecture (ISA), extended ISA, Peripheral Component Interconnect (PCI), PCI express, serial AT attachment (SATA), Hypertransport (HT), USB (universal serial bus), thunderbolt, and/or IEEE 1394 interface (firewire).
Fig. 3 illustrates an example of sensor-based semantic object generation including image capture according to an example embodiment of the present disclosure. Fig. 3 includes an illustration of an environment 300, one or more portions of which environment 300 may be detected, identified, and/or processed by one or more systems (e.g., one or more computing systems) or devices (e.g., one or more computing devices), including the user device 102 shown in fig. 1, the remote computing device 104 shown in fig. 1, and/or the computing device 200 shown in fig. 2. Additionally, the detection, identification, and/or processing of one or more portions of environment 300 may be implemented as algorithms on hardware components of one or more devices or systems (e.g., user device 102, remote computing device 104, and/or computing device 200) to generate one or more semantic objects and outputs, e.g., based on one or more objects. As shown in FIG. 3, environment 300 includes a semantic processing system 310, a display component 312, an edge portion 314, an object 320, and a text portion 322.
The display component 312 of the semantic processing system 310 may display one or more images of the environment including the environment 300. One or more images displayed by the display component 312 can be captured by one or more sensors (e.g., one or more cameras) of the semantic processing system 310. In this example, the display component 312 uses a camera (e.g., a panoramic camera) located on an edge portion 314 of the semantic processing system 310 that captures an image of the object 320, the object 320 being a poster with text in a combination of languages (english and chinese). In some embodiments, one or more sensors may be located anywhere on the semantic processing system 310. Additionally, the semantic processing system 310 may receive sensory output from one or more external devices (e.g., a remote camera may provide video imagery to the semantic processing system 310).
The semantic processing system 310 may output one or more images of the object 320 including the text portion 322 on the display component 312. As shown in fig. 3, the disclosed technology may output an image of an environment onto a display component of a device that may receive one or more inputs from a user.
Fig. 4 illustrates an example of sensor-based semantic object generation including audio generation according to an example embodiment of the present disclosure. Fig. 4 includes an illustration of an environment 400, one or more portions of which 400 may be detected, identified, and/or processed by one or more systems (e.g., one or more computing systems) or devices (e.g., one or more computing devices) including a semantic processing system audio component 410 that may include one or more portions of the user device 102 shown in fig. 1, the remote computing device 104 shown in fig. 1, and/or the computing device 200 shown in fig. 2. Additionally, the detection, identification, and/or processing of one or more portions of environment 400 may be implemented as algorithms on hardware components of one or more devices or systems (e.g., user device 102, remote computing device 104, and/or computing device 200) to generate one or more semantic objects and outputs, e.g., based on one or more objects. As shown in FIG. 4, the environment 400 includes a semantic processing system audio output component 410.
The semantic processing system audio output component 410 may include one or more components that may output sound, including sound output via one or more speakers of the semantic processing system audio output component 410. For example, the semantic processing system audio output component 410 may receive one or more signals (e.g., one or more signals comprising data) from a system or device, such as the user device 102 or the computing device 200. One or more signals may be transmitted wirelessly or via a wire and received by a receiving component (not shown) of the semantic processing system audio output component 410. The one or more signals may include data associated with one or more indications of a state of an environment including the one or more objects. For example, the one or more signals may include audio based on a portion of the recognized text (e.g., text-to-speech translation) or audio of a direction to a location (e.g., audio instructions of a direction to a destination location).
Fig. 5 illustrates an example of sensor-based semantic object generation including text translation, according to an example embodiment of the present disclosure. Fig. 5 includes an illustration of a semantic processing system 500, which semantic processing system 500 may include one or more systems (e.g., one or more computing systems) or devices (e.g., one or more computing devices) including the user device 102 shown in fig. 1, the remote computing device 104 shown in fig. 1, and/or the computing device 200 shown in fig. 2. Additionally, the detection, identification, and/or processing by one or more portions of the environment of the semantic processing system 500 may be implemented as algorithms on hardware components of one or more devices or systems (e.g., the user device 102, the remote computing device 104, and/or the computing device 200) to generate one or more semantic objects and outputs, e.g., based on one or more objects. As shown in FIG. 5, the semantic processing system 500 includes a display component 510 and a text portion 512.
The semantic processing system 500 can display one or more images of the environment including one or more objects on the display component 510. The one or more images may be captured by one or more sensors (not shown) of the semantic processing system 500. In this example, display component 510 outputs a display of a poster with text in a combination of languages (English and Chinese). The semantic processing system 500 can generate semantic objects corresponding to text detected in the environment, translate the text, and output a text portion 512 shown on the display component 510. For example, the semantic processing system 500 may overlay translated English text ("Qingdao Daily") on top of Chinese text captured by the semantic processing system 500.
Fig. 6 illustrates an example of sensor-based semantic object generation including text recognition, according to an example embodiment of the present disclosure. Fig. 6 includes an illustration of an environment 600, one or more portions of which 600 may be detected, identified, and/or processed by one or more systems (e.g., one or more computing systems) or devices (e.g., one or more computing devices) including a semantic processing system 610 that may include one or more portions of the user device 102 shown in fig. 1, the remote computing device 104 shown in fig. 1, and/or the computing device 200 shown in fig. 2. Additionally, the detection, identification, and/or processing of one or more portions of environment 600 may be implemented as algorithms on hardware components of one or more devices or systems (e.g., user device 102, remote computing device 104, and/or computing device 200) to generate one or more semantic objects and outputs, e.g., based on one or more objects.
As shown in FIG. 6, environment 600 includes a semantic processing system 610, an object 620, and a text portion 622.
The semantic processing system 610 may capture one or more images via one or more sensors (e.g., one or more cameras). The semantic processing system 610 may include one or more perimetry cameras (not shown) located on the semantic processing system 610 such that a wide field of view of the one or more perimetry cameras may capture a state of the environment 600 that includes an object 620 (e.g., a poster), the object 620 including a text portion 622 ("Juanita de Flor"). The positioning of the one or more panoramic cameras allows a user of the semantic processing system 610 to capture one or more images of one or more objects in the environment while gripping the semantic processing system 610 in an ergonomically comfortable position.
Fig. 7 illustrates an example of sensor-based semantic object generation including text recognition, according to an example embodiment of the present disclosure. Fig. 7 includes an illustration of a semantic processing system 700, which semantic processing system 700 may include one or more portions of one or more systems (e.g., one or more computing systems) or devices (e.g., one or more computing devices) including the user device 102 shown in fig. 1, the remote computing device 104 shown in fig. 1, and/or the computing device 200 shown in fig. 2. Additionally, the detection, identification, and/or processing by one or more portions of the environment of the semantic processing system 700 may be implemented as algorithms on hardware components of one or more devices or systems (e.g., the user device 102, the remote computing device 104, and/or the computing device 200) to generate one or more semantic objects and outputs, e.g., based on one or more objects. As shown in FIG. 7, semantic processing system 700 includes a display component 710, an image object 712, and an interface element 714.
The semantic processing system 700 may display one or more images of the environment including one or more objects on a display component 710. One or more images displayed on the display component 710 can be captured by one or more sensors (not shown) of the semantic processing system 700. In this example, the display component 710 outputs an image object 712 that includes a visual representation of a portion of a poster with text ("Juanita de Flor"). Semantic processing system 700 may identify that an object (e.g., a poster) associated with image object 712 includes text, and may generate a semantic object based on image object 712 (e.g., a semantic object based on the identified object). Based on the semantic object, the semantic processing system 700 may determine that the image object 712 is associated with the musician "Juanita de Flor," and may access a remote computing device (e.g., the remote computing device 104) that includes data (e.g., music audio files) associated with the generated semantic object. Based on the identification of the semantic object (e.g., the name of the musician), the semantic processing system 700 may generate one or more interface elements on the display component 710, including interface element 714, that allow the user to access or control information related to the semantic object. For example, interface element 714 may be used to copy music audio files associated with semantic objects generated by semantic processing system 700.
Fig. 8 illustrates an example of sensor-based semantic object generation including object recognition, according to an example embodiment of the present disclosure. Fig. 8 includes an illustration of an environment 800, one or more portions of which environment 800 may be detected, identified, and/or processed by one or more systems (e.g., one or more computing systems) or devices (e.g., one or more computing devices) including a semantic processing system that may include one or more portions of the user device 102 shown in fig. 1, the remote computing device 104 shown in fig. 1, and/or the computing device 200 shown in fig. 2. Additionally, the detection, identification, and/or processing of one or more portions of environment 800 may be implemented as algorithms on hardware components of one or more devices or systems (e.g., user device 102, remote computing device 104, and/or computing device 200) to generate one or more semantic objects and outputs, e.g., based on one or more objects. As shown in FIG. 8, environment 800 includes a semantic processing system 810, a display component 812, an object 820, and an object tag 822.
A display component 812 of the semantic processing system 810 can display one or more images based on the environment 800. The one or more images displayed by the display component 812 can be captured by one or more sensors (not shown) of the semantic processing system 810. Semantic processing system 810 can capture an image of object 820 that is a handbag. The semantic processing system 810 can generate the semantic object based on the recognition object 820 of the semantic processing system 810 being a bag. Semantic processing system 810 may detect object tag 822 and, based on detecting object tag 822, may generate one or more attributes of the semantic object associated with object 820, including, for example, object brand attributes that may be based on a brand assignment value for object 820 determined by semantic processing system 810. For example, to determine the value of an object brand attribute, the semantic processing system 810 may access a remote computing system that may include data associated with the object brand attribute, and may use the data to associate a value (e.g., a trademark of a handbag manufacturer) with the object brand attribute.
Fig. 9 illustrates an example of sensor-based semantic object generation including object recognition, according to an example embodiment of the present disclosure. Fig. 9 includes an illustration of a semantic processing system 900, which semantic processing system 900 may include one or more systems (e.g., one or more computing systems) or one or more portions of a device (e.g., one or more computing devices) including the user device 102 shown in fig. 1, the remote computing device 104 shown in fig. 1, and/or the computing device 200 shown in fig. 2. Additionally, the detection, identification, and/or processing of one or more portions of the environment by the semantic processing system 900 may be implemented as algorithms on hardware components of one or more devices or systems (e.g., the user device 102, the remote computing device 104, and/or the computing device 200) to generate one or more semantic objects and outputs, e.g., based on one or more objects. As shown in FIG. 9, semantic processing system 900 includes a display component 910, an image object 920, an image object portion 922, an object identifier 924, and an interface element 926.
The semantic processing system 900 can display one or more images of an environment (e.g., an environment including one or more objects) on a display component 910. The one or more images may be captured by one or more sensors (e.g., one or more cameras) of the semantic processing system 900, which may be located on one or more portions of the semantic processing system 900. In this example, display component 910 outputs a display of object 920. The semantic processing system 900 may recognize that the object 920 is a handbag that includes an object tag 922. The semantic processing system 900 may generate semantic object attributes based on the object tags 922. Based on the attributes of the semantic object (e.g., the object is a handbag with tags from a particular manufacturer), semantic processing system 900 may generate a display output that includes object identifier 924 ("bag") and an interface element that includes interface element 926. Interface element 926 may be a control element that, when activated by a user (e.g., touching interface element 926 and/or issuing a voice command with respect to interface element 926), may perform one or more actions including accessing an internet website that sells goods or services including object 920 and/or providing more information about object 920.
Fig. 10 illustrates an example of sensor-based semantic object generation including location identification, according to an example embodiment of the present disclosure. Fig. 10 includes an illustration of a semantic processing system 1000, which semantic processing system 1000 may include one or more systems (e.g., one or more computing systems) or one or more portions of a device (e.g., one or more computing devices) including the user device 102 shown in fig. 1, the remote computing device 104 shown in fig. 1, and/or the computing device 200 shown in fig. 2. Additionally, the detection, identification, and/or processing of one or more portions of the environment by the semantic processing system 1000 may be implemented as algorithms on hardware components of one or more devices or systems (e.g., the user device 102, the remote computing device 104, and/or the computing device 200) to generate one or more semantic objects and outputs, e.g., based on one or more objects. As shown in FIG. 10, semantic processing system 1000 includes display component 1010, object 1020, object 1022, object 1024, and object 1026.
In this example, the display component 1010 of the semantic processing system 1000 displays an environment including one or more objects (e.g., people, buildings, streets, and vehicles) captured by a camera (not shown) of the semantic processing system 1000. The display component 1010 shows objects that have been detected and/or identified by the semantic processing system 1000, including objects 1020 that are determined to be street addresses; objects 1022 determined to be markers associated with the service (transport service); an object 1024 determined to be a face; and an object 1026 determined to be a tag associated with the service (restaurant).
Semantic processing system 1000 may generate semantic objects based on objects 1020, 1022, 1024, and/or 1026. For example, semantic objects based on object 1020 may be used to determine location (e.g., location may be determined based on street address when GPS service is not available); semantic objects based on object 1022 may be used to determine whether a transit vehicle with a user's package is nearby; and/or semantic objects based on object 1026 may be used to identify restaurants associated with object 1026 and to provide information (e.g., ratings of food and services) to a user of semantic processing system 1000.
Additionally, semantic objects based on objects 1024 may be used to determine whether a person (e.g., a friend of the user of semantic processing system 1000) who has explicitly given permission to the user of semantic processing system 1000 to identify their face is nearby. In some embodiments, to protect the privacy of an individual whose image is captured by the semantic processing system 1000, personal identification data (e.g., facial recognition data) may be stored locally on the semantic processing system 1000 in a secure portion (e.g., an encrypted storage area) of the semantic processing system 1000 that is not shared with or accessible to any other device.
Display component 1010 may be configured to receive one or more inputs to interact with interface elements displayed on display component 1010. For example, based on a user being able to touch a portion of the display component 1010 displaying the identified object, the semantic processing system 1000 can access information associated with the semantic object associated with the identified object.
Fig. 11 illustrates an example of sensor-based semantic object generation including location identification, according to an example embodiment of the present disclosure. Fig. 11 includes an illustration of a semantic processing system 1100, which semantic processing system 1100 can include one or more portions of one or more systems (e.g., one or more computing systems) or devices (e.g., one or more computing devices) including the user device 102 shown in fig. 1, the remote computing device 104 shown in fig. 1, and/or the computing device 200 shown in fig. 2. Additionally, the detection, identification, and/or processing of one or more portions of the environment by the semantic processing system 1100 can be implemented as algorithms on hardware components of one or more devices or systems (e.g., the user device 102, the remote computing device 104, and/or the computing device 200) to generate one or more semantic objects and outputs, e.g., based on one or more objects. As shown in FIG. 11, the semantic processing system 1000 includes a display component 1110 and an object 1120.
In this example, the display component 1110 of the semantic processing system 1100 displays an environment captured by a camera (not shown) of the semantic processing system 1100. The display component 1110 displays objects that have been detected and/or identified by the semantic processing system 1100, including objects 1120 that are determined to be entries for locations that the user travels to. The semantic processing system may generate semantic objects based on the objects 1120 that may be used to provide navigation instructions to a user of the semantic processing system 1100. In some circumstances, portals to different locations may be very close to each other, and the geographic location signal (e.g., GPS) may not be available or may be too inaccurate to distinguish between a correct portal and an incorrect portal. Thus, the semantic processing system 1100 can identify the correct portal by generating semantic objects based on visual input from the location and providing guidance to a user of the semantic processing system 1100 based on the generated semantic objects.
Fig. 12 illustrates an example of sensor-based semantic object generation including navigation according to an example embodiment of the present disclosure. Fig. 12 includes an illustration of a semantic processing system 1200, which semantic processing system 1200 may include one or more portions of one or more systems (e.g., one or more computing systems) or devices (e.g., one or more computing devices) including the user device 102 shown in fig. 1, the remote computing device 104 shown in fig. 1, and/or the computing device 200 shown in fig. 2. Additionally, the detection, identification, and/or processing of one or more portions of the environment by the semantic processing system 1200 may be implemented as algorithms on hardware components of one or more devices or systems (e.g., the user device 102, the remote computing device 104, and/or the computing device 200) to generate one or more semantic objects and outputs, e.g., based on one or more objects. As shown in fig. 12, the semantic processing system 1200 includes a display component 1210, a navigation indicator 1212, a status indicator 1214, a destination indicator 1216, a status region 1220, and a status indicator 1222.
In this example, the semantic processing system 1200 includes a display component 1210 that displays one or more images and/or text. At the top of the display component 1210, a status area 1220 may include various indicators, including a status indicator 1222 that indicates that the semantic processing system 1200 is in a navigation mode. The semantic processing system 1200 can generate other indicators in various sizes, shapes, and/or colors, including a status indicator 1214 over a destination indicator 1216, the destination indicator 1216 indicating a destination to which a user of the semantic processing system 1200 is traveling. Display component 1210 may also generate a graphical indicator (arrow) comprising the textual instruction "120 feet west" and a direction to the destination location. The semantic processing system 1200 may also include generating a navigation indicator 1212, the navigation indicator 1212 including an identifier associated with the destination location "Joshua tree country park," which may, in some embodiments, receive one or more inputs from a user to provide more information associated with the destination location. In some embodiments, the status indicator 1214 may change color, shape, and/or size upon reaching the destination location.
Fig. 13 illustrates an example of sensor-based semantic object generation including location identification, according to an example embodiment of the present disclosure. Fig. 13 includes an illustration of a semantic processing system 1300, which semantic processing system 1300 can include one or more systems (e.g., one or more computing systems) or one or more portions of a device (e.g., one or more computing devices) including the user device 102 shown in fig. 1, the remote computing device 104 shown in fig. 1, and/or the computing device 200 shown in fig. 2. Additionally, the detection, identification, and/or processing of one or more portions of the environment by the semantic processing system 1300 can be implemented as algorithms on hardware components of one or more devices or systems (e.g., the user device 102, the remote computing device 104, and/or the computing device 200) to generate one or more semantic objects and outputs, e.g., based on one or more objects. As shown in fig. 13, semantic processing system 1300 includes display component 1310, status region 1320, status indicator 1322, interface element 1324, and interface element 1326.
In this example, the semantic processing system 1300 includes a display component 1310, the display component 1310 including a status region 1320 (e.g., a status bar), the status region 1320 may generate an indicator of a status of a device or semantic object that has been generated by the semantic processing system 1300 in response to identification of one or more statuses of one or more objects in the environment. The status region 1320 may include a status indicator 1322, which status indicator 1322 may indicate that the semantic processing system 1300 has performed the identification of the context and has provided information that results with the context. In this example, the semantic processing system 1300 provides an interface element 1324 that includes an indication of the location of the environment ("concert hall"), and also provides an interface element 1326 that provides the user with different ways to interact with semantic objects associated with the environment. For example, a user of the semantic processing system 1300 may touch the interface element 1326 to access information about the object (e.g., a rating of a concert hall).
Fig. 14 illustrates a flowchart of an example method of sensor-based semantic object generation according to an example embodiment of the present disclosure. One or more portions of method 1400 may be run or implemented on one or more computing devices or computing systems, including user device 102, remote computing device 104, and/or computing device 200, for example. One or more portions of the method 1400 may also run or be implemented as algorithms on hardware components of the apparatus disclosed herein. Fig. 14 illustrates the steps performed in a particular order for purposes of example and discussion. Using the disclosure provided herein, one of ordinary skill in the art will appreciate that any of the steps of the methods disclosed herein may be adapted, modified, rearranged, omitted, and/or expanded without departing from the scope of the present disclosure.
At 1402, method 1400 may include receiving data, for example, status data including sensor output based in part on one or more sensors that detect a status of an environment including one or more objects including a physical object (e.g., an entrance to a building, a street address, a sign, and/or an electronic device).
The state data may include information associated with a state of the environment including one or more objects in the environment. The state of the environment including one or more objects may include: a temporal status (e.g., a time of day when sensor outputs associated with the state of the environment are output by one or more sensors) that may also include one or more durations of events associated with the environment (e.g., a duration of a scheduled event); a location state associated with a location of one or more objects in the environment (e.g., latitude and longitude and/or a relative location of one or more objects to another one or more objects to each other or to a reference location point); and/or a physical state comprising one or more physical characteristics (e.g., appearance including color and/or texture; physical dimensions including size, volume, mass, and/or weight; and/or audio characteristics).
In some embodiments, the one or more sensors may include one or more light sensors (e.g., one or more cameras); one or more panoramic cameras comprising one or more cameras having a field of view in excess of one hundred eighty degrees; one or more audio sensors (e.g., one or more microphones); one or more tactile sensors (e.g., a surface that can detect pressure or capacitance); one or more pressure sensors including a barometric pressure sensor; one or more gyro sensors; one or more accelerometers including configurations in which the one or more accelerometers can determine acceleration along any of three axes (e.g., the X-axis, Y-axis, and Z-axis); one or more humidity sensors including one or more sensors that can detect a level of moisture in the air; one or more electromagnetic sensors; and/or one or more thermal sensors.
In some embodiments, the semantic processing system may include a display component (e.g., a Liquid Crystal Display (LCD), an Organic Light Emitting Diode (OLED), a plasma display panel, an electronic ink, and/or a cathode ray tube) configured to display one or more images, which may include images of an environment containing one or more objects detected by one or more sensors. Additionally, in some embodiments, the display component may include one or more sensors (e.g., a touch screen) such that the display component may be used as an input device.
Additionally, the one or more panoramic cameras may be configured or positioned to capture one or more images including one or more objects or portions of one or more objects that are not within the visual plane of the display assembly. For example, one or more panoramic cameras may be located on any portion of the semantic computing system, including the side facing the user grasping the semantic computing system (e.g., on the same side as the display component), the side facing away from the user grasping the semantic computing system (e.g., the side opposite the display component), and/or any edge of the device.
The display component of the semantic computing system may include a visual plane, which may include a plane that, if an optical sensor, would capture one or more images within less than one hundred eighty degrees of a portion of the optical sensor (e.g., images perpendicular to or behind the visual plane would not be captured). For example, if the semantic processing device is in the shape of a rectangular cuboid, one or more panoramic cameras may be located on either side of the cuboid.
At 1404, method 1400 can include generating one or more semantic objects corresponding to the one or more objects. The one or more semantic objects may be generated, for example, based in part on data comprising state data and/or an object recognition model comprising a machine learning model.
The semantic processing system may analyze the state data and perform one or more operations on the state data, including comparing the state data to information associated with one or more portions of the state data. For example, the appearance of one or more objects may be compared to a database of objects that may be used to identify the one or more objects. Based on the identification of the one or more objects, the semantic processing system may generate additional information including attributes of the one or more objects. In another example, the status data may include a location and a time, which may be used to determine whether one of the events in the database will occur within a given location for a period of time that a user of the device will be present at the location based on a comparison to the database of events.
In some embodiments, the semantic processing system may access a machine learning model that has been created using a classification dataset (e.g., access a machine learning model that has been locally stored and/or stored on a remote computing device) that includes classifier data that includes a set of classified object labels and a set of classified features associated with training data that may be based on or associated with a plurality of training objects (e.g., physical objects or simulated objects that are used as training inputs for the machine learning model). The classification dataset may be based in part on input from one or more sensors (e.g., cameras and/or microphones) that have been used to generate visual and audio outputs based on visual and audio inputs, respectively. For example, a machine learning model may be created using a set of cameras and microphones that capture training data including video and audio of an urban area that includes various objects including bodies of water, waterways, buildings (e.g., houses and/or hotels), streets, alleys, vehicles (e.g., cars and/or trams), people, and/or surfaces with text (e.g., movie posters).
The one or more semantic objects may include a set of attributes (e.g., a set of attributes for each of the one or more semantic objects). For example, the set of attributes associated with one or more semantic objects may include one or more object identifications, including an identification of one or more objects associated with the one or more semantic objects (e.g., a make and model of an automobile); one or more object types associated with a type, category, or category of one or more objects associated with one or more semantic objects (e.g., a car may be associated with a vehicle type); an object location (e.g., an address of a building object) including a geographic location associated with one or more objects associated with one or more semantic objects; monetary value (e.g., one or more prices associated with the object); ownership status including the owner of the object (e.g., owner of the house); and/or a set of physical characteristics (e.g., a size, appearance, or quality associated with the object).
At 1406, the method 1400 may include determining one or more operating modes associated with the one or more semantic objects based in part on the set of attributes of the one or more semantic objects. The one or more operating modes may determine the manner in which one or more semantic objects are processed and/or used by the semantic processing system. Thus, the semantic processing system may selectively dedicate computing resources to a subset of possible operations based on one or more attributes of one or more semantic objects (e.g., detecting a tag that includes text may result in determining that a text recognition pattern is to be used to process one or more semantic objects associated with the tag).
The one or more modes of operation may include: a text recognition mode associated with recognizing text information in the environment (e.g., recognizing when an object includes text or a pictogram); a location identification pattern associated with identifying one or more locations in the environment (e.g., locating a portal to a restaurant); an object recognition pattern associated with recognizing one or more objects in the environment (e.g., recognizing a car in a parking lot); and/or an event recognition pattern associated with recognizing the occurrence of one or more events in the environment (e.g., associating a time and a location with a planned event).
At 1408, the method 1400 may include determining one or more relevance values corresponding to the one or more semantic objects. The one or more relevance values may be based in part on a degree to which each of the one or more semantic objects is associated with the context data. The contextual data may include various characteristics associated with the environment, including data associated with a time of day, a current location (e.g., a geographic location and/or address associated with the environment), one or more scheduled events (e.g., one or more events that will occur within a predetermined time period), one or more user locations, or one or more user preferences (e.g., one or more preferences of a user including restaurant preferences, literature preferences, and/or beverage preferences). In some embodiments, the one or more object outputs may be based in part on one or more relevance values corresponding to the one or more semantic objects.
At 1410, method 1400 may include generating one or more object outputs associated with the one or more semantic objects based in part on the one or more operating modes. The one or more object outputs may include one or more outputs via one or more output devices of the semantic processing system (e.g., one or more display devices, audio devices, and/or tactile output devices). The text recognition mode may produce one or more object outputs that include an output related to the text that includes a translation of the recognized text (e.g., generating russian text based on the detection and translation of english text).
In some embodiments, the one or more object outputs may include one or more visual indications (e.g., one or more visual images produced by a display device of the semantic processing system) and/or one or more audio indications (e.g., one or more sounds produced by an audio output device of the semantic processing system). For example, the one or more object outputs may include translations displayed on a display device, audio indications including an audio version of written text (e.g., text-to-speech), and/or one or more images superimposed on a camera imagery of the environment.
At 1412, the method 1400 may include modifying one or more visual indications or one or more audio indications based in part on the state data or the semantic data. Modifying the one or more visual indications or the one or more audio indications may include: converting the one or more visual indications into one or more modified audio indications (e.g., generating an artificial language based on the detected text); converting the one or more audio indications into one or more modified visual indications (e.g., generating text based on audio input to a microphone); modifying a size of the one or more visual indications (e.g., increasing a size of an object captured by the camera); modifying one or more color characteristics of the one or more visual indicators (e.g., illuminating the one or more visual indicators); and/or modify the amplitude of (e.g., increase the volume of) one or more audio indications. Such modification of the one or more visual indications and/or the one or more audio indications may be used to enhance the experience of any user, and may be particularly useful for individuals with visual or auditory impairments. For example, the semantic processing system may increase the volume of sound that would otherwise be inaudible to individuals with hearing impairment.
Fig. 15 illustrates a flowchart of an example method of sensor-based semantic object generation according to an example embodiment of the present disclosure. One or more portions of method 1500 may be run or implemented on one or more computing devices or computing systems, including user device 102, remote computing device 104, and/or computing device 200, for example. One or more portions of the method 1500 may also run or be implemented as algorithms on hardware components of the apparatus disclosed herein. Fig. 15 illustrates the steps performed in a particular order for purposes of example and discussion. Using the disclosure provided herein, one of ordinary skill in the art will appreciate that any of the steps of the methods disclosed herein may be adapted, modified, rearranged, omitted, and/or expanded without departing from the scope of the present disclosure.
At 1502, method 1500 can include determining object data that matches one or more semantic objects (e.g., one or more semantic objects in method 1400) based in part on a set of attributes (e.g., a set of attributes in method 1400) of the one or more semantic objects (e.g., one or more semantic objects in method 1400). For example, the semantic processing system may match the set of attributes to the object data based on one or more comparisons between portions of the set of attributes and the object data. The object data may include information associated with: one or more related objects (e.g., semantic objects for hats may be associated with other clothing); one or more remote data sources (e.g., semantic objects for a song may be associated with a website associated with the artist of the song); one or more locations; and/or one or more events.
At 1504, method 1500 may include accessing one or more portions of object data that match one or more semantic objects. For example, the semantic processing system may access one or more portions of object data stored on one or more remote computing devices. In some embodiments, the one or more object outputs may be based in part on one or more portions of the object data that match the one or more semantic objects. For example, when the object data includes links to one or more remote computing devices associated with one or more semantic objects, the one or more object outputs may include those links.
At 1506, the method 1500 may include generating one or more interface elements associated with the one or more objects based in part on the state data or the one or more semantic objects. The one or more interface elements may include one or more images (e.g., graphical user interface elements including still or animated images, pictograms, and/or text) responsive to one or more inputs (e.g., the one or more interface elements may initiate or trigger one or more operations based on tactile and/or audio inputs). For example, one or more interface elements may include a status indicator (e.g., a status bar displayed on a display component of the semantic processing system) that may provide one or more incremental (e.g., every minute, hour, and/or day) and/or continuous (e.g., real-time) indications associated with a status of one or more objects (e.g., a location of a restaurant and/or a closing time).
In some embodiments, the identification of the one or more objects may be performed as a continuous process (e.g., continuous identification of the one or more objects) such that the one or more objects may be detected, identified, and/or identified in real-time (e.g., sensor outputs including visual and/or audio sensor outputs associated with the one or more objects), and the one or more interface elements including the status indicator may also be continuously updated (e.g., when the one or more objects are identified in real-time). In addition, one or more interface elements may be used to provide navigation instructions (e.g., text or audio instructions associated with a path to a location) and other information related to one or more objects in the environment.
At 1508, the method 1500 may include determining whether one or more inputs are received by the semantic processing system, when one or more inputs are received by the semantic processing system, or one or more inputs are received by the semantic processing system. The one or more inputs may include one or more inputs from a user of the semantic processing system, including: one or more visual inputs (e.g., waving or blinking a hand in front of a camera of the semantic processing system); one or more audio inputs (e.g., speaking a command to a microphone of a semantic processing system); and/or one or more tactile inputs (e.g., touching a portion of a display component of a semantic processing system). Additionally, the one or more inputs may include one or more inputs to a device associated with the semantic processing system, the device including a computing device and/or an input device (e.g., a pen and/or a mouse).
In response to receiving the one or more inputs, method 1500 proceeds to 1510. In response to not receiving the one or more inputs, the method may end or return to a previous portion of method 1500, including 1502, 1504, or 1506.
At 1510, method 1500 may include determining one or more remote computing devices that include at least a portion of the object data (e.g., one or more remote computing devices that store some portion of the object data) in response to receiving one or more inputs to the one or more interface elements. The one or more object outputs may include one or more remote source indications associated with one or more remote computing devices (e.g., IP addresses associated with the one or more remote computing devices) that include at least a portion of the object data.
Fig. 16 illustrates a flowchart of an example method of sensor-based semantic object generation according to an example embodiment of the present disclosure. One or more portions of method 1600 may be run or implemented on one or more computing devices or computing systems, including user device 102, remote computing device 104, and/or computing device 200, for example. One or more portions of method 1600 may also run or be implemented as algorithms on hardware components of the apparatus disclosed herein. Fig. 16 illustrates the steps performed in a particular order for purposes of example and discussion. Using the disclosure provided herein, one of ordinary skill in the art will appreciate that any of the steps of the methods disclosed herein may be adapted, modified, rearranged, omitted, and/or expanded without departing from the scope of the present disclosure.
At 1602, method 1600 can include determining one or more objects (e.g., one or more objects in method 1400) that include one or more semantic symbols (e.g., one or more phonemes including one or more letters, one or more logograms, one or more syllabic characters, and/or one or more pictograms) based in part on the state data (e.g., state data in method 1400) or one or more semantic objects (e.g., one or more semantic objects in method 1400).
At 1604, method 1600 can include determining one or more words associated with the one or more semantic symbols based in part on the one or more semantic symbols (e.g., using a list of words, certain combinations of the one or more semantic symbols can be associated with the words). In some embodiments, the set of attributes of one or more semantic objects (e.g., the set of attributes in method 1400) may include one or more words. For example, a semantic object for a poster with text indicating "winter palace restaurants 8 month 24 day open" may include a poster semantic object that includes a set of attributes including restaurant opening as a value for an event type attribute, 8 month 24 day as a value for an event date attribute, and geographic coordinates associated with winter palace restaurants as a value for a location attribute.
At 1606, method 1600 can include determining a detected language associated with the one or more semantic symbols. For example, based in part on a combination of one or more semantic symbols (e.g., a word associated with one or more semantic symbols), the semantic processing system may determine a language associated with the one or more semantic symbols (e.g., a language including spanish, english, russian, and/or japanese).
At 1608, the method 1600 can include generating an output of the translation based in part on the translation data when the detected language is not associated with a default language (e.g., a language that a user of the semantic processing system has selected as the language to which the detected language is to be translated when the detected language is not the same as the default language). The translation data may include one or more semantic symbols of a default language and one or more semantic symbols of a detected language. The semantic processing system may compare one or more semantic symbols of the detected language with one or more semantic symbols of a default language to determine and perform an analysis of the translated detected language.
The output of the translation may include one or more semantic symbols of the default language corresponding to a portion of the one or more semantic symbols of the detected language (e.g., a multilingual thesaurus including a list of one or more words of the default language, each word associated with a respective word of the detected language). In some embodiments, the one or more object outputs may be based in part on the translated output (e.g., the one or more object outputs may include a visual indication or an audio indication of the translation).
Fig. 17 illustrates a flowchart of an example method of sensor-based semantic object generation according to an example embodiment of the present disclosure. One or more portions of method 1700 may be run or implemented on one or more computing devices or computing systems, including user device 102, remote computing device 104, and/or computing device 200, for example. One or more portions of method 1700 may also run or be implemented as algorithms on hardware components of the apparatus disclosed herein. Fig. 17 illustrates the steps performed in a particular order for purposes of example and discussion. Using the disclosure provided herein, one of ordinary skill in the art will appreciate that any of the steps of the methods disclosed herein may be adapted, modified, rearranged, omitted, and/or expanded without departing from the scope of the present disclosure.
At 1702, the method 1700 may include receiving data including location data including information associated with a current location (e.g., latitude and longitude of the current location) and a destination location (e.g., the destination location includes an address and/or latitude and longitude selected by a user of a semantic processing system) of an environment. In some embodiments, the location data may include a relative location (e.g., the current location is southwest of the user's place of business).
At 1704, method 1700 may include: a path from the current location to the destination location (e.g., a path between the current location and the destination location that avoids an intervening obstacle) is determined based in part on the status and location data of one or more objects (e.g., one or more objects in the method 1400) within the field of view of the one or more sensors. For example, the semantic processing system may determine the shortest path from the current location to the destination location that does not pass through any obstacles (e.g., rivers or building areas).
At 1706, the method 1700 may include generating one or more directions based in part on the one or more semantic objects and the path from the current location to the destination location (e.g., based on a series of steps along the path for the location or one or more general directions to travel in a compass direction over a period of time). In addition, the semantic processing system may determine one or more semantic objects that may be used as landmarks associated with one or more directions (e.g., a semantic object associated with a restaurant may be used as part of one or more directions "turn left one block before a winter palace restaurant"). In some embodiments, the one or more object outputs may be based in part on one or more directions (e.g., one or more visual indications or one or more audio indications may include directions).
The technology discussed herein makes reference to servers, databases, software applications, and other computer-based systems, as well as the actions taken by such systems and the information sent to and from such systems. Those skilled in the art will recognize that the inherent flexibility of a computer-based system allows for a wide variety of possible configurations, combinations, and divisions of tasks and functionality between and among components. For example, the server processes discussed herein may be implemented using a single server or multiple servers operating in combination. Databases and applications may be implemented on a single system or distributed across multiple systems. The distributed components may operate sequentially or in parallel.
While the present subject matter has been described in detail with respect to specific exemplary embodiments thereof, it will be appreciated that those skilled in the art, upon attaining an understanding of the foregoing may readily produce alterations to, variations of, and equivalents to such embodiments. Accordingly, the scope of the present disclosure is by way of example rather than by way of limitation, and the present disclosure does not preclude inclusion of such modifications, variations and/or additions to the present subject matter as would be readily apparent to one of ordinary skill in the art.
Claims (22)
1. A computer-implemented method of object recognition, the method comprising:
receiving, by a computing system comprising one or more computing devices, state data based in part on sensor output from one or more sensors that detect a state of an environment comprising one or more objects;
generating, by a computing system and based in part on state data, one or more semantic objects corresponding to one or more objects, wherein the one or more semantic objects comprise a set of attributes;
determining, by the computing system, based in part on the set of attributes of the one or more semantic objects, one or more operating modes associated with the one or more semantic objects; and
generating, by the computing system and based in part on the one or more operating modes, one or more object outputs associated with the one or more semantic objects, wherein the one or more object outputs comprise one or more visual indications or one or more audio indications.
2. The computer-implemented method of claim 1, wherein the computing system comprises a display component configured to display one or more images including an image of an environment, the environment including one or more objects detected by one or more sensors.
3. The computer-implemented method of claim 2, wherein the one or more sensors comprise one or more panoramic cameras positioned to capture one or more images comprising one or more objects or portions of one or more objects not within a visual plane of the display assembly.
4. The computer-implemented method of any preceding claim, further comprising:
determining, by the computing system, object data that matches the one or more semantic objects based in part on a set of attributes of the one or more semantic objects, wherein the object data comprises information associated with one or more related objects, one or more remote data sources, one or more locations, or one or more events; and
accessing, by a computing system, one or more portions of object data that match one or more semantic objects, wherein the one or more object outputs are based in part on the one or more portions of object data that match the one or more semantic objects.
5. The computer-implemented method of claim 4, further comprising:
generating, by a computing system and based in part on state data or one or more semantic objects, one or more interface elements associated with one or more objects, wherein the one or more interface elements comprise one or more images responsive to one or more inputs; and
in response to receiving one or more inputs to the one or more interface elements, determining, by the computing system, one or more remote computing devices that include at least a portion of the object data, wherein the one or more object outputs include one or more remote source indications associated with the one or more remote computing devices that include at least a portion of the object data.
6. The computer-implemented method of any preceding claim, wherein the one or more operating modes comprise: a text recognition pattern associated with text information in the recognition environment, a location recognition pattern associated with one or more locations in the recognition environment, an object recognition pattern associated with one or more objects in the recognition environment, or an event recognition pattern associated with the occurrence of one or more events in the recognition environment.
7. The computer-implemented method of any preceding claim, further comprising:
determining, by the computing system, based in part on the state data or the one or more semantic objects, one or more objects comprising one or more semantic symbols, wherein the one or more semantic symbols comprise one or more letters, one or more logograms, one or more syllabic characters, and/or one or more pictograms; and
determining, by the computing system, based in part on the one or more semantic symbols, one or more words associated with the one or more semantic symbols, wherein the set of attributes of the one or more semantic objects includes the one or more words.
8. The computer-implemented method of claim 7, further comprising:
determining, by the computing system, a detected language associated with the one or more words; and
generating, by the computing system, a translated output based in part on the translation data when the detected language is not associated with the default language, the translation data including one or more words of the default language and one or more words of the detected language, the translated output including one or more words of the default language corresponding to a portion of the one or more words of the detected language, wherein the one or more object outputs are based in part on the translated output.
9. The computer-implemented method of any preceding claim, further comprising:
receiving location data comprising information associated with a current location and a destination location of an environment;
determining, by the computing system, a path from the current location to the destination location based in part on the location data and a state of an environment of one or more objects within a field of view including the one or more sensors; and
generating, by a computing system, one or more directions based in part on one or more semantic objects and a path from a current location to a destination location, wherein the one or more object outputs are based in part on the one or more directions.
10. The computer-implemented method of any preceding claim, further comprising:
determining, by the computing system, one or more relevance values corresponding to the one or more semantic objects based in part on a degree to which each of the one or more semantic objects is associated with context data comprising data associated with a time of day, a current location, one or more planned events, one or more user locations, or one or more user preferences, wherein the one or more object outputs are based in part on the one or more relevance values corresponding to the one or more semantic objects.
11. The computer-implemented method of any preceding claim, further comprising:
modifying, by the computing system, the one or more visual indications or the one or more audio indications based in part on the state data or the semantic data, wherein modifying comprises converting the one or more visual indications to the one or more modified audio indications, converting the one or more audio indications to the one or more modified visual indications, modifying a size of the one or more visual indications, modifying one or more color characteristics of the one or more visual indications, or modifying a magnitude of the one or more audio indications.
12. The computer-implemented method of any preceding claim, wherein the set of attributes associated with the one or more semantic objects comprises a set of one or more object identifications, one or more object types, object locations, monetary value, ownership status, stock keeping units, or physical characteristics.
13. One or more tangible, non-transitory computer-readable media storing computer-readable instructions that, when executed by one or more processors, cause the one or more processors to perform operations comprising:
receiving status data based in part on sensor output from one or more sensors that detect a status of an environment including one or more objects;
generating one or more semantic objects corresponding to one or more objects based in part on the state data, wherein the one or more semantic objects comprise a set of attributes;
determining one or more operating modes associated with the one or more semantic objects based in part on the set of attributes of the one or more semantic objects; and
generating one or more object outputs associated with one or more semantic objects based in part on one or more operating modes, wherein the one or more object outputs comprise one or more visual indications or one or more audio indications.
14. The one or more tangible, non-transitory computer-readable mediums of claim 13, further comprising:
determining object data matching the one or more semantic objects based in part on a set of attributes of the one or more semantic objects, wherein the object data comprises information associated with one or more related objects, one or more remote data sources, one or more locations, or one or more events; and
accessing one or more portions of object data that match one or more semantic objects, wherein the one or more object outputs are based in part on the one or more portions of object data that match the one or more semantic objects.
15. The one or more tangible, non-transitory computer-readable mediums of claim 14, further comprising:
generating one or more interface elements associated with one or more objects based in part on the state data or one or more semantic objects, wherein the one or more interface elements comprise one or more images responsive to one or more inputs; and
in response to receiving one or more inputs to one or more interface elements, one or more remote computing devices comprising at least a portion of the object data are determined, wherein the one or more object outputs comprise one or more remote source indications associated with one or more remote computing devices comprising at least a portion of the object data.
16. The one or more tangible, non-transitory computer-readable mediums of any of claims 13-15, further comprising:
modifying one or more visual indications or one or more audio indications based in part on the state data or semantic data, wherein modifying comprises converting the one or more visual indications to the one or more modified audio indications, converting the one or more audio indications to the one or more modified visual indications, modifying a size of the one or more visual indications, or modifying a magnitude of the one or more audio indications.
17. A computing system, comprising:
one or more processors;
one or more non-transitory computer-readable media storing instructions that, when executed by the one or more processors, cause the one or more processors to perform operations comprising:
receiving status data based in part on sensor output from one or more sensors that detect a status of an environment including one or more objects;
generating one or more semantic objects corresponding to one or more objects based in part on the state data, wherein the one or more semantic objects comprise a set of attributes;
determining one or more operating modes associated with the one or more semantic objects based in part on the set of attributes of the one or more semantic objects; and
generating one or more object outputs associated with one or more semantic objects based in part on one or more operating modes, wherein the one or more object outputs comprise one or more visual indications or one or more audio indications.
18. The computing system of claim 17, further comprising:
determining object data matching the one or more semantic objects based in part on a set of attributes of the one or more semantic objects, wherein the object data comprises information associated with one or more related objects, one or more remote data sources, one or more locations, or one or more events; and
accessing one or more portions of object data that match one or more semantic objects, wherein the one or more object outputs are based in part on the one or more portions of object data that match the one or more semantic objects.
19. The computing system of claim 18, further comprising:
generating one or more interface elements associated with one or more objects based in part on the state data or one or more semantic objects, wherein the one or more interface elements comprise one or more images responsive to one or more inputs; and
in response to receiving one or more inputs to one or more interface elements, one or more remote computing devices comprising at least a portion of the object data are determined, wherein the one or more object outputs comprise one or more remote source indications associated with one or more remote computing devices comprising at least a portion of the object data.
20. The computing system of any of claims 17 to 19, further comprising:
modifying one or more visual indications or one or more audio indications based in part on the state data or semantic data, wherein modifying comprises converting the one or more visual indications to the one or more modified audio indications, converting the one or more audio indications to the one or more modified visual indications, modifying a size of the one or more visual indications, or modifying a magnitude of the one or more audio indications.
21. A computing system comprising at least one processing device and at least one memory storing computer executable instructions that, when executed by the at least one processing device, cause the at least one processing device to perform the method of any of claims 1 to 12.
22. A computer program comprising computer executable instructions which, when executed by at least one computing device, cause the at least one computing device to perform the method of any one of claims 1 to 12.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/792,393 | 2017-10-24 | ||
US15/792,393 US10685233B2 (en) | 2017-10-24 | 2017-10-24 | Sensor based semantic object generation |
PCT/US2018/041854 WO2019083582A1 (en) | 2017-10-24 | 2018-07-12 | Sensor based semantic object generation |
Publications (1)
Publication Number | Publication Date |
---|---|
CN110832477A true CN110832477A (en) | 2020-02-21 |
Family
ID=63165465
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201880044505.4A Pending CN110832477A (en) | 2017-10-24 | 2018-07-12 | Sensor-based semantic object generation |
Country Status (4)
Country | Link |
---|---|
US (3) | US10685233B2 (en) |
EP (1) | EP3625693A1 (en) |
CN (1) | CN110832477A (en) |
WO (1) | WO2019083582A1 (en) |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN111507355A (en) * | 2020-04-17 | 2020-08-07 | 北京百度网讯科技有限公司 | Character recognition method, device, equipment and storage medium |
Families Citing this family (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
EP3625697A1 (en) * | 2017-11-07 | 2020-03-25 | Google LLC | Semantic state based sensor tracking and updating |
US10970549B1 (en) * | 2017-11-14 | 2021-04-06 | Wells Fargo Bank, N.A. | Virtual assistant of safe locker |
US10699140B2 (en) | 2018-05-04 | 2020-06-30 | Qualcomm Incorporated | System and method for capture and distribution of information collected from signs |
US11562565B2 (en) * | 2019-01-03 | 2023-01-24 | Lucomm Technologies, Inc. | System for physical-virtual environment fusion |
US11604832B2 (en) * | 2019-01-03 | 2023-03-14 | Lucomm Technologies, Inc. | System for physical-virtual environment fusion |
KR102608127B1 (en) * | 2019-04-08 | 2023-12-01 | 삼성전자주식회사 | Electronic device for performing image processing and method thereof |
KR20200144196A (en) * | 2019-06-17 | 2020-12-29 | 삼성전자주식회사 | Electronic device and method for providing function using corneal image thereof |
CN110716706B (en) * | 2019-10-30 | 2023-11-14 | 华北水利水电大学 | Intelligent man-machine interaction instruction conversion method and system |
JP2021128683A (en) * | 2020-02-17 | 2021-09-02 | 東芝テック株式会社 | Information processing apparatus |
US11869262B1 (en) * | 2020-03-24 | 2024-01-09 | Amazon Technologies, Inc. | System for access control of image data using semantic data |
US11776253B2 (en) * | 2020-03-27 | 2023-10-03 | Snap Inc. | Displaying object names in association with augmented reality content |
Citations (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN101288077A (en) * | 2005-08-15 | 2008-10-15 | 埃韦里克斯技术股份有限公司 | Use of image-derived information as search criteria for internet and other search engines |
US20090249887A1 (en) * | 2008-04-02 | 2009-10-08 | Gysling Daniel L | Process Fluid Sound Speed Determined by Characterization of Acoustic Cross Modes |
CN102884779A (en) * | 2010-02-24 | 2013-01-16 | 数字标记公司 | Intuitive computing methods and systems |
CN103399932A (en) * | 2013-08-06 | 2013-11-20 | 武汉大学 | Situation identification method based on semantic social network entity analysis technique |
CN103729476A (en) * | 2014-01-26 | 2014-04-16 | 王玉娇 | Method and system for correlating contents according to environmental state |
US20150085184A1 (en) * | 2013-09-25 | 2015-03-26 | Joel Vidal | Smartphone and tablet having a side-panel camera |
CN105573436A (en) * | 2010-03-26 | 2016-05-11 | 谷歌公司 | Predictive pre-recording of audio for voice input |
CN105793870A (en) * | 2013-11-27 | 2016-07-20 | 夏普株式会社 | Translation display device, translation display method, and control program |
US20170286383A1 (en) * | 2016-03-30 | 2017-10-05 | Microsoft Technology Licensing, Llc | Augmented imaging assistance for visual impairment |
US20170301203A1 (en) * | 2016-04-15 | 2017-10-19 | Vivint, Inc. | Reducing bandwidth via voice detection |
Family Cites Families (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9836456B2 (en) * | 2015-01-12 | 2017-12-05 | Google Llc | Techniques for providing user image capture feedback for improved machine language translation |
CA3193007A1 (en) * | 2016-01-12 | 2017-07-20 | Esight Corp. | Language element vision augmentation methods and devices |
US10223613B2 (en) * | 2016-05-31 | 2019-03-05 | Microsoft Technology Licensing, Llc | Machine intelligent predictive communication and control system |
US10217375B2 (en) * | 2016-12-13 | 2019-02-26 | Bank Of America Corporation | Virtual behavior training using augmented reality user devices |
US10909371B2 (en) * | 2017-01-19 | 2021-02-02 | Samsung Electronics Co., Ltd. | System and method for contextual driven intelligence |
US10402995B2 (en) * | 2017-07-27 | 2019-09-03 | Here Global B.V. | Method, apparatus, and system for real-time object detection using a cursor recurrent neural network |
KR102425578B1 (en) * | 2017-08-08 | 2022-07-26 | 삼성전자주식회사 | Method and apparatus for recognizing an object |
-
2017
- 2017-10-24 US US15/792,393 patent/US10685233B2/en active Active
-
2018
- 2018-07-12 EP EP18752910.2A patent/EP3625693A1/en active Pending
- 2018-07-12 WO PCT/US2018/041854 patent/WO2019083582A1/en unknown
- 2018-07-12 CN CN201880044505.4A patent/CN110832477A/en active Pending
-
2020
- 2020-05-21 US US15/929,776 patent/US11514672B2/en active Active
-
2021
- 2021-08-05 US US17/394,467 patent/US20210365684A1/en active Pending
Patent Citations (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN101288077A (en) * | 2005-08-15 | 2008-10-15 | 埃韦里克斯技术股份有限公司 | Use of image-derived information as search criteria for internet and other search engines |
US20090249887A1 (en) * | 2008-04-02 | 2009-10-08 | Gysling Daniel L | Process Fluid Sound Speed Determined by Characterization of Acoustic Cross Modes |
CN102884779A (en) * | 2010-02-24 | 2013-01-16 | 数字标记公司 | Intuitive computing methods and systems |
CN105573436A (en) * | 2010-03-26 | 2016-05-11 | 谷歌公司 | Predictive pre-recording of audio for voice input |
CN103399932A (en) * | 2013-08-06 | 2013-11-20 | 武汉大学 | Situation identification method based on semantic social network entity analysis technique |
US20150085184A1 (en) * | 2013-09-25 | 2015-03-26 | Joel Vidal | Smartphone and tablet having a side-panel camera |
CN105793870A (en) * | 2013-11-27 | 2016-07-20 | 夏普株式会社 | Translation display device, translation display method, and control program |
CN103729476A (en) * | 2014-01-26 | 2014-04-16 | 王玉娇 | Method and system for correlating contents according to environmental state |
US20170286383A1 (en) * | 2016-03-30 | 2017-10-05 | Microsoft Technology Licensing, Llc | Augmented imaging assistance for visual impairment |
US20170301203A1 (en) * | 2016-04-15 | 2017-10-19 | Vivint, Inc. | Reducing bandwidth via voice detection |
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN111507355A (en) * | 2020-04-17 | 2020-08-07 | 北京百度网讯科技有限公司 | Character recognition method, device, equipment and storage medium |
CN111507355B (en) * | 2020-04-17 | 2023-08-22 | 北京百度网讯科技有限公司 | Character recognition method, device, equipment and storage medium |
Also Published As
Publication number | Publication date |
---|---|
WO2019083582A1 (en) | 2019-05-02 |
US10685233B2 (en) | 2020-06-16 |
US20190122046A1 (en) | 2019-04-25 |
US20210365684A1 (en) | 2021-11-25 |
US20200311423A1 (en) | 2020-10-01 |
US11514672B2 (en) | 2022-11-29 |
EP3625693A1 (en) | 2020-03-25 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11514672B2 (en) | Sensor based semantic object generation | |
CN111652678B (en) | Method, device, terminal, server and readable storage medium for displaying article information | |
JP6456901B2 (en) | System and method for presenting media content in an autonomous vehicle | |
US9563623B2 (en) | Method and apparatus for correlating and viewing disparate data | |
US8494215B2 (en) | Augmenting a field of view in connection with vision-tracking | |
US8943420B2 (en) | Augmenting a field of view | |
US11289084B2 (en) | Sensor based semantic object generation | |
CN110998563B (en) | Method, device and drawing system for disambiguating points of interest in a video field | |
US10043069B1 (en) | Item recognition using context data | |
CN111372192A (en) | Information recommendation method and device, terminal and storage medium | |
Hersh | Mobility technologies for blind, partially sighted and deafblind people: Design issues | |
CN107430738B (en) | Inferred user intent notification | |
JP6956232B2 (en) | Search system, search method, and search program | |
US20220198771A1 (en) | Discovery, Management And Processing Of Virtual Real Estate Content | |
KR20210120203A (en) | Method for generating metadata based on web page | |
TWI661351B (en) | System of digital content as in combination with map service and method for producing the digital content | |
JP7187597B2 (en) | Information processing device, information processing method and information processing program | |
CN115526602A (en) | Memo reminding method, device, terminal and storage medium | |
JP7090779B2 (en) | Information processing equipment, information processing methods and information processing systems | |
US20210263975A1 (en) | Electronic device and operation method thereof | |
US11828616B2 (en) | Search system, search method, and recording medium for recording search program | |
JP7240358B2 (en) | Information processing system, information processing method, information processing program, and server | |
JP7145247B2 (en) | Information processing device, information processing method and information processing program | |
JP7159373B2 (en) | Information processing device, information processing method and information processing program | |
CN111797875B (en) | Scene modeling method and device, storage medium and electronic equipment |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |