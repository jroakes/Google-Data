EP3014444A1 - Computing connected components in large graphs - Google Patents
Computing connected components in large graphsInfo
- Publication number
- EP3014444A1 EP3014444A1 EP14744682.7A EP14744682A EP3014444A1 EP 3014444 A1 EP3014444 A1 EP 3014444A1 EP 14744682 A EP14744682 A EP 14744682A EP 3014444 A1 EP3014444 A1 EP 3014444A1
- Authority
- EP
- European Patent Office
- Prior art keywords
- nodes
- node
- graph
- map
- cluster
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
- 230000006870 function Effects 0.000 claims abstract description 63
- 230000015654 memory Effects 0.000 claims abstract description 57
- 238000000034 method Methods 0.000 claims abstract description 55
- 238000013507 mapping Methods 0.000 claims abstract description 6
- 238000012545 processing Methods 0.000 claims description 16
- 230000008569 process Effects 0.000 description 24
- 238000004891 communication Methods 0.000 description 16
- 239000003638 chemical reducing agent Substances 0.000 description 11
- 238000004422 calculation algorithm Methods 0.000 description 8
- 238000004364 calculation method Methods 0.000 description 8
- 230000008859 change Effects 0.000 description 6
- 238000004590 computer program Methods 0.000 description 6
- 230000003287 optical effect Effects 0.000 description 6
- 238000010586 diagram Methods 0.000 description 5
- 238000009826 distribution Methods 0.000 description 3
- 238000011156 evaluation Methods 0.000 description 2
- 230000006855 networking Effects 0.000 description 2
- 239000007787 solid Substances 0.000 description 2
- 238000013459 approach Methods 0.000 description 1
- 230000001174 ascending effect Effects 0.000 description 1
- 230000001413 cellular effect Effects 0.000 description 1
- 230000001427 coherent effect Effects 0.000 description 1
- 230000007423 decrease Effects 0.000 description 1
- 230000001419 dependent effect Effects 0.000 description 1
- 239000011159 matrix material Substances 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 238000009828 non-uniform distribution Methods 0.000 description 1
- 229920001296 polysiloxane Polymers 0.000 description 1
- 230000000135 prohibitive effect Effects 0.000 description 1
- 238000000638 solvent extraction Methods 0.000 description 1
- 230000001360 synchronised effect Effects 0.000 description 1
- 238000012360 testing method Methods 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L67/00—Network arrangements or protocols for supporting network services or applications
- H04L67/01—Protocols
- H04L67/10—Protocols in which an application is distributed across nodes in the network
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/46—Multiprogramming arrangements
- G06F9/50—Allocation of resources, e.g. of the central processing unit [CPU]
- G06F9/5061—Partitioning or combining of resources
- G06F9/5066—Algorithms for mapping a plurality of inter-dependent sub-tasks onto a plurality of physical CPUs
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/46—Multiprogramming arrangements
- G06F9/54—Interprogram communication
- G06F9/546—Message passing systems or structures, e.g. queues
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06Q—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES; SYSTEMS OR METHODS SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES, NOT OTHERWISE PROVIDED FOR
- G06Q10/00—Administration; Management
- G06Q10/06—Resources, workflows, human or project management; Enterprise or organisation planning; Enterprise or organisation modelling
Definitions
- Graphs are a basic modeling tool to model social, communication, and information networks.
- a graph G(V, E) consists of a set of nodes V , and a set of edges E £ V 2 where each edge connects two nodes in the graph.
- analysis is performed on large graphs that do not fit on one machine. Consequently, the graph is stored in several machines and mined in a distributed manner, for example by applying distributed programming tools like Map-Reduce or Hadoop.
- a basic analysis tool for graphs is to compute connected components of the graph.
- a connected component of a graph G(V,E) is a maximal set of nodes that can be reached from each other via sequences of edges of the graph.
- FIG. 2 illustrates a graph G with three connected components.
- Connected component 205 includes nodes A, B, C, and D
- connected component 210 includes nodes F, G, I, and H
- connected component 215 includes nodes J, K, L, and M.
- the connected components may also be referred to as a cluster of nodes.
- Computing connected components in graphs is a basic tool for computing coherent clusters of nodes and also to perform hierarchical clustering.
- computing clusters of nodes distributed across multiple machines can be time and cost prohibitive as the running time of the hashing functions are dependent on the size of the graph, the number of messages sent between machines during the rounds of Map-Reduce, and the number of rounds of Map-Reduce performed. It is a challenge is to compute connected components for a large graph.
- Implementations provide two methods of improving the time and cost to calculate connected components in a large distributed graph.
- the first method alternates rounds of Map-Reduce between Hash-Greater-to-Min and Hash-Lesser- to-Min functions. Alternating the functions used in each round of Map-Reduce results in computing times 2 to 4.5 times faster than using Hash-to-Min in each round on a graph with 10-500 billion edges.
- the second method reduces the number of messages sent by performing a predetermined number of rounds of Map-Reduce and finishing the Map-Reduce using a table in memory. For example, after the predetermined number of rounds, for example 2, a table in memory is created that represents the still active cluster identifiers and their associated nodes.
- the remaining rounds of Map-Reduce which will reduce the active cluster identifiers to actual cluster identifiers, can be performed using the table, which eliminates messaging between nodes and machines.
- Using the memory table after a predetermined number of rounds results in computing times 4.5 to 15 times faster than without the table.
- Another implementation optimizes the alternating with load- balancing highly connected nodes, reducing the maximum number of values a reducer node receives.
- One aspect of the disclosure can be embodied in a system that includes distributed computing devices, memory storing a graph of nodes and edges, the graph being distributed across the computing devices and at least one root computing device.
- the root computing device includes at least one processor and memory storing instructions that, when executed by the at least one processor, cause the system to determine connected components for the graph by performing rounds of a map stage and a reduce stage for the nodes of the graph, wherein the map stage alternates between two hashing functions.
- the graph can include more than one billion nodes.
- a first hashing function of the two hashing functions may be a Hash-Greater- to-Min hashing function that emits (Vmin, C>v) and (U, ⁇ Vmin ⁇ ) for nodes U in the set of C>v and a second hashing function of the two hashing functions is a Hash-Lesser- to-Min hashing function that emits (Vmin, C ⁇ v) and (U, ⁇ Vmin ⁇ ) for nodes U in the set of C ⁇ v.
- determining the connected components may include load balancing nodes with a neighborhood size larger than a bounded limit during a first of the two hashing functions.
- the system may also include memory storing a state for a first node of nodes, the state including a set of possible cluster identifiers, and a first hashing function of the two hashing functions can include determining a minimum cluster identifier in the set of possible cluster identifiers, determining a first subset of the cluster identifiers, sending a message with the minimum cluster identifier to nodes corresponding to the first subset, and sending a message that includes the first subset of cluster identifiers to a node associated with the minimum cluster identifier.
- a second hashing function of the two hashing functions can include determining a minimum cluster identifier in the set of possible cluster identifiers, determining a second subset of the cluster identifiers, the second subset including the cluster identifiers not included in the first subset, sending a message with the minimum cluster identifier to nodes corresponding to the second subset, and sending a message that includes the second subset of cluster identifiers to a node associated with the minimum cluster identifier.
- Another aspect of the disclosure can be embodied in a method that includes reducing processing time during map-reduce rounds used to determine a cluster assignment for a node in a large distributed graph, a map-reduce round including a map stage and a reduce stage, the processing time being reduced by alternating between two hashing functions in the map stage and storing the cluster assignment for the node in a memory.
- a first of the two hashing functions may be a Hash-Greater-to-Min hashing function and a second of the two hashing functions may be a Hash-Lesser-to-Min hashing function.
- Another aspect of the disclosure can be embodied in a method that includes reducing a quantity of messages sent during map-reduce rounds used to determine a cluster assignment for a node in a large distributed graph by; 1) performing a predetermined quantity of rounds to generate, for each node, a set of potential cluster assignments, 2) generating a data structure in memory to store a mapping between each node and its potential cluster assignment, and 3) using the data structure during remaining map-reduce rounds, wherein the remaining map- reduce rounds do not use messages sent between nodes.
- the method may also include storing the cluster assignment for the node in a memory.
- the data structure may be an SSTable or a Bigtable.
- Another aspect of the disclosure can be embodied in a system that includes distributed computing devices represented by leaf servers, memory storing a graph of nodes and edges, the graph being distributed across the leaf servers, and at least one root computing device.
- the root computing device includes at least one processor and memory storing instructions that, when executed by the at least one processor, cause the system to determine connected components for the graph.
- the system determines connected components by performing a predetermined number of rounds of a map stage and a reduce stage for the nodes of the graph, the map stage causing messages to be sent between the leaf servers, and wherein performing the predetermined number of rounds generates a set of potential clusters for each node, generating a data structure in memory to store a mapping between each node and its set of potential clusters, and using the data structure during remaining rounds of the map stages and the reduce stages, wherein no messages are sent between the leaf servers during the remaining rounds.
- the map stage may use a Hash-to-Min hashing function.
- Another aspect of the disclosure can be embodied on a computer- readable medium having recorded and embodied thereon instructions that, when executed by a processor of a computer system, cause the computer system to perform any of the methods disclosed herein.
- FIG. 1 illustrates an example system in accordance with the disclosed subject matter.
- FIG. 2 illustrates an example graph distributed across three leaves and including three clusters.
- FIG. 3 illustrates a flow diagram of an example process for computing connected components of a large distributed graph.
- FIG. 4 illustrates a flow diagram of an example of another process for computing connected components of a large distributed graph.
- FIG. 5 shows an example of a general Map-Reduce function.
- FIG. 6 illustrates a star graph with a root node having high connectivity and a load-balanced version of the star graph, according to an implementation.
- FIG. 7 illustrates processing time enhancements that result from various implementations used to compute connected components on various real-world graphs.
- FIG. 8 illustrates scalability of various implementations on graphs of various sizes.
- FIG. 9 demonstrates the efficiency of various implementations as resources are added.
- FIG. 10 shows an example of a computer device that can be used to implement the described techniques.
- FIG. 11 shows an example of a distributed computer device that can be used to implement the described techniques.
- FIG. 1 is a block diagram of a distributed graph system 100 in accordance with an example implementation.
- the system 100 may be used to calculate connected components of a large distributed graph using the techniques described herein.
- the graph system 100 may include root 120 and graph cluster 160.
- Root 120 and graph cluster 160 may be computing devices that take the form of a number of different devices, for example a standard server, a group of such servers, or a rack server system.
- the root 120 and the graph cluster 160 may be distributed systems implemented in a series of computing devices, such as a group of servers.
- the servers may be organized into a tree structure, with at least a root server 120 and leaf servers 150A to 150w.
- the tree may include intermediate servers, so that there are one or more layers between the root 120 and the leaf servers 150A to 150K.
- the root 120 and graph cluster 160 may be examples of computer device 1 100, as depicted in FIG. 1 1.
- the graph system 100 illustrated in FIG. 1 operates over a large graph with, for example, billions of nodes.
- the root 120 may include one or more hardware processors 142 for one or more computing devices, such as servers, that operate with the graph cluster 160 to perform operations on the data graph represented by nodes and edges 154.
- the root 120 may include one or more servers that receive commands or requests from a requester, such as client 170.
- the root 120 may initiate and monitor calculations performed on the graph and may manage the results of the calculations.
- the root 120 may facilitate searches and queries on the graph.
- the root 120 may also store a memory table, such as SSTable 144, that can be used to reduce the number of messages sent during later rounds of a Map-Reduce process.
- System 100 may also include a graph cluster 160.
- Graph cluster 160 may be a collection of distributed computing devices each with its own hardware processor and memory. The number of computing devices that comprise graph cluster 160 can vary.
- the graph cluster 160 may be divided into one or more leaf servers, such as leaf 150A, leaf 150B, leaf 150/7, with n representing any positive integer.
- a leaf server may be associated with a logical division of nodes in the graph, with each graph node being assigned to a leaf server.
- a leaf server may correspond to one computing device, or a leaf server may be a logical computing device and may share a physical computing device with other leaves.
- a node's assigned leaf may change as the graph is updated, making leaf assignments flexible.
- the root 120 may determine which nodes are assigned to each leaf as the nodes are added to the graph or updated. [00028] The root 120 may route processing requests to the leaf servers and act as the primary means of coordination between the leaves at processing time. Of course, leaves may send messages directly to each other, and nodes in the graph may send messages to each other as part of graph processing.
- the graph cluster 160 may include one or more layers or intermediate servers between the root node 120 and the leaf servers, but are not shown in FIG. 1 for the sake of brevity.
- an intermediate server may be associated with, for example, 20 leaf servers.
- the intermediate server may be connected directly to the root, or there may be one or more additional layers between the intermediate server and the root 120.
- the root, intermediate, and leaf servers that make up the tree may, collectively, be referred to as the graph.
- Each of the leaf servers that make up graph cluster 160 can include node states 152 and nodes and edges 154.
- a state for a node may be used in calculating connected components for the graph and may indicate the cluster of nodes (Cv) that a node belongs to.
- the cluster Cv may include a set of node identifiers, but at the conclusion of the calculation the Cv for each node will include one identifier. This identifier becomes the identifier for the cluster and, accordingly, the identifier for the component the node belongs to.
- the state, or set of nodes in Cv may be altered during each round of Map-Reduce.
- Graph system 100 may be in communication with clients 170 over network 180.
- Network 180 may be for example, the Internet or the network 180 can be a wired or wireless local area network (LAN), wide area network (WAN), etc., implemented using, for example, gateway devices, bridges, switches, and/or so forth.
- the system 100 may communicate with and transmit data to/from clients 170 or other computing devices.
- FIG. 3 illustrates a flow diagram of an example process 300 for computing connected components of a large distributed graph.
- the process 300 shown in FIG. 3 may be performed by a graph system distributed across multiple computing devices, such as graph system 100 of FIG. 1.
- the process 300 may speed the calculation of connected components in a distributed graph by using an alternating algorithm for Map-Reduce rounds to calculate the cluster each node belongs to.
- a root such as root 120, may initiate process 300 to determine which cluster the nodes belong to.
- the process may begin with the system initializing the cluster Cv for each node in the graph.
- Cv is the state of a node and includes a set of possible clusters the node is a member of.
- each node in the graph may be considered a cluster, e.g., a cluster with a single member.
- the reduce stage of the Map-Reduce rounds will shrink the number of clusters, so that after each round there are fewer possible clusters.
- Process 300 ends when each node is a member of one possible cluster, or in other words when the Cv for each V in the graph does not change.
- the node with the smallest identifier (Vmin) in the cluster may be used to identify the cluster, as will be illustrated below.
- the identifier of Vmin in the cluster may be the cluster identifier.
- members of Cv may also represent nodes in the graph.
- the system may set Cv to the set of V and the neighbors of V. For example, in the graph of FIG. 2, the system initializes CAto ⁇ A, B ⁇ , CB to ⁇ A, B, D, C ⁇ , Cc to ⁇ B, C ⁇ , and CD to ⁇ B, D ⁇ .
- the system then begins the first round of Map-Reduce with a map stage performing a Hash-Greater-to-Min hashing function at each node (310).
- a Hash-Greater-to-Min hashing function the set of members of Cv that are greater than V are sent via a message to the reducer node Vmin and the set ⁇ Vmin ⁇ is sent via a message to all reducer nodes U in Cv that are greater than V (C>v).
- the Hash-Greater-to-Min hashing function emits (Vmin, C>v) and (U, ⁇ Vmin ⁇ ) for nodes U in the set of C>v. For example, using the graph of FIG. 2:
- Node A sends ⁇ A ⁇ to node B and ⁇ B ⁇ to itself;
- Node B sends ⁇ A ⁇ to Nodes C and D and ⁇ C, D ⁇ to node A;
- Node C sends ⁇ ⁇ (empty set) to Node B;
- Node D sends ⁇ ⁇ to Node B.
- Nodes C and D send empty sets because there are no nodes greater than themselves in their corresponding potential clusters, Cc and CD. Messages may be sent between nodes using any now known or later discovered method, including remote procedure calls. It is understood that some of these messages may travel between leaves. For example, for Node I of FIG. 2 to send a message to Node F, the message is sent between Leaf 150B and Leaf 150A but a message sent to Node H can be sent within Leaf 150B.
- Process 300 continues with the nodes calculating a new Cv (315), either as messages are received or after all nodes are finished sending messages in the reduce stage. If the node calculates Cv as messages are received, the node may still wait to proceed to the next Map-Reduce round because Map-Reduce rounds are synchronized. In other words, in each found of Map-Reduce the nodes begin the map stage together.
- the new Cv may be a union of the sets received from other nodes. Accordingly in the example of FIG. 2, the new CA is ⁇ B, C, D ⁇ , the new CB is ⁇ A ⁇ , the new Cc is ⁇ A ⁇ , and the new CD is ⁇ A ⁇ . After a new Cv is calculated for each reducer node, or the nodes that received a message, the first round of Map- Reduce is complete.
- the system may determine whether the clusters are stable (320). The clusters are stable if no node V had a change C v during the last round. If no node V changed its Cv the clusters are stable (320, Yes), and process 300 ends. Otherwise, another round of Map-Reduce begins with another map stage. In this round, the system uses a Hash-Lesser-to-Min hashing function (325). In Hash-Lesser-to-Min, the set of members of Cv that are less than V are sent via a message to a reducer node Vmin and the set ⁇ Vmin ⁇ is sent via a message to all reducer nodes U in Cv that are less than or equal to V (C ⁇ v). In other words, the Hash-Lesser-to-Min hashing function emits (Vmin, C ⁇ v) and (U, ⁇ Vmin ⁇ ) for nodes U in the set of C ⁇ v. For example, using the graph of FIG. 2:
- Node A sends ⁇ B ⁇ to itself
- Node B sends ⁇ A ⁇ to Node A;
- Node C sends ⁇ A ⁇ to Node A;
- Node D sends ⁇ A ⁇ to Node A.
- Process 300 continues with the nodes calculating a new Cv (330), as explained above with regard to step 315.
- the new Cv for each reducer node is calculated, the reduce stage of the round is complete, so the second round of Map- Reduce is complete.
- the new CA is ⁇ A, B ⁇ .
- nodes B, C and D did not receive messages and, thus, are not reducer nodes for this round.
- their Cr does not change but remains ⁇ A ⁇ .
- the system determines again whether the clusters are stable (335). If so (335, Yes), process 300 ends. If not (335, No), another round of Map- Reduce begins.
- Hash function alternates in each round, the system will start a Hash-Greater-to-Min function in each node, as described above with regard to steps 310 to 320.
- the rounds continue, alternating between Hash-Greater-to-Min and Hash-Lesser-to-Min until the clusters are stable.
- the state or Cv of each node contains one member, representing the cluster that the node belongs to.
- This cluster identifier may represent one of the connected components for the graph.
- the cluster identifier may be stored as a property of the node.
- the clusters may be used for various graph processing tasks such as suggesting new connections in a social networking graph or load distribution for traffic in a computer network graph.
- FIG. 4 illustrates a flow diagram of an example of another process 400 for computing connected components of a large distributed graph.
- the process 400 shown in FIG. 4 may be performed by a graph system distributed across multiple computing devices, such as graph system 100 of FIG. 1.
- the process 400 may speed the calculation of connected components in a distributed graph by using an in-memory table to resolve cluster mapping and reduce the messages sent.
- a root such as root 120, may initiate process 400 to determine which cluster the nodes belong to.
- Process 400 may be used as an alternative to process 300.
- the process may begin with the system initializing the cluster Cv for each node in the graph (405). The initialization of Cv may depend on the hashing function used in the map stage of the Map-Reduce.
- Cv may be initialized to the set of V and the neighbors of V. For example, in the graph of FIG. 2, the system initializes CF to ⁇ F, G, H, I ⁇ , Co to ⁇ F, G ⁇ , Ci to ⁇ F, I, H ⁇ , and C H to ⁇ F, I, H ⁇ .
- the system may then start a round of Map-Reduce by performing the hashing function in each node (410).
- the hashing function may be the Hash-to-Min function that emits (Vmin, Cv) and (U, ⁇ Vmin ⁇ ) for all nodes U £ Cv.
- the reducer nodes may calculate their new Cv and all nodes may wait for the remaining nodes to finish the round (420).
- the system may perform the Map-Reduce rounds a predetermined quantity of times. If the system has not completed the predetermined number of rounds (420, Yes), steps 410 through 420 may be repeated until the system has completed the predetermined number of rounds. In some
- the predetermined can be as low as two.
- the system may create a table in memory to hold the active cluster identifiers, identified in Cv for each node V.
- the active cluster identifiers are those identifiers left in the set represented by Cv for each V in the graph.
- many of the potential cluster identifiers have fallen out of the set and the remaining identifiers are small enough to transfer to a memory table.
- the remaining identifiers may be referred to as active clusters or potential clusters.
- the table may be an SSTable.
- An SSTable is a file format that stores immutable string-to-string maps. SSTable data is sorted in ascending order by its keys and can have multiple values for a single key. The SSTable can be sharded, or divided into multiple files.
- the table may also be a Bigtable or some other type of data structure that maps nodes with potential cluster identifiers.
- the system may generate one or more entries in the table for each C v . For example, the system may create one entry for each current member of Cv. Thus, for example, after two rounds the nodes in cluster 215 of FIG.
- step 425 the system may create the following table:
- the system may then perform the hash function using the table instead of sending messages between nodes. Eliminating the messaging speeds the calculation by eliminating delay due to slow nodes, crashed nodes, or network communication conditions.
- the table may be stored on the root, such as SSTable 144 of FIG. 1.
- the system may continue simulating Map- Reduce rounds (430) and (435) using the table rather than sending messages.
- the cluster a node belongs to may be determined from the table and, in some implementations, may be stored as an attribute of the node.
- FIG. 5 shows an example of pseudo-code for a general Map-Reduce function that can be used to alternate the algorithm used in Map-Reduce rounds.
- the algorithm can be a Hash-to-Min, Hash-Greater-to-Min, or Hash-Lesser-to-Min, as discussed above.
- nodes may have a high degree of connectivity compared to other nodes, such as a root node in a star graph.
- the root node of a star graph 600 is illustrated as node 605 of FIG. 6.
- the implementations described above result in a star graph for each connected component, with the node having the lowest label as the root of each star.
- the reducer corresponding to the node with the lowest label will receive the entire connected component, which can cause a single machine to be a bottleneck.
- computing the connected components may be optimized by load balancing, ensuring that a reducer looks at a bounded number of pairs independent of the size of the largest connected component during application of the hash-greater-to-min hashing function.
- load balancing may occur during the first round (e.g., the hash-greater-to-min round) of the alternating rounds of Map-Reduce.
- the bounded number of pairs may be represented by b.
- b represents the bounded limit.
- the set of nodes in a neighborhood for node V may be represented by T(V).
- the system may determine whether the node is a root node.
- a node may be marked as a root node in the reduce phase, so the first time the map phase is run, no node is marked as a root node. If the node is a root node, the system may emit it's identifier (or state) to it's neighbors T(V). Because it is a root, it is by definition the node with the smallest identifier (e.g., Vmin).
- the system may make copies of the node V.
- the quantity of copies is equal to the bounded limit b.
- the neighboring nodes of V may be assigned to one of the copies of V. For example, ii b is 4, the star graph 600 of FIG. 6 becomes the star graph 650 of FIG. 6, with nodes 655, 660, 665, and 670 being copies of node 605.
- a hash may be used to assign the neighbors to one of the copies, e.g. a hash of the node identifier.
- each copy may be assigned an identifier or label that is based on the identifier (or label) of node V, but differs in an infinites imally small way. For example, if node V has a label of fv and i ⁇ 1, 2, 3 , ... b) so that there are 1 -b copies of node V, the i th copy V may have a label of fv + ( ⁇ * ⁇ ) where ⁇ is greater than zero but infinitesimally small so that ⁇ * ⁇ is not greater than one. Thus, the copies of V are in the set of nodes that are greater than V.
- the system may then connect V to each of its copies (e.g., by sending a message from V to each Vi where i ⁇ 1, 2, 3, ... b) that includes V), and may send a message from each Vi to the nodes U that are associated with the copy.
- the system may check to see if the node has the smallest identifier of the neighborhood. If it does, the node is marked as a root node. The system may then send a message to all nodes in the neighborhood greater than V with Vmin (e.g., the node with the smallest label).
- FIG. 7 illustrates the gains in processing time observed for the various implementations across the various graphs.
- the gains are over the hash-to- min algorithm.
- the Optimized Alternating uses the load balancing during the first round of the alternating rounds, as described above.
- the two phase DHT uses the in-memory table, also as described above.
- all implementations see significant decreases in processing times to compute connected components over hash-to-min alone, with the in-memory table implementation (Two Phase DHT) processing the connected components almost 30 times faster on very large graphs.
- Two Phase DHT In terms of absolute running time, for some big graphs, the actual running time went down from more than a day to a couple of hours, or from a few hours to ten minutes, when running over several hundreds of machines.
- RMAT is a recursive model of randomly generating a graph with several desirable properties such as power-law degree distribution property, small world property, and inclusion of many dense bipartite subgraphs.
- RMAT graph one recursively subdivides the adjacency matrix in four equal quadrants and elects to recurse on one of the four quadrants with unequal probability (a, b, c, or d).
- the size of the seven RMAT graphs is illustrated in Table 2 below:
- Hash-to-min algorithm and various implementations were used to compute connected components on the seven RMAT graphs. Table 3 below shows the number of Map-Reduce rounds used by each implementation and the hash-to- min. On the two largest graphs, hash-to-min failed to finish in more than a day, so the run was terminated before completion. Hash-to-Min Alternating Optimized Two Phase
- FIG. 8 illustrates the running time versus the log of the number of vertices.
- the hash-to-min algorithm has a fast-growing running time, which is why the algorithm did not finish in a day for the larger graphs.
- the various implementations have slower-growing running times, with the optimized alternating (e.g., load balancing), having the most reduced running time because this implementation breaks up high degree nodes.
- FIG. 9 demonstrates the performance of various implementations in relation to the number of resources (cores) available.
- the graph of FIG. 9 plots the relative running times of each approach on the two largest RMAT graphs, RMAT 3 32 and RMAT 2 34 .
- a perfectly work efficient algorithm would take twice as much time when the number of resources are halved and three times as much time when the number of resources are reduced by a factor of three.
- FIG. 9 illustrates that disclosed implementations are for the most part work efficient.
- One exception comes from computing large graphs on small resources; here the slow down is nonlinear due to machines getting overwhelmed.
- the in-memory implementation (Two-Phase DHT) does not finish on the regular number of resources on the largest RMAT graph because of the in- memory table getting overwhelmed.
- FIG. 9 illustrates only two of the three bars for the Two-Phase DHT implementation.
- FIG. 10 shows an example of a generic computer device 1000, which may be system 100 or client 170 of FIG. 1, which may be used with the techniques described here.
- Computing device 1000 is intended to represent various example forms of computing devices, such as laptops, desktops, workstations, personal digital assistants, cellular telephones, smart phones, tablets, servers, and other computing devices, including wearable devices
- computing devices such as laptops, desktops, workstations, personal digital assistants, cellular telephones, smart phones, tablets, servers, and other computing devices, including wearable devices
- the components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document.
- Computing device 1000 includes a hardware (e.g., silicone-based) processor 1002, memory 1004, a storage device 1006, and expansion ports 1010 connected via an interface 1008.
- computing device 1000 may include transceiver 1046, communication interface 1044, and a GPS (Global Positioning System) receiver module 1048, among other components, connected via interface 1008.
- Device 1000 may communicate wirelessly through communication interface 1044, which may include digital signal processing circuitry where necessary.
- Each of the components 1002, 1004, 1006, 1008, 1010, 1040, 1044, 1046, and 1048 may be mounted on a common motherboard or in other manners as appropriate.
- the processor 1002 can process instructions for execution within the computing device 1000, including instructions stored in the memory 1004 or on the storage device 1006 to display graphical information for a GUI on an external input/output device, such as display 1016.
- Display 1016 may be a monitor or a flat touchscreen display.
- multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory.
- multiple computing devices 1000 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system).
- the memory 1004 stores information within the computing device 1000.
- the memory 1004 is a volatile memory unit or units.
- the memory 1004 is a non-volatile memory unit or units.
- the memory 1004 may also be another form of computer-readable medium, such as a magnetic or optical disk.
- the memory 1004 may include expansion memory provided through an expansion interface.
- the storage device 1006 is capable of providing mass storage for the computing device 1000.
- the storage device 1006 may be or contain a computer-readable medium, such as a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations.
- a computer program product can be tangibly embodied in such a computer-readable medium.
- the computer program product may also contain instructions that, when executed, perform one or more methods, such as those described above.
- the computer- or machine-readable medium is a storage device such as the memory 1004, the storage device 1006, or memory on processor 1002.
- the interface 1008 may be a high speed controller that manages bandwidth- intensive operations for the computing device 1000 or a low speed controller that manages lower bandwidth-intensive operations, or a combination of such controllers.
- An external interface 1040 may be provided so as to enable near area communication of device 1000 with other devices.
- controller 1008 may be coupled to storage device 1006 and expansion port 1014.
- the expansion port which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet) may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- the computing device 1000 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a standard server 1030, or multiple times in a group of such servers. It may also be implemented as part of a rack server system. In addition, it may be implemented in a personal computer such as a laptop computer 1022, or smart phone 1036. An entire system may be made up of multiple computing devices 1000 communicating with each other. Other configurations are possible.
- FIG. 8 shows an example of a generic computer device 1100, which may be system 100 of FIG. 1, which may be used with the techniques described here.
- Computing device 1100 is intended to represent various example forms of large-scale data processing devices, such as servers, blade servers, datacenters, mainframes, and other large-scale computing devices.
- Computing device 1100 may be a distributed system having multiple processors, possibly including network attached storage nodes, that are interconnected by one or more communication networks.
- the components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document.
- Distributed computing system 1100 may include any number of computing devices 1180.
- Computing devices 1180 may include a server or rack servers, mainframes, etc. communicating over a local or wide-area network, dedicated optical links, modems, bridges, routers, switches, wired or wireless networks, etc.
- each computing device may include multiple racks.
- computing device 1180a includes multiple racks 1158a - 1158n.
- Each rack may include one or more processors, such as processors 1152a- 1152n and 1162a-l 162n.
- the processors may include data processors, network attached storage devices, and other computer controlled devices.
- one processor may operate as a master processor and control the scheduling and data distribution tasks.
- Processors may be interconnected through one or more rack switches 1158, and one or more racks may be connected through switch 1178. Switch 1178 may handle communications between multiple connected computing devices 1100.
- Each rack may include memory, such as memory 1154 and memory 1164, and storage, such as 1156 and 1166.
- Storage 1156 and 1166 may provide mass storage and may include volatile or non-volatile storage, such as network-attached disks, floppy disks, hard disks, optical disks, tapes, flash memory or other similar solid state memory devices, or an array of devices, including devices in a storage area network or other configurations.
- Storage 1156 or 1166 may be shared between multiple processors, multiple racks, or multiple computing devices and may include a computer-readable medium storing instructions executable by one or more of the processors.
- Memory 1154 and 1164 may include, e.g., volatile memory unit or units, a non- volatile memory unit or units, and/or other forms of computer-readable media, such as a magnetic or optical disks, flash memory, cache, Random Access Memory (RAM), Read Only Memory (ROM), and combinations thereof. Memory, such as memory 1154 may also be shared between processors 1152a-l 152n. Data structures, such as an index, may be stored, for example, across storage 1156 and memory 1154. Computing device 1100 may include other components not shown, such as controllers, buses, input/output devices, communications modules, etc.
- An entire system such as system 100, may be made up of multiple computing devices 1100 communicating with each other.
- device 1180a may communicate with devices 1180b, 1180c, and 1180d, and these may collectively be known as system 100.
- system 100 of FIG. 1 may include one or more computing devices 1100 as graph system 100, a separate computing device 1100 as root 120, and one or more computing devices 1100 as graph cluster 160.
- some of the computing devices may be located geographically close to each other, and others may be located geographically distant.
- the layout of system 1100 is an example only and the system may take on other layouts or configurations.
- Various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- a programmable processor which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- the systems and techniques described here can be implemented in a computing system that includes a back end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front end component (e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the systems and techniques described here), or any combination of such back end, middleware, or front end components.
- the components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include a local area network ("LAN”), a wide area network (“WAN”), and the Internet.
- LAN local area network
- WAN wide area network
- the Internet the global information network
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network.
- the relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
Abstract
Description
Claims
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
EP20202618.3A EP3786798A1 (en) | 2013-06-29 | 2014-06-27 | Computing connected components in large graphs |
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201361841337P | 2013-06-29 | 2013-06-29 | |
US14/143,894 US9596295B2 (en) | 2013-06-29 | 2013-12-30 | Computing connected components in large graphs |
PCT/US2014/044636 WO2014210499A1 (en) | 2013-06-29 | 2014-06-27 | Computing connected components in large graphs |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
EP20202618.3A Division EP3786798A1 (en) | 2013-06-29 | 2014-06-27 | Computing connected components in large graphs |
Publications (2)
Publication Number | Publication Date |
---|---|
EP3014444A1 true EP3014444A1 (en) | 2016-05-04 |
EP3014444B1 EP3014444B1 (en) | 2020-10-21 |
Family
ID=52116712
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
EP20202618.3A Pending EP3786798A1 (en) | 2013-06-29 | 2014-06-27 | Computing connected components in large graphs |
EP14744682.7A Active EP3014444B1 (en) | 2013-06-29 | 2014-06-27 | Computing connected components in large graphs |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
EP20202618.3A Pending EP3786798A1 (en) | 2013-06-29 | 2014-06-27 | Computing connected components in large graphs |
Country Status (3)
Country | Link |
---|---|
US (1) | US9596295B2 (en) |
EP (2) | EP3786798A1 (en) |
WO (1) | WO2014210499A1 (en) |
Families Citing this family (35)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9146830B2 (en) * | 2012-10-26 | 2015-09-29 | Jsmapreduce Corporation | Hybrid local/remote infrastructure for data processing with lightweight setup, powerful debuggability, controllability, integration, and productivity features |
WO2016032485A1 (en) * | 2014-08-28 | 2016-03-03 | Hewlett Packard Enterprise Development Lp | Identify server cluster by applying social group analysis |
US10103913B2 (en) * | 2015-03-25 | 2018-10-16 | Washington State University | Systems and methods for network routing in small-world network-on-chip devices |
US10860622B1 (en) | 2015-04-06 | 2020-12-08 | EMC IP Holding Company LLC | Scalable recursive computation for pattern identification across distributed data processing nodes |
US10515097B2 (en) * | 2015-04-06 | 2019-12-24 | EMC IP Holding Company LLC | Analytics platform for scalable distributed computations |
US10791063B1 (en) | 2015-04-06 | 2020-09-29 | EMC IP Holding Company LLC | Scalable edge computing using devices with limited resources |
US10404787B1 (en) | 2015-04-06 | 2019-09-03 | EMC IP Holding Company LLC | Scalable distributed data streaming computations across multiple data processing clusters |
US10348810B1 (en) * | 2015-04-06 | 2019-07-09 | EMC IP Holding Company LLC | Scalable distributed computations utilizing multiple distinct clouds |
US10425350B1 (en) | 2015-04-06 | 2019-09-24 | EMC IP Holding Company LLC | Distributed catalog service for data processing platform |
US10528875B1 (en) | 2015-04-06 | 2020-01-07 | EMC IP Holding Company LLC | Methods and apparatus implementing data model for disease monitoring, characterization and investigation |
US10015106B1 (en) | 2015-04-06 | 2018-07-03 | EMC IP Holding Company LLC | Multi-cluster distributed data processing platform |
US10511659B1 (en) * | 2015-04-06 | 2019-12-17 | EMC IP Holding Company LLC | Global benchmarking and statistical analysis at scale |
US10505863B1 (en) | 2015-04-06 | 2019-12-10 | EMC IP Holding Company LLC | Multi-framework distributed computation |
US10509684B2 (en) | 2015-04-06 | 2019-12-17 | EMC IP Holding Company LLC | Blockchain integration for scalable distributed computations |
US10541938B1 (en) | 2015-04-06 | 2020-01-21 | EMC IP Holding Company LLC | Integration of distributed data processing platform with one or more distinct supporting platforms |
US10496926B2 (en) | 2015-04-06 | 2019-12-03 | EMC IP Holding Company LLC | Analytics platform for scalable distributed computations |
US10331380B1 (en) | 2015-04-06 | 2019-06-25 | EMC IP Holding Company LLC | Scalable distributed in-memory computation utilizing batch mode extensions |
US10776404B2 (en) * | 2015-04-06 | 2020-09-15 | EMC IP Holding Company LLC | Scalable distributed computations utilizing multiple distinct computational frameworks |
US10366111B1 (en) * | 2015-04-06 | 2019-07-30 | EMC IP Holding Company LLC | Scalable distributed computations utilizing multiple distinct computational frameworks |
US10706970B1 (en) | 2015-04-06 | 2020-07-07 | EMC IP Holding Company LLC | Distributed data analytics |
US10541936B1 (en) * | 2015-04-06 | 2020-01-21 | EMC IP Holding Company LLC | Method and system for distributed analysis |
US10812341B1 (en) | 2015-04-06 | 2020-10-20 | EMC IP Holding Company LLC | Scalable recursive computation across distributed data processing nodes |
CN104765875B (en) * | 2015-04-24 | 2016-09-28 | 海南易建科技股份有限公司 | A kind of passenger's behavior data distributed approach and system |
US10656861B1 (en) | 2015-12-29 | 2020-05-19 | EMC IP Holding Company LLC | Scalable distributed in-memory computation |
US10374968B1 (en) | 2016-12-30 | 2019-08-06 | EMC IP Holding Company LLC | Data-driven automation mechanism for analytics workload distribution |
US11792284B1 (en) | 2017-11-27 | 2023-10-17 | Lacework, Inc. | Using data transformations for monitoring a cloud compute environment |
US10419469B1 (en) | 2017-11-27 | 2019-09-17 | Lacework Inc. | Graph-based user tracking and threat detection |
US20220232024A1 (en) | 2017-11-27 | 2022-07-21 | Lacework, Inc. | Detecting deviations from typical user behavior |
US11979422B1 (en) | 2017-11-27 | 2024-05-07 | Lacework, Inc. | Elastic privileges in a secure access service edge |
US11201955B1 (en) | 2019-12-23 | 2021-12-14 | Lacework Inc. | Agent networking in a containerized environment |
US11188571B1 (en) | 2019-12-23 | 2021-11-30 | Lacework Inc. | Pod communication graph |
US11256759B1 (en) | 2019-12-23 | 2022-02-22 | Lacework Inc. | Hierarchical graph analysis |
US10873592B1 (en) | 2019-12-23 | 2020-12-22 | Lacework Inc. | Kubernetes launch graph |
WO2021252203A1 (en) * | 2020-06-09 | 2021-12-16 | Liveramp, Inc. | Graph data structure edge profiling in mapreduce computational framework |
EP4191487A1 (en) * | 2021-12-06 | 2023-06-07 | Tata Consultancy Services Limited | Enhancing batch predictions by localizing jobs contributing to time deviation and generating fix recommendations |
Family Cites Families (18)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20030033582A1 (en) | 2001-05-09 | 2003-02-13 | Wavemarket, Inc. | Representations for estimating distance |
WO2005033884A2 (en) | 2003-10-03 | 2005-04-14 | Alphatech, Inc. | Methods and systems for determining a set of costs of routes in a network |
WO2005036839A2 (en) | 2003-10-03 | 2005-04-21 | Avici Systems, Inc. | Rapid alternate paths for network destinations |
US7827181B2 (en) | 2004-09-30 | 2010-11-02 | Microsoft Corporation | Click distance determination |
US7761448B2 (en) | 2004-09-30 | 2010-07-20 | Microsoft Corporation | System and method for ranking search results using click distance |
CN100558038C (en) | 2006-03-31 | 2009-11-04 | 国际商业机器公司 | Service logger and related system and method |
US20070297332A1 (en) | 2006-06-22 | 2007-12-27 | James Andrew Broberg | Distributed resource allocation in stream processing systems |
US8656017B2 (en) | 2007-05-16 | 2014-02-18 | Microsoft Corporation | Peer-to-peer collaboration system with edge routing |
EP2176754B1 (en) | 2007-06-26 | 2019-10-16 | Softlife Projects Limited Doing Business as Applied Cytometry Systems | System and method for optimizing data analysis |
US20090210489A1 (en) | 2008-02-18 | 2009-08-20 | Supratim Deb | Methods for peer-caching for faster lookups in peer-to-peer systems |
US8036220B2 (en) | 2008-06-20 | 2011-10-11 | Cisco Technology, Inc | Pre-dropping of a packet if its time-to-live (TTL) value is not large enough to reach a destination |
US8631094B1 (en) | 2008-08-08 | 2014-01-14 | Google Inc. | Distributed parallel determination of single and multiple source shortest paths in large directed graphs |
US20100083194A1 (en) | 2008-09-27 | 2010-04-01 | Yahoo! Inc. | System and method for finding connected components in a large-scale graph |
US8239847B2 (en) * | 2009-03-18 | 2012-08-07 | Microsoft Corporation | General distributed reduction for data parallel computing |
US8572575B2 (en) | 2009-09-14 | 2013-10-29 | Myspace Llc | Debugging a map reduce application on a cluster |
US8521782B2 (en) * | 2011-07-20 | 2013-08-27 | Salesforce.Com, Inc. | Methods and systems for processing large graphs using density-based processes using map-reduce |
US9053067B2 (en) * | 2011-09-30 | 2015-06-09 | International Business Machines Corporation | Distributed data scalable adaptive map-reduce framework |
US9235446B2 (en) * | 2012-06-22 | 2016-01-12 | Microsoft Technology Licensing, Llc | Parallel computing execution plan optimization |
-
2013
- 2013-12-30 US US14/143,894 patent/US9596295B2/en active Active
-
2014
- 2014-06-27 EP EP20202618.3A patent/EP3786798A1/en active Pending
- 2014-06-27 EP EP14744682.7A patent/EP3014444B1/en active Active
- 2014-06-27 WO PCT/US2014/044636 patent/WO2014210499A1/en active Application Filing
Also Published As
Publication number | Publication date |
---|---|
US9596295B2 (en) | 2017-03-14 |
WO2014210499A1 (en) | 2014-12-31 |
US20150006619A1 (en) | 2015-01-01 |
EP3014444B1 (en) | 2020-10-21 |
EP3786798A1 (en) | 2021-03-03 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US9596295B2 (en) | Computing connected components in large graphs | |
US10209908B2 (en) | Optimization of in-memory data grid placement | |
US9852230B2 (en) | Asynchronous message passing for large graph clustering | |
US8849756B2 (en) | Selecting data nodes in distributed storage system | |
US8713125B2 (en) | Method and system for scaling usage of a social based application on an online social network | |
US10257255B2 (en) | Hierarchical organization for scale-out cluster | |
CN107085539B (en) | cloud database system and dynamic cloud database resource adjustment method | |
CN110798517B (en) | Decentralized cluster load balancing method and system, mobile terminal and storage medium | |
CN111723073B (en) | Data storage processing method, device, processing system and storage medium | |
CN103929454A (en) | Load balancing storage method and system in cloud computing platform | |
CN103345508A (en) | Data storage method and system suitable for social network graph | |
US11221890B2 (en) | Systems and methods for dynamic partitioning in distributed environments | |
US9164800B2 (en) | Optimizing latencies in cloud systems by intelligent compute node placement | |
US20170371892A1 (en) | Systems and methods for dynamic partitioning in distributed environments | |
CN114567634B (en) | Method, system, storage medium and electronic device for calculating E-level map facing backward | |
US10915704B2 (en) | Intelligent reporting platform | |
CN112395282A (en) | Graph reconstruction method and device | |
Gangeshwari et al. | Hpcloud: A novel fault tolerant architectural model for hierarchical mapreduce | |
JP2024514467A (en) | Geographically distributed hybrid cloud cluster | |
Zuo et al. | Reliability-aware virtual data center embedding | |
CN116436978B (en) | Cloud computing-oriented memory allocation method, memory acquisition method, device and equipment | |
Grangia et al. | Balancing the storage in a deduplication cluster | |
US11442792B2 (en) | Systems and methods for dynamic partitioning in distributed environments | |
WO2017191489A1 (en) | Distributed network-aware service placement utilizing constrained kernighan-lin heuristic models | |
Odarchenko et al. | Evaluation of sdn network scalability with different management level structure |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PUAI | Public reference made under article 153(3) epc to a published international application that has entered the european phase |
Free format text: ORIGINAL CODE: 0009012 |
|
17P | Request for examination filed |
Effective date: 20151111 |
|
AK | Designated contracting states |
Kind code of ref document: A1Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
AX | Request for extension of the european patent |
Extension state: BA ME |
|
DAX | Request for extension of the european patent (deleted) | ||
RAP1 | Party data changed (applicant data changed or rights of an application transferred) |
Owner name: GOOGLE LLC |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: EXAMINATION IS IN PROGRESS |
|
17Q | First examination report despatched |
Effective date: 20180131 |
|
GRAP | Despatch of communication of intention to grant a patent |
Free format text: ORIGINAL CODE: EPIDOSNIGR1 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: GRANT OF PATENT IS INTENDED |
|
INTG | Intention to grant announced |
Effective date: 20200325 |
|
RIN1 | Information on inventor provided before grant (corrected) |
Inventor name: RASTOGI, VIBHORInventor name: LATTANZI, SILVIOInventor name: VASSILVITSKII, SERGEIInventor name: BANADAKI, SEYED VAHAB MIRROKNIInventor name: KIVERIS, RAIMONDAS |
|
GRAJ | Information related to disapproval of communication of intention to grant by the applicant or resumption of examination proceedings by the epo deleted |
Free format text: ORIGINAL CODE: EPIDOSDIGR1 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: EXAMINATION IS IN PROGRESS |
|
INTC | Intention to grant announced (deleted) | ||
GRAR | Information related to intention to grant a patent recorded |
Free format text: ORIGINAL CODE: EPIDOSNIGR71 |
|
GRAS | Grant fee paid |
Free format text: ORIGINAL CODE: EPIDOSNIGR3 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: GRANT OF PATENT IS INTENDED |
|
GRAA | (expected) grant |
Free format text: ORIGINAL CODE: 0009210 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: THE PATENT HAS BEEN GRANTED |
|
INTG | Intention to grant announced |
Effective date: 20200910 |
|
AK | Designated contracting states |
Kind code of ref document: B1Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
REG | Reference to a national code |
Ref country code: GBRef legal event code: FG4D |
|
REG | Reference to a national code |
Ref country code: CHRef legal event code: EP |
|
REG | Reference to a national code |
Ref country code: IERef legal event code: FG4D |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R096Ref document number: 602014071453Country of ref document: DE |
|
REG | Reference to a national code |
Ref country code: ATRef legal event code: REFRef document number: 1326519Country of ref document: ATKind code of ref document: TEffective date: 20201115 |
|
REG | Reference to a national code |
Ref country code: ATRef legal event code: MK05Ref document number: 1326519Country of ref document: ATKind code of ref document: TEffective date: 20201021 |
|
REG | Reference to a national code |
Ref country code: NLRef legal event code: MPEffective date: 20201021 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: GRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20210122Ref country code: NLFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201021Ref country code: PTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20210222Ref country code: NOFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20210121Ref country code: RSFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201021Ref country code: FIFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201021 |
|
REG | Reference to a national code |
Ref country code: LTRef legal event code: MG4D |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: PLFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201021Ref country code: ISFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20210221Ref country code: SEFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201021Ref country code: LVFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201021Ref country code: ESFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201021Ref country code: ATFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201021Ref country code: BGFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20210121 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: HRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201021 |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R097Ref document number: 602014071453Country of ref document: DE |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: SKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201021Ref country code: ROFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201021Ref country code: LTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201021Ref country code: EEFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201021Ref country code: CZFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201021Ref country code: SMFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201021 |
|
PLBE | No opposition filed within time limit |
Free format text: ORIGINAL CODE: 0009261 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: NO OPPOSITION FILED WITHIN TIME LIMIT |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: DKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201021 |
|
26N | No opposition filed |
Effective date: 20210722 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ALFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201021Ref country code: ITFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201021 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: SIFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201021 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: MCFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201021 |
|
REG | Reference to a national code |
Ref country code: CHRef legal event code: PL |
|
GBPC | Gb: european patent ceased through non-payment of renewal fee |
Effective date: 20210627 |
|
REG | Reference to a national code |
Ref country code: BERef legal event code: MMEffective date: 20210630 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: LUFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20210627 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: LIFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20210630Ref country code: GBFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20210627Ref country code: CHFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20210630 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ISFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20210221 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: BEFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20210630 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: HUFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMIT; INVALID AB INITIOEffective date: 20140627 |
|
P01 | Opt-out of the competence of the unified patent court (upc) registered |
Effective date: 20230505 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: CYFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201021 |
|
PGFP | Annual fee paid to national office [announced via postgrant information from national office to epo] |
Ref country code: IEPayment date: 20230627Year of fee payment: 10Ref country code: FRPayment date: 20230626Year of fee payment: 10Ref country code: DEPayment date: 20230626Year of fee payment: 10 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: MKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201021 |