US20230162010A1 - Synthesizing Zero-Loss Low-Power Approximate DNN Accelerators With Large-Scale Search - Google Patents
Synthesizing Zero-Loss Low-Power Approximate DNN Accelerators With Large-Scale Search Download PDFInfo
- Publication number
- US20230162010A1 US20230162010A1 US17/532,572 US202117532572A US2023162010A1 US 20230162010 A1 US20230162010 A1 US 20230162010A1 US 202117532572 A US202117532572 A US 202117532572A US 2023162010 A1 US2023162010 A1 US 2023162010A1
- Authority
- US
- United States
- Prior art keywords
- approximate
- systolic
- arrays
- approximate systolic
- array
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/06—Physical realisation, i.e. hardware implementation of neural networks, neurons or parts of neurons
- G06N3/063—Physical realisation, i.e. hardware implementation of neural networks, neurons or parts of neurons using electronic means
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F11/00—Error detection; Error correction; Monitoring
- G06F11/30—Monitoring
- G06F11/3003—Monitoring arrangements specially adapted to the computing system or computing system component being monitored
- G06F11/3024—Monitoring arrangements specially adapted to the computing system or computing system component being monitored where the computing system component is a central processing unit [CPU]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F11/00—Error detection; Error correction; Monitoring
- G06F11/30—Monitoring
- G06F11/3058—Monitoring arrangements for monitoring environmental properties or parameters of the computing system or of the computing system component, e.g. monitoring of power, currents, temperature, humidity, position, vibrations
- G06F11/3062—Monitoring arrangements for monitoring environmental properties or parameters of the computing system or of the computing system component, e.g. monitoring of power, currents, temperature, humidity, position, vibrations where the monitored property is the power consumption
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F11/00—Error detection; Error correction; Monitoring
- G06F11/30—Monitoring
- G06F11/34—Recording or statistical evaluation of computer activity, e.g. of down time, of input/output operation ; Recording or statistical evaluation of user activity, e.g. usability assessment
- G06F11/3409—Recording or statistical evaluation of computer activity, e.g. of down time, of input/output operation ; Recording or statistical evaluation of user activity, e.g. usability assessment for performance assessment
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F15/00—Digital computers in general; Data processing equipment in general
- G06F15/76—Architectures of general purpose stored program computers
- G06F15/80—Architectures of general purpose stored program computers comprising an array of processing units with common control, e.g. single instruction multiple data processors
- G06F15/8046—Systolic arrays
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2119/00—Details relating to the type or aim of the analysis or the optimisation
- G06F2119/06—Power analysis or power optimisation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F30/00—Computer-aided design [CAD]
- G06F30/30—Circuit design
- G06F30/32—Circuit design at the digital level
- G06F30/337—Design optimisation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F30/00—Computer-aided design [CAD]
- G06F30/30—Circuit design
- G06F30/39—Circuit design at the physical level
- G06F30/394—Routing
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
- G06N20/10—Machine learning using kernel methods, e.g. support vector machines [SVM]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N7/00—Computing arrangements based on specific mathematical models
- G06N7/01—Probabilistic graphical models, e.g. probabilistic networks
Definitions
- Scaling neural network models often increases the accuracy of the outputs provided by these neural network models. However, as the size of these neural network models continues to grow, the amount of energy consumed by inference accelerators implementing these deep neural network models also increases. Thus, even minor efficiency improvements in deep learning neural network models may drastically reduce global energy consumption by the inference accelerators implementing the deep learning models.
- Arithmetic units that perform basic mathematical operations when implementing a deep learning model are often responsible for most of the energy consumed by the inference accelerators.
- Full-precision floating-point calculations may be replaced with low-bit precision quantized operations to improve the power efficiency of these arithmetic units.
- the improved efficiency of low-bit quantization may result in degraded accuracy of outputs by the deep learning model.
- a model weight adjustment may be made to recover lost accuracy from approximation. However, this weight adjustment step may not be sufficient to avoid an accuracy degradation.
- the technology described herein is directed to generating an inference chip designs for production.
- One aspect of the disclosure is directed to a method for generating an inference chip.
- the method may include, generating, by one or more processors, a set of approximate systolic arrays; determining by the one or more processors, the performance of each approximate systolic array in the set of approximate systolic arrays processing a deep neural network (DNN); mapping, by the one or more processors, each layer in the DNN to an approximate systolic array in the set of approximate systolic arrays; and selecting, for inclusion in the inference chip design, by the one or more processors, a subset of the set of approximate systolic arrays based on the mapping and the performance of each approximate systolic array in the set of approximate systolic arrays.
- DNN deep neural network
- the system may include one or more processors; and memory storing instructions, the instructions, when executed by the one or more processors, causing the one or more processors to: generate a set of approximate systolic arrays; determine the performance of each approximate systolic array in the set of approximate systolic arrays processing a deep neural network (DNN); map each layer in the DNN to an approximate systolic array in the set of approximate systolic arrays; and select, for inclusion in the inference chip design, a subset of the set of approximate systolic arrays based on the mapping and the performance of each approximate systolic array in the set of approximate systolic arrays.
- DNN deep neural network
- a second set of approximate systolic arrays may be generated; the performance of each approximate systolic array in the second set of approximate systolic arrays processing the DNN may be determined; each layer in the DNN to an approximate systolic array in the second set of approximate systolic arrays may be mapped; and the subset of systolic arrays may be updated based on the mapping and the performance of each approximate systolic array in the second set of approximate systolic arrays, wherein the updated subset of approximate systolic arrays includes at least one systolic array from the second set of approximate systolic arrays.
- the set of approximate systolic arrays is generated to satisfy an architectural template, wherein the architectural template defines a number and/or a size of systolic arrays for the inference chip design.
- each approximate systolic array in the set of approximate systolic arrays are generated using predefined multiply-accumulate units (MACs), the predefined MACs being stored in a bank.
- MACs multiply-accumulate units
- each approximate systolic array in the set of approximate systolic arrays are generated using multiply-accumulate units (MACs), wherein the at least one of the MACs are generated based on predefined criteria.
- the predefined criteria includes one or more of power usage or accuracy.
- determining the performance of each approximate systolic array includes determining the power usage of each approximate systolic array. In some examples, determining the performance of each approximate systolic array further includes determining the accuracy of each approximate systolic array.
- mapping each layer in the DNN to an approximate systolic array in the set of approximate systolic arrays includes: for each layer in the DNN, configuring a router in the inference chip design, to a particular approximate systolic array in the subset of approximate systolic array based on the power usage and the accuracy of each approximate systolic array.
- the inference chip design includes at least one full-precision systolic array.
- FIG. 1 is a block diagram of an example framework for generating optimized inference chips, according to aspects of the disclosure.
- FIG. 2 of an optimized inference chip, according to aspects of the disclosure.
- FIG. 3 is a graph illustrating the principal components for various MACs, in accordance with aspects of the disclosure.
- FIG. 4 is a graph illustrating power consumption of a single MAC and systolic arrays, according to aspects of the disclosure.
- FIG. 5 is a graph illustrating the operating temperature of an optimized MXU including a collection of approximate systolic arrays relative to an MXU formed from exact systolic arrays, in accordance with aspects of the technology.
- FIG. 6 is a block diagram of an example computing environment implementing the framework, in accordance with aspects of the technology.
- the technology described herein is directed to designing approximate, low-power deep learning accelerator chips that have little to no accuracy loss when executing a deep learning model. Further, the approximate, low-power deep learning accelerator chips (“inference chips”) described herein do not require the deep learning model to be retrained to maintain accuracy. Instead, the inference chips are designed to process a deep learning model with the same, or nearly the same, accuracy as a chip constructed from “exact” units (e.g., arithmetic units capable of full-precision calculations).
- the inference chips may be designed using an architectural template.
- the template may generate a diverse set of efficient designs with a reconfigurable routing array to a bank of systolic arrays containing approximate units, such as approximate adders and/or approximate multipliers.
- this enables dynamic routing of error-tolerant layers within the deep learning model to more approximate units within the inference chip accelerator, yielding significant power savings.
- more error-sensitive layers within the deep learning model may be evaluated on more accurate approximate units and/or exact units to minimize or otherwise avoid introducing errors. Power savings may be achieved without reducing accuracy by co-designing the approximate units on an inference chip with the software mapping of layers within the deep learning model to systolic arrays.
- FIG. 1 illustrates a flow diagram showing the design process for creating a low-power, high-accuracy inference chip.
- the task of selecting approximate multiply-accumulate approximate units for an inference chip design occurs concurrently with mapping DNN layers onto the chip's systolic arrays, which include a collection of processing elements (PEs).
- PEs processing elements
- Each systolic array may include a two-dimensional array of PEs, with each PE containing one or more multiply-accumulate units (MACs). Additionally, each PE may include buffers for input operands and output results. Each systolic array may be globally activated, and parameter memory may be shared across all systolic arrays. Typically, each systolic array includes a single type of MAC, although some systolic arrays may include more than one type of MAC.
- the inference chip may be custom-tailored to a particular class of neural networks.
- the flow diagram illustrates an inference chip design framework 100 for generating systolic arrays of approximate and/or exact units that satisfy a given architectural template 101 and architecture parameters 115 input into an accelerator architecture generator 106 , that includes a systolic array generator 105 .
- the architectural template 101 of framework 100 may define requirements corresponding to the layout and configuration of an inference chip.
- the architectural template 101 may define the number of systolic arrays and the sizes of the systolic arrays for the inference chip.
- the architectural template may define that the inference chip is to have three systolic arrays, with two being constructed from approximate units and one being constructed from exact units.
- the architecture parameters 115 may include the composition of the heterogeneous systolic arrays, memory and system parameters, etc. For the initial process by the framework 100 , the architecture parameters may be “seeded.” Seeded parameters may be predefined or input each time the framework begins a process.
- the MACs that may be used to create the processing elements in the systolic arrays may be stored in a bank 103 of the framework.
- the MACs may have attributes that satisfy certain criteria, such as error tolerance and power usage. For instance, a first MAC may have attributes such as 1% error relative to an exact MAC and 99% power usage relative to an exact MAC. In another example, a second MAC may have a 3% error relative to an exact MAC and 90% power usage relative to an exact MAC.
- Bank 103 may store any number of configured MACs, including, in some instances, exact MACs that perform full-precision calculations (not approximate calculations). In some instances, MACs may be generated by the framework.
- the framework may generate a MAC to fit a particular use-case, such as a MAC with a particular error percentage and power usage.
- the framework may generate a MAC to fit a particular use-case not satisfied by a predefined MAC stored in the bank 103 .
- the systolic array generator 105 of the accelerator architecture generator 106 may generate a diverse set of systolic arrays using the MACs stored in bank 103 to satisfy the requirements of the given architectural template (and architecture parameters 115 ) while assuring the set of systolic arrays can accurately execute the DNN 107 .
- the accelerator architecture generator 106 may receive the architectural template 101 and architecture parameters 115 .
- the accelerator architecture generator 106 may then provide the systolic array generator with some or all of the received data.
- the accelerator architecture generator 106 may inform the systolic array generator 105 of the parameters of the design of the systolic arrays, such as the size of the systolic arrays, the composition (e.g. inference chip contains x % MACx, y % MACy,) etc.
- MACx and MACy may be selected from the bank 103 .
- the systolic array generator 105 may generate a set of systolic arrays according to the parameters received.
- the systolic array generator 105 of the framework may select MACs with varying accuracy and power attributes according to the received parameters.
- the systolic array generator 105 may generate code representative of the systolic arrays, such as in Verilog, C++, or other such language.
- Each layer of the DNN 107 may be input into the layerwise mapping 120 function of the framework 100 .
- the layerwise mapping 120 function may assign each layer of the DNN 107 to a particular systolic array of the set of systolic arrays generated by the systolic array generator 105 .
- the optimal approximation level for processing a DNN may change dramatically between different layers of a deep neural network.
- the layerwise mapping function 120 may map each DNN level based on its approximation tolerance.
- each generated set of systolic arrays may be tested, and an optimal set of systolic arrays selected for an inference chip design custom-tailored to execute the DNN 107 .
- DNNs are described herein, the design process for creating a low-power, high-accuracy inference chips may be performed for other types of neural networks.
- each set of generated systolic arrays may be tested for accuracy using fast accuracy estimation 109 , described herein.
- the power and performance estimations (collectively “performance estimations) for each set of generated systolic arrays may be determined, as shown by block 111 , and as described herein.
- the accuracy and performance estimations may be input into an objective function 112 , described herein to determine an optimized combination of systolic arrays.
- the objective function 112 may evaluates how good the mapping of each DNN layer is from an accuracy and performance perspective.
- the architecture search algorithm 113 may then take the results of the objective function (as well as other previous results of the objective function) and determine whether improvements can be made with different sets of systolic arrays.
- the architecture search algorithm 113 may receive the current objective function result for the current parameters and past objective function results for previous parameters. Based on these parameters, the architecture search algorithm 113 may determine the next set of parameters to try (illustrated by the arrow from 113 to 115 in FIG. 1 .)
- the framework 100 aims to find the optimal value of the objective function through an iterative search of the parameter space to yield the optimal configuration.
- This process may repeat for a certain number of times, or until all possible combinations of systolic arrays, layerwise mapping, etc., are completed.
- the most efficient and accurate set of systolic arrays may then be selected.
- the framework may output design plans for constructing an inference chip according to the selected set of systolic arrays.
- the optimized combination of systolic arrays may make up a multi-tile matrix multiplier unit (MXU).
- MXU multi-tile matrix multiplier unit
- Each “tile” of the multi-tile MXU may include a systolic array with particular accuracy and power attribute.
- FIG. 2 illustrates a multi-tile MXU 205 that includes three systolic arrays, including Array A 207 , Array B 209 , and Array Z, 211 .
- MXU 205 is shown as including only three arrays, an MXU may include any number of systolic arrays.
- the activation buffer 201 may receive the results of a previous layer of the DNN.
- FIG. 2 shows the current DNN layer being processed by Array Z 211 .
- a full 2 N+N look-up table for an N-bit multiplier may not scale to wide bit-widths. For instance, with 16-bit inputs, the look-up table would exceed 16 gigabytes. Given the large size, the use of GPU acceleration would be precluded.
- FIG. 3 illustrates the principal components for various MACs, including a MAC with a low error level, a MAC with a mid-error level, and a MAC with a high error level relative to a normalized eigenvalue.
- minimal errors are introduced with compression.
- LUT the LUT is precomputed in memory.
- a low-rank eigen decomposition may then be computed.
- An approximation error matrix ⁇ i,j ⁇ tilde over (m) ⁇ (i,j) ⁇ i ⁇ j where ⁇ tilde over (m) ⁇ is an approximate multiplier, and i and j are indexes of the matrix.
- a truncated singular value decomposition may be computed with k ⁇ 2 N using the following formula (1):
- ⁇ is the error matrix
- ⁇ i is the ith eigenvalue of the error matrix
- u i and v i are the ith left-singular vector and right-singular vector of the error matrix, respectively.
- the parameter k ⁇ 2 N control the amount of approximation, with a smaller k increasing the approximation but reducing the memory usage for storing the error matrix. With a small k ⁇ 50, the total memory consumption may be under 20 MB.
- the result of the approximate multiplication of i ⁇ j may be recomputed.
- All results may be evaluated using a classification dataset, such as a large scale image dataset, such as the ImageNet 2012 dataset.
- a small sample such as a 10% sample, of the full validation set may be evaluated.
- a sample of 5000 images may be used.
- Ranking models on a sampled validation set typically correlates with performance on the full dataset.
- semiconductor design and testing software may be used.
- the testing may be capable of evaluation of semiconductor designs at sub-10 nm processes.
- a single clock domain may be applied across each systolic array, with the clock frequency being dictated by the slowest systolic array.
- the clock frequency is typically based on the “exact” systolic array, other “approximate” systolic arrays may be slower and therefore be used to set the clock frequency.
- this example describes a single clock frequency, some or all systolic arrays in a design of an MXU may be tested at different frequencies.
- the different MXU variants are constrained to the slowest clock frequency—this enables some power and area savings as the synthesis tool is able to select smaller gate sizes for the approximate MXU variants which have higher intrinsic performance.
- DNN accelerators are typically thermal design power (TDP) limited, rather than delay limited.
- TDP thermal design power
- FIG. 5 illustrates that an optimized approximate MXU operates at a significantly reduced temperature relative to an MXU with only an “exact” systolic array.
- the performance of a particular design also depends on how the convolution loop nest is mapped onto the array. Mapping DNN layers to particular systolic arrays ensures that generated accelerators match what would happen post-synthesis.
- O is “Big O” notation that expresses the complexity of the search space. Additionally, there may be a large number of workloads, with up to a 2 268 search space, or more or less. In this case, the search space defines the ways an approximate MAC may be designed. The goal of the optimization algorithm is to find optimized designs within this search space. Therefore, random search will not perform well.
- Bayesian optimization with Gaussian Process (GP) bandits may be leveraged to efficiently discover high-accuracy yet energy-efficient configurations of cross-layer approximate circuits. This approach improves the sample efficiency of black-box optimization by modelling the unknown reward function f: x ⁇ y with a Gaussian Process.
- the decision variable Zi represents a one-hot vector to denote which of the K approximate circuits are mapped to layer i and where j is an index.
- the objective (2a) models the total energy consumption to evaluate a single forward pass where q i ⁇ R + K represents a vector containing the energy to evaluate layer i for each of the K approximate multipliers.
- Constraints (2d) and (2e) ensure that Zi is one-hot and is binary/integral.
- Constraint (2b) defines a minimum accuracy target for the neural network.
- Constraint (2c) constrains the area of the final chip to avoid degenerate solutions with many similar redundant multipliers.
- the accuracy oracle “ACC” models the effect of cross-layer interactions from approximations. Given a particular assignment of approximate multipliers to layers, ACC calculates the expected accuracy of the model over a specific dataset. As errors introduced in one layer are compounded through subsequent layers, the accuracy oracle is exceptionally challenging to model. Thus, the accuracy of a model may be made over the validation set.
- Bayesian optimization methods typically fail when applied to the above optimization problem. This is because Bayesian optimization methods demonstrate slow convergence with a performance similar to random search. Further, Bayesian optimization struggles with high dimensional states, discrete structures, and constrained search spaces. Therefore the above optimization problem may be reformulated such that known Bayesian optimization tools may be used.
- Bayesian optimization frameworks typically support discrete search spaces by embedding them in a real-valued box.
- this embedding is sample-inefficient as it does not consider the relation between different categorical variables. For example, this solution has the challenge of instability due to quantization error from rounding continuous predicted variables to the nearest feasible points.
- an estimate of per-layer accuracy degradation from single layer approximation may be used to compute an ordered set representing the relative ranking of each approximate multiplier.
- the ordering may be defined as the profiled end-to-end accuracy for approximating a single layer k with a particular multiplier.
- direct search with unrounded accuracy results in an unstable relaxed optimization problem. This results from two approximate multipliers which achieve similar accuracy, but have very different power consumption.
- linear order of multipliers may be relaxed to a partially ordered set where ties within a fixed threshold of accuracy are considered incomparable. A completed linear order may then be resolved by eliminating the least efficient multiplier in each pair of incomparable multipliers with similar accuracy.
- This procedure derives a linear order of multipliers for each of the N layers in the neural network.
- a min-max scaling may be applied to the resulting top-1 accuracy (i.e., high probability) for each multiplier from single layer approximation calibration.
- the following cost optimization objective may be defined.
- a step-wise cost function Q i R ⁇ R may be defined to map a real-valued choice of an approximate multiplier (from 0 to 1) to the energy-consumption for the closest layer, rounding down.
- ⁇ i 1 N Q i ( Z i ) + ⁇ 1 ⁇ B ⁇ ( ⁇ , ACC ⁇ ( Z ) ) + ⁇ 2 ⁇ B ⁇ ( AREA ( Z ) , ⁇ )
- a target accuracy ⁇ may be 0.68, or more or less and weight ⁇ 2 may be 8, or more or less.
- FIG. 6 is a block diagram of an example computing environment 600 implementing an example framework as illustrated in FIG. 1 .
- the framework may be implemented on one or more devices having one or more processors in one or more locations, such as in server computing device 615 .
- User computing device 612 and the server computing device 615 can be communicatively coupled to one or more storage devices 630 over a network 660 .
- the storage device(s) 630 can be a combination of volatile and non-volatile memory, and can be at the same or different physical locations than the computing devices 612 , 615 .
- the storage device(s) 630 can include any type of non-transitory computer readable medium capable of storing information, such as a hard-drive, solid state drive, tape drive, optical storage, memory card, ROM, RAM, DVD, CD-ROM, write-capable, and read-only memories.
- the server computing device 615 can include one or more processors 613 and memory 614 .
- the memory 614 can store information accessible by the processor(s) 613 , including instructions 621 that can be executed by the processor(s) 613 .
- the memory 614 can also include data 623 that can be retrieved, manipulated or stored by the processor(s) 613 .
- the memory 614 can be a type of non-transitory computer readable medium capable of storing information accessible by the processor(s) 613 , such as volatile and non-volatile memory.
- the processor(s) 613 can include one or more central processing units (CPUs), graphic processing units (GPUs), field-programmable gate arrays (FPGAs), and/or application-specific integrated circuits (ASICs), such as tensor processing units (TPUs).
- CPUs central processing units
- GPUs graphic processing units
- FPGAs field-programmable gate arrays
- ASICs application-specific integrated circuits
- TPUs tensor processing units
- the instructions 621 can include one or more instructions that when executed by the processor(s) 613 , causes the one or more processors to perform actions defined by the instructions.
- the instructions 621 can be stored in object code format for direct processing by the processor(s) 613 , or in other formats including interpretable scripts or collections of independent source code modules that are interpreted on demand or compiled in advance.
- the instructions 621 can include instructions for implementing the framework 100 consistent with aspects of this disclosure.
- the framework 100 can be executed using the processor(s) 613 , and/or using other processors remotely located from the server computing device 615 .
- the data 623 can be retrieved, stored, or modified by the processor(s) 613 in accordance with the instructions 621 , such as the bank of approximate MACs 103 , architectural template 101 , architecture parameters 115 , etc.
- the data 623 can be stored in computer registers, in a relational or non-relational database as a table having a plurality of different fields and records, or as JSON, YAML, proto, or XML documents.
- the data 623 can also be formatted in a computer-readable format such as, but not limited to, binary values, ASCII or Unicode.
- the data 623 can include information sufficient to identify relevant information, such as numbers, descriptive text, proprietary codes, pointers, references to data stored in other memories, including other network locations, or information that is used by a function to calculate relevant data.
- the user computing device 612 can also be configured similar to the server computing device 615 , with one or more processors 616 , memory 617 , instructions 618 , and data 619 .
- the user computing device 612 can also include a user output 626 , and a user input 624 .
- the user input 624 can include any appropriate mechanism or technique for receiving input from a user, such as keyboard, mouse, mechanical actuators, soft actuators, touchscreens, microphones, and sensors.
- the server computing device 615 can be configured to transmit data to the user computing device 612 , and the user computing device 612 can be configured to display at least a portion of the received data on a display implemented as part of the user output 626 .
- the user output 626 can also be used for displaying an interface between the user computing device 612 and the server computing device 615 .
- the user output 626 can alternatively or additionally include one or more speakers, transducers or other audio outputs, a haptic interface or other tactile feedback that provides non-visual and non-audible information to the platform user of the user computing device 612 .
- FIG. 6 illustrates the processors 613 , 616 and the memories 614 , 617 as being within the computing devices 615 , 612
- components described in this specification, including the processors 613 , 616 and the memories 614 , 617 can include multiple processors and memories that can operate in different physical locations and not within the same computing device.
- some of the instructions 621 , 618 and the data 623 , 619 can be stored on a removable SD card and others within a read-only computer chip. Some or all of the instructions and data can be stored in a location physically remote from, yet still accessible by, the processors 613 , 616 .
- the processors 613 , 616 can include a collection of processors that can perform concurrent and/or sequential operation.
- the computing devices 615 , 612 can each include one or more internal clocks providing timing information, which can be used for time measurement for operations and programs run by the computing devices 615 , 612 .
- the server computing device 615 can be configured to receive requests to process data from the user computing device 612 .
- the environment 600 can be part of a computing platform configured to provide a variety of services to users, through various user interfaces and/or APIs exposing the platform services.
- One or more services can be a machine learning framework or a set of tools for generating neural networks or other machine learning models according to a specified task and training data.
- the user computing device 612 may receive and transmit data specifying target computing resources to be allocated for executing a neural network trained to perform a particular neural network task.
- the devices 612 , 615 can be capable of direct and indirect communication over the network 660 .
- the devices 615 , 612 can set up listening sockets that may accept an initiating connection for sending and receiving information.
- the network 660 itself can include various configurations and protocols including the Internet, World Wide Web, intranets, virtual private networks, wide area networks, local networks, and private networks using communication protocols proprietary to one or more companies.
- the network 660 can support a variety of short- and long-range connections.
- the short- and long-range connections may be made over different bandwidths, such as 2.402 GHz to 2.480 GHz (commonly associated with the Bluetooth® standard), 2.4 GHz and 5 GHz (commonly associated with the Wi-Fi® communication protocol); or with a variety of communication standards, such as the LTE® standard for wireless broadband communication.
- the network 660 in addition or alternatively, can also support wired connections between the devices 612 , 615 , including over various types of Ethernet connection.
- FIG. 6 Although a single server computing device 615 and user computing device 612 are shown in FIG. 6 , it is understood that the aspects of the disclosure can be implemented according to a variety of different configurations and quantities of computing devices, including in paradigms for sequential or parallel processing, or over a distributed network of multiple devices. In some implementations, aspects of the disclosure can be performed on a single device, and any combination thereof.
- aspects of this disclosure can be implemented in digital circuits, computer-readable storage media, as one or more computer programs, or a combination of one or more of the foregoing.
- the computer-readable storage media can be non-transitory, e.g., as one or more instructions executable by a cloud computing platform and stored on a tangible storage device.
- the phrase “configured to” is used in different contexts related to computer systems, hardware, or part of a computer program, engine, or module.
- a system is said to be configured to perform one or more operations, this means that the system has appropriate software, firmware, and/or hardware installed on the system that, when in operation, causes the system to perform the one or more operations.
- some hardware is said to be configured to perform one or more operations, this means that the hardware includes one or more circuits that, when in operation, receive input and generate output according to the input and corresponding to the one or more operations.
- a computer program, engine, or module is said to be configured to perform one or more operations, this means that the computer program includes one or more program instructions, that when executed by one or more computers, causes the one or more computers to perform the one or more operations.
Abstract
Systems and methods are provided for designing approximate, low-power deep learning accelerator chips that have little to no accuracy loss when executing a deep learning model. A set of approximate systolic arrays may be generated. The performance of each approximate systolic array in the set of approximate systolic arrays processing a deep neural network (DNN) may be determined. Each layer in the DNN may be mapped to an approximate systolic array in the set of approximate systolic arrays. A subset of the set of approximate systolic arrays may be selected for inclusion in the inference chip design based on the mapping and the performance of each approximate systolic array in the set of approximate systolic arrays.
Description
- Scaling neural network models often increases the accuracy of the outputs provided by these neural network models. However, as the size of these neural network models continues to grow, the amount of energy consumed by inference accelerators implementing these deep neural network models also increases. Thus, even minor efficiency improvements in deep learning neural network models may drastically reduce global energy consumption by the inference accelerators implementing the deep learning models.
- Arithmetic units that perform basic mathematical operations when implementing a deep learning model are often responsible for most of the energy consumed by the inference accelerators. Full-precision floating-point calculations may be replaced with low-bit precision quantized operations to improve the power efficiency of these arithmetic units. However, the improved efficiency of low-bit quantization may result in degraded accuracy of outputs by the deep learning model. In some instances, a model weight adjustment may be made to recover lost accuracy from approximation. However, this weight adjustment step may not be sufficient to avoid an accuracy degradation.
- Further efforts to reduce energy consumption by inference accelerators implementing deep learning models include using approximate units that are more power-efficient than quantized operators. These approximate units may implement approximate operators (e.g., multipliers and adders) that tailor approximations to a numerical distribution observed during neural network evaluation. By tailoring the approximations to a particular application during evaluation, better power to accuracy trade-offs may be incurred than when quantization is uniformly approximated across all inputs. However, errors introduced by approximate units may be compounded as the deep learning model proceeds through each subsequent neural network layer, leading to inaccurate outputs.
- The technology described herein is directed to generating an inference chip designs for production. One aspect of the disclosure is directed to a method for generating an inference chip. The method may include, generating, by one or more processors, a set of approximate systolic arrays; determining by the one or more processors, the performance of each approximate systolic array in the set of approximate systolic arrays processing a deep neural network (DNN); mapping, by the one or more processors, each layer in the DNN to an approximate systolic array in the set of approximate systolic arrays; and selecting, for inclusion in the inference chip design, by the one or more processors, a subset of the set of approximate systolic arrays based on the mapping and the performance of each approximate systolic array in the set of approximate systolic arrays.
- Another aspect of the disclosure is directed to a system for generating an inference chip design for production. The system may include one or more processors; and memory storing instructions, the instructions, when executed by the one or more processors, causing the one or more processors to: generate a set of approximate systolic arrays; determine the performance of each approximate systolic array in the set of approximate systolic arrays processing a deep neural network (DNN); map each layer in the DNN to an approximate systolic array in the set of approximate systolic arrays; and select, for inclusion in the inference chip design, a subset of the set of approximate systolic arrays based on the mapping and the performance of each approximate systolic array in the set of approximate systolic arrays.
- In some instances, a second set of approximate systolic arrays may be generated; the performance of each approximate systolic array in the second set of approximate systolic arrays processing the DNN may be determined; each layer in the DNN to an approximate systolic array in the second set of approximate systolic arrays may be mapped; and the subset of systolic arrays may be updated based on the mapping and the performance of each approximate systolic array in the second set of approximate systolic arrays, wherein the updated subset of approximate systolic arrays includes at least one systolic array from the second set of approximate systolic arrays.
- In some instances, the set of approximate systolic arrays is generated to satisfy an architectural template, wherein the architectural template defines a number and/or a size of systolic arrays for the inference chip design. In some examples, each approximate systolic array in the set of approximate systolic arrays are generated using predefined multiply-accumulate units (MACs), the predefined MACs being stored in a bank.
- In some instances, each approximate systolic array in the set of approximate systolic arrays are generated using multiply-accumulate units (MACs), wherein the at least one of the MACs are generated based on predefined criteria. In some examples, the predefined criteria includes one or more of power usage or accuracy.
- In some instances, determining the performance of each approximate systolic array includes determining the power usage of each approximate systolic array. In some examples, determining the performance of each approximate systolic array further includes determining the accuracy of each approximate systolic array.
- In some instances, mapping each layer in the DNN to an approximate systolic array in the set of approximate systolic arrays includes: for each layer in the DNN, configuring a router in the inference chip design, to a particular approximate systolic array in the subset of approximate systolic array based on the power usage and the accuracy of each approximate systolic array.
- In some instances, the inference chip design includes at least one full-precision systolic array.
-
FIG. 1 is a block diagram of an example framework for generating optimized inference chips, according to aspects of the disclosure. -
FIG. 2 of an optimized inference chip, according to aspects of the disclosure. -
FIG. 3 is a graph illustrating the principal components for various MACs, in accordance with aspects of the disclosure. -
FIG. 4 is a graph illustrating power consumption of a single MAC and systolic arrays, according to aspects of the disclosure. -
FIG. 5 is a graph illustrating the operating temperature of an optimized MXU including a collection of approximate systolic arrays relative to an MXU formed from exact systolic arrays, in accordance with aspects of the technology. -
FIG. 6 is a block diagram of an example computing environment implementing the framework, in accordance with aspects of the technology. - The technology described herein is directed to designing approximate, low-power deep learning accelerator chips that have little to no accuracy loss when executing a deep learning model. Further, the approximate, low-power deep learning accelerator chips (“inference chips”) described herein do not require the deep learning model to be retrained to maintain accuracy. Instead, the inference chips are designed to process a deep learning model with the same, or nearly the same, accuracy as a chip constructed from “exact” units (e.g., arithmetic units capable of full-precision calculations).
- The inference chips may be designed using an architectural template. The template may generate a diverse set of efficient designs with a reconfigurable routing array to a bank of systolic arrays containing approximate units, such as approximate adders and/or approximate multipliers. At runtime, this enables dynamic routing of error-tolerant layers within the deep learning model to more approximate units within the inference chip accelerator, yielding significant power savings. In addition, more error-sensitive layers within the deep learning model may be evaluated on more accurate approximate units and/or exact units to minimize or otherwise avoid introducing errors. Power savings may be achieved without reducing accuracy by co-designing the approximate units on an inference chip with the software mapping of layers within the deep learning model to systolic arrays.
-
FIG. 1 illustrates a flow diagram showing the design process for creating a low-power, high-accuracy inference chip. As described herein, the task of selecting approximate multiply-accumulate approximate units for an inference chip design occurs concurrently with mapping DNN layers onto the chip's systolic arrays, which include a collection of processing elements (PEs). - Each systolic array may include a two-dimensional array of PEs, with each PE containing one or more multiply-accumulate units (MACs). Additionally, each PE may include buffers for input operands and output results. Each systolic array may be globally activated, and parameter memory may be shared across all systolic arrays. Typically, each systolic array includes a single type of MAC, although some systolic arrays may include more than one type of MAC. Moreover, although MACs are discussed in the examples described herein, other arithmetic units may also be used (e.g., adders.) By designing the hardware and mapping DNN layers to the systolic arrays concurrently, the inference chip may be custom-tailored to a particular class of neural networks.
- The flow diagram illustrates an inference
chip design framework 100 for generating systolic arrays of approximate and/or exact units that satisfy a givenarchitectural template 101 andarchitecture parameters 115 input into anaccelerator architecture generator 106, that includes asystolic array generator 105. Thearchitectural template 101 offramework 100 may define requirements corresponding to the layout and configuration of an inference chip. For instance, thearchitectural template 101 may define the number of systolic arrays and the sizes of the systolic arrays for the inference chip. For example, the architectural template may define that the inference chip is to have three systolic arrays, with two being constructed from approximate units and one being constructed from exact units. In other templates, other amounts and types of arrays may be defined, which approximate units (e.g., MACs) to use, etc. Thearchitecture parameters 115 may include the composition of the heterogeneous systolic arrays, memory and system parameters, etc. For the initial process by theframework 100, the architecture parameters may be “seeded.” Seeded parameters may be predefined or input each time the framework begins a process. - The MACs that may be used to create the processing elements in the systolic arrays may be stored in a
bank 103 of the framework. The MACs may have attributes that satisfy certain criteria, such as error tolerance and power usage. For instance, a first MAC may have attributes such as 1% error relative to an exact MAC and 99% power usage relative to an exact MAC. In another example, a second MAC may have a 3% error relative to an exact MAC and 90% power usage relative to an exact MAC.Bank 103 may store any number of configured MACs, including, in some instances, exact MACs that perform full-precision calculations (not approximate calculations). In some instances, MACs may be generated by the framework. In this regard, the framework may generate a MAC to fit a particular use-case, such as a MAC with a particular error percentage and power usage. During generation of the systolic arrays, the framework may generate a MAC to fit a particular use-case not satisfied by a predefined MAC stored in thebank 103. - The
systolic array generator 105 of theaccelerator architecture generator 106 may generate a diverse set of systolic arrays using the MACs stored inbank 103 to satisfy the requirements of the given architectural template (and architecture parameters 115) while assuring the set of systolic arrays can accurately execute theDNN 107. In this regard, theaccelerator architecture generator 106 may receive thearchitectural template 101 andarchitecture parameters 115. Theaccelerator architecture generator 106 may then provide the systolic array generator with some or all of the received data. For example, theaccelerator architecture generator 106 may inform thesystolic array generator 105 of the parameters of the design of the systolic arrays, such as the size of the systolic arrays, the composition (e.g. inference chip contains x % MACx, y % MACy,) etc. MACx and MACy may be selected from thebank 103. - The
systolic array generator 105 may generate a set of systolic arrays according to the parameters received. In this regard, thesystolic array generator 105 of the framework may select MACs with varying accuracy and power attributes according to the received parameters. Thesystolic array generator 105 may generate code representative of the systolic arrays, such as in Verilog, C++, or other such language. - Each layer of the
DNN 107 may be input into thelayerwise mapping 120 function of theframework 100. Thelayerwise mapping 120 function may assign each layer of theDNN 107 to a particular systolic array of the set of systolic arrays generated by thesystolic array generator 105. As described herein, the optimal approximation level for processing a DNN may change dramatically between different layers of a deep neural network. Thelayerwise mapping function 120 may map each DNN level based on its approximation tolerance. - As described herein, the power and accuracy of each generated set of systolic arrays may be tested, and an optimal set of systolic arrays selected for an inference chip design custom-tailored to execute the
DNN 107. Although DNNs are described herein, the design process for creating a low-power, high-accuracy inference chips may be performed for other types of neural networks. For instance, each set of generated systolic arrays may be tested for accuracy usingfast accuracy estimation 109, described herein. Additionally, the power and performance estimations (collectively “performance estimations) for each set of generated systolic arrays may be determined, as shown byblock 111, and as described herein. - The accuracy and performance estimations may be input into an
objective function 112, described herein to determine an optimized combination of systolic arrays. In this regard, theobjective function 112 may evaluates how good the mapping of each DNN layer is from an accuracy and performance perspective. - The
architecture search algorithm 113 may then take the results of the objective function (as well as other previous results of the objective function) and determine whether improvements can be made with different sets of systolic arrays. In this regard, thearchitecture search algorithm 113 may receive the current objective function result for the current parameters and past objective function results for previous parameters. Based on these parameters, thearchitecture search algorithm 113 may determine the next set of parameters to try (illustrated by the arrow from 113 to 115 inFIG. 1 .) Theframework 100 aims to find the optimal value of the objective function through an iterative search of the parameter space to yield the optimal configuration. - This process may repeat for a certain number of times, or until all possible combinations of systolic arrays, layerwise mapping, etc., are completed. The most efficient and accurate set of systolic arrays may then be selected. Although not illustrated, the framework may output design plans for constructing an inference chip according to the selected set of systolic arrays.
- The optimized combination of systolic arrays may make up a multi-tile matrix multiplier unit (MXU). Each “tile” of the multi-tile MXU may include a systolic array with particular accuracy and power attribute. For example,
FIG. 2 illustrates amulti-tile MXU 205 that includes three systolic arrays, includingArray A 207,Array B 209, and Array Z, 211. AlthoughMXU 205 is shown as including only three arrays, an MXU may include any number of systolic arrays. As further shown inFIG. 2 , theactivation buffer 201 may receive the results of a previous layer of the DNN. These previous results may then be routed by learnedrouter 203 to a systolic array for processing the next layer of the DNN. The routing of the layers may be pre-programmed such that the router directs particular layers of the DNN to predefined systolic arrays for processing. For example,FIG. 2 shows the current DNN layer being processed byArray Z 211. - Both training and inference of large-scale deep neural networks may entail trillions of arithmetic operations. With exact arithmetic and standard datatypes, this evaluation can be parallelized on high-throughput GPUs. However, for simulating inexact arithmetic with approximate MACs, direct evaluation of the inexact arithmetic operations may be difficult as existing hardware does not support such operations. For instance, direct evaluation of an approximate MAC with a circuit simulator, such as the Verilator circuit simulator, performing a single exact 8-bit multiplication may take 3.75±0.95 microseconds on a high-performance server. Thus, evaluation of a large neural network, such as ResNet-50, which is 50 layers deep, may take around 4.2 hours at 4 GFLOPs per 224×224 frame. For the entire ImageNet validation set, evaluating a single approximate multiplier would take around 23 years.
- Furthermore, while calls may be cached, a full 2N+N look-up table for an N-bit multiplier may not scale to wide bit-widths. For instance, with 16-bit inputs, the look-up table would exceed 16 gigabytes. Given the large size, the use of GPU acceleration would be precluded.
- To address these issues, the look-up table may be compressed without meaningful error using matrix decomposition.
FIG. 3 illustrates the principal components for various MACs, including a MAC with a low error level, a MAC with a mid-error level, and a MAC with a high error level relative to a normalized eigenvalue. As can be seen, minimal errors are introduced with compression. To compress the look-up table (LUT), the LUT is precomputed in memory. A low-rank eigen decomposition may then be computed. An approximation error matrix ϵi,j={tilde over (m)}(i,j)−i×j where {tilde over (m)} is an approximate multiplier, and i and j are indexes of the matrix. - In order to save memory when storing the N-bit error matrix ϵ∈R2N+N a truncated singular value decomposition may be computed with k<<2N using the following formula (1):
-
ϵ≈Σi=1 kσiμiν*i (1) - with total memory consumption of O(nk), down from O(n2). Within formula (1), Σ is the error matrix, σi is the ith eigenvalue of the error matrix, and ui and vi are the ith left-singular vector and right-singular vector of the error matrix, respectively. The parameter k<<2N control the amount of approximation, with a smaller k increasing the approximation but reducing the memory usage for storing the error matrix. With a small k≤50, the total memory consumption may be under 20 MB. During the evaluation, the result of the approximate multiplication of i×j may be recomputed.
- All results may be evaluated using a classification dataset, such as a large scale image dataset, such as the ImageNet 2012 dataset. To increase the evaluation speed of an end-to-end model on a target dataset, a small sample, such as a 10% sample, of the full validation set may be evaluated. For example, when using ImageNet 2012, a sample of 5000 images may be used. Ranking models on a sampled validation set typically correlates with performance on the full dataset.
- Overall, the optimization described herein may result in a 7200× speedup over direct circuit simulation in Verilator. This strategy also makes GPU evaluation feasible with future potential for automatic retraining. Compressing look-up tables with a low-rank decomposition enables the model evaluation described herein to scale to circuits at wider bit-widths.
- Moreover, the approaches described herein are complementary with other known quantization methods. For instance, dynamic range post-training quantization where weights are statically quantized to eight-bits prior to inference may be performed. During inference, activations may be scaled to the uint8 range of [0; 255] and then quantized, although other ranges may be used. Dequantization may then be performed using the following formula (2):
-
q 3 (i,k) =Z 3 +MNZ 1 Z 2 −MZ 1Σj=1 N q 2 (j,k) −MZ 2Σj=1 N q 1 (i,j) +MΣ j=1 N q 1 (i,j) q 2 (j,k) (2) - where q1 represents the weight matrix, q2 represents the activation matrix, and Z represents respective zero points. Higher power-savings could be accomplished with a more advanced quantization method utilizing quantization-aware training.
- When considering performance metrics, such as power consumption, prior work considers the performance of a single MAC. However, constructing a systolic array from low power MACs may not result in a lower power systolic array. Such cases are illustrated in
FIG. 4 , where low power MACs, when implemented in a systolic array, may use higher power levels. This may be due to the impact of MAC area on interconnect power. In this regard, as area A increases, interconnect wire power must increase by O(√{square root over (A)}). - To evaluate performance of systolic arrays across an MXU, semiconductor design and testing software may be used. The testing may be capable of evaluation of semiconductor designs at sub-10 nm processes. For evaluation, a single clock domain may be applied across each systolic array, with the clock frequency being dictated by the slowest systolic array. In this regard, the clock frequency is typically based on the “exact” systolic array, other “approximate” systolic arrays may be slower and therefore be used to set the clock frequency. Further, although this example describes a single clock frequency, some or all systolic arrays in a design of an MXU may be tested at different frequencies. During synthesis, the different MXU variants are constrained to the slowest clock frequency—this enables some power and area savings as the synthesis tool is able to select smaller gate sizes for the approximate MXU variants which have higher intrinsic performance.
- In practice, DNN accelerators are typically thermal design power (TDP) limited, rather than delay limited. Thus, further gains may be possible by overclocking an inference chip using the thermal savings from the approximate systolic arrays. For example,
FIG. 5 illustrates that an optimized approximate MXU operates at a significantly reduced temperature relative to an MXU with only an “exact” systolic array. - The performance of a particular design also depends on how the convolution loop nest is mapped onto the array. Mapping DNN layers to particular systolic arrays ensures that generated accelerators match what would happen post-synthesis.
- In order to preserve high end-to-end task accuracy, consideration of which layers of the deep learning model are approximation tolerant should be made. In this regard, the optimal approximation level may change dramatically between different layers of a deep neural network. However, jointly considering the task of selecting approximate systolic arrays for a chip design concurrently with the mapping of layers of a DNN onto the systolic arrays is challenging. In this regard, each of these two subproblems—selecting approximate units for the chip design and mapping the layers of the DNN onto the approximate units—are each challenging combinatorial optimization problems. Together, they represent a O(KN) search space with K candidate MAC designs and N deep neural network layers to map. O is “Big O” notation that expresses the complexity of the search space. Additionally, there may be a large number of workloads, with up to a 2268 search space, or more or less. In this case, the search space defines the ways an approximate MAC may be designed. The goal of the optimization algorithm is to find optimized designs within this search space. Therefore, random search will not perform well.
- Bayesian optimization with Gaussian Process (GP) bandits may be leveraged to efficiently discover high-accuracy yet energy-efficient configurations of cross-layer approximate circuits. This approach improves the sample efficiency of black-box optimization by modelling the unknown reward function f: x→y with a Gaussian Process.
- For formalization of the approximate circuit mapping problem, consider the following optimization problem to find the lowest power mapping of approximate circuits to deep neural network layers:
-
- The decision variable Zi represents a one-hot vector to denote which of the K approximate circuits are mapped to layer i and where j is an index. The objective (2a) models the total energy consumption to evaluate a single forward pass where qi∈R+ K represents a vector containing the energy to evaluate layer i for each of the K approximate multipliers. Constraints (2d) and (2e) ensure that Zi is one-hot and is binary/integral. Constraint (2b) defines a minimum accuracy target for the neural network. Finally, Constraint (2c) constrains the area of the final chip to avoid degenerate solutions with many similar redundant multipliers.
- The accuracy oracle “ACC” models the effect of cross-layer interactions from approximations. Given a particular assignment of approximate multipliers to layers, ACC calculates the expected accuracy of the model over a specific dataset. As errors introduced in one layer are compounded through subsequent layers, the accuracy oracle is exceptionally challenging to model. Thus, the accuracy of a model may be made over the validation set.
- This reduces the optimization problem to a black-box combinatorial optimization problem and further, Bayesian optimization methods typically fail when applied to the above optimization problem. This is because Bayesian optimization methods demonstrate slow convergence with a performance similar to random search. Further, Bayesian optimization struggles with high dimensional states, discrete structures, and constrained search spaces. Therefore the above optimization problem may be reformulated such that known Bayesian optimization tools may be used.
- To find high-accuracy approximate circuit designs, feasible solutions may be constrained with a minimum validation accuracy threshold in constraint (2b). However, the accuracy oracle “ACC” is not known. Thus, to reduce the complexity of the search space, an offline study may be performed where only approximate multipliers for the target layer are used. All other layers may be evaluated with exact multipliers. This model provides an upper-bound on the expected accuracy from cross-layer approximation. Mappings with exceptionally poor expected accuracy may be pruned.
- GP bandits are predominantly designed to optimize over discrete search spaces. Bayesian optimization frameworks typically support discrete search spaces by embedding them in a real-valued box. However, this embedding is sample-inefficient as it does not consider the relation between different categorical variables. For example, this solution has the challenge of instability due to quantization error from rounding continuous predicted variables to the nearest feasible points.
- To address this issue, an estimate of per-layer accuracy degradation from single layer approximation may be used to compute an ordered set representing the relative ranking of each approximate multiplier. In this regard, the ordering may be defined as the profiled end-to-end accuracy for approximating a single layer k with a particular multiplier. However, direct search with unrounded accuracy results in an unstable relaxed optimization problem. This results from two approximate multipliers which achieve similar accuracy, but have very different power consumption. As such, linear order of multipliers may be relaxed to a partially ordered set where ties within a fixed threshold of accuracy are considered incomparable. A completed linear order may then be resolved by eliminating the least efficient multiplier in each pair of incomparable multipliers with similar accuracy.
- This procedure derives a linear order of multipliers for each of the N layers in the neural network. To define distance in the new dimension after mapping, a min-max scaling may be applied to the resulting top-1 accuracy (i.e., high probability) for each multiplier from single layer approximation calibration. Given this new formulation of the search space, the following cost optimization objective may be defined. For each of N layers, a step-wise cost function Qi: R→R may be defined to map a real-valued choice of an approximate multiplier (from 0 to 1) to the energy-consumption for the closest layer, rounding down.
- The relaxed optimization problem then becomes:
-
- Unconstrained Optimization with Barrier Functions
- While recent work has begun to explore multi-objective optimization using Bayesian optimization, these approaches are generally significantly less sample-efficient than single-objective optimizers. For optimizing inference chip designs, a two-dimensional pareto frontier between accuracy and energy consumption may be reviewed. In practice, it may be useful to also limit the area of the final systolic array to avoid degenerate solutions where redundant approximate multipliers with similar accuracy are instantiated on a single chip.
- The barrier method may be used to remove constraints (3b) and (3c). Barrier methods replace each constraint of form x≤b with a penalty in the objective function β(x, b)=−log(b−x) or β(x; b)=ex−b. As x approaches the constraint b, the penalty trends to ∞. Utilizing a barrier method, the objective may be expressed as:
-
- This updated objective now allows the removal of constraints (3b) and (3c). The exponential barrier function may be leveraged as it allows for soft constraint violations. For the accuracy term, a target accuracy τ may be 0.68, or more or less and weight α2 may be 8, or more or less. For the area term, a target area percentage (including exact multiplier) such as ϕ=400%, or more or less, and a scale α2 may be 1:2, or more or less.
-
FIG. 6 is a block diagram of anexample computing environment 600 implementing an example framework as illustrated inFIG. 1 . For example, the framework may be implemented on one or more devices having one or more processors in one or more locations, such as inserver computing device 615.User computing device 612 and theserver computing device 615 can be communicatively coupled to one ormore storage devices 630 over anetwork 660. The storage device(s) 630 can be a combination of volatile and non-volatile memory, and can be at the same or different physical locations than thecomputing devices - The
server computing device 615 can include one ormore processors 613 andmemory 614. Thememory 614 can store information accessible by the processor(s) 613, including instructions 621 that can be executed by the processor(s) 613. Thememory 614 can also includedata 623 that can be retrieved, manipulated or stored by the processor(s) 613. Thememory 614 can be a type of non-transitory computer readable medium capable of storing information accessible by the processor(s) 613, such as volatile and non-volatile memory. The processor(s) 613 can include one or more central processing units (CPUs), graphic processing units (GPUs), field-programmable gate arrays (FPGAs), and/or application-specific integrated circuits (ASICs), such as tensor processing units (TPUs). - The instructions 621 can include one or more instructions that when executed by the processor(s) 613, causes the one or more processors to perform actions defined by the instructions. The instructions 621 can be stored in object code format for direct processing by the processor(s) 613, or in other formats including interpretable scripts or collections of independent source code modules that are interpreted on demand or compiled in advance. The instructions 621 can include instructions for implementing the
framework 100 consistent with aspects of this disclosure. Theframework 100 can be executed using the processor(s) 613, and/or using other processors remotely located from theserver computing device 615. - The
data 623 can be retrieved, stored, or modified by the processor(s) 613 in accordance with the instructions 621, such as the bank ofapproximate MACs 103,architectural template 101,architecture parameters 115, etc. Thedata 623 can be stored in computer registers, in a relational or non-relational database as a table having a plurality of different fields and records, or as JSON, YAML, proto, or XML documents. Thedata 623 can also be formatted in a computer-readable format such as, but not limited to, binary values, ASCII or Unicode. Moreover, thedata 623 can include information sufficient to identify relevant information, such as numbers, descriptive text, proprietary codes, pointers, references to data stored in other memories, including other network locations, or information that is used by a function to calculate relevant data. - The
user computing device 612 can also be configured similar to theserver computing device 615, with one ormore processors 616,memory 617,instructions 618, anddata 619. Theuser computing device 612 can also include a user output 626, and a user input 624. The user input 624 can include any appropriate mechanism or technique for receiving input from a user, such as keyboard, mouse, mechanical actuators, soft actuators, touchscreens, microphones, and sensors. - The
server computing device 615 can be configured to transmit data to theuser computing device 612, and theuser computing device 612 can be configured to display at least a portion of the received data on a display implemented as part of the user output 626. The user output 626 can also be used for displaying an interface between theuser computing device 612 and theserver computing device 615. The user output 626 can alternatively or additionally include one or more speakers, transducers or other audio outputs, a haptic interface or other tactile feedback that provides non-visual and non-audible information to the platform user of theuser computing device 612. - Although
FIG. 6 illustrates theprocessors memories computing devices processors memories instructions 621, 618 and thedata processors processors computing devices computing devices - The
server computing device 615 can be configured to receive requests to process data from theuser computing device 612. For example, theenvironment 600 can be part of a computing platform configured to provide a variety of services to users, through various user interfaces and/or APIs exposing the platform services. One or more services can be a machine learning framework or a set of tools for generating neural networks or other machine learning models according to a specified task and training data. Theuser computing device 612 may receive and transmit data specifying target computing resources to be allocated for executing a neural network trained to perform a particular neural network task. - The
devices network 660. Thedevices network 660 itself can include various configurations and protocols including the Internet, World Wide Web, intranets, virtual private networks, wide area networks, local networks, and private networks using communication protocols proprietary to one or more companies. Thenetwork 660 can support a variety of short- and long-range connections. The short- and long-range connections may be made over different bandwidths, such as 2.402 GHz to 2.480 GHz (commonly associated with the Bluetooth® standard), 2.4 GHz and 5 GHz (commonly associated with the Wi-Fi® communication protocol); or with a variety of communication standards, such as the LTE® standard for wireless broadband communication. Thenetwork 660, in addition or alternatively, can also support wired connections between thedevices - Although a single
server computing device 615 anduser computing device 612 are shown inFIG. 6 , it is understood that the aspects of the disclosure can be implemented according to a variety of different configurations and quantities of computing devices, including in paradigms for sequential or parallel processing, or over a distributed network of multiple devices. In some implementations, aspects of the disclosure can be performed on a single device, and any combination thereof. - Aspects of this disclosure can be implemented in digital circuits, computer-readable storage media, as one or more computer programs, or a combination of one or more of the foregoing. The computer-readable storage media can be non-transitory, e.g., as one or more instructions executable by a cloud computing platform and stored on a tangible storage device.
- In this specification the phrase “configured to” is used in different contexts related to computer systems, hardware, or part of a computer program, engine, or module. When a system is said to be configured to perform one or more operations, this means that the system has appropriate software, firmware, and/or hardware installed on the system that, when in operation, causes the system to perform the one or more operations. When some hardware is said to be configured to perform one or more operations, this means that the hardware includes one or more circuits that, when in operation, receive input and generate output according to the input and corresponding to the one or more operations. When a computer program, engine, or module is said to be configured to perform one or more operations, this means that the computer program includes one or more program instructions, that when executed by one or more computers, causes the one or more computers to perform the one or more operations.
- While operations shown in the drawings and recited in the claims are shown in a particular order, it is understood that the operations can be performed in different orders than shown, and that some operations can be omitted, performed more than once, and/or be performed in parallel with other operations. Further, the separation of different system components configured for performing different operations should not be understood as requiring the components to be separated. The components, modules, programs, and engines described can be integrated together as a single system, or be part of multiple systems. One or more processors in one or more locations implementing an example STOC according to aspects of the disclosure can perform the operations shown in the drawings and recited in the claims.
- Unless otherwise stated, the foregoing alternative examples are not mutually exclusive, but may be implemented in various combinations to achieve unique advantages. As these and other variations and combinations of the features discussed above can be utilized without departing from the subject matter defined by the claims, the foregoing description of the examples should be taken by way of illustration rather than by way of limitation of the subject matter defined by the claims. In addition, the provision of the examples described herein, as well as clauses phrased as “such as,” “including” and the like, should not be interpreted as limiting the subject matter of the claims to the specific examples; rather, the examples are intended to illustrate only one of many possible implementations. Further, the same reference numbers in different drawings can identify the same or similar elements.
Claims (20)
1. A computer-implemented method for generating an inference chip design, comprising:
generating, by one or more processors, a set of approximate systolic arrays;
determining by the one or more processors, the performance of each approximate systolic array in the set of approximate systolic arrays processing a deep neural network (DNN);
mapping, by the one or more processors, each layer in the DNN to an approximate systolic array in the set of approximate systolic arrays; and
selecting, for inclusion in the inference chip design, by the one or more processors, a subset of the set of approximate systolic arrays based on the mapping and the performance of each approximate systolic array in the set of approximate systolic arrays.
2. The method of claim 1 , further comprising:
generating a second set of approximate systolic arrays;
determining the performance of each approximate systolic array in the second set of approximate systolic arrays processing the DNN;
mapping each layer in the DNN to an approximate systolic array in the second set of approximate systolic arrays; and
updating the subset of systolic arrays based on the mapping and the performance of each approximate systolic array in the second set of approximate systolic arrays, wherein the updated subset of approximate systolic arrays includes at least one systolic array from the second set of approximate systolic arrays.
3. The method of claim 1 , wherein the set of approximate systolic arrays is generated to satisfy an architectural template, wherein the architectural template defines a number and/or a size of systolic arrays for the inference chip design.
4. The method of claim 3 , wherein each approximate systolic array in the set of approximate systolic arrays are generated using predefined multiply-accumulate units (MACs), the predefined MACs being stored in a bank.
5. The method of claim 3 wherein each approximate systolic array in the set of approximate systolic arrays are generated using multiply-accumulate units (MACs), wherein the at least one of the MACs are generated based on predefined criteria.
6. The method of claim 5 , wherein the predefined criteria includes one or more of power usage or accuracy.
7. The method of claim 1 , wherein determining the performance of each approximate systolic array includes determining the power usage of each approximate systolic array.
8. The method of claim 7 , wherein determining the performance of each approximate systolic array further includes determining the accuracy of each approximate systolic array.
9. The method of claim 8 , wherein mapping each layer in the DNN to an approximate systolic array in the set of approximate systolic arrays includes:
for each layer in the DNN, configuring a router in the inference chip design, to a particular approximate systolic array in the subset of approximate systolic array based on the power usage and the accuracy of each approximate systolic array.
10. The method of claim 1 , wherein the inference chip design includes at least one full-precision systolic array.
11. A system for generating an inference chip design, the system comprising:
one or more processors; and
memory storing instructions, the instructions, when executed by the one or more processors, causing the one or more processors to:
generate a set of approximate systolic arrays;
determine the performance of each approximate systolic array in the set of approximate systolic arrays processing a deep neural network (DNN);
map each layer in the DNN to an approximate systolic array in the set of approximate systolic arrays; and
select, for inclusion in the inference chip design, a subset of the set of approximate systolic arrays based on the mapping and the performance of each approximate systolic array in the set of approximate systolic arrays.
12. The system of claim 11 , wherein the instructions further cause the one or more processors to:
generate a second set of approximate systolic arrays;
determine the performance of each approximate systolic array in the second set of approximate systolic arrays processing the DNN;
map each layer in the DNN to an approximate systolic array in the second set of approximate systolic arrays; and
update the subset of systolic arrays based on the mapping and the performance of each approximate systolic array in the second set of approximate systolic arrays, wherein the updated subset of approximate systolic arrays includes at least one systolic array from the second set of approximate systolic arrays.
13. The system of claim 11 , wherein the set of approximate systolic arrays is generated to satisfy an architectural template, wherein the architectural template defines a number and/or a size of systolic arrays for the inference chip design.
14. The system of claim 13 , wherein each approximate systolic array in the set of approximate systolic arrays are generated using predefined multiply-accumulate units (MACs), the predefined MACs being stored in a bank.
15. The system of claim 13 , wherein each approximate systolic array in the set of approximate systolic arrays are generated using multiply-accumulate units (MACs), wherein the at least one of the MACs are generated based on predefined criteria.
16. The system of claim 15 , wherein the predefined criteria includes one or more of power usage or accuracy.
17. The system of claim 11 , wherein determining the performance of each approximate systolic array includes determining the power usage of each approximate systolic array.
18. The system of claim 17 , wherein determining the performance of each approximate systolic array further includes determining the accuracy of each approximate systolic array.
19. The system of claim 18 , wherein mapping each layer in the DNN to an approximate systolic array in the set of approximate systolic arrays includes:
for each layer in the DNN, configuring a router in the inference chip design, to a particular approximate systolic array in the subset of approximate systolic array based on the power usage and the accuracy of each approximate systolic array.
20. The system of claim 11 , wherein the inference chip design includes at least one full-precision systolic array.
Priority Applications (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/532,572 US20230162010A1 (en) | 2021-11-22 | 2021-11-22 | Synthesizing Zero-Loss Low-Power Approximate DNN Accelerators With Large-Scale Search |
PCT/US2022/048055 WO2023091281A1 (en) | 2021-11-22 | 2022-10-27 | Synthesizing zero-loss low-power approximate dnn accelerators with large-scale search |
EP22817444.7A EP4323905A1 (en) | 2021-11-22 | 2022-10-27 | Synthesizing zero-loss low-power approximate dnn accelerators with large-scale search |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/532,572 US20230162010A1 (en) | 2021-11-22 | 2021-11-22 | Synthesizing Zero-Loss Low-Power Approximate DNN Accelerators With Large-Scale Search |
Publications (1)
Publication Number | Publication Date |
---|---|
US20230162010A1 true US20230162010A1 (en) | 2023-05-25 |
Family
ID=84369597
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US17/532,572 Pending US20230162010A1 (en) | 2021-11-22 | 2021-11-22 | Synthesizing Zero-Loss Low-Power Approximate DNN Accelerators With Large-Scale Search |
Country Status (3)
Country | Link |
---|---|
US (1) | US20230162010A1 (en) |
EP (1) | EP4323905A1 (en) |
WO (1) | WO2023091281A1 (en) |
-
2021
- 2021-11-22 US US17/532,572 patent/US20230162010A1/en active Pending
-
2022
- 2022-10-27 WO PCT/US2022/048055 patent/WO2023091281A1/en active Application Filing
- 2022-10-27 EP EP22817444.7A patent/EP4323905A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
EP4323905A1 (en) | 2024-02-21 |
WO2023091281A1 (en) | 2023-05-25 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US20230267319A1 (en) | Training neural network accelerators using mixed precision data formats | |
US11645493B2 (en) | Flow for quantized neural networks | |
US20190340499A1 (en) | Quantization for dnn accelerators | |
US11586883B2 (en) | Residual quantization for neural networks | |
US20200210840A1 (en) | Adjusting precision and topology parameters for neural network training based on a performance metric | |
US11386256B2 (en) | Systems and methods for determining a configuration for a microarchitecture | |
CN110826708B (en) | Method for realizing neural network model splitting by using multi-core processor and related product | |
Chaudhuri et al. | mfEGRA: Multifidelity efficient global reliability analysis through active learning for failure boundary location | |
EP4315173A1 (en) | Generating and globally tuning application-specific machine learning accelerators | |
WO2022247092A1 (en) | Methods and systems for congestion prediction in logic synthesis using graph neural networks | |
WO2021070204A1 (en) | Machine learning, deep learning and artificial intelligence for physical transport phenomenon in thermal management | |
US20230162010A1 (en) | Synthesizing Zero-Loss Low-Power Approximate DNN Accelerators With Large-Scale Search | |
US20230062600A1 (en) | Adaptive design and optimization using physics-informed neural networks | |
US11900239B2 (en) | Systems and methods for accelerating sparse neural network execution | |
Li et al. | An experimental evaluation of extreme learning machines on several hardware devices | |
Al-Zoubi et al. | Design space exploration of the KNN imputation on FPGA | |
Shi et al. | NASA: Neural architecture search and acceleration for hardware inspired hybrid networks | |
Srivastava | Design and Generation of Efficient Hardware Accelerators for Sparse and Dense Tensor Computations | |
Naous et al. | Tuning algorithms and generators for efficient edge inference | |
Mohaidat et al. | A Survey on Neural Network Hardware Accelerators | |
US20240095424A1 (en) | Alignment Cost for Integrated Circuit Placement | |
US20230004430A1 (en) | Estimation of power profiles for neural network models running on ai accelerators | |
Anderson et al. | Toward Energy–Quality Scaling in Deep Neural Networks | |
Hanif et al. | Cross-Layer Optimizations for Efficient Deep Learning Inference at the Edge | |
CN117421703A (en) | Depth sign regression accelerator and depth sign regression method |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:MIRHOSEINI, AZALIA;HUDA, SAFEEN;MAAS, MARTIN CHRISTOPH;AND OTHERS;SIGNING DATES FROM 20211210 TO 20211214;REEL/FRAME:058433/0001 |