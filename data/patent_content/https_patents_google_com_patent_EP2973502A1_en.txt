EP2973502A1 - Motion data sharing - Google Patents
Motion data sharingInfo
- Publication number
- EP2973502A1 EP2973502A1 EP14716657.3A EP14716657A EP2973502A1 EP 2973502 A1 EP2973502 A1 EP 2973502A1 EP 14716657 A EP14716657 A EP 14716657A EP 2973502 A1 EP2973502 A1 EP 2973502A1
- Authority
- EP
- European Patent Office
- Prior art keywords
- user
- physical movement
- computer system
- computer
- information
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Ceased
Links
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L65/00—Network arrangements, protocols or services for supporting real-time applications in data packet communication
- H04L65/40—Support for services or applications
- H04L65/403—Arrangements for multi-party communication, e.g. for conferences
-
- A—HUMAN NECESSITIES
- A63—SPORTS; GAMES; AMUSEMENTS
- A63B—APPARATUS FOR PHYSICAL TRAINING, GYMNASTICS, SWIMMING, CLIMBING, OR FENCING; BALL GAMES; TRAINING EQUIPMENT
- A63B24/00—Electric or electronic controls for exercising apparatus of preceding groups; Controlling or monitoring of exercises, sportive games, training or athletic performances
- A63B24/0003—Analysing the course of a movement or motion sequences during an exercise or trainings sequence, e.g. swing for golf or tennis
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/011—Arrangements for interaction with the human body, e.g. for user immersion in virtual reality
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/016—Input arrangements with force or tactile feedback as computer generated output to the user
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0481—Interaction techniques based on graphical user interfaces [GUI] based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance
-
- G—PHYSICS
- G09—EDUCATION; CRYPTOGRAPHY; DISPLAY; ADVERTISING; SEALS
- G09B—EDUCATIONAL OR DEMONSTRATION APPLIANCES; APPLIANCES FOR TEACHING, OR COMMUNICATING WITH, THE BLIND, DEAF OR MUTE; MODELS; PLANETARIA; GLOBES; MAPS; DIAGRAMS
- G09B19/00—Teaching not covered by other main groups of this subclass
- G09B19/003—Repetitive work cycles; Sequence of movements
Definitions
- This specification generally relates to sharing motion data between computer systems.
- Instruction that is adapted specifically to an individual user has traditionally been provided in-person.
- a golf instructor may be present to watch the user's golf swing.
- Such in-person instruction can allow for the instructor to identify deviations from an optimal golf swing and to suggest correction's to the user's golf swing motion which will help the user match his/her swing more closely with the optimal golf swing.
- This specification generally describes technologies relating to sharing motion data between computer systems and allowing for motion-based interaction between computer users.
- an expert user and a novice user can use separate computer systems to share motion data regarding a physical movement (e.g., swinging a golf club) and can remotely interact with each other for the expert user to provide instructions to the novice user regarding the physical movement.
- a physical movement e.g., swinging a golf club
- one innovative aspect of the subject matter described in this specification can be embodied in methods that include the actions of receiving, at a first computer system that is associated with a first user, first information that defines a first physical movement that was performed by a second user; outputting, by the first computer system using one or more output devices, a representation of the first physical movement; detecting a second physical movement performed by the first user;
- users can readily obtain personalized help from experts without meeting in-person.
- a user who is located several hours from the nearest expert in a particular field e.g., plumbing, fly fishing, surgery
- FIG. 1 is a block diagram of an example environment in which motion data is captured, analyzed, and shared between computer systems.
- FIG. 2 is a conceptual diagram of a system for an interactive session between an expert user and a novice user.
- FIG. 3 depicts example screenshots of a display provided to an expert user and a display provided to a novice user for an example interactive session.
- FIG. 4 depicts examples of feedback being provided to a user based on detected differences between the user's motion and the motion of another user.
- FIG. 5 is a diagram of an example system for sharing motion data between computer systems.
- FIGS. 6A-B are flowcharts of an example technique for sharing motion data between computing devices.
- This document generally describes providing interactive sessions through which computing devices can capture, share, and present motion data.
- an expert user that is associated with a first computing device (e.g., a laptop computer, a smartphone, a wearable computing device) can capture and transmit information regarding a physical movement to another computing device for presentation to another user (e.g., novice user).
- Such interactive sessions between computing devices can allow for a physical movement performed by one user to be presented within the context of a physical environment of another user.
- physical movements performed by an expert user can be transmitted to and, through a display device, superimposed over in relevant items within a novice user's environment to which the expert user's physical movements pertain.
- an expert user showing a novice user how to access the trap of a plumbing fixture may initially place his/her hands at the appropriate locations along a physical plumbing line, which may be a physical plumbing line that is located in front of the expert user and/or a representation of the novice user's plumbing fixture, to remove the trap.
- a physical plumbing line which may be a physical plumbing line that is located in front of the expert user and/or a representation of the novice user's plumbing fixture, to remove the trap.
- the location of the expert user's hands and subsequent actions can be superimposed over relevant locations on the novice user's plumbing fixture.
- Differences between physical movements performed by two or more users can be identified and used to determine whether corrective instructions/actions are needed. For example, if an expert performs and transmits an example golf swing to a novice user and the novice user's attempt to replicate the expert's swing is inaccurate (e.g., the novice is not extending his arms enough and his/her hands are too close to his/her body throughout the swing), the differences between the expert's swing and the novice's swing can be identified. These differences can be presented to the expert and/or the novice user (e.g., highlighting through augmented reality presentation, use of haptic feedback), and an expert user may generate and transmit additional motion data to the novice user that identifies one or more corrections to be made by the novice user. Such performance by a novice user and corrective instructions from an expert can be iteratively performed until the novice user has improved and/or attained sufficient skill level to perform the physical action under instruction.
- FIG. 1 is a block diagram of an example environment 100 in which motion data is captured, analyzed, and shared between computer systems.
- a data communication network 102 enables data communication between multiple electronic devices. Users can access content, provide content, exchange information, and participate in interactive sessions, such as motion data sharing sessions, by use of the devices and systems that can communicate with each other over the network 102.
- the network 102 can include, for example, a local area network (LAN), a cellular phone network, a wide area network (WAN), e.g., the Internet, or a combination of them.
- the links on the network can be wireline or wireless links or both.
- a publisher website 104 includes one or more resources 105 associated with a domain and hosted by one or more servers in one or more locations.
- a website is a collection of web pages formatted in hypertext markup language (HTML) that can contain text, images, multimedia content, and programming elements, for example, scripts.
- HTML hypertext markup language
- Each website 104 is maintained by a content publisher, which is an entity that controls, manages and/or owns the website 104.
- the publisher websites 104 can provide a variety of different web pages, such as web pages through which novice users can identify and initiate instructional motion data sharing sessions with expert users.
- a resource is any data that can be provided by a publisher website 104 over the network 102 and that has a resource address, e.g., a uniform resource locator (URL).
- Resources may be HTML pages, electronic documents, images files, video files, audio files, and feed sources, to name just a few.
- the resources may include embedded information, e.g., meta information and hyperlinks, and/or embedded instructions, e.g., client- side scripts.
- a search engine 110 crawls the publisher web sites 104 and indexes the resources 105 provided by the publisher web sites 104 in an index 112.
- the search engine 110 can receive queries from user devices 130. In response to each query, the search engine 110 searches the index 112 to identify resources and information that are relevant to the query.
- the search engine 110 identifies the resources in the form of search results and returns the search results to the user device 130.
- a search result is data generated by the search engine 110 that identifies a resource or provides information that satisfies a particular search query.
- a search result for a resource can include a web page title, a snippet of text extracted from the web page, and a resource locator for the resource, e.g., the URL of a web page.
- the resources 105 can include information that identifies expert users and their corresponding areas of expertise.
- the search results provided by the search engine 110 can allow novice users to identify expert users who have expertise in one or more fields that are relevant to the notice users.
- the search results are ranked based on scores related to the resources identified by the search results, e.g., information retrieval ("IR") scores, and optionally a separate ranking of each resource relative to other resources, e.g., an authority score.
- the search results are ordered according to these scores and provided to the user device according to the order.
- the search results can include information identifying expert users who are relevant to a novice user's query and the expert users can be ranked according to their relevance to the query.
- a user device 130 receives the search results and presents them to a user. If a user selects a search result, the user device 130 requests the corresponding resource.
- the publisher of the web site 104 hosting the resource receives the request for the resource and provides the resource to the user device 130.
- the queries submitted from user devices 130 are stored in query logs 114.
- Selection data for the queries and the web pages referenced by the search results and selected by users are stored in selection logs 116.
- the query logs 114 and the selection logs 116 define search history data 117 that include data from and related to previous search requests associated with unique identifiers.
- the selection logs represent actions taken responsive to search results provided by the search engine 110.
- the query logs 114 and selection logs 116 can be used to map queries submitted by user devices to resources that were identified in search results and the actions taken by users when presented with the search results in response to the queries.
- data are associated with the identifiers from the search requests so that a search history for each identifier can be accessed.
- the selection logs 116 and query logs 114 can thus be used by the search engine to determine the respective sequences of queries submitted by the user devices, the actions taken in response to the queries, and how often the queries have been submitted.
- the users may be provided with an opportunity to control whether programs or features collect user information (e.g., information about a user's social network, social actions or activities, profession, a user's preferences, or a user's current location), or to control whether and/or how to receive content from the content server that may be more relevant to the user.
- user information e.g., information about a user's social network, social actions or activities, profession, a user's preferences, or a user's current location
- certain data may be treated in one or more ways before it is stored or used, so that personally identifiable information is removed.
- a user's identity may be treated so that no personally identifiable information can be determined for the user, or a user's geographic location may be generalized where location information is obtained (such as to a city, ZIP code, or state level), so that a particular location of a user cannot be determined.
- location information such as to a city, ZIP code, or state level
- the user may have control over how information is collected about the user and used by a content server.
- the content item management system 120 provides content items for presentation with the resources 105.
- a variety of appropriate content items can be provided - one example content item is an advertisement. In the case of
- the content item management system 120 allows advertisers to define selection rules that take into account attributes of the particular user to provide relevant advertisements for the users.
- Example selection rules include keyword selection, in which advertisers provide bids for keywords that are present in either search queries or resource content or metadata. Advertisements that are associated with keywords having bids that result in an advertisement slot being awarded in response to an auction are selected for displaying in the advertisement slots. For example, advertisements can be provided that offer the instructional services of particular expert users.
- the user device 130 When a user of a user device 130 selects an advertisement, the user device 130 generates a request for a landing page of the advertisement, which is typically a web page of the advertiser. For example, selection of an advertisement for an expert user can cause a request for a landing page for the corresponding expert user.
- the relevant advertisements can be provided for presentation on the resources 105 of the publishers 104, or on a search results page resource.
- a resource 105 from a publisher 104 may include instructions that cause a user device to request advertisements from the content item management system 120.
- the request includes a publisher identifier and, optionally, keyword identifiers related to the content of the resource 105.
- the content item management system 120 provides advertisements to the requesting user device. With respect to a search results page, the user device renders the search results page and sends a request to the content item management system 120, along with one or more keywords related to the query that the user provide to the search engine 110.
- the content item management system 120 provides
- the content item management system 120 includes a data storage system that stores campaign data 122 and performance data 124.
- the campaign data 122 stores advertisements, selection information, and budgeting information for advertisers.
- the performance data 124 stores data indicating the performance of the advertisements that are served. Such performance data can include, for example, click-through rates for advertisements, the number of impressions for advertisements, and the number of conversions for advertisements. Other performance data can also be stored.
- the campaign data 122 and the performance data 124 are used as input to an advertisement auction.
- the content item management system 120 in response to each request for advertisements, conducts an auction to select
- the advertisements are ranked according to a score that, in some implementations, is proportional to a value based on an advertisement bid and one or more parameters specified in the performance data 124.
- the highest ranked advertisements resulting from the auction are selected and provided to the requesting user device.
- a user device 130 is an electronic device, or collection of devices, that is capable of requesting and receiving resources over the network 102.
- Example user systems 106 include personal computers 132, mobile communication devices 134, and other devices that can send and receive data 136 over the network 102.
- a user device 130 typically includes a user application, e.g., a web browser that sends and receives data over the network 102, generally in response to user actions.
- the web browser can enable a user to display and interact with text, images, videos, music and other information typically located on a web page at a website on the world wide web or a local area network. For example, a novice user and an expert user can use separate user devices, such as the user device 130, to interact with each other remotely over the network 102 through a motion data sharing session.
- An interactive session system 140 is also accessible by the user devices 130 over the network 102.
- the interactive session system 140 serves interactive sessions and data related to interactive sessions to users of user devices 130.
- interactive session is used in this specification to refer to a presentation that allows a user to experience an event or receive data related to the event.
- Events of different types can be presented.
- events may be "assistance” events, for which interactive sessions provide step-by-step assistance to users to accomplish a particular task, or events may be "experience” events, for which interactive sessions provide users with an experience of participating in an activity.
- An example interactive session for an assistance event is a session that describes a step-by-step process to build a computer.
- An example interactive session for an experience event is a session that provides the experience of driving a certain make and model of an automobile.
- the interactive session system 140 may also provide interactive sessions for other appropriate event types.
- the interactive session system 140 can serve interactive sessions and data for sharing motion data between separate computer systems, such as a computer system associated with a novice user and a computer system associated with an expert user.
- the data that the interactive session system 140 provides for an event may also differ based on the event type and based on the intent of the user.
- interactive sessions for repair events may provide users with a list of tools and parts required to accomplish a task at the beginning of an interactive session.
- a user may have implicitly or explicitly specified an intent for viewing an interactive session.
- the user may explicitly specify an intent, for example, by interacting with a user interface element that represents their intent.
- a user may implicitly specify an intent, for example, by submitting a search query that is related to the intent, or by requesting other information that is related to the intent.
- a user request for information about purchasing tools needed to repair a computer may be considered an implicit indication of the user's intent to repair a computer.
- the interactive session system 140 may also determine specific data to provide based on the intent. For example, a user that is viewing a session that describes building a computer, and with the intent to build the computer, may be presented with additional information, e.g., a list of parts, tools and the time required to complete the task. Another user that is watching the same session with the intent to learn about computers may be presented with other information, e.g., articles about memory, heat dissipation, or other computer-related topics, in a side panel of a viewing environment as the interactive session is presented.
- additional information e.g., a list of parts, tools and the time required to complete the task.
- Another user that is watching the same session with the intent to learn about computers may be presented with other information, e.g., articles about memory, heat dissipation, or other computer-related topics, in a side panel of a viewing environment as the interactive session is presented.
- the sessions can be created by expert users or non-expert (novice) users.
- expert user is used in this specification to refer to a user or entity that has been accepted by the system 140 for a category, e.g., as a result of the user's or entity's having provided credentials or demonstrated a high level of skill. Examples include a licensed contractor for construction related videos or a company that produces sessions for a particular product the company manufactures and a user that has produced a large number of highly rated sessions.
- the content item management system 120 can provide content items with the interactive sessions.
- the content item management system 120 may select advertisements based on the subject matter of a session, the event type, and the user's intent. For example, for a repair event, the content item management system 120 may provide advertisements for providers of tools and parts that are listed in the list of tools and parts required to accomplish the repair task.
- Production systems 150 can be used to create sessions. Production systems 150 may range from studios to simple hand-held video recording systems. Generally, a production system 150 is a system that includes one or more of an audio input device 150-1, a video input device 150-2, an optional display device 150-3, and optionally other input and output devices and production processes that are used to create sessions. For example, post production processes may be used to add metadata to an interactive session. Such metadata may include, for example, keywords and topical information that can be used to classify the session to one or more topical categories; a list of tools and parts required for a particular session and descriptions of the tools and parts; and so on.
- metadata may include, for example, keywords and topical information that can be used to classify the session to one or more topical categories; a list of tools and parts required for a particular session and descriptions of the tools and parts; and so on.
- Tactical sensory input devices may also be used in a production system 150.
- a particular interactive session may provide input data for a "G-suit" that applies pressure to a user's body and that the user interprets as simulated motion.
- appropriate input systems are used in the production system 150 to generate and store the input data for the interactive session.
- Production systems 150 may also be or include devices that are attached to a person.
- wearable computer devices that include a camera input device and microphone input device may be worn on a user's person during the time the user is creating the session.
- the sessions are stored as sessions data 142 and are associated with authoring entities by entity data 144.
- a user can use a user device 130 to access the interactive session system 140 to request a session.
- the interactive session system 140 can provide a user interface to the user devices 130 in which interactive sessions are arranged according to a topical hierarchy.
- the interactive session system 140 includes a search subsystem that allows users to search for interactive sessions.
- the search system 110 can search the session data 142 and the entity data 144.
- Other types of input and output devices may also be used, depending on the type of interactive session.
- an augmented reality visor that provides a view of a real-world environment augmented by computer-generated graphics may be used.
- a tactical sensory input device and a tactical sensory output device that applies pressure to a user's body and that the user interprets as simulated motion or other type of feedback may also be used.
- an interactive session system 140 provide interactive sessions in real time or near-real time.
- a real time or near-real time interactive session can be an interactive session that is created in response to a user request for the interactive session.
- real-time or near-real time sessions may be provided by a company for repairing a product sold by the company when the user cannot find a stored interactive session that fulfills the user's informational needs.
- interactive sessions may be provided as part of a consultation process. For example, an automobile mechanic may contact a user at another location, e.g., the user's home, to consult with the user regarding an automobile repair.
- the automobile mechanic may then explain to the user, by means of an interactive session that highlights certain parts of the automobile engine as seen from the point of view of the automobile mechanic, certain repairs that are necessary and request authorization from the user to proceed.
- the user can ask questions and discuss alternatives with the automobile mechanic during the interactive session to make an informed decision.
- FIG. 2 is a conceptual diagram of a system 200 for an interactive session between an expert user 202 and a novice user 204.
- the expert user 202 and the novice user 204 can each be associated with a computer system that includes, at least, one or more computing devices (e.g., laptop computer, desktop computer, smartphone, tablet computing device, wearable computing device), an input subsystem with one or more input devices (e.g., cameras, motion sensing devices, microphones, tactile sensing devices, depth sensing devices, physiological sensing devices), an output subsystem (e.g., display device, haptic feedback device, speakers, physiological interfaces).
- an expert user can be a user who is providing instructions to another user and a novice user can be a user to who is receiving instructions. Users may be designated as experts in some contexts and as novices in other contexts, including within the same interactive session (e.g., during an interactive session a user may provide instruction regarding a first topic and receive instruction regarding a second topic).
- the interactions between the expert user 202 and the novice user 204 that are described below are provided through the use of such computer systems and a computer network 206.
- the computer network 206 can be any of a variety of appropriate communication networks, such as the network 102.
- the interactions between the expert user 202 and the novice user 204 are further illustrated in an example expert view and an example novice view that are depicted in FIG. 3, which provides example screenshots of a display provided to an expert user and a display provided to a novice user for an interactive session regarding repair/maintenance of a plumbing trap.
- the example interactive session between the expert user 202 and the novice user 204 that is depicted in FIG. 2 begins with a computer system that is associated with the expert user 202 detecting one or more motions performed by the expert user 202, as indicated by step A (208).
- the expert user places his/her right hand 300 on a rotatable connector 302 (e.g., threaded connector) between the trap 304 and a drain pipe 306 that leads into the trap 304.
- the expert also places his/her left hand 308 on the trap 304 so as to minimize rotation of the trap 304 when the connector 302 is rotated.
- Such movement by the expert user, placing his/her hands 300 and 308 at particular places along the drain system, can be detected by a computer system associated with the expert user 202, as indicated by step A (208).
- the motion by the expert user can be detected by a computer system in any of a variety of appropriate ways, such as through analysis of a video stream of the expert user's actions, through use of one or more motion sensing devices (e.g., G-suit embedded with motion, pressure, and/or touch sensors), and/or through the use of one or more depth sensors (e.g., sensors that determine the x, y, and z coordinates ("3D coordinates") of physical objects that are located in front of the depth sensors using, for example, an infrared laser projector and a monochrome CMOS sensor to determine 3D coordinates of objects in real time).
- Other devices and techniques for detecting motion are also possible.
- the plumbing components 302-306 to which the expert user's motions pertain may be physical plumbing components that are at the expert user's location and/or the plumbing components 302-306 can be virtual representations of the novice user's physical plumbing components.
- the configuration of the plumbing components at the novice user's location may be detected, through the use of one or more sensors (e.g., cameras, depth sensors), and transmitted to a computing device associated with the expert user for presentation in the expert user's view.
- the expert view la that is depicted in FIG. 3 may be the view as presented to the expert user 202 through one or more display devices, such through a wearable pair of glasses capable of augmenting the expert user's view of his/her surroundings with additional graphical information.
- the example view la does not include any additional augmentation other than the expert user's physical environment, the example view la may include augmentation, such as a display of the novice user's environment that is overlaid on the expert user's environment (e.g., plumbing trap) and/or highlighting that identifies the detected motion performed by the expert (e.g., placing his/her hands on the connector 302 and on the trap 304).
- step B (210) information regarding the detected motion of the expert user 202 is transmitted over the network 206 and to a computer system that is associated with the novice user 204.
- the computer system associated with the expert user 202 may replay the detected motion to the expert user 202, may allow the expert user 202 to edit and/or modify the detected motion (e.g., trim the detected motion, modify hand placement within the detected motion), and may allow the expert user 202 to confirm that such detected motion should be transmitted to the novice user 204 before it is transmitted over the network 206.
- the computer system associated with the novice user 204 can present the expert motion to the novice user, as indicated by step CI (212). For example, referring to view lb from FIG.
- the detected motions of the expert user are superimposed onto the physical environment of the novice user.
- a representation 310 of the right hand 300 of the expert user is superimposed in the novice's view at a location (rotatable connector 312) that corresponds to the location where the expert user's right hand 300 was located (rotatable connector 302) during the detected motion.
- a representation 314 of the expert user's left hand 308 is superimposed over a trap 316, which is the same location in the expert's environment where the expert user's left hand 308 was positioned during the detected motion.
- the representations 310 and 314 of the expert user's hands can be placed in the novice user's view at appropriate locations over the novice user's physical environment using any of a variety of appropriate techniques (e.g., object detection techniques, object matching techniques, image processing techniques), such as by identifying one or more objects (e.g., trap 316, connector 312) from the novice user's view that correspond to one or more objects (e.g., trap 304, connector 302) from the expert user's view.
- object detection techniques e.g., object matching techniques, image processing techniques
- object matching techniques e.g., image processing techniques
- image processing techniques e.g., image processing techniques
- Such contextual information e.g., objects that are located within a view
- the representations 310 and 314 can be displayed in the view using one or more visual effects to indicate to the novice user that the representations correspond to the expert user, such as through the use of particular colors, particular levels of transparency (e.g., can be displayed semi-transparent), and/or other visual effects.
- the representations 310 and 314 can be animated according to the motion detected for the expert user. For instance, the representations 310 and 314 can be displayed with movement in the novice's view that corresponds to the motion detected for the expert user. Displaying the representations 310 and 314 in the view lb can provide instruct the novice user on how to perform the first step as indicated by the expert user in view la.
- the computer system associated with the novice user 204 can detect mimicking motion from the novice user 204, as indicated by step C2 (214). Mimicking motion can be the novice user's attempt to perform the motions as instructed by the expert user 202 but within the novice user's physical environment.
- Mimicking motions can be detected in any of a variety of appropriate ways, such as by monitoring motions performed within a window of time (e.g., previous 5 seconds, previous 10 seconds) to determine whether the motions match or resemble at least a portion of the motions performed by the expert user, based on an cue from the user that he/she is about to attempt a mimicking motion (e.g., verbal cue such as "here goes," physical cues such as changes in posture or breathing patterns), and/or based on a prompt from the computing device associated with the novice user 204 to attempt to perform the expert user's action.
- Example mimicking motion by the novice user 204 is depicted in view 2b of FIG. 3.
- the novice user places his/her right hand 318 at the correct location (at the rotatable connector 312) but does not place his/her left hand 320 in the appropriate corresponding location (trap 316). Instead, the novice user has placed his/her left hand 320 at a location (crown 322) that does not correspond to the placement of the expert user's left hand 308 at the trap 304.
- the mimicking motion of the novice user 204 can be detected at step C2 (214) in a similar manner to the detected motion of the expert user 202 at step A (208).
- the computing system that is associated with the novice user 204 can additionally provide feedback that indicates where, if at all, the novice user's mimicking motion has deviated from the motion performed by the expert user 202, as indicated by step C3 (216).
- FIG. 4 depicts examples of feedback being provided to a user based on detected differences between the user's motion and the motion of another user. For instance, as depicted in view (a) of FIG. 4, a representation 400 of a physical movement performed by another user is presented, similar to the representations 310 and 314 described above with regard to view lb of FIG. 3. As depicted in view (b) of FIG.
- these representations 400 of movement by another user can continue to be superimposed over the view as the user attempts to mimic the representations 400.
- motion 402 performed by the user is depicted with the solid lines and the representations 400 are depicted with the dashed lines.
- deviations 404 from the representations 400 can be highlighted and provided to the user as visual feedback. Such deviations can be identified in real-time as the user is performing the motion 402 so that the user can know how his/her movements vary from those of another user and can modify the motion 402 to more closely align with the representations 400.
- one or more haptic devices 406 that are being worn or are otherwise attached to the user can be used to provide haptic feedback to guide the user in a direction to correct the deviations 404.
- the user may be wearing clothing (e.g., shirt, pants, sleeve, glove, suit, helmet, hat, shoes, socks, and/or undergarments) that includes haptic devices and, based on the deviations 404, one or more
- appropriately located haptic devices 406 can be identified and activated to indicate a direction in which the user should move.
- the haptic devices 406 may vibrate and/or otherwise apply physical force in one or more appropriate directions to correct the deviations 404.
- a computer system that is associated with the novice user 204 can transmit information regarding the mimicking motion that was detected for the novice user 204, as indicated by step D (218).
- the computer system associated with the expert user 202 can receive the mimic information for the novice user 204 and can present the mimic information, as indicated by step El (220).
- step El For example, referring to view 2a of FIG. 3, representations 324 and 326 of the novice user mimicking motion using his/her hands 318 and 320 are superimposed over the plumbing components 302-306 that are present in the expert's physical environment.
- the computer system associated with the expert user can identify where, if at all, the mimicking motion for the novice user has deviated from the expert's motion as depicted in view la.
- Such deviations can be determined in any of a variety of appropriate ways, such as through image comparison techniques, pattern matching techniques, and/or object comparison techniques. If a difference of at least a threshold amount is detected, the difference can be highlighted, as indicated by the representation 326 that corresponds to the novice user's left hand 320 being bolded/weighted.
- corrective motion can be provided by the expert user 202 and detected by a computer system that is associated with the expert user 202, as indicated by step E2 (222).
- the expert user can physically move the representations 324 and 326 of the novice user's hands 318 and 320 to place them in the appropriate locations that correspond to the expert's motion in view la.
- the expert user uses his/her left hand 308 to move the representation 326 of the novice user's left hand 320 down and to the right so that it is holding on to the trap 304.
- the computer system associated with the expert user 202 can transmit corrective motion information over the network 206 to the computer system associated with the novice user 204, as indicated by step F (224).
- the computer system that is associated with the novice user 204 can present the corrective motion, as indicated by step Gl (226). For example, referring to view 3b that is presented in FIG. 3, the representation 314 of the left hand is displayed with highlighting (bold) and is moved to the trap 316 as indicated by the arrow 328 and as instructed by the expert's corrective motion in view 3a.
- the novice user can received personalized instruction that aids the novice user in learning an appropriate technique from the expert user.
- the novice user 204 can attempt to mimic the movements of the expert user 202 and a computer system associated with the novice user 204 can detect the second attempt at mimicking motion, as indicated by step G2 (228).
- the novice user may place his/her left hand 320 at the trap 316 (appropriate location that corresponds to the location of the expert user's left hand 308 at the trap 304) and his/her right hand 318 at the connector 312.
- the novice user's mimicking motion can be detected in a similar manner as the mimicking motion described above with regard to step C2 (214) and can be transmitted to the computer system associated with the expert user 202, as indicated by step H (230).
- Representations 324 and 326 that correspond to the novice user's hands 318 and 320 can be presented in a subsequent view 4a for the expert user, as depicted in FIG. 3.
- the representations 324 and 326 which lack any highlighting in view 4a which may indicate at least a threshold difference from the expert user's original motion in view la, the novice user appears to have mimicked the expert user's motion with at least a threshold degree of accuracy.
- the expert user may then proceed to perform a next step in the process of the servicing the novice user's plumbing, such as unscrewing the connector 312 and the other connector for the trap.
- steps C1-C2, G1-G2, and H may be repeatedly performed until the novice user has successfully performed the instructed motion (or until another event, such as the end of a training session).
- FIGS. 2-4 are presented with regard to movements of a user's hands, such techniques can be performed additionally and/or alternatively with regard to movement of other body parts, such as a user's torso, legs, feet, neck, and/or head.
- instructions for kicking a ball could be transmitted between the expert user 202 and the novice user 206 using the same techniques and systems described above.
- the interactive session system 140 may perform tasks such as identifying differences between physical movements performed by the expert user 202 and mimicking movements performed by the novice user 204, and may provide information regarding such differences to one or both of the computer systems associated with the expert user 202 and the novice user 204.
- FIG. 5 is a diagram of an example system 500 for sharing motion data between computer systems.
- the system 500 includes an example user computer system 502 that can detect physical movements of one or more associated users and that present the physical movements of other users within the context of the physical environment of the one or more associated users.
- the computer system 502 can be similar to the computer systems described above as being associated with the expert user 202 and/or the novice user 204.
- the computer system 502 can also be similar to the user systems 130 described above with regard to FIG. 1.
- the computer system 502 can include one or more computing devices.
- the computer system 502 includes an input/output (I/O) interface 504 through the computer system 500 can communicate with other computing devices over a network 506.
- the network 506 can be similar to the network 102 and/or the network 206.
- the I/O interface 504 can be any of a variety of appropriate interfaces, such as a wired network interface (e.g., Ethernet card) and/or a wireless network interface (e.g., WiFi chipset, mobile data network chipset).
- the computer system 502 includes an input subsystem 508 through which user input can be detected and obtained, such as the detection of physical movements of one or more associated users.
- the input subsystem 508 can include one or more of a variety of devices through which user input, and particularly physical movement, can be detected.
- the input subsystem 508 can include one or more digital cameras 510, one or more motion sending devices 512 (e.g., accelerometers, gyroscopes), one or more microphones 514, one or more tactile sensitive devices 516 (e.g., capacitive touch sensors, pressure sensitive surfaces), one or more physiological sensing devices (e.g., electromyographs to detect electrical activity produced by skeletal muscles as part of an electromyography (EMG) technique, brain wave detecting devices to record electrical activity along the scalp of a user, for example, as part of a electroencephalography (EEG) technique), and/or one or more depth sensors (e.g., sensors that determine the x, y, and z coordinates ("3D coordinates") of physical objects that are located in front of the depth sensors using, for example, an infrared laser projector and a monochrome CMOS sensor to determine 3D coordinates of objects in real time).
- motion sending devices 512 e.g., accelerometers, gyroscopes
- One or more of the devices that are part of the input subsystem 508 can be embedded within and/or tethered to one or more wearable I/O devices 522, such as gloves, suits, shirts, sleeves, hats, pants, shoes, socks, and/or undergarments.
- Input detected through the input subsystem 508 can be provided to and/or be analyzed by a motion capture unit 524 that is programmed to detect physical movements performed by one or more associated users.
- the motion capture unit 524 can use a variety of device drivers and/or application programming interfaces (APIs) to interpret the input received through the devices of the input subsystem 508.
- APIs application programming interfaces
- the motion capture unit 524 can detect motion using any of a variety of appropriate techniques, such as determining whether input from the motion sensing devices 512 and/or tactile sensing devices 516 match one or more patterns, identifying objects and analyzing changes in such objects from images captured through the digital cameras 510 over a period of time, and/or determining whether physiological signals detected by the one or more physiological sensing devices 518 match one or more patterns.
- Movements that are detected by the motion capture unit 524 may be provided to a movement comparison unit 526, which is programmed to determine whether the detected movements differ from physical movements by one or more other users and, if so, to what degree. Movements may be represented in a variety of ways, such as a series images (e.g., a video) and/or as 2D/3D wireframe model with one or more points that move over time.
- the movement comparison unit 526 can compare movements by comparing such data representations of movement (e.g., performing image
- Physical movements performed by other users and/or detected differences from such movements can be used by an augmented reality engine 528 and/or a haptic feedback unit 530 to provide a depiction of the motions of other users within the context of a user's physical environment.
- the augmented reality engine 528 can superimpose physical movements by other users can highlight differences between such movements and the movements detected by the motion capture unit 524, similar to the augmented views depicted in FIGS. 3-4.
- the haptic feedback unit 530 can identify appropriate haptic devices to activate and appropriate levels of force for such activations, and can activate such haptic devices to provide an appropriate feedback to the user.
- the augmented reality engine 528 and the haptic feedback unit 530 can use an output subsystem 532 of the computer system 502 to provide such feedback to a user of the computer system 502.
- the output subsystem 502 includes one or more devices, such as one or more display devices 534 (e.g., computer display, touchscreen, glasses with embedded display), one or more haptic devices 536 (e.g., devices providing tactile feedback to a user), one or more speakers 538, and/or one or more physiological interfaces 540 (e.g., devices providing low voltage electrical impulses to stimulate one or more specific muscles).
- the system 500 can also include one or more other user computer systems 542, which are similar to the user computer system 502.
- the expert user 202 can be associated with the user computer system 502 and the novice user 204 can be associated with the other user computer system 542, which can be the same as or similar to the user computer system 502.
- Interaction among the user computer system 502 and the other user computer system 542 to share motion data may be facilitated by an interactive session computer system 544, which can be similar to the interactive session system 140 described above with regard to FIG. 1.
- Processing associated with one or more components of the user computer system 502, such as processing associated with the movement comparison unit 526, may be performed, at least in part, by the interactive session computer system 544.
- the user computer system 502, the other user computer system 542, and/or the interactive session computer system 544 may be configured to store information regarding detected user motion in a motion data repository 546.
- detected motion data may provide a series of motions that can be shared with the user computer system 502 and/or the other user computer system 542, and that can serve to supplement the current unavailability of other users.
- the user computer system 502 may contact the interactive session computer system 544 to connect with another computer system associated with an expert in a particular subject matter. If such an expert is not available, the interactive session computer system 544 may suggest one or more motions from the motion data repository 546. If accepted by the user computer system 502, appropriate data can be retrieved from the repository 546 and can either be directly provided to the user computer system 502 or served by the interactive session computer system 544.
- the system 500 can also include an automated motion system 548 that includes one or more robotic motion components 550.
- the automated motion system 548 may be a manufacturing facility that includes machines that use motion patterns to perform one or more automated tasks.
- Such an automated motion system 548 may obtain and use motion data from the user computer system 502 and/or the motion data repository 546 to instruct the robotic motion components 550 to perform one or more tasks.
- FIGS. 6A-B are flowcharts of an example technique 600 for sharing motion data between computing devices.
- the example technique 600 is depicted as being performed in part by an expert computer system 602 that is associated with an expert user and in part by a novice computer system 604 that is associated with a novice user.
- the expert computer system 602 and/or the novice computer system 604 can be any of a variety of appropriate computer systems, such as the user systems 130, the computer systems described as being associated with the expert user 202 and/or the novice user 204, the user computer system 502, and/or the other user computer system 542.
- the expert computer system 602 can detect motion of an expert user that is associated with the expert computer system 602 (606).
- the user computer system 502 can use the input subsystem 508 and the motion capture unit 524 to detect motion of a user associated with the computer system 502.
- the expert computer system 602 can provide information for the detected expert motion to the novice computer system 604 (608).
- the user computer system 502 can transmit information regarding motion detected by the motion capture unit 524 to the other user computer system 542 through the network 506.
- the novice computer system 604 can receive the information for the expert motion (610) and, in some implementations, can detect a current body position of the novice user (612) and determine differences between the body position of the novice user and the position of the expert user (614). Such differences may be used to provide the novice user with suggestive feedback as to which way the novice user should move his/her body from its current position so that his/her body correctly aligns with the position of the expert's body.
- the other user computer system 542 can use its input subsystem (similar to the input subsystem 508) and motion capture unit (similar to the motion capture unit 524) to detect the current position of at least a portion of a corresponding user's body, and can use a movement comparison unit (similar to the movement comparison unit 528) to determine differences between the user's current position (e.g., based on data describing the user's current position) and another user's body position (e.g., based on data describing the expert's motion).
- a movement comparison unit similar to the movement comparison unit 528) to determine differences between the user's current position (e.g., based on data describing the user's current position) and another user's body position (e.g., based on data describing the expert's motion).
- the novice computer system 604 can output representations of the expert motion (616).
- the output representations may be output with information that identifies differences between the novice user's body position and the expert user's body position, such as through visually highlighting such differences on a display device and/or providing haptic feedback at one or more appropriate haptic feedback devices located on or near the novice user's body.
- an augmented reality engine similar to the augmented reality engine 528) and/or a haptic feedback unit (similar to the haptic feedback unit 530) of the other user computer system 542 can direct one or more components of the output subsystem 532 to output one or more representations of another user's detected motion, such as the representations depicted in FIGS. 3-4.
- the novice computer system 604 can detect motion of the novice user that attempts to mimic the expert user's motion (618).
- the other user computer system 542 can use an input subsystem (similar to the input subsystem 508) and a motion capture unit (similar to the motion capture unit 524) to detect the mimicking motion of an associated user.
- the novice computer system 604 can identify differences between the expert and novice motion (620) and can output representations of the detected differences (622).
- differences can be detected by a movement comparison unit (similar to the movement comparison unit 526) and an augmented reality engine (similar to the augmented reality engine 528) and/or a haptic feedback unit (similar to the haptic feedback unit 530) of the other user computer system 542 can direct one or more components of the output subsystem 532 to output one or more representations of the differences between the associated user's motion and the motion of another user, such as the representations depicted in FIGS. 3-4.
- a movement comparison unit similar to the movement comparison unit 526) and an augmented reality engine (similar to the augmented reality engine 528) and/or a haptic feedback unit (similar to the haptic feedback unit 530) of the other user computer system 542 can direct one or more components of the output subsystem 532 to output one or more representations of the differences between the associated user's motion and the motion of another user, such as the representations depicted in FIGS. 3-4.
- the novice computer system 604 can provide information regarding the mimicking motion performed by the novice user (624) and the expert computer system 602 can receive such information (626).
- the expert computer system 602 can identify differences between the expert and novice motion (628) and can output one or more representations of the differences (630).
- the user computer system 502 can use the movement comparison unit 526 to detect differences between the motions performed by different users and the augmented reality engine 528 and/or haptic feedback unit 530 can provide output that represents the differences via the output subsystem 532, such as the representations depicted in FIGS. 3-4.
- the expert computer system 602 can detect one or more corrective motions performed by the expert user (632) and can provide information regarding the corrective motion to the novice computer system (604).
- the user computer system 502 can detect the corrective motion using the input subsystem 508 and the motion capture unit 524, similar to the description above with regard to view 3a in FIG. 3.
- the novice computer system 604 receives the information for the corrective expert motion (636) and, in some implementations, detects a current body position of the novice user (638) and determines differences between the current body position of the novice user and the corrective motion of the expert user (640).
- the steps 636-640 can be similar to the steps 610-614 described above with regard to FIG. 6A.
- the novice computer system 604 can output a representation of the corrective motion provided by expert computer system 602 (642).
- the other user computer system 542 can use an augmented reality engine (similar to the augmented reality engine 528) and/or a haptic feedback unit (similar to the haptic feedback unit 530) to provide output that identifies and/or highlights the corrective motion provided by another user, such as the output depicted in view 3b of FIG. 3.
- the novice computer system 604 can detect subsequent motion by the novice user (644) and can identify (and output) differences between the detected motion of the novice user and the corrective motion of the expert user (646).
- the steps 644 and 646 can be similar to the steps 618-622.
- the novice computer system 604 can determine whether additional corrections to the motion of the novice user are needed based, at least in part, on the identified differences between the novice user motion and the corrective motion from the expert user (648). If additional corrections are determined to be needed, then the novice computer system 604 can provide information regarding the novice input detected at step 644 to the expert computer system 602 and the technique can resume at step 626. Such a loop of, first, the expert computer system 602 determining and providing corrective motion to the novice computer system 604 and, second, the novice computer system 604 outputting the corrective motion and determining whether subsequent user motion has matched the corrective motion with at least a threshold degree of accuracy, may be repeatedly performed until no further correction is determined to be needed at step 648.
- the novice computer system 604 can provide a request for a next step to the expert computer system 602 (652).
- the expert computer system 602 can determine whether any additional steps are included in the information that is being taught to the novice user (654). If further steps are included in the information that is being taught to the novice user, then the technique 600 can repeat beginning at step 606 for the next step. If no further steps are included in the information, then the technique 600 can end.
- Embodiments of the subject matter and the operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions, encoded on computer storage medium for execution by, or to control the operation of, data processing apparatus.
- the program instructions can be encoded on an artificially-generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.
- a computer storage medium can be, or be included in, a computer-readable storage device, a computer-readable storage substrate, a random or serial access memory array or device, or a combination of one or more of them.
- a computer storage medium is not a propagated signal
- a computer storage medium can be a source or destination of computer program instructions encoded in an artificially-generated propagated signal.
- the computer storage medium can also be, or be included in, one or more separate physical components or media.
- the operations described in this specification can be implemented as operations performed by a data processing apparatus on data stored on one or more computer- readable storage devices or received from other sources.
- data processing apparatus encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, a system on a chip, or multiple ones, or combinations, of the foregoing.
- the apparatus can also include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, a cross-platform runtime environment, a virtual machine, or a combination of one or more of them.
- the apparatus and execution environment can realize various different computing model infrastructures, e.g., web services, distributed computing and grid computing infrastructures.
- a computer program (also known as a program, software, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment.
- a computer program may, but need not, correspond to a file in a file system.
- a program can be stored in a portion of a file that holds other programs or data, e.g., one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, e.g., files that store one or more modules, sub-programs, or portions of code.
- a computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer.
- a processor will receive instructions and data from a read-only memory or a random access memory or both.
- the essential elements of a computer are a processor for performing actions in accordance with instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data.
- a computer need not have such devices.
- a computer can be embedded in another device, e.g., a mobile telephone, a smart phone, a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, and a wearable computer device, to name just a few.
- Devices suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, magnetic disks, and the like.
- the processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- embodiments of the subject matter described in this specification can be implemented on a computer having a display device for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device for displaying information to the user
- a keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input and output.
Abstract
Description
Claims
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201361786081P | 2013-03-14 | 2013-03-14 | |
US14/103,341 US9854014B2 (en) | 2013-03-14 | 2013-12-11 | Motion data sharing |
PCT/US2014/020544 WO2014158851A1 (en) | 2013-03-14 | 2014-03-05 | Motion data sharing |
Publications (1)
Publication Number | Publication Date |
---|---|
EP2973502A1 true EP2973502A1 (en) | 2016-01-20 |
Family
ID=51534441
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
EP14716657.3A Ceased EP2973502A1 (en) | 2013-03-14 | 2014-03-05 | Motion data sharing |
Country Status (3)
Country | Link |
---|---|
US (1) | US9854014B2 (en) |
EP (1) | EP2973502A1 (en) |
WO (1) | WO2014158851A1 (en) |
Families Citing this family (63)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20150077234A1 (en) * | 2011-07-12 | 2015-03-19 | Aliphcom | System of wearable devices with sensors for synchronization of body motions based on haptic prompts |
US10223926B2 (en) | 2013-03-14 | 2019-03-05 | Nike, Inc. | Skateboard system |
EP2973406B1 (en) | 2013-03-14 | 2019-11-27 | NIKE Innovate C.V. | Athletic attribute determinations from image data |
US10466787B2 (en) * | 2013-04-17 | 2019-11-05 | Provenance Asset Group Llc | Haptic device for pedestrian navigation |
US9390630B2 (en) * | 2013-05-03 | 2016-07-12 | John James Daniels | Accelerated learning, entertainment and cognitive therapy using augmented reality comprising combined haptic, auditory, and visual stimulation |
US9620026B2 (en) * | 2014-02-28 | 2017-04-11 | Red Hat, Inc. | Contextual graphical user interface training |
US9690370B2 (en) * | 2014-05-05 | 2017-06-27 | Immersion Corporation | Systems and methods for viewport-based augmented reality haptic effects |
US20150352404A1 (en) * | 2014-06-06 | 2015-12-10 | Head Technology Gmbh | Swing analysis system |
KR101936532B1 (en) * | 2014-10-10 | 2019-04-03 | 후지쯔 가부시끼가이샤 | Storage medium, skill determination method, and skill determination device |
EP3062142B1 (en) | 2015-02-26 | 2018-10-03 | Nokia Technologies OY | Apparatus for a near-eye display |
WO2016168117A2 (en) | 2015-04-14 | 2016-10-20 | John James Daniels | Wearable electric, multi-sensory, human/machine, human/human interfaces |
US20180203516A1 (en) * | 2015-07-14 | 2018-07-19 | Hitachi Systems, Ltd. | Sense shared system, operation device, and sense shared method |
JP2020516327A (en) | 2016-11-25 | 2020-06-11 | キナプティック・エルエルシー | Haptic human/mechanical interface and wearable electronics methods and apparatus |
US10650552B2 (en) | 2016-12-29 | 2020-05-12 | Magic Leap, Inc. | Systems and methods for augmented reality |
EP3343267B1 (en) | 2016-12-30 | 2024-01-24 | Magic Leap, Inc. | Polychromatic light out-coupling apparatus, near-eye displays comprising the same, and method of out-coupling polychromatic light |
US10140773B2 (en) | 2017-02-01 | 2018-11-27 | Accenture Global Solutions Limited | Rendering virtual objects in 3D environments |
US10497163B1 (en) * | 2017-05-16 | 2019-12-03 | Electronic Arts Inc. | Computer architecture for animation of a character in a simulation based on muscle activation data |
US10578870B2 (en) | 2017-07-26 | 2020-03-03 | Magic Leap, Inc. | Exit pupil expander |
WO2019113570A1 (en) | 2017-12-10 | 2019-06-13 | Magic Leap, Inc. | Anti-reflective coatings on optical waveguides |
KR101918262B1 (en) * | 2017-12-19 | 2018-11-13 | (주) 알큐브 | Method and system for providing mixed reality service |
EP3729172A4 (en) | 2017-12-20 | 2021-02-24 | Magic Leap, Inc. | Insert for augmented reality viewing device |
WO2019178567A1 (en) | 2018-03-15 | 2019-09-19 | Magic Leap, Inc. | Image correction due to deformation of components of a viewing device |
EP3803450A4 (en) | 2018-05-31 | 2021-08-18 | Magic Leap, Inc. | Radar head pose localization |
CN112400157A (en) | 2018-06-05 | 2021-02-23 | 奇跃公司 | Homography transformation matrix based temperature calibration of viewing systems |
US11579441B2 (en) | 2018-07-02 | 2023-02-14 | Magic Leap, Inc. | Pixel intensity modulation using modifying gain values |
WO2020010226A1 (en) | 2018-07-03 | 2020-01-09 | Magic Leap, Inc. | Systems and methods for virtual and augmented reality |
US11856479B2 (en) | 2018-07-03 | 2023-12-26 | Magic Leap, Inc. | Systems and methods for virtual and augmented reality along a route with markers |
JP7125872B2 (en) * | 2018-07-13 | 2022-08-25 | 株式会社日立製作所 | Work support device and work support method |
EP4270016A3 (en) | 2018-07-24 | 2024-02-07 | Magic Leap, Inc. | Temperature dependent calibration of movement detection devices |
WO2020023543A1 (en) | 2018-07-24 | 2020-01-30 | Magic Leap, Inc. | Viewing device with dust seal integration |
WO2020028834A1 (en) | 2018-08-02 | 2020-02-06 | Magic Leap, Inc. | A viewing system with interpupillary distance compensation based on head motion |
EP3830631A4 (en) | 2018-08-03 | 2021-10-27 | Magic Leap, Inc. | Unfused pose-based drift correction of a fused pose of a totem in a user interaction system |
US11645930B2 (en) * | 2018-11-08 | 2023-05-09 | International Business Machines Corporation | Cognitive recall of study topics by correlation with real-world user environment |
CN117111304A (en) | 2018-11-16 | 2023-11-24 | 奇跃公司 | Image size triggered clarification for maintaining image sharpness |
JP2022519292A (en) | 2019-02-06 | 2022-03-22 | マジック リープ， インコーポレイテッド | Target Intent-based Clock Rate Determination and Adjustment to Limit Total Heat Generated by Multiple Processors |
US11175728B2 (en) * | 2019-02-06 | 2021-11-16 | High Fidelity, Inc. | Enabling negative reputation submissions in manners that reduce chances of retaliation |
EP3939030A4 (en) | 2019-03-12 | 2022-11-30 | Magic Leap, Inc. | Registration of local content between first and second augmented reality viewers |
US20200311396A1 (en) * | 2019-03-25 | 2020-10-01 | Microsoft Technology Licensing, Llc | Spatially consistent representation of hand motion |
US11562598B2 (en) * | 2019-03-25 | 2023-01-24 | Microsoft Technology Licensing, Llc | Spatially consistent representation of hand motion |
US11445232B2 (en) * | 2019-05-01 | 2022-09-13 | Magic Leap, Inc. | Content provisioning system and method |
TW202116381A (en) * | 2019-07-10 | 2021-05-01 | 日商Ｉｐｃ股份有限公司 | Action experience system and actuator wearing device |
CN114174895A (en) | 2019-07-26 | 2022-03-11 | 奇跃公司 | System and method for augmented reality |
JP2023502927A (en) | 2019-11-15 | 2023-01-26 | マジック リープ， インコーポレイテッド | Visualization system for use in a surgical environment |
US11263570B2 (en) * | 2019-11-18 | 2022-03-01 | Rockwell Automation Technologies, Inc. | Generating visualizations for instructional procedures |
US11733667B2 (en) * | 2019-11-18 | 2023-08-22 | Rockwell Automation Technologies, Inc. | Remote support via visualizations of instructional procedures |
US11638147B2 (en) * | 2019-11-22 | 2023-04-25 | International Business Machines Corporation | Privacy-preserving collaborative whiteboard using augmented reality |
US11960651B2 (en) * | 2020-03-30 | 2024-04-16 | Snap Inc. | Gesture-based shared AR session creation |
US11743340B2 (en) * | 2020-06-10 | 2023-08-29 | Snap Inc. | Deep linking to augmented reality components |
US11233973B1 (en) * | 2020-07-23 | 2022-01-25 | International Business Machines Corporation | Mixed-reality teleconferencing across multiple locations |
US11562528B2 (en) | 2020-09-25 | 2023-01-24 | Apple Inc. | Devices, methods, and graphical user interfaces for interacting with three-dimensional environments |
CN112230836B (en) * | 2020-11-02 | 2022-05-27 | 网易（杭州）网络有限公司 | Object moving method and device, storage medium and electronic device |
US11954242B2 (en) | 2021-01-04 | 2024-04-09 | Apple Inc. | Devices, methods, and graphical user interfaces for interacting with three-dimensional environments |
EP4288950A1 (en) | 2021-02-08 | 2023-12-13 | Sightful Computers Ltd | User interactions in extended reality |
JP2024507749A (en) | 2021-02-08 | 2024-02-21 | サイトフル コンピューターズ リミテッド | Content sharing in extended reality |
KR20230144042A (en) | 2021-02-08 | 2023-10-13 | 사이트풀 컴퓨터스 리미티드 | Extended Reality for Productivity |
JP7082384B1 (en) * | 2021-05-28 | 2022-06-08 | 浩平 田仲 | Learning system and learning method |
US20220413433A1 (en) * | 2021-06-28 | 2022-12-29 | Meta Platforms Technologies, Llc | Holographic Calling for Artificial Reality |
WO2023009580A2 (en) | 2021-07-28 | 2023-02-02 | Multinarity Ltd | Using an extended reality appliance for productivity |
US11934569B2 (en) * | 2021-09-24 | 2024-03-19 | Apple Inc. | Devices, methods, and graphical user interfaces for interacting with three-dimensional environments |
US11948263B1 (en) | 2023-03-14 | 2024-04-02 | Sightful Computers Ltd | Recording the complete physical and extended reality environments of a user |
US20230334795A1 (en) | 2022-01-25 | 2023-10-19 | Multinarity Ltd | Dual mode presentation of user interface elements |
KR20230147312A (en) * | 2022-04-14 | 2023-10-23 | 주식회사 피아몬드 | Method and system for providing privacy in virtual space |
US20240073372A1 (en) * | 2022-08-31 | 2024-02-29 | Snap Inc. | In-person participant interaction for hybrid event |
Family Cites Families (16)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
DE2221767A1 (en) | 1972-05-04 | 1973-11-15 | Johann Prof Dr-I Kleinwaechter | TEACHER-STUDENT EQUIPMENT FOR LEARNING PHYSICAL MOVEMENTS |
US5649104A (en) * | 1993-03-19 | 1997-07-15 | Ncr Corporation | System for allowing user of any computer to draw image over that generated by the host computer and replicating the drawn image to other computers |
US5846086A (en) * | 1994-07-01 | 1998-12-08 | Massachusetts Institute Of Technology | System for human trajectory learning in virtual environments |
US5679004A (en) * | 1995-12-07 | 1997-10-21 | Movit, Inc. | Myoelectric feedback system |
US20030061285A1 (en) * | 2001-09-25 | 2003-03-27 | Tatsuo Usui | Interactive communication system and method using an image |
EP1686554A3 (en) * | 2005-01-31 | 2008-06-18 | Canon Kabushiki Kaisha | Virtual space generating system, image processing apparatus and information processing method |
JP4738870B2 (en) | 2005-04-08 | 2011-08-03 | キヤノン株式会社 | Information processing method, information processing apparatus, and remote mixed reality sharing apparatus |
US8953909B2 (en) | 2006-01-21 | 2015-02-10 | Elizabeth T. Guckenberger | System, method, and computer software code for mimic training |
DE102009002747A1 (en) | 2009-04-30 | 2010-11-04 | Robert Bosch Gmbh | Method and device for evaluating a movement of a patient |
US20110275045A1 (en) * | 2010-01-22 | 2011-11-10 | Foerster Bhupathi International, L.L.C. | Video Overlay Sports Motion Analysis |
IT1399855B1 (en) * | 2010-04-28 | 2013-05-09 | Technogym Spa | APPARATUS FOR THE ASSISTED EXECUTION OF A GYMNASTIC EXERCISE. |
US20110316845A1 (en) * | 2010-06-25 | 2011-12-29 | Palo Alto Research Center Incorporated | Spatial association between virtual and augmented reality |
KR101007944B1 (en) * | 2010-08-24 | 2011-01-14 | 윤상범 | System and method for cyber training of martial art on network |
US9345957B2 (en) * | 2011-09-30 | 2016-05-24 | Microsoft Technology Licensing, Llc | Enhancing a sport using an augmented reality display |
US20130222565A1 (en) * | 2012-02-28 | 2013-08-29 | The Johns Hopkins University | System and Method for Sensor Fusion of Single Range Camera Data and Inertial Measurement for Motion Capture |
US10163049B2 (en) * | 2013-03-08 | 2018-12-25 | Microsoft Technology Licensing, Llc | Inconspicuous tag for generating augmented reality experiences |
-
2013
- 2013-12-11 US US14/103,341 patent/US9854014B2/en active Active
-
2014
- 2014-03-05 WO PCT/US2014/020544 patent/WO2014158851A1/en active Application Filing
- 2014-03-05 EP EP14716657.3A patent/EP2973502A1/en not_active Ceased
Non-Patent Citations (2)
Title |
---|
None * |
See also references of WO2014158851A1 * |
Also Published As
Publication number | Publication date |
---|---|
WO2014158851A1 (en) | 2014-10-02 |
US20140282105A1 (en) | 2014-09-18 |
US9854014B2 (en) | 2017-12-26 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US9854014B2 (en) | Motion data sharing | |
LaViola Jr et al. | 3D user interfaces: theory and practice | |
Durupinar et al. | Perform: Perceptual approach for adding ocean personality to human motion using laban movement analysis | |
Steinicke et al. | Human walking in virtual environments | |
US20140298200A1 (en) | Providing user interface elements for interactive sessions | |
Caggianese et al. | Freehand-steering locomotion techniques for immersive virtual environments: A comparative evaluation | |
Pfeiffer et al. | EyeSee3D 2.0: Model-based real-time analysis of mobile eye-tracking in static and dynamic three-dimensional scenes | |
Chen et al. | Sensecollect: We need efficient ways to collect on-body sensor-based human activity data! | |
CN109144244A (en) | A kind of method, apparatus, system and the augmented reality equipment of augmented reality auxiliary | |
Gieser et al. | Real-time static gesture recognition for upper extremity rehabilitation using the leap motion | |
Tricomi et al. | You can’t hide behind your headset: User profiling in augmented and virtual reality | |
US8655804B2 (en) | System and method for determining a characteristic of an individual | |
Arai et al. | Eye-based human computer interaction allowing phoning, reading e-book/e-comic/e-learning, internet browsing, and tv information extraction | |
Heimerdinger et al. | Modeling the interactions of context and style on affect in motion perception: stylized gaits across multiple environmental contexts | |
US9053363B1 (en) | Object identification in visual media | |
Neumann et al. | AVIKOM: towards a mobile audiovisual cognitive assistance system for modern manufacturing and logistics | |
Zhang et al. | A novel animation authoring framework for the virtual teacher performing experiment in mixed reality | |
Mohajir | Identifying learning style through eye tracking technology in adaptive learning systems | |
JP2018205978A (en) | Information extracting device and information extracting method | |
CN112424736A (en) | Machine interaction | |
Shumaker et al. | Virtual, Augmented and Mixed Reality: Applications of Virtual and Augmented Reality: 6th International Conference, VAMR 2014, Held as Part of HCI International 2014, Heraklion, Crete, Greece, June 22-27, 2014, Proceedings, Part II | |
Trajkova et al. | Current use, non-use, and future use of ballet learning technologies | |
Gutiérrez et al. | Characterization of Quality Attributes to Evaluate the User Experience in Augmented Reality | |
Guo et al. | Comparing the tangible tutorial system and the human teacher in intangible cultural heritage education | |
Thalmann | Sensors and actuators for HCI and VR: a few case studies |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PUAI | Public reference made under article 153(3) epc to a published international application that has entered the european phase |
Free format text: ORIGINAL CODE: 0009012 |
|
17P | Request for examination filed |
Effective date: 20150915 |
|
AK | Designated contracting states |
Kind code of ref document: A1Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
AX | Request for extension of the european patent |
Extension state: BA ME |
|
DAX | Request for extension of the european patent (deleted) | ||
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: EXAMINATION IS IN PROGRESS |
|
17Q | First examination report despatched |
Effective date: 20170601 |
|
RAP1 | Party data changed (applicant data changed or rights of an application transferred) |
Owner name: GOOGLE LLC |
|
RAP1 | Party data changed (applicant data changed or rights of an application transferred) |
Owner name: VERILY LIFE SCIENCES LLC |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: EXAMINATION IS IN PROGRESS |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R003 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: THE APPLICATION HAS BEEN REFUSED |
|
18R | Application refused |
Effective date: 20201014 |