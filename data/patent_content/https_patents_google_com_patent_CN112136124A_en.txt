CN112136124A - Dependency graph conversation modeling for human-machine conversation sessions with computer-implemented automated assistants - Google Patents
Dependency graph conversation modeling for human-machine conversation sessions with computer-implemented automated assistants Download PDFInfo
- Publication number
- CN112136124A CN112136124A CN201880093458.2A CN201880093458A CN112136124A CN 112136124 A CN112136124 A CN 112136124A CN 201880093458 A CN201880093458 A CN 201880093458A CN 112136124 A CN112136124 A CN 112136124A
- Authority
- CN
- China
- Prior art keywords
- action
- user
- parameters
- assistant
- dependency graph
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
- 230000009471 action Effects 0.000 claims abstract description 190
- 238000000034 method Methods 0.000 claims description 235
- 230000004044 response Effects 0.000 claims description 28
- 230000015654 memory Effects 0.000 claims description 10
- 230000000977 initiatory effect Effects 0.000 claims description 9
- 238000004891 communication Methods 0.000 claims description 7
- 230000003993 interaction Effects 0.000 abstract description 11
- 235000013550 pizza Nutrition 0.000 description 12
- 239000008186 active pharmaceutical agent Substances 0.000 description 8
- 230000008569 process Effects 0.000 description 8
- 238000010801 machine learning Methods 0.000 description 6
- 238000010586 diagram Methods 0.000 description 5
- 230000002452 interceptive effect Effects 0.000 description 5
- 239000000463 material Substances 0.000 description 5
- 238000012549 training Methods 0.000 description 5
- 230000008901 benefit Effects 0.000 description 4
- 230000008859 change Effects 0.000 description 4
- 238000012790 confirmation Methods 0.000 description 4
- 230000000007 visual effect Effects 0.000 description 4
- 238000013475 authorization Methods 0.000 description 3
- 230000000694 effects Effects 0.000 description 3
- 235000013305 food Nutrition 0.000 description 3
- 230000006870 function Effects 0.000 description 3
- 238000012545 processing Methods 0.000 description 3
- 238000013528 artificial neural network Methods 0.000 description 2
- 239000003795 chemical substances by application Substances 0.000 description 2
- 230000007246 mechanism Effects 0.000 description 2
- 230000002093 peripheral effect Effects 0.000 description 2
- 238000010079 rubber tapping Methods 0.000 description 2
- 240000005561 Musa balbisiana Species 0.000 description 1
- 235000018290 Musa x paradisiaca Nutrition 0.000 description 1
- 238000013473 artificial intelligence Methods 0.000 description 1
- 230000003190 augmentative effect Effects 0.000 description 1
- 238000004590 computer program Methods 0.000 description 1
- 238000001514 detection method Methods 0.000 description 1
- 238000011161 development Methods 0.000 description 1
- 230000018109 developmental process Effects 0.000 description 1
- 235000013399 edible fruits Nutrition 0.000 description 1
- 239000011521 glass Substances 0.000 description 1
- 230000006266 hibernation Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000004519 manufacturing process Methods 0.000 description 1
- 238000013507 mapping Methods 0.000 description 1
- 235000012054 meals Nutrition 0.000 description 1
- 230000003278 mimic effect Effects 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 238000003058 natural language processing Methods 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 230000002085 persistent effect Effects 0.000 description 1
- 230000009467 reduction Effects 0.000 description 1
- 235000013580 sausages Nutrition 0.000 description 1
- 238000004088 simulation Methods 0.000 description 1
- 230000011273 social behavior Effects 0.000 description 1
- 230000029305 taxis Effects 0.000 description 1
- 230000001960 triggered effect Effects 0.000 description 1
- 238000012795 verification Methods 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/953—Querying, e.g. by the use of web search engines
- G06F16/9535—Search customisation based on user profiles and personalisation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/30—Semantic analysis
- G06F40/35—Discourse or dialogue representation
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/183—Speech classification or search using natural language modelling using context dependencies, e.g. language models
- G10L15/19—Grammatical context, e.g. disambiguation of the recognition hypotheses based on word sequence rules
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L51/00—User-to-user messaging in packet-switching networks, transmitted according to store-and-forward or real-time protocols, e.g. e-mail
- H04L51/02—User-to-user messaging in packet-switching networks, transmitted according to store-and-forward or real-time protocols, e.g. e-mail using automatic reactions or user delegation, e.g. automatic replies or chatbot-generated messages
Abstract
The conversation is modeled using a dependency graph data structure to facilitate user interaction with the automated assistant when performing actions performed by the computing service. The automated assistant may utilize the dependency graph data structure to direct or otherwise control a human-machine conversation session with the user, for example, by generating one or more outputs or prompts presented to the user on a computing device operated by the user, thereby enabling efficient use of the technical hardware.
Description
Background
A human being may have a human-to-human conversation with an interactive software application, also referred to herein as an "automated assistant" (also referred to as a "chat bot," "interactive personal assistant," "intelligent personal assistant," "personal voice assistant," "conversation agent," etc.). For example, humans-which may be referred to as "users" when they interact with an automated assistant-may provide commands, queries, and/or requests using free-form natural language input. Free-form natural language may include spoken utterances that are converted to text using speech recognition and/or typed free-form language input.
Automated assistants enable users to obtain information, access services, and/or perform various tasks. For example, a user may be able to perform searches, obtain directions, and in some cases interact with third party computing services. These third-party computing service interactions, which may facilitate access using application programming interfaces ("APIs") provided by the automated assistant and/or the third-party computing service, allow the user to perform various actions, such as calling a car from a car-sharing application, ordering goods or services (e.g., pizza), making reservations, and so forth.
Automated assistants use speech recognition and natural language processing to speak with users, some of which also utilize machine learning and other artificial intelligence techniques, for example, to predict user intent. Automated assistants may be adapted to keep talking to users in a natural, intuitive manner, in part because they understand the context of the conversation. To take advantage of the conversation context, the automated assistant can retain the latest input from the user, questions from the user, and/or responses/questions provided by the automated assistant. For example, a user may ask "Where is the close coffee shop? (where the nearest coffee shop. The user may then ask: "How late is it open? (business to several points. By preserving at least some form of dialog context, the automated assistant is able to determine that the pronoun "it" refers to a "coffee shop" (i.e., to disambiguation).
However, with conventional automated assistants, particularly for actions associated with third parties and other computing services that require a large number of input parameters to be performed efficiently, modeling the conversation to collect the required input parameters can be complex and labor intensive, and can necessitate a large development overhead associated with dealing with all possible inputs and combinations and sequences thereof. These complex models may occupy a relatively large amount of space in computer memory, and may also need to be updated over time to account for previously unpredictable inputs and their combinations and orderings. Conversation may also be constrained and inflexible to various user inputs, resulting in more robotic, less-realistic interaction with the automated assistant.
Disclosure of Invention
Techniques for modeling conversations using dependency graph data structures to facilitate user interaction with an automated assistant when performing actions performed by a computing service are described herein. In some implementations, the automated assistant can utilize the dependency graph data structure to direct or otherwise control a human-machine conversation session with the user, e.g., by generating one or more outputs or prompts that are presented to the user on a computing device operated by the user.
As will be explained in more detail below, the concepts described herein may enable an automated assistant to obtain selected desired information in an efficient manner, and in so doing, may allow the automated assistant to operate in an efficient manner while also efficiently using hardware resources on a user computing device (e.g., a client computing device). For example, the concepts described herein may allow an automated assistant to most efficiently use microphone and/or speaker hardware on a user computing device so that power consumption on the device may be reduced.
Thus, according to an aspect of the invention, a method implemented using one or more processors may comprise: determining an action to be performed by a computing service for a user of a computing device in communication with an automated assistant implemented at least in part by one or more processors; and causing, by the automated assistant, the computing device to perform an action for a user of the computing device by: accessing a dependency graph data structure for the action, the dependency graph data structure comprising a plurality of nodes and a plurality of directed edges, each node identifying an action method of accessing the computing service or an assistant method of collecting one or more parameters for performing the action, and each directed edge connecting a respective pair of nodes from the plurality of nodes and identifying at least one parameter generated by the action method or the assistant method identified by one node of the respective pair of nodes and utilized by the action method or the assistant method identified by another node of the respective pair of nodes; conducting a human-machine dialog session between a user and an automated assistant to determine one or more parameters for performing an action, including generating one or more natural language outputs using a dependency graph data structure for presentation by a computing device operated by the user and receiving one or more instances of free-form natural language received at one or more input components of the computing device operated by the user; and initiating execution of an action by the computing service using the determined one or more parameters.
In some implementations, a first node of the plurality of nodes in the dependency graph data structure recognizes a first action method that invokes the computing service, and initiating initiation of execution of the action by the computing service includes executing the first action method. Also, in some implementations, the first action method includes a call operation, the call operation includes a plurality of inputs, and executing the first action method includes executing the call operation using the determined one or more parameters as inputs of the plurality of inputs for the call operation.
Further, in some embodiments, the dependency graph data structure includes a specification for defining methods and dependencies between them, and the first action method is marked indexable in the specification to indicate that the first action method calls the computing service to perform the action. In some implementations, the first action method invokes a computing service to perform the action, and a second node of the plurality of nodes in the dependency graph data structure recognizes a second action method that invokes the computing service to obtain intermediate data for performing the action.
Additionally, in some implementations, a first node of the plurality of nodes in the dependency graph data structure recognizes a first assistant method that includes a first prompt requesting a first parameter, and generating the one or more natural language outputs includes executing the first assistant method to generate a first natural language output that includes the first prompt. In some implementations, determining that the action to be performed by the computing service is performed by the automated assistant and based on an initial natural language input received at the computing device of the user and specifying a first parameter recognized by a first directed edge in the dependency graph data structure, the first directed edge connecting a first node and a second node, the first node recognizing a first assistant method that generates the first parameter, the second node recognizing a first action method that utilizes the first parameter, and conducting a human-machine dialog session between the user and the automated assistant includes bypassing generating the natural language output to request the first parameter in response to determining that the first parameter is specified in the initial natural language input.
Additionally, in some embodiments, a first node and a second node are connected by a first parameter identified by a first directed edge in the dependency graph data structure, the first node identifies a first assistant method that generates the first parameter, the second node identifies a first action method that utilizes the first parameter, and conducting a human-machine dialog session between the user and the automated assistant includes bypassing generating a natural language output to request the first parameter in response to determining that the first parameter is determinable without requiring a request for the first parameter from the user. Further, in some implementations, determining that the first parameter is determinable without requiring a request for the first parameter from the user includes determining that the first parameter is determinable from stored data associated with the user.
In some implementations, conducting a human-machine dialog session between a user and an automated assistant to determine one or more parameters for performing an action includes ranking one or more generated natural language outputs using a dependency graph data structure. Some embodiments may also include: responsive to receiving a first instance of free-form natural language in one or more instances of free-form natural language input, invalidating one or more of the determined one or more parameters during the human-machine dialog session; accessing the dependency graph data structure to identify one or more action methods or assistant methods identified in the dependency graph data structure to re-execute in order to update one or more invalid parameters; and re-executing one or more of the identified action methods or assistant methods to update one or more invalid parameters.
Further, in some embodiments, invalidating one or more parameters keeps one or more other parameters valid, so as to bypass re-execution of one or more action methods or assistant methods for determining one or more other parameters when re-executing one or more recognized action methods or assistant methods. In some implementations, determining the action is performed by an automated assistant and includes selecting the action from a plurality of actions using a dependency graph data structure. Additionally, in some implementations, the computing service is a third party computing service, and initiating performance of the action by the computing service using the determined one or more parameters includes invoking the third party computing service using the determined one or more parameters.
In some implementations, the computing service is a cloud computing service, and in some implementations, the computing service resides on a computing device operated by a user. Also, in some embodiments, the dependency graph data structure defines a directed acyclic graph. In some embodiments, the action creates a reservation, a first node of the plurality of nodes recognizes a first action method, the first action method invokes the computing service to search for an available time slot, and a second node of the plurality of nodes recognizes a second action method, the second action method invokes the computing service to reserve the available time slot. Additionally, in some embodiments, the action obtains a ticket to the product or event.
According to another aspect of the invention, a method implemented using one or more processors may comprise: receiving, by a computer interface, a plurality of parameters to be used by an automated assistant, thereby causing a computing service to perform an action with respect to a user of a computing device in communication with the automated assistant; determining a plurality of dependencies between a plurality of parameters; building, using one or more processors, a dependency graph data structure configured for conducting a human-machine conversation session between a user and an automated assistant to determine one or more parameters for performing an action, including for generating one or more natural language outputs to be presented by a computing device operated by the user, wherein building the dependency graph data structure for the action comprises: building a plurality of nodes for the dependency graph data structure using the received plurality of parameters, including building at least one node that recognizes an action method for accessing the computing service, and building at least one assistant method that collects one or more parameters from the received plurality of parameters; and constructing a plurality of directed edges for the dependency graph data structure using the determined plurality of dependencies, each directed edge connecting a respective pair of nodes of the plurality of nodes and identifying at least one parameter from the received plurality of parameters, the at least one parameter being generated by an action method or an assistant method identified by one node of the respective pair of nodes and being utilized by an action method or an assistant method identified by another node of the respective pair of nodes; and generating a call operation in the action method that calls the computing service through the one or more received plurality of parameters to perform the action.
Moreover, in some implementations, the computing service is a third party computing service, and the invoking operation is configured to access the third party computing service. In some implementations, determining the plurality of dependencies includes receiving the plurality of dependencies through a computer interface. Further, in some embodiments, receiving the plurality of parameters includes receiving a plurality of action invocation definitions.
Further, some embodiments may include a system comprising one or more processors and memory operably coupled to the one or more processors, wherein the memory stores instructions that, in response to execution of the instructions by the one or more processors, cause the one or more processors to perform any of the foregoing methods. Some embodiments also include at least one non-transitory computer-readable medium comprising instructions that, in response to execution of the instructions by one or more processors, cause the one or more processors to perform any of the foregoing methods.
It should be appreciated that all combinations of the above concepts and additional concepts described in greater detail herein are contemplated as being part of the subject matter disclosed herein. For example, all combinations of claimed subject matter appearing at the end of this disclosure are contemplated as being part of the subject matter disclosed herein.
Drawings
FIG. 1 is a block diagram of an example computing environment in which embodiments disclosed herein may be implemented.
FIG. 2 is a block diagram of an example dependency graph data structure for modeling conversations reserving a reservation action for an example restaurant.
Fig. 3A-3G illustrate example embodiments of actions and assistant methods represented in the dependency graph data structure of fig. 2.
FIG. 4 is a block diagram of an example dependency graph data structure for modeling a conversation for an example event ticket purchase action.
5A-5I illustrate example embodiments of actions and assistant methods represented in the dependency graph data structure of FIG. 4.
FIG. 6 is a flow diagram illustrating an example sequence of operations for conducting a human-machine conversation session using a dependency graph data structure, in accordance with various embodiments.
FIG. 7 illustrates an example restaurant reservation conversation conducted between a user and an automated assistant using the user's computing device, according to various embodiments.
FIG. 8 illustrates another example restaurant reservation conversation conducted between a user and an automated assistant using the user's computing device, according to various embodiments.
FIG. 9 illustrates an example event ticket purchase conversation between a user and an automated assistant using a separate interactive speaker, according to various embodiments.
FIG. 10 illustrates a flowchart of an example sequence of operations for generating and deploying a dependency graph data structure, in accordance with various embodiments.
Fig. 11 illustrates an example architecture of a computing device.
Detailed Description
Turning now to fig. 1, an example environment is illustrated in which the techniques disclosed herein may be implemented. The example environment includes a plurality of client computing devices 1061-N. Each client device 106 may execute a respective instance of the automated assistant client 118. One or more cloud-based automated assistant components 119, such as natural language understanding engine 135, may be implemented on one or more computing systems (collectively "cloud" computing systems) communicatively coupled to client device 106 via one or more local and/or wide area networks (e.g., the internet), indicated generally at 1101-N。
In some implementations, the instance of the automated assistant client 118 through its interaction with the one or more cloud-based automated assistant components 119 can form a logical instance of the automated assistant 120 that appears from the user's perspective to be the user with whom the user can engage in a human-machine conversation. Two examples of such automated assistants 120 are depicted in fig. 1. The first automated assistant 120A, which is included by the dashed line, is for operating the first client device 1061And includes an automated assistant client 1181And one or more cloud-based automated assistant components 119.
It should therefore be appreciated that each user engaged with the automated assistant client 118 executing on the client device 106 may actually engage with his or her own logical instance of the automated assistant 120. For the sake of brevity and simplicity, the term "automatic assistant" as used herein as "serving" a particular user will refer to a combination of an automatic assistant client 118 and one or more cloud-based automatic assistant components 119 (which may be shared among multiple automatic assistant clients 118) executing on a client device 106 operated by the user. It should also be understood that in some implementations, the automated assistant 120 may respond to requests from any user, regardless of whether the user is actually "served" by that particular instance of the automated assistant 120.
As described in more detail herein, the automated assistant 120 is via one or more client devices 1061-NTo conduct a human-machine conversation session with one or more users. In some implementations, the automated assistant 120 can be responsive to being responded to by a user via the client device 1061-NIs provided by one or more user interface input devices of one of the plurality of devices to conduct a human-machine conversation session with the user. At this pointIn some of these implementations, the user interface input is explicitly directed to the automated assistant 120. For example, message exchange client 1071-NMay be a personal assistant messaging service dedicated to talking with the automated assistant 120, and user interface input provided via the personal assistant messaging service may be automatically provided to the automated assistant 120. Additionally, for example, user interface inputs may be explicitly directed to the messaging client 107 based on a particular user interface input indicating that the automated assistant 120 is to be invoked1-NAn automated assistant 120 in one or more of. For example, the particular user interface input may be one or more typed characters (e.g., @ automated assistant), user interaction with hardware buttons and/or virtual buttons (e.g., tapping, long tapping), spoken commands (e.g., "hi automated assistant"), and/or other particular user interface inputs.
In some implementations, the automated assistant 120 can conduct a human-to-machine conversation session in response to user interface input even when the user interface input is not explicitly directed to the automated assistant 120. For example, the automated assistant 120 may examine the content of the user interface input and conduct a dialog session in response to the presence of certain terms in the user interface input and/or based on other cues. In many implementations, the user can speak a command, search, etc., and the automated assistant 120 can utilize speech recognition to convert the utterance into text and respond to the text accordingly, e.g., by providing search results, general information, and/or taking one or more responsive actions (e.g., playing media, starting a game, ordering food, etc.). In some implementations, the automated assistant 120 can additionally or alternatively respond to the utterance without converting the utterance to text. For example, the automated assistant 120 may convert the voice input into an embedding, into an entity representation (that indicates one or more entities present in the voice input), and/or other "non-text" representations and operate on such non-text representations. Thus, embodiments described herein as operating based on text converted from voice input may additionally and/or alternatively operate directly on voice input and/or other non-text representations of voice input.
As noted above, in various embodiments, the client computing device 1061-NEach of which may operate the automated assistant client 118. In various implementations, each automatic assistant client 118 may include a respective speech capture/text-to-speech ("TTS")/speech-to-text ("STT") module 114. In other implementations, one or more aspects of the speech capture/TTS/STT module 114 may be implemented separately from the automated assistant client 118. In various implementations, the speech capture/TTS/STT module 114 may generate speech recognition output based on the spoken query.
Each speech capture/TTS/STT module 114 may be configured to perform one or more functions: capturing a user's voice, for example, via a microphone; converting the captured audio to text (and/or to other representations or embeddings); and/or converting text to speech. For example, in some implementations, because the client devices 106 may be relatively constrained in terms of computing resources (e.g., processor cycles, memory, battery, etc.), the speech capture/TTS/STT module 114 local to each client device 106 may be configured to convert a limited number of different spoken phrases, particularly phrases invoking the automatic assistant 120, into text (or into other forms, such as reduced dimensionality embedding). Other speech inputs may be sent to a cloud-based automated assistant component 119, which is implemented at one or more cloud-based computer servers, which may include a cloud-based TTS module 116 and/or a cloud-based STT module 117.
The cloud-based STT module 117 may be configured to utilize the virtually unlimited resources of the cloud to convert audio data captured by the speech capture/TTS/STT module 114 into text (which may then be provided to the natural language processor 122). Cloud-based TTS module 116 may be configured to utilize the virtually unlimited resources of the cloud to convert text data (e.g., natural language responses formulated by automated assistant 120) into computer-generated speech output. In some implementations, the TTS module 116 can provide the computer-generated speech output to the client device 106 for direct output, e.g., using one or more speakers. In other implementations, the text data (e.g., natural language response) generated by the automated assistant 120 can be provided to the speech capture/TTS/STT module 114, which can then convert the text data into computer-generated speech that is output locally.
The automatic assistant 120 (and in particular the cloud-based automatic assistant component 119) can include a natural language understanding engine 135, the aforementioned TTS module 116, the aforementioned STT module 117, and other components described in greater detail below. In some implementations, one or more of the engines and/or modules of the automated assistant 120 may be omitted, combined, and/or implemented in a component separate from the automated assistant 120. In some implementations, to protect privacy, one or more components of the automatic assistant 120, such as the natural language understanding engine 135, the speech capture/TTS/STT module 114, and so forth, may be implemented at least partially on the client device 106 (e.g., excluded from the cloud).
In some implementations, the automated assistant 120 responds by the client device 1061-NGenerates responsive content from various inputs generated by a user of one of the automated assistants 120 during a human-to-machine conversation session with the automated assistant 120. The automated assistant 120 can provide responsive content (e.g., over one or more networks when separate from the user's client device) for presentation to the user as part of a conversation session. For example, the automated assistant 120 may respond via the client device 1061-NThe response content is generated from the free-form natural language input provided by one of the first and second processors. As used herein, free-form natural language input is input that is formulated by a user and is not limited to a set of options presented for selection by the user.
As used herein, a "conversation session" may include a logically self-contained exchange of one or more messages between a user and the automated assistant 120 (and in some cases, other human participants). The automatic assistant 120 can distinguish between multiple conversation sessions with the user based on various signals, such as the passage of time between sessions, a change in user context between sessions (e.g., location, before/during/after scheduling a meeting, etc.), detection of one or more intermediate interactions between the user and the client device rather than a conversation between the user and the automatic assistant (e.g., the user temporarily switches applications, the user walks away and then later returns to a standalone voice activated product), locking/hibernation of the client device between sessions, a change in the client device for interfacing with one or more instances of the automatic assistant 120, a change in input/output ("I/O") modality employed by the user, and so forth.
In some implementations, the natural language processor 122 is configured to recognize and annotate various types of grammar information in the natural language input. For example, the natural language processor 122 may include a morpheme engine that may separate individual words into morphemes and/or annotate morphemes, e.g., with their categories. The natural language processor 122 may also include a portion of a part-of-speech tagger configured to annotate items with their grammatical roles. For example, a part-of-speech tagger may tag each item with a part-of-speech such as "noun," "verb," "adjective," "pronoun," and the like. Additionally, for example, in some implementations, the natural language processor 122 can additionally and/or alternatively include a dependency parser (not depicted) configured to determine syntactic relationships between terms in the natural language input. For example, the dependency parser may determine which terms modify other terms, subjects and verbs of the sentence, etc. (e.g., parse trees) — and may annotate such dependencies.
In some implementations, the natural language processor 122 can additionally and/or alternatively include an entity annotator (not depicted) configured to annotate entity references in one or more segments, such as references to persons (including, for example, literary characters, celebrities, public characters, etc.), organizations, locations (both real and fictional), and so forth. In some implementations, data about the entities may be stored in one or more databases, such as in a knowledge graph (not depicted). In some implementations, the knowledge graph can include nodes representing known entities (and in some cases, entity attributes) and edges connecting the nodes and representing relationships between the entities. For example, a "banana" node may be connected (e.g., as a child node) to a "fruit" node, which in turn may be connected (e.g., as a child node) to a "production" and/or "food" node. As another example, a restaurant called a "fictitious cafe" may be represented by a node that also includes attributes such as its address, type of food served, business hours, contact information, and the like. A "fictitious cafe" node may in some embodiments be connected by edges (e.g., representing a child-parent relationship) to one or more other nodes, such as a "restaurant" node, a node representing a city and/or state in which the restaurant is located, and so forth.
The entity labeler of the natural language processor 122 may annotate references to entities at a high level of granularity (e.g., to enable recognition of all references to a class of entities such as a person) and/or at a lower level of granularity (e.g., to enable recognition of all references to a particular entity such as a particular person). The entity labeler can rely on the content of the natural language input to resolve a particular entity and/or can optionally communicate with a knowledge graph or other entity database to resolve a particular entity.
In some implementations, the natural language processor 122 can additionally and/or alternatively include a coreference parser (not depicted) configured to group or "cluster" references to the same entity based on one or more contextual cues. For example, a coreference parser may be utilized to parse an item "there" in the natural language input "I like a hypothetical cafe where we last used a meal" into a "hypothetical cafe".
One or more components of the natural language processor 122 may rely on annotations from one or more other components of the natural language processor 122. For example, in some implementations, a named entity tagger can rely on annotations from co-referent resolvers and/or dependency resolvers in annotating all references to a particular entity. Additionally, for example, in some implementations, the coreference resolver may rely on annotations from dependency resolvers when clustering references to the same entity. In some implementations, in processing a particular natural language input, one or more components of the natural language processor 122 may determine one or more annotations using related prior inputs and/or other related data in addition to the particular natural language input.
The natural language understanding engine 135 may also include an intent matcher 136, the intent matcher 136 configured to determine an intent of a user conducting a human-machine dialog session with the automatic assistant 120 based on the annotation output of the natural language processor 122. Although shown separately from the natural language processor 122 in fig. 1, in other embodiments, the intent matcher 136 may be an integrated part of the natural language processor 122 (or more generally, a pipeline including the natural language processor 122). In some implementations, the natural language processor 122 and the intent matcher 136 may collectively form the "natural language understanding" engine 135 described above.
The intent matcher 136 may use various techniques to determine the intent of the user. In some implementations, the intent matcher 136 may access one or more databases 137, the databases 137 including, for example, a plurality of mappings between grammars and response actions (or, more generally, intents). Additionally or alternatively, in some implementations, the one or more databases 137 may store one or more machine learning models trained to generate an output indicative of the user's intent based on the user's input.
Grammars may be selected, formulated (e.g., manually) and/or learned over time, for example, to represent the most common intent of a user. For example, a grammar "play < artist >" may be mapped to an intent that invokes a response action that causes music of < artist > to be played on a user-operated client device 106. Another grammar "[ weather | for ] today ([ weather forecast today ])" may be matched to user queries, such as "what's the weather today" and "what's the weather for today? (how is the weather forecast today). As shown in the "play < artist >" example syntax, some syntaxes have slots (e.g., < artist >) that can be filled with slot values (or "parameters"). The slot value may be determined in various ways. The user will typically actively provide the slot value. For example, for the grammar "Order me a < ordering > pizza", the user might speak the phrase "Order me a usage pizza" (i Order me a sausage pizza), in which case the slot < ordering > is automatically filled. Additionally or alternatively, if the user invokes a grammar that includes slots to be filled with slot values, without the user having to actively provide the slot values, the automated assistant 120 may solicit those slot values from the user (e.g., "what type of crust is desired on your pizza.
In contrast to many grammars (which may be created manually), for example, a log of interactions between a user and an automated assistant may be used to automatically train a machine learning model. The machine learning model may take various forms, such as a neural network. They may be trained in various ways to predict user intent from user input. For example, in some implementations, training data including individual training examples may be provided. Each training example may include free-form input (e.g., in textual or non-textual form), such as from a user, and may be labeled with intent (e.g., manually). The training examples may be applied as inputs across a machine learning model (e.g., a neural network) to generate an output. The output may be compared to the tag to determine an error. For example, the error may be used to train the model, e.g., using techniques such as gradient descent (e.g., random, batch, etc.) and/or back propagation to adjust weights associated with hidden layers of the model. Once such a model is trained over a (typically large) number of training examples, it can be used to generate an output of predicted intent from untagged free-form natural language input.
In some implementations, the automated assistant 120 can facilitate (or "coordinate") the computing service 150 to perform actions on behalf of the user. These actions (typically an intent that the intent matcher 136 can recognize or determine) typically issue requests to the computing service, and typically do so in connection with providing one or more parameters to the computing service to indicate how such computing service performs the action.
In this regard, a computing service may be viewed as comprising virtually any type of computer functionality that can be invoked to perform some activity on behalf of a user. The computing service may be implemented by one or more processors and may reside on the same computer system as the automated assistant or may reside on a different computer system than the computer system hosting the automated assistant. Some computing services may be accessible over a network (e.g., as in the case of cloud-based computing services), while other computing services may reside on a user's client computing device. Some computing services may also be considered third-party computing services associated with third parties other than the automated assistant and the user, regardless of where those computing services are hosted (e.g., on a third-party computer system, on an automated assistant computer system, on a user computing device, on a cloud computing system, etc.).
Thus, one user intent that may be recognized by the intent matcher 136 is an action that causes a third party computing service to perform a transaction or other operation on behalf of the user. For example, the automated assistant 120 may provide third party services, such as travel services, with access to an application programming interface ("API"). The user may invoke the automated assistant 120 and provide a command such as "I'd like to order a vehicle with a where room to the nearest station with unobstructed access" and so on. The intent matcher 136 may map the command to a grammar (which in some cases may be added to the database 137 by a third party) that triggers the automated assistant 120 to interact with the third party travel service via one or more API calls to the travel service. In another example, the automated assistant 120 may provide access to APIs to different third party services (such as a pizza delivery service). The user may invoke the automated assistant 120 and provide a command such as "I'd like to order a pizza". The intent matcher 136 may map the command to a grammar (which in some cases, a third party may add to the database 137) that triggers the automated assistant 120 to interact with the third party pizza delivery service via one or more API calls to the pizza delivery service. As will become more apparent below, in some embodiments, a third-party service, such as a travel service or pizza delivery service, may construct conversations using dependency graph data structures created by the third-party service or otherwise generated based on user input provided by the third-party service to enable an order to be made, such as ordering a suitable vehicle or ordering a pizza to be delivered. The dependency graph data structure may provide a set of input parameters that need to be populated in order to fulfill an order for a suitable vehicle or a pizza delivery order, for example. The automatic assistant 120 may generate and provide to the user (via the client device 106) a natural language output that requests input of any parameters that are not available from the user data and/or input provided in connection with specifying intent to the automatic assistant. In this manner, the automated assistant 120 may be configured to request input of only parameters that are not available from the user data and/or input provided in connection with specifying intent to the automated assistant. This allows the automated assistant 12 to efficiently recognize and obtain the selected missing information. In doing so, the automated assistant 120 efficiently utilizes hardware resources on the client computing device, including, for example, microphone resources and speaker resources. In this process, the power consumption of the device can be reduced.
In particular, to request input of parameters that are not available from the user data and/or input provided in connection with specifying intent to the automated assistant, the fulfillment engine 124 may be configured to receive the intent output by the intent matcher 136 and any associated parameters (whether actively provided by the user or requested by the user) and fulfill the intent. In various embodiments, the implementation of the user intent may result in the generation/obtaining of various implementation information. As described below, in some implementations, the implementation information can be provided to a natural language generator ("NLG") 126, and the NLG126 can generate a natural language output based on the implementation information.
The implementation information may take various forms, as the intent may be implemented in various ways. Suppose that a user requests pure information, such as "Where wee The outdoor notes of The Shining' filtered? (where was the outdoor shot of the flashing >) ". The user's intent may be determined as a search query, for example, by the intent matcher 136. The intent and content of the search query can be provided to one or more search engines to search a corpus of documents and/or other data sources (e.g., knowledge graphs, etc.) for responsive information. The implementation engine 124 may provide data indicative of the search query (e.g., text of the query, dimension reduction embedding, etc.) to the search engine, which may in turn provide responsive information such as GPS coordinates, or other more specific information such as "Timberline Lodge, mt. The response information may form part of the implementation information generated by implementation engine 124.
Additionally or alternatively, the implementation engine 124 may be configured to receive the user's intent, e.g., from the natural language understanding engine 135, and then engage in a human-machine dialog session with the user using the dialog engine 138 to perform an action associated with the intent, e.g., to invoke or evoke execution of one or more operations by the computing service 150. Responsive actions may include, for example, ordering goods/services, purchasing event tickets, reserving restaurants, hotels, planes, trains, ordering taxis or car sharing services, starting timers, setting reminders, initiating telephone calls, playing media, sending messages, etc. In some such embodiments, the implementation information may include parameters associated with the action, the confirmation response (which may be selected from predetermined responses in some cases), and the like.
As described above, the natural language generator 126 may be configured to generate and/or select natural language output (e.g., spoken words/phrases designed to mimic human speech) based on data obtained from various sources. In some implementations, the natural language generator 126 can be configured to receive as input implementation information associated with the implementation intent of the implementation engine 124 and generate a natural language output based on the implementation information. Additionally or alternatively, the natural language generator 126 may receive information from other sources, such as third party computing services, and, as will become more apparent below, may receive information from a dependency graph data structure, which may be used to formulate natural language output for a user.
The dialog engine 138 may be used to manage the conduct of conversations associated with the actions and may use the dependency graph data structures associated with particular actions to control interactions between the user and the automated assistant. Thus, the dialog engine 138 can store various dependency graph data structures associated with different actions or intents (e.g., with the database 142), and access those dependency graph data structures upon interaction with a user.
Additionally, in some embodiments, the conversation builder component 140 can also be supported to generate, modify, update, or otherwise manage dependency graph data structures. For example, in some implementations, the conversation builder component 140 can also provide a web-based user interface to enable internal and/or third party developers to generate dependency graph data structures for particular actions and store those data structures in the database 142 for later access by the conversation engine 138.
The various components of FIG. 1, such as the conversation engine 138 and the conversation builder component 140, can be configured to perform selected aspects of the present disclosure to utilize and/or generate dependency graph data structures for conducting human-machine conversation sessions with users to initiate performance of actions by one or more computing services. However, it should be appreciated that the functionality disclosed herein may be implemented in myriad other ways with other combinations of computing devices, computer systems, and/or software components, as would be apparent to one of ordinary skill in the art with the benefit of this disclosure. Accordingly, the present invention is not limited to the specific embodiments discussed herein.
Turning now to FIG. 2, this figure depicts an example dependency graph data structure 200 in accordance with some embodiments. As discussed above, in many instances, building conversations for implementation by an automated assistant can be problematic. For example, conversation models based on form and slot filling have been found to produce undesirable conversation processes, e.g., conversation processes that are robotic or not realistic simulations of human interaction.
Modeling these conversations may be particularly problematic for some types of conversations, such as conversations associated with actions that access computing services. For example, consider making a "reserve table" conversation to make a restaurant reservation. The user is typically required to specify at least a number of people and a preferred time, and the computing service is typically invoked to obtain an available date and time that the user can select. The user may then be required to provide additional input, such as contact information, and to invoke the computing service again to complete the subscription. In this process, various hardware elements of the client computing device (such as speakers, microphones, and other user interface hardware) may be activated and deactivated to provide information, sometimes after a reasonable time delay, when the user is prompted in various portions of the session.
Formal and slot fill models are often difficult to model for these types of conversations, due in part to the fact that intermediate calls may need to be made before some user input, and some user input may be needed before such intermediate calls may be made. Such a conversation process is typical of many transaction use cases, such as the above-described reserved case, as well as various shopping, event ticket purchases, and the like.
However, in the illustrated embodiment, the conversation is modeled using a dependency graph data structure that defines, in part, the dependencies between parameters and operations performed in the conversation. In some implementations, for example, the dependency graph data structure can be created based on an API of the computing service and can be accessed at runtime to generate a conversation with a user accessing the computing service.
In some embodiments, the dependency graph data structure may be based in part on multiple methods and dependencies therebetween. In some implementations, a specification may be defined for a conversation that defines one or more objects that may be generated by a particular method and that may be used as parameters by other methods.
The first type of method that may be defined in the specification is referred to herein as an action method, which generally specifies the implementation of a computing service. In the case of a third party computing service, for example, the action method may be associated with a proxy that accesses the third party computing service (e.g., an HTTP proxy that includes a URL and one or more parameters to make an HTTP call). In some embodiments, a field may also be provided in the specification of the action method to specify whether the method is indexable, meaning that the method can be triggered by an agent to perform the action.
The second method, which may be defined in the specification, is referred to herein as the assistant method, which generally specifies how the automated assistant obtains information from the user. In some embodiments, the assistant method may specify user input, e.g., text input, selection of a date, etc., and in some embodiments, the assistant method may be a platform provided by a third party.
Referring now to FIG. 2, in the illustrated embodiment, the dependency graph data structure 200 is implemented as a Directed Acyclic Graph (DAG) including a plurality of nodes 202 and 214 linked to one another by directed edges 216. It should be noted that FIG. 2 represents a simplified dependency graph data structure suitable for explanatory purposes, and not all relationships and parameters are represented in this graph for clarity.
It should be appreciated that DAGs are types of graphs in which edges have directions associated with them (i.e., edges are not bi-directional), and types of graphs in which directed rings are not defined (i.e., edges do not form a closed loop of nodes). Additionally, in some embodiments, and in contrast to tree data structures, the DAGs used in some embodiments may support defining directed edges for multiple parent and/or child relationships for any given node. However, it should be appreciated that in other embodiments, other data structures may be used, and thus the invention is not particularly limited to DAGs.
Each node 202-214 identifies an action method for accessing the computing service or an assistant method for collecting one or more parameters. In addition, each directed edge 216 connects a respective pair of nodes 202 and 214 and effectively identifies at least one parameter generated, or otherwise output by one node of the pair and used, received, or otherwise utilized by the other node of the pair. To represent this directional relationship, each directional edge 216 is represented in FIG. 2 by an arrow pointing to the node in the pair with the associated parameter.
The dependency graph data structure 200 models the above-described "reserve table" conversation by each node 202 labeled with an associated action or assistant method 214, and the object or parameter output by the associated method, in this case the hypothetical restaurant name "O' Briens". For this conversation, four methods of action are defined and represented by blocks 202, 204, 208, and 210:
reserve (Block 202): a final reservation is made and a receipt object is generated. In the illustrated embodiment, this is a method of indexing a "book a table at O 'Briens (booking a table at O' Briens restaurant)" use case.
Slotlock (Block 204): the available time slots are locked and the SlotLockID used by the reservation action method is generated.
SearchSlotsByPartySize (Block 208): generating available time slots (denoted by SearchSlotoObjects) based on the expected party size
Searchslots (Block 210): generating available time slots (represented by SearchSlotobjects) based on the expected party size and preferred date and time
Furthermore, to model conversations, three assistant methods are defined and represented by blocks 206, 212, and 214:
SelectDateTime (Block 206): the user is prompted to select the available time slot returned by the SearchSlots action method.
GetParatySize (Box 212): the user is prompted to specify a desired party size and generate a PartySize object.
GetPreferredDateTime (block 214): the user is prompted to specify a Preferred date and time and a Preferred DateTime object is generated.
It can be seen that the directed edges 216 in FIG. 2 represent parameters that are generated or output by methods defined by certain nodes and used or utilized by other nodes. Thus, it should be appreciated that the searchslotsbypartisisize action method of node 208 uses the partisisize object generated by the getpotysize method of node 212 as an input parameter, while the SearchSlots action method of node 210 uses not only the partisisze object generated by the getpotysize method of node 212, but also the PreferredDateTime object generated by the getrefrefreddatetime method of node 214. Likewise, the SelectDateTime method of node 206 may use searchslottoobjects generated by the methods of nodes 208 and 210, the SlotLock method of node 204 may use the SelectDateTime object generated by the SelectDateTime method of 206, and the Reserve method of node 202 may use the SlotLockID object generated by the SlotLock method of node 204.
As is apparent from FIG. 2, in some embodiments, the dependency graph data structure may support a variety of alternative assistant methods, just as the methods of nodes 208 and 210, which search for available time slots based on different input parameters and provide similar outputs to node 206. Likewise, the node's method may use input parameters generated by a plurality of other nodes, as with the node 210 method, which uses parameters output by each of the nodes 212 and 214.
Fig. 3A-3G illustrate specifications for the action method and the assistant method defined by block 202-214 of fig. 2. For example, FIG. 3A illustrates an example specification for a Reserve action method at 220. For example, the specification indicates that the method is indexable and thus can be invoked to perform an action intended for a conversation. The specification defines an HTTP proxy to invoke a hypothetical third party computing service hosted on api. It should be noted that all parameters used for the call are not shown in fig. 3A for simplicity of illustration.
It is also apparent from fig. 3A that in some embodiments, the parameters may be provided not only by requesting user input (e.g., user input provided in an initial natural language input, or user input provided during a conversation in response to a prompt generated by an assistant method), but also based on stored user data that is not explicitly requested from the user or provided by the user in connection with the conversation. For example, one parameter shown in FIG. 3A is a "firstname" parameter, which in some embodiments may be provided by a user profile maintained for the user, since in many cases the user's contact information may be accessed through an automated assistant interacting with the user.
Fig. 3B-3D also show specifications for SlotLock, SearchSlots, and searchslotsbyparkysize action methods at 222-226, and each includes a reference to an HTTP proxy to invoke the hypothetical third party computing service. However, each of these methods generates intermediate data for performing the reservation action, and therefore none of these methods are marked as indexable.
Fig. 3E to 3G show specifications for the SelectDateTime, getprotsize, and GetPreferredDateTime assistant methods at 228-. In some implementations, each specification can also include desired user input as well as text or other data for use in generating prompts to the user to provide the desired user input. Although text for prompts may be specified in some embodiments, in other embodiments, other ways of representing natural language output, such as grammars, may be defined for the assistant method, and in other embodiments, prompts may not need to be determined for any particular definition in the specification of the method based on the desired user input.
Fig. 4 and 5A-5I illustrate another example use case, namely a use case for a conversation purchasing event tickets. In particular, FIG. 4 illustrates an example dependency graph data structure 250 that includes nodes 252 and 270 and directed edges 272. The dependency graph data structure 250 is also used to perform actions on the same third party computing service used in the example use cases of fig. 2 and 3A-3G, which may be accessed at api.
BuyTicket (Box 252): tickets are purchased based on the user's authorization token, the SKU of the tickets to be purchased, and the number of tickets to be purchased entered. The specification of this method of action is shown at 280 in FIG. 5A. The specification indicates, for example, that the method is indexable and thus can be invoked to perform an intentional action for a conversation.
GetTicketSku (Box 256): based on event ID and assigned ticketType obtains SKU of ticket. The specification of this action method is shown at 282 in fig. 5B.
GetAvailableTicketTypes (Block 262): the ticket types available for a given event ID are obtained. The specification of this method of action is shown at 284 in FIG. 5C.
SearchEvents (block 266): available events on a particular date are searched and matched to a desired keyword (or set of keywords). The specification of this method of action is shown at 286 in fig. 5D.
AssistentUser (Block 254): an authorization token for the user is obtained from the stored user data. The assistant method only accesses the stored data of the user, and thus the specification of the method is not shown.
SelectTicketType (Box 258): the user is prompted to select a ticket type from those retrieved by the getavailabletickettettypes action method and a selectedticktype object is generated. The specification of the assistant method is shown at 288 in fig. 5E.
GetNumTicks (Block 260): the user is prompted to specify the desired number of tickets and generate a NumTickets object. The specification of the assistant method is shown at 290 in fig. 5F.
SelectEvent (Block 264): the user is prompted to select events from those retrieved by the SearchEvents action method and a SelectedEventId object is generated. The specification of the assistant method is shown at 292 in fig. 5G.
GetPreferredDate (Block 268): the user is prompted to specify a preferred Date and a Date object is generated for searching for events. The specification of the assistant method is shown at 294 in fig. 5H.
GetKeyword (Block 270): the user is prompted to specify one or more keywords to use for searching for events and generating a SearchKeyword object. The specification of the assistant method is shown at 296 in fig. 5I.
In the above example, all the parameters shown are mandatory. However, in other embodiments, the parameters may be designated as optional and may be provided to the computing service when provided by the user or otherwise available, but omitted if not determined during the dialog session.
Turning now to FIG. 6, an example sequence of operations 300 for conducting a human-machine conversation session using a dependency graph data structure is shown. For example, the sequence 300 may be implemented using one or more processors and using one or more of the various components shown in fig. 1, for example, under the control of the dialog engine 138 and with other automated assistant components 119. In block 302, for example, a user input associated with an intent may be initially received, and in block 304, the intent may be determined, for example, by the intent matcher 136 of fig. 1. In block 306, it may be determined whether the intent is associated with an action with which the dependency graph data structure is associated, e.g., one of various use cases that have modeled the conversation through the dependency graph data structure. If not, control may pass to block 308 to process the intent in a conventional manner.
Further, it should also be noted in block 304 that, in some embodiments, the intent may also be determined using a dependency graph data structure. For example, in some implementations, it may be desirable to model multiple types of actions in the same dependency graph, thereby enabling traversal of the dependency graph to be used to select from among multiple actions. As an example, where multiple actions are supported using similar types of input parameters, selecting an action from the multiple modeled actions may be based in part on available parameters specified by the user in the initial input.
Returning to block 306, if the intent is determined to be associated with an action for the dependency graph data structure to exist to model the conversation, control may pass from block 306 to block 310 to access and traverse the dependency graph data structure and optionally set one or more parameters based on previous user input and/or stored user data. For example, if the initial input from the user includes text defining one or more parameters used by the action when requesting the action, the user typically need not be prompted for those parameters. Thus, if the user enters "please book a table for four at O' Briens," the size of the party required is known, and thus the user need not be required to provide this data. Likewise, in the case where the dependency graph data structure models reservation conversations for multiple restaurants, restaurant "O' Briens" is already known from the initial inputs. This may feed into the effects discussed above, resulting in more efficient overall use of hardware aspects at the client computing device.
Likewise, where one or more parameters used by an action are available as stored data (e.g., from a user profile), the user may also not need to be prompted for this data, or, alternatively, the data may be provided for confirmation purposes (e.g., "Would you like me to use you move mobile number XXX-XXXX as you connect number.
Then, in block 312, a determination is made as to whether any parameters needed to perform the action are unset (i.e., not yet determined). For example, if the user still needs to be prompted to provide the parameter, the parameter may not be set. Additionally, if the method of action needs to be performed in order to generate some intermediate data that is needed before the parameter is obtained (e.g., a list of available time slots from which the user may then select to book the table), the parameter may not yet be set.
If any of the parameters have not been set, control passes to block 314 to select one or more unset parameters based on the dependency graph data structure. In some implementations, for example, a dependency graph data structure can be used, at least in part, to determine an order in which to select parameters, e.g., by prioritizing parameters used by an action method that generates intermediate data that is then used as parameters by another method.
In block 316, a prompt for the selected one or more parameters is generated and presented to the user, for example, by executing one or more assistant methods associated with the selected parameters, and then in block 318, the user's input to the prompt is received and any parameters specified in the input are set. It should be appreciated that in some implementations, the prompts and responses generated in the dialog session may be in the form of natural language output presented in visual and/or audio form, while the user input may include an example of free-form natural language input received by one or more input components of the computing device (e.g., a touchscreen, keyboard, microphone, etc.), although the invention is not limited thereto.
In block 320, it is determined whether the user input changed any parameters that have been set, and if not, control passes to block 322, where it is determined whether any intermediate action methods are ready to be performed (i.e., all parameters for the action are set). If so, control passes to block 324 to perform the intermediate action method, and control returns to block 312 to continue the dialog session to determine any remaining unset parameters. If not, block 324 is skipped and block 322 returns control directly to block 312. Then, once all parameters have been set, block 312 passes control to block 326 to perform the final action method, e.g., by performing a call operation to a computing service using the parameters determined during the dialog session as input to the call operation. A response is then generated and presented to the user in block 328 to report the results of the action and the dialog session is complete.
Turning again to fig. 320, as noted above, in some instances, user input in a dialog session may include changes to already set parameters. Thus, in this case, control may pass to block 330 to invalidate one or more affected parameters. By so doing, subsequent operations in the sequence 300 may re-perform one or more actions or assistant methods to update any invalid parameters. It should be noted that when one or more parameters are invalid, one or more other parameters not affected by the change may remain valid, and as a result, re-executing one or more action methods or assistant methods for determining that they are not affected may be effectively bypassed when re-executing one or more action methods or assistant methods associated with the invalid parameters.
Turning now to fig. 7, which illustrates an example conversation session between a user and an automated assistant 120 implemented at least in part on a computing device 340 operated by the user, the computing device 340 includes a touch screen 342, various user interface buttons 344, 346, 348, a text entry field 350, and a microphone icon 352. The microphone icon 352 is operable to enable a user to provide a voice input, which may be, for example, speech recognized by the speech capture/TTS/STT module 114 and used, for example, to populate the text input field 350.
In this dialog session, the user operates the client device 340 ("You") to provide the following free-form natural language input (e.g., using input field 350): "HEY ASSISTANT, CAN You BOOK A DINNER RESERVATION AT O' BRIENS THIS WEEKEND? "(hi, assistant, you can book dinner at the O' bright restaurant on the weekend. In some implementations, for example, an automated assistant using the intent matcher 136 may determine that the input is associated with an action of booking a table at O' Briens restaurant, and that the action has an associated dependency graph data structure (e.g., as discussed above in connection with block 304 and 306 of fig. 6). The dependency graph data structure of this example may be similar to the dependency graph data structure 200 discussed above in connection with fig. 2 and 3A-3G.
Further, by accessing and traversing the dependency graph data structure as discussed above in connection with block 310 of fig. 6, it may be determined that input data pertaining to the PreferredDateTime parameter ("diner" (dinner) and "this weekend" (weekend) has been provided, but input data pertaining to the partisisze parameter has not been provided, and the SearchSlots action method cannot be performed without both parameters. Thus, as discussed above in connection with blocks 314 and 316 of fig. 6, a parameter such as "spare, FOR HOW MANY manual job? A prompt such as "(of course, how many people. As previously discussed, the efficiency of the process and the relevance of the generated hints enables efficient use of hardware resources at the client computing device.
The user may then respond with a number such as "FOUR" (FOUR), which is then used to set the partiysize parameter. It may then be determined that there is sufficient data to perform the SearchSlots action method to generate intermediate data (here the available time slots), and the SearchSlots action method may be performed, for example, as discussed above in connection with block 322 and 324 of fig. 6. It should be noted that the SearchSlots action method itself or the automated assistant can be configured to determine the appropriate input to the SearchSlots call to the computing service. For example, assuming that the user enters "dinner" (dinner) and "this weekend" (weekend), a time range such as 5:00pm to 10:00pm and a date range including the next friday, saturday and sunday may be used. In some instances, the computing service may be invoked multiple times, such as once per day over a range of dates.
Upon returning an AVAILABLE time slot from the computing service, the user may be presented with ONE or more AVAILABLE time slots and prompted TO select a time slot TO reserve ("RESERVATION for area AVAILABLE AT 7:00PM ON FRIDAY AND 8:00PM ON satellite. work YOU LIKE TO MAKE a RESERVATION AT ONE OF the TIMES.
Then, based on the user response ("yes. let' S DO SATURDAY AT 8:00 PM" (good, SATURDAY night 8:00 bar)) selecting the time slot, the automated assistant may determine that the parameters needed to execute the SlotLock action method are now set, thereby executing the SlotLock action method to lock onto the time slot and then providing all necessary parameters for the Reserve action method. Executing the Reserve action method may result in a ReceiptObject being returned, which may then be processed by the automated assistant to output a CONFIRMATION response ("you Reserve IS confirmed. you Reserve a CONFIRMATION EMAIL short").
Fig. 8 illustrates another example dialog session with a user of the computing device 340, beginning with an initial input "HEY ASSISTANT, CAN YOU BOOK a RESERVATION FOR FOUR AT O' bridges? "(hi, assistant, how can you reserve four places at the O' BRIENS restaurant. Traversing the dependency graph may determine that the PartySize parameter is not set, thereby resulting in a prompt of "WHAT DAY AND TIME ARE YOU LOOKING FOR? "(what time of day you want to order. In response to the user entering "TONIGHT AT 7:00 PM" (7:00 TONIGHT), the automated assistant may execute the SearchSlots action method to invoke the computing service using the four party size and the preferred date and time of the day and 7:00 PM.
Assuming that THE response to THE call to THE computing service returns ONLY an AVAILABLE time slot of 6:00PM, a response such as "THE ONLY AVAILABLE RESERVATION TONIGHT IS AT 6:00PM. SHOULD I BOOK IT FOR YOU? "(the only predeterminable for this evening is 6:00pm. Likewise, assuming that the time is unacceptable to the user, the user may use a program such as "no. An input such as "(not, how late. The automated assistant may then generate a query such as "7:00 PM IS AVAILABLE Tomorrow N ü r? A new prompt such as "(available tomorrow 7: 00.
Fig. 9 then depicts an example conversation session conducted between a user 360 and an automated assistant 120 through a separate interactive speaker 362 operated by the user 360, in accordance with various embodiments. In this dialog session, the initial input "ANY THEATER PERFORMANCES TONIGHT? "(what shows is there a tonight theater.
Initial analysis of the dependency graph data structure may determine the values of the searchkey parameter ("the" and Date parameter ("tonight"), and thus the SearchEvents action method may be performed before prompting the user for any additional parameters. Thus, the automated assistant may execute the SelectEvent assistant method TO output one or more possible events TO the user ("SHAKESPEARE IN THE PARK IS PLAYING TONIGHT, good YOU LIKE TO GO. Assuming also that the user responds with "YES, I' D LIKE TO BUY THEE" (YES, I want TO BUY THREE), a GetTicketSku action method may be performed TO determine the SKU of the selected ticket. Further, because THE user input specifies THE number of tickets, THE automated assistant can bypass execution of THE GetNumTickers assistant method, and can use THE input of THE number of tickets SKU and tickets, and THE authentication information provided by THE automated assistant to execute THE BuyTicket action method to invoke THE computing service, and can generate and present a response to THE user, such as "OK," I' VE PURCHASED THE TICKETS.
The dependency graph data structure may be generated in a variety of ways in different embodiments, and may be generated using a variety of different graphical, visual, and/or textual programming interfaces. For example, FIG. 10 illustrates an example sequence of operations 370 for generating a dependency graph data structure. For example, in block 372, the computer system may generate a programming user interface and present the interface to a user (e.g., a developer). The computer system may then receive one or more action call definitions corresponding to, for example, a third party proxy API in block 374. One or more parameters may then be received and/or determined in block 376. For example, the received parameters may be provided manually by the user, while the determined parameters may be derived, for example, from an analysis defined by the action call.
Then, in block 378, a dependency relationship may be received and/or determined. For example, dependencies may be received based on user input linking the output of some action calls to the input of other action calls, while dependencies may be determined based on program analysis of the input and output of those action calls. One or more assistant method and/or parameter prompts may then be received and/or determined in block 380. For example, the user may manually enter prompts for particular parameters or draft program code for various assistant methods. As another example, the computer system may determine which parameters require user input, and may derive an assistant method, an assistant method template, and/or a prompt for collecting the user input.
Then, in block 382, the computer system creates an action method node for each action call definition, and in block 384, the computer system creates an assistant method node for each assistant method. Then, in block 386, directed edges are created to link the nodes together according to the determined dependencies. Additionally, in some embodiments, and as shown in blocks 388 and 390, a specification similar to that shown in fig. 3A-3G and 5A-5I may be generated, and program code for each method may be determined, for example, using automatic code generation, or alternatively, the specification may be drafted manually by a user or by some combination of the two methods.
Then, in block 392, the computer system may index all action methods identified in the specification as indexable, thereby registering the action methods in the automated assistant. Then in block 394, the dependency graph data structure may be deployed and made available to the automated assistant.
It should be appreciated that a wide variety of programming models, languages, and interfaces may be used in various embodiments, including different levels of automatic code generation and manual input from a user, and thus the invention is not limited to the specific embodiments discussed herein. For example, in some embodiments, automation may be used to generate or suggest data types, dependencies, authorization or authentication methods, and the like, and in some instances verification of the dependency graph data structure may be performed, e.g., to confirm that each parameter used by each method has a corresponding directed edge from the other method that outputs the parameter. Additionally, in some embodiments, methods and/or portions of the dependency graph data structure may be stored and/or reused with other dependency graph data structures, e.g., as may be provided by third party computing service providers, by assistant providers, via a community repository, and so forth.
The above-described use of dependency graph data structures provides a number of technical advantages in different embodiments. For example, more realistic and less robotic conversations may be supported than some low-level API or formal population methods, and the automated assistant may have greater flexibility in deciding the order in which parameters may be requested and which method of action may be performed. In addition, the automated assistant can use machine learning techniques to optimize the use of dependency graph data structures to provide more realistic conversations and conversations that are more tailored to a particular user. The method can also be applied to a variety of use cases and can be extended to additional use cases that require little or no additional programming. Furthermore, the need for a developer to anticipate all possible inputs and combinations of input parameters may be avoided.
Furthermore, in some embodiments, computational resources may be conserved by the ability to dynamically invalidate parameters while keeping other parameters active, thereby avoiding the need to resume a conversation.
Fig. 11 is a block diagram of an example computing device 410. Computing device 410 typically includes at least one processor 402 that communicates with a number of peripheral devices via bus subsystem 404. These peripheral devices may include a storage subsystem 406 (including, for example, a memory subsystem 408 and a file storage subsystem 410), user interface input devices 412, user interface output devices 414, and a network interface subsystem 416. The input and output devices allow a user to interact with the computing device 400. Network interface subsystem 416 provides an interface to external networks and couples to corresponding interface devices in other computing devices.
The user interface input devices 412 may include a keyboard, a pointing device such as a mouse, trackball, touchpad, or graphics tablet, a scanner, a touch screen incorporated into a display, an audio input device such as a voice recognition system, a microphone, and/or other types of input devices. In general, use of the term "input device" is intended to include all possible types of devices and ways to input information into computing device 400 or onto a communication network.
User interface output devices 414 may include a display subsystem, a printer, a facsimile machine, or a non-visual display such as an audio output device. The display subsystem may include a Cathode Ray Tube (CRT), a flat panel device such as a Liquid Crystal Display (LCD), a projection device, or some other mechanism for creating a visible image. The display subsystem may also provide non-visual displays, for example, via an audio output device. In general, use of the term "output device" is intended to include all possible types of devices and ways to output information from computing device 400 to a user or to another machine or computing device.
These software modules are typically executed by the processor 402 alone or in combination with other processors. Memory 408 used in storage subsystem 406 may include a number of memories, including a main Random Access Memory (RAM)418 for storing instructions and data during program execution and a Read Only Memory (ROM)420 in which fixed instructions are stored. File storage subsystem 410 may provide persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive along with associated removable media, a CD-ROM drive, an optical drive, or removable media cartridges. Modules implementing the functionality of certain embodiments may be stored by file storage subsystem 410 in storage subsystem 406 or in other machines accessible by processor 402.
Where the system described herein collects or can utilize personal information about a user, the user may be provided with the following opportunities: control whether programs or functions collect user information (e.g., information about the user's social network, social behaviors or activities, profession, the user's preferences, or the user's current geographic location), or control whether and/or how to receive content from a content server that is more relevant to the user. Also, some data may be processed in one or more ways before being stored or used in order to delete personally identifiable information. For example, the identity of the user may be processed so that no personally identifiable information can be determined for the user, or the geographic location of the user may be generalized where geographic location information is obtained (such as for a city, zip code, or state level), so that a particular geographic location of the user cannot be determined. Thus, the user may control how information about the user is collected and/or used.
While several embodiments have been described and illustrated herein, one or more of various other means and/or structures for performing the function and/or obtaining the result and/or the advantages described herein may be utilized and each of such variations and/or modifications is deemed to be within the scope of the embodiments described herein. More generally, all parameters, dimensions, materials, and configurations described herein are intended to be exemplary, and the actual parameters, dimensions, materials, and/or configurations will depend upon the specific application or applications for which the teachings are used. Those skilled in the art will recognize, or be able to ascertain using no more than routine experimentation, many equivalents to the specific embodiments described herein. It is, therefore, to be understood that the foregoing embodiments are presented by way of example only and that, within the scope of the appended claims and equivalents thereto, the embodiments may be practiced otherwise than as specifically described and claimed. Embodiments of the present disclosure are directed to each individual feature, system, article, material, kit, and/or method described herein. In addition, any combination of two or more such features, systems, articles, materials, kits, and/or methods, if such features, systems, articles, materials, kits, and/or methods are not mutually inconsistent, is included within the scope of the present disclosure.
Claims (25)
1. A method implemented using one or more processors, comprising:
determining an action to be performed by a computing service for a user of a computing device in communication with an automated assistant implemented at least in part by the one or more processors; and
with the automated assistant, causing the computing service to perform actions for a user of the computing device by:
accessing a dependency graph data structure for the action, the dependency graph data structure comprising a plurality of nodes and a plurality of directed edges, each node identifying an action method to access the computing service or an assistant method to collect one or more parameters for performing the action, and each directed edge connecting a respective pair of nodes from the plurality of nodes and identifying at least one parameter generated by the action method or the assistant method identified by one node of the respective pair of nodes and utilized by the action method or the assistant method identified by another node of the respective pair of nodes;
conducting a human-machine dialog session between the user and the automated assistant to determine one or more parameters for performing the action, including generating one or more natural language outputs using the dependency graph data structure for presentation by the computing device operated by the user, and receiving one or more instances of free-form natural language received at one or more input components of the computing device operated by the user; and
initiating performance of the action by the computing service using the determined one or more parameters.
2. The method of claim 1, wherein a first node of the plurality of nodes in the dependency graph data structure recognizes a first action method that invokes the computing service, and wherein initiating performance of the action by the computing service comprises performing the first action method.
3. The method of claim 2, wherein the first action method comprises a call operation comprising a plurality of inputs, and wherein executing the first action method comprises executing the call operation using the determined one or more parameters as inputs of the plurality of inputs for the call operation.
4. The method of claim 2 or claim 3, wherein the dependency graph data structure includes a specification for defining methods and dependencies therebetween, and wherein the first action method is marked indexable in the specification to indicate that the first action method invokes the computing service to perform the action.
5. The method of any of claims 2, 3, and 4, wherein the first action method invokes the computing service to perform the action, and wherein a second node of the plurality of nodes in the dependency graph data structure recognizes a second action method that invokes the computing service to obtain intermediate data for performing the action.
6. The method of any of the above claims, wherein a first node of the plurality of nodes in the dependency graph data structure recognizes a first assistant method that includes a first prompt requesting a first parameter, and wherein generating the one or more natural language outputs includes executing the first assistant method to generate a first natural language output that includes the first prompt.
7. The method of any of the above claims, wherein determining the action to be performed by the computing service is performed by the automated assistant, and based on an initial natural language input received at the computing device of the user and specifying a first parameter recognized by a first directed edge in the dependency graph data structure, wherein the first directed edge connects a first node and a second node, the first node identifying a first assistant method that generates the first parameter, the second node identifying a first action method that utilizes the first parameter, and wherein, conducting the human-machine dialog session between the user and the automated assistant includes requesting the first parameter in response to determining that the first parameter is specified in the initial natural language input bypassing generating a natural language output.
8. The method of any of the above claims, wherein a first node and a second node are connected by a first parameter identified by a first directed edge in the dependency graph data structure, the first node identifying a first assistant method that generates the first parameter, the second node identifying a first action method that utilizes the first parameter, and wherein conducting the human-machine dialog session between the user and the automated assistant comprises requesting the first parameter by bypassing generating a natural language output in response to determining that the first parameter is determinable without requiring a request for the first parameter from the user.
9. The method of claim 8, wherein determining that the first parameter is determinable without requiring a request from the user for the first parameter comprises determining that the first parameter is determinable from stored data associated with the user.
10. The method of any of the above claims, wherein conducting the human-machine dialog session between the user and the automated assistant to determine the one or more parameters for performing the action comprises ranking the generated one or more natural language outputs using the dependency graph data structure.
11. The method of any of the preceding claims, further comprising:
responsive to receiving a first instance of free-form natural language in one or more instances of the free-form natural language input, invalidating one or more of the determined one or more parameters during the human-machine dialog session;
accessing the dependency graph data structure to identify one or more action methods or assistant methods identified in the dependency graph data structure to re-execute in order to update the invalidated one or more parameters; and
re-executing the identified one or more action methods or assistant methods to update the invalidated one or more parameters.
12. The method of claim 11, wherein invalidating the one or more parameters keeps one or more other parameters valid such that re-execution of the one or more action methods or assistant methods for determining the one or more other parameters is bypassed when re-executing the recognized one or more action methods or assistant methods.
13. The method of any of the above claims, wherein determining the action is performed by the automated assistant and comprises selecting the action from a plurality of actions using the dependency graph data structure.
14. The method of any of the above claims, wherein the computing service is a third party computing service, and wherein initiating performance of the action by the computing service using the determined one or more parameters comprises invoking the third party computing service using the determined one or more parameters.
15. The method of any of claims 1 to 14, wherein the computing service is a cloud computing service.
16. The method of any of claims 1 to 14, wherein the computing service resides on the computing device operated by the user.
17. The method of any of the above claims, wherein the dependency graph data structure defines a directed acyclic graph.
18. The method of any of the above claims, wherein the action creates a reservation, wherein a first node of the plurality of nodes recognizes a first action method that invokes the computing service to search for an available time slot, and wherein a second node of the plurality of nodes recognizes a second action method that invokes the computing service to reserve an available time slot.
19. The method of any of claims 1-17, wherein the action obtains a product or event ticket.
20. A method implemented using one or more processors, comprising:
receiving, by a computer interface, a plurality of parameters to be used by an automated assistant to cause a computing service to perform an action with respect to a user of a computing device in communication with the automated assistant;
determining a plurality of dependencies between the plurality of parameters;
constructing, using the one or more processors, a dependency graph data structure for the action, the dependency graph data structure configured for conducting a human-to-machine conversation session between the user and the automated assistant to determine one or more parameters for performing the action, including for generating one or more natural language outputs for presentation by the computing device operated by the user, wherein constructing the dependency graph data structure for the action comprises:
constructing a plurality of nodes for the dependency graph data structure using the received plurality of parameters, including constructing at least one node that recognizes an action method for accessing the computing service, and constructing at least one assistant method that collects one or more parameters from the received plurality of parameters; and
constructing a plurality of directed edges for the dependency graph data structure using the determined plurality of dependencies, each directed edge connecting a respective pair of nodes from the plurality of nodes and identifying at least one parameter from the received plurality of parameters, the at least one parameter being generated by the action method or assistant method identified by one node of the respective pair of nodes and utilized by the action method or assistant method identified by the other node of the respective pair of nodes; and
generating a call operation in the action method that calls the computing service to perform the action using one or more of the received plurality of parameters.
21. The method of claim 20, wherein the computing service is a third party computing service, and wherein the invoking operation is configured to access the third party computing service.
22. The method of claim 20 or claim 21, wherein determining the plurality of dependencies comprises receiving the plurality of dependencies through the computer interface.
23. The method of any of claims 20, 21 and 22, wherein receiving the plurality of parameters comprises receiving a plurality of action invocation definitions.
24. A system comprising one or more processors and memory operably coupled with the one or more processors, wherein the memory stores instructions that, in response to execution of the instructions by the one or more processors, cause the one or more processors to perform the method of any one of claims 1-23.
25. At least one non-transitory computer-readable medium comprising instructions that, in response to execution of the instructions by one or more processors, cause the one or more processors to perform the method of any one of claims 1-23.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201862660129P | 2018-04-19 | 2018-04-19 | |
US62/660,129 | 2018-04-19 | ||
PCT/US2018/030451 WO2019203859A1 (en) | 2018-04-19 | 2018-05-01 | Dependency graph conversation modeling for use in conducting human-to-computer dialog sessions with a computer-implemented automated assistant |
Publications (2)
Publication Number | Publication Date |
---|---|
CN112136124A true CN112136124A (en) | 2020-12-25 |
CN112136124B CN112136124B (en) | 2024-02-20 |
Family
ID=62223255
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201880093458.2A Active CN112136124B (en) | 2018-04-19 | 2018-05-01 | Dependency graph conversation modeling for human-machine conversation sessions with computer-implemented automated assistants |
Country Status (4)
Country | Link |
---|---|
US (2) | US10803860B2 (en) |
EP (1) | EP3776246A1 (en) |
CN (1) | CN112136124B (en) |
WO (1) | WO2019203859A1 (en) |
Families Citing this family (17)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11200893B2 (en) * | 2018-05-07 | 2021-12-14 | Google Llc | Multi-modal interaction between users, automated assistants, and other computing services |
JP7203865B2 (en) | 2018-05-07 | 2023-01-13 | グーグル エルエルシー | Multimodal interaction between users, automated assistants, and other computing services |
US11295213B2 (en) * | 2019-01-08 | 2022-04-05 | International Business Machines Corporation | Conversational system management |
US11392773B1 (en) * | 2019-01-31 | 2022-07-19 | Amazon Technologies, Inc. | Goal-oriented conversational training data generation |
KR20200123945A (en) * | 2019-04-23 | 2020-11-02 | 현대자동차주식회사 | Natural language generating apparatus, vehicle having the same and natural language generating method |
US11206229B2 (en) * | 2019-04-26 | 2021-12-21 | Oracle International Corporation | Directed acyclic graph based framework for training models |
US11120790B2 (en) | 2019-09-24 | 2021-09-14 | Amazon Technologies, Inc. | Multi-assistant natural language input processing |
US11393477B2 (en) * | 2019-09-24 | 2022-07-19 | Amazon Technologies, Inc. | Multi-assistant natural language input processing to determine a voice model for synthesized speech |
US11449578B2 (en) * | 2019-09-27 | 2022-09-20 | Botty Todorov DIMANOV | Method for inspecting a neural network |
US11275902B2 (en) * | 2019-10-21 | 2022-03-15 | International Business Machines Corporation | Intelligent dialog re-elicitation of information |
US11501776B2 (en) * | 2020-01-14 | 2022-11-15 | Kosmos Ai Tech Inc | Methods and systems for facilitating accomplishing tasks based on a natural language conversation |
US11698779B2 (en) * | 2020-09-01 | 2023-07-11 | Ansys, Inc. | Systems using computation graphs for flow solvers |
US11206190B1 (en) | 2021-02-01 | 2021-12-21 | International Business Machines Corporation | Using an artificial intelligence based system to guide user dialogs in designing computing system architectures |
US20230099368A1 (en) * | 2021-09-30 | 2023-03-30 | Intuit Inc. | Conversational user interfaces based on knowledge graphs |
US11922938B1 (en) | 2021-11-22 | 2024-03-05 | Amazon Technologies, Inc. | Access to multiple virtual assistants |
US20230259714A1 (en) * | 2022-02-14 | 2023-08-17 | Google Llc | Conversation graph navigation with language model |
US11516158B1 (en) * | 2022-04-20 | 2022-11-29 | LeadIQ, Inc. | Neural network-facilitated linguistically complex message generation systems and methods |
Citations (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN1321296A (en) * | 1998-10-02 | 2001-11-07 | 国际商业机器公司 | Conversational computing via conversational virtual machine |
CN102236701A (en) * | 2010-05-07 | 2011-11-09 | 微软公司 | Dependency graphs for multiple domains |
US20130275164A1 (en) * | 2010-01-18 | 2013-10-17 | Apple Inc. | Intelligent Automated Assistant |
WO2014197635A2 (en) * | 2013-06-07 | 2014-12-11 | Apple Inc. | Intelligent automated assistant |
CN105808200A (en) * | 2010-01-18 | 2016-07-27 | 苹果公司 | Intelligent automated assistant |
CN106528613A (en) * | 2016-05-26 | 2017-03-22 | 中科鼎富（北京）科技发展有限公司 | Intelligent question-answer (Q&A) method and device |
WO2017070257A1 (en) * | 2015-10-21 | 2017-04-27 | Genesys Telecommunications Laboratories, Inc. | Data-driven dialogue enabled self-help systems |
US20170300831A1 (en) * | 2016-04-18 | 2017-10-19 | Google Inc. | Automated assistant invocation of appropriate agent |
WO2018067368A1 (en) * | 2016-10-04 | 2018-04-12 | Google Llc | Hierarchical annotation of dialog acts |
Family Cites Families (13)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20060150188A1 (en) * | 2004-12-21 | 2006-07-06 | Manuel Roman | Method and apparatus for supporting soft real-time behavior |
US9202171B2 (en) | 2008-11-11 | 2015-12-01 | Digideal Corporation | Virtual game assistant based on artificial intelligence |
US8689191B2 (en) * | 2010-03-05 | 2014-04-01 | International Business Machines Corporation | Correct refactoring of concurrent software |
US9930092B2 (en) * | 2010-12-06 | 2018-03-27 | Zoho Corporation Private Limited | Editing an unhosted third party application |
US9201865B2 (en) * | 2013-03-15 | 2015-12-01 | Bao Tran | Automated assistance for user request that determines semantics by domain, task, and parameter |
US9594542B2 (en) * | 2013-06-20 | 2017-03-14 | Viv Labs, Inc. | Dynamically evolving cognitive architecture system based on training by third-party developers |
US10170106B2 (en) | 2015-10-21 | 2019-01-01 | Google Llc | Parameter collection and automatic dialog generation in dialog systems |
US10757048B2 (en) * | 2016-04-08 | 2020-08-25 | Microsoft Technology Licensing, Llc | Intelligent personal assistant as a contact |
US20180033017A1 (en) * | 2016-07-29 | 2018-02-01 | Ramesh Gopalakrishnan IYER | Cognitive technical assistance centre agent |
US10467345B2 (en) * | 2016-08-30 | 2019-11-05 | Microsoft Technology Licensing, Llc | Framework for language understanding systems that maximizes reuse through chained resolvers |
US10217462B2 (en) * | 2016-08-31 | 2019-02-26 | Microsoft Technology Licensing, Llc | Automating natural language task/dialog authoring by leveraging existing content |
US11755997B2 (en) * | 2017-02-22 | 2023-09-12 | Anduin Transactions, Inc. | Compact presentation of automatically summarized information according to rule-based graphically represented information |
US11170285B2 (en) * | 2017-05-05 | 2021-11-09 | Google Llc | Virtual assistant configured to recommended actions in furtherance of an existing conversation |
-
2018
- 2018-05-01 WO PCT/US2018/030451 patent/WO2019203859A1/en unknown
- 2018-05-01 CN CN201880093458.2A patent/CN112136124B/en active Active
- 2018-05-01 EP EP18726624.2A patent/EP3776246A1/en active Pending
- 2018-05-01 US US15/772,998 patent/US10803860B2/en active Active
-
2020
- 2020-10-05 US US17/063,196 patent/US11488601B2/en active Active
Patent Citations (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN1321296A (en) * | 1998-10-02 | 2001-11-07 | 国际商业机器公司 | Conversational computing via conversational virtual machine |
US20130275164A1 (en) * | 2010-01-18 | 2013-10-17 | Apple Inc. | Intelligent Automated Assistant |
CN105808200A (en) * | 2010-01-18 | 2016-07-27 | 苹果公司 | Intelligent automated assistant |
CN102236701A (en) * | 2010-05-07 | 2011-11-09 | 微软公司 | Dependency graphs for multiple domains |
WO2014197635A2 (en) * | 2013-06-07 | 2014-12-11 | Apple Inc. | Intelligent automated assistant |
WO2017070257A1 (en) * | 2015-10-21 | 2017-04-27 | Genesys Telecommunications Laboratories, Inc. | Data-driven dialogue enabled self-help systems |
US20170300831A1 (en) * | 2016-04-18 | 2017-10-19 | Google Inc. | Automated assistant invocation of appropriate agent |
CN106528613A (en) * | 2016-05-26 | 2017-03-22 | 中科鼎富（北京）科技发展有限公司 | Intelligent question-answer (Q&A) method and device |
WO2018067368A1 (en) * | 2016-10-04 | 2018-04-12 | Google Llc | Hierarchical annotation of dialog acts |
Non-Patent Citations (2)
Title |
---|
MARTIN WITZEL 等: "User-Centered Multimodal Interaction Graph for Design Reviews", 《2008 IEEE VIRTUAL REALITY CONFERENCE》, pages 299 - 300 * |
龚凯乐 等: "基于"问题-用户"的网络问答社区专家发现方法研究", 《图书情报工作》, pages 115 - 121 * |
Also Published As
Publication number | Publication date |
---|---|
US20200258509A1 (en) | 2020-08-13 |
EP3776246A1 (en) | 2021-02-17 |
CN112136124B (en) | 2024-02-20 |
US11488601B2 (en) | 2022-11-01 |
WO2019203859A1 (en) | 2019-10-24 |
US10803860B2 (en) | 2020-10-13 |
US20210020178A1 (en) | 2021-01-21 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11488601B2 (en) | Dependency graph conversation modeling for use in conducting human-to-computer dialog sessions with a computer-implemented automated assistant | |
US10679622B2 (en) | Dependency graph generation in a networked system | |
US11971936B2 (en) | Analyzing web pages to facilitate automatic navigation | |
US11727220B2 (en) | Transitioning between prior dialog contexts with automated assistants | |
US11941420B2 (en) | Facilitating user device and/or agent device actions during a communication session | |
US10431219B2 (en) | User-programmable automated assistant | |
CN110770694B (en) | Obtaining response information from multiple corpora | |
US10826856B2 (en) | Automated generation of prompts and analyses of user responses to the prompts to determine an entity for an action and perform one or more computing actions related to the action and the entity | |
CN110582765A (en) | Proactive incorporation of unsolicited content into human-to-computer conversations | |
CN115643307A (en) | Method, system and storage medium for generating and transmitting call requests | |
US20180314532A1 (en) | Organizing messages exchanged in human-to-computer dialogs with automated assistants | |
JP6797212B2 (en) | User Interface of User's Computing Device Automatic launch and adaptation of dialogs with users through devices |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |