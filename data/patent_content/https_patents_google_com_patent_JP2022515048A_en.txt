JP2022515048A - Transliteration for speech recognition training and scoring - Google Patents
Transliteration for speech recognition training and scoring Download PDFInfo
- Publication number
- JP2022515048A JP2022515048A JP2021533448A JP2021533448A JP2022515048A JP 2022515048 A JP2022515048 A JP 2022515048A JP 2021533448 A JP2021533448 A JP 2021533448A JP 2021533448 A JP2021533448 A JP 2021533448A JP 2022515048 A JP2022515048 A JP 2022515048A
- Authority
- JP
- Japan
- Prior art keywords
- script
- speech recognition
- language
- word
- recognition model
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/06—Creation of reference templates; Training of speech recognition systems, e.g. adaptation to the characteristics of the speaker's voice
- G10L15/063—Training
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/40—Processing or translation of natural language
- G06F40/58—Use of machine translation, e.g. for multi-lingual retrieval, for server-side translation for client devices or for real-time translation
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/16—Speech classification or search using artificial neural networks
Abstract
音声認識の訓練（学習）および採点用の音訳のための、コンピュータ可読記憶媒体に格納されたコンピュータプログラムを備えている方法、システム、および装置が提供される。いくつかの実装では、言語例にアクセスし、そのうちのいくつかは、第１スクリプトの単語と、１つまたは複数の他のスクリプトの単語とを備えている。言語例の少なくとも一部が第１スクリプトに音訳され、訓練データセットが生成される。第１スクリプト内の訓練データセットに含まれる異なる単語シーケンスの出現率に基づき、言語モデルが生成される。言語モデルは、発話に対する音声認識を行うために使用される。Methods, systems, and devices are provided that include computer programs stored in computer-readable storage media for speech recognition training (learning) and transliteration for scoring. Some implementations access language examples, some of which include words from the first script and words from one or more other scripts. At least part of the language example is transliterated into the first script to generate a training dataset. A language model is generated based on the rate of occurrence of different word sequences contained in the training data set in the first script. The language model is used to perform speech recognition for utterances.
Description
本開示は、音声認識の訓練と採点とのための音訳に関する。 The present disclosure relates to transliteration for speech recognition training and scoring.
話し言葉や書き言葉（ｓｐｏｋｅｎ ａｎｄ ｗｒｉｔｔｅｎ ｌａｎｇｕａｇｅ）の多くのインスタンスには、２つ以上の異なる言語や方言（ｄｉａｌｅｃｔｓ）の単語（ｗｏｒｄｓ）が含まれている。多くの音声認識システム（ｓｐｅｅｃｈ ｒｅｃｏｇｎｉｔｉｏｎ ｓｙｓｔｅｍ）は、異なる言語または方言からの単語を組み合わせた音声（ｓｐｅｅｃｈ）を正確に転写（ｔｒａｎｓｃｒｉｂｉｎｇ）することが困難である。同様に、異なる言語や方言の単語を備えている、特に複数の書記体系（ｗｒｉｔｉｎｇ ｓｙｓｔｅｍｓ）からの言語を備えている書き言葉の例を用いて音声認識システムを訓練することは、困難な場合が多い。 Many instances of spoken and written language contain words in two or more different languages and dialects. Many speech recognition systems have difficulty in accurately transcribing a speech that combines words from different languages or dialects. Similarly, it is often difficult to train speech recognition systems with examples of written words that have words in different languages and dialects, especially those that have languages from multiple writing systems. ..
音声認識の訓練と採点のための音訳（ｔｒａｎｓｌｉｔｅｒａｔｉｏｎ）を改善する余地がある。 There is room for improvement in transliteration for speech recognition training and scoring.
いくつかの実装では、音声認識モデル（ｓｐｅｅｃｈ ｒｅｃｏｇｎｉｔｉｏｎ ｍｏｄｅｌ）の訓練を強化するとともに、音声認識モデルの評価をより正確に行うべく、言語例（ｌａｎｇｕａｇｅ ｅｘａｍｐｌｅｓ）を処理することで、少なくとも一部の単語を異なるスクリプトに音訳（ｔｒａｎｓｌｉｔｅｒａｔｅ）することができる。いくつかのインスタンスでは、言語モデルの訓練またはテストに使用されるような書き言語の例は、１つのスクリプト（例えば、第１スクリプトまたは主（ｐｒｉｍａｒｙ）スクリプト）で書かれた単語だけでなく、異なるスクリプトの１つまたは複数の単語を備えていることができる。モデルを学習または評価する前に、スクリプト外の単語を主スクリプトに音訳することで、言語サンプルを正規化できる。これは、スクリプト外の単語すべてに対して行うことも、より選択的に行うこともできる（例えば、固有名詞を音訳しないなど）。結果として得られるモデルは、スクリプトが混在した例を訓練から除外したモデルよりも、および混在したスクリプトを音訳せずに使用したモデルよりも、優れた精度を提供する。 In some implementations, at least some words are processed by processing language examples to enhance training in the speech recognition model and to evaluate the speech recognition model more accurately. Can be transliterated into different scripts. In some instances, examples of writing languages, such as those used to train or test a language model, differ not only from words written in one script (eg, the first script or the primary script). It can have one or more words in the script. Language samples can be normalized by transcribing non-script words into the main script before training or evaluating the model. This can be done for all words outside the script or more selectively (for example, do not transliterate proper nouns). The resulting model provides better accuracy than a model that excludes mixed script examples from training and a model that uses mixed scripts without transliteration.
一般的に、複数のスクリプトが１つのフレーズや文に使用されている場合には、言語モデルが、主スクリプトにはない単語の適切な使用法（ｐｒｏｐｅｒ ｕｓｅ）を適切に学習することが困難になる場合がある。そのため、スクリプトが混在する例は、言語モデルの訓練に使用されないように、従来では学習データセットから削除されていた。これには、モデルが学習できる例の量が制限されるという欠点がある。特に、複数のスクリプト内の単語が一緒に使用されている場合、モデルが文脈（ｃｏｎｔｅｘｔ）や用法（ｕｓａｇｅ）を学習する機会が失われてしまう。認識精度を向上させるために、音訳を使用して所望のスクリプトの訓練データセットを正規化し、モデル化（ｍｏｄｅｌｉｎｇ）の精度を向上させることができる。 In general, when multiple scripts are used in a phrase or sentence, it becomes difficult for the language model to properly learn the proper usage of words that the main script does not have. May be. Therefore, mixed script examples have traditionally been removed from the training dataset so that they are not used to train language models. This has the disadvantage of limiting the amount of examples the model can train. In particular, when words in multiple scripts are used together, the model loses the opportunity to learn context and usage. In order to improve the recognition accuracy, transliteration can be used to normalize the training data set of the desired script and improve the accuracy of modeling.
言語モデル訓練データの音訳（ｔｒａｎｓｌｉｔｅｒａｔｉｏｎ）は、転写（ｔｒａｎｓｃｒｉｐｔｉｏｎ）の不一致を減らし、より良い正規化を提供し、自動音声認識システムの全体的な性能を向上させる。この機能によって、言語モデル訓練データを増強することができ、認識器の仮説を１つの書記体系に適合させることを強制することができる。対照的に、モデルの訓練で副次的な書記体系の音声をすべて許容した場合、結果として得られるモデルは副次的な書記体系の音声を多く出力する傾向があり、モデルの仮説とトランスクリプトの真実との間に既に存在する書記体系の不一致をさらに増大させることになる。また、単語カウントが２つ以上の表現に拡散してしまう。例えば、第１スクリプトの単語と第２スクリプトの単語が同じ意味と発音を指しているにもかかわらず、第１スクリプトの単語と第２スクリプトの単語が混在してしまう。訓練データを音訳することで、モデル出力は主に所望のスクリプトで維持され、同じ単語の単語カウントが任意のスクリプトのインスタンスに対して結合されることで精度が向上する。 Transliteration of language model training data reduces transcription discrepancies, provides better normalization, and improves the overall performance of automated speech recognition systems. This feature can enhance language model training data and force the recognizer hypothesis to fit into one secretary system. In contrast, if the training of the model allows all the audio of the secondary secretary system, the resulting model tends to output more audio of the secondary secretary system, the model hypothesis and transcript. It will further increase the inconsistency of the existing writing system with the truth of. Also, the word count spreads to two or more expressions. For example, even though the words in the first script and the words in the second script have the same meaning and pronunciation, the words in the first script and the words in the second script are mixed. By transliterating the training data, the model output is mainly maintained in the desired script, and the word counts of the same word are combined for any script instance to improve accuracy.
いくつかの実装では、１つまたは複数のコンピュータによって実行される方法は、第１スクリプトの言語例を示すデータセットにアクセスする工程であって、言語例の少なくとも一部（ａｔ ｌｅａｓｔ ｓｏｍｅ ｏｆ ｔｈｅ ｌａｎｇｕａｇｅ ｅｘａｍｐｌｅｓ）は、第１スクリプトの単語（ｗｏｒｄｓ）および１つまたは複数の他のスクリプトの単語を備えている、アクセスする工程と、第１スクリプトに音訳された単語を有する訓練データセットを生成するべく、言語例の一部の少なくとも一部（ａｔ ｌｅａｓｔ ｐｏｒｔｉｏｎｓ ｏｆ ｓｏｍｅ ｏｆ ｔｈｅ ｌａｎｇｕａｇｅ ｅｘａｍｐｌｅｓ）を第１スクリプトに音訳する工程と、第１スクリプトに音訳された単語を有する訓練データセット内の単語シーケンスの出現（ｏｃｃｕｒｒｅｎｃｅｓ）に基づき、音声認識モデルを生成する工程とを備えている。本方法はさらに、オプションとして、発話（ｕｔｔｅｒａｎｃｅ）に対する音声認識を実行するべく、音声認識モデルを使用する工程を備えていることができる。 In some implementations, the method performed by one or more computers is the step of accessing the dataset showing the language example of the first script, at least part of the language example (at least some of the language). expands) to generate a training dataset with access steps and transliterated words in the first script, including words in the first script and words in one or more other scripts. , The process of transliterating at least some of the language examples (at least parts of the language exchanges) into the first script, and the appearance of a word sequence in the training dataset that has the transliterated words in the first script. It includes a step of generating a voice recognition model based on (occurrences). The method can further optionally include a step of using a speech recognition model to perform speech recognition for utterances.
いくつかの実装では、音声認識モデルは、言語モデル、音響モデル、シーケンスツーシーケンスモデル、またはエンドツーエンドモデルである。
いくつかの実装では、音訳する工程は、異なるスクリプトからのテキストを表す（ｒｅｐｒｅｓｅｎｔ）異なるトークンを、単一の正規化された音訳表現に写像（ｍａｐｐｉｎｇ）する工程を備えている。
In some implementations, the speech recognition model is a language model, an acoustic model, a sequence-to-sequence model, or an end-to-end model.
In some implementations, the transliteration process comprises mapping different tokens representing text from different scripts into a single normalized transliteration representation.
いくつかの実装では、言語例を音訳する工程は、第１スクリプトにはない言語例内の単語を、第１スクリプトに音訳する工程を備えている。
いくつかの実装では、言語例を音訳する工程は、第１スクリプトとは異なるスクリプト内の用語（ｔｅｒｍｓ）のブラックリストにアクセスする工程と、言語例に出現（ｏｃｃｕｒ）するブラックリストからの用語のインスタンスの音訳をバイパスする工程とを備えている。
In some implementations, the process of transliterating a language example comprises translating a word in the language example that is not in the first script into the first script.
In some implementations, the process of transliterating a language example is the process of accessing a blacklist of terms (terms) in a script that is different from the first script, and the process of occurning the terms from the blacklist. It has a process of bypassing the transliteration of the instance.
いくつかの実装では、言語例を音訳する工程は、変更された言語例を生成する工程を備えており、前記変更された言語例では、第１スクリプトとは異なる第２スクリプトで書かれた単語は、第１スクリプト内の単語の音響特性に近似する第１スクリプト内の１つまたは複数の単語に置き換えられる。 In some implementations, the process of transliterating a language example comprises the process of generating a modified language example, in which the modified language example is a word written in a second script that is different from the first script. Is replaced with one or more words in the first script that closely resemble the acoustic properties of the words in the first script.
いくつかの実装では、第２スクリプトに書かれた単語は、単語ごとベースで第１スクリプトに個別に音訳される。
いくつかの実装形態において、本方法は、音声認識モデルをテストする言語例のテストセットを判定する工程と、テストセット内の言語例のうち、第１スクリプトに書かれていない単語を第１スクリプトに音訳することで、正規化テストセットを生成する工程と、テストセット内の言語例に対応する音声認識モデルの出力を取得する工程と、音声認識モデル出力のうち、第１スクリプトに書かれていない音声認識モデル出力の単語を第１スクリプトに音訳することで、音声認識モデルの出力を正規化する工程と、正規化テストセットと正規化音声認識モデル出力との比較に基づき、音声認識モデルの誤り率を判定する工程と、を備えている。
In some implementations, the words written in the second script are individually transliterated into the first script on a word-by-word basis.
In some implementations, the method determines the test set of language examples that test the speech recognition model, and the first script is a word that is not written in the first script among the language examples in the test set. It is written in the first script of the process of generating the normalization test set, the process of acquiring the output of the speech recognition model corresponding to the language example in the test set, and the speech recognition model output by transcribing to. Based on the process of normalizing the output of the speech recognition model by transcribing the words of the non-speech recognition model output into the first script, and the comparison between the normalization test set and the normalized speech recognition model output, the speech recognition model It includes a step of determining an error rate.
いくつかの実装では、誤り率は単語誤り率であり、本方法は、単語誤り率に基づき、音声認識モデルの訓練を継続するか終了するかを判定する工程と、音声認識モデルを訓練するために使用される訓練データセットを変更する工程と、音声認識モデルのサイズ、構造、または他の特性を設定する工程と、音声認識タスクのために１つまたは複数の音声認識モデルを選択する工程と、を備えている。 In some implementations, the error rate is the word error rate, and the method is based on the word error rate to determine whether to continue or end the training of the speech recognition model and to train the speech recognition model. The process of modifying the training data set used for, setting the size, structure, or other characteristics of the speech recognition model, and selecting one or more speech recognition models for the speech recognition task. , Is equipped.
いくつかの実装において、本方法は、複数のスクリプトのいずれかで書かれた音響的に類似した単語が、参照転写における対応する単語とは異なるスクリプトでの単語の出力にペナルティを課すことなく、正しい転写として受け入れられる、音声認識モデルのモデル化誤り率を判定する工程を備えている。 In some implementations, the method does not penalize the output of an acoustically similar word written in one of multiple scripts in a different script than the corresponding word in reference transcription. It includes a step of determining the modeling error rate of the speech recognition model, which is accepted as correct transcription.
いくつかの実装では、本方法は、参照転写における対応する単語のスクリプトに対する、音声認識モデルの出力における単語のスクリプト間の差異の尺度である、音声認識モデルのレンダリング誤り率を判定する工程を備えている。 In some implementations, the method comprises the step of determining the rendering error rate of a speech recognition model, which is a measure of the difference between word scripts in the output of the speech recognition model for the corresponding word script in reference transcription. ing.
いくつかの実装では、音訳は、第１スクリプトへの音訳を実行するように訓練された有限状態変換器ネットワークを使用して実行される。
いくつかの実装では、音訳は、少なくとも１つの言語例について、第１スクリプトにおける訓練データセットに含まれる第１スクリプトにおける音訳された表現に到達するために、スクリプト同士間で複数の音訳のラウンドを実行する工程を備えている。
In some implementations, transliteration is performed using a finite state transducer network trained to perform transliteration to a first script.
In some implementations, the transliteration makes multiple rounds of transliteration between the scripts in order to reach the transliterated representation in the first script contained in the training dataset in the first script for at least one language example. It has a process to carry out.
いくつかの実装では、本方法は、言語例におけるスクリプト同士の混合のレベルを示すスコアを判定する工程を備えるとともに、スコアに基づき音訳のために有限状態変換器ネットワークをプルーニングするためのパラメータを選択する工程か、スコアに基づき音声認識モデルをプルーニングするためのパラメータを選択する工程か、またはスコアに基づき音声認識モデルのサイズまたは構造を選択する工程を備えている。 In some implementations, the method comprises determining a score that indicates the level of mixing between scripts in a language example, and selects parameters for pruning the finite state converter network for transliteration based on the score. It comprises a step of selecting parameters for pruning the speech recognition model based on the score, or a step of selecting the size or structure of the speech recognition model based on the score.
いくつかの実装では、音声認識モデルを生成する工程は、いくつかの言語例の少なくとも一部を第１スクリプトに音訳した後、１つまたは複数のコンピュータによって、第１スクリプトの訓練データセットにおける異なる単語シーケンスの出現回数を判定する工程と、１つまたは複数のコンピュータによって、第１スクリプトの訓練データセットにおける異なる単語シーケンスの出現回数に基づき、音声認識モデルを生成する工程と、を備えている。 In some implementations, the process of generating a speech recognition model differs in the training data set of the first script by one or more computers after transliterating at least some of some language examples into the first script. It comprises a step of determining the number of occurrences of a word sequence and a step of generating a speech recognition model based on the number of occurrences of different word sequences in the training data set of the first script by one or a plurality of computers.
いくつかの実装では、音声認識モデルはリカレントニューラルネットワークを備えており、音声認識モデルを生成する工程は、リカレントニューラルネットワークを訓練する工程を備えている。 In some implementations, the speech recognition model comprises a recurrent neural network, and the process of generating the speech recognition model comprises the step of training the recurrent neural network.
本開示はまた、音声認識を実行する方法であって、前記方法は、１つまたは複数のコンピュータによって、発話を表す音声データを受け取る工程と、１つまたは複数のコンピュータによって、発話を表すテキスト（または他の記号表現）へと音声データを写像するべく音声認識モデルを使用する工程であって、音声認識モデルは本明細書に開示されるいずれかに従って以前に生成されている、前記音声認識モデルを使用する工程とを備えている、方法を提供する。音声認識モデルを生成するために使用されるコンピュータは、音声認識を実行するために使用されるコンピュータとは異なる場合があることが理解されるであろう。本方法はさらに、出力を表すテキストを出力する工程を含んでもよい。 The present disclosure is also a method of performing speech recognition, wherein the method receives speech data representing speech by one or more computers and text representing speech by one or more computers ( The speech recognition model is a step of using the speech recognition model to map the speech data to (or other symbolic representation), wherein the speech recognition model is previously generated in accordance with any of the disclosures herein. Provides a method that comprises the steps of using. It will be appreciated that the computer used to generate the speech recognition model may differ from the computer used to perform speech recognition. The method may further include the step of outputting a text representing the output.
この局面（ａｓｐｅｃｔ）の他の実施形態は、それぞれが方法の動作を実行するように構成された、対応するシステム、装置、およびコンピュータ記憶装置に記録されたコンピュータプログラムを備えている。 Another embodiment of this aspect comprises a computer program recorded in a corresponding system, device, and computer storage device, each configured to perform the operation of the method.
本明細書に記載されている主題の１つまたは複数の実施形態の詳細は、添付の図面および以下の詳細な説明に記載されている。主題の他の特徴、局面、および利点は、詳細な説明、図面、および特許請求の範囲から明らかになるであろう。 Details of one or more embodiments of the subject matter described herein are described in the accompanying drawings and the detailed description below. Other features, aspects, and advantages of the subject will become apparent from the detailed description, drawings, and claims.
様々な図面における同様の参照番号および指定は、同様の要素を示す。
図１は、音声認識および評価のための音訳のためのシステム１００の一例を示す図である。このシステムは、コンピュータシステム１１０を備えており、このコンピュータシステム１１０は、一緒にまたは互いに遠隔地に配置された１つまたは複数のコンピュータを含んでもよい。コンピュータシステム１１０は、音訳モジュール１２０と、モデル訓練モジュール１３０と、採点（スコアリング）モジュール１４０とを有する。図１の例では、システム１００は、言語モデル１５０などの音声認識モデルを訓練および評価するために使用される。
Similar reference numbers and designations in various drawings indicate similar elements.
FIG. 1 is a diagram showing an example of a
実施例では、言語例１１２のセットは、クエリログ、ウェブページ、書籍、人間または機械によって認識された音声トランスクリプションなど、様々なソースのいずれかから得られる。言語例１１２は、主に第１スクリプト内にある。本明細書では、「スクリプト」という用語は、一般的に、書記体系（ｗｒｉｔｉｎｇ ｓｙｓｔｅｍ）を指す。書記体系とは、自然言語を表現するために使用されるシンボルのシステムである。本明細書に開示された技術が使用できるスクリプトの例には、ラテン語、キリル文字、ギリシャ語、アラビア語、インド系語、または別の書記体系が含まれる。言語モデル１５０は、第１スクリプトのテキストを主に表す出力を提供するように訓練される。それにもかかわらず、主に１つのスクリプトで書かれたフレーズまたは文が、別のスクリプトを使用して書かれた１つまたは複数の単語を備えている場合がよくある。言語例１１２は、通常、純粋に第１スクリプトで書かれた例をほとんど備えているが、第１スクリプトの単語だけでなく、別のスクリプトの単語の組み合わせを備えているいくつかの言語例も備えている。
In an embodiment, the set of language examples 112 is obtained from any of a variety of sources, such as query logs, web pages, books, human or machine-recognized voice transcriptions, and so on. The language example 112 is mainly in the first script. As used herein, the term "script" generally refers to a writing system. The writing system is a system of symbols used to represent natural language. Examples of scripts for which the techniques disclosed herein can be used include Latin, Cyrillic, Greek, Arabic, Indian, or another writing system. The
音訳モジュール１２０は、言語例１１２を処理して、第１スクリプトの正規化されたデータセットを生成する。言語モデル１５０を訓練する前に、音訳モジュール１２０は、言語例１１２を処理して、第１スクリプトにはない単語のインスタンスを第１スクリプトに音訳する。音訳モジュール１２０は、有限状態変換器（ＦＳＴ）ネットワークを使用して音訳を行うことができる。異なるスクリプトの書記素（ｇｒａｐｈｅｍｅｓ）と単語との間の関係は、言語例１１２の分析を通じて、または他のデータから学習できる。いくつかの実装では、音訳における高品質の対応関係を実現するために、個々の単語に対して個別に、例えば、単語ごとベース（ｗｏｒｄ－ｂｙ－ｗｏｒｄ ｂａｓｉｓ）で音訳が行われる。この処理では、高精度の音訳を提供するために、周囲の単語の文脈（ｃｏｎｔｅｘｔ）を任意に考慮してもよい。音訳処理を通じて、第１スクリプトに音訳された結果としての単語は、言語例１１２に出現する対応するオリジナルの単語の発音に一致または密接に近似した発音を有するものとすることができる。このように、音訳処理は、第１スクリプトに元々書かれていないオリジナルの単語のために、第１スクリプト内の置き換えられた単語がオリジナルの単語と同じまたは類似の音響特性または音（ｓｏｕｎｄｓ）を表すように、書記体系を変更することができる。
The
いくつかの実装では、第１スクリプトにはないすべての単語は、第１スクリプトに音訳される。他の実施態様では、音訳は選択的に行われる。例えば、音訳ブラックリスト１２６は、音訳されるべきではない単語またはフレーズを示すことができる。音訳モジュール１２０は、そうして、訓練データおよびテストデータを生成する際に、オリジナルのスクリプトが第１スクリプトとは異なっていても、これらの用語をオリジナルのスクリプトに保持する。ブラックリストは、例えば、人、場所、会社、およびその他のエンティティのための固有名詞（ｐｒｏｐｅｒ ｎａｍｅｓ）を備えていることが特に有用であり、これらの固有名詞は、音訳されたバージョンと比較して、それらのネイティブな書記体系においてより一般的であるか、またはより認識可能である場合がある。例えば、「Ｇｅｏｒｇｅ Ｗａｓｈｉｎｇｔｏｎ」（ジョージ ワシントン）、「Ｎｅｗ Ｙｏｒｋ」（ニューヨーク）、「Ｇｏｏｇｌｅ」（グーグル）などの名前は、インド語、キリル語、ハンジ（中国漢字）、仮名、漢字などの他のスクリプトが主に使われているテキストでも、ラテン語のスクリプトのままにしておくことが好ましい場合がある。スクリプト外の単語を学習データセットに含めることで、言語モデル１５０は、これらの単語の出力を、たとえそのスクリプトが優勢（ｄｏｍｉｎａｎｔ）なスクリプトとは異なっていても、そのネイティブのスクリプトで予測することを学習することができる。
In some implementations, all words that are not in the first script are transliterated into the first script. In other embodiments, transliteration is done selectively. For example, the
言語例１１２から、音訳モジュール１２０は、言語モデル１５０を訓練するためのスクリプト正規化された訓練データ１２２を生成する。また、音訳モジュール１２０は、言語モデル１５０のテストに使用するスクリプト正規化されたテストデータ１２４を生成する。テストは、例えば、一定量のテストが完了した後など、様々な段階で行われてもよい。言語モデル１５０は、所望のレベルの性能が達成されるまで、訓練処理同士の間で、繰り返しテストされてもよい。
From the language example 112, the
いくつかの実装では、言語モデル１５０は、入力として、言語シーケンスを表す音響単位または言語単位（ｌｉｎｇｕｉｓｔｉｃ ｕｎｉｔｓ）を表すデータ、例えば、言語シーケンスの発音を表すデータを受け取るように構成される。例えば、入力は、単音シリーズ（ａ ｓｅｒｉｅｓ ｏｆ ｐｈｏｎｅｓ）（これは、文脈依存または文脈非依存であってもよい）、または単音セットに対するスコアの分布を示してもよい。これらの発音は、複数の方法のいずれかで言語例１１２に対して判定することができる。音声データが利用可能な言語例については、発音は、音声データに対する音響モデルの出力であってもよい。ウェブページ、書籍、キーボード入力されたクエリなどからの例のように、対応するオーディオデータがない言語例については、システムは、発音生成器１２６を使用して、書かれたテキストから自動的に発音を生成することができる。発音生成器１２６は、言語例１１２の言語における単語の発音を示す辞書（ｌｅｘｉｃｏｎ）１２８にアクセスして、例えば、書記素（ｇｒａｐｈｅｍｅ）シーケンスから音素シーケンスへの写像にアクセスして、書記素から音素への変換を行うことができる。さらに、いくつかの言語例では、単語は、言語学者からの手動アノテーションによって提供される発音を有してもよい。
In some implementations, the
図１には、発音生成器１２６および辞書１２８が示されているが、これらの要素は、いくつかの実装では任意である。例えば、テキストに関連付けられた対応する発音を既に有する言語例１１２が受け取られてもよい。また、いくつかの実装では、言語モデル１５０は、言語単位情報を入力として受け取らず、単に、書記素または単語のシーケンスを示すデータを受け取るだけである。例えば、いくつかの言語モデルは、転写候補（ｃａｎｄｉｄａｔｅ ｔｒａｎｓｃｒｉｐｔｉｏｎｓ）のセカンドパスの再採点に使用されることがあり、そのため、提案された単語シーケンスを示すデータを受け取ることがある。このシナリオにおける言語モデルは、発音の言語単位ではなく、単語自体を示すデータを受け取り、言語モデルの学習を考慮して、全体的なシーケンスの可能性を示すスコアを提供するように構成されていてもよい。
Although FIG. 1 shows the
モデル訓練モジュール１３０は、言語モデル１５０の学習を行う。例えば、言語モデル１５０は、スクリプト正規化された訓練データ１２２における異なる単語およびフレーズの出現カウントに基づき生成されるｎ－ｇｒａｍモデルなどの統計的言語モデルであり得る。ニューラルネットワークモデルなどの他のタイプのモデルは、追加的または代替的に使用されてもよい。例えば、言語モデル１５０は、モデル訓練モジュール１３０が誤差逆伝播、確率的勾配降下法、または他の技術を使用して訓練するニューラルネットワークを備えていることができる。ニューラルネットワークは、長短期記憶（ＬＳＴＭ）セルを有する１つまたは複数の層を備えているようなリカレントニューラルネットワークであってもよい。モデルは、ＣＴＣ（Ｃｏｎｎｅｃｔｉｏｎｉｓｔ Ｔｅｍｐｏｒａｌ Ｃｌａｓｓｉｆｉｃａｔｉｏｎ）目的関数、状態レベル最小ベイズリスク（ｓＭＢＲ：ｓｔａｔｅ－ｌｅｖｅｌ Ｍｉｎｉｍｕｍ Ｂａｙｅｓｉａｎ Ｒｉｓｋ）目的関数、または別の目的関数などの目的関数を最小化することで訓練されてもよい。
The
一般に、言語モデル１５０は、言語的文脈、例えば、１つまたは複数の周囲の単語またはフレーズに少なくとも部分的に基づき、単語の予測、例えば、出現の相対的な可能性を示すように訓練される。したがって、言語モデル１５０は、１つまたは複数の直前の単語の出現を考慮して、語彙内の異なる単語が出現する確率を示す出力を提供するように構成されてもよい。また、予測は、単音シーケンスを示す発音や言語モデルの出力など、音響単位または言語単位に基づくこともできる。音声に存在する音を、例えば、単音や他の言語単位として示す音響モデルと共に使用すると、言語モデル１５０は、スクリプト正規化された訓練データ１２２から観察された実際の言語使用のパターンに応じて、どの単語および単語シーケンスがそれらの音を最もよく表すかを示すことができる。言語モデル１５０は、異なる単語が互いに続くであろう相対的な可能性を示すスコア、例えば、確率スコアまたは信頼スコアを生成するために使用することができ、これは、音声格子を生成するために使用することができる。次いで、最も可能性が高いと考えられる転写を表す格子を通る最良の経路、例えば、最高スコアまたは最低コストの経路を判定するために、ビームサーチ処理を使用することができる。
In general, the
訓練後または訓練中に、採点モジュール１４０は、言語モデル１５０の精度（ａｃｃｕｒａｃｙ）を評価するために使用される。評価の結果は、言語モデル１５０の性能を示す１つまたは複数のスコア１４２の生成を備えている。採点モジュール１４０は、スクリプト正規化されたテストデータ１２４からの例を言語モデル１５０への入力として提供し、言語モデル１５０に、各入力例に対する出力を、例えば、予測または確率スコアを生成させることができる。言語モデルの出力は、各入力例についての言語シーケンス出力を生成するために、例えば、格子およびビーム探索を使用して、または他の技術を介して、さらに処理されてもよい。
After or during training, the
任意で、出力シーケンス内における第１書記体系ではない単語は、音訳モジュール１２０（図示を明確にするために、図中では重複している）を使用して音訳される。上述のように生成された言語モデル１５０について、モデル１５０は、音訳ブラックリスト１２６内の単語を除いて、支配的な第１スクリプトの単語のみを示すように学習する。その結果、このように訓練されたモデルは、第１スクリプトに存在する出力シーケンスを示すことになる。それにもかかわらず、より一般化されたアプローチをするべく、また同じ条件またはデータセットの下で訓練されていないものを備えているあらゆるタイプの言語モデルの評価を可能にするべく、音訳を使用することで、より正確な比較のために出力シーケンスを正規化することができる。
Optionally, words that are not the First Secretary system in the output sequence are transliterated using transliteration module 120 (overlapping in the figure for clarity of illustration). For the
採点モジュール１４０は、オリジナルの言語例１１２とは異なる単語を出力シーケンスが備えている割合を示す単語誤り率を生成することができる。しかし、従来の単語誤り率の計算では、２つの単語が互いに音訳として同等である（例えば、同じ音と意味的な意味（セマンティックｍｅａｎｉｎｇ）を表す）場合でも、オリジナルとは異なるスクリプト（文字）である場合には、その単語は正しくないとみなされる。言語や書記体系のスイッチングが一般的な場合、モデルが実質的に正しい単語を予測しているにもかかわらず、見かけ上の誤り率が人為的に高くなってしまうことがある。このような不正確さを避けるために、採点モジュール１４０は、モデル化誤り率と呼ばれる、修正された単語誤り率を生成することができる。モデル化誤り率は、第１スクリプトに正規化された出力シーケンスと、第１スクリプトに正規化されたテストデータ１２４とを比較する。その結果、異なるスクリプトで書かれた１つまたは複数の同等の単語をオリジナルの言語例が有していても、言語モデル１５０が完全に第１スクリプトで言語シーケンスを生成する場合には、誤りはカウントされない。別のスコアであるスクリプト誤り率は、出力シーケンスのスクリプトがオリジナルの言語例１１２のスクリプトとは異なる割合を測定するために生成される。
The
採点モジュール１４０からの評価スコア１４２は、ユーザに出力するために提供することができ、また言語モデル１５０の訓練を管理するために使用することができる。例えば、モデル化誤り率は、言語モデル１５０の訓練を継続するかを判定するために、または訓練を終了するかを判定するために使用することができる。誤り率が閾値を下回るまで、訓練を継続するように設定してもよい。別の例として、誤り率の１つまたは複数に基づき、コンピュータシステム１１０は、言語モデル１５０を訓練するために使用される訓練データセットを変更してもよく、例えば、より良い精度を達成するために、異なるデータセットまたは拡張されたデータセットを導入してもよい。別の例として、スコア１４２の１つまたは複数に基づき、コンピュータシステム１１０は、言語モデル１５０のサイズ、構造、または他の特性を設定してもよい。別の例として、言語モデル１５０の評価および１つまたは複数の他の言語モデルを評価するための潜在的なスコアに基づき、コンピュータシステム１１０は、音声認識タスクを実行するために使用される１つまたは複数の言語モデルを選択してもよい。
The rating score 142 from the
いくつかの実装では、他のスコアが判定され、言語モデル１５０および／または音訳モジュール１２０の訓練を調整するために使用される。例えば、コンピュータシステム１１０は、或る言語について一般的なまたは特定のデータセット（言語例１１２など）において、スクリプトの混合使用、例えば「コードスイッチング」が出現する割合を示すデータを得ることができる。このスコアによって、システムは、音訳用の有限状態変換器ネットワークをプルーニングするためのパラメータを選択したり、言語モデルをプルーニングするためのパラメータを選択したり、言語モデルのサイズや構造を選択したりすることができる。実際、言語モデル１５０の構造、訓練、および動作のための多くの異なるパラメータは、データの開発セット（例えば、訓練アルゴリズムを調整し、オーバーフィッティングを防ぐために使用される検証セット）、訓練データセット、モデルサイズ、訓練中の学習率、使用されるモデルのタイプ（例えば、ｎ－ｇｒａｍ、ニューラルネットワーク、最大エントロピーモデルなど）、またはモデルの出力ターゲットのセット（例えば、単語、単語片、または書記素を予測するべく）の選択を備えているスコアを使用して設定することができる。
In some implementations, other scores are determined and used to coordinate the training of the
一般的に、本願の技術は、言語モデルに限らず、様々な異なるタイプのモデルのいずれかを訓練およびテストするために使用することができる。例えば、音訳されたデータは、言語モデル、音響モデル、シーケンスツーシーケンス（ｓｅｑｕｅｎｃｅ－ｔｏ－ｓｅｑｕｅｎｃｅ）モデル、および／またはエンドツーエンドモデル（例えば、音響情報または特徴を受け取り、単語、単語の断片、または書記素の尤度を示す出力を提供するもの）を訓練および採点するために使用することができる。シーケンスツーシーケンスモデルは、入力シーケンスを出力シーケンスに写像することができる。例えば、シーケンスツーシーケンスモデルは、１つまたは複数の話し言葉（ｓｐｏｋｅｎ ｗｏｒｄｓ）を表す音響情報または特徴を受け取り、それらの言葉を表す記号的出力（例えば、テキスト）を生成することができる。 In general, the techniques of the present application can be used to train and test any of a variety of different types of models, not just language models. For example, transliterated data can be a language model, an acoustic model, a sequence-to-sequence model, and / or an end-to-end model (eg, receiving acoustic information or features and receiving words, word fragments, or (Providing an output showing the likelihood of grapheme) can be used for training and scoring. The sequence-to-sequence model can map the input sequence to the output sequence. For example, a sequence-to-sequence model can receive acoustic information or features representing one or more spoken words and generate symbolic output (eg, text) representing those words.
コードスイッチングは、多くの多言語コミュニティで一般的に出現する現象である。コードスイッチングとは、一般に、話者が１つの発話の中で言語を切り替えることを指す。従来の単語誤り率（ワードエラーレート：ＷＥＲ）測定は、転写（ｔｒａｎｓｃｒｉｐｔｉｏｎ）における曖昧さ、スペルミス、２つの異なる書記体系からの単語の借用などの理由によって、コード混合言語の性能を測定するのに十分ではない。これらのレンダリング誤りは、自動音声認識（ＡＳＲ：Ａｕｔｏｍａｔｅｄ Ｓｐｅｅｃｈ Ｒｅｃｏｇｎｉｔｉｏｎ）システムのＷＥＲを人為的に増大させ、その評価を複雑にする。さらに、これらの誤りは、コードスイッチされた言語や音響モデルに起因するモデル化誤りを、正確に評価することを困難にしている。後述するように、新しい指標（メトリック）である音訳最適化単語誤り率（ｔｏＷＥＲ）は、すべてのテキストを１つの書記体系に写像することで、これらの不規則性の多くを平滑化することができる。或る言語に存在するコードスイッチングの量との相関関係があることを示している。 Code-switching is a common phenomenon in many multilingual communities. Code-switching generally refers to the speaker switching languages in one utterance. Traditional word error rate (WER) measurements are used to measure the performance of mixed code languages due to reasons such as ambiguity in translation, misspelling, and borrowing of words from two different writing systems. Not enough. These rendering errors artificially increase the WER of the Automated Speech Recognition (ASR) system and complicate its evaluation. Moreover, these errors make it difficult to accurately assess modeling errors due to code-switched language and acoustic models. As will be described later, a new metric, transliteration-optimized word error rate (toWER), can smooth out many of these irregularities by mapping all text to a single secretary system. can. It shows that there is a correlation with the amount of code switching present in a language.
これらの技術は、バイリンガルのコードスイッチされた発話のための音響および言語モデル化を改善するために使用することもできる。インド系言語を論じた例を詳細に説明するが、これらの技術は、言語と書記体系のあらゆる組み合わせに使用することができる。音訳アプローチは、従来のｎ－ｇｒａｍ言語モデル、最大エントロピーベース言語モデル、およびＬＳＴＭ（Ｌｏｎｇ Ｓｈｏｒｔ Ｔｅｒｍ Ｍｅｍｏｒｙ）言語モデルといった３種類の言語モデルと、最先端のＣＴＣ（Ｃｏｎｎｅｃｔｉｏｎｉｓｔ Ｔｅｍｐｏｒａｌ Ｃｌａｓｓｉｆｉｃａｔｉｏｎ）音響モデルのデータを正規化するために使用できる。提案されたアプローチのロバスト性は、音声検索トラフィックから得られたいくつかのインド系言語で実証され、自動音声認識ＡＳＲ性能が大幅に向上した（例えば、最先端のベースラインに比べて相対値で最大１０％）。 These techniques can also be used to improve acoustic and language modeling for bilingual code-switched utterances. We will elaborate on examples of discussing Indian languages, but these techniques can be used in any combination of language and secretary system. The transliteration approach uses data from three types of language models, the traditional n-gram language model, the maximum entropy-based language model, and the LSTM (Long Short Term Memory) language model, and the state-of-the-art CTC (Connectionist Temporal Classification) acoustic model. Can be used for normalization. The robustness of the proposed approach has been demonstrated in several Indian languages derived from voice search traffic, with significant improvements in automated speech recognition ASR performance (eg, relative to state-of-the-art baselines). Up to 10%).
コードスイッチングは、ヒンディー語－英語、ベンガル語－英語、アラビア語－英語、および中国語－英語などの多くのバイリンガル話者によく見られる。外国語（例えば、英語）からの単語が母語（例えば、ヒンディー語）の語彙の一部になったとき、コードスイッチングと、借用語と、および母語の語彙（辞書、ｌｅｘｉｃｏｎ）内での新しい単語の作成との区別は、しばしばあまり明確ではなく、連続したものになる。このような現象は、コードスイッチされた音声の転写（ｔｒａｎｓｃｒｉｐｔｉｏｎ）を困難にし、一貫性のないものにしている。その結果、同じ単語が異なる書記体系で転写されてしまう。このような不整合は、音響モデルと言語モデルの両方において、音響的にも語彙的（ｌｅｘｉｃａｌ）にも類似した文脈を持つ単語同士におけるカウント分布が正しくないことにつながる。 Code-switching is common in many bilingual speakers such as Hindi-English, Bengali-English, Arabic-English, and Chinese-English. When a word from a foreign language (eg English) becomes part of a native (eg Hindi) vocabulary, code switching, loanwords, and new words within the native vocabulary (dictionary, lexicon). The distinction from the creation of is often less clear and continuous. Such a phenomenon makes transcription of code-switched speech difficult and inconsistent. As a result, the same word is transcribed in a different secretary system. Such inconsistencies lead to incorrect count distributions between words that have acoustically and lexically similar contexts in both acoustic and linguistic models.
コードスイッチ音声認識のためにいくつかのアプローチが提案されている。１つのアプローチは、マルチ経路音声認識の使用であり、コードスイッチの領域は、まず、音響情報のみを使用する言語識別方法で識別される。その後、対応する単一言語の音響モデルおよび言語モデルを使用して音声セグメントを認識し、２回目の結合（ｍｅｒｇｉｎｇ）と再採点を行うことがある。このような方法では、複数回の処理が必要になるだけでなく、コードスイッチの検出および言語識別の精度にも依存する。また、コードスイッチの境界にある文脈情報を捉えることができない。他のアプローチは、複数の言語同士間で統一された発音アルファベット（ｐｈｏｎｅｔｉｃ ａｌｐｈａｂｅｔ）、辞書同士（ｌｅｘｉｃｏｎｓ）における発音のバリエーションのモデル化、その他の技術などの制約を必要とするが、一貫して音声認識性能を向上させることはできない。 Several approaches have been proposed for code switch speech recognition. One approach is the use of multi-path speech recognition, where the area of the code switch is first identified by a language identification method that uses only acoustic information. The corresponding monolingual acoustic and linguistic models may then be used to recognize speech segments for a second merging and rescoring. Such a method not only requires multiple processes, but also depends on the accuracy of code switch detection and language identification. Also, the contextual information at the boundaries of the code switch cannot be captured. Other approaches require constraints such as unified phonetic alphabets across multiple languages, modeling of pronunciation variations between dictionaries, and other techniques, but consistently speech. The recognition performance cannot be improved.
本明細書の技術は、自動音声認識ＡＳＲの性能を向上させるための音訳に基づく新しい戦略を提供する。重み付き有限状態変換器（ＷＦＳＴ：ｗｅｉｇｈｔｅｄ ｆｉｎｉｔｅ ｓｔａｔｅ ｔｒａｎｓｄｕｃｅｒｓ）は、音声認識復号に使用されてきた。ここで、文脈依存の単音シーケンスモデル（Ｃ）、発音レキシコン（Ｌ）および言語モデル（Ｇ）を表すＷＦＳＴは、文脈依存単音シーケンスを単語シーケンスに写像する単一の大きな変換器（Ｃ・Ｌ・Ｇ、略してＣＬＧ）（・は白丸の中点であるビュレット記号を示す。すなわち、ＣとＬの間に白ビュレット記号があり、ＬとＧの間に白ビュレット記号がある）に構成することができる。コードスイッチ言語では話者は、ローマ字シーケンスと、母語のスクリプト（ヒンディー語の場合ではデーヴァナーガリー文字など）を混在させる。処理フローも同様のアプローチで、音響モデルと言語モデルの両方で音訳モデルを使用し、コードスイッチを検出する。ＷＦＳＴの一連の最適化によって、精度、レイテンシ（ｌａｔｅｎｃｙ）、メモリ使用量の動作ポイントを達成し、効果的かつ効率的に運用できるように改善した。重要なのは、このシステムは、音訳ベースアプローチを使用して、インド系言語の自動音声認識ＡＳＲ性能を大幅に向上させていることである。 The techniques herein provide a transliteration-based new strategy for improving the performance of automated speech recognition ASRs. Weighted finite state transducers (WFSTs) have been used for speech recognition and decoding. Here, the WFST representing the context-dependent single note sequence model (C), the pronunciation lexicon (L) and the language model (G) is a single large converter (CL) that maps the context-dependent single note sequence into a word sequence. G, abbreviated as CLG) (・ indicates the bullet symbol that is the midpoint of the white circle. That is, there is a white bullet symbol between C and L, and there is a white bullet symbol between L and G). Can be done. In the Code Switch language, the speaker mixes Roman alphabet sequences with native scripts (such as Devanagari characters in Hindi). The processing flow takes a similar approach, using transliteration models in both acoustic and linguistic models to detect code switches. Through a series of optimizations of WFST, the operating points of accuracy, latency, and memory usage have been achieved and improved for effective and efficient operation. Importantly, the system uses a transliteration-based approach to significantly improve the automatic speech recognition ASR performance of Indian languages.
インド系言語の音声認識における課題のいくつかを以下に説明する。新しい指標である音訳最適化単語誤り率（ｔｏＷＥＲ：ｔｒａｎｓｌｉｔｅｒａｔｉｏｎ ｏｐｔｉｍｉｚｅｄ Ｗｏｒｄ Ｅｒｒｏｒ Ｒａｔｅ）を、提案されたアプローチおよびＷＳＦＴ最適化とともに紹介する。また、ヒンディー語を例に、コードスイッチを排除して自動音声認識ＡＳＲシステムを評価することの重要性と、一般的な音訳空間での採点とについても議論する。音響モデルおよび言語モデルの学習に音訳を組み込むことで、大幅な性能向上が実現できる。最後に、提案されたアプローチの一般性を、他のインド系言語の例で検証する。 Some of the challenges in speech recognition for Indian languages are described below. A new index, transliteration optimized word error rate (toWER), is introduced along with the proposed approach and WSFT optimization. We will also discuss the importance of evaluating an automatic speech recognition ASR system by eliminating code switches and scoring in a general transliteration space, using Hindi as an example. By incorporating transliteration into the learning of acoustic and language models, significant performance improvements can be achieved. Finally, the generality of the proposed approach is examined with examples from other Indian languages.
本文書の技術は、任意の２つの言語間のコードスイッチングに適用可能であるが、インド系言語は特に興味深い課題を提示する。インドでは、バイリンガルが一般的であり、母語と英語の間のコードスイッチングが頻繁に行われている。表１は、インド系言語の言語モデルを構築する際に使用した２つのコーパスにおけるラテン文字（Ｌａｔｉｎ ｓｃｒｉｐｔ）の分布を示している。タイピングされた検索クエリを備えているコーパスは、音声（ｓｐｏｋｅｎ）クエリの転写を備えているコーパスよりもはるかに多くのラテン文字を含んでいる。これは、ウェブベースの検索クエリには何の制限もないのに対し、転写の際には転写用の規則（ｔｒａｎｓｃｒｉｐｔｉｏｎ ｃｏｎｖｅｎｔｉｏｎｓ）に従っていたためだと思われる。 While the techniques of this document are applicable to code-switching between any two languages, Indian languages present a particularly interesting challenge. Bilingual is common in India, and code switching between the mother tongue and English is frequent. Table 1 shows the distribution of Latin scripts in the two corpora used to build the language model for Indian languages. A corpus with typed search queries contains far more Latin letters than a corpus with transcriptions of spoken queries. This is probably because web-based search queries have no restrictions, but transcription follows the rules for transcription.
提案されたアプローチの詳細な分析は、アプローチの一般化と他のインド言語への影響を説明しながら、コードスイッチおよび訓練データの数が多い言語の１つであるヒンディー語を用いて以下に議論する。 A detailed analysis of the proposed approach is discussed below using Hindi, one of the languages with the highest number of code switches and training data, explaining the generalization of the approach and its impact on other Indian languages. do.
コードスイッチングは、複数の書記体系に存在する。例えば、ヒンディー語はデーヴァナーガリー文字を使用しているが、ウルドゥー語はアラビア語の書記体系を使用している。ヒンディー語を話す人の多くはバイリンガルであるので、コードスイッチングは日常生活の一部であり、この現象はカジュアルな会話、音声検索クエリ、プレゼンテーションで日常的に出現し、一般的にヒングリッシュと呼ばれる現象につながる。このようなコードスイッチングは、一文の中でも、フレーズレベルでも出現する。表２には、一般的に書き取られる話し発話の例をいくつか示している。第１列には、一般的に使用されている混合書記体系を示している。第２列は、読みやすくするためにラテン文字で書かれた同等のテキストを示し、データに見られるヒンディー語と英語の混在を説明している。ヒンディー語の単語はデーヴァナーガリー文字で転写し、英語由来の単語はラテン文字で転写するように指示されているが、バイリンガルの転写者による転写結果には多くの不一致が見られる。ヒンディー語は、他のインドの言語と同様に、ソーシャルメディアやニュースメディア、ユーザが作成したテキスト、特に名前付きエンティティの言及、ＵＲＬ、数値エンティティ、頭字語などでローマ字化されているので、転写者にとってヒンディー語のデーヴァナーガリー文字への転写はさらに困難になっている。このような不整合は、誤りの定義に対して、および自動音声認識ＡＳＲシステムの評価に使用される指標であるＷＥＲ（Ｗｏｒｄ Ｅｒｒｏｒ Ｒａｔｅ）に対して直接影響する。これらはレンダリング誤りと呼ばれる。 Code-switching exists in multiple secretary systems. For example, Hindi uses the Devanagari alphabet, while Urdu uses the Arabic secretary system. Code-switching is part of everyday life, as many Hindi speakers are bilingual, and this phenomenon appears routinely in casual conversations, voice search queries, and presentations, commonly referred to as Hinglish. It leads to the phenomenon. Such code-switching appears both in one sentence and at the phrase level. Table 2 shows some examples of commonly dictated speech utterances. The first column shows a commonly used mixed secretary system. The second column shows equivalent text written in Latin for readability and describes the mix of Hindi and English found in the data. Hindi words are instructed to be transcribed in Devanagari letters, and English-derived words are instructed to be transcribed in Latin letters, but there are many discrepancies in the transcription results by bilingual transcribers. Hindi, like other Indian languages, is romanized with social and news media, user-created texts, especially mentions of named entities, URLs, numeric entities, acronyms, etc. It is even more difficult for Hindi to be transcribed into Devanagari script. Such inconsistencies directly affect the definition of error and the WER (Word Error Rate), an indicator used to evaluate automated speech recognition ASR systems. These are called rendering errors.
母語（ヒンディー語）と外国語（英語）の使用法が異なるので、コードスイッチングが出現する状況で文脈をモデル化することは困難である。コードスイッチング現象をモデル化して捉えるために、言語的、韻律的、意味的な手がかりを使用するいくつかの方法が提案されているが、自動音声認識ＡＳＲシステムの性能向上に成功した方法は非常に少ない。転写の一貫性の欠如や誤った正規化は、言語モデルや音響モデルのモデル化能力にも影響を与える。このような誤りはモデル化誤り（ｍｏｄｅｌｉｎｇ ｅｒｒｏｒ）と呼ばれている。次に説明する技術は、モデル化誤りおよびレンダリング誤りの両方に対処する統一的なアプローチを提供する。 Due to the different usage of native (Hindi) and foreign (English), it is difficult to model the context in the context of code switching. Several methods have been proposed that use linguistic, prosodic, and semantic cues to model and capture code-switching phenomena, but the methods that have successfully improved the performance of automated speech recognition ASR systems are very high. Few. Inconsistent transcription and incorrect normalization also affect the ability to model language and acoustic models. Such errors are called modeling errors. The techniques described below provide a unified approach to address both modeling and rendering errors.
音訳とは、ある書記体系から別の書記体系にシーケンス（配列）を変換する処理である。インド系言語をラテン文字（ｓｃｒｉｐｔ）に音訳することは、子音、母音、発音記号（ｄｉａｃｒｉｔｉｃｓ）の組み合わせが多く、一意でない写像になるので、特に困難である。例えば、デーヴァナーガリーで可変の可能な綴りを持つ借用語や、可変のローマ字表記を持つヒンディー語など、非標準の綴りが両方のスクリプトに存在することは注目に値する。 Transliteration is the process of converting a sequence from one secretary system to another. Translating Indian languages into Latin letters is particularly difficult because of the many combinations of consonants, vowels, and phonetic symbols, which result in non-unique mappings. It is worth noting that non-standard spelling is present in both scripts, for example, Loanword with variable spelling in Devanagari and Hindi with variable Romanization.
一般的な音訳アプローチは、任意の２つの言語または書記体系のためのアドレスコードスイッチングに適用可能である。音訳は、重み付け有限状態変換器ＷＦＳＴを介して効果的に行うことができる。ソースデータを生成するために、人間の転写者は、ネイティブの筆記スクリプト（この場合はデーヴァナーガリー語）で話し言葉（ｓｐｏｋｅｎ ｕｔｔｅｒａｎｃｅｓ）を転写するように求められたが、例外的に、一般的に使用される特定の英単語はラテン文字で書かれていた。このように、キーボードによる自由なテキスト入力とは異なり、２つの書記体系からの入力の文脈と範囲は、発話の内容に限定されていた。しかし、２つの書記体系同士間には正規（ｃａｎｏｎｉｃａｌ）の音訳が存在せず、転写同士の間にも不整合があるので、多数のモデル化誤りやレンダリング誤りが出現してしまう。この問題を解決するために、音訳変換器Ｔは、３つの変換器Ｉ・Ｐ・Ｏ（・は白丸の中点であるビュレット記号を示す）の組み合わせである。ここでＩは、入力されたＵｎｉｃｏｄｅシンボルを、ペア言語モデルのシンボルに写像する。Ｐは、英語とデーヴァナーガリー語の２つの書記スクリプトのシンボル同士間を写像するビグラムペア（ｂｉｇｒａｍ ｐａｉｒ）言語モデルである。Ｏは、ペア言語モデルのシンボルを、ターゲット出力のデーヴァナーガリー語のシンボルに写像する（図２に図示）。音訳された単語の条件付き確率は、Ｔからの結合確率（ｊｏｉｎｔ ｐｒｏｂａｂｉｌｉｔｙ）を、すべての入力および出力シーケンスに対する限界和（ｍａｒｇｉｎａｌｉｚａｔｉｏｎ ｓｕｍ）で割ることで得られる。この計算は、Ｔにおける最短経路を計算することで効率的に実施される。 The general transliteration approach is applicable to address code switching for any two languages or secretary systems. Transliteration can be done effectively via the weighted finite state transducer WFST. To generate the source data, human transcribers were required to transcribe spoken words (spoken utternances) in native written scripts (in this case Devanagari), but in exceptional cases, in general. The specific English words used were written in Latin letters. Thus, unlike free text input using the keyboard, the context and scope of input from the two secretary systems was limited to the content of the utterance. However, since there is no canonical transliteration between the two secretary systems and there is inconsistency between the transcriptions, a large number of modeling errors and rendering errors appear. To solve this problem, the transliteration transducer T is a combination of three transducers I, P, O (where is the bullet symbol, which is the midpoint of the white circle). Here, I maps the input Unicode symbol to the symbol of the pair language model. P is a bigrampair language model that maps between the symbols of two secretary scripts, English and Devanagari. O maps the symbol of the paired language model to the Devanagari symbol of the target output (shown in FIG. 2). The conditional probability of a transliterated word is obtained by dividing the joint probability from T by the marginalization sum for all input and output sequences. This calculation is efficiently performed by calculating the shortest path at T.
大規模な言語モデルを構築するためのメモリ、速度、およびレイテンシの要件に関する音訳の性能を向上させるために、いくつかの最適化を行うことができる。
いくつかの実装では、音訳変換器は最短経路を計算し、検索空間を効率的にプルーニングすることで、大幅な速度向上が得られた。プルーニング閾値以下のスコアを持つ経路はすべて破棄された。この閾値は、自動音声認識ＡＳＲの性能に影響を与えないように経験的に判定した。特に、最良の経路が最も関心のある経路であるので、「５」のプルーニング重み閾値が良好な動作点として判定された。
Several optimizations can be made to improve transliteration performance with respect to memory, speed, and latency requirements for building large language models.
In some implementations, transliteration transducers calculated the shortest path and efficiently pruned the search space, resulting in significant speed gains. All routes with scores below the pruning threshold were discarded. This threshold was empirically determined so as not to affect the performance of the automatic speech recognition ASR. In particular, since the best route is the route of greatest interest, the pruning weight threshold of "5" was determined to be a good operating point.
いくつかの実装では、削除および挿入の数を減らすためのイプシロン遷移（ε－ｔｒａｎｓｉｔｉｏｎｓ）の使用は、ＷＦＳＴにおけるイプシロンサイクルを減らすときに重要である。イプシロン除去（ｅｐｓｉｌｏｎ ｒｅｍｏｖａｌ）の並列実装を使用して、８つのスレッドを並列に利用した。 In some implementations, the use of epsilon transitions to reduce the number of deletes and inserts is important when reducing the epsilon cycle in WFST. Eight threads were utilized in parallel using a parallel implementation of epsilon remote.
いくつかの実装では、イプシロン除去のための操作はメモリ使用量の劇的な増加を引き起こしたので、大規模な言語モデルでは音訳処理が使用できなくなった。この問題は、イプシロン除去の前における重みベースのプルーニングによって対処されたが、音訳の性能には影響がなかった。 In some implementations, the operation for epsilon removal caused a dramatic increase in memory usage, making transliteration processing unavailable in large language models. This problem was addressed by weight-based pruning prior to epsilon removal, but did not affect transliteration performance.
いくつかの実装では、対訳語（バイリンガル語）の使用率の分布が一様ではないことを考慮して、いくつかの単語が学習データに非常に頻繁に現れた（ｒｅａｐｐｅａｒｅｄ）。これを利用するために、成功した音訳のキャッシュを最大サイズ１００Ｋ要素で導入し、それによって、頻出用語の音訳をテーブルルックアップへと減らすことができた。 In some implementations, some words appeared very often in the training data (reaped), given the uneven distribution of bilingual usage. To take advantage of this, we were able to introduce a cache of successful transliterations with a maximum size of 100K elements, thereby reducing the transliteration of frequent terms into table lookups.
表３には、上記の最適化工程によるスピードアップ／メモリ削減の貢献度を示す。一例として、上記の最適化は、２，８００億語で訓練された言語モデルの全体的な訓練時間を、１６５時間～１２時間に短縮することができる。 Table 3 shows the contribution of speed-up / memory reduction by the above optimization process. As an example, the above optimization can reduce the overall training time of a language model trained with 280 billion words to 165 to 12 hours.
以下の実験は、匿名化され、インド系言語の音声検索トラフィックの代表的な手書きされた発話を集めた訓練セットとテストセットを用いて行われた。訓練セットは、全体のＳＮＲが０～２０ｄｂの間で変化するように、ルームシミュレータを用いて様々な程度のノイズと残響を加えることで人為的に破損させたオリジナルについての複数のコピーで増強されている。すべての言語の信号処理パイプラインは、１０ｍｓの標準フレームレートで、８０次元のログメル（ｌｏｇ ｍｅｌ）－フィルタバンク出力特徴を抽出した。すべての言語の音響モデルは、各層が７６８個のＬＳＴＭセルで構成される５層のＬＳＴＭである。音響モデルは、ＴｅｎｓｏｒＦｌｏｗで、ＣＴＣおよび状態レベル最小ベイズリスクｓＭＢＲ目的関数を最小化する非同期の確率的勾配降下法を用いて学習された。インド系言語のそれぞれについて、実験に使用した学習データの量は、表４に示したとおりである。テストデータは６，０００語から１０，０００語の間で変化した。これらの言語同士では、利用可能なデータに大きなばらつきがあることがわかる。ヒンディー語は、英語とのコードスイッチングが最も多く、訓練トークンの数が最も多い言語の一つであるので、ヒンディー語に関する詳細な分析を行った。ヒンディー語の訓練データセットは、１，０００万の発話を備えている約１万時間の訓練データで構成されている。提案されたアプローチは、ヒンディー語の１０～２０％のデータを持つ他のインド系言語でも検証された。 The following experiments were performed using a training set and a test set that were anonymized and collected the representative handwritten utterances of voice search traffic in Indian languages. The training set is augmented with multiple copies of the artificially corrupted original by adding varying degrees of noise and reverberation using a room simulator so that the overall SNR varies between 0 and 20db. ing. Signal processing pipelines in all languages extracted 80-dimensional logmel-filter bank output features at a standard frame rate of 10 ms. The acoustic model for all languages is a five-layer LSTM, each layer consisting of 768 LSTM cells. The acoustic model was trained in TensorFlow using an asynchronous stochastic gradient descent method that minimizes the CTC and state-level minimum Bayesian risk sMBR objectives. The amount of learning data used in the experiment for each of the Indian languages is shown in Table 4. Test data varied between 6,000 and 10,000 words. It can be seen that there is a great deal of variation in the available data between these languages. Since Hindi is one of the languages with the most code-switching with English and the largest number of training tokens, we conducted a detailed analysis of Hindi. The Hindi training dataset consists of approximately 10,000 hours of training data with 10 million utterances. The proposed approach was also validated in other Indian languages with 10-20% data in Hindi.
表５は、書記体系に関連する誤りを補正した後のＷＥＲ測定値の有意差を示している。提案されているｔｏＷＥＲ指標は、参照語および仮説語の両方を、ネイティブロケールに対応する１つの書記体系に音訳した後に計算される。ラテン文字の割合と、提案された指標との間には相関関係があるので、提案された指標は、これらの言語におけるコードスイッチングの程度を示す良い指標になっていることがわかる。マラヤーラム語、テルグ語、マラーティー語、およびウルドゥー語などの言語では、ヒンディー語やベンガル語などの言語に比べてコードスイッチングの量が少なく、ｔｏＷＥＲはそれを反映している。このように、音訳は、不整合に起因する書記体系の誤りを修正する手段として、またモデル化誤りをレンダリング誤りから分離する手段として利用することができる。音訳された採点は、コードスイッチングによってもたらされる曖昧さを軽減し、曖昧さや転記誤りを滑らかにすることができる。そのため、任意のアルゴリズムの改良を評価するには、提案されているｔｏＷＥＲの方が優れた指標になる。 Table 5 shows the significant differences in WER measurements after correcting for errors related to the secretary system. The proposed toWER index is calculated after transliterating both the reference and hypothetical words into one secretary system corresponding to the native locale. Since there is a correlation between the percentage of Latin letters and the proposed indicators, it can be seen that the proposed indicators are good indicators of the degree of code switching in these languages. Languages such as Malayalam, Telugu, Marathi, and Urdu have less code-switching than languages such as Hindi and Bengali, and toWER reflects that. In this way, transliteration can be used as a means of correcting errors in the writing system due to inconsistencies and as a means of separating modeling errors from rendering errors. Transliterated scoring can reduce the ambiguity caused by code-switching and smooth out ambiguities and transcription errors. Therefore, the proposed toWER is a better indicator for evaluating the improvement of any algorithm.
音訳に最適化された採点の影響に触発されて、音訳を用いた言語モデルの訓練データの正規化についても検討した。まず、音訳された言語モデル（ＬＭ）をヒンディー語で訓練した結果について、説明する。ヒンディー語の言語モデルを構築するために使用された多様なコーパスのテキストは、まずラテン文字を排除するためにすべて音訳された。その後、正規化されたデーヴァナーガリー文字を使用して、第１経路では５グラムの言語モデルＬＭを訓練し、第２経路ではクラスベースの最大エントロピーベースモデルを訓練した。表６は、これらのモデルを使用して、音声検索クエリおよびディクテーションデータを備えている２つの異なるテストセットを復号したときに得られた結果を示している。言語モデルへの入力として様々な書記体系を比較するために、デーヴァナーガリー文字（スクリプト）のみを備えているすべての発話を用いて構築された言語モデルＬＭとして「デーヴァナーガリー文字のみのデータに基づく言語モデルＬＭ」を定義した。デーヴァナーガリー文字とラテン文字のバイリンガルテキストを備えている発話は、言語モデルの構築に使用されなかった。その結果、予想されたように文脈に基づいたモデル化ができず、データ量が少なくなり、訓練セットとテストセットの分布にミスマッチが生じた。この言語モデルＬＭによって生成された仮説を音訳して採点すると、参照転写とのミスマッチが修正される（第２行）。両方の書記体系のデータを保持することで、コードスイッチによる文脈を確実に維持することができるが、同じ単語がデーヴァナーガリー語とラテン語の両方に現れるなど、セクション２で述べたような課題がすべて出現する。ラテン語からの追加データをすべてそのまま言語モデルＬＭに含めると、参照と仮説の間のミスマッチがさらに大きくなり、従来のＷＥＲが人為的に高くなってしまう（第３行）。ｔｏＷＥＲ指標は、実際の誤り率を反映している（第４行）。すべてのデータをデーヴァナーガリー語に音訳して言語モデルＬＭを再訓練すると、音声検索とディクテーションのテストセットで大きな利得が得られることがわかる（第５行）。このように、すべての訓練データをデーヴァナーガリー語に音訳して言語モデルＬＭを構築し、一貫したテキスト正規化を導入することで、２つのテストセットにおいてＷＥＲを３～８％相対的に向上させることができた。
Inspired by the effects of transliteration-optimized scoring, we also examined normalization of training data for language models using transliteration. First, the results of training the transliterated language model (LM) in Hindi will be described. The texts of the various corpora used to build the Hindi language model were all transliterated, first to eliminate Latin letters. Then, using normalized Devanagari characters, the first pathway trained a 5-gram language model LM and the second pathway trained a class-based maximum entropy-based model. Table 6 shows the results obtained when using these models to decode two different test sets with voice search queries and dictation data. In order to compare various writing systems as input to the language model, as a language model LM constructed using all utterances that have only Devanagari characters (scripts), "To Devanagari character-only data" Based on the language model LM "was defined. Utterances with bilingual text in Devanagari and Latin letters were not used to build the language model. As a result, context-based modeling was not possible as expected, the amount of data was small, and there was a mismatch in the distribution of training sets and test sets. Transliteration and scoring of the hypothesis generated by this language model LM corrects the mismatch with reference transcription (line 2). Retaining data from both secretary systems ensures that the context of the code switch is maintained, but there are challenges such as those mentioned in
また、音訳がＬＳＴＭ（Ｌｏｎｇ Ｓｈｏｒｔ Ｔｅｒｍ Ｍｅｍｏｒｙ）ニューラルネットワークモデルに与える影響についても調査した。表７に見られるように、音訳されたテキストでモデルを訓練すると、音声検索タスクでは最大エントロピーベースの言語モデルＬＭで見られたものと同様の性能の向上が得られるが、ディクテーションタスクではそれほどではない。驚くべきことではないが、音訳ベースの正規化を訓練や採点に使用することで、モデル化誤りをレンダリング誤りから分離し、モデルの性能を正確に評価することができるという仮説が実証された。表６および表７に示す音声検索タスクでは、従来のＷＥＲを使用した場合にはＬＳＴＭ言語モデルＬＭと最大エントロピーベースの言語モデルＬＭの性能は非常によく似ているが（３２．０対３１．５）、ｔｏＷＥＲを使用した場合には最大エントロピーベースの言語モデルＬＭがＬＳＴＭよりもはるかに優れていることがわかる（２０．６対２２．３）。このような利得の重要性は、実際に、以下に説明するサイドバイサイドの比較研究において、人間の評価者によって測定することができる。 We also investigated the effect of transliteration on LSTM (Long Short Term Memory) neural network models. As can be seen in Table 7, training the model with transliterated text yields similar performance improvements in the voice search task as seen in the maximum entropy-based language model LM, but not so much in the dictation task. do not have. Not surprisingly, the hypothesis that transliteration-based normalization can be used for training and scoring to separate modeling errors from rendering errors and accurately assess model performance has been demonstrated. In the voice search tasks shown in Tables 6 and 7, the performance of the LSTM language model LM and the maximum entropy-based language model LM are very similar when using conventional WER (32.0 vs. 31. 5) It can be seen that the maximum entropy-based language model LM is much better than the LSTM when using toWER (20.6 vs. 22.3). The importance of such gains can actually be measured by a human evaluator in a side-by-side comparative study described below.
音訳は、音響モデル化の精度を向上させることもできる。ヒンディー語の音響モデル（ＡＭ）の訓練において、各発話の音訳を用いた様々な実験が行われた。ラテン語で書かれた音響モデルＡＭ訓練データのすべての単語は、まずデーヴァナーガリー文字に音訳され、発音はヒンディー語の単音アルファベットで導き出された。ＣＴＣ基準を用いて収束するようにモデルを訓練した後、音訳された音響モデルＡＭは、両方の書記体系で学習したモデルよりもわずかながら性能が向上した（表８参照）。状態レベル最小ベイズリスクｓＭＢＲの訓練に必要な分子と分母の格子（ｌａｔｔｉｃｅｓ）はデーヴァナーガリー文字で一貫してレンダリングされるので、状態レベル最小ベイズリスクｓＭＢＲの訓練による改善はさらに顕著になると予想される。 Transliteration can also improve the accuracy of acoustic modeling. In the training of the Hindi acoustic model (AM), various experiments were conducted using transliterations of each utterance. All words in the acoustic model AM training data written in Latin were first transliterated into Devanagari letters, and the pronunciation was derived in the Hindi monophonic alphabet. After training the model to converge using CTC criteria, the transliterated acoustic model AM performed slightly better than the model trained in both secretary systems (see Table 8). Since the numerator and denominator lattices required for training state-level minimum Bayes risk sMBR are consistently rendered in Devanagari characters, the improvement with state-level minimum Bayes risk sMBR training is expected to be even more pronounced. To.
次に、以下に示す表１０は、他のいくつかのインド系言語に対する提案されたアプローチの影響を示している。マラヤーラム語とタミル語を除くすべての言語で、有意で一貫した利得が得られている。これは、訓練コーパスに存在するラテン語の量に起因すると考えられる。この２つの言語では、図２から、音声によるクエリを備えている音声検索コーパスにはほとんどラテン語が含まれていないのに対し、ウェブベースのクエリを備えているコーパスには多くのラテン語が含まれていることがわかる。しかし、ウェブベースのコーパスは、このタスクのために非常に低い補間重みを受けたので、ＷＥＲにはほとんど影響を与えなかった。同様の傾向は、ディクテーションタスクにおける音訳された言語モデルＬＭにも見られる（表１０を参照。ｔｏＷＥＲの相対的な減少は最大１０％である）。 Next, Table 10 below shows the impact of the proposed approach to some other Indian languages. Significant and consistent gains have been obtained in all languages except Malayalam and Tamil. This is believed to be due to the amount of Latin present in the training corpus. In these two languages, from Figure 2, the voice search corpus with voice queries contains very little Latin, while the corpus with web-based queries contains many Latin. You can see that. However, the web-based corpus received very low interpolation weights for this task, so it had little effect on WER. A similar trend is seen in the transliterated language model LM in dictation tasks (see Table 10; relative reduction in toWER is up to 10%).
図３は、ＷＥＲ値およびｔｏＷＥＲ値を示すとともに、データ中のラテン語の割合として測定されたコードスイッチングの割合との相関関係を示すチャートである。
音訳は、認識結果にプラスとマイナスの変化をもたらす可能性がある。表１１は、従来のＷＥＲ指標が誤りを人為的に膨らませるいくつかの例を示している。第１例では、発話「Ｓａｔｔａ Ｍａｔｋａ」がラテン文字で転写されているのに対し、自動音声認識ＡＳＲシステムはデーヴァナーガリー語で仮説を立てたので、２つの置換誤りとしてカウントされている。しかし、ｔｏＷＥＲは誤り率を計算する前にデーヴァナーガリー語に音訳しているので、正しく提供しており誤りは出現しない。同様のことが、第２例のＤｉｓｃｏｖｅｒｙ（発見）という単語にも見られる。
FIG. 3 is a chart showing the WER value and the toWER value, as well as the correlation with the rate of code switching measured as the rate of Latin in the data.
Transliteration can have positive and negative changes in recognition results. Table 11 shows some examples of traditional WER indicators artificially inflating errors. In the first example, the utterance "Satta Matka" is transcribed in Latin letters, whereas the automatic speech recognition ASR system hypothesized in Devanagari, so it is counted as two replacement errors. However, since toWER transliterates it into Devanagari before calculating the error rate, it is provided correctly and no error appears. The same can be seen in the second example, the word Discovery.
しかし、コードスイッチングに関するすべての問題が、音訳だけで修正できるわけではない。そのようないくつかの例が表１２で強調されており、以前には存在しなかった誤りが音訳処理によって導入される。第１例では、ラテン語の発話は「Ｔｉｇｅｒ ｚｉｎｄａ ｈａｉ ｆｕｌｌ ｍｏｖｉｅ」と読まれている。この参照（リファレンス）には、最初の３つの単語がラテン語で記載されており、最後の２つの単語がデーヴァナーガリー語で記載されていた。そのため、自動音声認識ＡＳＲの仮説（ｈｙｐｏｔｈｅｓｉｓ）はデーヴァナーガリー語になった。参照語と仮説の両方を共通のデーヴァナーガリー書記体系に音訳した結果、「Ｚｉｎｄａ」対「Ｊｉｎｄａ」という誤りが導入された。同様に、第２例では、参照語はＪａｍｕｎａに音訳されていたが、仮説ではＪｕｍｎａになっていた。これは音訳処理の際に、どちらの形でもよいという曖昧さがあるからである。第３例では、より古典的な誤りが出現している。この発話は、ラテン語で「ＢＡ ｆｉｒｓｔ ｙｅａｒ ｔｉｍｅ ｔａｂｌｅ」と読む。この例では、転写者は一貫してデーヴァナーガリー語のみでテキストを作成していたことに注意されたい。自動音声認識ＡＳＲシステムは発話を正しく仮定したが、書記体系が組み合わされていたので、ＷＥＲ指標では３つの置換誤りがカウントされた。仮説を音訳する過程で、ＢＡがデーヴァナーガリー語のＢａ（単語「ｂａｒ」の「ｂａｈ」として発音される）に写像され、ＢＡという頭字語の最後の「Ａ」が失われた。これによって、ｔｏＷＥＲ指標で置換誤りが出現する。以上のような状況にもかかわらず、全体的に見ると、提案された基準は、従来のＷＥＲよりもシステムの性能を大幅に正確に反映している。 However, not all code-switching problems can be fixed by transliteration alone. Some such examples are highlighted in Table 12, where previously non-existent errors are introduced by transliteration. In the first example, the Latin utterance is read as "Tiger zinda hi full movie". In this reference, the first three words were written in Latin and the last two words were written in Devanagari. Therefore, the hypothesis of automatic speech recognition ASR has become Devanagari. As a result of transliterating both the reference word and the hypothesis into a common Devanagari secretary system, the error "Zinda" vs. "Jinda" was introduced. Similarly, in the second example, the reference word was transliterated to Jamuna, but the hypothesis was Juna. This is because there is an ambiguity that either form may be used during transliteration processing. In the third example, a more classical error has emerged. This utterance is read in Latin as "BA first year time table". Note that in this example, the transcriber consistently wrote the text in Devanagari only. The automatic speech recognition ASR system correctly assumed utterances, but because of the combination of the secretary system, the WER index counted three substitution errors. In the process of translating the hypothesis, BA was mapped to the Devanagari word Ba (pronounced as "bah" in the word "bar"), and the final "A" in the acronym BA was lost. As a result, a substitution error appears in the toWER index. Despite these circumstances, overall, the proposed criteria reflect the performance of the system much more accurately than traditional WER.
提案されたアプローチの追加評価として、いくつかの「サイド－バイ－サイド」（ＳｘＳ）実験が実施された。この実験では、匿名化された各テスト発話は、２つの自動音声認識ＡＳＲシステム（ベースラインｖｓ．音訳された言語モデルＬＭ）によって自動的に転写される。２つの仮説が異なる場合には、人間の評価者に提示される。ＳｘＳ実験では、些細な語彙の違いではなく、意味的な変化を正確に測定することができる。我々は、仮説が単一の自動音声認識ＡＳＲシステムによって生成されるが、評価者は生の仮説を単一のネイティブ書記体系（この例ではデーヴァンガリー）だけでなく複数の書記体系で見るというシナリオでも、ＳｘＳ実験を行う。表１３では、５００の異なる発話に関する以下の結果が示されている。（１）変化：２つのシステムが異なる転写物（ｔｒａｎｓｃｒｉｐｔｓ）を生成したトラフィックの割合。（２）勝ち／負け：実験システム対ベースラインの勝敗の比率。ｐ値が＜５％未満の場合は、統計的に有意であると考えられる。 Several "side-by-side" (SxS) experiments were performed as an additional assessment of the proposed approach. In this experiment, each anonymized test utterance is automatically transcribed by two automatic speech recognition ASR systems (baseline vs. transliterated language model LM). If the two hypotheses are different, they are presented to a human evaluator. In the SxS experiment, it is possible to accurately measure semantic changes rather than trivial vocabulary differences. We have a scenario in which the hypothesis is generated by a single automatic speech recognition ASR system, but the evaluator sees the raw hypothesis not only in a single native writing system (Davungary in this example) but also in multiple writing systems. But I will do an SxS experiment. Table 13 shows the following results for 500 different utterances. (1) Change: Percentage of traffic in which the two systems produced different transcripts. (2) Win / Loss: The ratio of the experimental system to the baseline win / loss. If the p-value is less than <5%, it is considered statistically significant.
表１３から、人間の評価者（ｒａｔｅｒｓ）は、混合書記体系に基づく仮説と比較して、音訳された仮説に中立的な評価を与えていることが明らかである。比較される２つの体系の意味内容（ｓｅｍａｎｔｉｃ ｃｏｎｔｅｎｔ）は変わっていないので、これは予想外ではない。しかし、ｔｏＷＥＲはレンダリング誤りを滑らかにし、より良い視点を提供する。同様に、第２行は２つの言語モデルＬＭ、すなわちベースライン体系（表６の第２行、ｔｏＷＥＲは２０．６％）と、音訳された言語モデルＬＭを使った体系（表６の第８行、ｔｏＷＥＲは１７．２％）とを比較している。音訳された言語モデルＬＭを使った体系（実験的な体系）では、負けよりも勝ちが圧倒的に多い。 From Table 13, it is clear that the human evaluators give a neutral rating to the transliterated hypothesis as compared to the hypothesis based on the mixed secretary system. This is not unexpected, as the semantic content of the two systems being compared has not changed. However, toWER smoothes out rendering errors and provides a better perspective. Similarly, the second line is a system using two language model LMs, that is, a baseline system (2nd line in Table 6, toWER is 20.6%) and a transliterated language model LM (8th line in Table 6). The line, toWER, is compared with 17.2%). In the system using the transliterated language model LM (experimental system), there are overwhelmingly more wins than losses.
すべてのインド系言語において、ラテン語で書かれたテキストの量と、音声検索タスクにおいて音訳された言語モデルＬＭｓおよびｔｏＷＥＲ指標で得られた利得（ｇａｉｎ）との間に相関関係が見られた。しかし、カンナダ語のディクテーションタスクでは、性能の低下が見られた。これらの誤りの原因をよりよく理解するために、カンナダ語の音訳空間で書記素誤り率を生成し、音訳された言語モデルＬＭで大きな利得を示したベンガル語などの言語と比較した。 In all Indian languages, there was a correlation between the amount of text written in Latin and the gains obtained from the transliterated language model LMs and toWER indicators in the voice search task. However, the Kannada dictation task showed a performance degradation. To better understand the causes of these errors, we generated grapheme error rates in the Kannada transliteration space and compared them to languages such as Bengali, which showed significant gains in the transliterated language model LM.
興味深いことに、表１４から、ベースラインと、音訳された言語モデルＬＭとの間の削除誤りの数は、ベンガル語（一定に留まる）よりもカンナダ語（相対的に３０％増加）の方がはるかに多いことがわかる。置換誤りも、カンナダ語では約３％相対的に増加しているが、ベンガル語では６．４％相対的に減少している。しかし、ベンガル語の書記素誤り率は、音訳された言語モデルＬＭによって１８．７％から１７．６％に減少したのに対し、カンナダ語は１０．２７から１０．２３にしか減少しなかった。仮説を詳細に観察すると、誤りの多くは、２つの単語が１つに統合されたときに音訳によってもたらされたものであるか、統合された形と分割された形との両方が正しいと考えられる言語の曖昧さの結果であることがわかる。これらの要因が重なって、結果的にｔｏＷＥＲが低下している。書記素誤り率の改善は、音訳された言語モデルＬＭが依然として有用であることを示す良好な指標である。音訳処理に起因する誤りの一部は、マッチしたデータでモデルを訓練することで修正できる可能性が高い。 Interestingly, from Table 14, the number of deletion errors between the baseline and the transliterated language model LM is higher in Kannada (relatively 30% increase) than in Bengali (remains constant). It turns out that there are far more. Substitution errors also increased relatively by about 3% in Kannada, but decreased by 6.4% in Bengali. However, the grapheme error rate in Bengali was reduced from 18.7% to 17.6% by the transliterated language model LM, while that in Kannada was only reduced from 10.27 to 10.23. .. A closer look at the hypothesis reveals that many of the mistakes were brought about by transliteration when the two words were merged into one, or that both the merged and split forms were correct. It turns out to be the result of possible language ambiguity. As a result, toWER is lowered due to the combination of these factors. Improving the grapheme error rate is a good indicator that the transliterated language model LM is still useful. Some of the errors caused by transliteration processing are likely to be corrected by training the model with matched data.
要するに、従来の単語誤り率（ＷＥＲ）指標は、転写の曖昧さ、スペルミス、２つの異なる書記体系からの単語の借用などの理由によって、コード混合言語の性能を測定するには十分ではないということである。モデル化誤りは、レンダリング誤りを平滑化する提案された音訳ベースの「ｔｏＷＥＲ」指標を用いて正確に測定することができる。言語モデル化と音響モデル化の両方について、訓練用のトランスクリプトを一貫して正規化することで、音声検索とディクテーションのトラフィックを使用して、複数のコードスイッチされたインド系言語同士の間で、相対的に最大１０％の大きな利得を得ることができる。訓練データを一貫して正規化し、モデルのロバスト性と精度を正確に測定するための音訳に基づく単純なアプローチによって、大きな利得を得ることができる。 In short, traditional word error rate (WER) indicators are not sufficient to measure the performance of mixed code languages due to reasons such as transcriptional ambiguity, misspelling, and word borrowing from two different writing systems. Is. Modeling errors can be accurately measured using the proposed transliteration-based "toWER" index that smoothes rendering errors. By consistently normalizing training transcripts for both language and acoustic modeling, voice search and dictation traffic is used between multiple code-switched Indian languages. , A relatively large gain of up to 10% can be obtained. Great gains can be gained by a simple transliteration-based approach to consistently normalize training data and accurately measure the robustness and accuracy of the model.
本発明の実施形態および本明細書に記載されているすべての機能動作は、デジタル電子回路、または本明細書に開示されている構造およびその構造的等価物を含むコンピュータソフトウェア、ファームウェア、もしくはハードウェア、またはそれらの１つまたは複数の組み合わせで実装されてもよい。本発明の実施形態は、１つまたは複数のコンピュータプログラム製品として、すなわち、データ処理装置によって実行するために、またはデータ処理装置の動作を制御するために、コンピュータ可読媒体にコード化されたコンピュータプログラム命令の１つまたは複数のモジュールとして実装されてもよい。コンピュータ可読媒体は、非一時的なコンピュータ可読記憶媒体、機械可読記憶装置、機械可読記憶基板、記憶装置、機械可読伝搬信号を作用させる物質の組成物、またはそれらの１つまたは複数の組み合わせであってもよい。コンピュータ可読媒体は、電気信号、光学信号、電磁信号などの一時的な媒体であってもよい。「データ処理装置」という用語は、データを処理するためのすべての装置、デバイス、および機械を包含し、例として、プログラマブルプロセッサ、コンピュータ、または複数のプロセッサもしくはコンピュータを含む。装置は、ハードウェアに加えて、対象になっている当該コンピュータプログラムの実行環境を構築するコード、例えば、プロセッサのファームウェアを構成するコード、プロトコルスタック、データベース管理システム、オペレーティングシステム、またはそれらの１つまたは複数の組み合わせを含むことができる。伝播信号とは、人工的に生成された信号であり、例えば、機械で生成された電気信号、光信号、電磁信号であり、適切な受信装置に送信するために情報をエンコードするために生成される。 Embodiments of the invention and all functional operations described herein are computer software, firmware, or hardware that includes digital electronic circuits, or the structures and structural equivalents thereof disclosed herein. , Or a combination thereof. An embodiment of the invention is a computer program encoded in a computer-readable medium as one or more computer program products, i.e., to be executed by a data processing device or to control the operation of the data processing device. It may be implemented as one or more modules of instructions. A computer-readable medium is a non-temporary computer-readable storage medium, a machine-readable storage device, a machine-readable storage board, a storage device, a composition of substances acting on a machine-readable propagation signal, or a combination thereof. You may. The computer-readable medium may be a temporary medium such as an electrical signal, an optical signal, or an electromagnetic signal. The term "data processor" includes all devices, devices, and machines for processing data, including, by example, programmable processors, computers, or multiple processors or computers. In addition to the hardware, the device is the code that builds the execution environment of the target computer program, such as the code that makes up the firmware of the processor, the protocol stack, the database management system, the operating system, or one of them. Or it can include multiple combinations. Propagation signals are artificially generated signals, such as machine-generated electrical, optical, and electromagnetic signals that are generated to encode information for transmission to the appropriate receiver. To.
コンピュータプログラム（プログラム、ソフトウェア、ソフトウェアアプリケーション、スクリプト、コードとも呼ばれる）は、コンパイルされた言語やインタープリタ型言語など、どのような形式のプログラミング言語で書かれていてもよく、また、スタンドアローンのプログラムとして、あるいはコンピューティング環境での使用に適したモジュール、構成要素、サブルーチン、その他のユニットとしてなど、どのような形式で展開されていてもよい。コンピュータプログラムは、必ずしもファイルシステム上のファイルに対応するものではない。プログラムは、他のプログラムやデータを格納するファイルの一部（例えば、マークアップ言語の文書に格納された１つまたは複数のスクリプト）、当該プログラム専用の単一のファイル、または複数の調整されたファイル（例えば、１つまたは複数のモジュール、サブプログラム、またはコードの一部を格納するファイル）に格納されていてもよい。コンピュータプログラムは、１台のコンピュータ上で実行されるように展開されてもよいし、１つのサイトに配置された複数のコンピュータ上で実行されてもよいし、複数のサイトに分散して通信ネットワークで相互に接続されていてもよい。 Computer programs (also known as programs, software, software applications, scripts, and code) can be written in any form of programming language, including compiled and interpreted languages, and as stand-alone programs. , Or as a module, component, subroutine, or other unit suitable for use in a computing environment. Computer programs do not necessarily correspond to files on the file system. A program may be part of a file that stores other programs or data (eg, one or more scripts stored in a markup language document), a single file dedicated to that program, or multiple tailors. It may be stored in a file (eg, a file that contains one or more modules, subprograms, or parts of code). The computer program may be deployed to be executed on one computer, may be executed on multiple computers located at one site, or may be distributed to multiple sites and communicate networks. May be interconnected with.
本明細書に記載されている処理および論理フローは、入力データを操作して出力を生成することで機能を実行するために、１つまたは複数のコンピュータプログラムを実行する１つまたは複数のプログラマブルプロセッサによって実行されてもよい。また、処理および論理フローは、特別な目的の論理回路、例えば、ＦＰＧＡ（ｆｉｅｌｄ ｐｒｏｇｒａｍｍａｂｌｅ ｇａｔｅ ａｒｒａｙ）またはＡＳＩＣ（ａｐｐｌｉｃａｔｉｏｎ ｓｐｅｃｉｆｉｃ ｉｎｔｅｇｒａｔｅｄ ｃｉｒｃｕｉｔ）によって実行されてもよく、また、装置は、それとして実装されてもよい。 The processing and logical flows described herein are one or more programmable processors that execute one or more computer programs to perform functions by manipulating input data to produce output. May be performed by. Further, the processing and the logical flow may be executed by a logic circuit of a special purpose, for example, an FPGA (field programgable gate array) or an ASIC (application specific integrated circuit), and the device may be implemented as such. good.
コンピュータプログラムの実行に適したプロセッサには、例として、汎用および特殊目的のマイクロプロセッサの両方、および任意の種類のデジタルコンピュータの任意の１つまたは複数のプロセッサが含まれる。一般に、プロセッサは、読み取り専用メモリまたはランダムアクセスメモリ、あるいはその両方から命令やデータを受け取る。コンピュータの本質的な要素は、命令を実行するためのプロセッサと、命令やデータを格納するための１つまたは複数のメモリデバイスである。一般に、コンピュータは、データを格納するための１つまたは複数の大容量記憶装置、例えば、磁気ディスク、光磁気ディスク、または光ディスクを備えているか、またはその両方からデータを受け取るか、またはデータを転送するように動作可能に結合される。しかし、コンピュータはそのようなデバイスを持っている必要はない。さらに、コンピュータは、タブレットコンピュータ、携帯電話、ＰＤＡ（Ｐｅｒｓｏｎａｌ Ｄｉｇｉｔａｌ Ａｓｓｉｓｔａｎｔ）、携帯オーディオプレーヤー、ＧＰＳ（Ｇｌｏｂａｌ Ｐｏｓｉｔｉｏｎｉｎｇ Ｓｙｓｔｅｍ）受信機など、他の機器に組み込まれている場合もある。コンピュータプログラムの命令やデータを格納するのに適したコンピュータ可読媒体には、あらゆる形態の不揮発性メモリ、媒体、およびメモリデバイスが含まれ、例として、半導体メモリデバイス、例えばＥＰＲＯＭ、ＥＥＰＲＯＭ、およびフラッシュメモリデバイス、磁気ディスク、例えば内蔵ハードディスクまたはリムーバブルディスク、光磁気ディスク、およびＣＤＲＯＭおよびＤＶＤ－ＲＯＭディスクなどが挙げられる。プロセッサとメモリは、特別な目的の論理回路によって補完されるか、またはそれに組み込まれてもよい。 Suitable processors for running computer programs include, for example, both general purpose and special purpose microprocessors, and any one or more processors of any type of digital computer. In general, processors receive instructions and data from read-only memory and / or random access memory. The essential elements of a computer are a processor for executing instructions and one or more memory devices for storing instructions and data. In general, a computer is equipped with one or more mass storage devices for storing data, such as magnetic disks, magneto-optical disks, or optical disks, or both, to receive data or transfer data. It is operably combined so that it can be operated. However, the computer does not have to have such a device. Further, the computer may be incorporated in other devices such as a tablet computer, a mobile phone, a PDA (Personal Digital Assistant), a portable audio player, and a GPS (Global Positioning System) receiver. Computer-readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media, and memory devices, such as semiconductor memory devices such as EPROM, EEPROM, and flash memory. Devices, magnetic disks, such as internal hard disks or removable disks, magneto-optical disks, and CDROMs and DVD-ROM disks. Processors and memory may be complemented or incorporated into logic circuits of special purpose.
ユーザとのインタラクションを提供するために、本発明の実施形態は、ユーザに情報を表示するためのディスプレイデバイス、例えばＣＲＴ（ｃａｔｈｏｄｅ ｒａｙ ｔｕｂｅ）またはＬＣＤ（ｌｉｑｕｉｄ ｃｒｙｓｔａｌ ｄｉｓｐｌａｙ）モニタと、ユーザがコンピュータに入力を提供することができるキーボードおよびポインティングデバイス、例えばマウスまたはトラックボールとを有するコンピュータ上で実施されてもよい。他の種類のデバイスを使用して、ユーザとの対話を行うこともできる。例えば、ユーザに提供されるフィードバックは、視覚的フィードバック、聴覚的フィードバック、触覚的フィードバックなど、どのような形態の感覚的フィードバックであってもよく、また、ユーザからの入力は、音響、音声、触覚など、どのような形態で受け取られてもよい。 In order to provide interaction with the user, embodiments of the present invention include a display device for displaying information to the user, such as a CRT (catagraphary tabe) or LCD (liquid crystal display) monitor, which the user inputs into a computer. It may be carried out on a computer having a keyboard and a pointing device capable of providing, for example, a mouse or a trackball. Other types of devices can also be used to interact with the user. For example, the feedback provided to the user may be any form of sensory feedback, such as visual feedback, auditory feedback, tactile feedback, and user input may be acoustic, audio, or tactile. It may be received in any form.
本発明の実施形態は、バックエンド構成要素、例えば、データサーバ、またはミドルウェア構成要素、例えば、アプリケーションサーバ、またはフロントエンド構成要素、例えば、ユーザが本発明の実施形態と対話することができるグラフィカルユーザインタフェースまたはウェブブラウザを有するクライアントコンピュータ、または１つまたは複数のそのようなバックエンド、ミドルウェア、またはフロントエンド構成要素の任意の組み合わせを備えているコンピューティングシステムに実装されてもよい。システムの構成要素は、デジタルデータ通信の任意の形式または媒体、例えば通信ネットワークによって相互に接続されてもよい。通信ネットワークの例には、ローカルエリアネットワーク（「ＬＡＮ」）およびワイドエリアネットワーク（「ＷＡＮ」）、例えば、インターネットが含まれる。 An embodiment of the invention is a backend component, eg, a data server, or a middleware component, eg, an application server, or a frontend component, eg, a graphical user through which a user can interact with an embodiment of the invention. It may be implemented in a client computer with an interface or web browser, or a computing system with any combination of one or more such backends, middleware, or frontend components. The components of the system may be interconnected by any form or medium of digital data communication, such as a communication network. Examples of communication networks include local area networks (“LAN”) and wide area networks (“WAN”), such as the Internet.
コンピューティングシステムは、クライアントおよびサーバを備えていることができる。クライアントとサーバは、一般的に互いに離れており、通常、通信ネットワークを介して相互に作用する。クライアントとサーバの関係は、それぞれのコンピュータ上で実行されるコンピュータプログラムが、互いにクライアントとサーバの関係を有することで生じる。 The computing system can include clients and servers. Clients and servers are generally separated from each other and usually interact over a communication network. The client-server relationship arises when the computer programs running on each computer have a client-server relationship with each other.
本明細書には多くの具体的な内容が含まれているが、これらは本発明の範囲や請求できる内容の制限として解釈されるべきではなく、本発明の特定の実施形態に特有の特徴の説明として解釈されるべきである。別々の実施形態の文脈（ｃｏｎｔｅｘｔ）で本明細書に記載されている特定の特徴は、単一の実施形態において組み合わせて実施することもできる。逆に、単一の実施形態の文脈で説明されている様々な特徴は、複数の実施形態において別々に、または任意の適切なサブコンビネーションで実施することもできる。さらに、特徴は、特定の組み合わせで作用するものとして上記で説明され、さらに最初にそのように主張されることがあるが、主張された組み合わせからの１つまたは複数の特徴は、場合によっては組み合わせから抜粋され、主張された組み合わせは、サブコンビネーションまたはサブコンビネーションのバリエーションに向けられてもよい。 Although this specification contains many specific contents, these should not be construed as a limitation of the scope of the present invention or claims, and features specific to a particular embodiment of the present invention. Should be interpreted as an explanation. The particular features described herein in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, the various features described in the context of a single embodiment can also be implemented separately in multiple embodiments or in any suitable subcombination. Further, features are described above as acting in a particular combination, and may be claimed as such first, but one or more features from the claimed combination may be a combination. The combinations excerpted from and claimed from may be directed to sub-combinations or variations of sub-combinations.
同様に、図面には操作が特定の順序で描かれているが、これは、望ましい結果を得るために、そのような操作が示された特定の順序で実行されること、または連続した順序で実行されること、または図示された操作がすべて実行されることを要求していると理解すべきではない。特定の状況下では、マルチタスクや並列処理が有利な場合がある。さらに、上述した実施形態における様々なシステム構成要素の分離は、すべての実施形態においてそのような分離を必要とすると理解されるべきではなく、上述したプログラム構成要素およびシステムは、一般に、単一のソフトウェア製品に一緒に統合されてもよいし、複数のソフトウェア製品にパッケージ化されてもよいと理解されるべきである。 Similarly, the drawings show the operations in a specific order, which means that such operations are performed in the specific order shown, or in a contiguous order, in order to obtain the desired results. It should not be understood that it requires that it be performed or that all the operations shown are performed. Under certain circumstances, multitasking and parallel processing may be advantageous. Moreover, the separation of the various system components in the embodiments described above should not be understood as requiring such separation in all embodiments, and the program components and systems described above are generally single. It should be understood that they may be integrated together in a software product or packaged in multiple software products.
ＨＴＭＬファイルが言及されている各例では、他のファイルタイプまたはフォーマットが代用されてもよい。例えば、ＨＴＭＬファイルは、ＸＭＬ、ＪＳＯＮ、プレーンテキスト、または他のタイプのファイルで置き換えられてもよい。さらに、テーブルまたはハッシュテーブルが言及されている場合、他のデータ構造（スプレッドシート、リレーショナルデータベース、または構造化ファイルなど）が使用されてもよい。 In each example where the HTML file is mentioned, other file types or formats may be substituted. For example, HTML files may be replaced with XML, JSON, plain text, or other types of files. In addition, other data structures (such as spreadsheets, relational databases, or structured files) may be used where tables or hash tables are mentioned.
このように、本発明の特定の実施形態について説明してきた。他の実施形態は、以下の請求項の範囲内である。例えば、特許請求の範囲に記載されている動作を異なる順序で実行しても、望ましい結果を得ることができる。 As described above, the specific embodiment of the present invention has been described. Other embodiments are within the scope of the following claims. For example, the operations described in the claims may be performed in different orders to obtain the desired results.
Claims (20)
前記１つまたは複数のコンピュータによって、第１スクリプトの言語例を示すデータセットにアクセスする工程であって、前記言語例の少なくとも一部は、前記第１スクリプトの単語および１つまたは複数の他のスクリプトの単語を備えている、アクセスする工程と
前記１つまたは複数のコンピュータによって、前記第１スクリプトに音訳された単語を有する訓練データセットを生成するべく、前記言語例の一部の少なくとも一部を前記第１スクリプトに音訳する工程と、
前記１つまたは複数のコンピュータによって、前記第１スクリプトに音訳された単語を有する前記訓練データセット内の単語シーケンスの出現に基づき、音声認識モデルを生成する工程と
を備えている、方法。 A method performed by one or more computers, said method.
The step of accessing a dataset showing a language example of a first script by the one or more computers, wherein at least a portion of the language example is a word of the first script and one or more other words. At least a portion of the language example to generate a training data set with the words transliterated into the first script by the accessing process and the one or more computers comprising the words of the script. In the process of transliterating to the first script,
A method comprising the steps of generating a speech recognition model based on the appearance of a word sequence in the training dataset having words transliterated into the first script by the one or more computers.
請求項１に記載の方法。 The speech recognition model is a language model, an acoustic model, a sequence-to-sequence model, or an end-to-end model.
The method according to claim 1.
請求項１または２に記載の方法。 The transliteration step comprises mapping different tokens representing texts from different scripts into a single normalized transliteration representation.
The method according to claim 1 or 2.
請求項１～３のいずれか一項に記載の方法。 The step of transliterating the language example includes a step of transliterating a word in the language example that is not in the first script into the first script.
The method according to any one of claims 1 to 3.
前記第１スクリプトとは異なるスクリプト内の用語のブラックリストにアクセスする工程と、
前記言語例に出現するブラックリストからの用語のインスタンスの音訳をバイパスする工程と
を備えている、請求項１～４のいずれか一項に記載の方法。 The process of translating the language example is
The process of accessing the blacklist of terms in a script different from the first script,
The method according to any one of claims 1 to 4, comprising a step of bypassing the transliteration of an instance of a term from a blacklist appearing in the language example.
請求項１～５のいずれか一項に記載の方法。 The step of transliterating the language example includes a step of generating a modified language example, and in the modified language example, a word written in a second script different from the first script is the first. Replaced by one or more words in the first script that closely resemble the acoustic properties of the words in one script.
The method according to any one of claims 1 to 5.
請求項６に記載の方法。 The words written in the second script are individually transliterated into the first script on a word-by-word basis.
The method according to claim 6.
前記音声認識モデルをテストするために、言語例のテストセットを判定する工程と、
前記テストセット内の言語例のうち、前記第１スクリプトに書かれていない単語を前記第１スクリプトに音訳することで、正規化テストセットを生成する工程と、
前記テストセットの言語例に対応する前記音声認識モデルの出力を得る工程と、
前記音声認識モデルの出力のうち前記第１スクリプトに書かれていない単語を前記第１スクリプトに音訳することで、前記音声認識モデルの出力を正規化する工程と、
前記正規化テストセットと、正規音声認識モデル出力との比較に基づき、前記音声認識モデルの誤り率を判定する工程と
を備えている、請求項１～７のいずれか一項に記載の方法。 The method further
In order to test the speech recognition model, a step of determining a test set of language examples and
A process of generating a normalized test set by transliterating a word not written in the first script into the first script among the language examples in the test set.
The process of obtaining the output of the speech recognition model corresponding to the language example of the test set, and
A process of normalizing the output of the speech recognition model by translating a word not written in the first script into the first script among the outputs of the speech recognition model.
The method according to any one of claims 1 to 7, further comprising a step of determining an error rate of the speech recognition model based on a comparison between the normalization test set and the normal speech recognition model output.
前記単語誤り率に基づき前記方法はさらに、
前記音声認識モデルの訓練を継続するか終了するかを判定する工程と、
前記音声認識モデルの訓練に使用される訓練データセットを変更する工程と
前記音声認識モデルのサイズ、構造、または他の特性を設定する工程と、または
音声認識タスクのために１つまたは複数の音声認識モデルを選択する工程と
を備えている、請求項８に記載の方法。 The error rate is a word error rate, and is
Based on the word error rate, the method further
The process of determining whether to continue or end the training of the speech recognition model, and
One or more speeches for modifying the training data set used to train the speech recognition model and setting the size, structure, or other characteristics of the speech recognition model, or for speech recognition tasks. 8. The method of claim 8, comprising the step of selecting a recognition model.
請求項１～９のいずれか一項に記載の方法。 The method further reduces the modeling error rate of the speech recognition model in which an acoustically similar word written in one of a plurality of scripts is accepted as the correct transcription in a script different from the corresponding word in the reference transcription. It has a process to judge the output of a word without imposing a penalty.
The method according to any one of claims 1 to 9.
請求項１０に記載の方法。 The method further comprises a step of determining the rendering error rate of the speech recognition model, wherein the rendering error rate is between the word scripts in the output of the speech recognition model relative to the corresponding word script in reference transcription. A measure of difference,
The method according to claim 10.
請求項１～１１のいずれか一項に記載の方法。 The transliteration step is performed using a finite state converter network trained to perform transliteration to the first script.
The method according to any one of claims 1 to 11.
請求項１～１２のいずれか一項に記載の方法。 The transliteration step is a plurality of rounds of transliteration between scripts for at least one language example in order to reach the transliterated representation in the first script contained in the training data set in the first script. Has a process to perform,
The method according to any one of claims 1 to 12.
前記言語例におけるスクリプトの混合のレベルを示すスコアを判定する工程を備えるとともに、
前記スコアに基づき、音訳用の有限状態変換器ネットワークをプルーニングするためのパラメータを選択する工程か、
前記スコアに基づき、前記音声認識モデルをプルーニングするためのパラメータを選択する工程か、または
前記スコアに基づき、前記音声認識モデルのサイズまたは構造を選択する工程か
を備えている、請求項１～１３のいずれか一項に記載の方法。 The method further
A step of determining a score indicating the level of mixing of scripts in the above language example is provided, and the process is provided.
The process of selecting parameters for pruning the finite state transducer network for transliteration based on the above score, or
Claims 1 to 13 include a step of selecting a parameter for pruning the speech recognition model based on the score, or a step of selecting the size or structure of the speech recognition model based on the score. The method described in any one of the above.
前記言語例の少なくとも一部を前記第１スクリプトに音訳した後、前記１つまたは複数のコンピュータによって、前記第１スクリプトの前記訓練データセットにおける異なる単語シーケンスの出現回数を判定する工程と、
前記１つまたは複数のコンピュータによって、前記第１スクリプトの前記訓練データセットにおける前記異なる単語シーケンスの出現回数に基づき、前記音声認識モデルを生成する工程と
を備えている、請求項１～１４のいずれか一項に記載の方法。 The step of generating the speech recognition model is
A step of transliterating at least a part of the language example into the first script, and then determining the number of occurrences of different word sequences in the training data set of the first script by the one or more computers.
13. The method described in item 1.
前記音声認識モデルを生成する工程は、前記リカレントニューラルネットワークを訓練する工程を備えている、
請求項１～１５のいずれか一項に記載の方法。 The speech recognition model is equipped with a recurrent neural network.
The step of generating the speech recognition model includes a step of training the recurrent neural network.
The method according to any one of claims 1 to 15.
請求項１～１６のいずれか一項に記載の方法。 The method further comprises the step of using the speech recognition model to perform speech recognition for an utterance by the one or more computers.
The method according to any one of claims 1 to 16.
前記１つまたは複数のコンピュータによって、前記音声データを、前記発話を表すテキストに写像するべく、前記音声認識モデルを使用する工程と
を備えている、音声認識を実行する方法であって、
前記音声認識モデルは、請求項１～１６のいずれか一項に記載の方法に従って生成されたものである、
音声認識を実行する方法。 The process of receiving voice data representing an utterance by one or more computers,
A method of performing speech recognition, comprising the step of using the speech recognition model to map the speech data to a text representing the utterance by the one or more computers.
The speech recognition model is generated according to the method according to any one of claims 1 to 16.
How to perform speech recognition.
前記１つまたは複数のコンピュータによって実行されると、前記１つまたは複数のコンピュータに請求項１～１８のいずれか一項に記載の方法の動作を実行させる命令を格納する１つまたは複数のコンピュータ可読媒体と、
を備えているシステム。 An instruction that, when executed by one or more computers and by the one or more computers, causes the one or more computers to perform the operation of the method according to any one of claims 1-18. With one or more computer-readable media to store
A system that features.
１つまたは複数の非一時的なコンピュータ可読媒体。 When executed by the one or more computers, the instruction for causing the one or more computers to perform the operation of the method according to any one of claims 1 to 18 is stored.
One or more non-temporary computer-readable media.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201862778431P | 2018-12-12 | 2018-12-12 | |
US62/778,431 | 2018-12-12 | ||
PCT/US2019/017258 WO2020122974A1 (en) | 2018-12-12 | 2019-02-08 | Transliteration for speech recognition training and scoring |
Publications (3)
Publication Number | Publication Date |
---|---|
JP2022515048A true JP2022515048A (en) | 2022-02-17 |
JPWO2020122974A5 JPWO2020122974A5 (en) | 2022-08-25 |
JP7208399B2 JP7208399B2 (en) | 2023-01-18 |
Family
ID=65520451
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2021533448A Active JP7208399B2 (en) | 2018-12-12 | 2019-02-08 | Transliteration for speech recognition training and scoring |
Country Status (5)
Country | Link |
---|---|
EP (1) | EP3877973A1 (en) |
JP (1) | JP7208399B2 (en) |
KR (1) | KR20210076163A (en) |
CN (1) | CN113396455A (en) |
WO (1) | WO2020122974A1 (en) |
Families Citing this family (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
KR102616598B1 (en) * | 2023-05-30 | 2023-12-22 | 주식회사 엘솔루 | Method for generating original subtitle parallel corpus data using translated subtitles |
Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2009157888A (en) * | 2007-12-28 | 2009-07-16 | National Institute Of Information & Communication Technology | Transliteration model generation device, transliteration apparatus, and computer program therefor |
JP2018028848A (en) * | 2016-08-19 | 2018-02-22 | 日本放送協会 | Conversion processor, transliteration processor, and program |
Family Cites Families (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8335688B2 (en) * | 2004-08-20 | 2012-12-18 | Multimodal Technologies, Llc | Document transcription system training |
US20080221866A1 (en) * | 2007-03-06 | 2008-09-11 | Lalitesh Katragadda | Machine Learning For Transliteration |
US7472061B1 (en) * | 2008-03-31 | 2008-12-30 | International Business Machines Corporation | Systems and methods for building a native language phoneme lexicon having native pronunciations of non-native words derived from non-native pronunciations |
WO2009129315A1 (en) * | 2008-04-15 | 2009-10-22 | Mobile Technologies, Llc | System and methods for maintaining speech-to-speech translation in the field |
US9176936B2 (en) * | 2012-09-28 | 2015-11-03 | International Business Machines Corporation | Transliteration pair matching |
US10540957B2 (en) * | 2014-12-15 | 2020-01-21 | Baidu Usa Llc | Systems and methods for speech transcription |
US10255909B2 (en) * | 2017-06-29 | 2019-04-09 | Intel IP Corporation | Statistical-analysis-based reset of recurrent neural networks for automatic speech recognition |
-
2019
- 2019-02-08 WO PCT/US2019/017258 patent/WO2020122974A1/en unknown
- 2019-02-08 KR KR1020217017741A patent/KR20210076163A/en unknown
- 2019-02-08 JP JP2021533448A patent/JP7208399B2/en active Active
- 2019-02-08 CN CN201980082043.XA patent/CN113396455A/en active Pending
- 2019-02-08 EP EP19707226.7A patent/EP3877973A1/en active Pending
Patent Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2009157888A (en) * | 2007-12-28 | 2009-07-16 | National Institute Of Information & Communication Technology | Transliteration model generation device, transliteration apparatus, and computer program therefor |
JP2018028848A (en) * | 2016-08-19 | 2018-02-22 | 日本放送協会 | Conversion processor, transliteration processor, and program |
Also Published As
Publication number | Publication date |
---|---|
WO2020122974A1 (en) | 2020-06-18 |
KR20210076163A (en) | 2021-06-23 |
EP3877973A1 (en) | 2021-09-15 |
JP7208399B2 (en) | 2023-01-18 |
CN113396455A (en) | 2021-09-14 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
Sitaram et al. | A survey of code-switched speech and language processing | |
Lee et al. | Massively multilingual pronunciation modeling with WikiPron | |
US11417322B2 (en) | Transliteration for speech recognition training and scoring | |
Karpov et al. | Large vocabulary Russian speech recognition using syntactico-statistical language modeling | |
US11942076B2 (en) | Phoneme-based contextualization for cross-lingual speech recognition in end-to-end models | |
Emond et al. | Transliteration based approaches to improve code-switched speech recognition performance | |
US20110184723A1 (en) | Phonetic suggestion engine | |
Li et al. | Improving text normalization using character-blocks based models and system combination | |
Sagae et al. | Hallucinated n-best lists for discriminative language modeling | |
Althobaiti | Automatic Arabic dialect identification systems for written texts: A survey | |
Alsharhan et al. | Evaluating the effect of using different transcription schemes in building a speech recognition system for Arabic | |
Al-Anzi et al. | The impact of phonological rules on Arabic speech recognition | |
Juhár et al. | Recent progress in development of language model for Slovak large vocabulary continuous speech recognition | |
Anoop et al. | Suitability of syllable-based modeling units for end-to-end speech recognition in Sanskrit and other Indian languages | |
JP7208399B2 (en) | Transliteration for speech recognition training and scoring | |
Karim et al. | On the training of deep neural networks for automatic Arabic-text diacritization | |
Pellegrini et al. | Automatic word decompounding for asr in a morphologically rich language: Application to amharic | |
Adda-Decker et al. | A first LVCSR system for luxembourgish, a low-resourced european language | |
Núñez et al. | Phonetic normalization for machine translation of user generated content | |
Duan et al. | Pinyin as a feature of neural machine translation for Chinese speech recognition error correction | |
Saychum et al. | Efficient Thai Grapheme-to-Phoneme Conversion Using CRF-Based Joint Sequence Modeling. | |
Dasgupta et al. | Resource creation and development of an English-Bangla back transliteration system | |
Lyes et al. | Building a pronunciation dictionary for the Kabyle language | |
US11893349B2 (en) | Systems and methods for generating locale-specific phonetic spelling variations | |
Begum Mustafa et al. | Code-Switching in Automatic Speech Recognition: The Issues and Future Directions |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
A621 | Written request for application examination |
Free format text: JAPANESE INTERMEDIATE CODE: A621Effective date: 20210721 |
|
A977 | Report on retrieval |
Free format text: JAPANESE INTERMEDIATE CODE: A971007Effective date: 20220720 |
|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20220817 |
|
A871 | Explanation of circumstances concerning accelerated examination |
Free format text: JAPANESE INTERMEDIATE CODE: A871Effective date: 20220817 |
|
A131 | Notification of reasons for refusal |
Free format text: JAPANESE INTERMEDIATE CODE: A131Effective date: 20220830 |
|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20221007 |
|
TRDD | Decision of grant or rejection written | ||
A01 | Written decision to grant a patent or to grant a registration (utility model) |
Free format text: JAPANESE INTERMEDIATE CODE: A01Effective date: 20221213 |
|
A61 | First payment of annual fees (during grant procedure) |
Free format text: JAPANESE INTERMEDIATE CODE: A61Effective date: 20230105 |
|
R150 | Certificate of patent or registration of utility model |
Ref document number: 7208399Country of ref document: JPFree format text: JAPANESE INTERMEDIATE CODE: R150 |