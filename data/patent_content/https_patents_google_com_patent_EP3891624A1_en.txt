EP3891624A1 - Embedding-based retrieval for image search - Google Patents
Embedding-based retrieval for image searchInfo
- Publication number
- EP3891624A1 EP3891624A1 EP20714794.3A EP20714794A EP3891624A1 EP 3891624 A1 EP3891624 A1 EP 3891624A1 EP 20714794 A EP20714794 A EP 20714794A EP 3891624 A1 EP3891624 A1 EP 3891624A1
- Authority
- EP
- European Patent Office
- Prior art keywords
- image
- embedding
- landing page
- image search
- pair
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Withdrawn
Links
- 238000013528 artificial neural network Methods 0.000 claims abstract description 72
- 238000000034 method Methods 0.000 claims abstract description 44
- 238000012549 training Methods 0.000 claims description 79
- 238000012545 processing Methods 0.000 claims description 20
- 238000003062 neural network model Methods 0.000 abstract description 18
- 230000008569 process Effects 0.000 abstract description 18
- 238000004590 computer program Methods 0.000 abstract description 14
- 239000013598 vector Substances 0.000 description 23
- 230000006870 function Effects 0.000 description 16
- 230000004044 response Effects 0.000 description 7
- 238000004891 communication Methods 0.000 description 6
- 238000010801 machine learning Methods 0.000 description 6
- 230000008901 benefit Effects 0.000 description 4
- 230000003287 optical effect Effects 0.000 description 4
- 230000003993 interaction Effects 0.000 description 3
- 239000011159 matrix material Substances 0.000 description 3
- 241000282326 Felis catus Species 0.000 description 2
- 238000000605 extraction Methods 0.000 description 2
- 238000007667 floating Methods 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 241000009334 Singa Species 0.000 description 1
- 238000013459 approach Methods 0.000 description 1
- 230000009286 beneficial effect Effects 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 238000004422 calculation algorithm Methods 0.000 description 1
- 230000001149 cognitive effect Effects 0.000 description 1
- 238000007796 conventional method Methods 0.000 description 1
- 238000001514 detection method Methods 0.000 description 1
- 238000010586 diagram Methods 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000004519 manufacturing process Methods 0.000 description 1
- 230000001537 neural effect Effects 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 238000013519 translation Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/953—Querying, e.g. by the use of web search engines
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/953—Querying, e.g. by the use of web search engines
- G06F16/9538—Presentation of query results
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/53—Querying
- G06F16/532—Query formulation, e.g. graphical querying
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/53—Querying
- G06F16/538—Presentation of query results
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/58—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/58—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/583—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/58—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/587—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using geographical or spatial information, e.g. location
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
Definitions
- This specification generally relates to retrieving image search results.
- Online search engines generally retrieve candidate resources, e.g., images, in response to received search queries to present search results identifying resources that are responsive to the search query.
- Search engines generally retrieve search results by a term- based retrieval system that identifies search results based on key terms of the search query. Search engines may retrieve the resources based on various factors.
- Some conventional image search engines i.e., search engines configured to identify images on landing pages, e.g., on webpages on the Internet, in response to received search queries, generate separate signals from the i) features of the image and ii) features of the landing page and then combine the separate signals according to a fixed weighting scheme that is the same for each received search query.
- This specification describes technologies for retrieving image search results in response to an image search query using a trained embedding neural network model.
- receiving an image search query determining a respective pair numeric embedding for each of a plurality of image - landing page pairs, each image - landing page pair including a respective image and a respective landing page for the respective image, wherein each pair numeric embedding is a numeric representation in an embedding space; processing features of the image search query using an image search query embedding neural network to generate a query numeric embedding of the image search query, and wherein the query numeric embedding is a numeric representation in the same embedding space; and identifying, as first candidate image search results for the image search query, image search results that identify a subset of the image - landing page pairs having pair numeric embeddings that are closest to the query numeric embedding of the image search query in the embedding space.
- inventions of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.
- a system of one or more computers to be configured to perform particular operations or actions means that the system has installed on it software, firmware, hardware, or a combination of them that in operation cause the system to perform the operations or actions.
- one or more computer programs to be configured to perform particular operations or actions means that the one or more programs include instructions that, when executed by data processing apparatus, cause the apparatus to perform the operations or actions.
- the subject matter described in this specification can be implemented in particular embodiments so as to realize one or more of the following advantages.
- Retrieving image- search query pairs by evaluating closeness in an embedding space defined by embeddings generated by a trained embedding neural network model allows images to be provided in response to the image search query that are examples of the search query. That is, the images provided in response to the image search query respond to the image search query.
- the embedding neural network model receives a single input that includes features of the image search query, landing page and the image identified by a given image search result and generates an embedding representation of the image search result in the same embedding space as a generated embedding representation of the received query.
- This embedding representation can model more general semantic relationships between features.
- the distance in the embedding space reflects the similarity of one point to another, and any query or search result can be represented as a point in that embedding space.
- This can allow relevant image search results to be effectively retrieved.
- Retrieval in the embedding space can be computationally efficient because fast algorithms can be developed to efficiently find nearest neighbors or approximately nearest neighbors in the embedding space.
- distances in the embedding space can be used for ranking. For example, given a query and a set of image - landing page pairs, the set of image - landing page pairs can be sorted and ranked by corresponding distances in the embedding space.
- the system can retrieve relevant candidate search results that do not fully match all the terms of the search query, which is beneficial for long or obscure search queries.
- Having queries and image-landing page pairs in the same embedding space can enable features that requires identifying relationships between different queries and different landing pages.
- the features can include one or more of the following: obtaining related queries based on a query, obtaining related documents based on a document, obtaining related queries based on a document, or obtaining related documents based on a query.
- an embedding space for queries and image - landing page pairs in different languages can be simultaneously learned.
- the distances in the embedding space can be used to relate landing pages with similar content in different languages.
- the distances in the embedding space can be used to understand that queries in different languages have similar content.
- These connections provided by images can be obtained with the embedding neural network model. Images which are identical or similar may exist on landing pages that are in different languages.
- the embedding neural network model can help identify the connections by taking advantage of this language independent similarity in the embedding space.
- FIG. 1 A is a block diagram of an example search system.
- FIG. IB shows an example of identifying an image - landing page pair as a candidate image search result for an image search query.
- FIG. 2 illustrates an example architecture of an embedding neural network for generating a candidate image search result from an image - landing page pair and an image search query.
- FIG. 3 is a flowchart of an example process for generating image search results from an image search query.
- FIG. 4 is a flowchart of an example process for training an embedding neural network. Like reference numbers and designations in the various drawings indicate like elements.
- FIG. 1 A shows an example image search system 114.
- the image search system 114 is an example of an information retrieval system in which the systems, components, and techniques described below can be implemented.
- a user 102 can interact with the image search system 114 through a user device 104.
- the user device 104 can be a computer coupled to the image search system 114 through a data communication network 112, e.g., local area network (LAN) or wide area network (WAN), e.g., the Internet, or a combination of networks.
- the image search system 114 can be implemented on the user device 104, for example, if a user installs an application that performs searches on the user device 104.
- the user device 104 will generally include a memory, e.g., a random access memory (RAM) 106, for storing instructions and data and a processor 108 for executing stored instructions.
- the memory can include both read only and writable memory.
- the image search system 114 is configured to search a collection of images.
- the images in the collection are images that are found on web pages on the Internet or on a private network, e.g., an Intranet.
- a web page on which an image is found, i.e., in which an image is included, will be referred to in this specification as a landing page for the image.
- the user 102 can submit search queries 110 to the image search system 114 using the user device 104.
- the search query 110 is transmitted through the network 112 to the image search system 114.
- a search engine 130 within the image search system 114 identifies image - landing page pairs that satisfy the search query 110 and responds to the query 110 by generating search results 128 that each identify a respective image - landing page pair satisfying the search query 110.
- Each image - landing page pair includes an image and the landing page on which the image is found.
- the image search result can include a lower-resolution version of the image or a crop from the image and data identifying the landing page, e.g., the resource locator of the landing page, the title of the landing page, or other identifying information.
- the image search system 114 transmits the search results 128 through the network 112 to the user device 104 for presentation to the user 102, i.e., in a form that can be presented to the user 102
- the search engine 130 may include an indexing engine 132, a ranking engine 134 and a retrieval engine 135.
- the indexing engine 132 indexes image - landing page pairs, and adds the indexed image - landing page pairs to an index database 122. That is, the index database 122 includes data identifying images and, for each image, a corresponding landing page.
- the index database 122 also associates the image - landing page pairs with (i) features of the images, i.e., features that characterize the images, and (ii) features of the landing pages, i.e., features that characterize the landing page. Examples of features of images and landing pages are described in more detail below.
- the retrieval engine 135 identifies candidate image - landing page pairs for the search query 110.
- the candidate image - landing page pairs includes a subset of available image - landing page pairs, i.e., a subset of the pairs that are identified in the index database 122
- the retrieval engine 135 can map the search query 110 and each of the image - landing page pairs to the same embedding space by using a trained embedding neural network model 136.
- the distance between an embedding of an image - landing page pair and the embedding of a search query 110 in the embedding space can reflect the relevance of the image - landing page pair to the search query 110.
- the retrieval engine 135 identifies, as candidate image search results, a subset of available image - landing page pairs that are closest to the search query in the embedding space.
- the candidate image search results can later be ranked by the ranking engine 134.
- the retrieval engine 135 determines a pair numeric embedding which is a numeric representation of the image - landing page pair in an embedding space.
- the system can access an index database 122 that associates image - landing page pairs with corresponding previously generated pair numeric embeddings.
- the system can process features of each image - landing page pair using a trained embedding neural network to generate the respective pair numeric embedding for the image - landing page pair at query time.
- the retrieval engine 135 can include two or more retrieval systems that each generate a set of candidate image - landing page pairs.
- the retrieval engine 135 can include a term-based retrieval system that identifies image - landing page pairs based on key terms.
- the retrieval engine 135 can merge retrieval results from the embedding-based retrieval system and retrieval results from the term-based retrieval system to generate the final set of candidate image - landing page pairs.
- the retrieval engine 135 can retrieve relevant results that do not fully match all terms of the query. This benefit is useful for long or obscure queries.
- the ranking engine 134 generates respective ranking scores for the candidate image - landing page pairs.
- the ranking engine 134 can generate relevance scores based on stored scores in the index database 122 or relevance scores that are computed at query time and then rank the candidate image - landing page pairs based on the respective ranking scores.
- the relevance score for a given image - landing page pair reflects the relevance of the image - landing page pair to the received search query 110, the quality of the given image - landing page pair, or both.
- the embedding neural network model 136 can be any of a variety of kinds of embedding neural network models.
- the embedding neural network model 1360 can be a deep machine learning model, e.g., a neural network that includes multiple layers of non-linear operations.
- the image search system 114 includes a training engine 160.
- the training engine 160 trains the embedding neural network model 136 on training data generated using image - landing page pairs that are already associated with ground truth or known search queries. Training the machine learning model will be described in greater detail below with reference to FIG. 4.
- FIG. IB shows an example of identifying an image - landing page pair as a candidate image search result for an image search query.
- the user submits an image search query 170 (“coniferous trees”).
- the system generates image query features 172 based on the user submitted image search query 170. Examples of query features 172 are described below with reference to FIG. 2.
- the system also generates or obtains landing page features 174 for a landing page that is part of a particular image - landing page pair that is identified in the index database and image features 176 for the image in the particular image - landing page. Examples of landing page features 174 and image features 176 are described below with reference to FIG. 2.
- the system then provides the landing page feature 174 and the image features 176 as input to a pair embedding neural network 178.
- the system also provides the query feature 172 as input to an image search query embedding neural network 180.
- the pair embedding neural network 178 receives input that includes features of the landing page and features of the image and generates a pair numeric embedding for the image - landing page pair.
- the pair numeric embedding is a numeric representation of the image - landing page pair in an embedding space.
- the image search query embedding neural network 180 receives input that includes features of the image search query and generates a query numeric embedding of the image search query.
- the query numeric embedding is a numeric representation of the image search query in the same embedding space as the pair numeric embedding for the image - landing page pair.
- the system determines 186 whether the pair numeric embedding 182 is sufficiently close to the query numeric embedding 184 in the embedding space. For example, the system can identify K candidate image - landing page pairs that have pair numeric embeddings that are closest to the query numeric embedding out of the image - landing page pairs in the index. If the system determines that the pair numeric embedding 182 is sufficiently close to the query numeric embedding 184, the system identifies 188 the image - landing page pair as a candidate image search result. The candidate image search result can be later processed by the ranking engine 134.
- FIG. 2 illustrates an example architecture of an embedding neural network 200 for generating a candidate image search result from an image - landing page pair and an image search query.
- the embedding neural network 200 For each image - landing page pair and image search query, the embedding neural network 200 takes query features 202, image features 206 and landing page features 208 as input and can generate output that can help the system identify whether the image - landing page pair is a candidate image search result.
- the embedding neural network 200 includes two sub neural networks: image search query embedding neural network 204 and pair embedding neural network 210.
- the image search query embedding neural network 204 takes as input the query features 202 and generates a query numeric embedding representation 184 of the search query.
- the query features 202 can include a plurality of features, such as, location features, text features, etc.
- the location features can characterize a location from which the image search query was submitted.
- the text features can include unigrams or bigrams of the image search query.
- the image search query embedding neural network 204 can be a deep neural network that includes a number of embedding subnetworks for each feature of the plurality of query features.
- Each embedding subnetwork can generate embedding representations for examples of a corresponding feature.
- a location embedding subnetwork can generate embedding representations for the location features
- a text embedding subnetwork can generate embedding representations for query unigrams or bigrams.
- the unigrams or bigrams in text features can be represented as individual tokens.
- An embedding of a unigram or a bigram can be calculated using a look-up table.
- the look-up table can be an embedding weight matrix and can be a shortcut for matrix multiplication in order to improve efficiency.
- the look-up table can be trained similar to training parameters in weight matrix.
- the output of the look-up table can be a one dimensional integer vector.
- the word “cat” can be represented as a token 543.
- the embedding for the word “cat” can be the values in the 543th row of the look-up table, e.g., a vector [1 46 79] with an embedding dimension or length of 5.
- the numerical embedding representation of a text feature can be an average value of the embeddings of all the tokens.
- the output of each embedding subnetwork can be a numeric vector.
- the numeric vector can be a vector of length 128 with floating numbers.
- Each embedding subnetwork is previously trained to generate embedding vectors for query features of a particular type.
- the trained subnetwork can map different query features of a particular type into a common space.
- the text embedding subnetwork can map different kinds of query text into a common space by generating corresponding embedding vectors.
- a query text [red hats] can be mapped to a numeric vector [0.1, -0.2, 0.0, ..., -0.3, 0.2] that is a vector of length 128.
- each embedding subnetwork is merged together through operations such as concatenation or addition to generate an embedding representation for the image search query. For example, suppose the output from the location embedding subnetwork is a vector of length 128 and the output from the text embedding subnetwork is also a vector of length 128, these outputs can be concatenated together and can generate a vector of length 256 that summarizes the embedding representations of the text features and location features of the image search query.
- the merged features are processed through one or more fully connected layers that further extract features from the merged features in order to generate a final query numerical embedding 184 for the image search query.
- the pair embedding neural network 210 takes as input the image features 206 and landing page features 208 and generates a pair numeric embedding 182 of the image - landing page pair.
- the image features 206 and landing page features 208 can be from an index database 122 or from other data maintained by the system that associates images and landing pages with corresponding features.
- the image features 206 can include one or more of pixel data of the image or an embedding of the image that characterizes content of the image.
- the image features can include all or part of the pixels of an image that can represent raw content information of an image.
- the image features 206 can include embedding vectors that represent the content of the image. These embedding vectors to represent the image may be derived by processing the image through another embedding neural network. Alternatively, the embedding vectors can be generated through other image processing techniques for feature extraction.
- Example feature extraction techniques include edge, corner, ridge and blob detection.
- embedding vectors of the image content can be previously generated and saved in an index database. Therefore, the embedding representation of an image content can be directly obtained by accessing the index database without the need to compute it within the embedding neural network 200.
- the image features 206 can also include data identifying a domain of an image, and/or text from a Uniform Resource Locator (URL) of an image, e.g., unigrams or bigrams.
- URL Uniform Resource Locator
- the text features of an image and the text feature from a search query both include unigrams or bigrams. Therefore, both of them can be later mapped to a same embedding space through the embedding neural network 200.
- the corresponding embedding representations of relevant text features are closer to each other in the embedding space than those of the less relevant or irrelevant text features.
- the landing page features 208 can include one or more of text from a title of the landing page, salient terms that appear on the landing page, text from a URL of the landing page and data identifying a domain of the landing page. Additionally, examples of features extracted from the landing page can include the date the page was first crawled or updated, data characterizing the author of the landing page, the language of the landing page, keywords representing the content of the landing page, features of the links to the image and landing page such as the anchor text or source page for the links, features that describe the context of the image in the landing page and so on.
- the landing page features 208 can also include features extracted from the landing page that describe the context of the image in the landing page.
- features extracted from the landing page that describe the context of the image in the landing page include data characterizing the location of the image within the landing page, the prominence of the image on the landing page, textual descriptions of the image on the landing page etc.
- the location of the image within the landing page can be pin-pointed using pixel-based geometric location in horizontal and vertical dimensions, user-device based length (e.g., in inches) in horizontal and vertical dimensions, an HTML/XML DOM-based XPATH-like identifier, a CSS-based selector, etc.
- the prominence of the image on the landing page can be measured using relative size of the image as displayed on a generic device and on a specific user device.
- Textual descriptions of the image on the landing page can include alt-text labels for the image, text surrounding the image, and so on.
- the pair embedding neural network 210 can be a deep neural network that includes a number of embedding subnetworks for each feature of the plurality of image - landing page pair features.
- Each embedding subnetwork can generate embedding representations for examples of a corresponding feature.
- a domain embedding subnetwork can generate embedding representation for page domain features
- a text embedding subnetwork can generate embedding for text data of an image URL.
- the output of each embedding subnetwork can be a numeric vector.
- the numeric vector can be a vector of length 128 with floating numbers.
- outputs of each embedding subnetworks are merged together through operations such as concatenation or addition to generate an embedding representation for the image - landing page pair.
- outputs from a plurality of embedding subnetworks can be a plurality of embedding vectors, each having length 128, for page title uni grams/bigrams, page salient terms, page URL unigrams/bigrams, image URL unigrams/bigrams, and image domain, etc.
- an embedding vector of length 128 for the image content can be obtained from the index database.
- the plurality of N embedding vectors can be concatenated together and can generate a vector of length 128xN that summarizes embedding representations of features of the image - landing page pair.
- the merged features are processed through one or more fully connected layers that further extract features from the merged features in order to generate a final pair numeric embedding 182 for the image - landing page pair.
- the pair numerical embedding 182 and the query numerical embedding 184 are in the same embedding space.
- the outputs of the embedding subnetworks can be partially merged or not merged. Instead of merging the outputs of embedding subnetworks and generating a single embedding representation for the image - landing page pair, the outputs of the embedding subnetworks can be merged into two or more embedding representations for the image - landing page pair. Accordingly, the corresponding final pair numeric embedding 182 can include two or more embedding representations that are in the same embedding space as the query numerical embedding 184.
- the image search query embedding neural network 204 and the pair embedding neural network 210 share at least some parameters.
- two or more of the subnetworks such as a query text embedding subnetwork, a landing page title embedding subnetwork, a landing page salient terms embedding subnetwork, a landing page URL embedding subnetwork and an image URL embedding network, etc., can share parameters because these features are drawn from a same vocabulary.
- Two neural networks sharing parameters refers to the two neural networks being constrained to have the same value for each parameter that is shared.
- the image search query embedding neural network 204 and the pair embedding neural network 210 can be trained jointly to facilitate training of the shared parameters between these networks. More details about training the embedding neural network will be described in greater detail below with reference to FIG. 4.
- the prediction layer 212 compares the pair numeric embedding 182 with the query numeric embedding 184 in the same embedding space.
- the prediction layer 212 can output a distance value that can measure the closeness of the pair numeric embedding 182 and the query numeric embedding 184.
- the prediction layer 212 can include a dot product between the pair numeric embedding 182 and the query numeric embedding 184.
- the output from the prediction layer 212 can be used differently during training of the embedding neural network 200 and during an image search.
- the retrieval engine 135 can identify candidate image search results for a search query based on the output from the prediction layer 212 that measures the closeness of embedding representations of image - landing page pairs to the embedding representation of the search query.
- the training engine 160 can jointly train the pair embedding neural network and the image search query embedding neural network to minimize a loss function that depends on the output from the prediction layer 212, e.g., the dot product.
- FIG. 3 is a flowchart of an example process 300 for generating image search results from an image search query.
- the process 300 will be described as being performed by a system of one or more computers located in one or more locations.
- an image search system e.g., the image search system 114 of FIG. 1A, appropriately programmed in accordance with this specification, can perform the process 300.
- the image search system receives an image search query from a user device (302).
- the image search query is submitted through a dedicated image search interface provided by the image search system, i.e., a user interface for submitting image search queries.
- the search query is submitted through a generic Internet search interface and image search results are displayed in response to the image search query along with other kinds of search results, i.e., search results that identify other types of content available on the Internet.
- the image search system Upon receiving the image search query, the image search system identifies initial image - landing page pairs (304). For example, the system can identify the initial image - landing page pairs from the pairs indexed in a search engine index database based on signals that measure the quality of the pairs, the relevance of the pairs to the search query, or both.
- the system determines a respective pair numeric embedding (306) which is a numeric representation of the image - landing page pair in an embedding space.
- the system can access an index database that associates image - landing page pairs with corresponding pair numeric embeddings that have been previously generated using a pair embedding neural network. This can save image search time because the pair numeric embeddings have been previously computed and stored.
- the system can process features of each image - landing page pair using a pair embedding neural network to generate the respective pair numeric embedding for the image - landing page pair.
- the features of each image - landing page pair can include features of the image and features of the landing page. These features can be from the index database or from other data maintained by the system that associates images and landing pages with corresponding features. These features may be represented categorically or discretely.
- additional relevant features can be created through pre-existing features. For example, a system may create relationships between one or more features through a combination of addition, multiplication, or other mathematical operations.
- the system obtains features of the image search query (308) and processes features of the image search query using an image search query embedding neural network (310).
- the image search query embedding neural network can generate a query numeric embedding of the image search query.
- the generated query numeric embedding is a numeric representation of the image search query in the same embedding space as the pair numeric representation of the image - landing page pair.
- the system identifies a subset of the initial image - landing page pairs as first candidate image search results (312).
- the subset of the initial image - landing page pairs have pair numeric embeddings that are closest to the query numeric embeddings of the image search query in the embedding space. For example, among the initial image - landing page pairs, top K image - landing page pairs that have embedding representations closest to the embedding representation of the search query can be selected using nearest neighbor search.
- Feature embeddings can model more general semantic relationships between features.
- the closeness of the numeric embeddings of the features can be trained to measure a relevance of the candidate image search result to the image search query.
- the closeness of the numeric embeddings can be trained to measure a likelihood of a user submitting the search query would interact with the search result.
- the numeric embeddings that are closer to each other indicates the user submitting the search query would find the candidate image search result more relevant and interact with it. Training the embedding neural network to generate numeric embeddings will be described below with reference to FIG. 4.
- the first candidate image search results generally include much fewer candidates than the initial image search results.
- the number of first candidate image search results can be limited to less than on the order of one hundred results. This is much fewer than the initial image search results, which can be thousands or millions of image search results.
- the system upon receiving the first candidate image search results, the system then generates a plurality of second candidate image search results that includes at least some of the first candidate image search results. For example, the system may obtain other candidates retrieved by a term-based retrieval system that is based on key terms. The system can merge the term-based candidates and the embedding-based candidates and send the merged candidates for a second round of relevance scoring. After the second round of relevance scoring, the second candidate image search results can be selected from the embedding-based first candidate image search results and the term-based candidate image search results.
- the system ranks the plurality of second candidate image search results (314) by using the ranking engine.
- the ranking engine can generate relevance scores based on stored scores in the index database or computed at query time, and ranks the plurality of second image - landing page pairs based on the respective ranking scores.
- the relevance score for a candidate image - landing page pair reflects the relevance of the image - landing page pair to the received search query, the quality of the given image - landing page pair, or both.
- the system ranks the image search results based on the relevance scores for the corresponding image - landing page pairs.
- the system generates an image search results presentation that shows the image search results ordered according to the ranking (316) and provides the image search results presentation for presentation (318) by sending the search result presentation through a network to the user device from which the image search query was received in a form that can be presented to a user.
- FIG. 4 is a flowchart of an example process 400 for training an embedding neural network.
- the process 400 will be described as being performed by a system of one or more computers located in one or more locations.
- an image search system e.g., the image search system 114 of FIG. 1A, appropriately programmed in accordance with this specification, can perform the process 400.
- the system receives a set of training image search queries, and, for each training image search query, training image search results for the query (402).
- Each training image search result can be identified as a positive training example or a negative training example.
- the system identifies the training image search query and the training image - landing page pair as a positive training example when a user interacted with a search result identifying the training image - landing page pair after submitting the training image search query.
- the system For each of the training image search queries, the system generates training examples using features of the image search queries (404). For each of the training image search results, the system generates training examples using features of image - landing page pairs (408). For each training pair, the system identifies (i) features of the image search query (ii) features of the image and (iii) features of the landing page. Extracting, generating and selecting features may occur prior to training or using other embedding models. Examples of features are described above with reference to FIG. 2.
- the system trains a pair embedding neural network (410) and trains an image query embedding neural network (406) jointly.
- the system jointly trains the two neural networks to minimize a loss function that depends on a dot product between (i) a query numeric embedding for a training image search query and (ii) a pair numeric embedding for a training image - landing page pair.
- the loss function can encourage dot products to be higher when the training image search query and the training image - landing page pair have been identified as a positive training example than when the training image search query and the training image - landing page pair have been identified as a negative training example.
- the image search query embedding neural network can be previously trained for other embedding representation tasks.
- the image search query embedding neural network can be implemented with a look-up table with parameters that are previously determined or trained.
- the numeric representations of the training image search queries can be calculated by indexing the look-up table using token representations of the training image search queries.
- the pair embedding neural network can be previously trained for other embedding representation tasks.
- the pair embedding neural network and the image search query embedding neural network can share at least some parameters.
- the pair embedding neural network and the image search query embedding neural network may share parameters corresponding to any features that are drawn from a same vocabulary.
- the shared neural network parameters can be effectively trained by the joint training method discussed above.
- the system can implement the loss function using any of a variety of available loss functions in training the embedding neural network model in order to improve effective utilization of the large amount of data that is available.
- loss functions that can be used to train the model include softmax with cross-entropy loss, sampled softmax loss (Jean, Sebastien, et al. "On using very large target vocabulary for neural machine translation.” arXiv preprint arXiv: 1412.2007. 2014), contrastive loss functions or a combination of two or more of them.
- the system can train the embedding neural network model in several stages and the system can implement different kinds of loss functions at each stage of the training process.
- the system can use a softmax loss function in a first stage, and can use a contrastive loss function or an asymmetric scaled sigmoid loss function in the following stages.
- hard negative samples e.g., training samples that have large loss values in one or more previous training stages, can be used during training to improve convergence speed of the training process or to improve the performance of a final model that is being trained.
- the system receives a set of 4096 training image search queries, and, for each training image search query query an image search result Selectedlmage i.e., a selected image - landing page pair, for the query.
- the index i 1,2, ... , 4096.
- the system For each training image search query query the system generates one positive training example (query i.Selectedlmagei) and generates 4095 negative training examples (query i.Selectedlmage j ), where i 1 j .
- the embedding neural network can output a dot product that can be normalized to a range of [0, 1] by a softmax function.
- the system can compute a sampled softmax loss for each training image search query query t using the normalized dot products computed from its corresponding 4095 negative training examples and one positive training example. Instead of computing the softmax loss over all the 4096 training image search queries, the sampled softmax loss only takes into account a subset of training examples to calculate the loss because the number of training image search queries is very large. The overall loss is the sum of loss computed for each of the 4096 training image search queries.
- the system trains the embedding neural network by minimizing the loss function.
- the system may train the embedding neural network model to determine trained values of the weights of the neural network from initial values of the weights by repeatedly performing a neural network training procedure to compute a gradient of the loss function with respect to the weights, e.g., using backpropagation, and determining updates to the weights from the gradient, e.g., using the update rule corresponding to the neural network training procedure.
- Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly-embodied computer software or firmware, in computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible non transitory storage medium for execution by, or to control the operation of, data processing apparatus.
- the computer storage medium can be a machine- readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them.
- the program instructions can be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.
- data processing apparatus refers to data processing hardware and encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers.
- the apparatus can also be, or further include, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- the apparatus can optionally include, in addition to hardware, code that creates an execution environment for computer programs, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- a computer program which may also be referred to or described as a program, software, a software application, an app, a module, a software module, a script, or code, can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages; and it can be deployed in any form, including as a stand alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.
- a program may, but need not, correspond to a file in a file system.
- a program can be stored in a portion of a file that holds other programs or data, e.g., one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, e.g., files that store one or more modules, sub programs, or portions of code.
- a computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a data communication network.
- the term “database” is used broadly to refer to any collection of data: the data does not need to be structured in any particular way, or structured at all, and it can be stored on storage devices in one or more locations.
- the index database can include multiple collections of data, each of which may be organized and accessed differently.
- engine is used broadly to refer to a software-based system, subsystem, or process that is programmed to perform one or more specific functions.
- an engine will be implemented as one or more software modules or components, installed on one or more computers in one or more locations. In some cases, one or more computers will be dedicated to a particular engine; in other cases, multiple engines can be installed and running on the same computer or computers.
- the processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows can also be performed by special purpose logic circuitry, e.g., an FPGA or an ASIC, or by a combination of special purpose logic circuitry and one or more programmed computers.
- Computers suitable for the execution of a computer program can be based on general or special purpose microprocessors or both, or any other kind of central processing unit.
- a central processing unit will receive instructions and data from a read only memory or a random access memory or both.
- the essential elements of a computer are a central processing unit for performing or executing instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- a computer need not have such devices.
- a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, e.g., a universal serial bus (USB) flash drive, to name just a few.
- PDA personal digital assistant
- GPS Global Positioning System
- USB universal serial bus
- Computer readable media suitable for storing computer program instructions and data include all forms of non volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks.
- semiconductor memory devices e.g., EPROM, EEPROM, and flash memory devices
- magnetic disks e.g., internal hard disks or removable disks
- magneto optical disks e.g., CD ROM and DVD-ROM disks.
- embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.
- a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web browser on a user’s device in response to requests received from the web browser.
- a computer can interact with a user by sending text messages or other forms of message to a personal device, e.g., a smartphone that is running a messaging application, and receiving responsive messages from the user in return.
- Data processing apparatus for implementing machine learning models can also include, for example, special-purpose hardware accelerator units for processing common and compute-intensive parts of machine learning training or production, i.e., inference, workloads.
- Machine learning models can be implemented and deployed using a machine learning framework, e.g., a TensorFlow framework, a Microsoft Cognitive Toolkit framework, an Apache Singa framework, or an Apache MXNet framework.
- a machine learning framework e.g., a TensorFlow framework, a Microsoft Cognitive Toolkit framework, an Apache Singa framework, or an Apache MXNet framework.
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface, a web browser, or an app through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back end, middleware, or front end components.
- the components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (LAN) and a wide area network (WAN), e.g., the Internet.
- LAN local area network
- WAN wide area network
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- a server transmits data, e.g., an HTML page, to a user device, e.g., for purposes of displaying data to and receiving user input from a user interacting with the device, which acts as a client.
- Data generated at the user device e.g., a result of the user interaction, can be received at the server from the device.
Abstract
Description
Claims
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2020/020459 WO2021173158A1 (en) | 2020-02-28 | 2020-02-28 | Embedding-based retrieval for image search |
Publications (1)
Publication Number | Publication Date |
---|---|
EP3891624A1 true EP3891624A1 (en) | 2021-10-13 |
Family
ID=70009428
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
EP20714794.3A Withdrawn EP3891624A1 (en) | 2020-02-28 | 2020-02-28 | Embedding-based retrieval for image search |
Country Status (4)
Country | Link |
---|---|
US (2) | US11782998B2 (en) |
EP (1) | EP3891624A1 (en) |
CN (1) | CN113614711A (en) |
WO (1) | WO2021173158A1 (en) |
Families Citing this family (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20210286851A1 (en) * | 2020-03-11 | 2021-09-16 | Microsoft Technology Licensing, Llc | Guided query recommendations |
US11636291B1 (en) * | 2020-04-06 | 2023-04-25 | Amazon Technologies, Inc. | Content similarity determination |
US11682060B2 (en) * | 2021-01-30 | 2023-06-20 | Walmart Apollo, Llc | Methods and apparatuses for providing search results using embedding-based retrieval |
US11893385B2 (en) | 2021-02-17 | 2024-02-06 | Open Weaver Inc. | Methods and systems for automated software natural language documentation |
US11836069B2 (en) | 2021-02-24 | 2023-12-05 | Open Weaver Inc. | Methods and systems for assessing functional validation of software components comparing source code and feature documentation |
US11947530B2 (en) | 2021-02-24 | 2024-04-02 | Open Weaver Inc. | Methods and systems to automatically generate search queries from software documents to validate software component search engines |
US11960492B2 (en) | 2021-02-24 | 2024-04-16 | Open Weaver Inc. | Methods and systems for display of search item scores and related information for easier search result selection |
US11921763B2 (en) | 2021-02-24 | 2024-03-05 | Open Weaver Inc. | Methods and systems to parse a software component search query to enable multi entity search |
US11836202B2 (en) * | 2021-02-24 | 2023-12-05 | Open Weaver Inc. | Methods and systems for dynamic search listing ranking of software components |
US11853745B2 (en) | 2021-02-26 | 2023-12-26 | Open Weaver Inc. | Methods and systems for automated open source software reuse scoring |
Family Cites Families (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9158857B2 (en) * | 2012-06-05 | 2015-10-13 | Google Inc. | Identifying landing pages for images |
US8995716B1 (en) * | 2012-07-12 | 2015-03-31 | Google Inc. | Image search results by seasonal time period |
US10831820B2 (en) | 2013-05-01 | 2020-11-10 | Cloudsight, Inc. | Content based image management and selection |
EP3300002A1 (en) | 2016-09-22 | 2018-03-28 | Styria medijski servisi d.o.o. | Method for determining the similarity of digital images |
US11074289B2 (en) | 2018-01-31 | 2021-07-27 | Microsoft Technology Licensing, Llc | Multi-modal visual search pipeline for web scale images |
US11809822B2 (en) * | 2020-02-27 | 2023-11-07 | Adobe Inc. | Joint visual-semantic embedding and grounding via multi-task training for image searching |
-
2020
- 2020-02-28 EP EP20714794.3A patent/EP3891624A1/en not_active Withdrawn
- 2020-02-28 CN CN202080006089.6A patent/CN113614711A/en active Pending
- 2020-02-28 WO PCT/US2020/020459 patent/WO2021173158A1/en unknown
- 2020-02-28 US US17/277,820 patent/US11782998B2/en active Active
-
2023
- 2023-09-05 US US18/461,049 patent/US20230409653A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
CN113614711A (en) | 2021-11-05 |
US20230409653A1 (en) | 2023-12-21 |
US11782998B2 (en) | 2023-10-10 |
US20220012297A1 (en) | 2022-01-13 |
WO2021173158A1 (en) | 2021-09-02 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11782998B2 (en) | Embedding based retrieval for image search | |
US8429173B1 (en) | Method, system, and computer readable medium for identifying result images based on an image query | |
US20230273923A1 (en) | Generating and/or utilizing a machine learning model in response to a search request | |
RU2720905C2 (en) | Method and system for expanding search queries in order to rank search results | |
US9177046B2 (en) | Refining image relevance models | |
US20200250538A1 (en) | Training image and text embedding models | |
US20160378863A1 (en) | Selecting representative video frames for videos | |
US8832096B1 (en) | Query-dependent image similarity | |
CA2774278C (en) | Methods and systems for extracting keyphrases from natural text for search engine indexing | |
US11586927B2 (en) | Training image and text embedding models | |
US20200201915A1 (en) | Ranking image search results using machine learning models | |
US20210103622A1 (en) | Information search method, device, apparatus and computer-readable medium | |
US9218366B1 (en) | Query image model | |
US20200192921A1 (en) | Suggesting text in an electronic document | |
EP3485394B1 (en) | Contextual based image search results | |
EP3682309A1 (en) | Performing image search using content labels | |
EP4162372A1 (en) | Generating a graph data structure that identifies relationships among topics expressed in web documents | |
US11379527B2 (en) | Sibling search queries | |
US20230205824A1 (en) | Contextual Clarification and Disambiguation for Question Answering Processes | |
KR20120038418A (en) | Searching methods and devices | |
CN114282528A (en) | Keyword extraction method, device, equipment and storage medium | |
WO2023175089A1 (en) | Generating output sequences with inline evidence using language model neural networks |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: UNKNOWN |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: THE INTERNATIONAL PUBLICATION HAS BEEN MADE |
|
PUAI | Public reference made under article 153(3) epc to a published international application that has entered the european phase |
Free format text: ORIGINAL CODE: 0009012 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: REQUEST FOR EXAMINATION WAS MADE |
|
17P | Request for examination filed |
Effective date: 20210310 |
|
AK | Designated contracting states |
Kind code of ref document: A1Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
RAP3 | Party data changed (applicant data changed or rights of an application transferred) |
Owner name: GOOGLE LLC |
|
RIN1 | Information on inventor provided before grant (corrected) |
Inventor name: KARANJKAR, SUSHRUTInventor name: PATHAK, MANAS ASHOKInventor name: VERMA, SHUBHANGInventor name: STROHMANN, THOMAS RICHARDInventor name: TIRUMALAREDDY, SUNDEEPInventor name: GLASNER, DANIELInventor name: FAN, WEIInventor name: YIN, WENYUANInventor name: BASU, SUDDHA KALYAN |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: EXAMINATION IS IN PROGRESS |
|
17Q | First examination report despatched |
Effective date: 20220708 |
|
DAV | Request for validation of the european patent (deleted) | ||
DAX | Request for extension of the european patent (deleted) | ||
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: THE APPLICATION HAS BEEN WITHDRAWN |
|
18W | Application withdrawn |
Effective date: 20240202 |