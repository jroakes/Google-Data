EP3404654B1 - Providing suggested voice-based action queries - Google Patents
Providing suggested voice-based action queries Download PDFInfo
- Publication number
- EP3404654B1 EP3404654B1 EP18179479.3A EP18179479A EP3404654B1 EP 3404654 B1 EP3404654 B1 EP 3404654B1 EP 18179479 A EP18179479 A EP 18179479A EP 3404654 B1 EP3404654 B1 EP 3404654B1
- Authority
- EP
- European Patent Office
- Prior art keywords
- voice
- query
- computing device
- based action
- action
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 230000009471 action Effects 0.000 title claims description 357
- 238000000034 method Methods 0.000 claims description 30
- 230000004044 response Effects 0.000 claims description 27
- 230000015654 memory Effects 0.000 claims description 6
- 230000000977 initiatory effect Effects 0.000 description 17
- 238000013507 mapping Methods 0.000 description 10
- 235000015220 hamburgers Nutrition 0.000 description 5
- 239000000463 material Substances 0.000 description 5
- 238000012552 review Methods 0.000 description 4
- 230000000007 visual effect Effects 0.000 description 4
- 238000004891 communication Methods 0.000 description 3
- 238000010079 rubber tapping Methods 0.000 description 3
- 230000003190 augmentative effect Effects 0.000 description 2
- 230000000295 complement effect Effects 0.000 description 2
- 238000004883 computer application Methods 0.000 description 2
- 230000001419 dependent effect Effects 0.000 description 2
- 238000010586 diagram Methods 0.000 description 2
- 230000006870 function Effects 0.000 description 2
- 230000007246 mechanism Effects 0.000 description 2
- 230000002093 peripheral effect Effects 0.000 description 2
- 230000008569 process Effects 0.000 description 2
- 230000009118 appropriate response Effects 0.000 description 1
- 230000001413 cellular effect Effects 0.000 description 1
- 238000006243 chemical reaction Methods 0.000 description 1
- 230000000694 effects Effects 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 230000014509 gene expression Effects 0.000 description 1
- 239000011521 glass Substances 0.000 description 1
- 230000003993 interaction Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000006855 networking Effects 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 230000002085 persistent effect Effects 0.000 description 1
- 238000012545 processing Methods 0.000 description 1
- 230000005236 sound signal Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/1822—Parsing for meaning understanding
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/24—Querying
- G06F16/242—Query formulation
- G06F16/2428—Query predicate definition using graphical user interfaces, including menus and forms
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/24—Querying
- G06F16/245—Query processing
- G06F16/2452—Query translation
- G06F16/24522—Translation of natural language queries to structured queries
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/33—Querying
- G06F16/332—Query formulation
- G06F16/3322—Query formulation using system suggestions
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/33—Querying
- G06F16/332—Query formulation
- G06F16/3322—Query formulation using system suggestions
- G06F16/3323—Query formulation using system suggestions using document space presentation or visualization, e.g. category, hierarchy or range presentation and selection
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/33—Querying
- G06F16/3331—Query processing
- G06F16/3332—Query translation
- G06F16/3334—Selection or weighting of terms from queries, including natural language queries
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
- G06F3/167—Audio in a user interface, e.g. using voice commands for navigating, audio feedback
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/26—Speech to text systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L17/00—Speaker identification or verification
- G10L17/22—Interactive procedures; Man-machine interfaces
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/223—Execution procedure of a spoken command
Definitions
- Voice query applications are increasingly being used in the control of computing devices.
- One use of a voice query application is with portable computing devices such as mobile phones, watches, tablet computers, head-mounted devices, virtual or augmented reality devices, vehicular electronic systems (e.g ., automotive systems that incorporate navigation and audio capabilities), etc.
- Many voice query applications have evolved to respond to natural language requests and/or manage back-and-forth dialogs or conversations with users.
- Many voice query applications incorporate both an initial speech-to-text conversion that converts an audio recording of a human voice to text, and a semantic analysis that analyzes the text in an attempt to determine the meaning of a user's request.
- a computer-based action may be undertaken such as performing a search, providing driving directions, or otherwise controlling one or more applications of a computing device.
- a voice-query application is provided in US 2013/091463 A1 .
- voice query applications have increased and may continue to increase. However, in many instances a user may not fully appreciate the capabilities afforded by the voice query application of a computing device being utilized by the user.
- Some implementations of the technology of this specification may facilitate user discovery of various voice-based action queries that can be spoken to initiate computer-based actions, such as voice-based action queries that can be provided as spoken input to a computing device to initiate computer-based actions that are particularized to content being viewed or otherwise consumed by the user on the computing device.
- voice-based action queries that can be provided as spoken input to a computing device to initiate computer-based actions that are particularized to content being viewed or otherwise consumed by the user on the computing device.
- a method, a computing device and at least one non-transitory computer-readable medium according to independent claims 1, 10 and 11 are proposed, respectively.
- techniques are disclosed for generating one or more suggested voice-based action queries in view of content being accessed on a computing device.
- Techniques are also disclosed that facilitate user discovery of various voice-based action queries that can be spoken to initiate computer-based actions, such as voice-based action queries that can be provided as spoken input to a computing device to initiate computer-based actions that are particularized to content currently being accessed by the user on the computing device. Further details regarding selected implementations are discussed hereinafter. It will be appreciated however that other implementations are contemplated so the implementations disclosed herein are not exclusive.
- a user is using an application on the user's phone that provides detailed information related to selected movies, actresses, actors, etc. Further assume the user has used the application to navigate to content about the film Blade Runner such as content that includes the title of the film, a synopsis of the film, actors in the film, etc.
- the user may initiate a voice query via the phone and an indication of the content presented in the application may be sent by the user's phone to a server.
- the server may utilize the indication of content to determine a dominant entity of the content is the entity related to the movie "Blade Runner", may determine computer-based actions related to the movie, and may determine suggested voice-based action queries that will cause performance of those actions for the movie.
- suggested voice-based action queries of "how can I watch it”, “watch it”, and “tell me more about it” may be determined.
- the suggested voice-based action queries may be provided to the user's phone for presentation to the user.
- the suggested voice-based action queries may be graphically displayed in one or more information "cards” and/or in a "drop down menu” near a voice query graphical interface, optionally along with text such as "try speaking any one of the following.”
- the suggested voice-based action queries are not presented to the user via the user's phone until after at least a threshold period of time has passed since the voice query was initiated.
- the user may select one of the suggested voice-based action queries without speaking (e.g ., via tapping it) to execute the query for performance of the associated action and/or the user may speak one of the suggested voice-based action queries to execute the query for performance of the associated action.
- a voice-based action query that initiates performance of a computer-based action is a query that includes an action term mapped to the computer-based action and an entity term that is the focus of the action.
- the action term may be a verb and the entity term may be a noun or pronoun.
- a voice-based action query causes performance of a computer-based action mapped to the action term and causes performance of the computer-based action in view of the entity mapped to the entity term.
- a voice-based action query of "tell me more about blade runner” may cause a search query to be submitted that is particularized to "blade runner", and search results to be provided responsive to the search query.
- a voice-based action query of "navigate to a coffee shop” may cause a navigation application to provide the user with active navigation directions to a nearby coffee shop.
- a voice-based action query of "call restaurant A” may cause a phone dialing application to prepopulate and/or automatically call the phone number for Restaurant A.
- a computer-based action may be mapped to one or more computer applications that can perform the action and execution of a voice-based action query that includes a term associated with the action may cause at least one of those computer applications to automatically perform the action, or present an option for performance of the action.
- a voice query application may perform some of an action, the entirety of an action, and/or may process a voice-based action query to facilitate performance of an action by another application (e.g., parse the query and interface with another application based on the parsed query and an API of the application).
- the example environment includes a voice-enabled computing device 120, a suggested voice-based action query system 140, an entities and actions database 162, and a network 101.
- the network 101 may comprise one or more networks such as a local area network (LAN) or wide area network (WAN) (e.g ., the Internet).
- the voice-enabled computing device 120 is a portable computing device such as cellular phone, tablet computer, laptop computer, watch, head-mounted device ( e.g ., glasses), virtual or augmented reality device, other wearable device, an audio/video system, a navigation system, automotive and other vehicular system, etc.
- voice input received by voice-enabled computing device 120 is processed by a voice query application 122, which in some implementations may be a search application that includes voice query functionality.
- voice query application 122 may be a stand-alone application.
- voice query application 122 may be integrated, in whole or in part, as part of the operating system or firmware of the computing device 120.
- Voice query application 122 in the illustrated implementation includes a voice action module 123, an interface module 124, and a render/synchronization module 125.
- Voice action module 123 monitors for voice input directed to the voice query application 122, coordinates the analysis of received voice input, and coordinates performance of one or more computer-based actions that are responsive to the received voice input. As described herein, voice action module 123 further coordinates the providing of information to the suggested voice-based action query system 140 and the presentation of suggested voice-based action queries received from the system 140 as suggestions for a voice query.
- Interface module 124 provides an interface with suggested voice-based action query system 140 and/or other systems.
- the interface module 124 provides information to the suggested voice-based action query system 140, such as indications of content accessed on the computing device 120, indications of a preferred language of the computing device 120, information related to one or more applications 126 of computing device 120, and/or voice-based query input indications.
- the interface module 124 further receives suggested voice-based action queries from suggested voice-based action query system 140 in response to information provided by the interface module 124.
- Render/synchronization module 125 manages the presenting of suggested voice-based action queries to a user, e.g., via a visual display, spoken audio, or other feedback interface suitable for a particular voice-enabled device. In addition, in some implementations, module 125 also handles synchronization with other online services, such as when a response or action affects data maintained for the user in another online service (e.g ., where voice input requests creation of an appointment that is maintained in a cloud-based calendar).
- modules 123-125 may be combined and/or implemented in another module.
- one or more aspects of module 124 may be incorporated in module 123.
- modules 123-125 are illustrated in the example environment of FIG. 1 as being provided on computing device 120, this is not meant to be limiting. In other implementations, all or aspects of one or more of the modules 123-125 may be implemented on suggested voice-based action query system 140 and/or another computing device. Additional description of modules 123-125 is provided herein ( e.g ., in description related to FIG. 3 ).
- Voice query application 122 may rely on various middleware, framework, operating system and/or firmware modules to handle voice input, including, for example, a voice to text module 126 and/or a semantic processor module 127.
- a voice to text module 126 and/or a semantic processor module 127 One or more (e.g ., all) aspects of modules 126 and/or 127 may be implemented as part of voice query application 122 and/or in another computing device, such as system 140.
- Voice to text module 126 receives an audio recording of voice input (e.g ., in the form of digital audio data), and converts the digital audio data into one or more text words or phrases (also referred to herein as tokens).
- voice to text module 126 is also a streaming module, such that voice input is converted to text on a token-by-token basis and in real time or near-real time, such that tokens may be output from module 126 effectively concurrently with a user's speech, and thus prior to a user enunciating a complete spoken request.
- Voice to text module 126 may rely on one or more acoustic and/or language models, which together model a relationship between an audio signal and phonetic units in a language, along with word sequences in the language.
- a single model may be used, while in other implementations, multiple models may be supported, e.g., to support multiple languages, multiple speakers, etc.
- semantic processor module 127 attempts to discern the semantics or meaning of the text output by voice to text module 126 for the purpose or formulating an appropriate response.
- the semantic processor module 127 may rely on one or more grammar models to map action text to particular computer-based actions and to identify entity text and/or other text that constrains the performance of such actions.
- a single model may be used, while in other implementations, multiple models may be supported, e.g., to support different computer-based actions or computer-based action domains (i.e., collections of related actions such as communication-related actions, search-related actions, audio/visual-related actions, calendar-related actions, device control-related actions, etc.)
- a grammar model (stored on computing device 120 and/or remote computing device(s)) may map computer-based actions to action terms of voice-based action queries such as the action terms “tell me more about”, “directions to”, “navigate to”, “watch”, “call”, “email”, “contact”, etc.
- the action term “tell me more” may be mapped to a search query and presentation of search results action; the action term “watch” may be mapped to a video viewing action performed via one or more of the applications 126; and the action term "call” may be mapped to a calling action preformed via one or more of the applications 126.
- some parameters may be directly received as voice input, while some parameters may be determined in other manners, e.g., based upon an indication of content most recently viewed on the computing device, a geographic location of the computing device, etc. For example, if a user were to say “remind me to email John when I get to work,” the "work” entity text may not be used to identify a particular location without additional information such as the user's assigning of a particular address as a work location. Also, for example, if a user were to say "tell me more about it", the "it” entity text may not be used to identify a particular entity without additional information such as a dominant entity of content currently being viewed on the computing device 120.
- suggested voice-based action query system 140 and/or another system may include complementary functionality for handling voice input, e.g., using a voice-based query processor that relies on various acoustic/language, grammar, and/or action models. In other implementations, however, no complementary functionality may be used.
- the computer-based actions that are initiated by semantic processor module 127 may be dependent on the rules available to semantic processor module 127. Also, the computer-based actions that are initiated by the semantic processor module 127 may be dependent on the applications 126 that are installed on the computing device 120 and/or the versions of the applications 126 that are installed on the computing device 120. For example, certain computer-based actions may only be performable by certain applications 126 and/or certain versions of applications 126. For instance, a "call" action may only be performable if a phone application is included in the applications 126.
- the suggested voice-based action query system 140 in some implementations may be implemented as a cloud-based service employing a cloud infrastructure, e.g., using a server farm or cluster of high performance computers running software suitable for handling high volumes of requests from multiple users' computing devices.
- the suggested voice-based action query system 140 is capable of querying one or more databases, such as entities and actions database 162, to locate information for generating suggested voice-based action queries.
- the suggested voice-based action query system 140 includes an entity determination module 142, an action determination module 144, and a suggested voice-based action query generation module 145.
- the suggested voice-based action query system 140 receives, from the computing device 120, an indication of the content recently accessed on the computing device 120 (e.g., the content currently being displayed by the computing device 120).
- the content may be accessed on the computing device 120 in one of the applications 126.
- the applications 126 may include one or more of a variety of applications that may be installed on the computing device 120 such as, for example, a web browser application, a personal assistant application, a business reviews application, a social networking application, a music application, a video application, and/or an application that provides an interface for exploring information about movies, tv shows, and other media.
- the indication of the content may comprise indications that are specific to the "view port" of the content on the computing device 120.
- text of the content and a screenshot of the content may be provided for only that portion of the content that is actively displayed on the computing device 120.
- the indication of the content may additionally and/or alternatively comprise indications for portions of the content that are not actively displayed on the computing device 120.
- metadata that is not actively displayed may be provided and/or text from other portions of the content that are not actively displayed may be provided.
- text that it is not displayed, but would be displayed by scrolling up or down may be provided.
- the entity determination module 142 determines one or more entities referenced in the content based on the indication of the content.
- An entity may be, for example, associated with one of a person, a location of interest, an address, a phone number, etc.
- determining the entity comprises identifying text associated with the entity based on position, format, frequency, and/or other property of the text in the content.
- the entity determination module 142 may identify multiple entities, such as a dominant entity in the content and one or more additional prominent entities in the content.
- the indication of the content may comprise text, metadata, images, tags applied to image(s), and/or screenshots of the content most recently viewed on the computing device and the entity determination module 142 may determine an entity referenced in the content based on such information.
- the indication of the content may include text and properties of the text in the content and entity determination module 142 may determine a dominant entity associated with the content based on position, format, frequency, and/or other property of the text in the content.
- a dominant entity in content refers to the entity in the content that is most prevalent in that content based on analysis of one or more properties of that content.
- the text "Blade Runner” may be identified as the dominant entity based on it appearing in larger font than other text, in a more prominent position than other text, and/or more frequently than other text.
- the text "Blade Runner” itself may be utilized as the dominant entity, or the entity determination module 142 may resolve a particular entity based on the text and with reference to one or more databases such as a knowledge graph.
- the indication of the content may comprise a URL or another identifier of the content
- the entity determination module 142 may utilize the identifier to determine an entity referenced in the content.
- the entity determination module 142 may access a database that maps identifiers of content to one or more entities referenced in the content (e.g ., a database that defines a dominant entity and/or other entities for each of a plurality of documents such as publicly accessible documents).
- the entity determination module 142 may utilize the identifier to locate the content and may directly analyze the content to determine an entity referenced in the content. For instance, where the identifier is a URL, the entity determination module 142 may access the URL and determine the entity based on content provided by accessing the URL.
- the entity determination module 142 may determine the entity based on text in the content that has one or more specific properties. For instance, the entity determination module 142 may determine a phone number as an entity based on one or more regular expressions that identify text that conforms to the format "XXX-XXX-XXX" or "(XXX) XXX-XXX". Also, for instance, the entity determination module 142 may determine an address as an entity based on text that is associated with metadata identifying it as an address and/or text that is in the form of an address ( e.g ., City, State Zip).
- an address e.g ., City, State Zip
- the entity determination module 142 may determine certain text as an entity based on it being in a "white list” of entities, being mapped to a "location of interest”, being mapped to a "famous person”, and/or based on other properties of the text.
- the action determination module 144 determines one or more computer-based actions that can be performed for the entity (or entities) determined by entity determination module 142. In some implementations, the action determination module 144 determines the computer-based actions based on the computer-based actions being mapped to the entity in one or more databases such as entities and actions database 162.
- the entities and actions database 162 includes a mapping of each of a plurality of entities to one or more computer-based actions associated with the entity.
- An action may be directly mapped with an entity and/or may be indirectly mapped to the entity via a mapping with a class of the entity. For example, the action of dialing a phone number may be mapped to each of a plurality of particular phone numbers and/or may be associated with the class of phone numbers in general.
- the action of playing a movie may be mapped to each of a plurality of movies, movies in general, and/or only movies that are available for on-demand viewing via one of the applications installed on the computing device 120.
- the action determination module 144 may optionally rank and/or filter the identified actions based on one or more factors such as, for example: strengths of association of the actions to the entity and/or a class of the entity; historical popularity of the actions in general; historical popularity of the actions for the application from which the content originated; whether the actions are performable via one or more applications 126 installed on the computing device 120; historical popularity of performance of the actions via one or more applications 126 installed on the computing device 120; etc.
- the entities and actions database 162 may include, for a mapping between an action and an entity or entity class, a strength of association for that mapping.
- the strength of association of an action to an entity may optionally be based on analysis of past voice search queries. For instance, a computer-based action of providing navigation may be more strongly associated with an entity class of restaurants than a computer-based action of calling if 1,000 analyzed past voice search queries generally conform to the format "navigate to [restaurant]" (where "[restaurant]" indicates reference to an entity that is a member of the class of restaurants), but only 100 analyzed past voice search queries generally conform to the format "call [restaurant]".
- the historical popularity of a computer-based action in general may be based on a frequency of appearance of terms that initiate the computer-based action in past voice search queries.
- the historical popularity of a computer-based action for an application from which the content originated may be based on a frequency of appearance of terms that initiate the computer-based action in past voice search queries that were issued while using the application and/or that were issued within a threshold time period of using the application. For instance, analysis of past voice search queries may indicate a computer-based action of calling is less popular for a web browser application than it is for an application that provides consumer reviews for businesses.
- the computer-based action identified by the action determination module 144 may be a computer identifier of the action that is not itself an action term that would initiate performance of the action if provided as a voice query.
- the computer identifier of the action of providing active navigation directions may be and an alpha and/or numerical identifier such as "ID_NAVIGATE", "42", and/or "AE5".
- the suggested voice-based action query generation module 145 generates one or more suggested voice-based action queries each based on one or more action terms to perform one of the computer-based actions determined by action determination module 144 and one or more entity terms that reference the entity determined by entity determination module 142.
- the suggested voice-based action query generation module 145 determines the action term(s) for a computer-based action based on a preferred language indicated by the computing device 120. For example, a computer identifier of the computer-based action may be determined by the action determination module 144 and a first term may be identified as the action term if the computing device 120 has a preferred language of English, whereas a second term would be identified as the action term if the computing device 120 had a preferred language of German.
- the entities and actions database 162 and/or other database may include, for a given computer-based action, action terms mapped to that action. Each of the action terms may further be mapped to a preferred language of the action term.
- the action determination module 144 may select an action term for a computer-based action in view of a preferred language based on identifying a mapping of the action term to the computer-based action and further identifying a mapping of the action term to the preferred language.
- the suggested voice-based action query generation module 145 further determines one or more terms that reference the identified entity. For example, where the identified entity is itself is a term, that term may be utilized. For instance, where "Restaurant A" is the entity, “Restaurant A” may be utilized as the entity term. In some implementations, a pronoun or other generic descriptor of the entity may be utilized. For example, where "Restaurant A" is the entity, the suggested voice-based action query generation module 145 may determine an entity term of "it” or "there". Also, for example, where a famous male is the entity, the suggested voice-based action query generation module 145 may determine an entity term of "he” or "him”.
- the particular generic descriptor selected may be determined based on a mapping of the generic descriptor to the entity and/or a class of the entity. Also, the particular generic descriptor may optionally be further selected to provide grammatical coherence with the action term. For instance, the entity term "there” may be selected for an action term of "navigate”, whereas “it” may be selected for action terms of "tell me more about”.
- the suggested voice-based action query generation module 145 determines the entity term based on a preferred language indicated by the computing device 120. For example, a first generic descriptor may be identified as the entity term if the computing device 120 has a preferred language of English, whereas a second generic descriptor would be identified as the entity term if the computing device 120 had a preferred language of German.
- the suggested voice-based action query system 140 provides the generated one or more suggested voice-based action queries to the voice query application 122 for presentation, by the voice query application 122, as a suggested voice-based action query for a voice-based query.
- they may optionally be provided with ranking information based on, for example, the ranking of the actions described with respect to action determination module 144.
- the computing device 120 may determine a display order of the suggested voice-based action queries based on the provided ranking information.
- the ranking information may optionally be an order in which the suggested voice-based action queries are transmitted or included in a data packet.
- the suggested voice-based action query system 140 may optionally provide the suggested voice-based action queries with metadata that more particularly identifies the entity.
- the metadata may be utilized by the computing device 120 to "replace" the generic descriptor with the more particular identification of the entity in performing the action.
- the more specific identification of the entity may be performed at the computing device 120 ( e.g., by semantic processor module 127), based on analysis of the most recently accessed content.
- the suggested voice-based action query system 140 provides the generated one or more suggested voice-based action queries to the voice query application 122 in response to receiving a voice-based query input indication from the computing device 120.
- the voice-based query input indication indicates receipt of input of the user, via the computing device 120, to initiate providing of a voice-based query via the computing device 120.
- the voice-based query input indication is the receiving of the indication of content from the computing device 120 and/or the receiving of other information in combination with the indication of content.
- the computing device 120 may only provide the indication of content in response to input of the user that initiates providing of a voice-based query.
- the voice-based query input indication may be received separate from the indication of content.
- the computing device 120 may provide the indication of content, then only provide the voice-based query input indication after a certain amount of time has passed, since receiving the voice-input indication, without receiving any spoken input from the user.
- the suggested voice-based action query system 140 may not be limited to generating suggested voice-based action queries.
- the suggested voice-based action query system 140 may also be capable of handling all or aspects of parsing submitted voice-based action queries, determining appropriate computer-based action(s) for submitted voice-based action queries, instructing one or more applications of computing device 120 to perform determined computer-based actions for submitted voice-based action queries, and/or performing one or more computer-based actions for submitted voice-based action queries.
- suggested voice-based action query system 140 and computing device 120 are illustrated as separate components in FIG. 1 , in other implementations one or more aspects of voice-based action query system 140 may be implemented on computing device 120, or vice versa.
- FIG. 2 illustrates an example of determining, in view of content being accessed on the computing device 120, at least one suggested voice-based action query 155 for presentation via the computing device 120.
- an indication of content 131 from the computing device 120 is provided to the entity determination module 142.
- the indication of content 131 may include text and properties of the text for content being viewed on the computing device 120 immediately prior to input of a user via the computing device 120 to initiate providing of a voice-based query.
- the user may be viewing the content in an application of the computing device 120 and the user may provide input to initiate providing of a voice-based query while that application is still active and displaying the content, and the content may be provided in response to the input.
- the user may be viewing the content on a first application of the computing device 120, may provide input to initiate providing of a voice-based query that causes additional content to be displayed (either supplanting the content of the first application or provided "over" portions of the content of the first application) by a second application (or the operating system) of the computing device 120, and the content recently displayed by the first application may be provided in response to the input.
- the computing device 120 may provide an indication of currently viewed content without first requiring input of a user to initiate providing of a voice-based query.
- the entity determination module 142 determines at least one entity 151 based on the indication of the content 131. For example, the entity determination module 142 may determine a dominant entity associated with the content based on position, format, frequency, and/or other property of the text in the content. For example, certain text may be identified as the dominant entity based on it appearing in the title of the content, in larger font than other text in the content, in a more prominent position than other text in the content, and more frequently than other text in the content.
- the entity determination module 142 provides the determined entity 151 to the action determination module 144.
- the action determination module 144 determines at least one computer-based action 152 that is mapped to the entity 151 in the entities and actions database 162.
- the action determination module 144 may determine the action 152 based on a direct mapping of the entity 151 to the action 152, or based on a mapping of the action 152 to a class of the entity 151, and a mapping of the class of the entity 151 to the action 152.
- Application information 132 from the computing device 120 may also be provided to the action determination module 144.
- the action determination module 144 may rank and/or filter computer-based actions based on the application information 132.
- the application information 132 may indicate one or more applications installed on the computing device (e.g ., applications 126) and/or versions for one or more applications installed on the computing device ( e.g ., application 126 and/or 122).
- entities and actions database 162 may include, for each of a plurality of computer-based actions, data defining one or more applications and/or application versions (for applications 126 and/or 122) via which the computer-based action may be performed.
- the action determination module 144 may utilize such data to filter out one or more computer-based actions that are not compatible with the computing device 120 based on the application information 132.
- the application information 132 may indicate which application was generating the content indicated by indication of content 131.
- the entities and actions database 162 may include the historical popularity of one or more candidate computer-based actions with respect to that application from which the content originated (e.g ., based on a frequency of appearance of terms that initiate the computer-based action in past voice search queries that were issued while using the application and/or that were issued within a threshold time period of using the application).
- the action determination module 144 may utilize such historical popularity to select the action 152 and/or rank the action 152 relative to other selected actions.
- the action determination module 144 provides the entity 151 and the action 152 to the suggested voice-based action query generation module 145 (optionally with other determined entities and/or actions).
- the suggested voice-based action query generation module 145 generates a suggested voice-based action query 155 based on one or more action terms to perform the action 151 and one or more entity terms that reference the entity 152.
- a preferred language 133 from the computing device 120 may also be provided to the suggested voice-based action query generation module 145.
- the suggested voice-based action query generation module 145 determines the action term(s) for a computer-based action and/or the entity terms based on a preferred language indicated by the computing device 120.
- the suggested voice-based action query generation module 145 provides the suggested voice-based action query 155 to the computing device 120.
- the suggested voice-based action query 155 may be provided to the computing device 120 as a text string that includes the one or more action terms and the one or more entity terms.
- the suggested voice-based action generation module 145 provides the generated one or more suggested voice-based action queries to the computing device 120 in response to receiving a voice-based query input indication from the computing device 120.
- the voice-based query input indication is the receiving of the indication of content 131 from the computing device 120 and/or the receiving of other information in combination with the indication of content 131.
- the suggested voice-based action query generation module 145 provides annotation data with the suggested voice-based action query 155.
- the annotation data is data that may be displayed with the suggested voice-based action query 155 to help clarify the suggested voice-based action query 155, but doesn't constitute the suggested query itself.
- a pronoun is used as the entity term of the suggested voice-based action query 155
- an image of the entity and/or a more specific alias of the entity may also be provided for display visually set off from the voice-based action query 155 (e.g., provided in parentheses and/or positionally offset).
- FIG. 3 illustrates an example of receiving at least one suggested voice-based action query at a computing device and providing the suggested voice-based action query as a suggestion in response to input to initiate providing of a voice-based query.
- a voice-based query indication input 110 is received at voice action module 123.
- Voice-based query indication input 110 may include, for example, a user selecting a voice query icon via a graphical user interface, the user speaking a phrase that initiates a voice-based query (e.g ., "OK computer"), the user actuating a touch-sensitive hardware element of the computing device 120 or in communication with the computing device 120 ( e.g., a mechanical button, a capacitive button), and/or performing a gesture in view of a camera or other sensor of the computing device 120.
- a voice-based query icon via a graphical user interface
- the user speaking a phrase that initiates a voice-based query e.g ., "OK computer”
- the voice action module 123 monitors for voice input in response to the voice-based query initiation input 110 and also sends a request voice-based action queries command 135 to interface module 124.
- the interface module 124 provides information 130 to the suggested voice-based action query system 140, such as an indication of content 131 most recently accessed via one of the applications 126, indications of a preferred language of the computing device 120, information related to one or more applications 126 of computing device 120, and/or voice-based query input indications.
- the indication of content 131 may be the content displayed by one of the applications 126 most recently relative to receiving the voice-based query initiation input 110.
- the interface module 124 further receives a suggested voice-based action query 155 from suggested voice-based action query system 140.
- the suggested voice-based action query 155 is in response to the information 130 provided by the interface module 124 and may optionally be based on one or more aspects of the information 130.
- the interface module 124 provides the suggested voice-based action query 155 to the render/synchronization module 125.
- the voice action module 123 provides a suggested voice-based action queries command 137 to the render/synchronization module 125.
- the render/synchronization module 125 presents (e.g ., displays) the suggested voice-based action query 155 as a suggestion for the voice query initiated by the voice-based query initiation input 110.
- the voice action module 123 provides the command 137 based on the voice-based query initiation input 110 being followed by an indication of a need for a suggested voice-based action query.
- an indication of the need for a suggested voice-based action query may include the lack of any spoken input from a user within a threshold amount of time following the user input initiating the voice-based query.
- a suggested voice-based action query may be presented in response to the user not providing any spoken input within four seconds (or other threshold amount of time) of the user input initiating the voice-based query.
- the command 137 may additionally and/or alternatively be provided in response to other indications of the need for a suggested voice-based action query.
- other indications may include one or more phrases that can be spoken by the user (e.g ., "tell me what I can do"), selection of a user interface element provided for requesting suggested voice-based action queries, and/or detecting of at least a threshold noise level following the voice-based query initiation input 110 ( e.g ., detecting that the environment is "too loud" to properly process spoken input).
- the render/synchronization module 125 may present the suggested voice-based action query as a suggestion for the voice query without receiving the command 137.
- FIG. 4 is a flowchart illustrating an example method 400 of determining, in view of content being accessed on a computing device, at least one suggested voice-based action query for presentation via the computing device.
- the operations of the flow chart are described with reference to a system that performs the operations. This system may include various components of various computer systems, such as suggested voice-based action query system 140.
- operations of method 400 are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted or added.
- the system receives an indication of content recently viewed on a computing device.
- the indication of content may include text and properties of the text for content currently being displayed by the computing device.
- the indication of content may be provided by the computing device in response to input of a user via the computing device to initiate providing of a voice-based query.
- the system determines, based on the indication of the content, an entity referenced in the content. For example, where the indication of the content includes text and properties of the text, the system may determine a dominant entity associated with the content based on position, format, frequency, and/or other property of the text in the content. For example, certain text may be identified as the dominant entity based on it appearing in the title of the content, in larger font than other text in the content, in a more prominent position than other text in the content, and more frequently than other text in the content.
- the system determines a computer-based action that can be mapped to the entity. For example, the system may determine at least one computer-based action that is mapped to the entity in the entities and actions database 162. In some implementations, the system may rank and/or filter computer-based actions based on one or more factors such as, for example: strengths of association of the actions to the entity and/or a class of the entity; historical popularity of the actions in general; historical popularity of the actions for the application from which the content originated; whether the actions are performable via one or more applications installed on the computing device; historical popularity of performance of the actions via one or more applications installed on the computing device; etc.
- factors such as, for example: strengths of association of the actions to the entity and/or a class of the entity; historical popularity of the actions in general; historical popularity of the actions for the application from which the content originated; whether the actions are performable via one or more applications installed on the computing device; historical popularity of performance of the actions via one or more applications installed on the computing device; etc.
- the system generates a suggested voice-based action query that includes an action term mapped to the computer-based action and an entity term mapped to the entity.
- the system determines the action term(s) and/or the entity terms based on a preferred language indicated by information received from the computing device.
- the system provides the suggested voice-based action query for display as a suggestion for a voice query.
- the system provides the generated one or more suggested voice-based action queries to the computing device for display as a suggestion for a voice query in response to receiving a voice-based query input indication from the computing device.
- the voice-based query input indication is the receiving of the indication of content at block 402 and/or the receiving of other information from the computing device.
- FIG. 5 illustrates an example method of receiving at least one suggested voice-based action query at a computing device and providing the suggested voice-based action query as a suggestion in response to input to initiate providing of a voice-based query.
- This system may include various components of various computer systems, such as voice query application 122 of computing device 120.
- operations of method 500 are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted or added.
- the system receives input to initiate providing of a voice-based query.
- the input may include, for example, a user selecting a voice query icon via a graphical user interface, the user speaking a phrase that initiates a voice-based query (e.g ., "OK computer"), the user actuating a touch-sensitive hardware element, and/or performing a gesture.
- the system identifies content displayed most recently relative to the input at block 502. For example, the system may provide an identifier of the content or text, metadata, images, tags applied to image(s), and/or screenshots of the content displayed by the computing device when, or just before, the input was received at block 502.
- the system provides an indication of the content identified at block 504.
- the system may provide the indication of the content to suggested voice-based action query system 140.
- the system receives a suggested voice-based action query that is based on the indication of block 506 and that includes an action term and an entity term.
- the system may receive a suggested voice-based action query from the voice-based action query system 140 in response to providing the indication of the content (and optionally other information) at block 506.
- the system determines whether voice input has been received within a threshold amount of time. If the answer is yes, the system proceeds to block 520 and performs a computer-based action based on the received voice input. If the answer is no, the system proceeds to step 510 and provides the suggested voice-based action query as a suggestion for the voice-based query.
- the system determines whether the suggested voice-based action query has been selected. If the answer is yes, the system proceeds to block 514 and performs a computer-based action based on the suggested voice-based action query. If the answer is no, the system proceeds to block 516 and performs a further action. For example if the user provides input to remove the suggested voice-based action query from the display, the answer is no and the system will respond to the provided input at block 516.
- FIG. 6A illustrates an example graphical user interface 680A showing content displayed in an application of a computing device.
- the graphical user interface 680A may be displayed on a mobile phone computing device.
- the content is displayed in the graphical user interface 680A by an application of the computing device, such as an application that provides information and reviews for restaurants.
- the particular content is focused on information and reviews for a fictional restaurant "Up and Down Burger Bar.”
- the graphical user interface 680A also includes graphical interface elements 681, 682, and 683 that may each, when actuated, cause one or more actions to be performed. Additional and/or alternative graphical and/or other (e.g ., mechanical) interface elements may be provided.
- FIG. 6B illustrates an example graphical user interface 680B for displaying suggested voice-based action queries 685A-C that are generated based on the content of FIG. 6A .
- FIG. 6B illustrates an example of the user, while provided with the display of FIG. 6A , providing a voice-based query initiation input.
- the voice-based query initiation input may be, for example, speaking one or more terms or selecting an interface element (e.g ., actuating one or more of elements 681-683 in a certain manner or actuating a mechanical interface element).
- Providing the voice-based query initiation input caused a voice query interface 684 to be displayed with the content of FIG. 6A .
- the voice query interface 684 includes an icon of a microphone to indicate a voice query and also includes the text "Waiting " to indicate to the user that the computing device is awaiting spoken input from the user.
- the suggested voice-based action queries 685A-C are displayed below the voice query interface 684 as individual "cards” that may be selected ( e.g ., "tapped” or spoken) by the user.
- the suggested voice-based action queries 685A-C are based on the content of FIG. 6A and may be determined, for example, as described herein with respect to suggested voice-based action query system 140, FIG. 2 , and/or FIG. 4 . For example, a screenshot and/or text from FIG.
- suggested voice-based action query 685A may have been provided as the indication of content and the suggested voice-based action queries 685A-C received in response. It is noted that suggested voice-based action query 685A is provided with an annotation "[Up and Down Burger Bar]" to provide the user an indication that speaking "Navigate there” or tapping the suggested voice-based action query 685A will result in a computer-based action of providing navigation directions to "Up and Down Burger Bar”.
- FIG. 6A Much of the content of FIG. 6A is still displayed in FIG. 6B , with other of the content being "hidden” under the voice query interface 684 and the voice-based action queries 685A-C.
- the voice query interface 684 and/or one or more of the voice-based action queries 685A-C may be at least partially transparent to enable viewing of the content that is "hidden” in FIG. 6B .
- the suggested voice-based action queries 685A-C may not have been displayed in the graphical user interface 680B until identification of a need for a suggested voice-based action query following the voice-based query initiation input.
- the voice query interface 684 may have been initially displayed without the suggested voice-based action queries 685A-C and the suggested voice-based action queries 685A-C displayed only upon determining a lack of any spoken input from a user within a threshold amount of time following the user input initiating the voice-based query.
- the voice query interface 684 may have been initially displayed without the suggested voice-based action queries 685A-C and the suggested voice-based action queries 685A-C displayed only upon receiving specific spoken input requesting suggestions and/or determining that an ambient noise level is greater than a threshold noise level for receiving and accurately parsing spoken input.
- FIG. 6C illustrates an example graphical user interface 680C for displaying suggested voice-based action queries "Navigate there", “Make reservations there", and “Tell me more about it” that are generated based on the content of FIG. 6A .
- the suggested voice-based action queries of FIG. 6C are displayed in a card 687 based on the content of FIG. 6A and may be determined, for example, as described herein with respect to suggested voice-based action query system 140, FIG. 2 , and/or FIG. 4 .
- a screenshot and/or text from FIG. 6A may have been provided as the indication of content and the suggested voice-based action queries received in response.
- the card 687 is displayed in an interface element 686 that shows ( e.g ., based on the phrase "ON SCREEN:”) that the card 687 (and optionally other non-displayed cards) is particularly tailored to the content of FIG. 6A (which remains partially displayed in FIG. 6C ).
- FIG. 6C illustrates an example of the user, while provided with the display of FIG. 6A , providing a request for suggested voice-based query suggestions related to content on the screen (without necessarily providing a voice-based query initiation input).
- the request for suggested voice-based query suggestions may be "touching" graphical interface element 682 and "swiping up" and/or speaking one or more terms.
- Providing the request for suggested voice-based query suggestions caused the interface element 686 and the card 687 to be displayed over portions of the content of FIG. 6A .
- Selection of one of the suggested voice-based action queries of FIG. 6C (by "tapping" or speaking (optionally after a voice-based query initiation input)) will cause the computing device to initiate performance of a computer-based action based on the selected voice-based action query.
- FIG. 6C Much of the content of FIG. 6A is still displayed in FIG. 6C , with other of the content being "hidden” under the interface element 686 and the card 687.
- the interface element 686 and/or the card 687 may be at least partially transparent to enable viewing of the content that is "hidden” in FIG. 6C .
- FIG. 7A illustrates an example graphical user interface 780A showing content displayed in an application of a computing device.
- the graphical user interface 780A may be displayed in an instant messaging application of a mobile phone computing device.
- the particular content is a conversation between a user of the mobile phone and another user "Bob".
- Bob has asked the user if he wants to head to Up and Down Burger Bar for dinner.
- the graphical user interface 780A also includes graphical interface elements 681, 682, and 683 that may each, when actuated, cause one or more actions to be performed.
- FIG. 7B illustrates an example graphical user interface 780B for displaying suggested voice-based action queries 785A and 785B that are generated based on the content of FIG. 7A .
- FIG. 7B illustrates an example of the user, while provided with the display of FIG. 7A , providing a voice-based query initiation input.
- the voice-based query initiation input may be, for example, speaking one or more terms or selecting an interface element (e.g ., actuating one or more of elements 681-683 in a certain manner or actuating a mechanical interface element).
- Providing the voice-based query initiation input caused a voice query interface 784 to be displayed with the content of FIG. 7A .
- the voice query interface 784 includes an icon of a microphone to indicate a voice query and also includes the text "Waiting " to indicate to the user that the computing device is awaiting spoken input from the user.
- the suggested voice-based action queries 785A and 785B are displayed below the voice query interface 784 as individual "cards” that may be selected ( e.g ., "tapped” or spoken) by the user.
- the suggested voice-based action queries 785A and 785B are based on the content of FIG. 7A and may be determined, for example, as described herein with respect to suggested voice-based action query system 140, FIG. 2 , and/or FIG. 4 . For example, a screenshot and/or text from FIG.
- suggested voice-based action query 785A is provided with a suggested time of "7 PM" for making a reservation.
- the suggested time may be determined based on the context of FIG. 7A ("dinner"), past user reservation history, and/or arbitrarily to provide the user an indication that spoken input can be utilized to make a reservation for "Up and Down Burger Bar” at a desired time.
- FIG. 8 is a block diagram of an example computing device 810 that may optionally be utilized to perform one or more aspects of techniques described herein.
- computing device 120 and/or suggested voice-based action query system 140 may comprise one or more components of the example computing device 810.
- Computing device 810 typically includes at least one processor 814 which communicates with a number of peripheral devices via bus subsystem 812. These peripheral devices may include a storage subsystem 824, including, for example, a memory subsystem 825 and a file storage subsystem 826, user interface output devices 820, user interface input devices 822, and a network interface subsystem 816. The input and output devices allow user interaction with computing device 810.
- Network interface subsystem 816 provides an interface to outside networks and is coupled to corresponding interface devices in other computing devices.
- User interface input devices 822 may include a keyboard, pointing devices such as a mouse, trackball, touchpad, or graphics tablet, a scanner, a touchscreen incorporated into the display, audio input devices such as voice recognition systems, microphones, and/or other types of input devices.
- pointing devices such as a mouse, trackball, touchpad, or graphics tablet
- audio input devices such as voice recognition systems, microphones, and/or other types of input devices.
- use of the term "input device” is intended to include all possible types of devices and ways to input information into computing device 810 or onto a communication network.
- User interface output devices 820 may include a display subsystem, a printer, a fax machine, or non-visual displays such as audio output devices.
- the display subsystem may include a cathode ray tube (CRT), a flat-panel device such as a liquid crystal display (LCD), a projection device, or some other mechanism for creating a visible image.
- the display subsystem may also provide non-visual display such as via audio output devices.
- output device is intended to include all possible types of devices and ways to output information from computing device 810 to the user or to another machine or computing device.
- Storage subsystem 824 stores programming and data constructs that provide the functionality of some or all of the modules described herein.
- the storage subsystem 824 may include the logic to perform selected aspects of the method of FIGS. 3 and/or 4.
- Memory 825 used in the storage subsystem 824 can include a number of memories including a main random access memory (RAM) 830 for storage of instructions and data during program execution and a read only memory (ROM) 832 in which fixed instructions are stored.
- a file storage subsystem 826 can provide persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive along with associated removable media, a CD-ROM drive, an optical drive, or removable media cartridges.
- the modules implementing the functionality of certain implementations may be stored by file storage subsystem 826 in the storage subsystem 824, or in other machines accessible by the processor(s) 814.
- Bus subsystem 812 provides a mechanism for letting the various components and subsystems of computing device 810 communicate with each other as intended. Although bus subsystem 812 is shown schematically as a single bus, alternative implementations of the bus subsystem may use multiple busses.
- Computing device 810 can be of varying types including a workstation, server, computing cluster, blade server, server farm, or any other data processing system or computing device. Due to the ever-changing nature of computers and networks, the description of computing device 810 depicted in Fig. 8 is intended only as a specific example for purposes of illustrating some implementations. Many other configurations of computing device 810 are possible having more or fewer components than the computing device depicted in Fig. 8 .
- the users may be provided with an opportunity to control whether programs or features collect user information (e.g ., information about a user's social network, social actions or activities, profession, a user's preferences, or a user's current geographic location), or to control whether and/or how to receive content from the content server that may be more relevant to the user.
- user information e.g ., information about a user's social network, social actions or activities, profession, a user's preferences, or a user's current geographic location
- certain data may be treated in one or more ways before it is stored or used, so that personal identifiable information is removed.
- a user's identity may be treated so that no personal identifiable information can be determined for the user, or a user's geographic location may be generalized where geographic location information is obtained (such as to a city, ZIP code, or state level), so that a particular geographic location of a user cannot be determined.
- geographic location information such as to a city, ZIP code, or state level
- the user may have control over how information is collected about the user and/or used.
Description
- Voice query applications are increasingly being used in the control of computing devices. One use of a voice query application is with portable computing devices such as mobile phones, watches, tablet computers, head-mounted devices, virtual or augmented reality devices, vehicular electronic systems (e.g., automotive systems that incorporate navigation and audio capabilities), etc. Many voice query applications have evolved to respond to natural language requests and/or manage back-and-forth dialogs or conversations with users. Many voice query applications incorporate both an initial speech-to-text conversion that converts an audio recording of a human voice to text, and a semantic analysis that analyzes the text in an attempt to determine the meaning of a user's request. Based upon a determined meaning of a user's spoken input, a computer-based action may be undertaken such as performing a search, providing driving directions, or otherwise controlling one or more applications of a computing device. An example of a voice-query application is provided in
US 2013/091463 A1 . - The prevalence and/or capabilities of voice query applications has increased and may continue to increase. However, in many instances a user may not fully appreciate the capabilities afforded by the voice query application of a computing device being utilized by the user.
- Some implementations of the technology of this specification may facilitate user discovery of various voice-based action queries that can be spoken to initiate computer-based actions, such as voice-based action queries that can be provided as spoken input to a computing device to initiate computer-based actions that are particularized to content being viewed or otherwise consumed by the user on the computing device. According to the present invention, a method, a computing device and at least one non-transitory computer-readable medium according to independent claims 1, 10 and 11 are proposed, respectively.
- It should be appreciated that all combinations of the foregoing concepts and additional concepts described in greater detail herein are contemplated as being part of the subject matter disclosed herein. For example, all combinations of claimed subject matter appearing at the end of this disclosure are contemplated as being part of the subject matter disclosed herein.
-
-
FIG. 1 is a block diagram of an example environment in which techniques disclosed herein may be implemented. -
FIG. 2 illustrates an example of determining, in view of content being accessed on a computing device, at least one suggested voice-based action query for presentation via the computing device. -
FIG. 3 illustrates an example of receiving at least one suggested voice-based action query at a computing device and providing the suggested voice-based action query as a suggestion in response to input to initiate providing of a voice-based query. -
FIG. 4 is a flowchart illustrating an example method of determining, in view of content being accessed on a computing device, at least one suggested voice-based action query for presentation via the computing device. -
FIG. 5 illustrates an example method of receiving at least one suggested voice-based action query at a computing device and providing the suggested voice-based action query as a suggestion in response to input to initiate providing of a voice-based query. -
FIG. 6A illustrates an example graphical user interface showing content displayed in an application of a computing device. -
FIG. 6B illustrates an example graphical user interface for displaying suggested voice-based action queries that are generated based on the content ofFIG. 6A . -
FIG. 6C illustrates another example graphical user interface for displaying suggested voice-based action queries that are generated based on the content ofFIG. 6A . -
FIG. 7A illustrates another example graphical user interface showing content displayed in an application of a computing device. -
FIG. 7B illustrates an example graphical user interface for displaying suggested voice-based action queries that are generated based on the content ofFIG. 7A . -
FIG. 8 illustrates an example architecture of a computing device. - In implementations described herein, techniques are disclosed for generating one or more suggested voice-based action queries in view of content being accessed on a computing device. Techniques are also disclosed that facilitate user discovery of various voice-based action queries that can be spoken to initiate computer-based actions, such as voice-based action queries that can be provided as spoken input to a computing device to initiate computer-based actions that are particularized to content currently being accessed by the user on the computing device. Further details regarding selected implementations are discussed hereinafter. It will be appreciated however that other implementations are contemplated so the implementations disclosed herein are not exclusive.
- As one example of implementations described herein, assume a user is using an application on the user's phone that provides detailed information related to selected movies, actresses, actors, etc. Further assume the user has used the application to navigate to content about the film Blade Runner such as content that includes the title of the film, a synopsis of the film, actors in the film, etc. The user may initiate a voice query via the phone and an indication of the content presented in the application may be sent by the user's phone to a server. The server may utilize the indication of content to determine a dominant entity of the content is the entity related to the movie "Blade Runner", may determine computer-based actions related to the movie, and may determine suggested voice-based action queries that will cause performance of those actions for the movie. For example, suggested voice-based action queries of "how can I watch it", "watch it", and "tell me more about it" may be determined. The suggested voice-based action queries may be provided to the user's phone for presentation to the user. For example, the suggested voice-based action queries may be graphically displayed in one or more information "cards" and/or in a "drop down menu" near a voice query graphical interface, optionally along with text such as "try speaking any one of the following."
- In some implementations, the suggested voice-based action queries are not presented to the user via the user's phone until after at least a threshold period of time has passed since the voice query was initiated. In some implementations, the user may select one of the suggested voice-based action queries without speaking (e.g., via tapping it) to execute the query for performance of the associated action and/or the user may speak one of the suggested voice-based action queries to execute the query for performance of the associated action.
- Generally, a voice-based action query that initiates performance of a computer-based action is a query that includes an action term mapped to the computer-based action and an entity term that is the focus of the action. In some implementations, the action term may be a verb and the entity term may be a noun or pronoun. When executed, a voice-based action query causes performance of a computer-based action mapped to the action term and causes performance of the computer-based action in view of the entity mapped to the entity term.
- For example, a voice-based action query of "tell me more about blade runner" may cause a search query to be submitted that is particularized to "blade runner", and search results to be provided responsive to the search query. Also, for example, a voice-based action query of "navigate to a coffee shop" may cause a navigation application to provide the user with active navigation directions to a nearby coffee shop. As yet another example, a voice-based action query of "call restaurant A" may cause a phone dialing application to prepopulate and/or automatically call the phone number for Restaurant A.
- In some implementations, a computer-based action may be mapped to one or more computer applications that can perform the action and execution of a voice-based action query that includes a term associated with the action may cause at least one of those computer applications to automatically perform the action, or present an option for performance of the action. For example, an action associated with "calling" may be mapped to a phone application, an action associated with "making a restaurant reservation" may be mapped to a restaurant reservation application, etc. In some implementations, a voice query application may perform some of an action, the entirety of an action, and/or may process a voice-based action query to facilitate performance of an action by another application (e.g., parse the query and interface with another application based on the parsed query and an API of the application).
- Now turning to
FIG. 1 , an example environment in which techniques disclosed herein may be implemented is illustrated. The example environment includes a voice-enabledcomputing device 120, a suggested voice-basedaction query system 140, an entities andactions database 162, and anetwork 101. Thenetwork 101 may comprise one or more networks such as a local area network (LAN) or wide area network (WAN) (e.g., the Internet). In some implementations, the voice-enabledcomputing device 120 is a portable computing device such as cellular phone, tablet computer, laptop computer, watch, head-mounted device (e.g., glasses), virtual or augmented reality device, other wearable device, an audio/video system, a navigation system, automotive and other vehicular system, etc. - In the implementation of
FIG. 1 , voice input received by voice-enabledcomputing device 120 is processed by avoice query application 122, which in some implementations may be a search application that includes voice query functionality. In some implementations,voice query application 122 may be a stand-alone application. In some implementations,voice query application 122 may be integrated, in whole or in part, as part of the operating system or firmware of thecomputing device 120. -
Voice query application 122 in the illustrated implementation includes avoice action module 123, aninterface module 124, and a render/synchronization module 125.Voice action module 123 monitors for voice input directed to thevoice query application 122, coordinates the analysis of received voice input, and coordinates performance of one or more computer-based actions that are responsive to the received voice input. As described herein,voice action module 123 further coordinates the providing of information to the suggested voice-basedaction query system 140 and the presentation of suggested voice-based action queries received from thesystem 140 as suggestions for a voice query. -
Interface module 124 provides an interface with suggested voice-basedaction query system 140 and/or other systems. Theinterface module 124 provides information to the suggested voice-basedaction query system 140, such as indications of content accessed on thecomputing device 120, indications of a preferred language of thecomputing device 120, information related to one ormore applications 126 ofcomputing device 120, and/or voice-based query input indications. Theinterface module 124 further receives suggested voice-based action queries from suggested voice-basedaction query system 140 in response to information provided by theinterface module 124. - Render/
synchronization module 125 manages the presenting of suggested voice-based action queries to a user, e.g., via a visual display, spoken audio, or other feedback interface suitable for a particular voice-enabled device. In addition, in some implementations,module 125 also handles synchronization with other online services, such as when a response or action affects data maintained for the user in another online service (e.g., where voice input requests creation of an appointment that is maintained in a cloud-based calendar). - In some implementations, all or aspects of one or more of the modules 123-125 may be combined and/or implemented in another module. For example, in some implementations one or more aspects of
module 124 may be incorporated inmodule 123. Also, although modules 123-125 are illustrated in the example environment ofFIG. 1 as being provided oncomputing device 120, this is not meant to be limiting. In other implementations, all or aspects of one or more of the modules 123-125 may be implemented on suggested voice-basedaction query system 140 and/or another computing device. Additional description of modules 123-125 is provided herein (e.g., in description related toFIG. 3 ). -
Voice query application 122 may rely on various middleware, framework, operating system and/or firmware modules to handle voice input, including, for example, a voice totext module 126 and/or asemantic processor module 127. One or more (e.g., all) aspects ofmodules 126 and/or 127 may be implemented as part ofvoice query application 122 and/or in another computing device, such assystem 140. Voice totext module 126 receives an audio recording of voice input (e.g., in the form of digital audio data), and converts the digital audio data into one or more text words or phrases (also referred to herein as tokens). In some implementations, voice totext module 126 is also a streaming module, such that voice input is converted to text on a token-by-token basis and in real time or near-real time, such that tokens may be output frommodule 126 effectively concurrently with a user's speech, and thus prior to a user enunciating a complete spoken request. Voice totext module 126 may rely on one or more acoustic and/or language models, which together model a relationship between an audio signal and phonetic units in a language, along with word sequences in the language. In some implementations, a single model may be used, while in other implementations, multiple models may be supported, e.g., to support multiple languages, multiple speakers, etc. - Whereas voice to
text module 126 converts speech to text,semantic processor module 127 attempts to discern the semantics or meaning of the text output by voice to textmodule 126 for the purpose or formulating an appropriate response. For example, thesemantic processor module 127 may rely on one or more grammar models to map action text to particular computer-based actions and to identify entity text and/or other text that constrains the performance of such actions. In some implementations, a single model may be used, while in other implementations, multiple models may be supported, e.g., to support different computer-based actions or computer-based action domains (i.e., collections of related actions such as communication-related actions, search-related actions, audio/visual-related actions, calendar-related actions, device control-related actions, etc.) - As an example, a grammar model (stored on
computing device 120 and/or remote computing device(s)) may map computer-based actions to action terms of voice-based action queries such as the action terms "tell me more about", "directions to", "navigate to", "watch", "call", "email", "contact", etc. For instance, the action term "tell me more" may be mapped to a search query and presentation of search results action; the action term "watch" may be mapped to a video viewing action performed via one or more of theapplications 126; and the action term "call" may be mapped to a calling action preformed via one or more of theapplications 126. - The grammar model(s) and/or other models relied upon by
semantic processor module 127 may incorporate various rules to initiate performance of a computer-based action based on text input provided by voice to textmodule 126. In some implementations, for example, actions may be defined as functions F such that F(i T) = AU, where T represents the type of the input interpretation and U represents the type of output action. F may therefore include a plurality of input pairs (T, U) that are mapped to one another, e.g., as f(i t) = au, where i t is an input proto variable of type t, and au is an output modular argument or parameter of type u. It will be appreciated that some parameters may be directly received as voice input, while some parameters may be determined in other manners, e.g., based upon an indication of content most recently viewed on the computing device, a geographic location of the computing device, etc. For example, if a user were to say "remind me to email John when I get to work," the "work" entity text may not be used to identify a particular location without additional information such as the user's assigning of a particular address as a work location. Also, for example, if a user were to say "tell me more about it", the "it" entity text may not be used to identify a particular entity without additional information such as a dominant entity of content currently being viewed on thecomputing device 120. In some implementations, suggested voice-basedaction query system 140 and/or another system may include complementary functionality for handling voice input, e.g., using a voice-based query processor that relies on various acoustic/language, grammar, and/or action models. In other implementations, however, no complementary functionality may be used. - In some implementations, the computer-based actions that are initiated by
semantic processor module 127 may be dependent on the rules available tosemantic processor module 127. Also, the computer-based actions that are initiated by thesemantic processor module 127 may be dependent on theapplications 126 that are installed on thecomputing device 120 and/or the versions of theapplications 126 that are installed on thecomputing device 120. For example, certain computer-based actions may only be performable bycertain applications 126 and/or certain versions ofapplications 126. For instance, a "call" action may only be performable if a phone application is included in theapplications 126. - The suggested voice-based
action query system 140 in some implementations may be implemented as a cloud-based service employing a cloud infrastructure, e.g., using a server farm or cluster of high performance computers running software suitable for handling high volumes of requests from multiple users' computing devices. The suggested voice-basedaction query system 140 is capable of querying one or more databases, such as entities andactions database 162, to locate information for generating suggested voice-based action queries. The suggested voice-basedaction query system 140 includes anentity determination module 142, anaction determination module 144, and a suggested voice-based actionquery generation module 145. - The suggested voice-based
action query system 140 receives, from thecomputing device 120, an indication of the content recently accessed on the computing device 120 (e.g., the content currently being displayed by the computing device 120). The content may be accessed on thecomputing device 120 in one of theapplications 126. Theapplications 126 may include one or more of a variety of applications that may be installed on thecomputing device 120 such as, for example, a web browser application, a personal assistant application, a business reviews application, a social networking application, a music application, a video application, and/or an application that provides an interface for exploring information about movies, tv shows, and other media. In some implementations, the indication of the content may comprise indications that are specific to the "view port" of the content on thecomputing device 120. For example, text of the content and a screenshot of the content may be provided for only that portion of the content that is actively displayed on thecomputing device 120. In some implementations, the indication of the content may additionally and/or alternatively comprise indications for portions of the content that are not actively displayed on thecomputing device 120. For example, metadata that is not actively displayed may be provided and/or text from other portions of the content that are not actively displayed may be provided. For instance, text that it is not displayed, but would be displayed by scrolling up or down, may be provided. - The
entity determination module 142 determines one or more entities referenced in the content based on the indication of the content. An entity may be, for example, associated with one of a person, a location of interest, an address, a phone number, etc. In some implementations, determining the entity comprises identifying text associated with the entity based on position, format, frequency, and/or other property of the text in the content. In some implementations, theentity determination module 142 may identify multiple entities, such as a dominant entity in the content and one or more additional prominent entities in the content. - As one example, the indication of the content may comprise text, metadata, images, tags applied to image(s), and/or screenshots of the content most recently viewed on the computing device and the
entity determination module 142 may determine an entity referenced in the content based on such information. For instance, the indication of the content may include text and properties of the text in the content andentity determination module 142 may determine a dominant entity associated with the content based on position, format, frequency, and/or other property of the text in the content. As used herein, a dominant entity in content refers to the entity in the content that is most prevalent in that content based on analysis of one or more properties of that content. For example, for content about the movie Blade Runner, the text "Blade Runner" may be identified as the dominant entity based on it appearing in larger font than other text, in a more prominent position than other text, and/or more frequently than other text. The text "Blade Runner" itself may be utilized as the dominant entity, or theentity determination module 142 may resolve a particular entity based on the text and with reference to one or more databases such as a knowledge graph. - As another example, the indication of the content may comprise a URL or another identifier of the content, and the
entity determination module 142 may utilize the identifier to determine an entity referenced in the content. For example, theentity determination module 142 may access a database that maps identifiers of content to one or more entities referenced in the content (e.g., a database that defines a dominant entity and/or other entities for each of a plurality of documents such as publicly accessible documents). Also, for example, theentity determination module 142 may utilize the identifier to locate the content and may directly analyze the content to determine an entity referenced in the content. For instance, where the identifier is a URL, theentity determination module 142 may access the URL and determine the entity based on content provided by accessing the URL. - As yet another example of determining an entity based on an indication of content, the
entity determination module 142 may determine the entity based on text in the content that has one or more specific properties. For instance, theentity determination module 142 may determine a phone number as an entity based on one or more regular expressions that identify text that conforms to the format "XXX-XXX-XXXX" or "(XXX) XXX-XXX". Also, for instance, theentity determination module 142 may determine an address as an entity based on text that is associated with metadata identifying it as an address and/or text that is in the form of an address (e.g., City, State Zip). Also, for instance, theentity determination module 142 may determine certain text as an entity based on it being in a "white list" of entities, being mapped to a "location of interest", being mapped to a "famous person", and/or based on other properties of the text. - The
action determination module 144 determines one or more computer-based actions that can be performed for the entity (or entities) determined byentity determination module 142. In some implementations, theaction determination module 144 determines the computer-based actions based on the computer-based actions being mapped to the entity in one or more databases such as entities andactions database 162. The entities andactions database 162 includes a mapping of each of a plurality of entities to one or more computer-based actions associated with the entity. An action may be directly mapped with an entity and/or may be indirectly mapped to the entity via a mapping with a class of the entity. For example, the action of dialing a phone number may be mapped to each of a plurality of particular phone numbers and/or may be associated with the class of phone numbers in general. Also, for example, the action of playing a movie may be mapped to each of a plurality of movies, movies in general, and/or only movies that are available for on-demand viewing via one of the applications installed on thecomputing device 120. - In implementations where multiple actions are identified for an entity, the
action determination module 144 may optionally rank and/or filter the identified actions based on one or more factors such as, for example: strengths of association of the actions to the entity and/or a class of the entity; historical popularity of the actions in general; historical popularity of the actions for the application from which the content originated; whether the actions are performable via one ormore applications 126 installed on thecomputing device 120; historical popularity of performance of the actions via one ormore applications 126 installed on thecomputing device 120; etc. - For example, the entities and
actions database 162 may include, for a mapping between an action and an entity or entity class, a strength of association for that mapping. The strength of association of an action to an entity may optionally be based on analysis of past voice search queries. For instance, a computer-based action of providing navigation may be more strongly associated with an entity class of restaurants than a computer-based action of calling if 1,000 analyzed past voice search queries generally conform to the format "navigate to [restaurant]" (where "[restaurant]" indicates reference to an entity that is a member of the class of restaurants), but only 100 analyzed past voice search queries generally conform to the format "call [restaurant]". - The historical popularity of a computer-based action in general may be based on a frequency of appearance of terms that initiate the computer-based action in past voice search queries. The historical popularity of a computer-based action for an application from which the content originated may be based on a frequency of appearance of terms that initiate the computer-based action in past voice search queries that were issued while using the application and/or that were issued within a threshold time period of using the application. For instance, analysis of past voice search queries may indicate a computer-based action of calling is less popular for a web browser application than it is for an application that provides consumer reviews for businesses.
- In some implementations, the computer-based action identified by the
action determination module 144 may be a computer identifier of the action that is not itself an action term that would initiate performance of the action if provided as a voice query. For example, the computer identifier of the action of providing active navigation directions may be and an alpha and/or numerical identifier such as "ID_NAVIGATE", "42", and/or "AE5". - The suggested voice-based action
query generation module 145 generates one or more suggested voice-based action queries each based on one or more action terms to perform one of the computer-based actions determined byaction determination module 144 and one or more entity terms that reference the entity determined byentity determination module 142. - In some implementations, the suggested voice-based action
query generation module 145 determines the action term(s) for a computer-based action based on a preferred language indicated by thecomputing device 120. For example, a computer identifier of the computer-based action may be determined by theaction determination module 144 and a first term may be identified as the action term if thecomputing device 120 has a preferred language of English, whereas a second term would be identified as the action term if thecomputing device 120 had a preferred language of German. For instance, the entities andactions database 162 and/or other database may include, for a given computer-based action, action terms mapped to that action. Each of the action terms may further be mapped to a preferred language of the action term. Theaction determination module 144 may select an action term for a computer-based action in view of a preferred language based on identifying a mapping of the action term to the computer-based action and further identifying a mapping of the action term to the preferred language. - The suggested voice-based action
query generation module 145 further determines one or more terms that reference the identified entity. For example, where the identified entity is itself is a term, that term may be utilized. For instance, where "Restaurant A" is the entity, "Restaurant A" may be utilized as the entity term. In some implementations, a pronoun or other generic descriptor of the entity may be utilized. For example, where "Restaurant A" is the entity, the suggested voice-based actionquery generation module 145 may determine an entity term of "it" or "there". Also, for example, where a famous male is the entity, the suggested voice-based actionquery generation module 145 may determine an entity term of "he" or "him". The particular generic descriptor selected may be determined based on a mapping of the generic descriptor to the entity and/or a class of the entity. Also, the particular generic descriptor may optionally be further selected to provide grammatical coherence with the action term. For instance, the entity term "there" may be selected for an action term of "navigate", whereas "it" may be selected for action terms of "tell me more about". - In some implementations, the suggested voice-based action
query generation module 145 determines the entity term based on a preferred language indicated by thecomputing device 120. For example, a first generic descriptor may be identified as the entity term if thecomputing device 120 has a preferred language of English, whereas a second generic descriptor would be identified as the entity term if thecomputing device 120 had a preferred language of German. - The suggested voice-based
action query system 140 provides the generated one or more suggested voice-based action queries to thevoice query application 122 for presentation, by thevoice query application 122, as a suggested voice-based action query for a voice-based query. In implementations where multiple suggested voice-based action queries are provided, they may optionally be provided with ranking information based on, for example, the ranking of the actions described with respect toaction determination module 144. In some of those implementations, thecomputing device 120 may determine a display order of the suggested voice-based action queries based on the provided ranking information. The ranking information may optionally be an order in which the suggested voice-based action queries are transmitted or included in a data packet. - Where the provided suggested voice-based action queries include a generic descriptor of the entity (e.g., him), the suggested voice-based
action query system 140 may optionally provide the suggested voice-based action queries with metadata that more particularly identifies the entity. The metadata may be utilized by thecomputing device 120 to "replace" the generic descriptor with the more particular identification of the entity in performing the action. In other implementations, the more specific identification of the entity may be performed at the computing device 120 (e.g., by semantic processor module 127), based on analysis of the most recently accessed content. - In some implementations, the suggested voice-based
action query system 140 provides the generated one or more suggested voice-based action queries to thevoice query application 122 in response to receiving a voice-based query input indication from thecomputing device 120. The voice-based query input indication indicates receipt of input of the user, via thecomputing device 120, to initiate providing of a voice-based query via thecomputing device 120. In some implementations, the voice-based query input indication is the receiving of the indication of content from thecomputing device 120 and/or the receiving of other information in combination with the indication of content. For example, in some implementations thecomputing device 120 may only provide the indication of content in response to input of the user that initiates providing of a voice-based query. In some implementations, the voice-based query input indication may be received separate from the indication of content. For example, in some implementations thecomputing device 120 may provide the indication of content, then only provide the voice-based query input indication after a certain amount of time has passed, since receiving the voice-input indication, without receiving any spoken input from the user. - In some implementations, the suggested voice-based
action query system 140 may not be limited to generating suggested voice-based action queries. For example, the suggested voice-basedaction query system 140 may also be capable of handling all or aspects of parsing submitted voice-based action queries, determining appropriate computer-based action(s) for submitted voice-based action queries, instructing one or more applications ofcomputing device 120 to perform determined computer-based actions for submitted voice-based action queries, and/or performing one or more computer-based actions for submitted voice-based action queries. Although suggested voice-basedaction query system 140 andcomputing device 120 are illustrated as separate components inFIG. 1 , in other implementations one or more aspects of voice-basedaction query system 140 may be implemented oncomputing device 120, or vice versa. -
FIG. 2 illustrates an example of determining, in view of content being accessed on thecomputing device 120, at least one suggested voice-basedaction query 155 for presentation via thecomputing device 120. InFIG. 2 , an indication ofcontent 131 from thecomputing device 120 is provided to theentity determination module 142. For example, the indication ofcontent 131 may include text and properties of the text for content being viewed on thecomputing device 120 immediately prior to input of a user via thecomputing device 120 to initiate providing of a voice-based query. For instance, the user may be viewing the content in an application of thecomputing device 120 and the user may provide input to initiate providing of a voice-based query while that application is still active and displaying the content, and the content may be provided in response to the input. As another example, the user may be viewing the content on a first application of thecomputing device 120, may provide input to initiate providing of a voice-based query that causes additional content to be displayed (either supplanting the content of the first application or provided "over" portions of the content of the first application) by a second application (or the operating system) of thecomputing device 120, and the content recently displayed by the first application may be provided in response to the input. As yet another example, thecomputing device 120 may provide an indication of currently viewed content without first requiring input of a user to initiate providing of a voice-based query. - The
entity determination module 142 determines at least oneentity 151 based on the indication of thecontent 131. For example, theentity determination module 142 may determine a dominant entity associated with the content based on position, format, frequency, and/or other property of the text in the content. For example, certain text may be identified as the dominant entity based on it appearing in the title of the content, in larger font than other text in the content, in a more prominent position than other text in the content, and more frequently than other text in the content. - The
entity determination module 142 provides thedetermined entity 151 to theaction determination module 144. Theaction determination module 144 determines at least one computer-basedaction 152 that is mapped to theentity 151 in the entities andactions database 162. Theaction determination module 144 may determine theaction 152 based on a direct mapping of theentity 151 to theaction 152, or based on a mapping of theaction 152 to a class of theentity 151, and a mapping of the class of theentity 151 to theaction 152. -
Application information 132 from thecomputing device 120 may also be provided to theaction determination module 144. In some implementations, theaction determination module 144 may rank and/or filter computer-based actions based on theapplication information 132. For example, theapplication information 132 may indicate one or more applications installed on the computing device (e.g., applications 126) and/or versions for one or more applications installed on the computing device (e.g.,application 126 and/or 122). For example, entities andactions database 162 may include, for each of a plurality of computer-based actions, data defining one or more applications and/or application versions (forapplications 126 and/or 122) via which the computer-based action may be performed. Theaction determination module 144 may utilize such data to filter out one or more computer-based actions that are not compatible with thecomputing device 120 based on theapplication information 132. - As another example, the
application information 132 may indicate which application was generating the content indicated by indication ofcontent 131. The entities andactions database 162 may include the historical popularity of one or more candidate computer-based actions with respect to that application from which the content originated (e.g., based on a frequency of appearance of terms that initiate the computer-based action in past voice search queries that were issued while using the application and/or that were issued within a threshold time period of using the application). Theaction determination module 144 may utilize such historical popularity to select theaction 152 and/or rank theaction 152 relative to other selected actions. - The
action determination module 144 provides theentity 151 and theaction 152 to the suggested voice-based action query generation module 145 (optionally with other determined entities and/or actions). The suggested voice-based actionquery generation module 145 generates a suggested voice-basedaction query 155 based on one or more action terms to perform theaction 151 and one or more entity terms that reference theentity 152. In some implementations, a preferred language 133 from thecomputing device 120 may also be provided to the suggested voice-based actionquery generation module 145. In some of those implementations, the suggested voice-based actionquery generation module 145 determines the action term(s) for a computer-based action and/or the entity terms based on a preferred language indicated by thecomputing device 120. - The suggested voice-based action
query generation module 145 provides the suggested voice-basedaction query 155 to thecomputing device 120. For example, the suggested voice-basedaction query 155 may be provided to thecomputing device 120 as a text string that includes the one or more action terms and the one or more entity terms. In some implementations, the suggested voice-basedaction generation module 145 provides the generated one or more suggested voice-based action queries to thecomputing device 120 in response to receiving a voice-based query input indication from thecomputing device 120. In some of those implementations, the voice-based query input indication is the receiving of the indication ofcontent 131 from thecomputing device 120 and/or the receiving of other information in combination with the indication ofcontent 131. - In some implementations, the suggested voice-based action
query generation module 145 provides annotation data with the suggested voice-basedaction query 155. The annotation data is data that may be displayed with the suggested voice-basedaction query 155 to help clarify the suggested voice-basedaction query 155, but doesn't constitute the suggested query itself. For example, where a pronoun is used as the entity term of the suggested voice-basedaction query 155, an image of the entity and/or a more specific alias of the entity may also be provided for display visually set off from the voice-based action query 155 (e.g., provided in parentheses and/or positionally offset). -
FIG. 3 illustrates an example of receiving at least one suggested voice-based action query at a computing device and providing the suggested voice-based action query as a suggestion in response to input to initiate providing of a voice-based query. A voice-based query indication input 110 is received atvoice action module 123. Voice-based query indication input 110 may include, for example, a user selecting a voice query icon via a graphical user interface, the user speaking a phrase that initiates a voice-based query (e.g., "OK computer"), the user actuating a touch-sensitive hardware element of thecomputing device 120 or in communication with the computing device 120 (e.g., a mechanical button, a capacitive button), and/or performing a gesture in view of a camera or other sensor of thecomputing device 120. - The
voice action module 123 monitors for voice input in response to the voice-based query initiation input 110 and also sends a request voice-based action queries command 135 tointerface module 124. In response to thecommand 135, theinterface module 124 providesinformation 130 to the suggested voice-basedaction query system 140, such as an indication ofcontent 131 most recently accessed via one of theapplications 126, indications of a preferred language of thecomputing device 120, information related to one ormore applications 126 ofcomputing device 120, and/or voice-based query input indications. For example, the indication ofcontent 131 may be the content displayed by one of theapplications 126 most recently relative to receiving the voice-based query initiation input 110. - The
interface module 124 further receives a suggested voice-basedaction query 155 from suggested voice-basedaction query system 140. The suggested voice-basedaction query 155 is in response to theinformation 130 provided by theinterface module 124 and may optionally be based on one or more aspects of theinformation 130. Theinterface module 124 provides the suggested voice-basedaction query 155 to the render/synchronization module 125. - The
voice action module 123 provides a suggested voice-based action queries command 137 to the render/synchronization module 125. In response to thecommand 137, the render/synchronization module 125 presents (e.g., displays) the suggested voice-basedaction query 155 as a suggestion for the voice query initiated by the voice-based query initiation input 110. In some implementations, thevoice action module 123 provides thecommand 137 based on the voice-based query initiation input 110 being followed by an indication of a need for a suggested voice-based action query. In some of those implementations, an indication of the need for a suggested voice-based action query may include the lack of any spoken input from a user within a threshold amount of time following the user input initiating the voice-based query. For example, in versions of those implementations a suggested voice-based action query may be presented in response to the user not providing any spoken input within four seconds (or other threshold amount of time) of the user input initiating the voice-based query. In some implementations, thecommand 137 may additionally and/or alternatively be provided in response to other indications of the need for a suggested voice-based action query. For example, other indications may include one or more phrases that can be spoken by the user (e.g., "tell me what I can do"), selection of a user interface element provided for requesting suggested voice-based action queries, and/or detecting of at least a threshold noise level following the voice-based query initiation input 110 (e.g., detecting that the environment is "too loud" to properly process spoken input). In some implementations, the render/synchronization module 125 may present the suggested voice-based action query as a suggestion for the voice query without receiving thecommand 137. -
FIG. 4 is a flowchart illustrating anexample method 400 of determining, in view of content being accessed on a computing device, at least one suggested voice-based action query for presentation via the computing device. For convenience, the operations of the flow chart are described with reference to a system that performs the operations. This system may include various components of various computer systems, such as suggested voice-basedaction query system 140. Moreover, while operations ofmethod 400 are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted or added. - At
block 402, the system receives an indication of content recently viewed on a computing device. For example, the indication of content may include text and properties of the text for content currently being displayed by the computing device. In some implementations, the indication of content may be provided by the computing device in response to input of a user via the computing device to initiate providing of a voice-based query. - At
block 404, the system determines, based on the indication of the content, an entity referenced in the content. For example, where the indication of the content includes text and properties of the text, the system may determine a dominant entity associated with the content based on position, format, frequency, and/or other property of the text in the content. For example, certain text may be identified as the dominant entity based on it appearing in the title of the content, in larger font than other text in the content, in a more prominent position than other text in the content, and more frequently than other text in the content. - At
block 406, the system determines a computer-based action that can be mapped to the entity. For example, the system may determine at least one computer-based action that is mapped to the entity in the entities andactions database 162. In some implementations, the system may rank and/or filter computer-based actions based on one or more factors such as, for example: strengths of association of the actions to the entity and/or a class of the entity; historical popularity of the actions in general; historical popularity of the actions for the application from which the content originated; whether the actions are performable via one or more applications installed on the computing device; historical popularity of performance of the actions via one or more applications installed on the computing device; etc. - At
block 408, the system generates a suggested voice-based action query that includes an action term mapped to the computer-based action and an entity term mapped to the entity. In some of those implementations, the system determines the action term(s) and/or the entity terms based on a preferred language indicated by information received from the computing device. - At
block 410, the system provides the suggested voice-based action query for display as a suggestion for a voice query. In some implementations, the system provides the generated one or more suggested voice-based action queries to the computing device for display as a suggestion for a voice query in response to receiving a voice-based query input indication from the computing device. In some of those implementations, the voice-based query input indication is the receiving of the indication of content atblock 402 and/or the receiving of other information from the computing device. -
FIG. 5 illustrates an example method of receiving at least one suggested voice-based action query at a computing device and providing the suggested voice-based action query as a suggestion in response to input to initiate providing of a voice-based query. For convenience, the operations of the flow chart are described with reference to a system that performs the operations. This system may include various components of various computer systems, such asvoice query application 122 ofcomputing device 120. Moreover, while operations ofmethod 500 are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted or added. - At
block 502, the system receives input to initiate providing of a voice-based query. The input may include, for example, a user selecting a voice query icon via a graphical user interface, the user speaking a phrase that initiates a voice-based query (e.g., "OK computer"), the user actuating a touch-sensitive hardware element, and/or performing a gesture. - At
block 504, the system identifies content displayed most recently relative to the input atblock 502. For example, the system may provide an identifier of the content or text, metadata, images, tags applied to image(s), and/or screenshots of the content displayed by the computing device when, or just before, the input was received atblock 502. - At
block 506, the system provides an indication of the content identified atblock 504. For example, the system may provide the indication of the content to suggested voice-basedaction query system 140. - At
block 508, the system receives a suggested voice-based action query that is based on the indication ofblock 506 and that includes an action term and an entity term. For example, the system may receive a suggested voice-based action query from the voice-basedaction query system 140 in response to providing the indication of the content (and optionally other information) atblock 506. - At
block 518, the system determines whether voice input has been received within a threshold amount of time. If the answer is yes, the system proceeds to block 520 and performs a computer-based action based on the received voice input. If the answer is no, the system proceeds to step 510 and provides the suggested voice-based action query as a suggestion for the voice-based query. - At
step 512, the system determines whether the suggested voice-based action query has been selected. If the answer is yes, the system proceeds to block 514 and performs a computer-based action based on the suggested voice-based action query. If the answer is no, the system proceeds to block 516 and performs a further action. For example if the user provides input to remove the suggested voice-based action query from the display, the answer is no and the system will respond to the provided input atblock 516. -
FIG. 6A illustrates an examplegraphical user interface 680A showing content displayed in an application of a computing device. For example, thegraphical user interface 680A may be displayed on a mobile phone computing device. The content is displayed in thegraphical user interface 680A by an application of the computing device, such as an application that provides information and reviews for restaurants. The particular content is focused on information and reviews for a fictional restaurant "Up and Down Burger Bar." Thegraphical user interface 680A also includesgraphical interface elements -
FIG. 6B illustrates an examplegraphical user interface 680B for displaying suggested voice-based action queries 685A-C that are generated based on the content ofFIG. 6A .FIG. 6B illustrates an example of the user, while provided with the display ofFIG. 6A , providing a voice-based query initiation input. The voice-based query initiation input may be, for example, speaking one or more terms or selecting an interface element (e.g., actuating one or more of elements 681-683 in a certain manner or actuating a mechanical interface element). Providing the voice-based query initiation input caused avoice query interface 684 to be displayed with the content ofFIG. 6A . Thevoice query interface 684 includes an icon of a microphone to indicate a voice query and also includes the text "Waiting ..." to indicate to the user that the computing device is awaiting spoken input from the user. The suggested voice-based action queries 685A-C are displayed below thevoice query interface 684 as individual "cards" that may be selected (e.g., "tapped" or spoken) by the user. The suggested voice-based action queries 685A-C are based on the content ofFIG. 6A and may be determined, for example, as described herein with respect to suggested voice-basedaction query system 140,FIG. 2 , and/orFIG. 4 . For example, a screenshot and/or text fromFIG. 6A may have been provided as the indication of content and the suggested voice-based action queries 685A-C received in response. It is noted that suggested voice-basedaction query 685A is provided with an annotation "[Up and Down Burger Bar]" to provide the user an indication that speaking "Navigate there" or tapping the suggested voice-basedaction query 685A will result in a computer-based action of providing navigation directions to "Up and Down Burger Bar". - Much of the content of
FIG. 6A is still displayed inFIG. 6B , with other of the content being "hidden" under thevoice query interface 684 and the voice-based action queries 685A-C. In some implementations, thevoice query interface 684 and/or one or more of the voice-based action queries 685A-C may be at least partially transparent to enable viewing of the content that is "hidden" inFIG. 6B . - As described herein, in some implementations the suggested voice-based action queries 685A-C may not have been displayed in the
graphical user interface 680B until identification of a need for a suggested voice-based action query following the voice-based query initiation input. For example, thevoice query interface 684 may have been initially displayed without the suggested voice-based action queries 685A-C and the suggested voice-based action queries 685A-C displayed only upon determining a lack of any spoken input from a user within a threshold amount of time following the user input initiating the voice-based query. Also, for example, thevoice query interface 684 may have been initially displayed without the suggested voice-based action queries 685A-C and the suggested voice-based action queries 685A-C displayed only upon receiving specific spoken input requesting suggestions and/or determining that an ambient noise level is greater than a threshold noise level for receiving and accurately parsing spoken input. -
FIG. 6C illustrates an examplegraphical user interface 680C for displaying suggested voice-based action queries "Navigate there", "Make reservations there", and "Tell me more about it" that are generated based on the content ofFIG. 6A . The suggested voice-based action queries ofFIG. 6C are displayed in acard 687 based on the content ofFIG. 6A and may be determined, for example, as described herein with respect to suggested voice-basedaction query system 140,FIG. 2 , and/orFIG. 4 . For example, a screenshot and/or text fromFIG. 6A may have been provided as the indication of content and the suggested voice-based action queries received in response. Thecard 687 is displayed in aninterface element 686 that shows (e.g., based on the phrase "ON SCREEN:") that the card 687 (and optionally other non-displayed cards) is particularly tailored to the content ofFIG. 6A (which remains partially displayed inFIG. 6C ). -
FIG. 6C illustrates an example of the user, while provided with the display ofFIG. 6A , providing a request for suggested voice-based query suggestions related to content on the screen (without necessarily providing a voice-based query initiation input). For example, the request for suggested voice-based query suggestions may be "touching"graphical interface element 682 and "swiping up" and/or speaking one or more terms. Providing the request for suggested voice-based query suggestions caused theinterface element 686 and thecard 687 to be displayed over portions of the content ofFIG. 6A . Selection of one of the suggested voice-based action queries ofFIG. 6C (by "tapping" or speaking (optionally after a voice-based query initiation input)) will cause the computing device to initiate performance of a computer-based action based on the selected voice-based action query. - Much of the content of
FIG. 6A is still displayed inFIG. 6C , with other of the content being "hidden" under theinterface element 686 and thecard 687. In some implementations, theinterface element 686 and/or thecard 687 may be at least partially transparent to enable viewing of the content that is "hidden" inFIG. 6C . -
FIG. 7A illustrates an examplegraphical user interface 780A showing content displayed in an application of a computing device. For example, thegraphical user interface 780A may be displayed in an instant messaging application of a mobile phone computing device. The particular content is a conversation between a user of the mobile phone and another user "Bob". Bob has asked the user if he wants to head to Up and Down Burger Bar for dinner. Thegraphical user interface 780A also includesgraphical interface elements -
FIG. 7B illustrates an examplegraphical user interface 780B for displaying suggested voice-based action queries 785A and 785B that are generated based on the content ofFIG. 7A .FIG. 7B illustrates an example of the user, while provided with the display ofFIG. 7A , providing a voice-based query initiation input. The voice-based query initiation input may be, for example, speaking one or more terms or selecting an interface element (e.g., actuating one or more of elements 681-683 in a certain manner or actuating a mechanical interface element). Providing the voice-based query initiation input caused avoice query interface 784 to be displayed with the content ofFIG. 7A . Thevoice query interface 784 includes an icon of a microphone to indicate a voice query and also includes the text "Waiting ..." to indicate to the user that the computing device is awaiting spoken input from the user. The suggested voice-based action queries 785A and 785B are displayed below thevoice query interface 784 as individual "cards" that may be selected (e.g., "tapped" or spoken) by the user. The suggested voice-based action queries 785A and 785B are based on the content ofFIG. 7A and may be determined, for example, as described herein with respect to suggested voice-basedaction query system 140,FIG. 2 , and/orFIG. 4 . For example, a screenshot and/or text fromFIG. 7A may have been provided as the indication of content and the suggested voice-based action queries 785A and 785B received in response. It is noted that suggested voice-basedaction query 785A is provided with a suggested time of "7 PM" for making a reservation. The suggested time may be determined based on the context ofFIG. 7A ("dinner"), past user reservation history, and/or arbitrarily to provide the user an indication that spoken input can be utilized to make a reservation for "Up and Down Burger Bar" at a desired time. -
FIG. 8 is a block diagram of anexample computing device 810 that may optionally be utilized to perform one or more aspects of techniques described herein. In some implementations,computing device 120 and/or suggested voice-basedaction query system 140 may comprise one or more components of theexample computing device 810. -
Computing device 810 typically includes at least oneprocessor 814 which communicates with a number of peripheral devices viabus subsystem 812. These peripheral devices may include astorage subsystem 824, including, for example, amemory subsystem 825 and afile storage subsystem 826, userinterface output devices 820, userinterface input devices 822, and anetwork interface subsystem 816. The input and output devices allow user interaction withcomputing device 810.Network interface subsystem 816 provides an interface to outside networks and is coupled to corresponding interface devices in other computing devices. - User
interface input devices 822 may include a keyboard, pointing devices such as a mouse, trackball, touchpad, or graphics tablet, a scanner, a touchscreen incorporated into the display, audio input devices such as voice recognition systems, microphones, and/or other types of input devices. In general, use of the term "input device" is intended to include all possible types of devices and ways to input information intocomputing device 810 or onto a communication network. - User
interface output devices 820 may include a display subsystem, a printer, a fax machine, or non-visual displays such as audio output devices. The display subsystem may include a cathode ray tube (CRT), a flat-panel device such as a liquid crystal display (LCD), a projection device, or some other mechanism for creating a visible image. The display subsystem may also provide non-visual display such as via audio output devices. In general, use of the term "output device" is intended to include all possible types of devices and ways to output information fromcomputing device 810 to the user or to another machine or computing device. -
Storage subsystem 824 stores programming and data constructs that provide the functionality of some or all of the modules described herein. For example, thestorage subsystem 824 may include the logic to perform selected aspects of the method ofFIGS. 3 and/or 4. - These software modules are generally executed by
processor 814 alone or in combination with other processors.Memory 825 used in thestorage subsystem 824 can include a number of memories including a main random access memory (RAM) 830 for storage of instructions and data during program execution and a read only memory (ROM) 832 in which fixed instructions are stored. Afile storage subsystem 826 can provide persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive along with associated removable media, a CD-ROM drive, an optical drive, or removable media cartridges. The modules implementing the functionality of certain implementations may be stored byfile storage subsystem 826 in thestorage subsystem 824, or in other machines accessible by the processor(s) 814. -
Bus subsystem 812 provides a mechanism for letting the various components and subsystems ofcomputing device 810 communicate with each other as intended. Althoughbus subsystem 812 is shown schematically as a single bus, alternative implementations of the bus subsystem may use multiple busses. -
Computing device 810 can be of varying types including a workstation, server, computing cluster, blade server, server farm, or any other data processing system or computing device. Due to the ever-changing nature of computers and networks, the description ofcomputing device 810 depicted inFig. 8 is intended only as a specific example for purposes of illustrating some implementations. Many other configurations ofcomputing device 810 are possible having more or fewer components than the computing device depicted inFig. 8 . - In situations in which the systems described herein collect personal information about users, or may make use of personal information, the users may be provided with an opportunity to control whether programs or features collect user information (e.g., information about a user's social network, social actions or activities, profession, a user's preferences, or a user's current geographic location), or to control whether and/or how to receive content from the content server that may be more relevant to the user. Also, certain data may be treated in one or more ways before it is stored or used, so that personal identifiable information is removed. For example, a user's identity may be treated so that no personal identifiable information can be determined for the user, or a user's geographic location may be generalized where geographic location information is obtained (such as to a city, ZIP code, or state level), so that a particular geographic location of a user cannot be determined. Thus, the user may have control over how information is collected about the user and/or used.
- While several implementations have been described and illustrated herein, a variety of other means and/or structures for performing the function and/or obtaining the results and/or one or more of the advantages described herein may be utilized, and each of such variations and/or modifications is deemed to be within the scope of the implementations described herein. More generally, all parameters, dimensions, materials, and configurations described herein are meant to be exemplary and that the actual parameters, dimensions, materials, and/or configurations will depend upon the specific application or applications for which the teachings is/are used.
- Those skilled in the art will recognize, or be able to ascertain using no more than routine experimentation, many equivalents to the specific implementations described herein. It is, therefore, to be understood that the foregoing implementations are presented by way of example only and that, within the scope of the appended clauses, implementations may be practiced otherwise than as specifically described and clauseed. Implementations of the present disclosure are directed to each individual feature, system, article, material, kit, and/or method described herein. In addition, any combination of two or more such features, systems, articles, materials, kits, and/or methods, if such features, systems, articles, materials, kits, and/or methods are not mutually inconsistent, is included within the scope of the present disclosure.
Claims (11)
- A method implemented by one or more processors, the method comprising:receiving, while a first application of a computing device is displaying content, an input to initiate providing of a voice-based query via the computing device;generating, based on the content currently being displayed by the first application, a suggested voice-based action query that can be spoken to initiate performance of a computer-based action;in response to receiving the input to initiate providing of the voice-based query:
causing the computing device to display an interface element over the content currently being displayed by the first application, wherein the interface element includes a selectable display of the generated suggested voice-based action query in which suggested voice-based action queries are displayed as one or more cards or in a drop down menu;in response to receiving a selection of the selectable display of the suggested voice-based action query:causing performance of the computer-based action of the suggested voice-based action query,wherein including the selectable display of the suggested voice-based action query in the interface element is based on detecting at least a threshold noise level for receiving and accurately parsing spoken input by the computing device following the input of the user to initiate providing of the voice-based action query, and/or, wherein including the selectable display of the suggested voice-based action query in the interface element is based on detecting that no spoken input has been received within a threshold amount of time following the input of the user to initiate providing of the voice-based action query. - The method of claim 1, wherein the input to initiate providing of a voice-based query is a spoken phrase, an actuation of a touch-sensitive hardware element of the computing device, or performing a gesture in view of a camera of the computing device.
- The method of claim 1 or claim 2, wherein performance of the computer-based action is by a second application of the computing device.
- The method of claim 3, wherein generating the suggested voice-based action query is based on the second application being installed on the computing device, and based on the computer-based action being mapped to the second application.
- The method of claim 4, wherein generating the suggested voice-based action query is based on a version of the second application that is installed on the computing device, and based on the computer-based action being mapped to the version of the second application.
- The method of any preceding claim, wherein generating the suggested voice-based action query based on the content currently being displayed by the first application comprises:determining an entity referenced in the content;determining the computer-based action is mapped to the entity in one or more electronic databases; andgenerating the suggested voice-based action query to include at least one term that initiates performance of the computer based action and to include at least one entity term selected based on the entity.
- The method of claim 6, wherein generating the suggested voice-based action query comprises determining a pronoun mapped to the entity, and using the pronoun as the at least one entity term.
- The method of claim 6, wherein generating the suggested voice-based action query comprises determining a preferred language of the computing device, and generating the suggested voice-based action query based on the preferred language of the computing device.
- The method of any preceding claim, wherein the second application is integrated as part of an operating system of the computing device.
- A computing device comprising one or more processors and memory operably coupled with the one or more processors, wherein the memory stores instructions that, in response to execution of the instructions by one or more processors, cause the one or more processors to perform the method of any of the preceding claims.
- At least one non-transitory computer-readable medium comprising instructions that, in response to execution of the instructions by one or more processors, cause the one or more processors to perform the method of any of the preceding claims.
Applications Claiming Priority (4)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201562167195P | 2015-05-27 | 2015-05-27 | |
US14/808,919 US10504509B2 (en) | 2015-05-27 | 2015-07-24 | Providing suggested voice-based action queries |
EP16730101.9A EP3262636B1 (en) | 2015-05-27 | 2016-05-26 | Providing suggested voice-based action queries |
PCT/US2016/034290 WO2016191540A1 (en) | 2015-05-27 | 2016-05-26 | Providing suggested voice-based action queries |
Related Parent Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
EP16730101.9A Division EP3262636B1 (en) | 2015-05-27 | 2016-05-26 | Providing suggested voice-based action queries |
EP16730101.9A Division-Into EP3262636B1 (en) | 2015-05-27 | 2016-05-26 | Providing suggested voice-based action queries |
Publications (2)
Publication Number | Publication Date |
---|---|
EP3404654A1 EP3404654A1 (en) | 2018-11-21 |
EP3404654B1 true EP3404654B1 (en) | 2020-11-25 |
Family
ID=56134594
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
EP16730101.9A Active EP3262636B1 (en) | 2015-05-27 | 2016-05-26 | Providing suggested voice-based action queries |
EP18179479.3A Active EP3404654B1 (en) | 2015-05-27 | 2016-05-26 | Providing suggested voice-based action queries |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
EP16730101.9A Active EP3262636B1 (en) | 2015-05-27 | 2016-05-26 | Providing suggested voice-based action queries |
Country Status (8)
Country | Link |
---|---|
US (4) | US10504509B2 (en) |
EP (2) | EP3262636B1 (en) |
JP (3) | JP6437669B2 (en) |
KR (3) | KR102369953B1 (en) |
CN (2) | CN110851470B (en) |
DE (1) | DE112016000986T5 (en) |
GB (1) | GB2553936B (en) |
WO (1) | WO2016191540A1 (en) |
Families Citing this family (30)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10504509B2 (en) | 2015-05-27 | 2019-12-10 | Google Llc | Providing suggested voice-based action queries |
US20170024424A1 (en) * | 2015-07-26 | 2017-01-26 | Saad Almohizea | Suggestive search engine |
US10739960B2 (en) * | 2015-09-22 | 2020-08-11 | Samsung Electronics Co., Ltd. | Performing application-specific searches using touchscreen-enabled computing devices |
US11294908B2 (en) * | 2015-11-10 | 2022-04-05 | Oracle International Corporation | Smart search and navigate |
CN108781175B (en) | 2015-12-21 | 2021-09-21 | 谷歌有限责任公司 | Method, medium, and system for automatic suggestion of message exchange contexts |
EP3395019B1 (en) | 2015-12-21 | 2022-03-30 | Google LLC | Automatic suggestions and other content for messaging applications |
US10387461B2 (en) | 2016-08-16 | 2019-08-20 | Google Llc | Techniques for suggesting electronic messages based on user activity and other context |
US10015124B2 (en) * | 2016-09-20 | 2018-07-03 | Google Llc | Automatic response suggestions based on images received in messaging applications |
CN117634495A (en) | 2016-09-20 | 2024-03-01 | 谷歌有限责任公司 | Suggested response based on message decal |
US10511450B2 (en) | 2016-09-20 | 2019-12-17 | Google Llc | Bot permissions |
US10531227B2 (en) | 2016-10-19 | 2020-01-07 | Google Llc | Time-delimited action suggestion system |
US10416846B2 (en) | 2016-11-12 | 2019-09-17 | Google Llc | Determining graphical element(s) for inclusion in an electronic communication |
US10446144B2 (en) * | 2016-11-21 | 2019-10-15 | Google Llc | Providing prompt in an automated dialog session based on selected content of prior automated dialog session |
US10146768B2 (en) | 2017-01-25 | 2018-12-04 | Google Llc | Automatic suggested responses to images received in messages using language model |
US10860854B2 (en) | 2017-05-16 | 2020-12-08 | Google Llc | Suggested actions for images |
US10348658B2 (en) | 2017-06-15 | 2019-07-09 | Google Llc | Suggested items for use with embedded applications in chat conversations |
US10404636B2 (en) | 2017-06-15 | 2019-09-03 | Google Llc | Embedded programs and interfaces for chat conversations |
TWI651714B (en) * | 2017-12-22 | 2019-02-21 | 隆宸星股份有限公司 | Voice option selection system and method and smart robot using the same |
US10891526B2 (en) | 2017-12-22 | 2021-01-12 | Google Llc | Functional image archiving |
KR102661340B1 (en) | 2018-09-21 | 2024-04-30 | 삼성전자주식회사 | Electronic device and control method thereof |
US11553063B2 (en) * | 2018-12-07 | 2023-01-10 | Google Llc | System and method for selecting and providing available actions from one or more computer applications to a user |
CN113330512A (en) * | 2018-12-28 | 2021-08-31 | 谷歌有限责任公司 | Supplementing an automated assistant with speech input according to a selected suggestion |
JP7155439B2 (en) * | 2019-02-12 | 2022-10-18 | グーグル エルエルシー | Directing vehicle client devices to use on-device functionality |
KR20200129346A (en) * | 2019-05-08 | 2020-11-18 | 삼성전자주식회사 | Display apparatus and method for controlling thereof |
WO2020226650A1 (en) | 2019-05-09 | 2020-11-12 | Google Llc | Automated assistant suggestions for third-party vehicle computing devices with restricted architecture |
KR102208387B1 (en) * | 2020-03-10 | 2021-01-28 | 주식회사 엘솔루 | Method and apparatus for reconstructing voice conversation |
CN114007117B (en) * | 2020-07-28 | 2023-03-21 | 华为技术有限公司 | Control display method and device |
CN112115282A (en) * | 2020-09-17 | 2020-12-22 | 北京达佳互联信息技术有限公司 | Question answering method, device, equipment and storage medium based on search |
US11657817B2 (en) | 2020-10-16 | 2023-05-23 | Google Llc | Suggesting an alternative interface when environmental interference is expected to inhibit certain automated assistant interactions |
WO2023244212A1 (en) * | 2022-06-14 | 2023-12-21 | Google Llc | Providing a recommendation for selection of a user interface element of a user interface |
Family Cites Families (59)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2000244609A (en) | 1999-02-23 | 2000-09-08 | Omron Corp | Speaker's situation adaptive voice interactive device and ticket issuing device |
JP2002259114A (en) | 2001-03-05 | 2002-09-13 | Nec Corp | Voice recognition computer system |
JP2002278591A (en) | 2001-03-22 | 2002-09-27 | Sharp Corp | Information processing device, information processing method and program recording medium |
US7167832B2 (en) | 2001-10-15 | 2007-01-23 | At&T Corp. | Method for dialog management |
JP4223832B2 (en) | 2003-02-25 | 2009-02-12 | 富士通株式会社 | Adaptive spoken dialogue system and method |
JP2004272363A (en) | 2003-03-05 | 2004-09-30 | Canon Inc | Voice input/output device |
US20050054381A1 (en) | 2003-09-05 | 2005-03-10 | Samsung Electronics Co., Ltd. | Proactive user interface |
DE10348408A1 (en) | 2003-10-14 | 2005-05-19 | Daimlerchrysler Ag | User-adaptive dialog-supported system for auxiliary equipment in road vehicle can distinguish between informed and uninformed users and gives long or short introduction as required |
JP4156563B2 (en) | 2004-06-07 | 2008-09-24 | 株式会社デンソー | Word string recognition device |
US9224394B2 (en) * | 2009-03-24 | 2015-12-29 | Sirius Xm Connected Vehicle Services Inc | Service oriented speech recognition for in-vehicle automated interaction and in-vehicle user interfaces requiring minimal cognitive driver processing for same |
US7672931B2 (en) * | 2005-06-30 | 2010-03-02 | Microsoft Corporation | Searching for content using voice search queries |
EP1922340B1 (en) * | 2005-08-08 | 2016-12-28 | Arkema Inc. | Polymerization of fluoropolymers using non-fluorinated surfactants |
TWI277948B (en) * | 2005-09-02 | 2007-04-01 | Delta Electronics Inc | Method and system for template inquiry dialogue system |
US9318108B2 (en) | 2010-01-18 | 2016-04-19 | Apple Inc. | Intelligent automated assistant |
US7657493B2 (en) | 2006-09-28 | 2010-02-02 | Microsoft Corporation | Recommendation system that identifies a valuable user action by mining data supplied by a plurality of users to find a correlation that suggests one or more actions for notification |
US8370145B2 (en) * | 2007-03-29 | 2013-02-05 | Panasonic Corporation | Device for extracting keywords in a conversation |
US8650030B2 (en) * | 2007-04-02 | 2014-02-11 | Google Inc. | Location based responses to telephone requests |
US9978365B2 (en) * | 2008-10-31 | 2018-05-22 | Nokia Technologies Oy | Method and system for providing a voice interface |
US9009053B2 (en) * | 2008-11-10 | 2015-04-14 | Google Inc. | Multisensory speech detection |
US20110099507A1 (en) * | 2009-10-28 | 2011-04-28 | Google Inc. | Displaying a collection of interactive elements that trigger actions directed to an item |
US8650210B1 (en) | 2010-02-09 | 2014-02-11 | Google Inc. | Identifying non-search actions based on a search query |
US20110202864A1 (en) | 2010-02-15 | 2011-08-18 | Hirsch Michael B | Apparatus and methods of receiving and acting on user-entered information |
US8234111B2 (en) * | 2010-06-14 | 2012-07-31 | Google Inc. | Speech and noise models for speech recognition |
US8473289B2 (en) * | 2010-08-06 | 2013-06-25 | Google Inc. | Disambiguating input based on context |
US8359020B2 (en) * | 2010-08-06 | 2013-01-22 | Google Inc. | Automatically monitoring for voice input based on context |
US9405848B2 (en) * | 2010-09-15 | 2016-08-02 | Vcvc Iii Llc | Recommending mobile device activities |
US9015043B2 (en) * | 2010-10-01 | 2015-04-21 | Google Inc. | Choosing recognized text from a background environment |
US20130066634A1 (en) * | 2011-03-16 | 2013-03-14 | Qualcomm Incorporated | Automated Conversation Assistance |
JP5673330B2 (en) | 2011-04-25 | 2015-02-18 | 株式会社デンソー | Voice input device |
US9043350B2 (en) * | 2011-09-22 | 2015-05-26 | Microsoft Technology Licensing, Llc | Providing topic based search guidance |
US9652556B2 (en) | 2011-10-05 | 2017-05-16 | Google Inc. | Search suggestions based on viewport content |
US9305108B2 (en) | 2011-10-05 | 2016-04-05 | Google Inc. | Semantic selection and purpose facilitation |
KR102022318B1 (en) | 2012-01-11 | 2019-09-18 | 삼성전자 주식회사 | Method and apparatus for performing user function by voice recognition |
US20140046891A1 (en) * | 2012-01-25 | 2014-02-13 | Sarah Banas | Sapient or Sentient Artificial Intelligence |
CN104115147B (en) | 2012-02-16 | 2018-06-29 | 微软技术许可有限责任公司 | Location-aware applications are searched for |
US20130325839A1 (en) * | 2012-03-05 | 2013-12-05 | TeleCommunication Communication Systems, Inc. | Single Search Box Global |
US8719025B2 (en) * | 2012-05-14 | 2014-05-06 | International Business Machines Corporation | Contextual voice query dilation to improve spoken web searching |
US9317709B2 (en) * | 2012-06-26 | 2016-04-19 | Google Inc. | System and method for detecting and integrating with native applications enabled for web-based storage |
US9799328B2 (en) * | 2012-08-03 | 2017-10-24 | Veveo, Inc. | Method for using pauses detected in speech input to assist in interpreting the input during conversational interaction for information retrieval |
US9268462B2 (en) | 2012-08-14 | 2016-02-23 | Google Inc. | External action suggestions in search results |
KR102081925B1 (en) | 2012-08-29 | 2020-02-26 | 엘지전자 주식회사 | display device and speech search method thereof |
US9547647B2 (en) * | 2012-09-19 | 2017-01-17 | Apple Inc. | Voice-based media searching |
US8543397B1 (en) | 2012-10-11 | 2013-09-24 | Google Inc. | Mobile device voice activation |
US9734151B2 (en) * | 2012-10-31 | 2017-08-15 | Tivo Solutions Inc. | Method and system for voice based media search |
KR20140089876A (en) * | 2013-01-07 | 2014-07-16 | 삼성전자주식회사 | interactive interface apparatus and method for comtrolling the server |
JP6122642B2 (en) | 2013-01-10 | 2017-04-26 | 株式会社Ｎｔｔドコモ | Function execution system and utterance example output method |
CN104969289B (en) * | 2013-02-07 | 2021-05-28 | 苹果公司 | Voice trigger of digital assistant |
US9361885B2 (en) * | 2013-03-12 | 2016-06-07 | Nuance Communications, Inc. | Methods and apparatus for detecting a voice command |
US20140280289A1 (en) * | 2013-03-12 | 2014-09-18 | Microsoft Corporation | Autosuggestions based on user history |
JP2014203207A (en) | 2013-04-03 | 2014-10-27 | ソニー株式会社 | Information processing unit, information processing method, and computer program |
US9892729B2 (en) * | 2013-05-07 | 2018-02-13 | Qualcomm Incorporated | Method and apparatus for controlling voice activation |
US9483565B2 (en) | 2013-06-27 | 2016-11-01 | Google Inc. | Associating a task with a user based on user selection of a query suggestion |
US8990079B1 (en) * | 2013-12-15 | 2015-03-24 | Zanavox | Automatic calibration of command-detection thresholds |
US8849675B1 (en) | 2013-12-18 | 2014-09-30 | Google Inc. | Suggested query constructor for voice actions |
US9489171B2 (en) * | 2014-03-04 | 2016-11-08 | Microsoft Technology Licensing, Llc | Voice-command suggestions based on user identity |
US9338493B2 (en) * | 2014-06-30 | 2016-05-10 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US9378740B1 (en) * | 2014-09-30 | 2016-06-28 | Amazon Technologies, Inc. | Command suggestions during automatic speech recognition |
US9646611B2 (en) * | 2014-11-06 | 2017-05-09 | Microsoft Technology Licensing, Llc | Context-based actions |
US10504509B2 (en) | 2015-05-27 | 2019-12-10 | Google Llc | Providing suggested voice-based action queries |
-
2015
- 2015-07-24 US US14/808,919 patent/US10504509B2/en active Active
-
2016
- 2016-05-26 KR KR1020197030804A patent/KR102369953B1/en active IP Right Grant
- 2016-05-26 KR KR1020187017226A patent/KR102036786B1/en active IP Right Grant
- 2016-05-26 GB GB1715688.6A patent/GB2553936B/en active Active
- 2016-05-26 KR KR1020177026523A patent/KR101870934B1/en active IP Right Grant
- 2016-05-26 WO PCT/US2016/034290 patent/WO2016191540A1/en active Application Filing
- 2016-05-26 CN CN201910977782.7A patent/CN110851470B/en active Active
- 2016-05-26 EP EP16730101.9A patent/EP3262636B1/en active Active
- 2016-05-26 EP EP18179479.3A patent/EP3404654B1/en active Active
- 2016-05-26 CN CN201680019315.8A patent/CN107430626B/en active Active
- 2016-05-26 JP JP2017550867A patent/JP6437669B2/en active Active
- 2016-05-26 DE DE112016000986.0T patent/DE112016000986T5/en not_active Withdrawn
-
2018
- 2018-11-12 JP JP2018212055A patent/JP6710740B2/en active Active
-
2019
- 2019-09-27 US US16/586,612 patent/US11238851B2/en active Active
-
2020
- 2020-05-26 JP JP2020091323A patent/JP6993466B2/en active Active
-
2022
- 2022-01-28 US US17/587,450 patent/US11869489B2/en active Active
-
2024
- 2024-01-08 US US18/406,484 patent/US20240144924A1/en active Pending
Non-Patent Citations (1)
Title |
---|
None * |
Also Published As
Publication number | Publication date |
---|---|
EP3262636A1 (en) | 2018-01-03 |
KR102036786B1 (en) | 2019-11-26 |
CN110851470A (en) | 2020-02-28 |
KR20180072845A (en) | 2018-06-29 |
JP6993466B2 (en) | 2022-01-13 |
GB201715688D0 (en) | 2017-11-15 |
JP2018523144A (en) | 2018-08-16 |
GB2553936A (en) | 2018-03-21 |
JP2020144932A (en) | 2020-09-10 |
CN110851470B (en) | 2024-03-01 |
US20200027448A1 (en) | 2020-01-23 |
US11238851B2 (en) | 2022-02-01 |
GB2553936B (en) | 2020-09-23 |
CN107430626A (en) | 2017-12-01 |
US11869489B2 (en) | 2024-01-09 |
JP6437669B2 (en) | 2018-12-12 |
DE112016000986T5 (en) | 2017-11-16 |
KR20170117590A (en) | 2017-10-23 |
EP3404654A1 (en) | 2018-11-21 |
KR20190121876A (en) | 2019-10-28 |
WO2016191540A1 (en) | 2016-12-01 |
US20220157302A1 (en) | 2022-05-19 |
KR102369953B1 (en) | 2022-03-04 |
US20160350304A1 (en) | 2016-12-01 |
US10504509B2 (en) | 2019-12-10 |
KR101870934B1 (en) | 2018-06-25 |
US20240144924A1 (en) | 2024-05-02 |
EP3262636B1 (en) | 2018-08-01 |
JP6710740B2 (en) | 2020-06-17 |
CN107430626B (en) | 2019-11-08 |
JP2019050019A (en) | 2019-03-28 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11869489B2 (en) | Providing suggested voice-based action queries | |
US11875086B2 (en) | Using user input to adapt search results provided for presentation to the user | |
US11922945B2 (en) | Voice to text conversion based on third-party agent content | |
US20220215179A1 (en) | Rendering content using a content agent and/or stored content parameter(s) |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PUAI | Public reference made under article 153(3) epc to a published international application that has entered the european phase |
Free format text: ORIGINAL CODE: 0009012 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: THE APPLICATION HAS BEEN PUBLISHED |
|
AC | Divisional application: reference to earlier application |
Ref document number: 3262636Country of ref document: EPKind code of ref document: P |
|
AK | Designated contracting states |
Kind code of ref document: A1Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: REQUEST FOR EXAMINATION WAS MADE |
|
17P | Request for examination filed |
Effective date: 20190509 |
|
RBV | Designated contracting states (corrected) |
Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: EXAMINATION IS IN PROGRESS |
|
17Q | First examination report despatched |
Effective date: 20191016 |
|
GRAP | Despatch of communication of intention to grant a patent |
Free format text: ORIGINAL CODE: EPIDOSNIGR1 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: GRANT OF PATENT IS INTENDED |
|
RIC1 | Information provided on ipc code assigned before grant |
Ipc: G06F 3/16 20060101ALI20200608BHEPIpc: G10L 15/26 20060101AFI20200608BHEPIpc: G10L 15/22 20060101ALN20200608BHEPIpc: G10L 15/18 20130101ALI20200608BHEP |
|
INTG | Intention to grant announced |
Effective date: 20200625 |
|
GRAS | Grant fee paid |
Free format text: ORIGINAL CODE: EPIDOSNIGR3 |
|
GRAA | (expected) grant |
Free format text: ORIGINAL CODE: 0009210 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: THE PATENT HAS BEEN GRANTED |
|
AC | Divisional application: reference to earlier application |
Ref document number: 3262636Country of ref document: EPKind code of ref document: P |
|
AK | Designated contracting states |
Kind code of ref document: B1Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
REG | Reference to a national code |
Ref country code: GBRef legal event code: FG4D |
|
REG | Reference to a national code |
Ref country code: CHRef legal event code: EP |
|
REG | Reference to a national code |
Ref country code: ATRef legal event code: REFRef document number: 1339202Country of ref document: ATKind code of ref document: TEffective date: 20201215 |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R096Ref document number: 602016048874Country of ref document: DE |
|
REG | Reference to a national code |
Ref country code: IERef legal event code: FG4D |
|
REG | Reference to a national code |
Ref country code: ATRef legal event code: MK05Ref document number: 1339202Country of ref document: ATKind code of ref document: TEffective date: 20201125 |
|
REG | Reference to a national code |
Ref country code: NLRef legal event code: MPEffective date: 20201125 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: GRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20210226Ref country code: NOFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20210225Ref country code: RSFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201125Ref country code: PTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20210325Ref country code: FIFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201125 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ATFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201125Ref country code: SEFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201125Ref country code: PLFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201125Ref country code: ISFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20210325Ref country code: LVFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201125Ref country code: BGFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20210225 |
|
REG | Reference to a national code |
Ref country code: LTRef legal event code: MG9D |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: HRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201125 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: LTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201125Ref country code: SKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201125Ref country code: ROFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201125Ref country code: EEFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201125Ref country code: CZFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201125Ref country code: SMFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201125 |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R097Ref document number: 602016048874Country of ref document: DE |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: DKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201125 |
|
PLBE | No opposition filed within time limit |
Free format text: ORIGINAL CODE: 0009261 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: NO OPPOSITION FILED WITHIN TIME LIMIT |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ITFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201125Ref country code: ALFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201125Ref country code: NLFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201125 |
|
26N | No opposition filed |
Effective date: 20210826 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: SIFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201125 |
|
REG | Reference to a national code |
Ref country code: CHRef legal event code: PL |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ESFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201125Ref country code: LUFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20210526Ref country code: MCFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201125Ref country code: LIFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20210531Ref country code: CHFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20210531 |
|
REG | Reference to a national code |
Ref country code: BERef legal event code: MMEffective date: 20210531 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: IEFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20210526 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ISFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20210325 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: BEFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20210531 |
|
P01 | Opt-out of the competence of the unified patent court (upc) registered |
Effective date: 20230506 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: CYFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201125 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: HUFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMIT; INVALID AB INITIOEffective date: 20160526 |
|
PGFP | Annual fee paid to national office [announced via postgrant information from national office to epo] |
Ref country code: FRPayment date: 20230525Year of fee payment: 8Ref country code: DEPayment date: 20230530Year of fee payment: 8 |
|
PGFP | Annual fee paid to national office [announced via postgrant information from national office to epo] |
Ref country code: GBPayment date: 20230529Year of fee payment: 8 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: MKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20201125 |