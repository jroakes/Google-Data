US20220383204A1 - Ascertaining and/or mitigating extent of effective reconstruction, of predictions, from model updates transmitted in federated learning - Google Patents
Ascertaining and/or mitigating extent of effective reconstruction, of predictions, from model updates transmitted in federated learning Download PDFInfo
- Publication number
- US20220383204A1 US20220383204A1 US17/535,405 US202117535405A US2022383204A1 US 20220383204 A1 US20220383204 A1 US 20220383204A1 US 202117535405 A US202117535405 A US 202117535405A US 2022383204 A1 US2022383204 A1 US 2022383204A1
- Authority
- US
- United States
- Prior art keywords
- reconstruction
- model
- prediction
- machine learning
- generating
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/098—Distributed learning, e.g. federated learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
- G06N20/20—Ensemble learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/09—Supervised learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
Definitions
- Federated learning of machine learning (ML) model(s) is an increasingly popular ML technique for training ML model(s).
- an on-device ML model is stored locally on a client device of a user, and a global ML model, that is a cloud-based counterpart of the on-device ML model, is stored remotely at a remote system (e.g., a cluster of servers).
- the client device using the on-device ML model, can process input detected at the client device to generate a prediction, and can compare the prediction to ground truth output to generate a client gradient. Further, the client device can transmit, to the remote system, a client model update that is based on the client gradient.
- the client model update can be the client gradient or can be based on the client gradient and additional generated client gradient(s).
- the client model update can be generated from a mini-batch of client gradients (e.g., 1-step, N-samples), from client gradients over several steps (e.g., N-steps, 1-sample each), or, more generally, based on gradients from K-step(s) with N-sample(s) at each step.
- the remote system can utilize the client model update, and optionally additional client model updates generated in a similar manner at additional client devices, to update weights of the global ML model.
- the remote system can transmit the global ML model, or updated weights of the global ML model, to the client device and/or to other client devices.
- Each client device can then replace the on-device ML model with the global ML model, or replace the weights of the on-device ML model with the updated weights of the global ML model, thereby updating the on-device ML model.
- federated learning enables a client device to transmit a locally generated model update, without transmitting the underlying data utilized to generate the model update (i.e., without transmitting the corresponding input(s), prediction(s), or ground truth output(s)).
- the remote system can effectively update the global ML model utilizing the model update, and without any need to access or utilize the underlying data.
- federated learning can provide a degree of data security by obviating the need to transmit the underlying (and potentially sensitive) data and instead transmitting only the model update generated based on such data.
- model updates cannot be reverse engineered to reveal information regarding the underlying data utilized to generate the model update (e.g., to reveal the input(s), the prediction(s), and/or the ground truth output(s)).
- Implementations disclosed herein relate to various techniques for ascertaining to what extent predictions, generated using a machine learning model, can be effectively reconstructed from model updates, where the model updates are generated based on those predictions and based on applying a particular loss technique (e.g., a particular cross-entropy loss technique).
- a particular loss technique e.g., a particular cross-entropy loss technique.
- some examples described herein will be described with respect to a model update that is a single gradient. However, as described herein, implementations disclosed herein can be utilized in conjunction with model updates that are based on multiple gradients.
- the predictions can each be a probability distribution or a sequence of probability distributions and the gradients can each be generated based on applying a cross-entropy based loss technique in view of the prediction and in view of a corresponding ground truth one-hot vector (when the prediction is the probability distribution) or a corresponding sequence of ground truth one-hot vectors (when the prediction is the sequence of probability distributions).
- a corresponding reconstruction of each of the predictions can be generated using matrix factorization on the gradient and using a known vocabulary of a projection output layer of the machine learning model. More generally, a corresponding reconstruction of each model update can be generated using matrix factorization on the model update and using a known vocabulary of the projection output layer.
- each reconstruction of a model update can include, for example, a bag of vocabulary reconstruction (e.g., a bag of words reconstruction when the vocabulary elements include words or word pieces) that reconstructs the vocabulary elements of the prediction(s) used in generating the model update, but not necessarily their order.
- a bag of vocabulary reconstruction e.g., a bag of words reconstruction when the vocabulary elements include words or word pieces
- Such reconstructions can each be generated using the model update and the known vocabulary, and without any reference to corresponding current weights of the machine learning model when the corresponding prediction(s) were generated and/or without reference to any other feature(s).
- each reconstruction can additionally or alternatively include an ordered sequence reconstruction.
- the ordered sequence reconstruction can be generated using a language model (or other model(s) that dictate probabilities of various sequences of the vocabulary elements) and optionally without reference to corresponding current weights of the machine learning model.
- the language model can be utilized to determine which, of multiple candidate ordered sequences of the bag of vocabulary reconstruction, is most probable, and that candidate ordered sequence utilized as the ordered sequence reconstruction.
- the ordered sequence reconstructions can be generated based on the bag of vocabulary reconstruction and further based on the corresponding current weights of the machine learning model when the corresponding prediction(s) were generated.
- a gradients matching reconstruction technique and/or other reconstruction technique(s), that rely on corresponding current weights can be utilized in generating the ordered sequence reconstructions.
- reconstruction techniques can be used with a search space that is constrained in view of (e.g., constrained to) the bag of vocabulary reconstruction. This can enable such reconstruction techniques to be performed more efficiently (i.e., with less utilization of processor resources) and/or to be more accurate (i.e., by constraining the search space to the resolved bag of vocabulary reconstruction).
- Some implementations disclosed herein generate measures that each indicate a degree of conformity between a corresponding reconstruction, generated using a corresponding model update, and corresponding prediction(s).
- the measures collectively reflect how effectively predictions can be generated from model update generated using the particular loss technique. Accordingly, the measures and/or an overall measure generated based on the measures, can indicate a degree of data security that is provided by the gradients generated using the particular loss technique.
- the measures are utilized in determining whether to utilize the particular loss technique (utilized in generating the gradients) in federated learning of the machine learning model and/or of additional machine learning model(s). For example, the measures, and/or overall measure(s) generated based on the measures, can be compared to threshold(s) and the particular loss technique utilized in federated learning only when the measures and/or overall measure(s) satisfy the threshold(s). As an additional example, the measures and/or overall measure(s) that are generated based on model updates generated utilizing a particular loss technique can additionally or alternatively be compared to alternate measures and/or alternate overall measures that are each generated based on model updates generated utilizing a corresponding alternative particular loss technique.
- the particular loss technique can be utilized only when the comparison indicates that the particular loss technique provides a greater degree of data security than the alternate particular loss technique(s).
- the particular loss technique can be cross-entropy loss with sign gradient descent
- an alternate loss technique can be cross-entropy loss with adaptive federated optimization
- an additional alternate loss can be cross-entropy loss with gradient sparsification
- a further additional alternate loss technique can be cross-entropy loss without any gradient modification technique.
- the particular loss technique can be utilized only when its measure(s) are more indicative of data security than the measure(s) for the alternate loss technique, the measure(s) for the additional alternate loss technique, and the measure(s) for the further additional loss technique.
- a certain degree of data security that is provided by gradients, generated using the particular loss technique can be ensured prior to utilization of the particular loss technique in federated learning. This can mitigate occurrences of a potentially nefarious actor being able to effectively reconstruct intercepted model updates and/or can prevent those actors from being able to differentiate between effective and ineffective reconstructions of intercepted model updates.
- a request that is transmitted by a computing device can be received over one or more networks and the request can include model update, prediction(s) pairs.
- the model update of the pairs can each be generated based on the prediction(s) of the pair and based on applying a particular loss technique.
- a reconstruction for each pair can be generated based on the model update of the pair, and measure(s) then generated that indicates a degree of conformity between the reconstruction and the prediction(s) of the pair.
- the measure(s) can reflect how effectively (e.g., whether and/or to what extent) the reconstruction conforms to the prediction(s).
- the measure(s) can include: a measure that indicates whether the bag of vocabulary reconstruction includes all elements of the prediction(s) and does not include any extra elements not in the prediction(s); and/or a measure that indicates a quantity of elements that differ between the bag of vocabulary reconstruction and the prediction(s) (e.g., a quantity of element(s) that are in the reconstruction but not the prediction(s) and a quantity of element(s) that are in the prediction(s) but not the reconstruction).
- the measure(s) can include: a measure that indicates whether the reconstruction includes all elements of the prediction(s) and in the order of the prediction(s) and does not include any extra element(s) not in the prediction(s); and/or a measure that indicates an extent to which the reconstruction and the prediction(s) differ, if at all (e.g., an edit-distance based measure or other measure that reflects difference(s) in element(s) and/or order between the reconstruction and the prediction(s)).
- the generated measures, and/or overall measure(s) generated based on the measures can be transmitted to the computing device in response to the request.
- the computing device can utilize the measure(s) and/or overall measure(s) in automatically determining whether to utilize the particular loss technique in federated learning and/or other machine learning model training.
- the transmission can additionally or alternatively cause the measures and/or overall measure(s) to be rendered (e.g., visually) at the computing device. This can enable user(s) of the computing device to ascertain (e.g., through viewing of the visual rendering) a degree a data security that is provided by the gradients and to determine, based on the degree, whether to utilize the particular loss technique in federated learning and/or other machine learning model training. In these and other manners, a certain degree of data security that is provided by gradients, generated using the particular loss technique, can be ensured prior to utilization of the particular loss technique in machine learning model training.
- the machine learning model is one that includes a projection layer having a projection input layer, weight matrix layer(s), and a projection output layer.
- the projection input layer can accept a lower dimensional generated embedding as input and the weight matrix layer(s) can be used to process the generated embedding, using current weights of the weight matrix layer(s), to generate corresponding projection output of the projection output layer.
- the projection output layer has a size that conforms to a vocabulary for the machine learning model. Put another way, the quantity of output nodes of the projection output layer can conform to the vocabulary size and each node will correspond to a particular discrete element of the vocabulary.
- the output generated over the projection output layer can be, for example, a probability distribution over the vocabulary.
- a sequence of audio data embeddings of dimension S by d (where S is the quantity of audio data embeddings and d is the dimension of each embedding) can be provided to the projection input layer and sequence and the projection output can be a sequence of outputs that are collectively of length S by V, where V is the vocabulary size.
- the elements of the vocabulary in such an example can be words or word pieces.
- the embedding provided as input to the projection input layer can be an image embedding, of an image, of dimension d (where d is the dimension of the embedding) and the projection output can be of length V, where V is the vocabulary size.
- the elements of the vocabulary in such an example can be classifications.
- Additional and/or alternative machine learning models can be utilized that can include different vocabularies and/or can accept different types of embeddings as input.
- various implementations set forth techniques to ensure at least a certain degree of security is afforded by a particular loss technique utilized in federated learning, and can be utilized to ensure that degree of security is afforded before the particular loss technique is utilized in federated learning of particular machine learning model(s).
- security of data can be enhanced for various client devices that participate in the federated learning. This can enable the benefits of federated learning to be achieved, while ensuring a certain degree of security.
- FIG. 1 illustrates an example environment in which implementations described herein can be implemented.
- FIG. 2 is a flowchart illustrating an example method of: generating, utilizing corresponding model updates, corresponding reconstructions of corresponding predictions utilized in generating the corresponding model updates; determining measures based on comparing the corresponding reconstructions to the corresponding model updates; and, optionally, performing one or more further actions based on the determined measures.
- FIG. 3 is a flowchart illustrating an example method of generating a reconstruction of a prediction using matrix factorization on a corresponding gradient and using a known vocabulary of projection output of a machine learning model utilized in generating the prediction.
- FIG. 4 illustrates an example of a projection layer of a machine learning model.
- FIG. 5 illustrates an example of an invertible matrix, an orthogonal matrix generated based on decomposing a gradient, and a resulting matrix from performing a cross product of the invertible matrix and the orthogonal matrix.
- FIG. 6 schematically depicts an example architecture of a computer system.
- Many deep learning models such as classification models, include a fully-connected layer to map a d-dimensional representation extracted from an input h to a C-dimensional vector z.
- the vector z represents the unnormalized log probabilities of its class, and Cis the number of classes.
- This fully-connected layer is referred to herein as the projection layer.
- the probability distribution over all classes ⁇ is derived by applying the softmax function on z:
- Training such a model usually involves minimizing the cross-entropy loss, as represented by
- Equation (1) applies to a loss computed from a single sample with a single label. Since introducing a new label means adding a new term to the loss, equation (1) can be generalized to various settings. For example, a model update of a N-sample mini-batch or a sequence of length N is averaged from model updates computed from each sample in the batch or each label in the sequence. In such a scenario, equation (1) can be generalized by equation (2):
- equation (1) can be generalized by equation (3):
- ⁇ W can be represented as the product of two lower-rank matrices H T ⁇ dxS and G ⁇ sxc , where S is the number of terms used to compute the model update ⁇ W. For example, if the model update is computed from a batch, S is the batch size. As another example, if the model update is aggregated from several step updates, S is the total numbers of samples at these steps.
- each row in G has a unique negative coordinate corresponding to the ground-truth label.
- Neg(u) define the indices of negative coordinates in a vector u.
- ⁇ W can be decomposed into P ⁇ Q, where P ⁇ dxs and Q ⁇ sxc are orthogonal matrices, and ⁇ ⁇ SXS is diagonal matrix with non-negative elements on the diagonal.
- a screening round can be applied to filter inseparable columns.
- the screening round returns all points that are separable from a sampled subset of remaining points (e.g., using the Perceptron algorithm). This can be significantly faster and/or more computationally efficient than solving the LP problem.
- the below algorithm provides an overview of some implementations of obtaining a set of labels (i.e., a bag of vocabulary) from a model update.
- FIG. 1 illustrates an example environment in which implementations described herein can be implemented.
- the example environment includes client devices 106 A-N, a federated learning system 110 , a reconstruction system 120 , and one or more networks 108 .
- the client devices 106 A-N, the federated learning system 110 , and/or the reconstruction system 120 can communicate with one another via the network(s) 108 .
- the network(s) 108 can include wide area network(s) (WAN(s)) (e.g., the Internet) and/or local area network(s) (LAN(s)).
- WAN(s) wide area network
- LAN(s) local area network
- the client devices 106 A-N can include a client device via which a user can interact with the reconstruction system 120 , which can be located remote from the client device (in other implementations reconstruction system 120 can be implemented in whole or in part on the client device).
- the user can interact with client device 108 A (via user interface input device(s) of the client device 108 A) to cause the client device to transmit model update, prediction(s) pairs to reconstruction system 120 .
- the reconstruction system 120 can generate measure(s) based on the transmitted pairs and then transmit the measure(s) to the client device 108 A.
- the client device 108 A can utilize the measure(s) and/or overall measure(s) in automatically determining whether to utilize the particular loss technique in federated learning and/or other machine learning model training.
- the client device 108 A can additionally or alternatively cause the measures and/or overall measure(s) to be rendered (e.g., visually) at the client device 108 A. This can enable user(s) of the client device 108 A to ascertain (e.g., through viewing of the visual rendering via a screen of the client device 108 A) a degree a data security that is provided by the gradients and to determine, based on the degree, whether to utilize the particular loss technique in federated learning and/or other machine learning model training.
- the user can interact with client device 108 A (via user interface input device(s) of the client device 108 A) to cause the client device to transmit model updates to reconstruction system 120 .
- the reconstruction system 120 can generate reconstructions that each correspond to one of the transmitted model updates, and then transmit, to the client device 108 A, the reconstructions and indications of which reconstructions correspond to which model updates.
- the client device 108 A can generate measure(s) and/or overall measure(s) based on comparing the reconstructions to actual predictions that are stored locally at the client device 108 A or otherwise accessible at the client device 108 A.
- the client device 108 A can match the received reconstructions to corresponding predictions based on the received indications of which reconstructions correspond to which model updates (e.g., using a locally stored mapping of the model updates to the predictions). Accordingly, in such an example, the client device 108 A transmits only the model updates to the reconstruction system 120 , without transmitting the predictions. Moreover, the reconstruction system 120 returns reconstructions generated based on the model updates, enabling the client device 108 A to generate the measures based on the returned reconstructions.
- the client devices 106 A-N can additionally or alternatively include client devices that interact with the federated learning system 110 in participating in federated learning of a global machine learning (ML) model 118 .
- ML machine learning
- each of the client devices 106 A-N is illustrated as including a corresponding one of local ML models 108 A-N stored locally at the client device.
- the local ML models 108 A-N are each a local counterpart to the global ML model 118 , which is managed by the federated learning system 110 .
- each of the client devices 106 A-N using its corresponding one of the on-device ML models 108 A-N, can process corresponding input (e.g., an input based on user interface input detected at the client device and/or based on corresponding locally stored data at the client device) to generate a prediction, and can compare the prediction to ground truth output to generate a client gradient.
- corresponding input e.g., an input based on user interface input detected at the client device and/or based on corresponding locally stored data at the client device
- ground truth output can be utilized in generating the client gradients.
- the ground truth output can be based on other data generated locally at the client device, and can optionally be based on user input(s) (e.g., that explicitly or implicitly confirm the prediction or that explicitly or implicitly indicate an alternate ground truth that is different from the prediction).
- the client devices 106 A-N can transmit model updates, that are based on their locally generated client gradients, to the federated learning system 110 .
- the model updates can be transmitted to the federated learning system 110 without transmission of the predictions or the ground truth outputs that were utilized in generating the model updates.
- the federated learning system 110 can utilize received client model updates, and optionally additional client model updates generated in a similar manner at additional client devices, to update weights of the global ML model 118 .
- the federated learning system 110 can transmit the updated global ML model 118 , or updated weights of the global ML model 118 , to the client devices 108 A-N and/or to other client devices.
- Each client device can then replace the on-device ML model with the updated global ML model, or replace the weights of the on-device ML model with the updated weights of the global ML model 118 , thereby updating the on-device ML models.
- Further federated learning can optionally occur based on the updated on-device ML models, resulting in a further updated global ML model 118 , which can again be provided (or weights thereof provided) with the client devices 106 A-N. This process can continue for multiple iterations, optionally until the ML models are deemed final based on one or more conditions being satisfied.
- the federated learning system 110 can be implemented, for example, by one or more servers, such as a cluster of optionally distributed high-performance servers.
- the client devices 106 A-N can include one or more of: a desktop computing device, a laptop computing device, a standalone hardware device at least in part dedicated to an automated assistant, a tablet computing device, a mobile phone computing device, a computing device of a vehicle (e.g., an in-vehicle communications system, and in-vehicle entertainment system, an in-vehicle navigation system, an in-vehicle navigation system), or a wearable apparatus of the user that includes a computing device (e.g., a watch of the user having a computing device, glasses of the user having a computing device, a virtual or augmented reality computing device). Additional and/or alternative client devices may be provided.
- Client devices 106 A-N can each include one or more memories for storage of data and software applications, one or more processors for accessing data and executing applications, and other components that facilitate communication over a network.
- the reconstruction system 120 can be implemented, for example, by a client device and/or by one or more servers, such as a cluster of optionally distributed high-performance servers.
- the reconstruction system 120 is illustrated in FIG. 1 as including a reconstruction engine 122 , a measure engine 124 , and a selection engine 126 .
- the reconstruction engine 122 processes model updates 134 and generates, for each of the model updates 134 , a corresponding reconstruction.
- the model updates 134 processed at a given time can be provided by one of the client devices 106 A, the federated learning system 110 , or even locally generated by the reconstruction system 120 .
- the model updates 134 can optionally each be paired with corresponding one(s) of the predictions 136 as described herein (e.g., paired via mappings defining associations between corresponding model updates and predictions).
- the reconstruction engine 122 can use matrix factorization on the model update and use a known vocabulary 132 of a projection output layer of a corresponding ML model.
- a known vocabulary 132 of the projection output layer of the local ML models 108 A-N can be used in generating the reconstruction.
- the known vocabulary 132 is provided by a developer or other user with knowledge of the vocabulary of the machine learning model (e.g., provided in or along with a request that includes corresponding gradients).
- the known vocabulary 132 is determined from inspection of the machine learning model and/or from providing multiple known inputs to the machine learning model and inspecting corresponding predictions and/or model updates. Providing multiple known inputs to the machine learning model and inspecting corresponding predictions and/or model update enables resolution of which output dimensions correspond to which elements of the vocabulary. For example, since the prediction, that should be generated from the known input, is also known, it can be determined from the prediction and/or the model update, which output dimensions correspond to element(s) of the vocabulary for the prediction. Through utilization of multiple known inputs and corresponding known predictions, some, or all, of the vocabulary can be effectively derived through analysis of actually generated predictions and/or model updates.
- the reconstruction engine 122 generates a reconstruction that includes, or is restricted to, a bag of vocabulary reconstruction that reconstructs the vocabulary elements of the prediction(s), but not necessarily their order. Put another way, the reconstruction seeks to reconstruct the vocabulary elements of the prediction(s) without regard to their order. While reconstruction could, by happenstance, include the vocabulary elements in the correct order, the reconstruction does not seek to determine the correct order.
- Such reconstructions can each be generated the reconstruction engine 122 using the model update and the known vocabulary, and without any reference to corresponding current weights of the machine learning model when the corresponding prediction was generated and/or without reference to any other feature(s).
- the reconstruction engine 122 can additionally or alternatively generate a reconstruction that is an ordered sequence reconstruction.
- the reconstruction engine 122 can generate the ordered sequence reconstruction based on the bag of vocabulary reconstruction and further based on a language model (or more generally, a vocabulary model) and/or based on the corresponding current weights of the machine learning model when the corresponding prediction was generated. In various implementations, in generating a reconstruction, the reconstruction engine 122 performs some or all aspects of step 256 A of FIG. 3 (described below).
- the measure engine 124 compares reconstructions, generated by the reconstruction engine, to their corresponding predictions 136 , and generates measure(s) based on the comparisons. For example, the measure engine can compare a generated reconstruction, generated based on a given one of the model updates 134 , to a given one of the predictions 136 that is indicated as paired with the given one of the model updates 134 .
- the given one of the predictions 136 can be prediction(s) that were actually generated, using the corresponding ML model, and that were utilized in generating the given one of the model updates 134 (e.g., based on comparing the prediction(s) to ground truth output(s)).
- the measure generated by the measure engine 124 for a reconstruction can reflect how effectively (e.g., whether and/or to what extent) the reconstruction conforms to the prediction(s). For example, if the reconstruction is a bag of vocabulary reconstruction, the measure engine 124 can generate a measure that is a “1.0” if the bag of vocabulary reconstruction includes all elements of the prediction(s) and does not include any extra elements not in the prediction(s), and that otherwise is a “0.0”. As another example, if the reconstruction is a bag of vocabulary reconstruction, the measure engine 124 can additionally or alternatively generate a measure that is non-binary and that reflects a quantity of elements that differ between the bag of vocabulary reconstruction and the prediction(s).
- the measure can be “1.0” is no elements differ, “0.75” is one of four elements differ, “0.5” if three of six elements differ, “0.0” if all elements differ, and so forth.
- the measure engine 124 can also optionally generate overall measure(s) as a function of the individual measures for the reconstructions.
- the overall measure(s) can include a mean of the individual measures, a median of the individual measures, a standard deviation the individual measures, and/or other overall measure(s) that are a function of the individual measures.
- the measure engine 124 in generating an individual measure, performs some or all aspects of step 258 of FIG. 2 (described below).
- the selection engine 126 analyzes measure(s) (e.g., individual and/or overall), generated by the measures engine 124 for model updates generated according to a particular loss technique, in determining whether to utilize the particular loss technique (e.g., in federated learning of the corresponding machine learning model and/or of additional machine learning model(s)). Accordingly, in various implementations the selection engine 126 can determine whether to select the particular loss technique for usage or, instead, to select an alternative loss technique for usage.
- measure(s) e.g., individual and/or overall
- the measures engine 124 for model updates generated according to a particular loss technique, in determining whether to utilize the particular loss technique (e.g., in federated learning of the corresponding machine learning model and/or of additional machine learning model(s)). Accordingly, in various implementations the selection engine 126 can determine whether to select the particular loss technique for usage or, instead, to select an alternative loss technique for usage.
- the selection engine 126 compares the individual measures and/or overall measure(s), generated by the measures engine 124 for model updates generated according to a particular loss technique, to threshold(s). In those implementations, the selection engine 126 can determine whether to utilize the particular loss technique based at least in part (e.g., solely and/or based on other consideration(s)) based on whether the measures and/or overall measure(s) satisfy the threshold(s).
- the selection engine 126 compares: (a) the individual measures and/or overall measure(s), generated by the measures engine 124 for model updates generated according to a particular loss technique to (b) alternate individual measures and/or alternate overall measure(s), generated by the measures engine 124 for alternate gradients generated according to an alternate particular loss technique.
- the selection engine 126 can determine whether to select the particular loss technique or, instead, the alternate particular loss technique, based on the comparison (e.g., solely based on the comparison or also based on the threshold being satisfied as described in the preceding paragraph). For example, the measures engine 124 can determine to select the particular loss technique for utilization only when the comparison indicates that the particular loss technique provides a greater degree of data security than the alternate particular loss technique(s).
- the selection engine 126 performs some or all aspects of step 262 of FIG. 2 (described below).
- FIG. 2 is a flowchart illustrating an example method 200 of: generating, utilizing corresponding model updates, corresponding reconstructions of corresponding predictions utilized in generating the corresponding gradients; determining measures based on comparing the corresponding reconstructions to the corresponding predictions; and, optionally, performing one or more further actions based on the determined measures.
- This system may include various components of various computer systems, such as one or more components of reconstruction system 120 of FIG. 1 .
- operations of process 200 are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted, and/or added.
- the system receives model update, prediction(s) pairs.
- the model update, prediction(s) pairs each include: prediction(s) generated based on processing corresponding input(s) using a machine learning model; and a model update generated based on applying a particular loss technique and based on corresponding ground truth input(s) (e.g., generated based on gradient(s) each generated based on comparing the corresponding ground truth input(s) to the prediction(s).
- the model update can be a single gradient generated based on comparing a single prediction and ground truth pair or can be a model update generated based on a batch of gradients generated based on comparing multiple predictions and their corresponding ground truths.
- the model update, prediction(s) pairs received at block 252 can be generated by a component of the system and received from that component, or can be received in a transmission, and via a network, from another system or client device.
- the system identifies a model update, prediction(s) pair from those received at block 252 .
- block 256 the system generates, using the model update of the identified pair and independent of the prediction of the identified pair, a reconstruction of the prediction.
- block 256 includes sub-block 256 A, in which the system generates the reconstruction using matrix factorization on the model update and using a known vocabulary of projection output of the machine learning model.
- block 256 A is described below with respect to FIG. 3 .
- the system generates a measure based on comparing the reconstruction, of block 256 for the pair, to the prediction(s) of the pair.
- the system can store (e.g., in ROM or RAM) the measure.
- the system determines whether there are any unprocessed model update, prediction(s) pairs. If so, the system proceeds back to block 254 and identifies an unprocessed pair. If not, the system optionally proceeds to optional block 262 and/or optional block 264 . It is noted that, although shown serially in FIG. 2 for convenience, in various implementations the system can perform multiple iterations of blocks 254 , 256 , and 258 in parallel (i.e., each iteration being performed in parallel will involve processing a different pair).
- the system determines, based on the measures generated via multiple iterations of block 258 (e.g., retrieving them from RAM or ROM), whether to utilize the particular loss technique in federated training. In some implementations, the system determines whether to utilize the particular loss technique in federated training based on the measures themselves and/or based on overall measure(s) that are generated based on the individual measure generated via multiple iterations of block 258 .
- the system determines to utilize the particular loss technique in federated training only when some (e.g., X % of) or all of the individual measures satisfy an individual threshold and/or only when some (e.g., X % of) or all of the overall measure(s) satisfy a corresponding overall threshold. In some additional or alternative implementations, the system determines whether to utilize the particular loss technique in federated training based on comparing the individual measures and/or overall measure(s), for the particular loss technique, to individual measures and/or overall measure(s) for one or more alternate particular loss techniques. The individual measures and/or overall measure(s) for an alternate particular loss technique can be generated based on performing blocks 252 , 254 , 256 , and 258 based on pairs that include model updates generated using the alternate particular loss technique.
- the system transmits, in response to receiving the pairs at block 252 , the individual measures generated via multiple iterations of block 258 and/or overall measure(s) generated based on the generated individual measures.
- the pairs at block 252 can be received in a request from a server or a client device, and the system can transmit the individual measures and/or the overall measure(s) to the server or the client device.
- the individual measures and/or the overall measure(s) can be included in a graphical user interface that is generated by the system, and the graphical user interface transmitted to the client device. Transmitting of the graphical user interface to the client device can cause (e.g., after corresponding user input(s) at the client device) the client device to visually render the individual measures and/or overall measure(s).
- FIG. 3 is a flowchart illustrating one non-limiting example of block 256 A of FIG. 2 .
- the system identifies a gradient.
- the gradient can be the model update from one of the pairs of FIG. 2 .
- the gradient can optionally be generated based on applying a cross-entropy based loss technique and based on a ground truth output and a prediction.
- the prediction is one generated based on processing input using a machine learning model.
- the system decomposes the gradient into at least an S by V orthogonal matrix (Q), where S corresponds to a number of sequences in the prediction and V corresponds to a vocabulary size of the machine learning model.
- Q an S by V orthogonal matrix
- Each column, in the matrix Q can represent an S-dimensional point that corresponds to an element in the vocabulary.
- the system can decompose the gradient into orthogonal matrix Q using singular value decomposition.
- the system can use singular value decomposition to decompose the gradient into two orthogonal matrices, Q and P (which can also be an S by V matrix) and a diagonal matrix ⁇ .
- Block 256 A 3 the system determines which columns in Q include a separating classifier.
- Block 256 A 3 can include sub-block 256 A 3 A, in which the system performs a dot product of Q and Z, where Z is an S by S invertible matrix, and identifies, based on the result, resulting column(s) of Q that include a separating value (e.g., negative value).
- the system can identify, in the matrix ⁇ 0 that results from the dot product of Q and Z, row(s) that include a separating value (e.g., a negative value) and identify the column(s) of Q that have the same index value as the row(s).
- a row that has a separating value in the resulting matrix will indicate that the corresponding column of Q likewise has a separating value.
- an example Z invertible matrix 123 A (of size S by S) is illustrated being crossed with an example Q orthogonal matrix 123 B (of size S by V), resulting in an example ⁇ 0 matrix 123 C (of size S by V).
- the second row of ⁇ 0 matrix 123 C (illustrated with shading) is the result of the cross product of the second row of Z invertible matrix 123 A (illustrated with shading) and the second column of Q orthogonal matrix 123 B (illustrated with shading).
- the second row of ⁇ 0 matrix 123 C includes a separating value, indicated by the vertical shading of the cell in the second row and second column (as opposed to the diagonal shading of the other cells of the second row.
- the second row of ⁇ 0 matrix 123 C can be determined to have a separating value based on one of the cells being differentiable with respect to all other cells of the row. For example, one of the cells of the second can be negative and all other cells of the row can be positive. This can indicate that the second column of Q orthogonal matrix 123 B (having the same “second” index value) likewise has a separating value. It is noted that additional rows of ⁇ 0 matrix 123 C can have separating values and, as a result, additional columns of Q can be determined to have additional separating values. However, only one such example is illustrated in FIG. 5 for simplicity.
- the matrices 123 A, 123 B, and 123 C are only illustrated with some of their cells, as indicated by the ellipsis, for purposes of simplicity.
- Various dimensioned matrices can be provided, and the dimensions will be dependent on the corresponding vocabulary size and sequence length, as described herein.
- block 256 A 4 the system generates a reconstruction of the prediction, using the column(s) of Q determined to include a separating classifier and a mapping of the columns of Q to the vocabulary of the machine learning models.
- block 256 A 4 includes sub-block 256 A 4 A and optionally sub-block 256 A 4 B.
- the system generates a bag of vocabulary reconstruction which can include an unordered listing of those elements of the vocabulary that correspond to the columns of Q determined to include a separating classifier.
- the system generates, optionally using the current state of model and using the bag of vocabulary reconstruction of sub-block 256 A 4 A, an ordered sequence reconstruction. It is noted that the current state of the model is not utilized in generating the bag of vocabulary reconstruction at sub-block 256 A 4 A. In some implementations, at sub-block 256 A 4 B, the system does not utilize the current state of the model but, rather, relies on the bag of vocabulary reconstruction and a vocabulary model that dictates probabilities of various sequences of the vocabulary elements. For example, where the vocabulary includes words or word sequences, the vocabulary model can be a language model.
- the system can utilize the language model to determine which, of multiple candidate ordered sequences of the bag of vocabulary reconstruction, is most probable, and that candidate ordered sequence utilized as the ordered sequence reconstruction.
- the system at sub-block 256 A 4 B, the system generates the ordered sequence reconstruction based on the bag of vocabulary reconstruction and further based on the corresponding current weights of the machine learning model when the corresponding prediction was generated.
- the system uses gradients matching reconstruction technique and/or other reconstruction technique(s), that rely on corresponding current weights, in generating the ordered sequence reconstructions.
- the system uses such reconstruction technique(s) with a search space that is constrained in view of (e.g., constrained to) the bag of vocabulary reconstruction.
- the system stores the reconstruction, generated at block 256 A 4 , and an association of the reconstruction to the gradient utilized in generating the reconstruction.
- ⁇ W represents the gradient, which is with respect to the weight matrix (W) of a corresponding projection layer.
- a T represents the transpose of the projection input (i.e., the dimensions of the embedding(s) as well as a length of the sequence of the embedding(s)).
- ⁇ 0 represents the gradient with respect to the projection output, which is unknown but can be resolved as described herein.
- P ⁇ Q By decomposing ⁇ W into P ⁇ Q, P ⁇ Q can be rewritten as P ⁇ (Z ⁇ 1 Z)Q, where Z is any S by S invertible matrix, and further rewritten as (P ⁇ Z ⁇ 1 )(ZQ).
- (P ⁇ Z ⁇ 1 ) is equivalent to A T , meaning that ⁇ 0 is equal to (ZQ) and, thus, ⁇ 0 can be resolved by a cross product of Z and Q.
- a row in ⁇ 0 that includes a separating value e.g., a negative value
- FIG. 4 illustrates an example of a projection layer of a machine learning model, such as global model 118 ( FIG. 1 ) and local models 108 A-N( FIG. 1 ).
- the projection layer includes a projection input layer 118 A, weight matrix layer(s) 118 B, and a projection output layer 118 C.
- the projection input layer 118 A can accept a lower dimensional generated embedding (of dimension d) as input and the weight matrix layer(s) 118 B can be used to process the generated embedding, using current weights of the weight matrix layer(s) 118 B, to generate corresponding projection output (of dimension V) of the projection output layer 118 C.
- the projection output layer 118 C has a size (V) that conforms to a vocabulary for the machine learning model.
- the quantity of output nodes of the projection output layer 118 C can conform to the vocabulary size and each node will correspond to a particular discrete element of the vocabulary.
- the output generated over the projection output layer 118 C can be, for example, a probability distribution over the vocabulary.
- FIG. 6 is a block diagram of an example computing device 610 that may optionally be utilized to perform one or more aspects of techniques described herein.
- a client device can include one or more aspects of the example computing device 610 and/or a server can include one or more aspects of the example computing device 610 .
- Computing device 610 typically includes at least one processor 614 which communicates with a number of peripheral devices via bus subsystem 612 .
- peripheral devices may include a storage subsystem 624 , including, for example, a memory subsystem 625 and a file storage subsystem 626 , user interface output devices 620 , user interface input devices 622 , and a network interface subsystem 616 .
- the input and output devices allow user interaction with computing device 610 .
- Network interface subsystem 616 provides an interface to outside networks and is coupled to corresponding interface devices in other computing devices.
- User interface input devices 622 may include a keyboard, pointing devices such as a mouse, trackball, touchpad, or graphics tablet, a scanner, a touchscreen incorporated into the display, audio input devices such as voice recognition systems, microphones, and/or other types of input devices.
- pointing devices such as a mouse, trackball, touchpad, or graphics tablet
- audio input devices such as voice recognition systems, microphones, and/or other types of input devices.
- use of the term “input device” is intended to include all possible types of devices and ways to input information into computing device 610 or onto a communication network.
- User interface output devices 620 may include a display subsystem, a printer, a fax machine, or non-visual displays such as audio output devices.
- the display subsystem may include a cathode ray tube (CRT), a flat-panel device such as a liquid crystal display (LCD), a projection device, or some other mechanism for creating a visible image.
- the display subsystem may also provide non-visual display such as via audio output devices.
- output device is intended to include all possible types of devices and ways to output information from computing device 610 to the user or to another machine or computing device.
- Storage subsystem 624 stores programming and data constructs that provide the functionality of some or all of the modules described herein.
- the storage subsystem 624 may include the logic to perform selected aspects of the methods of FIGS. 2 , 3 , and/or other methods described herein.
- Memory 625 used in the storage subsystem 624 can include a number of memories including a main random access memory (RAM) 630 for storage of instructions and data during program execution and a read only memory (ROM) 632 in which fixed instructions are stored.
- a file storage subsystem 626 can provide persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive along with associated removable media, a CD-ROM drive, an optical drive, or removable media cartridges.
- the modules implementing the functionality of certain implementations may be stored by file storage subsystem 626 in the storage subsystem 624 , or in other machines accessible by the processor(s) 614 .
- Bus subsystem 612 provides a mechanism for letting the various components and subsystems of computing device 610 communicate with each other as intended. Although bus subsystem 612 is shown schematically as a single bus, alternative implementations of the bus subsystem may use multiple busses.
- Computing device 610 can be of varying types including a workstation, server, computing cluster, blade server, server farm, or any other data processing system or computing device. Due to the ever-changing nature of computers and networks, the description of computing device 610 depicted in FIG. 6 is intended only as a specific example for purposes of illustrating some implementations. Many other configurations of computing device 610 are possible having more or fewer components than the computing device depicted in FIG. 6 .
- the systems described herein collect personal information about users (or as often referred to herein, “participants”), or may make use of personal information
- the users may be provided with an opportunity to control whether programs or features collect user information (e.g., information about a user's social network, social actions or activities, profession, a user's preferences, or a user's current geographic location), or to control whether and/or how to receive content from the content server that may be more relevant to the user.
- user information e.g., information about a user's social network, social actions or activities, profession, a user's preferences, or a user's current geographic location
- certain data may be treated in one or more ways before it is stored or used, so that personal identifiable information is removed.
- a user's identity may be treated so that no personal identifiable information can be determined for the user, or a user's geographic location may be generalized where geographic location information is obtained (such as to a city, ZIP code, or state level), so that a particular geographic location of a user cannot be determined.
- geographic location information such as to a city, ZIP code, or state level
- the user may have control over how information is collected about the user and/or used.
- a method implemented by one or more processors includes receiving a plurality of model update, prediction(s) pairs.
- Each of the model update, prediction(s) pairs include: (a) at least one corresponding prediction, generated based on processing a corresponding input using a machine learning model with corresponding current weights; and (b) a corresponding model update generated based on at least one gradient, where the at least one gradient is generated based on applying a particular loss technique and generated based at least in part on the corresponding prediction and a corresponding ground truth output.
- the method further includes, for each of the model update, prediction(s) pairs: generating, using the corresponding model update and a known vocabulary of a projection output of the machine learning model, a reconstruction of the corresponding prediction; and generating, based on comparing the reconstruction to the corresponding prediction, a corresponding measure that reflects a degree of conformity between the reconstruction to the corresponding prediction. Generating the reconstruction is performed independent of the corresponding prediction.
- the method further includes determining, based on the corresponding measures for the model update, prediction(s) pairs, whether to utilize the particular loss technique in federated training of the machine learning model or of an additional machine learning model.
- the method further includes, in response to determining to utilize the particular loss technique in federated training of the machine learning model or of the additional machine learning model: causing the machine learning model or the additional machine learning model to be locally stored on a plurality of client devices, along with corresponding instructions.
- the corresponding instructions cause the client devices to locally generate model updates, for the machine learning model or the additional machine learning model, using the particular loss techniques, and transmit the model updates to one or more remote servers.
- determining, based on the corresponding measures, whether to utilize the particular loss technique in federated training of the machine learning model or of an additional machine learning model includes: generating an overall measure based on the corresponding measures; comparing the overall measure to a threshold; and determining to utilize the particular loss technique in federated training in response to the overall measure satisfying the threshold.
- determining, based on the corresponding measures, whether to utilize the particular loss technique in federated training of the machine learning model or of an additional machine learning model includes: generating an overall measure based on the corresponding measures; comparing the overall measure to an alternate overall measure, the alternate overall measure generated based on alternate model update, prediction(s) pairs having alternate corresponding model updates generated based on an alternate particular loss technique that differs from the particular loss technique; and determining, in response to the comparing, to utilize the particular loss technique in federated training.
- the particular loss technique is cross-entropy loss without any gradient modification technique and the alternate particular loss technique is cross-entropy loss with at least one gradient modification technique.
- the at least one gradient modification technique includes sign gradient descent and/or adaptive federated optimization.
- the particular loss technique is cross-entropy loss with a first gradient modification technique (or a first combination of gradient modification techniques) and the alternate particular loss technique is cross-entropy loss with a second gradient modification technique (or a second combination of gradient modification techniques).
- generating, using the corresponding model update and known labels of a projection output of the machine learning model, the reconstruction of the corresponding prediction includes generating the reconstruction using matrix factorization on the model update and using the known vocabulary of projection output of the machine learning model.
- the reconstruction can include (e.g., be restricted to) a bag of vocabulary reconstruction.
- the reconstruction can additionally or alternatively include an ordered sequence reconstruction and generating the reconstruction can further include generating the ordered sequence reconstruction using the corresponding current weights of the model.
- generating the reconstruction using matrix factorization on the model update and using a known vocabulary of projection output of the machine learning model includes: decomposing the model update into an S by V orthogonal matrix, where S corresponds to a number of sequences in the prediction and where V corresponds to a size of the known vocabulary; determining which columns, in the S by V orthogonal matrix, include a separating classifier; and generating the reconstruction using the columns, determined to include the separating classifier, and a mapping of the columns to the known vocabulary.
- determining which columns, in the S by V orthogonal matrix, include the separating classifier includes: performing a dot product of the S by V orthogonal matrix and an S by S invertible matrix; and determining, based on analysis of rows of the resulting matrix from the dot product, which rows include a negative value; and determining the columns include the separating classifier based on the columns corresponding to (e.g., having the same index value as) the rows that include the negative value.
- a method implemented by one or more processors includes receiving, via a network, a request from a computing device.
- the request includes a plurality of model update, prediction(s) pairs.
- Each of the model update, prediction(s) pairs include: (a) at least one corresponding prediction, generated based on processing a corresponding input using a machine learning model with corresponding current weights; and (b) a corresponding model update generated based on at least one gradient, where the at least one gradient is generated based on applying a particular loss technique and generated based at least in part on the corresponding prediction and a corresponding ground truth output.
- the method further includes, for each of the model update, prediction(s) pairs: generating, using the corresponding model update and a known vocabulary of a projection output of the machine learning model, a reconstruction of the corresponding prediction; and generating, based on comparing the reconstruction to the corresponding prediction, a corresponding measure that reflects conformity of the reconstruction to the corresponding prediction. Generating the reconstruction is performed independent of the corresponding prediction.
- the method further includes transmitting, via the network and to the computing device in response to the request, the corresponding measures for the model update, prediction(s) pairs, and/or an overall measure based on the corresponding measures.
- a method implemented by one or more processors includes receiving, via a network, a request from a computing device.
- the request includes a plurality of model updates.
- Each of the model updates is generated based on applying a particular loss technique and based at least in part on a corresponding prediction and a corresponding ground truth output.
- the corresponding prediction is generated based on processing a corresponding input using a machine learning model with corresponding current weights.
- the method further includes, for each of the model updates, generating, using the corresponding model update and a known vocabulary of a projection output of the machine learning model, a reconstruction of the corresponding prediction. Generating the reconstruction is performed independent of the corresponding prediction.
- the method further includes transmitting, via the network and to the computing device in response to the request, the reconstructions of the corresponding predictions.
- some implementations include one or more processors (e.g., central processing unit(s) (CPU(s)), graphics processing unit(s) (GPU(s), and/or tensor processing unit(s) (TPU(s)) of one or more computing devices, where the one or more processors are operable to execute instructions stored in associated memory, and where the instructions are configured to cause performance of any of the methods described herein.
- processors e.g., central processing unit(s) (CPU(s)), graphics processing unit(s) (GPU(s), and/or tensor processing unit(s) (TPU(s)
- Some implementations also include one or more transitory or non-transitory computer readable storage media storing computer instructions executable by one or more processors to perform any of the methods described herein.
Abstract
Implementations relate to ascertaining to what extent predictions, generated using a machine learning model, can be effectively reconstructed from model updates, where the model updates are generated based on those predictions and based on applying a particular loss technique (e.g., a particular cross-entropy loss technique). Some implementations disclosed generate measures that each indicate a degree of conformity between a corresponding reconstruction, generated using a corresponding model update, and a corresponding prediction. In some of those implementations, the measures are utilized in determining whether to utilize the particular loss technique (utilized in generating the model updates) in federated learning of the machine learning model and/or of additional machine learning model(s).
Description
- Federated learning of machine learning (ML) model(s) is an increasingly popular ML technique for training ML model(s). In federated learning, an on-device ML model is stored locally on a client device of a user, and a global ML model, that is a cloud-based counterpart of the on-device ML model, is stored remotely at a remote system (e.g., a cluster of servers). The client device, using the on-device ML model, can process input detected at the client device to generate a prediction, and can compare the prediction to ground truth output to generate a client gradient. Further, the client device can transmit, to the remote system, a client model update that is based on the client gradient. For example, the client model update can be the client gradient or can be based on the client gradient and additional generated client gradient(s). For instance, the client model update can be generated from a mini-batch of client gradients (e.g., 1-step, N-samples), from client gradients over several steps (e.g., N-steps, 1-sample each), or, more generally, based on gradients from K-step(s) with N-sample(s) at each step. The remote system can utilize the client model update, and optionally additional client model updates generated in a similar manner at additional client devices, to update weights of the global ML model. The remote system can transmit the global ML model, or updated weights of the global ML model, to the client device and/or to other client devices. Each client device can then replace the on-device ML model with the global ML model, or replace the weights of the on-device ML model with the updated weights of the global ML model, thereby updating the on-device ML model.
- Accordingly, federated learning enables a client device to transmit a locally generated model update, without transmitting the underlying data utilized to generate the model update (i.e., without transmitting the corresponding input(s), prediction(s), or ground truth output(s)). Further, the remote system can effectively update the global ML model utilizing the model update, and without any need to access or utilize the underlying data. In these and other manners, federated learning can provide a degree of data security by obviating the need to transmit the underlying (and potentially sensitive) data and instead transmitting only the model update generated based on such data. However, to ensure data security and/or increase the degree of data security, it is important that at least some (e.g., all, more than half, etc.) generated model updates cannot be reverse engineered to reveal information regarding the underlying data utilized to generate the model update (e.g., to reveal the input(s), the prediction(s), and/or the ground truth output(s)).
- Implementations disclosed herein relate to various techniques for ascertaining to what extent predictions, generated using a machine learning model, can be effectively reconstructed from model updates, where the model updates are generated based on those predictions and based on applying a particular loss technique (e.g., a particular cross-entropy loss technique). For the sake of simplicity, some examples described herein will be described with respect to a model update that is a single gradient. However, as described herein, implementations disclosed herein can be utilized in conjunction with model updates that are based on multiple gradients.
- As an example, the predictions can each be a probability distribution or a sequence of probability distributions and the gradients can each be generated based on applying a cross-entropy based loss technique in view of the prediction and in view of a corresponding ground truth one-hot vector (when the prediction is the probability distribution) or a corresponding sequence of ground truth one-hot vectors (when the prediction is the sequence of probability distributions). Continuing with the example, a corresponding reconstruction of each of the predictions can be generated using matrix factorization on the gradient and using a known vocabulary of a projection output layer of the machine learning model. More generally, a corresponding reconstruction of each model update can be generated using matrix factorization on the model update and using a known vocabulary of the projection output layer.
- In some implementations, each reconstruction of a model update can include, for example, a bag of vocabulary reconstruction (e.g., a bag of words reconstruction when the vocabulary elements include words or word pieces) that reconstructs the vocabulary elements of the prediction(s) used in generating the model update, but not necessarily their order. Such reconstructions can each be generated using the model update and the known vocabulary, and without any reference to corresponding current weights of the machine learning model when the corresponding prediction(s) were generated and/or without reference to any other feature(s). In some implementations, each reconstruction can additionally or alternatively include an ordered sequence reconstruction. In some of those implementations, the ordered sequence reconstruction can be generated using a language model (or other model(s) that dictate probabilities of various sequences of the vocabulary elements) and optionally without reference to corresponding current weights of the machine learning model. For example, the language model can be utilized to determine which, of multiple candidate ordered sequences of the bag of vocabulary reconstruction, is most probable, and that candidate ordered sequence utilized as the ordered sequence reconstruction. As another example, the ordered sequence reconstructions can be generated based on the bag of vocabulary reconstruction and further based on the corresponding current weights of the machine learning model when the corresponding prediction(s) were generated. Optionally, in such an example, a gradients matching reconstruction technique and/or other reconstruction technique(s), that rely on corresponding current weights, can be utilized in generating the ordered sequence reconstructions. However, it is noted that such reconstruction techniques can be used with a search space that is constrained in view of (e.g., constrained to) the bag of vocabulary reconstruction. This can enable such reconstruction techniques to be performed more efficiently (i.e., with less utilization of processor resources) and/or to be more accurate (i.e., by constraining the search space to the resolved bag of vocabulary reconstruction).
- Some implementations disclosed herein generate measures that each indicate a degree of conformity between a corresponding reconstruction, generated using a corresponding model update, and corresponding prediction(s). The measures collectively reflect how effectively predictions can be generated from model update generated using the particular loss technique. Accordingly, the measures and/or an overall measure generated based on the measures, can indicate a degree of data security that is provided by the gradients generated using the particular loss technique.
- In some of those implementations, the measures are utilized in determining whether to utilize the particular loss technique (utilized in generating the gradients) in federated learning of the machine learning model and/or of additional machine learning model(s). For example, the measures, and/or overall measure(s) generated based on the measures, can be compared to threshold(s) and the particular loss technique utilized in federated learning only when the measures and/or overall measure(s) satisfy the threshold(s). As an additional example, the measures and/or overall measure(s) that are generated based on model updates generated utilizing a particular loss technique can additionally or alternatively be compared to alternate measures and/or alternate overall measures that are each generated based on model updates generated utilizing a corresponding alternative particular loss technique. In such an additional example, the particular loss technique can be utilized only when the comparison indicates that the particular loss technique provides a greater degree of data security than the alternate particular loss technique(s). For instance, the particular loss technique can be cross-entropy loss with sign gradient descent, an alternate loss technique can be cross-entropy loss with adaptive federated optimization, an additional alternate loss can be cross-entropy loss with gradient sparsification, and a further additional alternate loss technique can be cross-entropy loss without any gradient modification technique. The particular loss technique can be utilized only when its measure(s) are more indicative of data security than the measure(s) for the alternate loss technique, the measure(s) for the additional alternate loss technique, and the measure(s) for the further additional loss technique. In these and other manners, a certain degree of data security that is provided by gradients, generated using the particular loss technique, can be ensured prior to utilization of the particular loss technique in federated learning. This can mitigate occurrences of a potentially nefarious actor being able to effectively reconstruct intercepted model updates and/or can prevent those actors from being able to differentiate between effective and ineffective reconstructions of intercepted model updates.
- In some additional or alternative implementations, a request that is transmitted by a computing device can be received over one or more networks and the request can include model update, prediction(s) pairs. The model update of the pairs can each be generated based on the prediction(s) of the pair and based on applying a particular loss technique. In those implementations, a reconstruction for each pair can be generated based on the model update of the pair, and measure(s) then generated that indicates a degree of conformity between the reconstruction and the prediction(s) of the pair. The measure(s) can reflect how effectively (e.g., whether and/or to what extent) the reconstruction conforms to the prediction(s). For example, if the reconstruction is a bag of vocabulary reconstruction, the measure(s) can include: a measure that indicates whether the bag of vocabulary reconstruction includes all elements of the prediction(s) and does not include any extra elements not in the prediction(s); and/or a measure that indicates a quantity of elements that differ between the bag of vocabulary reconstruction and the prediction(s) (e.g., a quantity of element(s) that are in the reconstruction but not the prediction(s) and a quantity of element(s) that are in the prediction(s) but not the reconstruction). As another example, if the reconstruction is an ordered sequence reconstruction, the measure(s) can include: a measure that indicates whether the reconstruction includes all elements of the prediction(s) and in the order of the prediction(s) and does not include any extra element(s) not in the prediction(s); and/or a measure that indicates an extent to which the reconstruction and the prediction(s) differ, if at all (e.g., an edit-distance based measure or other measure that reflects difference(s) in element(s) and/or order between the reconstruction and the prediction(s)). The generated measures, and/or overall measure(s) generated based on the measures, can be transmitted to the computing device in response to the request. In response to the transmission, the computing device can utilize the measure(s) and/or overall measure(s) in automatically determining whether to utilize the particular loss technique in federated learning and/or other machine learning model training. The transmission can additionally or alternatively cause the measures and/or overall measure(s) to be rendered (e.g., visually) at the computing device. This can enable user(s) of the computing device to ascertain (e.g., through viewing of the visual rendering) a degree a data security that is provided by the gradients and to determine, based on the degree, whether to utilize the particular loss technique in federated learning and/or other machine learning model training. In these and other manners, a certain degree of data security that is provided by gradients, generated using the particular loss technique, can be ensured prior to utilization of the particular loss technique in machine learning model training.
- In various implementations, the machine learning model is one that includes a projection layer having a projection input layer, weight matrix layer(s), and a projection output layer. The projection input layer can accept a lower dimensional generated embedding as input and the weight matrix layer(s) can be used to process the generated embedding, using current weights of the weight matrix layer(s), to generate corresponding projection output of the projection output layer. The projection output layer has a size that conforms to a vocabulary for the machine learning model. Put another way, the quantity of output nodes of the projection output layer can conform to the vocabulary size and each node will correspond to a particular discrete element of the vocabulary. The output generated over the projection output layer can be, for example, a probability distribution over the vocabulary. When a sequence of inputs is applied to the projection input layer, a sequence of outputs can be generated over the projection output layer and will be of a size that conforms to the vocabulary and to a length of the input sequence.
- As one example, when the machine learning model is an automatic speech recognition model (e.g., a listen-attend-spell LAS model), a sequence of audio data embeddings of dimension S by d (where S is the quantity of audio data embeddings and d is the dimension of each embedding) can be provided to the projection input layer and sequence and the projection output can be a sequence of outputs that are collectively of length S by V, where V is the vocabulary size. The elements of the vocabulary in such an example can be words or word pieces.
- As another example, when the machine learning model is an image classification model, the embedding provided as input to the projection input layer can be an image embedding, of an image, of dimension d (where d is the dimension of the embedding) and the projection output can be of length V, where V is the vocabulary size. The elements of the vocabulary in such an example can be classifications. Additional and/or alternative machine learning models can be utilized that can include different vocabularies and/or can accept different types of embeddings as input.
- Accordingly, various implementations set forth techniques to ensure at least a certain degree of security is afforded by a particular loss technique utilized in federated learning, and can be utilized to ensure that degree of security is afforded before the particular loss technique is utilized in federated learning of particular machine learning model(s). In these and other manners, security of data can be enhanced for various client devices that participate in the federated learning. This can enable the benefits of federated learning to be achieved, while ensuring a certain degree of security.
- The above description is provided only as an overview of some implementations disclosed herein. These and other implementations of the technology are disclosed in additional detail below.
- It should be appreciated that all combinations of the foregoing concepts and additional concepts described in greater detail herein are contemplated as being part of the subject matter disclosed herein. For example, all combinations of claimed subject matter appearing at the end of this disclosure are contemplated as being part of the subject matter disclosed herein.
-
FIG. 1 illustrates an example environment in which implementations described herein can be implemented. -
FIG. 2 is a flowchart illustrating an example method of: generating, utilizing corresponding model updates, corresponding reconstructions of corresponding predictions utilized in generating the corresponding model updates; determining measures based on comparing the corresponding reconstructions to the corresponding model updates; and, optionally, performing one or more further actions based on the determined measures. -
FIG. 3 is a flowchart illustrating an example method of generating a reconstruction of a prediction using matrix factorization on a corresponding gradient and using a known vocabulary of projection output of a machine learning model utilized in generating the prediction. -
FIG. 4 illustrates an example of a projection layer of a machine learning model. -
FIG. 5 illustrates an example of an invertible matrix, an orthogonal matrix generated based on decomposing a gradient, and a resulting matrix from performing a cross product of the invertible matrix and the orthogonal matrix. -
FIG. 6 schematically depicts an example architecture of a computer system. - Before turning to the figures, a non-limiting overview is presented of some implementations of generating a reconstruction using a generated model update. Many deep learning models, such as classification models, include a fully-connected layer to map a d-dimensional representation extracted from an input h to a C-dimensional vector z. The vector z represents the unnormalized log probabilities of its class, and Cis the number of classes. This fully-connected layer is referred to herein as the projection layer. The probability distribution over all classes ŷ is derived by applying the softmax function on z:
-
- Training such a model usually involves minimizing the cross-entropy loss, as represented by
-
- Assume W and b represent the weight and the bias of the projection layer, respectively. Since z=Wh+b, this results in
-
- Further assume
-
- With these assumptions, the model update for the projection layer can be represented by equation (1):
-
- Equation (1) applies to a loss computed from a single sample with a single label. Since introducing a new label means adding a new term to the loss, equation (1) can be generalized to various settings. For example, a model update of a N-sample mini-batch or a sequence of length N is averaged from model updates computed from each sample in the batch or each label in the sequence. In such a scenario, equation (1) can be generalized by equation (2):
-
- In equation (2),
-
- As another example, a model update after K steps is the sum of the model update at each of the K steps. In such a scenario, equation (1) can be generalized by equation (3):
-
- In equation (3), ΔW(t) and αi are the softmax gradient and learning rate at the time step i, respectively, H=[αiH(1), . . . , αKHK], and G=[G(1), . . . , GK].
- In all of these scenarios, ΔW can be represented as the product of two lower-rank matrices HT ∈
- In many implementations, in large-scale deep learning models, d and Care in the order of thousands. Accordingly, in those implementations it can be assumed that S<min{d, C}. Since H and G are usually full-rank matrices and their rows and columns have no linear dependency, this quantity S can be inferred from the rank of the weight matrix update, i.e. S=rank(ΔW). Therefore, an entity seeking to reconstruct predictions based on model updates can already know the number of labels (including repetitions) from the knowledge of ΔW. This is especially helpful, for example, when ΔW is computed from a sequence of labels. In that situation, the length of the sequence is immediately revealed to the entity given access to ΔW.
-
-
- Since the softmax function always returns a value in (0, 1), each row in G has a unique negative coordinate corresponding to the ground-truth label. Stated formally, let Neg(u) define the indices of negative coordinates in a vector u. Each row gi in G satisfies that Neg(gi)={
-
-
- If a label c appears in the batch,
-
- In practice, solving LP(c) for each c may take time, as the number of words in the vocabulary may be large. In view of observing that many columns in Q are clearly inseparable, a screening round can be applied to filter inseparable columns. Consider each columns in Q as a data point in a S-dimensional space, the screening round returns all points that are separable from a sampled subset of remaining points (e.g., using the Perceptron algorithm). This can be significantly faster and/or more computationally efficient than solving the LP problem.
- The below algorithm provides an overview of some implementations of obtaining a set of labels (i.e., a bag of vocabulary) from a model update.
- Turning now to the Figures,
FIG. 1 illustrates an example environment in which implementations described herein can be implemented. The example environment includesclient devices 106A-N, afederated learning system 110, areconstruction system 120, and one ormore networks 108. Theclient devices 106A-N, thefederated learning system 110, and/or thereconstruction system 120 can communicate with one another via the network(s) 108. The network(s) 108 can include wide area network(s) (WAN(s)) (e.g., the Internet) and/or local area network(s) (LAN(s)). - The
client devices 106A-N can include a client device via which a user can interact with thereconstruction system 120, which can be located remote from the client device (in otherimplementations reconstruction system 120 can be implemented in whole or in part on the client device). For example, the user can interact withclient device 108A (via user interface input device(s) of theclient device 108A) to cause the client device to transmit model update, prediction(s) pairs toreconstruction system 120. In response to such a transmission, thereconstruction system 120 can generate measure(s) based on the transmitted pairs and then transmit the measure(s) to theclient device 108A. In response to receiving the measure(s), theclient device 108A can utilize the measure(s) and/or overall measure(s) in automatically determining whether to utilize the particular loss technique in federated learning and/or other machine learning model training. In response to receiving the measure(s), theclient device 108A can additionally or alternatively cause the measures and/or overall measure(s) to be rendered (e.g., visually) at theclient device 108A. This can enable user(s) of theclient device 108A to ascertain (e.g., through viewing of the visual rendering via a screen of theclient device 108A) a degree a data security that is provided by the gradients and to determine, based on the degree, whether to utilize the particular loss technique in federated learning and/or other machine learning model training. - As another example, the user can interact with
client device 108A (via user interface input device(s) of theclient device 108A) to cause the client device to transmit model updates toreconstruction system 120. In response to such a transmission, thereconstruction system 120 can generate reconstructions that each correspond to one of the transmitted model updates, and then transmit, to theclient device 108A, the reconstructions and indications of which reconstructions correspond to which model updates. In response to receiving the reconstructions and indications of which reconstructions correspond to which model updates, theclient device 108A can generate measure(s) and/or overall measure(s) based on comparing the reconstructions to actual predictions that are stored locally at theclient device 108A or otherwise accessible at theclient device 108A. Theclient device 108A can match the received reconstructions to corresponding predictions based on the received indications of which reconstructions correspond to which model updates (e.g., using a locally stored mapping of the model updates to the predictions). Accordingly, in such an example, theclient device 108A transmits only the model updates to thereconstruction system 120, without transmitting the predictions. Moreover, thereconstruction system 120 returns reconstructions generated based on the model updates, enabling theclient device 108A to generate the measures based on the returned reconstructions. - The
client devices 106A-N can additionally or alternatively include client devices that interact with thefederated learning system 110 in participating in federated learning of a global machine learning (ML)model 118. For example, each of theclient devices 106A-N is illustrated as including a corresponding one oflocal ML models 108A-N stored locally at the client device. Thelocal ML models 108A-N are each a local counterpart to theglobal ML model 118, which is managed by thefederated learning system 110. - In participating in federated learning, each of the
client devices 106A-N, using its corresponding one of the on-device ML models 108A-N, can process corresponding input (e.g., an input based on user interface input detected at the client device and/or based on corresponding locally stored data at the client device) to generate a prediction, and can compare the prediction to ground truth output to generate a client gradient. For example, a cross-entropy based loss technique can be utilized in generating the client gradients. The ground truth output can be based on other data generated locally at the client device, and can optionally be based on user input(s) (e.g., that explicitly or implicitly confirm the prediction or that explicitly or implicitly indicate an alternate ground truth that is different from the prediction). Further, theclient devices 106A-N can transmit model updates, that are based on their locally generated client gradients, to thefederated learning system 110. Notably, the model updates can be transmitted to thefederated learning system 110 without transmission of the predictions or the ground truth outputs that were utilized in generating the model updates. - The
federated learning system 110 can utilize received client model updates, and optionally additional client model updates generated in a similar manner at additional client devices, to update weights of theglobal ML model 118. Thefederated learning system 110 can transmit the updatedglobal ML model 118, or updated weights of theglobal ML model 118, to theclient devices 108A-N and/or to other client devices. Each client device can then replace the on-device ML model with the updated global ML model, or replace the weights of the on-device ML model with the updated weights of theglobal ML model 118, thereby updating the on-device ML models. Further federated learning can optionally occur based on the updated on-device ML models, resulting in a further updatedglobal ML model 118, which can again be provided (or weights thereof provided) with theclient devices 106A-N. This process can continue for multiple iterations, optionally until the ML models are deemed final based on one or more conditions being satisfied. Thefederated learning system 110 can be implemented, for example, by one or more servers, such as a cluster of optionally distributed high-performance servers. - The
client devices 106A-N can include one or more of: a desktop computing device, a laptop computing device, a standalone hardware device at least in part dedicated to an automated assistant, a tablet computing device, a mobile phone computing device, a computing device of a vehicle (e.g., an in-vehicle communications system, and in-vehicle entertainment system, an in-vehicle navigation system, an in-vehicle navigation system), or a wearable apparatus of the user that includes a computing device (e.g., a watch of the user having a computing device, glasses of the user having a computing device, a virtual or augmented reality computing device). Additional and/or alternative client devices may be provided.Client devices 106A-N can each include one or more memories for storage of data and software applications, one or more processors for accessing data and executing applications, and other components that facilitate communication over a network. - The
reconstruction system 120 can be implemented, for example, by a client device and/or by one or more servers, such as a cluster of optionally distributed high-performance servers. Thereconstruction system 120 is illustrated inFIG. 1 as including a reconstruction engine 122, ameasure engine 124, and aselection engine 126. - The reconstruction engine 122 processes model updates 134 and generates, for each of the model updates 134, a corresponding reconstruction. The model updates 134 processed at a given time can be provided by one of the
client devices 106A, thefederated learning system 110, or even locally generated by thereconstruction system 120. Further, the model updates 134 can optionally each be paired with corresponding one(s) of thepredictions 136 as described herein (e.g., paired via mappings defining associations between corresponding model updates and predictions). - In generating a reconstruction based on one of the model updates 134, the reconstruction engine 122 can use matrix factorization on the model update and use a known
vocabulary 132 of a projection output layer of a corresponding ML model. For example, where the model update is generated based on one of thelocal ML models 108A-N, a knownvocabulary 132 of the projection output layer of thelocal ML models 108A-N can be used in generating the reconstruction. In some implementations, the knownvocabulary 132 is provided by a developer or other user with knowledge of the vocabulary of the machine learning model (e.g., provided in or along with a request that includes corresponding gradients). In some other implementations, the knownvocabulary 132 is determined from inspection of the machine learning model and/or from providing multiple known inputs to the machine learning model and inspecting corresponding predictions and/or model updates. Providing multiple known inputs to the machine learning model and inspecting corresponding predictions and/or model update enables resolution of which output dimensions correspond to which elements of the vocabulary. For example, since the prediction, that should be generated from the known input, is also known, it can be determined from the prediction and/or the model update, which output dimensions correspond to element(s) of the vocabulary for the prediction. Through utilization of multiple known inputs and corresponding known predictions, some, or all, of the vocabulary can be effectively derived through analysis of actually generated predictions and/or model updates. - In some implementations, the reconstruction engine 122 generates a reconstruction that includes, or is restricted to, a bag of vocabulary reconstruction that reconstructs the vocabulary elements of the prediction(s), but not necessarily their order. Put another way, the reconstruction seeks to reconstruct the vocabulary elements of the prediction(s) without regard to their order. While reconstruction could, by happenstance, include the vocabulary elements in the correct order, the reconstruction does not seek to determine the correct order. Such reconstructions can each be generated the reconstruction engine 122 using the model update and the known vocabulary, and without any reference to corresponding current weights of the machine learning model when the corresponding prediction was generated and/or without reference to any other feature(s). In some implementations, the reconstruction engine 122 can additionally or alternatively generate a reconstruction that is an ordered sequence reconstruction. The reconstruction engine 122 can generate the ordered sequence reconstruction based on the bag of vocabulary reconstruction and further based on a language model (or more generally, a vocabulary model) and/or based on the corresponding current weights of the machine learning model when the corresponding prediction was generated. In various implementations, in generating a reconstruction, the reconstruction engine 122 performs some or all aspects of
step 256A ofFIG. 3 (described below). - The
measure engine 124 compares reconstructions, generated by the reconstruction engine, to theircorresponding predictions 136, and generates measure(s) based on the comparisons. For example, the measure engine can compare a generated reconstruction, generated based on a given one of the model updates 134, to a given one of thepredictions 136 that is indicated as paired with the given one of the model updates 134. The given one of thepredictions 136 can be prediction(s) that were actually generated, using the corresponding ML model, and that were utilized in generating the given one of the model updates 134 (e.g., based on comparing the prediction(s) to ground truth output(s)). The measure generated by themeasure engine 124 for a reconstruction can reflect how effectively (e.g., whether and/or to what extent) the reconstruction conforms to the prediction(s). For example, if the reconstruction is a bag of vocabulary reconstruction, themeasure engine 124 can generate a measure that is a “1.0” if the bag of vocabulary reconstruction includes all elements of the prediction(s) and does not include any extra elements not in the prediction(s), and that otherwise is a “0.0”. As another example, if the reconstruction is a bag of vocabulary reconstruction, themeasure engine 124 can additionally or alternatively generate a measure that is non-binary and that reflects a quantity of elements that differ between the bag of vocabulary reconstruction and the prediction(s). For instance, the measure can be “1.0” is no elements differ, “0.75” is one of four elements differ, “0.5” if three of six elements differ, “0.0” if all elements differ, and so forth. In addition, themeasure engine 124 can also optionally generate overall measure(s) as a function of the individual measures for the reconstructions. For example, the overall measure(s) can include a mean of the individual measures, a median of the individual measures, a standard deviation the individual measures, and/or other overall measure(s) that are a function of the individual measures. In various implementations, in generating an individual measure, themeasure engine 124 performs some or all aspects ofstep 258 ofFIG. 2 (described below). - The
selection engine 126 analyzes measure(s) (e.g., individual and/or overall), generated by themeasures engine 124 for model updates generated according to a particular loss technique, in determining whether to utilize the particular loss technique (e.g., in federated learning of the corresponding machine learning model and/or of additional machine learning model(s)). Accordingly, in various implementations theselection engine 126 can determine whether to select the particular loss technique for usage or, instead, to select an alternative loss technique for usage. - In some implementations, the
selection engine 126 compares the individual measures and/or overall measure(s), generated by themeasures engine 124 for model updates generated according to a particular loss technique, to threshold(s). In those implementations, theselection engine 126 can determine whether to utilize the particular loss technique based at least in part (e.g., solely and/or based on other consideration(s)) based on whether the measures and/or overall measure(s) satisfy the threshold(s). - In some implementations, the
selection engine 126 compares: (a) the individual measures and/or overall measure(s), generated by themeasures engine 124 for model updates generated according to a particular loss technique to (b) alternate individual measures and/or alternate overall measure(s), generated by themeasures engine 124 for alternate gradients generated according to an alternate particular loss technique. In those implementations, theselection engine 126 can determine whether to select the particular loss technique or, instead, the alternate particular loss technique, based on the comparison (e.g., solely based on the comparison or also based on the threshold being satisfied as described in the preceding paragraph). For example, themeasures engine 124 can determine to select the particular loss technique for utilization only when the comparison indicates that the particular loss technique provides a greater degree of data security than the alternate particular loss technique(s). Although the preceding example is provided with respect to comparing corresponding measures for two different particular loss techniques, more than two particular loss techniques can be considered and the comparison (and resulting selection) can consider corresponding measures for all. In various implementations, theselection engine 126 performs some or all aspects ofstep 262 ofFIG. 2 (described below). -
FIG. 2 is a flowchart illustrating anexample method 200 of: generating, utilizing corresponding model updates, corresponding reconstructions of corresponding predictions utilized in generating the corresponding gradients; determining measures based on comparing the corresponding reconstructions to the corresponding predictions; and, optionally, performing one or more further actions based on the determined measures. For convenience, the operations of the flowchart are described with reference to a system that performs the operations. This system may include various components of various computer systems, such as one or more components ofreconstruction system 120 ofFIG. 1 . Moreover, while operations ofprocess 200 are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted, and/or added. - At block 252, the system receives model update, prediction(s) pairs. The model update, prediction(s) pairs each include: prediction(s) generated based on processing corresponding input(s) using a machine learning model; and a model update generated based on applying a particular loss technique and based on corresponding ground truth input(s) (e.g., generated based on gradient(s) each generated based on comparing the corresponding ground truth input(s) to the prediction(s). For example, the model update can be a single gradient generated based on comparing a single prediction and ground truth pair or can be a model update generated based on a batch of gradients generated based on comparing multiple predictions and their corresponding ground truths. The model update, prediction(s) pairs received at block 252 can be generated by a component of the system and received from that component, or can be received in a transmission, and via a network, from another system or client device.
- At
block 254, the system identifies a model update, prediction(s) pair from those received at block 252. - At block 256, the system generates, using the model update of the identified pair and independent of the prediction of the identified pair, a reconstruction of the prediction. In some implementations, block 256 includes sub-block 256A, in which the system generates the reconstruction using matrix factorization on the model update and using a known vocabulary of projection output of the machine learning model. One non-limiting particular example of
block 256A is described below with respect toFIG. 3 . - At
block 258, the system generates a measure based on comparing the reconstruction, of block 256 for the pair, to the prediction(s) of the pair. The system can store (e.g., in ROM or RAM) the measure. - At
block 260, the system determines whether there are any unprocessed model update, prediction(s) pairs. If so, the system proceeds back to block 254 and identifies an unprocessed pair. If not, the system optionally proceeds tooptional block 262 and/oroptional block 264. It is noted that, although shown serially inFIG. 2 for convenience, in various implementations the system can perform multiple iterations ofblocks - At
optional block 262, the system determines, based on the measures generated via multiple iterations of block 258 (e.g., retrieving them from RAM or ROM), whether to utilize the particular loss technique in federated training. In some implementations, the system determines whether to utilize the particular loss technique in federated training based on the measures themselves and/or based on overall measure(s) that are generated based on the individual measure generated via multiple iterations ofblock 258. In some implementations, the system determines to utilize the particular loss technique in federated training only when some (e.g., X % of) or all of the individual measures satisfy an individual threshold and/or only when some (e.g., X % of) or all of the overall measure(s) satisfy a corresponding overall threshold. In some additional or alternative implementations, the system determines whether to utilize the particular loss technique in federated training based on comparing the individual measures and/or overall measure(s), for the particular loss technique, to individual measures and/or overall measure(s) for one or more alternate particular loss techniques. The individual measures and/or overall measure(s) for an alternate particular loss technique can be generated based on performingblocks - At
optional block 264, the system transmits, in response to receiving the pairs at block 252, the individual measures generated via multiple iterations ofblock 258 and/or overall measure(s) generated based on the generated individual measures. For example, the pairs at block 252 can be received in a request from a server or a client device, and the system can transmit the individual measures and/or the overall measure(s) to the server or the client device. For instance, the individual measures and/or the overall measure(s) can be included in a graphical user interface that is generated by the system, and the graphical user interface transmitted to the client device. Transmitting of the graphical user interface to the client device can cause (e.g., after corresponding user input(s) at the client device) the client device to visually render the individual measures and/or overall measure(s). -
FIG. 3 is a flowchart illustrating one non-limiting example ofblock 256A ofFIG. 2 . - At block 256A1, the system identifies a gradient. The gradient can be the model update from one of the pairs of
FIG. 2 . The gradient can optionally be generated based on applying a cross-entropy based loss technique and based on a ground truth output and a prediction. The prediction is one generated based on processing input using a machine learning model. - At block 256A2, the system decomposes the gradient into at least an S by V orthogonal matrix (Q), where S corresponds to a number of sequences in the prediction and V corresponds to a vocabulary size of the machine learning model. Each column, in the matrix Q, can represent an S-dimensional point that corresponds to an element in the vocabulary. In some implementations, the system can decompose the gradient into orthogonal matrix Q using singular value decomposition. For example, the system can use singular value decomposition to decompose the gradient into two orthogonal matrices, Q and P (which can also be an S by V matrix) and a diagonal matrix Σ.
- At block 256A3, the system determines which columns in Q include a separating classifier. Block 256A3 can include sub-block 256A3A, in which the system performs a dot product of Q and Z, where Z is an S by S invertible matrix, and identifies, based on the result, resulting column(s) of Q that include a separating value (e.g., negative value). For example, at sub-block 156A3A, the system can identify, in the matrix ∇0 that results from the dot product of Q and Z, row(s) that include a separating value (e.g., a negative value) and identify the column(s) of Q that have the same index value as the row(s). A row that has a separating value in the resulting matrix will indicate that the corresponding column of Q likewise has a separating value.
- As one example, and with reference to
FIG. 5 , an exampleZ invertible matrix 123A (of size S by S) is illustrated being crossed with an example Qorthogonal matrix 123B (of size S by V), resulting in an example ∇0matrix 123C (of size S by V). The second row of ∇0matrix 123C (illustrated with shading) is the result of the cross product of the second row ofZ invertible matrix 123A (illustrated with shading) and the second column of Qorthogonal matrix 123B (illustrated with shading). Further, the second row of ∇0matrix 123C includes a separating value, indicated by the vertical shading of the cell in the second row and second column (as opposed to the diagonal shading of the other cells of the second row. The second row of ∇0matrix 123C can be determined to have a separating value based on one of the cells being differentiable with respect to all other cells of the row. For example, one of the cells of the second can be negative and all other cells of the row can be positive. This can indicate that the second column of Qorthogonal matrix 123B (having the same “second” index value) likewise has a separating value. It is noted that additional rows of ∇0matrix 123C can have separating values and, as a result, additional columns of Q can be determined to have additional separating values. However, only one such example is illustrated inFIG. 5 for simplicity. Further, it is noted that thematrices - Turning again to
FIG. 3 , at block 256A4, the system generates a reconstruction of the prediction, using the column(s) of Q determined to include a separating classifier and a mapping of the columns of Q to the vocabulary of the machine learning models. In some implementations, block 256A4 includes sub-block 256A4A and optionally sub-block 256A4B. At sub-block 256A4A, the system generates a bag of vocabulary reconstruction which can include an unordered listing of those elements of the vocabulary that correspond to the columns of Q determined to include a separating classifier. - At sub-block 256A4B, the system generates, optionally using the current state of model and using the bag of vocabulary reconstruction of sub-block 256A4A, an ordered sequence reconstruction. It is noted that the current state of the model is not utilized in generating the bag of vocabulary reconstruction at sub-block 256A4A. In some implementations, at sub-block 256A4B, the system does not utilize the current state of the model but, rather, relies on the bag of vocabulary reconstruction and a vocabulary model that dictates probabilities of various sequences of the vocabulary elements. For example, where the vocabulary includes words or word sequences, the vocabulary model can be a language model. For example, the system can utilize the language model to determine which, of multiple candidate ordered sequences of the bag of vocabulary reconstruction, is most probable, and that candidate ordered sequence utilized as the ordered sequence reconstruction. In some implementations, at sub-block 256A4B, the system generates the ordered sequence reconstruction based on the bag of vocabulary reconstruction and further based on the corresponding current weights of the machine learning model when the corresponding prediction was generated. Optionally, in those implementations the system uses gradients matching reconstruction technique and/or other reconstruction technique(s), that rely on corresponding current weights, in generating the ordered sequence reconstructions. However, the system uses such reconstruction technique(s) with a search space that is constrained in view of (e.g., constrained to) the bag of vocabulary reconstruction.
- At block 256A5, the system stores the reconstruction, generated at block 256A4, and an association of the reconstruction to the gradient utilized in generating the reconstruction.
- Implementations of the example of
block 256A, that is illustrated inFIG. 3 , can be motivated based on knowledge that ∇W=AT ∇0. In the preceding equation, ∇W represents the gradient, which is with respect to the weight matrix (W) of a corresponding projection layer. Further, AT represents the transpose of the projection input (i.e., the dimensions of the embedding(s) as well as a length of the sequence of the embedding(s)). Finally, ∇0 represents the gradient with respect to the projection output, which is unknown but can be resolved as described herein. By decomposing ∇W into PΣQ, PΣQ can be rewritten as PΣ(Z−1Z)Q, where Z is any S by S invertible matrix, and further rewritten as (PΣZ−1)(ZQ). With the preceding, (PΣZ−1) is equivalent to AT, meaning that ∇0 is equal to (ZQ) and, thus, ∇0 can be resolved by a cross product of Z and Q. Moreover, a row in ∇0 that includes a separating value (e.g., a negative value) will indicate that a column of Q, having the same index value as the row, likewise has a separating value. This indicates that an element, of the vocabulary, that corresponds to that column of Q, was included in the prediction used to generate the gradient ∇W. Through identification of the column(s) of Q that have separating values, and mapping those columns to element(s) of the known vocabulary for Q, a bag—of vocabulary reconstruction can be generated. It is noted that this general technique still applies for multi-sample/batch gradients and/or for multi-step gradients. In both cases ΔW is the sum of several updates ΔWi and matrix factorization still works (e.g., the sum of products is still a product). -
FIG. 4 illustrates an example of a projection layer of a machine learning model, such as global model 118 (FIG. 1 ) andlocal models 108A-N(FIG. 1 ). The projection layer includes aprojection input layer 118A, weight matrix layer(s) 118B, and aprojection output layer 118C. Theprojection input layer 118A can accept a lower dimensional generated embedding (of dimension d) as input and the weight matrix layer(s) 118B can be used to process the generated embedding, using current weights of the weight matrix layer(s) 118B, to generate corresponding projection output (of dimension V) of theprojection output layer 118C. Theprojection output layer 118C has a size (V) that conforms to a vocabulary for the machine learning model. Put another way, the quantity of output nodes of theprojection output layer 118C can conform to the vocabulary size and each node will correspond to a particular discrete element of the vocabulary. The output generated over theprojection output layer 118C can be, for example, a probability distribution over the vocabulary. When a sequence of inputs of length S (indicated by the “S” in “S×d” inFIG. 4 ) is applied to theprojection input layer 118A, a sequence of outputs of length S (indicated by the “S” in “S×V” inFIG. 4 ) can be generated over theprojection output layer 118C and will be of a size that conforms to the vocabulary and to a length of the input sequence. -
FIG. 6 is a block diagram of anexample computing device 610 that may optionally be utilized to perform one or more aspects of techniques described herein. For example, a client device can include one or more aspects of theexample computing device 610 and/or a server can include one or more aspects of theexample computing device 610.Computing device 610 typically includes at least oneprocessor 614 which communicates with a number of peripheral devices viabus subsystem 612. These peripheral devices may include astorage subsystem 624, including, for example, amemory subsystem 625 and afile storage subsystem 626, userinterface output devices 620, userinterface input devices 622, and anetwork interface subsystem 616. The input and output devices allow user interaction withcomputing device 610.Network interface subsystem 616 provides an interface to outside networks and is coupled to corresponding interface devices in other computing devices. - User
interface input devices 622 may include a keyboard, pointing devices such as a mouse, trackball, touchpad, or graphics tablet, a scanner, a touchscreen incorporated into the display, audio input devices such as voice recognition systems, microphones, and/or other types of input devices. In general, use of the term “input device” is intended to include all possible types of devices and ways to input information intocomputing device 610 or onto a communication network. - User
interface output devices 620 may include a display subsystem, a printer, a fax machine, or non-visual displays such as audio output devices. The display subsystem may include a cathode ray tube (CRT), a flat-panel device such as a liquid crystal display (LCD), a projection device, or some other mechanism for creating a visible image. The display subsystem may also provide non-visual display such as via audio output devices. In general, use of the term “output device” is intended to include all possible types of devices and ways to output information fromcomputing device 610 to the user or to another machine or computing device. -
Storage subsystem 624 stores programming and data constructs that provide the functionality of some or all of the modules described herein. For example, thestorage subsystem 624 may include the logic to perform selected aspects of the methods ofFIGS. 2, 3 , and/or other methods described herein. - These software modules are generally executed by
processor 614 alone or in combination with other processors.Memory 625 used in thestorage subsystem 624 can include a number of memories including a main random access memory (RAM) 630 for storage of instructions and data during program execution and a read only memory (ROM) 632 in which fixed instructions are stored. Afile storage subsystem 626 can provide persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive along with associated removable media, a CD-ROM drive, an optical drive, or removable media cartridges. The modules implementing the functionality of certain implementations may be stored byfile storage subsystem 626 in thestorage subsystem 624, or in other machines accessible by the processor(s) 614. -
Bus subsystem 612 provides a mechanism for letting the various components and subsystems ofcomputing device 610 communicate with each other as intended. Althoughbus subsystem 612 is shown schematically as a single bus, alternative implementations of the bus subsystem may use multiple busses. -
Computing device 610 can be of varying types including a workstation, server, computing cluster, blade server, server farm, or any other data processing system or computing device. Due to the ever-changing nature of computers and networks, the description ofcomputing device 610 depicted inFIG. 6 is intended only as a specific example for purposes of illustrating some implementations. Many other configurations ofcomputing device 610 are possible having more or fewer components than the computing device depicted inFIG. 6 . - In situations in which the systems described herein collect personal information about users (or as often referred to herein, “participants”), or may make use of personal information, the users may be provided with an opportunity to control whether programs or features collect user information (e.g., information about a user's social network, social actions or activities, profession, a user's preferences, or a user's current geographic location), or to control whether and/or how to receive content from the content server that may be more relevant to the user. Also, certain data may be treated in one or more ways before it is stored or used, so that personal identifiable information is removed. For example, a user's identity may be treated so that no personal identifiable information can be determined for the user, or a user's geographic location may be generalized where geographic location information is obtained (such as to a city, ZIP code, or state level), so that a particular geographic location of a user cannot be determined. Thus, the user may have control over how information is collected about the user and/or used.
- In some implementations, a method implemented by one or more processors is provided and includes receiving a plurality of model update, prediction(s) pairs. Each of the model update, prediction(s) pairs include: (a) at least one corresponding prediction, generated based on processing a corresponding input using a machine learning model with corresponding current weights; and (b) a corresponding model update generated based on at least one gradient, where the at least one gradient is generated based on applying a particular loss technique and generated based at least in part on the corresponding prediction and a corresponding ground truth output. The method further includes, for each of the model update, prediction(s) pairs: generating, using the corresponding model update and a known vocabulary of a projection output of the machine learning model, a reconstruction of the corresponding prediction; and generating, based on comparing the reconstruction to the corresponding prediction, a corresponding measure that reflects a degree of conformity between the reconstruction to the corresponding prediction. Generating the reconstruction is performed independent of the corresponding prediction. The method further includes determining, based on the corresponding measures for the model update, prediction(s) pairs, whether to utilize the particular loss technique in federated training of the machine learning model or of an additional machine learning model.
- These and other implementations of the technology can include one or more of the following features.
- In some implementations, the method further includes, in response to determining to utilize the particular loss technique in federated training of the machine learning model or of the additional machine learning model: causing the machine learning model or the additional machine learning model to be locally stored on a plurality of client devices, along with corresponding instructions. The corresponding instructions cause the client devices to locally generate model updates, for the machine learning model or the additional machine learning model, using the particular loss techniques, and transmit the model updates to one or more remote servers.
- In some implementations, determining, based on the corresponding measures, whether to utilize the particular loss technique in federated training of the machine learning model or of an additional machine learning model includes: generating an overall measure based on the corresponding measures; comparing the overall measure to a threshold; and determining to utilize the particular loss technique in federated training in response to the overall measure satisfying the threshold.
- In some implementations, determining, based on the corresponding measures, whether to utilize the particular loss technique in federated training of the machine learning model or of an additional machine learning model includes: generating an overall measure based on the corresponding measures; comparing the overall measure to an alternate overall measure, the alternate overall measure generated based on alternate model update, prediction(s) pairs having alternate corresponding model updates generated based on an alternate particular loss technique that differs from the particular loss technique; and determining, in response to the comparing, to utilize the particular loss technique in federated training. In some versions of those implementations, the particular loss technique is cross-entropy loss without any gradient modification technique and the alternate particular loss technique is cross-entropy loss with at least one gradient modification technique. In some of those versions, the at least one gradient modification technique includes sign gradient descent and/or adaptive federated optimization. In some other versions of those implementations, the particular loss technique is cross-entropy loss with a first gradient modification technique (or a first combination of gradient modification techniques) and the alternate particular loss technique is cross-entropy loss with a second gradient modification technique (or a second combination of gradient modification techniques).
- In some implementations, generating, using the corresponding model update and known labels of a projection output of the machine learning model, the reconstruction of the corresponding prediction, includes generating the reconstruction using matrix factorization on the model update and using the known vocabulary of projection output of the machine learning model. The reconstruction can include (e.g., be restricted to) a bag of vocabulary reconstruction. The reconstruction can additionally or alternatively include an ordered sequence reconstruction and generating the reconstruction can further include generating the ordered sequence reconstruction using the corresponding current weights of the model.
- In some versions of those implementations, generating the reconstruction using matrix factorization on the model update and using a known vocabulary of projection output of the machine learning model includes: decomposing the model update into an S by V orthogonal matrix, where S corresponds to a number of sequences in the prediction and where V corresponds to a size of the known vocabulary; determining which columns, in the S by V orthogonal matrix, include a separating classifier; and generating the reconstruction using the columns, determined to include the separating classifier, and a mapping of the columns to the known vocabulary. In some of those versions, determining which columns, in the S by V orthogonal matrix, include the separating classifier, includes: performing a dot product of the S by V orthogonal matrix and an S by S invertible matrix; and determining, based on analysis of rows of the resulting matrix from the dot product, which rows include a negative value; and determining the columns include the separating classifier based on the columns corresponding to (e.g., having the same index value as) the rows that include the negative value.
- In some implementations, a method implemented by one or more processors is provided and includes receiving, via a network, a request from a computing device. The request includes a plurality of model update, prediction(s) pairs. Each of the model update, prediction(s) pairs include: (a) at least one corresponding prediction, generated based on processing a corresponding input using a machine learning model with corresponding current weights; and (b) a corresponding model update generated based on at least one gradient, where the at least one gradient is generated based on applying a particular loss technique and generated based at least in part on the corresponding prediction and a corresponding ground truth output. The method further includes, for each of the model update, prediction(s) pairs: generating, using the corresponding model update and a known vocabulary of a projection output of the machine learning model, a reconstruction of the corresponding prediction; and generating, based on comparing the reconstruction to the corresponding prediction, a corresponding measure that reflects conformity of the reconstruction to the corresponding prediction. Generating the reconstruction is performed independent of the corresponding prediction. The method further includes transmitting, via the network and to the computing device in response to the request, the corresponding measures for the model update, prediction(s) pairs, and/or an overall measure based on the corresponding measures.
- In some implementations, a method implemented by one or more processors is provided and includes receiving, via a network, a request from a computing device. The request includes a plurality of model updates. Each of the model updates is generated based on applying a particular loss technique and based at least in part on a corresponding prediction and a corresponding ground truth output. The corresponding prediction is generated based on processing a corresponding input using a machine learning model with corresponding current weights. The method further includes, for each of the model updates, generating, using the corresponding model update and a known vocabulary of a projection output of the machine learning model, a reconstruction of the corresponding prediction. Generating the reconstruction is performed independent of the corresponding prediction. The method further includes transmitting, via the network and to the computing device in response to the request, the reconstructions of the corresponding predictions.
- In addition, some implementations include one or more processors (e.g., central processing unit(s) (CPU(s)), graphics processing unit(s) (GPU(s), and/or tensor processing unit(s) (TPU(s)) of one or more computing devices, where the one or more processors are operable to execute instructions stored in associated memory, and where the instructions are configured to cause performance of any of the methods described herein. Some implementations also include one or more transitory or non-transitory computer readable storage media storing computer instructions executable by one or more processors to perform any of the methods described herein.
Claims (20)
1. A method implemented by one or more processors, the method comprising:
receiving a plurality of model update, prediction(s) pairs, each of the model update, prediction(s) pairs including:
at least one corresponding prediction, generated based on processing a corresponding input using a machine learning model with corresponding current weights;
a corresponding model update generated based on at least one gradient, the at least one gradient generated based on applying a particular loss technique and generated based at least in part on the corresponding prediction and a corresponding ground truth output;
for each of the model update, prediction(s) pairs:
generating, using the corresponding model update and a known vocabulary of a projection output of the machine learning model, a reconstruction of the corresponding prediction, wherein generating the reconstruction is performed independent of the corresponding prediction; and
generating, based on comparing the reconstruction to the corresponding prediction, a corresponding measure that reflects a degree of conformity between the reconstruction to the corresponding prediction;
determining, based on the corresponding measures for the model update, prediction(s) pairs, whether to utilize the particular loss technique in federated training of the machine learning model or of an additional machine learning model.
2. The method of claim 1 , further comprising:
in response to determining to utilize the particular loss technique in federated training of the machine learning model or of the additional machine learning model:
causing the machine learning model or the additional machine learning model to be locally stored on a plurality of client devices, along with corresponding instructions that cause the client devices to:
locally generate model updates, for the machine learning model or the additional machine learning model, using the particular loss techniques, and
transmit the model updates to one or more remote servers.
3. The method of claim 1 , wherein determining, based on the corresponding measures, whether to utilize the particular loss technique in federated training of the machine learning model or of an additional machine learning model comprises:
generating an overall measure based on the corresponding measures;
comparing the overall measure to a threshold; and
determining to utilize the particular loss technique in federated training in response to the overall measure satisfying the threshold.
4. The method of claim 1 , wherein determining, based on the corresponding measures, whether to utilize the particular loss technique in federated training of the machine learning model or of an additional machine learning model comprises:
generating an overall measure based on the corresponding measures;
comparing the overall measure to an alternate overall measure, the alternate overall measure generated based on alternate model update, prediction(s) pairs having alternate corresponding model updates generated based on an alternate particular loss technique that differs from the particular loss technique; and
determining, in response to the comparing, to utilize the particular loss technique in federated training.
5. The method of claim 4 , wherein the particular loss technique is cross-entropy loss without any gradient modification technique and wherein the alternate particular loss technique is cross-entropy loss with at least one gradient modification technique.
6. The method of claim 5 , wherein the at least one gradient modification technique includes sign gradient descent, gradient sparsification, and/or adaptive federated optimization.
7. The method of claim 1 , wherein generating, using the corresponding model update and known labels of a projection output of the machine learning model, the reconstruction of the corresponding prediction, comprises:
generating the reconstruction using matrix factorization on the model update and using the known vocabulary of projection output of the machine learning model.
8. The method of claim 7 , wherein the reconstructions includes a bag of vocabulary reconstruction.
9. The method of claim 7 , wherein generating the reconstruction using matrix factorization on the model update and using a known vocabulary of projection output of the machine learning model comprises:
decomposing the model update into an S by V orthogonal matrix, wherein S corresponds to a number of sequences in the prediction(s) and wherein V corresponds to a size of the known vocabulary;
determining which columns, in the S by V orthogonal matrix, include a separating classifier; and
generating the reconstruction using the columns, determined to include the separating classifier, and a mapping of the columns to the known vocabulary.
10. The method of claim 9 , wherein determining which columns, in the S by V orthogonal matrix, include the separating classifier, comprises:
performing a dot product of the S by V orthogonal matrix and an S by S invertible matrix; and
determining, based on analysis of rows of the resulting matrix from the dot product, which rows include a negative value; and
determining the columns include the separating classifier based on the columns corresponding to the rows that include the negative value.
11. The method of claim 7 , wherein the reconstructions includes an ordered sequence reconstruction and wherein generating the reconstruction further comprises generating the ordered sequence reconstruction using the corresponding current weights of the model.
12. A method implemented by one or more processors, the method comprising:
receiving, via a network, a request from a computing device, wherein the request includes a plurality of model update, prediction(s) pairs, each of the model update, prediction(s) pairs including:
at least one corresponding prediction, generated based on processing a corresponding input using a machine learning model with corresponding current weights; and
a corresponding model update generated based on at least one gradient, the at least one gradient generated based on applying a particular loss technique and generated based at least in part on the corresponding prediction and a corresponding ground truth output;
for each of the model update, prediction(s) pairs:
generating, using the corresponding model update and a known vocabulary of a projection output of the machine learning model, a reconstruction of the corresponding prediction, wherein generating the reconstruction is performed independent of the corresponding prediction; and
generating, based on comparing the reconstruction to the corresponding prediction, a corresponding measure that reflects conformity of the reconstruction to the corresponding prediction;
transmitting, via the network and to the computing device in response to the request:
the corresponding measures for the model update, prediction(s) pairs, and/or
an overall measure based on the corresponding measures.
13. The method of claim 12 , wherein generating, using the corresponding model update and known labels of a projection output of the machine learning model, the reconstruction of the corresponding prediction, comprises:
generating the reconstruction using matrix factorization on the model update and using the known vocabulary of projection output of the machine learning model.
14. The method of claim 13 , wherein the reconstructions includes a bag of vocabulary reconstruction.
15. The method of claim 14 , wherein the reconstructions includes an ordered sequence reconstruction and wherein generating the reconstruction further comprises generating the ordered sequence reconstruction using the corresponding current weights of the model and the bag of vocabulary reconstruction.
16. The method of claim 13 , wherein generating the reconstruction using matrix factorization on the gradient and using a known vocabulary of projection output of the machine learning model comprises:
decomposing the model update into an S by V orthogonal matrix, wherein S corresponds to a number of sequences in the prediction(s) and wherein V corresponds to a size of the known vocabulary;
determining which columns, in the S by V orthogonal matrix, include a separating classifier; and
generating the reconstruction using the columns, determined to include the separating classifier, and a mapping of the columns to the known vocabulary.
17. The method of claim 16 , wherein determining which columns, in the S by V orthogonal matrix, include the separating classifier, comprises:
performing a dot product of the S by V orthogonal matrix and an S by S invertible matrix; and
determining, based on analysis of rows of the resulting matrix from the dot product, which rows include a negative value; and
determining the columns include the separating classifier based on the columns corresponding to the rows that include the negative value.
18. A method implemented by one or more processors, the method comprising:
receiving, via a network, a request from a computing device, wherein the request includes a plurality of model updates, wherein each of the model updates is generated based on applying a particular loss technique and based at least in part on corresponding prediction(s) and corresponding ground truth output(s), wherein the corresponding prediction(s) are generated based on processing corresponding input(s) using a machine learning model with corresponding current weights;
for each of the model updates:
generating, using the corresponding model update and a known vocabulary of a projection output of the machine learning model, a reconstruction of the corresponding prediction(s), wherein generating the reconstruction is performed independent of the corresponding prediction(s); and
transmitting, via the network and to the computing device in response to the request:
the reconstructions of the corresponding predictions.
19. The method of claim 18 , wherein generating, using the corresponding model update and known labels of a projection output of the machine learning model, the reconstruction of the corresponding prediction, comprises:
generating the reconstruction using matrix factorization on the model update and using the known vocabulary of projection output of the machine learning model.
20. The method of claim 18 , wherein the reconstructions includes a bag of vocabulary reconstruction.
Priority Applications (4)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/535,405 US20220383204A1 (en) | 2021-05-28 | 2021-11-24 | Ascertaining and/or mitigating extent of effective reconstruction, of predictions, from model updates transmitted in federated learning |
EP21844108.7A EP4241208A1 (en) | 2021-05-28 | 2021-12-13 | Ascertaining and/or mitigating extent of effective reconstruction, of predictions, from model updates transmitted in federated learning |
CN202180088371.8A CN116783601A (en) | 2021-05-28 | 2021-12-13 | Determining and/or mitigating an effective degree of reconstruction of predictions based on model updates transmitted in federal learning |
PCT/US2021/063122 WO2022250732A1 (en) | 2021-05-28 | 2021-12-13 | Ascertaining and/or mitigating extent of effective reconstruction, of predictions, from model updates transmitted in federated learning |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202163194663P | 2021-05-28 | 2021-05-28 | |
US17/535,405 US20220383204A1 (en) | 2021-05-28 | 2021-11-24 | Ascertaining and/or mitigating extent of effective reconstruction, of predictions, from model updates transmitted in federated learning |
Publications (1)
Publication Number | Publication Date |
---|---|
US20220383204A1 true US20220383204A1 (en) | 2022-12-01 |
Family
ID=84193143
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US17/535,405 Pending US20220383204A1 (en) | 2021-05-28 | 2021-11-24 | Ascertaining and/or mitigating extent of effective reconstruction, of predictions, from model updates transmitted in federated learning |
Country Status (1)
Country | Link |
---|---|
US (1) | US20220383204A1 (en) |
-
2021
- 2021-11-24 US US17/535,405 patent/US20220383204A1/en active Pending
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11829880B2 (en) | Generating trained neural networks with increased robustness against adversarial attacks | |
US11804069B2 (en) | Image clustering method and apparatus, and storage medium | |
US20210150412A1 (en) | Systems and methods for automated machine learning | |
EP3542319A1 (en) | Training neural networks using a clustering loss | |
US11373117B1 (en) | Artificial intelligence service for scalable classification using features of unlabeled data and class descriptors | |
US20190311258A1 (en) | Data dependent model initialization | |
CN112395487A (en) | Information recommendation method and device, computer-readable storage medium and electronic equipment | |
CN112667979A (en) | Password generation method and device, password identification method and device, and electronic device | |
CN110489613B (en) | Collaborative visual data recommendation method and device | |
WO2021012691A1 (en) | Method and device for image retrieval | |
CN112749737A (en) | Image classification method and device, electronic equipment and storage medium | |
US20220383204A1 (en) | Ascertaining and/or mitigating extent of effective reconstruction, of predictions, from model updates transmitted in federated learning | |
CN111597336A (en) | Processing method and device of training text, electronic equipment and readable storage medium | |
US20220374655A1 (en) | Data summarization for training machine learning models | |
Scrucca et al. | Projection pursuit based on Gaussian mixtures and evolutionary algorithms | |
WO2022250732A1 (en) | Ascertaining and/or mitigating extent of effective reconstruction, of predictions, from model updates transmitted in federated learning | |
CN116783601A (en) | Determining and/or mitigating an effective degree of reconstruction of predictions based on model updates transmitted in federal learning | |
US20230111978A1 (en) | Cross-example softmax and/or cross-example negative mining | |
CN113505838B (en) | Image clustering method and device, electronic equipment and storage medium | |
US20240028828A1 (en) | Machine learning model architecture and user interface to indicate impact of text ngrams | |
CN117009534B (en) | Text classification method, apparatus, computer device and storage medium | |
US20230016231A1 (en) | Learning apparatus, learning method and program | |
CN112509640B (en) | Gene ontology item name generation method and device and storage medium | |
US20240152760A1 (en) | Method, apparatus, device and medium for training and applying a contrastive learning model | |
US20240143696A1 (en) | Generating differentiable order statistics using sorting networks |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:THAKKAR, OM DIPAKBHAI;DANG, TRUNG;INDRA RAMASWAMY, SWAROOP;AND OTHERS;REEL/FRAME:058375/0943Effective date: 20211123 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |