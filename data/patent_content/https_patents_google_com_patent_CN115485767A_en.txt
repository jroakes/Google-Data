CN115485767A - Issuing word timing using end-to-end model - Google Patents
Issuing word timing using end-to-end model Download PDFInfo
- Publication number
- CN115485767A CN115485767A CN202180032576.4A CN202180032576A CN115485767A CN 115485767 A CN115485767 A CN 115485767A CN 202180032576 A CN202180032576 A CN 202180032576A CN 115485767 A CN115485767 A CN 115485767A
- Authority
- CN
- China
- Prior art keywords
- word
- alignment
- attention
- training
- decoder
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/26—Speech to text systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/06—Creation of reference templates; Training of speech recognition systems, e.g. adaptation to the characteristics of the speaker's voice
- G10L15/063—Training
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/04—Segmentation; Word boundary detection
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/16—Speech classification or search using artificial neural networks
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/27—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 characterised by the analysis technique
- G10L25/30—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 characterised by the analysis technique using neural networks
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/78—Detection of presence or absence of voice signals
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/78—Detection of presence or absence of voice signals
- G10L25/87—Detection of discrete points within a voice signal
Abstract
A method (400) includes receiving a training example (302), the training example (302) including audio data (202) representing a spoken utterance (12) and a true value transcription (204). For each word (310) in the utterance, the method further includes inserting a placeholder symbol before the word to identify a respective real-value alignment (312, 314) of a beginning and an end of the word, and generating a first constraint alignment (330) for the beginning word piece and a second constraint alignment for the ending word piece. The first constraint alignment aligns with a true value of a beginning of the corresponding word and the second constraint alignment aligns with a true value of an end of the corresponding word. The method also includes constraining the attention head of the second traversal decoder (230) by applying the first and second constraint alignments.
Description
Technical Field
The present disclosure relates to two-pass end-to-end speech recognition.
Background
Modern Automatic Speech Recognition (ASR) systems focus on providing not only high quality (e.g., low Word Error Rate (WER)), but also low latency (e.g., short delay between user speaking and transcription occurring). Furthermore, when ASR systems are in use today, there is a need for ASR systems to decode utterances in a streaming manner that corresponds to real-time or even faster than real-time. For example, when an ASR system is deployed on a mobile phone that experiences direct user interactivity, an application on the mobile phone that uses the ASR system may require that speech recognition be streamed such that the word appears on the screen as soon as it is spoken. Here, it is also possible that the user of the mobile phone has a low tolerance to delay. Because of this low tolerance, speech recognition efforts are run on mobile devices in a manner that minimizes delays and inaccuracies that may adversely affect the user experience.
Disclosure of Invention
One aspect of the present disclosure provides a computer-implemented method that, when executed on data processing hardware, causes the data processing hardware to perform operations comprising receiving a training example of a second traversal decoder of a two-pass neural network model. The training examples include audio data representing a spoken utterance of one or more words and a corresponding real-valued transcription of the spoken utterance. For each word in the spoken utterance, the operations further include: inserting placeholder symbols before corresponding words; identifying a respective true value alignment of a beginning of a respective word and an end of the respective word; determining a beginning word piece (word piece) of the corresponding word and an ending word piece of the corresponding word; and generating a first constrained alignment for a beginning word piece of the corresponding word and a second constrained alignment for an ending word piece of the corresponding word. The first constraint alignment aligns with a true value of a beginning of the corresponding word and the second constraint alignment aligns with a true value of an end of the corresponding word. The operations further include constraining an attention head of a second traversal decoder of the two-pass neural network model by applying a training example including all of the first constraint alignments and the second constraint alignments to each word of the training example.
Implementations of the disclosure may include one or more of the following optional features. In some implementations, the operations further include, when training the second traversal decoder on the training examples: identifying an expected attention probability for the portion of the training example; determining that the constrained attention head produces an attention probability for at least one of the portions of the training example that fails to match the expected attention probability; and applying a training penalty to the constrained attention head. In these implementations, the attention probability of the at least one of the portions of the training examples may occur at a time corresponding to the first constraint alignment or the second constraint alignment. On the other hand, the attention probability of the at least one of the portions of the training examples may optionally occur at a time that corresponds to neither the first constraint alignment nor the second constraint alignment.
The start word piece and the end word piece may include the same word piece of the respective word, and the second traversal decoder may include multiple attention heads. In some examples, constraining the attention head includes constraining an attention probability derived from the attention head of the second traversal decoder. Each respective constraint alignment may include a timing buffer aligned with respect to a respective true value. Here, the timing buffer constrains each of the first constraint alignment and the second constraint alignment to a time interval that includes a first time period before the respective true value alignment and a second time period after the respective true value alignment.
In some examples, the operations further comprise, when training the second traversal decoder on the training example: determining that the constrained attention head produces a non-zero probability of attention outside of a boundary corresponding to the first and second constrained alignments; and applying a training penalty to the constrained attention head. Additionally or alternatively, the operations may further include, when training the second traversal decoder on the training example: minimizing the loss of attention of the constrained attention head; and minimizing cross-entropy loss for the second traversal decoder. During execution of the two-pass neural network when using a second traversal decoder trained on the training example, in some additional examples, the operations further comprise receiving audio data of the utterance, determining a time corresponding to a maximum probability at a constrained attention head of the second traversal decoder, and generating a word start time or a word end time for the determined time corresponding to the maximum probability at the constrained attention head of the second traversal decoder.
Another aspect of the disclosure provides a system comprising data processing hardware and memory hardware in communication with the data processing hardware. The memory hardware stores instructions that, when executed by the data processing hardware, cause the data processing hardware to perform operations comprising receiving a training example of a second traversal decoder that traverses a neural network model. The training examples include audio data representing a spoken utterance of one or more words and a corresponding real-valued transcription of the spoken utterance. For each word in the spoken utterance, the operations further include: inserting placeholder symbols before corresponding words; identifying a respective true value alignment of a beginning of the respective word and an end of the respective word; determining a start word piece of the corresponding word and an end word piece of the corresponding word; and generating a first constrained alignment for a beginning word piece of the corresponding word and a second constrained alignment for an ending word piece of the corresponding word. The first constraint alignment aligns with a true value of a beginning of the corresponding word and the second constraint alignment aligns with a true value of an end of the corresponding word. The operations further include constraining an attention head of a second traversal decoder of the two-pass neural network model by applying a training example including all of the first constraint alignments and the second constraint alignments to each word of the training example.
This aspect may include one or more of the following optional features. In some implementations, the operations further include, when training the second traversal decoder on the training examples: identifying an expected attention probability for the portion of the training example; determining that the constrained attention head produces an attention probability for at least one of the portions of the training example that fails to match the expected attention probability; and applying a training penalty to the constrained attention head. In these implementations, the attention probability of at least one of the portions of the training examples may occur at a time corresponding to the first constraint alignment or the second constraint alignment. On the other hand, the attention probability of at least one of the portions of the training examples may optionally occur at a time that corresponds to neither the first constraint alignment nor the second constraint alignment.
The beginning word piece and the ending word piece may include the same word piece of the corresponding word, and the second traversal decoder may include multiple attention heads. In some examples, constraining the attention head includes constraining an attention probability derived from the attention head of the second traversal decoder. Each respective constraint alignment may include a timing buffer aligned with respect to a respective true value. Here, the timing buffer constrains each of the first constraint alignment and the second constraint alignment to a time interval that includes a first time period before the respective true value alignment and a second time period after the respective true value alignment.
In some examples, the operations further comprise, when training the second traversal decoder on the training examples: determining that the constrained attention head produces a non-zero probability of attention outside of boundaries corresponding to the first and second constrained alignments; and applying a training penalty to the constrained attention head. Additionally or alternatively, the operations may further include, when training the second traversal decoder on the training example: minimizing the loss of attention of the constrained attention head; and minimizing cross-entropy loss for the second traversal decoder. During execution of the two-pass neural network when using a second traversal decoder trained on the training example, in some additional examples, the operations further comprise receiving audio data of the utterance, determining a time corresponding to a maximum probability at a constrained attention head of the second traversal decoder, and generating a word start time or a word end time for the determined time corresponding to the maximum probability at the constrained attention head of the second traversal decoder.
The details of one or more embodiments of the disclosure are set forth in the accompanying drawings and the description below. Other aspects, features, and advantages will be apparent from the description and drawings, and from the claims.
Drawings
1A-1C are schematic diagrams of an exemplary speech environment using a two-pass architecture with joint acoustic and text models.
FIG. 2 is a schematic diagram of an exemplary two-pass architecture for speech recognition.
FIG. 3A is a schematic diagram of an exemplary training process for the two-pass architecture of FIG. 2 for speech recognition.
FIG. 3B is a diagram of exemplary constraints in a training example of the training process of FIG. 3A.
FIG. 4 is a flow chart of an exemplary arrangement of operations for implementing a method incorporating the two-pass architecture of FIG. 2 of constraining attention.
FIG. 5 is a schematic diagram of an exemplary computing device that may be used to implement the systems and methods described herein.
Like reference symbols in the various drawings indicate like elements.
Detailed Description
Speech recognition continues to evolve to meet the unconstrained and flexible requirements of mobile environments. New speech recognition architectures or improvements to existing architectures are continually being developed in an attempt to improve the quality of automatic speech recognition systems (ASR). For example, speech recognition initially employs multiple models, each of which has a dedicated purpose. For example, ASR systems include an Acoustic Model (AM), a Pronunciation Model (PM), and a Language Model (LM). The acoustic model maps audio segments (i.e., audio frames) to phonemes. A pronunciation model connects these phonemes together to form words, and a language model is used to express the likelihood of a given phrase (i.e., the probability of a sequence of words). However, although these separate models work together, each model is trained independently and is typically designed manually on a different data set.
The approach of individual models enables the speech recognition system to be quite accurate, particularly when the training corpus of a given model (i.e., the subject of the training data) caters to the effectiveness of the model, but requires the individual models to be trained independently, introducing their own complexity, and results in an architecture with integrated models. These integrated models attempt to map audio waveforms (i.e., input sequences) directly to output sentences (i.e., output sequences) using a single neural network. This results in a sequence-to-sequence approach that produces a sequence of words (or graphemes) when given a sequence of audio features. Examples of sequence-to-sequence models include an "attention-based" model and a "listen-attention-spelling" (LAS) model. The LAS model transcribes a speech utterance into characters using a listener component, an attentive component, and a speller component. Here, a listener is a Recurrent Neural Network (RNN) encoder that receives an audio input (e.g., a time-frequency representation of a speech input) and maps the audio input to a higher-level feature representation. Note that the notes note the more advanced features to learn the alignment between the input features and predicted sub-word units (e.g., grapheme or word fragments) or other phonetic units (e.g., phonemes, phones, or phonemes (senomes)). The speller is an attention-based RNN decoder that generates a sequence of characters from an input by generating a probability distribution over a set of hypothesized words. With the integrated structure, all components of the model can be jointly trained as a single end-to-end (E2E) neural network. Here, the E2E model refers to a model whose architecture is constructed entirely by a neural network. An all-neural network operates without externally and/or manually designed components (e.g., finite State transducers, dictionaries, or text normalization modules). Furthermore, when training E2E models, these models typically do not need to be guided from a decision tree or time aligned with a separate system.
Although early E2E models proved accurate and were training improvements to individually trained models, these E2E models, such as LAS models, operate by examining the entire input sequence before generating output text, and therefore, do not allow streaming output when input is received. Without streaming capability, the LAS model cannot perform real-time voice transcription. Due to this deficiency, deploying LAS models for speech applications that are delay sensitive and/or require real-time speech transcription can be problematic. This makes the separate LAS model a less than ideal model for mobile technologies (e.g., mobile phones) that often rely on real-time applications (e.g., real-time communication applications).
Furthermore, speech recognition systems having acoustic, pronunciation, and language models, or a combination of such models, may rely on a decoder that must search for relatively large search maps associated with the models. For large search graphs, it is disadvantageous to host this type of speech recognition system completely on the device. Here, when the speech recognition system is hosted "on the device," the device receiving the audio input uses its processor to perform the functionality of the speech recognition system. For example, when the speech recognition system is completely hosted on the device, the processor of the device need not coordinate with any off-device computing resources to perform the functionality of the speech recognition system. Devices that do not completely perform speech recognition on the device rely on remote computing (e.g., a remote computing system or cloud computing) and thus on online connectivity to perform at least some functions of the speech recognition system. For example, speech recognition systems use network connections with server-based models to perform decoding using large search graphs.
Unfortunately, relying on a remote connection makes the speech recognition system susceptible to latency problems and/or the inherent unreliability of the communication network. To improve the usefulness of speech recognition by avoiding these problems, speech recognition systems have evolved again into a form of sequence-to-sequence model called a recurrent neural network converter (RNN-T). RNN-T does not employ a mechanism of attention, and unlike other sequence-to-sequence models that typically require processing of an entire sequence (e.g., audio waveform) to produce an output (e.g., sentence), RNN-T continuously processes input samples and streams output symbols, a feature that is particularly attractive for real-time communications. For example, speech recognition using RNN-T may output characters one after another as they are spoken. Here, RNN-T feeds back the symbol predicted by the model to itself using a feedback loop to predict the next symbol. Because decoding the RNN-T involves beam searching through a single neural network rather than a large decoder graph, the RNN-T can be scaled to a fraction of the size of the server-based speech recognition model. As the size decreases, RNN-T may be fully deployed on the device and able to run offline (i.e., without network connectivity); thus avoiding the unreliability problems of the communication network.
In addition to speech recognition systems that operate with low latency, speech recognition systems also need to accurately recognize speech. Often for models that perform speech recognition, a metric that can define the accuracy of the model is the Word Error Rate (WER). WER refers to a measure of how many words have been altered compared to the number of words actually spoken. Typically, these word changes refer to replacement (i.e., when a word is replaced), insertion (i.e., when a word is added), and/or deletion (i.e., when a word is omitted). For example, the speaker says "car," but the ASR system transcribes the word "car" to "bar. This is an example of an alternative due to speech similarity. When measuring the capabilities of an ASR system as compared to other ASR systems, the WER may indicate some measure of improved or quality capabilities relative to another system or some baseline.
Although the RNN-T model shows promise as a powerful candidate model for on-device speech recognition, the RNN-T model alone still lags behind the large most advanced conventional models (e.g., server-based models with individual AM, PM, and LM) in quality (e.g., speech recognition accuracy). However, the non-streaming E2E, LAS model has speech recognition quality comparable to the large most advanced conventional model. To take advantage of the quality of the non-streaming E2E LAS model, a two-pass speech recognition system (e.g., as shown in FIG. 2A) was developed that includes a first traversal component of the RNN-T network followed by a second traversal component of the LAS network. With this design, the two-pass model benefits from the streaming nature of the RNN-T model with low latency, while improving the accuracy of the RNN-T model by incorporating a second pass of the LAS network. Although LAS networks add latency compared to RNN-T only models, the increase in latency is quite slight and conforms to the latency constraints of on-device operation. In terms of accuracy, the two-pass model achieved a 17-22% reduction in WER compared to RNN-T alone, while having a similar WER compared to the large conventional model.
Unfortunately, this two-pass model with the first traversal of the RNN-T network and the second traversal of the LAS network has some drawbacks. For example, this type of two-pass model is generally unable to communicate the timing of words (e.g., the start time or end time of each word) because the two-pass model is not trained with alignment information as is done with conventional models. Without alignment information, the two-pass model often delays its output prediction, making it difficult to determine the timing of the words. In contrast, conventional models are trained with alignment information, such as phoneme alignment or word alignment, which allows the conventional models to produce accurate word timing. This is a compromise for the user of the speech recognition system. On the one hand, the two-pass model has the benefit that it takes place on the device to provide privacy and minimal latency, but does not have the ability to issue word timing. On the other hand, large conventional models may produce accurate word timing, but are too large to be implemented on a device, forcing users to use remote-based non-streaming speech recognition systems with the potential for increased latency (e.g., as compared to two-pass models).
In order for the two-pass model to emit word timing without compromising latency or quality loss, the two-pass model may be adapted to take advantage of its own architecture with additional constraints. In other words, based on the size constraints of the two-pass model, the two-pass model neither incorporates the elements of a large conventional model to fit on the device, nor increases its overall latency by using a post-processing module after generating the final assumptions. Fortunately, in training the LAS network for the second traversal, the attention probabilities of the LAS network learn the alignment between the audio corresponding to the training examples and the predicted subword units (e.g., graphemes, word pieces, etc.) of the training examples. By constraining the probability of attention of the LAS network based on word-level alignments, the two-pass model may produce a start time and an end time for each word. With these word timings, a user can use the two-pass model on a device for various applications, such as a voice assistant, a dictation application, or video transcription.
Fig. 1A to 1C are examples of a speech environment 100. In speech environment 100, the way in which user 10 interacts with a computing device, such as user device 110, may be through speech input. User devices 110 (also commonly referred to as devices 110) are configured to capture sound (e.g., streaming audio data) from one or more users 10 within speech-enabled environment 100. Here, streaming audio data 202 may refer to a spoken utterance 12 of the user 10 that serves as an audible query (e.g., fig. 1C), command, or audible communication captured by the device 110 (e.g., fig. 1B). The voice-enabled system of device 110 may respond to the query or command by answering the query and/or causing the command to be executed.
Here, the user device 110 captures audio data 202 of the spoken utterance 12 of the user 10. The user device 110 may correspond to any computing device associated with the user 10 and capable of receiving the audio data 202. Some examples of user devices 110 include, but are not limited to, mobile devices (e.g., mobile phones, tablet computers, laptop computers, etc.), computers, wearable devices (e.g., smart watches), smart appliances, internet of things (IoT) devices, smart speakers, and so forth. The user device 110 includes data processing hardware 112 and memory hardware 114, the memory hardware 114 being in communication with the data processing hardware 112 and storing instructions that, when executed by the data processing hardware 112, cause the data processing hardware 112 to perform one or more operations. The user device 110 also includes an audio subsystem 116 having audio capture devices (e.g., microphones) 116, 116a for capturing and converting the spoken utterance 12 within the speech-enabled system 100 into an electrical signal and speech output devices (e.g., speakers) 116, 116b for delivering audible audio signals (e.g., as output audio data from the device 110). Although in the illustrated example the user device 110 implements a single audio capture device 116a, the user device 110 may implement an array of audio capture devices 116a without departing from the scope of this disclosure, whereby one or more capture devices 116a in the array may not reside physically on the user device 110, but rather are in communication with the audio subsystem 116.
The user device 110 (e.g., using hardware 112, 114) is also configured to perform speech recognition processing on the streaming audio data 202 using a speech recognizer 200. In some examples, the audio subsystem 116 of the user device 110, including the audio capture device 116a, is configured to receive audio data 202 (e.g., the spoken utterance 12) and convert the audio data 202 into a digital format compatible with the speech recognizer 200. The digital format may correspond to an acoustic frame (e.g., a parameterized acoustic frame), such as a mel (mel) frame. For example, the parameterized acoustic frame corresponds to a log mel filter bank energy.
In some implementations, such as FIG. 1A, the user 10 interacts with a program or application 118 of the user device 110 that uses the speech recognizer 200. For example, FIG. 1A depicts the user 10 in communication with a transcription application 118 that is capable of transcribing the utterance 12 spoken by the user 10. In this example, the spoken utterance 12 of the user 10 is "What time is the conceert tonight? (what time is a concert tonight?) this question from the user 10 is the spoken utterance 12 captured by the audio capture device 116a and processed by the audio subsystem 116 of the user device 110. In this example, the speech recognizer 200 of the user device 110 receives the audio input 202 of "what time this evening concert is" (e.g., as an acoustic frame) and transcribes the audio input 202 into a transcription 204 (e.g., a textual representation of "what time this evening concert is. Here, transcription application 118 marks each word of transcription 204 with corresponding start and end times based on word timing 206 generated by speech recognizer 200. For example, using these start and end times, the user 10 can edit the transcription 204 or audio corresponding to the transcription 204. In some examples, the transcription application 118 corresponds to a video transcription application configured to edit and/or process audio/video data on the user device 110 based on, for example, start and end times of the speech recognizer 200 associated with words of the transcription 204.
FIG. 1B is another example of speech recognition with speech recognizer 200. In this example, the user 10 associated with the user device 110 is communicating with a friend named Jane Doe using the communication application 118. Here, the user 10 named Ted communicates with Jane by having the speech recognizer 200 transcribe his speech input. The audio capture device 116 captures these speech inputs and passes them in digital form (e.g., acoustic frames) to the speech recognizer 200. The speech recognizer 200 transcribes these acoustic frames into text that is sent to Jane via the communication application 118. Because this type of application 118 communicates via text, the transcription 204 from the speech recognizer 200 can be sent to Jane without further processing (e.g., natural language processing). Here, the communication application 118 using the speech recognizer 200 may associate a time with one or more portions of the conversation. Depending on the communication application 118, these times may correspond in some detail to the word timing 206 of each word of the conversation processed by the speech recognizer 200 (e.g., the start time and end time of each word), or more generally to the time associated with the portion of the conversation of each speaker (e.g., as shown in FIG. 1B).
FIG. 1C depicts a dialog very similar to that of FIG. 1B, but with the voice assistant application 118. In this example, the user 10 asks the automated assistant, "what time is a concert tonight? "this issue from the user 10 is the spoken utterance 12 captured by the audio capture device 116a and processed by the audio subsystem 116 of the user device 110. In this example, the speech recognizer 200 of the user device 110 receives the audio input 202 of "what time this evening concert is" (e.g., as an acoustic frame) and transcribes the audio input 202 into a transcription 204 (e.g., a textual representation of "what time this evening concert is. Here, the automated assistant of the application 118 may respond to the question posed by the user 10 using natural language processing. Natural language processing generally refers to the process of interpreting a written language (e.g., transcription 204) and determining whether the written language prompts any action. In this example, the automated assistant uses natural language processing to identify that the question from the user 10 relates to the user's schedule, and more particularly to a concert on the user's schedule. By identifying these details with natural language processing, the automated assistant returns a response to the user query, with the response stating that "Doors open at 8. Similar to FIG. 1B, the speech recognizer 200 can emit a time (e.g., word timing 206) that can be used by the speech assistant application 118 to provide additional detail about the conversation between the user 10 and the automated assistant. For example, FIG. 1C shows the voice assistant application 118 tagging a question of the user with the time that the question occurred.
In some examples, such as fig. 2, the speech recognizer 200 is configured as a two-pass architecture. In general, the two-pass architecture of the speech recognizer 200 includes at least one encoder 210, an RNN-T decoder 220, and an LAS decoder 230. In two-pass decoding, the second traversal 208, 208b (e.g., shown as the LAS decoder 230) may utilize techniques such as trellis rescoring or n-best re-ranking to improve the initial output from the first traversal 208, 208a (e.g., shown as the RNN-T decoder 220). In other words, the RNN-T decoder 220 generates a streaming prediction, and the LAS decoder 230 completes the prediction. Here, in particular, the LAS decoder 230 pairs the hypotheses y from streaming in the RNN-T decoder 220 R And performing re-scoring. Although it is generally discussed that the LAS decoder 230 operates in the repartitioning mode, this mode is valid for the assumption y of streaming from the RNN-T decoder 220 R Rescoring is performed, but the LAS decoder 230 can also operate in different modes such as a beam search mode depending on design or other factors (e.g., utterance length).
The at least one encoder 210 is configured to receive, as the audio input 202, an acoustic frame corresponding to the streaming spoken utterance 12. The acoustic frames may be pre-processed by the audio subsystem 116 into parameterized acoustic frames (e.g., mel frames and/or spectral frames). In some embodiments, the parameterized acoustic frame corresponds to a log mel filter bank energy with log mel features. For example, a parameterized input acoustic frame output by the audio subsystem 116 and input to the encoder 210 may be represented as x = (x) 1 ,...,x T ) Whereinaudio input 202, the encoder 210 is configured to generate an encoding e. For example, the encoder 210 generates encoded acoustic frames (e.g., encoded mel frames or acoustic embedding).
Although the structure of the encoder 210 may be implemented in different ways, in some embodiments the encoder 210 is a Long Short Term Memory (LSTM) neural network. For example, encoder 210 includes eight LSTM layers. Here, each layer may have 2048 hidden units followed by a 640-dimensional projection layer. In some examples, a temporal reduction layer with a reduction factor of N =2 is inserted after the second LSTM layer of the encoder 210 (e.g., to ensure that the encoding features occur at a particular frame rate).
In some configurations, encoder 210 is a shared encoder network. In other words, each traversal network 208 shares a single encoder 210, rather than each traversal 208 having its own separate encoder. By sharing the encoder, the ASR speech recognizer 200 using the two-traversal architecture can reduce its model size and/or its computational cost. Here, the reduction in model size may help enable the speech recognizer 200 to function well on the device at all.
In some examples, the speech recognizer 200 of fig. 2 further includes an additional encoder, such as an acoustic encoder 250, to fit the output 212 of the encoder 210 into the second traversal 208b of the LAS decoder 230. The acoustic encoder 250 is configured to further encode the output 212 into an encoded output 252. In some implementations, the acoustic encoder 250 is an LSTM encoder (e.g., a two-layer LSTM encoder) that further encodes the output 212 from the encoder 210. By including additional encoders, encoder 210 may still be retained as a shared encoder between traversals 208.
During the first pass 208a, the encoder 210 receives each acoustic frame of the audio input 202 and produces an output 212 (e.g., shown as encoding e of the acoustic frame). RNN-T decoder 220 receives output 212 for each frame and streams output 222 at each time step, shown as hypothesis y R . In some embodiments, the RNN-T decoder 220 includes a predictive network and a joint network. Here, the prediction network may have two LSTM layers of 2048 hidden units and 640-dimensional projections per layer and 128 units of embedded layers. The output 212 of the encoder 210 and the prediction network may be fed into a joint network that includes a softmax prediction layer. In some examples, the joint network of RNN-T decoders 220 includes 640 hidden units followed by prediction of the softmax layer of 4096 mixed-case word fragments.
In the two-pass model of FIG. 2, during the second pass 208b, the LAS decoder 230 receives the output 212 from the encoder 210 for each frame and generates the output designated as hypothesis y L And an output 232. When the LAS decoder 230 operates in the beam search mode, the LAS decoder 230 generates only an output 232 from the output 212; the output 222 of the RNN-T decoder 220 is ignored. When the LAS decoder 230 operates in the re-score mode, the LAS decoder 230 obtains the first K hypotheses from the RNN-T decoder 220, and then the LAS decoder 230 operates on each sequence in the teacher-forced mode to calculate a score, with attention to the output 212. For example, the score incorporates the log probability and attention cover penalty of the sequence. The LAS decoder 230 selects the sequence with the highest score as output 232. Here, in the re-scoring mode, the LAS decoder 230 may include multi-headed attention (e.g., having four heads) to pay attention to the output 212. Also, the LAS decoder 230 may be a two-layer LAS decoder 230 having a softmax layer for prediction. For example, each layer of the LAS decoder 230 has 2048 hidden units followed by 640-dimensional projection. The softmax layer may include 4096 dimensions to predict the same mixed-case word-slice from the softmax layer of the RNN-T decoder 220.
In general, the two-pass model of FIG. 2 without any additional constraints has difficulty detecting word timing 206. This difficulty exists at least in part because the two-pass model tokenizes or breaks words into one or more word pieces. Here, for example, when a single piece of word corresponds to an entire word, the start time and the end time of the entire word coincide with the start time and the end time of the single piece of word. However, when a word is composed of a plurality of word pieces, the start time of the word may correspond to one word piece, and the end time of the word corresponds to a different word piece. Unfortunately, conventional two-pass models may therefore have difficulty identifying when a word begins and when a word ends based on a piece of words. To overcome these problems, the two-pass model may be trained with certain constraints regarding the alignment of the word pieces with respect to the start time and end time of a given word of the training example.
The conventional training process for the two-pass model of FIG. 2 may be performed in twoA stage occurs. During the first stage, the encoder 210 and the RNN-T decoder 220 are trained to maximizeencoder 210 is fixed, and the LAS decoder 230 is trained such thatadditional encoder 250, the additional encoder 250 is trained in the second stage such thatencoder 210 is fixed. However, as shown in fig. 3A and 3B, the conventional training process may be applied to the training process 300 including additional constraints on the LAS decoder 230. For example, the training process 300 constrains an attention head of the LAS decoder 230 (e.g., one of a plurality of attention heads at the LAS decoder 230) to produce an attention probability indicative of the word timing 206 corresponding to the output 232 of the second traversal 208b. In some configurations, by including this additional constraint, training process 300 trains to minimize standard cross-entropy loss at LAS decoder 230 and to minimize attention alignment loss of LAS decoder 230 (e.g., attention alignment loss of the attention head of LAS decoder 230).
In some examples, such as fig. 3A and 3B, the training process 300 trains the two-pass model architecture 200 over a plurality of training examples 302, each training example 302 including audio data representing a spoken utterance and a corresponding real-valued transcription of the spoken utterance. For each word of the corresponding spoken utterance, the corresponding training example 302 also includes a real-value start time 312 for the word, a real-value end time 314 for the word, and a constraint 304 indicating where each word piece in the word emanating from the LAS decoder 230 should appear. Training process 300 may be performed on system 500 (fig. 5) to train speech recognizer 200. The trained speech recognizer 200 may be deployed to run on the user equipment 110 of fig. 1A through 1C. Alternatively, trained speech recognizer 200 may be running on system 500 or another system in communication with user device 110. The training process 300 uses the constraints 304 of the training example 302 to teach the two-pass model to generate (or insert) placeholder symbols before each word 310 to indicate the beginning of the corresponding word 310 in the utterance and/or the placeholder symbols after the last word 310 of the spoken utterance 12. In some configurations, the placeholder symbols are word boundaries < wb > word tiles 320 (e.g., shown as word tiles 320,320a, d, g) before each word 310 and/or utterance boundaries </s > word tiles 320 after the last word 310 of the utterance 12. With the training example 302 having placeholder symbols corresponding to the words 310, the two-pass model learns to include the placeholder symbols as word patches 320 during its generation of the transcription 204. Using a two-pass model (i.e., the use of a two-pass model) trained to produce boundary word patches 320 (e.g., word boundaries < wb > and/or utterance boundaries) during inference, the boundary word patches 320 enable the speech recognizer 200 to have further detail in order to determine the word timing 206.
To issue the word timing 206 from the two-pass model using the word piece 320, the two-pass model is configured to focus on a particular word piece corresponding to the beginning of the corresponding word 310 or the end of the corresponding word 310. More specifically, the training process 300 wants to constrain the first word piece 320 corresponding to the beginning of the respective word 310 to appear as close as possible to the beginning of the alignment 312 of the respective word 310, and to constrain the last word piece 320 corresponding to the end of the respective word 310 to appear as close as possible to the end of the alignment 314 of the respective word 310. Here, constraint 304 constrains all other word pieces 320 that make up word 310 to appear anywhere within the bounds of the real value start time 312 and the real value end time 314 of word 310.
Referring to fig. 3B, during the training process 300, the LAS decoder 230 is trained using training examples 302 that include training example constraints 304. As discussed above, the training example constraint 304 is configured to constrain a first word piece 320 corresponding to the beginning of a respective word 310 to appear as close as possible to the beginning of the alignment of the respective word 310, and to constrain a last word piece 320 corresponding to the end of the respective word 310 to appear as close as possible to the end of the alignment of the respective word 310. For example, FIG. 3B depicts a simple training example 302 with three words 310,310a through c "the cat sat". Here, each word 310 of the training example 302 has a known true value alignment, with a true value alignment start time 312 and a true value alignment end time 314. In FIG. 3B, the "first word 310a has a first true value start time 312,312a and a first true value end time 314,314a. The second word 310b 'cat' has a second true value start time 312,312b and a second true value end time 314,314b. The third word 310c "sat" has a third true value start time 312,12c and a third true value end time 314,314c.
Based on the real- value alignments 312, 314 of each word 310, the training examples 302 include training example constraints 304 that constrain each word slice 320 corresponding to the word 310 to align with the real- value alignments 312, 314. Here, the first word 310a comprises three word pieces 320,320a-c: a first word tile 320a, which is a boundary word tile 320 (e.g., shown as < wb >); a second word tile 320b, "_ th"; and a third word piece 320c, "e". The second word 310b 'cat' includes three word pieces 320,320d-f: a fourth word tile 320d, which is a boundary word tile 320 (e.g., shown as < wb >); a fifth word slice 320e, "_ c"; and a sixth word piece 320f, "at". The third word 310c, sat, includes three word slices 320,320g-i: a seventh word tile 320g, which is a boundary word tile 320 (e.g., shown as < wb >); eighth token 320h, "sat"; and a ninth word tile 320i, which is a speech boundary 320 (e.g., shown as).
The training process 300 is configured to determine which token piece 320 of the respective word 310 corresponds to the beginning (i.e., the beginning token piece) of the respective word 310 and which token piece 320 of the respective word 310 corresponds to the end (i.e., the ending token piece) of the respective word 310. For example, in the example of fig. 3B, the training process 300 determines that the first word piece 320a is the beginning word piece of the first word 310a, the fourth word piece 320d is the beginning word piece of the second word 310B, and the seventh word piece 320g is the beginning word piece of the third word 310 c. Likewise, the training process 300 determines that the third word piece 320c is the ending word piece of the first word 310a, the sixth word piece 320f is the ending word piece of the second word 310b, and the ninth word piece 320i is the ending word piece of the third word 310 c. In some examples, the beginning word piece 320 and the ending word piece 320 are the same word piece 320 because a particular word 310 includes only one word piece 320.
Once the training process 300 determines the beginning word piece and the ending word piece for each word 310 in the training example 302, the training process 300 is configured to generate a constrained alignment 330 for each of the beginning word piece 320 and the ending word piece 320. In other words, the training process 300 generates an alignment constraint that aims to establish when a particular word slice 320 should occur during the time index based on the timing of the true value alignments 312, 314. In some implementations, the constraint alignments 330 of the word tiles 320 span a time interval ranging from the word tile start time 322 to the word tile end time 324. When a word slice 320 is a beginning word slice 320 of a word 310, the beginning word slice 320 has a constrained alignment 330 that aligns with the true value alignment start time 312. For example, constraint alignment 330 of the start word slice 320 spans a time interval centered on the real-value alignment start time 312. On the other hand, when word slice 320 is the ending word slice 320 of word 310, ending word slice 320 has a constrained alignment 330 that aligns with true value alignment ending time 314. For example, constraint alignment 330 of the end word slice 320 spans a time interval centered on the real value alignment start time 312. When a word slice 320 corresponds to neither a beginning word slice 320 nor an ending word slice 320, the word slice 320 may have a constraint alignment 330 corresponding to a time interval ranging from a real value alignment start time 312 to a real value alignment end time 314. In other words, the training example constraint 304 indicates that a word slice 320 that corresponds to neither a beginning word slice 320 nor an ending word slice 320 may occur at any point in time between the times at which the real values of the words 310 corresponding to the word slices 320 occur.
In some configurations, the training process 300 includes a tunable constraint alignment 330. In other words, the word-slice start time 322 and/or the word-slice end time 324 may be adjusted to define different time intervals for the true value alignments 312, 314. Here, the time interval may be referred to as a timing buffer, such that the timing buffer includes a first time period before the real value alignments 312, 314 and a second time period after the real value alignments 312, 314. In other words, the first time period of the timing buffer is equal to the length of time between the word-slice start time 322 and the real- value alignments 312, 314, and the second time period of the timing buffer is equal to the length of time between the word-slice end time 324 and the real- value alignments 312, 314. By tuning the timing buffer, training the example constraints 304 can optimize the WER of the two-pass model while attempting to minimize latency. For example, experimentation with timing buffers has resulted in timing buffers of approximately 180 milliseconds being superior to 60 or 300 milliseconds timing buffers in terms of WER and latency.
In some examples, the training process 300 applies the constraint alignments 330 (e.g., constraint alignments 330, 330a-i) to an attention mechanism associated with the LAS decoder 230. In other words, the training process 300 trains the LAS decoder 230 (e.g., the attention head of the LAS decoder 230) using one or more training examples 302 that include training example constraints 304. In some embodiments, although LAS decoder 230 includes multiple attention heads, training process 300 constrains one or less than all of the attention heads of LAS decoder 230 in order to allow one or more of the attention heads to operate unconstrained. Here, during the training process 300, the constrained attention head generates an attention probability for each training example 302. When the attention probability produced by the attention head corresponds to the constraint alignment 330, the training process 300 is configured to compare the attention probability to the expected attention probability of the training example 302 at the constraint alignment 330. In some configurations, training example constraints 304 indicate expected attention probabilities for constraint alignments 330 for each word slice 320. For example, the expected probability of a constrained alignment 330 between word slice start time 322 and word slice end time 324 is set to a high or non-zero value (e.g., a value of one) to indicate that the alignment of word slice 320 occurs at an allowable time (i.e., within constrained alignment 330). In some examples, training example 302 includes an expected attention probability set to a low or zero value to indicate that the alignment of word tile 320 occurred at a disallowed alignment time (e.g., not within constrained alignment 330). When, during the training process 300, the attention probability fails to match or satisfy the expected attention probability, the training process 300 is configured to apply a training penalty to the constrained attention head. In some examples, the training process 300 applies a training penalty such that the training penalty minimizes attention loss of the LAS decoder 230 during training. In some examples, the attention loss is represented by the following equation:
where beta is the loss of control attentiontraining example constraint 304 of each word-slice unit U as a function of time T, and a (U, T) corresponds to a constraint attention head of each word-slice unit U as a function of time T. Additionally or alternatively, the training process 300 may apply a training penalty to minimize the total loss of the LAS decoder 230 during training, where the total loss is represented by the following equation:
by applying the training penalty, the training process 300 teaches the constrained attention head of the LAS decoder 230 over a plurality of training examples 302 to have a maximum probability of attention for each word slice 320 at a time corresponding to when that word slice 320 occurred in time. For example, once the training process 300 has trained the two-pass model, during decoding, the LAS decoder 230 operates a beam search that emits word-tile units u at each step of the beam search. Here, the speech recognizer 200 determines the word slice timing for each word slice unit u by finding the time index that results in the maximum constrained attention head probability for that particular word slice unit u. From the word slice timing, the actual word timing 206 can be derived. For example, the word-slice timing of the boundary word-slice 320 corresponds to the beginning of the word 310 and the end of the word 310. Here, the beginning word slice 320 of a word 310 (e.g., word boundary < wb > word slice 320) will have a timing corresponding to the beginning time of the corresponding word 310, and the ending word slice 320 of the corresponding word 310 (e.g., shown in FIG. 3B as utterance boundary </s > word slice 320) will have a timing corresponding to the ending time of the corresponding word 310. In other words, the speech recognizer 200 may determine that the actual word timing 206 (e.g., the start time of the word 310 and the end time of the word 310) is equal to the word slice timing of the beginning word slice 320 and the ending word slice 320. Based on this determination, the speech recognizer 200 is configured to generate word timings 206 for words output by the speech recognizer 200 (e.g., as shown in fig. 1A-1C).
FIG. 4 is a flow diagram of an exemplary arrangement of operations of a method 400 of implementing the speech recognizer with constrained attention 200. At operation 402, the method 400 receives a training example 302 of an LAS decoder 230 that traverses a neural network model twice. At operation 404, the method 400 performs operations 404, 404a-d on each word 310 of the training example 302. At operation 404a, the method 400 inserts a placeholder symbol before the corresponding word 310. At operation 404b, the method 400 identifies the respective real value alignments 312, 314 of the beginning of the respective word 310 and the end of the respective word 310. At operation 404c, the method 400 determines a beginning word piece 320 of the corresponding word 310 and an ending word piece 320 of the corresponding word 310. At operation 404d, the method 400 generates a first constraint alignment 330 for the beginning word piece 320 of the corresponding word 310 and a second constraint alignment 330 for the ending word piece 320 of the corresponding word 310. Here, the first constraint alignment 330 is aligned with the real value alignments 312, 314 (e.g., real value alignment start time 312) of the beginning of the respective words 310, and the second constraint alignment 330 is aligned with the real value alignments 312, 314 (e.g., real value alignment end time 314) of the end of the respective words 310. At operation 406, the method 400 constrains the attention of the LAS decoder 230 that traverses the neural network model by applying a training example 302 that includes all of the first constraint alignment 330 and the second constraint alignment 330 to each word 310 of the training example 302.
FIG. 5 is a schematic diagram of an exemplary computing device 500 that may be used to implement the systems (e.g., speech recognizer 200) and methods (e.g., training process 300 and/or method 400) described in this document. Computing device 500 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers. The components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document.
The memory 520 stores information within the computing device 500 non-temporarily. The memory 520 may be a computer-readable medium, a volatile memory unit or a nonvolatile memory unit. Non-transitory memory 520 may be a physical device used to temporarily or permanently store programs (e.g., sequences of instructions) or data (e.g., program state information) for use by computing device 500. Examples of non-volatile memory include, but are not limited to, flash memory and read-only memory (ROM)/programmable read-only memory (PROM)/erasable programmable read-only memory (EPROM)/electrically erasable programmable read-only memory (EEPROM) (e.g., commonly used for firmware, such as boot programs). Examples of volatile memory include, but are not limited to, random Access Memory (RAM), dynamic Random Access Memory (DRAM), static Random Access Memory (SRAM), phase Change Memory (PCM), and magnetic disks or tape.
The storage device 530 is capable of providing mass storage for the computing device 500. In some implementations, the storage device 530 is a computer-readable medium. In various different implementations, the storage device 530 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations. In additional embodiments, a computer program product is tangibly embodied in an information carrier. The computer program product contains instructions that, when executed, perform one or more methods such as those described above. The information carrier is a computer-readable medium or machine-readable medium, such as the memory 520, the storage device 530, or memory on processor 510.
The high-speed controller 540 manages bandwidth-intensive operations for the computing device 500, while the low-speed controller 560 manages lower bandwidth-intensive operations. Such allocation of duties is exemplary only. In some implementations, the high-speed controller 540 is coupled to memory 520, display 580 (e.g., through a graphics processor or accelerator), and to high-speed expansion ports 550, which may accept various expansion cards (not shown). In some embodiments, low-speed controller 560 is coupled to storage device 530 and low-speed expansion port 590. The low-speed expansion port 590, which may include various communication ports (e.g., USB, bluetooth, ethernet, wireless ethernet), may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a network device such as a switch or router, for example, through a network adapter.
The computing device 500 may be implemented in many different forms, as shown in the figures. For example, it may be implemented as a standard server 500a or multiple times in a group of such servers 500a as a laptop computer 500b or as part of a rack server system 500 c.
Various implementations of the systems and techniques described here can be realized in digital electronic and/or optical circuits, integrated circuits, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof. These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
These computer programs (also known as programs, software applications or code) include machine instructions for a programmable processor, and may be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. As used herein, the terms "machine-readable medium" and "computer-readable medium" refer to any computer program product, non-transitory computer-readable medium, apparatus and/or device (e.g., magnetic discs, optical disks, memory, programmable Logic Devices (PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term "machine-readable signal" refers to any signal used to provide machine instructions and/or data to a programmable processor.
The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, and in particular by, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a processor for executing instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, the computer need not have such devices. Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example: semiconductor memory devices, such as EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
To provide for interaction with a user, one or more aspects of the disclosure may be implemented on a computer having a display device, e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor or touch screen, for displaying information to the user and optionally a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user may provide input to the computer. Other types of devices may also be used to provide for interaction with a user; for example, feedback provided to the user can be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input. Further, the computer may interact with the user by sending and receiving documents to and from the device used by the user; for example, by sending a Web page to a Web browser on the user's client device in response to a request received from the Web browser.
A number of embodiments have been described. Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the disclosure. Accordingly, other implementations are within the scope of the following claims.
Claims (22)
1. A computer-implemented method (400) that, when executed on data processing hardware (112), causes the data processing hardware (112) to perform operations comprising:
receiving a training example (302) of a second traversal decoder (230) of a two-pass neural network model, the training example (302) containing audio data (202) of a spoken utterance (12) representing one or more words (310) and a corresponding real-valued transcription (204) of the spoken utterance (12);
for each word (310) in the spoken utterance (12):
inserting placeholder symbols before the corresponding word (310);
identifying a respective real-value alignment (312, 314) of a beginning of the respective word (310) and an end of the respective word (310);
determining a beginning word piece (320) of the respective word (310) and an ending word piece (320) of the respective word (310); and
generating a first constraint alignment (330) of the beginning word piece (320) of the respective word (310) and a second constraint alignment (330) of the ending word piece (320) of the respective word (310), the first constraint alignment (330) being aligned with the real value alignment (312, 314) of the beginning of the respective word (310), the second constraint alignment (330) being aligned with the real value alignment (312, 314) of the ending of the respective word (310); and
constraining an attention head of the second traversal decoder (230) of the two-pass neural network model by applying the training example (302) to each word (310) of the training example (302), the training example (302) containing all of the first constraint alignment (330) and the second constraint alignment (330).
2. The computer-implemented method (400) of claim 1, wherein the beginning word piece (320) and the ending word piece (320) contain the same word piece (320) of the respective word (310).
3. The computer-implemented method (400) of claim 1 or 2, wherein the second traversal decoder (230) contains a plurality of heads of attention.
4. The computer-implemented method (400) of any of claims 1-3, wherein constraining the attention head comprises: constraining attention probabilities derived from the attention head of the second traversal decoder (230).
5. The computer-implemented method (400) of any of claims 1-4, wherein the operations further comprise, when training the second traversal decoder (230) on the training examples (302):
identifying an expected attention probability for the training example (302) of a portion;
determining that the constrained attention head produces an attention probability for at least one of the portions of the training examples (302) that fail to match the expected attention probability; and
applying a training penalty to the constrained attention head.
6. The computer-implemented method (400) of claim 5, wherein the attention probability of the at least one of the portions of the training examples (302) occurs at a time corresponding to the first constraint alignment (330) or the second constraint alignment (330).
7. The computer-implemented method (400) of claim 5, wherein the attention probability of the at least one of the portions of the training examples (302) occurs at a time that corresponds to neither the first constraint alignment (330) nor the second constraint alignment (330).
8. The computer-implemented method (400) of any of claims 1-7, wherein the operations further comprise, when training the second traversal decoder (230) on the training examples (302):
determining that the constrained attention head produces a non-zero probability of attention outside a boundary corresponding to the first constrained alignment (330) and the second constrained alignment (330); and
applying a training penalty to the constrained attention head.
9. The computer-implemented method (400) of any of claims 1-8, wherein the operations further comprise, when training the second traversal decoder (230) on the training examples (302):
minimizing the loss of attention of the constrained attention head; and
minimizing cross-entropy loss of the second traversal decoder (230).
10. The computer-implemented method (400) of any of claims 1-9, wherein each respective constraint alignment (330) includes a timing buffer with respect to the respective real value alignment (312, 314) that constrains each of the first constraint alignment (330) and the second constraint alignment (330) to a time interval that includes a first time period before the respective real value alignment (312, 314) and a second time period after the respective real value alignment (312, 314).
11. The computer-implemented method (400) of any of claims 1-10, wherein the operations further comprise, during execution of the two-pass neural network, using the second traversal decoder (230) trained on the training examples (302):
receiving audio data (202) of an utterance (12);
determining a time corresponding to a maximum probability at the constrained attention head of the second traversal decoder (230); and
generating a word start time or a word end time for the determined time corresponding to the maximum probability at the constrained attention head of the second traversal decoder (230).
12. A system (500) comprising:
data processing hardware (112); and
memory hardware (114) in communication with the data processing hardware (112), the memory hardware (114) storing instructions that, when executed on the data processing hardware (112), cause the data processing hardware (112) to perform operations comprising:
receiving a training example (302) of a second traversal decoder (230) of a two-pass neural network model, the training example (302) containing audio data (202) of a spoken utterance (12) representing one or more words (310) and a corresponding real-valued transcription (204) of the spoken utterance (12);
for each word (310) of the spoken utterance (12):
inserting placeholder symbols before the corresponding word (310);
identifying a respective real-value alignment (312, 314) of a beginning of the respective word (310) and an end of the respective word (310);
determining a beginning word piece (320) of the respective word (310) and an ending word piece (320) of the respective word (310); and
generating a first constraint alignment (330) of the beginning word piece (320) of the respective word (310) and a second constraint alignment (330) of the ending word piece (320) of the respective word (310), the first constraint alignment (330) being aligned with the real value alignment (312, 314) of the beginning of the respective word (310), the second constraint alignment (330) being aligned with the real value alignment (312, 314) of the ending of the respective word (310); and
constraining an attention head of the LAS decoder (230) of the two-pass neural network model by applying the training example (302) to each word (310) of the training example (302), the training example (302) containing all of the first constraint alignment (330) and the second constraint alignment (330).
13. The system (500) of claim 12, wherein said beginning word-slice (320) and said ending word-slice (320) contain the same word-slice (320) of said respective word (310).
14. The system (500) according to claim 12 or 13, wherein the second traversal decoder (230) includes a plurality of heads of attention.
15. The system (500) according to any one of claims 12-14, wherein constraining the attention head includes: constraining attention probabilities derived from the attention head of the second traversal decoder (230).
16. The system (500) according to any one of claims 12-15, wherein the operations further include, when training the second traversal decoder (230) on the training examples (302):
identifying an expected attention probability for the training example (302) of a portion;
determining that the constrained attention head produces an attention probability for at least one of the portions of the training examples (302) that fail to match the expected attention probability; and
applying a training penalty to the constrained attention head.
17. The system (500) of claim 16, wherein the attention probability of the at least one of the portions of the training examples (302) occurs at a time corresponding to the first constraint alignment (330) or the second constraint alignment (330).
18. The system (500) of claim 16, wherein the attention probability of the at least one of the portions of the training examples (302) occurs at a time that corresponds to neither the first constraint alignment (330) nor the second constraint alignment (330).
19. The system (500) according to any one of claims 12-18, wherein the operations further include, when training the second traversal decoder (230) on the training examples (302):
determining that the constrained attention head produces a non-zero probability of attention outside a boundary corresponding to the first constrained alignment (330) and the second constrained alignment (330); and
applying a training penalty to the constrained attention head.
20. The system (500) according to any one of claims 12-19, wherein the operations further include, when training the second traversal decoder (230) on the training examples (302):
minimizing the loss of attention of the constrained attention head; and
minimizing cross-entropy loss of the second traversal decoder (230).
21. The system (500) according to any one of claims 12 to 20, wherein each respective constraint alignment (330) includes a timing buffer with respect to the respective real value alignment (312, 314), the timing buffer constraining each of the first constraint alignment (330) and the second constraint alignment (330) to a time interval including a first time period before the respective real value alignment (312, 314) and a second time period after the respective real value alignment (312, 314).
22. The system (500) according to any one of claims 12-21, wherein the operations further include, during execution of the two-pass neural network, using the second traversal decoder (230) trained on the training instance (302):
receiving audio data (202) of an utterance (12);
determining a time corresponding to a maximum probability at the constrained attention head of the second traversal decoder (230); and
generating a word start time or a word end time of the determined time corresponding to a maximum probability at the constrained attention head of the second traversal decoder (230).
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202063021660P | 2020-05-07 | 2020-05-07 | |
US63/021,660 | 2020-05-07 | ||
PCT/US2021/022851 WO2021225699A1 (en) | 2020-05-07 | 2021-03-17 | Emitting word timings with end-to-end models |
Publications (1)
Publication Number | Publication Date |
---|---|
CN115485767A true CN115485767A (en) | 2022-12-16 |
Family
ID=75439561
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202180032576.4A Pending CN115485767A (en) | 2020-05-07 | 2021-03-17 | Issuing word timing using end-to-end model |
Country Status (6)
Country | Link |
---|---|
US (2) | US11580956B2 (en) |
EP (2) | EP4128219B1 (en) |
JP (2) | JP7286888B2 (en) |
KR (1) | KR20230006010A (en) |
CN (1) | CN115485767A (en) |
WO (1) | WO2021225699A1 (en) |
Family Cites Families (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11195057B2 (en) * | 2014-03-18 | 2021-12-07 | Z Advanced Computing, Inc. | System and method for extremely efficient image and pattern recognition and artificial intelligence platform |
US11321609B2 (en) * | 2016-10-19 | 2022-05-03 | Samsung Electronics Co., Ltd | Method and apparatus for neural network quantization |
US20180330718A1 (en) | 2017-05-11 | 2018-11-15 | Mitsubishi Electric Research Laboratories, Inc. | System and Method for End-to-End speech recognition |
US10706840B2 (en) * | 2017-08-18 | 2020-07-07 | Google Llc | Encoder-decoder models for sequence to sequence mapping |
AU2020288565B2 (en) | 2019-06-04 | 2023-02-16 | Google Llc | Two-pass end to end speech recognition |
WO2020256749A1 (en) * | 2019-06-20 | 2020-12-24 | Google Llc | Word lattice augmentation for automatic speech recognition |
-
2021
- 2021-03-17 US US17/204,852 patent/US11580956B2/en active Active
- 2021-03-17 KR KR1020227042620A patent/KR20230006010A/en active Search and Examination
- 2021-03-17 CN CN202180032576.4A patent/CN115485767A/en active Pending
- 2021-03-17 EP EP21717715.3A patent/EP4128219B1/en active Active
- 2021-03-17 JP JP2022567384A patent/JP7286888B2/en active Active
- 2021-03-17 EP EP23213490.8A patent/EP4307299A3/en active Pending
- 2021-03-17 WO PCT/US2021/022851 patent/WO2021225699A1/en unknown
-
2023
- 2023-02-09 US US18/167,050 patent/US20230206907A1/en active Pending
- 2023-05-23 JP JP2023084811A patent/JP2023109914A/en active Pending
Also Published As
Publication number | Publication date |
---|---|
US20230206907A1 (en) | 2023-06-29 |
EP4307299A3 (en) | 2024-05-01 |
US20210350794A1 (en) | 2021-11-11 |
US11580956B2 (en) | 2023-02-14 |
WO2021225699A1 (en) | 2021-11-11 |
KR20230006010A (en) | 2023-01-10 |
EP4307299A2 (en) | 2024-01-17 |
JP7286888B2 (en) | 2023-06-05 |
EP4128219B1 (en) | 2024-01-10 |
EP4128219A1 (en) | 2023-02-08 |
JP2023109914A (en) | 2023-08-08 |
JP2023521248A (en) | 2023-05-23 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11908461B2 (en) | Deliberation model-based two-pass end-to-end speech recognition | |
JP7436760B1 (en) | Learning word-level confidence for subword end-to-end automatic speech recognition | |
US11749259B2 (en) | Proper noun recognition in end-to-end speech recognition | |
US20230186901A1 (en) | Attention-Based Joint Acoustic and Text On-Device End-to-End Model | |
JP2024511176A (en) | Multitask learning for end-to-end automatic speech recognition confidence and deletion estimation | |
EP4128219B1 (en) | Emitting word timings with end-to-end models | |
US20230298563A1 (en) | Deliberation by Text-Only and Semi-Supervised Training | |
JP2024515713A (en) | Improving Streaming Automatic Speech Recognition by Non-Streaming Model Distillation | |
WO2024019859A1 (en) | Context-aware neural confidence estimation for rare word speech recognition |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |