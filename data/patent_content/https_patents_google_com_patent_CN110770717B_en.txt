CN110770717B - Automatic image sharing with designated users over a communication network - Google Patents
Automatic image sharing with designated users over a communication network Download PDFInfo
- Publication number
- CN110770717B CN110770717B CN201880012338.5A CN201880012338A CN110770717B CN 110770717 B CN110770717 B CN 110770717B CN 201880012338 A CN201880012338 A CN 201880012338A CN 110770717 B CN110770717 B CN 110770717B
- Authority
- CN
- China
- Prior art keywords
- image
- user
- images
- person
- criteria
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 238000004891 communication Methods 0.000 title claims abstract description 54
- 238000000034 method Methods 0.000 claims abstract description 146
- 238000004321 preservation Methods 0.000 claims description 50
- 230000001815 facial effect Effects 0.000 claims description 45
- 230000004044 response Effects 0.000 claims description 37
- 239000013598 vector Substances 0.000 claims description 18
- 238000004590 computer program Methods 0.000 claims description 7
- 230000000977 initiatory effect Effects 0.000 claims description 3
- 238000012549 training Methods 0.000 description 50
- 238000010801 machine learning Methods 0.000 description 46
- 238000012545 processing Methods 0.000 description 31
- 230000015654 memory Effects 0.000 description 23
- 230000000007 visual effect Effects 0.000 description 18
- 238000010586 diagram Methods 0.000 description 13
- 230000000694 effects Effects 0.000 description 13
- 230000008569 process Effects 0.000 description 11
- 230000009471 action Effects 0.000 description 10
- 241001465754 Metazoa Species 0.000 description 9
- 238000013528 artificial neural network Methods 0.000 description 9
- 230000006870 function Effects 0.000 description 8
- 238000004458 analytical method Methods 0.000 description 6
- 230000008901 benefit Effects 0.000 description 5
- 238000013475 authorization Methods 0.000 description 4
- 238000001914 filtration Methods 0.000 description 4
- 238000010191 image analysis Methods 0.000 description 4
- 230000006855 networking Effects 0.000 description 4
- 239000011521 glass Substances 0.000 description 3
- 239000011159 matrix material Substances 0.000 description 3
- 230000004048 modification Effects 0.000 description 3
- 238000012986 modification Methods 0.000 description 3
- 230000003287 optical effect Effects 0.000 description 3
- 230000008859 change Effects 0.000 description 2
- 238000012512 characterization method Methods 0.000 description 2
- 239000003086 colorant Substances 0.000 description 2
- 239000002131 composite material Substances 0.000 description 2
- 230000007423 decrease Effects 0.000 description 2
- 238000001514 detection method Methods 0.000 description 2
- 230000007613 environmental effect Effects 0.000 description 2
- 230000033001 locomotion Effects 0.000 description 2
- 230000001537 neural effect Effects 0.000 description 2
- 239000004065 semiconductor Substances 0.000 description 2
- 239000007787 solid Substances 0.000 description 2
- 108010006519 Molecular Chaperones Proteins 0.000 description 1
- 230000003213 activating effect Effects 0.000 description 1
- 230000004913 activation Effects 0.000 description 1
- 238000013459 approach Methods 0.000 description 1
- 230000003190 augmentative effect Effects 0.000 description 1
- 238000012790 confirmation Methods 0.000 description 1
- 238000013527 convolutional neural network Methods 0.000 description 1
- 238000012217 deletion Methods 0.000 description 1
- 230000037430 deletion Effects 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 230000008921 facial expression Effects 0.000 description 1
- 239000010931 gold Substances 0.000 description 1
- 229910052737 gold Inorganic materials 0.000 description 1
- 230000003993 interaction Effects 0.000 description 1
- 230000014759 maintenance of location Effects 0.000 description 1
- 238000005259 measurement Methods 0.000 description 1
- 238000000691 measurement method Methods 0.000 description 1
- 230000007246 mechanism Effects 0.000 description 1
- 238000003062 neural network model Methods 0.000 description 1
- 230000008520 organization Effects 0.000 description 1
- 238000005192 partition Methods 0.000 description 1
- 238000012797 qualification Methods 0.000 description 1
- 230000000717 retained effect Effects 0.000 description 1
- 230000006403 short-term memory Effects 0.000 description 1
- 230000036548 skin texture Effects 0.000 description 1
- 238000012706 support-vector machine Methods 0.000 description 1
- 230000001360 synchronised effect Effects 0.000 description 1
- 230000029305 taxis Effects 0.000 description 1
- 230000002123 temporal effect Effects 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 238000010200 validation analysis Methods 0.000 description 1
- XLYOFNOQVPJJNP-UHFFFAOYSA-N water Substances O XLYOFNOQVPJJNP-UHFFFAOYSA-N 0.000 description 1
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L67/00—Network arrangements or protocols for supporting network services or applications
- H04L67/01—Protocols
- H04L67/10—Protocols in which an application is distributed across nodes in the network
- H04L67/1097—Protocols in which an application is distributed across nodes in the network for distributed storage of data in networks, e.g. transport arrangements for network file system [NFS], storage area networks [SAN] or network attached storage [NAS]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F21/00—Security arrangements for protecting computers, components thereof, programs or data against unauthorised activity
- G06F21/30—Authentication, i.e. establishing the identity or authorisation of security principals
- G06F21/31—User authentication
- G06F21/32—User authentication using biometric data, e.g. fingerprints, iris scans or voiceprints
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/58—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/583—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F21/00—Security arrangements for protecting computers, components thereof, programs or data against unauthorised activity
- G06F21/60—Protecting data
- G06F21/62—Protecting access to data via a platform, e.g. using keys or access control rules
- G06F21/6218—Protecting access to data via a platform, e.g. using keys or access control rules to a system of files or objects, e.g. local or distributed file system or database
- G06F21/6245—Protecting personal data, e.g. for financial or medical purposes
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0484—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range
- G06F3/04842—Selection of displayed objects or displayed text elements
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06Q—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES; SYSTEMS OR METHODS SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES, NOT OTHERWISE PROVIDED FOR
- G06Q50/00—Systems or methods specially adapted for specific business sectors, e.g. utilities or tourism
- G06Q50/01—Social networking
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L67/00—Network arrangements or protocols for supporting network services or applications
- H04L67/01—Protocols
Abstract
Embodiments relate to automatic image sharing with a designated user over a communication network. In some implementations, a method includes causing an image to be displayed in a user interface on a first device, each of the images depicting a different person. An image is obtained from an image library associated with a first user. A selection of a particular one of the images is received based on user input and a person identifier indicative of a particular person depicted in the selected image is determined. The person identifier is specified as a person sharing criteria. A first image not included in the image is obtained and programmatically analyzed to determine that the first image depicts a person matching the person sharing criteria, and access permissions are updated to grant access rights to the first image to a second user of the second device over the communication network.
Description
Technical Field
The present disclosure relates to the field of communication technology, and in particular, to automatic image sharing with designated users over a communication network.
Cross Reference to Related Applications
The present application claims priority from U.S. provisional patent application No.62/507,756, entitled "AUTOMATIC image sharing with designated USERS (AUTOMATIC IMAGE SHARING WITH DESIGNATED USERS)" filed on 5/17 of 2017, the entire contents of which are incorporated herein by reference.
Background
The popularity and convenience of digital cameras and the widespread use of internet communications have led to user-generated images (user-generated images) such as photographs becoming ubiquitous. For example, users of internet platforms and services, such as email, forum, photo store, and web services, post images for viewing by themselves and others. The user may publish the image to a web service for private viewing (e.g., friends or other small group of users) and/or public viewing by many users.
The background description provided herein is for the purpose of generally presenting the context of the disclosure. Work of the presently named inventors, to the extent it is described in this background section, as well as aspects of the description that may not qualify as prior art at the time of filing, are neither expressly nor impliedly admitted as prior art against the present disclosure.
Disclosure of Invention
Embodiments of the present application relate to automatic image sharing with a designated user over a communications network. In some implementations, a computer-implemented method includes causing a plurality of images in a user interface on a first device to be displayed to a first user, wherein each of the plurality of images depicts a different person, wherein the plurality of images are obtained from a library of images associated with the first user. A selection of a particular image of the plurality of images is received based on user input received by the first device, and a person identifier is determined that indicates a particular person depicted in the selected particular image, wherein the person identifier is designated as a person sharing criteria. A first image associated with the first user is obtained from an image library associated with the first user, wherein the first image is not included in the plurality of images. The method programmatically analyzes the first image to determine that the first image (e.g., in pixels of the first image) depicts a person matching the person sharing criteria. Based on determining that the first image depicts a person matching the person sharing criteria, access permissions to the first image of an image library associated with the first user are updated to grant access rights to the first image to a second user of the second device over the communication network.
Various embodiments and examples of the method are described. For example, in some implementations, the person sharing criteria includes facial criteria that specify a particular facial identity, and programmatically analyzing the first image includes: an image classifier is applied to determine that the first image depicts a face of the particular facial identity, wherein the image classifier is applied based on pixel values of pixels of the first image. In some implementations, updating access permissions to the first image is based on determining a confidence level that the first image depicts a person matching the person sharing criteria, wherein the confidence level meets a first threshold. In some examples, the method includes checking whether the confidence level does not satisfy a first threshold and satisfies a second threshold that indicates that the first image depicts less confidence of a person matching the person sharing criteria, and in response to determining that the confidence level satisfies the second threshold, causing the first image to be displayed for selection from the first user to a user input of the first device to update access permissions to the first image to grant access rights to the second user.
In some implementations, updating the access permissions for the first image includes automatically adding the first image to an image library of the second user. In some implementations, adding the first image to the image gallery of the second user includes adding the first image to an image album in the image gallery of the second user, wherein the image album is designated to store images that match the people sharing criteria. In some embodiments, the image data of the first image is stored on a server device in communication with the first device over a communication network, and a first data pointer relating to the image data of the first image is stored on the first device, wherein updating the access permission of the first image includes transmitting a second data pointer relating to the image data of the first image to the second device over the communication network.
In some implementations, programmatically analyzing the first image includes determining whether the first image meets one or more additional sharing criteria. In some examples, the one or more additional sharing criteria include an image feature criterion that specifies an image content feature, and programmatically analyzing the first image includes applying an image classifier to determine a feature vector that includes a plurality of content features of the first image, wherein the feature vector is based on pixel values of the first image, and comparing the feature vector to the image feature criterion to determine whether the first image matches the image feature criterion. In a further example, the one or more additional sharing criteria include a time criterion, and programmatically analyzing the first image includes determining a creation time of the first image based on timestamp metadata associated with the first image, and comparing the creation time to the time criterion to determine whether the first image matches the time criterion. In a further example, the one or more additional sharing criteria include a location criteria, and the location criteria specify a particular location, and programmatically analyzing the first image includes determining a location associated with the first image based on one or more of: the location metadata associated with the first image and the detected content characteristics as depicted in one or more pixels of the first image, and the location associated with the first image is compared to a location criterion to determine whether the first image matches a particular location.
In some embodiments, the method further comprises: in response to determining that the first image depicts a person matching the person sharing criteria, a delay period associated with the first user for the first image is initiated, a status indicator is assigned to the first image, wherein the status indicator indicates a first status that the first image is to be shared with the second user, determining whether user input from the first user is received at the first device during the delay period, indicating that the first image is not to be shared with the second user, and if it is determined that user input is received, updating the status indicator to a second status indicating that the first image is not to be shared with the second user, wherein updating access permissions for the first image is responsive to expiration of the delay period and the status indicator indicating the first status.
In some implementations, a computer-implemented method includes selecting a plurality of images from a library of images associated with a user and causing the plurality of images to be displayed in a user interface on a user device of the user. The method includes receiving a selection of a particular image of the plurality of images based on user input received through the user device, and determining a person identifier indicative of a particular person depicted in the selected particular image. The person identifier is designated as a person preservation criteria and included in stored automatic preservation criteria associated with the user, wherein the automatic preservation criteria includes one or more person identifiers, each person identifier identifying a different person. The method includes receiving, over a communication network, an indication that a user has been granted access to a first image, the first image being associated with a different user and provided in a stored different image repository associated with the different user, wherein the first image is not included in the image repository associated with the user. The indication includes access permissions to the first image, wherein the indication is based on shared input received at different user devices of different users. In response to receiving an indication that the user has been granted access to the first image, the method obtains the first image and stored automatic save criteria associated with the user. The method programmatically analyzes the first image to determine whether the first image depicts a person matching one of the one or more person identifiers of the automatic save criteria (e.g., in a pixel of the first image). In response to determining that the first image depicts a person matching one of the one or more person identifiers, the first image is added to an image library associated with the user. For example, the first image may be for display on a user device of the user.
Various embodiments and examples of the method are described. For example, in some embodiments, each of the plurality of images depicts a unique person that is not depicted in other images of the plurality of images, wherein each image of the plurality of images is associated with a different person identifier. In some examples, each image of the plurality of images is selected from a different image cluster from a plurality of image clusters, wherein each image cluster includes one or more images depicting the unique person.
In some implementations, automatically adding the first image to the image library includes storing a data pointer on the user device, wherein the data pointer relates to the first image data stored on the server device and the first image data is associated with the first image. In some implementations, the automatic save criteria includes one or more additional save criteria, wherein the additional save criteria include a location save criteria specifying a location indicated by user input received at the user device and/or a time-based save criteria specifying a time period indicated by user input received at the user device, and the method further includes determining whether metadata of the first image meets the one or more additional save criteria, wherein the first image is automatically added to the image library in response to determining that the metadata of the first image meets the one or more additional save criteria.
In some implementations, the person preservation criteria include face preservation criteria that specify a particular facial identity, and programmatically analyzing the first image includes applying an image classifier to determine that the first image depicts a face of the particular facial identity. The image classifier is applied based on pixel values of pixels of the first image. In some implementations, the method further includes causing, by the user device, a display of a first user interface element, wherein the first user interface includes selectable options to accept access rights to one or more images associated with different users, and the method further includes receiving user input from the user indicating acceptance of access rights to the one or more images, wherein in response to receiving the user input indicating acceptance, a plurality of images are selected from the image library and caused to be displayed. In some examples, the method further comprises: in response to receiving the user input indicating acceptance, a second user interface is caused to be displayed that enables a user to update access permissions for one or more images in an image library associated with the user to grant access rights to the one or more images to a different user.
In some implementations, a computer program product includes portions of program code that, when executed by a computing device, cause the computing device to perform operations. The operations include causing a plurality of images in a user interface on a first device to be displayed to a first user, wherein each image of the plurality of images depicts a different person, and wherein the plurality of images is obtained from a library of images associated with the first user. A selection of a particular image of the plurality of images is received based on user input received by the first device. The operations include determining a person identifier indicative of a particular person depicted in the selected particular image, wherein the person identifier is designated as a person sharing criteria. The operations include obtaining a first image associated with a first user, wherein the first image is provided in an image repository associated with the first user and the first image is not included in the image repository. The operations include programmatically analyzing the first image to determine that the first image depicts, for example, in pixels of the first image, persons matching the person sharing criteria, and updating access permissions to the first image in an image library associated with the first user to grant access permissions to the first image to a second user of the second device over the communication network based on determining that the first image depicts persons matching the person sharing criteria.
In some implementations, a computer program product includes portions of program code that, when executed by a computing device, cause the computing device to perform operations. The operations include selecting a plurality of images from a library of images associated with a user. The operations include causing a plurality of images to be displayed in a user interface on a user device of a user. The operations include receiving a selection of a particular image of the plurality of images based on user input received through the user device. The operations include determining a person identifier indicative of a particular person depicted in the selected particular image, wherein the person identifier is designated as a person preservation criteria and included in stored automatic preservation criteria associated with the user, wherein the automatic preservation criteria includes one or more person identifiers, each person identifier identifying a different person. The operations include receiving, over a communication network, an indication that a user has been granted access to a first image, the first image being associated with a different user and provided in a stored different image repository associated with the different user, wherein the first image is not included in the image repository associated with the user, wherein the indication includes access permissions to the first image. The operations include obtaining the first image and stored automatic save criteria associated with the user in response to receiving an indication that the user has been granted access to the first image. The operations include programmatically analyzing the first image to determine if the first image depicts a person matching one of the one or more person identifiers of the automated preservation criteria, and adding the first image to an image library associated with the user in response to determining that the first image depicts a person matching one of the one or more person identifiers.
In some implementations, a system includes a storage device and at least one processor configured to access the storage device and to perform operations. The operations include causing a plurality of images in a user interface on a first device to be displayed to a first user, wherein each image of the plurality of images depicts a different person, and wherein the plurality of images is obtained from a library of images associated with the first user. A selection of a particular image of the plurality of images is received based on user input received by the first device. The operations include determining a person identifier indicative of a particular person depicted in the selected particular image, wherein the person identifier is designated as a person sharing criteria. The operations include obtaining a first image associated with a first user, wherein the first image is provided in an image repository associated with the first user and the first image is not included in the image repository. The operations include programmatically analyzing the first image to determine that the first image depicts, for example, in pixels of the first image, a person that matches the person sharing criteria, and based on determining that the first image depicts a person that matches the person sharing criteria, updating access permissions to the first image in an image library associated with the first user to grant access rights to the first image to a second user of the second device over the communication network. For example, the operations include causing a first image to be added to an image album in an image gallery of a second user, wherein the image album is designated to store images matching a person sharing criteria.
In some implementations, a system includes a storage device and at least one processor configured to access the storage device and to perform operations. The operations include selecting a plurality of images from a library of images associated with a user. The operations include causing a plurality of images to be displayed in a user interface on a user device of a user. The operations include receiving a selection of a particular image of the plurality of images based on user input received through the user device. The operations include determining a person identifier indicative of a particular person depicted in the selected particular image, wherein the person identifier is designated as a person preservation criteria and included in stored automatic preservation criteria associated with the user, wherein the automatic preservation criteria includes one or more person identifiers, each person identifier identifying a different person. The operations include receiving, over a communication network, an indication that a user has been granted access to a first image, the first image being associated with a different user and provided in a stored different image repository associated with the different user, wherein the first image is not included in the image repository associated with the user, wherein the indication includes access permissions to the first image. The operations include obtaining the first image and stored automatic save criteria associated with the user in response to receiving an indication that the user has been granted access to the first image. The operations include programmatically analyzing the first image to determine if the first image depicts a person matching one of the one or more person identifiers of the automated preservation criteria, and adding the first image to an image library associated with the user in response to determining that the first image depicts a person matching one of the one or more person identifiers.
Drawings
FIG. 1 is a block diagram of an example network environment that may be used for one or more embodiments described herein;
FIG. 2 is a flow chart illustrating an example of a method of providing features that share images from a sending user, according to some embodiments;
FIG. 3 is a flow chart illustrating an example of a method provided to receive a feature of a user shared image, according to some embodiments;
FIG. 4 is a schematic diagram of an example user interface, according to some embodiments;
FIG. 5 is a schematic diagram of another example user interface according to some embodiments;
FIG. 6 is a schematic diagram of another example user interface according to some embodiments;
FIG. 7A is a schematic diagram of another example user interface, according to some embodiments;
FIG. 7B is a schematic diagram of another example user interface, according to some embodiments;
FIG. 7C is a schematic diagram of another example user interface, according to some embodiments; and
FIG. 8 is a block diagram of an example device that may be used in one or more embodiments described herein.
Detailed Description
Embodiments of the present application relate to automatic image sharing with a designated user using devices that communicate over a network. In some embodiments, if allowed by the sending user (sending user), the images stored in and added to the sending user's image library are programmatically analyzed. If it is determined that the image matches the one or more sharing criteria specified by the sending user, then access to the image is automatically granted to the receiving user (RECIPIENT USER) specified by the sending user. In some implementations, the receiving user may indicate a preference to automatically save certain of the shared images to which they have been granted access when they meet the automatic save criteria. In these embodiments, images meeting the automatic save criteria are automatically added to the recipient's image library without user input.
The described features include receiving a selection of a sharing criteria image to specify sharing criteria based on user input from a sending (sharing) user. The shared standard image may be obtained and processed from the image library of the sending user. For example, a person identifier may be determined in the selected standard image and used as a person sharing standard. Images in the user image library are analyzed based on the sharing criteria and designated as shared images based on a match with the sharing criteria. The sharing criteria may include various image characteristics including image content characteristics, capture time, capture location, etc.
At the receiving user device, save criteria images may be displayed and based on user input, a selection from among the save criteria images may specify an automatic save criteria. The save standard image may be obtained and/or processed from a receiving user's image library. The shared images from the sending user may be compared to automatic save criteria, such as personnel save criteria, to determine which shared images are to be automatically stored in the receiving user's image library. The automatic save criteria may include one or more image characteristics including image content characteristics, capture time, capture location, etc.
The techniques described herein may automatically grant recipients access to images from a sending user and may automatically save images that meet user criteria. Thus, the technique reduces or eliminates the need for a user to take specific actions with the device to share images and provides the convenience of automatically building an image library associated with the user. In one example, the described sharing techniques alleviate the burden of users to continually manually share a large number of photos with family members, and reduce the frustration that users cannot continually access all of their family's photos captured by family members.
Automatically sharing images continuously with one or more designated people provides technical advantages. For example, the captured image may be immediately shared with designated people without additional operations and actions on the device and without the need to process, store, and display other applications (e.g., separate shared user interfaces, messaging, email, posting to a social network, etc.). Such shared images may be displayed with lower latency than if a separate interface or application for sharing were required to be used, by eliminating the time required to provide a separate interface or application to share the images. A further technical advantage is that the processing costs and device resources required for sharing images using separate interfaces or applications are reduced. For example, applying predefined sharing criteria and save criteria to automatic sharing of images saves system processing resources that are typically used for display interfaces and data for manual browsing and selection of images to be shared and saved to a particular store. The use of various sharing criteria and preservation criteria allows for sharing and/or preservation of related images in an image library without requiring additional processing and storage resources for manual browsing, selection, deletion, and validation of images to be shared and/or stored. This is useful, for example, for users who wish to share photos but have a large number of photos to sort or users who take a large number of photos. A recipient user of the shared image may be provided with a save criteria option that allows related images to be automatically stored in their image library (and a notification may be provided periodically on the recipient device to indicate that the newly shared image is available for viewing). It is thus possible to enable users to share more images with each other on the basis of having a continuous strong user interest, thereby establishing a stronger sharing and collaboration network between users.
By reducing the display interface and user input for a user to view, select, and send images using a large number of device options, device memory usage and processing resource usage may be reduced. Furthermore, the described features may reduce processing and communication resources used by a device receiving a user by reducing display interfaces and user inputs for viewing, planning, and selecting shared images to store. The technical effect of one or more of the described embodiments is therefore that the operation of the device is reduced in terms of computational time and resources expended to obtain results, thereby providing a technical solution to the technical problem of requiring significant device resources to generate, transmit and view images shared between multiple devices and users over a communication network.
Herein, features related to an image are described. Other types of content may also be used with the features described herein. For example, a video (e.g., a movie) having a plurality of image frames may be included in the term "image" and analyzed for sharing and/or storage, e.g., as described herein. Similarly, content types such as documents, audio clips and files, games, and other media types may be similarly used with the described features.
In the case where certain embodiments discussed herein may collect or use personal information about a user (e.g., user data, information about a user's social network, a user's location and time at the location, biometric information of the user, activities and demographic information of the user), the user is provided with one or more opportunities to control whether information is collected, whether personal information is stored, whether personal information is used, and how information about the user is collected, stored, and used. That is, the systems and methods discussed herein collect, store, and/or use user personal information, particularly upon receipt of explicit authorization from an associated user. For example, a user is provided with control over whether a program or feature gathers user information about that particular user or other users associated with the program or feature. Each user whose personal information is to be collected is presented with one or more options to allow control of the collection of information related to that user to provide permissions or authorizations as to whether information is to be collected and as to which portions of the information are to be collected. For example, one or more such control options may be provided to the user over a communications network. In addition, certain data may be processed in one or more ways prior to storage or use to remove personally identifiable information. As one example, the identity of the user may be processed such that personally identifiable information cannot be determined. As another example, the geographic location of a user may be generalized to a larger region such that the particular location of the user cannot be determined.
Fig. 1 illustrates a block diagram of an example network environment 100, which may be used in some embodiments described herein. In some implementations, the network environment 100 includes one or more server systems, such as the server system 102 in the example of fig. 1. Server system 102 may be in communication with network 130, for example. The server system 102 may include a server device 104 and a database 106 or other storage device. In some implementations, the server device 104 can provide the image classifier 156b and/or the sharing application 158b.
Network environment 100 may also include one or more client devices, such as client devices 120, 122, 124, and 126, that may communicate with each other and/or server system 102 via network 130. The network 130 may be any type of communication network including one or more of the internet, a local area network (Local Area Networks, LAN), a wireless network, a switch or hub connection, etc. In some implementations, the network 130 can include peer-to-peer communication between devices, e.g., using a peer-to-peer wireless protocol (e.g., bluetooth Wi-Fi direct, etc.), etc. Arrow 132 illustrates one example of peer-to-peer communication between two client devices 120 and 122.
For ease of illustration, FIG. 1 shows one block of server system 102, server device 104, database 106, and four blocks for client devices 120, 122, 124, and 126. Server boxes 102, 104, and 106 may represent a number of systems, server devices, and network databases, and the boxes may be provided in configurations other than those shown. For example, server system 102 may represent multiple server systems that may communicate with other server systems via network 130. In some implementations, for example, server system 102 can include a cloud hosting server. In some examples, database 106 and/or other storage devices may be disposed in a server system block separate from server device 104 and may communicate with server device 104 and other server systems via network 130. Further, there may be any number of client devices. Each client device may be any type of electronic device, such as a desktop computer, laptop computer, portable or mobile device, cell phone, smart phone, tablet, television set-top box or entertainment device, wearable device (e.g., display glasses or goggles, watch, headset, arm band, jewelry, etc.), personal digital assistant (Personal DIGITAL ASSISTANT, PDA), media player, gaming device, etc. Some client devices may also have a local database similar to database 106 or other storage. In some implementations, network environment 100 may not have all of the components shown and/or may have other elements including other types of elements in place of or in addition to those described herein.
In various implementations, end users U1, U2, U3, and U4 may communicate with server system 102 and/or each other using respective client devices 120, 122, 124, and 126. In some examples, users U1, U2, U3, and U4 may interact with each other via applications running on respective client devices and/or server systems 102, and/or via network services implemented on server systems 102, such as social networking services or other types of network services. For example, each client device 120, 122, 124, and 126 may transmit data to and from one or more server systems, such as server system 102. In some implementations, the server system 102 can provide appropriate data to the client devices such that each client device can receive the transmitted content or shared content uploaded to the server system 102 and/or the network service.
In some examples, users U1-U4 may interact via audio or video conferences, audio, video or text chat, or other communication modes or applications. Web services implemented by server system 102 may include systems that allow users to communicate, form links and associations, upload and publish shared content, such as images, text, video, audio, and other types of content, and/or perform other functions. For example, the client device may display received data, such as content posts (content posts) sent or streamed to the client device via a server and/or web service (or directly from a different client device) and originating from a different client device, or from a different server system and/or web service. In some implementations, the client devices may communicate directly with each other, e.g., using peer-to-peer communication between the client devices as described above. In some embodiments, a "user" may include one or more programs or virtual entities, as well as people engaged with a system or network.
In some implementations, any of the client devices 120, 122, 124, and/or 126 can provide one or more applications. For example, as shown in fig. 1, the client device 120 may provide a camera application 152, an image classifier 156a, a sharing application 158a, and one or more other applications 154. Client devices 122-126 may also provide similar applications. For example, the camera application 152 may provide the user of the respective client device (e.g., users U1-U4) with the ability to activate and utilize a camera (not shown) of the client device 122 to capture images and/or video. For example, the camera application 152 may be a software application running on the client device 120.
In some implementations, the camera application 152 can provide a camera user interface. In some implementations, for example, if the client device 120 has multiple cameras, such as a front-facing camera and a rear-facing camera, the camera user interface of the camera application 152 may provide the user UI with an option to select a particular camera on the client device 120. Further, the camera user interface of the camera application 152 may provide the user UI with the ability to control one or more settings of the selected camera, such as aperture, shutter speed, zoom level, and the like. The camera user interface of the camera application 152 may also provide the user UI with the ability to control modes of operation, such as slow motion mode, single image capture mode, video capture mode, and the like. In some implementations, the camera application 152 can provide the user with options related to image settings (e.g., image resolution, image size, image orientation, image format such as raw, JPEG, etc.) and/or image effects (e.g., lens blur effects, panoramic effects, etc.). In some implementations, the camera application 152 may provide access to settings, modes of operation, image processing effects, etc., via, for example, a user interface displayed on the client device 120. Images and/or video captured by the cameras of the client device 120 may be stored, for example, in a local store of the client device 120 and/or in a store provided by the server system 102.
In some implementations, the client device 120 can include an image classifier 156a. The image classifier 156a may be implemented using hardware and/or software of the client device 120, as described with reference to fig. 8. In various embodiments, image classifier 156a may be a stand-alone image classifier running on any one of client devices 120-124, for example, or may work with image classifier 156b provided on server system 102. Image classifier 156a and image classifier 156b may provide image analysis functionality based on user permissions, for example, assigning images to one or more clusters based on analyzing the images to identify one or more faces depicted in the images. In some examples, image analysis and image feature recognition may be performed using any of a variety of techniques, including machine learning techniques as described below.
In some implementations, the image classifier 156a on the client device 120 may be used instead of the image classifier 156b on the server. In some implementations, server storage of the facial recognition data of U1 (e.g., facial models as described herein) may be omitted, and the facial recognition data is stored locally on the device 120 of the client under the supervision and control of the user of the client device. In some implementations, the client device 120 can communicate with the server 102 to send the selected shared image directly to the receiving user (e.g., via the receiving user server account and/or the receiving client device). Accordingly, in some implementations, the image classifier 156a and facial recognition data may be provided on the recipient's client device to determine the automatic save criteria and select a shared image for storage in the recipient user's image library without using the image classifier 156b and facial recognition data on the server.
In some implementations, the client device 120 can also include a sharing application 158a. The sharing application 158b may be implemented using hardware and/or software of the client device 120. In various embodiments, shared application 158a may be a stand-alone application running on any of client devices 120-124, for example, or may work with shared application 158b disposed on server system 102. The sharing application 158a and the sharing application 158b may enable a user (e.g., a sending user) to grant access rights to images in a user image library to one or more other people. The sharing application 158a and the sharing application 158b may further enable a user (e.g., recipient) to set an automatic save function to save one or more images that other users have granted access to in the recipient's image library. The images granted access rights may include, for example, images captured by the client device 120, images stored on the client device 120, images accessed by the client device 120, for example, through the network 130, etc.
In some implementations, the client device 120 may include one or more other applications 154. For example, other applications 154 may be applications that provide various types of functionality, such as calendars, address books, email, web browsers, shopping, traffic (e.g., taxis, trains, airline reservations, etc.), entertainment (e.g., music players, video players, gaming applications, etc.), social networks (e.g., messaging or chat, audio/video calls, shared images/video, etc.), and so forth. In some implementations, one or more of the other applications 154 may be stand-alone applications running on the client device 120. In some implementations, one or more of the other applications 154 may access a server system that provides data and/or functionality of the other applications 154.
The user interfaces on client devices 120, 122, 124, and/or 126 can display user content and other content, including images, video, data, and other content, as well as communications, privacy settings, notifications, and other data. Such a user interface may be displayed using software on a client device, software on a server device, and/or a combination of server software and client software running on server device 104 (e.g., application software or client software in communication with server system 102). The user interface may be displayed by a display device (e.g., a touch screen or other display screen, projector, etc.) of the client device or the server device. In some implementations, an application running on a server system may communicate with a client device to receive user input at the client device and output data such as visual data, audio data, and the like at the client device. In some implementations, the server system 102 and/or any of the one or more client devices 120-126 can provide a communication application. The communication program may allow a system (e.g., a client device or server system) to provide options for communicating with other devices. The communication program may provide one or more associated user interfaces that are displayed on a display device associated with the server system or the client device. The user interface may provide the user with various options to select a communication mode, a user or device with which to communicate, and the like. In some examples, the communication program may provide an option to broadcast the content post to the broadcast area, and/or may output a notification indicating that the device has received the content post and that the device is in the defined broadcast area for posting. The communication program may, for example, display or otherwise output the transmitted content posts and the received content posts in any of a variety of formats.
Other implementations of the features described herein may use any type of system and/or service. For example, other network services (e.g., connected to the Internet) may be used in place of or in addition to the social network service. Any type of electronic device may utilize the features described herein. Some implementations may provide one or more features described herein on one or more client or server devices that are disconnected or intermittently connected to a computer network. In some examples, a client device that includes or is connected to a display device may display content posts stored on a storage device local to the client device, e.g., content posts previously received over a communication network.
Fig. 2 is a flow chart illustrating an example of a method 200 of sharing images from a sending user, according to some embodiments. In some implementations, the method 200 may be implemented, for example, on the server system 102 as shown in fig. 1. In some implementations, some or all of the method 200 may be implemented on one or more client devices 120, 122, 124, or 126, one or more server devices, and/or both server devices and client devices as shown in fig. 1. In the depicted example, the implementation system includes one or more digital processors or processing circuits ("processors") and one or more storage devices (e.g., database 106 or other storage). In some implementations, different components of one or more servers and/or clients may perform different blocks or other portions of method 200. In some examples, a first device is described as performing the blocks of method 200. Some implementations may have one or more blocks of method 200 performed by one or more other devices (e.g., other client devices or server devices) that may send results or data to the first device. The various examples and embodiments described with respect to method 200 may be combined in various permutations or combinations.
In some embodiments, the method 200 or portions of the method may be initiated automatically by the system. In some embodiments, the implementation system is a first device. For example, the method (or portions thereof) may be performed periodically or based on one or more particular events or conditions, such as launching an application by a user, receiving one or more images that have been newly uploaded to or accessible by the system, having elapsed a predetermined period of time since the last execution of method 200, and/or one or more other conditions that may be specified in settings read by the method. In some implementations, such conditions can be specified by the user in stored user-defined preferences of the user.
In one example, the first device may be a camera, a cell phone, a smartphone, a tablet, a wearable device, or other client device that may receive user content input (e.g., image capture) to the client device and may perform method 200. In another example, a client device or server device may receive one or more images uploaded from one or more users or received over a network connection and may perform the method 200 for the one or more images. In another example, a client device may send an image to a server over a network, and the server may process content using the method 200. Some embodiments may initiate the method 200 based on user input. For example, a user (e.g., an operator or end user) may have selected the initiation of method 200 from a displayed user interface (e.g., an application user interface or other user interface).
An image referred to herein may comprise a digital image having pixels with one or more pixel values (e.g., color values, luminance values, etc.). The images may be still images (e.g., still photographs, images with a single frame, etc.), moving images (e.g., animations, animated GIFs, movie images in which a portion of the image includes motion and other portions are still, etc.), and video (e.g., images or sequences of image frames that may include audio). Although the remainder of this document refers to images as still images, it is understood that the techniques described herein are applicable to dynamic images, video, and the like. For example, embodiments described herein may be used with still images (e.g., photographs, emoticons, or other images), video, or dynamic images. As referred to herein, text may include alphanumeric characters, emoticons, symbols, or other characters.
In block 202, it is checked whether user consent (e.g., user permissions) has been obtained to use the user data in an embodiment of the method 200. For example, the user data may include messages sent or received by the user, user preferences, user biometric information, user characteristics (identification, name, age, gender, occupation, etc.), information about the user's social network and contacts, social and other types of actions and activities, content, ratings, and opinions created or submitted by the user, the user's current location, historical user data, images generated, received and/or accessed by the user, videos viewed or shared by the user, and so forth. In some implementations, one or more blocks of the methods described herein may use such user data.
If user consent has been obtained from the relevant user whose user data was available in method 200, then in block 204, the blocks of the method herein are determined to be applicable where possible using the user data described for those blocks, and the method continues to block 208. If user consent has not been obtained, then in block 206 it is determined that the block will be implemented without using the user data, and the method continues to block 208. In some implementations, if user consent is not obtained, the block is implemented without using user data and/or data that is generic or publicly accessible and publicly available.
In block 208, user input is received at the first device from a first user (e.g., a sending user) to establish sharing of an image (and/or other content item or media item) with a particular user (also referred to as a second user, recipient, or receiving user). For example, the particular user may be a partner, spouse, designated friend, etc. of the first user. For example, user input may be received via a user interface displayed on the first device, and/or user input may be input via an input device of the first device, such as a touch screen, physical controls (e.g., pointing device, joystick, touchpad, etc.), voice commands received by a microphone of the first device, and so forth. In some implementations, the shared settings interface is displayed in response to user input. In some implementations, the shared settings interface can be automatically displayed by the first device in response to the obtained image, e.g., based on user preferences, etc.
In block 210, a selection of an identification of a second user (e.g., a second user different from the first user) that is a receiving user (or recipient) of the shared image is received from the first user. In some implementations, if user consent has been obtained from the first user to access the first user's social graph (e.g., social network, messaging network, email network, etc.), various users in social connection with the first user may be presented for selection as receiving users. For example, a menu of contacts stored in the address book of the first user may be presented from which the first user may input a selection. In another example, users from the first user of one or more user groups organized and stored on the social networking service, users connected via the first device as indicated in the stored call log, and so forth may be presented for selection. In some implementations, a single user is selected as the recipient of the shared image. In some implementations, multiple users may be selected as recipients of a shared image. In some implementations, multiple users may not be selected as recipients of an automatically shared image as described herein, and only a single user may be selected as a recipient.
In block 212, a selection of settings for sharing an image, including sharing criteria, is received from a first user. For example, the sharing criteria may specify one or more criteria characteristics compared to image characteristics of the stored image, e.g., image characteristics such as depicted image features. If the standard characteristic matches the image characteristic of the image, the image is determined to be a matching share. Based on specified sharing criteria, images from the image library of the first user are identified as images to be provided to the second user (receiving user). The sharing criteria may include time criteria, image content feature criteria (e.g., criteria based on image content or image features such as faces), location criteria, and the like. The sharing criteria may be specified by the first user, e.g., via user input to the first device, via stored user preferences, etc.
In some implementations, the time criteria may include one or more specified times, dates, or time/date ranges for which the time stamp of the image will match (where "time" may include a date) in order for the image to qualify as a shared image for the first user. For example, the image timestamp may indicate the time at which the image was captured by the camera or other device (or the time at which the generated image was created). In some implementations, the image timestamp can indicate a time of last modification of the image by the first user (or any user), an upload time of the image received via an upload from the client device to the server over the network, a time of first access to the image by the first user, and so forth.
The time criteria may specify various time periods (e.g., time or time ranges). The time standard may specify one or more particular times, or time ranges of times or dates. For example, the time criterion may specify a particular time (e.g., a start time or a start date) and specify that an image having a timestamp after the particular time is eligible as a shared image. The particular time may be specified as a time of day, a particular day or date, an event (e.g., a standard holiday, a sports game, a celebration event, a date when the sending user first encountered the recipient, etc.). In a further example, the time criterion may specify a time range (e.g., between a start time/date and an end time/date), e.g., which may coincide with a particular event (vacation, meeting, etc.). In some examples, when the sending user (first user) allows access to the user data, the date that the sending user first encountered the recipient may be determined based on such data, e.g., a stored calendar of the first user indicating past events and future scheduled events. In another example, the sending user may enter a date of the first meeting with the recipient. In various examples, the time criteria may specify that an image having a time stamp prior to a particular time (e.g., end time or end date) is eligible as a shared image, or that an image having a time stamp within a specified period of time is eligible as a shared image.
In some implementations, the particular time may be determined based on particular conditions of the first user, other users, images, etc. For example, if user consent has been obtained, the time criteria may specify a period of time until now after the sending user first contacted the receiving user, for example, using the first device (e.g., if user consent has been obtained, as indicated in a contact list, call log, message log, or other database indicating a history of previous communications of the first device). An image with a timestamp within the time period qualifies as a shared image.
The image content feature criteria may specify a particular image content feature or type of image content feature visually depicted in a pixel of the image. Image content features may include faces, people (e.g., whole or parts of a person's body), animals (e.g., identifiable animals such as pets if user consent has been obtained), historical trails (e.g., eiffel towers, gold bridges, etc.), landscape features (e.g., trees, lakes, bridges, mountains, buildings, sky, sunset, etc.), physical objects (e.g., vehicles, items, etc.), logos, objects indicating utility images, etc. Specific image content features may be specified in the image content feature standard by identifiers (e.g., person name, eiffel tower, specific location name, pet name, etc.), and the types of image content features may be specified in the standard by category or classification (e.g., dog, baby, truck, ball, etc.). Image content features may be detected in an image using one or more image detection and/or image recognition techniques.
In some implementations, machine learning techniques may be used to implement an image classifier to classify features such as regions and objects in an image based on pixel values of the image. For example, a feature vector comprising a plurality of content features may be determined and the feature vector may be compared to an image content feature criterion to determine whether the image matches the image content feature criterion. The feature vector may be a condensed numerical representation of the visual pixel content of the image. For example, a feature vector may be a vector having a particular number of dimensions, each dimension having a value. In some embodiments, 128 or other number of dimensions may be used. In some implementations, the feature vector can be generated by the neural network based on image pixel values (e.g., color values). In some implementations, one or more feature vectors may be determined from particular portions of an image, such as "local" image content features detected based on image detection or object recognition techniques (e.g., pattern matching, machine learning, etc.).
For example, image content features including faces (no identification), animals, objects, landscape features (leaves, buildings, sky, sunset, etc.) may be detected. In some implementations, image features of the "utility image" can be detected, such as text indicating a depiction of a receipt in the image, a shape of a screen capture image indicating a video screen (e.g., a user interface displayed in a captured state), and so forth. In some implementations, utility images depicting these features may be excluded from sharing. In a further example, if a subject of an image (e.g., a maximum feature detected in the image, a feature detected at the center of an image frame or other particular location, etc.) is determined, a feature vector may be created from a portion of the image depicting the subject. In some implementations, the feature vector of the subject of the image may be considered to represent the entire image.
In some implementations, the sharing criteria may be specified as one or more object models, such as models of faces or other objects. The face model indicates characteristics of the face, such as the size, color, etc., of the face and face features associated with a particular person. This allows the method to detect various faces of different images, gestures, angles, etc., and if these face models are contained in the sharing criteria, match these faces with the sharing criteria face models to indicate that the depicted faces belong to a particular person (e.g., person identifier) associated with the face models and are therefore to be shared. If the face model is designated as the sharing criteria, the system may determine the face in the image that matches the face model. The method 200 may access a number of facial models, for example, facial models stored locally on a first device of a first user.
In some implementations, the first device may store a facial model generated based on images stored in an image library of the first user. In some implementations, the facial model may additionally or alternatively be stored on a network storage and/or a server accessible to the first device over a network, and such facial model may also be used as an image standard. In some implementations, the facial model is retained only in local storage of the user device and is not transmitted to the server over the network. In some implementations, face model clusters are provided, where each cluster represents a particular person (e.g., each cluster has an associated person identifier) and includes a plurality of face models that have been determined or validated to represent the particular person associated with the cluster. In some implementations, techniques including vector analysis (e.g., capturing a change in dark to light pixel values, comparing feature vectors of images to find similar image features) and/or skin texture analysis (e.g., surface texture analysis of facial skin features) can be used to detect image content features in an image, including faces.
Some implementations of image content feature criteria may include designated persons that qualify as a shared image if any of those persons are detected as being depicted in the image. In some embodiments, for images that qualify as shared images, those designated persons are depicted exclusively in the image, and those not designated persons are depicted. In some examples, the first user may enter particular names of the persons and/or may specify categories of persons (e.g., my child, family, work colleague, my team's person, etc.).
In some examples, an image (e.g., a "shared standard image") from a library (set of images) of images of a first user may be displayed on a device (e.g., a first device). The sharing criteria image may be selected to indicate sharing criteria, as described below. For example, the save standard image may be selected by an input from the first user. The shared standard images may be selected in other ways, for example, by the system based on stored user preferences and/or based on features similar to other shared standard images (e.g., having a capture time or capture location within a particular time period or geographic distance of a time and location associated with one or more other shared standard images, having the same type of image content features as the other shared standard images, etc.). For example, characteristics of the sharing standard image may be used as the sharing standard. Such sharing criteria may include image content characterization criteria.
For example, the shared standard image may depict sample faces from different, unique persons (or face models) depicted in the library image of the first user, and images showing duplicate faces are not displayed in the menu (e.g., multiple images showing faces of the same person are not displayed). The first user may select one or more sharing criteria images to indicate one or more faces depicted in the selected sharing criteria images as facial criteria, e.g., to identify a person that qualifies the image as a sharing image if depicted in the image. For example, in some implementations, one or more person identifiers are determined from the selected sharing criteria image based on the detected face, and the person identifiers are designated as person sharing criteria. In some examples, the person sharing criteria may include facial criteria that specify a particular facial identity. In some implementations, a face model (or a cluster of face models) may have an associated facial identity to identify an associated person.
Similarly, a sharing standard image depicting image features other than faces, such as a sharing standard image depicting sunset, landscape, building, animal, etc., may be displayed, which the user may select to indicate sharing standards based on one or more of the depicted image features. For example, selecting a shared standard image depicting sunset makes a library image having sunset qualify as a shared image.
In some embodiments, the sharing criteria are determined from particular image features in the sharing criteria image that have predetermined or specified characteristics, and the sharing criteria are not determined from other image features depicted in the sharing criteria image that do not have these characteristics. In some implementations, the specified characteristics for the sharing criteria may be specified or indicated by user input, user preferences, device settings, and the like. For example, certain types of image features may be used as or indicative of sharing criteria, such as facial, animal type, landscape features, and the like. In some examples, image features having a threshold size or greater (e.g., a threshold area percentage of the total image area, a threshold pixel count percentage of the total pixel count, a threshold pixel width and/or height of the total width and/or height of the image, etc.) may be used as the sharing criteria, while image features less than the threshold size are not used as the sharing criteria. In some examples, image features that cover and/or have pixels that are within a certain threshold distance (or image center) from the image center (or other specific location within the image region or frame) are used as sharing criteria, while other features outside such threshold location are not used as sharing criteria. In further examples, image features with specified colors, brightness, or other image characteristics may be used for the sharing criteria.
The location criteria may specify a particular location that the location associated with the image is to match in order for the image to qualify as a shared image for the first user. For example, the image may be associated with metadata that includes the geographic location of the captured image. For example, a geographic location may be specified as coordinates (e.g., latitude and longitude) by an address (e.g., postal address) or otherwise. An image qualifies as a shared image if the location associated with the image matches a location specified by a location criterion. The location criteria may be specified by the first user as a distinct place name (e.g., a ken court, a gatekeeper park), by address (e.g., postal address, postal code, area code, etc.), or otherwise. For example, a map may be displayed, for example, by the first device, and the first user may select a particular location or region/zone that is entered as a location criterion. In some implementations, the location criteria may be specified by the first user as tags (e.g., categories or classifications), such as "home," "work," "school," etc., and if consent of the first user has been obtained, the system may determine the particular geographic location to which the tags correspond, for example, by examining other user data of the first user, such as contact information (specifying the locations), travel history of the first user (e.g., work or home indicated by time and length of occurrence at the particular location), communications of the first user (e.g., corporate email address, etc.), etc.
Other or additional sharing criteria may be used in some embodiments. For example, the sharing criteria may be based on visual image characteristics, user ratings, and the like. Other sharing criteria examples are described in various portions of the disclosure and may include visual image characteristics, including quality characteristics of the image (e.g., blur, contrast, brightness, color noise, etc., where threshold levels of these characteristics may qualify the image as a shared image), and so forth.
In other examples, the sharing criteria may include a source storage location of the shared image. For example, the sharing criteria may specify a particular storage location, such as a folder, group, or other collection of storage from which to select the shared image. In some examples, storage locations associated with a particular application may be designated as sharing criteria such that a shared image is selected from those storage locations. For example, the particular application may be a camera application that may be used to capture images, a chat application that receives images from other users in chat conversations and text messages, a communication application that receives images from emails, an image editing application that may be used to edit captured images (e.g., adding effects to images, selecting a particular image from a group of images captured in a small time interval such as half a second, adding text or graphical elements to images, etc.), etc.
In a further example, the sharing criteria may include a particular user and/or device that has provided the image. For example, a particular user may be designated such that images received from the particular user qualify as shared images. In some examples, a particular user device (e.g., a camera device) is designated such that images captured or created by or received from those devices qualify as shared images.
In a further example, the sharing criteria may include previous actions of the first user related to the image, e.g., from a history of these actions that may indicate that the image is eligible as a shared image. For example, previous display and viewing, sharing to other users, selection, and other actions performed by the first user on the particular image using the first device and/or other devices may qualify the particular image as a shared image in fig. 2. In some implementations, the type of previous action that is considered to qualify the image for sharing may be specified by the first user (e.g., in block 212 or in a stored user preference). In some implementations, one or more relevant time periods (e.g., in block 212 or in stored user preferences) during which to take previous actions (e.g., time of day, particular month or year, etc.) may be specified by the first user.
In some embodiments, images having characteristics matching certain standard characteristics may be excluded from images shared to the second user. For example, such criteria may be filtering criteria that cause a particular image to be filtered from a set of shared images based on characteristics of the image that match the filtering criteria.
In some implementations, one or more sharing criteria images may be displayed as suggested sharing criteria, and which allow user input from the first user to select at least one of the sharing criteria images to specify the sharing criteria. In some examples, if a sharing standard image is selected by the first user input, such sharing standard image may display image content features that may be used as image standards as described above. In further examples, some of the shared standard images may suggest other shared criteria associated with the shared standard image, such as time or date (time or date the standard image was captured), location (location of capture of the standard image), etc., wherein if the standard image is selected, the relevant shared criteria is selected.
In some implementations, the shared standard image may be an image with particular characteristics selected by the system from a library of images of the first user, such as being shared with other users (not receiving users), being viewed on the first user's device, being most frequently shared/viewed in the past (e.g., within a particular period of time prior to the current time) through the first user input. In some examples, the sharing criteria image may depict a content type previously selected or viewed by the first user and/or matching stored preferences or settings set by the first user. In some examples, such shared standard images may be obtained from a list that stores the most images that multiple users view or download from a network site that multiple users access. The repeated shared standard image, for example, an image depicting the content repeated with other shared standard images may be omitted from the display. For example, if it is determined that two images depicting the face of the same person are sharing standard images, one of these images may be omitted from display as the sharing standard image.
In some embodiments, the shared standard image may be generated or modified from other images from the image library of the first user, for example, by editing the other images and/or modifying pixel values of the other images. For example, the images from the library may be cropped (e.g., by a system or other system) such that the images after cropping (e.g., the remaining portion of the images after cropping) are used as the shared standard image. In some examples, the system may crop (e.g., remove) areas of the image other than the portion depicting the face to form the shared standard image. The shared standard image is a portion of the image depicting a face such that the face is the only face depicted in the shared standard image. In some implementations, a shared standard image may be similarly generated for each face in the image (e.g., each face is above a threshold size of pixel size). In another example, text or symbols may be added to the image to form a shared standard image, e.g., names or other identifiers of depicted people, locations, historical remains, etc. may be added.
In a further example of a sharing setting, a delay period (e.g., a grace period) may be specified by the first user. The delay period is a period of time that occurs after the system determines that the images are shared and before the receiving user is actually granted access to the first user's images during which the first user may specify that particular ones of those images are not shared, e.g., are not provisioned or provided with access to the second user. For example, in response to determining that a particular image matches a specified sharing criteria, a delay period may be provided during which a status indicator is assigned to the particular image that indicates a first status, wherein the first status indicates that the image is to be shared with a receiving user. The first device may display a notification indicating that the shared image is determined. In some examples, the notification provides a displayed option (e.g., a selectable button or other displayed control) that causes the determined shared image to be displayed for viewing by the user.
In some implementations, the status indicator indicating the first status is modifiable and may be updated by the first user input received during the delay period to a status indicator indicating a second status, wherein the second status indicates that the image is not to be shared with the receiving user. If the delay period expires and the image still has a first state (e.g., the first user does not specify a particular image that is not to be served or provided for sharing to the recipient), then access to the image is provided to the second user (as described below with respect to block 220). If an image is assigned a second status based on the first user input, the image is removed from the sharing qualification, e.g., from a stored list indicating an identification of images to be shared with the receiving user. In some examples, the shared images may be stored or categorized in a queue, and the first user may remove one or more shared images from the queue during the delay period (e.g., by providing user input selecting one or more images displayed in the user interface) such that when the delay period expires, the second user is provided access to the (remaining) images in the queue.
In another example of a delay period, the delay period of the image may begin after a certain amount of time (e.g., N minutes) has elapsed since the image was backed up, e.g., after the image was saved to a server account from a device such as a client device. During this delay period, the first user may mark the image as not shared through the sharing process of method 200. In some implementations, the delay period may be adjusted by user input from the first user.
In further examples, the delay period may be applied to a new image obtained by the first user or the first device that is determined to be a shared image, as described below with respect to blocks 222-230.
In block 214, a shared image of the first user is determined based on the sharing criteria. For example, images are programmatically analyzed by the first device and/or other devices in communication with the first device to determine which images qualify as shared images based on sharing criteria. The analyzed images may be compared to multiple sharing criteria or a sharing criteria to determine whether they qualify as shared images. In some implementations, different images may be compared to different sharing criteria, for example, based on one or more characteristics of the image (image content characteristics, visual (pixel) characteristics, capture time, capture location, etc.).
The analyzed images may be stored, for example, in one or more stored collections or albums on one or more servers associated with the first user (e.g., in an image library of the first user), and/or in other client devices stored locally on or accessible by the first user, and so forth. In some implementations, other processes may affect which images in the image library are available for processing in block 214. In some implementations, the archiving criteria (ARCHIVING CRITERIA) cause one or more images to be archived (not displayed in the image vault, and/or omitted from being sent to a server for storage, etc.). Such archiving may enable the sharing criteria to be used without considering one or more images of the image library as shared images.
In some embodiments, the sharing criteria is stored in a local store of the first device and/or the shared image is determined by the first device. In some implementations, one or more sharing criteria can be stored at a server (or other device) remote from the first device (e.g., connectable to the first device over a network), and/or the server (or other device) can determine the shared image. In some implementations, the first device and the one or more remote devices both store sharing criteria and/or determine a shared image.
In some embodiments in which faces are detected in images of the first user's image library, persons of the faces are determined and compared to the person sharing criteria determined in block 212 (e.g., facial criteria specifying a particular facial identity). The shared images include those images determined to depict persons matching one or more person sharing criteria in pixels of the images. Similarly, other image content characteristics may be compared to other types of sharing criteria, and other image characteristics, such as time and/or location, may be compared to time and location sharing criteria to determine a shared image of the first user's image library. The images determined to qualify as shared images may be designated as shared images to be shared with a recipient (second user), e.g., a respective sharing designation may be associated with each shared image and stored in a device store.
In some implementations, determining a match between the image and the one or more sharing criteria may be associated with a confidence level of the match between the image and the one or more sharing criteria, where the confidence level may indicate a confidence of the match and/or a likelihood of the match being accurate. In one example, the closer a match is determined, the higher the confidence level assigned to the match. In some implementations, different confidence level thresholds may be employed, where different thresholds indicate confidence levels of different levels or categories.
In some implementations, the image may be compared to the sharing criteria and/or the shared image determined as described above at different times and/or in response to one or more particular conditions of the first device (e.g., may be specified by the first user in some implementations). For example, in some implementations, the images may be compared and determined for sharing at the time the first device captures and/or at the time the first device obtains access to the image or receives the image, or under one or more operating conditions of the first device (e.g., during a particular period of time or not during a particular period of time during which the first device is performing a particular device operation, e.g., a device operation such as running a particular application, transmitting or receiving data in a particular application, etc.).
In some implementations, previews of images that qualify as shared images may be displayed, wherein the preview images are selected (e.g., on the first device) from a first user's image library based on specified sharing criteria (e.g., by the system or device), e.g., after the first user has entered one or more sharing criteria (or based on sharing criteria specified in user preferences, etc.). Previewing may include previewing images from the first user's library, e.g., images that match one or more sharing criteria currently specified by the first user and that qualify for sharing to the receiving user.
In some implementations, the user interface displaying the preview may provide a displayed interface element or control that allows the first user to remove one or more images from the shared state (e.g., by selecting an image in the preview with user input or by deselecting an image) before sharing the image with the second user. In some implementations, all images that qualify as shared images (e.g., as scrollable lists or grids of images with image order) can be displayed in the preview. In some implementations, a subset of representative images that qualify as shared images may be displayed in the preview, e.g., images having visual similarity to other images below a threshold, images having visual characteristics relative to a particular threshold (e.g., for sharpness, noise, exposure, etc.), as determined by one or more image pixel-based similarity measurement techniques. In some implementations, such representative subset of images may be displayed first in preview order of the images before other images are displayed in order (e.g., threshold similarity through pixel similarity measurement, which may be repeated or have similar content, for example, with the representative image).
In some implementations, a preview of images currently eligible as shared images may be displayed for a particular type of sharing criteria, e.g., associated with the first user selecting additional sharing criteria by selecting images from a list of images (e.g., blocks 212 and 214 may be performed multiple iterations). For example, the preview may be updated after each selection of an image specifying one or more sharing criteria such that only images matching the selected sharing criteria are displayed in the preview.
In block 216, an invitation is sent to the second user. The invitation requests the second user to accept access rights to the image that the first user has shared. The invitation may be sent over the network to a second device associated with the second user.
In block 218, data is received over the network from the second user and the second device indicating acceptance of the sharing invitation. For example, the server system and/or the first device may receive the acceptance. If the second user refuses to share the invitation (e.g., an indication of refusal is received in block 218), the execution of blocks 220-230 may be omitted.
In block 220, the second user is provided access to the shared image of the first user. For example, a second user may be granted permission to access a shared image of the image library of the first user, e.g., one or more stored collections or albums associated with and accessible by the first user. The stored collection may be stored on one or more servers, and/or on one or more client devices (including one or more client devices associated with the first user), and so forth.
In some implementations, access permissions (e.g., permission status data) associated with the shared image are automatically updated (without user input) to allow the second user to access the shared image, e.g., to retrieve data associated with the shared image for storage and/or display on the second user's user device. For example, in some implementations, access permissions are updated for each shared image and a confidence level associated with a determination that one or more sharing criteria match the shared image is based. For example, if the shared image depicts a person, the confidence level indicates an estimated confidence or probability that the depicted person matches the person sharing criteria.
In some implementations, if the confidence level of the sharing criteria match meets a threshold confidence level, the access permissions are updated to allow access to the particular image by the second user. In some implementations, multiple confidence level thresholds may be used to determine different forms of access level updates. In some examples, if the confidence level of the match of the shared image with the sharing criteria meets a first threshold, the access permissions are automatically updated without user input to allow the second user to access the shared image. If the confidence level of the match meets a second threshold indicating that the confidence level on the match is below the first threshold, the first device displays a shared image that is available for selection from the first user to a user input of the first device to update access permissions of the first image to grant access rights to the second user. If the first user manually indicates that these images are to be shared, this allows sharing of images with another user with a lower confidence of a match with the sharing criteria.
In some implementations, server system 102 can store respective image libraries for one or more users. For example, the image library may store images associated with the user (e.g., captured using a device associated with the user's account, uploaded by the user to a server or the like), images shared with the user, and permissions associated with the images in the library, e.g., the permissions specifying one or more other users to whom access rights to the images are granted. In some implementations, a user's image library may be organized into a plurality of albums, each album including one or more images. The images may be included in a plurality of albums.
In some implementations, when the second user is granted access to the shared image of the first user, the shared image is automatically added to the image library of the second user (e.g., automatically save features, e.g., as described with reference to fig. 3). In some implementations, when the second user is granted access to the shared image of the first user, the shared image is automatically added to a separate album in the second user's image library that includes the image shared by the first user. In some embodiments, a portion of the user's image library may be synchronized to one or more devices associated with, for example, the user's user account, such that images stored on the one or more devices are also stored in the portion of the image library, and in some embodiments, vice versa.
In some examples, the image data of the image library is stored in the storage of the second device such that the image data of the shared image is stored in the second device. In some embodiments, the image data of the image library is stored on a server. In some implementations, the image data of the image library is stored on the server and one or more client devices, and/or is stored in part on the server and in part on one or more client devices. The image data includes pixel values (e.g., color values or other values such as luminance values, etc.) for pixels of images of the library. For example, the user device may store a data pointer to the relevant image of the library stored on the server. Thus, the user device may access the user's image library on the server by following such a data pointer.
In some implementations, the first device of the first user can access the image data stored on the server using the data pointer. If the second user is granted access to the shared image of the image library of the first user, the access permission of the shared image of the first user is updated to allow the second user (e.g., a device used by the second user) to access the shared image. In an example, one or more second data pointers are sent from the server to a second device of the second user over the network, wherein the second data pointers point to the shared image and allow the second user device to access image data for the shared image stored on the server. In some implementations, the first data pointer of the first user and the second data pointer of the second user can be uniform resource locators (Uniform Resource Locators, URLs). In some embodiments, the data pointer and the second data pointer of the first user may be the same URL. In some embodiments, the data pointer and the second data pointer of the first user may be different URLs pointing to the same image. This allows for storing (e.g., on a server or other device) a single copy of image data to save storage requirements for the image data, where different user devices may retrieve the image data of the single copy to store or view the image data.
In block 222, a new image is obtained by the first device. This may occur at a time after the sharing settings described above with respect to blocks 208-220. For example, the first device may capture a new image from a camera component of the first device, may receive the new image from the device over a network, may generate the new image in an editing application, and so on.
In block 224, the new image is programmatically analyzed to determine if there are any matches to the sharing criteria. For example, the metadata of the new image may be checked and compared to appropriate sharing criteria. In some examples, a timestamp indicating the date of image capture, image creation, image modification, etc. may be compared to the above-described time criteria to determine whether the new image qualifies as a shared image. The location metadata of the new image may be analyzed to determine whether the new image qualifies as a shared image. The image content may be analyzed for image features that meet image feature criteria.
In block 226, a check is made to determine in block 224 whether one or more matches are determined for the new image. If not, the method continues to block 228 where the second user is not provided access to the new image (e.g., as determined for method 200; the second user may be otherwise granted access to the new image, such as through a particular input from the first user). If one or more matches to the sharing criteria are determined for the new image, the method continues to block 230 where the second user is provided with access rights to the new image, similar to the access rights provided in block 220. Multiple new images obtained by the first user or the first device may be similarly processed if the sharing criteria are met.
In some implementations, similar to the images described herein, content items (e.g., content data) other than images may be shared according to the methods and implementations described herein. For example, the content item may include audio data or segments of audio data (e.g., speech, music, etc.) without accompanying or corresponding images. In some implementations, if user consent has been obtained, voice recognition techniques can be used to detect or identify speaker users whose voice is captured in the voice-audio data and apply sharing criteria based on the detected users. In another example, a type of sound effect (e.g., water flow, ringtone, bird or other animal sound, etc.) may be detected, and sharing criteria may be applied based on the detected type to determine whether the sound data is eligible for sharing. In another example, the content item may comprise a document, such as a text document or a document comprising text and images, wherein the topic or theme of the document may be programmatically analyzed for keywords or key phrases or based on machine learning techniques to determine whether the document matches the sharing criteria of the text and thus whether the document qualifies as a shared content item.
In some embodiments, a first user may establish and implement sharing of the image described in method 200 with a single other user (receiving user), and when implementing the sharing, the first user may not otherwise be able to provide access to the shared image to the other user using method 200. In some examples, a first user may cancel image sharing with a second user and establish image sharing with a different user. In some embodiments, the method 200 may be used while providing multiple receiving users with access to the shared image of the first user. For example, the sharing criteria may be applied as described above, and a plurality of users are each provided with access rights to the shared image.
In some implementations, a different and independent set of sharing criteria may be associated with each of the plurality of receiving users. For image sharing with a particular receiving user, a set of sharing criteria associated with the particular receiving user is applied to the image of the transmitting user, and this process is repeated for each of a plurality of designated receiving users. For example, the sharing criteria may be different for each such receiving user (e.g., including at least one different sharing criteria option for each receiving user if selected by the first user). For example, a first user may specify a set of sharing criteria to share with a grandparent receiving user only images depicting infants, a different set of sharing criteria to share with a friend receiving user only images depicting faces of the friend captured in 2013 or later, and a third set of sharing criteria to share with a spouse receiving user all images captured at a home location.
In some implementations, sending the sharing request and/or sharing image to other devices (e.g., user devices) as described with respect to fig. 2 may be temporarily enabled and/or disabled, e.g., based on whether one or more particular conditions apply. For example, a first device may be enabled to send invitations for sharing and/or images for sharing to other user devices (such as a second device) at a particular time, and may be disabled from sending such invitations and/or images for sharing at other times. In some implementations, a particular condition can be specified in the stored first user preference. In some implementations, the particular condition may include detecting that one or more events have occurred. For example, the first device may not allow sharing, and then the first device is carried or moved to a particular geographic location (e.g., as specified in the stored preferences), such as a specified geographic area, which allows sending invitations and sharing images to other user devices. In some examples, sharing continues to be allowed while the first device is in the location, or may have a duration from the time the first device first entered the location specified in the preference or setting, such that sharing is not enabled after expiration of the duration.
In some implementations, a control (e.g., a button or network link) is displayed on the first device, and if the control is selected by the first user input, the first device can share the image to other devices (e.g., for a particular duration and/or at a particular location) as shown in fig. 2. For example, a particular uniform resource link (Uniform Resource Link, URL) may be specified in the network link to which the first device may connect over the network to send the sharing invitation and the sharing image. In some implementations, the second device may be made temporarily available to send invitations and/or shared images to other devices, e.g., similar to the first device described above.
In some implementations, if no connection to an intermediate sharing device (e.g., a server) is available (e.g., no wireless service is available at a particular location or under a particular environmental condition), an alternate communication channel may be established between the first device and the second device for sharing the image. For example, a peer-to-peer network connection (e.g., bluetooth Wi-Fi direct, etc.) may be established and used to transmit a sharing invitation and images between the first device and the second device. In some implementations, different network links may be available and may be used.
In some implementations, for a shared image using the method of fig. 2, a first user and/or first device may designate one or more users and/or user devices (e.g., a second user or second user device in the example of fig. 2) as temporary shared users or partners (and/or temporary shared user devices). For example, if one or more particular conditions are determined by the first device to be met, e.g., if the current time is within a particular time period, the sharing invitation and the sharing image may be sent to a temporary sharing partner (e.g., the second device and/or the second user server account). A particular condition may be specified in the stored user preferences or the first device settings. For example, the particular condition may include a specified time period, a specified geographic location of the first device and/or the second device, a current use of a particular application on the first device and/or the second device by a user, and so forth.
In some implementations, the sharing user may be allowed to initially access the first user's shared image, as depicted in fig. 2, and then be prevented from accessing the first user's shared image after a certain period of time has expired and/or other circumstances have occurred. In some implementations, user devices may be allowed to share images with each other (e.g., give each other access to another user's shared image) when the users are co-located in a particular geographic location, e.g., when the user devices are within a particular threshold distance of each other. In some implementations, for example, in a messaging application running on all user devices, the user devices may be allowed to share images with each other while the users of the devices are participating in a shared conversation group or chat.
In some implementations, one or more users and/or user devices may be designated as sharing users or partners, e.g., for which no temporary shared access rights are imposed on the first user's shared invitation and shared image.
Fig. 3 is a flow chart illustrating an example of a method 300 of providing features to share images for a receiving user, according to some embodiments. In some implementations, the method 300 may be implemented, for example, on the server system 102 as shown in fig. 1. In some implementations, some or all of the method 300 may be implemented on one or more client devices 120, 122, 124, or 126, one or more server devices, and/or on both the server device and the client device as shown in fig. 1. In some implementations, the implementation system includes one or more digital processors or processing circuits ("processors") and one or more storage devices (e.g., database 106 or other storage). In some implementations, different components of one or more servers and/or clients may perform different blocks or other portions of method 300. In some implementations, the second device is described as performing the blocks of method 300. Some implementations may have one or more blocks of method 300 performed by one or more other devices (e.g., other client devices or server devices) that may send results or data to the second device. The various examples and embodiments described with respect to method 300 may be combined in various permutations or combinations.
In some embodiments, the method 300 or portions of the method may be initiated automatically by the system. In some embodiments, the implementation system is a second device. In some embodiments, the method (or portions thereof) may be performed periodically or based on one or more particular events or conditions, such as the launching of an application by a user, the receipt of one or more images that have been newly uploaded or accessible by the system, the passage of a predetermined period of time since the last execution of method 300, and/or one or more other conditions that may be specified in settings read by the method. In some implementations, such conditions can be specified by the user in stored user-defined preferences of the user.
In some implementations, the second device may be a camera, a cell phone, a smart phone, a tablet, a wearable device, or other client device that may receive user content input (e.g., image capture) to the client device and may perform the method 300. In some implementations, a client device or server device may receive one or more images uploaded from one or more users or received over a network connection, and may perform the method 300 for the one or more images. In some implementations, the client device may send the image to the server over the network, and the server may process the content using the method 300. Some embodiments may initiate the method 300 based on user input. For example, a user (e.g., an operator or end user) may select initiation of method 300 from a displayed user interface, such as an application user interface or other user interface.
In block 302, it is checked whether user consent (e.g., user permissions) has been obtained to use the user data in the implementation of method 300. The user data may include messages sent or received by the user, user preferences, user biometric information, user characteristics (identification, name, age, gender, occupation, etc.), information about the user's social network and contacts, social and other types of actions and activities, content, ratings, and opinions created or submitted by the user, the user's current location, historical user data, images generated, received, and/or accessed by the user, videos viewed or shared by the user, and so forth. In some implementations, one or more blocks of the methods described herein may use such user data.
If user consent has been obtained from the relevant user whose user data was available in method 300, then in block 304, the blocks of the method herein are determined to be applicable where possible using the user data described for those blocks, and the method continues to block 308. If user consent has not been obtained, then in block 306 it is determined that the block will be implemented without using the user data, and the method continues to block 308. In some implementations, if user consent is not obtained, the block is implemented without using user data and/or data that is generic or publicly accessible and publicly available.
In block 308, an invitation to share an image is received at the second device and the invitation is output by the second device. In some implementations, the invitation may be from the first device (and the first user) to obtain access rights to the shared image of the first user, as described in block 216 of fig. 2. In some implementations, the invitation can be displayed in a user interface, as an audio output from a speaker, and so forth.
If the second user and/or second device did not previously accept the shared connection with the first user and/or first device, for example, as described in FIG. 2, an invitation to share an image may be received and output if the second user and/or second device has not previously been granted access to the shared image. If the second user and/or the second device has previously accepted the shared connection of the shared image of the first user and the second user and/or the second device has previously received an indication of the grant of access rights to the shared image of the first user, then the display of the invitation is omitted and the shared image may be automatically saved to the second user image library based on an automatic save criteria as described herein, for example, in blocks 314-317 below. In some implementations, the second device does not receive the invitation if such an existing shared connection exists.
In block 310, the acceptance of the invitation is received by the second device as input by the second user and the acceptance is sent to the first user, e.g., the acceptance is sent such that the acceptance is received by the first device over the network. In some examples, the acceptance may include user input provided to a user interface displayed by the second device, e.g., selection of a displayed acceptance button, etc.
In block 312, access rights to the shared image of the first user are obtained for the second user. For example, a second user (e.g., a second device and/or an account of the second user) may receive an indication that access permissions to a shared image of the first user have been granted to the second user. In some implementations, the second user and/or the second device may be provided with access permission data (as determined with respect to the method 200 of fig. 2) required to access the shared image associated with the first user. Permissions (e.g., permission data) may be stored on one or more server devices, client devices, and so forth. Thus, the second device is able to receive image data of the shared image and/or display the shared image of the first user on its display. As described with respect to the method 200 of fig. 2, access rights may be provided in response to the first user sharing the images.
In block 314, an autosave option may be displayed by the second device, wherein the autosave option provides a setting to allow the second user to specify autosave criteria defining which of the first user's shared images are to be automatically saved in a store associated with the second user. The storage of the second user may be storage provided on one or more servers, provided on one or more client devices (e.g., the second device), provided partially on the servers and partially on one or more client devices, and so forth. For example, if the second user's store includes one or more client devices, the shared image stored on the server may be downloaded to those client devices over the network. The storage of the second user may include an image library associated with the second user.
In response to the second user providing an acceptance of the sharing invitation received in block 308, an automatic save option may be displayed. In some implementations, the automatic save option may be displayed at any time by the second device based on user input, such as adding or changing automatic save criteria as user preferences or device settings.
The automatic save criteria may be specified in various ways to indicate which shared images are to be automatically saved to the second user's store. In some implementations, one or more of the autosave criteria may be similar to, and may be specified similar to, the sharing criteria described herein, such as the sharing criteria described with respect to block 212 of fig. 2.
In some implementations, the second user may specify image feature criteria for automatic saving, where specified visual image content (e.g., image features) will match visual content in a particular image to qualify the particular image for automatic saving to the second user's image library. In some implementations, the second user can specify the image feature criteria by identifying a person, a feature type, or the like. In some implementations, images from the second user's image library can be displayed in a user interface (e.g., on the second device), and the second user can select the displayed images to identify one or more image features in the images as auto-save criteria. This allows, for example, the second user to select persons known to the second user as automatic save criteria through the second user's image library.
In some implementations, the images from the second user's library may depict the faces of different, unique people. Displaying duplicate images with visual content that is duplicate or similar to other images in the library may be omitted from the user interface so that the user interface displays images depicting unique people that are not duplicate in all images displayed in the user interface. When the second user selects a particular one of those images, the selection specifies an automatic save criterion (e.g., a person save criterion) such that a shared image depicting the person (e.g., as determined based on facial recognition techniques if user consent has been obtained) is eligible for automatic save to the second user's library. A person identifier indicating a particular person depicted in the selected image may be determined and may be designated as a person preservation criteria, similar to that described with respect to fig. 2. In some implementations, the person preservation criteria may be face preservation criteria that specify a particular facial identity, e.g., based on a face model or a cluster of face models as described herein.
In some embodiments, the user interface for specifying the automatic save criteria may display a shared image of the first user to whom the second user has previously been granted access, in addition to or in lieu of displaying images from the second user's library, and the user may select one or more such shared images as the automatic save criteria.
In some implementations, a model of an object, such as a facial model of a face, is accessible by the second user device and may be designated as an automatic save criterion. In some implementations, facial models similar to those described above for block 212 of fig. 2 may be used in block 314. This allows the system to detect various faces in different images and match those faces with a save standard face model to determine that the depicted face belongs to a particular person associated with the face model and will therefore be saved in the second user image repository. In some implementations, such a face model may be stored locally to the second user device and may be generated based on detected faces depicted in images stored in the second user image repository. In some implementations, the facial model may additionally or alternatively be stored on a network storage and/or a server accessible to the second device over a network, and such facial model may also be used as an image standard. In some implementations, the facial model is maintained only in local storage of the user device and is not transmitted to the server over the network. In some implementations, face model clusters are provided, where each cluster represents a particular person and includes a plurality of face models that have been determined to represent the particular person based on faces depicted in images and/or have been confirmed to represent the particular person by user input.
In some implementations, one or more images may be displayed, for example, by the second device as a "save standard image" having characteristics that may be indicated as automatically saving the standard. For example, user input from the second user may select at least one save criteria image to specify automatic save criteria. The save standard images may be selected in other ways, for example, by the system based on stored user preferences and/or based on characteristics similar to other save standard images (e.g., having a capture time or capture location within a particular time period or geographic distance of a time and location associated with one or more other save standard images, having the same type of image content characteristics as the other save standard images, etc.), etc. In some embodiments, if a save standard image is selected by a second user input, such a save standard image as described above may display image content used as an automatic save standard. Such a save standard image may be, for example, an image provided by the system from a second user image library.
Such automatic preservation criteria may include image content characterization criteria. For example, the save standard image may depict a sample face from a different person (or face/object model) depicted in the library image of the second user. In some examples, a save criteria image of a particular person may be displayed, which if selected by the user, selects the corresponding person identifier as the automatic save criteria. In some implementations, a person identifier (e.g., determined from a face model or a cluster of face models) is stored on the second device and created based on a library of images of the second user.
In some implementations, the second device may display one or more save-standard images that correspond to one or more corresponding specific identifications (e.g., person identifiers) from the first user and/or the first user device, such as created based on an image library of the first user (e.g., based on a facial model determined from the image of the first user) or otherwise identified by the first device. Such an identification may be received by the second device, for example, from the first user and/or the first device. For example, the second device may not have stored any person identifiers corresponding to persons depicted in the shared image, and the corresponding person identifiers from the first user may be used to represent persons for saving the standard image. This may allow the second user to select content features that have been identified by the first user and/or the first device as the automatic saving criteria.
In some implementations, saving the standard image may depict a content type that was previously selected or viewed by the second user and/or matches a stored preference or setting set by the second user. In some embodiments, such saved standard images may be obtained from a list that stores the most images that multiple users view or download from a network site that multiple users access.
Similar to the above, the repeated save standard image, for example, an image depicting the content repeated with other save standard images may be omitted from the display. In some implementations, for a save standard image depicting a person, each of the save standard images may be selected for system display to depict a different, unique person and associated with a different person identifier. In another example, each of the retention standard images may be selected from different image clusters, where each image cluster is associated with a different unique person, and each image cluster includes one or more images depicting the associated unique person. In some implementations, the clusters can correspond to face model clusters as described herein.
Similarly, a save standard image may be displayed that depicts image features other than a face, e.g., sunset, landscape, building, animal, etc., which the user may select to indicate a save standard based on one or more of the depicted image features. The save standard image may be a cropped portion of the image from the second user's image library and/or may be a modified version of such an image, similar to that described above for the shared standard image of fig. 2.
In some embodiments, the automatic preservation criteria are determined from specific image features in the preservation criteria image, wherein those image features have predetermined or specified characteristics, and the automatic preservation criteria are not determined from other image features depicted in the preservation criteria image that do not have such characteristics. In some implementations, the specified characteristics for automatically saving the criteria may be specified or indicated by user input, user preferences, device settings, and the like. Specific types of image features, such as faces, animal types, landscape features, etc., may specify preservation criteria. In some embodiments, image features having characteristics such as a threshold size or greater, overlaying and/or having pixels, specified colors, brightnesses or other characteristics that are within a particular threshold distance (or center of the image) from the center of the image (or other particular location within the image region or frame) may be used as an automatic preservation criteria, while other features that do not have such characteristics are not used as an automatic preservation criteria, e.g., similar to those described above for the sharing criteria.
In some embodiments, images that have been shared to the second user device and stored in the second user image repository may be provided as save standard images to allow their characteristics to be specified as auto save standards. In some implementations, a face model may be generated from the faces depicted in the received images, and the faces presented as automatic preservation criteria that will allow further received shared images depicting faces matching the face model to be automatically preserved in the second user image repository.
The second user may specify other automatic save criteria such as time criteria, location criteria, and other criteria similar to the sharing criteria as described herein to qualify the shared image for saving.
In block 315, a selection of the displayed autosave option may be received from the second user, including a user input received by the second user device specifying autosave criteria. The automatic save criteria indicates which of the shared images are to be stored in a store associated with the second user. In some implementations, the automatic save criteria may be received from the second user in other types of interfaces or user inputs.
In block 316, the selected shared image is determined from the shared image of the first user based on the automatic save criteria. These are the shared images of the first user that have been selected for saving in the second user image repository, e.g. a subset of the shared images in case not all the shared images are selected. In some implementations, the shared image of the first user can be filtered based on an automatic save criterion. In some implementations, the shared images and/or information associated with the shared images may be processed to determine which shared images match the second user's auto-save criteria.
In some implementations, the filtering and/or selecting of block 316 is performed by the second user device. In some implementations, the filtering and/or selecting of block 316 may be performed partially or fully by a server device in communication with the second user device. The automatic save criteria may be stored in a local store of the second device and/or the selected shared image may be determined by the second device. In some implementations, one or more automatic save criteria may be stored on a server (or other device) remote from the second device (e.g., connectable to the second device over a network), and/or the server (or other device) may determine the selected shared image. In some implementations, the second device and the one or more remote devices both store the automatic save criteria and/or determine the selected shared image.
In some implementations, the shared images may be programmatically analyzed to determine which shared images depict people that match the person identifier of the second user's automatic save criteria (if user consent has been obtained). In some implementations, the programming analysis may include detecting image content features as described herein. In some implementations, the programmatic analysis may include obtaining metadata of the shared image, such as a timestamp indicating a time of capture of the shared image and/or location information indicating a location of capture of the shared image.
In some implementations, a receiving device (e.g., a second device) can determine an identification of one or more content features in a shared image. In some implementations, the identification can be a person identifier determined based on a match with a facial model (e.g., a local facial model and/or a server-stored facial model) derived from images in the image repository of the second user similar to that described above. In some implementations, the server can be used to determine a content identification (e.g., a person identifier) of the shared image. In some implementations, the second device (and/or the second user server account) receives association information for the shared image, e.g., identification information indicating one or more identifications of one or more image content features depicted in the associated shared image. In some implementations, the identification can include a person identifier received by the second device. In some implementations, an identification (e.g., a person identifier) is determined for a shared image shared by the first user and/or the first device (e.g., the image is programmatically analyzed to identify image content features to determine the person identifier, and/or the person identifier is manually entered by the first user, etc., as described with respect to fig. 2). In some implementations, such received person identifiers (associated with the shared image) may be compared to person identifiers associated with the second user's automatically saved criteria to determine whether there is a match.
In some implementations, a confidence level may be determined for matches between the shared images and the auto-save criteria (e.g., between each shared image and each auto-save criteria). If the confidence level meets the first confidence level threshold, the shared image may be automatically considered a selected shared image. In some implementations, if the confidence level of the match meets a second confidence level threshold that indicates a lesser confidence than the first confidence level threshold, the shared image may be displayed in a user interface of the second device that allows user input from the second user to select whether to save the shared image in the image repository of the second user.
In some implementations, the second user may also or alternatively manually browse the shared images of the first user as displayed in the user interface, or a subset of the shared images of the first user, to select which of these images are saved to the second user's image library. In some implementations, such displayed shared images may be images that meet a lower confidence threshold as described above, and/or shared images having other predetermined characteristics (e.g., particular image content type, capture time, capture location, etc.). In some implementations, a preview of the shared images may be displayed on the second user device, and a particular one of the shared images may be selected by user input received by the second user device from the second user. In some implementations, after the shared image is shared by the first user, the preview of the shared image may omit the display of images that have been "not shared" (e.g., the sharing has been revoked) as described above based on user input received by the first user device from the first user.
In block 317, the selected shared image determined in block 316 (e.g., a shared image that meets the second user's auto-save criteria) is stored in the second user's image library. In some embodiments, the image data of the selected shared image is stored in a storage accessible to the second device or the second user, such as a local storage and/or a network storage. In some implementations, a data pointer is stored in the image repository of the second user, the data pointer pointing to image data of a selected shared image stored on a different device. In some implementations, the data pointer can be stored at the second device, which allows the second device to retrieve and display image data of the selected shared image.
In some implementations, the selected shared image can be automatically changed by the system executing the storing. In some implementations, the selected shared image may be downgraded to a particular resolution and/or quality and stored in a modified form. For example, in some implementations, the system may examine stored user preferences specified by the user prior to method 300 and/or prior to block 314 and/or block 316, and may change the resolution of the shared image based on the user specified preferences (e.g., create a copy of the shared image with that resolution). In some implementations, the second user's image library can be associated with settings that indicate stored settings of the image (e.g., settings that downsample the image to a resolution specified by the relevant settings).
In block 318, in some implementations, the second device displays a prompt requesting whether the second user wants to share the second user's image with the first user. In some implementations, the prompt may ask the second user if he wants to provide the first user (e.g., the first device) with access to any of the second user's images. In some implementations, a prompt is provided to only share the image of the second user to the first user (who sent the invitation and shared image received in fig. 3), e.g., a "reply" type option, and no prompt is provided to share the image with other users. Other embodiments may provide a prompt to share an image with one or more other users, e.g., users socially connected to the second user and/or the first user as determined herein, if user consent has been obtained.
In block 320, it is determined whether the second user has indicated sharing of one or more images of the second user with the first user. In some implementations, this may be determined based on the selection of the prompt of block 318 by the second user, or by other user input from the second user to share the image. If sharing is not indicated, the method continues to block 326, described below. If the second user has indicated sharing the image, the method continues to block 322 where a selection is received from the second user to specify settings for sharing. In some implementations, similar to the sharing settings specified by the first user and obtained in block 212 of fig. 2, the second user may specify various sharing criteria, delay periods, and/or other sharing settings. The method continues to block 324.
In block 324, a shared image of the second user is determined based on the sharing criteria specified by the second user, for example in block 322. In some implementations, images are programmatically analyzed by the second device and/or other devices in communication with the second device based on sharing criteria to determine which images qualify as shared images. The analyzed images may be stored in an image repository, for example, an image repository including one or more storage sets or albums associated with the second user, for example, stored on one or more servers, in a local store of the second device or other client device accessible by the second user. An image determined to qualify as a shared image may be designated as an image to be shared with a first user (transmitting user), e.g., a respective designation associated with each shared image and stored in a device store.
In block 326, the first user is provided access to the shared image of the second user. In some implementations, a first user may be granted permission to access a shared image of an image library (e.g., one or more stored image collections or albums) associated with and accessible by a second user. In some implementations, access permissions associated with each shared image of the second user may be updated to allow the first user and/or the first device access to the shared image. In some implementations, this may be similar to granting the second user access to the shared image of the first user as described above for block 220 of fig. 2. The stored collection may be stored on one or more servers, one or more client devices associated with the second user, etc. The method continues to block 328.
Additionally, the new image obtained by the second user (e.g., the second device) may be analyzed for a match with the sharing criteria of the second user and shared with the first user if appropriate, similar to the new image obtained by the first user sharing being shared with the second user as described in blocks 222-230 of fig. 2.
In some implementations, block 328 may be performed on the second device at any time after the second user has received access rights to the shared image of the first user. For example, the shared image view may be provided in response to the second user providing user input to the second device to display the view. The shared image view displays the shared image of the first user and does not display the image of the second user. In some implementations, the shared image view can be in a user interface that is visually distinguished from a visual interface for displaying the image of the second user (e.g., displayed in a different window, color, shape, visual texture, a different title, etc.). In some implementations, the shared image view may be a separate view from the second user image view that the second user may access to display the second user's image without displaying the first user's shared image. In some implementations, the display of the shared image view and the second user image view may be switched on a display screen of the second device.
In some examples, the shared images of the first user may be presented chronologically by the time/date of capture of the images and/or may be ordered by the time/date of sharing the images with the second user, e.g., the time/date the second user obtained access rights to the images.
In some implementations, there is mutual sharing in which the second user shares the second user's image with the first user in addition to having access to the first user's shared image. In some embodiments where there is mutual sharing, the shared image view of the second user does not display an image to which the first user receives access rights through sharing from the second user.
Similarly, if the second user has provided the first user with access to the second user's shared image, a similar shared image view may be provided on the first device for the first user to view the second user's shared image.
In some implementations, a notification may be provided periodically on the receiving device (e.g., the second device) to indicate that one or more images have been newly shared and automatically stored in the second user's image library and available for viewing (e.g., the notification may provide a link or button to display the newly shared images). In some implementations, the notification may be displayed periodically or in response to a device state of the second device (e.g., powering on, opening a screen, running or accessing a particular application, etc.). In some implementations, notification conditions can be specified in stored user preferences. In some implementations, after the user selects to view the shared images, no notifications are provided for those images.
In some implementations, the ability to receive a sharing request and/or share an image by the second device as described with respect to fig. 3 may be temporarily available and/or unavailable, e.g., based on whether one or more particular conditions apply. In some implementations, the second device may be made available to receive the shared invitation and/or shared image at a particular time, and the second device may be made unavailable to such invitation and/or shared image at other times. In some implementations, the first device may query the second device to determine whether it is available to receive a sharing invitation and a sharing image from the first device. In some implementations, particular conditions may be specified in the associated stored user preferences. In some implementations, the particular condition may include detecting that one or more events have occurred. In some implementations, the second device may not allow sharing and then be moved to a particular geographic location (e.g., as specified in the stored preferences), such as a specified geographic area, which allows for receiving the shared image as in fig. 3. In some implementations, sharing is allowed continuously while the second device is in the location, or sharing may have a duration from the time the second device first entered the location specified in the preference or setting, such that sharing is not allowed after expiration of the duration. In some implementations, the control or web link is displayed on the second device and if the control is selected by the second user input, the second device is made available for image sharing (e.g., for a particular duration and/or at a particular location) as in fig. 3. In some implementations, a particular URL may be specified in a network link to which the second device may connect to receive the sharing invitation and the shared image (e.g., a new shared image for the second device since the last time the second device received the shared image). In some implementations, the first device may be similarly or correspondingly made temporarily available to receive invitations and/or share images from the second device.
In some implementations, if no connection to the sharing server is available (e.g., no wireless service is available at a particular location or under a particular environmental condition), an alternate communication channel for sharing the image may be established between the first device and the second device. In some implementations, a peer-to-peer network connection (e.g., bluetooth Wi-Fi direct, etc.) may be established and used to transmit sharing invitations and images between the first device and the second device. In some implementations, different network links are available and may be used.
In some implementations, for the shared image depicted in fig. 3, the second user and/or the second device may designate one or more users and/or user devices (e.g., the first user or the first user device in the example of fig. 3) as temporary shared users or partners (and/or temporary shared user devices). In some implementations, if one or more particular conditions are determined by the second device to be met, e.g., if an image is received within a particular period of time, a shared image from the temporary sharing partner may be received by the second device (and/or the second user server account) and stored in the second user image repository. A particular condition may be specified in the stored user preferences or the second device settings. In some implementations, one or more users and/or user devices may be designated as sharing users or partners, e.g., for which no conditions of temporary access rights to the second user image library by those users and devices are imposed.
FIG. 4 is a schematic diagram of an example user interface 400, according to some embodiments. In some implementations, the user interface 400 (and other user interfaces described herein) can be displayed on a display (e.g., a display screen) of a user device, such as the first device or the second device described herein. As shown in fig. 4, the user interface 400 allows a sending user (e.g., a first user) to select sharing settings, including specifying one or more sharing criteria. User interface element 402 indicates that the sending user is sharing with a second user (receiving user) identified as "John Doe". In some implementations, sharing with the second user can utilize one or more identifiers associated with John Doe, such as an email address, a social media username, an identifier of an image sharing application, a telephone number, and so forth.
The user interface element 404 provides an option to the sending user to grant access to the second user to all or some of the sending user's images (e.g., photos). In some implementations, if the sending user selects "all photos," the entire content of the sending user's image library can be shared with the second user. In another example, if the sending user selects "some photos," the sending user may specify one or more sharing criteria.
In some implementations, the one or more sharing criteria may include an image feature criteria as a person criteria, such as based on the person depicted in the images in the image library of the sending user. As shown in fig. 4, the user interface 400 may include a user interface element 406 that enables a sending user to indicate a selection of one or more people depicted in images in the image library to share with a second user. In response to the sending user activating user interface element 406, another user interface may be presented on the sending user's device indicating such selection to the sending user as a user. An example of such a user interface is shown in fig. 5.
In some implementations, the one or more sharing criteria may include a temporal criteria of the image. In some implementations, the sending user may specify any time period, e.g., a particular day, start day, end day, etc. In some implementations, the user interface 400 includes a user interface element 408 that enables a sending user to indicate a time criterion. As shown in fig. 4, the sending user may indicate a start date ("beginning 5 months 10 days 2017"), a particular time period ("last month", "last year", "forever"), or a specified custom time criteria ("custom"), e.g., a particular upcoming/future day, week, etc.; the next day or other specified period of time (measured from the current time); other user-specified time ranges, etc. In some implementations, for example, if user consent has been obtained, one or more time-standard options may be displayed as events based on user data (e.g., a user's calendar, a user's task list, etc.). In some implementations, the standard option of "spring fraud" may specify a time standard that is a period of time for the event indicated by user data or other reference data (standard calendar, etc.). In some implementations, one or more timestamps (e.g., creation date, last modification date, upload date, etc.) associated with images in the image repository of the sending user can be compared to a time standard to identify images to be shared to the second user.
In some implementations, the one or more sharing criteria can include a location criteria of the image. In some implementations, the sending user can specify a particular location, geographic area (e.g., city, state), etc. In some implementations, the user interface 400 includes a user interface element 410 that enables a sending user to indicate location criteria. As shown in fig. 4, a sending user may indicate one or more particular locations as tags or categories ("home", "distance") or specify custom location criteria ("custom"). In some implementations, when the user provides permission, location data associated with images in the sending user's image library can be compared to location criteria to identify images to be shared with the second user. In some implementations, the location data can include location metadata of the image, e.g., location metadata included by a capture device that includes a location feature (e.g., GPS). In some implementations, the location data may be determined, for example, based on one or more landmarks identified in the image.
In some embodiments, any combination of personnel criteria, time criteria, and location criteria may be used as the sharing criteria. For example, the user interface 400 may enable a sending user to specify a particular time period (e.g., "last month"), location (e.g., "san francisco"), and selection person (e.g., "John," "Tim," "Jane," etc.). In response to the sending user indicating the selection, images in the sending user's image library that match all sharing criteria are shared with the second user. In some implementations, one or more of the sharing criteria may be optional, e.g., the user may specify personnel criteria, but not any time criteria or location criteria. In these embodiments, the images matching the personal criteria in the user's image library are transmitted for sharing with the user.
Although fig. 4 shows personnel criteria, time criteria, and location criteria, the sending user may specify one or more other criteria for sharing. In some implementations, the sending user may specify an image quality criterion (e.g., a visual quality criterion) for the shared image. For example, in some implementations, the image library may store user-assigned image quality ratings (user ratings), such as 1 star, 2 star, 5 star, and so on. In these embodiments, the sending user may specify sharing criteria such as "share 4 stars and above" images.
In some implementations, for example, based on programmatically analyzing the images, a sharing application implementing the method described with reference to fig. 2 may automatically detect visual characteristics of the images in the image library to assign a quality rating. The quality rating of the image may be a visual quality rating based on one or more visual image features. For example, based on a user's permission to perform such analysis, programmatically analyzing the image may be based on determining one or more factors/characteristics, such as lighting, focus, color noise, location (relative to the image area), and/or orientation of a face or other feature in the image, facial expression, blur, etc. The quality rating may be determined as a composite rating based on these factors. In some implementations, programmatically analyzing the image may include analyzing the image using a machine learning application that includes an inference engine that provides quality ratings (e.g., "high," "medium," and "low") of the image based on an application training model.
In some implementations, the sending user may specify a copy or approximate copy, e.g., images depicting similar subject matter will be excluded from sharing (e.g., similarity may be determined based on image pixel similarity techniques). In these embodiments, images in the image library may be identified based on programmatically analyzing such images, and such images may be excluded from the images shared with the second user. In some implementations, the sharing criteria may specify that certain types of images are to be excluded from sharing, for example, even when the sending user has selected "all photos" in the user interface 400. In some implementations, the sending user may specify that images that do not depict any personnel (e.g., images depicting documents, receipts, animals, trees, food, etc.) are to be excluded from sharing. In these embodiments, such images are excluded, for example, based on programmatically analyzing the images to determine that the images do not depict one or more people. In some implementations, one or more other sharing criteria can include whether the sending user has posted the image to a web service for display by other users, such as users having social connections with the sending user (e.g., social networking service, image collection service, etc.). In some implementations, if a sending user has published an image to such a service, the image may qualify as a shared image for a receiving user.
FIG. 5 is an illustration of an example user interface 500 according to some embodiments. As shown in FIG. 5, images 502-518 are displayed in user interface 500, for example, as an image grid. Each image in the grid of images corresponds to a particular person depicted in one or more images in the image library of the sending user. In some implementations, when the sending user agrees to analyze images in the image library, face matching techniques may be applied to the images in the sending user's image library to identify images depicting particular faces. Each image detected to depict a particular person based on face matching techniques may be grouped into a person cluster (or face cluster) corresponding to the particular person. In some implementations, such grouping can include adding metadata to the image that identifies one or more people clusters of the image.
As shown in the example of FIG. 5, images 502-518 each correspond to a different person, and no person is depicted in two or more of the images. In some implementations, each image displayed in the user interface 500 may display a different, unique group of people, e.g., each displayed image depicts multiple people and/or at least one person that is different from the other displayed images. The image library of the sending user may include one or more images of each of the different people shown in images 502-518. For example, user interface 500 enables a sending user to indicate a selection of one or more people by selecting a corresponding image from images 502-518. In the example shown in FIG. 5, the sending user has selected the people corresponding to images 514 and 516, as indicated by the check marks shown in FIG. 5. In response to the selection of the sending user indicating a person, images in the sending user's image library that are detected to include the selected person corresponding to images 514 and 516 may be shared with the second user. In the example shown in FIG. 5, images of the sending user's image library corresponding to the people in images 514 and 516 may be shared with the second user. In some implementations, the sending user is provided with a preview of the image to be shared with the second user, as shown in fig. 6.
FIG. 6 is a schematic diagram of an example user interface 600 according to some embodiments. As shown in fig. 6, when the sending user selects the person depicted in images 514 and 516, a preview of the images from the sending user's image library is provided. In the preview, images 602 and 604 corresponding to the person depicted in image 516 are shown, as well as images 606, 608, and 610 corresponding to the person depicted in image 514, indicating which images from the sending user's library are to be shared. In some implementations, the user interface 600 enables a sending user to specify one or more eligible images to be excluded from sharing before sharing the eligible images with a second user (e.g., by selecting an image in the preview).
Fig. 7A is a schematic diagram of an example user interface 700, according to some embodiments. In some implementations, the user interface 700 may be displayed on a device of the receiving user, for example, on a second device of the second user described above. As shown in fig. 7A, the user interface 700 includes an indication that the sending user ("Jane") has granted the second user (receiving user) access to images in her image library. In fig. 7A, user interface element 702 depicts a representative image of a sending user ("Jane") that provides a visual indication of the identity of the sending user ("Jane"). In some implementations, the user interface element 702 may additionally or alternatively include other information such as the user's name, email address, social media user name, phone number, and the like. The user interface 700 also includes a message 704 indicating that the second user has been granted access by the sending user. In the example shown in fig. 7A, the receiving user is provided with the option to accept or decline the access rights granted by the sending user, for example, by selecting user interface element 706 ("accept") or user interface element 708 ("decline"). For example, in some embodiments, a receiving user may be limited to receiving access rights to images from a limited number of sending users, e.g., a single sending user (buddy), three sending users, etc. By providing an accept or reject option, the user interface 700 enables the receiving user to select the user (e.g., spouse or chaperone, parent or other important person, etc.) that they want to access their image, and reject access rights to images from other users. In some implementations, the receiving user may be automatically granted access to the image of the sending user.
Fig. 7B is an illustration of an example user interface 720 according to some embodiments. As shown in fig. 7B, the user interface 720 includes an image grid depicting representative images 722, 724, 726, and 728. For example, images 722-728 may correspond to people whose images are in the image library of the receiving user. For example, image 724 corresponds to the person whose image is in the image that the sending user has authorized access to, e.g., images 602 and 604 shared by the sending user depict the same person as image 724 in the recipient's library. In this example, the receiving user may select one or more persons, for example, by selecting one or more images 722-728, to indicate automatic save criteria for the shared images. In the example shown in fig. 7B, the recipient has selected an image 724 corresponding to a particular person. Then, by selecting user interface element 730 ("save"), the recipient may indicate that an image of the selected person, such as the person depicted in image 724, which is the same person depicted in images 602 and 604 shared by the sending user, is to be saved into the recipient's image library.
Upon receiving a user selection, the image shared by the sending user depicting the person is automatically added to the recipient's image library. Further, for example, at a later time after receiving the user selection to save user interface element 730, any additional images of the person shared from the sending user are automatically added to the recipient's image library without any further user input from the recipient. Such automatic addition to the image library may provide the advantage of reducing user interaction (e.g., receiving user confirmation to save the image to the image library).
Although fig. 7B illustrates an example in which the recipient selects one or more people whose images are to be automatically saved, in various embodiments, the recipient may specify one or more other automatic save criteria. The automatic save criteria may include location criteria (e.g., "automatically save me images that have been granted access to not taken at home", "automatically save me images that have been granted access to taken at paris", etc.), time criteria (e.g., "automatically save me images that have been granted access to taken on holidays", "automatically save me images that have been granted access to taken in the last month", etc.). In some implementations, the automatic save criteria may enable the recipient to specify that certain images are not to be automatically saved, e.g., copies or approximate copies of images already in the recipient's image library, images that do not meet quality criteria, etc.
Fig. 7C is an illustration of an example user interface 740 according to some embodiments. As shown in fig. 7C, user interface 740 provides a prompt or message 742 to the recipient to respond by sharing her photo with the sending user ("share your photo return Jane |"). In addition, the user interface 740 includes a user interface element 744 that enables the recipient to specify sharing criteria to share photos with the sending user. In some implementations, the sharing criteria may be specified similar to the sharing criteria described with reference to fig. 4. In some implementations, granting access to "some photos" allows the receiving user to specify sharing criteria in more detail on the otherwise displayed user interface screen. In some implementations, the receiving user may initiate mutual automatic sharing with the sending user, but not with other users.
Fig. 8 is a block diagram of an example device 800 that may be used to implement one or more features described herein. In some implementations, device 800 may be used to implement a client device, such as any of client devices 120-126 shown in FIG. 1. Alternatively, device 800 may implement a server device, such as server system 102. In some implementations, the device 800 may be used to implement a client device, a server device, or both a client and a server device. Device 800 may be any suitable computer system, server, or other electronic or hardware device as described above.
One or more methods described herein may be run in a standalone program, which may run on any type of computing device, one or more methods described herein may be a program running on a web browser, a mobile application ("app") running on a mobile computing device (e.g., a cell phone, a smartphone, a tablet, a wearable device (watch, arm band, jewelry, headwear, virtual reality goggles or glasses, augmented reality goggles or glasses, head-mounted display, etc.), a notebook computer, etc.). In some implementations, a client/server architecture may be used, for example, a mobile computing device (as a client device) to send user input data to a server device and to receive final output data from the server for output (e.g., for display). In some implementations, all of the computations may be performed within a mobile app (and/or other apps) on the mobile computing device. In some implementations, the computation may be split between the mobile computing device and one or more server devices.
In some implementations, the device 800 includes a processor 802, memory 804, and an input/output (I/O) interface 806. The processor 802 may be one or more processors and/or processing circuits that execute program code and control the basic operations of the device 800. A "processor" includes any suitable hardware system, mechanism, or component that processes data, signals, or other information. The processor may include a system having a general purpose central processing unit (Central Processing Unit, CPU) including one or more cores (e.g., in a single-core, dual-core, or multi-core configuration), multiple processing units (e.g., in a multi-processor configuration), a graphics processing unit (Graphics Processing Unit, GPU), a field-programmable gate array (field-programmable GATE ARRAY, FPGA), an application-specific integrated circuit (ASIC), a complex programmable logic device (Complex Programmable Logic Device, CPLD), a special purpose processor implementing functions, a special purpose processor implementing neural network model-based processing, a neural circuit, a processor optimized for matrix computation (e.g., matrix multiplication), or other systems. In some implementations, the processor 802 may include one or more coprocessors that implement neural network processing. In some implementations, the processor 802 may be a processor that processes data to produce a probabilistic output, e.g., the output produced by the processor 802 may be inaccurate or may be accurate within the range of expected outputs. The processing need not be limited to a particular geographic location, or have time constraints. For example, a processor may perform its functions "in real time", "offline", in "batch mode", etc. Portions of the processing may be performed by different (or the same) processing systems at different times and at different locations. The computer may be any processor in communication with the memory.
Memory 804 is typically provided in device 800 for access by processor 802 and may be any suitable processor-readable storage medium, such as random access memory (Random Access Memory, RAM), read-only memory (ROM), electrically erasable read-only memory (EEPROM), flash memory, etc. adapted to store instructions for execution by the processor and provided separately from processor 802 and/or integrated with processor 802. The memory 804 may store software that is run by the processor 802 on the server device 800, including an operating system 808, machine learning applications 830, other applications 812, and application data 814. Other applications 812 may include applications such as a data display engine, a network hosting engine (web hosting engine), an image display engine, a notification engine, a social networking engine, and the like. In some implementations, the machine learning application 830 and the other applications 812 may each include instructions that enable the processor 802 to perform some or all of the functions described herein, e.g., the methods of fig. 2 and 3.
Other application programs 812 may include, for example, image editing applications, media display applications, communication applications, web hosting engines or applications, drawing applications, media sharing applications, and the like. One or more of the methods disclosed herein may operate in several environments and platforms, for example, as stand-alone computer programs that may run on any type of computing device, as a web application with web pages, as a mobile application ("app") running on a mobile computing device, and so on.
In various implementations, the machine learning application 830 may utilize a Bayesian (Bayesian) classifier, a support vector machine, a neural network, or other learning techniques. In some implementations, the machine learning application 830 can include a training model 834, an inference engine 836, and data 832. In some implementations, the data 832 may include training data, e.g., data for generating a training model 834. In some implementations, the training data may include any type of data, such as text, images, audio, video, and the like. The training data may be obtained from any source, such as, for example, a data store specially marked for training, data providing permission to use as training data, and the like. In embodiments where one or more users allow for training a machine learning model (e.g., training model 834) using their respective user data, the training data may include such user data. In embodiments where users allow access to their respective user data, the data 832 may include allowed data such as images (e.g., photographs or other user-generated images), communications (e.g., emails; chat data such as text messages, voice, video, etc.), documents (e.g., spreadsheets, text documents, presentations, etc.).
In some implementations, the data 832 may include collected data, such as map data, image data (e.g., satellite images, overhead images, etc.), game data, and the like. In some implementations, the training data may include synthetic data generated for training purposes, such as data that is not based on user input or activity in the context being trained, e.g., data generated from simulated conversations, computer generated images, and so forth. In some implementations, the machine learning application 830 does not include data 832. For example, in these embodiments, training model 834 may be generated, for example, on a different device and provided as part of machine learning application 830. In various embodiments, training model 834 may be provided as a data file containing model structure or form and associated weights. Inference engine 836 may read the data files of training model 834 and implement a neural network with node connections, layers, and weights based on the model structure or form specified in training model 834.
The machine learning application 830 also includes a training model 834. In some implementations, the training model may include one or more model forms or structures. In some implementations, the model form or structure may include any type of neural network, such as a linear network, a deep neural network implementing multiple layers (e.g., a "hidden layer" between an input layer and an output layer, each layer being a linear network), a convolutional neural network (e.g., a network that segments or partitions input data into multiple portions or tiles (tiles), each tile is processed separately using one or more neural network layers, and the results from the processing of each tile are aggregated), a sequence-to-sequence neural network (e.g., a network that takes sequence data as input, such as words in a sentence, frames in a video, etc., and produces a sequence of results as output), and so forth. The model form or structure may specify the connections between the various nodes and the organization of the nodes to the layers. In some implementations, a node of a first layer (e.g., an input layer) can receive data as input data 832 or application data 814. For example, when using a training model for image analysis, such data may include, for example, one or more pixels per node. The subsequent middle tier may receive as input the output of the nodes of the previous tier in accordance with the connectivity specified in the model form or structure. These layers may also be referred to as hidden layers. The last layer (e.g., output layer) produces the output of the machine learning application. For example, depending on the particular training model, the output may be a set of labels for the image, a representation of the image that allows the image to be compared to other images (e.g., feature vectors for the image), an output sentence that is responsive to the input sentence, one or more categories of input data, and so forth. In some implementations, the model form or structure also specifies the number and/or type of nodes in each layer.
In various embodiments, the training model 834 may comprise a plurality of nodes arranged in layers according to a model structure or form. In some embodiments, a node may be a computational node without memory, e.g., configured to process an input of one unit to produce an output of one unit. The computation performed by the node may include, for example, multiplying each of a plurality of node inputs by a weight, obtaining a weighted sum, and adjusting the weighted sum with a bias or intercept value (INTERCEPT VALUE) to produce a node output. In some implementations, the computation performed by the node may further include applying a ladder/activation function to the adjusted weighted sum. In some implementations, the step/activate function may be a nonlinear function. In various embodiments, such computation may include operations such as matrix multiplication. In some implementations, for example, the computation of multiple nodes may be performed in parallel using multiple processor cores of a multi-core processor, a separate processing unit using a GPU, or dedicated neural circuitry. In some implementations, a node may include memory, for example, may be able to store and use one or more earlier inputs when processing subsequent inputs. In some embodiments, the node with Memory may include a Long Short-term Memory (LSTM) node. LSTM nodes may use memory to maintain a "state" that allows the node to behave as a finite state machine (FINITE STATE MACHINE, FSM). Models with such nodes may be used to process sequence data, such as words in sentences or paragraphs, frames in video, speech or other audio, and so forth.
In some implementations, the training model 834 can include embeddings or weights for individual nodes. In some implementations, a model may be initialized to a plurality of nodes organized into layers specified by the model form or structure. At initialization, a corresponding weight may be applied to the connections between each pair of nodes connected in a model form, e.g., nodes in successive layers of a neural network. In some embodiments, the corresponding weights may be randomly assigned or initialized to default values. The model may then be trained, for example, using data 832 to produce results.
In some implementations, training may include applying supervised learning techniques. In supervised learning, training data may include a plurality of inputs (e.g., a set of images) and a corresponding expected output for each input (e.g., one or more labels for each image). The value of the weight is automatically adjusted based on a comparison of the model's output to the expected output, e.g., in a manner that increases the probability that the model will produce the expected output when similar inputs are provided.
In some implementations, training may include applying unsupervised learning techniques. In unsupervised learning, only input data may be provided and a model may be trained to distinguish the data, e.g., to cluster the input data into groups, where each group includes input data that are similar in some way. In some implementations, the model can be trained to distinguish images such that the model distinguishes abstract images (e.g., composite images, artificial images, etc.) from natural images (e.g., photographs).
In some implementations, a model trained using unsupervised learning may cluster words based on the use of words in the input sentence. In some implementations, unsupervised learning may be used to generate a knowledge representation, which may be used by machine learning application 830, for example. In various embodiments, the training model includes a set of weights or embeddings corresponding to the model structure. In embodiments where the data 832 is omitted, the machine learning application 830 may include a training model 834 based on prior training, for example, by a developer of the machine learning application 830, by a third party, or the like. In some implementations, the training model 834 may include a fixed set of weights, e.g., downloaded from a server that provides the weights.
The machine learning application 830 also includes an inference engine 836. The inference engine 836 is configured to apply a training model 834 to data, such as application data 814, to provide inference. In some implementations, the inference engine 836 can include software code to be run by the processor 802. In some implementations, the inference engine 836 may specify a circuit configuration (e.g., for a programmable processor, for a Field Programmable Gate Array (FPGA), etc.) that enables the processor 802 to apply a training model. In some implementations, the inference engine 836 may include software instructions, hardware instructions, or a combination. In some implementations, the inference engine 836 can provide an application programming interface (Application Programming Interface, API) that can be used by the operating system 808 and/or other applications 812 to invoke the inference engine 836, for example, to apply the training model 834 to the application data 814 to generate inferences.
The machine learning application 830 may provide several technical advantages. For example, when training model 834 is generated based on unsupervised learning, training model 834 may be applied by inference engine 836 to generate a knowledge representation (e.g., a digital representation) from input data such as application data 814. For example, a training model for image analysis may produce a representation of an image having a smaller data size (e.g., 1 KB) than the input image (e.g., 10 MB). In some implementations, such representations may help reduce processing costs (e.g., computational costs, memory usage, etc.) of generating output (e.g., tags, classifications, sentences describing images, etc.). In some implementations, such representations may be provided as inputs to different machine learning applications that produce outputs from the outputs of inference engine 836. In some implementations, the knowledge representation generated by the machine learning application 830 may be provided to different devices for further processing, e.g., over a network. In such embodiments, providing a knowledge representation rather than an image may provide technical advantages, such as faster data transfer at reduced cost. In another example, a model trained to cluster documents may generate document clusters from input documents. Document clustering may be suitable for further processing (e.g., determining whether a document is related to a topic, determining a classification category of a document, etc.) without accessing the original document, thus saving computational costs.
In some implementations, the machine learning application 830 may be implemented in an offline manner. In these embodiments, training model 834 may be generated in a first stage and provided as part of machine learning application 830. In some implementations, the machine learning application 830 may be implemented in an online manner. In some of these embodiments, the application (e.g., operating system 808, one or more other applications 812) invoking the machine learning application 830 may utilize the reasoning generated by the machine learning application 830, e.g., to provide the reasoning to the user, and may generate a system log (e.g., which is an action taken by the user based on the reasoning if allowed by the user, or which is the result of further processing if used as input to further processing). The system log may be generated periodically, e.g., hourly, monthly, quarterly, etc., and may be used to update the training model 834, e.g., update the embedding of the training model 834, if permitted by the user.
In some implementations, the machine learning application 830 may be implemented in a manner that can accommodate the particular configuration of the device 800 on which the machine learning application 830 is running. In some implementations, the machine learning application 830 can determine a computational graph that utilizes available computing resources, such as the processor 802. In some implementations, if the machine learning application 830 is implemented as a distributed application across multiple devices, the machine learning application 830 may determine the computations to be performed on the individual devices in a manner that optimizes the computations. In some implementations, the machine learning application 830 may determine that the processor 802 includes a GPU having a particular number (e.g., 1000) of GPU cores, and implement the inference engine accordingly (e.g., as 1000 separate processes or threads).
In some implementations, the machine learning application 830 can implement an ensemble of training models. In some implementations, the training model 834 may include multiple training models, each adapted for the same input data. In these embodiments, the machine learning application 830 may select a particular training model based on, for example, available computing resources, success rates of previous inferences, and the like. In some implementations, the machine learning application 830 can run an inference engine 836 such that multiple training models are applied. In these embodiments, the machine learning application 830 may combine the outputs from the various models of the application, for example, using voting techniques that score the various outputs of each training model of the application, or by selecting one or more particular outputs. Further, in these embodiments, the machine learning application may apply a time threshold (e.g., 0.5 ms) to each training model of the application and utilize only those individual outputs that are available within the time threshold. The output that was not received within the time threshold may not be utilized (e.g., discarded). In some implementations, such an approach may be appropriate when the machine learning application is invoked, for example, by the operating system 808 or one or more other applications 812, when a time limit is specified.
In different implementations, the machine learning application 830 may generate different types of outputs. In some implementations, the machine learning application 830 can provide representations or clusters (e.g., digital representations of input data), tags (e.g., for input data including images, documents, etc.), phrases or sentences (e.g., describing images or videos, suitable for use as responses to input sentences, etc.), images (e.g., generated by the machine learning application in response to input), audio or video (e.g., in response to input video, the machine learning application 830 can generate output video with particular application effects, e.g., when training the training model 834 using training data from a comic book or particular artist, etc., the output video is presented in the style of a comic book or particular artist). In some implementations, the machine learning application 830 can generate the output based on a format specified by a calling application, such as the operating system 808 or one or more other applications 812. In some implementations, the calling application may be another machine learning application. In some implementations, such a configuration may be used for a generated countermeasure network in which the machine learning application is invoked using output training from the machine learning application 830, and vice versa.
Any software in the memory 804 may alternatively be stored on any other suitable storage location or computer readable medium. In addition, the memory 804 (and/or other connected storage devices) may store one or more messages, one or more classification criteria, electronic encyclopedias, dictionaries, knowledge bases, message data, grammars, user preferences, and/or other instructions and data used in the features described herein. Memory 804 and any other type of storage (magnetic disk, optical disk, magnetic tape, or other tangible medium) may be considered a "storage" or "storage device.
The I/O interface 806 may provide functionality to interface the server device 800 with other systems and devices. The interface device may be included as part of device 800 or may be separate from and in communication with device 800. In some implementations, network communication devices, storage devices (e.g., memory and/or database 106), and input/output devices may communicate through I/O interface 806. In some implementations, the I/O interface may be connected to interface devices such as input devices (keyboard, pointing device, touch screen, microphone, camera, scanner, sensor, etc.) and/or output devices (display device, speaker device, printer, motor, etc.).
Some examples of interface devices that may be connected to the I/O interface 806 may include one or more display devices 820 that may be used to display, for example, images, video, and/or a user interface that outputs an application as described herein. Display device 820 may be connected to device 800 via a local connection (e.g., a display bus) and/or via a network connection, and may be any suitable display device. Display device 820 may include any suitable display device, such as an LCD, LED, or plasma display screen, CRT, television, monitor, touch screen, 3D display screen, or other visual display device. In some implementations, the display device 820 may be a flat display screen provided on a mobile device, multiple display screens provided in a goggle or headset device, or a monitor screen for a computer device.
The I/O interface 806 may be coupled to other input and output devices. Some examples include one or more cameras that may capture images. Some embodiments may provide a microphone for capturing sound (e.g., as part of capturing an image, voice commands, etc.), an audio speaker device for outputting sound, or other input and output devices.
For ease of illustration, FIG. 8 shows one block for each of the processor 802, memory 804, I/O interface 806, and shows software blocks 808, 812, and 830. These blocks may represent one or more processors or processing circuits, operating systems, memory, I/O interfaces, applications, and/or software modules. In other embodiments, device 800 may not have all of the components shown and/or may have other elements including other types of elements in place of or in addition to those shown herein. Although some components are described as performing the blocks and operations as described in some embodiments herein, any suitable component or combination of components of network environment 100, device 800, a similar system, or any suitable processor associated with such a system may perform the described blocks and operations.
The methods described herein may be implemented by computer program instructions or code that may run on a computer. In some implementations, the code may be implemented by one or more digital processors (e.g., microprocessors or other processing circuits) and may be stored on a computer program product comprising a non-transitory computer readable medium (e.g., a storage medium), such as magnetic, optical, electromagnetic, or semiconductor storage media including semiconductor or solid state memory, magnetic tape, removable computer diskette, random Access Memory (RAM), read-only memory (ROM), flash memory, rigid magnetic disk, optical disk, solid state memory drive, and the like. The program instructions may also be embodied in or provided as electronic signals, for example, in the form of Software as a service (SaaS) transmitted from a server (e.g., a distributed system and/or cloud computing system). Alternatively, one or more of the methods may be implemented in hardware (logic gates, etc.) or a combination of hardware and software. Example hardware may be a programmable processor (e.g., a Field Programmable Gate Array (FPGA), a complex programmable logic device), a general purpose processor, a graphics processor, an Application Specific Integrated Circuit (ASIC), etc. One or more methods may be performed as part or component of an application running on a system or as an application or software running with other applications and operating systems.
Although described with respect to particular embodiments thereof, these particular embodiments are merely illustrative and not limiting. The concepts illustrated in the examples may be applied to other examples and implementations.
In the case where certain embodiments discussed herein may collect or use personal information about a user (e.g., user data, information about a user's social network, a user's location and time at the location, biometric information of the user, activities and demographic information of the user), the user is provided with one or more opportunities to control whether information is collected, whether personal information is stored, whether personal information is used, and how information about the user is collected, stored, and used. That is, the systems and methods discussed herein collect, store, and/or use user personal information, particularly upon receipt of explicit authorization from an associated user. In some implementations, a user is provided with control over whether a program or feature gathers user information about that particular user or other users related to the program or feature. Each user whose personal information is to be collected is presented with one or more options to allow control of the collection of information related to that user to provide permissions or authorizations as to whether information is to be collected and as to which portions of the information are to be collected. In some embodiments, one or more such control options may be provided to the user over a communications network. In addition, certain data may be processed in one or more ways prior to storage or use to remove personally identifiable information. In some implementations, the identity of the user may be processed such that personally identifiable information cannot be determined. In some implementations, the geographic location of the user may be generalized to a larger region such that the particular location of the user cannot be determined.
Note that the functional blocks, operations, features, methods, devices, and systems described in this disclosure may be integrated or separated into different combinations of systems, devices, and functional blocks as known to those of skill in the art. The routines of the particular embodiments may be implemented using any suitable programming language and programming technique. Different programming techniques may be employed, such as, for example, a program or object-oriented. The routines may run on a single processing device or multiple processors. Although steps, operations, or computations may be presented in a specific order, the order may be changed in different specific implementations. In some embodiments, a plurality of steps or operations shown in sequence in the present specification may be performed simultaneously.
Claims (24)
1.A computer-implemented automatic image sharing method, comprising:
Determining and causing to be displayed to a first user a plurality of shared standard images in a user interface on a first device, wherein each image of the plurality of shared standard images depicts a different person and the plurality of shared standard images does not depict a repeated face of any person, wherein determining the plurality of shared standard images comprises generating the plurality of shared standard images from an image library associated with the first user by editing images;
receiving a selection of one or more particular images of the plurality of shared standard images based on user input received at the first device;
determining one or more person identifiers indicative of one or more particular persons depicted in the selected one or more particular images, wherein each of the one or more person identifiers is designated as a person sharing criteria;
Obtaining a first image associated with the first user from the image library associated with the first user, wherein the first image is not included in the plurality of shared standard images;
programmatically analyzing the first image to determine that the first image depicts the particular person matching the person sharing criteria; and
Updating access permissions to the first image in the image library associated with the first user to grant access rights to the first image to a second user of a second device over a communications network based on determining that the first image depicts the particular person;
Wherein the person sharing criteria includes facial criteria specifying a particular facial identity, and wherein programmatically analyzing the first image includes:
An image classifier is applied to determine that the first image depicts a face of the particular facial identity, wherein the image classifier is applied based on pixel values of pixels of the first image.
2. The computer-implemented method of claim 1, wherein generating the plurality of shared standard images comprises:
Selecting a plurality of images from the library of images depicting different faces, wherein for each face of the selected plurality of images a respective one of the plurality of shared standard images is selected; and
Clipping at least one of the selected plurality of images in the image library to remove image areas other than portions depicting only faces, wherein at least one clipped image is provided as at least one of the plurality of shared standard images.
3. The computer-implemented method of claim 1, wherein updating the access permissions for the first image is based on determining a confidence level that the first image depicts the particular person, wherein the confidence level meets a first threshold.
4. The computer-implemented method of claim 3, further comprising:
Determining that the confidence level does not meet the first threshold and meets a second threshold, the second threshold indicating a lower confidence that the first image depicts the particular person; and
In response to determining that the confidence level meets the second threshold, causing the first image to be displayed on the first device to be selectable by further user input from the first user to the first device to update the access permissions to the first image.
5. The computer-implemented method of claim 1, wherein updating the access permissions for the first image includes automatically adding the first image to an image library of the second user.
6. The computer-implemented method of claim 5, wherein adding the first image to the image repository of the second user comprises adding the first image to an image album in the image repository of the second user, wherein the image album is designated to store images that match the people sharing criteria.
7. The computer-implemented method of claim 1, wherein image data of the first image is stored on a server device in communication with the first device over the communication network, and wherein a first data pointer relating to the image data of the first image is stored on the first device, wherein updating the access permissions for the first image comprises sending a second data pointer relating to the image data of the first image to the second device over the communication network.
8. The computer-implemented method of claim 1, wherein programmatically analyzing the first image comprises determining whether the first image meets one or more additional sharing criteria.
9. The computer-implemented method of claim 8, wherein the one or more additional sharing criteria include an image feature criteria specifying an image content feature, and wherein programmatically analyzing the first image further comprises:
Applying an image classifier to determine a feature vector comprising a plurality of content features of the first image, wherein the feature vector is based on pixel values of pixels of the first image; and
The feature vector is compared to the image feature criteria to determine whether the first image matches the image feature criteria.
10. The computer-implemented method of claim 8, wherein the one or more additional sharing criteria comprise a time criterion, and wherein programmatically analyzing the first image further comprises:
Determining a creation time of the first image based on timestamp metadata associated with the first image; and
The creation time is compared to the time criterion to determine whether the first image meets the time criterion.
11. The computer-implemented method of claim 8, wherein the one or more additional sharing criteria include a location criteria, wherein the location criteria specifies a particular location, and wherein programmatically analyzing the first image further comprises:
Determining a location associated with the first image based on one or more of: location metadata associated with the first image or detected content features as depicted in one or more pixels of the first image; and
The location associated with the first image is compared to the location criteria to determine whether the first image matches the particular location.
12. The computer-implemented method of claim 1, further comprising:
in response to determining that the first image depicts the particular person matching the person sharing criteria,
Initiating a delay period for the first image associated with the first user;
Assigning a status indicator to the first image, wherein the status indicator indicates a first status that the first image is to share with the second user;
Determining whether a second user input from the first user is received at the first device during the delay period, the second user input indicating that the first image is not to be shared with the second user; and
If it is determined that the second user input is received, updating the status indicator to a second status indicating that the first image is not to be shared with the second user,
Wherein updating the access permissions of the first image to grant access to the first image to the second user is in response to expiration of the delay period and the status indicator indicating the first status.
13. A computer-implemented automatic image sharing method, comprising:
Determining a plurality of save standard images from an image library associated with a first user, wherein each image of the plurality of save standard images depicts a different person that is not depicted in other images of the plurality of save standard images, and the plurality of save standard images does not depict a duplicate face of any person, wherein determining the plurality of save standard images comprises generating the plurality of save standard images by editing a plurality of images in the image library associated with the first user;
causing the plurality of save standard images to be displayed in a user interface on a user device of the first user;
Receiving a selection of one or more particular images of the plurality of saved standard images based on user input received by the user device;
Determining one or more person identifiers indicative of one or more particular persons depicted in the selected one or more particular images, wherein each of the one or more person identifiers is designated as a person preservation criteria and included in stored automatic preservation criteria associated with the first user;
receiving, over a communication network, an indication that the first user has been granted access to a first image, the first image being associated with a different user and provided in a stored different image repository associated with the different user, wherein the first image is not included in the image repository associated with the first user, wherein the indication includes access permissions to the first image;
Obtaining the first image and the stored automatic save criteria associated with the first user in response to receiving the indication that the first user has been granted access to the first image;
Programmatically analyzing the first image to determine if the first image depicts a person matching one of the one or more person identifiers in the automated preservation criteria; and
In response to determining that the first image depicts the person matching one of the one or more person identifiers, automatically adding the first image to the image library associated with the first user without user input;
The person preservation criteria includes face preservation criteria specifying a particular face identification, and wherein programmatically analyzing the first image comprises: an image classifier is applied to determine that the first image depicts a face that matches the particular face identification, wherein the image classifier is applied based on pixel values of pixels of the first image.
14. The method of claim 13, wherein each image of the plurality of stored standard images is associated with a different person identifier.
15. The computer-implemented method of claim 14, wherein each image of the plurality of stored standard images is selected from a different image cluster from a plurality of image clusters, wherein each image cluster comprises one or more images depicting a corresponding unique person.
16. The computer-implemented method of claim 13, wherein automatically adding the first image to the image library comprises storing a data pointer on the user device, wherein the data pointer points to first image data stored on a server device, wherein the first image data is associated with the first image.
17. The computer-implemented method of claim 13, wherein the automatic preservation criteria comprises one or more additional preservation criteria, wherein the additional preservation criteria comprises one or more of: a location preservation criterion specifying a location indicated by user input received at the user device; or a time-based preservation criterion specifying a time period indicated by user input received at the user device, and further comprising:
determining whether metadata of the first image meets the one or more additional preservation criteria,
Wherein automatically adding the first image to the image library is in response to determining that the first image depicts the person matching one of the one or more person identifiers and that the metadata of the first image meets the one or more additional preservation criteria.
18. The computer-implemented method of claim 13, wherein generating the plurality of save standard images comprises:
Selecting a plurality of images from the library of images depicting different faces, wherein a respective one of the plurality of preservation standard images is selected for each face of the selected plurality of images; and
Clipping at least one of the selected plurality of images in the image library to remove image areas other than portions depicting only faces, wherein at least one clipped image is provided as at least one of the plurality of saved standard images.
19. The computer-implemented method of claim 13, further comprising:
Causing display, by the user device, of a first user interface element, wherein the first user interface includes selectable options to indicate acceptance of access rights to one or more images associated with the different user; and
Receiving user input from the first user at the first user interface element, the user input indicating acceptance of the access rights to the one or more images,
Wherein selecting the plurality of save standard images from the image library and causing the plurality of save standard images to be displayed is performed in response to receiving the user input indicating acceptance of the rights.
20. The computer-implemented method of claim 19, further comprising:
In response to receiving the user input indicating the acceptance, a second user interface is caused to be displayed, the second user interface enabling the first user to update access permissions for one or more images in the image library associated with the first user to grant access rights to the one or more images to the different user.
21. A computer program product comprising portions of program code which, when executed by a computing device, cause the computing device to perform operations comprising:
Determining and causing to be displayed to a first user a plurality of shared standard images in a user interface on a first device, wherein each image of the plurality of shared standard images depicts a different person and the plurality of shared standard images does not depict a repeated face of any person, wherein determining the plurality of shared standard images comprises generating the plurality of shared standard images from an image library associated with the first user by editing images;
Receiving a selection of one or more particular images of the plurality of shared standard images based on user input received by the first device;
determining one or more person identifiers indicative of one or more particular persons depicted in the selected one or more particular images, wherein each of the one or more person identifiers is designated as a person sharing criteria;
Obtaining a first image associated with the first user from the image library associated with the first user, wherein the first image is not included in the plurality of shared standard images;
programmatically analyzing the first image to determine that the first image depicts the particular person matching the person sharing criteria; and
Updating access permissions to the first image in the image library associated with the first user to grant access rights to the first image to a second user of a second device over a communications network based on determining that the first image depicts the particular person;
Wherein the person sharing criteria includes facial criteria specifying a particular facial identity, and wherein programmatically analyzing the first image includes:
An image classifier is applied to determine that the first image depicts a face of the particular facial identity, wherein the image classifier is applied based on pixel values of pixels of the first image.
22. A computer program product comprising portions of program code which, when executed by a computing device, cause the computing device to perform operations comprising:
Determining a plurality of save standard images from an image library associated with a first user, wherein each image of the plurality of save standard images depicts a different person that is not depicted in other images of the plurality of save standard images, and the plurality of save standard images does not depict a duplicate face of any person, wherein determining the plurality of save standard images comprises generating the plurality of save standard images by editing a plurality of images in the image library associated with the first user;
causing the plurality of save standard images to be displayed in a user interface on a user device of the first user;
Receiving a selection of one or more particular images of the plurality of saved standard images based on user input received by the user device;
Determining one or more person identifiers indicative of one or more particular persons depicted in the selected one or more particular images, wherein each of the one or more person identifiers is designated as a person preservation criteria and included in stored automatic preservation criteria associated with the first user;
receiving, over a communication network, an indication that the first user has been granted access to a first image, the first image being associated with a different user and provided in a stored different image repository associated with the different user, wherein the first image is not included in the image repository associated with the first user, wherein the indication includes access permissions to the first image;
Obtaining the first image and the stored automatic save criteria associated with the first user in response to receiving the indication that the first user has been granted access to the first image;
Programmatically analyzing the first image to determine if the first image depicts a person matching one of the one or more person identifiers in the automated preservation criteria; and
In response to determining that the first image depicts the person matching one of the one or more person identifiers, automatically adding the first image to the image library associated with the first user without user input;
wherein the person preservation criteria includes face preservation criteria that specify a particular face identification, and wherein programmatically analyzing the first image includes: an image classifier is applied to determine that the first image depicts a face of the particular facial identity, wherein the image classifier is applied based on pixel values of pixels of the first image.
23. An automated image sharing system, comprising:
A storage device; and
At least one processor configured to access the storage device and configured to perform operations comprising:
Determining and causing to be displayed to a first user a plurality of shared standard images in a user interface on a first device, wherein each image of the plurality of shared standard images depicts a different person and the plurality of shared standard images does not depict a repeated face of any person, wherein determining the plurality of shared standard images comprises generating the plurality of shared standard images from an image library associated with the first user by editing an image;
Receiving a selection of one or more particular images of the plurality of shared standard images based on user input received by the first device;
determining one or more person identifiers indicative of one or more particular persons depicted in the selected one or more particular images, wherein each of the one or more person identifiers is designated as a person sharing criteria;
Obtaining a first image associated with the first user from the image library associated with the first user, wherein the first image is not included in the plurality of shared standard images;
programmatically analyzing the first image to determine that the first image depicts the particular person matching the person sharing criteria; and
Updating access permissions to the first image in the image library associated with the first user to grant access rights to the first image to a second user of a second device over a communications network based on determining that the first image depicts the particular person;
Wherein the person sharing criteria includes facial criteria specifying a particular facial identity, and wherein programmatically analyzing the first image includes:
An image classifier is applied to determine that the first image depicts a face of the particular facial identity, wherein the image classifier is applied based on pixel values of pixels of the first image.
24. An automated image sharing system, comprising:
A storage device; and
At least one processor configured to access the storage device and configured to perform operations comprising:
determining a plurality of save standard images from an image library associated with a first user, wherein each image of the plurality of save standard images depicts a different person that is not depicted in other images of the plurality of save standard images, and the plurality of save standard images does not depict a duplicate face of any person, wherein determining the plurality of save standard images comprises generating the plurality of save standard images from the image library associated with the first user by editing an image;
causing the plurality of save standard images to be displayed in a user interface on a user device of the first user;
Receiving a selection of one or more particular images of the plurality of saved standard images based on user input received by the user device;
Determining one or more person identifiers indicative of one or more particular persons depicted in the selected one or more particular images, wherein each of the one or more person identifiers is designated as a person preservation criteria and included in stored automatic preservation criteria associated with the first user;
receiving, over a communication network, an indication that the first user has been granted access to a first image, the first image being associated with a different user and provided in a stored different image repository associated with the different user, wherein the first image is not included in the image repository associated with the first user, wherein the indication includes access permissions to the first image;
Obtaining the first image and the stored automatic save criteria associated with the first user in response to receiving the indication that the first user has been granted access to the first image;
Programmatically analyzing the first image to determine if the first image depicts a person matching one of the one or more person identifiers in the automated preservation criteria; and
In response to determining that the first image depicts the person matching one of the one or more person identifiers, automatically adding the first image to the image library associated with the first user without user input;
The person preservation criteria includes face preservation criteria specifying a particular face identification, and wherein programmatically analyzing the first image comprises:
An image classifier is applied to determine that the first image depicts a face that matches the particular face identification, wherein the image classifier is applied based on pixel values of pixels of the first image.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201762507756P | 2017-05-17 | 2017-05-17 | |
US62/507,756 | 2017-05-17 | ||
PCT/US2018/018447 WO2018212815A1 (en) | 2017-05-17 | 2018-02-15 | Automatic image sharing with designated users over a communication network |
Publications (2)
Publication Number | Publication Date |
---|---|
CN110770717A CN110770717A (en) | 2020-02-07 |
CN110770717B true CN110770717B (en) | 2024-04-16 |
Family
ID=61599583
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201880012338.5A Active CN110770717B (en) | 2017-05-17 | 2018-02-15 | Automatic image sharing with designated users over a communication network |
Country Status (4)
Country | Link |
---|---|
US (4) | US10432728B2 (en) |
EP (1) | EP3568787B1 (en) |
CN (1) | CN110770717B (en) |
WO (1) | WO2018212815A1 (en) |
Families Citing this family (38)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN103338256B (en) * | 2013-06-28 | 2015-09-23 | 腾讯科技（深圳）有限公司 | Image sharing method, device, server and system |
EP3322149B1 (en) * | 2016-11-10 | 2023-09-13 | Tata Consultancy Services Limited | Customized map generation with real time messages and locations from concurrent users |
WO2018212599A1 (en) * | 2017-05-17 | 2018-11-22 | Samsung Electronics Co., Ltd. | Super-resolution processing method for moving image and image processing apparatus therefor |
US10922354B2 (en) * | 2017-06-04 | 2021-02-16 | Apple Inc. | Reduction of unverified entity identities in a media library |
US10728343B2 (en) * | 2018-02-06 | 2020-07-28 | Citrix Systems, Inc. | Computing system providing cloud-based user profile management for virtual sessions and related methods |
US10217029B1 (en) * | 2018-02-26 | 2019-02-26 | Ringcentral, Inc. | Systems and methods for automatically generating headshots from a plurality of still images |
KR102185369B1 (en) * | 2018-03-28 | 2020-12-01 | 삼성전자주식회사 | System and mehtod for generating information for conversation with user |
US10957110B2 (en) * | 2018-04-17 | 2021-03-23 | Edx Technologies, Inc. | Systems, devices, and methods for tracing paths in augmented reality |
US10375432B1 (en) * | 2018-06-05 | 2019-08-06 | Rovi Guides, Inc. | Systems and methods for seamlessly connecting devices based on relationships between the users of the respective devices |
US11068705B2 (en) * | 2018-06-11 | 2021-07-20 | Cisco Technology, Inc. | Vector based object recognition in hybrid cloud |
CN109101801B (en) * | 2018-07-12 | 2021-04-27 | 北京百度网讯科技有限公司 | Method, apparatus, device and computer readable storage medium for identity authentication |
US20200074217A1 (en) * | 2018-08-28 | 2020-03-05 | Sony Corporation | Techniques for providing user notice and selection of duplicate image pruning |
JP7171349B2 (en) * | 2018-09-28 | 2022-11-15 | 富士フイルム株式会社 | Image processing device, image processing method, program and recording medium |
US11029741B2 (en) * | 2018-12-28 | 2021-06-08 | Baidu Usa Llc | Deactivating a display of a smart display device based on a vision-based mechanism |
US10790056B1 (en) | 2019-04-16 | 2020-09-29 | International Medical Solutions, Inc. | Methods and systems for syncing medical images across one or more networks and devices |
CN111753078B (en) * | 2019-07-12 | 2024-02-02 | 北京京东尚科信息技术有限公司 | Image paragraph description generation method, device, medium and electronic equipment |
US11283937B1 (en) * | 2019-08-15 | 2022-03-22 | Ikorongo Technology, LLC | Sharing images based on face matching in a network |
US11557208B2 (en) * | 2019-10-02 | 2023-01-17 | Samsara Networks Inc. | Facial recognition technology for improving motor carrier regulatory compliance |
US11036456B1 (en) * | 2019-10-18 | 2021-06-15 | Splunk Inc. | Control of a display device included in a display grid |
TWI817014B (en) * | 2019-11-25 | 2023-10-01 | 仁寶電腦工業股份有限公司 | Method, system and storage medium for providing a timeline-based graphical user interface |
KR20210067442A (en) * | 2019-11-29 | 2021-06-08 | 엘지전자 주식회사 | Automatic labeling apparatus and method for object recognition |
CN111159587B (en) * | 2019-12-13 | 2023-08-29 | 深圳市思为软件技术有限公司 | User access information processing method and device and terminal equipment |
WO2021141568A1 (en) * | 2020-01-06 | 2021-07-15 | Google Llc | Privacy controls for sharing embeddings for searching and indexing media content |
US11687778B2 (en) | 2020-01-06 | 2023-06-27 | The Research Foundation For The State University Of New York | Fakecatcher: detection of synthetic portrait videos using biological signals |
US11496897B2 (en) * | 2020-02-24 | 2022-11-08 | Citrix Systems, Inc. | Biometric identification of information recipients |
US11665116B2 (en) | 2020-04-27 | 2023-05-30 | Snap Inc. | Invitation media overlays for private collections of media content items |
US11750542B2 (en) | 2020-04-27 | 2023-09-05 | Snap Inc. | Invitation media overlays for shared collections of media content items |
US11775689B2 (en) * | 2020-05-29 | 2023-10-03 | Docusign, Inc. | Integration of pictorial content into secure signature documents |
US11539647B1 (en) | 2020-06-17 | 2022-12-27 | Meta Platforms, Inc. | Message thread media gallery |
CN111857498B (en) * | 2020-06-30 | 2022-05-20 | 维沃移动通信有限公司 | Data interaction method and device and electronic equipment |
US11157723B1 (en) | 2021-02-12 | 2021-10-26 | Samsara Networks lac. | Facial recognition for drivers |
US11875016B2 (en) | 2021-05-17 | 2024-01-16 | Apple Inc. | Devices, methods, and graphical user interfaces for displaying media items shared from distinct applications |
US11941237B2 (en) | 2021-05-17 | 2024-03-26 | Apple Inc. | Devices, methods, and graphical user interfaces for automatically providing shared content to applications |
CN113626626B (en) * | 2021-07-08 | 2023-12-12 | 支付宝（中国）网络技术有限公司 | Photo storage method, device and equipment for sharing by multiple persons |
JP2023028746A (en) * | 2021-08-20 | 2023-03-03 | キヤノン株式会社 | Information processing apparatus, control method for the same, and program |
US11538578B1 (en) | 2021-09-23 | 2022-12-27 | International Medical Solutions, Inc. | Methods and systems for the efficient acquisition, conversion, and display of pathology images |
CN113900754B (en) * | 2021-10-09 | 2023-10-31 | 深圳技德智能科技研究院有限公司 | Shared desktop method, shared desktop device, computer equipment and storage medium |
US20230198989A1 (en) * | 2021-12-16 | 2023-06-22 | Lenovo (United States) Inc. | Context based data sharing |
Citations (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN102025654A (en) * | 2009-09-15 | 2011-04-20 | 联发科技股份有限公司 | Picture sharing methods for portable device |
CN103635892A (en) * | 2011-07-07 | 2014-03-12 | 索尼电脑娱乐美国公司 | Auto-creating groups for sharing photos |
US8798401B1 (en) * | 2012-06-15 | 2014-08-05 | Shutterfly, Inc. | Image sharing with facial recognition models |
WO2014178853A1 (en) * | 2013-04-30 | 2014-11-06 | Hewlett-Packard Development Company, L.P. | Ad-hoc, face-recognition-driven content sharing |
CN104317932A (en) * | 2014-10-31 | 2015-01-28 | 小米科技有限责任公司 | Photo sharing method and device |
CN104834665A (en) * | 2015-02-28 | 2015-08-12 | 小米科技有限责任公司 | Target picture acquiring method and device |
CN105531701A (en) * | 2014-07-04 | 2016-04-27 | 微软技术许可有限责任公司 | Personalized trending image search suggestion |
US9338242B1 (en) * | 2013-09-09 | 2016-05-10 | Amazon Technologies, Inc. | Processes for generating content sharing recommendations |
Family Cites Families (253)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6222939B1 (en) | 1996-06-25 | 2001-04-24 | Eyematic Interfaces, Inc. | Labeled bunch graphs for image analysis |
US6513014B1 (en) | 1996-07-24 | 2003-01-28 | Walker Digital, Llc | Method and apparatus for administering a survey via a television transmission network |
JP4214568B2 (en) | 1998-08-18 | 2009-01-28 | コニカミノルタビジネステクノロジーズ株式会社 | Image data sequential display device, image data sequential display method, and recording medium |
US6697869B1 (en) | 1998-08-24 | 2004-02-24 | Koninklijke Philips Electronics N.V. | Emulation of streaming over the internet in a broadcast application |
US6940998B2 (en) | 2000-02-04 | 2005-09-06 | Cernium, Inc. | System for automated screening of security cameras |
US6963848B1 (en) | 2000-03-02 | 2005-11-08 | Amazon.Com, Inc. | Methods and system of obtaining consumer reviews |
USD451536S1 (en) | 2000-04-28 | 2001-12-04 | Honeywell International Inc. | Symbol set for fireplace control unit |
US6947976B1 (en) | 2000-07-31 | 2005-09-20 | Vindigo, Inc. | System and method for providing location-based and time-based information to a user of a handheld device |
US6421358B1 (en) | 2000-09-15 | 2002-07-16 | Mediaone Group, Inc. | Method and system for delivering a synchronized stream of broadcast event data, live event data, and static event data over a hybrid fiber coax network |
US7099510B2 (en) | 2000-11-29 | 2006-08-29 | Hewlett-Packard Development Company, L.P. | Method and system for object detection in digital images |
US6917703B1 (en) | 2001-02-28 | 2005-07-12 | Nevengineering, Inc. | Method and apparatus for image analysis of a gabor-wavelet transformed image using a neural network |
US7539747B2 (en) | 2001-03-14 | 2009-05-26 | Microsoft Corporation | Schema-based context service |
US20030171930A1 (en) * | 2002-03-07 | 2003-09-11 | Junqua Jean-Claude | Computer telephony system to access secure resources |
US7298960B1 (en) | 2002-05-10 | 2007-11-20 | Microsoft Corporation | Playback diagnostics |
AU2003234588A1 (en) | 2002-05-17 | 2003-12-02 | Flipp Sports | Ergonomic multimedia flipbook |
US8359540B2 (en) | 2002-10-09 | 2013-01-22 | Goldman, Sachs & Co. | Apparatus, methods, and articles of manufacture for constructing and maintaining a calendaring interface |
US20040174434A1 (en) * | 2002-12-18 | 2004-09-09 | Walker Jay S. | Systems and methods for suggesting meta-information to a camera user |
JP3908171B2 (en) | 2003-01-16 | 2007-04-25 | 富士フイルム株式会社 | Image storage method, apparatus, and program |
US20040199514A1 (en) | 2003-04-02 | 2004-10-07 | Ira Rosenblatt | Techniques for facilitating item sharing |
JP2007502092A (en) | 2003-05-16 | 2007-02-01 | ピカサ インコーポレイテッド | Method and system for image sharing over a network |
US7310681B2 (en) | 2003-06-23 | 2007-12-18 | Hewlett-Packard Development Company, L.P. | System and method for modeling the memory state of a streaming media server |
US20050010470A1 (en) | 2003-07-09 | 2005-01-13 | Annette Marino | Collaborative marketing mangement systems |
US7109848B2 (en) | 2003-11-17 | 2006-09-19 | Nokia Corporation | Applications and methods for providing a reminder or an alert to a digital media capture device |
US7685134B2 (en) | 2003-12-31 | 2010-03-23 | Nokia Corporation | Media file sharing, correlation of metadata related to shared media files and assembling shared media file collections |
US20050187943A1 (en) | 2004-02-09 | 2005-08-25 | Nokia Corporation | Representation of media items in a media file management application for use with a digital device |
US8099407B2 (en) | 2004-03-31 | 2012-01-17 | Google Inc. | Methods and systems for processing media files |
US7457823B2 (en) * | 2004-05-02 | 2008-11-25 | Markmonitor Inc. | Methods and systems for analyzing data related to possible online fraud |
US20080201299A1 (en) | 2004-06-30 | 2008-08-21 | Nokia Corporation | Method and System for Managing Metadata |
US7890871B2 (en) | 2004-08-26 | 2011-02-15 | Redlands Technology, Llc | System and method for dynamically generating, maintaining, and growing an online social network |
JP4557658B2 (en) | 2004-09-29 | 2010-10-06 | 富士フイルム株式会社 | Event image disclosure method and system |
US20060112080A1 (en) | 2004-11-23 | 2006-05-25 | Flipclips, Inc. | Converting digital video into a printed format |
US7671902B2 (en) | 2004-12-10 | 2010-03-02 | Making Everlasting Memories, Llc | Image capture and distribution system and method |
US8055743B2 (en) | 2005-01-19 | 2011-11-08 | Siemens Industry, Inc. | System and method for configuring a network after replacing a node |
US20070043583A1 (en) | 2005-03-11 | 2007-02-22 | The Arizona Board Of Regents On Behalf Of Arizona State University | Reward driven online system utilizing user-generated tags as a bridge to suggested links |
US20060252435A1 (en) | 2005-03-18 | 2006-11-09 | Yahoo! Inc. | Enabling application wakeup on a mobile device with a hybrid client |
US7489946B2 (en) | 2005-04-14 | 2009-02-10 | Sudharshan Srinivasan | Cellular phone in form factor of a conventional audio cassette |
US7620902B2 (en) | 2005-04-20 | 2009-11-17 | Microsoft Corporation | Collaboration spaces |
JP5701482B2 (en) | 2005-05-17 | 2015-04-15 | グーグル・インコーポレーテッド | Method and system for enhancing video games and video game systems |
US7512829B2 (en) | 2005-06-09 | 2009-03-31 | Microsoft Corporation | Real time event stream processor to ensure up-to-date and accurate result |
US8370639B2 (en) * | 2005-06-16 | 2013-02-05 | Sensible Vision, Inc. | System and method for providing secure access to an electronic device using continuous facial biometrics |
AU2006272401B2 (en) | 2005-07-22 | 2011-03-31 | Fanvision Entertainment Llc | System and methods for enhancing the experience of spectators attending a live sporting event |
CN100568235C (en) | 2005-09-09 | 2009-12-09 | 国际商业机器公司 | Be used to carry out instant messaging client computer and the method that project is shared |
US8402094B2 (en) | 2006-08-11 | 2013-03-19 | Facebook, Inc. | Providing a newsfeed based on user affinity for entities and monitored actions in a social network environment |
US8127331B2 (en) | 2005-12-20 | 2012-02-28 | Bce Inc. | Method, system and apparatus for conveying personalized content to a viewer |
US8135799B2 (en) | 2006-01-11 | 2012-03-13 | Mekikian Gary C | Electronic media download and distribution using real-time message matching and concatenation |
US20090019902A1 (en) | 2006-02-02 | 2009-01-22 | Richard Baranek | Vehicle anti-theft device |
KR100641791B1 (en) | 2006-02-14 | 2006-11-02 | (주)올라웍스 | Tagging Method and System for Digital Data |
US7844482B1 (en) | 2006-02-28 | 2010-11-30 | Intuit Inc. | Mechanism for collecting feedback from users |
JP2007249821A (en) | 2006-03-17 | 2007-09-27 | Nec Corp | Content sharing system |
WO2007113462A1 (en) | 2006-03-30 | 2007-10-11 | British Telecommunications Public Limited Company | Content processing |
WO2007115224A2 (en) | 2006-03-30 | 2007-10-11 | Sri International | Method and apparatus for annotating media streams |
US7668405B2 (en) | 2006-04-07 | 2010-02-23 | Eastman Kodak Company | Forming connections between image collections |
WO2007122726A1 (en) | 2006-04-21 | 2007-11-01 | Mitsubishi Denki Kabushiki Kaisha | Authenticating server device, terminal device, authenticating system and authenticating method |
US7804426B2 (en) | 2006-05-08 | 2010-09-28 | Drivecam, Inc. | System and method for selective review of event data |
US7962634B2 (en) | 2006-05-15 | 2011-06-14 | Apple Inc. | Submission of metadata content and media content to a media distribution system |
US8015237B2 (en) | 2006-05-15 | 2011-09-06 | Apple Inc. | Processing of metadata content and media content received by a media distribution system |
JP4482941B2 (en) | 2006-05-18 | 2010-06-16 | ブラザー工業株式会社 | Interrogator for RFID tag communication system, RFID tag circuit element, and RFID tag communication system |
JPWO2007135871A1 (en) | 2006-05-23 | 2009-10-01 | コニカミノルタホールディングス株式会社 | Information management system |
US8571580B2 (en) | 2006-06-01 | 2013-10-29 | Loopt Llc. | Displaying the location of individuals on an interactive map display on a mobile communication device |
US20070294177A1 (en) | 2006-06-15 | 2007-12-20 | William Volk | Mobile content publishing system and method |
US8065699B2 (en) | 2006-06-20 | 2011-11-22 | Symantec Corporation | Providing rating information for an event based on user feedback |
US8412773B1 (en) | 2006-06-28 | 2013-04-02 | Insors Integrated Communications | Methods, systems and program products for initiating a process on data network |
JP2008077445A (en) | 2006-09-22 | 2008-04-03 | Fujifilm Corp | Image reproducing device, its control method and its control program |
US7916976B1 (en) | 2006-10-05 | 2011-03-29 | Kedikian Roland H | Facial based image organization and retrieval method |
US20080086368A1 (en) | 2006-10-05 | 2008-04-10 | Google Inc. | Location Based, Content Targeted Online Advertising |
US8190634B2 (en) | 2006-10-10 | 2012-05-29 | Canon Kabushiki Kaisha | Image display controlling apparatus, method of controlling image display, and storage medium |
JP4829762B2 (en) | 2006-12-06 | 2011-12-07 | キヤノン株式会社 | Information processing apparatus, control method therefor, and program |
US7812998B2 (en) | 2006-11-01 | 2010-10-12 | Jason Miers | Method of making an animated flipbook |
US7698660B2 (en) | 2006-11-13 | 2010-04-13 | Microsoft Corporation | Shared space for communicating information |
US20080133697A1 (en) | 2006-12-05 | 2008-06-05 | Palm, Inc. | Auto-blog from a mobile device |
US9665597B2 (en) | 2006-12-05 | 2017-05-30 | Qualcomm Incorporated | Method and system for processing images using time and location filters |
JP5074752B2 (en) | 2006-12-07 | 2012-11-14 | キヤノン株式会社 | Image request method |
US9122645B1 (en) | 2006-12-20 | 2015-09-01 | Qurio Holdings, Inc. | Method and system for tagging within virtual groups |
US20080184139A1 (en) | 2007-01-29 | 2008-07-31 | Brian Robert Stewart | System and method for generating graphical user interfaces and graphical user interface models |
US8599801B2 (en) | 2007-02-01 | 2013-12-03 | Yahoo! Inc. | Collecting implicit information for determining context of event actions |
US20080189175A1 (en) | 2007-02-01 | 2008-08-07 | Ic.Com, Inc. | Method and System for In-Depth Advertising for Interactive Gaming |
US7903904B1 (en) | 2007-02-16 | 2011-03-08 | Loeb Enterprises LLC. | System and method for linking data related to a set of similar images |
US8788529B2 (en) | 2007-02-26 | 2014-07-22 | Microsoft Corp. | Information sharing between images |
JP5164398B2 (en) | 2007-03-08 | 2013-03-21 | キヤノン株式会社 | Information processing apparatus and control method thereof |
US7849481B2 (en) | 2007-03-29 | 2010-12-07 | Verizon Patent And Licensing Inc. | Notification for interactive content |
US8732161B2 (en) | 2007-04-27 | 2014-05-20 | The Regents Of The University Of California | Event based organization and access of digital photos |
BRPI0812392A2 (en) | 2007-06-12 | 2015-07-21 | Facebook Inc | System and methods of accessing and sharing user profile data between social networking website and third party application server |
US7930420B2 (en) | 2007-06-25 | 2011-04-19 | University Of Southern California | Source-based alert when streaming media of live event on computer network is of current interest and related feedback |
JP4931707B2 (en) | 2007-06-26 | 2012-05-16 | ソニー・エリクソン・モバイルコミュニケーションズ株式会社 | Content management system and content management method |
US20090199093A1 (en) | 2007-09-04 | 2009-08-06 | Tridib Chakravarty | Image Capture And Sharing System and Method |
US7917859B1 (en) | 2007-09-21 | 2011-03-29 | Adobe Systems Incorporated | Dynamic user interface elements |
US8700636B2 (en) | 2010-09-16 | 2014-04-15 | Facebook, Inc. | Action clustering for news feeds |
US8385950B1 (en) | 2007-11-09 | 2013-02-26 | Google Inc. | Capturing and automatically uploading media content |
CN101911693A (en) | 2007-12-03 | 2010-12-08 | 诺基亚公司 | Systems and methods for storage of notification messages in ISO base media file format |
KR20100120282A (en) | 2007-12-12 | 2010-11-15 | 구글 인코포레이티드 | User-created content aggregation and sharing |
JP5045413B2 (en) | 2007-12-13 | 2012-10-10 | 日本電気株式会社 | Photo output system |
US20090171873A1 (en) | 2007-12-31 | 2009-07-02 | Microsoft Corporation | Determining the interestingness of content update notifications |
CN104866553A (en) | 2007-12-31 | 2015-08-26 | 应用识别公司 | Method, system, and computer program for identification and sharing of digital images with face signatures |
US20090191902A1 (en) | 2008-01-25 | 2009-07-30 | John Osborne | Text Scripting |
US20090234876A1 (en) | 2008-03-14 | 2009-09-17 | Timothy Schigel | Systems and methods for content sharing |
US8166034B2 (en) | 2008-03-26 | 2012-04-24 | Fujifilm Corporation | Saving device for image sharing, image sharing system, and image sharing method |
JP5080524B2 (en) | 2008-03-26 | 2012-11-21 | 富士フイルム株式会社 | Storage device for image sharing, image sharing and method |
JP2011516966A (en) | 2008-04-02 | 2011-05-26 | グーグル インコーポレイテッド | Method and apparatus for incorporating automatic face recognition in a digital image collection |
US8676001B2 (en) | 2008-05-12 | 2014-03-18 | Google Inc. | Automatic discovery of popular landmarks |
US8352493B2 (en) | 2008-06-04 | 2013-01-08 | Hw Llc | Searchable health events directory |
US8085982B1 (en) | 2008-06-20 | 2011-12-27 | Google Inc. | Object tracking in video with visual constraints |
KR20110043612A (en) | 2008-06-24 | 2011-04-27 | 코닌클리케 필립스 일렉트로닉스 엔.브이. | Image processing |
WO2010028169A2 (en) | 2008-09-05 | 2010-03-11 | Fotonauts, Inc. | Reverse tagging of images in system for managing and sharing digital images |
US8611677B2 (en) | 2008-11-19 | 2013-12-17 | Intellectual Ventures Fund 83 Llc | Method for event-based semantic classification |
US8442922B2 (en) | 2008-12-24 | 2013-05-14 | Strands, Inc. | Sporting event image capture, processing and publication |
US20100169153A1 (en) | 2008-12-26 | 2010-07-01 | Microsoft Corporation | User-Adaptive Recommended Mobile Content |
US8495074B2 (en) | 2008-12-30 | 2013-07-23 | Apple Inc. | Effects application based on object clustering |
KR101233582B1 (en) | 2008-12-31 | 2013-02-15 | 애플 인크. | Method for streaming multimedia data over a non-streaming protocol |
US8194136B1 (en) * | 2009-01-26 | 2012-06-05 | Amazon Technologies, Inc. | Systems and methods for lens characterization |
US8265658B2 (en) | 2009-02-02 | 2012-09-11 | Waldeck Technology, Llc | System and method for automated location-based widgets |
US8363888B2 (en) | 2009-03-18 | 2013-01-29 | Shutterfly, Inc. | Proactive creation of photobooks |
AU2010226395B2 (en) | 2009-03-20 | 2016-10-13 | Ad-Vantage Networks, Llc | Methods and systems for searching, selecting, and displaying content |
US20130124311A1 (en) | 2009-03-23 | 2013-05-16 | Sujai Sivanandan | System and Method for Dynamic Integration of Advertisements in a Virtual Environment |
JP5374209B2 (en) | 2009-03-30 | 2013-12-25 | Ｎｅｃパーソナルコンピュータ株式会社 | Content sharing system, content sharing server and program |
US9424549B2 (en) | 2009-04-09 | 2016-08-23 | New Jersey Institute Of Technology | System and method for facilitating user-generated content relating to social networks |
WO2010126412A1 (en) | 2009-04-28 | 2010-11-04 | Telefonaktiebolaget Lm Ericsson (Publ) | Predicting presence of a mobile user equipment |
US8763140B2 (en) * | 2009-05-20 | 2014-06-24 | Evizone Ip Holdings, Ltd. | Secure workflow and data management facility |
US8868662B2 (en) | 2009-06-10 | 2014-10-21 | Silverpop Systems, Inc. | Methods and systems for tracking shared content |
US20120082378A1 (en) | 2009-06-15 | 2012-04-05 | Koninklijke Philips Electronics N.V. | method and apparatus for selecting a representative image |
JPWO2011001587A1 (en) | 2009-07-01 | 2012-12-10 | 日本電気株式会社 | Content classification device, content classification method, and content classification program |
US8730397B1 (en) | 2009-08-31 | 2014-05-20 | Hewlett-Packard Development Company, L.P. | Providing a photobook of video frame images |
US8510383B2 (en) | 2009-09-14 | 2013-08-13 | Clixtr, Inc. | Method for providing event based media streams |
US8359285B1 (en) | 2009-09-18 | 2013-01-22 | Amazon Technologies, Inc. | Generating item recommendations |
US8396813B2 (en) | 2009-09-22 | 2013-03-12 | Xerox Corporation | Knowledge-based method for using social networking site content in variable data applications |
US20110099199A1 (en) | 2009-10-27 | 2011-04-28 | Thijs Stalenhoef | Method and System of Detecting Events in Image Collections |
JP5473551B2 (en) * | 2009-11-17 | 2014-04-16 | 富士フイルム株式会社 | Auto focus system |
US8571331B2 (en) | 2009-11-30 | 2013-10-29 | Xerox Corporation | Content based image selection for automatic photo album generation |
CA2696345C (en) | 2009-12-04 | 2016-12-20 | 3Pd Inc. | Automated survey system |
US20110138003A1 (en) | 2009-12-07 | 2011-06-09 | Electronics And Telecommunications Research Institute | System and method for providing offline based simple social network service |
US8180146B2 (en) | 2009-12-22 | 2012-05-15 | The Chinese University Of Hong Kong | Method and apparatus for recognizing and localizing landmarks from an image onto a map |
US8862663B2 (en) | 2009-12-27 | 2014-10-14 | At&T Intellectual Property I, L.P. | Method and system for providing a collaborative event-share service |
US20110197200A1 (en) | 2010-02-11 | 2011-08-11 | Garmin Ltd. | Decoding location information in content for use by a native mapping application |
US20110208822A1 (en) * | 2010-02-22 | 2011-08-25 | Yogesh Chunilal Rathod | Method and system for customized, contextual, dynamic and unified communication, zero click advertisement and prospective customers search engine |
US20110211737A1 (en) | 2010-03-01 | 2011-09-01 | Microsoft Corporation | Event Matching in Social Networks |
US9189143B2 (en) | 2010-04-30 | 2015-11-17 | American Teleconferencing Services, Ltd. | Sharing social networking content in a conference user interface |
US20110276513A1 (en) | 2010-05-10 | 2011-11-10 | Avaya Inc. | Method of automatic customer satisfaction monitoring through social media |
US20120123867A1 (en) | 2010-05-11 | 2012-05-17 | Scott Hannan | Location Event Advertising |
WO2011149961A2 (en) | 2010-05-24 | 2011-12-01 | Intersect Ptp, Inc. | Systems and methods for identifying intersections using content metadata |
US20110295667A1 (en) | 2010-06-01 | 2011-12-01 | David Butler | Interactive advertising System |
US20120007995A1 (en) | 2010-07-10 | 2012-01-12 | Meredith Goldia Barrett | Electronic flipbook systems and methods |
US20120016948A1 (en) | 2010-07-15 | 2012-01-19 | Avaya Inc. | Social network activity monitoring and automated reaction |
US8478717B2 (en) | 2010-07-26 | 2013-07-02 | Oracle International Corporation | Enterprise collaboration with reusable content |
US8655854B2 (en) * | 2010-07-27 | 2014-02-18 | Avid Technology, Inc. | Hierarchical multimedia program composition |
US8270684B2 (en) | 2010-07-27 | 2012-09-18 | Google Inc. | Automatic media sharing via shutter click |
US20120030194A1 (en) | 2010-07-29 | 2012-02-02 | Research In Motion Limited | Identification and scheduling of events on a communication device |
US20120038665A1 (en) | 2010-08-14 | 2012-02-16 | H8it Inc. | Systems and methods for graphing user interactions through user generated content |
US8832093B2 (en) | 2010-08-18 | 2014-09-09 | Facebook, Inc. | Dynamic place visibility in geo-social networking system |
US8630494B1 (en) | 2010-09-01 | 2014-01-14 | Ikorongo Technology, LLC | Method and system for sharing image content based on collection proximity |
US20120214568A1 (en) | 2010-09-13 | 2012-08-23 | Herrmann Mark E | Apparatus and method for supporting applications in a distributed network |
KR101700365B1 (en) | 2010-09-17 | 2017-02-14 | 삼성전자주식회사 | Method for providing media-content relation information, device, server, and storage medium thereof |
US9319227B2 (en) | 2010-09-28 | 2016-04-19 | T-Mobile Usa, Inc. | Automatic content creation based on group collaboration spaces |
US20120092685A1 (en) | 2010-10-13 | 2012-04-19 | Meredith Goldia Barrett | Printed flipbook systems and methods |
US20140108547A1 (en) | 2010-10-21 | 2014-04-17 | Bindu Rama Rao | Automated blogging, skills portfolio management and syndication system |
US9143881B2 (en) | 2010-10-25 | 2015-09-22 | At&T Intellectual Property I, L.P. | Providing interactive services to enhance information presentation experiences using wireless technologies |
US8958822B2 (en) | 2010-10-25 | 2015-02-17 | Alohar Mobile Inc. | Determining points of interest of a mobile user |
US8380039B2 (en) | 2010-11-09 | 2013-02-19 | Eastman Kodak Company | Method for aligning different photo streams |
US9009770B2 (en) | 2010-11-11 | 2015-04-14 | Turner Broadcasting System, Inc. | Methods and systems for media consumption |
US20120122554A1 (en) | 2010-11-12 | 2012-05-17 | Disney Enterprises, Inc. | System and method for in-game interactive advertising |
US9253615B2 (en) | 2010-11-30 | 2016-02-02 | Microsoft Technology Licensing, Llc | Event planning within social networks |
US8397982B2 (en) | 2010-12-17 | 2013-03-19 | Motorola Mobility Llc | Method and device for recognition of docking stations |
JP5713340B2 (en) | 2010-12-21 | 2015-05-07 | インターナショナル・ビジネス・マシーンズ・コーポレーションＩｎｔｅｒｎａｔｉｏｎａｌ Ｂｕｓｉｎｅｓｓ Ｍａｃｈｉｎｅｓ Ｃｏｒｐｏｒａｔｉｏｎ | Method for transmitting event notification, and computer and computer program thereof |
US20120213404A1 (en) | 2011-02-18 | 2012-08-23 | Google Inc. | Automatic event recognition and cross-user photo clustering |
US20120221687A1 (en) | 2011-02-27 | 2012-08-30 | Broadcastr, Inc. | Systems, Methods and Apparatus for Providing a Geotagged Media Experience |
US8914483B1 (en) | 2011-03-17 | 2014-12-16 | Google Inc. | System and method for event management and information sharing |
US20120246003A1 (en) | 2011-03-21 | 2012-09-27 | Hart Gregory M | Advertisement Service |
US8831352B2 (en) | 2011-04-04 | 2014-09-09 | Microsoft Corporation | Event determination from photos |
CN102737062B (en) | 2011-04-15 | 2016-08-17 | 腾讯科技（深圳）有限公司 | A kind of good friend's Notification Method and device |
US8918463B2 (en) | 2011-04-29 | 2014-12-23 | Facebook, Inc. | Automated event tagging |
US10509466B1 (en) * | 2011-05-11 | 2019-12-17 | Snap Inc. | Headwear with computer and optical element for use therewith and systems utilizing same |
EP2721566A1 (en) | 2011-06-20 | 2014-04-23 | Giulio Galliani | Promotion via social currency |
US9130763B2 (en) | 2011-06-20 | 2015-09-08 | Microsoft Technology Licensing, Llc | Automatic sharing of event content by linking devices |
US8627096B2 (en) * | 2011-07-14 | 2014-01-07 | Sensible Vision, Inc. | System and method for providing secure access to an electronic device using both a screen gesture and facial biometrics |
US20130030875A1 (en) * | 2011-07-29 | 2013-01-31 | Panasonic Corporation | System and method for site abnormality recording and notification |
US20130027561A1 (en) * | 2011-07-29 | 2013-01-31 | Panasonic Corporation | System and method for improving site operations by detecting abnormalities |
WO2013029235A1 (en) | 2011-08-30 | 2013-03-07 | Nokia Corporation | Method and apparatus for managing the presenting of location-based events |
US8725858B1 (en) | 2011-08-31 | 2014-05-13 | Google Inc. | Method and system for selecting content based on a user's viral score |
US8732255B2 (en) | 2011-09-09 | 2014-05-20 | Facebook, Inc. | Dynamically created shared spaces |
US8437500B1 (en) | 2011-10-19 | 2013-05-07 | Facebook Inc. | Preferred images from captured video sequence |
US9280545B2 (en) | 2011-11-09 | 2016-03-08 | Microsoft Technology Licensing, Llc | Generating and updating event-based playback experiences |
US9143601B2 (en) | 2011-11-09 | 2015-09-22 | Microsoft Technology Licensing, Llc | Event-based media grouping, playback, and sharing |
US8761523B2 (en) | 2011-11-21 | 2014-06-24 | Intellectual Ventures Fund 83 Llc | Group method for making event-related media collection |
WO2013081517A1 (en) * | 2011-12-01 | 2013-06-06 | Telefonaktiebolaget L M Ericsson (Publ) | Method for performing face recognition in a radio access network |
US9942533B2 (en) | 2011-12-02 | 2018-04-10 | Provenance Asset Group Llc | Method and apparatus for generating multi-channel video |
US9256620B2 (en) | 2011-12-20 | 2016-02-09 | Amazon Technologies, Inc. | Techniques for grouping images |
US9111317B2 (en) | 2011-12-21 | 2015-08-18 | Facebook, Inc. | Tagging posted content in a social networking system with media information |
WO2013100898A1 (en) * | 2011-12-27 | 2013-07-04 | Intel Corporation | Turing test based user authentication and user presence verification system, device, and method |
US9519769B2 (en) * | 2012-01-09 | 2016-12-13 | Sensible Vision, Inc. | System and method for disabling secure access to an electronic device using detection of a predetermined device orientation |
CN103593594A (en) * | 2012-01-09 | 2014-02-19 | 明智视觉有限公司 | System and method for providing secure access to an electronic device using facial biometric identification and screen gesture |
USD696266S1 (en) | 2012-01-19 | 2013-12-24 | Pepsico, Inc. | Display screen with graphical user interface |
US8676814B2 (en) * | 2012-02-16 | 2014-03-18 | Yahoo! Inc. | Automatic face annotation of images contained in media content |
US8832264B2 (en) | 2012-03-01 | 2014-09-09 | Justin Pauley | Network appliance for monitoring network requests for multimedia content |
US8963962B2 (en) | 2012-03-06 | 2015-02-24 | Apple Inc. | Display of multiple images |
US20130339180A1 (en) | 2012-03-16 | 2013-12-19 | Ronald Aaron LaPierre | Collection creator and organizer for social media |
US20130305324A1 (en) * | 2012-05-09 | 2013-11-14 | International Business Machines Corporation | Incremental Password Barriers to Prevent Malevolent Intrusions |
US8713606B2 (en) | 2012-05-14 | 2014-04-29 | United Video Properties, Inc. | Systems and methods for generating a user profile based customized media guide with user-generated content and non-user-generated content |
US20130332854A1 (en) | 2012-06-10 | 2013-12-12 | Apple Inc. | Creating image streams and sharing the image streams across different devices |
USD687461S1 (en) | 2012-06-20 | 2013-08-06 | Microsoft Corporation | Display screen with icon |
US9792585B2 (en) | 2012-06-21 | 2017-10-17 | Google Inc. | Mobile application management |
US8542879B1 (en) * | 2012-06-26 | 2013-09-24 | Google Inc. | Facial recognition |
US9391792B2 (en) | 2012-06-27 | 2016-07-12 | Google Inc. | System and method for event content stream |
US8873851B2 (en) | 2012-06-29 | 2014-10-28 | Intellectual Ventures Fund 83 Llc | System for presenting high-interest-level images |
US10394429B2 (en) | 2012-06-29 | 2019-08-27 | Qualcomm Incorporated | Sharing of user interface objects via a shared space |
US20140002644A1 (en) | 2012-06-29 | 2014-01-02 | Elena A. Fedorovskaya | System for modifying images to increase interestingness |
US10567376B2 (en) * | 2012-08-24 | 2020-02-18 | Sensible Vision, Inc. | System and method for providing secure access to an electronic device using multifactor authentication |
US8799756B2 (en) | 2012-09-28 | 2014-08-05 | Interactive Memories, Inc. | Systems and methods for generating autoflow of content based on image and user analysis as well as use case data for a media-based printable product |
US8782158B2 (en) | 2012-10-02 | 2014-07-15 | Tanner Cropper | System for sharing and tracking review of rich content, and methods associated therewith |
US20150262208A1 (en) * | 2012-10-04 | 2015-09-17 | Bernt Erik Bjontegard | Contextually intelligent communication systems and processes |
CN103780652B (en) | 2012-10-23 | 2017-12-01 | 腾讯科技（深圳）有限公司 | A kind of method and system of microblogging resource sharing |
US9418370B2 (en) | 2012-10-23 | 2016-08-16 | Google Inc. | Obtaining event reviews |
US9311310B2 (en) | 2012-10-26 | 2016-04-12 | Google Inc. | System and method for grouping related photographs |
US20140189010A1 (en) | 2012-11-27 | 2014-07-03 | ThymeVine LLC | Scrapbooking |
US10051103B1 (en) | 2013-01-10 | 2018-08-14 | Majen Tech, LLC | Screen interface for a mobile device apparatus |
US9087389B2 (en) | 2013-02-15 | 2015-07-21 | Bank Of America Corporation | Reducing image size at point of capture |
USD729846S1 (en) | 2013-02-23 | 2015-05-19 | Samsung Electronics Co., Ltd. | Display screen or portion thereof with icon |
US20140310351A1 (en) | 2013-04-12 | 2014-10-16 | Damon Danielson | System and method for social networking based on family relationships |
US10218783B2 (en) | 2013-05-13 | 2019-02-26 | Intel Corporation | Media sharing techniques |
USD730381S1 (en) | 2013-06-24 | 2015-05-26 | Tencent Technology (Shenzhen) Company Limited | Portion of a display screen with animated graphical user interface |
CN103338256B (en) | 2013-06-28 | 2015-09-23 | 腾讯科技（深圳）有限公司 | Image sharing method, device, server and system |
KR101997454B1 (en) * | 2013-07-31 | 2019-10-01 | 엘지전자 주식회사 | A mobile device and method of controlling thereof |
US9704137B2 (en) | 2013-09-13 | 2017-07-11 | Box, Inc. | Simultaneous editing/accessing of content by collaborator invitation through a web-based or mobile application to a cloud-based collaboration platform |
US10430986B2 (en) | 2013-10-10 | 2019-10-01 | Pushd, Inc. | Clustering photographs for display on a digital picture frame |
USD750131S1 (en) | 2013-10-11 | 2016-02-23 | Microsoft Corporation | Display screen with transitional graphical user interface |
US9692840B2 (en) | 2013-11-11 | 2017-06-27 | Dropbox, Inc. | Systems and methods for monitoring and applying statistical data related to shareable links associated with content items stored in an online content management service |
US9690910B2 (en) | 2013-11-11 | 2017-06-27 | Dropbox, Inc. | Systems and methods for monitoring and applying statistical data related to shareable links associated with content items stored in an online content management service |
US20150143256A1 (en) | 2013-11-20 | 2015-05-21 | Memoreze LLC | Interface for Interaction with a Compendium by Members of a Group |
US9288283B2 (en) | 2013-12-04 | 2016-03-15 | Dropbox, Inc. | Systems and methods for managing shared content based on sharing profiles |
US10290062B2 (en) | 2013-12-04 | 2019-05-14 | Michael Stewart Shunock | System and method for utilizing annotated images to facilitate interactions between commercial and social users |
US20170116498A1 (en) * | 2013-12-04 | 2017-04-27 | J Tech Solutions, Inc. | Computer device and method executed by the computer device |
US9292756B2 (en) * | 2013-12-10 | 2016-03-22 | Dropbox, Inc. | Systems and methods for automated image cropping |
US20150180980A1 (en) | 2013-12-24 | 2015-06-25 | Dropbox, Inc. | Systems and methods for preserving shared virtual spaces on a content management system |
CA2863124A1 (en) | 2014-01-03 | 2015-07-03 | Investel Capital Corporation | User content sharing system and method with automated external content integration |
WO2015103615A1 (en) * | 2014-01-06 | 2015-07-09 | Yyesit, Llc | Method and apparatus of surveillance system |
US9420017B2 (en) | 2014-01-10 | 2016-08-16 | Kuhoo Edson | Information organization, management, and processing system and methods |
US10885104B2 (en) | 2014-02-27 | 2021-01-05 | Dropbox, Inc. | Systems and methods for selecting content items to store and present locally on a user device |
US8943140B1 (en) | 2014-03-26 | 2015-01-27 | Ankit Dilip Kothari | Assign photographers on an event invite and automate requesting, uploading, and sharing of photos and videos for an event |
USD756377S1 (en) | 2014-04-17 | 2016-05-17 | Google Inc. | Portion of a display panel with an animated computer icon |
AU2015297035B2 (en) | 2014-05-09 | 2018-06-28 | Google Llc | Systems and methods for biomechanically-based eye signals for interacting with real and virtual objects |
USD757790S1 (en) | 2014-05-30 | 2016-05-31 | Microsoft Corporation | Display screen with animated graphical user interface |
USD757749S1 (en) | 2014-05-30 | 2016-05-31 | Microsoft Corporation | Display screen with animated graphical user interface |
US9584874B1 (en) * | 2014-06-16 | 2017-02-28 | Juan José Farías | Portal for collection and distribution of web-based audiovisual content blocks and creation of audience statistics |
US9195912B1 (en) * | 2014-07-24 | 2015-11-24 | National Taipei University Of Technology | Face annotation method and a face annotation system |
WO2016012859A1 (en) * | 2014-07-25 | 2016-01-28 | Snapfile Ltd. | System and method for securely managing integrity-verifiable and authenticable information |
US10127662B1 (en) * | 2014-08-11 | 2018-11-13 | D.R. Systems, Inc. | Systems and user interfaces for automated generation of matching 2D series of medical images and efficient annotation of matching 2D medical images |
US10423495B1 (en) * | 2014-09-08 | 2019-09-24 | Veritas Technologies Llc | Deduplication grouping |
US20160149956A1 (en) * | 2014-11-21 | 2016-05-26 | Whip Networks, Inc. | Media management and sharing system |
CN104484342B (en) | 2014-11-24 | 2018-09-21 | 广州华多网络科技有限公司 | A kind of contact data shared system and method, relevant device |
CN108604383A (en) * | 2015-12-04 | 2018-09-28 | 奇跃公司 | Reposition system and method |
US10204116B2 (en) * | 2016-02-08 | 2019-02-12 | Corelogic Solutions, Llc | Image metadata enrichment generator |
US10298663B2 (en) * | 2016-04-27 | 2019-05-21 | International Business Machines Corporation | Method for associating previously created social media data with an individual or entity |
US20170344900A1 (en) * | 2016-05-24 | 2017-11-30 | Sultan Saad ALZAHRANI | Method and apparatus for automated organization of visual-content media files according to preferences of a user |
US10313470B2 (en) * | 2016-09-06 | 2019-06-04 | Integrated Device Technology, Inc. | Hierarchical caching and analytics |
US10325372B2 (en) * | 2016-12-20 | 2019-06-18 | Amazon Technologies, Inc. | Intelligent auto-cropping of images |
US20180190024A1 (en) * | 2016-12-30 | 2018-07-05 | Therese E. Dugan | Space based correlation to augment user experience |
-
2018
- 2018-02-15 EP EP18709833.0A patent/EP3568787B1/en active Active
- 2018-02-15 WO PCT/US2018/018447 patent/WO2018212815A1/en unknown
- 2018-02-15 US US15/898,182 patent/US10432728B2/en active Active
- 2018-02-15 CN CN201880012338.5A patent/CN110770717B/en active Active
-
2019
- 2019-09-05 US US16/562,166 patent/US11212348B2/en active Active
-
2021
- 2021-12-06 US US17/543,119 patent/US11778028B2/en active Active
-
2023
- 2023-09-14 US US18/368,450 patent/US20240007528A1/en active Pending
Patent Citations (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN102025654A (en) * | 2009-09-15 | 2011-04-20 | 联发科技股份有限公司 | Picture sharing methods for portable device |
CN103635892A (en) * | 2011-07-07 | 2014-03-12 | 索尼电脑娱乐美国公司 | Auto-creating groups for sharing photos |
US8798401B1 (en) * | 2012-06-15 | 2014-08-05 | Shutterfly, Inc. | Image sharing with facial recognition models |
WO2014178853A1 (en) * | 2013-04-30 | 2014-11-06 | Hewlett-Packard Development Company, L.P. | Ad-hoc, face-recognition-driven content sharing |
US9338242B1 (en) * | 2013-09-09 | 2016-05-10 | Amazon Technologies, Inc. | Processes for generating content sharing recommendations |
CN105531701A (en) * | 2014-07-04 | 2016-04-27 | 微软技术许可有限责任公司 | Personalized trending image search suggestion |
CN104317932A (en) * | 2014-10-31 | 2015-01-28 | 小米科技有限责任公司 | Photo sharing method and device |
CN104834665A (en) * | 2015-02-28 | 2015-08-12 | 小米科技有限责任公司 | Target picture acquiring method and device |
Also Published As
Publication number | Publication date |
---|---|
US11778028B2 (en) | 2023-10-03 |
US10432728B2 (en) | 2019-10-01 |
EP3568787B1 (en) | 2024-04-10 |
US11212348B2 (en) | 2021-12-28 |
EP3568787A1 (en) | 2019-11-20 |
US20180337994A1 (en) | 2018-11-22 |
US20220094745A1 (en) | 2022-03-24 |
US20240007528A1 (en) | 2024-01-04 |
WO2018212815A1 (en) | 2018-11-22 |
US20190394276A1 (en) | 2019-12-26 |
CN110770717A (en) | 2020-02-07 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN110770717B (en) | Automatic image sharing with designated users over a communication network | |
US11146520B2 (en) | Sharing images and image albums over a communication network | |
US11829404B2 (en) | Functional image archiving | |
US11209442B2 (en) | Image selection suggestions | |
US20190138656A1 (en) | Systems and methods for providing recommended media content posts in a social networking system | |
KR102437640B1 (en) | Image selection suggestions | |
US11539647B1 (en) | Message thread media gallery | |
KR102589154B1 (en) | Automatic creation of groups of people and image-based creations | |
CN116636190A (en) | Messaging system for redisplaying content items |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |