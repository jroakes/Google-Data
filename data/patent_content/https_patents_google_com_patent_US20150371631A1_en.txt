US20150371631A1 - Caching speech recognition scores - Google Patents
Caching speech recognition scores Download PDFInfo
- Publication number
- US20150371631A1 US20150371631A1 US14/311,557 US201414311557A US2015371631A1 US 20150371631 A1 US20150371631 A1 US 20150371631A1 US 201414311557 A US201414311557 A US 201414311557A US 2015371631 A1 US2015371631 A1 US 2015371631A1
- Authority
- US
- United States
- Prior art keywords
- values
- acoustic model
- model score
- scores
- index value
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/14—Speech classification or search using statistical models, e.g. Hidden Markov Models [HMMs]
- G10L15/142—Hidden Markov Models [HMMs]
- G10L15/148—Duration modelling in HMMs, e.g. semi HMM, segmental models or transition probabilities
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/28—Constructional details of speech recognition systems
- G10L15/285—Memory allocation or algorithm optimisation to reduce hardware requirements
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L19/00—Speech or audio signals analysis-synthesis techniques for redundancy reduction, e.g. in vocoders; Coding or decoding of speech or audio signals, using source filter models or psychoacoustic analysis
- G10L19/02—Speech or audio signals analysis-synthesis techniques for redundancy reduction, e.g. in vocoders; Coding or decoding of speech or audio signals, using source filter models or psychoacoustic analysis using spectral analysis, e.g. transform vocoders or subband vocoders
- G10L19/032—Quantisation or dequantisation of spectral components
- G10L19/038—Vector quantisation, e.g. TwinVQ audio
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L19/00—Speech or audio signals analysis-synthesis techniques for redundancy reduction, e.g. in vocoders; Coding or decoding of speech or audio signals, using source filter models or psychoacoustic analysis
- G10L2019/0001—Codebooks
- G10L2019/0004—Design or structure of the codebook
- G10L2019/0005—Multi-stage vector quantisation
Definitions
- This disclosure relates to caching speech recognition scores.
- Speech recognition systems typically aim to convert a voice input from a user into a natural language transcription.
- Speech recognition systems may use an acoustic model in the process of determining the words that a user has spoken.
- an acoustic model may include statistical information that can be used to estimate which speech sounds occur in audio.
- speech recognition scores such as acoustic model scores
- Information about an utterance such as a set of speech features or data derived from speech features, can be mapped to the stored scores.
- a speech recognition module can use scores from the cache to generate a transcription for the utterance, instead of evaluating a speech recognition model to generate speech recognition scores.
- an utterance is detected at a client device, and a server system performs speech recognition to determine a transcription for the utterance.
- the client device may process detected audio and extract speech features.
- the client device may compress the speech features, for example, using vector quantization to map feature vectors to lower dimensional vectors.
- the client device may then send the compressed data to the server system, allowing the server system to perform speech recognition using the compressed data.
- a method includes receiving one or more values comprising data about an utterance; determining an index value for the one or more values; selecting, from a cache of acoustic model scores that were computed before receiving the one or more values, an acoustic model score for the one or more received values based on the index value; and determining a transcription for the utterance using the selected acoustic model score.
- implementations of this and other aspects include corresponding systems, apparatus, and computer programs, configured to perform the actions of the methods, encoded on computer storage devices.
- a system of one or more computers can be so configured by virtue of software, firmware, hardware, or a combination of them installed on the system that in operation cause the system to perform the actions.
- One or more computer programs can be so configured by virtue of having instructions that, when executed by data processing apparatus, cause the apparatus to perform the actions.
- Implementations may include one or more of the following features.
- generating the index value includes generating a hash value for the one or more values using a locality sensitive hash function.
- Selecting the acoustic model score for the received one or more values includes selecting an acoustic model score previously generated for a set of values that matches the received one or more values.
- Selecting the acoustic model score for the received one or more values includes selecting an acoustic model score previously generated for a set of values that does not exactly match the received one or more values, the acoustic model score approximating an acoustic model score for the received one or more values.
- the one or more values include information derived from a particular speech frame and information derived from contextual speech frames occurring before or after the particular frame. Determining the index value for the one or more values includes determining the hash value for the particular speech frame based on the information derived from a particular speech frame and information derived from the contextual speech frames occurring before or after the particular frame. Selecting the acoustic model score includes selecting an acoustic model score indicating a likelihood that the particular speech frame is an occurrence of a particular phonetic unit in the utterance.
- Receiving the one or more values includes, receiving the one or more values from a client device over a network.
- the transcription is provided to the client device over the network.
- the one or more values include results of compressing speech features for the utterance using vector quantization.
- the speed of speech recognition processing can be increased. Delays in providing speech recognition results can be decreased. The computational requirements for generating transcriptions of speech can be reduced.
- FIG. 1 is a diagram that illustrates an example of a system for performing speech recognition using cached speech recognition scores.
- FIG. 2 is a flow diagram that illustrates an example of a process for using cached speech recognition scores.
- FIG. 3 is a flow diagram that illustrates an example of a process for training a speech recognition model.
- FIG. 4 is a flow diagram that illustrates an example of a process for preparing a cache of speech recognition scores.
- FIG. 5 is a schematic diagram that shows an example of a computing device and a mobile computing device.
- FIG. 1 is a diagram that illustrates an example of a system 100 for performing speech recognition using cached speech recognition scores.
- the system 100 includes a client device 110 , a server system 120 which may include one or more computers, and one or more data storage devices 130 that are in communication with the server system 120 .
- FIG. 1 shows stages (A) to (H) which illustrate a flow of data.
- the client device 110 can be, for example, a desktop computer, laptop computer, a tablet computer, a wearable computer, a cellular phone, a smart phone, a music player, an e-book reader, a navigation system, or any other appropriate computing device.
- the functions performed by the server system 120 can be performed by individual computer systems or can be distributed across multiple computer systems.
- the network 130 can be wired or wireless or a combination of both and can include the Internet.
- Some speech recognition systems allow a client device, such as a mobile phone, to send speech data to a speech recognition server or other back-end system for processing.
- Servers typically have much greater computing resources than client devices, which may allow a server to determine transcriptions more quickly or more accurately than a client device.
- a client device makes an audio recording of a user's speech and sends the recorded audio, e.g., an audio waveform, to a speech recognition server.
- the recorded audio e.g., an audio waveform
- sending recorded audio over a network from a mobile device may cause a user to incur substantial network usage charges.
- a client device computes speech features from detected audio and sends the feature values to a server system instead of sending audio recordings.
- the client device may compress speech feature vectors or other speech data to additionally reduce the overall amount of data that is transferred over a network.
- the client device uses quantization techniques to map speech features to more compact representations. For example, vector quantization can be used to map speech feature vectors to lower dimensional vectors.
- a speech recognition engine may use one or more models to evaluate speech and estimate what a user has spoken. For example, an acoustic model may generate scores indicating likelihoods that a portion of an utterance corresponds to particular speech sounds.
- a language model may generate scores indicating likelihoods of candidate transcriptions of the speech recording, and may be used in conjunction with the acoustic model to produce the overall likelihood that the sounds correspond to particular words and sequences of words. Evaluating a model to produce speech recognition scores can be computationally demanding, and the time required to obtain scores from a model may result in a significant delay in producing a transcription.
- speech recognition scores are obtained from a cache rather than evaluating a model.
- Speech recognition scores may be pre-computed for a variety of potential inputs to a model, and the speech recognition scores may be stored in the cache.
- a speech recognition engine may look up appropriate scores in the cache.
- the cache may be structured as a hash table, and a hash of the data about the utterance may be used to locate appropriate scores.
- the time required to obtain the scores can be much less than the delay to compute scores using a model such as a Gaussian mixture model (GMM) or deep neural network (DNN).
- GMM Gaussian mixture model
- DNN deep neural network
- an acoustic model such as the acoustic model 160 shown in FIG. 1
- frames of the input speech may be assigned the scores that were previously assigned to similar frames in the collection of labeled samples.
- Speech frames may be identified as similar to each other based on, for example, having hash values that are identical or nearby, e.g., within a certain distance, when the hash values are produced using a locality sensitive hashing algorithm.
- Storing and retrieving speech recognition scores can be facilitated by using vector quantization of speech features.
- Vector quantization can be used to map a highly variable set of data to a smaller range of discrete possibilities. For example, a large number of real-valued speech features can be mapped to a small number of integers.
- the vector quantization process greatly reduces the number of unique input combinations that may occur, which makes it more feasible to store a significant portion of the acoustic model scores in a cache.
- the server system 120 receives one or more values comprising data about an utterance.
- the server system 120 determines an index value for the one or more values.
- the server system 120 selects an acoustic model score based on the index value. For example, the acoustic model score is selected from a cache of acoustic model scores that were computed before the server system 120 received the one or more values.
- the server system 120 also determines a transcription for the utterance using the selected acoustic model score.
- the client device 110 generates data about an utterance and sends the data to the server system 120 to obtain a transcription for the utterance.
- the server system 120 uses the data to retrieve speech recognition scores from a cache, and uses the retrieved scores to determine a transcription for the utterance.
- a user 102 speaks an utterance to be transcribed, and the user's utterance is recorded by the client device 110 .
- the client device 110 may use a microphone to detect audio including the utterance, and the client device 110 may record audio waveform data 140 for the detected audio.
- the client device 110 computes speech features 142 for the utterance.
- Speech recognition systems may use only certain components of an audio signal to determine the content of an utterance.
- the speech features 142 omit information from the audio waveform data 140 that the server system 140 would not use during speech recognition.
- the client device 110 computes speech features, e.g., values indicating characteristics of the audio waveform data 140 , that will be useful to speech recognition algorithms.
- the speech features 142 may include, mel filterbank energies, which may be computed by binning energies at different frequency ranges on a logarithmic scale.
- the speech features 142 may include mel frequency cepstral coefficients (MFCCs) produced from filterbank channels.
- MFCCs mel frequency cepstral coefficients
- a set of speech features 142 may be computed for each of various segments of the audio waveform data 140 .
- the audio waveform data 140 may be divided into speech frames that each have a duration of, for example, 25 milliseconds, and a set of speech features may be computed for each speech frame.
- the speech features 142 for a particular speech frame may include contextual information about previous and subsequent values of the feature signal, e.g., information that reflects audio characteristics of previous or subsequent speech frames.
- information about first and second order derivatives of the feature signals may be included in a set of speech features 142 .
- a 13-dimensional MFCC vector for a particular speech frame may be augmented with a 13-dimensional vector representing first order derivatives and another 13-dimensional vector representing second order derivatives, resulting in a 39-dimensional vector.
- the speech features for previous and/or subsequent speech frames may be combined with the speech features computed for the current frame.
- the speech features 142 represent a set of values used to recognize a single speech frame, and the values indicate characteristics of the single speech frame as well as any contextual information to be used when recognizing the speech frame.
- the speech features 142 may be organized as one or more vectors. For example, information about a speech frame and contextual information may be combined into a single vector or used as separate vectors.
- the speech features 142 used may include more or fewer values than illustrated, and the speech vectors 142 may be organized in more or fewer vectors than illustrated.
- the client device 110 compresses the speech features 142 to generate compressed values 144 .
- the client device 110 may apply quantization algorithms to compress the speech features 142 .
- quantization algorithms may be used.
- each speech feature 142 may be quantized separately, for example, from a floating point value to an integer. For example, 64-bit floating point values may be compressed to 8-bit integer values, resulting in an 8-fold rate of compression.
- vector quantization is applied to the speech features 142 .
- Vector quantization can make use of a codebook that indicates values or codes that correspond to centroids of clusters in the feature space.
- the codebook may have been previously developed using a clustering algorithm.
- the client device 110 may determine which centroid the vector is closest to. For example, the centroid with the smallest Euclidean distance to the feature vector may be selected. The code corresponding to the centroid may then be used as a compressed version of the vector.
- a vector may be mapped to a single value. For example, a vector of 40 floating point values may be mapped to a single 8-bit integer, resulting in a 320-fold compression factor.
- the amount of compression may be set to achieve a desired compression rate or quality level.
- a set of speech features 142 is divided into multiple groups or “chunks,” and vector quantization is performed for each chunk. For example, a 40-dimensional feature vector may be divided into four 10-dimensional chunks.
- a different codebook e.g., a different mapping of codes to clusters or centroids, may be used for each chunk.
- the codebooks may be developed in advance, e.g., based on characteristics of training data indicating speech features of various speech samples.
- the client device 110 uses the codebooks to map each chunk of the speech features 142 to a particular code in the corresponding codebook. If the codebook divides the feature space into 256 clusters, each code may be a 1-byte integer.
- a feature vector of 40 4-byte floating point values may be mapped to 4 1-byte integers, resulting in a 40-fold compression factor.
- the compression level may be adjusted by adjusting the number of chunks that the speech features 142 are divided into, and/or adjusting the number of clusters per chunk, e.g., the number of different entries in each codebook.
- the client device 110 sends compressed values 144 to the server system 120 over the network 118 .
- the compressed values 144 may include significantly less data than the speech features 142 .
- the compressed values 144 may be provided as individual values, as one or more vectors of values, or in another form.
- the server system 120 computes an index value 150 using the compressed values 144 .
- the server system 120 determines the index value 150 in order to look up acoustic model scores from a cache 152 .
- the index value 150 allows the server system 120 to identify, in the cache, scores that an acoustic model 160 would produce, or an approximation of the scores the acoustic model 160 would produce, if the acoustic model 160 were evaluated with the compressed values 144 as input to the acoustic model 160 .
- the server system 120 may compute the index value 150 by applying a hash function to the compressed values 144 , so that the index value 150 is a hash value.
- the server system 120 may use a locality-sensitive hashing (LSH) algorithm.
- LSH locality-sensitive hashing
- the server system 120 uses the index value 150 to look up one or more scores in the cache 152 .
- the cache 152 is organized as a hash table, where the index values correspond to hash values determined from compressed speech data values 144 . Obtaining scores from the cache 152 may be done efficiently, since using the cache 152 may involve a single hash table lookup, while obtaining the same scores by evaluating the model 160 may require many floating point calculations.
- the cache 152 is populated with entries that respectively include one or more posterior probability scores for a set of compressed speech data values, associated with the hash value of the set of compressed speech data values.
- the cache 152 may not include scores for every possible set of compressed values. Rather, the cache 152 may store pre-computed scores for, for example, only a number of commonly occurring sets of compressed values.
- the cache 152 may be distributed across multiple nodes or processing modules that each provide access to a portion of the cache 152 . For example, access to different portions of a hash table that includes stored scores may be distributed across multiple server machines.
- the computing device 120 may access the cache 152 by issuing a remote procedure calls to multiple server computers or nodes to request cached scores.
- distributing the cache 152 across multiple machines may provide faster lookup of scores from the cache 152 , and may allow the cache 152 to store of more data than can be reasonably stored or served from a single machine.
- a distributed system may increase the overall throughput and responsiveness for the cache 152 when many requests are made, or when the cache 152 is accessed by many different computing systems.
- the scores associated with the index 150 may be selected and used. These scores may be the same posterior probability scores that the acoustic model 160 would produce if provided the compressed values 144 as input.
- scores in a nearby entry may be selected instead. For example, nearest-neighbor algorithms may be used to look up and return the scores for the most similar set of compressed values for which scores are stored.
- scores for the most similar set of compressed values may be the scores corresponding to the nearest index value.
- the server system 120 may use the acoustic model 160 to determine scores for the compressed values 144 . For example, if the nearest index value that has associated scores in the cache 152 has more than a threshold difference from the index value 150 , the server system 120 may determine that suitable scores are not stored in the cache 152 and may obtain scores from the acoustic model 160 rather than from the cache 152 .
- index value “1A63,” which represents the particular index value 150 generated for the compressed values 144 there are no scores stored in the cache for index value “1A63,” which represents the particular index value 150 generated for the compressed values 144 .
- the scores for a set of values similar to the compressed values 144 are found and returned instead.
- the scores associated with an index value that is closest to the index value 150 are selected.
- this nearby index value is illustrated as “1A64,” and a number of acoustic model scores 155 are associated with it.
- the acoustic model scores 155 are pre-computed outputs of the acoustic model 160 for a set of values that is not exactly the same as the compressed values 144 . Nevertheless, the nearness of the hash values “1A63” and “1A64” indicates that the scores 155 are a good approximation for the output of the acoustic model 160 for the compressed values 144 .
- the server system 120 uses the selected scores 155 to determine a transcription for the utterance.
- the selected scores 155 each indicate a likelihood that a speech frame corresponds to a different phonetic unit.
- the score 155 a is a likelihood value of “0.6” that the speech frame represents an “/s/” sound
- the score 155 b is a likelihood value of “0.3” that the speech frame represents a “/z/” sound
- the score 155 c is a likelihood value of “0.1” that the speech frame represents a “/c/” sound.
- the selected scores 155 a - 155 c indicate that the speech frame that corresponds to the compressed values 144 is most likely an “/s/” sound.
- each acoustic model score indicates the posterior probability for a component of a phoneme.
- each phoneme may be represented by three hidden Markov model (HMM) states.
- HMM hidden Markov model
- Each acoustic model score in the cache 152 may indicate a score for a particular HMM state of a particular phoneme, rather than simply a phoneme in its entirety.
- cached scores and/or acoustic model outputs correspond to context-dependent states, such as context-dependent tied triphone states.
- context-dependent states such as context-dependent tied triphone states.
- the acoustic model 160 or cache 152 may provide a score for each of the various context-dependent states, although, as discussed below, the cache 152 may approximate some scores with zero values to conserve storage space.
- the set of phonetic units used may include states associated with any appropriate amount of phonetic context, e.g., with states dependent on longer or shorter sequences of phones, in addition to or instead of using triphone states.
- the cache 152 may use a sparse encoding that stores acoustic model scores for only some of the possible phonetic units that may be occur. For example, if a total of 40 phonemes may be predicted, and each phoneme has three HMM states, the result would be 120 different phonetic units or unique HMM states. As another example, using context-dependent tied triphone states, there may be thousands of Scores for phonetic units having low or zero probability scores may be omitted from the cache 152 . For example, scores indicating a likelihood of at least 5%, or at least 10%, or another threshold value may be stored by the model, while scores indicating probabilities less than the threshold may be excluded.
- a certain number of scores such as the scores for the 5, 10, or 20 phonetic units with the highest likelihood may be stored while the rest are excluded.
- the omitted scores may be assumed to have a zero probability.
- the server system 120 may obtain acoustic model scores from the cache 152 in the same manner discussed above for the other speech frames of the utterance. The server system 120 may then use the acoustic model scores for multiple speech frames to construct a transcription for the utterance. In addition to using the likelihoods of the phonetic units indicated by the acoustic model scores, the server system 120 may identify which sequences of phonetic units correspond to valid words in a lexicon, and use a language model to determine which words and sequences of words are most likely to occur.
- the server system 120 provides a transcription of the utterance to the client device 110 , which may then store, display, or otherwise use the transcription.
- the actions described in FIG. 1 may be repeated for multiple speech frames, for example, for each speech frame in the utterance of the user 102 .
- Data from the client device 110 including sets of compressed values for various speech frames may be provided to the server system 120 in individually or in groups as the client device 110 receives more audio and additional sets of compressed values become available.
- the server system 120 may continue to provide additional transcription information as more transcriptions are determined for additional portions of an utterance.
- the functions illustrated as being performed by the client device 110 may be performed by the server system 120 , and vice versa.
- the client device 110 may provide uncompressed speech features 142 to the server system 120 , and the server system 120 may determine the compressed values 144 .
- the client device 110 may provide audio waveform data 140 to the server system 120 , and the server system 120 may determine speech features 142 and compressed values 144 .
- a single device may use the techniques disclosed without using client/server interactions.
- the client device 110 may independently receive audio, determine speech features 142 and compressed values 144 , look up scores in a cache, and determine a transcription without the assistance of the a server system and without providing speech data over a network.
- FIG. 2 is a flow diagram that illustrates an example of a process 200 for using cached speech recognition scores.
- the process 200 may be performed by a computing system, such as the server system 120 of FIG. 1 or another computing system.
- the computing system receives one or more values comprising data about an utterance ( 202 ).
- the one or more values may be data derived from speech features computed for the utterance.
- the one or more values may include output of vector quantization performed on speech features for the utterance.
- the computing system determines an index value for the one or more values ( 204 ). For example, the computing system may apply a hash function to the one or more values to determine a hash value that may serve as an index to a cache of speech recognition scores. The computing system may apply a locality-sensitive hash function to determine the index value.
- the one or more values comprise information derived from a particular speech frame and information derived from contextual speech frames occurring before or after the particular frame.
- the index value may be determined based on the information derived from a particular speech frame and information derived from the contextual speech frames occurring before or after the particular frame.
- the computing system selects an acoustic model score based on the index value ( 206 ).
- the acoustic model score may be selected from a cache of acoustic model scores that were computed before receiving the one or more values.
- the acoustic model score may indicate a likelihood that a particular speech frame that corresponds to the one or more values is an instance of a particular phonetic unit.
- the score can be a posterior probability score that was computed by an acoustic model for a set of input values that is the same as or is similar to the received one or more values.
- the computing system may select an acoustic model score that is associated with the index value in the cache.
- the score selected from the cache may be an acoustic model score previously generated for a set of values that matches the received one or more values.
- the computing system may identify an acoustic model score that is associated with an index value that is near the determined index value but is different from the determined index value.
- the score selected from the cache may be an acoustic model score previously generated for a set of values that does not exactly match the received one or more values, and the score approximates an acoustic model score for the received one or more values.
- the computing system selects a multiple acoustic model scores from the cache based on the index value. For example, the computing system may identify multiple scores that each correspond to different phonetic units. Each score can indicate a likelihood that the utterance includes a different phonetic unit in a portion corresponding to the one or more values.
- the phonetic units may be, for example, phonemes or sub-components of phonemes, such as different HMM model states of phonemes.
- the computing system determines a transcription for the utterance using the selected acoustic model score ( 208 ). For example, the computing system may use one or more acoustic model scores selected from the cache to estimate which phoneme or state of a phoneme corresponds to a particular speech frame. The selected acoustic model score may be used with acoustic model scores selected for other speech frames to determine likely sequences of phonemes or words occur in the utterance.
- the one or more values are received from a client device over a network, and the computing device provides the transcription to the client device over the network.
- the computing system obtains scores from an acoustic model instead of the cache. For example, if the cache does not include scores corresponding to the index value, or does not include scores associated with any index values within a particular range of the index value, the computing system determines scores using the acoustic model. For example, a second set of one or more values corresponding to the utterance may be received, e.g., for a second speech frame of the utterance, and a second index value for the second set of one or more values may be determined.
- the computing system may determine that the cache does not include an acoustic model score that is appropriate for the second index value, and in response, generate an acoustic model score corresponding to the second set of one or more vectors using an acoustic model.
- the transcription may be determined using the selected acoustic model score and the generated acoustic model score.
- FIG. 3 is a flow diagram that illustrates an example of a process 300 for training a speech recognition model.
- the process 300 may be performed to train an acoustic model to produce posterior probability scores using compressed values derived from speech features, rather than using speech features as input.
- the process 300 may be performed by a computing system, such as the server system 120 of FIG. 1 or another computing system.
- the computing system accesses a set of training data ( 302 ).
- the training data can include recorded audio for a variety of utterances.
- the training data may also include transcriptions of the utterances.
- the computing system determines speech features for the utterances in the training data ( 304 ). For example, the computing system may determine MFCCs and/or filterbank energies as discussed above. The speech features may be determined for each speech frame of each audio segment in the training data to be used for training the acoustic model.
- the computing system determines compressed values from the speech features ( 306 ). For example, vector quantization may be used to map the speech features for each speech frame to a set of vector quantization outputs, for example, a set of integers. As a result, a set of values representing compressed speech features may be determined for each of many or all of the speech frames in the training data.
- the speech features determined from the training data may be used to determine one or more vector quantization codebooks, and these codebooks may then be used to determine the compressed values.
- clustering algorithms may be applied to the sets of speech features determined from the training data. The identified clusters may be used to assign codes in the codebook. Multiple different codebooks can be generated for use with different sets of speech vectors or subsets of feature vectors.
- the computing system determines phonetic units corresponding to the compressed values ( 308 ). For example, phonetic representations of the transcriptions may be determined, and the phonetic representations may be aligned with the speech frames. The particular phonetic units that correspond to the respective speech frames and their compressed values may be determined.
- the computing system trains an acoustic model using the compressed values and the phonetic units corresponding to the compressed values ( 310 ).
- the acoustic model may be any of various types of models.
- the model may be a generative probabilistic model, such as a multivariate GMM.
- the model may be a discriminative classifier, such as a multilayer DNN.
- the acoustic model may be trained to predict the likelihood of the various potential phonetic units based on the compressed speech features being provided as input. Some training approaches may adjust model parameters to reflect the statistical distributions of the compressed values and the phonetic units of their corresponding speech frames.
- Some training approaches may involve inputting compressed values to the model, obtaining an output from the model indicating a probability or prediction for a phonetic unit, and comparing the output to the actual phonetic unit corresponding to the compressed values that were input. Training of the model may proceed until, for example, the output of the model provides probability scores that result in a desired level of accuracy or reflect a statistical distribution within a desired tolerance.
- FIG. 4 is a flow diagram that illustrates an example of a process 400 for preparing a cache of speech recognition scores.
- the process 400 may be performed to prepare a cache for use in speech recognition.
- the process 400 may be performed by a computing system, such as the server system 120 of FIG. 1 or another computing system.
- the computing system identifies compressed values that meet a set of criteria ( 402 ). For example, the computing system may analyze sets of compressed values derived from audio in a sample data set, which may be the same as or different from the training data used to train the acoustic model. The computing system may select each set of compressed values that occurs at least a minimum number of times. Alternatively, the computing system may select a particular amount of sets of compressed values that occur most frequently in the sample data set.
- Each set of compressed values may be a vector that represents compressed data from a common type of speech frame.
- each vector may be input, one at a time, to the trained acoustic model.
- the computing system receives, from the trained acoustic model, scores corresponding to the sets of compressed values ( 406 ). For example, the computing system may receive, for each input vector, a set of posterior probability scores for each of the possible phonetic units that may occur.
- the computing system stores the scores from the acoustic model in a cache ( 408 ). For example, for each set of compressed values selected during action ( 402 ), the computing system computes an index value, for example, using a LSH algorithm. The computing system then stores the probability scores produced by the acoustic model for a particular set of compressed values, in association with the index value for the particular set of compressed values. As a result, the cache stores scores generated for different sets of compressed values, each in association with hash value index values, as illustrated for the cache 152 in FIG. 1 .
- Each index in the hash, and thus each set of compressed values, may have multiple associated probability scores.
- the acoustic model may provide a score for each of the different phonetic units that may occur in speech.
- the computing system may prune the scores provided by the acoustic model and store only a proper subset of the acoustic model scores in the cache for a given index value. For example, the computing system may store scores for only the most likely phonetic units for each set of compressed values, or may store only scores indicating a likelihood that satisfies a minimum threshold.
- FIG. 5 shows an example of a computing device 500 and an example of a mobile computing device that can be used to implement the techniques described above.
- the computing device 500 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers.
- the mobile computing device is intended to represent various forms of mobile devices, such as personal digital assistants, cellular telephones, smart-phones, and other similar computing devices.
- the components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document.
- the computing device 500 includes a processor 502 , a memory 504 , a storage device 506 , a high-speed interface 508 connecting to the memory 504 and multiple high-speed expansion ports 510 , and a low-speed interface 512 connecting to a low-speed expansion port 514 and the storage device 506 .
- Each of the processor 502 , the memory 504 , the storage device 506 , the high-speed interface 508 , the high-speed expansion ports 510 , and the low-speed interface 512 are interconnected using various busses, and may be mounted on a common motherboard or in other manners as appropriate.
- the processor 502 can process instructions for execution within the computing device 500 , including instructions stored in the memory 504 or on the storage device 506 to display graphical information for a GUI on an external input/output device, such as a display 516 coupled to the high-speed interface 508 .
- an external input/output device such as a display 516 coupled to the high-speed interface 508 .
- multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory.
- multiple computing devices may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system).
- the memory 504 stores information within the computing device 500 .
- the memory 504 is a volatile memory unit or units.
- the memory 504 is a non-volatile memory unit or units.
- the memory 504 may also be another form of computer-readable medium, such as a magnetic or optical disk.
- the storage device 506 is capable of providing mass storage for the computing device 500 .
- the storage device 506 may be or contain a computer-readable medium, such as a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations.
- a computer program product can be tangibly embodied in an information carrier.
- the computer program product may also contain instructions that, when executed, perform one or more methods, such as those described above.
- the computer program product can also be tangibly embodied in a computer- or machine-readable medium, such as the memory 504 , the storage device 506 , or memory on the processor 502 .
- the high-speed interface 508 manages bandwidth-intensive operations for the computing device 500 , while the low-speed interface 512 manages lower bandwidth-intensive operations.
- the high-speed interface 508 is coupled to the memory 504 , the display 516 (e.g., through a graphics processor or accelerator), and to the high-speed expansion ports 510 , which may accept various expansion cards (not shown).
- the low-speed interface 512 is coupled to the storage device 506 and the low-speed expansion port 514 .
- the low-speed expansion port 514 which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet) may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- input/output devices such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- the computing device 500 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a standard server 520 , or multiple times in a group of such servers. In addition, it may be implemented in a personal computer such as a laptop computer 522 . It may also be implemented as part of a rack server system 524 . Alternatively, components from the computing device 500 may be combined with other components in a mobile device (not shown), such as a mobile computing device 550 . Each of such devices may contain one or more of the computing device 500 and the mobile computing device 550 , and an entire system may be made up of multiple computing devices communicating with each other.
- the mobile computing device 550 includes a processor 552 , a memory 564 , an input/output device such as a display 554 , a communication interface 566 , and a transceiver 568 , among other components.
- the mobile computing device 550 may also be provided with a storage device, such as a micro-drive or other device, to provide additional storage.
- a storage device such as a micro-drive or other device, to provide additional storage.
- Each of the processor 552 , the memory 564 , the display 554 , the communication interface 566 , and the transceiver 568 are interconnected using various buses, and several of the components may be mounted on a common motherboard or in other manners as appropriate.
- the processor 552 can execute instructions within the mobile computing device 550 , including instructions stored in the memory 564 .
- the processor 552 may be implemented as a chipset of chips that include separate and multiple analog and digital processors.
- the processor 552 may provide, for example, for coordination of the other components of the mobile computing device 550 , such as control of user interfaces, applications run by the mobile computing device 550 , and wireless communication by the mobile computing device 550 .
- the processor 552 may communicate with a user through a control interface 558 and a display interface 556 coupled to the display 554 .
- the display 554 may be, for example, a TFT (Thin-Film-Transistor Liquid Crystal Display) display or an OLED (Organic Light Emitting Diode) display, or other appropriate display technology.
- the display interface 556 may comprise appropriate circuitry for driving the display 554 to present graphical and other information to a user.
- the control interface 558 may receive commands from a user and convert them for submission to the processor 552 .
- an external interface 562 may provide communication with the processor 552 , so as to enable near area communication of the mobile computing device 550 with other devices.
- the external interface 562 may provide, for example, for wired communication in some implementations, or for wireless communication in other implementations, and multiple interfaces may also be used.
- the memory 564 stores information within the mobile computing device 550 .
- the memory 564 can be implemented as one or more of a computer-readable medium or media, a volatile memory unit or units, or a non-volatile memory unit or units.
- An expansion memory 574 may also be provided and connected to the mobile computing device 550 through an expansion interface 572 , which may include, for example, a SIMM (Single In Line Memory Module) card interface.
- SIMM Single In Line Memory Module
- the expansion memory 574 may provide extra storage space for the mobile computing device 550 , or may also store applications or other information for the mobile computing device 550 .
- the expansion memory 574 may include instructions to carry out or supplement the processes described above, and may include secure information also.
- the expansion memory 574 may be provide as a security module for the mobile computing device 550 , and may be programmed with instructions that permit secure use of the mobile computing device 550 .
- secure applications may be provided via the SIMM cards, along with additional information, such as placing identifying information on the SIMM card in a non-hackable manner.
- the memory may include, for example, flash memory and/or NVRAM memory (non-volatile random access memory), as discussed below.
- NVRAM memory non-volatile random access memory
- a computer program product is tangibly embodied in an information carrier.
- the computer program product contains instructions that, when executed, perform one or more methods, such as those described above.
- the computer program product can be a computer- or machine-readable medium, such as the memory 564 , the expansion memory 574 , or memory on the processor 552 .
- the computer program product can be received in a propagated signal, for example, over the transceiver 568 or the external interface 562 .
- the mobile computing device 550 may communicate wirelessly through the communication interface 566 , which may include digital signal processing circuitry where necessary.
- the communication interface 566 may provide for communications under various modes or protocols, such as GSM voice calls (Global System for Mobile communications), SMS (Short Message Service), EMS (Enhanced Messaging Service), or MMS messaging (Multimedia Messaging Service), CDMA (code division multiple access), TDMA (time division multiple access), PDC (Personal Digital Cellular), WCDMA (Wideband Code Division Multiple Access), CDMA2000, or GPRS (General Packet Radio Service), among others.
- GSM voice calls Global System for Mobile communications
- SMS Short Message Service
- EMS Enhanced Messaging Service
- MMS messaging Multimedia Messaging Service
- CDMA code division multiple access
- TDMA time division multiple access
- PDC Personal Digital Cellular
- WCDMA Wideband Code Division Multiple Access
- CDMA2000 Code Division Multiple Access
- GPRS General Packet Radio Service
- a GPS (Global Positioning System) receiver module 570 may provide additional navigation- and location-related wireless data to the mobile computing device 550 , which may be used as appropriate by applications running on the mobile computing device 550 .
- the mobile computing device 550 may also communicate audibly using an audio codec 560 , which may receive spoken information from a user and convert it to usable digital information.
- the audio codec 560 may likewise generate audible sound for a user, such as through a speaker, e.g., in a handset of the mobile computing device 550 .
- Such sound may include sound from voice telephone calls, may include recorded sound (e.g., voice messages, music files, etc.) and may also include sound generated by applications operating on the mobile computing device 550 .
- the mobile computing device 550 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a cellular telephone 580 . It may also be implemented as part of a smart-phone 582 , personal digital assistant, tablet computer, wearable computer, or other similar mobile device.
- vector quantization and caching scores for speech recognition
- the same techniques may be used in additional applications.
- the same approach may be used to efficiently store and retrieve scores, such as posterior probability scores or other scores, for any feature-based task.
- Scores such as posterior probability scores or other scores
- Features that are extracted for language identification, speaker identification, object identification in photographs or videos, document indexing, or other tasks may each be compressed, e.g., using vector quantization.
- a hash value or other index value may then be determined from the compressed feature data in order to look up previously-computed scores that are stored in a cache.
- All of the functional operations described in this specification may be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- the techniques disclosed may be implemented as one or more computer program products, i.e., one or more modules of computer program instructions encoded on a computer-readable medium for execution by, or to control the operation of, data processing apparatus.
- the computer readable-medium may be a machine-readable storage device, a machine-readable storage substrate, a memory device, a composition of matter affecting a machine-readable propagated signal, or a combination of one or more of them.
- the computer-readable medium may be a non-transitory computer-readable medium.
- data processing apparatus encompasses all apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers.
- the apparatus may include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- a propagated signal is an artificially generated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus.
- a computer program (also known as a program, software, software application, script, or code) may be written in any form of programming language, including compiled or interpreted languages, and it may be deployed in any form, including as a standalone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.
- a computer program does not necessarily correspond to a file in a file system.
- a program may be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code).
- a computer program may be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- the processes and logic flows described in this specification may be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows may also be performed by, and apparatus may also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- FPGA field programmable gate array
- ASIC application specific integrated circuit
- processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer.
- a processor will receive instructions and data from a read only memory or a random access memory or both.
- the essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- a computer need not have such devices.
- a computer may be embedded in another device, e.g., a tablet computer, a mobile telephone, a personal digital assistant (PDA), a mobile audio player, a Global Positioning System (GPS) receiver, to name just a few.
- Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks.
- the processor and the memory may be supplemented by, or incorporated in, special purpose logic circuitry.
- the techniques disclosed may be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user may provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- a keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices may be used to provide for interaction with a user as well; for example, feedback provided to the user may be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input.
- Implementations may include a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface or a Web browser through which a user may interact with an implementation of the techniques disclosed, or any combination of one or more such back end, middleware, or front end components.
- the components of the system may be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (“LAN”) and a wide area network (“WAN”), e.g., the Internet.
- LAN local area network
- WAN wide area network
- the computing system may include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network.
- the relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
Abstract
Description
- This disclosure relates to caching speech recognition scores.
- Speech recognition systems typically aim to convert a voice input from a user into a natural language transcription. Speech recognition systems may use an acoustic model in the process of determining the words that a user has spoken. For example, an acoustic model may include statistical information that can be used to estimate which speech sounds occur in audio.
- In some implementations, speech recognition scores, such as acoustic model scores, may be stored in a cache. Information about an utterance, such as a set of speech features or data derived from speech features, can be mapped to the stored scores. A speech recognition module can use scores from the cache to generate a transcription for the utterance, instead of evaluating a speech recognition model to generate speech recognition scores.
- In some implementations, an utterance is detected at a client device, and a server system performs speech recognition to determine a transcription for the utterance. To limit the amount of data transferred between the client device and the server system, the client device may process detected audio and extract speech features. The client device may compress the speech features, for example, using vector quantization to map feature vectors to lower dimensional vectors. The client device may then send the compressed data to the server system, allowing the server system to perform speech recognition using the compressed data.
- In a general aspect, a method includes receiving one or more values comprising data about an utterance; determining an index value for the one or more values; selecting, from a cache of acoustic model scores that were computed before receiving the one or more values, an acoustic model score for the one or more received values based on the index value; and determining a transcription for the utterance using the selected acoustic model score.
- Other implementations of this and other aspects include corresponding systems, apparatus, and computer programs, configured to perform the actions of the methods, encoded on computer storage devices. A system of one or more computers can be so configured by virtue of software, firmware, hardware, or a combination of them installed on the system that in operation cause the system to perform the actions. One or more computer programs can be so configured by virtue of having instructions that, when executed by data processing apparatus, cause the apparatus to perform the actions.
- Implementations may include one or more of the following features. For example, generating the index value includes generating a hash value for the one or more values using a locality sensitive hash function. Selecting the acoustic model score for the received one or more values includes selecting an acoustic model score previously generated for a set of values that matches the received one or more values. Selecting the acoustic model score for the received one or more values includes selecting an acoustic model score previously generated for a set of values that does not exactly match the received one or more values, the acoustic model score approximating an acoustic model score for the received one or more values. The one or more values include information derived from a particular speech frame and information derived from contextual speech frames occurring before or after the particular frame. Determining the index value for the one or more values includes determining the hash value for the particular speech frame based on the information derived from a particular speech frame and information derived from the contextual speech frames occurring before or after the particular frame. Selecting the acoustic model score includes selecting an acoustic model score indicating a likelihood that the particular speech frame is an occurrence of a particular phonetic unit in the utterance. Multiple acoustic model scores are selected from the cache based on the index value, each of the multiple acoustic model scores indicating a likelihood that the utterance includes a different phonetic unit in a portion corresponding to the one or more values. Receiving the one or more values includes, receiving the one or more values from a client device over a network. The transcription is provided to the client device over the network. The one or more values include results of compressing speech features for the utterance using vector quantization.
- Advantageous implementations can include one or more of the following features. The speed of speech recognition processing can be increased. Delays in providing speech recognition results can be decreased. The computational requirements for generating transcriptions of speech can be reduced.
- The details of one or more implementations are set forth in the accompanying drawings and the description, below. Other potential features and advantages of the disclosure will be apparent from the description and drawings, and from the claims.
-
FIG. 1 is a diagram that illustrates an example of a system for performing speech recognition using cached speech recognition scores. -
FIG. 2 is a flow diagram that illustrates an example of a process for using cached speech recognition scores. -
FIG. 3 is a flow diagram that illustrates an example of a process for training a speech recognition model. -
FIG. 4 is a flow diagram that illustrates an example of a process for preparing a cache of speech recognition scores. -
FIG. 5 is a schematic diagram that shows an example of a computing device and a mobile computing device. - Like reference numbers and designations in the various drawings indicate like elements.
-
FIG. 1 is a diagram that illustrates an example of asystem 100 for performing speech recognition using cached speech recognition scores. Thesystem 100 includes aclient device 110, aserver system 120 which may include one or more computers, and one or moredata storage devices 130 that are in communication with theserver system 120.FIG. 1 shows stages (A) to (H) which illustrate a flow of data. - In the
system 100, theclient device 110 can be, for example, a desktop computer, laptop computer, a tablet computer, a wearable computer, a cellular phone, a smart phone, a music player, an e-book reader, a navigation system, or any other appropriate computing device. The functions performed by theserver system 120 can be performed by individual computer systems or can be distributed across multiple computer systems. Thenetwork 130 can be wired or wireless or a combination of both and can include the Internet. - Some speech recognition systems allow a client device, such as a mobile phone, to send speech data to a speech recognition server or other back-end system for processing. Servers typically have much greater computing resources than client devices, which may allow a server to determine transcriptions more quickly or more accurately than a client device. In some speech recognition systems, a client device makes an audio recording of a user's speech and sends the recorded audio, e.g., an audio waveform, to a speech recognition server. However, in some instances, slow data transmission speeds or poor network connectivity can decrease performance. In addition, sending recorded audio over a network from a mobile device may cause a user to incur substantial network usage charges.
- In some implementations, a client device computes speech features from detected audio and sends the feature values to a server system instead of sending audio recordings. The client device may compress speech feature vectors or other speech data to additionally reduce the overall amount of data that is transferred over a network. In some implementations, the client device uses quantization techniques to map speech features to more compact representations. For example, vector quantization can be used to map speech feature vectors to lower dimensional vectors.
- The process of recognizing speech can also be enhanced by using cached speech recognition scores. As discussed further below, a speech recognition engine may use one or more models to evaluate speech and estimate what a user has spoken. For example, an acoustic model may generate scores indicating likelihoods that a portion of an utterance corresponds to particular speech sounds. A language model may generate scores indicating likelihoods of candidate transcriptions of the speech recording, and may be used in conjunction with the acoustic model to produce the overall likelihood that the sounds correspond to particular words and sequences of words. Evaluating a model to produce speech recognition scores can be computationally demanding, and the time required to obtain scores from a model may result in a significant delay in producing a transcription.
- In some implementations, speech recognition scores are obtained from a cache rather than evaluating a model. Speech recognition scores may be pre-computed for a variety of potential inputs to a model, and the speech recognition scores may be stored in the cache. When compressed speech features or other data about an utterance is received, a speech recognition engine may look up appropriate scores in the cache. For example, the cache may be structured as a hash table, and a hash of the data about the utterance may be used to locate appropriate scores. When the needed scores are located in the cache, the time required to obtain the scores can be much less than the delay to compute scores using a model such as a Gaussian mixture model (GMM) or deep neural network (DNN).
- In some implementations, an acoustic model, such as the
acoustic model 160 shown inFIG. 1 , may be replaced by a collection of labeled samples whose phonetic state label is known in advance. Scores in that case may be pre-assigned to each speech frame in the collection using force alignment against a large ASR model. For some or all of the speech frames, the corresponding scores may be stored in a cache and used to later recognize speech. In some implementations, the entire collection of labeled samples and their corresponding scores, or data derived from the collection such as averaged scores for sets of similar speech frames, may be stored and accessed to obtain scores without evaluating a model. When input speech to be recognized is received, frames of the input speech may be assigned the scores that were previously assigned to similar frames in the collection of labeled samples. Speech frames may be identified as similar to each other based on, for example, having hash values that are identical or nearby, e.g., within a certain distance, when the hash values are produced using a locality sensitive hashing algorithm. - Storing and retrieving speech recognition scores can be facilitated by using vector quantization of speech features. Vector quantization can be used to map a highly variable set of data to a smaller range of discrete possibilities. For example, a large number of real-valued speech features can be mapped to a small number of integers. The vector quantization process greatly reduces the number of unique input combinations that may occur, which makes it more feasible to store a significant portion of the acoustic model scores in a cache.
- In some implementations, the
server system 120 receives one or more values comprising data about an utterance. Theserver system 120 determines an index value for the one or more values. Theserver system 120 selects an acoustic model score based on the index value. For example, the acoustic model score is selected from a cache of acoustic model scores that were computed before theserver system 120 received the one or more values. Theserver system 120 also determines a transcription for the utterance using the selected acoustic model score. - In the example of
FIG. 1 , theclient device 110 generates data about an utterance and sends the data to theserver system 120 to obtain a transcription for the utterance. Theserver system 120 uses the data to retrieve speech recognition scores from a cache, and uses the retrieved scores to determine a transcription for the utterance. - In further detail, during stage (A), a
user 102 speaks an utterance to be transcribed, and the user's utterance is recorded by theclient device 110. For example, theclient device 110 may use a microphone to detect audio including the utterance, and theclient device 110 may recordaudio waveform data 140 for the detected audio. - During stage (B), the
client device 110 computes speech features 142 for the utterance. Speech recognition systems may use only certain components of an audio signal to determine the content of an utterance. The speech features 142 omit information from theaudio waveform data 140 that theserver system 140 would not use during speech recognition. Theclient device 110 computes speech features, e.g., values indicating characteristics of theaudio waveform data 140, that will be useful to speech recognition algorithms. As an example, the speech features 142 may include, mel filterbank energies, which may be computed by binning energies at different frequency ranges on a logarithmic scale. As another example, the speech features 142 may include mel frequency cepstral coefficients (MFCCs) produced from filterbank channels. - A set of speech features 142 may be computed for each of various segments of the
audio waveform data 140. As an example, theaudio waveform data 140 may be divided into speech frames that each have a duration of, for example, 25 milliseconds, and a set of speech features may be computed for each speech frame. - In some implementations, the speech features 142 for a particular speech frame may include contextual information about previous and subsequent values of the feature signal, e.g., information that reflects audio characteristics of previous or subsequent speech frames. For example, information about first and second order derivatives of the feature signals may be included in a set of speech features 142. A 13-dimensional MFCC vector for a particular speech frame may be augmented with a 13-dimensional vector representing first order derivatives and another 13-dimensional vector representing second order derivatives, resulting in a 39-dimensional vector. As another example, the speech features for previous and/or subsequent speech frames may be combined with the speech features computed for the current frame. For example, a 40-channel mel-filterbank energy vector may be augmented with the 40-channel energy vector for each of the previous twenty speech frames and each of the subsequent five speech frames, resulting in a total of 26×40=1040 feature values for each speech frame.
- In the example of
FIG. 1 , the speech features 142 represent a set of values used to recognize a single speech frame, and the values indicate characteristics of the single speech frame as well as any contextual information to be used when recognizing the speech frame. The speech features 142 may be organized as one or more vectors. For example, information about a speech frame and contextual information may be combined into a single vector or used as separate vectors. The speech features 142 used may include more or fewer values than illustrated, and thespeech vectors 142 may be organized in more or fewer vectors than illustrated. - During stage (C), the
client device 110 compresses the speech features 142 to generatecompressed values 144. Theclient device 110 may apply quantization algorithms to compress the speech features 142. A variety of different approaches may be used. In some implementations, eachspeech feature 142 may be quantized separately, for example, from a floating point value to an integer. For example, 64-bit floating point values may be compressed to 8-bit integer values, resulting in an 8-fold rate of compression. - In some implementations, vector quantization is applied to the speech features 142. Vector quantization can make use of a codebook that indicates values or codes that correspond to centroids of clusters in the feature space. The codebook may have been previously developed using a clustering algorithm. Once a vector of speech features 142 has been determined, the
client device 110 may determine which centroid the vector is closest to. For example, the centroid with the smallest Euclidean distance to the feature vector may be selected. The code corresponding to the centroid may then be used as a compressed version of the vector. In some instances, a vector may be mapped to a single value. For example, a vector of 40 floating point values may be mapped to a single 8-bit integer, resulting in a 320-fold compression factor. - The amount of compression may be set to achieve a desired compression rate or quality level. In some implementations, a set of speech features 142 is divided into multiple groups or “chunks,” and vector quantization is performed for each chunk. For example, a 40-dimensional feature vector may be divided into four 10-dimensional chunks. A different codebook, e.g., a different mapping of codes to clusters or centroids, may be used for each chunk. The codebooks may be developed in advance, e.g., based on characteristics of training data indicating speech features of various speech samples. The
client device 110 uses the codebooks to map each chunk of the speech features 142 to a particular code in the corresponding codebook. If the codebook divides the feature space into 256 clusters, each code may be a 1-byte integer. Thus a feature vector of 40 4-byte floating point values may be mapped to 4 1-byte integers, resulting in a 40-fold compression factor. The compression level may be adjusted by adjusting the number of chunks that the speech features 142 are divided into, and/or adjusting the number of clusters per chunk, e.g., the number of different entries in each codebook. - During stage (D), the
client device 110 sends compressedvalues 144 to theserver system 120 over thenetwork 118. Thecompressed values 144 may include significantly less data than the speech features 142. Thecompressed values 144 may be provided as individual values, as one or more vectors of values, or in another form. - During stage (E), the
server system 120 computes anindex value 150 using the compressed values 144. Theserver system 120 determines theindex value 150 in order to look up acoustic model scores from acache 152. Theindex value 150 allows theserver system 120 to identify, in the cache, scores that anacoustic model 160 would produce, or an approximation of the scores theacoustic model 160 would produce, if theacoustic model 160 were evaluated with thecompressed values 144 as input to theacoustic model 160. - The
server system 120 may compute theindex value 150 by applying a hash function to thecompressed values 144, so that theindex value 150 is a hash value. Theserver system 120 may use a locality-sensitive hashing (LSH) algorithm. As a result, sets ofcompressed values 144 that are similar but not identical may correspond to index values that are nearby in thecache 152. - During stage (F), the
server system 120 uses theindex value 150 to look up one or more scores in thecache 152. In the example, thecache 152 is organized as a hash table, where the index values correspond to hash values determined from compressed speech data values 144. Obtaining scores from thecache 152 may be done efficiently, since using thecache 152 may involve a single hash table lookup, while obtaining the same scores by evaluating themodel 160 may require many floating point calculations. Thecache 152 is populated with entries that respectively include one or more posterior probability scores for a set of compressed speech data values, associated with the hash value of the set of compressed speech data values. Thecache 152 may not include scores for every possible set of compressed values. Rather, thecache 152 may store pre-computed scores for, for example, only a number of commonly occurring sets of compressed values. - In some implementations, the
cache 152 may be distributed across multiple nodes or processing modules that each provide access to a portion of thecache 152. For example, access to different portions of a hash table that includes stored scores may be distributed across multiple server machines. Thecomputing device 120 may access thecache 152 by issuing a remote procedure calls to multiple server computers or nodes to request cached scores. In some instances, distributing thecache 152 across multiple machines may provide faster lookup of scores from thecache 152, and may allow thecache 152 to store of more data than can be reasonably stored or served from a single machine. Similarly, a distributed system may increase the overall throughput and responsiveness for thecache 152 when many requests are made, or when thecache 152 is accessed by many different computing systems. - When the
cache 152 includes scores associated with theindex value 150, the scores associated with theindex 150 may be selected and used. These scores may be the same posterior probability scores that theacoustic model 160 would produce if provided thecompressed values 144 as input. When thecache 152 does not include scores associated with theindex value 150, scores in a nearby entry may be selected instead. For example, nearest-neighbor algorithms may be used to look up and return the scores for the most similar set of compressed values for which scores are stored. When LSH is used, scores for the most similar set of compressed values may be the scores corresponding to the nearest index value. - As an alternative, if the
cache 152 does not include scores associated with theindex value 150, theserver system 120 may use theacoustic model 160 to determine scores for the compressed values 144. For example, if the nearest index value that has associated scores in thecache 152 has more than a threshold difference from theindex value 150, theserver system 120 may determine that suitable scores are not stored in thecache 152 and may obtain scores from theacoustic model 160 rather than from thecache 152. - In the illustrated example, there are no scores stored in the cache for index value “1A63,” which represents the
particular index value 150 generated for the compressed values 144. As a result, the scores for a set of values similar to thecompressed values 144 are found and returned instead. For example, the scores associated with an index value that is closest to theindex value 150 are selected. As illustrated, this nearby index value is illustrated as “1A64,” and a number of acoustic model scores 155 are associated with it. The acoustic model scores 155 are pre-computed outputs of theacoustic model 160 for a set of values that is not exactly the same as thecompressed values 144. Nevertheless, the nearness of the hash values “1A63” and “1A64” indicates that thescores 155 are a good approximation for the output of theacoustic model 160 for the compressed values 144. - During stage (G), the
server system 120 uses the selectedscores 155 to determine a transcription for the utterance. In the example, the selectedscores 155 each indicate a likelihood that a speech frame corresponds to a different phonetic unit. For example, thescore 155 a is a likelihood value of “0.6” that the speech frame represents an “/s/” sound, thescore 155 b is a likelihood value of “0.3” that the speech frame represents a “/z/” sound, and thescore 155 c is a likelihood value of “0.1” that the speech frame represents a “/c/” sound. Thus, the selectedscores 155 a-155 c indicate that the speech frame that corresponds to thecompressed values 144 is most likely an “/s/” sound. - While the illustrated example shows scores for phonemes, scores may correspond to other phonetic units. In some implementations, each acoustic model score indicates the posterior probability for a component of a phoneme. For example, each phoneme may be represented by three hidden Markov model (HMM) states. Each acoustic model score in the
cache 152 may indicate a score for a particular HMM state of a particular phoneme, rather than simply a phoneme in its entirety. - In some implementations, cached scores and/or acoustic model outputs correspond to context-dependent states, such as context-dependent tied triphone states. Using triphones, the number of total phonetic units may be, theoretically, as many as the number of potential phonemes raised to the third power. In some instances, there may be a few thousand different states to tens of thousands of states. The
acoustic model 160 orcache 152 may provide a score for each of the various context-dependent states, although, as discussed below, thecache 152 may approximate some scores with zero values to conserve storage space. The set of phonetic units used may include states associated with any appropriate amount of phonetic context, e.g., with states dependent on longer or shorter sequences of phones, in addition to or instead of using triphone states. - To conserve space and reduce processing requirements, the
cache 152 may use a sparse encoding that stores acoustic model scores for only some of the possible phonetic units that may be occur. For example, if a total of 40 phonemes may be predicted, and each phoneme has three HMM states, the result would be 120 different phonetic units or unique HMM states. As another example, using context-dependent tied triphone states, there may be thousands of Scores for phonetic units having low or zero probability scores may be omitted from thecache 152. For example, scores indicating a likelihood of at least 5%, or at least 10%, or another threshold value may be stored by the model, while scores indicating probabilities less than the threshold may be excluded. As another example, a certain number of scores, such as the scores for the 5, 10, or 20 phonetic units with the highest likelihood may be stored while the rest are excluded. When scores are stored for some phonetic units and not others, the omitted scores may be assumed to have a zero probability. - The
server system 120 may obtain acoustic model scores from thecache 152 in the same manner discussed above for the other speech frames of the utterance. Theserver system 120 may then use the acoustic model scores for multiple speech frames to construct a transcription for the utterance. In addition to using the likelihoods of the phonetic units indicated by the acoustic model scores, theserver system 120 may identify which sequences of phonetic units correspond to valid words in a lexicon, and use a language model to determine which words and sequences of words are most likely to occur. - During stage (H), the
server system 120 provides a transcription of the utterance to theclient device 110, which may then store, display, or otherwise use the transcription. - In some implementations, the actions described in
FIG. 1 may be repeated for multiple speech frames, for example, for each speech frame in the utterance of theuser 102. Data from theclient device 110 including sets of compressed values for various speech frames may be provided to theserver system 120 in individually or in groups as theclient device 110 receives more audio and additional sets of compressed values become available. Similarly, theserver system 120 may continue to provide additional transcription information as more transcriptions are determined for additional portions of an utterance. - In some implementations, the functions illustrated as being performed by the
client device 110 may be performed by theserver system 120, and vice versa. For example, theclient device 110 may provide uncompressed speech features 142 to theserver system 120, and theserver system 120 may determine the compressed values 144. As another example, theclient device 110 may provideaudio waveform data 140 to theserver system 120, and theserver system 120 may determine speech features 142 andcompressed values 144. As an alternative, a single device may use the techniques disclosed without using client/server interactions. For example, theclient device 110 may independently receive audio, determine speech features 142 andcompressed values 144, look up scores in a cache, and determine a transcription without the assistance of the a server system and without providing speech data over a network. -
FIG. 2 is a flow diagram that illustrates an example of aprocess 200 for using cached speech recognition scores. Theprocess 200 may be performed by a computing system, such as theserver system 120 ofFIG. 1 or another computing system. - The computing system receives one or more values comprising data about an utterance (202). The one or more values may be data derived from speech features computed for the utterance. For example, the one or more values may include output of vector quantization performed on speech features for the utterance.
- The computing system determines an index value for the one or more values (204). For example, the computing system may apply a hash function to the one or more values to determine a hash value that may serve as an index to a cache of speech recognition scores. The computing system may apply a locality-sensitive hash function to determine the index value.
- In some implementations, the one or more values comprise information derived from a particular speech frame and information derived from contextual speech frames occurring before or after the particular frame. The index value may be determined based on the information derived from a particular speech frame and information derived from the contextual speech frames occurring before or after the particular frame.
- The computing system selects an acoustic model score based on the index value (206). The acoustic model score may be selected from a cache of acoustic model scores that were computed before receiving the one or more values. The acoustic model score may indicate a likelihood that a particular speech frame that corresponds to the one or more values is an instance of a particular phonetic unit. For example, the score can be a posterior probability score that was computed by an acoustic model for a set of input values that is the same as or is similar to the received one or more values.
- The computing system may select an acoustic model score that is associated with the index value in the cache. As a result, the score selected from the cache may be an acoustic model score previously generated for a set of values that matches the received one or more values. As another example, if the cache does not include scores associated with the determined index value, the computing system may identify an acoustic model score that is associated with an index value that is near the determined index value but is different from the determined index value. Thus, the score selected from the cache may be an acoustic model score previously generated for a set of values that does not exactly match the received one or more values, and the score approximates an acoustic model score for the received one or more values.
- In some implementations, the computing system selects a multiple acoustic model scores from the cache based on the index value. For example, the computing system may identify multiple scores that each correspond to different phonetic units. Each score can indicate a likelihood that the utterance includes a different phonetic unit in a portion corresponding to the one or more values. The phonetic units may be, for example, phonemes or sub-components of phonemes, such as different HMM model states of phonemes.
- The computing system determines a transcription for the utterance using the selected acoustic model score (208). For example, the computing system may use one or more acoustic model scores selected from the cache to estimate which phoneme or state of a phoneme corresponds to a particular speech frame. The selected acoustic model score may be used with acoustic model scores selected for other speech frames to determine likely sequences of phonemes or words occur in the utterance.
- In some implementations, the one or more values are received from a client device over a network, and the computing device provides the transcription to the client device over the network.
- In some implementations, if a cache does not store scores that meet predetermined criteria, the computing system obtains scores from an acoustic model instead of the cache. For example, if the cache does not include scores corresponding to the index value, or does not include scores associated with any index values within a particular range of the index value, the computing system determines scores using the acoustic model. For example, a second set of one or more values corresponding to the utterance may be received, e.g., for a second speech frame of the utterance, and a second index value for the second set of one or more values may be determined. The computing system may determine that the cache does not include an acoustic model score that is appropriate for the second index value, and in response, generate an acoustic model score corresponding to the second set of one or more vectors using an acoustic model. The transcription may be determined using the selected acoustic model score and the generated acoustic model score.
-
FIG. 3 is a flow diagram that illustrates an example of a process 300 for training a speech recognition model. The process 300 may be performed to train an acoustic model to produce posterior probability scores using compressed values derived from speech features, rather than using speech features as input. The process 300 may be performed by a computing system, such as theserver system 120 ofFIG. 1 or another computing system. - The computing system accesses a set of training data (302). The training data can include recorded audio for a variety of utterances. The training data may also include transcriptions of the utterances.
- The computing system determines speech features for the utterances in the training data (304). For example, the computing system may determine MFCCs and/or filterbank energies as discussed above. The speech features may be determined for each speech frame of each audio segment in the training data to be used for training the acoustic model.
- The computing system determines compressed values from the speech features (306). For example, vector quantization may be used to map the speech features for each speech frame to a set of vector quantization outputs, for example, a set of integers. As a result, a set of values representing compressed speech features may be determined for each of many or all of the speech frames in the training data.
- In some implementations, the speech features determined from the training data may be used to determine one or more vector quantization codebooks, and these codebooks may then be used to determine the compressed values. For example, clustering algorithms may be applied to the sets of speech features determined from the training data. The identified clusters may be used to assign codes in the codebook. Multiple different codebooks can be generated for use with different sets of speech vectors or subsets of feature vectors.
- The computing system determines phonetic units corresponding to the compressed values (308). For example, phonetic representations of the transcriptions may be determined, and the phonetic representations may be aligned with the speech frames. The particular phonetic units that correspond to the respective speech frames and their compressed values may be determined.
- The computing system trains an acoustic model using the compressed values and the phonetic units corresponding to the compressed values (310). The acoustic model may be any of various types of models. For example, the model may be a generative probabilistic model, such as a multivariate GMM. As another example, the model may be a discriminative classifier, such as a multilayer DNN. The acoustic model may be trained to predict the likelihood of the various potential phonetic units based on the compressed speech features being provided as input. Some training approaches may adjust model parameters to reflect the statistical distributions of the compressed values and the phonetic units of their corresponding speech frames. Some training approaches may involve inputting compressed values to the model, obtaining an output from the model indicating a probability or prediction for a phonetic unit, and comparing the output to the actual phonetic unit corresponding to the compressed values that were input. Training of the model may proceed until, for example, the output of the model provides probability scores that result in a desired level of accuracy or reflect a statistical distribution within a desired tolerance.
-
FIG. 4 is a flow diagram that illustrates an example of aprocess 400 for preparing a cache of speech recognition scores. Theprocess 400 may be performed to prepare a cache for use in speech recognition. Theprocess 400 may be performed by a computing system, such as theserver system 120 ofFIG. 1 or another computing system. - The computing system identifies compressed values that meet a set of criteria (402). For example, the computing system may analyze sets of compressed values derived from audio in a sample data set, which may be the same as or different from the training data used to train the acoustic model. The computing system may select each set of compressed values that occurs at least a minimum number of times. Alternatively, the computing system may select a particular amount of sets of compressed values that occur most frequently in the sample data set.
- The computing system inputs each identified set of compressed values to the trained acoustic model (404). Each set of compressed values may be a vector that represents compressed data from a common type of speech frame. For example, each vector may be input, one at a time, to the trained acoustic model.
- The computing system receives, from the trained acoustic model, scores corresponding to the sets of compressed values (406). For example, the computing system may receive, for each input vector, a set of posterior probability scores for each of the possible phonetic units that may occur.
- The computing system stores the scores from the acoustic model in a cache (408). For example, for each set of compressed values selected during action (402), the computing system computes an index value, for example, using a LSH algorithm. The computing system then stores the probability scores produced by the acoustic model for a particular set of compressed values, in association with the index value for the particular set of compressed values. As a result, the cache stores scores generated for different sets of compressed values, each in association with hash value index values, as illustrated for the
cache 152 inFIG. 1 . - Each index in the hash, and thus each set of compressed values, may have multiple associated probability scores. For example, the acoustic model may provide a score for each of the different phonetic units that may occur in speech. The computing system may prune the scores provided by the acoustic model and store only a proper subset of the acoustic model scores in the cache for a given index value. For example, the computing system may store scores for only the most likely phonetic units for each set of compressed values, or may store only scores indicating a likelihood that satisfies a minimum threshold.
-
FIG. 5 shows an example of acomputing device 500 and an example of a mobile computing device that can be used to implement the techniques described above. Thecomputing device 500 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers. The mobile computing device is intended to represent various forms of mobile devices, such as personal digital assistants, cellular telephones, smart-phones, and other similar computing devices. The components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document. - The
computing device 500 includes aprocessor 502, amemory 504, astorage device 506, a high-speed interface 508 connecting to thememory 504 and multiple high-speed expansion ports 510, and a low-speed interface 512 connecting to a low-speed expansion port 514 and thestorage device 506. Each of theprocessor 502, thememory 504, thestorage device 506, the high-speed interface 508, the high-speed expansion ports 510, and the low-speed interface 512, are interconnected using various busses, and may be mounted on a common motherboard or in other manners as appropriate. Theprocessor 502 can process instructions for execution within thecomputing device 500, including instructions stored in thememory 504 or on thestorage device 506 to display graphical information for a GUI on an external input/output device, such as adisplay 516 coupled to the high-speed interface 508. In other implementations, multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory. Also, multiple computing devices may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system). - The
memory 504 stores information within thecomputing device 500. In some implementations, thememory 504 is a volatile memory unit or units. In some implementations, thememory 504 is a non-volatile memory unit or units. Thememory 504 may also be another form of computer-readable medium, such as a magnetic or optical disk. - The
storage device 506 is capable of providing mass storage for thecomputing device 500. In some implementations, thestorage device 506 may be or contain a computer-readable medium, such as a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations. A computer program product can be tangibly embodied in an information carrier. The computer program product may also contain instructions that, when executed, perform one or more methods, such as those described above. The computer program product can also be tangibly embodied in a computer- or machine-readable medium, such as thememory 504, thestorage device 506, or memory on theprocessor 502. - The high-
speed interface 508 manages bandwidth-intensive operations for thecomputing device 500, while the low-speed interface 512 manages lower bandwidth-intensive operations. Such allocation of functions is exemplary only. In some implementations, the high-speed interface 508 is coupled to thememory 504, the display 516 (e.g., through a graphics processor or accelerator), and to the high-speed expansion ports 510, which may accept various expansion cards (not shown). In the implementation, the low-speed interface 512 is coupled to thestorage device 506 and the low-speed expansion port 514. The low-speed expansion port 514, which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet) may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter. - The
computing device 500 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as astandard server 520, or multiple times in a group of such servers. In addition, it may be implemented in a personal computer such as alaptop computer 522. It may also be implemented as part of arack server system 524. Alternatively, components from thecomputing device 500 may be combined with other components in a mobile device (not shown), such as amobile computing device 550. Each of such devices may contain one or more of thecomputing device 500 and themobile computing device 550, and an entire system may be made up of multiple computing devices communicating with each other. - The
mobile computing device 550 includes aprocessor 552, amemory 564, an input/output device such as adisplay 554, acommunication interface 566, and atransceiver 568, among other components. Themobile computing device 550 may also be provided with a storage device, such as a micro-drive or other device, to provide additional storage. Each of theprocessor 552, thememory 564, thedisplay 554, thecommunication interface 566, and thetransceiver 568, are interconnected using various buses, and several of the components may be mounted on a common motherboard or in other manners as appropriate. - The
processor 552 can execute instructions within themobile computing device 550, including instructions stored in thememory 564. Theprocessor 552 may be implemented as a chipset of chips that include separate and multiple analog and digital processors. Theprocessor 552 may provide, for example, for coordination of the other components of themobile computing device 550, such as control of user interfaces, applications run by themobile computing device 550, and wireless communication by themobile computing device 550. - The
processor 552 may communicate with a user through acontrol interface 558 and adisplay interface 556 coupled to thedisplay 554. Thedisplay 554 may be, for example, a TFT (Thin-Film-Transistor Liquid Crystal Display) display or an OLED (Organic Light Emitting Diode) display, or other appropriate display technology. Thedisplay interface 556 may comprise appropriate circuitry for driving thedisplay 554 to present graphical and other information to a user. Thecontrol interface 558 may receive commands from a user and convert them for submission to theprocessor 552. In addition, anexternal interface 562 may provide communication with theprocessor 552, so as to enable near area communication of themobile computing device 550 with other devices. Theexternal interface 562 may provide, for example, for wired communication in some implementations, or for wireless communication in other implementations, and multiple interfaces may also be used. - The
memory 564 stores information within themobile computing device 550. Thememory 564 can be implemented as one or more of a computer-readable medium or media, a volatile memory unit or units, or a non-volatile memory unit or units. An expansion memory 574 may also be provided and connected to themobile computing device 550 through anexpansion interface 572, which may include, for example, a SIMM (Single In Line Memory Module) card interface. The expansion memory 574 may provide extra storage space for themobile computing device 550, or may also store applications or other information for themobile computing device 550. Specifically, the expansion memory 574 may include instructions to carry out or supplement the processes described above, and may include secure information also. Thus, for example, the expansion memory 574 may be provide as a security module for themobile computing device 550, and may be programmed with instructions that permit secure use of themobile computing device 550. In addition, secure applications may be provided via the SIMM cards, along with additional information, such as placing identifying information on the SIMM card in a non-hackable manner. - The memory may include, for example, flash memory and/or NVRAM memory (non-volatile random access memory), as discussed below. In some implementations, a computer program product is tangibly embodied in an information carrier. The computer program product contains instructions that, when executed, perform one or more methods, such as those described above. The computer program product can be a computer- or machine-readable medium, such as the
memory 564, the expansion memory 574, or memory on theprocessor 552. In some implementations, the computer program product can be received in a propagated signal, for example, over thetransceiver 568 or theexternal interface 562. - The
mobile computing device 550 may communicate wirelessly through thecommunication interface 566, which may include digital signal processing circuitry where necessary. Thecommunication interface 566 may provide for communications under various modes or protocols, such as GSM voice calls (Global System for Mobile communications), SMS (Short Message Service), EMS (Enhanced Messaging Service), or MMS messaging (Multimedia Messaging Service), CDMA (code division multiple access), TDMA (time division multiple access), PDC (Personal Digital Cellular), WCDMA (Wideband Code Division Multiple Access), CDMA2000, or GPRS (General Packet Radio Service), among others. Such communication may occur, for example, through thetransceiver 568 using a radio-frequency. In addition, short-range communication may occur, such as using a Bluetooth, Wi-Fi, or other such transceiver (not shown). In addition, a GPS (Global Positioning System)receiver module 570 may provide additional navigation- and location-related wireless data to themobile computing device 550, which may be used as appropriate by applications running on themobile computing device 550. - The
mobile computing device 550 may also communicate audibly using anaudio codec 560, which may receive spoken information from a user and convert it to usable digital information. Theaudio codec 560 may likewise generate audible sound for a user, such as through a speaker, e.g., in a handset of themobile computing device 550. Such sound may include sound from voice telephone calls, may include recorded sound (e.g., voice messages, music files, etc.) and may also include sound generated by applications operating on themobile computing device 550. - The
mobile computing device 550 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as acellular telephone 580. It may also be implemented as part of a smart-phone 582, personal digital assistant, tablet computer, wearable computer, or other similar mobile device. - A number of implementations have been described. Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the disclosure. For example, various forms of the flows shown above may be used, with steps re-ordered, added, or removed.
- In addition, while various examples describe the use of vector quantization and caching scores for speech recognition, the same techniques may be used in additional applications. For example, the same approach may be used to efficiently store and retrieve scores, such as posterior probability scores or other scores, for any feature-based task. Features that are extracted for language identification, speaker identification, object identification in photographs or videos, document indexing, or other tasks may each be compressed, e.g., using vector quantization. A hash value or other index value may then be determined from the compressed feature data in order to look up previously-computed scores that are stored in a cache.
- All of the functional operations described in this specification may be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. The techniques disclosed may be implemented as one or more computer program products, i.e., one or more modules of computer program instructions encoded on a computer-readable medium for execution by, or to control the operation of, data processing apparatus. The computer readable-medium may be a machine-readable storage device, a machine-readable storage substrate, a memory device, a composition of matter affecting a machine-readable propagated signal, or a combination of one or more of them. The computer-readable medium may be a non-transitory computer-readable medium. The term “data processing apparatus” encompasses all apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. The apparatus may include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them. A propagated signal is an artificially generated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus.
- A computer program (also known as a program, software, software application, script, or code) may be written in any form of programming language, including compiled or interpreted languages, and it may be deployed in any form, including as a standalone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A computer program does not necessarily correspond to a file in a file system. A program may be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code). A computer program may be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- The processes and logic flows described in this specification may be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows may also be performed by, and apparatus may also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks. However, a computer need not have such devices. Moreover, a computer may be embedded in another device, e.g., a tablet computer, a mobile telephone, a personal digital assistant (PDA), a mobile audio player, a Global Positioning System (GPS) receiver, to name just a few. Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks. The processor and the memory may be supplemented by, or incorporated in, special purpose logic circuitry.
- To provide for interaction with a user, the techniques disclosed may be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user may provide input to the computer. Other kinds of devices may be used to provide for interaction with a user as well; for example, feedback provided to the user may be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input.
- Implementations may include a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface or a Web browser through which a user may interact with an implementation of the techniques disclosed, or any combination of one or more such back end, middleware, or front end components. The components of the system may be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (“LAN”) and a wide area network (“WAN”), e.g., the Internet.
- The computing system may include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- While this specification contains many specifics, these should not be construed as limitations, but rather as descriptions of features specific to particular implementations. Certain features that are described in this specification in the context of separate implementations may also be implemented in combination in a single implementation. Conversely, various features that are described in the context of a single implementation may also be implemented in multiple implementations separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination may in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
- Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In certain circumstances, multitasking and parallel processing may be advantageous. Moreover, the separation of various system components in the implementations described above should not be understood as requiring such separation in all implementations, and it should be understood that the described program components and systems may generally be integrated together in a single software product or packaged into multiple software products.
- Thus, particular implementations have been described. Other implementations are within the scope of the following claims. For example, the actions recited in the claims may be performed in a different order and still achieve desirable results.
Claims (20)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/311,557 US9858922B2 (en) | 2014-06-23 | 2014-06-23 | Caching speech recognition scores |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/311,557 US9858922B2 (en) | 2014-06-23 | 2014-06-23 | Caching speech recognition scores |
Publications (2)
Publication Number | Publication Date |
---|---|
US20150371631A1 true US20150371631A1 (en) | 2015-12-24 |
US9858922B2 US9858922B2 (en) | 2018-01-02 |
Family
ID=54870208
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US14/311,557 Active 2034-12-14 US9858922B2 (en) | 2014-06-23 | 2014-06-23 | Caching speech recognition scores |
Country Status (1)
Country | Link |
---|---|
US (1) | US9858922B2 (en) |
Cited By (30)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN105810212A (en) * | 2016-03-07 | 2016-07-27 | 合肥工业大学 | Train whistle recognizing method for complex noise environment |
US9628620B1 (en) * | 2016-07-07 | 2017-04-18 | ClearCaptions, LLC | Method and system for providing captioned telephone service with automated speech recognition |
US20170256255A1 (en) * | 2016-03-01 | 2017-09-07 | Intel Corporation | Intermediate scoring and rejection loopback for improved key phrase detection |
US20180102136A1 (en) * | 2016-10-11 | 2018-04-12 | Cirrus Logic International Semiconductor Ltd. | Detection of acoustic impulse events in voice applications using a neural network |
US10044854B2 (en) | 2016-07-07 | 2018-08-07 | ClearCaptions, LLC | Method and system for providing captioned telephone service with automated speech recognition |
US10043521B2 (en) | 2016-07-01 | 2018-08-07 | Intel IP Corporation | User defined key phrase detection by user dependent sequence modeling |
US10083689B2 (en) * | 2016-12-23 | 2018-09-25 | Intel Corporation | Linear scoring for low power wake on voice |
US10204619B2 (en) | 2014-10-22 | 2019-02-12 | Google Llc | Speech recognition using associative mapping |
US10242696B2 (en) | 2016-10-11 | 2019-03-26 | Cirrus Logic, Inc. | Detection of acoustic impulse events in voice applications |
US10248119B2 (en) * | 2015-11-04 | 2019-04-02 | Zoox, Inc. | Interactive autonomous vehicle command controller |
US10319373B2 (en) * | 2016-03-14 | 2019-06-11 | Kabushiki Kaisha Toshiba | Information processing device, information processing method, computer program product, and recognition system |
US10325594B2 (en) | 2015-11-24 | 2019-06-18 | Intel IP Corporation | Low resource key phrase detection for wake on voice |
US10334050B2 (en) | 2015-11-04 | 2019-06-25 | Zoox, Inc. | Software application and logic to modify configuration of an autonomous vehicle |
US10401852B2 (en) | 2015-11-04 | 2019-09-03 | Zoox, Inc. | Teleoperation system and method for trajectory modification of autonomous vehicles |
US10446037B2 (en) | 2015-11-04 | 2019-10-15 | Zoox, Inc. | Software application to request and control an autonomous vehicle service |
US10591910B2 (en) | 2015-11-04 | 2020-03-17 | Zoox, Inc. | Machine-learning systems and techniques to optimize teleoperation and/or planner decisions |
US10650807B2 (en) | 2018-09-18 | 2020-05-12 | Intel Corporation | Method and system of neural network keyphrase detection |
US10712750B2 (en) | 2015-11-04 | 2020-07-14 | Zoox, Inc. | Autonomous vehicle fleet service and system |
US10714122B2 (en) | 2018-06-06 | 2020-07-14 | Intel Corporation | Speech classification of audio for wake on voice |
US10714077B2 (en) * | 2015-07-24 | 2020-07-14 | Samsung Electronics Co., Ltd. | Apparatus and method of acoustic score calculation and speech recognition using deep neural networks |
US11106218B2 (en) | 2015-11-04 | 2021-08-31 | Zoox, Inc. | Adaptive mapping to navigate autonomous vehicles responsive to physical environment changes |
US11127394B2 (en) | 2019-03-29 | 2021-09-21 | Intel Corporation | Method and system of high accuracy keyphrase detection for low resource devices |
US20210375260A1 (en) * | 2020-05-29 | 2021-12-02 | TCL Research America Inc. | Device and method for generating speech animation |
US11227606B1 (en) | 2019-03-31 | 2022-01-18 | Medallia, Inc. | Compact, verifiable record of an audio communication and method for making same |
US11283877B2 (en) | 2015-11-04 | 2022-03-22 | Zoox, Inc. | Software application and logic to modify configuration of an autonomous vehicle |
US11301767B2 (en) | 2015-11-04 | 2022-04-12 | Zoox, Inc. | Automated extraction of semantic information to enhance incremental mapping modifications for robotic vehicles |
US11308978B2 (en) * | 2017-03-31 | 2022-04-19 | Intel Corporation | Systems and methods for energy efficient and low power distributed automatic speech recognition on wearable devices |
WO2022119585A1 (en) * | 2020-12-02 | 2022-06-09 | Medallia, Inc. | Asr-enhanced speech compression |
US11398239B1 (en) | 2019-03-31 | 2022-07-26 | Medallia, Inc. | ASR-enhanced speech compression |
US11501759B1 (en) * | 2021-12-22 | 2022-11-15 | Institute Of Automation, Chinese Academy Of Sciences | Method, system for speech recognition, electronic device and storage medium |
Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7263486B1 (en) * | 2002-10-25 | 2007-08-28 | At&T Corp. | Active learning for spoken language understanding |
US20100114572A1 (en) * | 2007-03-27 | 2010-05-06 | Masahiro Tani | Speaker selecting device, speaker adaptive model creating device, speaker selecting method, speaker selecting program, and speaker adaptive model making program |
US20130151263A1 (en) * | 2010-08-24 | 2013-06-13 | Lg Electronics Inc. | Method and device for processing audio signals |
US8782012B2 (en) * | 2010-08-27 | 2014-07-15 | International Business Machines Corporation | Network analysis |
US20150371633A1 (en) * | 2012-11-01 | 2015-12-24 | Google Inc. | Speech recognition using non-parametric models |
Family Cites Families (103)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US4759068A (en) | 1985-05-29 | 1988-07-19 | International Business Machines Corporation | Constructing Markov models of words from multiple utterances |
US4819271A (en) | 1985-05-29 | 1989-04-04 | International Business Machines Corporation | Constructing Markov model word baseforms from multiple utterances by concatenating model sequences for word segments |
US4799262A (en) | 1985-06-27 | 1989-01-17 | Kurzweil Applied Intelligence, Inc. | Speech recognition |
US4868867A (en) | 1987-04-06 | 1989-09-19 | Voicecraft Inc. | Vector excitation speech or audio coder for transmission or storage |
US4817156A (en) | 1987-08-10 | 1989-03-28 | International Business Machines Corporation | Rapidly training a speech recognizer to a subsequent speaker given training data of a reference speaker |
US5033087A (en) | 1989-03-14 | 1991-07-16 | International Business Machines Corp. | Method and apparatus for the automatic determination of phonological rules as for a continuous speech recognition system |
US5018088A (en) | 1989-10-02 | 1991-05-21 | The Johns Hopkins University | Adaptive locally-optimum detection signal processor and processing methods |
US5268990A (en) | 1991-01-31 | 1993-12-07 | Sri International | Method for recognizing speech using linguistically-motivated hidden Markov models |
US5465318A (en) | 1991-03-28 | 1995-11-07 | Kurzweil Applied Intelligence, Inc. | Method for generating a speech recognition model for a non-vocabulary utterance |
US5680508A (en) | 1991-05-03 | 1997-10-21 | Itt Corporation | Enhancement of speech coding in background noise for low-rate speech coder |
EP0576765A1 (en) | 1992-06-30 | 1994-01-05 | International Business Machines Corporation | Method for coding digital data using vector quantizing techniques and device for implementing said method |
JPH0772840B2 (en) | 1992-09-29 | 1995-08-02 | 日本アイ・ビー・エム株式会社 | Speech model configuration method, speech recognition method, speech recognition device, and speech model training method |
GB9223066D0 (en) | 1992-11-04 | 1992-12-16 | Secr Defence | Children's speech training aid |
US5627939A (en) | 1993-09-03 | 1997-05-06 | Microsoft Corporation | Speech recognition system and method employing data compression |
US5625749A (en) | 1994-08-22 | 1997-04-29 | Massachusetts Institute Of Technology | Segment-based apparatus and method for speech recognition by analyzing multiple speech unit frames and modeling both temporal and spatial correlation |
US5729656A (en) | 1994-11-30 | 1998-03-17 | International Business Machines Corporation | Reduction of search space in speech recognition using phone boundaries and phone ranking |
US6038533A (en) | 1995-07-07 | 2000-03-14 | Lucent Technologies Inc. | System and method for selecting training text |
JP2871561B2 (en) | 1995-11-30 | 1999-03-17 | 株式会社エイ・ティ・アール音声翻訳通信研究所 | Unspecified speaker model generation device and speech recognition device |
US6067517A (en) | 1996-02-02 | 2000-05-23 | International Business Machines Corporation | Transcription of speech data with segments from acoustically dissimilar environments |
US5937384A (en) | 1996-05-01 | 1999-08-10 | Microsoft Corporation | Method and system for speech recognition using continuous density hidden Markov models |
US5745872A (en) | 1996-05-07 | 1998-04-28 | Texas Instruments Incorporated | Method and system for compensating speech signals using vector quantization codebook adaptation |
US5758024A (en) | 1996-06-25 | 1998-05-26 | Microsoft Corporation | Method and system for encoding pronunciation prefix trees |
US6038528A (en) | 1996-07-17 | 2000-03-14 | T-Netix, Inc. | Robust speech processing with affine transform replicated data |
US6151575A (en) | 1996-10-28 | 2000-11-21 | Dragon Systems, Inc. | Rapid adaptation of speech models |
US6260013B1 (en) | 1997-03-14 | 2001-07-10 | Lernout & Hauspie Speech Products N.V. | Speech recognition system employing discriminatively trained models |
US6633842B1 (en) | 1999-10-22 | 2003-10-14 | Texas Instruments Incorporated | Speech recognition front-end feature extraction for noisy speech |
CN1099662C (en) | 1997-09-05 | 2003-01-22 | 中国科学院声学研究所 | Continuous voice identification technology for Chinese putonghua large vocabulary |
US6108627A (en) | 1997-10-31 | 2000-08-22 | Nortel Networks Corporation | Automatic transcription tool |
US5953701A (en) | 1998-01-22 | 1999-09-14 | International Business Machines Corporation | Speech recognition models combining gender-dependent and gender-independent phone states and using phonetic-context-dependence |
US6381569B1 (en) | 1998-02-04 | 2002-04-30 | Qualcomm Incorporated | Noise-compensated speech recognition templates |
US6141641A (en) | 1998-04-15 | 2000-10-31 | Microsoft Corporation | Dynamically configurable acoustic model for speech recognition system |
KR100509797B1 (en) | 1998-04-29 | 2005-08-23 | 마쯔시다덴기산교 가부시키가이샤 | Method and apparatus using decision trees to generate and score multiple pronunciations for a spelled word |
US6243680B1 (en) | 1998-06-15 | 2001-06-05 | Nortel Networks Limited | Method and apparatus for obtaining a transcription of phrases through text and spoken utterances |
US6658385B1 (en) | 1999-03-12 | 2003-12-02 | Texas Instruments Incorporated | Method for transforming HMMs for speaker-independent recognition in a noisy environment |
US6434520B1 (en) | 1999-04-16 | 2002-08-13 | International Business Machines Corporation | System and method for indexing and querying audio archives |
CA2387079C (en) | 1999-10-19 | 2011-10-18 | Sony Electronics Inc. | Natural language interface control system |
US7310600B1 (en) | 1999-10-28 | 2007-12-18 | Canon Kabushiki Kaisha | Language recognition using a similarity measure |
US6681206B1 (en) | 1999-11-05 | 2004-01-20 | At&T Corporation | Method for generating morphemes |
US7085720B1 (en) | 1999-11-05 | 2006-08-01 | At & T Corp. | Method for task classification using morphemes |
US6591235B1 (en) | 2000-02-04 | 2003-07-08 | International Business Machines Corporation | High dimensional data mining and visualization via gaussianization |
US6631348B1 (en) | 2000-08-08 | 2003-10-07 | Intel Corporation | Dynamic speech recognition pattern switching for enhanced speech recognition accuracy |
DE10040063A1 (en) | 2000-08-16 | 2002-02-28 | Philips Corp Intellectual Pty | Procedure for assigning phonemes |
US6876966B1 (en) | 2000-10-16 | 2005-04-05 | Microsoft Corporation | Pattern recognition training method and apparatus using inserted noise followed by noise reduction |
JP4244514B2 (en) | 2000-10-23 | 2009-03-25 | セイコーエプソン株式会社 | Speech recognition method and speech recognition apparatus |
ATE297588T1 (en) | 2000-11-14 | 2005-06-15 | Ibm | ADJUSTING PHONETIC CONTEXT TO IMPROVE SPEECH RECOGNITION |
GB2370401A (en) | 2000-12-19 | 2002-06-26 | Nokia Mobile Phones Ltd | Speech recognition |
US20020087317A1 (en) | 2000-12-29 | 2002-07-04 | Lee Victor Wai Leung | Computer-implemented dynamic pronunciation method and system |
US7113903B1 (en) | 2001-01-30 | 2006-09-26 | At&T Corp. | Method and apparatus for providing stochastic finite-state machine translation |
US7062442B2 (en) | 2001-02-23 | 2006-06-13 | Popcatcher Ab | Method and arrangement for search and recording of media signals |
GB2375673A (en) | 2001-05-14 | 2002-11-20 | Salgen Systems Ltd | Image compression method using a table of hash values corresponding to motion vectors |
GB2375935A (en) | 2001-05-22 | 2002-11-27 | Motorola Inc | Speech quality indication |
US7668718B2 (en) | 2001-07-17 | 2010-02-23 | Custom Speech Usa, Inc. | Synchronized pattern recognition source data processed by manual or automatic means for creation of shared speaker-dependent speech user profile |
US20030033143A1 (en) | 2001-08-13 | 2003-02-13 | Hagai Aronowitz | Decreasing noise sensitivity in speech processing under adverse conditions |
US7571095B2 (en) | 2001-08-15 | 2009-08-04 | Sri International | Method and apparatus for recognizing speech in a noisy environment |
US7035789B2 (en) | 2001-09-04 | 2006-04-25 | Sony Corporation | Supervised automatic text generation based on word classes for language modeling |
US6950796B2 (en) | 2001-11-05 | 2005-09-27 | Motorola, Inc. | Speech recognition by dynamical noise model adaptation |
US6985861B2 (en) | 2001-12-12 | 2006-01-10 | Hewlett-Packard Development Company, L.P. | Systems and methods for combining subword recognition and whole word recognition of a spoken input |
JP3696231B2 (en) | 2002-10-08 | 2005-09-14 | 松下電器産業株式会社 | Language model generation and storage device, speech recognition device, language model generation method and speech recognition method |
US7467087B1 (en) | 2002-10-10 | 2008-12-16 | Gillick Laurence S | Training and using pronunciation guessers in speech recognition |
JP4352790B2 (en) | 2002-10-31 | 2009-10-28 | セイコーエプソン株式会社 | Acoustic model creation method, speech recognition device, and vehicle having speech recognition device |
US7149688B2 (en) | 2002-11-04 | 2006-12-12 | Speechworks International, Inc. | Multi-lingual speech recognition with cross-language context modeling |
KR100486735B1 (en) | 2003-02-28 | 2005-05-03 | 삼성전자주식회사 | Method of establishing optimum-partitioned classifed neural network and apparatus and method and apparatus for automatic labeling using optimum-partitioned classifed neural network |
US7571097B2 (en) | 2003-03-13 | 2009-08-04 | Microsoft Corporation | Method for training of subspace coded gaussian models |
US8849185B2 (en) | 2003-04-15 | 2014-09-30 | Ipventure, Inc. | Hybrid audio delivery system and method therefor |
JP2004325897A (en) | 2003-04-25 | 2004-11-18 | Pioneer Electronic Corp | Apparatus and method for speech recognition |
US7499857B2 (en) | 2003-05-15 | 2009-03-03 | Microsoft Corporation | Adaptation of compressed acoustic models |
JP4548646B2 (en) | 2003-09-12 | 2010-09-22 | 株式会社エヌ・ティ・ティ・ドコモ | Noise model noise adaptation system, noise adaptation method, and speech recognition noise adaptation program |
US7647263B2 (en) | 2003-09-19 | 2010-01-12 | Swiss Reinsurance Company | System and method for performing risk analysis |
US7650331B1 (en) | 2004-06-18 | 2010-01-19 | Google Inc. | System and method for efficient large-scale data processing |
JP4301102B2 (en) | 2004-07-22 | 2009-07-22 | ソニー株式会社 | Audio processing apparatus, audio processing method, program, and recording medium |
US20060031069A1 (en) | 2004-08-03 | 2006-02-09 | Sony Corporation | System and method for performing a grapheme-to-phoneme conversion |
US7418383B2 (en) | 2004-09-03 | 2008-08-26 | Microsoft Corporation | Noise robust speech recognition with a switching linear dynamic model |
US7584098B2 (en) | 2004-11-29 | 2009-09-01 | Microsoft Corporation | Vocabulary-independent search of spontaneous speech |
WO2006089055A1 (en) | 2005-02-15 | 2006-08-24 | Bbn Technologies Corp. | Speech analyzing system with adaptive noise codebook |
GB0513820D0 (en) | 2005-07-06 | 2005-08-10 | Ibm | Distributed voice recognition system and method |
US20070088552A1 (en) | 2005-10-17 | 2007-04-19 | Nokia Corporation | Method and a device for speech recognition |
US20070118372A1 (en) | 2005-11-23 | 2007-05-24 | General Electric Company | System and method for generating closed captions |
JP2009524273A (en) | 2005-11-29 | 2009-06-25 | グーグル・インコーポレーテッド | Repetitive content detection in broadcast media |
BRPI0706404B1 (en) | 2006-02-17 | 2019-08-27 | Google Inc | scalable, coding, and adaptive access to distributed models |
CN101166017B (en) | 2006-10-20 | 2011-12-07 | 松下电器产业株式会社 | Automatic murmur compensation method and device for sound generation apparatus |
JP4997601B2 (en) | 2006-11-30 | 2012-08-08 | 独立行政法人産業技術総合研究所 | WEB site system for voice data search |
WO2008108232A1 (en) | 2007-02-28 | 2008-09-12 | Nec Corporation | Audio recognition device, audio recognition method, and audio recognition program |
US7831587B2 (en) | 2007-05-10 | 2010-11-09 | Xerox Corporation | Event hierarchies and memory organization for structured data retrieval |
US20080300875A1 (en) | 2007-06-04 | 2008-12-04 | Texas Instruments Incorporated | Efficient Speech Recognition with Cluster Methods |
US8620662B2 (en) | 2007-11-20 | 2013-12-31 | Apple Inc. | Context-aware unit selection |
US8615397B2 (en) | 2008-04-04 | 2013-12-24 | Intuit Inc. | Identifying audio content using distorted target patterns |
US8239195B2 (en) | 2008-09-23 | 2012-08-07 | Microsoft Corporation | Adapting a compressed model for use in speech recognition |
US8463719B2 (en) | 2009-03-11 | 2013-06-11 | Google Inc. | Audio classification for information retrieval using sparse features |
US9009039B2 (en) | 2009-06-12 | 2015-04-14 | Microsoft Technology Licensing, Llc | Noise adaptive training for speech recognition |
US8886531B2 (en) | 2010-01-13 | 2014-11-11 | Rovi Technologies Corporation | Apparatus and method for generating an audio fingerprint and using a two-stage query |
US8700394B2 (en) | 2010-03-24 | 2014-04-15 | Microsoft Corporation | Acoustic model adaptation using splines |
US8234111B2 (en) | 2010-06-14 | 2012-07-31 | Google Inc. | Speech and noise models for speech recognition |
US8725506B2 (en) | 2010-06-30 | 2014-05-13 | Intel Corporation | Speech audio processing |
EP2431969B1 (en) | 2010-09-15 | 2013-04-03 | Svox AG | Speech recognition with small CPU footprint and reduced quantization error |
US20120143604A1 (en) | 2010-12-07 | 2012-06-07 | Rita Singh | Method for Restoring Spectral Components in Denoised Speech Signals |
ES2459391T3 (en) | 2011-06-06 | 2014-05-09 | Bridge Mediatech, S.L. | Method and system to get audio hashing invariant to the channel |
EP2851895A3 (en) | 2011-06-30 | 2015-05-06 | Google, Inc. | Speech recognition using variable-length context |
CA2806372C (en) | 2012-02-16 | 2016-07-19 | Qnx Software Systems Limited | System and method for dynamic residual noise shaping |
JP5875414B2 (en) | 2012-03-07 | 2016-03-02 | インターナショナル・ビジネス・マシーンズ・コーポレーションＩｎｔｅｒｎａｔｉｏｎａｌ Ｂｕｓｉｎｅｓｓ Ｍａｃｈｉｎｅｓ Ｃｏｒｐｏｒａｔｉｏｎ | Noise suppression method, program and apparatus |
US20130297299A1 (en) | 2012-05-07 | 2013-11-07 | Board Of Trustees Of Michigan State University | Sparse Auditory Reproducing Kernel (SPARK) Features for Noise-Robust Speech and Speaker Recognition |
US9123338B1 (en) | 2012-06-01 | 2015-09-01 | Google Inc. | Background audio identification for speech disambiguation |
US9514753B2 (en) | 2013-11-04 | 2016-12-06 | Google Inc. | Speaker identification using hash-based indexing |
US9299347B1 (en) | 2014-10-22 | 2016-03-29 | Google Inc. | Speech recognition using associative mapping |
-
2014
- 2014-06-23 US US14/311,557 patent/US9858922B2/en active Active
Patent Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7263486B1 (en) * | 2002-10-25 | 2007-08-28 | At&T Corp. | Active learning for spoken language understanding |
US20100114572A1 (en) * | 2007-03-27 | 2010-05-06 | Masahiro Tani | Speaker selecting device, speaker adaptive model creating device, speaker selecting method, speaker selecting program, and speaker adaptive model making program |
US20130151263A1 (en) * | 2010-08-24 | 2013-06-13 | Lg Electronics Inc. | Method and device for processing audio signals |
US8782012B2 (en) * | 2010-08-27 | 2014-07-15 | International Business Machines Corporation | Network analysis |
US20150371633A1 (en) * | 2012-11-01 | 2015-12-24 | Google Inc. | Speech recognition using non-parametric models |
Cited By (38)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10204619B2 (en) | 2014-10-22 | 2019-02-12 | Google Llc | Speech recognition using associative mapping |
US10714077B2 (en) * | 2015-07-24 | 2020-07-14 | Samsung Electronics Co., Ltd. | Apparatus and method of acoustic score calculation and speech recognition using deep neural networks |
US10591910B2 (en) | 2015-11-04 | 2020-03-17 | Zoox, Inc. | Machine-learning systems and techniques to optimize teleoperation and/or planner decisions |
US10248119B2 (en) * | 2015-11-04 | 2019-04-02 | Zoox, Inc. | Interactive autonomous vehicle command controller |
US11061398B2 (en) | 2015-11-04 | 2021-07-13 | Zoox, Inc. | Machine-learning systems and techniques to optimize teleoperation and/or planner decisions |
US11796998B2 (en) | 2015-11-04 | 2023-10-24 | Zoox, Inc. | Autonomous vehicle fleet service and system |
US10712750B2 (en) | 2015-11-04 | 2020-07-14 | Zoox, Inc. | Autonomous vehicle fleet service and system |
US11314249B2 (en) | 2015-11-04 | 2022-04-26 | Zoox, Inc. | Teleoperation system and method for trajectory modification of autonomous vehicles |
US11301767B2 (en) | 2015-11-04 | 2022-04-12 | Zoox, Inc. | Automated extraction of semantic information to enhance incremental mapping modifications for robotic vehicles |
US11106218B2 (en) | 2015-11-04 | 2021-08-31 | Zoox, Inc. | Adaptive mapping to navigate autonomous vehicles responsive to physical environment changes |
US11283877B2 (en) | 2015-11-04 | 2022-03-22 | Zoox, Inc. | Software application and logic to modify configuration of an autonomous vehicle |
US10446037B2 (en) | 2015-11-04 | 2019-10-15 | Zoox, Inc. | Software application to request and control an autonomous vehicle service |
US10401852B2 (en) | 2015-11-04 | 2019-09-03 | Zoox, Inc. | Teleoperation system and method for trajectory modification of autonomous vehicles |
US10334050B2 (en) | 2015-11-04 | 2019-06-25 | Zoox, Inc. | Software application and logic to modify configuration of an autonomous vehicle |
US10325594B2 (en) | 2015-11-24 | 2019-06-18 | Intel IP Corporation | Low resource key phrase detection for wake on voice |
US10937426B2 (en) | 2015-11-24 | 2021-03-02 | Intel IP Corporation | Low resource key phrase detection for wake on voice |
US20170256255A1 (en) * | 2016-03-01 | 2017-09-07 | Intel Corporation | Intermediate scoring and rejection loopback for improved key phrase detection |
US9972313B2 (en) * | 2016-03-01 | 2018-05-15 | Intel Corporation | Intermediate scoring and rejection loopback for improved key phrase detection |
CN105810212A (en) * | 2016-03-07 | 2016-07-27 | 合肥工业大学 | Train whistle recognizing method for complex noise environment |
US10319373B2 (en) * | 2016-03-14 | 2019-06-11 | Kabushiki Kaisha Toshiba | Information processing device, information processing method, computer program product, and recognition system |
US10043521B2 (en) | 2016-07-01 | 2018-08-07 | Intel IP Corporation | User defined key phrase detection by user dependent sequence modeling |
US10044854B2 (en) | 2016-07-07 | 2018-08-07 | ClearCaptions, LLC | Method and system for providing captioned telephone service with automated speech recognition |
US9628620B1 (en) * | 2016-07-07 | 2017-04-18 | ClearCaptions, LLC | Method and system for providing captioned telephone service with automated speech recognition |
US20180102136A1 (en) * | 2016-10-11 | 2018-04-12 | Cirrus Logic International Semiconductor Ltd. | Detection of acoustic impulse events in voice applications using a neural network |
US10475471B2 (en) * | 2016-10-11 | 2019-11-12 | Cirrus Logic, Inc. | Detection of acoustic impulse events in voice applications using a neural network |
US10242696B2 (en) | 2016-10-11 | 2019-03-26 | Cirrus Logic, Inc. | Detection of acoustic impulse events in voice applications |
US10170115B2 (en) * | 2016-12-23 | 2019-01-01 | Intel Corporation | Linear scoring for low power wake on voice |
US10083689B2 (en) * | 2016-12-23 | 2018-09-25 | Intel Corporation | Linear scoring for low power wake on voice |
US11308978B2 (en) * | 2017-03-31 | 2022-04-19 | Intel Corporation | Systems and methods for energy efficient and low power distributed automatic speech recognition on wearable devices |
US10714122B2 (en) | 2018-06-06 | 2020-07-14 | Intel Corporation | Speech classification of audio for wake on voice |
US10650807B2 (en) | 2018-09-18 | 2020-05-12 | Intel Corporation | Method and system of neural network keyphrase detection |
US11127394B2 (en) | 2019-03-29 | 2021-09-21 | Intel Corporation | Method and system of high accuracy keyphrase detection for low resource devices |
US11227606B1 (en) | 2019-03-31 | 2022-01-18 | Medallia, Inc. | Compact, verifiable record of an audio communication and method for making same |
US11398239B1 (en) | 2019-03-31 | 2022-07-26 | Medallia, Inc. | ASR-enhanced speech compression |
US11244668B2 (en) * | 2020-05-29 | 2022-02-08 | TCL Research America Inc. | Device and method for generating speech animation |
US20210375260A1 (en) * | 2020-05-29 | 2021-12-02 | TCL Research America Inc. | Device and method for generating speech animation |
WO2022119585A1 (en) * | 2020-12-02 | 2022-06-09 | Medallia, Inc. | Asr-enhanced speech compression |
US11501759B1 (en) * | 2021-12-22 | 2022-11-15 | Institute Of Automation, Chinese Academy Of Sciences | Method, system for speech recognition, electronic device and storage medium |
Also Published As
Publication number | Publication date |
---|---|
US9858922B2 (en) | 2018-01-02 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US9858922B2 (en) | Caching speech recognition scores | |
US9401143B2 (en) | Cluster specific speech model | |
US11620989B2 (en) | Sub-matrix input for neural network layers | |
US10249289B2 (en) | Text-to-speech synthesis using an autoencoder | |
US20180174576A1 (en) | Acoustic-to-word neural network speech recognizer | |
US8494850B2 (en) | Speech recognition using variable-length context | |
US9472187B2 (en) | Acoustic model training corpus selection | |
US10204619B2 (en) | Speech recognition using associative mapping | |
CN107045871B (en) | Re-recognition of speech using external data sources | |
Liu et al. | Voice Conversion Across Arbitrary Speakers Based on a Single Target-Speaker Utterance. | |
Wang et al. | An acoustic segment modeling approach to query-by-example spoken term detection | |
Weninger et al. | Deep Learning Based Mandarin Accent Identification for Accent Robust ASR. | |
WO2015017259A1 (en) | Context-based speech recognition | |
WO2012064765A1 (en) | Generating acoustic models | |
EP3376497B1 (en) | Text-to-speech synthesis using an autoencoder | |
Wang et al. | Unsupervised spoken term detection with acoustic segment model | |
US11017763B1 (en) | Synthetic speech processing | |
Lugosch et al. | DONUT: CTC-based query-by-example keyword spotting | |
Nijhawan et al. | Speaker recognition using support vector machine | |
US11735156B1 (en) | Synthetic speech processing | |
Liu et al. | Acoustic modeling with neural graph embeddings | |
Liu et al. | State-time-alignment phone clustering based language-independent phone recognizer front-end for phonotactic language recognition | |
Liu et al. | Improving the decoding efficiency of deep neural network acoustic models by cluster-based senone selection | |
Xue et al. | Rapid speaker adaptation based on D-code extracted from BLSTM-RNN in LVCSR | |
Sahu et al. | A quinphone-based context-dependent acoustic modeling for LVCSR |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:WEINSTEIN, EUGENE;KUMAR, SANJIV;MORENO, IGNACIO L.;AND OTHERS;SIGNING DATES FROM 20140616 TO 20140624;REEL/FRAME:033332/0068 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044567/0001Effective date: 20170929 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |