US20230089308A1 - Speaker-Turn-Based Online Speaker Diarization with Constrained Spectral Clustering - Google Patents
Speaker-Turn-Based Online Speaker Diarization with Constrained Spectral Clustering Download PDFInfo
- Publication number
- US20230089308A1 US20230089308A1 US17/644,261 US202117644261A US2023089308A1 US 20230089308 A1 US20230089308 A1 US 20230089308A1 US 202117644261 A US202117644261 A US 202117644261A US 2023089308 A1 US2023089308 A1 US 2023089308A1
- Authority
- US
- United States
- Prior art keywords
- speaker
- segment
- segments
- turn
- sequence
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 230000003595 spectral effect Effects 0.000 title claims abstract description 31
- 238000013518 transcription Methods 0.000 claims abstract description 64
- 230000035897 transcription Effects 0.000 claims abstract description 64
- 230000005236 sound signal Effects 0.000 claims abstract description 56
- 238000000034 method Methods 0.000 claims abstract description 48
- 238000012545 processing Methods 0.000 claims abstract description 31
- 230000015654 memory Effects 0.000 claims description 48
- 238000012549 training Methods 0.000 claims description 23
- 238000013528 artificial neural network Methods 0.000 claims description 9
- 238000004891 communication Methods 0.000 claims description 3
- 239000011159 matrix material Substances 0.000 description 20
- 230000011218 segmentation Effects 0.000 description 15
- 239000013598 vector Substances 0.000 description 15
- 230000008569 process Effects 0.000 description 14
- 238000004590 computer program Methods 0.000 description 8
- 230000003287 optical effect Effects 0.000 description 6
- 239000000284 extract Substances 0.000 description 4
- 230000008859 change Effects 0.000 description 3
- 238000010606 normalization Methods 0.000 description 3
- 238000001514 detection method Methods 0.000 description 2
- 238000000605 extraction Methods 0.000 description 2
- 230000006870 function Effects 0.000 description 2
- 230000003993 interaction Effects 0.000 description 2
- 230000006855 networking Effects 0.000 description 2
- 238000005192 partition Methods 0.000 description 2
- 230000000644 propagated effect Effects 0.000 description 2
- 230000004044 response Effects 0.000 description 2
- 239000004065 semiconductor Substances 0.000 description 2
- 230000003068 static effect Effects 0.000 description 2
- 230000008901 benefit Effects 0.000 description 1
- 230000001143 conditioned effect Effects 0.000 description 1
- 238000000354 decomposition reaction Methods 0.000 description 1
- 230000007423 decrease Effects 0.000 description 1
- 230000000694 effects Effects 0.000 description 1
- 230000002452 interceptive effect Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000012423 maintenance Methods 0.000 description 1
- 230000007246 mechanism Effects 0.000 description 1
- 239000000203 mixture Substances 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 238000005457 optimization Methods 0.000 description 1
- 230000000306 recurrent effect Effects 0.000 description 1
- 238000007670 refining Methods 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 230000006403 short-term memory Effects 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 238000000638 solvent extraction Methods 0.000 description 1
- 238000010025 steaming Methods 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000007704 transition Effects 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L21/00—Processing of the speech or voice signal to produce another audible or non-audible signal, e.g. visual or tactile, in order to modify its quality or its intelligibility
- G10L21/02—Speech enhancement, e.g. noise reduction or echo cancellation
- G10L21/0272—Voice signal separating
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/26—Speech to text systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/06—Creation of reference templates; Training of speech recognition systems, e.g. adaptation to the characteristics of the speaker's voice
- G10L15/063—Training
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/16—Speech classification or search using artificial neural networks
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/06—Creation of reference templates; Training of speech recognition systems, e.g. adaptation to the characteristics of the speaker's voice
- G10L15/063—Training
- G10L2015/0631—Creating reference templates; Clustering
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/27—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 characterised by the analysis technique
- G10L25/30—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 characterised by the analysis technique using neural networks
Definitions
- This disclosure relates to speaker-turn-based online speaker diarization with constrained spectral clustering.
- Speaker diarization is the process of partitioning an input audio stream into homogenous segments according to speaker identity.
- speaker diarization answers the question “who is speaking when” and has a variety of applications including multimedia information retrieval, speaker turn analysis, audio processing, and automatic transcription of conversational speech to name a few.
- speaker diarization involves the task of annotating speaker turns in a conversation by identifying that a first segment of an input audio stream is attributable to a first human speaker (without particularly identifying who the first human speaker is), a second segment of the input audio stream is attributable to a different second human speaker (without particularly identifying who the second human speaker is), a third segment of the input audio stream is attributable to the first human speaker, etc.
- One aspect of the disclosure provides a computer-implemented method that when executed on data processing hardware causes the data processing hardware to perform operations for speaker-turn-based online speaker diarization.
- the operations include receiving an input audio signal that corresponds to utterances spoken by multiple speakers.
- the operations also include processing, using a speech recognition model, the input audio signal to jointly generate as output from the speech recognition model a transcription of the utterances and a sequence of speaker turn tokens.
- Each speaker turn token indicates a location of a respective speaker turn detected in the transcription between a respective pair of adjacent terms.
- the operations also include segmenting the input audio signal into a plurality of speaker segments based on the sequence of speaker tokens.
- the operations include extracting a corresponding speaker-discriminative embedding from the speaker segment.
- the operations also include performing spectral clustering on the speaker-discriminative embeddings extracted from the plurality of speaker segments to cluster the plurality of speaker segments into k classes.
- the operations include assigning a respective speaker label to each speaker segment clustered into the respective class that is different than the respective speaker label assigned to the speaker segments clustered into each other class of the k classes.
- Implementations of the disclosure may include one or more of the following optional features.
- the operations further include annotating the transcription of the utterances based on the speaker label assigned to each speaker segment.
- each speaker turn token in the sequence of speaker turn tokens has a corresponding timestamp.
- segmenting the input audio signal into the plurality of speaker segments based on the sequence of speaker turn tokens includes segmenting the input audio signal into initial speaker segments each bounded by the corresponding timestamps of a respective pair of adjacent speaker turn tokens in the sequence of speaker turn tokens.
- the operations further include segmenting the initial speaker segment into two or more reduced-duration speaker segments that have respective durations less than or equal to the segment duration threshold.
- the plurality of speaker segments segmented from the input audio signal include the initial speaker segments that have respective durations less than or equal to the segment duration threshold and the reduced-duration speaker segments that are further segmented from any of the initial speaker segments having respective durations that exceed the segment duration threshold.
- extracting a corresponding speaker-discriminative embedding from the speaker segment includes receiving the speaker segment as input to a speaker encoder model and generating the corresponding speaker-discriminative embedding as output from the speaker encoder model.
- the speaker encoder model includes a long-short term memory-based (LSTM-based) speaker encoder model configured to extract the corresponding speaker-discriminative embedding from each speaker segment.
- the operations further include predicting a confidence of the respective speaker turn detected in the transcription for each speaker turn token in the sequence of speaker turn tokens generated as output from the speech recognition model and determining pairwise constraints based on the confidences predicted for the speaker turn token.
- the spectral clustering performed on the speaker-discriminative embeddings is constrained by the pairwise constraints.
- the speech recognition model includes a streaming transducer-based speech recognition model that includes: an audio encoder configured to receive as sequence of acoustic frames as input and generate, at each of a plurality of time steps, a higher order feature representations for a corresponding acoustic frame in the sequence of acoustic frames; a label encoder configured to receive a sequence of non-blank symbols output by a final softmax layer as input and generate a dense representation at each of the plurality of time steps; and a joint network configured to receive, as input, the higher order feature representation generated by the audio encoder at each of the plurality of time steps and the dense representation generated by the label encoder at each of the plurality of time steps and generate, at each of the plurality of time steps, a probability distribution over possible speech recognition hypotheses at the corresponding time step.
- the audio encoder may include a neural network that has a plurality of transformer layers.
- the label encoder includes a bigram embedding lookup decoder model
- the speech recognition model may be trained on training samples that each include training utterances spoken by two or more different speakers and are paired with a corresponding ground-truth transcription of the training utterances.
- each ground-truth transcription is injected with ground-truth speaker turn tokens that indicate location where speaker turns occur in the ground-truth transcriptions.
- the corresponding ground-truth transcription of each training sample may not be annotated with any timestamp information.
- Another aspect of the disclosure provides a system that includes data processing hardware and memory hardware storing instructions that when executed on the data processing hardware causes the data processing hardware to perform operations.
- the operations include receiving an input audio signal that corresponds to utterances spoken by multiple speakers.
- the operations also include processing, using a speech recognition model, the input audio signal to jointly generate as output from the speech recognition model a transcription of the utterances and a sequence of speaker turn tokens.
- Each speaker turn token indicates a location of a respective speaker turn detected in the transcription between a respective pair of adjacent terms.
- the operations also include segmenting the input audio signal into a plurality of speaker segments based on the sequence of speaker tokens.
- the operations include extracting a corresponding speaker-discriminative embedding from the speaker segment.
- the operations also include performing spectral clustering on the speaker-discriminative embeddings extracted from the plurality of speaker segments to cluster the plurality of speaker segments into k classes.
- the operations include assigning a respective speaker label to each speaker segment clustered into the respective class that is different than the respective speaker label assigned to the speaker segments clustered into each other class of the k classes.
- Implementations of the disclosure may include one or more of the following optional features.
- the operations further include annotating the transcription of the utterances based on the speaker label assigned to each speaker segment.
- each speaker turn token in the sequence of speaker turn tokens has a corresponding timestamp.
- segmenting the input audio signal into the plurality of speaker segments based on the sequence of speaker turn tokens includes segmenting the input audio signal into initial speaker segments each bounded by the corresponding timestamps of a respective pair of adjacent speaker turn tokens in the sequence of speaker turn tokens.
- the operations further include segmenting the initial speaker segment into two or more reduced-duration speaker segments that have respective durations less than or equal to the segment duration threshold.
- the plurality of speaker segments segmented from the input audio signal include the initial speaker segments that have respective durations less than or equal to the segment duration threshold and the reduced-duration speaker segments that are further segmented from any of the initial speaker segments having respective durations that exceed the segment duration threshold.
- extracting a corresponding speaker-discriminative embedding from the speaker segment includes receiving the speaker segment as input to a speaker encoder model and generating the corresponding speaker-discriminative embedding as output from the speaker encoder model.
- the speaker encoder model includes a long-short term memory-based (LSTM-based) speaker encoder model configured to extract the corresponding speaker-discriminative embedding from each speaker segment.
- the operations further include predicting a confidence of the respective speaker turn detected in the transcription for each speaker turn token in the sequence of speaker turn tokens generated as output from the speech recognition model and determining pairwise constraints based on the confidences predicted for the speaker turn token.
- the spectral clustering performed on the speaker-discriminative embeddings is constrained by the pairwise constraints.
- the speech recognition model includes a streaming transducer-based speech recognition model that includes: an audio encoder configured to receive as sequence of acoustic frames as input and generate, at each of a plurality of time steps, a higher order feature representations for a corresponding acoustic frame in the sequence of acoustic frames; a label encoder configured to receive a sequence of non-blank symbols output by a final softmax layer as input and generate a dense representation at each of the plurality of time steps; and a joint network configured to receive, as input, the higher order feature representation generated by the audio encoder at each of the plurality of time steps and the dense representation generated by the label encoder at each of the plurality of time steps and generate, at each of the plurality of time steps, a probability distribution over possible speech recognition hypotheses at the corresponding time step.
- the audio encoder may include a neural network that has a plurality of transformer layers.
- the label encoder includes a bigram embedding lookup decoder model
- the speech recognition model may be trained on training samples that each include training utterances spoken by two or more different speakers and are paired with a corresponding ground-truth transcription of the training utterances.
- each ground-truth transcription is injected with ground-truth speaker turn tokens that indicate location where speaker turns occur in the ground-truth transcriptions.
- the corresponding ground-truth transcription of each training sample may not be annotated with any timestamp information.
- FIG. 1 is a schematic view an example speaker diarization system for performing speaker diarization.
- FIG. 2 is a schematic view of the example speaker diarization system of FIG. 1 .
- FIG. 3 is a schematic view of an example automatic speech recognition model with a transducer-based architecture.
- FIG. 4 is a flowchart of an example arrangement of operations for a computer-implemented method of performing speaker diarization on an input audio signal containing utterances of speech spoken by multiple different speakers.
- FIG. 5 is a schematic view of an example computing device that may be used to implement the systems and methods described herein.
- ASR Automatic speech recognition
- speaker diarization is the process of segmenting speech from multiple speakers engaged in a larger conversation to not specifically determine who is talking (speaker recognition/identification), but rather, determine when someone is speaking.
- speaker diarization includes a series of speaker recognition tasks with short utterances and determines whether two segments of a given conversation were spoken by the same individual or different individuals, and is repeated for all segments of the conversation. Accordingly, speaker diarization detects speaker turns from a conversation that includes multiple speakers.
- the term ‘speaker turn’ refers to the transition from one individual speaking to a different individual speaking in a larger conversation.
- Existing speaker diarization systems generally include multiple relatively independent components, such as, without limitation, a speech segmentation module, an embedding extraction module, and a clustering module.
- the speech segmentation module is generally configured to remove non-speech parts from an input utterance and divide the entire input utterance into fixed-length segments and/or word-length segments. Although dividing the input utterance into fixed-length segments is easy to implement, often times it is difficult to find a good segment length. That is, long fixed-length segments may include several speaker turns, while short segments include insufficient speaker information. Moreover, ASR models that generate word-length segments are usually spoken by a single speaker, however, individual words also include insufficient speaker information.
- the embedding extraction module is configured to extract, from each segment, a corresponding speaker-discriminative embedding.
- the speaker-discriminative embedding may include i-vectors or d-vectors.
- the clustering modules employed by the existing speaker diarization systems are tasked with determining the number of speakers present in the input utterance and assign speaker identities (e.g., labels) to each segment.
- These clustering modules may use popular clustering algorithms that include Gaussian mixture models, links clustering, and spectral clustering. Speaker diarization systems may also use an additional re-segmentation module for further refining the diarization results output from the clustering module by enforcing additional constraints.
- the clustering module may execute online clustering algorithms that often have low quality or offline clustering algorithms that can only return diarization results at an end of an entire input sequence. In some examples, to achieve both high quality while minimizing latency, clustering algorithms are run offline in an online fashion.
- the clustering algorithm runs offline on the entire sequence of all existing embeddings.
- the sequence of speaker-discriminative embeddings is long.
- Implementations herein are directed toward an online speaker diarization system that includes a speech recognition model that performs both speech recognition and speaker turn detection (i.e., when the active speaker changes) on received utterances spoken by multiple speakers.
- the speaker diarization system segments the utterances into speaker segments based on detected speaker turns and extracts speaker-discriminative embeddings therefrom.
- each speaker segment segmented from the utterances based on speaker turn detection include continuous speech from a speaker that carries sufficient information to extract robust speaker-discriminative embeddings.
- the number of speaker turns i.e., number of speaker changes
- the number of speaker turns is usually much smaller than the number of fixed-length segments, thereby reducing the computational cost of executing the clustering algorithm since speaker-discriminative embeddings are only extracted from the speaker segments which are bounded by the speaker turns.
- the speaker diarization system executes spectral clustering on the entire sequence of all existing speaker-discriminative embeddings.
- the speech recognition model output speech recognition results and detected speaker turns in a streaming fashion to allow streaming execution of the spectral clustering.
- the turn-wise speaker-discriminative embeddings are sparsely extracted from speaker segments (i.e., only after speaker turns)
- the sequence of all existing speaker-discriminative embeddings is relatively short even for relatively long conversations (i.e., multiple hours). Therefore, the execution of the online spectral clustering is computationally inexpensive while maintaining low latency such that the spectral clustering may be deployed on-device.
- training time is drastically reduced since a human annotator is not required to assign accurate timestamps to speaker turns and manually identify different speakers across these turns.
- Annotating time stamps and identifying speakers across turns is a time consuming process that may take about two hours for a single annotator to annotate 10 minutes of audio for one pass.
- the speech recognition model is trained to detect speaker turns from the semantic information conveyed in the speech recognition results such that each detected speaker turn is associated with a corresponding timestamp known by the speech recognition model. As such, these timestamps are not annotated by a human and can be used to segment the training audio data into corresponding speaker segments.
- a system 100 includes a user device 110 capturing speech utterances 120 from a group of speakers (e.g., users) 10 , 10 a - n and communicating with a cloud computing environment 140 via a network 130 .
- the cloud computing environment 140 may be a distributed system having scalable/elastic resources 142 .
- the resources 142 include computing resources 142 (e.g., data processing hardware) and/or storage resources 146 (e.g., memory hardware).
- the user device 110 and/or the cloud computing environment 140 executes a diarization system 200 that is configured to receive an input audio signal (i.e., audio data) 122 that corresponds to the captured utterances 120 from the multiple speakers 10 .
- the diarization system 200 processes the input audio signal 122 and generates a transcription 220 of the captured utterances 120 and a sequence of speaker turn tokens 224 , 224 a - n .
- the speaker turn tokens 224 indicate a speaker turn (e.g., speaker change) detected in the transcription 220 between a respective pair of adjacent terms.
- the diarization system 200 segments the input audio signal 122 into a plurality of speaker segments 225 , 225 a - n each associated with a corresponding speaker discriminative embedding 240 extracted therefrom.
- the diarization system 200 generates diarization results 280 based on the speaker-discriminative embeddings 240 and pairwise constraints 226 .
- the diarization results 280 include a corresponding speaker label 250 assigned to each speaker segment 225 .
- the user device 110 includes data processing hardware 112 and memory hardware 114 .
- the user device 110 may include an audio capture device (e.g., microphone) for capturing and converting the utterances 120 from the speakers 10 into the audio data 122 (e.g., electrical signals).
- the data processing hardware 112 is configured to execute a portion of the diarization system 200 locally while a remaining portion of the diarization system 200 executes on the cloud computing environment 140 .
- the data processing hardware 112 may execute the diarization system 200 in lieu of executing the diarization system 200 on the cloud computing environment 140 .
- the user device 110 can be any computing device capable of communicating with the cloud computing environment 140 through the network 130 .
- the user device 110 includes, but is not limited to, desktop computing devices and mobile computing devices, such as laptops, tablets, smart phones, smart speakers/displays, smart appliances, internet-of-things (IoT) devices, and wearable computing devices (e.g., headsets and/or watches).
- desktop computing devices and mobile computing devices such as laptops, tablets, smart phones, smart speakers/displays, smart appliances, internet-of-things (IoT) devices, and wearable computing devices (e.g., headsets and/or watches).
- the speakers 10 and the user devices 110 may be located within an environment (e.g., a room) where the user device 110 is configured to capture and covert speech utterances 120 spoken by the speakers 10 into the input audio signal 122 (also referred to as audio data 122 ).
- the speakers may correspond to co-workers having a conversation during a meeting and the user device 110 may record and convert the speech utterances into the input audio signal 122 .
- the user device 110 may provide the input audio signal 122 to the diarization system 200 for predicting which speaker 10 is speaking for each segment of speech.
- the diarization system 200 is tasked with processing the input audio signal 122 to determine when someone is speaking without specifically determining who is talking via speaker recognition/identification.
- the user device 110 is remotely located from the speakers 10 .
- the user device may include a remote device (e.g., a network server) that captures speech utterances 120 from speakers that are participants in a phone call or video conference.
- each speaker 10 (or group of multiple speakers 10 ) would speak into their own device (e.g., phone, radio, computer, smartwatch, etc.) that captures and provides the speech utterances 120 to the remote user device 110 for converting the speech utterances 120 into the audio data 122 .
- the utterances 120 may undergo processing at each of the user devices and be converted into corresponding input audio signals 122 that are transmitted to the remote user device 110 which may additionally process the input audio signal 122 provided as input to the diarization system 200 .
- the diarization system 200 includes an ASR model 300 , a segmentation module 210 , a speaker encoder 230 , and a clustering module 260 .
- the ASR model 300 is configured to receive the input audio signal 122 and process the input audio signal 122 to jointly generate a transcription 220 of the utterances 120 and a sequence of speaker turn tokens 224 , 224 a - n .
- the ASR model 300 may include a streaming ASR model 300 that jointly generates the transcriptions 220 and the speaker turn tokens 224 in a streaming fashion as the input audio signal 122 is received.
- the transcription 220 includes the sequence of speaker turn tokens 224 that indicates a location of a respective speaker turn detected in the transcription 220 between a respective pair of adjacent terms.
- the utterance 120 may include “hello how are you I am good” and the ASR model 300 generates the transcription 220 “hello how are you ⁇ st> I am good.”
- ⁇ st> represents a speaker turn token 224 indicating the speaker turn between the adjacent terms ‘you’ and ‘I.’
- Each speaker turn token 224 in the sequence of speaker turn tokens 224 may also include a corresponding timestamp 223 .
- the ASR model 300 may utilize the diarization results 280 for improving speech recognition on the audio data 122 .
- the ASR model 300 may apply different speech recognition models (e.g., language models, prosody models) for different speakers identified from the diarization results 280 .
- the ASR model 300 and/or the diarization system 200 may index the transcription 220 of the audio data 122 using the speaker labels 250 of each speaker segment 225 . For instance, a transcription of a conversation between multiple co-workers (e.g., speakers 10 ) during a business meeting may be indexed by speaker to associate portions of the transcription 220 with the respective speaker 10 for identifying what each speaker said.
- the ASR model 300 may include any transducer-based architecture including, but not limited to, transformer-transducer (T-T), recurrent neural network transducer (RNN-T), and/or conformer-transducer (C-T).
- T-T transformer-transducer
- RNN-T recurrent neural network transducer
- C-T conformer-transducer
- the ASR model 300 is trained on training samples that each include training utterances spoken by two or more different speakers 10 paired with a corresponding ground-truth transcription of the training utterances.
- Each ground-truth transcription is injected with ground-truth speaker turn tokens that indicated locations where speaker turns occur in the ground-truth transcription.
- the corresponding ground-truth transcription of each training sample is not annotated with any timestamp information.
- the segmentation module 210 is configured to receive the audio data 122 corresponding to the speech utterance 120 (also referred to as ‘utterance of speech’) and segment the audio data 122 into a plurality of speaker segments 225 , 225 a - n .
- the segmentation module 210 receives the audio data 122 and the transcription 220 that includes the sequence of speaker turn tokens 224 with the corresponding timestamps 223 to segment the audio data 122 into a plurality of speaker segments 225 .
- each speaker segment 225 corresponds to audio data between two speaker turn tokens 224 .
- the segmentation module 210 may further remove non-speech parts from the audio data 122 , (e.g., by applying a voice activity detector).
- the segmentation module 210 further segments speaker segments 225 that exceed a segment duration threshold, described in greater detail below.
- the segmentation module 210 segments the input audio signal 122 into the plurality of speaker segments 225 by segmenting the input audio signal 122 into initial speaker segments 225 each bounded by the corresponding timestamps 223 of a respective pair of adjacent speaker turn tokens 224 .
- the input audio signal 122 may include fifteen seconds of audio with the sequence speaker turn tokens 224 having timestamps 223 at three seconds, six seconds, and fourteen seconds.
- the segmentation module 210 segments the input audio signal into three initial speaker segments 225 bounded by the speaker turn tokens 224 with timestamps 223 at three seconds, six seconds, and fourteen seconds.
- initial speaker segments 225 may have a respective duration that exceeds a segment duration threshold.
- the segmentation module 210 further segments initial speaker segments 225 into two or more reduced-duration speaker segments 225 that have respective durations less than or equal to the segment duration threshold.
- the segmentation module may determine the initial speaker segment 225 bounded by the speaker turn tokens 224 timestamped at six seconds and fourteen seconds (e.g., having a duration of eight seconds) exceeds a segment duration threshold of six seconds.
- the segmentation module 210 may further segment the initial speaker segment 225 into two or more reduced-duration speaker segments 225 having respective durations less than or equal to the segment duration threshold.
- the segmentation module 210 may segment the eight second initial speaker segment 225 into a first reduced-duration speaker segment 225 that has a duration of six seconds and a second reduced-duration speaker segment 225 that has a duration of two seconds.
- the plurality of speaker segments 225 segmented from the input audio signal 122 may include both the initial speaker segments 225 having respective durations less than or equal to the segment duration threshold and the reduced-duration speaker segments 225 further segmented from any of the initial speaker segments 225 having respective durations that exceed the segment duration threshold.
- the speaker-discriminative embeddings 240 may include speaker vectors such as d-vectors or i-vectors.
- the speaker encoder 230 may include a text-independent speaker encoder model trained with a generalized end-to-end extended-set softmax loss.
- the speaker encoder may include a long-short term memory-based (LSTM-based) speaker encoder model configured to extract the corresponding speaker-discriminative embedding 240 from each speaker segment 225 .
- speaker encoder 230 includes (3) long short-term memory (LSTM) layers with 768 nodes and a projection size of 256.
- LSTM long short-term memory
- the each speaker turn token 224 in the sequence of speaker turn tokens 224 resets the LSTM states of the speaker encoder 230 such that the speaker-discriminative embeddings 240 do not include information from other speaker segments 225 .
- the speaker encoder 230 may only extract a speaker-discriminative embedding 240 corresponding to a portion of the speaker segment 225 . Accordingly, the speaker-discriminative embedding 240 includes sufficient information from the speaker segment 225 , but is not too close to the speaker turn boundary such that the speaker-discriminative embedding 240 may include inaccurate information or contain overlapping speech from another speaker 10 .
- the clustering module 260 receives the speaker-discriminative embeddings 240 for each speaker segment 225 and the pairwise constraints 226 , and is configured to predict speaker labels 250 for each speaker-discriminative embedding 240 . Simply put, the clustering module 260 predicts which speaker 10 spoke each speaker segment 225 . More specifically, the clustering module 260 performs spectral clustering on the speaker-discriminative embeddings 240 extracted from the plurality of speaker segments 225 to cluster the plurality of speaker segments 225 into k classes 262 . The k classes 262 represents the predicted number of active speakers included in the received utterance 120 .
- the clustering module 260 assigns a respective speaker label 250 to each speaker segment 225 clustered into the respective class 262 that is different than the respective speaker label 250 assigned to the speaker segments 225 clustered into each other class 262 of the k classes 262 .
- the ASR model 300 may also generate the pairwise constraints 226 to further constrain the spectral clustering performed on the speaker-discriminative embeddings 240 . That is, the ASR model 300 may predict a confidence for each speaker turn token 224 detected in the transcription 220 .
- the pairwise constraints 226 may indicate contextual information about adjacent speaker segments 225 .
- adjacent speaker segments 225 may include any combination of both speaker segments 225 having a duration less than the segment duration threshold, one speaker segment 225 having a duration less than the segment duration threshold and one speaker segment 225 having a reduced-duration (i.e., initial speaker segment exceeded segment duration threshold), or both speaker segments 225 having reduced-durations.
- the confidence of the respective speaker turn detected in the transcription 220 and the context information are used to further constrain the spectral clustering by the clustering module 260 .
- the clustering module 260 performs spectral clustering performed on the speaker-discriminative embeddings 240 that is constrained by the pairwise constraints 226 received from the ASR model 300 . For instance, when the both speaker segments 225 have durations less than the segment duration threshold, spectral clustering is constrained to encourage speaker labels 250 to be different for adjacent speaker segments 225 separated by speaker turn tokens with a high confidence. In other instances, when both speaker segments 225 have reduced-durations, the spectral clustering is constrained to encourage speaker labels for adjacent speaker segments 225 to be the same.
- the spectral clustering is constrained based on the confidence of the speaker turn token 224 .
- the clustering module 260 is constrained to encourage different speaker labels 250 .
- the clustering module 260 may be constrained to encourage the same speaker label 250 .
- the diarization system 200 annotates the transcription 220 of the utterances 120 based on the speaker label 250 assigned to each speaker segment 225 (i.e., diarization results 280 ). For instance, a transcription 220 of a conversation between multiple speakers 10 may be indexed by speaker to associated portions of the transcription 220 with the respective speaker 10 for identifying what each speaker 10 said in the transcription 220 .
- the annotated transcription 220 may be stored in memory hardware 114 , 146 of the user device 110 or the cloud computing environment 140 to be accessed later by one of the speakers 10 .
- FIG. 2 illustrates a schematic view of the diarization system 200 that includes the ASR model (i.e., transformer transducer) 300 , the segmentation module 210 , the speaker encoder 230 , and the clustering module 260 .
- the ASR model 300 processes the input audio signal 122 corresponding to the utterances 120 spoken by the multiple speakers 10 ( FIG. 1 ) to generate the transcriptions 220 of the utterances and the sequence of speaker turn tokens 224 .
- the transcription 220 includes one or more terms 222 corresponding to words spoken by the multiple speakers.
- the sequence of speaker turn tokens 224 indicates a location of a respective speaker turn detected in the transcription 220 between a respective pair of adjacent terms 222 .
- the input audio signal 122 may include an utterance where first and second terms 222 were spoken by a first speaker 10 , third and fourth terms 222 were spoken by a second speaker 10 , and fifth and sixth terms 222 were spoken by a third speaker 10 .
- the ASR model 300 generates a first speaker token 224 between the second term 222 and the third term 222 to indicate the speaker turn from the first speaker to the second speaker, and a second speaker token 224 between the fourth term 222 and fifth term 222 to indicate the speaker turn from the second speaker to the third speaker.
- the ASR model 300 generates a start of speech (SOS) token 227 that indicates the start of an utterance and an end of speech (EOS) token 229 that indicates the end of an utterance.
- SOS start of speech
- EOS end of speech
- the ASR model 300 processes acoustic information and/or semantic information to detect speaker turns in the input audio signal 122 . That is, using natural language understanding (NLU) the ASR model 300 can determine for an utterance “How are you I'm good,” that “how are you” and “I'm good” were likely spoken by different users independent of any acoustic processing of the input audio signal 122 . This semantic interpretation of the transcription 220 may be used independently or in conjunction with acoustic processing of the input audio signal 122 .
- NLU natural language understanding
- the speaker encoder 230 receives the plurality of speaker segments 225 from the segmentation module 210 ( FIG. 1 ) and extracts the corresponding speaker-discriminative embedding 240 for each speaker segment 225 .
- the speaker-discriminative embeddings 240 may include speaker vectors such as d-vectors or i-vectors.
- the speaker encoder 230 provides the speaker-discriminative embeddings 240 associated with each speaker segment 225 to the clustering module 260 .
- the clustering module 260 receives the speaker-discriminative embeddings 240 for each speaker segment 225 and the pairwise constraints 226 , and is configured to predict speaker labels 250 for each speaker-discriminative embedding 240 .
- the clustering module 260 constructs a similarity graph by computing pairwise similarities a ij where A represents the affinity matrix E N ⁇ N of the similarity graph.
- the affinity of two samples x i and x j may be represented by
- a ij 1 2 ⁇ ( 1 + cos ⁇ ( x i , x j ) ) .
- the clustering module 260 identifies a partition so that edges connecting different clusters have low eights, and edges within a cluster have high weights.
- the similarity graph is connected or only includes a few connected components and very few isolated vertices.
- Spectral clustering is sensitive to quality and noises of the similarity graph, therefore, the clustering module 260 performs several refinement operations on the affinity matrix to model the local neighborhood relationships between data samples.
- One refinement operation includes row-wise thresholding with p-percentile that sets diagonal values of the affinity matrix to 0, sets affinity values that are larger than the p-percentile values to 1, multiply affinity values by 0.01 that are smaller than the p-percentile of the row, and resetting diagonal values of the affinity matrix to 1.
- Another refinement operation includes applying an average summarization operation to make the affinity matrix positive semi-definite using the following equation,
- the diarization error rate (DER) is significantly affected by the hyper parameter p for the p-percentile. Accordingly, a ratio value r(p) is a good proxy of the DER such that maximum eigengap is large while not generating an excessive amount of connections in the similarity graph.
- the clustering module 260 applies eigen-decomposition to estimate the number of k classes 262 using the maximum eigengap method.
- the clustering module 260 chooses the first class k 262 of eigen-vectors and applies a row-wise re-normalization of the spectral embeddings and applies k-means algorithm on the spectral embeddings to predict speaker labels 250 .
- the clustering module 260 receives pairwise constraints 226 indicating the confidence of the speaker turn tokens 224 and context information to constrain the spectral clustering.
- the pairwise constraints 226 are configured to encourage different speaker labels 250 for adjacent speaker segments 225 with a high confidence speaker turn token 224 and encourage the same speaker labels 250 for adjacent speaker segments 225 with a low confidence speaker turn token 224 .
- pairwise constraints 226 Q constrained spectral clustering identifies one or more partitions that maximize constraint satisfaction and minimizes the cost on the similarity graph G.
- the pairwise constraints 226 may be represented by Q ⁇ N ⁇ N .
- the clustering module 260 processes the constraint matrix Q by:
- the clustering module 260 defines the adjacent speaker segments 225 as “cannot-link” (CL).
- CL cannot-link
- the CL definition indicates that the speaker label 250 between the adjacent speaker segments 225 has a high likelihood of being different.
- ML muscle-link
- the ML definition indicates that the speaker label 250 between the adjacent speaker segments 225 has a high likelihood of being the same.
- the ML defined adjacent speaker segments 225 are treated as a positive class and the CL defined adjacent speaker segments 225 as a negative class.
- the initial constraint matrix is added to adjust Q(t).
- a parameter ⁇ is used to control the relative amount of constraint information from adjacent speaker segments 225 and the initial constraints 226 .
- the clustering module preforms vertical propagation first until the convergence and then horizontal propagation by the following algorithm:
- Q* has a closed-form solution formulated by:
- the clustering module 260 obtains an adjusted affinity matrix ⁇ ij by:
- ⁇ ij ⁇ 1 - ( 1 - Q ij * ) ⁇ ( 1 - A ij ) , If ⁇ Q ij * ⁇ 0 ; ( 1 + Q ij * ) ⁇ A ij , If ⁇ Q ij * ⁇ 0. ( 3 )
- the affinity matrix increases the similarity between sample x i and x j .
- the affinity matrix decreases the similarity between x i and x j .
- the clustering module 260 performs normalized Laplacian matrix based spectral clustering to predict speaker labels 250 for the speaker segments 225 .
- the clustering module 260 generates diarization results 280 include a first speaker label 250 a that indicates the first speaker spoke a first speaker segment 225 a (i.e., first and second terms 222 ), a second speaker label 250 b that indicates the second speaker spoke a second speaker segment 225 b (i.e., third and fourth terms 222 ), and a third speaker label 250 c that indicates the third speaker spoke a third speaker segment 225 c (i.e., fifth and sixth terms 222 ).
- the ASR model 300 may provide end-to-end (E2E) speech recognition by integrating acoustic, pronunciation, and language models into a single neural network, and does not require a lexicon or a separate text normalization component.
- E2E end-to-end
- the ASR model 300 may include a steaming Transformer-Transducer (T-T) model architecture, which adheres to latency constraints associated with interactive applications.
- T-T steaming Transformer-Transducer
- the ASR model 300 may similarly include a RNN-T model architecture or a Conformer-Transducer (C-T) model architecture.
- the ASR model 300 provides a small computational footprint and utilizes less memory requirements than conventional ASR architectures, making the T-T model architecture suitable for performing speech recognition entirely on the user device 110 (e.g., no communication with the cloud computing environment 140 is required).
- the ASR model 300 includes an audio encoder 310 , a label encoder 320 , and a joint network 330 .
- d-dimensional feature vectors e.g., speaker segments 225 ( FIG. 1 )
- each speaker segment 225 ( FIG. 1 ) includes a sequence of acoustic frames (e.g., audio data 122 ) that corresponds to the respective speaker segment 225 ( FIG. 1 ).
- This higher-order feature representation is denoted as ah 1 , . . . , ah T .
- the label encoder 320 may also include a neural network of transformer layers or a look-up table embedding model, which, like a language model (LM), processes the sequence of non-blank symbols output by a final Softmax layer 340 so far, y 0 , . . . , y ui-1 , (e.g., the one or more terms 222 including speaker turn tokens 224 as shown in FIG. 2 ) into a dense representation 322 (denoted by Ih u ) that encodes predicted label history.
- LM language model
- each transformer layer may include a normalization layer, a masked multi-head attention layer with relative position encoding, a residual connection, a feed forward layer, and a dropout layer.
- the label encoder 320 may include two transformer layers.
- the embedding model is configured to learn a weight vector of the d-dimension for each possible bigram label context, where d is the dimension of the outputs of the audio and label encoders 310 , 320 .
- the total number of parameters in the embedding model is N 2 ⁇ d where Nis the vocabulary size of the labels.
- the learned weight vector is then used as the embedding of the bigram label context in the ASR model 300 to produce fast label encoder 320 runtimes.
- the representations produced by the audio and label encoders 310 , 320 are combined by the joint network 330 using a dense layer Jo.
- the joint network 330 then predicts P(z u,t
- the joint network 330 generates, at each output step (e.g., time step), a probability distribution over possible speech recognition hypotheses 342 for the one or more terms 222 of the transcription 220 ( FIG. 2 ).
- the “possible speech recognition hypotheses” correspond to a set of output labels (also referred to as “speech units”) each representing a grapheme (e.g., symbol/character), term 222 ( FIG. 2 ), or a word piece in a specified natural language.
- the set of output labels may include twenty-seven (27) symbols, e.g., one label for each of the 26-letters in the English alphabet and one label designating a space.
- the joint network 330 may output a set of values indicative of the likelihood of occurrence of each of a predetermined set of output labels.
- This set of values can be a vector (e.g., a one-hot vector) and can indicate a probability distribution over the set of output labels.
- the output labels are graphemes (e.g., individual characters, and potentially punctuation and other symbols), but the set of output labels is not so limited.
- the set of output labels can include wordpieces and/or entire words, in addition to or instead of graphemes.
- the output distribution of the joint network 330 can include a posterior probability value for each of the different output labels. Thus, if there are 100 different output labels representing different graphemes or other symbols, the output z u,t of the joint network 330 can include 100 different probability values, one for each output label.
- the probability distribution can then be used to select and assign scores to candidate orthographic elements (e.g., graphemes, wordpieces, and/or words) in a beam search process (e.g., by the Softmax layer 340 ) for determining the transcription.
- the Softmax layer 340 may employ any technique to select the output label/symbol with the highest probability in the distribution as the next output symbol predicted by the ASR model 300 at the corresponding output step. In this manner, the ASR model 300 does not make a conditional independence assumption, rather the prediction of each symbol is conditioned not only on the acoustics but also on the sequence of labels output so far.
- FIG. 4 is a flowchart of an exemplary arrangement of operations for a computer-implemented method 400 of performing speaker diarization on a received utterance 120 of speech.
- the data processing hardware 112 , 144 may execute the operations for the method 400 by executing instructions stored on the memory hardware 114 , 146 .
- the method 400 includes receiving an input audio signal 122 corresponding to utterances 120 spoken by multiple speakers 10 , 10 a - n .
- the method 400 includes processing, using a speech recognition model (e.g., ASR model) 300 , the input audio signal 122 to jointly generate as output from the speech recognition model 300 a transcription 220 of the utterances 120 and a sequence of speaker turn tokens 224 , 224 a - n .
- Each speaker turn token 224 indicates a location of a respective speaker turn detected in the transcription 220 between a respective pair of adjacent terms 222 .
- the method 400 includes segmenting the input audio signal 122 into a plurality of speaker segments 225 based on the sequence of speaker tokens 224 .
- the method 400 includes, for each speaker segment 225 of the plurality of speaker segments 225 , extracting a corresponding speaker-discriminative embedding 240 from the speaker segment 225 .
- the method 400 includes performing spectral clustering on the speaker-discriminative embeddings 240 extracted from the plurality of speaker segments 225 to cluster the plurality of speaker segments 225 into k classes 262 .
- the method 400 includes, for each respective class 262 of the k classes 262 , assigning a respective speaker label 250 to each speaker segment 225 clustered into the respective class 262 that is different than the respective speaker label 250 assigned to the speaker segments 225 clustered into each other class 262 of the k classes 262 .
- a software application may refer to computer software that causes a computing device to perform a task.
- a software application may be referred to as an “application,” an “app,” or a “program.”
- Example applications include, but are not limited to, system diagnostic applications, system management applications, system maintenance applications, word processing applications, spreadsheet applications, messaging applications, media streaming applications, social networking applications, and gaming applications.
- the non-transitory memory may be physical devices used to store programs (e.g., sequences of instructions) or data (e.g., program state information) on a temporary or permanent basis for use by a computing device.
- the non-transitory memory may be volatile and/or non-volatile addressable semiconductor memory. Examples of non-volatile memory include, but are not limited to, flash memory and read-only memory (ROM)/programmable read-only memory (PROM)/erasable programmable read-only memory (EPROM)/electronically erasable programmable read-only memory (EEPROM) (e.g., typically used for firmware, such as boot programs).
- Examples of volatile memory include, but are not limited to, random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), phase change memory (PCM) as well as disks or tapes.
- FIG. 5 is schematic view of an example computing device 500 that may be used to implement the systems and methods described in this document.
- the computing device 500 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers.
- the components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document.
- the computing device 500 includes a processor 510 , memory 520 , a storage device 530 , a high-speed interface/controller 540 connecting to the memory 520 and high-speed expansion ports 550 , and a low speed interface/controller 560 connecting to a low speed bus 570 and a storage device 530 .
- Each of the components 510 , 520 , 530 , 540 , 550 , and 560 are interconnected using various busses, and may be mounted on a common motherboard or in other manners as appropriate.
- the processor 510 can process instructions for execution within the computing device 500 , including instructions stored in the memory 520 or on the storage device 530 to display graphical information for a graphical user interface (GUI) on an external input/output device, such as display 580 coupled to high speed interface 540 .
- GUI graphical user interface
- multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory.
- multiple computing devices 500 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system).
- the memory 520 stores information non-transitorily within the computing device 500 .
- the memory 520 may be a computer-readable medium, a volatile memory unit(s), or non-volatile memory unit(s).
- the non-transitory memory 520 may be physical devices used to store programs (e.g., sequences of instructions) or data (e.g., program state information) on a temporary or permanent basis for use by the computing device 500 .
- non-volatile memory examples include, but are not limited to, flash memory and read-only memory (ROM)/programmable read-only memory (PROM)/erasable programmable read-only memory (EPROM)/electronically erasable programmable read-only memory (EEPROM) (e.g., typically used for firmware, such as boot programs).
- volatile memory examples include, but are not limited to, random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), phase change memory (PCM) as well as disks or tapes.
- the storage device 530 is capable of providing mass storage for the computing device 500 .
- the storage device 530 is a computer-readable medium.
- the storage device 530 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations.
- a computer program product is tangibly embodied in an information carrier.
- the computer program product contains instructions that, when executed, perform one or more methods, such as those described above.
- the information carrier is a computer- or machine-readable medium, such as the memory 520 , the storage device 530 , or memory on processor 510 .
- the high speed controller 540 manages bandwidth-intensive operations for the computing device 500 , while the low speed controller 560 manages lower bandwidth-intensive operations. Such allocation of duties is exemplary only.
- the high-speed controller 540 is coupled to the memory 520 , the display 580 (e.g., through a graphics processor or accelerator), and to the high-speed expansion ports 550 , which may accept various expansion cards (not shown).
- the low-speed controller 560 is coupled to the storage device 530 and a low-speed expansion port 590 .
- the low-speed expansion port 590 which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet), may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- input/output devices such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- the computing device 500 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a standard server 500 a or multiple times in a group of such servers 500 a , as a laptop computer 500 b , or as part of a rack server system 500 c.
- implementations of the systems and techniques described herein can be realized in digital electronic and/or optical circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof.
- ASICs application specific integrated circuits
- These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- the processes and logic flows described in this specification can be performed by one or more programmable processors, also referred to as data processing hardware, executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows can also be performed by special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer.
- a processor will receive instructions and data from a read only memory or a random access memory or both.
- the essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- mass storage devices for storing data
- a computer need not have such devices.
- Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks.
- the processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- one or more aspects of the disclosure can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor, or touch screen for displaying information to the user and optionally a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor, or touch screen for displaying information to the user and optionally a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- Other kinds of devices can be used to provide interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input
Abstract
A method includes receiving an input audio signal that corresponds to utterances spoken by multiple speakers. The method also includes processing the input audio to generate a transcription of the utterances and a sequence of speaker turn tokens each indicating a location of a respective speaker turn. The method also includes segmenting the input audio signal into a plurality of speaker segments based on the sequence of speaker tokens. The method also includes extracting a speaker-discriminative embedding from each speaker segment and performing spectral clustering on the speaker-discriminative embeddings to cluster the plurality of speaker segments into k classes. The method also includes assigning a respective speaker label to each speaker segment clustered into the respective class that is different than the respective speaker label assigned to the speaker segments clustered into each other class of the k classes.
Description
- This U.S. Patent application claims priority under 35 U.S.C. § 119(e) to U.S. Provisional Application 63/261,536, filed on Sep. 23, 2021. The disclosure of this prior application is considered part of the disclosure of this application and is hereby incorporated by reference in its entirety.
- This disclosure relates to speaker-turn-based online speaker diarization with constrained spectral clustering.
- Speaker diarization is the process of partitioning an input audio stream into homogenous segments according to speaker identity. In an environment with multiple speakers, speaker diarization answers the question “who is speaking when” and has a variety of applications including multimedia information retrieval, speaker turn analysis, audio processing, and automatic transcription of conversational speech to name a few. For example, speaker diarization involves the task of annotating speaker turns in a conversation by identifying that a first segment of an input audio stream is attributable to a first human speaker (without particularly identifying who the first human speaker is), a second segment of the input audio stream is attributable to a different second human speaker (without particularly identifying who the second human speaker is), a third segment of the input audio stream is attributable to the first human speaker, etc.
- One aspect of the disclosure provides a computer-implemented method that when executed on data processing hardware causes the data processing hardware to perform operations for speaker-turn-based online speaker diarization. The operations include receiving an input audio signal that corresponds to utterances spoken by multiple speakers. The operations also include processing, using a speech recognition model, the input audio signal to jointly generate as output from the speech recognition model a transcription of the utterances and a sequence of speaker turn tokens. Each speaker turn token indicates a location of a respective speaker turn detected in the transcription between a respective pair of adjacent terms. The operations also include segmenting the input audio signal into a plurality of speaker segments based on the sequence of speaker tokens. For each speaker segment of the plurality of speaker segments, the operations include extracting a corresponding speaker-discriminative embedding from the speaker segment. The operations also include performing spectral clustering on the speaker-discriminative embeddings extracted from the plurality of speaker segments to cluster the plurality of speaker segments into k classes. For each respective class of the k classes, the operations include assigning a respective speaker label to each speaker segment clustered into the respective class that is different than the respective speaker label assigned to the speaker segments clustered into each other class of the k classes.
- Implementations of the disclosure may include one or more of the following optional features. In some implementations, the operations further include annotating the transcription of the utterances based on the speaker label assigned to each speaker segment. In some examples, each speaker turn token in the sequence of speaker turn tokens has a corresponding timestamp. In these examples, segmenting the input audio signal into the plurality of speaker segments based on the sequence of speaker turn tokens includes segmenting the input audio signal into initial speaker segments each bounded by the corresponding timestamps of a respective pair of adjacent speaker turn tokens in the sequence of speaker turn tokens. In some implementations, for each initial speaker segment that has a respective duration that exceeds a segment duration threshold, the operations further include segmenting the initial speaker segment into two or more reduced-duration speaker segments that have respective durations less than or equal to the segment duration threshold. Here, the plurality of speaker segments segmented from the input audio signal include the initial speaker segments that have respective durations less than or equal to the segment duration threshold and the reduced-duration speaker segments that are further segmented from any of the initial speaker segments having respective durations that exceed the segment duration threshold.
- In some implementations, extracting a corresponding speaker-discriminative embedding from the speaker segment includes receiving the speaker segment as input to a speaker encoder model and generating the corresponding speaker-discriminative embedding as output from the speaker encoder model. In these implementations, the speaker encoder model includes a long-short term memory-based (LSTM-based) speaker encoder model configured to extract the corresponding speaker-discriminative embedding from each speaker segment. In some examples, the operations further include predicting a confidence of the respective speaker turn detected in the transcription for each speaker turn token in the sequence of speaker turn tokens generated as output from the speech recognition model and determining pairwise constraints based on the confidences predicted for the speaker turn token. Here, the spectral clustering performed on the speaker-discriminative embeddings is constrained by the pairwise constraints.
- In some implementations, the speech recognition model includes a streaming transducer-based speech recognition model that includes: an audio encoder configured to receive as sequence of acoustic frames as input and generate, at each of a plurality of time steps, a higher order feature representations for a corresponding acoustic frame in the sequence of acoustic frames; a label encoder configured to receive a sequence of non-blank symbols output by a final softmax layer as input and generate a dense representation at each of the plurality of time steps; and a joint network configured to receive, as input, the higher order feature representation generated by the audio encoder at each of the plurality of time steps and the dense representation generated by the label encoder at each of the plurality of time steps and generate, at each of the plurality of time steps, a probability distribution over possible speech recognition hypotheses at the corresponding time step. Here, the audio encoder may include a neural network that has a plurality of transformer layers. In some examples, the label encoder includes a bigram embedding lookup decoder model.
- The speech recognition model may be trained on training samples that each include training utterances spoken by two or more different speakers and are paired with a corresponding ground-truth transcription of the training utterances. Here, each ground-truth transcription is injected with ground-truth speaker turn tokens that indicate location where speaker turns occur in the ground-truth transcriptions. Optionally, the corresponding ground-truth transcription of each training sample may not be annotated with any timestamp information.
- Another aspect of the disclosure provides a system that includes data processing hardware and memory hardware storing instructions that when executed on the data processing hardware causes the data processing hardware to perform operations. The operations include receiving an input audio signal that corresponds to utterances spoken by multiple speakers. The operations also include processing, using a speech recognition model, the input audio signal to jointly generate as output from the speech recognition model a transcription of the utterances and a sequence of speaker turn tokens. Each speaker turn token indicates a location of a respective speaker turn detected in the transcription between a respective pair of adjacent terms. The operations also include segmenting the input audio signal into a plurality of speaker segments based on the sequence of speaker tokens. For each speaker segment of the plurality of speaker segments, the operations include extracting a corresponding speaker-discriminative embedding from the speaker segment. The operations also include performing spectral clustering on the speaker-discriminative embeddings extracted from the plurality of speaker segments to cluster the plurality of speaker segments into k classes. For each respective class of the k classes, the operations include assigning a respective speaker label to each speaker segment clustered into the respective class that is different than the respective speaker label assigned to the speaker segments clustered into each other class of the k classes.
- Implementations of the disclosure may include one or more of the following optional features. In some implementations, the operations further include annotating the transcription of the utterances based on the speaker label assigned to each speaker segment. In some examples, each speaker turn token in the sequence of speaker turn tokens has a corresponding timestamp. In these examples, segmenting the input audio signal into the plurality of speaker segments based on the sequence of speaker turn tokens includes segmenting the input audio signal into initial speaker segments each bounded by the corresponding timestamps of a respective pair of adjacent speaker turn tokens in the sequence of speaker turn tokens. In some implementations, for each initial speaker segment that has a respective duration that exceeds a segment duration threshold, the operations further include segmenting the initial speaker segment into two or more reduced-duration speaker segments that have respective durations less than or equal to the segment duration threshold. Here, the plurality of speaker segments segmented from the input audio signal include the initial speaker segments that have respective durations less than or equal to the segment duration threshold and the reduced-duration speaker segments that are further segmented from any of the initial speaker segments having respective durations that exceed the segment duration threshold.
- In some implementations, extracting a corresponding speaker-discriminative embedding from the speaker segment includes receiving the speaker segment as input to a speaker encoder model and generating the corresponding speaker-discriminative embedding as output from the speaker encoder model. In these implementations, the speaker encoder model includes a long-short term memory-based (LSTM-based) speaker encoder model configured to extract the corresponding speaker-discriminative embedding from each speaker segment. In some examples, the operations further include predicting a confidence of the respective speaker turn detected in the transcription for each speaker turn token in the sequence of speaker turn tokens generated as output from the speech recognition model and determining pairwise constraints based on the confidences predicted for the speaker turn token. Here, the spectral clustering performed on the speaker-discriminative embeddings is constrained by the pairwise constraints.
- In some implementations, the speech recognition model includes a streaming transducer-based speech recognition model that includes: an audio encoder configured to receive as sequence of acoustic frames as input and generate, at each of a plurality of time steps, a higher order feature representations for a corresponding acoustic frame in the sequence of acoustic frames; a label encoder configured to receive a sequence of non-blank symbols output by a final softmax layer as input and generate a dense representation at each of the plurality of time steps; and a joint network configured to receive, as input, the higher order feature representation generated by the audio encoder at each of the plurality of time steps and the dense representation generated by the label encoder at each of the plurality of time steps and generate, at each of the plurality of time steps, a probability distribution over possible speech recognition hypotheses at the corresponding time step. Here, the audio encoder may include a neural network that has a plurality of transformer layers. In some examples, the label encoder includes a bigram embedding lookup decoder model.
- The speech recognition model may be trained on training samples that each include training utterances spoken by two or more different speakers and are paired with a corresponding ground-truth transcription of the training utterances. Here, each ground-truth transcription is injected with ground-truth speaker turn tokens that indicate location where speaker turns occur in the ground-truth transcriptions. Optionally, the corresponding ground-truth transcription of each training sample may not be annotated with any timestamp information.
- The details of one or more implementations of the disclosure are set forth in the accompanying drawings and the description below. Other aspects, features, and advantages will be apparent from the description and drawings, and from the claims.
-
FIG. 1 is a schematic view an example speaker diarization system for performing speaker diarization. -
FIG. 2 is a schematic view of the example speaker diarization system ofFIG. 1 . -
FIG. 3 is a schematic view of an example automatic speech recognition model with a transducer-based architecture. -
FIG. 4 is a flowchart of an example arrangement of operations for a computer-implemented method of performing speaker diarization on an input audio signal containing utterances of speech spoken by multiple different speakers. -
FIG. 5 is a schematic view of an example computing device that may be used to implement the systems and methods described herein. - Like reference symbols in the various drawings indicate like elements.
- Automatic speech recognition (ASR) systems generally rely on speech processing algorithms that assume only one speaker is present in a given input audio signal. An input audio signal that includes a presence of multiple speakers can potentially disrupt these speech processing algorithms, thereby leading to inaccurate speech recognition results output by the ASR systems. These ASR systems include a speaker diarization system to answer the question of “who is speaking when.” As such, speaker diarization is the process of segmenting speech from multiple speakers engaged in a larger conversation to not specifically determine who is talking (speaker recognition/identification), but rather, determine when someone is speaking. Put another way, speaker diarization includes a series of speaker recognition tasks with short utterances and determines whether two segments of a given conversation were spoken by the same individual or different individuals, and is repeated for all segments of the conversation. Accordingly, speaker diarization detects speaker turns from a conversation that includes multiple speakers. As used herein the term ‘speaker turn’ refers to the transition from one individual speaking to a different individual speaking in a larger conversation.
- Existing speaker diarization systems generally include multiple relatively independent components, such as, without limitation, a speech segmentation module, an embedding extraction module, and a clustering module. The speech segmentation module is generally configured to remove non-speech parts from an input utterance and divide the entire input utterance into fixed-length segments and/or word-length segments. Although dividing the input utterance into fixed-length segments is easy to implement, often times it is difficult to find a good segment length. That is, long fixed-length segments may include several speaker turns, while short segments include insufficient speaker information. Moreover, ASR models that generate word-length segments are usually spoken by a single speaker, however, individual words also include insufficient speaker information. The embedding extraction module is configured to extract, from each segment, a corresponding speaker-discriminative embedding. The speaker-discriminative embedding may include i-vectors or d-vectors.
- The clustering modules employed by the existing speaker diarization systems are tasked with determining the number of speakers present in the input utterance and assign speaker identities (e.g., labels) to each segment. These clustering modules may use popular clustering algorithms that include Gaussian mixture models, links clustering, and spectral clustering. Speaker diarization systems may also use an additional re-segmentation module for further refining the diarization results output from the clustering module by enforcing additional constraints. The clustering module may execute online clustering algorithms that often have low quality or offline clustering algorithms that can only return diarization results at an end of an entire input sequence. In some examples, to achieve both high quality while minimizing latency, clustering algorithms are run offline in an online fashion. For instance, responsive to receiving each speaker-discriminative embedding, the clustering algorithm runs offline on the entire sequence of all existing embeddings. Implementing these examples, however, can be very computationally expensive if the sequence of speaker-discriminative embeddings is long.
- Implementations herein are directed toward an online speaker diarization system that includes a speech recognition model that performs both speech recognition and speaker turn detection (i.e., when the active speaker changes) on received utterances spoken by multiple speakers. The speaker diarization system segments the utterances into speaker segments based on detected speaker turns and extracts speaker-discriminative embeddings therefrom. Advantageously, each speaker segment segmented from the utterances based on speaker turn detection include continuous speech from a speaker that carries sufficient information to extract robust speaker-discriminative embeddings. Moreover, for long duration conversational utterances, the number of speaker turns (i.e., number of speaker changes) is usually much smaller than the number of fixed-length segments, thereby reducing the computational cost of executing the clustering algorithm since speaker-discriminative embeddings are only extracted from the speaker segments which are bounded by the speaker turns.
- In response to receiving a new speaker-discriminative embedding, the speaker diarization system executes spectral clustering on the entire sequence of all existing speaker-discriminative embeddings. Thus, the speech recognition model output speech recognition results and detected speaker turns in a streaming fashion to allow streaming execution of the spectral clustering. Advantageously, since the turn-wise speaker-discriminative embeddings are sparsely extracted from speaker segments (i.e., only after speaker turns), the sequence of all existing speaker-discriminative embeddings is relatively short even for relatively long conversations (i.e., multiple hours). Therefore, the execution of the online spectral clustering is computationally inexpensive while maintaining low latency such that the spectral clustering may be deployed on-device.
- Moreover, training time is drastically reduced since a human annotator is not required to assign accurate timestamps to speaker turns and manually identify different speakers across these turns. Annotating time stamps and identifying speakers across turns is a time consuming process that may take about two hours for a single annotator to annotate 10 minutes of audio for one pass. Instead, the speech recognition model is trained to detect speaker turns from the semantic information conveyed in the speech recognition results such that each detected speaker turn is associated with a corresponding timestamp known by the speech recognition model. As such, these timestamps are not annotated by a human and can be used to segment the training audio data into corresponding speaker segments.
- Referring to
FIG. 1 , asystem 100 includes auser device 110capturing speech utterances 120 from a group of speakers (e.g., users) 10, 10 a-n and communicating with acloud computing environment 140 via anetwork 130. Thecloud computing environment 140 may be a distributed system having scalable/elastic resources 142. Theresources 142 include computing resources 142 (e.g., data processing hardware) and/or storage resources 146 (e.g., memory hardware). In some implementations, theuser device 110 and/or thecloud computing environment 140 executes adiarization system 200 that is configured to receive an input audio signal (i.e., audio data) 122 that corresponds to the capturedutterances 120 from themultiple speakers 10. Thediarization system 200 processes theinput audio signal 122 and generates atranscription 220 of the capturedutterances 120 and a sequence ofspeaker turn tokens speaker turn tokens 224 indicate a speaker turn (e.g., speaker change) detected in thetranscription 220 between a respective pair of adjacent terms. Using the sequence ofspeaker turn tokens 224, thediarization system 200 segments theinput audio signal 122 into a plurality ofspeaker segments diarization system 200 generates diarization results 280 based on the speaker-discriminative embeddings 240 andpairwise constraints 226. The diarization results 280 include acorresponding speaker label 250 assigned to eachspeaker segment 225. - The
user device 110 includesdata processing hardware 112 andmemory hardware 114. Theuser device 110 may include an audio capture device (e.g., microphone) for capturing and converting theutterances 120 from thespeakers 10 into the audio data 122 (e.g., electrical signals). In some implementations, thedata processing hardware 112 is configured to execute a portion of thediarization system 200 locally while a remaining portion of thediarization system 200 executes on thecloud computing environment 140. Alternatively, thedata processing hardware 112 may execute thediarization system 200 in lieu of executing thediarization system 200 on thecloud computing environment 140. Theuser device 110 can be any computing device capable of communicating with thecloud computing environment 140 through thenetwork 130. Theuser device 110 includes, but is not limited to, desktop computing devices and mobile computing devices, such as laptops, tablets, smart phones, smart speakers/displays, smart appliances, internet-of-things (IoT) devices, and wearable computing devices (e.g., headsets and/or watches). - In the example shown, the
speakers 10 and theuser devices 110 may be located within an environment (e.g., a room) where theuser device 110 is configured to capture andcovert speech utterances 120 spoken by thespeakers 10 into the input audio signal 122 (also referred to as audio data 122). For instance, the speakers may correspond to co-workers having a conversation during a meeting and theuser device 110 may record and convert the speech utterances into theinput audio signal 122. In turn, theuser device 110 may provide theinput audio signal 122 to thediarization system 200 for predicting whichspeaker 10 is speaking for each segment of speech. Thus, thediarization system 200 is tasked with processing theinput audio signal 122 to determine when someone is speaking without specifically determining who is talking via speaker recognition/identification. - In some examples, at least a portion of the
utterances 120 conveyed in theinput audio signal 122 are overlapping such that at a given instant in time, voices of two or more of thespeakers 10 are active. Notably, a number N of themultiple speakers 10 may be unknown when theinput audio signal 122 is provided as input to thediarization system 200 and the diarization system may predict the number N of themultiple speakers 10. In some implementations, theuser device 110 is remotely located from thespeakers 10. For instance, the user device may include a remote device (e.g., a network server) that capturesspeech utterances 120 from speakers that are participants in a phone call or video conference. In this scenario, each speaker 10 (or group of multiple speakers 10) would speak into their own device (e.g., phone, radio, computer, smartwatch, etc.) that captures and provides thespeech utterances 120 to theremote user device 110 for converting thespeech utterances 120 into theaudio data 122. Of course in this scenario, theutterances 120 may undergo processing at each of the user devices and be converted into corresponding input audio signals 122 that are transmitted to theremote user device 110 which may additionally process theinput audio signal 122 provided as input to thediarization system 200. - In the example shown, the
diarization system 200 includes anASR model 300, asegmentation module 210, aspeaker encoder 230, and aclustering module 260. TheASR model 300 is configured to receive theinput audio signal 122 and process theinput audio signal 122 to jointly generate atranscription 220 of theutterances 120 and a sequence ofspeaker turn tokens ASR model 300 may include astreaming ASR model 300 that jointly generates thetranscriptions 220 and thespeaker turn tokens 224 in a streaming fashion as theinput audio signal 122 is received. Thetranscription 220 includes the sequence ofspeaker turn tokens 224 that indicates a location of a respective speaker turn detected in thetranscription 220 between a respective pair of adjacent terms. For example, theutterance 120 may include “hello how are you I am good” and theASR model 300 generates thetranscription 220 “hello how are you <st> I am good.” In this example, <st> represents a speaker turn token 224 indicating the speaker turn between the adjacent terms ‘you’ and ‘I.’ Each speaker turn token 224 in the sequence ofspeaker turn tokens 224 may also include acorresponding timestamp 223. - Optionally, the
ASR model 300 may utilize the diarization results 280 for improving speech recognition on theaudio data 122. For instance, theASR model 300 may apply different speech recognition models (e.g., language models, prosody models) for different speakers identified from the diarization results 280. Additionally or alternatively, theASR model 300 and/or the diarization system 200 (or some other component) may index thetranscription 220 of theaudio data 122 using the speaker labels 250 of eachspeaker segment 225. For instance, a transcription of a conversation between multiple co-workers (e.g., speakers 10) during a business meeting may be indexed by speaker to associate portions of thetranscription 220 with therespective speaker 10 for identifying what each speaker said. - The
ASR model 300 may include any transducer-based architecture including, but not limited to, transformer-transducer (T-T), recurrent neural network transducer (RNN-T), and/or conformer-transducer (C-T). TheASR model 300 is trained on training samples that each include training utterances spoken by two or moredifferent speakers 10 paired with a corresponding ground-truth transcription of the training utterances. Each ground-truth transcription is injected with ground-truth speaker turn tokens that indicated locations where speaker turns occur in the ground-truth transcription. Here, the corresponding ground-truth transcription of each training sample is not annotated with any timestamp information. - The
segmentation module 210 is configured to receive theaudio data 122 corresponding to the speech utterance 120 (also referred to as ‘utterance of speech’) and segment theaudio data 122 into a plurality ofspeaker segments segmentation module 210 receives theaudio data 122 and thetranscription 220 that includes the sequence ofspeaker turn tokens 224 with the correspondingtimestamps 223 to segment theaudio data 122 into a plurality ofspeaker segments 225. Here, eachspeaker segment 225 corresponds to audio data between twospeaker turn tokens 224. Optionally, thesegmentation module 210 may further remove non-speech parts from theaudio data 122, (e.g., by applying a voice activity detector). In some examples, thesegmentation module 210 furthersegments speaker segments 225 that exceed a segment duration threshold, described in greater detail below. - The
segmentation module 210 segments theinput audio signal 122 into the plurality ofspeaker segments 225 by segmenting theinput audio signal 122 intoinitial speaker segments 225 each bounded by the correspondingtimestamps 223 of a respective pair of adjacentspeaker turn tokens 224. For example, theinput audio signal 122 may include fifteen seconds of audio with the sequencespeaker turn tokens 224 havingtimestamps 223 at three seconds, six seconds, and fourteen seconds. In this instance, thesegmentation module 210 segments the input audio signal into threeinitial speaker segments 225 bounded by thespeaker turn tokens 224 withtimestamps 223 at three seconds, six seconds, and fourteen seconds. - In some implementations,
initial speaker segments 225 may have a respective duration that exceeds a segment duration threshold. In these implementations, thesegmentation module 210 further segmentsinitial speaker segments 225 into two or more reduced-duration speaker segments 225 that have respective durations less than or equal to the segment duration threshold. Continuing with the above example, the segmentation module may determine theinitial speaker segment 225 bounded by thespeaker turn tokens 224 timestamped at six seconds and fourteen seconds (e.g., having a duration of eight seconds) exceeds a segment duration threshold of six seconds. In this scenario, thesegmentation module 210 may further segment theinitial speaker segment 225 into two or more reduced-duration speaker segments 225 having respective durations less than or equal to the segment duration threshold. Here, thesegmentation module 210 may segment the eight secondinitial speaker segment 225 into a first reduced-duration speaker segment 225 that has a duration of six seconds and a second reduced-duration speaker segment 225 that has a duration of two seconds. Accordingly, the plurality ofspeaker segments 225 segmented from theinput audio signal 122 may include both theinitial speaker segments 225 having respective durations less than or equal to the segment duration threshold and the reduced-duration speaker segments 225 further segmented from any of theinitial speaker segments 225 having respective durations that exceed the segment duration threshold. - The
speaker encoder 230 is configured to receive the plurality ofspeaker segments 225 and, for eachspeaker segment 225 of the plurality ofspeaker segments 225, extract a corresponding speaker-discriminative embedding 240 from thespeaker segment 225 as output. Thereafter, the speaker encoder provides an observation sequence of embeddings X=(x1, x2, . . . , xT) to theclustering module 260, where entry xT in the sequence represents a real-valued speaker-discriminative embedding 240 associated with acorresponding speaker segment 225 in theaudio data 122 of theoriginal utterance 120. The speaker-discriminative embeddings 240 may include speaker vectors such as d-vectors or i-vectors. - In some examples, the
speaker encoder 230 may include a text-independent speaker encoder model trained with a generalized end-to-end extended-set softmax loss. The speaker encoder may include a long-short term memory-based (LSTM-based) speaker encoder model configured to extract the corresponding speaker-discriminative embedding 240 from eachspeaker segment 225. In particular,speaker encoder 230 includes (3) long short-term memory (LSTM) layers with 768 nodes and a projection size of 256. Here, the output of the last LSTM is transformed to a final 256-dimsonon d-vector. - In some implementations, the each speaker turn token 224 in the sequence of
speaker turn tokens 224 resets the LSTM states of thespeaker encoder 230 such that the speaker-discriminative embeddings 240 do not include information fromother speaker segments 225. For instance, thespeaker encoder 230 may only extract a speaker-discriminative embedding 240 corresponding to a portion of thespeaker segment 225. Accordingly, the speaker-discriminative embedding 240 includes sufficient information from thespeaker segment 225, but is not too close to the speaker turn boundary such that the speaker-discriminative embedding 240 may include inaccurate information or contain overlapping speech from anotherspeaker 10. - The
clustering module 260 receives the speaker-discriminative embeddings 240 for eachspeaker segment 225 and thepairwise constraints 226, and is configured to predict speaker labels 250 for each speaker-discriminative embedding 240. Simply put, theclustering module 260 predicts whichspeaker 10 spoke eachspeaker segment 225. More specifically, theclustering module 260 performs spectral clustering on the speaker-discriminative embeddings 240 extracted from the plurality ofspeaker segments 225 to cluster the plurality ofspeaker segments 225 intok classes 262. The kclasses 262 represents the predicted number of active speakers included in the receivedutterance 120. Thereafter, for eachrespective class 262 of the kclasses 262, theclustering module 260 assigns arespective speaker label 250 to eachspeaker segment 225 clustered into therespective class 262 that is different than therespective speaker label 250 assigned to thespeaker segments 225 clustered into eachother class 262 of the kclasses 262. - The
ASR model 300 may also generate thepairwise constraints 226 to further constrain the spectral clustering performed on the speaker-discriminative embeddings 240. That is, theASR model 300 may predict a confidence for each speaker turn token 224 detected in thetranscription 220. Moreover, thepairwise constraints 226 may indicate contextual information aboutadjacent speaker segments 225. For instance,adjacent speaker segments 225 may include any combination of bothspeaker segments 225 having a duration less than the segment duration threshold, onespeaker segment 225 having a duration less than the segment duration threshold and onespeaker segment 225 having a reduced-duration (i.e., initial speaker segment exceeded segment duration threshold), or bothspeaker segments 225 having reduced-durations. The confidence of the respective speaker turn detected in thetranscription 220 and the context information (collectively referred to as constraints 226), are used to further constrain the spectral clustering by theclustering module 260. - In some implementations, the
clustering module 260 performs spectral clustering performed on the speaker-discriminative embeddings 240 that is constrained by thepairwise constraints 226 received from theASR model 300. For instance, when the bothspeaker segments 225 have durations less than the segment duration threshold, spectral clustering is constrained to encouragespeaker labels 250 to be different foradjacent speaker segments 225 separated by speaker turn tokens with a high confidence. In other instances, when bothspeaker segments 225 have reduced-durations, the spectral clustering is constrained to encourage speaker labels foradjacent speaker segments 225 to be the same. That is, because the adjacent reduced-duration speaker segments 225 were divided based on exceeding the segment duration threshold rather than a speaker turn token 224, there is a high likelihood that the adjacent reduced-duration speaker segments 225 are spoken by thesame speaker 10. In some examples, when onespeaker segment 225 having a duration less than the segment duration threshold is adjacent to anotherspeaker segment 225 having a reduced-duration, the spectral clustering is constrained based on the confidence of the speaker turn token 224. Here, when the speaker turn token 224 has a high confidence value, theclustering module 260 is constrained to encourage different speaker labels 250. Alternatively, when the speaker turn token 224 has a low confidence value, theclustering module 260 may be constrained to encourage thesame speaker label 250. - In some implementations, the
diarization system 200 annotates thetranscription 220 of theutterances 120 based on thespeaker label 250 assigned to each speaker segment 225 (i.e., diarization results 280). For instance, atranscription 220 of a conversation betweenmultiple speakers 10 may be indexed by speaker to associated portions of thetranscription 220 with therespective speaker 10 for identifying what eachspeaker 10 said in thetranscription 220. The annotatedtranscription 220 may be stored inmemory hardware user device 110 or thecloud computing environment 140 to be accessed later by one of thespeakers 10. -
FIG. 2 illustrates a schematic view of thediarization system 200 that includes the ASR model (i.e., transformer transducer) 300, thesegmentation module 210, thespeaker encoder 230, and theclustering module 260. TheASR model 300 processes theinput audio signal 122 corresponding to theutterances 120 spoken by the multiple speakers 10 (FIG. 1 ) to generate thetranscriptions 220 of the utterances and the sequence ofspeaker turn tokens 224. Thetranscription 220 includes one ormore terms 222 corresponding to words spoken by the multiple speakers. The sequence ofspeaker turn tokens 224 indicates a location of a respective speaker turn detected in thetranscription 220 between a respective pair ofadjacent terms 222. In the example shown, theinput audio signal 122 may include an utterance where first andsecond terms 222 were spoken by afirst speaker 10, third andfourth terms 222 were spoken by asecond speaker 10, and fifth andsixth terms 222 were spoken by athird speaker 10. Here, theASR model 300 generates afirst speaker token 224 between thesecond term 222 and thethird term 222 to indicate the speaker turn from the first speaker to the second speaker, and asecond speaker token 224 between thefourth term 222 andfifth term 222 to indicate the speaker turn from the second speaker to the third speaker. Moreover, in some examples, theASR model 300 generates a start of speech (SOS) token 227 that indicates the start of an utterance and an end of speech (EOS) token 229 that indicates the end of an utterance. - In some implementations, the
ASR model 300 processes acoustic information and/or semantic information to detect speaker turns in theinput audio signal 122. That is, using natural language understanding (NLU) theASR model 300 can determine for an utterance “How are you I'm good,” that “how are you” and “I'm good” were likely spoken by different users independent of any acoustic processing of theinput audio signal 122. This semantic interpretation of thetranscription 220 may be used independently or in conjunction with acoustic processing of theinput audio signal 122. - The
speaker encoder 230 receives the plurality ofspeaker segments 225 from the segmentation module 210 (FIG. 1 ) and extracts the corresponding speaker-discriminative embedding 240 for eachspeaker segment 225. The speaker-discriminative embeddings 240 may include speaker vectors such as d-vectors or i-vectors. Thespeaker encoder 230 provides the speaker-discriminative embeddings 240 associated with eachspeaker segment 225 to theclustering module 260. - The
clustering module 260 receives the speaker-discriminative embeddings 240 for eachspeaker segment 225 and thepairwise constraints 226, and is configured to predict speaker labels 250 for each speaker-discriminative embedding 240. Given a set of N data samples (e.g., x1, x2, . . . , xT), theclustering module 260 constructs a similarity graph by computing pairwise similarities aij where A represents the affinity matrix E -
- The
clustering module 260 identifies a partition so that edges connecting different clusters have low eights, and edges within a cluster have high weights. Generally, the similarity graph is connected or only includes a few connected components and very few isolated vertices. Spectral clustering is sensitive to quality and noises of the similarity graph, therefore, theclustering module 260 performs several refinement operations on the affinity matrix to model the local neighborhood relationships between data samples. One refinement operation includes row-wise thresholding with p-percentile that sets diagonal values of the affinity matrix to 0, sets affinity values that are larger than the p-percentile values to 1, multiply affinity values by 0.01 that are smaller than the p-percentile of the row, and resetting diagonal values of the affinity matrix to 1. Another refinement operation includes applying an average summarization operation to make the affinity matrix positive semi-definite using the following equation, -
- The diarization error rate (DER) is significantly affected by the hyper parameter p for the p-percentile. Accordingly, a ratio value r(p) is a good proxy of the DER such that maximum eigengap is large while not generating an excessive amount of connections in the similarity graph.
- Given the affinity matrix A, an unnormalized Laplacian matrix L is defined by L=D−A while a normalized Laplacian matrix
L is defined byL =D−1/2LD−1/2. Here, D represents the diagonal matrix defined as {di=Σj=1 N aij}. To perform spectral clustering, theclustering module 260 applies eigen-decomposition to estimate the number ofk classes 262 using the maximum eigengap method. Theclustering module 260 chooses thefirst class k 262 of eigen-vectors and applies a row-wise re-normalization of the spectral embeddings and applies k-means algorithm on the spectral embeddings to predict speaker labels 250. - The
clustering module 260 receivespairwise constraints 226 indicating the confidence of thespeaker turn tokens 224 and context information to constrain the spectral clustering. Thepairwise constraints 226 are configured to encouragedifferent speaker labels 250 foradjacent speaker segments 225 with a high confidence speaker turn token 224 and encourage the same speaker labels 250 foradjacent speaker segments 225 with a low confidence speaker turn token 224. With pairwise constraints 226 Q constrained spectral clustering identifies one or more partitions that maximize constraint satisfaction and minimizes the cost on the similarity graph G. Thepairwise constraints 226 may be represented by Q∈clustering module 260 processes the constraint matrix Q by: -
- Here, if there is a speaker turn between speaker segment 225 i and i+1, and the confidence of the speaker turn token c(<st>) is larger than a threshold σ, the
clustering module 260 defines theadjacent speaker segments 225 as “cannot-link” (CL). The CL definition indicates that thespeaker label 250 between theadjacent speaker segments 225 has a high likelihood of being different. If there is no speaker turn token 224 betweenadjacent speaker segments 225, the clustering module defines the adjacent speaker segments as “must-link” (ML). The ML definition indicates that thespeaker label 250 between theadjacent speaker segments 225 has a high likelihood of being the same. - The ML defined
adjacent speaker segments 225 are treated as a positive class and the CL definedadjacent speaker segments 225 as a negative class. The class labels (i.e., positive and negative), are propagated in vertical and horizontal directions respectively in the affinity matrix Ā=D−1/2 AD−1/2. In each iteration t, the initial constraint matrix is added to adjust Q(t). Moreover, a parameter α, is used to control the relative amount of constraint information fromadjacent speaker segments 225 and theinitial constraints 226. The clustering module preforms vertical propagation first until the convergence and then horizontal propagation by the following algorithm: -
Algorithm 1: Exhaustive and Efficient Constraint Propagation (E2CP) method Require: Initial constraint matrix Z = Q(0), matrix Ā, parameter α. While: Qv(t) not converge to Qv* do Qv(t + 1) = αĀQv(t) + (1 − α)Z Vertical Propoagation end while While: Qh(t) not converge to Qh* do Qh(t + 1) = αQh(t)Ā + (1 − α)Qv* Horizontal Propoagation end while Output Q* = Qh* as the final converged pairwise constraint matrix - Q* has a closed-form solution formulated by:
-
Q*=Q h*=(1−α)Q v*(I−αĀ T)−1=(1−α)2(1−αĀ)−1 Z(1−αĀ)−1 (2) - Using the propagated constraint matrix Q*, the
clustering module 260 obtains an adjusted affinity matrix Âij by: -
- For constraint Qij>0, the affinity matrix increases the similarity between sample xi and xj. Alternatively, for Qij<0, the affinity matrix decreases the similarity between xi and xj. After this operation, the
clustering module 260 performs normalized Laplacian matrix based spectral clustering to predict speaker labels 250 for thespeaker segments 225. Theclustering module 260 generates diarization results 280 include afirst speaker label 250 a that indicates the first speaker spoke afirst speaker segment 225 a (i.e., first and second terms 222), a second speaker label 250 b that indicates the second speaker spoke asecond speaker segment 225 b (i.e., third and fourth terms 222), and athird speaker label 250 c that indicates the third speaker spoke athird speaker segment 225 c (i.e., fifth and sixth terms 222). - With reference to
FIG. 3 , theASR model 300 may provide end-to-end (E2E) speech recognition by integrating acoustic, pronunciation, and language models into a single neural network, and does not require a lexicon or a separate text normalization component. Various structures and optimization mechanisms can provide increased accuracy and reduced model training time. TheASR model 300 may include a steaming Transformer-Transducer (T-T) model architecture, which adheres to latency constraints associated with interactive applications. TheASR model 300 may similarly include a RNN-T model architecture or a Conformer-Transducer (C-T) model architecture. TheASR model 300 provides a small computational footprint and utilizes less memory requirements than conventional ASR architectures, making the T-T model architecture suitable for performing speech recognition entirely on the user device 110 (e.g., no communication with thecloud computing environment 140 is required). TheASR model 300 includes anaudio encoder 310, alabel encoder 320, and ajoint network 330. Theaudio encoder 310, which is roughly analogous to an acoustic model (AM) in a traditional ASR system, includes a neural network having a plurality of transformer layers. For instance, theaudio encoder 310 reads a sequence of d-dimensional feature vectors (e.g., speaker segments 225 (FIG. 1 )) x=(x1, x2, . . . , xT), where xt ∈Rd, and produces at each time step a higher-order feature representation 312. Here, each speaker segment 225 (FIG. 1 ) includes a sequence of acoustic frames (e.g., audio data 122) that corresponds to the respective speaker segment 225 (FIG. 1 ). This higher-order feature representation is denoted as ah1, . . . , ahT. - Similarly, the
label encoder 320 may also include a neural network of transformer layers or a look-up table embedding model, which, like a language model (LM), processes the sequence of non-blank symbols output by afinal Softmax layer 340 so far, y0, . . . , yui-1, (e.g., the one ormore terms 222 includingspeaker turn tokens 224 as shown inFIG. 2 ) into a dense representation 322 (denoted by Ihu) that encodes predicted label history. In implementations when thelabel encoder 320 includes the neural network of transformer layers, each transformer layer may include a normalization layer, a masked multi-head attention layer with relative position encoding, a residual connection, a feed forward layer, and a dropout layer. In these implementations, thelabel encoder 320 may include two transformer layers. In implementations when thelabel encoder 320 includes the look-up table embedding model with a bi-gram label context, the embedding model is configured to learn a weight vector of the d-dimension for each possible bigram label context, where d is the dimension of the outputs of the audio andlabel encoders ASR model 300 to producefast label encoder 320 runtimes. - Finally, with the T-T model architecture, the representations produced by the audio and
label encoders joint network 330 using a dense layer Jo. Thejoint network 330 then predicts P(zu,t|x,t,y1, . . . , yu-1), which is a distribution over the next output symbol. Stated differently, thejoint network 330 generates, at each output step (e.g., time step), a probability distribution over possible speech recognition hypotheses 342 for the one ormore terms 222 of the transcription 220 (FIG. 2 ). Here, the “possible speech recognition hypotheses” correspond to a set of output labels (also referred to as “speech units”) each representing a grapheme (e.g., symbol/character), term 222 (FIG. 2 ), or a word piece in a specified natural language. For example, when the natural language is English, the set of output labels may include twenty-seven (27) symbols, e.g., one label for each of the 26-letters in the English alphabet and one label designating a space. Accordingly, thejoint network 330 may output a set of values indicative of the likelihood of occurrence of each of a predetermined set of output labels. This set of values can be a vector (e.g., a one-hot vector) and can indicate a probability distribution over the set of output labels. In some cases, the output labels are graphemes (e.g., individual characters, and potentially punctuation and other symbols), but the set of output labels is not so limited. For example, the set of output labels can include wordpieces and/or entire words, in addition to or instead of graphemes. The output distribution of thejoint network 330 can include a posterior probability value for each of the different output labels. Thus, if there are 100 different output labels representing different graphemes or other symbols, the output zu,t of thejoint network 330 can include 100 different probability values, one for each output label. The probability distribution can then be used to select and assign scores to candidate orthographic elements (e.g., graphemes, wordpieces, and/or words) in a beam search process (e.g., by the Softmax layer 340) for determining the transcription. - The
Softmax layer 340 may employ any technique to select the output label/symbol with the highest probability in the distribution as the next output symbol predicted by theASR model 300 at the corresponding output step. In this manner, theASR model 300 does not make a conditional independence assumption, rather the prediction of each symbol is conditioned not only on the acoustics but also on the sequence of labels output so far. -
FIG. 4 is a flowchart of an exemplary arrangement of operations for a computer-implementedmethod 400 of performing speaker diarization on a receivedutterance 120 of speech. Thedata processing hardware method 400 by executing instructions stored on thememory hardware operation 402, themethod 400 includes receiving aninput audio signal 122 corresponding toutterances 120 spoken bymultiple speakers operation 404, themethod 400 includes processing, using a speech recognition model (e.g., ASR model) 300, theinput audio signal 122 to jointly generate as output from the speech recognition model 300 atranscription 220 of theutterances 120 and a sequence ofspeaker turn tokens transcription 220 between a respective pair ofadjacent terms 222. Atoperation 406, themethod 400 includes segmenting theinput audio signal 122 into a plurality ofspeaker segments 225 based on the sequence ofspeaker tokens 224. - At
operation 408, themethod 400 includes, for eachspeaker segment 225 of the plurality ofspeaker segments 225, extracting a corresponding speaker-discriminative embedding 240 from thespeaker segment 225. Atoperation 410, themethod 400 includes performing spectral clustering on the speaker-discriminative embeddings 240 extracted from the plurality ofspeaker segments 225 to cluster the plurality ofspeaker segments 225 intok classes 262. Atoperation 412, themethod 400 includes, for eachrespective class 262 of the kclasses 262, assigning arespective speaker label 250 to eachspeaker segment 225 clustered into therespective class 262 that is different than therespective speaker label 250 assigned to thespeaker segments 225 clustered into eachother class 262 of the kclasses 262. - A software application (i.e., a software resource) may refer to computer software that causes a computing device to perform a task. In some examples, a software application may be referred to as an “application,” an “app,” or a “program.” Example applications include, but are not limited to, system diagnostic applications, system management applications, system maintenance applications, word processing applications, spreadsheet applications, messaging applications, media streaming applications, social networking applications, and gaming applications.
- The non-transitory memory may be physical devices used to store programs (e.g., sequences of instructions) or data (e.g., program state information) on a temporary or permanent basis for use by a computing device. The non-transitory memory may be volatile and/or non-volatile addressable semiconductor memory. Examples of non-volatile memory include, but are not limited to, flash memory and read-only memory (ROM)/programmable read-only memory (PROM)/erasable programmable read-only memory (EPROM)/electronically erasable programmable read-only memory (EEPROM) (e.g., typically used for firmware, such as boot programs). Examples of volatile memory include, but are not limited to, random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), phase change memory (PCM) as well as disks or tapes.
-
FIG. 5 is schematic view of anexample computing device 500 that may be used to implement the systems and methods described in this document. Thecomputing device 500 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers. The components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document. - The
computing device 500 includes aprocessor 510,memory 520, astorage device 530, a high-speed interface/controller 540 connecting to thememory 520 and high-speed expansion ports 550, and a low speed interface/controller 560 connecting to a low speed bus 570 and astorage device 530. Each of thecomponents processor 510 can process instructions for execution within thecomputing device 500, including instructions stored in thememory 520 or on thestorage device 530 to display graphical information for a graphical user interface (GUI) on an external input/output device, such asdisplay 580 coupled tohigh speed interface 540. In other implementations, multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory. Also,multiple computing devices 500 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system). - The
memory 520 stores information non-transitorily within thecomputing device 500. Thememory 520 may be a computer-readable medium, a volatile memory unit(s), or non-volatile memory unit(s). Thenon-transitory memory 520 may be physical devices used to store programs (e.g., sequences of instructions) or data (e.g., program state information) on a temporary or permanent basis for use by thecomputing device 500. Examples of non-volatile memory include, but are not limited to, flash memory and read-only memory (ROM)/programmable read-only memory (PROM)/erasable programmable read-only memory (EPROM)/electronically erasable programmable read-only memory (EEPROM) (e.g., typically used for firmware, such as boot programs). Examples of volatile memory include, but are not limited to, random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), phase change memory (PCM) as well as disks or tapes. - The
storage device 530 is capable of providing mass storage for thecomputing device 500. In some implementations, thestorage device 530 is a computer-readable medium. In various different implementations, thestorage device 530 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations. In additional implementations, a computer program product is tangibly embodied in an information carrier. The computer program product contains instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer- or machine-readable medium, such as thememory 520, thestorage device 530, or memory onprocessor 510. - The
high speed controller 540 manages bandwidth-intensive operations for thecomputing device 500, while thelow speed controller 560 manages lower bandwidth-intensive operations. Such allocation of duties is exemplary only. In some implementations, the high-speed controller 540 is coupled to thememory 520, the display 580 (e.g., through a graphics processor or accelerator), and to the high-speed expansion ports 550, which may accept various expansion cards (not shown). In some implementations, the low-speed controller 560 is coupled to thestorage device 530 and a low-speed expansion port 590. The low-speed expansion port 590, which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet), may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter. - The
computing device 500 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as astandard server 500 a or multiple times in a group ofsuch servers 500 a, as alaptop computer 500 b, or as part of arack server system 500 c. - Various implementations of the systems and techniques described herein can be realized in digital electronic and/or optical circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof. These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- These computer programs (also known as programs, software, software applications or code) include machine instructions for a programmable processor, and can be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. As used herein, the terms “machine-readable medium” and “computer-readable medium” refer to any computer program product, non-transitory computer readable medium, apparatus and/or device (e.g., magnetic discs, optical disks, memory, Programmable Logic Devices (PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term “machine-readable signal” refers to any signal used to provide machine instructions and/or data to a programmable processor.
- The processes and logic flows described in this specification can be performed by one or more programmable processors, also referred to as data processing hardware, executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks. However, a computer need not have such devices. Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- To provide for interaction with a user, one or more aspects of the disclosure can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor, or touch screen for displaying information to the user and optionally a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices can be used to provide interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input. In addition, a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web browser on a user's client device in response to requests received from the web browser.
- A number of implementations have been described. Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the disclosure. Accordingly, other implementations are within the scope of the following claims.
Claims (24)
1. A computer-implemented method when executed on data processing hardware causes the data processing hardware to perform operations comprising:
receiving an input audio signal corresponding to utterances spoken by multiple speakers;
processing, using a speech recognition model, the input audio signal to jointly generate as output from the speech recognition model:
a transcription of the utterances; and
a sequence of speaker turn tokens each indicating a location of a respective speaker turn detected in the transcription between a respective pair of adjacent terms;
segmenting the input audio signal into a plurality of speaker segments based on the sequence of speaker tokens;
for each speaker segment of the plurality of speaker segments, extracting a corresponding speaker-discriminative embedding from the speaker segment;
performing spectral clustering on the speaker-discriminative embeddings extracted from the plurality of speaker segments to cluster the plurality of speaker segments into k classes; and
for each respective class of the k classes, assigning a respective speaker label to each speaker segment clustered into the respective class that is different than the respective speaker label assigned to the speaker segments clustered into each other class of the k classes.
2. The computer-implemented method of claim 1 , wherein the operations further comprise annotating the transcription of the utterances based on the speaker label assigned to each speaker segment.
3. The computer-implemented method of claim 1 , wherein:
each speaker turn token in the sequence of speaker turn tokens has a corresponding timestamp; and
segmenting the input audio signal into the plurality of speaker segments based on the sequence of speaker turn tokens comprises segmenting the input audio signal into initial speaker segments each bounded by the corresponding timestamps of a respective pair of adjacent speaker turn tokens in the sequence of speaker turn tokens.
4. The computer-implemented method of claim 3 , wherein the operations further comprise:
for each initial speaker segment having a respective duration that exceeds a segment duration threshold, further segmenting the initial speaker segment into two or more reduced-duration speaker segments having respective durations less than or equal to the segment duration threshold,
wherein the plurality of speaker segments segmented from the input audio signal comprise:
the initial speaker segments having respective durations less than or equal to the segment duration threshold; and
the reduced-duration speaker segments further segmented from any of the initial speaker segments having respective durations that exceed the segment duration threshold.
5. The computer-implemented method of claim 1 , wherein extracting a corresponding speaker-discriminative embedding from the speaker segment comprises:
receiving, as input to a speaker encoder model, the speaker segment; and
generating, as output from the speaker encoder model, the corresponding speaker-discriminative embedding.
6. The computer-implemented method of claim 5 , wherein the speaker encoder model comprises a long-short term memory-based (LSTM-based) speaker encoder model configured to extract the corresponding speaker-discriminative embedding from each speaker segment.
7. The computer-implemented method of claim 1 , wherein the operations further comprise:
for each speaker turn token in the sequence of speaker turn tokens generated as output from the speech recognition model, predicting a confidence of the respective speaker turn detected in the transcription; and
determining pairwise constraints based on the confidences predicted for the speaker turn token,
wherein the spectral clustering performed on the speaker-discriminative embeddings is constrained by the pairwise constraints.
8. The computer-implemented method of claim 1 , wherein the speech recognition model comprises a streaming transducer-based speech recognition model comprising:
an audio encoder configured to:
receive, as input, a sequence of acoustic frames; and
generate, at each of a plurality of time steps, a higher order feature representation for a corresponding acoustic frame in the sequence of acoustic frames;
a label encoder configured to:
receive, as input, a sequence of non-blank symbols output by a final softmax layer; and
generate, at each of the plurality of time steps, a dense representation; and
a joint network configured to:
receive, as input, the higher order feature representation generated by the audio encoder at each of the plurality of time steps and the dense representation generated by the label encoder at each of the plurality of time steps; and
generate, at each of the plurality of time steps, a probability distribution over possible speech recognition hypotheses at the corresponding time step.
9. The computer-implemented method of claim 8 , wherein the audio encoder comprises a neural network having a plurality of transformer layers.
10. The computer-implemented method of claim 8 , wherein the label encoder comprises a bigram embedding lookup decoder model.
11. The computer-implemented method of claim 1 , wherein the speech recognition model is trained on training samples that each comprise training utterances spoken by two or more different speakers paired with a corresponding ground-truth transcription of the training utterances, each ground-truth transcription injected with ground-truth speaker turn tokens indicating locations where speaker turns occur in the ground-truth transcription.
12. The computer-implemented method of claim 11 , wherein the corresponding ground-truth transcription of each training sample is not annotated with any timestamp information.
13. A system comprising
data processing hardware;
memory hardware in communication with the data processing hardware and storing instructions, that when executed by the data processing hardware, cause the data processing hardware to perform operations comprising:
receiving an input audio signal corresponding to utterances spoken by multiple speakers;
processing, using a speech recognition model, the input audio signal to jointly generate as output from the speech recognition model:
a transcription of the utterances; and
a sequence of speaker turn tokens each indicating a location of a respective speaker turn detected in the transcription between a respective pair of adjacent terms;
segmenting the input audio signal into a plurality of speaker segments based on the sequence of speaker tokens;
for each speaker segment of the plurality of speaker segments, extracting a corresponding speaker-discriminative embedding from the speaker segment;
performing spectral clustering on the speaker-discriminative embeddings extracted from the plurality of speaker segments to cluster the plurality of speaker segments into k classes; and
for each respective class of the k classes, assigning a respective speaker label to each speaker segment clustered into the respective class that is different than the respective speaker label assigned to the speaker segments clustered into each other class of the k classes.
14. The system of claim 13 , wherein the operations further comprise annotating the transcription of the utterances based on the speaker label assigned to each speaker segment.
15. The system of claim 13 , wherein:
each speaker turn token in the sequence of speaker turn tokens has a corresponding timestamp; and
segmenting the input audio signal into the plurality of speaker segments based on the sequence of speaker turn tokens comprises segmenting the input audio signal into initial speaker segments each bounded by the corresponding timestamps of a respective pair of adjacent speaker turn tokens in the sequence of speaker turn tokens.
16. The system of claim 15 , wherein the operations further comprise:
for each initial speaker segment having a respective duration that exceeds a segment duration threshold, further segmenting the initial speaker segment into two or more reduced-duration speaker segments having respective durations less than or equal to the segment duration threshold,
wherein the plurality of speaker segments segmented from the input audio signal comprise:
the initial speaker segments having respective durations less than or equal to the segment duration threshold; and
the reduced-duration speaker segments further segmented from any of the initial speaker segments having respective durations that exceed the segment duration threshold.
17. The system of claim 13 , wherein extracting a corresponding speaker-discriminative embedding from the speaker segment comprises:
receiving, as input to a speaker encoder model, the speaker segment; and
generating, as output from the speaker encoder model, the corresponding speaker-discriminative embedding.
18. The system of claim 17 , wherein the speaker encoder model comprises a long-short term memory-based (LSTM-based) speaker encoder model configured to extract the corresponding speaker-discriminative embedding from each speaker segment.
19. The system of claim 13 , wherein the operations further comprise:
for each speaker turn token in the sequence of speaker turn tokens generated as output from the speech recognition model, predicting a confidence of the respective speaker turn detected in the transcription; and
determining pairwise constraints based on the confidences predicted for the speaker turn token,
wherein the spectral clustering performed on the speaker-discriminative embeddings is constrained by the pairwise constraints.
20. The system of claim 13 , wherein the speech recognition model comprises a streaming transducer-based speech recognition model comprising:
an audio encoder configured to:
receive, as input, a sequence of acoustic frames; and
generate, at each of a plurality of time steps, a higher order feature representation for a corresponding acoustic frame in the sequence of acoustic frames;
a label encoder configured to:
receive, as input, a sequence of non-blank symbols output by a final softmax layer; and
generate, at each of the plurality of time steps, a dense representation; and
a joint network configured to:
receive, as input, the higher order feature representation generated by the audio encoder at each of the plurality of time steps and the dense representation generated by the label encoder at each of the plurality of time steps; and
generate, at each of the plurality of time steps, a probability distribution over possible speech recognition hypothesis at the corresponding time step.
21. The system of claim 20 , wherein the audio encoder comprises a neural network having a plurality of transformer layers.
22. The system of claim 20 , wherein the label encoder comprises a bigram embedding lookup decoder model.
23. The system of claim 13 , wherein the speech recognition model is trained on training samples that each comprise training utterances spoken by two or more different speakers paired with a corresponding ground-truth transcription of the training utterances, each ground-truth transcription injected with ground-truth speaker turn tokens indicating locations where speaker turns occur in the ground-truth transcription.
24. The system of claim 13 , wherein the corresponding ground-truth transcription of each training sample is not annotated with any timestamp information.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/644,261 US20230089308A1 (en) | 2021-09-23 | 2021-12-14 | Speaker-Turn-Based Online Speaker Diarization with Constrained Spectral Clustering |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202163261536P | 2021-09-23 | 2021-09-23 | |
US17/644,261 US20230089308A1 (en) | 2021-09-23 | 2021-12-14 | Speaker-Turn-Based Online Speaker Diarization with Constrained Spectral Clustering |
Publications (1)
Publication Number | Publication Date |
---|---|
US20230089308A1 true US20230089308A1 (en) | 2023-03-23 |
Family
ID=79927078
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US17/644,261 Pending US20230089308A1 (en) | 2021-09-23 | 2021-12-14 | Speaker-Turn-Based Online Speaker Diarization with Constrained Spectral Clustering |
Country Status (4)
Country | Link |
---|---|
US (1) | US20230089308A1 (en) |
KR (1) | KR20240053639A (en) |
CN (1) | CN117980991A (en) |
WO (1) | WO2023048746A1 (en) |
Cited By (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN116050401A (en) * | 2023-03-31 | 2023-05-02 | 云南师范大学 | Method for automatically generating diversity problems based on transform problem keyword prediction |
US20230169981A1 (en) * | 2021-11-30 | 2023-06-01 | Samsung Electronics Co., Ltd. | Method and apparatus for performing speaker diarization on mixed-bandwidth speech signals |
US20230215439A1 (en) * | 2021-12-31 | 2023-07-06 | Microsoft Technology Licensing, Llc | Training and using a transcript generation model on a multi-speaker audio stream |
CN116757338A (en) * | 2023-08-21 | 2023-09-15 | 浙江天演维真网络科技股份有限公司 | Crop yield prediction method, device, electronic equipment and storage medium |
US11984127B2 (en) * | 2021-12-31 | 2024-05-14 | Microsoft Technology Licensing, Llc | Training and using a transcript generation model on a multi-speaker audio stream |
-
2021
- 2021-12-14 CN CN202180102646.9A patent/CN117980991A/en active Pending
- 2021-12-14 WO PCT/US2021/063343 patent/WO2023048746A1/en active Application Filing
- 2021-12-14 US US17/644,261 patent/US20230089308A1/en active Pending
- 2021-12-14 KR KR1020247011500A patent/KR20240053639A/en unknown
Cited By (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20230169981A1 (en) * | 2021-11-30 | 2023-06-01 | Samsung Electronics Co., Ltd. | Method and apparatus for performing speaker diarization on mixed-bandwidth speech signals |
US20230215439A1 (en) * | 2021-12-31 | 2023-07-06 | Microsoft Technology Licensing, Llc | Training and using a transcript generation model on a multi-speaker audio stream |
US11984127B2 (en) * | 2021-12-31 | 2024-05-14 | Microsoft Technology Licensing, Llc | Training and using a transcript generation model on a multi-speaker audio stream |
CN116050401A (en) * | 2023-03-31 | 2023-05-02 | 云南师范大学 | Method for automatically generating diversity problems based on transform problem keyword prediction |
CN116757338A (en) * | 2023-08-21 | 2023-09-15 | 浙江天演维真网络科技股份有限公司 | Crop yield prediction method, device, electronic equipment and storage medium |
Also Published As
Publication number | Publication date |
---|---|
KR20240053639A (en) | 2024-04-24 |
WO2023048746A1 (en) | 2023-03-30 |
CN117980991A (en) | 2024-05-03 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN110276259B (en) | Lip language identification method, device, computer equipment and storage medium | |
US10176804B2 (en) | Analyzing textual data | |
CN109117777B (en) | Method and device for generating information | |
CN109887497B (en) | Modeling method, device and equipment for speech recognition | |
CN110444198B (en) | Retrieval method, retrieval device, computer equipment and storage medium | |
US20230089308A1 (en) | Speaker-Turn-Based Online Speaker Diarization with Constrained Spectral Clustering | |
US9324323B1 (en) | Speech recognition using topic-specific language models | |
US20240021202A1 (en) | Method and apparatus for recognizing voice, electronic device and medium | |
US20230223009A1 (en) | Language-agnostic Multilingual Modeling Using Effective Script Normalization | |
CN110335608B (en) | Voiceprint verification method, voiceprint verification device, voiceprint verification equipment and storage medium | |
US11887623B2 (en) | End-to-end speech diarization via iterative speaker embedding | |
CN112151015A (en) | Keyword detection method and device, electronic equipment and storage medium | |
US20230096805A1 (en) | Contrastive Siamese Network for Semi-supervised Speech Recognition | |
US20230104228A1 (en) | Joint Unsupervised and Supervised Training for Multilingual ASR | |
CN115457938A (en) | Method, device, storage medium and electronic device for identifying awakening words | |
CN114254587A (en) | Topic paragraph dividing method and device, electronic equipment and storage medium | |
Jia et al. | A deep learning system for sentiment analysis of service calls | |
EP4024393A2 (en) | Training a speech recognition model | |
CN112735432B (en) | Audio identification method, device, electronic equipment and storage medium | |
CN114780757A (en) | Short media label extraction method and device, computer equipment and storage medium | |
CN113470617A (en) | Speech recognition method, electronic device and storage device | |
WO2024076365A1 (en) | Accelerating speaker diarization with multi-stage clustering | |
US20230107475A1 (en) | Exploring Heterogeneous Characteristics of Layers In ASR Models For More Efficient Training | |
US20240135934A1 (en) | Evaluation-based speaker change detection evaluation metrics | |
US20230298591A1 (en) | Optimizing Personal VAD for On-Device Speech Recognition |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:WANG, QUAN;LU, HAN;CLARK, EVAN;AND OTHERS;REEL/FRAME:059302/0327Effective date: 20210923 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |