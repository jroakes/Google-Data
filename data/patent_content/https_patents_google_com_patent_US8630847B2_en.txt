US8630847B2 - Word probability determination - Google Patents
Word probability determination Download PDFInfo
- Publication number
- US8630847B2 US8630847B2 US11/870,068 US87006807A US8630847B2 US 8630847 B2 US8630847 B2 US 8630847B2 US 87006807 A US87006807 A US 87006807A US 8630847 B2 US8630847 B2 US 8630847B2
- Authority
- US
- United States
- Prior art keywords
- word
- corpus
- words
- segmentation
- candidate
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/232—Orthographic correction, e.g. spell checking or vowelisation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/279—Recognition of textual entities
- G06F40/284—Lexical analysis, e.g. tokenisation or collocates
Definitions
- This disclosure relates to input methods.
- a logographic script in which one or two characters, e.g., glyphs, correspond roughly to one word or meaning have more characters than keys on a standard input device, such as a computer keyboard on a mobile device keypad.
- a standard input device such as a computer keyboard on a mobile device keypad.
- the Chinese language contains thousands of characters defined by base Pinyin characters and five tones.
- the mapping of these many-to-one associations can be implemented by input methods that facilitate entry of characters and symbols not found on input devices. Accordingly, a Western-style keyboard can be used to input Chinese, Japanese, or Korean characters.
- an input method editor can be used to search a dictionary of words to find candidate words that correspond to the Pinyin characters typed by a user.
- the dictionary can include data associated with the words, e.g., probability scores, that allows the IME to predict the user's intention and to identify and rank the candidates.
- the IME ranks the candidates based on, for example, probability or accuracy scores, and provides a list of the candidate words to the user in a sequence according to the ranking.
- a computer-implemented method includes identifying a word corpus, associating a word probability value with each word in the word corpus, identifying a sentence, determining candidate segmentations of the sentence based on the word corpus, and iteratively adjusting the associated probability value for each word in the word corpus based on the probability values associated with the words and the candidate segmentations.
- Implementations of the method can include one or more of the following features.
- the method includes storing a portion of the word corpus defined by words having the highest associated word probability values in an input method editor dictionary.
- the word probability values correspond to a probability of associated words appearing in the sentence.
- Determining possible segmentations of at least one sentence includes determining all possible segmentations of the at least one sentence based on the word corpus.
- the method includes determining a segmentation probability value for each candidate segmentation of the sentence. Determining a segmentation probability value for each candidate segmentation of the sentence includes determining the segmentation probability value for each candidate segmentation based on the word probability values associated with the words in the candidate segmentation.
- Determining a refined probability value of a word in the sentence includes multiplying (a) a sum of the probability values of candidate segmentations of a substring before the word, (b) the probability value of the word, and (c) a sum of the probability values of candidate segmentations of a substring after of the word.
- Determining candidate segmentations of the sentence includes determining candidate segmentations of a plurality of sentences in a document.
- the method includes identifying sentences in a plurality of documents, and for each sentence, determining candidate segmentations of the sentence based on the word corpus. Sentences in different types of documents are given different weights in determining the word probability values.
- the documents are accessible over a public network, the Internet, or provided by a plurality of third parties.
- the words include at least one of Chinese, Japanese, and Korean characters.
- the words include Hanzi characters.
- a computer-implemented method includes determining word probability values associated with words of a word corpus, determining candidate segmentations of sentences of documents in a document corpus, iteratively determining a segmentation probability value for each candidate segmentation of each sentence based on the word probability values associated with the words in the candidate segmentation, and iteratively adjusting the word probability value for each word based on the segmentation probability values for the candidate segmentations that include the word.
- Implementations of the method can include one or more of the following features.
- the method includes identifying document types for the documents, assigning weights to sentences in the documents based on the identified document type, and determining one of the segmentation probability values or the word probability values based on the weights.
- the method includes accumulating a count for each word based on the segmentation probability values of the segmentations that include the word. Accumulating the count for each word includes determining a sum of the segmentation probability values of the segmentations that include the word.
- the method includes determining the word probability value for each word by normalizing the count for the word based on a normalization factor.
- the method includes determining the normalization factor by determining a sum of the counts of all the words.
- a system in another aspect, includes a data store to store a word corpus and a document corpus, and a processing engine stored in computer readable medium and includes instructions executable by a processing device. Upon execution of the instructions, the processing device associates a word probability value with each word in the word corpus, determines candidate segmentations of each sentence of each document in the document corpus based on the word corpus, and iteratively adjusts the associated word probability value for each word in the word corpus based on the associated word probability values and the candidate segmentations.
- a system in another aspect, includes a data store to store a word corpus and a document corpus, and a processing device to associate a word probability value with each word in the word corpus, determine candidate segmentations of each sentence of each document in the document corpus based on the word corpus, and iteratively adjust the associated word probability value for each word in the word corpus based on the associated word probability values and the candidate segmentations.
- a system in another aspect, includes a data store to store a word corpus and a document corpus, and a processing device.
- the processing device determines word probability values associated with words of the word corpus, determines candidate segmentations of sentences of documents in the document corpus, iteratively determines a segmentation probability value for each candidate segmentation of each sentence based on the word probability values associated with the words in the candidate segmentation, and iteratively adjusts the word probability value for each word based on the segmentation probability values for the candidate segmentations that include the word.
- a system in another aspect, in general, includes a data store and a processing device.
- the data store stores a dictionary that includes words and associated word probability values that are determined using an iterative process, the iterative process including iteratively determining segmentation probability values for candidate segmentations of sentences of documents, and iteratively adjusting the word probability values for the word based on the segmentation probability values.
- the processing device provides an input method editor configured to select words from the dictionary.
- a system in another aspect, includes means for associating a word probability value with words in a word corpus, means for identifying sentences in a plurality of documents, means for determining candidate segmentations of each of the sentences based on the word corpus, and means for iteratively adjusting the associated word probability value for each word in the word corpus based on the associated word probability values and the candidate segmentations.
- a system in another aspect, includes means for determining word probability values associated with words of a word corpus, means for determining candidate segmentations of sentences of documents in a document corpus, means for iteratively determining a segmentation probability value for each candidate segmentation of each sentence based on the word probability values associated with the words in the candidate segmentation, and means for iteratively adjusting the word probability value for each word based on the segmentation probability values for the candidate segmentations that include the word.
- a dictionary can be automatically optimized base on a corpus of documents, and the optimized dictionary can facilitate the identification and selection for candidate words.
- an IME utilizing the dictionary can provide fewer and/or more accurate identifications of candidate words for selection.
- the speed and efficiency for the computer processing the logographic script, e.g., Chinese characters, can be improved.
- a user of the dictionary may easily obtain the desirable candidate words with highest probability values, so that the user's input speed of the logographic script can be increased.
- FIG. 1 is a block diagram of an example device that can be used to implement the systems and methods described herein.
- FIG. 2 is a block diagram of an example editing system.
- FIG. 3 is a diagram of an example input method editor environment.
- FIG. 4 is a diagram of an example word probability determination engine.
- FIGS. 5-7 are flow diagrams of processes for determining word probability values.
- FIG. 1 is a block diagram of an example device 100 that can be utilized to implement the systems and methods described herein.
- the device 100 can, for example, be implemented in a computer device, such as a personal computer device, or other electronic devices, such as a mobile phone, mobile communication device, personal digital assistant (PDA), and the like.
- a computer device such as a personal computer device
- PDA personal digital assistant
- the example device 100 includes a processing device 102 , a first data store 104 , a second data store 106 , input devices 108 , output devices 110 , and a network interface 112 .
- a bus system 114 including, for example, a data bus and a motherboard, can be used to establish and control data communication between the components 102 , 104 , 106 , 108 , 110 and 112 .
- Other example system architectures can also be used.
- the processing device 102 can, for example, include one or more microprocessors.
- the first data store 104 can, for example, include a random access memory storage device, such as a dynamic random access memory, or other types of computer-readable medium memory devices.
- the second data store 106 can, for example, include one or more hard drives, a flash memory, and/or a read only memory, or other types of computer-readable medium memory devices.
- Example input devices 108 can include a keyboard, a mouse, a stylus, etc.
- example output devices 110 can include a display device, an audio device, etc.
- the network interface 112 can, for example, include a wired or wireless network device operable to communicate data to and from a network 116 .
- the network 116 can include one or more local area networks (LANs) and/or a wide area network (WAN), such as the Internet.
- LANs local area networks
- WAN wide area network
- the device 100 can include input method editor (IME) code 101 in a data store, such as the data store 106 .
- the input method editor code 101 can be defined by instructions that upon execution cause the processing device 102 to carry out input method editing functions.
- the input method editor code 101 can, for example, comprise interpreted instructions, such as script instructions, e.g., JavaScript or ECMAScript instructions, that can be executed in a web browser environment.
- Other implementations can also be used, e.g., compiled instructions, a stand-alone application, an applet, a plug-in module, etc.
- Execution of the input method editor code 101 generates or launches an input method editor instance 103 .
- the input method editor instance 103 can define an input method editor environment, e.g., user interface, and can facilitate the processing of one or more input methods at the device 100 , during which time the device 100 can receive composition inputs for input characters, ideograms, or symbols, such as, for example, Hanzi characters.
- the user can use one or more of the input devices 108 (e.g., a keyboard, such as a Western-style keyboard, a stylus with handwriting recognition engines, etc.) to input composition inputs for identification of Hanzi characters.
- a Hanzi character can be associated with more than one composition input.
- the first data store 104 and/or the second data store 106 can store an association of composition inputs and characters. Based on a user input, the input method editor instance 103 can use information in the data store 104 and/or the data store 106 to identify one or more candidate characters represented by the input. In some implementations, if more than one candidate character is identified, the candidate characters are displayed on an output device 110 . Using the input device 108 , the user can select from the candidate characters a Hanzi character that the user desires to input.
- the input method editor instance 103 on the device 100 can receive one or more Pinyin composition inputs and convert the composition inputs into Hanzi characters.
- the input method editor instance 103 can, for example, use compositions of Pinyin syllables or characters received from keystrokes to represent the Hanzi characters.
- Each Pinyin syllable can, for example, correspond to a key in the Western style keyboard.
- a Pinyin input method editor a user can input a Hanzi character by using composition inputs that include one or more Pinyin syllables representing the sound of the Hanzi character.
- the user can also input a word that includes two or more Hanzi characters by using composition inputs that include two or more Pinyin syllables representing the sound of the Hanzi characters. Input methods for other languages, however, can also be facilitated.
- Other application software 105 can also be stored in data stores 104 and/or 106 , including web browsers, word processing programs, e-mail clients, etc. Each of these applications can generate a corresponding application instance 107 . Each application instance can define an environment that can facilitate a user experience by presenting data to the user and facilitating data input from the user. For example, web browser software can generate a search engine environment; e-mail software can generate an e-mail environment; a word processing program can generate an editor environment; etc.
- a remote computing system 118 having access to the device 100 can also be used to edit a logographic script.
- the device 100 may be a server that provides logographic script editing capability via the network 116 .
- a user can edit a logographic script stored in the data store 104 and/or the data store 106 using a remote computing system, e.g., a client computer.
- the device 100 can, for example, select a character and receive a composition input from a user over the network interface 112 .
- the processing device 102 can, for example, identify one or more characters adjacent to the selected character, and identify one or more candidate characters based on the received composition input and the adjacent characters.
- the device 100 can transmit a data communication that includes the candidate characters back to the remote computing system.
- FIG. 2 is a block diagram of an example input method editor system 120 .
- the input method editor system 120 can, for example, be implemented using the input method editor code 101 and associated data stores 104 and 106 .
- the input method editor system 120 includes an input method editor engine 122 , a dictionary 124 , and a composition input table 126 .
- Other storage architectures can also be used.
- a user can use the IME system 120 to enter, for example, Chinese words or phrases by typing Pinyin characters, and the IME engine 122 will search the dictionary 124 to identify candidate dictionary entries each including one or more Chinese words or phrases that match the Pinyin characters.
- the dictionary 124 includes entries 128 that correspond to characters, words, or phrases of a logographic script used in one or more language models, and characters, words, and phrases in Roman-based or western-style alphabets, for example, English, German, Spanish, etc.
- Each word corresponds to a meaning and may include one or more characters.
- a word having the meaning “apple” includes two Hanzi characters and that correspond to Pinyin inputs “ping” and “guo,” respectively.
- the character is also a word that has the meaning “fruit.”
- the dictionary entries 128 may include, for example, idioms (e.g., proper names (e.g., meaning “Republic of Austria”), names of historical characters or famous people (for example, meaning “Genghis Khan”), terms of art (e.g., meaning “Global Positioning System”), phrases book titles (for example, meaning “Dream of the Red Chamber”), titles of art works (for example, meaning “Upper River During the Qing Ming Festival”), and movie titles (for example, meaning “Crouching Tiger, Hidden Dragon”), etc., each including one or more characters.
- idioms e.g., proper names (e.g., meaning “Republic of Austria”), names of historical characters or famous people (for example, meaning “Genghis Khan”), terms of art (e.g., meaning “Global Positioning System”), phrases book titles (for example, meaning “Dream of the Red Chamber”), titles of art works (for example, meaning “Upper River
- the dictionary entries 128 may include, for example, names of geographical entities or political entities, names of business concerns, names of educational institutions, names of animals or plants, names of machinery, song names, titles of plays, names of software programs, names of consumer products, etc.
- the dictionary 124 may include, for example, thousands of characters, words and phrases.
- the dictionary 124 includes information about relationships between characters.
- the dictionary 124 can include scores or probability values assigned to a character depending on characters adjacent to the character.
- the dictionary 124 can include entry scores or entry probability values each associated with one of the dictionary entries 128 to indicate how often the entry 128 is used in general.
- the composition input table 126 includes an association of composition inputs and the entries 128 stored in the dictionary 124 .
- the composition input table 126 can link each of the entries in the dictionary 124 to a composition input (e.g., Pinyin input) used by the input method editor engine 122 .
- the input method editor engine 122 can use the information in the dictionary 124 and the composition input table 126 to associate and/or identify one or more entries in the dictionary 124 with one or more composition inputs in the composition input table 126 .
- Other associations can also be used.
- the candidate selections in the IME system 120 can be ranked and presented in the input method editor according to the rank.
- FIG. 3 is a diagram of an example input method editor environment 300 presenting five ranked candidate selections 302 .
- Each candidate selection can be a dictionary entry 128 or a combination of dictionary entries 128 .
- the candidate selections 302 are identified based on the Pinyin inputs 304 .
- a selection indicator 308 surrounds the first candidate selection, i.e., indicating that the first candidate selection is selected. The user can also use a number key to select a candidate selection, or use up and down arrow keys to move the selection indicator 308 to select the candidate selection.
- the IME engine 122 accesses the dictionary 124 to identify candidate entries 128 that are associated with Pinyin characters entered by the user.
- the IME engine 122 uses the entry probability values to rank the candidate entries and determine placement of the candidate entries in the IME environment 300 . For example, a candidate entry having the highest entry probability value may be placed by the IME engine 122 at the first position in the IME environment 300 .
- the dictionary 124 can be updated with new words, names, or phrases periodically.
- the probability values of the entries 128 in the dictionary 124 may change over time. For example, characters, words, and phrases that are commonly typed by users of the IME system 120 may change over time in response to news events and changes in the society.
- the entry probability values associated with the entries 128 of the dictionary 124 can be established and/or updated based on estimated frequencies of characters, words, and phrases in a document corpus.
- FIG. 4 is a diagram of an example word probability determination engine 400 that can, for example, generate a dictionary 406 that stores selected words and probability values associated with the selected words. Each word can have one or more characters, such as one or more Hanzi characters.
- the word corpus 402 can be provided in a table that includes entries, each entry including one word.
- the word corpus 402 can also be a text file that includes words separated by non-character symbols (e.g., commas or semi-colons).
- the word probability determination engine 400 uses an iterative process to determine the probability of occurrences of the word in a document corpus 404 .
- the word corpus 402 and the document corpus 404 can be stored in a data store.
- the term “word” depending on context may be broadly defined to include a sequence of consecutive characters that may include one or more words.
- Each “word” in the word corpus 402 is a candidate for an entry in the dictionary 124 , and each entry in the dictionary 124 may include one or more words.
- the word probability determination engine 400 treats each entry as a word, even though the entry may include more than one word.
- an entry may include a phrase, an idiom, a proper name, a name of a historical character or famous person, a term of art, a book title, a title of an art work, a movie title, etc.
- the word probability determination engine 400 treats each sequence of consecutive characters between two symbols as a word, even though the sequence of consecutive characters may include more than one word. Thus, for example, etc., are all treated as words by the word probability determination engine 400 .
- the word corpus 402 can include words of the Chinese language and other languages.
- the dictionary 406 can, for example, be generated from a word corpus 402 and a document corpus 404 .
- the word corpus 402 can be obtained from pre-established dictionaries, user search queries, or various types of documents.
- the word corpus 402 can, for example, include thousands or more words and phrases.
- the dictionary 406 can include a subset of words and phrases in the word corpus 402 .
- a number of highest ranking words and phrases in the set 402 are selected and added to the dictionary 406 .
- Each of the selected words and phrases becomes an entry of the dictionary 406 .
- the probability values associated with the words and phrases become the probability values associated with the dictionary entries.
- the entries of the dictionary 406 can be added to the dictionary 124 and be used by the IME engine 122 to identify candidate entries that match Pinyin characters entered by the user.
- the document corpus 404 can, for example, include documents that can be accessed over a network, e.g., web pages, e-mail messages, etc.
- the document corpus 404 can include, e.g., e-books, journal articles, advertisements, instant messages, blogs, legal documents, or other types of documents.
- the document corpus 404 may include documents that cover a wide variety of subjects, such as news, movies, music, political debates, scientific discoveries, legal issues, health issues, environmental issues, etc.
- the document corpus 404 can be established by gathering documents from, e.g., a corporate Intranet or the public Internet. The number of documents processed can thus be in the range of millions of documents, or more.
- the documents may include, e.g., Hanzi characters, English characters, numbers, punctuation marks, symbols, HTML codes, etc. Other documents can also be used, e.g., an electronic collection of literary works, an electronic library, etc.
- the word probability determination engine 400 utilizes an iterative process to determine probability values of words in a word corpus 402 based on the frequencies of occurrences of the words in a document corpus 404 .
- the word probability determination engine 400 assigns an initial “soft-count” to each word in the word corpus 402 and determines an initial probability value for each word.
- the soft-counts and probability values can, for example, be stored in one or more data stores, such as a table 412 of words and associated soft-count values, and a table 414 of words and associated probability values. Other data storage architectures can also be used.
- the word probability determination engine 400 can include a word segmentation engine 410 that divides each of the documents 404 into sentences, and identifies all possible segmentations (each referred to as a “candidate segmentation”) of each sentence based on the words in the word corpus 402 . After each sentence of the documents 404 is segmented into segments, each segment is a word in the word corpus 402 .
- a “sentence” refers to a continuous string of Hanzi characters between two non-Hanzi characters, e.g., punctuation marks, numbers, symbols, or HTML codes.
- each sentence being processed by the word probability determination engine 400 is not necessarily a complete sentence as defined by the language rules.
- the word probability values are used to determine probability values for each candidate segmentation, and the segmentation probability values are used to adjust the word probability values. Iterations can continue until a convergence condition or termination condition occurs, e.g., the highest 100,000 word probability values stabilize, or 100 iterations are completed. In some examples, the word probability values converge after less than 100 iterations.
- the sentences can be modeled using Hidden Markov Models, in which the correct segmentations of the sentences are unknown.
- an Expectation-Maximization algorithm can be utilized to implement an expectation process and a maximization process.
- the expectation process determines the expected likelihood (or probability) of each segmentation for all possible segmentations of all the sentences in the document corpus 404 .
- the maximization process determines the probability values of each word by adding the probability values of all segmentations in which the word appears, and normalizing the sum using a normalization factor.
- the Expectation-Maximization algorithm identifies word probability values and segmentations that tend to maximize the probability values of the segmentations. For example, a given sentence is segmented into [w 1 ][w 2 ] . . .
- the word corpus 402 may include the words where [ ] denotes a word.
- a sentence can be segmented into segments in several ways based on the words in the word corpus 402 , including the following:
- a sentence “ABCDAE” can be segmented in four different ways:
- the word probability determination engine 400 assigns an initial “soft-count” to each word in the word corpus 402 and determines an initial probability value for each word.
- the word soft-count corresponds to the number of occurrences of the word in the various segmentations of the sentences, taking into account the probability values of the segmentations.
- words in the document corpus 404 that are not in the word corpus 402 can be assigned low initial probability values. In other implementations, words in the document corpus 404 that are not in the word corpus 402 can be ignored.
- each of the words [A], [B], [C], [AB], [BC], [ABC], [D], and [E] is initially assigned a soft-count of 1.
- the probability value of each word is determined to be 1 ⁇ 8, as the sum of all the probability values of all words is equal to 1.
- the word probability determination engine 400 determines the probability value of each segmentation based on the probability values of the words in the segmentation.
- the segmentation probability value can be determined by, e.g., multiplying the word probability values of all the words in the segmentation.
- the segmentation probability values are then used to adjust the soft-counts of the words. If a word occurs in a segmentation with likelihood p i , the soft-count value for each word in the segmentation is increased by
- the soft-count of a particular word can be determined by adding the contributions of the segmentation probability values of all the segments in which the word appears, divided by a normalization factor.
- the word soft-counts are stored in a table 412 , and the next sentence, e.g., “ABABCDD” is processed to generate another set of word soft-counts. All the sentences in the documents 404 are processed in a similar manner, producing a set of word soft-counts for each of the sentences.
- different types of documents in the document corpus 404 can be given different weights, and the probability values of the candidate segmentations of a sentence from a particular type of document can be multiplied by a weight value associated with the particular type of document. For example, documents that are more formal and have words that are more accurate may be given higher weights than documents that are less formal and may include misspelled words. For example, news documents produced by major newspaper or magazine publishers may be given a weight of 17, blog documents generated by individuals may be given a weight of 10, other common web documents may be given a weight of 1, and e-mail messages may be given a weight of 0.7.
- the segmentation probability values derived from the news documents of major newspaper or magazine publishers may be multiplied by 17, the segmentation probability values derived from the blog documents of individuals may be multiplied by 10, the segmentation probability values derived from other types of web documents may be multiplied by 1, and the segmentation probability values derived from e-mail messages may be multiplied by 0.7. Because the soft-counts are derived from the segmentation probability values, the soft-counts are thus also weighted based on the document types.
- the word soft-counts derived from all the sentences in all the documents are combined by adding the respective soft-counts of each word.
- the final soft-count of the word [A] is obtained by accumulating (e.g., adding) the soft-counts of the word [A] derived from all of the sentences of all of the documents.
- the probability value of each word is determined by dividing the soft-count of the word by a normalization factor.
- the normalization factor can be, for example, the sum of soft-counts of all words.
- the words and the associated word probability values can be stored in the stored in the table 414 , thus completing the initial iteration
- the word soft-count values are adjusted according to the segmentation probability values, divide by the normalization factor, which can be the sum of probability values of all possible segmentations.
- the word soft-count values are as follows:
- the soft-counts from all the sentences are accumulated and normalized to obtain the probability values of the words, which are used to calculate the segmentation probability values in the next iteration, and so forth.
- the iterative process continues until a convergence or termination condition occurs. For example, the iterative process can be terminated after a predetermined number of iterations, or after the word probability values converge such that changes in the word probability values from one iteration to the next are less than a predetermined threshold. Other convergence or termination conditions can also be used.
- the soft-counting can be performed by dynamic programming. For example, assume that a sentence is “C 1 C 2 C 3 . . . C n ”, where each of “C 1 ”, “C 2 ”, “C 3 ”, etc., is a character. For any word C j1 . . . C j2 inside this sentence, its count can be increased by S j1 left ⁇ p(C j1 . . . C j2 ) ⁇ S j2 right / ⁇ , where
- S j1 left and S j2 right can likewise be computed by dynamic programming.
- the recursive function for S i left is
- the soft-count value of the word C j1 . . . C j2 can then be determined using the formula S j1 left ⁇ p(C j1 . . . C j2 ) ⁇ S j2 right / ⁇ .
- indexing and/or parallel processing of the documents can be utilized. Because the number of documents in the document corpus 404 can be large indexing and/or parallel processing can reduce processing times. In some implementations, processing of the document corpus 404 can be performed in parallel using, for example, a MapReduce programming model, described in “MapReduce: Simplified Data Processing on Large Clusters” by Jeffrey Dean and Sanjay Ghemawat, Sixth Symposium on Operating System Design and Implementation, San Francisco, Calif., December, 2004, the contents of which are herein incorporated by reference.
- the word probability determination engine 400 selects a predetermined number (e.g., 300,000) of words having the highest probability values and stores the selected words and their associated word probability values in the dictionary 406 to be accessed by the IME system 120 .
- the dictionary 406 can be part of, e.g., the dictionary 124 of FIG. 2 . In other implementations, the entire word corpus and associated probability values can be stored in the dictionary 406 .
- FIG. 5 is a flow diagram of an example process 500 to determine the probability values of words in a word corpus (e.g., the word corpus 402 ) based on occurrences of the words in a document corpus (e.g., the document corpus 404 ).
- the process 500 can, for example, be implemented in a system that includes one or more server computers.
- the process 500 identifies a word corpus ( 501 ).
- the word corpus can be the word corpus 402 of FIG. 4 .
- initial soft-count values are assigned to words in a word corpus and initial word probability values are determined ( 502 ).
- the word probability determination engine 400 can assign initial soft-count values and initial word probability values.
- the process 500 identifies sentences in documents of a document corpus ( 504 ).
- the process 500 determines candidate segmentations of each of the sentences based on the words in the word corpus ( 506 ). For example, candidate segmentations can be determined by the word segmentation engine 410 .
- the process 500 determines probability value for each of the candidate segmentations based on, e.g., probability values of the words in the candidate segmentation ( 508 ). For example, the probability value for a candidate segmentation can be determined by multiplying the probability values of the words in the candidate segmentation. In some implementations, the segmentation probability values are multiplied by weight values based on the type of documents from which the segmentations are derived.
- the process 500 determines a soft-count for each word based on the probability values of the candidate segmentations ( 510 ). For example, a soft-count of a word or phrase can be determined by adding the contributions of the probability values of all segmentations in which the word appears.
- the process 500 determines a probability value of each word by dividing the soft-count by a normalization factor ( 512 ).
- the normalization factor can be, e.g., the sum of all soft-counts.
- the process 500 determines if a termination condition as occurred ( 514 ).
- An termination condition can, for example, include the performance of a predetermined number of iterations, or a convergence of probability values, or some other condition.
- the process 500 identifies a predetermined number (e.g., 300,000) of words having highest probability values ( 516 ).
- the process 500 adds the selected predetermined number of words to a dictionary accessible to an input method editor ( 518 ).
- the IME can be the IME system 120 of FIG. 2 , and the predetermined number of words can be included in the dictionary 124 .
- FIG. 6 is a flow diagram of an example process 600 to determine the probability values of words in a word corpus based on occurrences of the words in a document corpus.
- the process 600 can, for example, be implemented in a system that includes one or more server computers.
- the process 600 identifies a word corpus ( 602 ).
- the word corpus can be the word corpus 402 of FIG. 4 .
- the process 600 associates a word probability value with each word in the word corpus ( 604 ).
- the word probability determination engine 400 can associate a word probability value with each word in the word corpus 402 .
- the process 600 identifies a sentence ( 606 ).
- the word probability determination engine 400 can identify a sentence from the document corpus 404 .
- the process 600 determines candidate segmentations of the sentence based on the word corpus ( 608 ).
- the word probability determination engine 400 can determine the candidate segmentations of the sentence.
- the process 600 iteratively adjusting the associated probability value for each word in the word corpus based on the probability values associated with the words and the candidate segmentations ( 610 ).
- the word probability determination engine 400 can iteratively adjust the associated probability value for each word.
- FIG. 7 is a flow diagram of an example process 700 to determine the probability values of words in a word corpus based on occurrences of the words in a document corpus.
- the process 700 can, for example, be implemented in a system that includes one or more server computers.
- the process 700 determines word probability values associated with words of a word corpus ( 702 ).
- the word corpus can be the word corpus 402 of FIG. 4 .
- the process 700 determines candidate segmentations of sentences of documents in a document corpus ( 704 ).
- the document corpus can be the document corpus 404 .
- the process 700 iteratively determines a segmentation probability value for each candidate segmentation of each sentence based on the word probability values associated with the words in the candidate segmentation ( 706 ).
- the word probability determination engine 400 can iteratively determine the segmentation probability values.
- the process 700 iteratively adjusts the word probability value for each word based on the segmentation probability values for the candidate segmentations that include the word ( 708 ).
- the word probability determination engine 400 can iteratively adjust the word probability values.
- the input engine 122 can be capable of mapping composition inputs from a western keyboard to input Chinese, Japanese, Korean and/or Indic characters.
- some or all implementations described can be applicable to other input methods, such as Cangjie input method, Jiufang input method, Wubi input method, or other input methods.
- the weight values for different types of documents, and the classification of types of documents, can be different from those described above.
- the number of words, phrases, and documents being processed, and the sources of the documents in the document corpus 404 can be different from those described above.
- dictionaries e.g., a legal dictionary, a medical dictionary, a science dictionary, and a general dictionary.
- Each dictionary can be established by starting with a dictionary associated with a particular field.
- the word probability determination engine 400 is used to process a document corpus having documents biased toward the field associated with the dictionary. For example, to establish the probability values of the words in the legal dictionary, a document corpus having documents biased toward the legal field can be used.
- the IME system 120 can allow the user to select the field of interest (e.g., legal, medical, science) when entering characters, and the candidate words can be selected from the dictionary related to the field of interest.
- Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- Embodiments of the subject matter described in this specification can be implemented as one or more computer program products, i.e., one or more modules of computer program instructions encoded on a tangible program carrier for execution by, or to control the operation of, data processing apparatus.
- the tangible program carrier can be a propagated signal or a computer readable medium.
- the propagated signal is an artificially generated signal, e.g., a machine generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a computer.
- the computer readable medium can be a machine readable storage device, a machine readable storage substrate, a memory device, a composition of matter effecting a machine readable propagated signal, or a combination of one or more of them.
- data processing apparatus encompasses all apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers.
- the apparatus can include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- a computer program (also known as a program, software, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages, and it can be deployed in any form, including as a stand alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.
- a computer program does not necessarily correspond to a file in a file system.
- a program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code).
- a computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- the processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer.
- a processor will receive instructions and data from a read only memory or a random access memory or both.
- the essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- a computer need not have such devices.
- a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, to name just a few.
- PDA personal digital assistant
- GPS Global Positioning System
- Computer readable media suitable for storing computer program instructions and data include all forms of non volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD ROM disks.
- semiconductor memory devices e.g., EPROM, EEPROM, and flash memory devices
- magnetic disks e.g., internal hard disks or removable disks
- magneto optical disks e.g., CD ROM and DVD ROM disks.
- the processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described is this specification, or any combination of one or more such back end, middleware, or front end components.
- the components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (“LAN”) and a wide area network (“WAN”), e.g., the Internet.
- LAN local area network
- WAN wide area network
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network.
- the relationship of client and server arises by virtue of computer programs running on the respective computers and having a client server relationship to each other.
Abstract
Description
-
-
In the above example segmentations, each segment is a word in the word corpus 402. Here, the term “segmentation” refers to the way that a sentence is segmented. Thus,
-
- [ABC][D][A][E],
- [AB][C][D][A][E],
- [A][BC][D][A][E], and
- [A][B][C][D][A][E].
P([ABC][D][A][E])=(⅛)^4=0.000244,
P([AB][C][D][A][E])=(⅛)^5=0.0000305,
P([A][BC][D][A][E])=(⅛)^5=0.0000305, and
P([A][B][C][D][A][E])=(⅛)^6=0.00000381,
where P([ ][ ][ ]) denotes the probability value of a segmentation [ ][ ][ ].
for each occurrence of the word, where t is the total number of possible segmentations, and
is a normalization factor that is equal to the sum of the likelihood of all possible segmentations. The soft-count of a particular word can be determined by adding the contributions of the segmentation probability values of all the segments in which the word appears, divided by a normalization factor. The normalization factor can be, for example, the sum of all segmentation probability values. In the above example, the normalization factor can be equal to (⅛)^4+(⅛)^5+(⅛)^5+(⅛)^6=0.000309.
S([A])=1.11,
S([B])=0.0123,
S([C])=0.111,
S([D])=1,
S([E])=1,
S([AB])=0.0988,
S([BC])=0.0988, and
S([ABC])=0.79,
where S([ ]) denotes the soft-count of the word [ ]. The word soft-counts are stored in a table 412, and the next sentence, e.g., “ABABCDD” is processed to generate another set of word soft-counts. All the sentences in the
S([A])=10,
S([B])=2,
S([C])=3,
S([D])=1,
S([E])=3,
S([AB])=2,
S([BC])=2, and
S([ABC])=1.
In the example above, the normalization factor can be equal to (10+2+3+1+3+2+2+1)=24. The word probability values are thus normalized as follows:
P([A])=10/24=0.417,
P([B])=2/24=0.083,
P([C])=3/24=0.125,
P([E])=3/24=0.125,
P([D])=1/24=0.0417,
P([AB])=2/24=0.0833,
P([BC])=2/24=0.0833, and
P([ABC])=1/24=0.0417.
The words and the associated word probability values can be stored in the stored in the table 414, thus completing the initial iteration
P([ABC][D][A][E])=0.0417*0.0417*0.417*0.125=0.0000906,
P([AB][C][D][A][E])=0.0833*0.125*0.0417*0.417*0.125=0.0000226,
P([A][BC][D][A][E])=0.417*0.0833*0.0417*0.417*0.125=0.0000755, and
P([A][B][C][D][A][E])=0.417*0.0833*0.125*0.0417*0.417*0.125=0.00000944.
-
- Sj1 left is the sum of the likelihood of all the possible segmentations of the substring to the left of Cj1,
- p(Cj1 . . . Cj2) is the current estimate of the probability of the word Cj1 . . . Cj2,
- Sj2 right is the sum of the likelihood of all the possible segmentations of the substring to the right of Cj2, and
- α is the normalizing constant, which is the sum of the likelihood of all the possible segmentations of this sentence. α is equal to Sn+1 left.
The values of Si left are computed for i=1, 2, . . . , n+1 from left to right of the sentence, at the end of which α=Sn+1 left is obtained. Then the values Si right are computed for i=n, n−1, . . . 3, 2, 1 from right to left of the sentence. The soft-count value of the word Cj1 . . . Cj2 can then be determined using the formula Sj1 left·p(Cj1 . . . Cj2)·Sj2 right/α.
Claims (26)
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/CN2007/001969 WO2009000103A1 (en) | 2007-06-25 | 2007-06-25 | Word probability determination |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
PCT/CN2007/001969 Continuation WO2009000103A1 (en) | 2007-06-25 | 2007-06-25 | Word probability determination |
Publications (2)
Publication Number | Publication Date |
---|---|
US20080319738A1 US20080319738A1 (en) | 2008-12-25 |
US8630847B2 true US8630847B2 (en) | 2014-01-14 |
Family
ID=40137418
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US11/870,068 Active 2029-06-28 US8630847B2 (en) | 2007-06-25 | 2007-10-10 | Word probability determination |
Country Status (5)
Country | Link |
---|---|
US (1) | US8630847B2 (en) |
JP (1) | JP2010531492A (en) |
KR (1) | KR101465770B1 (en) |
CN (1) | CN101785000B (en) |
WO (1) | WO2009000103A1 (en) |
Cited By (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20120296631A1 (en) * | 2011-05-20 | 2012-11-22 | Microsoft Corporation | Displaying key pinyins |
CN105334952A (en) * | 2014-07-11 | 2016-02-17 | 北京搜狗科技发展有限公司 | Input method and device of text information |
US10372714B2 (en) * | 2016-02-05 | 2019-08-06 | International Business Machines Corporation | Automated determination of document utility for a document corpus |
US20210042470A1 (en) * | 2018-09-14 | 2021-02-11 | Beijing Bytedance Network Technology Co., Ltd. | Method and device for separating words |
US20210224479A1 (en) * | 2020-01-19 | 2021-07-22 | Beijing Xiaomi Pinecone Electronics Co., Ltd. | Method for processing information, and storage medium |
US11100921B2 (en) * | 2018-04-19 | 2021-08-24 | Boe Technology Group Co., Ltd. | Pinyin-based method and apparatus for semantic recognition, and system for human-machine dialog |
Families Citing this family (134)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CA2376277C (en) | 1999-06-11 | 2011-03-15 | Telstra New Wave Pty Ltd | A method of developing an interactive system |
AU2002950336A0 (en) * | 2002-07-24 | 2002-09-12 | Telstra New Wave Pty Ltd | System and process for developing a voice application |
AU2002951244A0 (en) | 2002-09-06 | 2002-09-19 | Telstra New Wave Pty Ltd | A development system for a dialog system |
AU2003900584A0 (en) * | 2003-02-11 | 2003-02-27 | Telstra New Wave Pty Ltd | System for predicting speech recognition accuracy and development for a dialog system |
AU2003902020A0 (en) * | 2003-04-29 | 2003-05-15 | Telstra New Wave Pty Ltd | A process for grammatical inference |
US8677377B2 (en) | 2005-09-08 | 2014-03-18 | Apple Inc. | Method and apparatus for building an intelligent automated assistant |
US9318108B2 (en) | 2010-01-18 | 2016-04-19 | Apple Inc. | Intelligent automated assistant |
CN101779200B (en) * | 2007-06-14 | 2013-03-20 | 谷歌股份有限公司 | Dictionary word and phrase determination |
US10002189B2 (en) | 2007-12-20 | 2018-06-19 | Apple Inc. | Method and apparatus for searching using an active ontology |
US9330720B2 (en) | 2008-01-03 | 2016-05-03 | Apple Inc. | Methods and apparatus for altering audio output signals |
US8521516B2 (en) * | 2008-03-26 | 2013-08-27 | Google Inc. | Linguistic key normalization |
US8996376B2 (en) | 2008-04-05 | 2015-03-31 | Apple Inc. | Intelligent text-to-speech conversion |
US20100030549A1 (en) | 2008-07-31 | 2010-02-04 | Lee Michael M | Mobile device having human language translation capability with positional feedback |
US8433708B2 (en) * | 2008-09-16 | 2013-04-30 | Kendyl A. Román | Methods and data structures for improved searchable formatted documents including citation and corpus generation |
US8676904B2 (en) | 2008-10-02 | 2014-03-18 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
US20100094831A1 (en) * | 2008-10-14 | 2010-04-15 | Microsoft Corporation | Named entity resolution using multiple text sources |
US8798983B2 (en) * | 2009-03-30 | 2014-08-05 | Microsoft Corporation | Adaptation for statistical language model |
US10241752B2 (en) | 2011-09-30 | 2019-03-26 | Apple Inc. | Interface for a virtual digital assistant |
US10241644B2 (en) | 2011-06-03 | 2019-03-26 | Apple Inc. | Actionable reminder entries |
US9431006B2 (en) | 2009-07-02 | 2016-08-30 | Apple Inc. | Methods and apparatuses for automatic speech recognition |
JP5382651B2 (en) * | 2009-09-09 | 2014-01-08 | 独立行政法人情報通信研究機構 | Word pair acquisition device, word pair acquisition method, and program |
US8682667B2 (en) | 2010-02-25 | 2014-03-25 | Apple Inc. | User profiling for selecting user specific voice input processing information |
US8694304B2 (en) * | 2010-03-26 | 2014-04-08 | Virtuoz Sa | Semantic clustering and user interfaces |
US8676565B2 (en) * | 2010-03-26 | 2014-03-18 | Virtuoz Sa | Semantic clustering and conversational agents |
US9378202B2 (en) | 2010-03-26 | 2016-06-28 | Virtuoz Sa | Semantic clustering |
US9600566B2 (en) | 2010-05-14 | 2017-03-21 | Microsoft Technology Licensing, Llc | Identifying entity synonyms |
CN102411563B (en) * | 2010-09-26 | 2015-06-17 | 阿里巴巴集团控股有限公司 | Method, device and system for identifying target words |
US9524291B2 (en) | 2010-10-06 | 2016-12-20 | Virtuoz Sa | Visual display of semantic information |
US9262612B2 (en) | 2011-03-21 | 2016-02-16 | Apple Inc. | Device access using voice authentication |
US10057736B2 (en) | 2011-06-03 | 2018-08-21 | Apple Inc. | Active transport based notifications |
CN102929864B (en) * | 2011-08-05 | 2016-08-17 | 北京百度网讯科技有限公司 | A kind of tone-character conversion method and device |
US9305082B2 (en) * | 2011-09-30 | 2016-04-05 | Thomson Reuters Global Resources | Systems, methods, and interfaces for analyzing conceptually-related portions of text |
US10176168B2 (en) * | 2011-11-15 | 2019-01-08 | Microsoft Technology Licensing, Llc | Statistical machine translation based search query spelling correction |
US10134385B2 (en) | 2012-03-02 | 2018-11-20 | Apple Inc. | Systems and methods for name pronunciation |
US9280610B2 (en) | 2012-05-14 | 2016-03-08 | Apple Inc. | Crowd sourcing information to fulfill user requests |
US20130318075A1 (en) * | 2012-05-25 | 2013-11-28 | International Business Machines Corporation | Dictionary refinement for information extraction |
US9721563B2 (en) | 2012-06-08 | 2017-08-01 | Apple Inc. | Name recognition system |
US10032131B2 (en) | 2012-06-20 | 2018-07-24 | Microsoft Technology Licensing, Llc | Data services for enterprises leveraging search system data assets |
US9594831B2 (en) | 2012-06-22 | 2017-03-14 | Microsoft Technology Licensing, Llc | Targeted disambiguation of named entities |
US9229924B2 (en) * | 2012-08-24 | 2016-01-05 | Microsoft Technology Licensing, Llc | Word detection and domain dictionary recommendation |
US9547647B2 (en) | 2012-09-19 | 2017-01-17 | Apple Inc. | Voice-based media searching |
IL224482B (en) * | 2013-01-29 | 2018-08-30 | Verint Systems Ltd | System and method for keyword spotting using representative dictionary |
WO2014197334A2 (en) | 2013-06-07 | 2014-12-11 | Apple Inc. | System and method for user-specified pronunciation of words for speech synthesis and recognition |
WO2014197335A1 (en) | 2013-06-08 | 2014-12-11 | Apple Inc. | Interpreting and acting upon commands that involve sharing information with remote devices |
US10176167B2 (en) | 2013-06-09 | 2019-01-08 | Apple Inc. | System and method for inferring user intent from speech inputs |
KR101959188B1 (en) | 2013-06-09 | 2019-07-02 | 애플 인크. | Device, method, and graphical user interface for enabling conversation persistence across two or more instances of a digital assistant |
US9934217B2 (en) * | 2013-07-26 | 2018-04-03 | Facebook, Inc. | Index for electronic string of symbols |
WO2015018055A1 (en) * | 2013-08-09 | 2015-02-12 | Microsoft Corporation | Input method editor providing language assistance |
US10296160B2 (en) | 2013-12-06 | 2019-05-21 | Apple Inc. | Method for extracting salient dialog usage from live data |
US9966065B2 (en) | 2014-05-30 | 2018-05-08 | Apple Inc. | Multi-command single utterance input method |
US9430463B2 (en) | 2014-05-30 | 2016-08-30 | Apple Inc. | Exemplar-based natural language processing |
US9633004B2 (en) | 2014-05-30 | 2017-04-25 | Apple Inc. | Better resolution when referencing to concepts |
US10170123B2 (en) | 2014-05-30 | 2019-01-01 | Apple Inc. | Intelligent assistant for home automation |
US9338493B2 (en) | 2014-06-30 | 2016-05-10 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US9818400B2 (en) | 2014-09-11 | 2017-11-14 | Apple Inc. | Method and apparatus for discovering trending terms in speech requests |
US9606986B2 (en) * | 2014-09-29 | 2017-03-28 | Apple Inc. | Integrated word N-gram and class M-gram language models |
US10127911B2 (en) | 2014-09-30 | 2018-11-13 | Apple Inc. | Speaker identification and unsupervised speaker adaptation techniques |
US10074360B2 (en) | 2014-09-30 | 2018-09-11 | Apple Inc. | Providing an indication of the suitability of speech recognition |
US9668121B2 (en) | 2014-09-30 | 2017-05-30 | Apple Inc. | Social reminders |
EP3062212A1 (en) * | 2015-02-25 | 2016-08-31 | Kyocera Document Solutions Inc. | Text editing apparatus and print data storage apparatus |
US10152299B2 (en) | 2015-03-06 | 2018-12-11 | Apple Inc. | Reducing response latency of intelligent automated assistants |
US10567477B2 (en) | 2015-03-08 | 2020-02-18 | Apple Inc. | Virtual assistant continuity |
US9886953B2 (en) | 2015-03-08 | 2018-02-06 | Apple Inc. | Virtual assistant activation |
US9721566B2 (en) | 2015-03-08 | 2017-08-01 | Apple Inc. | Competing devices responding to voice triggers |
US10083688B2 (en) | 2015-05-27 | 2018-09-25 | Apple Inc. | Device voice control for selecting a displayed affordance |
US10325221B2 (en) * | 2015-06-02 | 2019-06-18 | Microsoft Technology Licensing, Llc | Metadata tag description generation |
US9578173B2 (en) | 2015-06-05 | 2017-02-21 | Apple Inc. | Virtual assistant aided communication with 3rd party service in a communication session |
US11025565B2 (en) | 2015-06-07 | 2021-06-01 | Apple Inc. | Personalized prediction of responses for instant messaging |
US10747498B2 (en) | 2015-09-08 | 2020-08-18 | Apple Inc. | Zero latency digital assistant |
US10671428B2 (en) | 2015-09-08 | 2020-06-02 | Apple Inc. | Distributed personal assistant |
US11010550B2 (en) | 2015-09-29 | 2021-05-18 | Apple Inc. | Unified language modeling framework for word prediction, auto-completion and auto-correction |
US10366158B2 (en) | 2015-09-29 | 2019-07-30 | Apple Inc. | Efficient word encoding for recurrent neural network language models |
US11587559B2 (en) | 2015-09-30 | 2023-02-21 | Apple Inc. | Intelligent device identification |
IL242219B (en) | 2015-10-22 | 2020-11-30 | Verint Systems Ltd | System and method for keyword searching using both static and dynamic dictionaries |
IL242218B (en) | 2015-10-22 | 2020-11-30 | Verint Systems Ltd | System and method for maintaining a dynamic dictionary |
US10691473B2 (en) | 2015-11-06 | 2020-06-23 | Apple Inc. | Intelligent automated assistant in a messaging environment |
US10049668B2 (en) | 2015-12-02 | 2018-08-14 | Apple Inc. | Applying neural network language models to weighted finite state transducers for automatic speech recognition |
CN105488209B (en) * | 2015-12-11 | 2019-06-07 | 北京奇虎科技有限公司 | A kind of analysis method and device of word weight |
US10223066B2 (en) | 2015-12-23 | 2019-03-05 | Apple Inc. | Proactive assistance based on dialog communication between devices |
US10446143B2 (en) | 2016-03-14 | 2019-10-15 | Apple Inc. | Identification of voice inputs providing credentials |
US9934775B2 (en) | 2016-05-26 | 2018-04-03 | Apple Inc. | Unit-selection text-to-speech synthesis based on predicted concatenation parameters |
US9972304B2 (en) | 2016-06-03 | 2018-05-15 | Apple Inc. | Privacy preserving distributed evaluation framework for embedded personalized systems |
US10249300B2 (en) | 2016-06-06 | 2019-04-02 | Apple Inc. | Intelligent list reading |
US10049663B2 (en) | 2016-06-08 | 2018-08-14 | Apple, Inc. | Intelligent automated assistant for media exploration |
DK179309B1 (en) | 2016-06-09 | 2018-04-23 | Apple Inc | Intelligent automated assistant in a home environment |
US10586535B2 (en) | 2016-06-10 | 2020-03-10 | Apple Inc. | Intelligent digital assistant in a multi-tasking environment |
US10067938B2 (en) | 2016-06-10 | 2018-09-04 | Apple Inc. | Multilingual word prediction |
US10509862B2 (en) | 2016-06-10 | 2019-12-17 | Apple Inc. | Dynamic phrase expansion of language input |
US10490187B2 (en) | 2016-06-10 | 2019-11-26 | Apple Inc. | Digital assistant providing automated status report |
US10192552B2 (en) | 2016-06-10 | 2019-01-29 | Apple Inc. | Digital assistant providing whispered speech |
DK201670540A1 (en) | 2016-06-11 | 2018-01-08 | Apple Inc | Application integration with a digital assistant |
DK179049B1 (en) | 2016-06-11 | 2017-09-18 | Apple Inc | Data driven natural language event detection and classification |
DK179415B1 (en) | 2016-06-11 | 2018-06-14 | Apple Inc | Intelligent device arbitration and control |
DK179343B1 (en) | 2016-06-11 | 2018-05-14 | Apple Inc | Intelligent task discovery |
US10474753B2 (en) | 2016-09-07 | 2019-11-12 | Apple Inc. | Language identification using recurrent neural networks |
US10043516B2 (en) | 2016-09-23 | 2018-08-07 | Apple Inc. | Intelligent automated assistant |
US11281993B2 (en) | 2016-12-05 | 2022-03-22 | Apple Inc. | Model and ensemble compression for metric learning |
US10593346B2 (en) | 2016-12-22 | 2020-03-17 | Apple Inc. | Rank-reduced token representation for automatic speech recognition |
US11204787B2 (en) | 2017-01-09 | 2021-12-21 | Apple Inc. | Application integration with a digital assistant |
DK201770383A1 (en) | 2017-05-09 | 2018-12-14 | Apple Inc. | User interface for correcting recognition errors |
US10417266B2 (en) | 2017-05-09 | 2019-09-17 | Apple Inc. | Context-aware ranking of intelligent response suggestions |
US10395654B2 (en) | 2017-05-11 | 2019-08-27 | Apple Inc. | Text normalization based on a data-driven learning network |
US10726832B2 (en) | 2017-05-11 | 2020-07-28 | Apple Inc. | Maintaining privacy of personal information |
DK201770439A1 (en) | 2017-05-11 | 2018-12-13 | Apple Inc. | Offline personal assistant |
DK179745B1 (en) | 2017-05-12 | 2019-05-01 | Apple Inc. | SYNCHRONIZATION AND TASK DELEGATION OF A DIGITAL ASSISTANT |
DK179496B1 (en) | 2017-05-12 | 2019-01-15 | Apple Inc. | USER-SPECIFIC Acoustic Models |
US11301477B2 (en) | 2017-05-12 | 2022-04-12 | Apple Inc. | Feedback analysis of a digital assistant |
DK201770427A1 (en) | 2017-05-12 | 2018-12-20 | Apple Inc. | Low-latency intelligent automated assistant |
DK201770431A1 (en) | 2017-05-15 | 2018-12-20 | Apple Inc. | Optimizing dialogue policy decisions for digital assistants using implicit feedback |
DK201770432A1 (en) | 2017-05-15 | 2018-12-21 | Apple Inc. | Hierarchical belief states for digital assistants |
US10311144B2 (en) | 2017-05-16 | 2019-06-04 | Apple Inc. | Emoji word sense disambiguation |
DK179560B1 (en) | 2017-05-16 | 2019-02-18 | Apple Inc. | Far-field extension for digital assistant services |
US10303715B2 (en) | 2017-05-16 | 2019-05-28 | Apple Inc. | Intelligent automated assistant for media exploration |
US10403278B2 (en) | 2017-05-16 | 2019-09-03 | Apple Inc. | Methods and systems for phonetic matching in digital assistant services |
US10657328B2 (en) | 2017-06-02 | 2020-05-19 | Apple Inc. | Multi-task recurrent neural network architecture for efficient morphology handling in neural language modeling |
US10445429B2 (en) | 2017-09-21 | 2019-10-15 | Apple Inc. | Natural language understanding using vocabularies with compressed serialized tries |
US10755051B2 (en) | 2017-09-29 | 2020-08-25 | Apple Inc. | Rule-based natural language processing |
US10636424B2 (en) | 2017-11-30 | 2020-04-28 | Apple Inc. | Multi-turn canned dialog |
US10733982B2 (en) | 2018-01-08 | 2020-08-04 | Apple Inc. | Multi-directional dialog |
US10733375B2 (en) | 2018-01-31 | 2020-08-04 | Apple Inc. | Knowledge-based framework for improving natural language understanding |
US10789959B2 (en) | 2018-03-02 | 2020-09-29 | Apple Inc. | Training speaker recognition models for digital assistants |
US10592604B2 (en) | 2018-03-12 | 2020-03-17 | Apple Inc. | Inverse text normalization for automatic speech recognition |
US10818288B2 (en) | 2018-03-26 | 2020-10-27 | Apple Inc. | Natural assistant interaction |
US10909331B2 (en) | 2018-03-30 | 2021-02-02 | Apple Inc. | Implicit identification of translation payload with neural machine translation |
US11145294B2 (en) | 2018-05-07 | 2021-10-12 | Apple Inc. | Intelligent automated assistant for delivering content from user experiences |
US10928918B2 (en) | 2018-05-07 | 2021-02-23 | Apple Inc. | Raise to speak |
US10984780B2 (en) | 2018-05-21 | 2021-04-20 | Apple Inc. | Global semantic word embeddings using bi-directional recurrent neural networks |
DK201870355A1 (en) | 2018-06-01 | 2019-12-16 | Apple Inc. | Virtual assistant operation in multi-device environments |
DK179822B1 (en) | 2018-06-01 | 2019-07-12 | Apple Inc. | Voice interaction at a primary device to access call functionality of a companion device |
US11386266B2 (en) | 2018-06-01 | 2022-07-12 | Apple Inc. | Text correction |
US10892996B2 (en) | 2018-06-01 | 2021-01-12 | Apple Inc. | Variable latency device coordination |
DK180639B1 (en) | 2018-06-01 | 2021-11-04 | Apple Inc | DISABILITY OF ATTENTION-ATTENTIVE VIRTUAL ASSISTANT |
US10496705B1 (en) | 2018-06-03 | 2019-12-03 | Apple Inc. | Accelerated task performance |
CN109325227A (en) * | 2018-09-14 | 2019-02-12 | 北京字节跳动网络技术有限公司 | Method and apparatus for generating amendment sentence |
Citations (13)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JPH10171819A (en) | 1996-12-06 | 1998-06-26 | Fuji Xerox Co Ltd | Information retrieving device |
US6356866B1 (en) * | 1998-10-07 | 2002-03-12 | Microsoft Corporation | Method for converting a phonetic character string into the text of an Asian language |
CN1447264A (en) | 2003-04-18 | 2003-10-08 | 清华大学 | Method for extracting words containing two Chinese characters based on restriction of semantic word forming |
US6640006B2 (en) * | 1998-02-13 | 2003-10-28 | Microsoft Corporation | Word segmentation in chinese text |
US6671683B2 (en) * | 2000-06-28 | 2003-12-30 | Matsushita Electric Industrial Co., Ltd. | Apparatus for retrieving similar documents and apparatus for extracting relevant keywords |
US6822585B1 (en) * | 1999-09-17 | 2004-11-23 | Nokia Mobile Phones, Ltd. | Input of symbols |
US6879951B1 (en) * | 1999-07-29 | 2005-04-12 | Matsushita Electric Industrial Co., Ltd. | Chinese word segmentation apparatus |
US20050209844A1 (en) * | 2004-03-16 | 2005-09-22 | Google Inc., A Delaware Corporation | Systems and methods for translating chinese pinyin to chinese characters |
US20050289463A1 (en) * | 2004-06-23 | 2005-12-29 | Google Inc., A Delaware Corporation | Systems and methods for spell correction of non-roman characters and words |
US20060015326A1 (en) | 2004-07-14 | 2006-01-19 | International Business Machines Corporation | Word boundary probability estimating, probabilistic language model building, kana-kanji converting, and unknown word model building |
US20060206313A1 (en) * | 2005-01-31 | 2006-09-14 | Nec (China) Co., Ltd. | Dictionary learning method and device using the same, input method and user terminal device using the same |
US7315982B2 (en) * | 2003-02-26 | 2008-01-01 | Xerox Corporation | User-tailorable romanized Chinese text input systems and methods |
US7917355B2 (en) * | 2007-08-23 | 2011-03-29 | Google Inc. | Word detection |
-
2007
- 2007-06-25 WO PCT/CN2007/001969 patent/WO2009000103A1/en active Application Filing
- 2007-06-25 CN CN2007801003679A patent/CN101785000B/en active Active
- 2007-06-25 KR KR1020107001484A patent/KR101465770B1/en active IP Right Grant
- 2007-06-25 JP JP2010513604A patent/JP2010531492A/en active Pending
- 2007-10-10 US US11/870,068 patent/US8630847B2/en active Active
Patent Citations (13)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JPH10171819A (en) | 1996-12-06 | 1998-06-26 | Fuji Xerox Co Ltd | Information retrieving device |
US6640006B2 (en) * | 1998-02-13 | 2003-10-28 | Microsoft Corporation | Word segmentation in chinese text |
US6356866B1 (en) * | 1998-10-07 | 2002-03-12 | Microsoft Corporation | Method for converting a phonetic character string into the text of an Asian language |
US6879951B1 (en) * | 1999-07-29 | 2005-04-12 | Matsushita Electric Industrial Co., Ltd. | Chinese word segmentation apparatus |
US6822585B1 (en) * | 1999-09-17 | 2004-11-23 | Nokia Mobile Phones, Ltd. | Input of symbols |
US6671683B2 (en) * | 2000-06-28 | 2003-12-30 | Matsushita Electric Industrial Co., Ltd. | Apparatus for retrieving similar documents and apparatus for extracting relevant keywords |
US7315982B2 (en) * | 2003-02-26 | 2008-01-01 | Xerox Corporation | User-tailorable romanized Chinese text input systems and methods |
CN1447264A (en) | 2003-04-18 | 2003-10-08 | 清华大学 | Method for extracting words containing two Chinese characters based on restriction of semantic word forming |
US20050209844A1 (en) * | 2004-03-16 | 2005-09-22 | Google Inc., A Delaware Corporation | Systems and methods for translating chinese pinyin to chinese characters |
US20050289463A1 (en) * | 2004-06-23 | 2005-12-29 | Google Inc., A Delaware Corporation | Systems and methods for spell correction of non-roman characters and words |
US20060015326A1 (en) | 2004-07-14 | 2006-01-19 | International Business Machines Corporation | Word boundary probability estimating, probabilistic language model building, kana-kanji converting, and unknown word model building |
US20060206313A1 (en) * | 2005-01-31 | 2006-09-14 | Nec (China) Co., Ltd. | Dictionary learning method and device using the same, input method and user terminal device using the same |
US7917355B2 (en) * | 2007-08-23 | 2011-03-29 | Google Inc. | Word detection |
Non-Patent Citations (21)
Title |
---|
Cui et al., "New Word Detection Based on Large-Scale Corpus," Journal of Computer Research and Development 43(5):927-932 (2006) (English version of abstract only). |
Feng et al., "Research and Realization on Self-Feeding Back Chinese Words Segmentation System," Computer Technology and Development, 16(5):7-9 (2006). |
Fuchun Peng and Dale Schuurmans. 2001. Self-Supervised Chinese Word Segmentation. InProceedings of the 4th International Conference on Advances in Intelligent Data Analysis (IDA '01), Frank Hoffmann, David J. Hand, Niall M. Adams, Douglas H. Fisher, and Gabriela Guimarães (Eds.). Springer-Verlag, London, UK, UK, 238-247. * |
Fuchun Peng, Supervisor Prof: Dale Schuurmans, Prof Frank Tompa. The Sparse Data Problem in Statistical Language Modeling and Unsupervised Word Segmentation. Oct. 2001. * |
Fuchun Peng, Xiangji Huang, Dale Schuurmans, and Nick Cercone. 2002. Investigating the relationship between word segmentation performance and retrieval performance in Chinese IR. In Proceedings of the 19th international conference on Computational linguistics-vol. 1 (COLING '02), vol. 1. * |
Fuchun Peng, Xiangji Huang, Dale Schuurmans, and Nick Cercone. 2002. Investigating the relationship between word segmentation performance and retrieval performance in Chinese IR. In Proceedings of the 19th international conference on Computational linguistics—vol. 1 (COLING '02), vol. 1. * |
Fuchun Peng, Xiangji Huang, Dale Schuurmans, and Nick Cercone. 2003. Applying Machine Learning to Text Segmentation for Information Retrieval. Information Retrieval, 6, 333-362, 2003. * |
Ge, X., Pratt, W., and Smyth, P. 1999. Discovering Chinese words from unsegmented text (poster abstract). In Proceedings of the 22nd Annual international ACM SIGIR Conference on Research and Development in information Retrieval (Berkeley, California, United States, Aug. 15-19, 1999). SIGIR '99. ACM, New York, NY, 271-272. DOI= http://doi.acm.or. * |
GenQing Wu and Fang Zheng. A method to build a super small but practically accurate language model for handheld devices. Journal of computer science and technology vol. 18, num. 6, 747-755. * |
Honglan Jin and Kam-Fai Wong. 2002. A Chinese dictionary construction algorithm for information retrieval. 1, 4 (Dec. 2002), 281-296. DOI=10.1145/795458.795460 http://doi.acm.org/10.1145/795458.795460. * |
International Preliminary Report on Patentability for PCT Application No. PCT/CN2007/001969, mailed Jan. 14, 2010, 6 pages. |
International Search Report and Written Opinion for PCT Application No. PCT/CN2007/001969, mailed Apr. 3, 2008, 10 pages. |
Jia-lin Tsai. Using Word-Pair Identifier to Improve Chinese Input System (2005). Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, IJCNLP2005. * |
Jing-shin Chang , Keh-yih Su. An Unsupervised Iterative Method for Chinese New Lexicon Extraction (1997). International Journal of Computational Linguistics & Chinese Language Processing. * |
Ming Zhou, Yuan Ding, Changning Huang. Improving Translation Selection with a New Translation Model Trained by Independent Monolingual Corpora. Computational Linguistics and Chinese Language Processing. vol. 6, No. 1, Feb. 2001, pp. 1-26. * |
Ponte, J. and Croft, W.; Useg: A retargetable word segmentation procedure for information retrieval. Symposium on Document Analysis and Information Retrival 96 (SDAIR). * |
R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos. 2004. A statistical model for multilingual entity detection and tracking. In of HLT-NAACL. * |
S.-S. Kang and C.-W. Woo. 2001. Automatic segmentation of words using syllable bigram statistics. In Proceedings of the 6th Natural Language Processing Pacific Rim Symposium, pp. 729-732. * |
Shiqi, C. et al., "New Word Detected Based on Large-Scale Corpus," Journal of Computer Research and Development, 2006, 43(5): 927-932. |
Sproat, R., Gale, W., Shih, C., and Chang, N. 1996. A stochastic finite-state word-segmentation algorithm for Chinese. Comput. Linguist. 22, 3 (Sep. 1996), 377-404. * |
Xiangji Huang, Fuchun Peng, Dale Schuurmans, Nick Cercone, Stephen E. Robertson. Applying Machine Learning to Text Segmentation for Information Retrieval. Information Retrieval.Sep. 2003, vol. 6, Issue 3-4, pp. 333-362. * |
Cited By (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20120296631A1 (en) * | 2011-05-20 | 2012-11-22 | Microsoft Corporation | Displaying key pinyins |
CN105334952A (en) * | 2014-07-11 | 2016-02-17 | 北京搜狗科技发展有限公司 | Input method and device of text information |
CN105334952B (en) * | 2014-07-11 | 2018-12-18 | 北京搜狗科技发展有限公司 | A kind of input method and device of text information |
US10372714B2 (en) * | 2016-02-05 | 2019-08-06 | International Business Machines Corporation | Automated determination of document utility for a document corpus |
US11550794B2 (en) | 2016-02-05 | 2023-01-10 | International Business Machines Corporation | Automated determination of document utility for a document corpus |
US11100921B2 (en) * | 2018-04-19 | 2021-08-24 | Boe Technology Group Co., Ltd. | Pinyin-based method and apparatus for semantic recognition, and system for human-machine dialog |
US20210042470A1 (en) * | 2018-09-14 | 2021-02-11 | Beijing Bytedance Network Technology Co., Ltd. | Method and device for separating words |
US20210224479A1 (en) * | 2020-01-19 | 2021-07-22 | Beijing Xiaomi Pinecone Electronics Co., Ltd. | Method for processing information, and storage medium |
US11475219B2 (en) * | 2020-01-19 | 2022-10-18 | Beijing Xiaomi Pinecone Electronics Co., Ltd. | Method for processing information, and storage medium |
Also Published As
Publication number | Publication date |
---|---|
CN101785000B (en) | 2013-04-24 |
US20080319738A1 (en) | 2008-12-25 |
KR20100052461A (en) | 2010-05-19 |
KR101465770B1 (en) | 2014-11-27 |
WO2009000103A1 (en) | 2008-12-31 |
CN101785000A (en) | 2010-07-21 |
JP2010531492A (en) | 2010-09-24 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US8630847B2 (en) | Word probability determination | |
US8386240B2 (en) | Domain dictionary creation by detection of new topic words using divergence value comparison | |
US8463598B2 (en) | Word detection | |
US10402493B2 (en) | System and method for inputting text into electronic devices | |
US8412517B2 (en) | Dictionary word and phrase determination | |
US8688727B1 (en) | Generating query refinements | |
US8010344B2 (en) | Dictionary word and phrase determination | |
US8046222B2 (en) | Segmenting words using scaled probabilities | |
US9026426B2 (en) | Input method editor | |
US9542476B1 (en) | Refining search queries | |
US8745051B2 (en) | Resource locator suggestions from input character sequence | |
JP5379138B2 (en) | Creating an area dictionary | |
Guy | The characteristics of voice search: Comparing spoken with typed-in mobile web search queries | |
US9881010B1 (en) | Suggestions based on document topics | |
US9411886B2 (en) | Ranking advertisements with pseudo-relevance feedback and translation models | |
JP2014120053A (en) | Question answering device, method, and program | |
KR102552811B1 (en) | System for providing cloud based grammar checker service | |
JP7139271B2 (en) | Information processing device, information processing method, and program |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:TANG, XI LIU;GE, XIANPING;REEL/FRAME:020074/0377;SIGNING DATES FROM 20070724 TO 20070725Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:TANG, XI LIU;GE, XIANPING;SIGNING DATES FROM 20070724 TO 20070725;REEL/FRAME:020074/0377 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
FPAY | Fee payment |
Year of fee payment: 4 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044101/0299Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 8 |