US20130138641A1 - Construction of text classifiers - Google Patents
Construction of text classifiers Download PDFInfo
- Publication number
- US20130138641A1 US20130138641A1 US12/650,443 US65044309A US2013138641A1 US 20130138641 A1 US20130138641 A1 US 20130138641A1 US 65044309 A US65044309 A US 65044309A US 2013138641 A1 US2013138641 A1 US 2013138641A1
- Authority
- US
- United States
- Prior art keywords
- classifier
- candidate
- topic
- document
- phrase
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/24—Querying
- G06F16/245—Query processing
- G06F16/2457—Query processing with adaptation to user needs
- G06F16/24578—Query processing with adaptation to user needs using ranking
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/35—Clustering; Classification
- G06F16/353—Clustering; Classification into predefined classes
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/951—Indexing; Web crawling techniques
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/953—Querying, e.g. by the use of web search engines
- G06F16/9535—Search customisation based on user profiles and personalisation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/953—Querying, e.g. by the use of web search engines
- G06F16/9538—Presentation of query results
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/30—Semantic analysis
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V30/00—Character recognition; Recognising digital ink; Document-oriented image-based pattern recognition
- G06V30/10—Character recognition
- G06V30/26—Techniques for post-processing, e.g. correcting the recognition result
- G06V30/262—Techniques for post-processing, e.g. correcting the recognition result using context analysis, e.g. lexical, syntactic or semantic context
- G06V30/274—Syntactic or semantic context, e.g. balancing
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F15/00—Digital computers in general; Data processing equipment in general
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/35—Clustering; Classification
Definitions
- This specification relates to constructing text classifiers.
- documents e.g., Web pages and Web sites
- a topic can be a subject, theme, or category of interest, for example, “baseball”, “politics”, “weather.”
- Linear classifiers include a number of phrases known to be indicative of a given topic and a value for each of the phrases. The document is classified as belonging to the topic in question if the sum of the values for all of the phrases occurring in the document exceeds a specified threshold.
- This specification describes technologies relating to constructing text classifiers.
- one aspect of the subject matter described in this specification can be embodied in methods that include the actions of receiving a collection of candidate phrases for a given topic; filtering the received candidate phrases to remove erroneously included candidate phrases; assigning weights to the candidate phrases including scoring each candidate phrase using an initial classifier and assigning weights to the candidate phrases based on the scores; and generating a linear classifier using the filtered and weighted candidate phrases, where the linear classifier varies the weights for each phrase candidate depending on the length of a document being classified.
- Other embodiments of this aspect include corresponding systems, apparatus, and computer program products.
- Generating phrase candidates further includes extracting n+k-grams from a collection of documents as extracted candidate phrases. Extracting n+k-grams from a particular document of a collection of documents includes receiving a document; breaking the document content into pieces of text; and extracting n+k-grams from each piece of text where n is a base order of the n-gram and k is a variable number of skip words.
- Generating phrase candidates further includes assigning scores to each Web site of a collection of Web sites corresponding to a probability of belonging to a certain topic; sorting n-grams in the collection of Web sites according to their occurrence in one or more of the plurality of Web sites having a threshold probability of belonging to the topic; and extracting n-grams of phrase candidates from a specified number of high probability Web sites.
- Generating phrase candidates further includes receiving query log information associating queries with one or more Web sites identified as responsive to the query; for a set of Web sites identified in the log for which an associated topic is known, identifying a number of frequently submitted queries associated with those Web sites of the set; and using the frequently submitted queries as candidate phrases for the topic of the associated Web pages.
- Generating phrase candidates further includes receiving a set of seed phrases, each with an estimate of a fraction of documents belonging to a particular topic among all documents containing the seed phrase; receiving an estimate of a fraction of all documents belonging to the topic; using the received seed phrases and estimates as an input along with a collection of labeled documents and a collection of unlabeled documents to train a linear classifier that approximately reproduces the estimated fractions; and using phrases identified by training the linear classifier as candidate phrases.
- the candidate phrases exclude the seed phrases. Scores for documents obtained from other classifiers are used as additional input to train the classifier.
- Filtering further includes issuing each of the received phrase candidates as queries to a search engine; classifying one or more search results identified by the search engine for each query as belonging to a specific topic; specifying a threshold of off-topic results with respect to search results identified by a query; and eliminating candidate phrases that exceed the threshold as candidate phrases for the topic.
- one aspect of the subject matter described in this specification can be embodied in methods that include the actions of receiving a collection of phrase candidates; receiving an initial classifier; assigning to each candidate phrase an average score of a document that contains the phrase candidate using the initial classifier; ordering candidate phrases by score and assigning weights to the phrase candidates as a function of the score; and generating a linear classifier using the phrase candidates and the assigned weights.
- Other embodiments of this aspect include corresponding systems, apparatus, and computer program products.
- the method further includes determining whether multiple iterations are to be performed; and when additional iterations are to be performed, using a previously generated linear classifiers as the initial classifier for the next iteration.
- the method further includes for each iteration determining whether supervised improvement is to be performed for the iteration; when supervised improvement is to be performed: sampling documents of a specified length; rating the sampled documents; and using the rated document to correct weights for the phrase candidates; and generating the linear classifier using the phrase candidates and the corrected weights.
- Correcting the assigned weights includes using a perceptron or support vector machine to correct phrase candidate weights.
- Performing additional iterations includes performing one or more iterations without supervised improvement and one or more iterations with supervised improvement.
- one aspect of the subject matter described in this specification can be embodied in methods that include the actions of receiving collection of phrase candidates; receiving an initial classifier; assigning a first weight to each phrase candidate using the initial classifier; generating a second classifier using the assigned first weights of the phrase candidates; assigning a second weight to each phrase candidate using the second classifier; and generating a third classifier using the assigned second weights of the phrase candidates.
- Other embodiments of this aspect include corresponding systems, apparatus, and computer program products.
- one aspect of the subject matter described in this specification can be embodied in methods that include the actions of receiving a collection of documents associated with a topic; generating a collection of candidate phrases using the collection of documents including extracting n+k-grams from the documents where n is a base order of an n+k-gram and k is a variable number of skip words where each n+k-gram begins and ends with a non skip word and includes exactly n non skip words; and generating a document classifier for the topic using the candidate phrases.
- Other embodiments of this aspect include corresponding systems, apparatus, and computer program products.
- FIG. 1 shows a flowchart of an example method of classifying a document.
- FIG. 2 shows a flowchart of an example method of generating candidate phrases using n+k-grams.
- FIG. 3 shows a flowchart of an example method of generating candidate phrases using expectation regularization.
- FIG. 4 shows a flowchart of an example method of filtering candidate phrases using web search.
- FIG. 5 shows a flowchart of an example method of using a collection of phrase candidates to iteratively generate a classifier.
- FIG. 6 illustrates an example system architecture
- FIG. 1 shows a flowchart of an example method 100 of classifying a document. For convenience, the method 100 is described with respect to a system that performs the method 100 .
- the system receives 102 a collection of candidate phrases.
- the collection of candidate phrases can be received from an external source of pre-generated candidate phrases or, alternatively, the candidate phrases can be generated by the system.
- the received candidate phrases are an existing collection generated, for example, from human experts.
- candidate phrases can be automatically generated from a collection of documents known to belong to a topic of interest (e.g., labeled by human raters) and a collection of documents known not to belong to the topic of interest.
- the generating system can then identify phrases that occur more often in the documents known to belong to the topic than they occur in the other documents. Those phrases are identified as candidate phrases.
- the candidate phrases can be n-grams of text extracted from the documents.
- a phrase can be defined as a specified sequence of words that have a particular semantic meaning when taken alone. For example, the phrases “sound of rain” or “heavy drops” as opposed to non-phrases “of rain” or “heavy”.
- any n-grams can be sued without being defined as phrases.
- n-gram phrases of order greater than 1 can provide higher quality candidate phrases.
- the order of n is fixed. However, in other implementations, the value of n can be flexible. Example techniques for generating candidate phrases (for example, from a collection of documents, Web pages scores, or from search queries) are described in greater detail below.
- the system filters 104 the received collection of candidate phrases.
- the system filters the candidate phrases to remove candidate phrases that are mistakenly part of the collection.
- using statistical methods to generate candidate phrases there are some number of phrases that are erroneously added to the collection of candidate phrases. Removing the erroneous phrases from the collection of candidate phrases can improve classifier results.
- the system assigns 106 weights to each candidate phrase.
- the system performs the assignment of weights and the filtering 104 concurrently or as part of a single technique. For example, an assigned weight of zero effectively filters out a given candidate phrase.
- Example techniques for filtering and assigning weights to candidate phrases are described in greater detail below.
- the system adjusts the classifier.
- the classifier is adjusted to correct weights assigned to the phrases in the classifier. For example, weights can need to be corrected when a systematic error allows some phrases to be part of the classifier that are not associated with the topic of interest to the classifier. Example correction techniques are described in greater detail below.
- the system generates 108 a linear document classifier using the filtered and weighted resulting phrases.
- the generated document classifier can be a linear classifier having the filtered candidate phrases as input phrases, each having an assigned weight.
- the generated classifier can be adjusted to take text length into account.
- using scores generated from phrase candidates only having positive weights can bias the classification of topicality to longer length documents. However, whether a given topic is affected by document length and to what degree can vary.
- the system can learn the dependency of text length from labeled examples which can then be used to adjust the classifier.
- One learning process to adjust the classifier for text length takes as input a set of tiples (s i , n i , l i ) where s i is a score assigned by a classifier for document i. n i is the number of words in the document, and l i is the label (e.g., true or false, depending on whether the document belongs to the topic of interest).
- the output of the learning process is a decision function that assigns a label to each pair (s, n) of score and document length.
- the decision function can be generated using various learning techniques, for example, adaptive boosting “AdaBoost” or support vector machines “SVM's”.
- AdaBoost adaptive boosting
- SVM's support vector machines
- monotonic regression can be used in place of a decision function, e.g., AdaBoost or SVM's, in order to provide a probability that a document belongs to the topic and learned in one step.
- a decision function e.g., AdaBoost or SVM's
- the score is less indicative of a particular topic the longer the document is. This is since even documents that do not belong to the particular topic have a certain probability of containing one or more of the candidate phrases for the topic. However, for longer documents there is a greater probability of such a phrase occurring than for shorter documents. Consequently, the probability that a document having a known score s and length n belongs to the given topic, expressed as a function p(i, ⁇ n) is monotonic in both variables. Therefore, a monotonic regression can be applied to the function p(s, ⁇ n) to learn the function p from labeled examples.
- the resulting document classifier can be applied to a set of documents in order to identify those documents of the collection which belong to a particular topic targeted by the classifier. For example, for a given document, the output of the classifier determines whether the document belongs to the topic or not based on a combination of weights for the phrases identified in the document (e.g., based on comparing a sum of phrase weights to a threshold).
- FIG. 2 shows a flowchart of an example method 200 of generating candidate phrases using n+k-grams. For convenience, the method 200 is described with respect to a system that performs the method 200 .
- the system receives 202 a document from a collection of documents from which phrase candidates are to be extracted.
- the system breaks 204 the content of the document into pieces of text that are likely to constitute logical units. Specifically, the system performs breaks at every HTML structure tag (e.g., div, tr, li, p, title, h1-h6, form, hr, dl, dd, dt, td, tr, th, option, img, pre, blockquote).
- the system also performs a break at each user-defined or language-specific punctuation character.
- the document text is further processed to drop any remaining punctuation, convert the text to lowercase, and remove text duplicates.
- HTML code For example, if the only punctuation character specified was a comma, the following HTML code:
- the system extracts 206 n+k-grams from each piece of text separately so that they do not cross piece boundaries.
- n the base order of an n+k-gram
- Skip words include common words with little or no semantic meaning on their own, for example, articles, prepositions, and pronouns.
- An n+k-gram starts and ends at a non-skip word, contains exactly n non-skip works and a variable number (k) of skip words that come in-between them.
- the system collects 208 the extracted n+k-grams for each document as candidate phrases of the collection for use in a document classifier for a particular topic.
- Web site scores are used to generate candidate phrases from a collection of Web documents.
- the system receives a collection of Web sites.
- the system assigns to each of the Web sites a probability of belonging to a certain topic based on a monotonic regression combining of scores of different site classifiers. For all n-grams (e.g., for n ⁇ 5) that appear on at least N (e.g., 100) Web sites, the system determines the portion of Web sites having a particular threshold probability of belonging to the topic or, alternatively, takes a top percentile of the Web sites (e.g., top 20%). The system then sorts the n-grams by the portion (e.g., based on probability). The n-gram phrases that tend to be particularly indicated of the given topic tend to appear early in this list. The system extracts the top n-grams as candidate phrases. For example, the top 1000 n-grams can be used as candidate phrases.
- particular search queries can be used as candidate phrases.
- the system receives query logs and associated Web sites (e.g., Web sites identified as responsive to the respective queries. For a set of Web sites in the collection having a known topic association (e.g., based on results from an initial classifier or from keywords in search results that roughly identify document topics) the system can obtain a number of most frequently submitted queries that result in a particular Web site being identified in the search results (e.g., from search logs). For example, the system can obtain a top 100 queries that result in the Web site being identified as a top search result. In some implementations, the most frequently submitted queries are assessed over a specified time period (e.g., 6 weeks). The resulting queries are used as candidate phrases for the topic associated with the particular Web site.
- a specified time period e.g., 6 weeks
- phrase candidates are generated using expectation regularization.
- FIG. 3 shows a flowchart of an example method 300 of generating candidate phrases using expectation regularization. For convenience, the method 300 is described with respect to a system that performs the method 300 .
- the system receives 302 a set of seed phrases with an estimate of a fraction of documents belonging to a given topic among all documents containing the feature.
- the set of seed phrases can be obtained from human experts. For example, human raters can identify that the phrase ‘Seattle Mariners’ is a seed phrase for the topic of ‘baseball’.
- the system also receives 304 an estimate for a fraction of all documents belonging to a given topic. Again, human experts can provide the estimates. For example, a human expert can determine that a document containing the phrase ‘Seattle Mariners’ has a 95% chance of being about baseball and that a document containing ‘Ichiro Suzuki’ has a 95% chance of being about baseball (e.g., based on rough estimates or experimental evidence from sampling for each phrase).
- the system uses 306 the received seed phrases and estimates as input along with a small set of labeled documents and a larger set of unlabeled documents to train a linear classifier that reproduces these fractions as closely as possible.
- a linear classifier that reproduces these fractions as closely as possible.
- all possible words (or n-grams) can occur as candidate phrases. Further details of expectation regularization techniques are described in Gideon S. Mann and Andrew McCallum, “Simple, Robust, Scalable Semi-supervised Learning via Expectation Regularization” in Proceedings of the 24 th International Conference on Machine Learning, Corvallis, Oreg., 2007.
- the system obtains 308 new candidate phrases from the output classifier. To obtain new phrases indicative of a given topic, the system examines those phrases of the output classifier having highest coefficients (i.e., the weights from the classifier where the sum for each phrase would be used when classifying documents based on the output classifier alone).
- the technique is modified to exclude input phrases in the linear classifier from being provided as outputs to the system.
- the system can use the classifier to identify new candidate phrases.
- the system uses the phrases generated by training the linear classifier as candidate phrases of a document classifier.
- ⁇ features can be used as input in addition to candidate phrases occurring in documents.
- scores of other classifiers e.g., image classifiers
- site scores as described above can be used as input features.
- a simple score of “1 if belonging to a known set of documents of this topic, 0 else” can be used as an input feature.
- the scores of additional classifiers provide additional input to the classifier.
- a classifier can have the following four input features for classifying a document: 1) the phrase ‘Seattle Mariners’ occurs in the document, 2) the phrase ‘Ichiro Suzuki’ occurs in the document, 3) the document has an image score above 0.9, and 4) the document is a known baseball Web site.
- the output features from the classifier will all be phrases.
- the non-phrase features are added as input to recognize more documents as belonging to a topic.
- all input features are discarded such that only newly generated phrases are used (e.g., if their coefficients/weights are above a threshold level).
- identified phrase candidates are positive features, that is the features are indicative that the document belongs to the given topic.
- the phrase candidates can also include negative features that indicate that the document does not belong to the topic (e.g., by assigning negative weights to the negative features).
- Phrase candidates can be pre-filtered before assigning weights in order to improve performance of weight assignment.
- One technique uses a Web search combined with a classifier of pages on a given topic.
- FIG. 4 shows a flowchart of an example method 400 of filtering candidate phrases using Web search.
- the method 400 is described with respect to a system that performs the method 400 .
- the system receives 402 a collection of candidate phrases.
- the candidate phrases can be generated as described above or received as a pre-generated collection.
- the system issues 404 the candidate phrases as queries to a search engine. For example, each candidate phrase can be specified in quotes and then submitted to the search engine. In some implementations, the system further specifies a language for the results.
- the system classifies 406 the top n search results (e.g., top ten results) with respect to being on a given topic (e.g., using an initial (less precise) classifier, keywords in URLs, or as belonging to a Web site known to be associated with the topic).
- the system specifies 408 a threshold on a number of off-topic results that a candidate phrase can yield (e.g., a number from 1-10). The system then drops 410 candidate phrases that exceed the specified threshold.
- the classifier is tuned for high recall to maximize the identification of good candidate phrases at the cost of allowing some bad phrase candidates to pass through. Consequently, the classifier can classify Web pages based on the URL of a search result. For example, the classifier can consider a Web page to be on a given topic if its URL either belongs to a list of URLs on the topic or contains any of the language-specific keywords characteristic for pages on the topic (e.g., based on human input identifying some keywords). For example, a human expert can enter keywords into a search engine and look at the URL's of the identified search results. Additionally, the URLs of the results can lead the human expert to identify additional keywords to a list commonly used in URLs of the topic. These can then be used to automatically identify a larger collection of potential keywords and phrase candidates.
- the classifier can consider a Web page to be on a given topic if its URL either belongs to a list of URLs on the topic or contains any of the language-specific keywords characteristic for pages on the topic (e.g., based on human
- each candidate phrase is assigned a weight for use with a classifier.
- an existing initial classifier is used to assign weights to phrase candidates.
- FIG. 5 shows a flowchart of an example method 500 of using a collection of phrase candidates to iteratively generate a classifier. For convenience, the method 500 is described with respect to a system that performs the method 500 .
- the system receives 502 a collection of phrase candidates.
- the phrase candidates can be generated as described above or received as a pre-generated collection.
- the received phrase candidates are presumed to be associated with a given topic of interest.
- the system optionally receives 504 an initial classifier or creates a classifier from the phrase candidates (assigning weight 1 to all phrase candidate that occur).
- the initial classifier is a classifier that assigns a score to a document such that the higher the score is, the more likely that the document belongs to the specified topic. If no initial classifier is given, the phrase candidates themselves can be used as a classifier by assigning each of them the weight 1.
- the initial classifier can be a basic linear classifier constructed using a few phrase candidates (e.g., to provide a rough estimate of whether documents belong to a topic), or an older classifier that is to be improved.
- the initial classifier can simply be represented by a collection of documents on the given topic (e.g., a collection of Web sites belonging to the topic). In such a scenario, the system assigns a score of 1 to documents in the collection on the topic and a score of zero to other documents.
- phrases candidates For a given set of phrase candidates, it can be assumed that most of the phrase candidates actually belong to the topic in question or that it is known that some portion of the phrase candidates are actually indicative of the topic. Therefore, documents of the topic usually contain several of the phrase candidates. Thus, if a feature often occurs alone on a document, the feature may not be a good phrase.
- the initial classifier weights documents by assigning a weight of 1 to the “most trusted” phrases and zero for other candidate phrases (or alternatively a higher weight for the “most trusted” phrases and a weight of 1 for other candidate phrases). In some implementations, where a part of the collection has a higher confidence than other parts, then only the portion with a high confidence is used.
- the system determines 506 whether to perform supervised improvement.
- Supervised improvements can be used to correct weights assigned to phrase candidates for use in a classifier, for example, using a perceptron or support vector machine technique described in greater detail below.
- the supervised improvement can identify and correct for systematic errors.
- the unsupervised technique is simpler and does not require human raters. Large amounts of data are used and can improve all phrase candidate weights, but can miss clusters of semantically related erroneous phrase candidate features.
- the supervised techniques are also more expensive (e.g., in terms of time and rater resources) to perform, and therefore for efficiency can be used only to correct a few weights of the most common phrase candidates as well as to identify clusters of semantically related, but wrong, phrase candidates.
- the system assigns 508 a score to each phrase candidate.
- the assigned score (e.g., as output from the initial classifier) can correspond to an average score of a document that contains the phrase candidate.
- the phrase candidates are ordered 510 by the score s and the weights are assigned 512 ⁇ (s) with a monotonic function ⁇ .
- a threshold is used to decide whether a given phrase has a particular assigned weight value (e.g., whether to give the phrase a weight of 2 or 3 based on the output score of the initial classifier).
- phrases removed from the classifier by having a weight of zero can be excluded from subsequent training iterations, which are described in greater detail below.
- long documents and short documents are considered separately or excluded since they may require different weighting thresholds.
- the initial classifier assigns a score of 1 for a document classified as belonging to the topic T and 0 otherwise (i.e., a binary classifier).
- the average score of the documents containing a phrase candidate X therefore corresponds to the probability p(T
- an automatic procedure based on a “Na ⁇ ve Bayes assumption” can alternatively be used to assign weights automatically.
- the Na ⁇ ve Bayes assumption is that the occurrence of different phrase candidates are statistically independent given the topic of the document.
- K log(1-p(T)) ⁇ log(p(T)).
- some phrase candidates that occur more often on documents of a given topic will also occur in clusters of features that are correlated because they also appear on documents of a different, but related topic. This violates the Na ⁇ ve Bayes assumption and results in pages that do not belong to the topic accumulating feature weights such that the pages will be classified as belonging to the topic. Consequently, the definition of w can be modified to select lower weights and/or to remove features with low weight in order to avoid misclassification of documents.
- the system samples 514 documents of a specific approximate length.
- the length used can vary depending on the application.
- the sampled documents have a length of approximately 1000 words (e.g., 750 to 1250 words).
- the number of sampled documents can also vary. In some implementations, 2000 documents are sampled.
- the system rates 516 each of the sampled documents.
- human raters can be used to manually rate the sampled documents as belonging to the given topic or not (e.g., a score of 1 or 0).
- One or more raters can be used for each document. In some implementations, three to five raters are used per document as a control.
- the system uses 518 the rated documents as input for a particular supervised improvement technique, e.g., SVM or perceptron. More specifically, a value is assigned to each phrase candidate occurring in each document. This value is assigned, for example, by an initial classifier, as a 1 if the phrase candidate occurs in the document and zero otherwise, as the number of times the phrase candidate occurs in the document, or according to a monotonic increasing function of the number of times the phrase candidate occurs. This value is then input to the supervised improvement technique. Additionally, the rater decision as well as initial classifier scores, if used, are input to the supervised improvement technique. The output of the supervised improvement technique is a weight correction for one or more of the phrase candidates.
- a particular supervised improvement technique e.g., SVM or perceptron. More specifically, a value is assigned to each phrase candidate occurring in each document. This value is assigned, for example, by an initial classifier, as a 1 if the phrase candidate occurs in the document and zero otherwise, as the number of times the phrase candidate
- the system can use a correction step during supervised improvement, for example, to remove erroneous phrases from the collection of candidate phrases or modify weights to generate correct classification results. For example, for a given linear classifier generated for a collection of phrases and associated weights, the system can test the classification for a set of documents for which it is known whether they belong to a particular topic. If a document is misclassified, the system adjusts the weight of all of the phrases up or down until the document is correctly classified. Testing a small set of known documents one or more times adjusts the weights such that more of the labeled documents are correctly classified by the classifier. For example, a perceptron technique can be used to correct the classifier.
- support vector machines are used to correct feature weights.
- Support vector machines (“SVM's”) are a set of related supervised learning methods used for classification and regression.
- a generated linear classifier can be input as one feature in the SVM.
- the other features can be other features (i.e. phrases) from a co-occurrence training technique described above as those phrases selected for use or those scoring over a specified threshold.
- An SVM regularization constant can be selected to provide a modest correction that limits the amount that weights are modified.
- SVM's In addition to using SVM's to tune the collection of phrases globally, they can alternatively be used to tune each individual feature by rescaling the particular feature. Consequently, instead of using a value of one for an input feature vector to the SVM if the phrase is present, the system uses an entry c for some number c that is lower for phrases that have a high level of confidence. As a result, the SVM has to “pay more” to change the weight assigned to the phrase, i.e. a change of that weight counts more towards the penalty introduced with the regularization.
- One way to identify phrases that have a high level of confidence is to identify those already observed in a number of documents in previous steps (e.g., in previous iterations).
- the additional weights are added to the existing weights for phrases in the classifier.
- the system uses documents having a common approximate length.
- a multiplier is applied to the SVM correction (e.g., 0.6) to ensure that the SVM does not over fit the training data.
- the system determines 522 whether additional iterations are to be performed. For each iteration, the process returns to the determining of whether to perform supervised improvement. Thus, some iterations can use supervised improvement while others do not.
- One example iterative schedule can begin with two iterations of the unsupervised method to remove any clearly erroneous features, then perform one supervised iteration to remove any clusters of semantically related wrong features. The system can then perform another two iterations of the unsupervised method, and then a final iteration of the supervised method to make sure the common features are optimally weighted. How the supervised and unsupervised methods are applied, as well as how many are scheduled, can depend on the application, quality of the initial classifier, and available resources.
- the iterative process reduces noise in the phrase weights. If there is a systematic error in the input collection of candidate phrases that includes a large number of phrases of one particular other topic, the phrases from this other topic may survive the iteration. Consequently, a supervised correction step can be included to eliminate such phrases.
- one or more new iterations can be performed either supervised or unsupervised.
- this co-occurrence technique is combined with techniques using existing classifiers in order to generate a weighted collection of phrase candidates for use in classifying documents.
- both techniques can be used and the resulting phrase weights averaged.
- the system When no additional iterations are performed, the system generates 524 a classifier using the phrase candidates and assigned weights from the last iteration.
- the number and type of iterations is fixed by a specified schedule.
- a stopping criteria is used, e.g., based on changes to weights made in the previous iteration.
- the resulting phrases candidates are to classify documents as belonging to a particular topic.
- documents can be input to the classifier and decisions output as to whether or not the document belongs to the topic classified by the classifier.
- n-grams X i and weights w(X i ) yield n-grams X i and weights w(X i ).
- n-grams it is possible for n-grams to overlap or to be substrings of each other. This can degrade performance since it effectively provide additional weight on longer n-grams that are more likely to have substrings among the collection of phrase candidates.
- the system can adjust the weights of substrings in order to correct for the bias given to longer n-grams.
- the score for the two n-grams will be independently calculated such that the score is w(x i )+w(x j ). In the Na ⁇ ve Bayes case described above, this corresponds to computing log(p(X i
- This correction can be applied to all chains of n-grams X ik where X ik is a substring of X i(k+1) by sorting the n-grams according to length and iteratively subtracting the weights of the shorter n-grams from all their parents. In cases where the sum of the weights of the substrings exceeds the weights of an n-gram, the corresponding n-gram may be dropped from the list of phrase candidates.
- FIG. 6 illustrates an example system architecture 600 .
- the system architecture 600 is capable of performing operations for constructing text classifiers.
- the system architecture 600 includes one or more processors 602 (e.g., IBM PowerPC, Intel Pentium 4, etc.), one or more display devices 604 (e.g., CRT, LCD), graphics processing units 606 (e.g., NVIDIA GeForce, etc.), a network interface 608 (e.g., Ethernet, FireWire, USB, etc.), input devices 610 (e.g., keyboard, mouse, etc.), and one or more computer-readable mediums 612 .
- processors 602 e.g., IBM PowerPC, Intel Pentium 4, etc.
- display devices 604 e.g., CRT, LCD
- graphics processing units 606 e.g., NVIDIA GeForce, etc.
- a network interface 608 e.g., Ethernet, FireWire, USB, etc.
- input devices 610 e.g., keyboard,
- the term “computer-readable medium” refers to any medium that participates in providing instructions to a processor 602 for execution.
- the computer-readable medium 612 further includes an operating system 616 (e.g., Mac OS®, Windows®, Linux, etc.), a network communication module 618 , a phrase generator 620 , and a document classifier 622 .
- the operating system 616 can be multi-user, multiprocessing, multitasking, multithreading, real-time and the like.
- the operating system 616 performs basic tasks, including but not limited to: recognizing input from input devices 610 ; sending output to display devices 604 ; keeping track of files and directories on computer-readable mediums 612 (e.g., memory or a storage device); controlling peripheral devices (e.g., disk drives, printers, etc.); and managing traffic on the one or more buses 614 .
- the network communications module 618 includes various components for establishing and maintaining network connections (e.g., software for implementing communication protocols, such as TCP/IP, HTTP, Ethernet, etc.).
- the phrase generator 620 and document classifier 622 provide various software components for performing the various functions for generating candidate phrases for use in training a document classifier and classifying documents as belonging to a topic as described with respect to FIGS. 1-5 .
- Embodiments of the subject matter and the operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions, encoded on a computer storage media for execution by, or to control the operation of, data processing apparatus.
- the computer storage medium can be, or be included in, a computer-readable storage device, a computer-readable storage substrate, a random or serial access memory array or device, or a combination of one or more of them.
- data processing apparatus encompasses all apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers.
- the apparatus can include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- a computer program (also known as a program, software, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.
- a computer program does not necessarily correspond to a file in a file system.
- a program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub-programs, or portions of code).
- a computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- the processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
- processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer.
- a processor will receive instructions and data from a read-only memory or a random access memory or both.
- the essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks.
- mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks.
- a computer need not have such devices.
- a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, to name just a few.
- PDA personal digital assistant
- GPS Global Positioning System
- Computer-readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks.
- semiconductor memory devices e.g., EPROM, EEPROM, and flash memory devices
- magnetic disks e.g., internal hard disks or removable disks
- magneto-optical disks e.g., CD-ROM and DVD-ROM disks.
- the processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front-end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described is this specification, or any combination of one or more such back-end, middleware, or front-end components.
- the components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (“LAN”) and a wide area network (“WAN”), e.g., the Internet.
- LAN local area network
- WAN wide area network
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network.
- the relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
Abstract
Description
- This specification relates to constructing text classifiers.
- In Web search, advertising, or for special content providers, documents (e.g., Web pages and Web sites) can be given a high value if they are associated with a particular topic of interest and a low value if they are associated with an irrelevant or offensive topic. A topic can be a subject, theme, or category of interest, for example, “baseball”, “politics”, “weather.”
- Thus, it is useful to be able to classify documents (e.g., particular Web pages or Web sites as a whole) as belonging to certain topics. One conventional technique for classifying documents is to use a linear classifier that uses the document text. Linear classifiers include a number of phrases known to be indicative of a given topic and a value for each of the phrases. The document is classified as belonging to the topic in question if the sum of the values for all of the phrases occurring in the document exceeds a specified threshold.
- This specification describes technologies relating to constructing text classifiers.
- In general, one aspect of the subject matter described in this specification can be embodied in methods that include the actions of receiving a collection of candidate phrases for a given topic; filtering the received candidate phrases to remove erroneously included candidate phrases; assigning weights to the candidate phrases including scoring each candidate phrase using an initial classifier and assigning weights to the candidate phrases based on the scores; and generating a linear classifier using the filtered and weighted candidate phrases, where the linear classifier varies the weights for each phrase candidate depending on the length of a document being classified. Other embodiments of this aspect include corresponding systems, apparatus, and computer program products.
- These and other embodiments can optionally include one or more of the following features. Generating phrase candidates further includes extracting n+k-grams from a collection of documents as extracted candidate phrases. Extracting n+k-grams from a particular document of a collection of documents includes receiving a document; breaking the document content into pieces of text; and extracting n+k-grams from each piece of text where n is a base order of the n-gram and k is a variable number of skip words. Generating phrase candidates further includes assigning scores to each Web site of a collection of Web sites corresponding to a probability of belonging to a certain topic; sorting n-grams in the collection of Web sites according to their occurrence in one or more of the plurality of Web sites having a threshold probability of belonging to the topic; and extracting n-grams of phrase candidates from a specified number of high probability Web sites.
- Generating phrase candidates further includes receiving query log information associating queries with one or more Web sites identified as responsive to the query; for a set of Web sites identified in the log for which an associated topic is known, identifying a number of frequently submitted queries associated with those Web sites of the set; and using the frequently submitted queries as candidate phrases for the topic of the associated Web pages.
- Generating phrase candidates further includes receiving a set of seed phrases, each with an estimate of a fraction of documents belonging to a particular topic among all documents containing the seed phrase; receiving an estimate of a fraction of all documents belonging to the topic; using the received seed phrases and estimates as an input along with a collection of labeled documents and a collection of unlabeled documents to train a linear classifier that approximately reproduces the estimated fractions; and using phrases identified by training the linear classifier as candidate phrases. The candidate phrases exclude the seed phrases. Scores for documents obtained from other classifiers are used as additional input to train the classifier.
- Filtering further includes issuing each of the received phrase candidates as queries to a search engine; classifying one or more search results identified by the search engine for each query as belonging to a specific topic; specifying a threshold of off-topic results with respect to search results identified by a query; and eliminating candidate phrases that exceed the threshold as candidate phrases for the topic.
- In general, one aspect of the subject matter described in this specification can be embodied in methods that include the actions of receiving a collection of phrase candidates; receiving an initial classifier; assigning to each candidate phrase an average score of a document that contains the phrase candidate using the initial classifier; ordering candidate phrases by score and assigning weights to the phrase candidates as a function of the score; and generating a linear classifier using the phrase candidates and the assigned weights. Other embodiments of this aspect include corresponding systems, apparatus, and computer program products.
- These and other embodiments can optionally include one or more of the following features. The method further includes determining whether multiple iterations are to be performed; and when additional iterations are to be performed, using a previously generated linear classifiers as the initial classifier for the next iteration. The method further includes for each iteration determining whether supervised improvement is to be performed for the iteration; when supervised improvement is to be performed: sampling documents of a specified length; rating the sampled documents; and using the rated document to correct weights for the phrase candidates; and generating the linear classifier using the phrase candidates and the corrected weights. Correcting the assigned weights includes using a perceptron or support vector machine to correct phrase candidate weights. Performing additional iterations includes performing one or more iterations without supervised improvement and one or more iterations with supervised improvement.
- In general, one aspect of the subject matter described in this specification can be embodied in methods that include the actions of receiving collection of phrase candidates; receiving an initial classifier; assigning a first weight to each phrase candidate using the initial classifier; generating a second classifier using the assigned first weights of the phrase candidates; assigning a second weight to each phrase candidate using the second classifier; and generating a third classifier using the assigned second weights of the phrase candidates. Other embodiments of this aspect include corresponding systems, apparatus, and computer program products.
- In general, one aspect of the subject matter described in this specification can be embodied in methods that include the actions of receiving a collection of documents associated with a topic; generating a collection of candidate phrases using the collection of documents including extracting n+k-grams from the documents where n is a base order of an n+k-gram and k is a variable number of skip words where each n+k-gram begins and ends with a non skip word and includes exactly n non skip words; and generating a document classifier for the topic using the candidate phrases. Other embodiments of this aspect include corresponding systems, apparatus, and computer program products.
- Particular embodiments of the subject matter described in this specification can be implemented to realize one or more of the following advantages. Accurate classifiers for particular topics can be generated with only small amounts of human labeled data.
- The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
-
FIG. 1 shows a flowchart of an example method of classifying a document. -
FIG. 2 shows a flowchart of an example method of generating candidate phrases using n+k-grams. -
FIG. 3 shows a flowchart of an example method of generating candidate phrases using expectation regularization. -
FIG. 4 shows a flowchart of an example method of filtering candidate phrases using web search. -
FIG. 5 shows a flowchart of an example method of using a collection of phrase candidates to iteratively generate a classifier. -
FIG. 6 illustrates an example system architecture. - Like reference numbers and designations in the various drawings indicate like elements.
-
FIG. 1 shows a flowchart of anexample method 100 of classifying a document. For convenience, themethod 100 is described with respect to a system that performs themethod 100. - The system receives 102 a collection of candidate phrases. The collection of candidate phrases can be received from an external source of pre-generated candidate phrases or, alternatively, the candidate phrases can be generated by the system. In some implementations, the received candidate phrases are an existing collection generated, for example, from human experts. Alternatively, candidate phrases can be automatically generated from a collection of documents known to belong to a topic of interest (e.g., labeled by human raters) and a collection of documents known not to belong to the topic of interest. The generating system can then identify phrases that occur more often in the documents known to belong to the topic than they occur in the other documents. Those phrases are identified as candidate phrases.
- The candidate phrases can be n-grams of text extracted from the documents. In particular, a phrase can be defined as a specified sequence of words that have a particular semantic meaning when taken alone. For example, the phrases “sound of rain” or “heavy drops” as opposed to non-phrases “of rain” or “heavy”. Alternatively, in some implementations, any n-grams can be sued without being defined as phrases. Typically, n-gram phrases of order greater than 1 can provide higher quality candidate phrases. In some implementations, the order of n is fixed. However, in other implementations, the value of n can be flexible. Example techniques for generating candidate phrases (for example, from a collection of documents, Web pages scores, or from search queries) are described in greater detail below.
- The system filters 104 the received collection of candidate phrases. In particular, the system filters the candidate phrases to remove candidate phrases that are mistakenly part of the collection. In particular, using statistical methods to generate candidate phrases, there are some number of phrases that are erroneously added to the collection of candidate phrases. Removing the erroneous phrases from the collection of candidate phrases can improve classifier results.
- The system assigns 106 weights to each candidate phrase. In some implementations, the system performs the assignment of weights and the
filtering 104 concurrently or as part of a single technique. For example, an assigned weight of zero effectively filters out a given candidate phrase. Example techniques for filtering and assigning weights to candidate phrases are described in greater detail below. - In some implementations, the system adjusts the classifier. In some implementations, the classifier is adjusted to correct weights assigned to the phrases in the classifier. For example, weights can need to be corrected when a systematic error allows some phrases to be part of the classifier that are not associated with the topic of interest to the classifier. Example correction techniques are described in greater detail below.
- The system generates 108 a linear document classifier using the filtered and weighted resulting phrases. In particular, the generated document classifier can be a linear classifier having the filtered candidate phrases as input phrases, each having an assigned weight.
- In some implementations, the generated classifier can be adjusted to take text length into account. In particular, using scores generated from phrase candidates only having positive weights can bias the classification of topicality to longer length documents. However, whether a given topic is affected by document length and to what degree can vary. The system can learn the dependency of text length from labeled examples which can then be used to adjust the classifier.
- One learning process to adjust the classifier for text length takes as input a set of tiples (si, ni, li) where si is a score assigned by a classifier for document i. ni is the number of words in the document, and li is the label (e.g., true or false, depending on whether the document belongs to the topic of interest). The output of the learning process is a decision function that assigns a label to each pair (s, n) of score and document length. The decision function can be generated using various learning techniques, for example, adaptive boosting “AdaBoost” or support vector machines “SVM's”. A particular function can be trained that translates from the score output by the decision function to a probability that the document has the given topic.
- Additionally, monotonic regression can be used in place of a decision function, e.g., AdaBoost or SVM's, in order to provide a probability that a document belongs to the topic and learned in one step. For a given text score s, it is possible that the score is less indicative of a particular topic the longer the document is. This is since even documents that do not belong to the particular topic have a certain probability of containing one or more of the candidate phrases for the topic. However, for longer documents there is a greater probability of such a phrase occurring than for shorter documents. Consequently, the probability that a document having a known score s and length n belongs to the given topic, expressed as a function p(i, −n) is monotonic in both variables. Therefore, a monotonic regression can be applied to the function p(s, −n) to learn the function p from labeled examples.
- The resulting document classifier can be applied to a set of documents in order to identify those documents of the collection which belong to a particular topic targeted by the classifier. For example, for a given document, the output of the classifier determines whether the document belongs to the topic or not based on a combination of weights for the phrases identified in the document (e.g., based on comparing a sum of phrase weights to a threshold).
- Generating Phrase Candidates
-
FIG. 2 shows a flowchart of anexample method 200 of generating candidate phrases using n+k-grams. For convenience, themethod 200 is described with respect to a system that performs themethod 200. - The system receives 202 a document from a collection of documents from which phrase candidates are to be extracted. The system breaks 204 the content of the document into pieces of text that are likely to constitute logical units. Specifically, the system performs breaks at every HTML structure tag (e.g., div, tr, li, p, title, h1-h6, form, hr, dl, dd, dt, td, tr, th, option, img, pre, blockquote). The system also performs a break at each user-defined or language-specific punctuation character. In some implementations, the document text is further processed to drop any remaining punctuation, convert the text to lowercase, and remove text duplicates.
- For example, if the only punctuation character specified was a comma, the following HTML code:
-
Come and hear the sound of rain, its <b>heavy</b> drops: <ul> <li>plim-plum <li>plim-plum </ul>
would be broken into the following pieces of text: -
come and hear the sound of rain its heavy drops plim plum - The system extracts 206 n+k-grams from each piece of text separately so that they do not cross piece boundaries. For each language, the system defines n (the base order of an n+k-gram) and a list of skip words. Skip words include common words with little or no semantic meaning on their own, for example, articles, prepositions, and pronouns. An n+k-gram starts and ends at a non-skip word, contains exactly n non-skip works and a variable number (k) of skip words that come in-between them.
- Continuing the above example, if n=2 and skip words include “and”, “its”, “of”, and “the” the following n+k-grams are extracted:
-
come and hear hear the sound sound of rain heavy drops plim plum - The system collects 208 the extracted n+k-grams for each document as candidate phrases of the collection for use in a document classifier for a particular topic.
- In some alternative implementations, Web site scores are used to generate candidate phrases from a collection of Web documents. The system receives a collection of Web sites. The system assigns to each of the Web sites a probability of belonging to a certain topic based on a monotonic regression combining of scores of different site classifiers. For all n-grams (e.g., for n<5) that appear on at least N (e.g., 100) Web sites, the system determines the portion of Web sites having a particular threshold probability of belonging to the topic or, alternatively, takes a top percentile of the Web sites (e.g., top 20%). The system then sorts the n-grams by the portion (e.g., based on probability). The n-gram phrases that tend to be particularly indicated of the given topic tend to appear early in this list. The system extracts the top n-grams as candidate phrases. For example, the top 1000 n-grams can be used as candidate phrases.
- In some other alternative implementations, particular search queries can be used as candidate phrases. The system receives query logs and associated Web sites (e.g., Web sites identified as responsive to the respective queries. For a set of Web sites in the collection having a known topic association (e.g., based on results from an initial classifier or from keywords in search results that roughly identify document topics) the system can obtain a number of most frequently submitted queries that result in a particular Web site being identified in the search results (e.g., from search logs). For example, the system can obtain a top 100 queries that result in the Web site being identified as a top search result. In some implementations, the most frequently submitted queries are assessed over a specified time period (e.g., 6 weeks). The resulting queries are used as candidate phrases for the topic associated with the particular Web site.
- In some other implementations, phrase candidates are generated using expectation regularization.
FIG. 3 shows a flowchart of anexample method 300 of generating candidate phrases using expectation regularization. For convenience, themethod 300 is described with respect to a system that performs themethod 300. - The system receives 302 a set of seed phrases with an estimate of a fraction of documents belonging to a given topic among all documents containing the feature. The set of seed phrases can be obtained from human experts. For example, human raters can identify that the phrase ‘Seattle Mariners’ is a seed phrase for the topic of ‘baseball’. The system also receives 304 an estimate for a fraction of all documents belonging to a given topic. Again, human experts can provide the estimates. For example, a human expert can determine that a document containing the phrase ‘Seattle Mariners’ has a 95% chance of being about baseball and that a document containing ‘Ichiro Suzuki’ has a 95% chance of being about baseball (e.g., based on rough estimates or experimental evidence from sampling for each phrase). The system uses 306 the received seed phrases and estimates as input along with a small set of labeled documents and a larger set of unlabeled documents to train a linear classifier that reproduces these fractions as closely as possible. In the linear classifier, all possible words (or n-grams) can occur as candidate phrases. Further details of expectation regularization techniques are described in Gideon S. Mann and Andrew McCallum, “Simple, Robust, Scalable Semi-supervised Learning via Expectation Regularization” in Proceedings of the 24th International Conference on Machine Learning, Corvallis, Oreg., 2007.
- The system obtains 308 new candidate phrases from the output classifier. To obtain new phrases indicative of a given topic, the system examines those phrases of the output classifier having highest coefficients (i.e., the weights from the classifier where the sum for each phrase would be used when classifying documents based on the output classifier alone).
- In some implementations, the technique is modified to exclude input phrases in the linear classifier from being provided as outputs to the system. Thus, the system can use the classifier to identify new candidate phrases. The system uses the phrases generated by training the linear classifier as candidate phrases of a document classifier.
- Other types of features can be used as input in addition to candidate phrases occurring in documents. For example, scores of other classifiers (e.g., image classifiers) or site scores as described above can be used as input features. Additionally, a simple score of “1 if belonging to a known set of documents of this topic, 0 else” can be used as an input feature. The scores of additional classifiers provide additional input to the classifier. For example, a classifier can have the following four input features for classifying a document: 1) the phrase ‘Seattle Mariners’ occurs in the document, 2) the phrase ‘Ichiro Suzuki’ occurs in the document, 3) the document has an image score above 0.9, and 4) the document is a known baseball Web site. The output features from the classifier will all be phrases. The non-phrase features are added as input to recognize more documents as belonging to a topic. In some implementations, all input features (phrases included) are discarded such that only newly generated phrases are used (e.g., if their coefficients/weights are above a threshold level).
- In some implementations, identified phrase candidates are positive features, that is the features are indicative that the document belongs to the given topic. However, the phrase candidates can also include negative features that indicate that the document does not belong to the topic (e.g., by assigning negative weights to the negative features).
- Filtering Candidate Phrases
- Phrase candidates can be pre-filtered before assigning weights in order to improve performance of weight assignment. One technique uses a Web search combined with a classifier of pages on a given topic.
-
FIG. 4 shows a flowchart of anexample method 400 of filtering candidate phrases using Web search. For convenience, themethod 400 is described with respect to a system that performs themethod 400. - The system receives 402 a collection of candidate phrases. The candidate phrases can be generated as described above or received as a pre-generated collection. The system issues 404 the candidate phrases as queries to a search engine. For example, each candidate phrase can be specified in quotes and then submitted to the search engine. In some implementations, the system further specifies a language for the results. For each submitted query, the system classifies 406 the top n search results (e.g., top ten results) with respect to being on a given topic (e.g., using an initial (less precise) classifier, keywords in URLs, or as belonging to a Web site known to be associated with the topic).
- The system specifies 408 a threshold on a number of off-topic results that a candidate phrase can yield (e.g., a number from 1-10). The system then drops 410 candidate phrases that exceed the specified threshold.
- In some implementations, the classifier is tuned for high recall to maximize the identification of good candidate phrases at the cost of allowing some bad phrase candidates to pass through. Consequently, the classifier can classify Web pages based on the URL of a search result. For example, the classifier can consider a Web page to be on a given topic if its URL either belongs to a list of URLs on the topic or contains any of the language-specific keywords characteristic for pages on the topic (e.g., based on human input identifying some keywords). For example, a human expert can enter keywords into a search engine and look at the URL's of the identified search results. Additionally, the URLs of the results can lead the human expert to identify additional keywords to a list commonly used in URLs of the topic. These can then be used to automatically identify a larger collection of potential keywords and phrase candidates.
- Assigning Weights to Candidate Phrases
- Once a collection of candidate phrases is received or generated, each candidate phrase is assigned a weight for use with a classifier. In some implementations, an existing initial classifier is used to assign weights to phrase candidates.
-
FIG. 5 shows a flowchart of anexample method 500 of using a collection of phrase candidates to iteratively generate a classifier. For convenience, themethod 500 is described with respect to a system that performs themethod 500. - The system receives 502 a collection of phrase candidates. The phrase candidates can be generated as described above or received as a pre-generated collection. The received phrase candidates are presumed to be associated with a given topic of interest.
- The system optionally receives 504 an initial classifier or creates a classifier from the phrase candidates (assigning weight 1 to all phrase candidate that occur). The initial classifier is a classifier that assigns a score to a document such that the higher the score is, the more likely that the document belongs to the specified topic. If no initial classifier is given, the phrase candidates themselves can be used as a classifier by assigning each of them the weight 1.
- The initial classifier can be a basic linear classifier constructed using a few phrase candidates (e.g., to provide a rough estimate of whether documents belong to a topic), or an older classifier that is to be improved. Alternatively, the initial classifier can simply be represented by a collection of documents on the given topic (e.g., a collection of Web sites belonging to the topic). In such a scenario, the system assigns a score of 1 to documents in the collection on the topic and a score of zero to other documents.
- For a given set of phrase candidates, it can be assumed that most of the phrase candidates actually belong to the topic in question or that it is known that some portion of the phrase candidates are actually indicative of the topic. Therefore, documents of the topic usually contain several of the phrase candidates. Thus, if a feature often occurs alone on a document, the feature may not be a good phrase.
- In some other implementations, the initial classifier weights documents by assigning a weight of 1 to the “most trusted” phrases and zero for other candidate phrases (or alternatively a higher weight for the “most trusted” phrases and a weight of 1 for other candidate phrases). In some implementations, where a part of the collection has a higher confidence than other parts, then only the portion with a high confidence is used.
- The system determines 506 whether to perform supervised improvement. Supervised improvements can be used to correct weights assigned to phrase candidates for use in a classifier, for example, using a perceptron or support vector machine technique described in greater detail below. In particular, the supervised improvement can identify and correct for systematic errors. The unsupervised technique is simpler and does not require human raters. Large amounts of data are used and can improve all phrase candidate weights, but can miss clusters of semantically related erroneous phrase candidate features. However, the supervised techniques are also more expensive (e.g., in terms of time and rater resources) to perform, and therefore for efficiency can be used only to correct a few weights of the most common phrase candidates as well as to identify clusters of semantically related, but wrong, phrase candidates.
- When not performing supervised improvement and given the initial classifier, the system assigns 508 a score to each phrase candidate. The assigned score (e.g., as output from the initial classifier) can correspond to an average score of a document that contains the phrase candidate. The phrase candidates are ordered 510 by the score s and the weights are assigned 512 ƒ(s) with a monotonic function ƒ.
- For example, the system can use the weights: 0=“not indicative of the topic”, 1=“could occur on a document with that topic, but also on other pages”, 2=“usually pages which contain this phrase candidate belongs to the topic”, and 3=“a document containing this phrase candidate almost certainly belongs to the topic”. A threshold is used to decide whether a given phrase has a particular assigned weight value (e.g., whether to give the phrase a weight of 2 or 3 based on the output score of the initial classifier). Optionally, phrases removed from the classifier by having a weight of zero can be excluded from subsequent training iterations, which are described in greater detail below. In some implementations, long documents and short documents are considered separately or excluded since they may require different weighting thresholds.
- In some implementations, the initial classifier assigns a score of 1 for a document classified as belonging to the topic T and 0 otherwise (i.e., a binary classifier). The average score of the documents containing a phrase candidate X therefore corresponds to the probability p(T|X) that a document that contains X is classified as belonging to T.
- In this scenario an automatic procedure based on a “Naïve Bayes assumption” can alternatively be used to assign weights automatically. The Naïve Bayes assumption is that the occurrence of different phrase candidates are statistically independent given the topic of the document. In this scenario the system calculates from the results of the document classification a fraction p(T) of documents belonging to the topic and calculates K=log(1-p(T))−log(p(T)). For each feature X the weight is calculated as w(X)=log(p(T|X)) log(1−p(T|X))+K. This is a specific function of p(T|X) that can be computed automatically and is a result of statistical assumptions. Under the same assumptions the threshold for the resulting classifier can be calculated.
- In some implementations, some phrase candidates that occur more often on documents of a given topic will also occur in clusters of features that are correlated because they also appear on documents of a different, but related topic. This violates the Naïve Bayes assumption and results in pages that do not belong to the topic accumulating feature weights such that the pages will be classified as belonging to the topic. Consequently, the definition of w can be modified to select lower weights and/or to remove features with low weight in order to avoid misclassification of documents.
- Supervised Improvement
- When the system determines 506 that supervised improvement is performed, the
system samples 514 documents of a specific approximate length. The length used can vary depending on the application. For example, in some implementations, the sampled documents have a length of approximately 1000 words (e.g., 750 to 1250 words). The number of sampled documents can also vary. In some implementations, 2000 documents are sampled. - The system rates 516 each of the sampled documents. In particular, human raters can be used to manually rate the sampled documents as belonging to the given topic or not (e.g., a score of 1 or 0). One or more raters can be used for each document. In some implementations, three to five raters are used per document as a control.
- The system uses 518 the rated documents as input for a particular supervised improvement technique, e.g., SVM or perceptron. More specifically, a value is assigned to each phrase candidate occurring in each document. This value is assigned, for example, by an initial classifier, as a 1 if the phrase candidate occurs in the document and zero otherwise, as the number of times the phrase candidate occurs in the document, or according to a monotonic increasing function of the number of times the phrase candidate occurs. This value is then input to the supervised improvement technique. Additionally, the rater decision as well as initial classifier scores, if used, are input to the supervised improvement technique. The output of the supervised improvement technique is a weight correction for one or more of the phrase candidates.
- The system can use a correction step during supervised improvement, for example, to remove erroneous phrases from the collection of candidate phrases or modify weights to generate correct classification results. For example, for a given linear classifier generated for a collection of phrases and associated weights, the system can test the classification for a set of documents for which it is known whether they belong to a particular topic. If a document is misclassified, the system adjusts the weight of all of the phrases up or down until the document is correctly classified. Testing a small set of known documents one or more times adjusts the weights such that more of the labeled documents are correctly classified by the classifier. For example, a perceptron technique can be used to correct the classifier.
- In some other implementations, support vector machines are used to correct feature weights. Support vector machines (“SVM's”) are a set of related supervised learning methods used for classification and regression. A generated linear classifier can be input as one feature in the SVM. The other features can be other features (i.e. phrases) from a co-occurrence training technique described above as those phrases selected for use or those scoring over a specified threshold. An SVM regularization constant can be selected to provide a modest correction that limits the amount that weights are modified.
- In addition to using SVM's to tune the collection of phrases globally, they can alternatively be used to tune each individual feature by rescaling the particular feature. Consequently, instead of using a value of one for an input feature vector to the SVM if the phrase is present, the system uses an entry c for some number c that is lower for phrases that have a high level of confidence. As a result, the SVM has to “pay more” to change the weight assigned to the phrase, i.e. a change of that weight counts more towards the penalty introduced with the regularization. One way to identify phrases that have a high level of confidence is to identify those already observed in a number of documents in previous steps (e.g., in previous iterations).
- When the SVM has computed the additional weights for one or more phrases, the additional weights are added to the existing weights for phrases in the classifier. In some implementations, when training the SVM, the system uses documents having a common approximate length.
- In some implementations, instead of, or in addition to, the regularization constant for the SVM, a multiplier is applied to the SVM correction (e.g., 0.6) to ensure that the SVM does not over fit the training data.
- In both supervised and non-supervised scenarios, the system determines 522 whether additional iterations are to be performed. For each iteration, the process returns to the determining of whether to perform supervised improvement. Thus, some iterations can use supervised improvement while others do not. One example iterative schedule can begin with two iterations of the unsupervised method to remove any clearly erroneous features, then perform one supervised iteration to remove any clusters of semantically related wrong features. The system can then perform another two iterations of the unsupervised method, and then a final iteration of the supervised method to make sure the common features are optimally weighted. How the supervised and unsupervised methods are applied, as well as how many are scheduled, can depend on the application, quality of the initial classifier, and available resources.
- The iterative process reduces noise in the phrase weights. If there is a systematic error in the input collection of candidate phrases that includes a large number of phrases of one particular other topic, the phrases from this other topic may survive the iteration. Consequently, a supervised correction step can be included to eliminate such phrases.
- Following such a correction, one or more new iterations can be performed either supervised or unsupervised. In some implementations, this co-occurrence technique is combined with techniques using existing classifiers in order to generate a weighted collection of phrase candidates for use in classifying documents. Alternatively, both techniques can be used and the resulting phrase weights averaged.
- When no additional iterations are performed, the system generates 524 a classifier using the phrase candidates and assigned weights from the last iteration. In some implementations, the number and type of iterations is fixed by a specified schedule. In some other implementations, a stopping criteria is used, e.g., based on changes to weights made in the previous iteration.
- Once the linear classifier has been generated for the identified and weighted phrase candidates, the resulting phrases candidates are to classify documents as belonging to a particular topic. For example, documents can be input to the classifier and decisions output as to whether or not the document belongs to the topic classified by the classifier.
- The above techniques for generating a collection of phrase candidates and assigning weights yield n-grams Xi and weights w(Xi). However, in general, it is possible for n-grams to overlap or to be substrings of each other. This can degrade performance since it effectively provide additional weight on longer n-grams that are more likely to have substrings among the collection of phrase candidates.
- The system can adjust the weights of substrings in order to correct for the bias given to longer n-grams. In particular, for two n-grams i and j where Xi is a substring of Xj, the score for the two n-grams will be independently calculated such that the score is w(xi)+w(xj). In the Naïve Bayes case described above, this corresponds to computing log(p(Xi|T))−log(p(Xi|N))+log(p(Xj|T))−log(p(Xj|N)). The system corrects the weight w(Xj) to w′(Xj) such that w′(Xj)=[log(p(Xj|T))−log(p(Xj|N))]−[log(p(Xi|T)−log(p(Xi|N))]=w(Xj)−w(Xi) to provide a final score of w′(Xj)+w(Xi)=log(p(Xj|N))−log(p(Xj|N).
- This correction can be applied to all chains of n-grams Xik where Xik is a substring of Xi(k+1) by sorting the n-grams according to length and iteratively subtracting the weights of the shorter n-grams from all their parents. In cases where the sum of the weights of the substrings exceeds the weights of an n-gram, the corresponding n-gram may be dropped from the list of phrase candidates.
-
FIG. 6 illustrates anexample system architecture 600. Thesystem architecture 600 is capable of performing operations for constructing text classifiers. Thesystem architecture 600 includes one or more processors 602 (e.g., IBM PowerPC, Intel Pentium 4, etc.), one or more display devices 604 (e.g., CRT, LCD), graphics processing units 606 (e.g., NVIDIA GeForce, etc.), a network interface 608 (e.g., Ethernet, FireWire, USB, etc.), input devices 610 (e.g., keyboard, mouse, etc.), and one or more computer-readable mediums 612. These components exchange communications and data using one or more buses 614 (e.g., EISA, PCI, PCI Express, etc.). - The term “computer-readable medium” refers to any medium that participates in providing instructions to a
processor 602 for execution. The computer-readable medium 612 further includes an operating system 616 (e.g., Mac OS®, Windows®, Linux, etc.), anetwork communication module 618, aphrase generator 620, and adocument classifier 622. - The
operating system 616 can be multi-user, multiprocessing, multitasking, multithreading, real-time and the like. Theoperating system 616 performs basic tasks, including but not limited to: recognizing input frominput devices 610; sending output to displaydevices 604; keeping track of files and directories on computer-readable mediums 612 (e.g., memory or a storage device); controlling peripheral devices (e.g., disk drives, printers, etc.); and managing traffic on the one ormore buses 614. Thenetwork communications module 618 includes various components for establishing and maintaining network connections (e.g., software for implementing communication protocols, such as TCP/IP, HTTP, Ethernet, etc.). - The
phrase generator 620 anddocument classifier 622 provide various software components for performing the various functions for generating candidate phrases for use in training a document classifier and classifying documents as belonging to a topic as described with respect toFIGS. 1-5 . - Embodiments of the subject matter and the operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions, encoded on a computer storage media for execution by, or to control the operation of, data processing apparatus. The computer storage medium can be, or be included in, a computer-readable storage device, a computer-readable storage substrate, a random or serial access memory array or device, or a combination of one or more of them.
- The term “data processing apparatus” encompasses all apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. The apparatus can include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- A computer program (also known as a program, software, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A computer program does not necessarily correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub-programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
- Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, a computer need not have such devices. Moreover, a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, to name just a few.
- Computer-readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front-end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described is this specification, or any combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (“LAN”) and a wide area network (“WAN”), e.g., the Internet.
- The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any implementation or of what may be claimed, but rather as descriptions of features that may be specific to particular embodiments of particular implementations. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
- Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In certain circumstances, multitasking and parallel processing may be advantageous. Moreover, the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
- Particular embodiments of the subject matter described in this specification have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In certain implementations, multitasking and parallel processing may be advantageous.
Claims (32)
Priority Applications (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US12/650,443 US8868402B2 (en) | 2009-12-30 | 2009-12-30 | Construction of text classifiers |
US14/518,891 US9317564B1 (en) | 2009-12-30 | 2014-10-20 | Construction of text classifiers |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US12/650,443 US8868402B2 (en) | 2009-12-30 | 2009-12-30 | Construction of text classifiers |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US14/518,891 Continuation US9317564B1 (en) | 2009-12-30 | 2014-10-20 | Construction of text classifiers |
Publications (2)
Publication Number | Publication Date |
---|---|
US20130138641A1 true US20130138641A1 (en) | 2013-05-30 |
US8868402B2 US8868402B2 (en) | 2014-10-21 |
Family
ID=48467754
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US12/650,443 Active 2031-06-11 US8868402B2 (en) | 2009-12-30 | 2009-12-30 | Construction of text classifiers |
US14/518,891 Active US9317564B1 (en) | 2009-12-30 | 2014-10-20 | Construction of text classifiers |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US14/518,891 Active US9317564B1 (en) | 2009-12-30 | 2014-10-20 | Construction of text classifiers |
Country Status (1)
Country | Link |
---|---|
US (2) | US8868402B2 (en) |
Cited By (38)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20100286979A1 (en) * | 2007-08-01 | 2010-11-11 | Ginger Software, Inc. | Automatic context sensitive language correction and enhancement using an internet corpus |
US20130018876A1 (en) * | 2010-09-28 | 2013-01-17 | International Business Machines Corporation | Providing answers to questions using hypothesis pruning |
US20130317804A1 (en) * | 2012-05-24 | 2013-11-28 | John R. Hershey | Method of Text Classification Using Discriminative Topic Transformation |
US20140180692A1 (en) * | 2011-02-28 | 2014-06-26 | Nuance Communications, Inc. | Intent mining via analysis of utterances |
US20140280011A1 (en) * | 2013-03-15 | 2014-09-18 | Google Inc. | Predicting Site Quality |
US9015036B2 (en) | 2010-02-01 | 2015-04-21 | Ginger Software, Inc. | Automatic context sensitive language correction using an internet corpus particularly for small keyboard devices |
US9135544B2 (en) | 2007-11-14 | 2015-09-15 | Varcode Ltd. | System and method for quality management utilizing barcode indicators |
US9372850B1 (en) * | 2012-12-19 | 2016-06-21 | Amazon Technologies, Inc. | Machined book detection |
US9400952B2 (en) | 2012-10-22 | 2016-07-26 | Varcode Ltd. | Tamper-proof quality management barcode indicators |
AU2016219728A1 (en) * | 2015-08-28 | 2017-03-16 | Accenture Global Services Limited | Automated term extraction |
US9646277B2 (en) | 2006-05-07 | 2017-05-09 | Varcode Ltd. | System and method for improved quality management in a product logistic chain |
US20170228361A1 (en) * | 2016-02-10 | 2017-08-10 | Yong Zhang | Electronic message information retrieval system |
US10083237B2 (en) | 2015-08-31 | 2018-09-25 | Google Llc | Protecting users from inappropriate sensitive or offensive search results |
CN108628873A (en) * | 2017-03-17 | 2018-10-09 | 腾讯科技（北京）有限公司 | A kind of file classification method, device and equipment |
US10176451B2 (en) | 2007-05-06 | 2019-01-08 | Varcode Ltd. | System and method for quality management utilizing barcode indicators |
CN109189932A (en) * | 2018-09-06 | 2019-01-11 | 北京京东尚科信息技术有限公司 | File classification method and device, computer readable storage medium |
US10242323B2 (en) * | 2015-09-17 | 2019-03-26 | Chatterbox Labs Limited | Customisable method of data filtering |
US20190205325A1 (en) * | 2017-12-29 | 2019-07-04 | Aiqudo, Inc. | Automated Discourse Phrase Discovery for Generating an Improved Language Model of a Digital Assistant |
US10437902B1 (en) * | 2013-04-17 | 2019-10-08 | A9.Com, Inc. | Extracting product references from unstructured text |
US10445678B2 (en) | 2006-05-07 | 2019-10-15 | Varcode Ltd. | System and method for improved quality management in a product logistic chain |
US10454776B2 (en) * | 2017-04-20 | 2019-10-22 | Cisco Technologies, Inc. | Dynamic computer network classification using machine learning |
US10467339B1 (en) * | 2018-06-28 | 2019-11-05 | Sap Se | Using machine learning and natural language processing to replace gender biased words within free-form text |
US20200125639A1 (en) * | 2018-10-22 | 2020-04-23 | Ca, Inc. | Generating training data from a machine learning model to identify offensive language |
US20200126533A1 (en) * | 2018-10-22 | 2020-04-23 | Ca, Inc. | Machine learning model for identifying offensive, computer-generated natural-language text or speech |
US10697837B2 (en) | 2015-07-07 | 2020-06-30 | Varcode Ltd. | Electronic quality indicator |
CN111552809A (en) * | 2020-04-22 | 2020-08-18 | 中国电力科学研究院有限公司 | Power grid field phrase identification and classification method and system based on Baidu encyclopedia |
CN112016323A (en) * | 2020-08-28 | 2020-12-01 | 中国科学技术大学 | Automatic extraction method of technical phrases in patent |
US20210034784A1 (en) * | 2019-08-01 | 2021-02-04 | International Business Machines Corporation | Detangling virtual world and real world portions of a set of articles |
US10929613B2 (en) | 2017-12-29 | 2021-02-23 | Aiqudo, Inc. | Automated document cluster merging for topic-based digital assistant interpretation |
US10957431B2 (en) | 2018-04-20 | 2021-03-23 | International Business Machines Corporation | Human resource selection based on readability of unstructured text within an individual case safety report (ICSR) and confidence of the ICSR |
US10963499B2 (en) | 2017-12-29 | 2021-03-30 | Aiqudo, Inc. | Generating command-specific language model discourses for digital assistant interpretation |
US11017180B2 (en) * | 2018-04-18 | 2021-05-25 | HelpShift, Inc. | System and methods for processing and interpreting text messages |
US11060924B2 (en) | 2015-05-18 | 2021-07-13 | Varcode Ltd. | Thermochromic ink indicia for activatable quality labels |
US11361165B2 (en) * | 2020-03-27 | 2022-06-14 | The Clorox Company | Methods and systems for topic detection in natural language communications |
US11397558B2 (en) | 2017-05-18 | 2022-07-26 | Peloton Interactive, Inc. | Optimizing display engagement in action automation |
CN115114915A (en) * | 2022-05-25 | 2022-09-27 | 腾讯科技（深圳）有限公司 | Phrase recognition method, apparatus, device and medium |
US11556737B2 (en) * | 2019-12-04 | 2023-01-17 | At&T Intellectual Property I, L.P. | System, method, and platform for auto machine learning via optimal hybrid AI formulation from crowd |
US11704526B2 (en) | 2008-06-10 | 2023-07-18 | Varcode Ltd. | Barcoded indicators for quality management |
Families Citing this family (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9311423B1 (en) * | 2010-02-19 | 2016-04-12 | Go Daddy Operating Company, LLC | System and method for website categorization |
WO2011118428A1 (en) * | 2010-03-26 | 2011-09-29 | 日本電気株式会社 | Requirement acquisition system, requirement acquisition method, and programme for requirement acquisition |
US20130173254A1 (en) * | 2011-12-31 | 2013-07-04 | Farrokh Alemi | Sentiment Analyzer |
KR102437689B1 (en) * | 2015-09-16 | 2022-08-30 | 삼성전자주식회사 | Voice recognition sever and control method thereof |
US10984475B1 (en) * | 2015-12-24 | 2021-04-20 | Jpmorgan Chase Bank, N.A. | Method and system for implementing counterparty credit risk calculations |
WO2020033804A1 (en) * | 2018-08-09 | 2020-02-13 | Walmart Apollo, Llc | System and method for electronic text classification |
US11087098B2 (en) * | 2018-09-18 | 2021-08-10 | Sap Se | Computer systems for classifying multilingual text |
US11922331B2 (en) | 2020-08-26 | 2024-03-05 | Northrop Grumman Systems Corporation | Machine-learning-based predictive ice detection |
Citations (21)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20020022956A1 (en) * | 2000-05-25 | 2002-02-21 | Igor Ukrainczyk | System and method for automatically classifying text |
US20020099730A1 (en) * | 2000-05-12 | 2002-07-25 | Applied Psychology Research Limited | Automatic text classification system |
US20020138529A1 (en) * | 1999-05-05 | 2002-09-26 | Bokyung Yang-Stephens | Document-classification system, method and software |
US20030225763A1 (en) * | 2002-04-15 | 2003-12-04 | Microsoft Corporation | Self-improving system and method for classifying pages on the world wide web |
US6704905B2 (en) * | 2000-12-28 | 2004-03-09 | Matsushita Electric Industrial Co., Ltd. | Text classifying parameter generator and a text classifier using the generated parameter |
US20040049498A1 (en) * | 2002-07-03 | 2004-03-11 | Dehlinger Peter J. | Text-classification code, system and method |
US6937994B1 (en) * | 2000-02-24 | 2005-08-30 | International Business Machines Corporation | System and method for efficiently generating models for targeting products and promotions using classification method by choosing points to be labeled |
US20050234953A1 (en) * | 2004-04-15 | 2005-10-20 | Microsoft Corporation | Verifying relevance between keywords and Web site contents |
US20060069678A1 (en) * | 2004-09-30 | 2006-03-30 | Wu Chou | Method and apparatus for text classification using minimum classification error to train generalized linear classifier |
US20060074908A1 (en) * | 2004-09-24 | 2006-04-06 | Selvaraj Sathiya K | Method and apparatus for efficient training of support vector machines |
US20060085405A1 (en) * | 2004-10-18 | 2006-04-20 | Fu-Chiang Hsu | Method for analyzing and classifying electronic document |
US20060142993A1 (en) * | 2004-12-28 | 2006-06-29 | Sony Corporation | System and method for utilizing distance measures to perform text classification |
US20060178869A1 (en) * | 2005-02-10 | 2006-08-10 | Microsoft Corporation | Classification filter for processing data for creating a language model |
US20060212423A1 (en) * | 2005-03-16 | 2006-09-21 | Rosie Jones | System and method for biasing search results based on topic familiarity |
US7254774B2 (en) * | 2004-03-16 | 2007-08-07 | Microsoft Corporation | Systems and methods for improved spell checking |
US20080243479A1 (en) * | 2007-04-02 | 2008-10-02 | University Of Washington | Open information extraction from the web |
US20080249764A1 (en) * | 2007-03-01 | 2008-10-09 | Microsoft Corporation | Smart Sentiment Classifier for Product Reviews |
US20090055381A1 (en) * | 2007-08-23 | 2009-02-26 | Google Inc. | Domain Dictionary Creation |
US7529731B2 (en) * | 2004-06-29 | 2009-05-05 | Xerox Corporation | Automatic discovery of classification related to a category using an indexed document collection |
US20100094875A1 (en) * | 2008-08-11 | 2010-04-15 | Collective Media, Inc. | Method and system for classifying text |
US7725414B2 (en) * | 2004-03-16 | 2010-05-25 | Buzzmetrics, Ltd An Israel Corporation | Method for developing a classifier for classifying communications |
Family Cites Families (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2940501B2 (en) * | 1996-12-25 | 1999-08-25 | 日本電気株式会社 | Document classification apparatus and method |
AU764415B2 (en) * | 1999-08-06 | 2003-08-21 | Lexis-Nexis | System and method for classifying legal concepts using legal topic scheme |
JP3856778B2 (en) * | 2003-09-29 | 2006-12-13 | 株式会社日立製作所 | Document classification apparatus and document classification method for multiple languages |
JP2005158010A (en) * | 2003-10-31 | 2005-06-16 | Hewlett-Packard Development Co Lp | Apparatus, method and program for classification evaluation |
US7672940B2 (en) * | 2003-12-04 | 2010-03-02 | Microsoft Corporation | Processing an electronic document for information extraction |
JP4713870B2 (en) * | 2004-10-13 | 2011-06-29 | ヒューレット−パッカード デベロップメント カンパニー エル．ピー． | Document classification apparatus, method, and program |
US7734554B2 (en) * | 2005-10-27 | 2010-06-08 | Hewlett-Packard Development Company, L.P. | Deploying a document classification system |
US7788292B2 (en) * | 2007-12-12 | 2010-08-31 | Microsoft Corporation | Raising the baseline for high-precision text classifiers |
WO2009097459A1 (en) * | 2008-01-29 | 2009-08-06 | Educational Testing Service | System and method for disambiguating the effect of text document length on vector-based similarit scores |
US8275803B2 (en) * | 2008-05-14 | 2012-09-25 | International Business Machines Corporation | System and method for providing answers to questions |
-
2009
- 2009-12-30 US US12/650,443 patent/US8868402B2/en active Active
-
2014
- 2014-10-20 US US14/518,891 patent/US9317564B1/en active Active
Patent Citations (23)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20020138529A1 (en) * | 1999-05-05 | 2002-09-26 | Bokyung Yang-Stephens | Document-classification system, method and software |
US6937994B1 (en) * | 2000-02-24 | 2005-08-30 | International Business Machines Corporation | System and method for efficiently generating models for targeting products and promotions using classification method by choosing points to be labeled |
US20020099730A1 (en) * | 2000-05-12 | 2002-07-25 | Applied Psychology Research Limited | Automatic text classification system |
US20060143175A1 (en) * | 2000-05-25 | 2006-06-29 | Kanisa Inc. | System and method for automatically classifying text |
US20020022956A1 (en) * | 2000-05-25 | 2002-02-21 | Igor Ukrainczyk | System and method for automatically classifying text |
US6704905B2 (en) * | 2000-12-28 | 2004-03-09 | Matsushita Electric Industrial Co., Ltd. | Text classifying parameter generator and a text classifier using the generated parameter |
US20030225763A1 (en) * | 2002-04-15 | 2003-12-04 | Microsoft Corporation | Self-improving system and method for classifying pages on the world wide web |
US20040049498A1 (en) * | 2002-07-03 | 2004-03-11 | Dehlinger Peter J. | Text-classification code, system and method |
US7254774B2 (en) * | 2004-03-16 | 2007-08-07 | Microsoft Corporation | Systems and methods for improved spell checking |
US7725414B2 (en) * | 2004-03-16 | 2010-05-25 | Buzzmetrics, Ltd An Israel Corporation | Method for developing a classifier for classifying communications |
US20050234953A1 (en) * | 2004-04-15 | 2005-10-20 | Microsoft Corporation | Verifying relevance between keywords and Web site contents |
US7529731B2 (en) * | 2004-06-29 | 2009-05-05 | Xerox Corporation | Automatic discovery of classification related to a category using an indexed document collection |
US7440944B2 (en) * | 2004-09-24 | 2008-10-21 | Overture Services, Inc. | Method and apparatus for efficient training of support vector machines |
US20060074908A1 (en) * | 2004-09-24 | 2006-04-06 | Selvaraj Sathiya K | Method and apparatus for efficient training of support vector machines |
US20060069678A1 (en) * | 2004-09-30 | 2006-03-30 | Wu Chou | Method and apparatus for text classification using minimum classification error to train generalized linear classifier |
US20060085405A1 (en) * | 2004-10-18 | 2006-04-20 | Fu-Chiang Hsu | Method for analyzing and classifying electronic document |
US20060142993A1 (en) * | 2004-12-28 | 2006-06-29 | Sony Corporation | System and method for utilizing distance measures to perform text classification |
US20060178869A1 (en) * | 2005-02-10 | 2006-08-10 | Microsoft Corporation | Classification filter for processing data for creating a language model |
US20060212423A1 (en) * | 2005-03-16 | 2006-09-21 | Rosie Jones | System and method for biasing search results based on topic familiarity |
US20080249764A1 (en) * | 2007-03-01 | 2008-10-09 | Microsoft Corporation | Smart Sentiment Classifier for Product Reviews |
US20080243479A1 (en) * | 2007-04-02 | 2008-10-02 | University Of Washington | Open information extraction from the web |
US20090055381A1 (en) * | 2007-08-23 | 2009-02-26 | Google Inc. | Domain Dictionary Creation |
US20100094875A1 (en) * | 2008-08-11 | 2010-04-15 | Collective Media, Inc. | Method and system for classifying text |
Non-Patent Citations (8)
Title |
---|
Guthrie, David, et al. "A closer look at skip-gram modelling." Proceedings of the 5th international Conference on Language Resources and Evaluation (LREC-2006). 2006. * |
Jarvelin et al. "s-grams: Defining generalized n-grams for information retrieval", Information Processing and Management 43 (2007) 1005-1019. * |
Lewis, et al. "Training algorithms for linear text classifier", Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, 1996. * |
Mann et al. ("Simple, Robust, Scalable Semi-Supervised learning via Expectation Regularization"), Proc. 24th International Conference on Machine Learning, 2007. * |
Matsumoto et al. ("Sentiment Classification Using Word Sub-sequences and Dependency Sub-trees"), PAKDD 2005, LNAI 3518, pp. 301-311, Springer-Verlag Berlin Heidelberg 2005. * |
McCallum et al. ("Employing EM and Pool-based active learning for text classification"), Proceedings of ICML-98, 15th International Conference on Machine Learning, San Francisco, 1998. * |
Peng et al. "Combining Naive Bayes and n-Gram Language Models for Text Classification", ECIR 2003, LNCS 2633, pp. 335-350, Springer-Verlag Berlin Heidelberg, 2003. * |
Siu et al. "Variable N-Grams and Extensions for Conversational Speech Language Modeling", IEEE TRANSACTIONS ON SPEECH AND AUDIO PROCESSING, VOL. 8, NO. 1, JANUARY 2000. * |
Cited By (93)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10445678B2 (en) | 2006-05-07 | 2019-10-15 | Varcode Ltd. | System and method for improved quality management in a product logistic chain |
US10037507B2 (en) | 2006-05-07 | 2018-07-31 | Varcode Ltd. | System and method for improved quality management in a product logistic chain |
US10726375B2 (en) | 2006-05-07 | 2020-07-28 | Varcode Ltd. | System and method for improved quality management in a product logistic chain |
US9646277B2 (en) | 2006-05-07 | 2017-05-09 | Varcode Ltd. | System and method for improved quality management in a product logistic chain |
US10504060B2 (en) | 2007-05-06 | 2019-12-10 | Varcode Ltd. | System and method for quality management utilizing barcode indicators |
US10776752B2 (en) | 2007-05-06 | 2020-09-15 | Varcode Ltd. | System and method for quality management utilizing barcode indicators |
US10176451B2 (en) | 2007-05-06 | 2019-01-08 | Varcode Ltd. | System and method for quality management utilizing barcode indicators |
US20100286979A1 (en) * | 2007-08-01 | 2010-11-11 | Ginger Software, Inc. | Automatic context sensitive language correction and enhancement using an internet corpus |
US8914278B2 (en) * | 2007-08-01 | 2014-12-16 | Ginger Software, Inc. | Automatic context sensitive language correction and enhancement using an internet corpus |
US9026432B2 (en) | 2007-08-01 | 2015-05-05 | Ginger Software, Inc. | Automatic context sensitive language generation, correction and enhancement using an internet corpus |
US9836678B2 (en) | 2007-11-14 | 2017-12-05 | Varcode Ltd. | System and method for quality management utilizing barcode indicators |
US10262251B2 (en) | 2007-11-14 | 2019-04-16 | Varcode Ltd. | System and method for quality management utilizing barcode indicators |
US9135544B2 (en) | 2007-11-14 | 2015-09-15 | Varcode Ltd. | System and method for quality management utilizing barcode indicators |
US9558439B2 (en) | 2007-11-14 | 2017-01-31 | Varcode Ltd. | System and method for quality management utilizing barcode indicators |
US10719749B2 (en) | 2007-11-14 | 2020-07-21 | Varcode Ltd. | System and method for quality management utilizing barcode indicators |
US9317794B2 (en) | 2008-06-10 | 2016-04-19 | Varcode Ltd. | Barcoded indicators for quality management |
US10417543B2 (en) | 2008-06-10 | 2019-09-17 | Varcode Ltd. | Barcoded indicators for quality management |
US10776680B2 (en) | 2008-06-10 | 2020-09-15 | Varcode Ltd. | System and method for quality management utilizing barcode indicators |
US9626610B2 (en) | 2008-06-10 | 2017-04-18 | Varcode Ltd. | System and method for quality management utilizing barcode indicators |
US9384435B2 (en) | 2008-06-10 | 2016-07-05 | Varcode Ltd. | Barcoded indicators for quality management |
US9646237B2 (en) | 2008-06-10 | 2017-05-09 | Varcode Ltd. | Barcoded indicators for quality management |
US10303992B2 (en) | 2008-06-10 | 2019-05-28 | Varcode Ltd. | System and method for quality management utilizing barcode indicators |
US9710743B2 (en) | 2008-06-10 | 2017-07-18 | Varcode Ltd. | Barcoded indicators for quality management |
US10885414B2 (en) | 2008-06-10 | 2021-01-05 | Varcode Ltd. | Barcoded indicators for quality management |
US10789520B2 (en) | 2008-06-10 | 2020-09-29 | Varcode Ltd. | Barcoded indicators for quality management |
US11704526B2 (en) | 2008-06-10 | 2023-07-18 | Varcode Ltd. | Barcoded indicators for quality management |
US11238323B2 (en) | 2008-06-10 | 2022-02-01 | Varcode Ltd. | System and method for quality management utilizing barcode indicators |
US11341387B2 (en) | 2008-06-10 | 2022-05-24 | Varcode Ltd. | Barcoded indicators for quality management |
US10572785B2 (en) | 2008-06-10 | 2020-02-25 | Varcode Ltd. | Barcoded indicators for quality management |
US9996783B2 (en) | 2008-06-10 | 2018-06-12 | Varcode Ltd. | System and method for quality management utilizing barcode indicators |
US11449724B2 (en) | 2008-06-10 | 2022-09-20 | Varcode Ltd. | System and method for quality management utilizing barcode indicators |
US10049314B2 (en) | 2008-06-10 | 2018-08-14 | Varcode Ltd. | Barcoded indicators for quality management |
US10089566B2 (en) | 2008-06-10 | 2018-10-02 | Varcode Ltd. | Barcoded indicators for quality management |
US9015036B2 (en) | 2010-02-01 | 2015-04-21 | Ginger Software, Inc. | Automatic context sensitive language correction using an internet corpus particularly for small keyboard devices |
US11409751B2 (en) | 2010-09-28 | 2022-08-09 | International Business Machines Corporation | Providing answers to questions using hypothesis pruning |
US20130018876A1 (en) * | 2010-09-28 | 2013-01-17 | International Business Machines Corporation | Providing answers to questions using hypothesis pruning |
US10216804B2 (en) | 2010-09-28 | 2019-02-26 | International Business Machines Corporation | Providing answers to questions using hypothesis pruning |
US9317586B2 (en) | 2010-09-28 | 2016-04-19 | International Business Machines Corporation | Providing answers to questions using hypothesis pruning |
US9323831B2 (en) * | 2010-09-28 | 2016-04-26 | International Business Machines Corporation | Providing answers to questions using hypothesis pruning |
US20140180692A1 (en) * | 2011-02-28 | 2014-06-26 | Nuance Communications, Inc. | Intent mining via analysis of utterances |
US20130317804A1 (en) * | 2012-05-24 | 2013-11-28 | John R. Hershey | Method of Text Classification Using Discriminative Topic Transformation |
US9069798B2 (en) * | 2012-05-24 | 2015-06-30 | Mitsubishi Electric Research Laboratories, Inc. | Method of text classification using discriminative topic transformation |
US9400952B2 (en) | 2012-10-22 | 2016-07-26 | Varcode Ltd. | Tamper-proof quality management barcode indicators |
US10552719B2 (en) | 2012-10-22 | 2020-02-04 | Varcode Ltd. | Tamper-proof quality management barcode indicators |
US9633296B2 (en) | 2012-10-22 | 2017-04-25 | Varcode Ltd. | Tamper-proof quality management barcode indicators |
US10242302B2 (en) | 2012-10-22 | 2019-03-26 | Varcode Ltd. | Tamper-proof quality management barcode indicators |
US10839276B2 (en) | 2012-10-22 | 2020-11-17 | Varcode Ltd. | Tamper-proof quality management barcode indicators |
US9965712B2 (en) | 2012-10-22 | 2018-05-08 | Varcode Ltd. | Tamper-proof quality management barcode indicators |
US9842103B1 (en) * | 2012-12-19 | 2017-12-12 | Amazon Technologies, Inc. | Machined book detection |
US9372850B1 (en) * | 2012-12-19 | 2016-06-21 | Amazon Technologies, Inc. | Machined book detection |
US9767157B2 (en) * | 2013-03-15 | 2017-09-19 | Google Inc. | Predicting site quality |
US20140280011A1 (en) * | 2013-03-15 | 2014-09-18 | Google Inc. | Predicting Site Quality |
US10437902B1 (en) * | 2013-04-17 | 2019-10-08 | A9.Com, Inc. | Extracting product references from unstructured text |
US11781922B2 (en) | 2015-05-18 | 2023-10-10 | Varcode Ltd. | Thermochromic ink indicia for activatable quality labels |
US11060924B2 (en) | 2015-05-18 | 2021-07-13 | Varcode Ltd. | Thermochromic ink indicia for activatable quality labels |
US11614370B2 (en) | 2015-07-07 | 2023-03-28 | Varcode Ltd. | Electronic quality indicator |
US11009406B2 (en) | 2015-07-07 | 2021-05-18 | Varcode Ltd. | Electronic quality indicator |
US11920985B2 (en) | 2015-07-07 | 2024-03-05 | Varcode Ltd. | Electronic quality indicator |
US10697837B2 (en) | 2015-07-07 | 2020-06-30 | Varcode Ltd. | Electronic quality indicator |
AU2017228580B2 (en) * | 2015-08-28 | 2019-08-08 | Accenture Global Services Limited | Automated functional diagram generation |
AU2017228575B2 (en) * | 2015-08-28 | 2019-06-20 | Accenture Global Services Limited | Automated term extraction |
AU2016219728A1 (en) * | 2015-08-28 | 2017-03-16 | Accenture Global Services Limited | Automated term extraction |
US10534861B2 (en) | 2015-08-28 | 2020-01-14 | Accenture Global Services Limited | Automated term extraction |
US10152474B2 (en) | 2015-08-28 | 2018-12-11 | Accenture Global Services Limited | Automated term extraction |
US10198430B2 (en) | 2015-08-28 | 2019-02-05 | Accenture Global Services Limited | Automated functional diagram generation |
US10083237B2 (en) | 2015-08-31 | 2018-09-25 | Google Llc | Protecting users from inappropriate sensitive or offensive search results |
US10242323B2 (en) * | 2015-09-17 | 2019-03-26 | Chatterbox Labs Limited | Customisable method of data filtering |
US20170228361A1 (en) * | 2016-02-10 | 2017-08-10 | Yong Zhang | Electronic message information retrieval system |
US10509860B2 (en) * | 2016-02-10 | 2019-12-17 | Weber State University Research Foundation | Electronic message information retrieval system |
WO2017139539A3 (en) * | 2016-02-10 | 2017-10-19 | Yong Zhang | Electronic message information retrieval system |
CN108628873A (en) * | 2017-03-17 | 2018-10-09 | 腾讯科技（北京）有限公司 | A kind of file classification method, device and equipment |
US10454776B2 (en) * | 2017-04-20 | 2019-10-22 | Cisco Technologies, Inc. | Dynamic computer network classification using machine learning |
US11900017B2 (en) | 2017-05-18 | 2024-02-13 | Peloton Interactive, Inc. | Optimizing display engagement in action automation |
US11397558B2 (en) | 2017-05-18 | 2022-07-26 | Peloton Interactive, Inc. | Optimizing display engagement in action automation |
US10963495B2 (en) * | 2017-12-29 | 2021-03-30 | Aiqudo, Inc. | Automated discourse phrase discovery for generating an improved language model of a digital assistant |
US10963499B2 (en) | 2017-12-29 | 2021-03-30 | Aiqudo, Inc. | Generating command-specific language model discourses for digital assistant interpretation |
US20190205325A1 (en) * | 2017-12-29 | 2019-07-04 | Aiqudo, Inc. | Automated Discourse Phrase Discovery for Generating an Improved Language Model of a Digital Assistant |
US10929613B2 (en) | 2017-12-29 | 2021-02-23 | Aiqudo, Inc. | Automated document cluster merging for topic-based digital assistant interpretation |
US11017180B2 (en) * | 2018-04-18 | 2021-05-25 | HelpShift, Inc. | System and methods for processing and interpreting text messages |
US10957432B2 (en) * | 2018-04-20 | 2021-03-23 | International Business Machines Corporation | Human resource selection based on readability of unstructured text within an individual case safety report (ICSR) and confidence of the ICSR |
US10957431B2 (en) | 2018-04-20 | 2021-03-23 | International Business Machines Corporation | Human resource selection based on readability of unstructured text within an individual case safety report (ICSR) and confidence of the ICSR |
US10467339B1 (en) * | 2018-06-28 | 2019-11-05 | Sap Se | Using machine learning and natural language processing to replace gender biased words within free-form text |
CN109189932A (en) * | 2018-09-06 | 2019-01-11 | 北京京东尚科信息技术有限公司 | File classification method and device, computer readable storage medium |
US10861439B2 (en) * | 2018-10-22 | 2020-12-08 | Ca, Inc. | Machine learning model for identifying offensive, computer-generated natural-language text or speech |
US20200125639A1 (en) * | 2018-10-22 | 2020-04-23 | Ca, Inc. | Generating training data from a machine learning model to identify offensive language |
US20200126533A1 (en) * | 2018-10-22 | 2020-04-23 | Ca, Inc. | Machine learning model for identifying offensive, computer-generated natural-language text or speech |
US20210034784A1 (en) * | 2019-08-01 | 2021-02-04 | International Business Machines Corporation | Detangling virtual world and real world portions of a set of articles |
US11556737B2 (en) * | 2019-12-04 | 2023-01-17 | At&T Intellectual Property I, L.P. | System, method, and platform for auto machine learning via optimal hybrid AI formulation from crowd |
US11842258B2 (en) | 2019-12-04 | 2023-12-12 | At&T Intellectual Property I, L.P. | System, method, and platform for auto machine learning via optimal hybrid AI formulation from crowd |
US11361165B2 (en) * | 2020-03-27 | 2022-06-14 | The Clorox Company | Methods and systems for topic detection in natural language communications |
CN111552809A (en) * | 2020-04-22 | 2020-08-18 | 中国电力科学研究院有限公司 | Power grid field phrase identification and classification method and system based on Baidu encyclopedia |
CN112016323A (en) * | 2020-08-28 | 2020-12-01 | 中国科学技术大学 | Automatic extraction method of technical phrases in patent |
CN115114915A (en) * | 2022-05-25 | 2022-09-27 | 腾讯科技（深圳）有限公司 | Phrase recognition method, apparatus, device and medium |
Also Published As
Publication number | Publication date |
---|---|
US8868402B2 (en) | 2014-10-21 |
US9317564B1 (en) | 2016-04-19 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US9317564B1 (en) | Construction of text classifiers | |
Sedhai et al. | Semi-supervised spam detection in Twitter stream | |
Biagioli et al. | Automatic semantics extraction in law documents | |
KR101737887B1 (en) | Apparatus and Method for Topic Category Classification of Social Media Text based on Cross-Media Analysis | |
AU2017355420B2 (en) | Systems and methods for event detection and clustering | |
US20130159277A1 (en) | Target based indexing of micro-blog content | |
US9361362B1 (en) | Synonym generation using online decompounding and transitivity | |
US20210209416A1 (en) | Method and apparatus for generating event theme | |
CN104504150A (en) | News public opinion monitoring system | |
US20090006391A1 (en) | Automatic categorization of document through tagging | |
US8983826B2 (en) | Method and system for extracting shadow entities from emails | |
US20110093459A1 (en) | Incorporating Recency in Network Search Using Machine Learning | |
US10019492B2 (en) | Stop word identification method and apparatus | |
CN109271514B (en) | Generation method, classification method, device and storage medium of short text classification model | |
Liliana et al. | Indonesian news classification using support vector machine | |
US20150006563A1 (en) | Transitive Synonym Creation | |
CN113780007A (en) | Corpus screening method, intention recognition model optimization method, equipment and storage medium | |
CN104361037A (en) | Microblog classifying method and device | |
KR102376489B1 (en) | Text document cluster and topic generation apparatus and method thereof | |
CN111754208A (en) | Automatic screening method for recruitment resumes | |
US9355099B2 (en) | System and method for detecting explicit multimedia content | |
US11595337B2 (en) | System and method for electronic chat production | |
CN111324705A (en) | System and method for adaptively adjusting related search terms | |
US20190236471A1 (en) | Identifying Intent in Dialog Data Through Variant Assessment | |
Patel et al. | Mobile sms classification |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:KOROLEV, DMITRY;MAENNEL, HARTMUT;HEILER, MATTHIAS;AND OTHERS;REEL/FRAME:023737/0458Effective date: 20091222 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044277/0001Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551)Year of fee payment: 4 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 8 |