CN110797027B - Multi-recognizer speech recognition - Google Patents
Multi-recognizer speech recognition Download PDFInfo
- Publication number
- CN110797027B CN110797027B CN201910931218.1A CN201910931218A CN110797027B CN 110797027 B CN110797027 B CN 110797027B CN 201910931218 A CN201910931218 A CN 201910931218A CN 110797027 B CN110797027 B CN 110797027B
- Authority
- CN
- China
- Prior art keywords
- transcription
- speech recognizer
- expanded
- speech
- limited
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 238000013518 transcription Methods 0.000 claims abstract description 224
- 230000035897 transcription Effects 0.000 claims abstract description 224
- 238000000034 method Methods 0.000 claims abstract description 45
- 230000009471 action Effects 0.000 claims description 43
- 238000002864 sequence alignment Methods 0.000 claims description 3
- 230000015654 memory Effects 0.000 description 33
- 230000008569 process Effects 0.000 description 26
- 238000004891 communication Methods 0.000 description 17
- 238000010586 diagram Methods 0.000 description 10
- 238000004590 computer program Methods 0.000 description 8
- 238000012549 training Methods 0.000 description 7
- 230000004044 response Effects 0.000 description 6
- 230000001413 cellular effect Effects 0.000 description 4
- 230000006870 function Effects 0.000 description 4
- 230000008439 repair process Effects 0.000 description 3
- 238000005516 engineering process Methods 0.000 description 2
- 230000003993 interaction Effects 0.000 description 2
- 238000004519 manufacturing process Methods 0.000 description 2
- 230000003287 optical effect Effects 0.000 description 2
- 238000012545 processing Methods 0.000 description 2
- 238000012360 testing method Methods 0.000 description 2
- 241000282324 Felis Species 0.000 description 1
- 238000004458 analytical method Methods 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 238000003032 molecular docking Methods 0.000 description 1
- 230000006855 networking Effects 0.000 description 1
- 238000009428 plumbing Methods 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 239000013589 supplement Substances 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/26—Speech to text systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/28—Constructional details of speech recognition systems
- G10L15/32—Multiple recognisers used in sequence or in parallel; Score combination systems therefor, e.g. voting systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/01—Assessment or evaluation of speech recognition systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/183—Speech classification or search using natural language modelling using context dependencies, e.g. language models
- G10L15/19—Grammatical context, e.g. disambiguation of the recognition hypotheses based on word sequence rules
- G10L15/197—Probabilistic grammars, e.g. word n-grams
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/28—Constructional details of speech recognition systems
- G10L15/30—Distributed recognition, e.g. in client-server systems, for mobile phones or network applications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M2250/00—Details of telephonic subscriber devices
- H04M2250/74—Details of telephonic subscriber devices with voice recognition means
Abstract
The application relates to multi-recognizer speech recognition. The subject matter of this specification can be embodied in, among other things, methods that include receiving audio data corresponding to a utterance, obtaining a first transcription of the utterance generated using a limited speech recognizer. The limited speech recognizer includes a speech recognizer that includes a language model trained by limited speech recognition vocabulary that includes one or more terms from a speech command grammar, but includes fewer than all terms from an expanded grammar. A second transcription of the utterance generated using the expanded speech recognizer is obtained. The expanded speech recognizer includes a speech recognizer that includes a language model trained by expanded speech recognition vocabulary that includes all terms of an expanded grammar. The utterance is classified based at least on a portion of the first transcription or the second transcription.
Description
Description of the division
The application belongs to a divisional application of Chinese application patent application No.201480027534.1 with the application date of 2014, 4 and 18.
Technical Field
The present disclosure relates generally to speech recognition.
Background
Speech recognition includes a process for converting spoken words into text. In general, speech recognition systems map spoken utterances into a series of computer-readable voices and compare those voices to known speech patterns associated with words. For example, a microphone may accept an analog signal that is converted to a digital form that is thereafter divided into smaller segments. The digital fields may be compared to the elements of the spoken language. Based on this comparison and analysis of the environment in which those voices were uttered, the system is able to recognize the voices.
A typical speech recognition system may include an acoustic model, a language model, and a dictionary. Briefly, an acoustic model includes digital representations of individual voices that can be combined to generate a set of words, phrases, etc. The language model assigns probabilities that a series of words will appear together in a particular sentence or phrase. The dictionary converts the phonetic sequences into words that are understandable to the language model.
Disclosure of Invention
In general, this document describes systems and techniques for performing speech recognition. Typically, the user's utterance is transcribed by two or more speech recognizers. Each recognizer is tuned to a different dictionary. For example, one recognizer may be tuned to recognize words from an extended (e.g., multi-purpose) dictionary, while another is tuned to a subset of words (e.g., command keywords) and/or a specialized dictionary such as an inherent name stored in a contact list and possibly not available in the extended dictionary. The transcriptions of the identifiers may be aligned to provide a transcription that includes selected elements in one or more identifiers as the transcription to the user.
Aspects of the subject matter described in this specification can be embodied as methods, systems, and computer-readable media storing executable instructions that perform operations comprising: receiving audio data corresponding to the utterance; obtaining a first transcription of a sound production generated using a limited speech recognizer, wherein the limited speech recognizer comprises a speech recognizer that includes a language model trained by limited speech recognition vocabulary that includes one or more terms from a speech command grammar but includes fewer than all terms of an expanded grammar; obtaining a second transcription of the utterance generated using an expanded speech recognizer, wherein the expanded speech recognizer includes a speech recognizer that includes a language model trained by expanded speech recognition vocabulary that includes all terms of an expanded grammar; and classifying the utterance based on at least a portion of the first transcription or the second transcription.
Implementations may include some, all, or none of the following features. The first and second transcriptions of the utterance may be aligned to produce an aligned transcription. The utterance may be classified as one of a voice command or a voice query, and the voice command is generated and the voice command is initiated using at least a portion of the first transcription and at least a portion of the second transcription in response to classifying the utterance as the voice command, and the voice query is generated and the voice query is initiated using at least a portion of the first transcription and at least a portion of the second transcription in response to classifying the utterance as the voice query. The limited speech recognizer may be configured to recognize one or more of a set of placeholder terms, a set of voice command terms, and a set of contact names from a contact list. The expanded speech recognizer may be configured to recognize one or more of a set of general grammar terms, a set of placeholder terms, a set of native names, and a set of voice command terms. The expanded speech recognizer may not be configured to recognize a set of contact names from a contact list. The operations of at least one of the limited speech recognizer and the extended speech recognizer may be performed at the mobile device. The operations of at least one of the limited speech recognizer and the expanded speech recognizer may be performed at a server computer device.
The systems and techniques described here can provide one or more of the following advantages. First, the system may provide for recognition of items and names that are not known to the generic speech recognition system. Second, the system may improve confidentiality of user information. Third, the system may provide improved recognition of spoken phrases including words found in the user dictionary and the universal dictionary.
The details of one or more implementations are set forth in the accompanying drawings and the description below. Other features and advantages will be apparent from the description and drawings, and from the claims.
Drawings
FIG. 1 shows a schematic diagram of an example of a system for performing speech recognition with multiple speech recognizers.
Fig. 2A-2F illustrate conceptual examples of utterances transcribed by a plurality of speech recognizers.
FIG. 3 is a flow chart of an exemplary process for performing speech recognition for a plurality of speech recognizers.
FIG. 4 is a block diagram of an exemplary computing device that may be used to implement multiple speech recognizer speech recognition.
Detailed Description
Fig. 1 shows a schematic diagram of an example of a system 100 for performing speech recognition with multiple speech recognizers. In the example of fig. 1, a user 102 issues a command to a mobile device 104. In this example, the mobile device 104 is a cellular telephone (also referred to as a smart phone) with advanced computing capabilities.
The mobile device 104 receives input audio, such as speech, provided by the user 102 and provides the audio to the limited speech recognizer 110 and the expanded speech recognizer 120 over a network 106, such as the internet or a cellular data network. The limited speech recognizer 110 and the expanded speech recognizer 120 perform speech-to-text transcription on the utterances of the user 102. In this example, mobile device 104 may include an application ("APP") that receives input audio. The APP may have any suitable functionality, e.g. it may be a search APP, a messaging APP, an email APP, etc. In this regard, APP is used as an example in this case. However, all or a portion of the functions of the APP may be part of another program downloaded to the mobile device 104, part of another program provisioned on the mobile device 104, part of the operating system of the mobile device 104, or part of a service available to the mobile device 104.
Grammar library 130 includes at least some words and grammars from one or more languages. Grammar library 130 includes an extended grammar set 132, such as all or a subset of the words and grammars contained within grammar library 130. Within the expanded grammar set 132 are subsets of items such as a set of placeholder items 134, a set of voice action items 136, and a set of native names 138. In some implementations, the set of voice action items 136 may include a set of known words and/or grammars that are related to the command. For example, the voice action items may include words such as "call", "text", "navigate", "email". To ". Theme". Message ". The user may use other items with a set of known commands (e.g.," set alarm clock at six pm "," send email to Hugh Briss, theme ' new phone ', message ' i'm want to show you my new phone ', period ").
In some implementations, the set of native names 138 may include names of commonly used people, such as "Bob", "Tiffany", "Smith", "Jones", "Wolfgang Amadeus Mozart", "Laurentian Abyss", "Walter Reed Army Medical Center".
In some implementations, the collection of placeholder terms 134 may include portions of speech that may be used as "connective" words, such as prepositions, conjunctions, and exclamation. In some implementations, the collection of placeholder terms 134 may include words that the speech recognizers 110 and 120 interpret as punctuation marks such as "period", "question mark", "exclamation mark", "hyphen", "dot", "reverse slash". In some implementations, the collection of placeholder items 134 may include strings that are well known to represent one or more words in a transcription. For example, the placeholder term "< target >" may be used as a placeholder for a utterance that requires further transcription in the context of "navigate to < target >". Other examples of placeholder items may include "< subject >", "< recipient >", "< message >", "< location >", "< song >", "< artist >", "< album >", "< unknown >", "< no identity >", or any other suitable human or machine-readable collection of characters that may be used to represent a word or phrase.
The expanded speech recognizer 120 includes an expanded language model 122. The extended language model 122 is a language model including relative extended grammars trained by the language model training engine 140. For example, the language model training engine 140 may access the expanded grammar 132 to train the expanded language model 122 based on terms and some or all of the grammars contained in the expanded grammar 132 (e.g., based on placeholder terms 134, voice action terms 136, native names 138).
The finite language recognizer 110 includes a finite language model 112. The limited language model 112 is a language model trained by the language model training engine 140 that includes a relatively limited subset of the expanded grammar 132 and a set of user contact names 139. The limited language model 112 is trained using placeholder terms 134 and voice action terms 136. For example, the limited language model 112 may be trained to recognize a collection of items related to voice commands such as "telephone", "text", "search", "navigation". In some implementations, the limited language model 112 may be trained on a set of user names 139 to provide identification of names in a personal contact list. For example, the user 102 may store the contact name locally on the device 104 instead of on a server. In such examples, the device 104 may operate the limited identifier to perform identifying names from the user's private contact list without sharing the contact list outside of the device 104. In another example, the user 102 may store a contact name on the device 104, where the contact name is unusual (e.g., the user's non-english name in english expanded grammar) or the name uses an item from expanded grammar 132 (e.g., "Tiger Woods" may be the name of a golf club and not a forest of large felines, "Redd Foxx" may be the name of a comedy actor and not a breed of dog). In some implementations, the limited language model may include words from a user dictionary or private contact list selected by the user 102 to remain unavailable to resources accessible through the network 106. For example, the user 102 may include words that are commonly used by the user 102 but may not be included in the expanded grammar 130, such as foreign words, industry terms, unusual place names, nicknames.
In some implementations, the limited speech recognizer 110 and/or the expanded speech recognizer 120 may reside on the mobile device 104. In some implementations, the limited speech recognizer 110 and/or the expanded speech recognizer 120 may reside on one or more servers remote from the mobile device 104. For example, limited speech recognizer 110 may be running locally on mobile device 104, e.g., to expedite recognition of common commands and/or to provide recognition of names in a local contact list, while extended speech recognizer 120 may be running on a remote server, e.g., to provide access to speech recognition processing that may also be computing, storing, or otherwise compressing data for execution on mobile device 104 and/or to perform speech recognition through a general dictionary or multiple languages. In another example, limited speech recognizer 110 may be run on a remote server to enhance the recognition capabilities of expanded speech recognizer 120 by recognizing names and words from a private contact list and/or a user dictionary that user 102 has selected to share with the server. In yet another example, the device 104 already has sufficient computing power to host the limited speech recognizer 110 locally and extend the speech recognizer to provide substantially full multi-recognizer capability in an offline mode, for example, when the network 106 is not available or not in need of use.
The limited speech recognizer 110 and the expanded speech recognizer 120 process utterances provided by the user 102 within the context and capabilities of their respective language models 112, 122 to form corresponding transcriptions of the utterances. For example, the device 104 may store a contact with a name "Nate Godbout" (pronouncing "good bo"). The user can say "send text to Nate Godbout: 'your package just arrived'. The expanded speech recognizer 120 may recognize the generic grammar term of the utterance rather than the unusual name relatively well, e.g., may transcribe the utterance as "send test to the index good bo: your package just arrived. The limited speech recognizer 110 may recognize contact names and/or command keywords relatively better than general grammar terms, for example, may transcribe the utterance into "send text < > < > < >" to Nate Godbout, where the string "< >" represents one or more utterances that the limited recognizer 110 recognizes as words but cannot transcribe with sufficient confidence within the context of the limited language model 112.
The transcriptions determined by the limited speech recognizer 110 and the expanded speech recognizer 120 are provided to a transcription aligner 150. The transcription aligner 150 processes the transcription to determine the grammatical alignment of the transcription. For example, the transcription aligner 150 may perform pairwise alignment, sequence alignment, or imprecise matching for comparing two transcriptions while allowing for some mismatch between the two. For example, the transcription aligner 150 may recognize that the words "send" and "to" are present in both transcriptions and that the two are separated by a different word. In this example, transcription aligner 150 may determine that the transcription should be aligned at the words "send" and/or "to".
The transcription aligner 150 provides the aligned transcription to the transcription disambiguator 160. The transcription disambiguator 160 analyzes the aligned transcriptions to determine at least the type of utterance provided by the user 102 (e.g., a voice action command or a voice search command) and what transcribed words are to be used to execute the command. For example, in terms of determining what type of command to issue, the transcript disambiguator 160 may analyze the aligned transcripts "send text < > < > < > to Nate Godbout and" send test your package just to "to the index good bo" and recognize that the limited speech recognizer 110 that it was tuned to recognize a speech action item "send text" at a grammatical position within the utterance that further increased the confidence that the utterance was in fact a speech action command. In other examples, the transcription disambiguator 160 may determine a relatively high level of confidence for the transcription provided by the expanded speech recognizer and determine that the utterance is a speech search command.
The transcript disambiguator 160 creates a combined transcript from the limited and expanded transcriptions. For example, words or phrases within each transcription may be associated with confidence scores and/or weights reflecting the likelihood of association of each word or phrase within the context of a voice action or voice query. The transcript disambiguator 160 compares the confidence scores and/or weights of the words or phrases of each corresponding pair of transcriptions to determine a combined transcription that represents the utterance and that is available for performing a voice action or voice query. Examples of disambiguated transcriptions are further discussed in the description of FIGS. 2A-2F.
The transcript disambiguator 160 provides the disambiguated transcript to the voice action engine 170 or the search engine 180, e.g., provides the disambiguated transcript to the voice action engine 170 when identifying the utterance as a voice action and provides the disambiguated transcript to the search engine 180 when identifying the utterance as a voice search. In response to receiving the disambiguated transcription, the speech action engine 170 performs a speech action according to the transcription. For example, the voice action engine 170 may receive a transcription such as "navigate to the nearest post office" and respond by providing the driving directions to the user 102. In response to receiving the disambiguated transcription, the search engine 180 performs a search operation. For example, the search engine 180 may receive a transcription such as "what the phone number of the nearest post office was" and respond by performing a web search and providing the results to the user 102.
2A-2F illustrate conceptual examples of utterances transcribed by multiple speech recognizers and thereafter disambiguated to determine combined transcriptions for use as a voice command or voice search. In some implementations, the system 100 of fig. 1 may perform one or more of the speech recognition, transcription, alignment, disambiguation, speech search, and/or speech actions illustrated in fig. 2A-2F.
Fig. 2A is a conceptual block diagram of an exemplary speech recognition process 200 a. In process 200a, receive "how much is plumber charged? "sound 210a. For example, utterance 210a may be made by user 102 speaking into device 104 of fig. 1. The utterance 210a is transcribed by a limited speech recognizer, such as limited speech recognizer 110, and an extended speech recognizer, such as extended speech recognizer 120. The limited speech recognizer provides limited transcription 220a and the expanded speech recognizer provides expanded transcription 230a.
In the illustrated example, the limited speech recognizer does not recognize any word in the utterance 210a and responds by providing a limited transcription 220a that includes one or more placeholder terms (as illustrated in this example as a string "< >") to represent words or phrases that the limited recognizer cannot transcribe with sufficiently high confidence within the context of the limited language model. In the illustrated example, the expanded speech recognizer attempts to recognize all words in the utterance 210a and responds by providing an expanded transcription 230a that includes substantially all of the recognized words.
The limited transcription 220a and the expanded transcription 230a are aligned, e.g., by the transcription aligner 150, and disambiguated, e.g., by the transcription disambiguator 160, to determine whether the sounder 210a is a voice action or a voice search, and to determine the disambiguated transcription 240a, e.g., "how much a plumber charges". In the illustrated example, the limited speech recognizer cannot provide any transcribed words in the limited transcription 220a that have a sufficiently high confidence, and the transcription disambiguator 160 may use the presence and/or content of the limited transcriber 220a as an indication that the utterance 210a is unlikely to be a speech action and likely to be a speech search.
In the illustrated example, because the limited transcriber 220a does not include any transcribed text, the transcription disambiguator selects substantially all expanded transcriptions 230a for inclusion in the provided disambiguated transcription 240a for use in the phonetic search operation 250 a. For example, the voice search operation 250a may execute a web search query using the disambiguated transcription 240a (e.g., "how much the plumber charges") to provide search results describing the costs the plumber requires for various tasks.
Fig. 2B is a conceptual block diagram of an exemplary speech recognition process 200B. In this and subsequent examples, we assume that the user has a contact with a surname of "Arnie plumber" in his contact list, and that the contact name has access to a limited speech recognizer but not to an extended speech recognizer.
In process 200b, what is the receive of the '"Arnie plumber' charged? "sound 210b. The utterance 210b is transcribed by a limited speech recognizer and an extended speech recognizer. The limited speech recognizer provides limited transcription 220b and the expanded speech recognizer provides expanded transcription 230b.
In the illustrated example, the limited speech recognizer recognizes the contact name "Arnie plumber" from the user's private contact list, among other words/phrases that it cannot transcribe. The limited speech recognizer responds by providing a limited transcription 220b, such as "< > Arnie plumber" < > ", which limited transcription 220b includes the recognized contact name and one or more placeholder terms, as illustrated as a string" < > ". In the illustrated example, the expanded speech recognizer attempts to recognize all words in the utterance 210b and responds by providing an expanded transcription 230b that includes nearly all of the recognized words that it has attempted to recognize. However, in the illustrated example, the expanded speech recognizer may not access the user's private contact and transcribe the contact name "Arnie plumber" to "army plumber", e.g., "how much is the ' army ' plumber charged? ".
The limited transcription 220b and the expanded transcription 230b are aligned and disambiguated to determine whether the sounder 210b is a voice action or a voice search, and the disambiguated transcription 240b is determined. In the illustrated example, the limited speech recognizer is capable of providing transcription of the contact name, but does not provide any placeholder terms that may represent issuing the contact name in the context of a speech action. The transcript disambiguator 160 may use the presence and/or content of the limited transcript 220b as an indication that the utterance 210b is unlikely to be a voice action but is likely to be a voice search (e.g., "how much is the ' Arnie plumber ' charged") that includes the name of one of the user's personal contacts.
In the illustrated example, because limited transcription 220B does not include any placeholder terms representing voice actions, the transcription disambiguator combines the contact name from the transcription of limited transcription 220B with the remainder of expanded transcription 230B to form a disambiguated transcription 240B, which disambiguated transcription 240B is provided for use in voice search operation 250B. For example, the voice search operation 250b may execute a web search query using the disambiguated transcription 240b (e.g., "Arnie plumber's charge.
In some implementations, the transcribed contact name can be considered a placeholder item for representing a contact record related to the contact name, and the contact record itself can be related to one or more data items. For example, the user may have the contact "Arnie plumber" in their contact list, and the user may also have an associated phone number, email address, physical address, website URL, or other information of a contact record with "Arnie plumber". In some implementations, the user may configure the limited speech recognizer to provide one or more data items in a disambiguated context of transcription, e.g., the disambiguated transcriber 240b may include metadata derived from the user's contact information, and the speech search operation may use the metadata to improve the quality of search results provided to the user. For example, there are two "Arnie plumbers" that are searchable on the network, but by including metadata such as URLs, addresses, and/or telephones from contact entries that describe the intended "Arnie plumber," the voice search operation 250b can provide search results that disambiguate the intended "Arnie plumber" from another "Arnie plumber" that may be unknown to the user.
Fig. 2C is a conceptual block diagram of an exemplary speech recognition process 200C. In process 200c, sound 210c of "text 'Arnie plumber" i need to repair a crack' "is received. The utterance 210c is transcribed by a limited speech recognizer and an extended speech recognizer. The limited speech recognizer provides limited transcription 220c and the expanded speech recognizer provides expanded transcription 230c.
In the illustrated example, the limited speech recognizer recognizes the speech command word "text," the contact name "Arnie plumber" from the user's private contact list, and other words/phrases that cannot be transcribed with a sufficiently high confidence. The limited speech recognizer responds by providing a limited transcription 220c (e.g., "< > Arnie plumber" < > ") that includes the recognized contact name and one or more placeholder terms. In the illustrated example, the expanded speech recognizer attempts to recognize all words in the utterance 210c and responds by providing an expanded transcription 230c that includes nearly all of the recognized words that it has attempted to recognize. However, in the illustrated example, the extended speech recognizer may not access the user's private contacts and incorrectly transcribe the command key "text" and the contact name "Arnie plumber" into "Texas" and "army plumber", e.g., "Texas army plumber I need to repair a crack".
The limited transcription 220c and the expanded transcription 230c are aligned and disambiguated to determine whether the enunciator 210c is a voice action or a voice search, and the disambiguated transcription 240c is determined. In the illustrated example, the limited speech recognizer is capable of providing transcription of command keywords, suggesting to the transcription disambiguator that the user wishes to perform a speech action involving information about the recognized contact.
In the illustrated example, because limited transcription 220c includes placeholder terms such as "text" for suggesting a voice action, the transcription disambiguator combines command keywords from limited transcription 220c and the transcribed contact name with the remainder of extended transcription 230c to form a disambiguated transcription 240c, which disambiguated transcription 240c is provided for use in voice search operation 250 c. For example, the voice search operation 250c may utilize the disambiguated transcription 240c (e.g., "text 'Arnie plumber" i need to repair a crack' ") to perform an operation that may cause a text message to be sent to" Arnie plumber.
FIG. 2D is a conceptual block diagram of an exemplary speech recognition process 200D, in which a sound 210D of "plumber Brownsdale Minnesota" is received in process 200D. The utterance 210d is transcribed by a limited speech recognizer and an extended speech recognizer. The limited speech recognizer provides limited transcription 220d and the expanded speech recognizer provides expanded transcription 230d.
In the illustrated example, the limited speech recognizer recognizes an unusual place name "Brownsdale" from the user's personal dictionary, which is a word that is not in the extended language model. Limited speech recognizers provide placeholder terms for other words/phrases that cannot be transcribed with a sufficiently high confidence. The limited speech recognizer responds by providing a limited transcription 220d (e.g., "< > Brownsdale <" ") that includes the recognized personal dictionary word and one or more placeholder terms. In the illustrated example, the expanded speech recognizer attempts to recognize all words in the utterance 210d and responds by providing an expanded transcription 230d that includes nearly all of the recognized words that it has attempted to recognize. However, in the illustrated example, the extended speech recognizer may not access the user's personal dictionary and incorrectly transcribe the word "brown sdale" to "bronzed ale", e.g., "plumber bronzed ale Minnesota.
The limited transcription 220d and the expanded transcription 230d are aligned and disambiguated to determine whether the enunciator 210d is a voice action or a voice search, and the disambiguated transcription 240d is determined. In the illustrated example, the limited speech recognizer is capable of providing transcription of the personal dictionary term "brown dale" rather than command keywords, suggesting to the transcription disambiguator that the user wishes to perform a speech search involving terms from the user's personal dictionary.
In the illustrated example, the transcript disambiguator combines the personal dictionary entry from the limited transcript 220d with the remainder of the expanded transcript 230b to form a disambiguated transcript 240d, the disambiguated transcript 240d being provided for use in a voice search operation 250 d. For example, the voice search operation 250d may perform a web search with a disambiguated transcription 240d (e.g., "plumber brown sdale minnesota") that provides information about the plumbing services available near towns of brown sdale.
Fig. 2E is a conceptual block diagram of an exemplary speech recognition process 200E. In process 200e, a sound 210e of "call A1 plumber" is received. The utterance 210d is transcribed by a limited speech recognizer and an extended speech recognizer. The limited speech recognizer provides limited transcription 220d and the expanded speech recognizer provides expanded transcription 230d.
In the illustrated example, the limited speech recognizer recognizes the speech command word "call" as well as recognized other words/phrases that cannot be transcribed with a sufficiently high confidence. The limited speech recognizer responds by providing a limited transcription 220e (e.g., "< call > < >") that includes the recognized command keywords and one or more placeholder terms. In the illustrated example, the expanded speech recognizer attempts to recognize all words in the utterance 210e and responds by providing an expanded transcription 230e that includes nearly all of the recognized words that it has attempted to recognize. However, in the illustrated example, the expanded speech recognizer wrongly transcribes the command keyword "call" into "cowl", e.g., "cowl A1 plumber".
The limited transcription 220e and the expanded transcription 230e are aligned and disambiguated to determine whether the enunciator 210e is a voice action or a voice search, and the disambiguated transcription 240e is determined. In the illustrated example, the limited speech recognizer is able to provide transcription of command keywords, suggesting to the transcription disambiguator that the user wishes to perform a specified speech action with some words that it cannot transcribe with a sufficiently high confidence.
In the illustrated example, because the limited transcription 220e includes a placeholder term such as "call" suggesting a voice action, the transcription disambiguator combines command keywords from the limited transcription 220e with the remainder of the expanded transcription 230e to form a disambiguated transcription 240e, which disambiguated transcription 240e is provided for use in the voice search operation 250 e. For example, the voice search operation 250e may utilize the disambiguated transcription 240c (e.g., "call A1 plumber") to perform an operation that may cause the device 104 to make a call using a telephone number obtained from a web search for "A1 plumber".
Fig. 2F is a conceptual block diagram of an exemplary speech recognition process 200F. In process 200f, a sound 210f of "call Arnie plumber" is received. The utterance 210f is transcribed by a limited speech recognizer and an extended speech recognizer. The limited speech recognizer provides limited transcription 220f and the expanded speech recognizer provides expanded transcription 230f.
In the illustrated example, the limited speech recognizer recognizes the speech command word "call" and the contact name "Arnie plumber". The limited speech recognizer responds by providing a limited transcription 220f (e.g., "< call > < Arnie plumber >") that includes the recognized command keywords and the contact name. In the illustrated example, the expanded speech recognizer attempts to recognize all words in the utterance 210f and responds by providing an expanded transcription 230f that includes nearly all of the recognized words that it has attempted to recognize. However, in the illustrated example, the expanded speech recognizer wrongly transcribes the command keyword "call" to "cowl" and the contact name "Arnie plumber" to "army of plumber", e.g., "cowl army of plumber".
The limited transcription 220f and the expanded transcription 230f are aligned and disambiguated to determine whether the enunciator 210f is a voice action or a voice search, and the disambiguated transcription 240f is determined. In the illustrated example, the limited speech recognizer is capable of providing transcription of command keywords, suggesting to the transcription disambiguator that the user wishes to perform specified speech actions on the recognized contacts.
In the illustrated example, because the limited transcription 220f includes a placeholder term such as "call" suggesting a voice action, the transcription disambiguator combines command keywords from the limited transcription 220f with the remainder of the expanded transcription 230f to form a disambiguated transcription 240f, which disambiguated transcription 240f is provided for use in the voice search operation 250 f. For example, the voice search operation 250f may perform such operations with the disambiguated transcription 240f (e.g., "call Arnie plumber") that may cause the device 104 to make a telephone call with a telephone number stored as part of the user's private contact information of "Arnie plumber".
FIG. 3 is a flow chart of an exemplary process 300 for performing multi-recognizer language recognition. In some implementations, the process 300 may be performed by the system 100 of fig. 1.
Process 300 begins when audio data corresponding to a sound production is received (310). For example, the user 102 of FIG. 1 may speak one of the utterances 210a-210f of FIG. 2 against the device 104.
A first transcription of sound produced with a limited speech recognizer is obtained (320). The limited speech recognizer includes a speech recognizer that includes a language model trained by limited speech recognition vocabulary that includes one or more words from a speech command grammar, but includes fewer than all words from an expanded grammar. For example, the limited speech recognizer 110 transcribes the utterance with the limited language model 112 and the limited language model 112 is trained by the language model training engine 140 to recognize fewer than the complete set of terms included in the expanded grammar 132.
In some implementations, the limited speech recognizer may be configured to recognize one or more of a set of placeholder terms, a set of voice command words, and a set of contact names from a contact list. For example, language model training engine 140 may utilize a set of placeholder terms 134, a set of voice action terms 136, and a set of user contact names 139 to train limited language model 112.
A second transcription of the utterance generated using the expanded speech recognizer is obtained (320). The expanded speech recognizer includes a speech recognizer that includes a language model trained by expanded speech recognition vocabulary that includes all terms of an expanded grammar. For example, the expanded speech recognizer 120 transcribes the utterance with the expanded language model 122 and the expanded language model 112 is trained by the language model training engine 140 to recognize a set of terms included in the expanded grammar 132 that is relatively larger than the set of expanded grammar terms included in the limited language model 112.
In some implementations, the expanded speech recognizer may be configured to recognize one or more of a set of general grammar terms, a set of placeholder terms, a set of natural names, and a set of voice command words. For example, language model training engine 140 may train extended language model 122 using general grammar 132, a set of placeholder terms 134, a set of voice action terms 136, and a set of native names 139. In some implementations, the expanded speech recognizer may not be configured to recognize a collection of contact names from a contact list. For example, the extended language model 122 may not be accessible or may not be trained to identify names stored by the user 102 as private contacts on the device 104.
In some implementations, the first and second transcriptions of the utterance can be aligned to produce an aligned transcription. For example, the transcription aligner 330 may process limited and expanded transcriptions to determine an alignment between the two so that words identified from one transcription may correspond to their identified counterparts in the other transcription.
The utterance is classified (340) according to at least a portion of the first transcription or the second transcription. In some implementations, the utterance can be classified (340) as one of a voice command or a voice query. In response to classifying the utterance as a voice command, a voice command is generated utilizing at least a portion of the first transcription and at least a portion of the second transcription, and the voice command is initiated (350). For example, in process 200c, transcript disambiguator 160 may determine that utterance 210c is a voice command, combine words from limited transcript 220c and expanded transcript 230c to generate a disambiguated transcript 240c, and initiate a voice action 250c based on the disambiguated transcript 240 c. In response to classifying the utterance as a voice query, a voice query is generated utilizing at least a portion of the first transcription and at least a portion of the second transcription, and the voice query is initiated (360). For example, in process 200b, transcript disambiguator 160 may determine that utterance 210b is a voice query, combine terms from limited transcript 220b and expanded transcript 230b to generate disambiguated transcript 240b, and initiate voice search 250b based on disambiguated transcript 240 b.
In some implementations, the operations of the limited speech recognizer and/or the extended speech recognizer may be performed at the mobile device. For example, limited speech recognizer 110 and/or extended speech recognizer 120 may be performed by mobile device 104. In some implementations, the operations of the limited speech recognizer and/or the extended speech recognizer may be performed at a server computer device. For example, limited speech recognizer 110 and/or extended speech recognizer 120 may be implemented by one or more server computers that may access mobile device 104 through network 106.
In some implementations, the operations of the limited speech recognizer may be performed at the mobile device and the operations of the extended speech recognizer may be performed at the server computer device. For example, limited speech recognizer 110 may operate on mobile device 104 to provide recognition of contact names, private dictionary entries, and/or limited offline speech recognition functions, while extended speech recognizer 120 may operate at a server device accessible to mobile device 104 over network 106 to provide speech recognition functions that may also be computationally intensive to perform by mobile device 104 in time.
FIG. 4 is a block diagram of a computing device 400, 450 that may be used to implement the systems and methods described in this document, either as a client or as a server or servers. Computing device 400 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers. Computing device 450 is intended to represent various forms of mobile devices, such as personal digital assistants, cellular telephones, smartphones, and other similar computing devices. The components shown herein, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the invention described and/or claimed in this document.
Computing device 400 includes a processor 402, memory 404, storage 406, a high-speed interface 408 coupled to memory 404 and high-speed expansion ports 410, and a low-speed interface 412 coupled to low-speed bus 414 and storage 406. Each of the components 402, 404, 406, 408, 410, 412 are interconnected using various buses, and may be mounted on a common motherboard or in other manners as appropriate. The processor 402 may process instructions executing within the computing device 400, including instructions stored in the memory 404 or on the storage device 406 to display graphical information of a GUI on an external input/output device, such as a display 416 coupled to the high-speed interface 408. In other implementations, multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and memory types. In addition, multiple computing devices 400 may be connected to each device (e.g., as a server cluster, a group of blade servers, or a multiprocessor system) that provides portions of the necessary operations.
Memory 404 stores information within computing device 400. In one implementation, the memory 404 is a computer-readable medium. In one implementation, the memory 404 is a volatile memory unit. In another implementation, the memory 404 is a non-volatile memory unit.
Storage device 406 is capable of providing mass storage for computing device 400. In one implementation, the storage device 406 is a computer-readable medium. In various different implementations, storage device 406 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including storage area networks or other configured devices. In one implementation, a computer program product is tangibly embodied as an information carrier. The computer program product contains instructions that, when executed, perform one or more methods such as those described above. The information carrier is a computer-or machine-readable medium, such as the memory 404, the storage device 406, or memory on processor 402.
The high speed controller 408 manages bandwidth-intensive operations of the computing device 400 while the low speed controller 412 manages lower bandwidth-intensive operations. This allocation of tasks is merely exemplary. In one implementation, the high-speed controller 408 is coupled to the memory 404, the display 416 (e.g., via a graphics processor or accelerator), and the high-speed expansion port 410, which may accept various expansion cards (not shown). In this implementation, low-speed controller 412 is coupled with storage device 406 and low-speed expansion port 414. The low-speed expansion port, which may include various communication ports (e.g., USB, bluetooth, ethernet, wireless ethernet) may be coupled with one or more input/output devices such as a keyboard, a pointing device, a scanner, a networking device such as a switch or router, for example, through a network adapter.
As shown, computing device 400 may be implemented in a number of different forms. For example, it may be implemented as a standard server 420 or multiple times in a group of such servers. May also be implemented as part of a rack server system 424. In addition, it may be implemented in a personal computer such as laptop computer 422. Alternatively, components from computing device 400 may be combined with other components (not shown) in a mobile device, such as device 450. Each of such devices may contain one or more computing devices 400, 450, and an entire system may be made up of multiple computing devices 400, 450 in communication with each other.
Computing device 450 includes a processor 452, memory 464, input/output devices such as a display 454, a communication interface 466, a transceiver 468, and other components. The device 450 may also be provided with a storage device, such as a micro drive or other device, for providing secondary storage. Each of the components 450, 452, 464, 454, 466, 468 are interconnected using various buses, and several of the components may be mounted on a common motherboard or in other manners as appropriate.
The processor 452 may process instructions, including instructions stored in the memory 464, for execution within the computing device 450. The processor may also include separate analog and digital processors. The processor may provide, for example, for collaboration of other components of the device 450, such as control of user interfaces, applications running through the device 450, and wireless communications through the device 450.
The processor 452 may communicate with a user through a control interface 458 and a display interface 456 coupled to a display 454. The display 454 may be, for example, a TFT LCD display or an OLED display or other suitable display technology. The display interface 456 may include suitable circuitry for driving the display 454 to present graphical and other information to a user. The control interface 458 may receive commands from a user and convert them for submission to the processor 452. In addition, an external interface 462 may be provided in communication with processor 452 to enable near area communication of device 450 with other devices. External interface 462 may provide, for example, wired communication (e.g., through a docking process) or wireless communication (e.g., through bluetooth or other such technology).
Computing device 450 stores information within computing device 450. In one implementation, the memory 464 is a computer-readable medium. In one implementation, the memory 464 is a volatile memory unit. In another implementation, the memory 464 is a non-volatile memory unit. Expansion memory 474 may also be provided and connected via expansion interface 472 to device 450, which may include, for example, a SIMM card interface. Such expansion memory 474 may provide additional storage for device 450, or may also store applications or other information for device 450. Specifically, expansion memory 474 may include instructions to perform or supplement the processes described above, and may include secure information as well. Thus, for example, expansion memory 474 may be provided as a secure module for device 450 and may be programmed with instructions that allow secure use of device 450. In addition, secure applications may be provided with additional information by the SIMM card, such as placing identification information on the SIMM card in a non-hacking manner.
The memory may include, for example, flash memory and/or MRAM memory as described below. In one implementation, a computer program product is tangibly embodied in an information carrier. The computer program product contains instructions that, when executed, perform one or more methods such as those described above. The information carrier is a computer-or machine-readable medium, such as the memory 464, expansion memory 474, or memory on processor 452.
The device 450 may communicate wirelessly through a communication interface 466 that may include digital signal processing circuitry where necessary 466. Communication interface 466 may provide communication under various modes or protocols such as, inter alia, GSM voice calls, voice LTE (VOLTE) telephony, SMS, EMS, or MMS messaging, CDMA, TDMA, PDC, WCDMA, CDMA2000, GPRS, wiMAX, LTE. Such communication may occur, for example, through the radio frequency transceiver 468. In addition, near field communication may occur, such as with Bluetooth, wiFi, or such other transceivers (not shown). In addition, the GPS receiver module 470 may provide additional wireless data to the device 450, which may be used by applications running on the device 450 as appropriate.
The device 450 may also communicate audibly using an audio codec 460 that the audio codec 460 may receive from the user and convert to usable digital information. The audio codec 460 may likewise produce voice audible to a user, such as through a speaker in a handset of the device 450, for example. Such speech may include speech from voice telephones, may include recorded speech (e.g., voice messages, music files, etc.), and may also include speech generated by applications operating on device 450.
As shown, computing device 450 may be implemented in a number of different forms. For example, it may be implemented as a cellular telephone 480. But may also be implemented as part of a smart phone 482, personal digital assistant, or other similar mobile device.
Various implementations of the systems and techniques described here can be realized in digital electronic circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof. These various implementations may include implementations in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
These computer programs (also referred to as programs, software applications or code) include machine instructions for a programmable processor, and may be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. As used herein, the terms "machine-readable medium," computer-readable medium "and/or" computer program product, apparatus, and/or device (e.g., magnetic discs, optical disks, memory, programmable Logic Devices (PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term "machine-readable signal" refers to any signal used to provide machine instructions and/or data to a programmable processor.
To provide for interaction with a user, the systems and techniques described here can be implemented on a computer having a display device (e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor) for displaying information to the user and a keyboard and a pointing device (e.g., a mouse or a trackball) by which the user can provide input to the computer. Other types of devices may also be used to provide interaction with a user; for example, feedback provided to the user may be any form of sensory feedback (e.g., visual feedback, auditory feedback, or tactile feedback); and may receive any form of input from a user including voice, speech, or tactile input.
The systems and descriptions described herein may be implemented on a computing system that includes a back-end component (e.g., as a data server) or that includes a middleware component (e.g., an application server) or that includes a front-end component (e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the systems and techniques described here), or any combination of such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include a local area network ("LAN"), a wide area network ("WAN"), and the Internet.
The computing system may include clients and servers. The client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
Various embodiments of the invention have been described. Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the invention. For example, various forms of steps that may be rearranged, added, or removed may use the illustrated flows. Further, while several applications of the system and method have been described, it should be appreciated that many other applications are contemplated. Accordingly, other embodiments are within the scope of the following claims.
Claims (16)
1. A computer-implemented method for performing speech recognition, the method comprising:
receiving (i) a first transcription of a particular utterance from the limited speech recognizer and (ii) a second transcription of a particular utterance from the expanded speech recognizer;
determining a grammatical alignment between the first transcription and the second transcription based on a comparison between the first transcription and the second transcription;
Associating each word or phrase within the first transcription and the second transcription with a metric calculated for each word or phrase within the first transcription and the second transcription, respectively, the metric corresponding to a likelihood of relatedness of each word or phrase within the context of a voice action or voice query within the first transcription and the second transcription;
comparing metrics associated with each word or phrase within the first transcription and the second transcription;
generating a combined transcription from the first transcription and the second transcription representing the particular utterance based on a comparison of metrics associated with each word or phrase within the first transcription and the second transcription; and
the combined transcription is provided as a speech recognizer output for the particular utterance.
2. The computer-implemented method for performing speech recognition of claim 1,
wherein the limited speech recognizer comprises a speech recognizer that includes a language model trained by limited speech recognition vocabulary that includes one or more terms from a speech command grammar but includes fewer than all terms from an expanded grammar; and
Wherein the expanded speech recognizer comprises a speech recognizer that includes a language model trained by expanded speech recognition vocabulary that includes all of the terms of the expanded grammar.
3. The computer-implemented method for performing speech recognition of claim 2, further comprising:
determining that the limited speech recognizer produces a particular word or phrase at a grammatical location within the particular utterance that indicates that the particular utterance includes a voice action command.
4. The computer-implemented method for performing speech recognition of claim 2, further comprising:
it is determined that the expanded speech recognizer produced a particular word or phrase indicating that the particular utterance includes a voice search command.
5. The computer-implemented method for performing speech recognition of claim 1, further comprising:
analyzing the aligned first transcription and the second transcription to determine a type of the particular utterance,
wherein the metric is based on the determined type of the particular utterance.
6. The computer-implemented method for performing speech recognition of claim 5, wherein the type of the particular utterance includes at least one of a voice action command and a voice search command.
7. The computer-implemented method for performing speech recognition of claim 1, wherein aligning the first transcription with the second transcription comprises at least one of pairwise alignment, sequence alignment, or imprecise matching.
8. A system for performing speech recognition, the system comprising:
one or more processors and one or more storage devices storing instructions that are operable when executed by the one or more processors to cause the one or more processors to perform operations comprising:
receiving (i) a first transcription of a particular utterance from the limited speech recognizer and (ii) a second transcription of a particular utterance from the expanded speech recognizer;
determining a grammatical alignment between the first transcription and the second transcription based on a comparison between the first transcription and the second transcription;
associating each word or phrase within the first transcription and the second transcription with a metric calculated for each word or phrase within the first transcription and the second transcription, respectively, the metric corresponding to a likelihood of relatedness of each word or phrase within the context of a voice action or voice query within the first transcription and the second transcription;
Comparing metrics associated with each word or phrase within the first transcription and the second transcription;
generating a combined transcription from the first transcription and the second transcription representing the particular utterance based on a comparison of metrics associated with each word or phrase within the first transcription and the second transcription; and
the combined transcription is provided as a speech recognizer output for the particular utterance.
9. The system for performing speech recognition of claim 8,
wherein the limited speech recognizer comprises a speech recognizer that includes a language model trained by limited speech recognition vocabulary that includes one or more terms from a speech command grammar but includes fewer than all terms from an expanded grammar; and
wherein the expanded speech recognizer comprises a speech recognizer that includes a language model trained by expanded speech recognition vocabulary that includes all of the terms of the expanded grammar.
10. The system for performing speech recognition of claim 9, further comprising:
determining that the limited speech recognizer produces a particular word or phrase at a grammatical location within the particular utterance that indicates that the particular utterance includes a voice action command.
11. The system for performing speech recognition of claim 9, further comprising:
it is determined that the expanded speech recognizer produced a particular word or phrase indicating that the particular utterance includes a voice search command.
12. The system for performing speech recognition of claim 8, further comprising:
analyzing the aligned first transcription and the second transcription to determine a type of the particular utterance,
wherein the metric is based on the determined type of the particular utterance.
13. The system for performing speech recognition according to claim 12, wherein the type of the particular utterance includes at least one of a voice action command and a voice search command.
14. The system for performing speech recognition of claim 8, wherein aligning the first transcription with the second transcription comprises at least one of pairwise alignment, sequence alignment, or imprecise matching.
15. A system for performing speech recognition, the system comprising:
means for receiving (i) a first transcription of a particular utterance from the limited speech recognizer and (ii) a second transcription of a particular utterance from the expanded speech recognizer;
Means for determining a grammatical alignment between the first transcription and the second transcription based on a comparison between the first transcription and the second transcription;
means for associating each word or phrase within the first transcription and the second transcription with a metric calculated for each word or phrase within the first transcription and the second transcription, respectively, the metric corresponding to a likelihood of relatedness of each word or phrase within the context of a voice action or voice query within the first transcription and the second transcription;
means for comparing metrics associated with each word or phrase within the first transcription and the second transcription;
means for generating a combined transcription from the first transcription and the second transcription representing the particular utterance based on a comparison of metrics associated with each word or phrase within the first transcription and the second transcription; and
means for providing the combined transcription as a speech recognizer output for the particular utterance.
16. The system for performing speech recognition of claim 15,
wherein the limited speech recognizer comprises a speech recognizer that includes a language model trained by limited speech recognition vocabulary that includes one or more terms from a speech command grammar but includes fewer than all terms from an expanded grammar; and
Wherein the expanded speech recognizer comprises a speech recognizer that includes a language model trained by expanded speech recognition vocabulary that includes all of the terms of the expanded grammar.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
CN201910931218.1A CN110797027B (en) | 2013-05-13 | 2014-04-18 | Multi-recognizer speech recognition |
Applications Claiming Priority (5)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/892,590 US9058805B2 (en) | 2013-05-13 | 2013-05-13 | Multiple recognizer speech recognition |
US13/892,590 | 2013-05-13 | ||
CN201480027534.1A CN105229728B (en) | 2013-05-13 | 2014-04-18 | More identifier speech recognitions |
PCT/US2014/034686 WO2014186090A1 (en) | 2013-05-13 | 2014-04-18 | Multiple recognizer speech recognition |
CN201910931218.1A CN110797027B (en) | 2013-05-13 | 2014-04-18 | Multi-recognizer speech recognition |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201480027534.1A Division CN105229728B (en) | 2013-05-13 | 2014-04-18 | More identifier speech recognitions |
Publications (2)
Publication Number | Publication Date |
---|---|
CN110797027A CN110797027A (en) | 2020-02-14 |
CN110797027B true CN110797027B (en) | 2023-11-21 |
Family
ID=50792577
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201910931218.1A Active CN110797027B (en) | 2013-05-13 | 2014-04-18 | Multi-recognizer speech recognition |
CN201480027534.1A Active CN105229728B (en) | 2013-05-13 | 2014-04-18 | More identifier speech recognitions |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201480027534.1A Active CN105229728B (en) | 2013-05-13 | 2014-04-18 | More identifier speech recognitions |
Country Status (4)
Country | Link |
---|---|
US (2) | US9058805B2 (en) |
EP (2) | EP2997571B1 (en) |
CN (2) | CN110797027B (en) |
WO (1) | WO2014186090A1 (en) |
Families Citing this family (46)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10354650B2 (en) * | 2012-06-26 | 2019-07-16 | Google Llc | Recognizing speech with mixed speech recognition models to generate transcriptions |
KR102394485B1 (en) | 2013-08-26 | 2022-05-06 | 삼성전자주식회사 | Electronic device and method for voice recognition |
US9659003B2 (en) * | 2014-03-26 | 2017-05-23 | Lenovo (Singapore) Pte. Ltd. | Hybrid language processing |
WO2015178715A1 (en) * | 2014-05-23 | 2015-11-26 | Samsung Electronics Co., Ltd. | System and method of providing voice-message call service |
US11676608B2 (en) | 2021-04-02 | 2023-06-13 | Google Llc | Speaker verification using co-location information |
US9257120B1 (en) | 2014-07-18 | 2016-02-09 | Google Inc. | Speaker verification using co-location information |
US11942095B2 (en) | 2014-07-18 | 2024-03-26 | Google Llc | Speaker verification using co-location information |
KR102292546B1 (en) * | 2014-07-21 | 2021-08-23 | 삼성전자주식회사 | Method and device for performing voice recognition using context information |
US9922138B2 (en) * | 2015-05-27 | 2018-03-20 | Google Llc | Dynamically updatable offline grammar model for resource-constrained offline device |
US10333904B2 (en) * | 2015-08-08 | 2019-06-25 | Peter J. Tormey | Voice access and control |
US20170047062A1 (en) * | 2015-08-14 | 2017-02-16 | Panasonic Automotive Systems Company Of America, Division Of Panasonic Corporation Of North America | Business name phonetic optimization method |
US9653075B1 (en) | 2015-11-06 | 2017-05-16 | Google Inc. | Voice commands across devices |
US9870765B2 (en) * | 2016-06-03 | 2018-01-16 | International Business Machines Corporation | Detecting customers with low speech recognition accuracy by investigating consistency of conversation in call-center |
US20180018961A1 (en) * | 2016-07-13 | 2018-01-18 | Google Inc. | Audio slicer and transcription generator |
US10019986B2 (en) * | 2016-07-29 | 2018-07-10 | Google Llc | Acoustic model training using corrected terms |
US9972320B2 (en) | 2016-08-24 | 2018-05-15 | Google Llc | Hotword detection on multiple devices |
US9959861B2 (en) * | 2016-09-30 | 2018-05-01 | Robert Bosch Gmbh | System and method for speech recognition |
CN106373571A (en) * | 2016-09-30 | 2017-02-01 | 北京奇虎科技有限公司 | Voice control method and device |
US20180158458A1 (en) * | 2016-10-21 | 2018-06-07 | Shenetics, Inc. | Conversational voice interface of connected devices, including toys, cars, avionics, mobile, iot and home appliances |
CN108010523B (en) * | 2016-11-02 | 2023-05-09 | 松下电器（美国）知识产权公司 | Information processing method and recording medium |
US10170110B2 (en) * | 2016-11-17 | 2019-01-01 | Robert Bosch Gmbh | System and method for ranking of hybrid speech recognition results with neural networks |
US10242673B2 (en) | 2016-12-07 | 2019-03-26 | Google Llc | Preventing of audio attacks using an input and an output hotword detection model |
US9940930B1 (en) | 2016-12-07 | 2018-04-10 | Google Llc | Securing audio data |
US10134396B2 (en) | 2016-12-07 | 2018-11-20 | Google Llc | Preventing of audio attacks |
US10831366B2 (en) * | 2016-12-29 | 2020-11-10 | Google Llc | Modality learning on mobile devices |
CN117577099A (en) | 2017-04-20 | 2024-02-20 | 谷歌有限责任公司 | Method, system and medium for multi-user authentication on a device |
US10395647B2 (en) * | 2017-10-26 | 2019-08-27 | Harman International Industries, Incorporated | System and method for natural language processing |
US10867129B1 (en) | 2017-12-12 | 2020-12-15 | Verisign, Inc. | Domain-name based operating environment for digital assistants and responders |
US10665230B1 (en) | 2017-12-12 | 2020-05-26 | Verisign, Inc. | Alias-based access of entity information over voice-enabled digital assistants |
CN108122555B (en) * | 2017-12-18 | 2021-07-23 | 北京百度网讯科技有限公司 | Communication method, voice recognition device and terminal device |
US11087766B2 (en) * | 2018-01-05 | 2021-08-10 | Uniphore Software Systems | System and method for dynamic speech recognition selection based on speech rate or business domain |
US10147428B1 (en) * | 2018-05-30 | 2018-12-04 | Green Key Technologies Llc | Computer systems exhibiting improved computer speed and transcription accuracy of automatic speech transcription (AST) based on a multiple speech-to-text engines and methods of use thereof |
CN113168830A (en) * | 2018-11-30 | 2021-07-23 | 谷歌有限责任公司 | Speech processing |
US11488589B1 (en) | 2018-12-21 | 2022-11-01 | Verisign, Inc. | Transitioning voice interactions |
US11875883B1 (en) | 2018-12-21 | 2024-01-16 | Cerner Innovation, Inc. | De-duplication and contextually-intelligent recommendations based on natural language understanding of conversational sources |
US11062704B1 (en) | 2018-12-21 | 2021-07-13 | Cerner Innovation, Inc. | Processing multi-party conversations |
US11398232B1 (en) | 2018-12-21 | 2022-07-26 | Cerner Innovation, Inc. | Natural language understanding of conversational sources |
US11410650B1 (en) | 2018-12-26 | 2022-08-09 | Cerner Innovation, Inc. | Semantically augmented clinical speech processing |
KR20210027991A (en) * | 2019-09-03 | 2021-03-11 | 삼성전자주식회사 | Electronic apparatus and control method thereof |
KR20210064594A (en) * | 2019-11-26 | 2021-06-03 | 삼성전자주식회사 | Electronic apparatus and control method thereof |
CN111081225B (en) * | 2019-12-31 | 2022-04-01 | 思必驰科技股份有限公司 | Skill voice awakening method and device |
WO2021183681A1 (en) * | 2020-03-10 | 2021-09-16 | MeetKai, Inc. | Parallel hypothetical reasoning to power a multi-lingual, multi-turn, multi-domain virtual assistant |
CN111816165A (en) * | 2020-07-07 | 2020-10-23 | 北京声智科技有限公司 | Voice recognition method and device and electronic equipment |
US11798530B2 (en) * | 2020-10-30 | 2023-10-24 | Google Llc | Simultaneous acoustic event detection across multiple assistant devices |
CN112164392A (en) * | 2020-11-13 | 2021-01-01 | 北京百度网讯科技有限公司 | Method, device, equipment and storage medium for determining displayed recognition text |
US11532312B2 (en) * | 2020-12-15 | 2022-12-20 | Microsoft Technology Licensing, Llc | User-perceived latency while maintaining accuracy |
Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6526380B1 (en) * | 1999-03-26 | 2003-02-25 | Koninklijke Philips Electronics N.V. | Speech recognition system having parallel large vocabulary recognition engines |
CN101164102A (en) * | 2005-02-03 | 2008-04-16 | 语音信号科技公司 | Methods and apparatus for automatically extending the voice vocabulary of mobile communications devices |
CN101454775A (en) * | 2006-05-23 | 2009-06-10 | 摩托罗拉公司 | Grammar adaptation through cooperative client and server based speech recognition |
CN101630333A (en) * | 2008-07-18 | 2010-01-20 | 谷歌公司 | Transliteration for query expansion |
CN103003875A (en) * | 2010-05-18 | 2013-03-27 | 沙扎姆娱乐有限公司 | Methods and systems for performing synchronization of audio with corresponding textual transcriptions and determining confidence values of the synchronization |
Family Cites Families (33)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
ZA948426B (en) | 1993-12-22 | 1995-06-30 | Qualcomm Inc | Distributed voice recognition system |
US6446076B1 (en) | 1998-11-12 | 2002-09-03 | Accenture Llp. | Voice interactive web-based agent system responsive to a user location for prioritizing and formatting information |
US7679534B2 (en) | 1998-12-04 | 2010-03-16 | Tegic Communications, Inc. | Contextual prediction of user words and user actions |
US7720682B2 (en) | 1998-12-04 | 2010-05-18 | Tegic Communications, Inc. | Method and apparatus utilizing voice input to resolve ambiguous manually entered text input |
US7881936B2 (en) | 1998-12-04 | 2011-02-01 | Tegic Communications, Inc. | Multimodal disambiguation of speech recognition |
DE60113644T2 (en) | 2001-03-27 | 2006-07-06 | Nokia Corp. | Method and system for managing a database in a communications network |
US20040019488A1 (en) | 2002-07-23 | 2004-01-29 | Netbytel, Inc. | Email address recognition using personal information |
US7570943B2 (en) | 2002-08-29 | 2009-08-04 | Nokia Corporation | System and method for providing context sensitive recommendations to digital services |
US7606714B2 (en) * | 2003-02-11 | 2009-10-20 | Microsoft Corporation | Natural language classification within an automated response system |
US20050246325A1 (en) | 2004-04-30 | 2005-11-03 | Microsoft Corporation | Method and system for recording and accessing usage of an item in a computer system |
US8589156B2 (en) | 2004-07-12 | 2013-11-19 | Hewlett-Packard Development Company, L.P. | Allocation of speech recognition tasks and combination of results thereof |
US20060069564A1 (en) | 2004-09-10 | 2006-03-30 | Rightnow Technologies, Inc. | Method of weighting speech recognition grammar responses using knowledge base usage data |
US7769142B2 (en) | 2005-07-14 | 2010-08-03 | Microsoft Corporation | Asynchronous discrete manageable instant voice messages |
US20110143731A1 (en) | 2005-09-14 | 2011-06-16 | Jorey Ramer | Mobile Communication Facility Usage Pattern Geographic Based Advertising |
US8131548B2 (en) | 2006-03-06 | 2012-03-06 | Nuance Communications, Inc. | Dynamically adjusting speech grammar weights based on usage |
US20110054900A1 (en) | 2007-03-07 | 2011-03-03 | Phillips Michael S | Hybrid command and control between resident and remote speech recognition facilities in a mobile voice-to-speech application |
US20110054896A1 (en) | 2007-03-07 | 2011-03-03 | Phillips Michael S | Sending a communications header with voice recording to send metadata for use in speech recognition and formatting in mobile dictation application |
US20110054894A1 (en) | 2007-03-07 | 2011-03-03 | Phillips Michael S | Speech recognition through the collection of contact information in mobile dictation application |
US20090030697A1 (en) | 2007-03-07 | 2009-01-29 | Cerra Joseph P | Using contextual information for delivering results generated from a speech recognition facility using an unstructured language model |
US8635243B2 (en) | 2007-03-07 | 2014-01-21 | Research In Motion Limited | Sending a communications header with voice recording to send metadata for use in speech recognition, formatting, and search mobile search application |
US8204746B2 (en) | 2007-03-29 | 2012-06-19 | Intellisist, Inc. | System and method for providing an automated call center inline architecture |
US8396713B2 (en) | 2007-04-30 | 2013-03-12 | Nuance Communications, Inc. | Method and system for using a statistical language model and an action classifier in parallel with grammar for better handling of out-of-grammar utterances |
US20090326937A1 (en) | 2008-04-21 | 2009-12-31 | Microsoft Corporation | Using personalized health information to improve speech recognition |
JP5530729B2 (en) | 2009-01-23 | 2014-06-25 | 本田技研工業株式会社 | Speech understanding device |
EP2211336B1 (en) | 2009-01-23 | 2014-10-08 | Harman Becker Automotive Systems GmbH | Improved speech input using navigation information |
US8543401B2 (en) * | 2009-04-17 | 2013-09-24 | Synchronoss Technologies | System and method for improving performance of semantic classifiers in spoken dialog systems |
US8892439B2 (en) | 2009-07-15 | 2014-11-18 | Microsoft Corporation | Combination and federation of local and remote speech recognition |
US8682669B2 (en) * | 2009-08-21 | 2014-03-25 | Synchronoss Technologies, Inc. | System and method for building optimal state-dependent statistical utterance classifiers in spoken dialog systems |
US8346549B2 (en) | 2009-12-04 | 2013-01-01 | At&T Intellectual Property I, L.P. | System and method for supplemental speech recognition by identified idle resources |
US8898065B2 (en) | 2011-01-07 | 2014-11-25 | Nuance Communications, Inc. | Configurable speech recognition system using multiple recognizers |
US9674328B2 (en) | 2011-02-22 | 2017-06-06 | Speak With Me, Inc. | Hybridized client-server speech recognition |
US8972260B2 (en) | 2011-04-20 | 2015-03-03 | Robert Bosch Gmbh | Speech recognition using multiple language models |
US8996381B2 (en) | 2011-09-27 | 2015-03-31 | Sensory, Incorporated | Background speech recognition assistant |
-
2013
- 2013-05-13 US US13/892,590 patent/US9058805B2/en active Active
-
2014
- 2014-04-18 EP EP14726279.4A patent/EP2997571B1/en active Active
- 2014-04-18 EP EP18182704.9A patent/EP3407349B1/en active Active
- 2014-04-18 CN CN201910931218.1A patent/CN110797027B/en active Active
- 2014-04-18 CN CN201480027534.1A patent/CN105229728B/en active Active
- 2014-04-18 WO PCT/US2014/034686 patent/WO2014186090A1/en active Application Filing
-
2015
- 2015-06-01 US US14/726,943 patent/US9293136B2/en active Active
Patent Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6526380B1 (en) * | 1999-03-26 | 2003-02-25 | Koninklijke Philips Electronics N.V. | Speech recognition system having parallel large vocabulary recognition engines |
CN101164102A (en) * | 2005-02-03 | 2008-04-16 | 语音信号科技公司 | Methods and apparatus for automatically extending the voice vocabulary of mobile communications devices |
CN101454775A (en) * | 2006-05-23 | 2009-06-10 | 摩托罗拉公司 | Grammar adaptation through cooperative client and server based speech recognition |
CN101630333A (en) * | 2008-07-18 | 2010-01-20 | 谷歌公司 | Transliteration for query expansion |
CN103003875A (en) * | 2010-05-18 | 2013-03-27 | 沙扎姆娱乐有限公司 | Methods and systems for performing synchronization of audio with corresponding textual transcriptions and determining confidence values of the synchronization |
Also Published As
Publication number | Publication date |
---|---|
US9293136B2 (en) | 2016-03-22 |
US20140337032A1 (en) | 2014-11-13 |
US20150262581A1 (en) | 2015-09-17 |
EP2997571A1 (en) | 2016-03-23 |
US9058805B2 (en) | 2015-06-16 |
EP2997571B1 (en) | 2018-07-11 |
CN110797027A (en) | 2020-02-14 |
CN105229728B (en) | 2019-10-29 |
EP3407349A1 (en) | 2018-11-28 |
CN105229728A (en) | 2016-01-06 |
EP3407349B1 (en) | 2019-12-04 |
WO2014186090A1 (en) | 2014-11-20 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN110797027B (en) | Multi-recognizer speech recognition | |
US11741970B2 (en) | Determining hotword suitability | |
EP3559944B1 (en) | Server side hotwording | |
EP3125234B1 (en) | Individualized hotword detection models | |
US20200066275A1 (en) | Providing pre-computed hotword models | |
CN109844740B (en) | Follow-up voice query prediction | |
US8775177B1 (en) | Speech recognition process | |
US9502032B2 (en) | Dynamically biasing language models | |
US20080130699A1 (en) | Content selection using speech recognition |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |