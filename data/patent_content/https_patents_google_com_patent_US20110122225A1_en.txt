US20110122225A1 - Depth Coding as an Additional Channel to Video Sequence - Google Patents
Depth Coding as an Additional Channel to Video Sequence Download PDFInfo
- Publication number
- US20110122225A1 US20110122225A1 US12/952,781 US95278110A US2011122225A1 US 20110122225 A1 US20110122225 A1 US 20110122225A1 US 95278110 A US95278110 A US 95278110A US 2011122225 A1 US2011122225 A1 US 2011122225A1
- Authority
- US
- United States
- Prior art keywords
- depth
- data
- view
- encoding
- combined set
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Abandoned
Links
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/70—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals characterised by syntax aspects related to video coding, e.g. related to compression standards
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/50—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding
- H04N19/597—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding specially adapted for multi-view video sequence encoding
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/134—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or criterion affecting or controlling the adaptive coding
- H04N19/146—Data rate or code amount at the encoder output
- H04N19/147—Data rate or code amount at the encoder output according to rate distortion criteria
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N2213/00—Details of stereoscopic systems
- H04N2213/003—Aspects relating to the "2D+depth" image format
Definitions
- the present invention relates to depth coding in a video image, such as in a 3D video image.
- 3D is becoming an attractive technology again, and this time it is gaining supports from content providers. Most of new animation movies and many films will be released also with 3D capability and can be watched in 3D movie theaters widespread across the country. Also there were several tests for real time broadcast of sports event, e.g., NBA and NFL games. To make 3D perceived in flat screens, stereopsis is used, which mimics human visual system and shows left and right views captured by stereo cameras to left and right eye, respectively. Therefore, it requires twice the bandwidth required for 2D sequences.
- 3D TV (3DTV) or 3D video (3DV) is the application which uses stereopsis to deliver 3D perception to viewers. However, because only two views for each eye are delivered in 3DTV, users can not change the view point which is fixed by contents provider.
- Free viewpoint TV is another 3D application which enables users to navigate through different view points and choose the one they want to watch.
- multi-view video sequences are transmitted to users.
- stereo sequences required for 3DTV can be regarded as a subset of multi-view video sequences if the distance between neighboring views satisfies the conditions for stereopsis. Because the amount of data increases linearly according to the number of views, multi-view video sequences need to be compressed efficiently for wide spread use.
- MVC multi-view video coding
- an apparatus of the invention may comprise an encoder configured to encode the video data by encoding a combined set of view data and depth data.
- the combined set of view data and depth data may include one of: RGBD, YUVD, or YCbCrD.
- the combined set of view data and depth data may be contained in at least one of: a group of pictures, a picture, a slice, a group of blocks, a macroblock, or a sub-macroblock.
- the apparatus may further comprise a depth format unit configured to identify a depth format of the video data.
- the encoder may select to encode the video data as a plurality of two dimensional images without including depth data when the depth format is set to 0, or the encoder may select to encode the video data as a combined set of view data and depth data when the depth format is set to a predetermined level.
- the encoder may further include a coding cost calculator which determines coding costs of joint encoding of said combined set of view data and depth data and separate encoding of said combined set of view data and depth data, and determines an encoding mode between joint encoding and separate encoding based on said coding cost.
- the encoder may encode the video data as a joint encoding of view data and depth data when the encoding cost is less than an encoding cost of separately encoding the view data and depth data.
- the video data may be one of a: multiview with depth, multiview without depth, single view with depth, single view without depth.
- a method of encoding video data may comprise encoding the video data by encoding a combined set of view data and depth data at an encoder.
- the combined set of view data and depth data may include one of: RGBD, YUVD, or YCbCrD.
- the combined set of view data and depth data is contained in at least one of: a group of pictures, a picture, a slice, a group of blocks, a macroblock, or a sub-macroblock.
- the method may further comprise identifying a depth format of the video data.
- the video data may be encoded as a plurality of two dimensional images without including depth data when the depth format is set to 0.
- the video data may be encoded as a combined set of view data and depth data when the depth format is set to a predetermined level.
- the method may further include determining a coding cost of joint encoding said combined set of view data and depth data and separate encoding of said combined set of view data and depth data, and determining an encoding mode between joint encoding and separate encoding based on said coding cost.
- the video data may be encoded as a joint encoding of view data and depth data when the encoding cost is less than an encoding cost of separately encoding the view data and depth data.
- the video data may be one of a: multiview with depth, multiview without depth, single view with depth, single view without depth.
- a non-transitory computer readable medium carrying instructions for an encoder to encode video data may comprise instructions to perform the step of: encoding the video data by encoding a combined set of view data and depth data.
- the combined set of view data and depth data may include one of: RGBD, YUVD, or YCbCrD.
- the combined set of view data and depth data is contained in at least one of: a group of pictures, a picture, a slice, a group of blocks, a macroblock, or a sub-macroblock.
- the instructions may further comprise identifying a depth format of the video data.
- the video data may be encoded as a plurality of two dimensional images without including depth data when the depth format is set to 0.
- the video data may be encoded as a combined set of view data and depth data when the depth format is set to a predetermined level.
- the instructions may further include determining a coding cost of joint encoding said combined set of view data and depth data and separate encoding of said combined set of view data and depth data, and determining an encoding mode between joint encoding and separate encoding based on said coding cost.
- the video data may be encoded as a joint encoding of view data and depth data when the encoding cost is less than an encoding cost of separately encoding the view data and depth data.
- the video data may be one of a: multiview with depth, multiview without depth, single view with depth, single view without depth.
- an apparatus for decoding video data may comprise: a decoder configured to decode the video data by decoding a combined set of view data and depth data.
- the combined set of view data and depth data may include one of: RGBD, YUVD, or YCbCrD.
- the combined set of view data and depth data may be contained in at least one of: a group of pictures, a picture, a slice, a group of blocks, a macroblock, or a sub-macroblock.
- the apparatus may further comprise a depth format unit configured to identify a depth format of the video data.
- the decoder may select to decode the video data as a plurality of two dimensional images without including depth data when the depth format is set to 0.
- the decoder may select to decode the video data as a combined set of view data and depth data when the depth format is set to a predetermined level.
- the decoder may selectively jointly decodes said combined set of view data and depth data when said combined set was jointly encoded or decodes said combined set of view data and depth data when said combined set was separately encoded.
- the video data may be one of a: multiview with depth, multiview without depth, single view with depth, single view without depth.
- a method decoding video data comprising: decoding the video data by decoding a combined set of view data and depth data at a decoder.
- the combined set of view data and depth data may include one of: RGBD, YUVD, or YCbCrD.
- the combined set of view data and depth data is contained in at least one of: a group of pictures, a picture, a slice, a group of blocks, a macroblock, or a sub-macroblock.
- the method may further comprise identifying a depth format of the video data.
- the video data may be decoded as a plurality of two dimensional images without including depth data when the depth format is set to 0.
- the video data may be decoded as a combined set of view data and depth data when the depth format is set to a predetermined level.
- the method may further include selectively jointly decoding said combined set of view data and depth data when said combined set was jointly encoded or decoding said combined set of view data and depth data when said combined set was separately encoded.
- the video data may be one of a: multiview with depth, multiview without depth, single view with depth, single view without depth.
- a non-transitory computer readable medium may carrying instructions for an decoder to decode video data, comprising instruction to perform the steps of: decoding the video data by encoding a combined set of view data and depth data.
- the combined set of view data and depth data may include one of: RGBD, YUVD, or YCbCrD.
- the combined set of view data and depth data is contained in at least one of: a group of pictures, a picture, a slice, a group of blocks, a macroblock, or a sub-macroblock.
- the instructions may further comprise identifying a depth format of the video data.
- the video data may be decoded as a plurality of two dimensional images without including depth data when the depth format is set to 0.
- the video data may be decoded as a combined set of view data and depth data when the depth format is set to a predetermined level.
- the instructions may further include selectively jointly decoding said combined set of view data and depth data when said combined set was jointly encoded or decoding said combined set of view data and depth data when said combined set was separately encoded.
- the video data may be one of a: multiview with depth, multiview without depth, single view with depth, single view without depth.
- the invention allows 3D encoding of a depth parameter jointly with view information.
- the invention allows for compatibility with 2D and may provide optimized encoding based on the RD costs in encoding depth jointly with view or separately.
- depth as a video component from the beginning thus, in inter prediction, block mode and reference index are shared between view and depth in addition to motion vector.
- intra prediction intra prediction mode can be shared also.
- the coding result of combined coding can be further optimized by considering depth information together with view. In the separate coding of view and depth, depth is coded independently to the view. It is also possible to have intra coded depth while view is inter coded.
- FIG. 1 illustrates an end-to-end 3D/FTV system.
- FIG. 2 illustrates an approach for depth estimation.
- FIGS. 3A-3D illustrate a sample video image in various forms.
- FIG. 4 illustrates an encoder and decoder arrangement in accordance with the principles of the invention.
- FIG. 5 illustrates a flowchart of RD optimization (RDO) in each macroblock between combined coding and separate coding in accordance with the principles of the invention.
- RDO RD optimization
- FIG. 6 illustrates a flowchart for adaptive coding of 3D video in accordance with the principles of the invention.
- FIGS. 7A-7D illustrate a sample image and a chart of PSNR of view and depth.
- FIGS. 8A and 8B illustrate the depth of Lovebird 1 , View 2 in time 0 and time 1 .
- FIG. 9A and 9B show RD curves of synthesized views for Lovebird 1 and Pantomime.
- FIGS. 10A and 10B illustrate luma and depth of Lovebirds from FIG. 3 .
- FIGS. 11A and B illustrate other sample images, including Lovebird 2 and Pantomine.
- FIG. 1 shows an exemplary diagram for end-to-end 3D/FTV system.
- multiple views are captured of a scene or object 1 by multiple cameras 2 .
- the captured views by the multiple cameras 2 are corrected or rectified and sent to a processor and storage system 7 prior to transmission by a transmitter 3 .
- the processor may include an encoder which encodes the image data into a specified format. At the encoder, multiple views are available which can be used to estimate depth more efficiently and correctly.
- a user's side generally includes a receiver 6 which receives the transmitted and encoded images from transmitter 3 .
- the received data is proved to a processor/buffer which typically includes a decoder.
- the decoded and otherwise processed image data is provided to display 5 for viewing by the user.
- MPEG started to search for a new standard for multi-view video sequence coding.
- depth information is exploited to improve overall coding efficiency.
- sub-sampled views 2 or 3 key views are sent with corresponding depth information and intermediate views are synthesized using key views and depths.
- Depth is assumed to be estimated (if not captured) before compression at the encoder and intermediate views are synthesized after decompression at the decoder. Note that not all captured views are compressed and transmitted in this scheme.
- EE 1 explores depth estimation from neighboring views and EE 2 explores view synthesis techniques which synthesize intermediate views using estimated depth from EE 1 .
- EE 3 searched techniques for generation of intermediate views based on layered depth video (LDV) representation.
- EE 4 explores how the depth map coding affects the quality of synthesized views.
- LDV layered depth video
- FIG. 2 EE 1 for depth estimation and EE 2 for view synthesis are described.
- any two views can be selected to estimate depth between them.
- View 1 and View 5 are used to estimate Depth 2 and Depth 4 , shown in row 23 .
- View 2 , Depth 2 , View 4 and Depth 4 can be encoded and transmitted to the users and intermediate views between View 2 and View 4 can be synthesized using Depth 2 and Depth 4 with corresponding camera parameters.
- View 3 is synthesized, shown in row 25 , and compared with original View 3 .
- rate and distortion (R-D) curves where rate is shown in Kbps for depth coding and distortion is shown in PSNR for synthesized view.
- rate is shown in Kbps for depth coding
- distortion is shown in PSNR for synthesized view.
- the quality of synthesized view does not change significantly in most range of bit rates for depth.
- C. Cheng, Y. Huo and Y. Liu, “3DV EE4 results on Dog sequence,” ISO/IEC JTC1/SC29/WG11 MPEG Document M16047, Lausanne, Switzerland, February 2009 multi-view video coding (MVC) is used to encode stereo views and depths and compared with coding results when H.264/AVC is used to encode each view independently.
- MVC showed less than 5% coding gains compared to simulcast by H.264/AVC.
- FIG. 3 a shows the original view 0
- FIG. 3 a shows the original view 0
- FIGS. 11A and 11B show other views, including Lovebird 2 View 7 and Pantomime View 37 .
- FIG. 3 b - 3 d from the comparison of Cb/Cr with depth, it can be seen that both Cb/Cr and depth share object boundary.
- an image is segmented based on color for the disparity (depth) estimation because color channel shares the information of object boundaries G. Um, T. Kim, N. Hur, and J. Kim, “Segment-based Disparity Estimation using Foreground Separation,” ISO/IEC JTC1/SC29/WG11 MPEG Document M15191, Antalya, Turkey, January 2008.
- depth should be encoded and transmitted with view for 3D services and an efficient and flexible coding scheme needs to be defined.
- the correlation between view and depth can be exploited, just as the correlation between luma and chroma is exploited during the transition from monochrome to color, we provide a new flexible depth format and coding scheme which is backward compatible and suitable for different objectives of new 3D services.
- the determination of the depth data may be performed by the techniques discussed above or another suitable approach.
- RGB or YCbCr format is expanded to RGBD or YCbCrD to include depth.
- the format for monochrome or color can be selected by chroma_format_idc flag.
- chroma_format_idc flag we may use a depth_format_idc flag to specify if a signal is 2D or 3D. Table 1 shows how to use chroma_format_idc and depth_format_idc to signal video format in 2D/3D and monochrome/color.
- Index 0 means YCbCrD are all grouped together and encoded by the same block mode. This is the case that the same motion vector (MV) or the same intra prediction direction is used for all channels.
- index 1 depth is encoded separately to view.
- Index 5 specifies each channel is encoded independently.
- channels can be grouped differently. For example, assume that YUV 420 is used for the view and depth is quite smooth, thus the same resolution to chroma is enough for depth signal. Then, Cb, Cr and D can be treated as a group and Y as another group. Then group index 2 can be used assuming Cb, Cr and D can be similarly encoded without affecting overall compression efficiency. If the resolution of depth is equal to that of luminance in YUV 420 format and depth needs to be coded in high quality, group index 1 or group index 4 can be used. If there is enough correlation between Y and D, group index 3 can be used additionally. In the next, we assume two different applications for 3D and show how we can exploit the correlation between view and depth under the new video signal format. Note that the approaches explained next can be similarly applied to different combination of groups.
- depth_format_idc may be defined in Table 3 to specify additional picture format YCbCrD. If sequence does not have depth for 3D application, it is set to 0 and sequence is encoded by standard H.264/AVC. If sequence carries depth channel, depth can be encoded in the same size to luma (Y) when depth format is ‘D 4 ’ or encoded in the same size to chroma (Cb/Cr) when depth format is ‘D 1 ’ where the width and height of D 1 can be half of D 4 or equal to D 4 depending on SubWidthC and SubHeightC, respectively.
- the associated syntax change in sequence parameter set of H.264/AVC is shown in Table 4. Those of skill in the art will appreciate that the encoder preferably sets the various syntax values in Table 4 during an encoding process, and the decoder may use the values during the decoding process.
- bit_depth_depth_minus8 is added in the sequence parameter set as shown in Table 4. BitDepth D and QpBdOffset D are specified as;
- BitDepth D 8+bit_depth_depth_minus8 (1)
- BitDepth D N+bit_depth_depth_minusN.
- depth_qp_offset is present in picture parameter set syntax when depth_format_idc>0.
- Table 5 associated syntax change in H.264/AVC is shown.
- QP D for depth component is determined as follows;
- variable qD offset for depth component is derived as follows.
- QP D Clip3( ⁇ QpBd Offset D , 51, QP Y +qD Offset ) (4)
- pic_parameter_set_rbsp( ) ⁇ C Descriptor pic_parameter_set_id 1 ue(v) seq_parameter_set_id 1 ue(v) entropy_coding_mode_flag 1 u(1) pic_order_present_flag 1 u(1) num_slice_groups_minus1 1 ue(v) if( num_slice_groups_minus1 > 0 ) ⁇ ⁇ num_ref_idx_10_active_minus1 1 ue(v) num_ref_idx_11_active_minus1 1 ue(v) weighted_pred_flag 1 u(1) weighted_bipred_idc 1 u(2) pic_init_qp_minus26 /* relative to 26 */ 1 se(v) pic_init_qs_minus26 /* relative to 26 */ 1 se(v) pic_init_qs_minus26
- the block coding may include using macroblocks or multiples of macroblocks, e.g. MB pairs.
- a YCrCbD MB may consist of Y 16 ⁇ 16, Cr 8 ⁇ 8, Cb 8 ⁇ 8 and D 8 ⁇ 8, for example.
- various block sizes may be used for each of Y, Cr, Cb and D.
- D may have a size of 8 ⁇ 8 or 16 ⁇ 16.
- depth format D 1 we encode depth map in a similar way that chroma is coded in H.264/AVC exploiting the correlation between Cb/Cr and D.
- depth is treated as if were a third chroma channel, Cb/Cr/D. Therefore, the same block mode, intra prediction direction, motion vector (MV) and reference index (refIdx) are applied to Cb/Cr and D.
- coded block pattern (CBP) in H.264/AVC is redefined in Table 6 to include CBP of depth.
- depth cost is added to calculate total cost for Cb/Cr/D and depth shares the same intra prediction direction with Cb/Cr.
- rate-distortion (RD) cost of depth is added to total RD cost for YCbCr, thus mode decision is optimized for both view and depth.
- the only information not shared with Cb/Cr is the residual of depth, which is encoded after residual coding of Cb/Cr depending on CBP.
- FIG. 4 illustrates an apparatus for estimating or simulating depth coding in accordance with the invention.
- a DERS 41 module for depth estimation and then downsample the depth map by 2 both horizontally and vertically using a down sampling module 42 , such as a polynorm filter David Baylon, “Polynorm Filters for Image Resizing: Additional Considerations,” Motorola, Home & Networks Mobility, Advanced Technology internal memo DSM2008-072r1, December 2008.
- the downsampled depth map has the same resolution as the chroma channels in YUV 4:2:0 format.
- encoders 48 which may be two H.264/AVC encoders, thus 2 independent bit streams are generated.
- the encoded image may be provided to a downstream transmitter 3 (see, FIG. 1 ) and transmitted to a remotely located decoder 45 , as generally shown by the direction arrow in FIG. 4 .
- the encoder may be in a network element, e.g. a headend unit, the decoder may be in a user device, e.g. a set top box. The decoder decodes and reconstructs the view and depth parameters.
- Reconstructed depths are upsampled to the original size using and up sampler 46 , such as a polynorm filter again in Baylon's approach, and fed into a view synthesis module 47 , which may include view synthesis reference software (VSRS) with reconstructed views to synthesize additional views.
- VSRS view synthesis reference software
- the encoding may be performed with Y and RD optimization.
- depth format D 4 we target coding efficiency of overall YCbCrD sequences exploiting the correlation between view and depth. Because depth resolution is equal to luma, Y, instead of Cb/Cr, coding information of Y is shared for efficient depth coding.
- FIGS. 10A and 10B show luma and depth of Lovebirds from FIG. 3 . Although the similarity in object shapes and boundaries can be observed, it is still possible that the best match minimizing distortion can be found in the different locations for Y and D, respectively. For example, in FIG.
- the best match of the grass in Y might not be the best match in D because the texture of the grass repeats in Y while depth of grass looks noisy. Therefore, instead of sharing coding information over the whole picture, we may select whether to share coding information of Y with depth or not in coding each macroblock depending on the RD cost between combined coding (share) and separate coding (not share) of view and depth.
- FIG. 5 illustrates a flowchart of rate distortion optimization (RDO) in each macroblock between combined coding and separate coding.
- a macroblock (MB) is received in step S 1 .
- View and depth is encoded as a combined YCbCrD and calculated RD cost in step S 3 , RDcost(YCbCrD).
- the best coding information found is saved, including intra prediction mode, motion vector and reference index, such as for both the joint coding of view and depth and the independent coding of view and depth.
- the view and depth are encoded independently and the individual RD cost, RDcost(YCbCr) and RDcost(D) are calculated, steps S 5 and S 7 .
- step S 11 The one with the minimum RD cost for the current macroblock is selected. That is, if the RD cost of the combined YCbCrD is less than the RD cost of the separate RD(YCbCr)+RD(D), the MB is updated with the combined results (YCbCrD), step S 15 . If the RD cost of the combined YCbCrD is not less than the RD cost of the separate RD(YCbCr)+RD(D), the MB is updated with the separate results (YCbCr and D), step S 13 . The next MB is taken to be processed in step S 17 . Two separate coded block information for YCbCr and D may be maintained, respectively, as a reference to encode future macroblocks.
- Non-shared Shared information information INTRA Block mode, intra CBP, Residual prediction direction INTER Block mode, CBP, Residual (16 ⁇ 16, . . . , 8 ⁇ 8) MV, RefIdx
- mb_YCbCrD_flag is introduced as a new flag which can be 0 or 1 indicating separate or combined coding, respectively.
- This flag may be encoded by CABAC and three contexts are defined by mb_YCbCrD_flag from the neighboring left and upper blocks.
- the context index c for current MB is defined as follows;
- 3D video signal e.g., YCbCrD
- depth is included as a video component.
- 2D video can be sent with depth_format_idc equal to 0 specifying there is no depth component.
- FIG. 6 shows a flowchart for adaptive coding of 3D video in accordance with the invention.
- the processes starts at step S 20 .
- step S 22 with the depth_format_idc flag equal to 0, video signal is treated as 2D and conventional 2D encoding (e.g. H.264/AVC, MPEG 2, or H.265/HEVC) is used, step S 24 . If the depth_format_idc flag is 1, depth is encoded as if it is a third chroma channel, which is the same resolution as for the chroma, step S 28 .
- conventional 2D encoding e.g. H.264/AVC, MPEG 2, or H.265/HEVC
- depth_format_idc flag 2
- depth is the same resolution as the luma and adaptive joint/separate coding is applied to view and depth based on RD cost (steps S 26 ).
- the RD cost may be determined according to the process shown in FIG. 5 . Note that we showed how the adaptive coding can be applied between group index 0 , 1 , 3 and 4 in Table 2. This approach can be extended to any group index in Table 2 according to the application, correlation between channels, etc.
- depth format D 1 depth is encoded in H.264/AVC, sharing coding information with Cb/Cr, therefore additional encoder complexity is negligible and overall encoder complexity is similar to the original H.264/AVC.
- depth format D 4 depth can be encoded sharing coding information with Y. Noting that the best predictions for Y and D can be different even for the same object, combined coding or separate coding of YCbCr and D is decided by RD cost of each approach.
- the YCbCrD coding in depth format D 1 was implemented in a Motorola H.264/AVC encoder (Zeus) and compared with independent coding of YCbCr and depth.
- View 1 , 2 , 3 , 4 and 5 from Lovebird 1 , and other images, e.g. View 36 , 37 , 38 , 39 and 40 from Pantomime following MPEG EE 1 and EE 2 procedure shown in FIG. 2 .
- View 3 in Lovebird 1 is synthesized and the qualities of synthesized views are compared with the original views.
- Original Lovebird 1 sequence is YUV 4:2:0 format and depth_format_idc is set to 1, thus depth array has the same size as Cb and Cr.
- FIGS. 7A-7D the Peak Signal to Noise Ratio (PSNR) of view and depth are shown with respect to total bit rate for Lovebird 1 and Pantomime, respectively. Images for Lovebird 2 and Pantomime may be found in FIGS. 11A and 11B , respectively. More specifically, FIG. 7A and 7B illustrates a chart of PSNR vs total bit rate for the image Loverbird 1 , and FIGS. 7C and 7D illustrates a chart for Pantomime. The charts illustrate that the quality of reconstructed depth by YCbCrD coding, shown by YUVD depth and triangles, is worse than that by independent depth coding, shown by IND depth and “x”s.
- PSNR Peak Signal to Noise Ratio
- YCbCrD coding shows the quality of reconstructed view by YCbCrD coding, shown by YUVD view and diamonds. This is because the estimated depth map is not consistent in time, as can be seen in FIGS. 8 a and 8 b. Also, in YCbCrD coding, the encoder is not fully optimized to handle the temporal inconsistency for depth which is regarded as only an additional channel in YCbCrD sequence.
- FIGS. 8A-8B illustrate the depth of Lovebird 1 , View 2 in time 0 and time 1 . Also note that in FIG. 8B , object boundaries in the estimated depth map are noisy and not aligned with object boundaries in view. Note in FIG. 8 , that red circled areas belong to static background in view but have different intensities in depth. Besides red circled area, temporal inconsistencies can be found easily.
- FIGS. 9A and 9B shows RD curves of synthesized views for Lovebird 1 and Pantomime. Because intermediate view is synthesized by two neighboring views, bit rates for two neighboring views are added and used in the plot. For distortion, PSNR of synthesized view is used. The quality of synthesized view by YCbCrD coding is similar to that of independent coding in RD sense. In FIGS. 7A-7D , it has been shown that the decoded left and right views have similar quality in RD. Thus, the combined coding and separate coding have similar results in RD sense for key views and synthesized view. Note that depth maps are used to synthesize views and are not displayed for viewing.
- YCbCrD coding provides a level of ease in implementation and provides for backward compatibility to existing coding standards in single bit stream.
- YCbCrD coding can be used as an extended format for depth coding and implemented easily in the conventional video coding standards.
- Table 9 the percentage of combined YCbCrD coding in each sequence is shown for different QPs. Note that in lower bit rates (higher QP), combined YCbCrD coding is preferred.
- Table 10 coding results of view and depth are shown for each sequence with IPPP and IBBP coding structure.
- RD calculation method by Bjontegaard Gisle Bjontegaard, “Calculation of Average PSNR Differences between RD curves”, ITU-T SC16/Q6, 13th VCEG Meeting, Austin, Tex., USA, April 2001, Doc. VCEG-M33 was used. Note that we achieved about 6% gains in depth by IPPP and about 5% gains in view by IBBP by our YCbCrD coding scheme.
- FIGS. 5-6 may be contained as a utility, program, or subprogram, in any desired computer readable storage medium, which may be a non-transitory medium.
- the operations may be embodied by computer programs, which can exist in a variety of forms both active and inactive.
- they may exist as software program(s) comprised of program instructions in source code, object code, executable code or other formats. Any of the above may be embodied on a computer readable storage medium, which include storage devices.
- Exemplary computer readable storage media include conventional computer system RAM, ROM, EPROM, EEPROM, and magnetic or optical disks or tapes. Concrete examples of the foregoing include distribution of the programs on a CD ROM or via Internet download. It is therefore to be understood that any electronic device capable of executing the above-described functions may perform those functions enumerated above.
- the invention allows 3D encoding of a depth parameter jointly with view information.
- the invention allows for compatibility with 2D and may provide optimized encoding based on the RD costs in encoding depth jointly with view or separately.
- depth as a video component from the beginning thus, in inter prediction, block mode and reference index are shared between view and depth in addition to motion vector.
- intra prediction intra prediction mode can be shared also.
- the coding result of combined coding can be further optimized by considering depth information together with view. In the separate coding of view and depth, depth is coded independently to the view. It is also possible to have intra coded depth while view is inter coded.
Abstract
Description
- This application claims the benefit of U.S. Provisional Application 61/263,516 filed on Nov. 23, 2009, which is herein incorporated by reference in its entirety.
- The present invention relates to depth coding in a video image, such as in a 3D video image.
- 3D is becoming an attractive technology again, and this time it is gaining supports from content providers. Most of new animation movies and many films will be released also with 3D capability and can be watched in 3D movie theaters widespread across the country. Also there were several tests for real time broadcast of sports event, e.g., NBA and NFL games. To make 3D perceived in flat screens, stereopsis is used, which mimics human visual system and shows left and right views captured by stereo cameras to left and right eye, respectively. Therefore, it requires twice the bandwidth required for 2D sequences. 3D TV (3DTV) or 3D video (3DV) is the application which uses stereopsis to deliver 3D perception to viewers. However, because only two views for each eye are delivered in 3DTV, users can not change the view point which is fixed by contents provider.
- Free viewpoint TV (FTV) is another 3D application which enables users to navigate through different view points and choose the one they want to watch. To make multiple viewpoints available, multi-view video sequences are transmitted to users. Actually, stereo sequences required for 3DTV can be regarded as a subset of multi-view video sequences if the distance between neighboring views satisfies the conditions for stereopsis. Because the amount of data increases linearly according to the number of views, multi-view video sequences need to be compressed efficiently for wide spread use.
- As an effort to reduce bitrates of multi-view video sequences, JVT had been working on multi-view video coding (MVC) and finalized it as an amendment to H.264/AVC. In MVC, multi-view video sequences are encoded using both temporal and cross-view correlations for higher coding efficiency while increasing dependency between frames both in time and across views. Therefore, when users want to watch a specific view, unnecessary views should be decoded according to the dependency. Furthermore, compression efficiency of MVC is not satisfactory when there are geometric distortions by camera disparity and the correlation between neighboring views is small.
- In accordance with the principles of the invention, an apparatus of the invention may comprise an encoder configured to encode the video data by encoding a combined set of view data and depth data. The combined set of view data and depth data may include one of: RGBD, YUVD, or YCbCrD. The combined set of view data and depth data may be contained in at least one of: a group of pictures, a picture, a slice, a group of blocks, a macroblock, or a sub-macroblock. The apparatus may further comprise a depth format unit configured to identify a depth format of the video data. The encoder may select to encode the video data as a plurality of two dimensional images without including depth data when the depth format is set to 0, or the encoder may select to encode the video data as a combined set of view data and depth data when the depth format is set to a predetermined level. The encoder may further include a coding cost calculator which determines coding costs of joint encoding of said combined set of view data and depth data and separate encoding of said combined set of view data and depth data, and determines an encoding mode between joint encoding and separate encoding based on said coding cost. The encoder may encode the video data as a joint encoding of view data and depth data when the encoding cost is less than an encoding cost of separately encoding the view data and depth data. The video data may be one of a: multiview with depth, multiview without depth, single view with depth, single view without depth.
- In accordance with the principles of the invention, a method of encoding video data may comprise encoding the video data by encoding a combined set of view data and depth data at an encoder. The combined set of view data and depth data may include one of: RGBD, YUVD, or YCbCrD. The combined set of view data and depth data is contained in at least one of: a group of pictures, a picture, a slice, a group of blocks, a macroblock, or a sub-macroblock. The method may further comprise identifying a depth format of the video data. The video data may be encoded as a plurality of two dimensional images without including depth data when the depth format is set to 0. The video data may be encoded as a combined set of view data and depth data when the depth format is set to a predetermined level. The method may further include determining a coding cost of joint encoding said combined set of view data and depth data and separate encoding of said combined set of view data and depth data, and determining an encoding mode between joint encoding and separate encoding based on said coding cost. The video data may be encoded as a joint encoding of view data and depth data when the encoding cost is less than an encoding cost of separately encoding the view data and depth data. The video data may be one of a: multiview with depth, multiview without depth, single view with depth, single view without depth.
- In accordance with the principles of the invention, a non-transitory computer readable medium carrying instructions for an encoder to encode video data, may comprise instructions to perform the step of: encoding the video data by encoding a combined set of view data and depth data. The combined set of view data and depth data may include one of: RGBD, YUVD, or YCbCrD. The combined set of view data and depth data is contained in at least one of: a group of pictures, a picture, a slice, a group of blocks, a macroblock, or a sub-macroblock. The instructions may further comprise identifying a depth format of the video data. The video data may be encoded as a plurality of two dimensional images without including depth data when the depth format is set to 0. The video data may be encoded as a combined set of view data and depth data when the depth format is set to a predetermined level. The instructions may further include determining a coding cost of joint encoding said combined set of view data and depth data and separate encoding of said combined set of view data and depth data, and determining an encoding mode between joint encoding and separate encoding based on said coding cost. The video data may be encoded as a joint encoding of view data and depth data when the encoding cost is less than an encoding cost of separately encoding the view data and depth data. The video data may be one of a: multiview with depth, multiview without depth, single view with depth, single view without depth.
- In accordance with the principles of the invention, an apparatus for decoding video data may comprise: a decoder configured to decode the video data by decoding a combined set of view data and depth data. The combined set of view data and depth data may include one of: RGBD, YUVD, or YCbCrD. The combined set of view data and depth data may be contained in at least one of: a group of pictures, a picture, a slice, a group of blocks, a macroblock, or a sub-macroblock. The apparatus may further comprise a depth format unit configured to identify a depth format of the video data. The decoder may select to decode the video data as a plurality of two dimensional images without including depth data when the depth format is set to 0. The decoder may select to decode the video data as a combined set of view data and depth data when the depth format is set to a predetermined level. The decoder may selectively jointly decodes said combined set of view data and depth data when said combined set was jointly encoded or decodes said combined set of view data and depth data when said combined set was separately encoded. The video data may be one of a: multiview with depth, multiview without depth, single view with depth, single view without depth.
- In accordance with the principles of the invention, a method decoding video data comprising: decoding the video data by decoding a combined set of view data and depth data at a decoder. The combined set of view data and depth data may include one of: RGBD, YUVD, or YCbCrD. The combined set of view data and depth data is contained in at least one of: a group of pictures, a picture, a slice, a group of blocks, a macroblock, or a sub-macroblock. The method may further comprise identifying a depth format of the video data. The video data may be decoded as a plurality of two dimensional images without including depth data when the depth format is set to 0. The video data may be decoded as a combined set of view data and depth data when the depth format is set to a predetermined level. The method may further include selectively jointly decoding said combined set of view data and depth data when said combined set was jointly encoded or decoding said combined set of view data and depth data when said combined set was separately encoded. The video data may be one of a: multiview with depth, multiview without depth, single view with depth, single view without depth.
- In accordance with the principles of the invention, a non-transitory computer readable medium may carrying instructions for an decoder to decode video data, comprising instruction to perform the steps of: decoding the video data by encoding a combined set of view data and depth data. The combined set of view data and depth data may include one of: RGBD, YUVD, or YCbCrD. The combined set of view data and depth data is contained in at least one of: a group of pictures, a picture, a slice, a group of blocks, a macroblock, or a sub-macroblock. The instructions may further comprise identifying a depth format of the video data. The video data may be decoded as a plurality of two dimensional images without including depth data when the depth format is set to 0. The video data may be decoded as a combined set of view data and depth data when the depth format is set to a predetermined level. The instructions may further include selectively jointly decoding said combined set of view data and depth data when said combined set was jointly encoded or decoding said combined set of view data and depth data when said combined set was separately encoded. The video data may be one of a: multiview with depth, multiview without depth, single view with depth, single view without depth.
- The invention allows 3D encoding of a depth parameter jointly with view information. The invention allows for compatibility with 2D and may provide optimized encoding based on the RD costs in encoding depth jointly with view or separately. Also, from the new definition of video format, we provide an adaptive coding method of 3D video signal. During the combined coding of YCbCrD in the adaptive coding of 3D signal, we treat depth as a video component from the beginning thus, in inter prediction, block mode and reference index are shared between view and depth in addition to motion vector. In intra prediction, intra prediction mode can be shared also. Note that the coding result of combined coding can be further optimized by considering depth information together with view. In the separate coding of view and depth, depth is coded independently to the view. It is also possible to have intra coded depth while view is inter coded.
-
FIG. 1 illustrates an end-to-end 3D/FTV system. -
FIG. 2 illustrates an approach for depth estimation. -
FIGS. 3A-3D illustrate a sample video image in various forms. -
FIG. 4 illustrates an encoder and decoder arrangement in accordance with the principles of the invention. -
FIG. 5 illustrates a flowchart of RD optimization (RDO) in each macroblock between combined coding and separate coding in accordance with the principles of the invention. -
FIG. 6 illustrates a flowchart for adaptive coding of 3D video in accordance with the principles of the invention. -
FIGS. 7A-7D illustrate a sample image and a chart of PSNR of view and depth. -
FIGS. 8A and 8B illustrate the depth ofLovebird 1,View 2 intime 0 andtime 1. -
FIG. 9A and 9B show RD curves of synthesized views for Lovebird1 and Pantomime. -
FIGS. 10A and 10B illustrate luma and depth of Lovebirds fromFIG. 3 . -
FIGS. 11A and B illustrate other sample images, includingLovebird 2 and Pantomine. - For simplicity and illustrative purposes, the present invention is described by referring mainly to exemplary embodiments thereof. In the following description, numerous specific details are set forth to provide a thorough understanding of the present invention. However, it will be apparent to one of ordinary skill in the art that the present invention may be practiced without limitation to these specific details. In other instances, well known methods and structures have not been described in detail to avoid unnecessarily obscuring the present invention.
-
FIG. 1 shows an exemplary diagram for end-to-end 3D/FTV system. As shown inFIG. 1 , multiple views are captured of a scene orobject 1 bymultiple cameras 2. The captured views by themultiple cameras 2 are corrected or rectified and sent to a processor andstorage system 7 prior to transmission by atransmitter 3. The processor may include an encoder which encodes the image data into a specified format. At the encoder, multiple views are available which can be used to estimate depth more efficiently and correctly. - As illustrated in
FIG. 1 , a user's side generally includes areceiver 6 which receives the transmitted and encoded images fromtransmitter 3. The received data is proved to a processor/buffer which typically includes a decoder. The decoded and otherwise processed image data is provided to display 5 for viewing by the user. - MPEG started to search for a new standard for multi-view video sequence coding. In MPEG activity, depth information is exploited to improve overall coding efficiency. Instead of sending all multi-view video sequences, sub-sampled views, 2 or 3 key views are sent with corresponding depth information and intermediate views are synthesized using key views and depths. Depth is assumed to be estimated (if not captured) before compression at the encoder and intermediate views are synthesized after decompression at the decoder. Note that not all captured views are compressed and transmitted in this scheme.
- To define suitable reference techniques, four exploration experiments (EE1-EE4) have been established in MPEG. EE1 explores depth estimation from neighboring views and EE2 explores view synthesis techniques which synthesize intermediate views using estimated depth from EE1. EE3 searched techniques for generation of intermediate views based on layered depth video (LDV) representation. EE4 explores how the depth map coding affects the quality of synthesized views.
- In
FIG. 2 , EE1 for depth estimation and EE2 for view synthesis are described. For multi-view sequences, e.g., fromView 1 to 5, shown inrow 21 inFIG. 2 , any two views can be selected to estimate depth between them. For example,View 1 andView 5 are used to estimateDepth 2 andDepth 4, shown inrow 23. Then View 2,Depth 2,View 4 andDepth 4 can be encoded and transmitted to the users and intermediate views betweenView 2 andView 4 can be synthesized usingDepth 2 andDepth 4 with corresponding camera parameters. InFIG. 2 ,View 3 is synthesized, shown inrow 25, and compared withoriginal View 3. - In O. Stankiewicz, K. Wegner and K. Klimaszewski, “Results of 3DV/FTV Exploration Experiments, described in w10173,” ISO/IEC JTC1/SC29/WG11 MPEG Document M16026, Lausanne, Switzerland, February 2009, it was observed that the quality of synthesized view depends more on the quality of encoded view than on the quality of encoded depth. In S. Tao, Y. Chen, M. Hannuksela and H. Li, “Depth Map Coding Quality Analysis for View Synthesis,” ISO/IEC JTC1/SC29/WG11 MPEG Document M16050, Lausanne, Switzerland, February 2009, view is synthesized depending on depth that is encoded in different bit rates. They provided rate and distortion (R-D) curves where rate is shown in Kbps for depth coding and distortion is shown in PSNR for synthesized view. As can be seen in Tao, et al., the quality of synthesized view does not change significantly in most range of bit rates for depth. In C. Cheng, Y. Huo and Y. Liu, “3DV EE4 results on Dog sequence,” ISO/IEC JTC1/SC29/WG11 MPEG Document M16047, Lausanne, Switzerland, February 2009, multi-view video coding (MVC) is used to encode stereo views and depths and compared with coding results when H.264/AVC is used to encode each view independently. MVC showed less than 5% coding gains compared to simulcast by H.264/AVC. For depth compression, in B. Zhu, G. Jiang, M. Yu, P. An and Z. Zhang, “Depth Map Compression for View Synthesis in FTV,” ISO/IEC JTC1/SC29/WG11 MPEG Document M16021, Lausanne, Switzerland, February 2009, depth is segmented and different regions are defined as edge (A), motion (B), inner part of moving object (C) and background (D). Depending on the region type, different block modes are applied, which resulted in less encoding complexity and improved coding efficiency in depth compression.
- During the 2D video capture, scenes or objects in 3D space are projected into image plane of the camera, where the pixel intensity represents the texture of the objects. In depth map, pixel intensity represents the distance of the corresponding 3D objects to/from the image plane. Therefore, both view and depth are captured (or estimated for depth) for the same scene or objects thus, they share the edges or the contours of the objects.
FIG. 3 a shows theoriginal view 0,FIG. 3 b-3 d show the corresponding Cb, Cr and depth of the sequence, Lovebirds, from ETRI/MPEG Korea Forum “Call for Proposals on Multi-view Video Coding,” ISO/IEC JTC1/SC29/WG11 MPEG Document N7327, Poznan, Poland, July 2005, herein incorporated by reference.FIGS. 11A and 11B show other views, includingLovebird 2View 7 andPantomime View 37. With reference toFIG. 3 b-3 d, from the comparison of Cb/Cr with depth, it can be seen that both Cb/Cr and depth share object boundary. For example, an image is segmented based on color for the disparity (depth) estimation because color channel shares the information of object boundaries G. Um, T. Kim, N. Hur, and J. Kim, “Segment-based Disparity Estimation using Foreground Separation,” ISO/IEC JTC1/SC29/WG11 MPEG Document M15191, Antalya, Turkey, January 2008. - According to O. Stankiewicz et al., Tao et al., Cheng, et al. and Zhu et al., it can be derived that the quality of depth does not change the quality of synthesized view significantly. However, all the results in these contributions are obtained using MPEG reference software for depth estimation and view synthesis which are often not the state-of-the-art technology. Estimated depths are often different even for the same smooth objects and temporal inconsistencies are easily observed. Therefore, it can not be concluded that the quality of the synthesized view does not depend on the quality of the depth. Furthermore, 8 bit depth quality currently assumed in MPEG activity may not be enough considering that 1 pixel error around object boundary in view synthesis may results in different synthesis results.
- However with all these uncertainties, depth should be encoded and transmitted with view for 3D services and an efficient and flexible coding scheme needs to be defined. Noting that the correlation between view and depth can be exploited, just as the correlation between luma and chroma is exploited during the transition from monochrome to color, we provide a new flexible depth format and coding scheme which is backward compatible and suitable for different objectives of new 3D services. The determination of the depth data may be performed by the techniques discussed above or another suitable approach.
- We treat depth as an additional component to conventional 2D video format, making a new 3D video format. Thus, for example, RGB or YCbCr format is expanded to RGBD or YCbCrD to include depth. In H.264/AVC, the format for monochrome or color can be selected by chroma_format_idc flag. Similarly we may use a depth_format_idc flag to specify if a signal is 2D or 3D. Table 1 shows how to use chroma_format_idc and depth_format_idc to signal video format in 2D/3D and monochrome/color.
-
TABLE 1 Different video format defined by depth— format_idc and chroma_format_idc Video format depth_format_idc chroma_format_idc 2D Monochrome 0 0 Color 0 >0 3D Monochrome >0 0 Color >0 >0 - In the extended video format definition, there would be the better grouping of channels for compression e.g., depending on the resolution of each channel or correlations among them. Table 2 exemplifies how video components can be grouped to exploit the correlation among them.
Index 0 means YCbCrD are all grouped together and encoded by the same block mode. This is the case that the same motion vector (MV) or the same intra prediction direction is used for all channels. Forindex 1, depth is encoded separately to view.Index 5 specifies each channel is encoded independently. -
TABLE 2 Grouping of Components for Compression. Group index Y (R) Cb (G) Cr (B) D 0 0 0 0 0 1 0 0 0 1 2 0 1 1 1 3 0 1 1 0 4 0 1 1 2 5 0 1 2 3 The same number means they are grouped together for encoding/decoding. - Depending on the correlations between each channel, channels can be grouped differently. For example, assume that YUV420 is used for the view and depth is quite smooth, thus the same resolution to chroma is enough for depth signal. Then, Cb, Cr and D can be treated as a group and Y as another group. Then
group index 2 can be used assuming Cb, Cr and D can be similarly encoded without affecting overall compression efficiency. If the resolution of depth is equal to that of luminance in YUV420 format and depth needs to be coded in high quality,group index 1 orgroup index 4 can be used. If there is enough correlation between Y and D,group index 3 can be used additionally. In the next, we assume two different applications for 3D and show how we can exploit the correlation between view and depth under the new video signal format. Note that the approaches explained next can be similarly applied to different combination of groups. - First, we assume that estimated depth quality is not accurate enough or is not required to be accurate thus, basic depth information e.g., the object boundaries and approximate depth values would be satisfactory for required view synthesis quality. Depth estimation or 3D services in mobile devices can be an example of this case, where the highest priority would be a less complex depth coding. Second, for 3D services in HD quality, high quality depth information would be required and coding efficiency would be the highest priority.
- In one implementation using H.264/AVC for 2D view compression, depth_format_idc may be defined in Table 3 to specify additional picture format YCbCrD. If sequence does not have depth for 3D application, it is set to 0 and sequence is encoded by standard H.264/AVC. If sequence carries depth channel, depth can be encoded in the same size to luma (Y) when depth format is ‘D4’ or encoded in the same size to chroma (Cb/Cr) when depth format is ‘D1’ where the width and height of D1 can be half of D4 or equal to D4 depending on SubWidthC and SubHeightC, respectively. The associated syntax change in sequence parameter set of H.264/AVC is shown in Table 4. Those of skill in the art will appreciate that the encoder preferably sets the various syntax values in Table 4 during an encoding process, and the decoder may use the values during the decoding process.
-
TABLE 3 SubWidthD and SubHeightD derived from depth_format_idc depth_format_idc Depth Format SubWidthD SubHeightD 0 No depth (2D) — — 1 D1 SubWidthC SubHeightC 2 D4 1 1 -
TABLE 4 Sequence parameter set RBSP syntax. seq_parameter_set_rbsp( ) { C Descriptor profile_idc 0 u(8) constraint_set0_flag 0 u(1) constraint_set1_flag 0 u(1) constraint_set2_flag 0 u(1) constraint_set3_flag 0 u(1) reserved_zero_4bits /* equal to 0 */ 0 u(4) level_idc 0 u(8) seq_parameter_set_id 0 ue(v) if( profile_idc = = 100 || profile_idc = = 110 || profile_idc = = 122 || profile_idc = = 144 ) { chroma_format_idc 0 ue(v) depth_format_idc 0 ue(v) if( chroma_format_idc = = 3 ) residual_colour_transform_flag 0 u(1) bit_depth_luma_minus8 0 ue(v) bit_depth_chroma_minus8 0 ue(v) if( depth_format_idc > 0) bit_depth_depth_minus8 0 ue(v) qpprime_y_zero_transform_bypass_flag 0 u(1) if( vui_parameters_present_flag ) vui_parameters( ) 0 rbsp_trailing_bits( ) 0 } Added syntaxes are ‘depth_format_idc’. - Assuming depth values may be mapped by a 8 bit signal, to specify the bit depth of the samples of the depth array and the value of the depth quantization parameter range offset QpBdOffsetD, bit_depth_depth_minus8 is added in the sequence parameter set as shown in Table 4. BitDepthD and QpBdOffsetD are specified as;
-
BitDepthD=8+bit_depth_depth_minus8 (1) -
QpBdOffsetD=6*bit_depth_depth_minus8 (2) - Note that if the depth values are decided to be represented by N bits basically, the equation can be changed accordingly, for example, BitDepthD=N+bit_depth_depth_minusN.
- To control the quality of encoded depth independent to YCbCr coding, depth_qp_offset is present in picture parameter set syntax when depth_format_idc>0. In Table 5, associated syntax change in H.264/AVC is shown. The value of QPD for depth component is determined as follows;
- The variable qDoffset for depth component is derived as follows.
-
qD offset=depth— qp_offset (3) - The value of QPD for depth component is derived as
-
QP D=Clip3(−QpBdOffsetD, 51, QP Y +qD Offset) (4) - The value of QP′D for the depth components is derived as
-
QP′ D =QP D +QpBdOffsetD (5) -
TABLE 5 Picture parameter set RBSP syntax. pic_parameter_set_rbsp( ) { C Descriptor pic_parameter_set_id 1 ue(v) seq_parameter_set_id 1 ue(v) entropy_coding_mode_flag 1 u(1) pic_order_present_flag 1 u(1) num_slice_groups_minus1 1 ue(v) if( num_slice_groups_minus1 > 0 ) { } num_ref_idx_10_active_minus1 1 ue(v) num_ref_idx_11_active_minus1 1 ue(v) weighted_pred_flag 1 u(1) weighted_bipred_idc 1 u(2) pic_init_qp_minus26 /* relative to 26 */ 1 se(v) pic_init_qs_minus26 /* relative to 26 */ 1 se(v) chroma_qp_index_offset 1 se(v) if (depth_format_idc > 0) depth_qp_offset 1 se(v) } Modified syntax is ‘depth_qp_offset’. - The block coding may include using macroblocks or multiples of macroblocks, e.g. MB pairs. A YCrCbD MB may consist of Y 16×16, Cr 8×8, Cb 8×8 and D 8×8, for example. However, various block sizes may be used for each of Y, Cr, Cb and D. For example, D may have a size of 8×8 or 16×16.
- Next, YCbCrD coding schemes for depth format D1 and D4 are explained. In one implementation for depth format D1, we encode depth map in a similar way that chroma is coded in H.264/AVC exploiting the correlation between Cb/Cr and D. For the implementation of depth coding, such as in H.264/AVC, depth is treated as if were a third chroma channel, Cb/Cr/D. Therefore, the same block mode, intra prediction direction, motion vector (MV) and reference index (refIdx) are applied to Cb/Cr and D. Also coded block pattern (CBP) in H.264/AVC is redefined in Table 6 to include CBP of depth. For example, when deciding intra prediction direction for chroma, depth cost is added to calculate total cost for Cb/Cr/D and depth shares the same intra prediction direction with Cb/Cr. In block mode decision at the encoder, rate-distortion (RD) cost of depth is added to total RD cost for YCbCr, thus mode decision is optimized for both view and depth. The only information not shared with Cb/Cr is the residual of depth, which is encoded after residual coding of Cb/Cr depending on CBP.
-
TABLE 6 Specification of modified CodedBlockPatternChroma values CodedBlockPatternChroma Description 0 All chroma/depth transform coefficient levels are equal to 0. 1 One or more chroma/depth DC transform coefficient levels shall be non-zero valued. All chroma/depth AC transform coefficient levels are equal to 0. 2 Zero or more chroma/depth DC transform coefficient levels are non-zero valued. One or more chroma/depth AC transform coefficient levels shall be non-zero valued. - When the computational power for depth estimation is limited, e.g., in mobile devices or real time depth estimation is required, it might be difficult to estimate a full resolution depth map equal to the original frame size or the estimated depth might not be accurate with incorrect information or noisy depth values around object boundaries. When estimated depth is not accurate, it might not be necessary to encode noisy depth in high bit rates. In I. Radulovic and P. Fröjdh, “3DTV Exploration Experiments on Pantomime sequence,” ISO/IEC JTC1/SC29/WG11 MPEG Document M15859, Busan, Korea, October 2008, it is shown that as the smoothing coefficient in depth estimation reference software (DERS) increases, less detailed and less noisy depth maps were obtained resulting in better qualities of synthesized views. In this case, our objective would be the simplicity of depth coding. We encode depth map in a similar way that chroma is coded in H.264/AVC exploiting the correlation between Cb/Cr and D. Next, we show how coding information can be shared between Cb/Cr and depth in the implementation in H.264/AVC.
-
FIG. 4 illustrates an apparatus for estimating or simulating depth coding in accordance with the invention. For given sequences, we use aDERS 41 module for depth estimation and then downsample the depth map by 2 both horizontally and vertically using adown sampling module 42, such as a polynorm filter David Baylon, “Polynorm Filters for Image Resizing: Additional Considerations,” Motorola, Home & Networks Mobility, Advanced Technology internal memo DSM2008-072r1, December 2008. The downsampled depth map has the same resolution as the chroma channels in YUV 4:2:0 format. As a baseline, view and depth are coded separately byencoders 48, which may be two H.264/AVC encoders, thus 2 independent bit streams are generated. While two encoders are illustrated for the baseline encoding, those of skill in the art will appreciate that the same (a single) encoder may be used for the baseline encoding processes. As a D1 encoding scheme, view and depth are coded jointly byencoder 44 to create a single bit stream. - The encoded image may be provided to a downstream transmitter 3 (see,
FIG. 1 ) and transmitted to a remotely locateddecoder 45, as generally shown by the direction arrow inFIG. 4 . Those of skill in the art will appreciate that the encoder may be in a network element, e.g. a headend unit, the decoder may be in a user device, e.g. a set top box. The decoder decodes and reconstructs the view and depth parameters. Reconstructed depths are upsampled to the original size using and upsampler 46, such as a polynorm filter again in Baylon's approach, and fed into aview synthesis module 47, which may include view synthesis reference software (VSRS) with reconstructed views to synthesize additional views. Because combined YCbCrD coding generates single bit stream for both view and depth, bit rates of 2 bit streams in separate codings (YCbCr+D) are summed and compared with bit rates of YCbCrD coding. - The encoding may be performed with Y and RD optimization. In one implementation for depth format D4, we target coding efficiency of overall YCbCrD sequences exploiting the correlation between view and depth. Because depth resolution is equal to luma, Y, instead of Cb/Cr, coding information of Y is shared for efficient depth coding.
FIGS. 10A and 10B show luma and depth of Lovebirds fromFIG. 3 . Although the similarity in object shapes and boundaries can be observed, it is still possible that the best match minimizing distortion can be found in the different locations for Y and D, respectively. For example, inFIG. 10A and 10B , the best match of the grass in Y might not be the best match in D because the texture of the grass repeats in Y while depth of grass looks noisy. Therefore, instead of sharing coding information over the whole picture, we may select whether to share coding information of Y with depth or not in coding each macroblock depending on the RD cost between combined coding (share) and separate coding (not share) of view and depth. -
FIG. 5 illustrates a flowchart of rate distortion optimization (RDO) in each macroblock between combined coding and separate coding. A macroblock (MB) is received in step S1. View and depth is encoded as a combined YCbCrD and calculated RD cost in step S3, RDcost(YCbCrD). The best coding information found is saved, including intra prediction mode, motion vector and reference index, such as for both the joint coding of view and depth and the independent coding of view and depth. The view and depth are encoded independently and the individual RD cost, RDcost(YCbCr) and RDcost(D) are calculated, steps S5 and S7. We compare RDcost(YCbCrD) and ‘RDcost(YCbCr)+RDcost(D)’, step S11. The one with the minimum RD cost for the current macroblock is selected. That is, if the RD cost of the combined YCbCrD is less than the RD cost of the separate RD(YCbCr)+RD(D), the MB is updated with the combined results (YCbCrD), step S15. If the RD cost of the combined YCbCrD is not less than the RD cost of the separate RD(YCbCr)+RD(D), the MB is updated with the separate results (YCbCr and D), step S13. The next MB is taken to be processed in step S17. Two separate coded block information for YCbCr and D may be maintained, respectively, as a reference to encode future macroblocks. - When combined YCbCrD coding is applied, the similarities of the edges and the contours of objects in Y and D are exploited by sharing block mode, intra prediction direction, MV and refIdx. However the textures of Y and D are not similar in general therefore, coded block pattern (CBP) and residual information are not shared in the combined coding. Table 7 summarizes shared and non-shared information in YCbCrD combined coding.
-
TABLE 7 Shared and non-shared information in YCbCrD combined coding Non-shared Shared information information INTRA Block mode, intra CBP, Residual prediction direction INTER Block mode, CBP, Residual (16 × 16, . . . , 8 × 8) MV, RefIdx - To signal whether combined coding or separate coding is used in each macroblock, mb_YCbCrD_flag is introduced as a new flag which can be 0 or 1 indicating separate or combined coding, respectively. This flag may be encoded by CABAC and three contexts are defined by mb_YCbCrD_flag from the neighboring left and upper blocks. The context index c for current MB is defined as follows;
-
c=mb_YCbCrD_flag (in the left MB)+mb_YCbCrD_flag (in the upper MB) - Under this approach, we provide a new video format which is compatible with conventional 2D video thus can be used for both 2D and 3D video signal. If 3D video signal, e.g., YCbCrD, is sent, depth is included as a video component. If only 2D video signal, e.g., YCbCr, is sent without depth, 2D video can be sent with depth_format_idc equal to 0 specifying there is no depth component.
- Also, from the new definition of video format, we provide an adaptive coding method of 3D video signal. During the joint coding of YCbCrD in the adaptive coding of 3D signal, we treat depth as a video component from the beginning thus, in inter prediction, block mode and reference index are shared between view and depth in addition to motion vector (MV). In intra prediction, intra prediction mode can be shared also. Note that the coding result of combined coding can be further optimized by considering depth information together with view. In the separate coding of view and depth, depth is coded independently to the view. For example, depth can be encoded/decoded by 16×16 inter block mode while view is coded as 8×8 inter block mode. It is also possible to have intra coded depth while view is inter coded. Note that RD optimized adaptive coding is possible by treating depth as an additional channel to view, not by re-using MV from view to depth.
- Combining foregoing,
FIG. 6 shows a flowchart for adaptive coding of 3D video in accordance with the invention. The processes starts at step S20. As shown in step S22, with the depth_format_idc flag equal to 0, video signal is treated as 2D and conventional 2D encoding (e.g. H.264/AVC,MPEG 2, or H.265/HEVC) is used, step S24. If the depth_format_idc flag is 1, depth is encoded as if it is a third chroma channel, which is the same resolution as for the chroma, step S28. With the depth_format_idc flag equal to 2, depth is the same resolution as the luma and adaptive joint/separate coding is applied to view and depth based on RD cost (steps S26). As shown inFIG. 6 , the RD cost may be determined according to the process shown inFIG. 5 . Note that we showed how the adaptive coding can be applied betweengroup index - For the D1 approach discussed above, which provides simplicity in depth coding, based on the observation of the correlation between view and depth, we extend the current YCbCr sequence format into YCbCrD so that depth can be treated and encoded as an additional channel to view. From this extended format, we showed two different compression schemes of YCbCrD. With depth format D1, depth is encoded in H.264/AVC, sharing coding information with Cb/Cr, therefore additional encoder complexity is negligible and overall encoder complexity is similar to the original H.264/AVC. In depth format D4, depth can be encoded sharing coding information with Y. Noting that the best predictions for Y and D can be different even for the same object, combined coding or separate coding of YCbCr and D is decided by RD cost of each approach.
- In the experimental results with depth format D1 and D4, it was verified that our encoding method for depth achieves the goals, less complex encoder for depth format D1 and higher coding efficiency for depth format D4.
- The YCbCrD coding in depth format D1 was implemented in a Motorola H.264/AVC encoder (Zeus) and compared with independent coding of YCbCr and depth. We used
View e.g. View FIG. 2 .View 3 in Lovebird1 is synthesized and the qualities of synthesized views are compared with the original views. Original Lovebird1 sequence is YUV 4:2:0 format and depth_format_idc is set to 1, thus depth array has the same size as Cb and Cr. - In
FIGS. 7A-7D the Peak Signal to Noise Ratio (PSNR) of view and depth are shown with respect to total bit rate for Lovebird1 and Pantomime, respectively. Images forLovebird 2 and Pantomime may be found inFIGS. 11A and 11B , respectively. More specifically,FIG. 7A and 7B illustrates a chart of PSNR vs total bit rate for theimage Loverbird 1, andFIGS. 7C and 7D illustrates a chart for Pantomime. The charts illustrate that the quality of reconstructed depth by YCbCrD coding, shown by YUVD depth and triangles, is worse than that by independent depth coding, shown by IND depth and “x”s. However, the quality of reconstructed view by YCbCrD coding, shown by YUVD view and diamonds, is similar to that by independent coding, shown by IND view and squares. This is because the estimated depth map is not consistent in time, as can be seen inFIGS. 8 a and 8 b. Also, in YCbCrD coding, the encoder is not fully optimized to handle the temporal inconsistency for depth which is regarded as only an additional channel in YCbCrD sequence. -
FIGS. 8A-8B illustrate the depth ofLovebird 1,View 2 intime 0 andtime 1. Also note that inFIG. 8B , object boundaries in the estimated depth map are noisy and not aligned with object boundaries in view. Note inFIG. 8 , that red circled areas belong to static background in view but have different intensities in depth. Besides red circled area, temporal inconsistencies can be found easily. -
FIGS. 9A and 9B shows RD curves of synthesized views for Lovebird1 and Pantomime. Because intermediate view is synthesized by two neighboring views, bit rates for two neighboring views are added and used in the plot. For distortion, PSNR of synthesized view is used. The quality of synthesized view by YCbCrD coding is similar to that of independent coding in RD sense. InFIGS. 7A-7D , it has been shown that the decoded left and right views have similar quality in RD. Thus, the combined coding and separate coding have similar results in RD sense for key views and synthesized view. Note that depth maps are used to synthesize views and are not displayed for viewing. However, the combined YCbCrD coding provides a level of ease in implementation and provides for backward compatibility to existing coding standards in single bit stream. YCbCrD coding can be used as an extended format for depth coding and implemented easily in the conventional video coding standards. - For the D4 approach discussed above, which provides encoding efficiency, three sequences, provided by MPEG, Lovebird1, Lovebird2 and Pantomime were tested, which are MPEG sequences and depths are estimated by DERS. As a baseline, H.264/AVC is used to code view and depth separately and bit rates are added to get total bit rate for view and depth. Table 8 shows how many bits are required for independent coding of view and depth, respectively. The ratio of bits for depth and view ranges from 4.5% to 98%. Estimated depths for Lovebird1 and Lovebird2 are noisier than Pantomime and views are relatively static in time (not fast motion). Therefore, relatively more bits are needed for depth coding and less bits needed for view coding.
-
TABLE 8 Ratio of bits required to encode depth and view (IPPP by Zeus) QP 22 QP 27 QP 32QP 37Average Lovebird1 Bit (depth, Kbps) 1159 536 236 106 509.25 View 2Bit (view, Kbps) 2356 1141 561 297 1088.75 Bit(depth)/Bit(view) 49.19% 46.98% 42.07% 35.69% 46.77% Lovebird2 Bit (depth) 925 490 247 126 447 View 7Bit (view) 958 487 245 131 455.25 Bit(depth)/Bit(view) 96.56% 100.62% 100.82% 96.18% 98.19% Pantomime Bit (depth) 273 142 80 51 136.5 View 37Bit (view) 6248 3223 1768 1029 3067 Bit(depth)/Bit(view) 4.37% 4.41% 4.52% 4.96% 4.45% - In Table 9, the percentage of combined YCbCrD coding in each sequence is shown for different QPs. Note that in lower bit rates (higher QP), combined YCbCrD coding is preferred. In Table 10, coding results of view and depth are shown for each sequence with IPPP and IBBP coding structure. To calculate gains for bit rate and distortion, RD calculation method by Bjontegaard Gisle Bjontegaard, “Calculation of Average PSNR Differences between RD curves”, ITU-T SC16/Q6, 13th VCEG Meeting, Austin, Tex., USA, April 2001, Doc. VCEG-M33 was used. Note that we achieved about 6% gains in depth by IPPP and about 5% gains in view by IBBP by our YCbCrD coding scheme.
-
TABLE 9 Percentage of combined YCbCrD coding (IPPP by Zeus) QP 22 QP 27 QP 32QP 37Average Lovebird1 Percentage of 60.54% 69.74% 78.87% 87.67% 74.20 % View 2 combined coding Lovebird2 Percentage of 42.98% 51.69% 60.03% 68.54% 55.81 % View 7 combined coding Pantomime Percentage of 60.29% 63.96% 71.06% 77.58% 68.22 % View 37 combined coding -
TABLE 10 Coding results of view and depth Coding structure IPPP IBBP Lovebird Lovebird Pantomime Lovebird Lovebird Pantomime Sequence 1 View 22 View 7View 371 View 22 View 7View 37PSNR Bit rate 1.92% 3.23% 0.51% 4.13% 6.27% 4.17% view PSNR 0.08 dB 0.13 dB 0.02 dB 0.166 dB 0.246 dB 0.162 dB PSNR Bit rate 5.10% 3.92% 9.32% 1.38% 0.88% 1.31% depth PSNR 0.18 dB 0.17 dB 0.57 dB 0.054 dB 0.041 dB 0.059 dB - In Table 11-13, view synthesis results are compared for our YCbCrD coding and separate coding (baseline) for IPPP coding results. The distortions measured by PSNR in each sequence are similar for both YCbCrD and baseline but total bit rates are reduced by YCbCrD coding. However overall coding gains in the synthesized views are less than what have been achieved by the depth coding from Table 8. This is because estimated depths by DERS are not accurate and the qualities of synthesized views depend on the accuracy of VSRS that was not confirmed yet.
-
TABLE 11 Experimental results of view synthesis for Lovebird1 Lovebird1 QP 22 QP 27 QP 32QP 37Gains YCbCrD PSNR (Syn Vew 3) 43.61 40.86 37.98 35.24 0.05 dB Bit rates (View 2) 3474 1652 778 388 Bit rates (View 4) 3348 1599 753 379 Total bit rates (Kbps) 6822 3251 1531 767 1.22% Baseline PSNR (Syn Vew 3) 43.63 40.87 38.03 35.27 Bit rates (View 2) 3515 1677 797 403 Bit rates (View 4) 3394 1624 767 390 Total bit rates (Kbps) 6909 3301 1564 793 -
TABLE 12 Experimental results of view synthesis for Lovebird2 Lovebird2 QP 22 QP 27 QP 32QP 37Gains YCbCrD PSNR (Syn Vew 8) 42.76 40.38 37.93 35.38 0.14 dB Bit rates (View 7) 1859 954 476 243 Bit rates (View 9) 1863 945 465 232 Total bit rates (Kbps) 3722 1899 941 475 3.78% Baseline PSNR (Syn Vew 8) 42.76 40.33 37.91 35.35 Bit rates (View 7) 1883 977 492 257 Bit rates (View 9) 1892 966 480 243 Total bit rates (Kbps) 3775 1943 972 500 -
TABLE 13 Experimental results of view synthesis for Pantomime Pantomime QP 22 QP 27 QP 32QP 37Gains YCbCrD PSNR (Syn Vew 38) 46.50 44.71 42.20 39.24 0.04 dB Bit rates (View 37) 6486 3344 1836 1077 Bit rates (View 39) 6330 3274 1810 1063 Total bit rates (Kbps) 12816 6618 3646 2140 0.93% Baseline PSNR (Syn Vew 38) 46.48 44.70 42.19 39.24 Bit rates (View 37) 6521 3365 1848 1080 Bit rates (View 39) 6377 3300 1823 1071 Total bit rates (Kbps) 12898 6665 3671 2151 - Some or all of the operations set forth in
FIGS. 5-6 may be contained as a utility, program, or subprogram, in any desired computer readable storage medium, which may be a non-transitory medium. In addition, the operations may be embodied by computer programs, which can exist in a variety of forms both active and inactive. For example, they may exist as software program(s) comprised of program instructions in source code, object code, executable code or other formats. Any of the above may be embodied on a computer readable storage medium, which include storage devices. - Exemplary computer readable storage media include conventional computer system RAM, ROM, EPROM, EEPROM, and magnetic or optical disks or tapes. Concrete examples of the foregoing include distribution of the programs on a CD ROM or via Internet download. It is therefore to be understood that any electronic device capable of executing the above-described functions may perform those functions enumerated above.
- What has been described and illustrated herein are embodiments of the invention along with some of their variations. The terms, descriptions and figures used herein are set forth by way of illustration only and are not meant as limitations. Those skilled in the art will recognize that many variations are possible within the spirit and scope of the embodiments of the invention.
- The invention allows 3D encoding of a depth parameter jointly with view information. The invention allows for compatibility with 2D and may provide optimized encoding based on the RD costs in encoding depth jointly with view or separately. Also, from the new definition of video format, we provide an adaptive coding method of 3D video signal. During the combined coding of RGBD, YUVD, and YCbCrD in the adaptive coding of 3D signal, we treat depth as a video component from the beginning thus, in inter prediction, block mode and reference index are shared between view and depth in addition to motion vector. In intra prediction, intra prediction mode can be shared also. Note that the coding result of combined coding can be further optimized by considering depth information together with view. In the separate coding of view and depth, depth is coded independently to the view. It is also possible to have intra coded depth while view is inter coded.
- Although described specifically throughout the entirety of the instant disclosure, representative embodiments of the present invention have utility over a wide range of applications, and the above discussion is not intended and should not be construed to be limiting, but is offered as an illustrative discussion of aspects of the invention.
Claims (51)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US12/952,781 US20110122225A1 (en) | 2009-11-23 | 2010-11-23 | Depth Coding as an Additional Channel to Video Sequence |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US26351609P | 2009-11-23 | 2009-11-23 | |
US12/952,781 US20110122225A1 (en) | 2009-11-23 | 2010-11-23 | Depth Coding as an Additional Channel to Video Sequence |
Publications (1)
Publication Number | Publication Date |
---|---|
US20110122225A1 true US20110122225A1 (en) | 2011-05-26 |
Family
ID=43406782
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US12/952,781 Abandoned US20110122225A1 (en) | 2009-11-23 | 2010-11-23 | Depth Coding as an Additional Channel to Video Sequence |
Country Status (4)
Country | Link |
---|---|
US (1) | US20110122225A1 (en) |
KR (1) | KR101365329B1 (en) |
CN (1) | CN102792699A (en) |
WO (1) | WO2011063397A1 (en) |
Cited By (32)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20110135199A1 (en) * | 2009-12-08 | 2011-06-09 | Electronics And Telecommunications Research Institute | Coding apparatus and method for simultaneous transmission of image processing information and color information |
US20130135435A1 (en) * | 2010-07-28 | 2013-05-30 | 3Dswitch S.R.L. | Method for combining images relating to a three-dimensional content |
US20130195350A1 (en) * | 2011-03-29 | 2013-08-01 | Kabushiki Kaisha Toshiba | Image encoding device, image encoding method, image decoding device, image decoding method, and computer program product |
US20130271565A1 (en) * | 2012-04-16 | 2013-10-17 | Qualcomm Incorporated | View synthesis based on asymmetric texture and depth resolutions |
US20130321564A1 (en) * | 2012-05-31 | 2013-12-05 | Microsoft Corporation | Perspective-correct communication window with motion parallax |
US20140044347A1 (en) * | 2011-04-25 | 2014-02-13 | Sharp Kabushiki Kaisha | Mage coding apparatus, image coding method, image coding program, image decoding apparatus, image decoding method, and image decoding program |
US20140294061A1 (en) * | 2013-03-27 | 2014-10-02 | Qualcomm Incorporated | Depth coding modes signaling of depth data for 3d-hevc |
US20140301454A1 (en) * | 2013-03-27 | 2014-10-09 | Qualcomm Incorporated | Depth coding modes signaling of depth data for 3d-hevc |
JP2014529214A (en) * | 2011-08-09 | 2014-10-30 | サムスン エレクトロニクス カンパニー リミテッド | Multi-view video data depth map encoding method and apparatus, decoding method and apparatus |
US20150093024A1 (en) * | 2013-09-27 | 2015-04-02 | Nvidia Corporation | System, method, and computer program product for joint color and depth encoding |
US9098911B2 (en) | 2012-11-01 | 2015-08-04 | Google Inc. | Depth map generation from a monoscopic image based on combined depth cues |
US9137519B1 (en) * | 2012-01-04 | 2015-09-15 | Google Inc. | Generation of a stereo video from a mono video |
US20150319459A1 (en) * | 2013-01-10 | 2015-11-05 | Huawei Technologies Co., Ltd. | Methods and apparatuses for coding and decoding depth map |
US9251623B2 (en) | 2012-05-31 | 2016-02-02 | Microsoft Technology Licensing, Llc | Glancing angle exclusion |
US9307252B2 (en) | 2012-06-04 | 2016-04-05 | City University Of Hong Kong | View synthesis distortion model for multiview depth video coding |
US9371099B2 (en) | 2004-11-03 | 2016-06-21 | The Wilfred J. and Louisette G. Lagassey Irrevocable Trust | Modular intelligent transportation system |
US9503702B2 (en) | 2012-04-13 | 2016-11-22 | Qualcomm Incorporated | View synthesis mode for three-dimensional video coding |
US9571811B2 (en) | 2010-07-28 | 2017-02-14 | S.I.Sv.El. Societa' Italiana Per Lo Sviluppo Dell'elettronica S.P.A. | Method and device for multiplexing and demultiplexing composite images relating to a three-dimensional content |
US20170134761A1 (en) | 2010-04-13 | 2017-05-11 | Ge Video Compression, Llc | Coding of a spatial sampling of a two-dimensional information signal using sub-division |
US9767598B2 (en) | 2012-05-31 | 2017-09-19 | Microsoft Technology Licensing, Llc | Smoothing and robust normal estimation for 3D point clouds |
US9860562B2 (en) * | 2014-09-30 | 2018-01-02 | Hfi Innovation Inc. | Method of lookup table size reduction for depth modelling mode in depth coding |
US10034021B2 (en) | 2013-01-10 | 2018-07-24 | Huawei Technologies Co., Ltd. | Methods and apparatuses for coding and decoding depth map |
US10038920B2 (en) | 2010-04-13 | 2018-07-31 | Ge Video Compression, Llc | Multitree subdivision and inheritance of coding parameters in a coding block |
CN108769684A (en) * | 2018-06-06 | 2018-11-06 | 郑州云海信息技术有限公司 | Image processing method based on WebP image compression algorithms and device |
US20190089962A1 (en) | 2010-04-13 | 2019-03-21 | Ge Video Compression, Llc | Inter-plane prediction |
US10248966B2 (en) | 2010-04-13 | 2019-04-02 | Ge Video Compression, Llc | Region merging and coding parameter reuse via merging |
CN109672885A (en) * | 2019-01-08 | 2019-04-23 | 中国矿业大学(北京) | A kind of video image encoding and decoding method for mine intelligent monitoring |
CN110493603A (en) * | 2019-07-25 | 2019-11-22 | 南京航空航天大学 | A kind of multi-view video transmission error control method of the rate-distortion optimization based on combined signal source channel |
WO2020195767A1 (en) * | 2019-03-25 | 2020-10-01 | シャープ株式会社 | 3d model transmitting device and 3d model receiving device |
CN113497943A (en) * | 2021-08-09 | 2021-10-12 | 杭州小影创新科技股份有限公司 | Depth information quantization and coding method |
AU2019345715B2 (en) * | 2018-09-30 | 2022-06-02 | Guangdong Oppo Mobile Telecommunications Corp., Ltd. | Methods and devices for data processing, electronic device |
US11983737B2 (en) | 2010-04-13 | 2024-05-14 | Ge Video Compression, Llc | Region merging and coding parameter reuse via merging |
Families Citing this family (15)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN105103543B (en) * | 2013-04-12 | 2017-10-27 | 寰发股份有限公司 | Compatible depth relies on coding method |
WO2014166119A1 (en) * | 2013-04-12 | 2014-10-16 | Mediatek Inc. | Stereo compatibility high level syntax |
US9483845B2 (en) * | 2013-04-26 | 2016-11-01 | Nvidia Corporation | Extending prediction modes and performance of video codecs |
KR102501752B1 (en) * | 2015-09-21 | 2023-02-20 | 삼성전자주식회사 | The method and apparatus for comppensating motion of the head mounted display |
CN106454204A (en) * | 2016-10-18 | 2017-02-22 | 四川大学 | Naked eye stereo video conference system based on network depth camera |
CN111447427B (en) * | 2019-01-16 | 2022-02-01 | 杭州云深弘视智能科技有限公司 | Depth data transmission method and device |
CN110111380A (en) * | 2019-03-18 | 2019-08-09 | 西安电子科技大学 | 3D rendering transmission and method for reconstructing based on depth camera |
CN110012294B (en) * | 2019-04-02 | 2021-03-23 | 上海工程技术大学 | Encoding method and decoding method for multi-component video |
CN111225218A (en) * | 2019-11-06 | 2020-06-02 | Oppo广东移动通信有限公司 | Information processing method, encoding device, decoding device, system, and storage medium |
WO2021087800A1 (en) * | 2019-11-06 | 2021-05-14 | Oppo广东移动通信有限公司 | Information processing method, encoding apparatus, decoding apparatus, system, and storage medium |
CN110784722B (en) * | 2019-11-06 | 2022-08-16 | Oppo广东移动通信有限公司 | Encoding and decoding method, encoding and decoding device, encoding and decoding system and storage medium |
CN112788325B (en) * | 2019-11-06 | 2023-06-02 | Oppo广东移动通信有限公司 | Image processing method, encoding device, decoding device and storage medium |
WO2021087810A1 (en) * | 2019-11-06 | 2021-05-14 | Oppo广东移动通信有限公司 | Information processing methods and systems, and encoding apparatus, decoding apparatus and storage medium |
CN110855997B (en) * | 2019-11-06 | 2023-03-28 | Oppo广东移动通信有限公司 | Image processing method and device and storage medium |
CN110809152A (en) * | 2019-11-06 | 2020-02-18 | Oppo广东移动通信有限公司 | Information processing method, encoding device, decoding device, system, and storage medium |
Citations (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20060031917A1 (en) * | 2004-08-03 | 2006-02-09 | Microsoft Corporation | Compressing and decompressing multiple, layered, video streams employing multi-directional spatial encoding |
US20080303892A1 (en) * | 2007-06-11 | 2008-12-11 | Samsung Electronics Co., Ltd. | Method and apparatus for generating block-based stereoscopic image format and method and apparatus for reconstructing stereoscopic images from block-based stereoscopic image format |
US20090015662A1 (en) * | 2007-07-13 | 2009-01-15 | Samsung Electronics Co., Ltd. | Method and apparatus for encoding and decoding stereoscopic image format including both information of base view image and information of additional view image |
US20100284466A1 (en) * | 2008-01-11 | 2010-11-11 | Thomson Licensing | Video and depth coding |
US20110096832A1 (en) * | 2009-10-23 | 2011-04-28 | Qualcomm Incorporated | Depth map generation techniques for conversion of 2d video data to 3d video data |
US20110216833A1 (en) * | 2008-10-17 | 2011-09-08 | Nokia Corporation | Sharing of motion vector in 3d video coding |
Family Cites Families (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9131247B2 (en) * | 2005-10-19 | 2015-09-08 | Thomson Licensing | Multi-view video coding using scalable video coding |
-
2010
- 2010-11-23 US US12/952,781 patent/US20110122225A1/en not_active Abandoned
- 2010-11-23 KR KR1020127016136A patent/KR101365329B1/en active IP Right Grant
- 2010-11-23 CN CN2010800529871A patent/CN102792699A/en active Pending
- 2010-11-23 WO PCT/US2010/057835 patent/WO2011063397A1/en active Application Filing
Patent Citations (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20060031917A1 (en) * | 2004-08-03 | 2006-02-09 | Microsoft Corporation | Compressing and decompressing multiple, layered, video streams employing multi-directional spatial encoding |
US20080303892A1 (en) * | 2007-06-11 | 2008-12-11 | Samsung Electronics Co., Ltd. | Method and apparatus for generating block-based stereoscopic image format and method and apparatus for reconstructing stereoscopic images from block-based stereoscopic image format |
US20090015662A1 (en) * | 2007-07-13 | 2009-01-15 | Samsung Electronics Co., Ltd. | Method and apparatus for encoding and decoding stereoscopic image format including both information of base view image and information of additional view image |
US20100284466A1 (en) * | 2008-01-11 | 2010-11-11 | Thomson Licensing | Video and depth coding |
US20110216833A1 (en) * | 2008-10-17 | 2011-09-08 | Nokia Corporation | Sharing of motion vector in 3d video coding |
US20110096832A1 (en) * | 2009-10-23 | 2011-04-28 | Qualcomm Incorporated | Depth map generation techniques for conversion of 2d video data to 3d video data |
Cited By (108)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9371099B2 (en) | 2004-11-03 | 2016-06-21 | The Wilfred J. and Louisette G. Lagassey Irrevocable Trust | Modular intelligent transportation system |
US10979959B2 (en) | 2004-11-03 | 2021-04-13 | The Wilfred J. and Louisette G. Lagassey Irrevocable Trust | Modular intelligent transportation system |
US20110135199A1 (en) * | 2009-12-08 | 2011-06-09 | Electronics And Telecommunications Research Institute | Coding apparatus and method for simultaneous transmission of image processing information and color information |
US10893301B2 (en) | 2010-04-13 | 2021-01-12 | Ge Video Compression, Llc | Coding of a spatial sampling of a two-dimensional information signal using sub-division |
US20190174148A1 (en) | 2010-04-13 | 2019-06-06 | Ge Video Compression, Llc | Inheritance in sample array multitree subdivision |
US10855990B2 (en) * | 2010-04-13 | 2020-12-01 | Ge Video Compression, Llc | Inter-plane prediction |
US11910030B2 (en) | 2010-04-13 | 2024-02-20 | Ge Video Compression, Llc | Inheritance in sample array multitree subdivision |
US11910029B2 (en) | 2010-04-13 | 2024-02-20 | Ge Video Compression, Llc | Coding of a spatial sampling of a two-dimensional information signal using sub-division preliminary class |
US20190089962A1 (en) | 2010-04-13 | 2019-03-21 | Ge Video Compression, Llc | Inter-plane prediction |
US11900415B2 (en) | 2010-04-13 | 2024-02-13 | Ge Video Compression, Llc | Region merging and coding parameter reuse via merging |
US11856240B1 (en) | 2010-04-13 | 2023-12-26 | Ge Video Compression, Llc | Coding of a spatial sampling of a two-dimensional information signal using sub-division |
US10848767B2 (en) * | 2010-04-13 | 2020-11-24 | Ge Video Compression, Llc | Inter-plane prediction |
US11810019B2 (en) | 2010-04-13 | 2023-11-07 | Ge Video Compression, Llc | Region merging and coding parameter reuse via merging |
US11785264B2 (en) | 2010-04-13 | 2023-10-10 | Ge Video Compression, Llc | Multitree subdivision and inheritance of coding parameters in a coding block |
US11778241B2 (en) | 2010-04-13 | 2023-10-03 | Ge Video Compression, Llc | Coding of a spatial sampling of a two-dimensional information signal using sub-division |
US11765362B2 (en) | 2010-04-13 | 2023-09-19 | Ge Video Compression, Llc | Inter-plane prediction |
US11765363B2 (en) | 2010-04-13 | 2023-09-19 | Ge Video Compression, Llc | Inter-plane reuse of coding parameters |
US11736738B2 (en) | 2010-04-13 | 2023-08-22 | Ge Video Compression, Llc | Coding of a spatial sampling of a two-dimensional information signal using subdivision |
US11734714B2 (en) | 2010-04-13 | 2023-08-22 | Ge Video Compression, Llc | Region merging and coding parameter reuse via merging |
US11611761B2 (en) | 2010-04-13 | 2023-03-21 | Ge Video Compression, Llc | Inter-plane reuse of coding parameters |
US10803483B2 (en) | 2010-04-13 | 2020-10-13 | Ge Video Compression, Llc | Region merging and coding parameter reuse via merging |
US11553212B2 (en) | 2010-04-13 | 2023-01-10 | Ge Video Compression, Llc | Inheritance in sample array multitree subdivision |
US10250913B2 (en) | 2010-04-13 | 2019-04-02 | Ge Video Compression, Llc | Coding of a spatial sampling of a two-dimensional information signal using sub-division |
US11546642B2 (en) | 2010-04-13 | 2023-01-03 | Ge Video Compression, Llc | Coding of a spatial sampling of a two-dimensional information signal using sub-division |
US10805645B2 (en) | 2010-04-13 | 2020-10-13 | Ge Video Compression, Llc | Coding of a spatial sampling of a two-dimensional information signal using sub-division |
US10855991B2 (en) | 2010-04-13 | 2020-12-01 | Ge Video Compression, Llc | Inter-plane prediction |
US20170134761A1 (en) | 2010-04-13 | 2017-05-11 | Ge Video Compression, Llc | Coding of a spatial sampling of a two-dimensional information signal using sub-division |
US10771822B2 (en) | 2010-04-13 | 2020-09-08 | Ge Video Compression, Llc | Coding of a spatial sampling of a two-dimensional information signal using sub-division |
US11546641B2 (en) | 2010-04-13 | 2023-01-03 | Ge Video Compression, Llc | Inheritance in sample array multitree subdivision |
US11102518B2 (en) | 2010-04-13 | 2021-08-24 | Ge Video Compression, Llc | Coding of a spatial sampling of a two-dimensional information signal using sub-division |
US11087355B2 (en) | 2010-04-13 | 2021-08-10 | Ge Video Compression, Llc | Region merging and coding parameter reuse via merging |
US20210211743A1 (en) | 2010-04-13 | 2021-07-08 | Ge Video Compression, Llc | Coding of a spatial sampling of a two-dimensional information signal using sub-division |
US11051047B2 (en) | 2010-04-13 | 2021-06-29 | Ge Video Compression, Llc | Inheritance in sample array multitree subdivision |
US10038920B2 (en) | 2010-04-13 | 2018-07-31 | Ge Video Compression, Llc | Multitree subdivision and inheritance of coding parameters in a coding block |
US10051291B2 (en) | 2010-04-13 | 2018-08-14 | Ge Video Compression, Llc | Inheritance in sample array multitree subdivision |
US11037194B2 (en) | 2010-04-13 | 2021-06-15 | Ge Video Compression, Llc | Region merging and coding parameter reuse via merging |
US10764608B2 (en) | 2010-04-13 | 2020-09-01 | Ge Video Compression, Llc | Coding of a spatial sampling of a two-dimensional information signal using sub-division |
US20180324466A1 (en) | 2010-04-13 | 2018-11-08 | Ge Video Compression, Llc | Inheritance in sample array multitree subdivision |
US10856013B2 (en) | 2010-04-13 | 2020-12-01 | Ge Video Compression, Llc | Coding of a spatial sampling of a two-dimensional information signal using sub-division |
US10748183B2 (en) | 2010-04-13 | 2020-08-18 | Ge Video Compression, Llc | Region merging and coding parameter reuse via merging |
US10803485B2 (en) | 2010-04-13 | 2020-10-13 | Ge Video Compression, Llc | Region merging and coding parameter reuse via merging |
US11983737B2 (en) | 2010-04-13 | 2024-05-14 | Ge Video Compression, Llc | Region merging and coding parameter reuse via merging |
US10880580B2 (en) | 2010-04-13 | 2020-12-29 | Ge Video Compression, Llc | Inheritance in sample array multitree subdivision |
US20190164188A1 (en) | 2010-04-13 | 2019-05-30 | Ge Video Compression, Llc | Region merging and coding parameter reuse via merging |
US10248966B2 (en) | 2010-04-13 | 2019-04-02 | Ge Video Compression, Llc | Region merging and coding parameter reuse via merging |
US10880581B2 (en) | 2010-04-13 | 2020-12-29 | Ge Video Compression, Llc | Inheritance in sample array multitree subdivision |
US20190197579A1 (en) | 2010-04-13 | 2019-06-27 | Ge Video Compression, Llc | Region merging and coding parameter reuse via merging |
US10432980B2 (en) | 2010-04-13 | 2019-10-01 | Ge Video Compression, Llc | Inheritance in sample array multitree subdivision |
US10432978B2 (en) | 2010-04-13 | 2019-10-01 | Ge Video Compression, Llc | Inheritance in sample array multitree subdivision |
US10432979B2 (en) | 2010-04-13 | 2019-10-01 | Ge Video Compression Llc | Inheritance in sample array multitree subdivision |
US10440400B2 (en) | 2010-04-13 | 2019-10-08 | Ge Video Compression, Llc | Inheritance in sample array multitree subdivision |
US10448060B2 (en) | 2010-04-13 | 2019-10-15 | Ge Video Compression, Llc | Multitree subdivision and inheritance of coding parameters in a coding block |
US10873749B2 (en) * | 2010-04-13 | 2020-12-22 | Ge Video Compression, Llc | Inter-plane reuse of coding parameters |
US10460344B2 (en) | 2010-04-13 | 2019-10-29 | Ge Video Compression, Llc | Region merging and coding parameter reuse via merging |
US10863208B2 (en) | 2010-04-13 | 2020-12-08 | Ge Video Compression, Llc | Inheritance in sample array multitree subdivision |
US10855995B2 (en) | 2010-04-13 | 2020-12-01 | Ge Video Compression, Llc | Inter-plane prediction |
US10621614B2 (en) | 2010-04-13 | 2020-04-14 | Ge Video Compression, Llc | Region merging and coding parameter reuse via merging |
US10672028B2 (en) | 2010-04-13 | 2020-06-02 | Ge Video Compression, Llc | Region merging and coding parameter reuse via merging |
US10681390B2 (en) | 2010-04-13 | 2020-06-09 | Ge Video Compression, Llc | Coding of a spatial sampling of a two-dimensional information signal using sub-division |
US10687086B2 (en) | 2010-04-13 | 2020-06-16 | Ge Video Compression, Llc | Coding of a spatial sampling of a two-dimensional information signal using sub-division |
US10687085B2 (en) | 2010-04-13 | 2020-06-16 | Ge Video Compression, Llc | Inheritance in sample array multitree subdivision |
US10694218B2 (en) | 2010-04-13 | 2020-06-23 | Ge Video Compression, Llc | Inheritance in sample array multitree subdivision |
US10708628B2 (en) | 2010-04-13 | 2020-07-07 | Ge Video Compression, Llc | Coding of a spatial sampling of a two-dimensional information signal using sub-division |
US10708629B2 (en) | 2010-04-13 | 2020-07-07 | Ge Video Compression, Llc | Inheritance in sample array multitree subdivision |
US10719850B2 (en) | 2010-04-13 | 2020-07-21 | Ge Video Compression, Llc | Region merging and coding parameter reuse via merging |
US10721495B2 (en) | 2010-04-13 | 2020-07-21 | Ge Video Compression, Llc | Coding of a spatial sampling of a two-dimensional information signal using sub-division |
US10721496B2 (en) | 2010-04-13 | 2020-07-21 | Ge Video Compression, Llc | Inheritance in sample array multitree subdivision |
US20130135435A1 (en) * | 2010-07-28 | 2013-05-30 | 3Dswitch S.R.L. | Method for combining images relating to a three-dimensional content |
US9571811B2 (en) | 2010-07-28 | 2017-02-14 | S.I.Sv.El. Societa' Italiana Per Lo Sviluppo Dell'elettronica S.P.A. | Method and device for multiplexing and demultiplexing composite images relating to a three-dimensional content |
US9549163B2 (en) * | 2010-07-28 | 2017-01-17 | S.I.Sv.El Societa' Italiana Per Lo Sviluppo Dell'elettronica S.P.A. | Method for combining images relating to a three-dimensional content |
US20130195350A1 (en) * | 2011-03-29 | 2013-08-01 | Kabushiki Kaisha Toshiba | Image encoding device, image encoding method, image decoding device, image decoding method, and computer program product |
US20140044347A1 (en) * | 2011-04-25 | 2014-02-13 | Sharp Kabushiki Kaisha | Mage coding apparatus, image coding method, image coding program, image decoding apparatus, image decoding method, and image decoding program |
JP2014529214A (en) * | 2011-08-09 | 2014-10-30 | サムスン エレクトロニクス カンパニー リミテッド | Multi-view video data depth map encoding method and apparatus, decoding method and apparatus |
US9137519B1 (en) * | 2012-01-04 | 2015-09-15 | Google Inc. | Generation of a stereo video from a mono video |
US9503702B2 (en) | 2012-04-13 | 2016-11-22 | Qualcomm Incorporated | View synthesis mode for three-dimensional video coding |
US20130271565A1 (en) * | 2012-04-16 | 2013-10-17 | Qualcomm Incorporated | View synthesis based on asymmetric texture and depth resolutions |
US9767598B2 (en) | 2012-05-31 | 2017-09-19 | Microsoft Technology Licensing, Llc | Smoothing and robust normal estimation for 3D point clouds |
US9846960B2 (en) | 2012-05-31 | 2017-12-19 | Microsoft Technology Licensing, Llc | Automated camera array calibration |
US9251623B2 (en) | 2012-05-31 | 2016-02-02 | Microsoft Technology Licensing, Llc | Glancing angle exclusion |
US20130321564A1 (en) * | 2012-05-31 | 2013-12-05 | Microsoft Corporation | Perspective-correct communication window with motion parallax |
US9256980B2 (en) | 2012-05-31 | 2016-02-09 | Microsoft Technology Licensing, Llc | Interpolating oriented disks in 3D space for constructing high fidelity geometric proxies from point clouds |
US10325400B2 (en) | 2012-05-31 | 2019-06-18 | Microsoft Technology Licensing, Llc | Virtual viewpoint for a participant in an online communication |
US9332218B2 (en) * | 2012-05-31 | 2016-05-03 | Microsoft Technology Licensing, Llc | Perspective-correct communication window with motion parallax |
US9836870B2 (en) | 2012-05-31 | 2017-12-05 | Microsoft Technology Licensing, Llc | Geometric proxy for a participant in an online meeting |
US9307252B2 (en) | 2012-06-04 | 2016-04-05 | City University Of Hong Kong | View synthesis distortion model for multiview depth video coding |
US9426449B2 (en) | 2012-11-01 | 2016-08-23 | Google Inc. | Depth map generation from a monoscopic image based on combined depth cues |
US9098911B2 (en) | 2012-11-01 | 2015-08-04 | Google Inc. | Depth map generation from a monoscopic image based on combined depth cues |
US20180332304A1 (en) * | 2013-01-10 | 2018-11-15 | Huawei Technologies Co., Ltd. | Methods and apparatuses for coding and decoding depth map |
US20150319459A1 (en) * | 2013-01-10 | 2015-11-05 | Huawei Technologies Co., Ltd. | Methods and apparatuses for coding and decoding depth map |
US10582217B2 (en) * | 2013-01-10 | 2020-03-03 | Huawei Technologies Co., Ltd. | Methods and apparatuses for coding and decoding depth map |
US10455251B2 (en) | 2013-01-10 | 2019-10-22 | Huawei Technologies Co., Ltd. | Methods and apparatuses for coding and decoding depth map |
US10034021B2 (en) | 2013-01-10 | 2018-07-24 | Huawei Technologies Co., Ltd. | Methods and apparatuses for coding and decoding depth map |
US10063883B2 (en) * | 2013-01-10 | 2018-08-28 | Huawei Technologies Co., Ltd. | Methods and apparatuses for coding and decoding depth map |
US9516306B2 (en) * | 2013-03-27 | 2016-12-06 | Qualcomm Incorporated | Depth coding modes signaling of depth data for 3D-HEVC |
US20140301454A1 (en) * | 2013-03-27 | 2014-10-09 | Qualcomm Incorporated | Depth coding modes signaling of depth data for 3d-hevc |
US9369708B2 (en) * | 2013-03-27 | 2016-06-14 | Qualcomm Incorporated | Depth coding modes signaling of depth data for 3D-HEVC |
US20140294061A1 (en) * | 2013-03-27 | 2014-10-02 | Qualcomm Incorporated | Depth coding modes signaling of depth data for 3d-hevc |
CN105103559A (en) * | 2013-03-27 | 2015-11-25 | 高通股份有限公司 | Depth coding modes signaling of depth data for 3D-HEVC |
US9355468B2 (en) * | 2013-09-27 | 2016-05-31 | Nvidia Corporation | System, method, and computer program product for joint color and depth encoding |
US20150093024A1 (en) * | 2013-09-27 | 2015-04-02 | Nvidia Corporation | System, method, and computer program product for joint color and depth encoding |
US9860562B2 (en) * | 2014-09-30 | 2018-01-02 | Hfi Innovation Inc. | Method of lookup table size reduction for depth modelling mode in depth coding |
US9986257B2 (en) | 2014-09-30 | 2018-05-29 | Hfi Innovation Inc. | Method of lookup table size reduction for depth modelling mode in depth coding |
CN108769684A (en) * | 2018-06-06 | 2018-11-06 | 郑州云海信息技术有限公司 | Image processing method based on WebP image compression algorithms and device |
AU2019345715B2 (en) * | 2018-09-30 | 2022-06-02 | Guangdong Oppo Mobile Telecommunications Corp., Ltd. | Methods and devices for data processing, electronic device |
CN109672885A (en) * | 2019-01-08 | 2019-04-23 | 中国矿业大学(北京) | A kind of video image encoding and decoding method for mine intelligent monitoring |
WO2020195767A1 (en) * | 2019-03-25 | 2020-10-01 | シャープ株式会社 | 3d model transmitting device and 3d model receiving device |
CN110493603A (en) * | 2019-07-25 | 2019-11-22 | 南京航空航天大学 | A kind of multi-view video transmission error control method of the rate-distortion optimization based on combined signal source channel |
CN113497943A (en) * | 2021-08-09 | 2021-10-12 | 杭州小影创新科技股份有限公司 | Depth information quantization and coding method |
Also Published As
Publication number | Publication date |
---|---|
CN102792699A (en) | 2012-11-21 |
WO2011063397A1 (en) | 2011-05-26 |
KR20120085326A (en) | 2012-07-31 |
KR101365329B1 (en) | 2014-03-14 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US20110122225A1 (en) | Depth Coding as an Additional Channel to Video Sequence | |
US10764596B2 (en) | Tiling in video encoding and decoding | |
KR101619450B1 (en) | Video signal processing method and apparatus using depth information | |
WO2022210101A1 (en) | Decoding method, encoding method, decoding device, and encoding device | |
EP3973709A2 (en) | A method, an apparatus and a computer program product for video encoding and video decoding | |
CN117957841A (en) | GPM-based image compiling method and device | |
Lee et al. | 3D video format and compression methods for Efficient Multiview Video Transfer | |
Valizadeh | Compression efficiency improvement for 2D and 3D video | |
Azimi Hashemi | Perceptually based compression of emerging digital media content |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GENERAL INSTRUMENT CORPORATION, PENNSYLVANIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:KIM, JAE HOON;WANG, LIMIN;SIGNING DATES FROM 20101206 TO 20101207;REEL/FRAME:025640/0380 |
|
AS | Assignment |
Owner name: MOTOROLA MOBILITY LLC, ILLINOISFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:GENERAL INSTRUMENT HOLDINGS, INC.;REEL/FRAME:030866/0113Effective date: 20130528Owner name: GENERAL INSTRUMENT HOLDINGS, INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:GENERAL INSTRUMENT CORPORATION;REEL/FRAME:030764/0575Effective date: 20130415 |
|
AS | Assignment |
Owner name: GOOGLE TECHNOLOGY HOLDINGS LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:MOTOROLA MOBILITY LLC;REEL/FRAME:034301/0001Effective date: 20141028 |
|
STCB | Information on status: application discontinuation |
Free format text: ABANDONED -- FAILURE TO RESPOND TO AN OFFICE ACTION |