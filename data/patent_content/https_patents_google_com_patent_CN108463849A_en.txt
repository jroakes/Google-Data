CN108463849A - Determine the dialogue state of language model - Google Patents
Determine the dialogue state of language model Download PDFInfo
- Publication number
- CN108463849A CN108463849A CN201680078893.9A CN201680078893A CN108463849A CN 108463849 A CN108463849 A CN 108463849A CN 201680078893 A CN201680078893 A CN 201680078893A CN 108463849 A CN108463849 A CN 108463849A
- Authority
- CN
- China
- Prior art keywords
- dialogue
- state
- voice input
- voice
- transcription
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/26—Speech to text systems
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/279—Recognition of textual entities
- G06F40/289—Phrasal analysis, e.g. finite state techniques or chunking
- G06F40/295—Named entity recognition
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/30—Semantic analysis
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/1822—Parsing for meaning understanding
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/183—Speech classification or search using natural language modelling using context dependencies, e.g. language models
- G10L15/19—Grammatical context, e.g. disambiguation of the recognition hypotheses based on word sequence rules
- G10L15/197—Probabilistic grammars, e.g. word n-grams
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/28—Constructional details of speech recognition systems
- G10L15/30—Distributed recognition, e.g. in client-server systems, for mobile phones or network applications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/06—Creation of reference templates; Training of speech recognition systems, e.g. adaptation to the characteristics of the speaker's voice
- G10L15/065—Adaptation
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/183—Speech classification or search using natural language modelling using context dependencies, e.g. language models
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/223—Execution procedure of a spoken command
Abstract
This document describes input corresponding dialogue state with voice and based on system, method, equipment and the other technologies of identified dialogue state bias speech model for determining.In some implementations, method is included in the audio data that instruction voice input is received at computing system, and certain dialog state corresponding with voice input is determined from multiple dialogue states.N members set associated with the certain dialog state corresponding to voice input can be identified.In response to identifying n members set associated with the certain dialog state corresponding to voice input, bias speech model can be come to the probability score of the n members instruction in n member set by adjusting language model.The language model transcription voice of adjustment can be used to input.
Description
Technical field
This document generally relates to computer based speech recognitions, and relate more specifically to bias based on dialogue state
(biasing) language model.
Background technology
Other than key entry inputs or key entry input is substituted, it is defeated that computing device has increasingly offered reception spoken user
The ability entered.For example, voice assistant application can based between voice assistant and user prompt and spoken responses it is multistage
Section talks with to determine executing equipment for task.Equally, the web browser in some equipment and other application are arranged at
Voice input is received in form fields, needs to key in input in field to avoid user.Computing device is supplied to by user
Voice input can be by speech recognizer processes.Speech recognition device may include being jointly configured as voice input being transcribed into text
The component of this such as acoustic model and language model.
Invention content
This document describes for determining dialogue state corresponding with voice input and based on identified dialogue state pair
System, method, equipment and the other technologies that language model is biased.In some implementations, speech recognition system can
To learn dialogue state set automatically and for the corresponding language model offset mode of each corresponding dialogue state.It can be with
In the predefined determining dialogue state of no user and offset mode.
Some realization methods of theme described herein include computer implemented method.Method may include：It is calculating
The audio data of instruction voice input is received at system；Determination is corresponding with voice input specific right from multiple dialogue states
Speech phase.It can identify and gather with the associated n of certain dialog state first (n-grams) corresponding to voice input.N member set
N members in can at least being inputted based on history voice corresponding with dialogue state in the n member set that frequently occurs are and specific right
Speech phase is associated.In response to identifying n members set associated with the certain dialog state corresponding to voice input, can pass through
Adjustment language model carrys out bias speech model to the probability score of the n members instruction in n member set.The language mould of adjustment can be used
Type inputs to transcribe voice.
These and other realization methods can optionally include one or more following characteristics.
Multiple dialogue states can indicate respectively multiple ranks that the computing device for being related to particular task is interacted with user speech
Section.
Computing system can receive the input of the second voice, and can be determined from multiple dialogue states defeated with the second voice
Enter corresponding second certain dialog state.It can identify related to the second certain dialog state corresponding to the input of the second voice
2nd n member set of connection.2nd n member set can be different from n associated with the certain dialog state corresponding to voice input
Member set.
Determine that certain dialog state corresponding with voice input may include：Mark and voice from multiple dialogue states
The second voice before input inputs corresponding second certain dialog state, and wherein voice input and the input of the second voice is respective
It is related to same task；And one in the multiple dialogue states that may be occurred after the second certain dialog state based on instruction
Or the data of multiple dialogue states, determine certain dialog state corresponding with voice input.
Determine that certain dialog state corresponding with voice input may include：Generate the transcription and determination of voice input
One in the one or more n members occurred in the transcription of voice input and n member set associated with certain dialog state
Or the matching between multiple n members.
Determine that matching can include determining that the one or more n members and and certain dialog occurred in the transcription that voice inputs
Semantic relation between one or more of the associated n members set of state n members.
The data of instruction context (context) associated with voice input can be received.It determines and voice the input phase pair
The certain dialog state answered may include identifying certain dialog state based on context associated with voice input.
It may include being characterized in receive the calculating for receiving voice input when voice input to input associated context with voice
The data of the display of user interface at equipment.It can be wrapped based on context associated with voice input mark certain dialog state
It includes：Based on data, the mark for being characterized in the display for receiving user interface when voice inputs at the computing device of reception voice input
Know certain dialog state.
The application identifier that instruction voice inputs the application at be oriented to computing device can be received at computing system.
Multiple dialogue states can be related to inputting the application particular task of be oriented to application for voice.
Some realization methods of theme described herein include another computer implemented method.Method may include obtaining
Obtain multiple transcriptions of voice input corresponding with the different conditions in multimode dialogue.Each of multiple transcriptions are turned
Record can identify the n member set appeared in transcription.It can be confirmed as by the n member set identified to each group of distribution
In associated multiple transcriptions, transcription respective subset, generates multiple transcript profiles.Multiple transcript profiles can be based on, are determined
Indicate in multimode dialogue multiple dialogue states for occurring and to the mould of the associated corresponding n members set of each dialogue state
Type.Determining model can be provided with the bias speech model in speech recognition.
These and other realization methods can optionally include one or more following characteristics.
It can be by the way that each of multiple transcript profiles to be distributed to the respective dialog state in multiple dialogue states come really
Fixed multiple dialogue states so that the respective subset for each corresponding to transcript profile in multiple dialogue states with for multiple dialogue shapes
The respective subset of transcript profile in each of state is different from each other.It can be based on the corresponding son in the transcript profile corresponding to dialogue state
The n member set identified occurred in the transcription of concentration selects associated with each dialogue state in multiple dialogue states
Corresponding n member set.
The first dialogue state in multiple dialogue states can be with the transcript profile of two or more groups including transcription
First subset is corresponding.
It may include being formed to be determined to be in transcript profile semantically similar to each other to generate multiple transcript profiles.
Computing system can receive the sequence that at least some of instruction voice input is submitted in multimode dialogue
Data.The data for the sequence that can be at least submitted in multimode dialogue based at least some of instruction voice input, really
The sequence of dialogue state in fixed multiple dialogue states.For each corresponding dialogue state, sequence information instruction follows phase
Answer other one or more dialogue states of dialogue state or other one or more dialogues before respective dialog state
State.
Voice input multiple transcriptions may include：Each of multiple dialogue states in talking with for multimode are corresponding
Multiple transcriptions that dialogue state, voice corresponding with the respective dialog state in multiple dialogue states input.
Computing system can receive instruction phase associated with the voice input corresponding at least some of multiple transcriptions
Answer the data of context.Generating multiple transcript profiles may include：It is based further on instruction and corresponding at least one in multiple transcriptions
A little voices inputs the data of associated corresponding context, is grouped to transcription.
Inputting associated corresponding context to the first voice corresponding to the first transcription in multiple transcriptions may include：Table
It levies in the data for receiving the display for receiving the user interface at the computing device of the first voice input when the input of the first voice.
Some realization methods of theme described herein may include computing system.Computing system may include one or more
A processor and one or more computer-readable mediums.Instruction is stored on computer-readable medium, instruction is being performed
Shi Yinqi executes operation, and operation includes：Receive the audio data of instruction voice input；The determining and voice from multiple dialogue states
Input corresponding certain dialog state；N members set associated with the certain dialog state corresponding to voice input is identified,
Middle n metasets be bonded to n members in few n member set frequently occurred in being inputted based on history voice corresponding with dialogue state, with
Certain dialog state is associated；In response to identifying n members set associated with the certain dialog state corresponding to voice input, lead to
It crosses to increase and language model is adjusted by the probability score of the language model instruction of the n members in n member set；And use adjustment
Language model transcribes voice input.
In some implementations, technique described herein may be implemented under specific circumstances one in following advantages or
It is multiple.Speech recognition device can use based on be confirmed as with voice input the language model of corresponding dialogue state biasing come
Generate the more accurate transcription of voice input.In addition, the dialogue state occurred in human-computer dialogue can be by computing system certainly
Main determination so that application developer need not track dialogue state in their corresponding applications, or by dialogue state identifier
It is supplied to speech recognition system of the transcription voice input for applying accordingly.In some implementations, computing system can be with
It is determined and the associated corresponding n members set of multiple dialogue states based on the analysis for inputting and transcribing to history voice.It is based on
For given transcription request detection to which kind of dialogue state, n members set bias speech model in different ways can be used.Have
Benefit, in some implementations, these technologies can eliminate application developer manually will be associated with each dialogue state
N member set be supplied to the needs of speech recognition system.
Description of the drawings
Fig. 1 is depicted to be inputted the dialogue state of associated identification with voice and biases for using to have been based on
(biased) voice input is transcribed into the conceptual system flow chart of the example process of text by language model.
Fig. 2 depict the language model using biasing voice input is transcribed into text example process it is second conceptual
System flow chart.Certain dialog state based on one or more dialogue states identification before being inputted based on the voice with system
Associated n members, bias the language model indicated in fig. 2.
Fig. 3 is the figure for indicating example dialogue state flow model.The figure include node corresponding with multiple dialogue states with
And the corresponding directionality side of conversion between certain dialog state.
Fig. 4 be for based on to history voice input or transcribe record analysis come determine dialogue state and with dialogue shape
The flow chart for the example process of the associated n members of state respectively gathered.
Fig. 5 A and 5B are corresponding with the dialogue state occurred in dialogue is determined to be formed for merging similar transcript profile
Final group set technology concept map.
Fig. 6 is the language model for using the biasing for inputting associated dialogue state selection based on the voice with identification
Come execute speech recognition example process flow chart.
Fig. 7 shows the computing device that can be used for executing computer implemented method and other technologies described herein
And the example of mobile computing device.
It is each that the same reference numbers in the drawings refer to identical elements.
Specific implementation mode
This document generally describes for identification and using dialogue state with the system, method, equipment of bias speech model
And other technologies, the language model such as can be used for voice input being transcribed by speech recognition device the language mould of text
Type.For example, describing associated with the input transcription request of given voice for clearly being identified in no application or application developer
Dialogue state or n members in the case of automatically determine the technology of dialogue state and n members.This is described in further detail relative to attached drawing
A little technologies and other technologies.
Referring to Fig.1, show that computing system executes the concept map of example process 100, which has been based on
Voice input 110 is transcribed into text by the language model 120 for inputting the dialogue state of 110 corresponding identifications with voice and biasing
This.System includes the speech recognition device 102 communicated with user equipment 108.User equipment 108 usually can be that can detect to come from
Any kind of computing device of the voice input of user.For example, user equipment 108 can be desktop computer, notebook electricity
Brain, smart phone, tablet computing device, television set or other multimedia equipments or wearable computing devices, such as intelligent hand
Table.User equipment 108 may include detecting the microphone of voice input and voice input being converted into digital audio-frequency data
Software and circuit (for example, analog-digital converter).
In some implementations, speech recognition device 102 can be calculated being remote from the one or more of user equipment 108
It is run on machine.For example, speech recognition device 102 can provide speech-recognition services based on cloud, transcription from by one or
The voice for many different user devices that multiple networks (for example, internet) communicate with speech recognition device 102 inputs.With this side
Formula can save each since voice recognition tasks are discharged into the remote server with potential bigger computing capability
Computing resource at user equipment.In some implementations, speech recognition device can locally be provided on user equipment 108
102, to realize speech recognition capabilities when user equipment 108 is offline and network connection is unavailable.
When user 106 provides voice input 110 to user equipment 108, processing 100 starts.Before speaking, user
106 may be had activated in equipment 108 enable equipment 108 detect voice input and communicated with speech recognition device 102
To generate the pattern of the transcription of the voice of detection.In the example depicted in fig. 1, user 106 had said phrase " the peppery perfume (or spice) of Italy
Intestines and mushroom ".The phrase can be in language using the multistage dialogue that participates in Pizza restaurant place an order of the user in equipment 108
It is said in border.For example, user 106 can be by the list that completion is presented in the user interface (for example, the webpage in equipment 108
Or the presentation of the machine application) come to select the option of his or her Pizza order, the user interface include the set (example of input field
Such as, the text field, drop-down menu or radio button selector), the input for receiving user.In some implementations, if
Standby 108 can allow user 106 to provide input to each field using the different input mechanisms such as keyed in or voice inputs
In.It being inputted using keying in, user 106 can directly be entered text by keyboard in each field, and voice is utilized to input,
User 106 gives an oral account the text that be provided in each field, is inputted as the voice for being converted into text.In some realization sides
In formula, user 106 can engage in the dialogue in equipment 108 with voice assistant, wherein equipment 108 prompt user 106 in response to
A series of problems that vision and/or audible means are presented to user 106 is (for example, " you want the Pizza of what size", " you think
Any dispensing added on your Pizza", " you want to add any side") voice input is provided.
At operation A (104), when receiving voice input 110, equipment 108, which generates, is used for speech recognition device 102 by voice
Input 110 is transcribed into the request 112 of text.If speech recognition device 102 is remote from user equipment 108, can be incited somebody to action by network
Transcription request 112 is sent to speech recognition device 102.Request 112 may include one or more components.In general, request will include
Digital audio-frequency data for voice input 110.In some implementations, request 112 can also include context data, application
Identifier, dialog identifier, dialogue state identifier or two or more combinations in these.In some realization methods
In, request 112 can only include the digital audio of voice input 110 without additional context data, application identifier, dialogue
Identifier or dialogue state identifier.
Context data in request 112 usually may include in addition to speech recognition device 102 is determined for the language of transcription
Any data except the audio data of sound input 110.Some type of context data can indicate user equipment 108 in
Or nearby device 108 detects the situation or state when voice inputs 110.As described further below, context data
Example include user account information, anonymous profile information (for example, gender, the age, browsing historical data, instruction setting
The data for the inquiry previously submitted on standby 108), location information and screen signature (indicate be in or nearby device 108 is examined
The data for the content that equipment 108 is shown when measuring voice input 110).In some implementations, application identifier, right
Words identifier and dialogue state identifier can be considered as the context data of specific type, but herein by exemplary mode
Individually discuss.
Application identifier can be included in request 112, in the multiple applications that can be identified by speech recognition device 102
The specific application that middle identification voice input 110 is directed toward.In some implementations, speech recognition device 102 can be stored for not
With application different dialogue state set or other language model biased datas, and speech recognition device 102 can use answer
The dialogue state set appropriate for transcribing voice input 110 is determined with identifier.For example, speech recognition device 102 can be with
Storage for Pizza place an order application the first dialogue state set and language model biased data and for Bank application not
Same the second dialogue state set and language model biased data.For example, by including application identifier, language in request 112
Sound identifier 102 can identify that request 112 is related with the Pizza application that places an order.It is thereby possible to select placing an order using related to Pizza
The the first dialogue state set and language model biased data of connection are for transcription voice input 110.
In some implementations, the request 112 of transcription voice input 110 may include dialog identifier.Dialogue identifier
Symbol can indicate the certain dialog belonging to the transcription request 112 in multiple dialogues.In some implementations, dialog identifier can
For distinguishing the multiple dialogues provided in single application.For example, Pizza places an order, the main dialogue of application can be for submitting
The dialogue of Pizza order.But application can also provide other dialogues, and user is allowed to interact with the application in different ways, such as permit
It submits the dialogue of client feedback or allows the dialogue of user's adjustment account setting in family allowable.Speech recognition device 102 can use pair
Words identifier will likely dialogue mutually distinguish, and select model appropriate and parameter to carry out bias speech model and transcribe language
Sound input 110.
In some implementations, the request 112 of transcription voice input 110 may include dialogue state identifier.Dialogue
Status identifier can indicate certain dialog shape among multiple dialogue states in given dialogue, belonging to transcription request 112
State.Dialogue state is typically the expression that user provides the one or more stages for the dialogue that voice inputs to computing device.Dialogue
Stage may be constructed to be interacted with particular task or the relevant sequence of user of activity.For example, for ordering the movable right of Pizza
Words may include receiving for selecting Pizza size, selection dispensing, specified Shipping Address and the corresponding language for providing payment information
The stage of sound input.Dialogue state can distribute to movable each stage, and some dialogue states can distribute to it is multiple
Stage.Dialogue state identifier can indicate which state voice input 110 is directed toward.In some implementations, dialogue state
Identifier can be the true of the dialogue state for the mode for being used for bias speech model 120 for determination by speech recognition device 102
Instruction.In some implementations, dialogue state " hint " (for example, integer form) can be provided, indicate talking phase or
Estimation of the equipment to real dialog state.Speech recognition device 102 can trust dialogue state prompt or can use other numbers
Whether accurate carry out verification tip according to (for example, context data, dialogue state historical data).
At operation B (114), the processing of speech recognition device 102 request 112 with determine it is associated with asking 112 (and therefore with
Voice input 110 is associated) dialogue state.In some implementations, speech recognition device 102 using request 112 in include
, except for determine dialogue state associated with asking 112 voice input 110 audio data in addition to information.For example,
If request 112 includes application identifier, dialog identifier and dialogue state identifier, all three letters can be used
It ceases to determine dialogue state.Application identifier can identify specific application associated with one or more dialogue set.Dialogue
Identifier can be related with which of identification request 112 and these dialogues dialogue, and dialogue state identifier can be identified and be asked
Ask 12 related with which dialogue state in the dialogue identified.In some implementations, though without or less than it is all this
Three information are provided to speech recognition device 102, can also determine dialogue state.
In some implementations, speech recognition device 102 can select dialogue shape using other kinds of context data
State.For example, request 112 may include being characterized in be in or close on when voice input 110 is provided to equipment 108 (that is, screen
Curtain signature) hashed value of user interface that is shown in equipment 108.Then, speech recognition device 102 can will ask in 112
The hashed value of reception and predetermined Hash value associated with different dialogue state are compared.It is then possible to select and be matched with
The associated certain dialog state of predetermined Hash value of the hashed value received in request 112, as voice input 110 and request
112 are confirmed as corresponding dialogue state.It in some implementations, can be based on the context number of two or more types
According to (for example, screen signature, dialogue state imply and the geographical location of indicating equipment 108 when detecting voice input 110
Location pointer) selection dialogue state.In some implementations, speech recognition device 102 can be selected by using grader
Dialogue state is selected, the grader including multiple signals of a plurality of types of context datas based on exporting the dialogue state of prediction.
Grader can be rule-based, or can handle (for example, neural network) by machine learning to train.
In the example of fig. 1, the expression for depicting data structure 116, by each and the phase in multiple dialogue states
The corresponding set (for example, language model biased data) of the context data and one or more n members answered is associated.It is specific next
It says, display corresponds to five dialogue states of the different phase in single session under Pizza.For example, dialogue state " 1 " is related to user
To dispensing or taking out the preference of option, and dialogue state " 2 " is related to preference of the user to cake skin type, and dialogue state " 3 "
It is related to preference of the user to Pizza dispensing.In some implementations, dialogue may include drawing in multiple and different stages of dialogue
Play the corresponding prompt of similar response from the user.Each of different prompt can be associated from different context datas,
But similar prompt and response can be merged into single dialogue state by speech recognition device 102.For example, Pizza order is answered
With may include the first prompt and the second prompt, the first prompt inquiry user 106 includes which dispensing, the second prompt on Pizza
Inquire whether he or she is ready that the price with discounting adds additional dispensing to user 106.Equipment 108 can be for the first prompt
Request 112 in provide context values " 1 " (for example, dialogue state identifier), and can for second prompt request 112
Middle offer context values " 7 ".But due in response to the similitude between voice input in each of these prompts, speech recognition
Device 102 can keep corresponding to two respective single dialogue states of prompt.
Data structure 116 identifies the corresponding set of one or more n members associated with each dialogue state.With given pair
The associated n members set of speech phase is indicated generally in being confirmed as voice input corresponding with given dialogue state often
Word, phrase, number or the other Languages unit (i.e. n members) of generation.For example, data structure 116 by n first " credit ", " payment ",
" dollar ", " cent ", " in shop " and " cash " be identified as may by user the Payment Options for being related to Pizza order dialogue
State is said in " 5 ".In some implementations, result of the speech recognition device 102 based on previous transcription request can be passed through
Analysis determines the corresponding n members for distributing to each dialogue state.For example, speech recognition device 102 (or another computing system) can be with
The transcription of voice input whithin a period of time from many different users is logged by.Then it can determine and be inputted in voice
The most frequently used term (n member) set that occurs in transcription and it is corresponding with dialogue state.In this way, it may not be necessary to which application is opened
Originator provides n member set associated with each dialogue state to speech recognition device 102.On the contrary, speech recognition device 102 can lead to
Analysis of history transcription result is crossed to automatically determine n member set.In some implementations, n associated with given dialogue state
Member set may include the combination of the n members automatically determined, without from application developer input and referred to by application developer
Fixed other n members.
At operation C (118), processing 100 is biased using n members set associated with selected dialogue state by language
Sound identifier 102 is used so that voice input 110 to be transcribed into the language model 120 of text.In general, language model 120 is configured
To determine that language sequence indicates the likelihood (for example, probability) of the accurate transcription of voice input.For example, language model 120 can be with
It handles by the aligned phoneme sequence or other phonetic features of the acoustic model generation of speech recognition device 102, to determine voice input 110
One or more candidate transcriptions.It is that voice is defeated that language model 120 can distribute instruction corresponding candidate transcription to each candidate transcription
The probability of the accurate transcription entered.Then preceding n candidate transcription can be returned to user equipment 108, wherein n as transcription result
It is predetermined integers (for example, 1,2,3,4 or more).
In some implementations, language model 120 can be based on the corpus indicated in one or more language datas
The probability data of the middle frequency for different term sequence occur determines the probability of candidate transcription.For example, language model 120 can be with
Higher to the transcription distribution ratio " tree zokor " (tree blind mice) of " three zokors " (three blind mice)
Point, because former language sequence more frequently occurs in search inquiry corpus than the latter.In some implementations, language
Speech model 120 can be n gram language models, and use condition probability is with a in preceding term based on n or n-1 in language sequence
Continuously predict that the term in the sequence, wherein n are predetermined integers (for example, 1,2,3,4 or more).
Speech recognition device 102 can with bias speech model 120 with increased or decrease voice input 110 transcription result include
The likelihood of n members associated with selected dialogue state in n member set.In some implementations, can to including
The preference candidate transcription bias speech model 120 of n members in the n member set for distributing to selected dialogue state.For example,
Bar chart 122 in Fig. 1 shows to be applied to each n members associated with selected dialogue state (3) of Pizza dispensing are related to
Probability " enhancing ".Therefore, general in view of that can be distributed candidate transcription " peperoni and mushroom " without biasing language model
Rate score 0.20, bias speech model can be with allocation probability scores 0.85, thereby indicate that candidate transcription accurately indicates the language of user
The high likeness of the content of sound input 110.
In some implementations, though the probability data stored by language model 120 can relative to it is selected
The associated n members set of dialogue state increases, but it includes selected set that bias speech model 120, which remains able to generate not,
The transcription of interior n members (or including at least selected set outside one or more n members).However, not including in this way with using
N members compared to generate the likelihood of transcription without biasing language model, using not including the n from selected n members set
The likelihood that the bias speech model 120 of member generates transcription may reduce.It in some implementations, can be by reducing and using
In the associated probability of n members of selected dialogue state, bias speech model 120.In some implementations, given pair
Speech phase can in bias speech model 120 with increased probability the first n members set and in bias speech model
Have the 2nd n member set of the probability reduced associated in 120.
At operation D (124), speech recognition device 102 determines the one of voice input 110 using bias speech model 120
A or multiple transcription results.In some implementations, it may be voice input that can select the instruction of bias speech model 120
The transcription result 126 (such as " peperoni and mushroom ") in the top of 110 most accurate transcription and in response to asking
112 are asked to be returned to user equipment 108 (operation E (128)).For example, user equipment 108 then can using transcription result as
In text input form fields.
Fig. 2 is the meter for executing the example process 200 that voice input 210 is transcribed into text using bias speech model 228
The concept map of calculation system.Processing 200 is similar to the processing 100 of Fig. 1, but in processing 200, dialogue state historical data is at least
Part inputs 210 corresponding dialogue states for determining with voice.In general, dialogue state historical data mark is in given user
Dialog session in one or more dialogue states for previously having had occurred and that.In some cases, dialogue state historical record number
According to the dialogue state that can be identified before the dialogue state of new transcription request.Then, speech recognition device 102 can be commented
Estimate dialogue state flow model, with based on which dialogue state immediately in newly ask voice input before instruction come predict newly turn
Record the dialogue state of request.In some implementations, as described in following paragraphs, it can be used alone dialogue state history number
Dialogue state associated with transcription request is determined according to (that is, without other context datas).It in some implementations, can be with
It is used together including multiple signals from both context data and dialogue state historical data with true by speech recognition device 102
Fixed dialogue state associated with transcription request.
At operation A (204), user equipment 208 generates transcription request 212 and request 212 is sent to speech recognition device
202.Request 212 includes the audio data for the voice input 210 that characterization is detected by equipment 208.In this example, voice inputs
210 be the record that user gives an oral account " peperoni and mushroom ".Request 212 may include or not include that can be used for speech recognition
Device 202 transcribes other data of voice input 210.In some implementations, request 212 can also include application identifier,
Dialog identifier, dialogue state identifier, other context datas, dialogue state historical data, Session ID or these in two
A or more combination.In some implementations, request 212 may include dialogue state historical data, identify right
The last n dialogue state being had occurred and that in words session.Speech recognition device 202 can be by sudden and violent by Application Programming Interface (API)
Reveal to equipment 208, or passes through the transcription result by the instruction of dialogue state identifier corresponding with given request together with request
230 are supplied to user equipment 208, make 208 available conversations status history data of user equipment.For example, user equipment 208 can be to
Speech recognition device 202 submits the first transcription request, and in response, speech recognition device 202 provides transcription to user equipment 208
And dialogue state identifier associated with the first request as a result.Then, user equipment 208 can be to speech recognition device 202
Submit the second transcription request, the second transcription request include a part as dialogue state historical data with first (previous)
Ask associated dialogue state identifier.As described further below, dialogue state mark associated with the first request
Symbol then can be used to determine dialogue shape associated with the second request together with dialogue state flow model by speech recognition device 202
State.
In some implementations, other than dialogue state historical data or substitute dialogue state historical data, request
212 may include dialog session identifier.Dialog session identifier is instruction and 212 associated certain dialog sessions of request
Data.Speech recognition device 202 can be related to a series of transcriptions of same dialog session and asked using dialog session identifier to be associated with
It asks.For example, can first including dialog session identifier be transcribed request is sent to speech recognition device from user equipment 208
202.Speech recognition device 202 can determine dialogue state associated with the first transcription request, and can store and will determine
Dialogue state record associated with Session ID.Include same session identifier when speech recognition device 202 receives later
The second transcription request when, the record identification that speech recognition device 202 can access storage is associated with the first request previous right
Speech phase.Based on previous dialogue state and dialogue state flow model, speech recognition device 202 can determine related to the second request
The dialogue state of connection, and storage makes the second dialogue state and the relevant record of Session ID.Speech recognition device 202 can be after
Continue the subsequent dialog state that the processing continuously determines follow-up transcription request based on the preceding dialog state in same session.
At operation B (214), when receiving request 212, speech recognition device 202 identify speech recognition device 202 be determined as with
Associated dialogue state is asked in the last transcription for same dialog session received from user equipment 208.For example, complete
When at Pizza order, user 206 can provide a series of voice inputs, such as " take out ", followed by " thin skin ", followed by " meaning
Big sharp peppery sausage and mushroom ", these inputs are provided to speech recognition device in continuous transcription request.In response to receiving third
Request 212, speech recognition device 202 (can be highlighted) in the identical dialog session with dialogue state " 2 " in chart 216
Mark dialogue state associated with previous Request.In some implementations, speech recognition device 202 can be by being included within
Ask the dialog session identifier in 212 related to the information progress stored by speech recognition device 202 to determine previous dialogue
Session, the last one or the multiple dialogue states which determines same dialog session.In some implementations,
Speech recognition device 202 can be based on the dialogue state historical data included in the directly mark original state asked in 212 come really
Determine preceding dialog state.
At operation C (218), speech recognition device 202 determine most possibly it is associated with transcription request 212 (and therefore
Most probable and the voices input that request 212 includes are 110 associated) dialogue state.In some implementations, it can be based on
Dialogue state is determined in the one or more for operating mark at B (214) previous dialogue states and dialogue state flow model.It is right
Speech phase flow model is indicated generally at the dialogue state sequence for it is expected to occur in given dialogue, and is carried out herein with reference to Fig. 3
Description.
Fig. 3 depicts the representation of concept of example dialogue state flow model.Dialogue state flow model is represented as including multiple
Figure 30 0 of multiple side 304a-1 of node 302a-g node 302a-gs different with connection.Each of the dialogue indicated by model
Respective dialog state is indicated by the respective nodes 302a-g in Figure 30 0.Side 304a-1 indicates that the candidate between dialogue state turns
It changes.(indicate the first dialogue state) for example, node 302a and have to be respectively directed to node 302b (indicating the second dialogue state) and
Two outside the arrow 304a and 304b of 302c (indicating third dialogue state).Therefore, according to the model, the first dialogue shape is followed
The dialogue state of state 302a may be the second dialogue state 302b or third dialogue state 302c.However, because not from first
The node 302a of dialogue state is directed toward the edge point of each node of the four, the five, the 6th or the 7th dialogue state (302d-g), institute
These other dialogue states may can not possibly be flowed to dialogue from the first dialogue state.
In some implementations, dialogue state flow model can be by probability assignments to each pair of dialogue shape indicated in model
Conversion between state.For example, model instruction is second or third dialogue state 302a, 302b after the first dialogue state 302a
There are equal probabilities.However, after the 4th dialogue state 302d only back to the likelihood of the first dialogue state 302a
It is 40 percent, and it is percent to talk with the likelihood for proceeding to the 7th dialogue state 302g from the 4th dialogue state 302d
60.In some implementations, dialogue state flow model can be based on by speech recognition device or another computing system to
The analysis for the dialogue state sequence that time occurs in many dialog sessions from one or more user equipmenies and learn automatically
It practises.In some implementations, dialogue state flow model can be by user (application developer of the given dialogue of such as exploitation) hand
Dynamic definition.In some implementations, speech recognition device can be able to access that and using for phase in each of multiple dialogues
Answer model.
Referring again to Fig. 2 and discuss that operation C (218) can be based upon dialogue state set in some implementations
The determining voice input 210 (and request 212) that is accordingly scored at determines dialogue state, and dialogue state set instruction is each corresponding
Dialogue state and 210 likelihoods that match of request 212 or voice input.It is shown in table 220 and single session phase under Pizza
The example score of the set of associated five dialogue states.In some implementations, the selection of speech recognition device 202 has highest
The dialogue state of probability score.In the figure 2 example, dialogue state " 3 " tool corresponding with the Pizza dispensing choice phase of dialogue
There is highest probability score, and is determined to correspond to the dialogue state of transcription request 212.
In some implementations, dialogue state probability score can be based further on context associated with asking 212
Data.Speech recognition device 202 can be included within request 212 in context data and to the associated corresponding language of each dialogue state
Border data are compared, and corresponding context similarity score is determined for each dialogue state.In general, higher context similitude obtains
Divide and indicates the context data in request 212 and the closer matching between context data associated with given dialogue state.
In some realization methods, context similarity score can be based on such as user interface hash, position data and user profiles number
According to a plurality of types of context datas between similarity score weighted array.In some implementations, it distributes to each
Dialogue state (for example, table 220 the rightmost side row in show) final probability score can be based on context similarity score and
The weighted array of subsequence score.Subsequence score can be exported from the probability in dialogue state flow model, and can indicate to give
Probability of the one or more in the dialogue state of preceding dialogue state.
At operation D (222), E (226) and F (232), processing 200 is with the behaviour similar to the processing 100 described in Fig. 1
The mode for making C (118), D (124) and E (128) carries out.Specifically, it at operation D (222), is based on and is operating C (218)
The associated n members set of dialogue state of period selection, bias speech model 228.In operation E (226), speech recognition device
202 generate the transcription result 230 of voice input 110 using bias speech model 228.Finally, at operation F (232), will turn
Record result 230 is supplied to user equipment 208 from speech recognition device 202.It in some implementations, can be by transcription result 230
It can includes in dialog session by dialogue state identifier to be supplied to equipment 208, equipment 208 in conjunction with dialogue state identifier
In follow-up transcription request next dialogue state is determined for speech recognition device 202.
Referring now to Figure 4, showing for analyzing the transcription of voice input with the dialogue in the more interactive voice dialogues of determination
The flow chart of the example process 400 of state.In some implementations, corresponding n members set can be distributed to each dialogue
State, and these n members set can be used by speech recognition device with bias speech model at runtime later.For example, response
In determining that voice input is corresponding with given dialogue state, can be adjusted in language model and the n for giving dialogue state
The associated probability data of member.In some implementations, processing 400 can be used by speech recognition system to learn automatically pair
The dialogue state of words, and learn how to bias the language model of each dialogue state automatically.Therefore, in some realization methods
In, it need not be tracked using the application of the speech-recognition services of speech recognition system during being interacted with the given sequence of speech of user
Dialogue state stream, and the given transcription of speech recognition system need not clearly be notified to ask corresponding dialogue state, use
In the n member set of language model or both of the given transcription request of biasing.
In the stage 402, computing system receives the audio data that the voice of instruction user inputs from multiple computing devices.Voice
Input is typically the language for the part as dialogue that user says.For each user, dialogue may include and specific work
A series of dynamic relevant voice inputs.For example, for date arrange it is movable dialogue may include mark appointment at the beginning of,
Duration, position, description and the corresponding voice input of invitee.In some cases, user can be all available
Appointment arrange parameter provide input.In other cases, user can be only that a part of available appointment arranges parameter to provide
Input.In addition, the sequence for providing parameter in voice input can change in user.Therefore, even if voice input may be all
It is related to the dialogue for same activity (for example, appointment arranges), the quantity and sequence of voice input may become in dialog session
Change.In some implementations, computing system can receive voice input corresponding with multiple and different dialogues.Due to processing
400, which are usually directed to the determining dialogue state for certain dialog (can for example, about arrange) and n member set, system, can filter reception
Data only to include the data of corresponding with dialogue voice input.With other dialogue (such as Pizza order, finance hand over
Easily, social media is issued) data of corresponding voice input can be dropped.
At the stage 404, computing system optionally identifies context data associated with the voice input received.At some
In realization method, context data can be provided in asking the transcription of computing system by user equipment.Transcription request may be used also
With the audio data of the voice input including request transcription.Context data generally includes speech recognition device and can be used in determining voice
Any data (being different from voice input audio data) of the transcription of input.For example, user equipment can detect language in equipment
The screen signature of the display of user interface when sound inputs or in neighbouring computational representation equipment.In some cases, screen is signed
Can be true based on value associated with the user interface element when equipment detects voice input or shown by neighbouring equipment
Fixed hashed value.Screen signature (or other individual context data segments) may enough or to may not be enough to computing system true
Fixed dialogue state associated with voice input.In some implementations, given voice input can be with single context number
It is associated according to segment (for example, screen signature).In some implementations, voice input can be with multiple context data segment phases
Association (for example, screen signature and dialog prompt).Dialog prompt is a kind of context data, indicates pair of the estimation of user equipment
Speech phase.If user equipment reliably tracks dialogue state in session, dialog prompt can effectively serve as dialogue shape
State identifier.If user equipment does not track dialogue state reliably, computing system can determine and user equipment phase
The dialogue of less weight is born when associated dialogue state.In some implementations, computing system can not be identified and be received
Voice input associated context data, and dialogue state and n member set can be determined without context data.
At the stage 406, system determines that received voice input (or is at least filtered voice for analysis
Input) transcription.Transcription can be determined by the way that voice input is converted to the speech recognition device of text.Speech recognition device can be with
Assembly set including the audio data that voice inputs can be converted to text.May include language mould in these components
Type is typically configured as determining the probability of the term sequence in language.Language model can assess the term for transcription
Candidate sequence, and the transcription for selecting most probable term sequence to be provided as the output eventually as speech recognition device.At some
In realization method, since dialogue state and language model biased data (for example, n members set) may be not yet by processing 400
The stage determines, it is possible to determine the voice input during the stage 406 using language model general, without biasing
Transcription.In some implementations, computing system can simply obtain the language determined by other one or more computing systems
The transcription of sound input, itself need not receive the audio data of voice input, and generate one of transcription as processing 400
Point.
At the stage 408, computing system analysis voice input transcription with identify appear in it is every at least some transcriptions
The corresponding set of one or more of a n members.The corresponding n of each transcription mark can be provided as in the vector for representing transcription
Member set.The vector can be with indicator to the corresponding n members set of each transcription mark, without considering what n members occurred in transcription
Sequentially.In some implementations, each n members occurred in transcription and the corresponding arrow for being added to the transcription can be identified
In amount.In some implementations, the proper subclass of all n members only occurred in transcription can be identified and be added to this turn
In the respective vectors of record.For example, the subset of n members can be selected based on conspicuousness score associated with n members.For example, language
The less n members of the middle frequency of occurrences can be assigned higher conspicuousness score, and the frequency of occurrences is more in language n member (for example,
Pronoun, article, common adjective and noun) lower conspicuousness score can be assigned.For example, can select to be assigned super
The N members of the conspicuousness score of threshold score are crossed with included in the vector of transcription, or can select that there are preceding n in transcription
The n members (wherein n be predefined integer, such as 1,2,3,4 or more) of conspicuousness score.
At the stage 410, computing system generates transcript profile based on the similitude between transcription.It is worth noting that, should
Description is only related to transcript profile by way of example.Because each transcription corresponds only to individual voice input and is identified as transcribing
The single n members set of middle generation, so computing system can be based on similar with as described herein for the technology of transcript profile is generated
Technology be equally generated voice input group or n metasets are combined.Present document relates to the discussion of transcript profile to be therefore also applied for these its
The group of his type.
In some implementations, can the Semantic Similarity based on the n members occurred in transcription to transcription be grouped.
For example, in determining the transcript profile for dating the voice input for arranging application, the first transcript profile can be with having with the appointment time
Pass n members (for example, " noon " ", at 12 points in afternoon " ", the morning 8:30 ", " evening ", " at 4 points in afternoon ") transcription between formed；The
Two transcript profiles can with n related with dating site member (such as " meeting room ", " coffee-house ", " downtown ", " office
Room ", " street ", " street ", " floor ", " suite ") transcription between formed；And third transcript profile can with appointment
It describes to be formed between the transcription of related n first (for example, " meeting ", " lunch ", " videoconference ", " investigation ", " party ").One
In a little realization methods, the Semantic Similarity of transcription can be associated with n members one or more main in given transcription by identifying
It inscribes to determine.Theme can be by name Entity recognition engine identification, for example, n members is associated with theme, and can use base
The theme of the n members found in samples of text carrys out retrtieval sample.Once it is determined that theme, can pair with one or more phases
It is grouped with the relevant transcription of theme.In some implementations, can pair from the different but similar relevant transcription of theme
It is grouped.
It in some implementations, can be based on the context number for inputting associated mark with the voice for therefrom exporting transcription
Similitude between is grouped transcription.For example, can be between the transcription for the voice input signed with the first screen
The first transcript profile is formed, second transcript profile, etc. can be formed between the transcription for the voice input signed with the second screen.
It in another example, can be in the voice input that the time at 12 points (for example, 8 a.m. in afternoon) in first time period says
Transcription between form the first transcript profile, can be in the time in second time period (for example, afternoon 12:01 to 4 point) say
Second transcript profile, etc. is formed between the transcription of voice input.For example, it is also possible to based on other kinds of matching or similar language
Border data formation group, context data such as position data, user profile data, user consensus data, dialog prompt or works as
Detect the instruction of the one or more application run in equipment when Oral input.In some implementations, it can be based on
Similitude between inputting associated a plurality of types of contexts with the voice for therefrom exporting transcription is grouped transcription.Example
Such as, there can be the voice of similitude in both periods for voice input of signing and detected at user equipment in screen
Transcript profile is formed between the transcription of input.In some implementations, different types of context number can be weighted relative to each other
According to so that the similitude between certain types of context is more likely to influence grouping than other kinds of context.
In some implementations, transcript profile can be generated based on the Semantic Similarity of the n members occurred in transcription, and
It is not based on context data associated with the voice input of transcription is therefrom exported.In some implementations, can be based on from
The voice of middle export transcription inputs associated context data and generates transcript profile, and is not based on the n members occurred in transcription
Semantic Similarity.However, in some implementations, can be based on the Semantic Similarities of the n members that (i) occurs in transcription with
(ii) both context datas associated with the voice input of transcription is therefrom exported generate transcript profile.In realization method below
In, for example, semantically similar and with similar context data transcription can be grouped together.Therefore, if having language
Only the context of the transcription of slightly similar n members is very similar in justice, then they can be grouped, and if with semantically
Only the transcription of slightly similar n members is highly relevant semantically, then they can be grouped.
In some implementations, after the initial sets that computing system generates transcript profile, it can merge and be confirmed as
Similar group to reduce the sum of transcript profile.The final transcript profile set generated by computing system occurs with determination in dialogue
Respective dialog state is corresponding.Since speech recognition device can use each dialogue state bias speech model in different ways,
So it may be beneficial to be synthesized to group with the quantity for reducing the detectable dialogue state of speech recognition device.In particular, right
Group, which carries out synthesis, can eliminate redundancy group, and the separation (and therefore diversity between dialogue state) between increase group, with true
Protect the significant difference that language model biases between each dialogue state.For example, the first stage of dialogue can solicit and be used for
It dates the voice input of time started user, the second stage of dialogue can be solicited defeated for the voice for the end time user that dates
The phase III for entering, and talking with can solicit the voice input of the user of the number for the participant that dates.Because at these
There may be substance weights between n members in the voice input (for example, statement of number and time) in each stage in the stage
It is folded, it is possible to which that the transcription that voice inputs is merged into single group.The dialogue state of the determination generated from merging group therefore can be with
It is corresponding with three separate phases of interactive voice, because it is these ranks that can use identical n members set in an identical manner
Each of section effectively bias speech model.
Computing system can be according to the various technology identifications transcript profile to be merged.In some implementations, it comes from each
The n members set of group can be compared to the corresponding n members set from other each groups, and can be determined based on this comparison every
To the similarity score between the n member set of group.If the given similarity score between n members in group meets (for example, super
Cross) threshold similarity score, then it can merge these groups.Combined result can be include from all of each merging group or
At least some transcript profiles.For example, similitude can be determined based on the matched quantity or frequency between the n members of the group compared
Score.For example, if the grouping of the initial transcription based on context leads to first group and the second screen of the first screen signature value
Second group of signature value, but the transcription of the voice input between each group be largely it is identical, then can be by these
Group is merged into single group in the final group set generated by computing system.It in some implementations, can be continuous
Group is merged in iteration, until reaching capacity (for example, until having completed the iteration of pre-determined number, until
Similarity score between group be all unsatisfactory for threshold similarity score until, until remaining set quantity be no more than group it is predetermined most
Until big quantity or these combination).In some implementations, score can be by fetching from the corresponding n metasets each organized
COS distance between conjunction determines the similarity score between transcript profile.Can by vector from every group of generation n member (for example,
The distance between according to word bag technique) and determine vector, calculate COS distance.In some implementations, score can be with
By being lower dimensional space by n metaset conjunction and determining the similitude in lower dimensional space, the similarity score between transcript profile is determined.
Fig. 5 A and 5B are depicted for merging similar transcript profile to form the dialogue state phase for determining and occurring in dialogue
The concept map of the technology of corresponding final group of set.
In fig. 5, the Semantic Similarity based on the term (n members) in transcription is to initial transcript profile 504,506 and 510
It is grouped.For example, when context data it is unavailable or speech recognition device is configured with dialogue state flow model rather than
When context data detects the dialogue state of voice input transcription request at runtime, such grouping may be beneficial.
After generating initial group 504,506 and 510, determine that two in group 504 and 506 include similar n members set, and therefore
It is merged into single group 502.Combined group 502 and third initial group 510 then in given dialogue with corresponding dialogue state
508 is related to 512.
In figure 5B, based on the context of (and therefore associated with the voice input transcribed therefrom is determined) associated with transcription
Initial transcript profile 552,556 and 558 is grouped by data.Context grouping may be beneficial, for example, when context data and dialogue
When state has strong correlation, speech recognition device can transcribe voice input at runtime.In some implementations, for example, base
It can be based on dialogue state historical data or dialogue state stream mould to avoid for speech recognition device in the dialogue state that context determines
Type determines any need of dialogue state.After generating initial group 552,556 and 558, two in group 556 and 558
It is confirmed as including similar n members set, and is therefore merged into single group 554.Combined group 554 and the first initial group 552
Then related with corresponding dialogue state 560 and 562 in given dialogue.
Referring again to FIGS. 4, at the stage 414, processing 400 divides for each dialogue state determined at stage 410 and 412
Corresponding n members set with speech recognition device, for being biased when transcribing speech model corresponding with respective dialogue state
Language model.It in some implementations, can be from including occurring in corresponding with given dialogue state group of transcription
The set of the n members of given dialogue state is distributed in selection in all n members.In some implementations, it may be determined that in transcript profile
The counting of the middle number that each n members occur, and can be first (for example, can select full based on the n for counting the most frequent generation of selection
The n members of sufficient threshold count and/or the n members counted with highest that predetermined quantity can be selected).In some implementations, may be used
With from for example exclude meaningless term (for example, " ", "the", "one", " extremely ", " being used for " etc.) filtering n member set in
Select selected n members.Selected n members set can in association be stored with their own dialogue state.
In some implementations, transcript profile is created being based at least partially on context data associated with transcription
In the case of, at the stage 416, computing system can distribute corresponding language to each dialogue state determined at stage 410 and 412
Border data acquisition system, corresponding context data set can be used by speech recognition device to input or transcribe request and phase by given voice
Answer dialogue state associated.Unique mark dialogue state can be extrapolated to by distributing to the context data set of given dialogue state
Fingerprint.It therefore, can will be from the request when it includes that the voice input transcription of context data is asked that speech recognition device, which is received,
Context data is compared to the corresponding context data set for distributing to each dialogue state.If context data in the request
Matching is determined between one in the context data set distributed or is associated with by force, speech recognition device can identify the request
Belong to dialogue state corresponding with matched context data set.In some implementations, computing system distributes to dialogue
The context data set of state can be based on context associated with all or some transcription in the group corresponding to dialogue state
Data.For example, if quite multiple or most of transcriptions in given group are associated with the first screen signature value, it can be by first
Screen signature value distributes to dialogue state corresponding with the group.
In some implementations, the computing system at the stage 418 can determine in instruction dialog session and may occur
Dialogue state sequence dialogue state flow model.In some implementations, dialogue state flow model can be directed in rank
Other the one or more dialogue states of each dialogue state instruction determined in section 410 and 412 will be next in dialog session
The probability of a dialogue state.For example, in flow model shown in Fig. 3, the probability of the dialogue state 7 after dialogue state 4 is
0.6, and the probability of the dialogue state 1 after dialogue state 4 is 0.4.Any other dialogue state after dialogue state 4
Probability is zero.In some implementations, computing system can be identified based on what is occurred in the record of dialog history session
Dialogue state sequence determine dialogue state flow model.For example, if in the record of dialog history session first dialogue shape
State is followed by the second dialogue state in 80% time and is followed by third dialogue state in 20% time,
Then can in dialogue state flow model to from the first dialogue state to the second dialogue state transformation distribution 0.8 probability, and
And it can be to the probability of the transformation distribution 0.2 from the first dialogue state to third dialogue state.
Fig. 6 is for using based on inputting associated dialogue state with voice and the bias speech model that selects is to voice
Input executes the flow chart of the example process 600 of speech recognition.It in some implementations, can be by being retouched in such as Fig. 1 and Fig. 2
The speech recognition computing system for the speech recognition device stated executes processing 600.It can also use in the processing 400 of Fig. 4 about right
Data that speech phase, n members, context data and dialogue state flow model determine execute processing 600.
At the stage 602, speech recognition system receives the voice input that request is transcribed.In some implementations, rank
The system of section 602 also receives context data associated with voice input.Context data can indicate detecting voice input
Time or the user equipment nearby detected situation.For example, context data can indicate to characterize when detecting voice input
The combination of the screen signature of the display of user equipment, the context data of dialog prompt or these and other types.
At the stage 606, the input of speech recognition system analysis voice, context data or both are to determine pair of voice input
Speech phase.In some implementations, can by will transcribe request in context data and it is stored and with it is corresponding
The associated special context data acquisition system of dialogue state is matched to determine dialogue state (stage 608).In some realization sides
In formula, dialogue shape can be determined based on the dialogue state flow model of dialogue state historical data and prediction dialogue state sequence
State.Dialogue state historical data can indicate one or more dialogue states before the dialogue state of current transcription request,
And it can be prestored, or can be provided in the request of the transcription from user equipment by speech recognition system.
At the stage 612, speech recognition system mark already is allocated to the n of the dialogue state determined at the stage 606
Member set.At the stage 614, the n members set of application identities is with bias speech model.For example, can increase in language model
With the relevant probability of n members in the n member set of mark so that language model more likely selects the n members of distribution.In some realizations
In mode, after being inputted receiving voice and determining the dialogue state of voice input, bias speech model.In some realities
In existing mode, speech recognition system can be that each dialogue state generates bias speech model before receiving voice input.With
Afterwards, when receiving the request of transcription voice input, the dialogue state that voice system can be accessed with voice inputs is corresponding pre-
Bias speech model.At the stage 616, it can use the language model of biasing that voice input is transcribed into text.It then can be with
The text of transcription is sent to the computing device of request transcription.
Fig. 7 shows the computing device that can be used to implement computer implemented method and other technologies described herein
700 and mobile computing device example.Computing device 700 is intended to mean that various forms of digital computers, meter such as on knee
Calculation machine, desktop computer, work station, personal digital assistant, server, blade server, mainframe and other meters appropriate
Calculation machine.Mobile computing device is intended to indicate various forms of mobile devices, such as personal digital assistant, cellular phone, intelligence electricity
Words and other similar computing devices.Component, their connection and relationship shown here and their function are only meaned
It illustratively, is not intended to limit described herein and/or claimed invention realization method.
Computing device 700 includes processor 702, memory 704, storage device 706, is connected to memory 704 and multiple
The high-speed interface 708 of high-speed expansion ports 710 and the low-speed interface for being connected to low-speed expansion port 714 and storage device 706
712.Processor 702, memory 704, storage device 706, high-speed interface 708, high-speed expansion ports 710 and low-speed interface
Each of 712 use various bus interconnections, and can be mounted on public mainboard or otherwise install as needed.
Processor 702 can handle the instruction for being executed in computing device 700, including be stored in memory 704 or storage device
Instruction in 706 shows on external input/output device (display 716 for being such as coupled to high-speed interface 708) and is used for
The graphical information of GUI.In other realization methods, it can be properly used multiple processors and/or multiple buses and multiple deposit
Reservoir and a plurality of types of memories.Moreover, multiple computing devices can be connected, each equipment provides a part for necessary operation
(for example, as server group, one group of blade server or multicomputer system).
Memory 704 is in 700 inner storag information of computing device.In some implementations, memory 704 is one or more
A volatile memory-elements.In some implementations, memory 704 is Nonvolatile memery unit.Memory 704
Can be another form of computer-readable medium, such as disk or CD.
Storage device 706 can be that computing device 700 provides massive store.In some implementations, storage device
706 can be or including computer-readable medium, such as floppy device, hard disc apparatus, compact disk equipment or tape unit, flash memory or
Other similar solid storage devices, or include the equipment array of equipment or other configurations in storage area network.Computer
Program product can also include the instruction for executing such as above-mentioned one or more methods upon being performed.Computer program product
Such as computer or machine of memory 704, storage device 706 or the memory on processor 702 can also be tangibly embodied in
In device readable medium.
High-speed interface 708 manages the bandwidth-intensive operations of computing device 700, and low-speed interface 712 manages lower bandwidth
Intensive.This function distribution is only exemplary.In some implementations, high-speed interface 708 is coupled to storage
Device 704, display 716 (for example, passing through graphics processor or accelerator) and high-speed expansion ports 710 can receive each
Kind expansion card (not shown).In the realization method, low-speed interface 712 is coupled to storage device 706 and low-speed expansion port
714.The low-speed expansion port 714 that may include various communication port (for example, USB, bluetooth, Ethernet, wireless ethernet) can
To be for example coupled to one or more input-output apparatus by network adapter, such as keyboard, pointing device, scanner or
The networked devices of such as switch or router.
As shown, computing device 700 can be realized with many different forms.For example, it may be implemented as standard
Server 720, or be implemented multiple times in such server group.In addition, it can be in such as laptop computer 722
It is realized in personal computer.It can also be implemented as a part for frame server system 724.Alternatively, it is set from calculating
Standby 700 component can be combined with the other assemblies (not shown) in such as mobile device of mobile computing device 750.These set
Each of standby can include one or more of computing device 700 and mobile computing device 750, and whole system can be with
It is made up of the multiple computing devices to communicate with one another.
Mobile computing device 750 include processor 752, memory 764, such as input-output apparatus of display 754,
Communication interface 766 and transceiver 768 etc. component.Mobile computing device 750 can also be provided with storage device, such as miniature
Driver or other equipment, to provide additional storage.Processor 752, memory 764, display 754, communication interface 766 with
And each of transceiver 768 uses various bus interconnections, and several components may be mounted on public mainboard or with other
Mode is suitably installed.
Processor 752 can execute the instruction in mobile computing device 750, including the instruction being stored in memory 764.
Processor 752 can be implemented as including the chipset detached with the chip of multiple analog- and digital- processors.Processor 752 can be with
Such as provide the control of such as user interface, by mobile computing device for the coordination of the other assemblies of mobile computing device 750
The application of 750 operations and the wireless communication of mobile computing device 750.
Processor 752 by control interface 758 and can be coupled to display interface device 756 and the user of display 754
Communication.Display 754 can be such as TFT (Thin Film Transistor-LCD) displays or OLED (organic light-emitting diodes
Pipe) display or other display technologies appropriate.Display interface 756 may include for driving display 754 to be in user
The proper circuit of existing figure and other information.Control interface 758 can receive it is from the user order and by they convert with
In submitting to processor 752.In addition, external interface 762 can provide the communication with processor 752, to realize mobile computing
The near region field communication of equipment 750 and other equipment.External interface 762 can for example provide cable modem in some implementations
Letter, or wireless communication is provided in other realization methods, and multiple interfaces can also be used.
Memory 764 stores the information in mobile computing device 750.Memory 764 may be implemented as computer-readable
One or more of medium, volatile memory-elements or Nonvolatile memery unit.Extended menory 774 can also be by
It provides and mobile computing device 750 is connected to by expansion interface 772, which may include such as SIMM (single
Row in-line memory module) card interface.Extended menory 774 can be that mobile computing device 750 provides additional storage sky
Between, or can also be the storage of mobile computing device 750 application or other information.Specifically, extended menory 774 can wrap
The instruction of execution or the above-mentioned processing of supplement is included, and can also include security information.Thus, for example, extension storage can be provided
Device 774 as mobile computing device 750 security module, and can with allow use safely mobile computing device 750
Instruction program.Furthermore it is possible to security application and other information are provided via SIMM cards, such as in a manner of it can not crack
Identification information is placed on SIMM cards.
As discussed below, memory may include that for example flash memory and or NVRAM memory (deposit by non-volatile random
Access to memory).Computer program product includes the instruction for executing such as above-mentioned one or more methods upon being performed.It calculates
Machine program product can be computer-readable medium or machine readable media, such as memory 764, extended menory 774 or place
Manage the memory on device 752.It in some implementations, can be for for example passing through transceiver 768 or external interface 762
Transmitting signal receives computer program product.
Mobile computing device 750 can be carried out wireless communication by communication interface 766, and communication interface 766 if necessary may be used
To include digital signal processing circuit.Communication interface 766 can provide such as GSM audio calls (global system for mobile communications),
SMS (short message service), EMS (enhanced messaging transmitting-receiving service) or MMS message transmitting-receiving (multimedia messaging service), CDMA (codes
Point multiple access), TDMA (time division multiple acess), PDC (personal digital cellular), WCDMA (wideband code division multiple access), CDMA2000 or GPRS
Communication under the various patterns or agreement of (General Packet Radio Service) etc..For example, this communication can be by using radio frequency
Transceiver 768 carries out.Further, it is possible to use short distance occurs for such as bluetooth, WiFi or other this transceiver (not shown)
Communication.In addition, GPS (global positioning system) receiver module 770 can provide additional and navigation to mobile computing device 750
Wireless data related and related with position can suitably be made by the application run on mobile computing device 750
With.
Mobile computing device 750 can also audibly be communicated using audio codec 760, audio codec
760 can receive verbal information from user and be converted into available digital information.Audio codec 760 equally can be all
Audible sound is such as generated for user for example, by the loud speaker in the receiver of mobile computing device 750.This sound can be with
Including sounds from voice phone calls, it may include the sound (for example, speech message, music file etc.) recorded and go back
May include the sound generated by the application run on mobile computing device 750.
As shown, mobile computing device 750 may be implemented as many different forms.For example, it can be implemented
For cellular phone 780.It can also be implemented as smart phone 782, personal digital assistant or other similar mobile devices
A part.
Can Fundamental Digital Circuit, integrated circuit, the ASIC (application-specific integrated circuit) specially designed, computer hardware,
The various realization methods of system described herein and technology are realized in firmware, software and/or a combination thereof.These various realization methods
May include the realization method in executable and/or interpretable on programmable systems one or more computer programs, it should
Programmable system includes：At least one programmable processor can be special or general, couple to be received from storage system
Data and instruction, and data and instruction are sent to storage system；At least one input equipment；And at least one output is set
It is standby.
These computer programs (also referred to as program, software, software application or code) include for programmable processing
The machine instruction of device, and can be realized with the programming language and/or compilation of level process and/or object-oriented/machine language.
As it is used herein, term machine readable media and computer-readable medium refer to for providing machine to programmable processor
Instruction and/or any computer program products of data, device and/or equipment are (for example, disk, CD, memory, programmable
Logical device (PLD)), including receive machine instruction machine readable medium as a machine-readable signal.The machine readable letter of term
It number refer to any signal for providing machine instruction and/or data to programmable processor.
In order to provide the interaction with user, system described herein and technology can be realized on computers, the computer
With the display equipment for showing information to user (for example, CRT (cathode-ray tube) or LCD (liquid crystal display) monitoring
Device) and user can by its to computer provide input keyboard and pointer device (for example, mouse or trace ball).Its
The equipment of his type can be used for providing the interaction with user；For example, it can be any type of to be supplied to the feedback of user
Sense feedback (such as visual feedback, audio feedback or touch feedback)；And it can be with including sound, voice or sense of touch
Any form receives input from the user.
System described herein and technology can be including aft-end assemblies (for example, as data server) or including in
Between part component (for example, application server) or including front end assemblies (for example, with graphic user interface or Web browser
Client computer, user can be interacted by the realization of itself and system described herein and technology) or it is this after
It is realized in any combination of computing system at end, middleware or front end assemblies.The component of system can pass through any form or Jie
The digital data communications (such as communication network) of matter interconnects.The example of communication network includes LAN (LAN), wide area network (WAN)
And internet.
Computing system may include client and server.Client and server is generally remote from each other and generally passes through
Communication network interacts.The relationship of client and server is by running computer program generation on the respective computers
, and there is client-server relation each other.
The personal information (for example, context data) about user is collected in this paper systems, method, equipment and other technologies
Or in the case of personal information being utilized, control program can be provided a user or feature collects user information (for example, closing
Information in the current location of the social networks of user, social action or activity, occupation, the preference of user or user) or control
Whether and/or how system receives the chance of possibility and the more relevant content of user from content server.Furthermore it is possible to depositing
Specific data is handled in one or more ways before storage or use so that removes personal recognizable information.For example, can to
The identity at family is handled so that cannot be determined personal recognizable information to user, or can obtained location information (such as
City, postcode or state grade) in the case of by the geographical location generalization of user so that not can determine that the certain bits of user
It sets.Therefore, user can be for controlling how collecting the information about user and being used by content server.
Although various realization methods are described in detail above, other modifications are possible.In addition, being retouched in attached drawing
The logic flow painted does not need shown particular order or consecutive order to realize desired result.Furthermore it is possible to from being retouched
The flow stated provides other steps, or can with removal step, and other assemblies can be added to described system or
It is removed from described system.Therefore, other realization methods are within the scope of the claims.
Claims (20)
1. a method of computer implementation, including：
The audio data of instruction voice input is received at computing system；
Certain dialog state corresponding with voice input is corresponded to is determined from multiple dialogue states；
N members set associated with the certain dialog state corresponding to voice input is identified, wherein the n members set
At least based on n members in the n members set, frequently occurring in history voice corresponding with dialogue state input, and with
The certain dialog state is associated；
In response to identifying n members set associated with the certain dialog state corresponding to voice input, by adjusting language
Model carrys out bias speech model to the probability score of the n members instruction in the n members set；And
The voice is transcribed using the language model of adjustment to input.
2. computer implemented method according to claim 1, wherein the multiple dialogue state indicates respectively and is related to spy
Multiple stages determining task, being interacted with the user speech that computing device carries out.
3. computer implemented method according to claim 1 or 2, further includes：
The input of the second voice is received at computing system；
The the second certain dialog state for corresponding to second voice input is determined from the multiple dialogue state；And
The 2nd n member set associated with the second certain dialog state corresponding to second voice input is identified,
In the 2nd n member set be different from n members set associated with the certain dialog state inputted corresponding to the voice.
4. computer implemented method according to any one of the preceding claims inputs wherein determining with the voice
The corresponding certain dialog state includes：
It is special from mark in the multiple dialogue state corresponding with the second voice input before voice input second
Dialogue state is determined, wherein voice input and second voice input all refer to same task；And
Occurred after the second certain dialog state based on the possibility indicated among the multiple dialogue state one or
The data of multiple dialogue states determine the certain dialog state corresponding with voice input.
5. computer implemented method according to any one of the preceding claims inputs wherein determining with the voice
The corresponding certain dialog state includes：
Generate the transcription of the voice input；And
Determine that the one or more n occurred in the transcription that the voice inputs are first and associated with the certain dialog state
One or more of n member set n members between matching.
6. computer implemented method according to any one of the preceding claims, wherein determining that the matching includes true
It is scheduled on the one or more n members occurred in the transcription of the voice input and n associated with certain dialog state members
Semantic relation between one or more of set n members.
7. computer implemented method according to any one of the preceding claims further includes receiving instruction and institute's predicate
Sound inputs the data of associated context,
Wherein determine that the certain dialog state corresponding with voice input includes：Based on related to voice input
The context of connection identifies the certain dialog state.
8. computer implemented method according to claim 7, wherein：
When context associated with voice input is received including being characterized in the voice input, receiving, the voice is defeated
The data of the display of the user interface at computing device entered, and
Identifying the certain dialog state based on context associated with voice input includes：Based on being characterized in reception voice
The data that the display of the user interface at the computing device of voice input is received when input, identify the certain dialog state.
9. computer implemented method according to any one of the preceding claims further includes being received at computing system
Indicate that the voice inputs the application identifier of the application at be oriented to computing device,
Wherein the multiple dialogue state is related to inputting the application particular task of be oriented to application for the voice.
10. a method of computer implementation, including：
Obtain the multiple transcriptions for the voice input for corresponding to the different conditions in multimode dialogue；
For each transcription in the multiple transcription, the n member set occurred in the transcription is identified；
It is confirmed as phase by distributing n member set in the multiple transcription, being identified to each of multiple transcript profiles
The corresponding transcription subset closed, generates the multiple transcript profile；
Based on the multiple transcript profile, determine multiple dialogue states that instruction occurs in multimode dialogue and with each dialogue
The model of the associated corresponding n members set of state；And
The model is provided in speech recognition with bias speech model.
11. computer implemented method according to claim 10, further includes：
By the way that each of the multiple transcript profile to be distributed to the respective dialog state in the multiple dialogue state come really
Fixed the multiple dialogue state so that each of the multiple dialogue state is corresponding with the respective subset of the transcript profile,
And it is different from each other for each subset of the transcript profile in each of the multiple dialogue state,
Wherein based on it is occurring in the transcription in the respective subset of the transcript profile corresponding with dialogue state, identified
N member set, selection and the associated corresponding n members set of each dialogue state in the multiple dialogue state.
12. according to the computer implemented method described in claim 10 or claim 11, wherein the multiple dialogue state
In the first dialogue state correspond to the transcript profile hostility it is self-supporting, first subset includes that two or more described turn
The transcript profile of record group.
13. the computer implemented method according to any one of claim 10 to 12, wherein generating the multiple turn
Record group includes：Formation is confirmed as semantic similar transcript profile each other.
14. the computer implemented method according to any one of claim 10 to 13, further includes：
Receive the data for at least some sequences being submitted in multimode dialogue for indicating the voice input；And
At least based on at least some sequences being submitted in multimode dialogue indicated in the voice input
Data determine that the sequence of the dialogue state in the multiple dialogue state, the sequence of the dialogue state cope with each phase
Other one or more dialogue states after the respective dialog state of speech phase instruction or the respective dialog state it
Other preceding one or more dialogue states.
15. the computer implemented method according to any one of claim 10 to 14, wherein the voice input
Multiple transcriptions include (each respective dialog state in the multiple dialogue state in talking with for the multimode) and institute
State multiple transcriptions of the corresponding voice input of the respective dialog state in multiple dialogue states.
16. the computer implemented method according to any one of claim 10 to 15, further includes：Receive instruction with it is right
The data of associated corresponding context should be inputted in the voice of at least some of the multiple transcription,
Wherein generating the multiple transcript profile includes：Be based further on instruction with corresponding in the multiple transcription it is described at least
The voice of some inputs the data of associated corresponding context, is grouped to transcription.
17. computer implemented method according to claim 16, wherein with corresponding to first in the multiple transcription
First voice of transcription inputs associated corresponding context：It is characterized in when the first voice input is received described in receiving
The data of the display of user interface at the computing device of first voice input.
18. a kind of computing system, including：
One or more processors；And
One or more is stored with the computer-readable medium of instruction on it, and described instruction causes holding for operation when executed
Row, the operation include：
Receive the audio data of instruction voice input；
Certain dialog state corresponding with voice input is determined from multiple dialogue states；
N members set associated with the certain dialog state corresponding to voice input is identified, wherein the n members set
At least based on it is in n member set, history voice corresponding with dialogue state input in frequently occur n member, and with it is described
Certain dialog state is associated；
In response to identifying n members set associated with the certain dialog state corresponding to voice input, by improving by n members
The probability score of the language model instruction of n members in set, adjusts language model；And
The voice is transcribed using the language model of adjustment to input.
19. computing system according to claim 18, wherein the multiple dialogue state indicates respectively and is related to particular task
, the multiple stages that interact of user speech carried out with computing device.
20. according to the computing system described in claim 18 or claim 19, wherein the operation further includes：
Receive the input of the second voice；
The second certain dialog state corresponding with the second voice input is determined from the multiple dialogue state；And
The 2nd n member set associated with the second certain dialog state corresponding to second voice input is identified,
Described in the 2nd n member set be different from n members set associated with the certain dialog state inputted corresponding to voice.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
CN202210438719.8A CN114974234A (en) | 2016-03-16 | 2016-11-30 | Determining dialog states for a language model |
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/071,651 | 2016-03-16 | ||
US15/071,651 US9978367B2 (en) | 2016-03-16 | 2016-03-16 | Determining dialog states for language models |
PCT/US2016/064065 WO2017160355A1 (en) | 2016-03-16 | 2016-11-30 | Determining dialog states for language models |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202210438719.8A Division CN114974234A (en) | 2016-03-16 | 2016-11-30 | Determining dialog states for a language model |
Publications (2)
Publication Number | Publication Date |
---|---|
CN108463849A true CN108463849A (en) | 2018-08-28 |
CN108463849B CN108463849B (en) | 2022-05-03 |
Family
ID=57570442
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201680078893.9A Active CN108463849B (en) | 2016-03-16 | 2016-11-30 | Computer-implemented method and computing system |
CN202210438719.8A Pending CN114974234A (en) | 2016-03-16 | 2016-11-30 | Determining dialog states for a language model |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202210438719.8A Pending CN114974234A (en) | 2016-03-16 | 2016-11-30 | Determining dialog states for a language model |
Country Status (6)
Country | Link |
---|---|
US (4) | US9978367B2 (en) |
EP (3) | EP3594941B1 (en) |
JP (1) | JP6569009B2 (en) |
KR (1) | KR102151681B1 (en) |
CN (2) | CN108463849B (en) |
WO (1) | WO2017160355A1 (en) |
Cited By (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN109492083A (en) * | 2018-11-05 | 2019-03-19 | 北京奥法科技有限公司 | A method of more wheel human-computer intellectualizations are realized based on list content |
CN110442438A (en) * | 2019-02-26 | 2019-11-12 | 北京蓦然认知科技有限公司 | Task cooperative method, equipment and system between a kind of more equipment |
CN112559721A (en) * | 2020-12-25 | 2021-03-26 | 北京百度网讯科技有限公司 | Method, apparatus, device, medium and program product for adjusting man-machine dialog system |
Families Citing this family (62)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
BR112015014830B1 (en) * | 2012-12-28 | 2021-11-16 | Sony Corporation | DEVICE AND METHOD OF INFORMATION PROCESSING, AND MEMORY STORAGE MEANS. |
US9978367B2 (en) * | 2016-03-16 | 2018-05-22 | Google Llc | Determining dialog states for language models |
US10677881B2 (en) * | 2016-03-29 | 2020-06-09 | Government of the United States as represented by Director National Security Agency | Map assisted inertial navigation |
CN107943405A (en) * | 2016-10-13 | 2018-04-20 | 广州市动景计算机科技有限公司 | Sound broadcasting device, method, browser and user terminal |
US20180114527A1 (en) * | 2016-10-25 | 2018-04-26 | IPsoft Incorporated | Methods and systems for virtual agents |
US9959864B1 (en) * | 2016-10-27 | 2018-05-01 | Google Llc | Location-based voice query recognition |
US10311860B2 (en) * | 2017-02-14 | 2019-06-04 | Google Llc | Language model biasing system |
US11093716B2 (en) * | 2017-03-31 | 2021-08-17 | Nec Corporation | Conversation support apparatus, conversation support method, and computer readable recording medium |
KR102380717B1 (en) * | 2017-04-30 | 2022-03-31 | 삼성전자주식회사 | Electronic apparatus for processing user utterance and controlling method thereof |
US10943583B1 (en) * | 2017-07-20 | 2021-03-09 | Amazon Technologies, Inc. | Creation of language models for speech recognition |
US10515625B1 (en) | 2017-08-31 | 2019-12-24 | Amazon Technologies, Inc. | Multi-modal natural language processing |
WO2019111346A1 (en) * | 2017-12-06 | 2019-06-13 | ソースネクスト株式会社 | Full-duplex speech translation system, full-duplex speech translation method, and program |
JP2019106054A (en) * | 2017-12-13 | 2019-06-27 | 株式会社東芝 | Dialog system |
DE102018202185A1 (en) * | 2018-02-13 | 2019-08-14 | Divvoice UG (haftungsbeschränkt) | Device for optimizing a gastronomic operation |
US10664512B1 (en) | 2018-02-13 | 2020-05-26 | Snap Inc. | Query matching to media collections in a messaging system |
US11676220B2 (en) | 2018-04-20 | 2023-06-13 | Meta Platforms, Inc. | Processing multimodal user input for assistant systems |
US11886473B2 (en) | 2018-04-20 | 2024-01-30 | Meta Platforms, Inc. | Intent identification for agent matching by assistant systems |
US10339919B1 (en) * | 2018-04-20 | 2019-07-02 | botbotbotbot Inc. | Task-independent conversational systems |
US11715042B1 (en) | 2018-04-20 | 2023-08-01 | Meta Platforms Technologies, Llc | Interpretability of deep reinforcement learning models in assistant systems |
US10782986B2 (en) | 2018-04-20 | 2020-09-22 | Facebook, Inc. | Assisting users with personalized and contextual communication content |
US11347801B2 (en) * | 2018-05-07 | 2022-05-31 | Google Llc | Multi-modal interaction between users, automated assistants, and other computing services |
US20220103683A1 (en) * | 2018-05-17 | 2022-03-31 | Ultratec, Inc. | Semiautomated relay method and apparatus |
KR20190134107A (en) * | 2018-05-24 | 2019-12-04 | 삼성전자주식회사 | Electronic device which is processing user's voice and method for providing voice recognition control thereof |
JP1621612S (en) | 2018-05-25 | 2019-01-07 | ||
US11093533B2 (en) * | 2018-06-05 | 2021-08-17 | International Business Machines Corporation | Validating belief states of an AI system by sentiment analysis and controversy detection |
US10956462B1 (en) * | 2018-06-21 | 2021-03-23 | Amazon Technologies, Inc. | System answering of user inputs |
US10923128B2 (en) * | 2018-08-29 | 2021-02-16 | Cirrus Logic, Inc. | Speech recognition |
US10891950B2 (en) * | 2018-09-27 | 2021-01-12 | International Business Machines Corporation | Graph based prediction for next action in conversation flow |
US11183176B2 (en) * | 2018-10-31 | 2021-11-23 | Walmart Apollo, Llc | Systems and methods for server-less voice applications |
US11195524B2 (en) | 2018-10-31 | 2021-12-07 | Walmart Apollo, Llc | System and method for contextual search query revision |
US11404058B2 (en) | 2018-10-31 | 2022-08-02 | Walmart Apollo, Llc | System and method for handling multi-turn conversations and context management for voice enabled ecommerce transactions |
US11238850B2 (en) | 2018-10-31 | 2022-02-01 | Walmart Apollo, Llc | Systems and methods for e-commerce API orchestration using natural language interfaces |
US11043214B1 (en) * | 2018-11-29 | 2021-06-22 | Amazon Technologies, Inc. | Speech recognition using dialog history |
CN113168830A (en) | 2018-11-30 | 2021-07-23 | 谷歌有限责任公司 | Speech processing |
US10573312B1 (en) | 2018-12-04 | 2020-02-25 | Sorenson Ip Holdings, Llc | Transcription generation from multiple speech recognition systems |
US11170761B2 (en) | 2018-12-04 | 2021-11-09 | Sorenson Ip Holdings, Llc | Training of speech recognition systems |
US10388272B1 (en) | 2018-12-04 | 2019-08-20 | Sorenson Ip Holdings, Llc | Training speech recognition systems using word sequences |
US11017778B1 (en) | 2018-12-04 | 2021-05-25 | Sorenson Ip Holdings, Llc | Switching between speech recognition systems |
US11151332B2 (en) | 2019-03-07 | 2021-10-19 | International Business Machines Business | Dialog based speech recognition |
US11954453B2 (en) * | 2019-03-12 | 2024-04-09 | International Business Machines Corporation | Natural language generation by an edge computing device |
CN110012166B (en) * | 2019-03-31 | 2021-02-19 | 联想(北京)有限公司 | Information processing method and device |
WO2020222539A1 (en) * | 2019-05-02 | 2020-11-05 | Samsung Electronics Co., Ltd. | Hub device, multi-device system including the hub device and plurality of devices, and method of operating the same |
CN113785354A (en) * | 2019-05-06 | 2021-12-10 | 谷歌有限责任公司 | Selectively activating on-device speech recognition and using recognized text in selectively activating NLUs on devices and/or fulfillment on devices |
CN110164020A (en) * | 2019-05-24 | 2019-08-23 | 北京达佳互联信息技术有限公司 | Ballot creation method, device, computer equipment and computer readable storage medium |
KR102281581B1 (en) * | 2019-07-17 | 2021-07-23 | 에스케이텔레콤 주식회사 | Method and Apparatus for Dialogue State Tracking for Use in Goal-oriented Dialogue System |
US11133006B2 (en) * | 2019-07-19 | 2021-09-28 | International Business Machines Corporation | Enhancing test coverage of dialogue models |
US11057330B2 (en) | 2019-08-26 | 2021-07-06 | International Business Machines Corporation | Determination of conversation threads in a message channel based on conversational flow and semantic similarity of messages |
KR20220010034A (en) * | 2019-10-15 | 2022-01-25 | 구글 엘엘씨 | Enter voice-controlled content into a graphical user interface |
KR20210044559A (en) | 2019-10-15 | 2021-04-23 | 삼성전자주식회사 | Method and device for determining output token |
US11567788B1 (en) | 2019-10-18 | 2023-01-31 | Meta Platforms, Inc. | Generating proactive reminders for assistant systems |
US11636438B1 (en) | 2019-10-18 | 2023-04-25 | Meta Platforms Technologies, Llc | Generating smart reminders by assistant systems |
US20210158803A1 (en) * | 2019-11-21 | 2021-05-27 | Lenovo (Singapore) Pte. Ltd. | Determining wake word strength |
US11270080B2 (en) | 2020-01-15 | 2022-03-08 | International Business Machines Corporation | Unintended bias detection in conversational agent platforms with machine learning model |
JP2021135412A (en) * | 2020-02-27 | 2021-09-13 | ソニーグループ株式会社 | Information processing device, information processing method, and program |
US11508361B2 (en) * | 2020-06-01 | 2022-11-22 | Amazon Technologies, Inc. | Sentiment aware voice user interface |
US11488604B2 (en) | 2020-08-19 | 2022-11-01 | Sorenson Ip Holdings, Llc | Transcription of audio |
US11532313B2 (en) * | 2020-08-27 | 2022-12-20 | Google Llc | Selectively storing, with multiple user accounts and/or to a shared assistant device: speech recognition biasing, NLU biasing, and/or other data |
DE102021109265A1 (en) | 2020-08-31 | 2022-03-03 | Cognigy Gmbh | Procedure for optimization |
US11533279B2 (en) | 2021-03-30 | 2022-12-20 | International Business Machines Corporation | Method for electronic messaging using image based noisy content |
US11683283B2 (en) | 2021-03-30 | 2023-06-20 | International Business Machines Corporation | Method for electronic messaging |
US11627223B2 (en) * | 2021-04-22 | 2023-04-11 | Zoom Video Communications, Inc. | Visual interactive voice response |
US11252113B1 (en) * | 2021-06-15 | 2022-02-15 | Drift.com, Inc. | Proactive and reactive directing of conversational bot-human interactions |
Citations (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20030091163A1 (en) * | 1999-12-20 | 2003-05-15 | Attwater David J | Learning of dialogue states and language model of spoken information system |
US20030149561A1 (en) * | 2002-02-01 | 2003-08-07 | Intel Corporation | Spoken dialog system using a best-fit language model and best-fit grammar |
CN101477798A (en) * | 2009-02-17 | 2009-07-08 | 北京邮电大学 | Method for analyzing and extracting audio data of set scene |
CN104508739A (en) * | 2012-06-21 | 2015-04-08 | 谷歌公司 | Dynamic language model |
US20150382047A1 (en) * | 2014-06-30 | 2015-12-31 | Apple Inc. | Intelligent automated assistant for tv user interactions |
CN105378708A (en) * | 2013-06-21 | 2016-03-02 | 微软技术许可有限责任公司 | Environmentally aware dialog policies and response generation |
Family Cites Families (192)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US4820059A (en) | 1985-10-30 | 1989-04-11 | Central Institute For The Deaf | Speech processing apparatus and methods |
US5477451A (en) | 1991-07-25 | 1995-12-19 | International Business Machines Corp. | Method and system for natural language translation |
US5267345A (en) | 1992-02-10 | 1993-11-30 | International Business Machines Corporation | Speech recognition apparatus which predicts word classes from context and words from word classes |
DE69326431T2 (en) | 1992-12-28 | 2000-02-03 | Toshiba Kawasaki Kk | Voice recognition interface system that can be used as a window system and voice mail system |
TW323364B (en) | 1993-11-24 | 1997-12-21 | At & T Corp | |
US5638487A (en) | 1994-12-30 | 1997-06-10 | Purespeech, Inc. | Automatic speech recognition |
US5715367A (en) | 1995-01-23 | 1998-02-03 | Dragon Systems, Inc. | Apparatuses and methods for developing and using models for speech recognition |
DE19533541C1 (en) | 1995-09-11 | 1997-03-27 | Daimler Benz Aerospace Ag | Method for the automatic control of one or more devices by voice commands or by voice dialog in real time and device for executing the method |
US6173261B1 (en) | 1998-09-30 | 2001-01-09 | At&T Corp | Grammar fragment acquisition using syntactic and semantic clustering |
US6397180B1 (en) | 1996-05-22 | 2002-05-28 | Qwest Communications International Inc. | Method and system for performing speech recognition based on best-word scoring of repeated speech attempts |
US6021403A (en) | 1996-07-19 | 2000-02-01 | Microsoft Corporation | Intelligent user assistance facility |
US5822730A (en) | 1996-08-22 | 1998-10-13 | Dragon Systems, Inc. | Lexical tree pre-filtering in speech recognition |
US6167377A (en) | 1997-03-28 | 2000-12-26 | Dragon Systems, Inc. | Speech recognition language models |
US6119186A (en) | 1997-05-30 | 2000-09-12 | Texas Instruments Incorporated | Computer system with environmental manager for detecting and responding to changing environmental conditions |
US6182038B1 (en) | 1997-12-01 | 2001-01-30 | Motorola, Inc. | Context dependent phoneme networks for encoding speech information |
US6317712B1 (en) | 1998-02-03 | 2001-11-13 | Texas Instruments Incorporated | Method of phonetic modeling using acoustic decision tree |
US6418431B1 (en) | 1998-03-30 | 2002-07-09 | Microsoft Corporation | Information retrieval and speech recognition based on language models |
US6574597B1 (en) | 1998-05-08 | 2003-06-03 | At&T Corp. | Fully expanded context-dependent networks for speech recognition |
US7881936B2 (en) | 1998-12-04 | 2011-02-01 | Tegic Communications, Inc. | Multimodal disambiguation of speech recognition |
US8938688B2 (en) | 1998-12-04 | 2015-01-20 | Nuance Communications, Inc. | Contextual prediction of user words and user actions |
US6922669B2 (en) | 1998-12-29 | 2005-07-26 | Koninklijke Philips Electronics N.V. | Knowledge-based strategies applied to N-best lists in automatic speech recognition systems |
US7058573B1 (en) | 1999-04-20 | 2006-06-06 | Nuance Communications Inc. | Speech recognition system to selectively utilize different speech recognition techniques over multiple speech recognition passes |
US6912499B1 (en) | 1999-08-31 | 2005-06-28 | Nortel Networks Limited | Method and apparatus for training a multilingual speech model set |
JP4292646B2 (en) | 1999-09-16 | 2009-07-08 | 株式会社デンソー | User interface device, navigation system, information processing device, and recording medium |
US6789231B1 (en) | 1999-10-05 | 2004-09-07 | Microsoft Corporation | Method and system for providing alternatives for text derived from stochastic input sources |
US6581033B1 (en) | 1999-10-19 | 2003-06-17 | Microsoft Corporation | System and method for correction of speech recognition mode errors |
US6778959B1 (en) | 1999-10-21 | 2004-08-17 | Sony Corporation | System and method for speech verification using out-of-vocabulary models |
JP2001125591A (en) * | 1999-10-27 | 2001-05-11 | Fujitsu Ten Ltd | Speech interactive system |
US6446041B1 (en) | 1999-10-27 | 2002-09-03 | Microsoft Corporation | Method and system for providing audio playback of a multi-source document |
US20020111990A1 (en) | 1999-11-01 | 2002-08-15 | Wood Christopher Noah | Internet based message management system |
US7403888B1 (en) | 1999-11-05 | 2008-07-22 | Microsoft Corporation | Language input user interface |
AU4869601A (en) | 2000-03-20 | 2001-10-03 | Robert J. Freeman | Natural-language processing system using a large corpus |
US6678415B1 (en) | 2000-05-12 | 2004-01-13 | Xerox Corporation | Document image decoding using an integrated stochastic language model |
US6539358B1 (en) | 2000-05-24 | 2003-03-25 | Delphi Technologies, Inc. | Voice-interactive docking station for a portable computing device |
US7149970B1 (en) | 2000-06-23 | 2006-12-12 | Microsoft Corporation | Method and system for filtering and selecting from a candidate list generated by a stochastic input method |
JP3563018B2 (en) * | 2000-07-21 | 2004-09-08 | シャープ株式会社 | Speech recognition device, speech recognition method, and program recording medium |
US7623648B1 (en) | 2004-12-01 | 2009-11-24 | Tellme Networks, Inc. | Method and system of generating reference variations for directory assistance data |
US7043422B2 (en) | 2000-10-13 | 2006-05-09 | Microsoft Corporation | Method and apparatus for distribution-based language model adaptation |
US7457750B2 (en) | 2000-10-13 | 2008-11-25 | At&T Corp. | Systems and methods for dynamic re-configurable speech recognition |
US7219058B1 (en) | 2000-10-13 | 2007-05-15 | At&T Corp. | System and method for processing speech recognition results |
US6876966B1 (en) | 2000-10-16 | 2005-04-05 | Microsoft Corporation | Pattern recognition training method and apparatus using inserted noise followed by noise reduction |
ATE297588T1 (en) | 2000-11-14 | 2005-06-15 | Ibm | ADJUSTING PHONETIC CONTEXT TO IMPROVE SPEECH RECOGNITION |
ATE391986T1 (en) | 2000-11-23 | 2008-04-15 | Ibm | VOICE NAVIGATION IN WEB APPLICATIONS |
US6915262B2 (en) | 2000-11-30 | 2005-07-05 | Telesector Resources Group, Inc. | Methods and apparatus for performing speech recognition and using speech recognition results |
US20020087309A1 (en) | 2000-12-29 | 2002-07-04 | Lee Victor Wai Leung | Computer-implemented speech expectation-based probability method and system |
DE10100725C1 (en) | 2001-01-10 | 2002-01-24 | Philips Corp Intellectual Pty | Automatic dialogue system for speech interrogation of databank entries uses speech recognition system assisted by speech model obtained before beginning of dialogue |
US7027987B1 (en) | 2001-02-07 | 2006-04-11 | Google Inc. | Voice interface for a search engine |
US6754626B2 (en) | 2001-03-01 | 2004-06-22 | International Business Machines Corporation | Creating a hierarchical tree of language models for a dialog system based on prompt and dialog context |
US7072838B1 (en) | 2001-03-20 | 2006-07-04 | Nuance Communications, Inc. | Method and apparatus for improving human-machine dialogs using language models learned automatically from personalized data |
US7778816B2 (en) | 2001-04-24 | 2010-08-17 | Microsoft Corporation | Method and system for applying input mode bias |
US6714778B2 (en) | 2001-05-15 | 2004-03-30 | Nokia Corporation | Context sensitive web services |
US20030008680A1 (en) | 2001-05-24 | 2003-01-09 | Huh Stephen S. | Using identification information obtained from a portable phone |
US7526431B2 (en) | 2001-09-05 | 2009-04-28 | Voice Signal Technologies, Inc. | Speech recognition using ambiguous or phone key spelling and/or filtering |
US7225130B2 (en) | 2001-09-05 | 2007-05-29 | Voice Signal Technologies, Inc. | Methods, systems, and programming for performing speech recognition |
US6901364B2 (en) | 2001-09-13 | 2005-05-31 | Matsushita Electric Industrial Co., Ltd. | Focused language models for improved speech input of structured documents |
US6959276B2 (en) | 2001-09-27 | 2005-10-25 | Microsoft Corporation | Including the category of environmental noise when processing speech signals |
US7533020B2 (en) | 2001-09-28 | 2009-05-12 | Nuance Communications, Inc. | Method and apparatus for performing relational speech recognition |
US6950796B2 (en) | 2001-11-05 | 2005-09-27 | Motorola, Inc. | Speech recognition by dynamical noise model adaptation |
US6914975B2 (en) | 2002-02-21 | 2005-07-05 | Sbc Properties, L.P. | Interactive dialog-based training method |
US7143035B2 (en) | 2002-03-27 | 2006-11-28 | International Business Machines Corporation | Methods and apparatus for generating dialog state conditioned language models |
US7174288B2 (en) | 2002-05-08 | 2007-02-06 | Microsoft Corporation | Multi-modal entry of ideogrammatic languages |
US7403890B2 (en) | 2002-05-13 | 2008-07-22 | Roushar Joseph C | Multi-dimensional method and apparatus for automated language interpretation |
US7224981B2 (en) | 2002-06-20 | 2007-05-29 | Intel Corporation | Speech recognition of mobile devices |
US7570943B2 (en) | 2002-08-29 | 2009-08-04 | Nokia Corporation | System and method for providing context sensitive recommendations to digital services |
JP4109063B2 (en) | 2002-09-18 | 2008-06-25 | パイオニア株式会社 | Speech recognition apparatus and speech recognition method |
US7184957B2 (en) | 2002-09-25 | 2007-02-27 | Toyota Infotechnology Center Co., Ltd. | Multiple pass speech recognition method and system |
JP4352790B2 (en) | 2002-10-31 | 2009-10-28 | セイコーエプソン株式会社 | Acoustic model creation method, speech recognition device, and vehicle having speech recognition device |
US7149688B2 (en) | 2002-11-04 | 2006-12-12 | Speechworks International, Inc. | Multi-lingual speech recognition with cross-language context modeling |
US6993615B2 (en) | 2002-11-15 | 2006-01-31 | Microsoft Corporation | Portable computing device-integrated appliance |
US7457745B2 (en) | 2002-12-03 | 2008-11-25 | Hrl Laboratories, Llc | Method and apparatus for fast on-line automatic speaker/environment adaptation for speech/speaker recognition in the presence of changing environments |
WO2004053836A1 (en) | 2002-12-10 | 2004-06-24 | Kirusa, Inc. | Techniques for disambiguating speech input using multimodal interfaces |
US7373300B1 (en) | 2002-12-18 | 2008-05-13 | At&T Corp. | System and method of providing a spoken dialog interface to a website |
US7698136B1 (en) | 2003-01-28 | 2010-04-13 | Voxify, Inc. | Methods and apparatus for flexible speech recognition |
US20040162724A1 (en) * | 2003-02-11 | 2004-08-19 | Jeffrey Hill | Management of conversations |
US7606714B2 (en) * | 2003-02-11 | 2009-10-20 | Microsoft Corporation | Natural language classification within an automated response system |
US7805299B2 (en) | 2004-03-01 | 2010-09-28 | Coifman Robert E | Method and apparatus for improving the transcription accuracy of speech recognition software |
CA2428821C (en) | 2003-05-15 | 2009-03-17 | Ibm Canada Limited - Ibm Canada Limitee | Accessing a platform independent input method editor from an underlying operating system |
US20040243415A1 (en) | 2003-06-02 | 2004-12-02 | International Business Machines Corporation | Architecture for a speech input method editor for handheld portable devices |
US7392188B2 (en) | 2003-07-31 | 2008-06-24 | Telefonaktiebolaget Lm Ericsson (Publ) | System and method enabling acoustic barge-in |
JP4548646B2 (en) | 2003-09-12 | 2010-09-22 | 株式会社エヌ・ティ・ティ・ドコモ | Noise model noise adaptation system, noise adaptation method, and speech recognition noise adaptation program |
US7634720B2 (en) | 2003-10-24 | 2009-12-15 | Microsoft Corporation | System and method for providing context to an input method |
FI20031566A (en) | 2003-10-27 | 2005-04-28 | Nokia Corp | Select a language for word recognition |
CA2486128C (en) | 2003-10-30 | 2011-08-23 | At&T Corp. | System and method for using meta-data dependent language modeling for automatic speech recognition |
CA2486125C (en) | 2003-10-30 | 2011-02-08 | At&T Corp. | A system and method of using meta-data in speech-processing |
US20050114474A1 (en) | 2003-11-20 | 2005-05-26 | International Business Machines Corporation | Automatic configuration of the network devices via connection to specific switch ports |
WO2005050621A2 (en) | 2003-11-21 | 2005-06-02 | Philips Intellectual Property & Standards Gmbh | Topic specific models for text formatting and speech recognition |
US7542907B2 (en) | 2003-12-19 | 2009-06-02 | International Business Machines Corporation | Biasing a speech recognizer based on prompt context |
US7634095B2 (en) | 2004-02-23 | 2009-12-15 | General Motors Company | Dynamic tuning of hands-free algorithm for noise and driving conditions |
US7400878B2 (en) | 2004-02-26 | 2008-07-15 | Research In Motion Limited | Computing device with environment aware features |
US20050246325A1 (en) | 2004-04-30 | 2005-11-03 | Microsoft Corporation | Method and system for recording and accessing usage of an item in a computer system |
JP3923513B2 (en) | 2004-06-08 | 2007-06-06 | 松下電器産業株式会社 | Speech recognition apparatus and speech recognition method |
US7299181B2 (en) | 2004-06-30 | 2007-11-20 | Microsoft Corporation | Homonym processing in the context of voice-activated command systems |
US7562069B1 (en) | 2004-07-01 | 2009-07-14 | Aol Llc | Query disambiguation |
US20060009974A1 (en) | 2004-07-09 | 2006-01-12 | Matsushita Electric Industrial Co., Ltd. | Hands-free voice dialing for portable and remote devices |
US7580363B2 (en) | 2004-08-16 | 2009-08-25 | Nokia Corporation | Apparatus and method for facilitating contact selection in communication devices |
JP4679254B2 (en) * | 2004-10-28 | 2011-04-27 | 富士通株式会社 | Dialog system, dialog method, and computer program |
US7698124B2 (en) | 2004-11-04 | 2010-04-13 | Microsoft Corporaiton | Machine translation system incorporating syntactic dependency treelets into a statistical framework |
JP3955880B2 (en) | 2004-11-30 | 2007-08-08 | 松下電器産業株式会社 | Voice recognition device |
US8498865B1 (en) | 2004-11-30 | 2013-07-30 | Vocera Communications, Inc. | Speech recognition system and method using group call statistics |
US8009678B2 (en) | 2005-03-17 | 2011-08-30 | Microsoft Corporation | System and method for generating a dynamic prioritized contact list |
US7739286B2 (en) | 2005-03-17 | 2010-06-15 | University Of Southern California | Topic specific language models built from large numbers of documents |
JP4804052B2 (en) * | 2005-07-08 | 2011-10-26 | アルパイン株式会社 | Voice recognition device, navigation device provided with voice recognition device, and voice recognition method of voice recognition device |
US20070060114A1 (en) | 2005-09-14 | 2007-03-15 | Jorey Ramer | Predictive text completion for a mobile communication facility |
US7672833B2 (en) | 2005-09-22 | 2010-03-02 | Fair Isaac Corporation | Method and apparatus for automatic entity disambiguation |
EP1791114B1 (en) | 2005-11-25 | 2009-08-12 | Swisscom AG | A method for personalization of a service |
JP4961755B2 (en) | 2006-01-23 | 2012-06-27 | 富士ゼロックス株式会社 | Word alignment device, word alignment method, word alignment program |
JP4197344B2 (en) * | 2006-02-20 | 2008-12-17 | インターナショナル・ビジネス・マシーンズ・コーポレーション | Spoken dialogue system |
JP5218052B2 (en) | 2006-06-26 | 2013-06-26 | 日本電気株式会社 | Language model generation system, language model generation method, and language model generation program |
US8001130B2 (en) | 2006-07-25 | 2011-08-16 | Microsoft Corporation | Web object retrieval based on a language model |
JP2008064885A (en) * | 2006-09-05 | 2008-03-21 | Honda Motor Co Ltd | Voice recognition device, voice recognition method and voice recognition program |
US7907705B1 (en) | 2006-10-10 | 2011-03-15 | Intuit Inc. | Speech to text for assisted form completion |
US8041568B2 (en) | 2006-10-13 | 2011-10-18 | Google Inc. | Business listing search |
US7890326B2 (en) | 2006-10-13 | 2011-02-15 | Google Inc. | Business listing search |
US8073681B2 (en) | 2006-10-16 | 2011-12-06 | Voicebox Technologies, Inc. | System and method for a cooperative conversational voice user interface |
US9128926B2 (en) | 2006-10-26 | 2015-09-08 | Facebook, Inc. | Simultaneous translation of open domain lectures and speeches |
WO2008067562A2 (en) | 2006-11-30 | 2008-06-05 | Rao Ashwin P | Multimodal speech recognition system |
US20080131851A1 (en) | 2006-12-04 | 2008-06-05 | Dimitri Kanevsky | Context-sensitive language learning |
US7941189B2 (en) | 2007-02-07 | 2011-05-10 | Denso Corporation | Communicating road noise control system, in-vehicle road noise controller, and server |
US20100325109A1 (en) | 2007-02-09 | 2010-12-23 | Agency For Science, Technology And Rearch | Keyword classification and determination in language modelling |
US20080221901A1 (en) | 2007-03-07 | 2008-09-11 | Joseph Cerra | Mobile general search environment speech processing facility |
US20090030687A1 (en) | 2007-03-07 | 2009-01-29 | Cerra Joseph P | Adapting an unstructured language model speech recognition system based on usage |
US20110060587A1 (en) | 2007-03-07 | 2011-03-10 | Phillips Michael S | Command and control utilizing ancillary information in a mobile voice-to-speech application |
US8838457B2 (en) | 2007-03-07 | 2014-09-16 | Vlingo Corporation | Using results of unstructured language model based speech recognition to control a system-level function of a mobile communications facility |
US8060373B2 (en) | 2007-03-21 | 2011-11-15 | At&T Intellectual Property I, L.P. | System and method of identifying contact information |
US8396713B2 (en) | 2007-04-30 | 2013-03-12 | Nuance Communications, Inc. | Method and system for using a statistical language model and an action classifier in parallel with grammar for better handling of out-of-grammar utterances |
US8762143B2 (en) | 2007-05-29 | 2014-06-24 | At&T Intellectual Property Ii, L.P. | Method and apparatus for identifying acoustic background environments based on time and speed to enhance automatic speech recognition |
US7895177B2 (en) | 2007-05-29 | 2011-02-22 | Yahoo! Inc. | Enabling searching of user ratings and reviews using user profile location, and social networks |
US7831427B2 (en) | 2007-06-20 | 2010-11-09 | Microsoft Corporation | Concept monitoring in spoken-word audio |
US7983902B2 (en) | 2007-08-23 | 2011-07-19 | Google Inc. | Domain dictionary creation by detection of new topic words using divergence value comparison |
US8321219B2 (en) | 2007-10-05 | 2012-11-27 | Sensory, Inc. | Systems and methods of performing speech recognition using gestures |
US7953692B2 (en) | 2007-12-07 | 2011-05-31 | Microsoft Corporation | Predicting candidates using information sources |
WO2009076026A2 (en) * | 2007-12-11 | 2009-06-18 | Dow Global Technologies Inc. | Extruded polymer foams containing brominated 2-oxo-1,3,2-dioxaphosphorinane compounds as flame retardant additives |
US8423362B2 (en) | 2007-12-21 | 2013-04-16 | General Motors Llc | In-vehicle circumstantial speech recognition |
US8473276B2 (en) | 2008-02-19 | 2013-06-25 | Google Inc. | Universal language input |
US8121837B2 (en) | 2008-04-24 | 2012-02-21 | Nuance Communications, Inc. | Adjusting a speech engine for a mobile computing device based on background noise |
US8090738B2 (en) | 2008-05-14 | 2012-01-03 | Microsoft Corporation | Multi-modal search wildcards |
US8364481B2 (en) | 2008-07-02 | 2013-01-29 | Google Inc. | Speech recognition with parallel recognition tasks |
US8027973B2 (en) | 2008-08-04 | 2011-09-27 | Microsoft Corporation | Searching questions based on topic and focus |
CA2680304C (en) | 2008-09-25 | 2017-08-22 | Multimodal Technologies, Inc. | Decoding-time prediction of non-verbalized tokens |
US8407236B2 (en) | 2008-10-03 | 2013-03-26 | Microsoft Corp. | Mining new words from a query log for input method editors |
GB2477653B (en) | 2008-10-10 | 2012-11-14 | Nuance Communications Inc | Generating and processing forms for receiving speech data |
US9798720B2 (en) | 2008-10-24 | 2017-10-24 | Ebay Inc. | Hybrid machine translation |
US9043209B2 (en) | 2008-11-28 | 2015-05-26 | Nec Corporation | Language model creation device |
US8352321B2 (en) | 2008-12-12 | 2013-01-08 | Microsoft Corporation | In-text embedded advertising |
US9741340B2 (en) * | 2014-11-07 | 2017-08-22 | Nuance Communications, Inc. | System and method for enhancing speech recognition accuracy using weighted grammars based on user profile including demographic, account, time and date information |
US8509398B2 (en) | 2009-04-02 | 2013-08-13 | Microsoft Corporation | Voice scratchpad |
US9858925B2 (en) * | 2009-06-05 | 2018-01-02 | Apple Inc. | Using context information to facilitate processing of commands in a virtual assistant |
US20100318531A1 (en) | 2009-06-10 | 2010-12-16 | Microsoft Corporation | Smoothing clickthrough data for web search ranking |
US9892730B2 (en) | 2009-07-01 | 2018-02-13 | Comcast Interactive Media, Llc | Generating topic-specific language models |
US8793119B2 (en) * | 2009-07-13 | 2014-07-29 | At&T Intellectual Property I, L.P. | System and method for generating manually designed and automatically optimized spoken dialog systems |
US8364612B2 (en) | 2009-09-15 | 2013-01-29 | Microsoft Corporation | Machine learning using relational databases |
US8255217B2 (en) | 2009-10-16 | 2012-08-28 | At&T Intellectual Property I, Lp | Systems and methods for creating and using geo-centric language models |
US8589163B2 (en) | 2009-12-04 | 2013-11-19 | At&T Intellectual Property I, L.P. | Adapting language models with a bit mask for a subset of related words |
EP2339576B1 (en) | 2009-12-23 | 2019-08-07 | Google LLC | Multi-modal input on an electronic device |
US20110162035A1 (en) | 2009-12-31 | 2011-06-30 | Apple Inc. | Location-based dock for a computing device |
US10276170B2 (en) * | 2010-01-18 | 2019-04-30 | Apple Inc. | Intelligent automated assistant |
US8996368B2 (en) | 2010-02-22 | 2015-03-31 | Nuance Communications, Inc. | Online maximum-likelihood mean and variance normalization for speech recognition |
US8265928B2 (en) | 2010-04-14 | 2012-09-11 | Google Inc. | Geotagged environmental audio for enhanced speech recognition accuracy |
US8694313B2 (en) | 2010-05-19 | 2014-04-08 | Google Inc. | Disambiguation of contact information using historical data |
US8468012B2 (en) | 2010-05-26 | 2013-06-18 | Google Inc. | Acoustic model adaptation using geographic information |
US8700392B1 (en) | 2010-09-10 | 2014-04-15 | Amazon Technologies, Inc. | Speech-inclusive device interfaces |
US8606581B1 (en) | 2010-12-14 | 2013-12-10 | Nuance Communications, Inc. | Multi-pass speech recognition |
US8352245B1 (en) | 2010-12-30 | 2013-01-08 | Google Inc. | Adjusting language models |
US9202465B2 (en) * | 2011-03-25 | 2015-12-01 | General Motors Llc | Speech recognition dependent on text message content |
US9176944B1 (en) * | 2011-08-23 | 2015-11-03 | Google Inc. | Selectively processing user input |
US9009025B1 (en) * | 2011-12-27 | 2015-04-14 | Amazon Technologies, Inc. | Context-based utterance recognition |
US8965763B1 (en) * | 2012-02-02 | 2015-02-24 | Google Inc. | Discriminative language modeling for automatic speech recognition with a weak acoustic model and distributed training |
US8775177B1 (en) | 2012-03-08 | 2014-07-08 | Google Inc. | Speech recognition process |
US9230556B2 (en) * | 2012-06-05 | 2016-01-05 | Apple Inc. | Voice instructions during navigation |
US10354650B2 (en) | 2012-06-26 | 2019-07-16 | Google Llc | Recognizing speech with mixed speech recognition models to generate transcriptions |
US9047868B1 (en) | 2012-07-31 | 2015-06-02 | Amazon Technologies, Inc. | Language model data collection |
US9117450B2 (en) | 2012-12-12 | 2015-08-25 | Nuance Communications, Inc. | Combining re-speaking, partial agent transcription and ASR for improved accuracy / human guided ASR |
KR101410163B1 (en) * | 2013-01-02 | 2014-06-20 | 포항공과대학교 산학협력단 | Method for understanding voice language based on statistical analysis |
JP2015018146A (en) * | 2013-07-12 | 2015-01-29 | 株式会社Ｎｔｔドコモ | Function management system and function management method |
US9606984B2 (en) * | 2013-08-19 | 2017-03-28 | Nuance Communications, Inc. | Unsupervised clustering of dialogs extracted from released application logs |
US9886950B2 (en) * | 2013-09-08 | 2018-02-06 | Intel Corporation | Automatic generation of domain models for virtual personal assistants |
CN105874531B (en) * | 2014-01-06 | 2020-06-26 | 株式会社Ntt都科摩 | Terminal device, server device, and computer-readable recording medium |
US20150309984A1 (en) * | 2014-04-25 | 2015-10-29 | Nuance Communications, Inc. | Learning language models from scratch based on crowd-sourced user text input |
US9966065B2 (en) * | 2014-05-30 | 2018-05-08 | Apple Inc. | Multi-command single utterance input method |
US9632748B2 (en) * | 2014-06-24 | 2017-04-25 | Google Inc. | Device designation for audio input monitoring |
US10317992B2 (en) * | 2014-09-25 | 2019-06-11 | Microsoft Technology Licensing, Llc | Eye gaze for spoken language understanding in multi-modal conversational interactions |
US10079785B2 (en) * | 2015-02-12 | 2018-09-18 | Google Llc | Determining reply content for a reply to an electronic communication |
US10347240B2 (en) * | 2015-02-26 | 2019-07-09 | Nantmobile, Llc | Kernel-based verbal phrase splitting devices and methods |
KR102325724B1 (en) * | 2015-02-28 | 2021-11-15 | 삼성전자주식회사 | Synchronization of Text Data among a plurality of Devices |
US9666183B2 (en) * | 2015-03-27 | 2017-05-30 | Qualcomm Incorporated | Deep neural net based filter prediction for audio event classification and extraction |
US9460713B1 (en) * | 2015-03-30 | 2016-10-04 | Google Inc. | Language model biasing modulation |
US9691380B2 (en) * | 2015-06-15 | 2017-06-27 | Google Inc. | Negative n-gram biasing |
US9473637B1 (en) * | 2015-07-28 | 2016-10-18 | Xerox Corporation | Learning generation templates from dialog transcripts |
US9978374B2 (en) * | 2015-09-04 | 2018-05-22 | Google Llc | Neural networks for speaker verification |
US10055403B2 (en) * | 2016-02-05 | 2018-08-21 | Adobe Systems Incorporated | Rule-based dialog state tracking |
US9978367B2 (en) * | 2016-03-16 | 2018-05-22 | Google Llc | Determining dialog states for language models |
US10503834B2 (en) * | 2017-11-17 | 2019-12-10 | Digital Genius Limited | Template generation for a conversational agent |
-
2016
- 2016-03-16 US US15/071,651 patent/US9978367B2/en active Active
- 2016-11-30 KR KR1020187019324A patent/KR102151681B1/en active IP Right Grant
- 2016-11-30 WO PCT/US2016/064065 patent/WO2017160355A1/en active Application Filing
- 2016-11-30 JP JP2018536424A patent/JP6569009B2/en active Active
- 2016-11-30 EP EP19194784.5A patent/EP3594941B1/en active Active
- 2016-11-30 CN CN201680078893.9A patent/CN108463849B/en active Active
- 2016-11-30 EP EP23179644.2A patent/EP4235647A3/en active Pending
- 2016-11-30 CN CN202210438719.8A patent/CN114974234A/en active Pending
- 2016-11-30 EP EP16813178.7A patent/EP3381034B1/en active Active
-
2018
- 2018-05-18 US US15/983,768 patent/US10553214B2/en active Active
-
2020
- 2020-01-02 US US16/732,645 patent/US11264028B2/en active Active
-
2022
- 2022-02-10 US US17/650,567 patent/US20220165270A1/en active Pending
Patent Citations (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20030091163A1 (en) * | 1999-12-20 | 2003-05-15 | Attwater David J | Learning of dialogue states and language model of spoken information system |
US20030149561A1 (en) * | 2002-02-01 | 2003-08-07 | Intel Corporation | Spoken dialog system using a best-fit language model and best-fit grammar |
CN101477798A (en) * | 2009-02-17 | 2009-07-08 | 北京邮电大学 | Method for analyzing and extracting audio data of set scene |
CN104508739A (en) * | 2012-06-21 | 2015-04-08 | 谷歌公司 | Dynamic language model |
CN105378708A (en) * | 2013-06-21 | 2016-03-02 | 微软技术许可有限责任公司 | Environmentally aware dialog policies and response generation |
US20150382047A1 (en) * | 2014-06-30 | 2015-12-31 | Apple Inc. | Intelligent automated assistant for tv user interactions |
Non-Patent Citations (2)
Title |
---|
ALEXANDROS POTAMIANOS等: "Adaptive Categorical Understanding for Spoken Dialogue Systems", 《IEEE TRANSACTIONS ON SPEECH AND AUDIO PROCESSING》 * |
ROGER ARGILES SOLSONA等: "Adaptive language models for spoken dialogue systems", 《2002 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH,AND SIGNAL PROCESSING》 * |
Cited By (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN109492083A (en) * | 2018-11-05 | 2019-03-19 | 北京奥法科技有限公司 | A method of more wheel human-computer intellectualizations are realized based on list content |
CN110442438A (en) * | 2019-02-26 | 2019-11-12 | 北京蓦然认知科技有限公司 | Task cooperative method, equipment and system between a kind of more equipment |
CN110442438B (en) * | 2019-02-26 | 2022-06-07 | 杭州蓦然认知科技有限公司 | Task cooperation method, device and system among multiple devices |
CN112559721A (en) * | 2020-12-25 | 2021-03-26 | 北京百度网讯科技有限公司 | Method, apparatus, device, medium and program product for adjusting man-machine dialog system |
CN112559721B (en) * | 2020-12-25 | 2023-10-20 | 北京百度网讯科技有限公司 | Method, device, equipment, medium and program product for adjusting man-machine dialogue system |
Also Published As
Publication number | Publication date |
---|---|
WO2017160355A1 (en) | 2017-09-21 |
EP3594941B1 (en) | 2023-07-26 |
KR102151681B1 (en) | 2020-09-04 |
EP3381034A1 (en) | 2018-10-03 |
JP2019507895A (en) | 2019-03-22 |
KR20180090869A (en) | 2018-08-13 |
CN114974234A (en) | 2022-08-30 |
CN108463849B (en) | 2022-05-03 |
US20220165270A1 (en) | 2022-05-26 |
EP3594941A1 (en) | 2020-01-15 |
EP3381034B1 (en) | 2019-10-02 |
US20200135203A1 (en) | 2020-04-30 |
US11264028B2 (en) | 2022-03-01 |
US9978367B2 (en) | 2018-05-22 |
US20170270929A1 (en) | 2017-09-21 |
US20180336895A1 (en) | 2018-11-22 |
US10553214B2 (en) | 2020-02-04 |
EP4235647A3 (en) | 2023-10-18 |
EP4235647A2 (en) | 2023-08-30 |
JP6569009B2 (en) | 2019-08-28 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN108463849A (en) | Determine the dialogue state of language model | |
CN110050302B (en) | Speech synthesis | |
US11450313B2 (en) | Determining phonetic relationships | |
US7966171B2 (en) | System and method for increasing accuracy of searches based on communities of interest | |
CN102906735B (en) | The note taking that voice flow strengthens | |
US11562744B1 (en) | Stylizing text-to-speech (TTS) voice response for assistant systems | |
CN102142253B (en) | Voice emotion identification equipment and method | |
US20160343366A1 (en) | Speech synthesis model selection | |
CN109801634A (en) | A kind of fusion method and device of vocal print feature | |
CN106782607A (en) | Determine hot word grade of fit | |
US10607601B2 (en) | Speech recognition by selecting and refining hot words | |
EP3593346B1 (en) | Graphical data selection and presentation of digital content | |
CN107707745A (en) | Method and apparatus for extracting information | |
Dua et al. | Discriminative training using noise robust integrated features and refined HMM modeling | |
CN108346426A (en) | Speech recognition equipment and audio recognition method | |
Trabelsi et al. | Evaluation of the efficiency of state-of-the-art Speech Recognition engines | |
Gilbert et al. | Intelligent virtual agents for contact center automation | |
Yadava et al. | Improvements in spoken query system to access the agricultural commodity prices and weather information in Kannada language/dialects | |
Fennir et al. | Acoustic scene classification for speaker diarization | |
CN114267341A (en) | Voice recognition processing method and device based on ATM service logic | |
CN115064177A (en) | Voice conversion method, apparatus, device and medium based on voiceprint encoder | |
CN118051582A (en) | Method, device, equipment and medium for identifying potential customers based on telephone voice analysis | |
CN115240658A (en) | Training method of audio text recognition model and audio text recognition method |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |