EP3323109B1 - Camera pose estimation for mobile devices - Google Patents
Camera pose estimation for mobile devices Download PDFInfo
- Publication number
- EP3323109B1 EP3323109B1 EP16756820.3A EP16756820A EP3323109B1 EP 3323109 B1 EP3323109 B1 EP 3323109B1 EP 16756820 A EP16756820 A EP 16756820A EP 3323109 B1 EP3323109 B1 EP 3323109B1
- Authority
- EP
- European Patent Office
- Prior art keywords
- feature points
- image
- pose
- computer
- images
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 238000000034 method Methods 0.000 claims description 66
- 230000015654 memory Effects 0.000 claims description 36
- 230000003287 optical effect Effects 0.000 claims description 31
- 238000004422 calculation algorithm Methods 0.000 claims description 8
- 230000008878 coupling Effects 0.000 claims description 6
- 238000010168 coupling process Methods 0.000 claims description 6
- 238000005859 coupling reaction Methods 0.000 claims description 6
- 230000004044 response Effects 0.000 claims description 2
- 238000012545 processing Methods 0.000 description 23
- 230000006870 function Effects 0.000 description 19
- 238000004891 communication Methods 0.000 description 18
- 238000009877 rendering Methods 0.000 description 18
- 230000008569 process Effects 0.000 description 14
- 238000004590 computer program Methods 0.000 description 13
- 230000033001 locomotion Effects 0.000 description 13
- 230000009471 action Effects 0.000 description 11
- 238000010586 diagram Methods 0.000 description 6
- 238000005457 optimization Methods 0.000 description 6
- 238000013519 translation Methods 0.000 description 6
- 238000010408 sweeping Methods 0.000 description 5
- 230000008901 benefit Effects 0.000 description 4
- 210000003128 head Anatomy 0.000 description 4
- 239000011159 matrix material Substances 0.000 description 4
- 230000009466 transformation Effects 0.000 description 4
- 238000005259 measurement Methods 0.000 description 3
- 239000000203 mixture Substances 0.000 description 3
- 239000013598 vector Substances 0.000 description 3
- 206010047571 Visual impairment Diseases 0.000 description 2
- 230000001413 cellular effect Effects 0.000 description 2
- 230000008859 change Effects 0.000 description 2
- 238000012937 correction Methods 0.000 description 2
- 230000003993 interaction Effects 0.000 description 2
- 239000004973 liquid crystal related substance Substances 0.000 description 2
- 238000000844 transformation Methods 0.000 description 2
- 230000000007 visual effect Effects 0.000 description 2
- 238000004458 analytical method Methods 0.000 description 1
- 238000004364 calculation method Methods 0.000 description 1
- 238000006073 displacement reaction Methods 0.000 description 1
- 230000000694 effects Effects 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 239000011521 glass Substances 0.000 description 1
- 230000004807 localization Effects 0.000 description 1
- 238000013507 mapping Methods 0.000 description 1
- 238000002156 mixing Methods 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000006855 networking Effects 0.000 description 1
- 238000003909 pattern recognition Methods 0.000 description 1
- 230000009467 reduction Effects 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 230000000087 stabilizing effect Effects 0.000 description 1
- 239000013589 supplement Substances 0.000 description 1
- 230000001131 transforming effect Effects 0.000 description 1
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/20—Image signal generators
- H04N13/204—Image signal generators using stereoscopic image cameras
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/70—Determining position or orientation of objects or cameras
- G06T7/73—Determining position or orientation of objects or cameras using feature-based methods
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/61—Control of cameras or camera modules based on recognised objects
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/10—Image acquisition modality
- G06T2207/10016—Video; Image sequence
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/30—Subject of image; Context of image processing
- G06T2207/30244—Camera pose
Definitions
- This description generally relates to pose estimation.
- the description relates to estimating camera pose metrics when generating 3D stereoscopic images for display in virtual reality (VR) environment.
- VR virtual reality
- the combination of orientation and position of an object observed in an image is typically referred to as the pose of the object.
- the pose of the object may be referenced in relation to the position or orientation of the object relative to a coordinate system.
- the coordinate system can be used to describe the pose with reference to a rotation and translation transformation.
- a system of one or more computers can be configured to perform particular operations or actions by virtue of having software, firmware, hardware, or a combination of them installed on the system that in operation causes or cause the system to perform the actions.
- One or more computer programs can be configured to perform particular operations or actions by virtue of including instructions that, when executed by data processing apparatus, cause the apparatus to perform the actions.
- a computer-implemented method includes obtaining a sequence of images including a plurality of image frames of a scene, detecting a first set of feature points in a first image frame, tracking the first set of feature points in a plurality of subsequent image frames and while continuing to track the first set of feature points, detecting a second set of feature points in a second image frame, wherein new features are represented by the second set of feature points, tracking the second set of feature points in the plurality of subsequent image frames, and selecting a first initial camera pose associated with the first image frame and a second initial camera pose associated with the second image frame.
- the method also includes determining a plurality of projection locations corresponding to each feature point in the first set of feature points and the second set of feature points in which the projection locations are based at least in part on the first initial camera pose and the second initial camera pose.
- the method also includes comparing the projection locations corresponding to each feature point in the first set of feature points and each feature point in the second set of feature points to the corresponding tracked first set of feature points and the tracked second set of feature points, and in response to determining that the projection locations meet a predefined threshold corresponding to distortion associated with the tracked first and second set of feature points, the method includes generating an updated camera pose for the first image frame and the second image frame.
- Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.
- Implementations may include one or more of the following features.
- the computer-implemented method further including generating a 3D image of the scene using the updated camera pose for the first image frames and the second image frames based at least in part on determining that the projection locations meet the predefined threshold.
- the computer-implemented method in which selecting a first initial camera pose and a second initial camera pose includes obtaining gyroscope data associated with each image frame in the plurality of image frames and estimating, using the gyroscope data associated with the first image frame, a pose corresponding to the first set of feature points.
- the computer-implemented method further including estimating, using the gyroscope data associated with the second image frame, a pose corresponding to the second set of feature points.
- the computer-implemented method in which the gyroscope data includes at least a raw estimation of a rotation associated with a mobile device that captured the plurality of image frames, and the gyroscope data is used to estimate pose and image orientation associated with each image frame.
- the computer-implemented method may also include determining a plurality of projection locations corresponding to each feature point in the first and second sets of feature points, the determining including for each feature point, coupling position data and orientation data, selecting at least one constraint and applying the constraint to the coupled position data and orientation data, and estimating a camera pose, and providing, to a stitching module, the updated camera pose associated with each feature point in which the stitching module is configured to stitch the plurality of image frames to generate a 3D image of the scene based on the estimated camera pose.
- the method may include at least one constraint of an equidistant orbit constraint or a concentric optical axis constraint.
- the computer-implemented method in which the constraint is selected to reduce search space for a pose estimation algorithm from six degrees of freedom to four degrees of freedom.
- the computer-implemented method where the sequence of images are captured with a mobile device.
- the computer-implemented method where the plurality of projection locations correspond to a center of projection of a mobile device that captured the plurality of image frames. Implementations of the described techniques may include hardware, a method or process, or computer software on a computer-accessible medium.
- a system in another general aspect, includes one or more computers configured to perform particular operations or actions by virtue of having software, firmware, hardware, or a combination of them installed on the system that in operation causes or cause the system to perform the actions.
- One or more computer programs can be configured to perform particular operations or actions by virtue of including instructions that, when executed by data processing apparatus, cause the apparatus to perform the actions.
- the system may include a computer-implemented system executing on a mobile computing device.
- the system may include a pose estimation module with at least one processor configured to estimate a camera pose for a sequence of images.
- the pose estimation module may simulate a uniform image capture path based on a non-uniform image capture path associated with capturing the sequence of images with the mobile computing device.
- the system may also include a bundle adjustment module with at least one processor configured to access at least a portion of the sequence of images and adjust the portion for rendering in 3D space.
- a bundle adjustment module with at least one processor configured to access at least a portion of the sequence of images and adjust the portion for rendering in 3D space.
- Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.
- Implementations of the system may also include a uniform image capture path with a path in a single plane and maintaining less than one inflection point in the image capture path. Implementations of the system may also include a non-uniform image capture path includes a path in more than one plane or a capture path with two or more inflection points in the image capture path.
- Implementations may include one or more of the following features.
- the system further including a stitching module configured to stitch a plurality of portions of the sequence of images to generate a 3D scene.
- Implementations of the described techniques may include hardware, a method or process, or computer software on a computer-accessible medium.
- inventions of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.
- the mobile device may be configured to capture and produce omnistereo panoramas.
- an application running on the mobile device can be used by a user to capture images of a scene.
- the application can receive the images as they are captured and begin to estimate camera position and camera orientation information (i.e., camera pose) for each image.
- the received image content includes video content and the application can estimate camera pose information for frames or portions of frames during or after image capture.
- the application can employ algorithms described in this disclosure to provide two-dimensional (2D) and/or three-dimensional (3D) panoramic or omnistereo panoramic images using captured image content and estimated camera pose information associated with the mobile device.
- Estimating camera pose can be performed using algorithms described in this disclosure in combination with a number of particular constraints. Such constraints can be used as stand-alone constraints, in combination, or altogether to estimate a camera pose associated with a mobile device.
- the algorithms described herein may function to estimate the camera pose by coupling together both camera orientation and camera position information in the same coordinate system. Coupling the camera orientation information with the camera position information can ease the calculations used to estimate camera pose. By coupling the information, the capture method for mobile device-based panoramic capture can be likened to capturing image content using a spherical-shaped trajectory in a coordinate system.
- Such a capture may be performed by the user holding the mobile device in her hand and swinging (e.g., move) an arm (partially or fully extended) in a circular path that can be formed by passing through points above and below the user's shoulder and directly in front and behind the user at the shoulder level.
- the circular path may shift toward the user (e.g., above her head) or away from the user (e.g., to the right or left of the user's head).
- the motion can be used during capture to collect images of a scene.
- the trajectory of the mobile device during the capture swing may be approximated by a spherical-shaped trajectory with an optical axis of the camera on the mobile device approximately passing through at a point between the user's eyes during the capture.
- This capture path is an example path and other shifted or alternate shaped paths are possible.
- the user can swing (e.g., move) the mobile device in a circular path (or substantially circular path or nearly circular path) to capture panoramic images using a partial arm extension or a full arm extension during a swing.
- the swing can be likened to an arm circle that swings on the left side of the body of a user or on the right side of the body of a user.
- the camera position and camera orientation can be coupled according to constraints based on arm extension and based on an orientation associated with the mobile device. For example, a maximum arm constraint can be used because the user's arm is a finite length and cannot be further extended.
- a mobile device orientation can be determined with (a) assumptions of a user viewing a screen on the mobile device (e.g., during capture), (b) actual measurements from gyroscopes or (c) other sensors on the mobile device. Particular techniques and constraints are described in detail below.
- FIG. 1 is a block diagram of an example system 100 for capturing images and estimating camera pose information for rendering the images in a 3D virtual reality (VR) environment.
- a mobile device 102 can be used to capture images and/or video and provide those images or video over a network 104, or alternatively, can provide the images directly to an image processing system 107 for analysis and processing.
- the image processing system 107 is provided as part of mobile device 102. In other implementations, portions of image processing system 107 are provided on mobile device 102, while other portions of image processing system 107 are provided on another computing system.
- the mobile device 102 may be a mobile phone, an electronic tablet, a laptop, a camera, or other such electronic device that may be used to capture and/or obtain image data.
- the mobile device 102 can be configured to capture still images.
- the mobile device 102 can be configured to capture video and store such content as individual frames or video (e.g., .avi files) and such stored images can be uploaded to the Internet, another server or device, or stored locally on the mobile device 102.
- incoming images can be stored as encoded images or encoded video.
- the images described throughout this disclosure may include a plurality of consecutive or non-consecutive image frames of a scene.
- the images described throughout this disclosure may include a combination of both consecutive sets of images and non-consecutive sets of images. The images may be captured and rendered in two or three dimensions.
- a user can access a camera (also can be referred to as a capture device) on device 102, hold the device 102 in a selected orientation and pan or sweep the device 102 around to capture images along a capture path.
- sweeping device 102 around to capture images along a capture path can include having the user move the device 102 around a circular capture path (or a capture path of another shape or profile) surrounding her head or body (or part of a body). In such a sweep, the device 102 may be directed outward away from the user and pointed toward scenery while the user follows the circular capture path (or a capture path of another shape or profile).
- the capture path can be disposed within, or aligned along, a single plane, can be curved, can have linear portions, can have one or more discontinuities, and/or so forth. In some implementations, the capture path can have portions that are disposed within, or aligned along, more than one plane. While sweeping a path (e.g., a capture path) and capturing images, the mobile device 102 can receive indications to modify particular capture configurations. Capture configurations can include, but are not limited to, mobile device pose/capture positions/angles/pitch/roll, field of view, camera settings for lighting/exposure, horizontal or vertical tilt or twist, speed of camera capture sweep, etc.
- the image processing system 107 can perform a number of processes on the images to generate panoramic images that can be provided to a head mounted display (HMD) device 110 for rendering over network 104, for example.
- the image processing system 107 can also provide the processed images to mobile device 102 and/or to computing device 112 for pose estimation, rendering, storage, or further processing.
- the image processing system 107 includes a pose estimation module 116, a bundle adjustment module 118, a stitching module 120, and a rendering module 122.
- the pose estimation module 116 can be configured to estimate a pose that is associated with a capture device on a mobile device that captures panoramic images.
- the pose estimation module 116 can be configured to determine, for a particular panoramic capture session, a probable position, orientation, and focal length associated with the capture device on the mobile device. The position, orientation, and focal length can be used as inputs into a pose estimation model carried out by the pose estimation module 116.
- the bundle adjustment module 118 is part of the pose estimation module 116.
- pose information for an object can be expressed by combining three linear displacement coordinates (x, y, z) of any reference point on the object, as well as the three inclination angles (i.e., Euler angles ( ⁇ , ⁇ , ⁇ )) that may describe the pitch, yaw and roll of the object.
- the absolute camera pose for a 3D environment can be measured using an invariant object/feature located in the 3D environment.
- the absolute pose is expressed with absolute pose data ( ⁇ , ⁇ , ⁇ , x, y ,z) that represents Euler rotated object coordinates expressed in world coordinates (X o , Y o , Z o ) with respect to a reference location, such as, for example, the world origin.
- Other conventions for expressing pose data in 3D space and representing all six degrees of freedom i.e., three translational degrees of freedom and three rotational degrees of freedom
- similar conventions for expressing estimated camera pose information will be used throughout this disclosure.
- the pose estimation module 116 can use one or more constraints 124 to estimate the pose for particular images.
- constraints 124 can be motivated by the available resources used (e.g., memory resources, computational resources, and capture configuration limitations) when capturing panoramic images with a mobile device. That is, memory and computational resources can be limited for mobile device image capture activities and as such, the pose estimation module 116 can employ one or more constraints and techniques to lessen the computational burden on the mobile device 102, while still providing a stereoscopic 3D image.
- use of constraints 124 can be used to reduce the search space for any of the algorithms used to estimate pose so that the estimation can be robust, yet computationally sparing.
- the search space for a pose estimation algorithm described herein can be reduced from six degrees of freedom into four degrees of freedom or three degrees of freedom.
- the number of degrees of freedom [DoF] may be 6 ⁇ N.
- the concentric optical axis constraint 128 is used alone, the DoF is reduced from 6 ⁇ N to 4 ⁇ N DoF.
- the equidistant orbiting constraint 126 is used, the DoF can be reduced from 6 ⁇ N DoF to 3 ⁇ N +3 DoF. If both the concentric optical axis constraint 128 and the equidistant orbiting constraint 126 are used, the DoF can be reduced to 3 ⁇ N+1 DoF.
- the system 100 can employ rules, assumptions, and/or constraints 124 based on two model constraints.
- the pose estimation module 116 can use a model based on an equidistant orbit constraint 126 to estimate a camera pose for mobile device 102.
- the equidistant orbit constraint 126 represents a constrained circular-shaped orbit as the user swings an arm around a capture path. This constraint 126 ensures that the orbit the capture path travels is equidistant from a center point because the arm length of the user will not change beyond a maximum limit during capture.
- the circular path can be constrained to a maximum of an arm length of the user or slightly less than arm length if, for example, the user swings a bent arm. Constraining the path can provide the advantage of lessening computational requirements used by the mobile device 102 to estimate camera pose for particular imagery.
- the pose estimation module 116 can use a model based on a concentric optical axis constraint 128 to estimate a camera pose.
- the concentric optical axis constraint 128 represents an assumption that a user can swing any mobile device with a camera in a circular path and in doing so the optical axis of the camera will pass through the world origin (X o , Y o , Z o ).
- all cameras on the rig may have optical axes that pass through the world origin (X o , Y o , Z o ).
- This constraint 128 can be further defined because when panoramic images are captured with the mobile device 102, the device 102 may be rotating around the head of a user and the optical axis passes through the center of the two eyes, approximately.
- the equidistant orbiting constraint 126 and the concentric optical axis constraint 128 can be used together or independently, resulting in the following example combinations.
- the pose estimation module 116 can use the equidistant orbiting constraint 126 and the concentric optical axis constraint 128, in combination.
- a center location on the mobile device i.e., capture device onboard mobile device 102
- the pose estimation module 116 can approximate the center of device 102 to be rotating on a sphere with the optical axis of device 102 passing through the world origin (X o , Y o , Z o ).
- the pose estimation module 116 can use the equidistant orbiting constraint 126 without the concentric optical axis constraint 128.
- the approximate center of device 102 can be modeled to rotate on a sphere with an optical axis of device 102 that may not pass through the center of the sphere.
- the pose estimation module 116 can use the concentric optical axis constraint 128 without the equidistant orbiting constraint 126.
- the mobile device 102 can be modeled to rotate on a sphere with an optical axis that passes through the world origin (X o , Y o , Z o ), but the distance to the world origin may vary.
- Using the concentric optical axis constraint 128 alone may provide image output that can be stitched into stereoscopic panoramic imagery.
- the pose estimation module 116 can use neither of the equidistant orbiting constraint 126 or the concentric optical axis constraint 128.
- the mobile device 102 can move freely without constrained axes or capture path, however, pose estimation may be time-consuming.
- the bundle adjustment module 118 can be configured to receive captured content (e.g., video or image frames) from one or more cameras (e.g., mobile device 102) and can estimate a camera pose that can be associated with the captured content.
- the bundle adjustment module 118 can access a portion (e.g., a bundle) of image frames and adjust the frames for rendering in 3D space.
- the bundle adjustment module 118 can be used to correct for deviations in camera pose or movement in a number of bundles and the corrected bundles can be stitched together to generate a 3D panoramic image.
- the bundle adjustment module 118 can retrieve or select a bundle of image frames.
- the bundle of image frames may represent a set of images that may or may not be in sequence.
- the module 118 can detect, from the bundle of image frames, a first set of feature points in a first frame.
- the module 118 can then track the first set of feature points in subsequent frames during playback, for example. While tracking the first set of feature points throughout the frames, the module 118 can detect new features that may come into a view as the sequence of frames is scrolled (or played).
- the new features may be represented by a second set of features in a second frame, for example.
- the module 118 can track the new features (i.e., the second set of features) in addition to tracking the first set of features.
- image processing system 107 may be installed on a mobile computing device.
- the system 107 may include, among other things, a pose estimation module 116 and a bundle adjustment module 118.
- the pose estimation module 116 may include at least one processor configured to estimate a camera pose for a sequence of images.
- the pose estimation module may simulate a uniform image capture path based on a non-uniform image capture path associated with capturing the sequence of images with the mobile computing device.
- a uniform image capture path may refer to a relatively smooth capture path that an image capture device follows during capture.
- the uniform image capture path may refer to a path remaining in a single plane around a shape without sharp turns or inflection points. Such examples include a circle, an ellipse, or similar shapes in which the radius remains constant or semi-constant.
- a uniform image capture path includes a path in a single plane in which the path maintains less than one inflection point in the image capture path.
- a non-uniform image capture path may refer to a combination of jagged and smooth movements along a capture path that an image capture device follows during capture.
- the non-uniform image capture path may refer to a path in multiple planes.
- a non-uniform image capture path includes a capture path with two or more inflection points in the image capture path.
- the bundle adjustment module 118 may include at least one processor configured to access at least a portion of the sequence of images to adjust the portion for rendering in 3D space, for example. In some implementations, the bundle adjustment module 118 is configured to correct for deviations in movement in a plurality of portions of the sequence of images. In some implementations, the system 107 may also include a stitching module configured to stitch a plurality of portions of the sequence of images to generate a 3D image of the scene. For example, the stitching module can access estimated poses to recraft scenes into a different pose than the scenes were originally captured. The different pose may correct for any number of things including, but not limited to errors, user configurations, display configurations, user capture mistakes, lighting, and the like.
- the system 100 can track and store gyroscope data from the mobile device, for example.
- the gyroscope data may pertain to specific image frames and/or feature points and may include raw estimation data of a camera rotation associated with the mobile device during capture of each specific frame. Such data can be used to estimate an initial estimate for a camera pose associated with the mobile device 102, for example.
- the system 100 can configure an objective function for an optimization procedure.
- the system 100 can configure variables as input to the objective function to include a) 3D locations of the tracked feature point, and b) camera positions and orientations for each of the captured frames (e.g., retrieved from the gyroscope data or other sensor or input to device 102).
- the system 100 can use the objective function and the 3D locations of the tracked feature points to generate an updated estimate.
- the system 100 can use a predefined initial estimate for the camera pose in lieu of gyroscope data.
- An initial estimate may be calculated using the camera pose with respect to calibrating a mapping of 3D feature points in a scene to 2D feature points in the scene in combination with using geometrical measurements of objects in the scene.
- the initial estimate can defined by system 100 to be used as the actual camera pose until the system 100 can perform additional estimation steps to calculate the camera pose.
- the objective function can be configured to project frames from a first image plane into a second image plane.
- the projection may function to determine any errors in the 3D feature points.
- the objective function can be used to project the 3D location of the feature points in each of the image frames using the estimated initial camera pose information to find one or more hypothetical projection locations that represent the 3D feature points.
- the hypothetical projection locations can be compared to the feature point locations that the system 100 has tracked. From this comparison, an error associated with particular feature points can be determined by assessing the differences between the hypothetical projection locations and the tracked feature points. Once the error is determined, system 100 can sum all errors associated with all feature points in a particular bundle to generate the objective function.
- the system 100 can repeat the initial estimate process to attempt to minimize the objective function and in turn, reduce errors associated with the pose corresponding to particular feature points.
- the system 100 can search for one or more camera poses (i.e., position and orientation) for the 3D locations that minimize the objective function.
- the objective function can be used to determine an accurate camera pose to associate with each bundle of images.
- the result of applying the objective function with a number of estimated camera poses, for each bundle, can enable generation of an accurate, distortion-free panoramic scene.
- the bundle adjustment module 118 can be used in combination with the pose estimation module 116 to estimate camera pose for a portion of frames in a particular capture path.
- the frames used in bundle adjustment module 118 may be encoded as video to optimize performance and playback.
- the bundle adjustment executed by the bundle adjustment module 118 may include selecting a portion (i.e., bundle) of frames within the image content in which to perform optimization adjustments.
- the bundle adjustment module 118 can receive estimated camera pose information from the pose estimation module 116, for example, and for each frame in the bundle, module 118 can use the estimated camera pose information to perform a course optimization for the frames in the bundle.
- the course optimization can be used to stitch the frames in the bundle together (i.e., using stitching module 120). Any number of bundles can be generated from frames within captured image content.
- the pose estimation module 116 can estimate pose for one or more frames in each bundle and the bundle adjustment module 118 and stitching module 120 can use the estimates to stitch together a panoramic scene.
- the bundle adjustment module 118 can be configured to correct images post-capture. For example, after images are captured, module 118 can compensate for a non-circular camera trajectory or sweep, a non-parallel principal (camera) axis, and/or an incorrect viewing-direction with respect to camera trajectory, just to name a few examples.
- the stitching module 120 can be configured to blend or stitch columns of pixels from several image frames to remove artifacts and provide distortion-free stereoscopic images.
- Example artifacts include artifacts due to poor exposure (or exposure changes from image frame to image frame) and/or artifacts due to misalignment errors based on a pose associated with a mobile device camera.
- the module 120 can blend additional content in between two columns of pixels to provide missing content in image frames. In other implementations, the module 120 can blend additional content in between two columns to remove artifacts in the image frames.
- the stitching module 120 can be configured to adjust captured images for rendering in 3D space by correcting for lateral roll movement that occurred during image capture. In some implementations, the stitching module 120 can be configured to adjust captured images for rendering in 3D space by correcting non-conformal camera arc movement that occurred during image capture. In some implementations, the stitching module 120 can be configured to adjust captured images for rendering in 3D space by correcting inconsistent radius measurements (related to the capture path) that occurred during image capture.
- the stitching module 120 can determine which columns to stitch (e.g., blend) together. For example, module 120 can analyze captured image data to determine at least one overlap between images. The overlap may include matched columns or regions of pixels from the images, for example. For each overlap found, the module 120 can select a portion of the matched pixels and combine them so that each portion is vertically aligned. The vertically aligned pixels can be combined to generate one segment of a 3D scene, for example. In the event that no overlap is detected, the stitching module 120 can generate additional image content to be blended between the images. The stitched content can be rendered in a head-mounted display to display the content in a VR environment, for example.
- the stitching module 120 can be configured to generate 3D stereoscopic images based on images obtained with mobile device 102.
- the stitching module 120 can be configured to blend pixels and/or image-strips from multiple image portions.
- blending can be based on flow fields as determined by an image interpolation component (not shown).
- the stitching module 120 can be configured to determine flow fields (and/or flow vectors) between related pixels in adjacent images. Flow fields can be used to compensate for both transformations that images have undergone and for processing images that have undergone transformations. For example, flow fields can be used to compensate for a transformation of a particular pixel grid of a captured image.
- module 120 can generate, by interpolation of surrounding images, one or more images that are not part of the captured images, and can interleave the generated images into the captured images to generate additional virtual reality content for a scene adapted for display in a VR environment.
- the system 100 can interleave the generated images using depth, alignment, and color correction into account to fill in any visual gaps.
- the stitching module 120 can be configured to adjust global color correction across all stitched elements in a particular scene.
- the system 100 can use estimated camera pose information and image pixel data to adjust color when stitching image frames together.
- the system 100 can also generate image data to color match other image data and can stitch the generated image data to the other image data.
- the system 100 can determine depth data from a number of different images and can stitch the different images together into a stereoscopic virtual reality video using the depth information.
- the stitching module 120 can estimate optical flow by adjusting particular images.
- the adjustments can include, for example, rectifying a portion of images, determining an estimated camera pose associated with the portion of images, and determining a flow between images in the portion.
- the stitching module 120 receives images depicting multiple sweeps of a particular capture path to capture images in a scene. The images corresponding to the multiple sweeps of the capture path can be stitched together to form a 3D stereoscopic view of the scene.
- the rendering module 122 can render the images into a realistic looking scene provided from a virtual viewpoint.
- module 122 can render a collection of images captured by a mobile device and provide virtual reality content, based on those images, to a user in HMD device 110, for example.
- Rendered scenes can be based on images captured in a single circular sweep, a partial sweep, multiple sweeps of a scene, and/or at a number of pose positions.
- the mobile device 102 can function as the image processing system 107.
- the same device 102 can perform processing on such images to improve or correct the accuracy and rendering of any subsequently generated 3D stereoscopic scene.
- the mobile device 102 can be configured with an image processing system to optimize the captured images using pose estimation and flow fields to provide an accurate rendering of a scene for presentation in a VR environment.
- the optimization can include stitching together particular image frames. For example, during capture of images, the mobile device 102 can determine which columns of pixels in each captured image frame can be stitched together. The mobile device 102 can analyze the image frames to determine feature points/pixel columns in which stitching would provide a cohesive scene. For each point/column that the mobile device 102 selects for stitching, device 102 can capture placement data for the points/columns and can linearly stitch appropriate points/columns together into a final image. Such optimizations can function to correct exposure inconsistencies, correct camera misalignment/pose errors, and/or correct for missing image frames.
- the methods and systems described herein can use a limited field of view associated with a camera on a mobile device, to produce 3D omnistereo images by stitching together content from multiple capture events of a scene.
- the methods and systems described herein can ensure that particular content or angles in a scene are correctly captured by estimating camera pose and adjusting the captured images according to the estimated camera pose.
- mobile device 102 may be one of many mobile devices arranged on a camera rig (not shown) to capture additional images of a particular scene.
- a camera rig can be configured for use as an image capture device and/or processing device to gather image data for rendering content in a VR environment, for example.
- the rig can include mobile devices configured with video cameras, image sensors, and/or infrared cameras, as well as processing circuitry to process captured images.
- Image processing system 107 can estimate the camera pose of any or all devices on the camera rig (not shown) for purposes of stitching together portions of images or video captured from the devices.
- HMD device 110 may represent a virtual reality headset, glasses, eyepiece, or other wearable device capable of displaying virtual reality content.
- the HMD device 110 can execute a VR application (not shown) which can playback received and/or processed images to a user.
- the VR application can be hosted by one or more of the devices 102, 106, or 112, shown in FIG. 1 .
- the HMD device 110 can provide a still representation and/or video playback of a scene captured by mobile device 102.
- the playback may include images stitched into a 3D stereoscopic and panoramic scene that provides a user with a realistic viewing of the scene.
- the devices 106 and 112 may be a laptop computer, a desktop computer, a mobile computing device, or a gaming console.
- the devices 106 and 112 can be a mobile computing device that can be placed and/or otherwise located near or within the HMD device 110.
- the mobile computing device can include a display device that can be used as the screen for the HMD device 110, for example.
- Devices 102, 106, and 112 can include hardware and/or software for executing a VR application.
- devices 102, 106, and 112 can include hardware and/or software that can recognize, monitor, and track 3D movement of the HMD device 110, when these devices are placed in front of or held within a range of positions relative to the HMD device 110.
- devices 102, 106, and 112 can provide additional content to HMD device 110 over network 104.
- devices 102, 106, 110, and 112 can be connected to/interfaced with one or all of each other either paired or connected through network 104.
- the connection can be wired or wireless.
- the network 104 can be a public communications network or a private communications network.
- the system 100 may include electronic storage.
- the electronic storage can include non-transitory storage media that electronically stores information.
- the electronic storage may be configured to store captured images, obtained images, preprocessed images, post-processed images, etc. Images captured with any of the disclosed cameras or devices can be processed and stored as one or more streams of video, or stored as individual frames. In some implementations, storage can occur during capture and rendering can occur directly after portions of images are captured to enable access to stereoscopic image content earlier than if capture and processing were concurrent.
- FIG. 2 is a diagram depicting an example capture path 202 for capturing images using a mobile device 102 (e.g., mobile device 204a-c).
- the mobile device 204a-c shown here represents the device 102 at three locations around the capture path 202 as a user 206 swings her arm in a circular motion to follow path 202.
- the capture path 202 can be modeled using a number of constraints described above with reference to FIG. 1 , for example. As shown, the capture path 202 is about circular in shape. In some implementations, the capture path 202 can be circular, semi-circular, partially circular, elliptical or other variant within arms-length of the user 206.
- capture resources can be tied to particular user capturing techniques, and movements.
- the trajectory of the mobile device e.g., mobile device 102
- the trajectory of the mobile device may be approximated by a spherical-shaped trajectory with an optical axis of the camera on device 102 approximately passing through at a point between the user's eyes during the capture.
- the user may unintentionally tilt, rotate or otherwise modify her mobile device (e.g., 204a) throughout a trajectory.
- her mobile device e.g., 204a
- these modifications can make pose estimation difficult.
- the constraints associated with the estimation techniques described herein can be used to simulate and/or estimate a stable camera capture path from an unknown or unexpected user-driven camera trajectory.
- constraints may provide the advantage of compact parameterization using minimal computational resources for pose estimation, while enabling faster reconstruction, and stabilizing pose estimation significantly.
- the constraints can also provide the advantage of ensuring that any subsequent image processing and rendering tasks become computationally less than traditional panoramic capture and pose estimation for mobile devices.
- FIG. 3 is a diagram depicting an example capture path 302 traversed by a mobile device 304 to capture panoramic images of a scene 300.
- the user may be actively capturing a view of a scene 300 using an onboard camera on device 304. Portions of the scene are displayed on the device 304 screen as the capture is occurring. The screen can be updated with new scenery as the user sweeps the device 304 across the scene 300.
- the user is holding mobile device 304 in a vertical (e.g., portrait) orientation and sweeping the device 304 along a capture path 302.
- the device 304 can be held horizontally (e.g., in a landscape orientation) or anywhere in the x-y-z plane 306 to capture a scene facing a camera associated with the mobile device 304.
- the user can sweep around a circular path around her body by sweeping her arm or moving her entire body in a portion of a circle, a semi-circle or full circle.
- the user can move the mobile device 304 at varying speeds and can change speeds mid-sweep.
- the user can sweep the device around an origin 308, which in some examples may represent the world origin (X o , Y o , Z o ).
- the capture path 302 may be represented as the surface of a sphere-shaped space in which mobile device 304 can sweep around path 302 while capturing images with the onboard camera. Capturing the images may correspond to recording image content with the mobile device 304 by aligning the device 304 to traverse the capture path 302.
- the following description includes a number of defined variables and equations that can be used with both the equidistant orbit constraint 126 and the concentric optical axis constraint 128 described in FIG. 1 above.
- the variables include [k] to represent a camera, [X_k] to represent a pixel location for a camera [k], [P] to represent a projection 310 or point in the world that a camera [k] is capturing, [R_k] to represent a rotation of a camera and [T_k] to represent a translation of a camera transform.
- the variables also include [O k] to represent the negative inverse of [R_k] ⁇ [T_k], which represents a position of the camera [k] at the camera center.
- the variables additionally include matrix [C] to represent a 3 x 3 camera intrinsic matrix where [X_k] is equal to [x_k; y_k; 1] to represent an x-y pixel location of the projection of [P] 310 in a camera [k].
- Matrix [C] can contain camera focal length data, skew factor data, pixel aspect ratio data, and principal points as elements.
- the matrix [C] can function to map a 3D point position in a scene to a 2D pixel coordinate in an image.
- the model can be applied using a number of combinations of the constraints. The following examples can be selectively implemented by the image processing system 107 and pose estimation module 116 to estimate camera pose associated with a camera [k].
- the pose estimation module 116 can use the equidistant orbit constraint 126 to estimate a camera pose.
- the camera translation vectors are the same in this example, the camera center position [O k] (which is equal to the negative inverse of [R_k] ⁇ [T 0]) for each of [T_k] can still differ from each other.
- all of the camera [k] centers may be considered to be constrained on a spherical surface. This constraint is generally motivated from the user-based motion of capturing a panoramic set of images by sweeping her arm and in doing so the position of the mobile device 304 in her hand may be assumed to be approximately rotating on a sphere.
- the concentric optical axis constraint 128 may define that any or all of the cameras' optical axis pass through the world origin 308.
- FIG. 4 is a flow chart diagramming one embodiment of a process 400 to estimate a camera pose for a mobile device.
- the procedure 400 may be used or produced by the systems such as those of FIGS. 1 and 5 .
- the process 400 is carried out on a mobile device.
- the process 400 is carried out on a computing device other than the mobile device.
- Block 402 illustrates that, in one embodiment, the system 100 can obtain a sequence of images.
- the sequence of images may be captured with a mobile device 102 and obtained from the device 102 or another computing device or storage device.
- the sequence of images can include a plurality of image frames of any or all portions of a scene.
- the system 100 can detect a first set of feature points in a first image frame and track the first set of feature points in a plurality of subsequent image frames. For example, as the sequence of images is presented to a computing device (e.g., during playback or editing), the system 100 can detect feature points and track the feature points as they appear during playback and after additional frames and feature points are presented on the computing device.
- the tracking can include indexing object location data, pose data, and/or other image data in such a way that system 100 can retrieve the data at a later time to perform stitching and rendering tasks.
- the system 100 can detect a second set of feature points in a second image frame.
- the system 100 can also track the second set of feature points in the plurality of subsequent image frames, at block 406.
- the system 100 can select a first initial camera pose associated with the first image frame and a second initial camera pose associated with the second image frame.
- the first and second initial camera poses may correspond to a hypothesis of location and pose information for image frames captured with mobile device 102. For example, the system 100 can determine that the user is swinging her arm while holding mobile device 102 to capture images and in doing so, she is looking at the screen of the mobile device 102. If the user is looking at the screen of the mobile device 102, the system 100 can determine specific angles of capture that can be used to calculated camera pose information. The system 100 can use this assumption and/or other constraints to determine a capture path and distances from the mobile device 102 to objects in the scene.
- selecting a first initial camera pose and a second initial camera pose can include obtaining gyroscope data associated with each image frame in the plurality of image frames and estimating, using the gyroscope data associated with the first image frame, a pose corresponding to the first set of feature points and estimating, using the gyroscope data associated with the second image frame, a pose corresponding to the second set of feature points.
- the gyroscope information can include at least a raw estimation of a rotation associated with a mobile device that captured the plurality of image frames.
- the gyroscope information may be used to estimate pose and image orientation associated with each image frame.
- the system 100 can determine a plurality of projection locations corresponding to each feature point in the first set of feature points and the second set of feature points.
- the projection locations in world coordinates, correspond to the center of projection of the camera on mobile device 102 during capture of particular feature points.
- the projection locations may correspond to a center of projection associated with device 102 for a time point at which the second image frame was captured.
- the projection locations may correspond to a center of projection associated with device 102 for a time point at which the second image frame was captured, for example
- determining a plurality of projection locations corresponding to each feature point in the first and second sets of feature points includes calculating the projections for each feature point by coupling position data and orientation data so that the pose is calculated using the coupled pair as a single representation instead of being calculated with an independent pose metric and an independent orientation metric.
- the method to determine the plurality of projection locations can also include selecting at least one constraint and applying the constraint to the coupled position data and orientation data for each feature point in the first and second sets of feature points.
- the system 100 can apply the equidistant orbit constraint or the concentric optical axis constraint separately.
- the system 100 can apply both the equidistant orbit constraint and the concentric optical axis constraint in a combined fashion.
- particular constraints are selected as a way to reduce search space for the algorithm described herein.
- the reduction can include reducing the search space from six degrees of freedom to four degrees of freedom.
- the method to determine the plurality of projection locations can further include estimating a camera pose for each feature point in the first and second sets of feature points.
- the process 400 can also include providing a stitching module, the estimated camera pose associated with each feature point, the stitching module configured to stitch the plurality of image frames to generate a 3D image of a scene.
- the process 400 can include performing the above on all of the feature points in all of the image frames captured by device 102. In other implementations, the process 400 can include performing the above steps on a portion of the feature points and/or a portion of the image frames.
- the system 100 can compare the projection locations corresponding to each feature point in the first set of feature points and each feature point in the second set of feature points to the corresponding tracked first and second set of feature points.
- the comparison can be used to determine errors in camera pose in order to improve the image content for stitching tasks at a later time. For example, using this comparison, an error associated with particular feature points can be determined by assessing differences between the hypothetical or initial projection locations and the tracked feature points. Once the error is determined, the system 100 can sum all errors associated with corresponding feature points in a particular bundle to generate a block of image content that can be stitched.
- the system 100 can provide an updated camera pose for the first and second image frames based at least in part on determining whether the projection locations meet a predefined threshold associated with the tracked first and second set of feature points.
- the predefined threshold may pertain to distortion level, parallax requirements, image metrics, etc.
- the system 100 can be configured to analyze the tracked first and second set of feature points to ensure (using a predefined distortion level) that the associated projection locations do not exceed a particular distortion percentage for a particular scene or image.
- the system 100 can be configured to ensure (using a predefined parallax threshold) that the projection locations do not exceed parallax discomfort levels for a human being.
- Such predefined thresholds can function to ensure the updated camera pose provides a scene or image with little or no distortion while providing proper parallax to a human being.
- FIG. 5 shows an example of a generic computer device 500 and a generic mobile computer device 550, which may be used with the techniques described here.
- Computing device 500 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers.
- Computing device 550 is intended to represent various forms of mobile devices, such as personal digital assistants, cellular telephones, smart phones, and other similar computing devices.
- the components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document.
- Computing device 500 includes a processor 502, memory 504, a storage device 506, a high-speed interface 508 connecting to memory 504 and high-speed expansion ports 510, and a low speed interface 512 connecting to low speed bus 514 and storage device 506.
- Each of the components 502, 504, 506, 508, 510, and 512 are interconnected using various busses, and may be mounted on a common motherboard or in other manners as appropriate.
- the processor 502 can process instructions for execution within the computing device 500, including instructions stored in the memory 504 or on the storage device 506 to display graphical information for a GUI on an external input/output device, such as display 516 coupled to high speed interface 508.
- multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory.
- multiple computing devices 500 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system).
- the memory 504 stores information within the computing device 500.
- the memory 504 is a volatile memory unit or units.
- the memory 504 is a non-volatile memory unit or units.
- the memory 504 may also be another form of computer-readable medium, such as a magnetic or optical disk.
- the storage device 506 is capable of providing mass storage for the computing device 500.
- the storage device 506 may be or contain a computer-readable medium, such as a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations.
- a computer program product can be tangibly embodied in an information carrier.
- the computer program product may also contain instructions that, when executed, perform one or more methods, such as those described above.
- the information carrier is a computer- or machine-readable medium, such as the memory 504, the storage device 506, or memory on processor 502.
- the high speed controller 508 manages bandwidth-intensive operations for the computing device 500, while the low speed controller 512 manages lower bandwidth-intensive operations.
- the high-speed controller 508 is coupled to memory 504, display 516 (e.g., through a graphics processor or accelerator), and to high-speed expansion ports 510, which may accept various expansion cards (not shown).
- low-speed controller 512 is coupled to storage device 506 and low-speed expansion port 514.
- the low-speed expansion port which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet) may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- input/output devices such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- the computing device 500 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a standard server 520, or multiple times in a group of such servers. It may also be implemented as part of a rack server system 524. In addition, it may be implemented in a personal computer such as a laptop computer 522. Alternatively, components from computing device 500 may be combined with other components in a mobile device (not shown), such as device 550. Each of such devices may contain one or more of computing device 500, 550, and an entire system may be made up of multiple computing devices 500, 550 communicating with each other.
- Computing device 550 includes a processor 552, memory 564, an input/output device such as a display 554, a communication interface 566, and a transceiver 568, among other components.
- the device 550 may also be provided with a storage device, such as a microdrive or other device, to provide additional storage.
- a storage device such as a microdrive or other device, to provide additional storage.
- Each of the components 550, 552, 564, 554, 566, and 568, are interconnected using various buses, and several of the components may be mounted on a common motherboard or in other manners as appropriate.
- the processor 552 can execute instructions within the computing device 550, including instructions stored in the memory 564.
- the processor may be implemented as a chipset of chips that include separate and multiple analog and digital processors.
- the processor may provide, for example, for coordination of the other components of the device 550, such as control of user interfaces, applications run by device 550, and wireless communication by device 550.
- Processor 552 may communicate with a user through control interface 558 and display interface 556 coupled to a display 554.
- the display 554 may be, for example, a TFT LCD (Thin-Film-Transistor Liquid Crystal Display) or an OLED (Organic Light Emitting Diode) display, or other appropriate display technology.
- the display interface 556 may comprise appropriate circuitry for driving the display 554 to present graphical and other information to a user.
- the control interface 558 may receive commands from a user and convert them for submission to the processor 552.
- an external interface 562 may be provide in communication with processor 552, so as to enable near area communication of device 550 with other devices.
- External interface 562 may provide, for example, for wired communication in some implementations, or for wireless communication in other implementations, and multiple interfaces may also be used.
- the memory 564 stores information within the computing device 550.
- the memory 564 can be implemented as one or more of a computer-readable medium or media, a volatile memory unit or units, or a non-volatile memory unit or units.
- Expansion memory 574 may also be provided and connected to device 550 through expansion interface 572, which may include, for example, a SIMM (Single In Line Memory Module) card interface.
- SIMM Single In Line Memory Module
- expansion memory 574 may provide extra storage space for device 550, or may also store applications or other information for device 550.
- expansion memory 574 may include instructions to carry out or supplement the processes described above, and may include secure information also.
- expansion memory 574 may be provide as a security module for device 550, and may be programmed with instructions that permit secure use of device 550.
- secure applications may be provided via the SIMM cards, along with additional information, such as placing identifying information on the SIMM card in a non-hackable manner.
- the memory may include, for example, flash memory and/or NVRAM memory, as discussed below.
- a computer program product is tangibly embodied in an information carrier.
- the computer program product contains instructions that, when executed, perform one or more methods, such as those described above.
- the information carrier is a computer- or machine-readable medium, such as the memory 564, expansion memory 574, or memory on processor 552, that may be received, for example, over transceiver 568 or external interface 562.
- Device 550 may communicate wirelessly through communication interface 566, which may include digital signal processing circuitry where necessary. Communication interface 566 may provide for communications under various modes or protocols, such as GSM voice calls, SMS, EMS, or MMS messaging, CDMA, TDMA, PDC, WCDMA, CDMA2000, or GPRS, among others. Such communication may occur, for example, through radio-frequency transceiver 568. In addition, short-range communication may occur, such as using a Bluetooth, Wi-Fi, or other such transceiver (not shown). In addition, GPS (Global Positioning System) receiver module 570 may provide additional navigation- and location-related wireless data to device 550, which may be used as appropriate by applications running on device 550.
- GPS Global Positioning System
- Device 550 may also communicate audibly using audio codec 560, which may receive spoken information from a user and convert it to usable digital information. Audio codec 560 may likewise generate audible sound for a user, such as through a speaker, e.g., in a handset of device 550. Such sound may include sound from voice telephone calls, may include recorded sound (e.g., voice messages, music files, etc.) and may also include sound generated by applications operating on device 550.
- Audio codec 560 may receive spoken information from a user and convert it to usable digital information. Audio codec 560 may likewise generate audible sound for a user, such as through a speaker, e.g., in a handset of device 550. Such sound may include sound from voice telephone calls, may include recorded sound (e.g., voice messages, music files, etc.) and may also include sound generated by applications operating on device 550.
- the computing device 550 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a cellular telephone 580. It may also be implemented as part of a smart phone 582, personal digital assistant, or other similar mobile device.
- implementations of the systems and techniques described here can be realized in digital electronic circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof.
- ASICs application specific integrated circuits
- These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- the systems and techniques described here can be implemented on a computer having a display device (e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor) for displaying information to the user and a keyboard and a pointing device (e.g., a mouse or a trackball) by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- a keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback (e.g., visual feedback, auditory feedback, or tactile feedback); and input from the user can be received in any form, including acoustic, speech, or tactile input.
- the systems and techniques described here can be implemented in a computing system that includes a back end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front end component (e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the systems and techniques described here), or any combination of such back end, middleware, or front end components.
- the components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include a local area network ("LAN”), a wide area network (“WAN”), and the Internet.
- LAN local area network
- WAN wide area network
- the Internet the global information network
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network.
- the relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
Description
- This description generally relates to pose estimation. In particular, the description relates to estimating camera pose metrics when generating 3D stereoscopic images for display in virtual reality (VR) environment.
- The combination of orientation and position of an object observed in an image is typically referred to as the pose of the object. The pose of the object may be referenced in relation to the position or orientation of the object relative to a coordinate system. In general, the coordinate system can be used to describe the pose with reference to a rotation and translation transformation.
- MOURAGNON E ET AL: "Real Time Localization and 3D Reconstruction", Conference on Computer Vision and Pattern Recognition, 17-22 June 2006, pages 363-370, XP031461925, ISBN: 978-0-7695-2597-6 describes a method of estimating the motion of a calibrated camera and the tri-dimensional geometry of the environment.
- A system of one or more computers can be configured to perform particular operations or actions by virtue of having software, firmware, hardware, or a combination of them installed on the system that in operation causes or cause the system to perform the actions. One or more computer programs can be configured to perform particular operations or actions by virtue of including instructions that, when executed by data processing apparatus, cause the apparatus to perform the actions.
- In one general aspect, a computer-implemented method includes obtaining a sequence of images including a plurality of image frames of a scene, detecting a first set of feature points in a first image frame, tracking the first set of feature points in a plurality of subsequent image frames and while continuing to track the first set of feature points, detecting a second set of feature points in a second image frame, wherein new features are represented by the second set of feature points, tracking the second set of feature points in the plurality of subsequent image frames, and selecting a first initial camera pose associated with the first image frame and a second initial camera pose associated with the second image frame. The method also includes determining a plurality of projection locations corresponding to each feature point in the first set of feature points and the second set of feature points in which the projection locations are based at least in part on the first initial camera pose and the second initial camera pose. The method also includes comparing the projection locations corresponding to each feature point in the first set of feature points and each feature point in the second set of feature points to the corresponding tracked first set of feature points and the tracked second set of feature points, and in response to determining that the projection locations meet a predefined threshold corresponding to distortion associated with the tracked first and second set of feature points, the method includes generating an updated camera pose for the first image frame and the second image frame. Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.
- Implementations may include one or more of the following features. The computer-implemented method further including generating a 3D image of the scene using the updated camera pose for the first image frames and the second image frames based at least in part on determining that the projection locations meet the predefined threshold. The computer-implemented method in which selecting a first initial camera pose and a second initial camera pose includes obtaining gyroscope data associated with each image frame in the plurality of image frames and estimating, using the gyroscope data associated with the first image frame, a pose corresponding to the first set of feature points. The computer-implemented method further including estimating, using the gyroscope data associated with the second image frame, a pose corresponding to the second set of feature points. The computer-implemented method in which the gyroscope data includes at least a raw estimation of a rotation associated with a mobile device that captured the plurality of image frames, and the gyroscope data is used to estimate pose and image orientation associated with each image frame.
- In some implementations, the computer-implemented method may also include determining a plurality of projection locations corresponding to each feature point in the first and second sets of feature points, the determining including for each feature point, coupling position data and orientation data, selecting at least one constraint and applying the constraint to the coupled position data and orientation data, and estimating a camera pose, and providing, to a stitching module, the updated camera pose associated with each feature point in which the stitching module is configured to stitch the plurality of image frames to generate a 3D image of the scene based on the estimated camera pose.
- In some implementations, the method may include at least one constraint of an equidistant orbit constraint or a concentric optical axis constraint. The computer-implemented method in which the constraint is selected to reduce search space for a pose estimation algorithm from six degrees of freedom to four degrees of freedom. The computer-implemented method where the sequence of images are captured with a mobile device. The computer-implemented method where the plurality of projection locations correspond to a center of projection of a mobile device that captured the plurality of image frames. Implementations of the described techniques may include hardware, a method or process, or computer software on a computer-accessible medium.
- In another general aspect, a system is described that includes one or more computers configured to perform particular operations or actions by virtue of having software, firmware, hardware, or a combination of them installed on the system that in operation causes or cause the system to perform the actions. One or more computer programs can be configured to perform particular operations or actions by virtue of including instructions that, when executed by data processing apparatus, cause the apparatus to perform the actions. The system may include a computer-implemented system executing on a mobile computing device. The system may include a pose estimation module with at least one processor configured to estimate a camera pose for a sequence of images. The pose estimation module may simulate a uniform image capture path based on a non-uniform image capture path associated with capturing the sequence of images with the mobile computing device.
- The system may also include a bundle adjustment module with at least one processor configured to access at least a portion of the sequence of images and adjust the portion for rendering in 3D space. Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.
- Implementations of the system may also include a uniform image capture path with a path in a single plane and maintaining less than one inflection point in the image capture path. Implementations of the system may also include a non-uniform image capture path includes a path in more than one plane or a capture path with two or more inflection points in the image capture path.
- Implementations may include one or more of the following features. The system where the bundle adjustment module is configured to correct for deviations in movement in a plurality of portions of the sequence of images. The system further including a stitching module configured to stitch a plurality of portions of the sequence of images to generate a 3D scene. Implementations of the described techniques may include hardware, a method or process, or computer software on a computer-accessible medium.
- Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.
- The details of one or more implementations are set forth in the accompanying drawings and the description below. Other features will be apparent from the description and drawings, and from the claims.
-
-
FIG. 1 is a block diagram of an example system for capturing images and estimating camera pose information for rendering the images in a 3D virtual reality (VR) environment. -
FIG. 2 is a diagram depicting an example capture path for capturing images using a mobile device. -
FIG. 3 is a diagram depicting an example capture path traversed by a mobile device to capture panoramic images of a scene. -
FIG. 4 is a flow chart diagramming one embodiment of a process to estimate a camera pose for a mobile device. -
FIG. 5 is an example of a computer device and a mobile computer device that can be used to implement the techniques described here. - Like reference symbols in the various drawings indicate like elements.
- The following disclosure describes a number of techniques that can be employed to estimate camera pose in association with a mobile device. The mobile device may be configured to capture and produce omnistereo panoramas. For example, an application running on the mobile device can be used by a user to capture images of a scene. The application can receive the images as they are captured and begin to estimate camera position and camera orientation information (i.e., camera pose) for each image. In some implementations, the received image content includes video content and the application can estimate camera pose information for frames or portions of frames during or after image capture. The application can employ algorithms described in this disclosure to provide two-dimensional (2D) and/or three-dimensional (3D) panoramic or omnistereo panoramic images using captured image content and estimated camera pose information associated with the mobile device.
- Estimating camera pose can be performed using algorithms described in this disclosure in combination with a number of particular constraints. Such constraints can be used as stand-alone constraints, in combination, or altogether to estimate a camera pose associated with a mobile device. In general, the algorithms described herein may function to estimate the camera pose by coupling together both camera orientation and camera position information in the same coordinate system. Coupling the camera orientation information with the camera position information can ease the calculations used to estimate camera pose. By coupling the information, the capture method for mobile device-based panoramic capture can be likened to capturing image content using a spherical-shaped trajectory in a coordinate system. Such a capture may be performed by the user holding the mobile device in her hand and swinging (e.g., move) an arm (partially or fully extended) in a circular path that can be formed by passing through points above and below the user's shoulder and directly in front and behind the user at the shoulder level. Alternatively, the circular path may shift toward the user (e.g., above her head) or away from the user (e.g., to the right or left of the user's head). The motion can be used during capture to collect images of a scene. In some implementations, the trajectory of the mobile device during the capture swing may be approximated by a spherical-shaped trajectory with an optical axis of the camera on the mobile device approximately passing through at a point between the user's eyes during the capture. This capture path is an example path and other shifted or alternate shaped paths are possible.
- In some implementations, the user can swing (e.g., move) the mobile device in a circular path (or substantially circular path or nearly circular path) to capture panoramic images using a partial arm extension or a full arm extension during a swing. The swing can be likened to an arm circle that swings on the left side of the body of a user or on the right side of the body of a user. The camera position and camera orientation can be coupled according to constraints based on arm extension and based on an orientation associated with the mobile device. For example, a maximum arm constraint can be used because the user's arm is a finite length and cannot be further extended. A mobile device orientation can be determined with (a) assumptions of a user viewing a screen on the mobile device (e.g., during capture), (b) actual measurements from gyroscopes or (c) other sensors on the mobile device. Particular techniques and constraints are described in detail below.
-
FIG. 1 is a block diagram of anexample system 100 for capturing images and estimating camera pose information for rendering the images in a 3D virtual reality (VR) environment. In theexample system 100, amobile device 102 can be used to capture images and/or video and provide those images or video over anetwork 104, or alternatively, can provide the images directly to animage processing system 107 for analysis and processing. In some implementations ofsystem 100, theimage processing system 107 is provided as part ofmobile device 102. In other implementations, portions ofimage processing system 107 are provided onmobile device 102, while other portions ofimage processing system 107 are provided on another computing system. - The
mobile device 102 may be a mobile phone, an electronic tablet, a laptop, a camera, or other such electronic device that may be used to capture and/or obtain image data. In some implementations, themobile device 102 can be configured to capture still images. In some implementations, themobile device 102 can be configured to capture video and store such content as individual frames or video (e.g., .avi files) and such stored images can be uploaded to the Internet, another server or device, or stored locally on themobile device 102. In some implementations, incoming images can be stored as encoded images or encoded video. In general, the images described throughout this disclosure may include a plurality of consecutive or non-consecutive image frames of a scene. In some implementations, the images described throughout this disclosure may include a combination of both consecutive sets of images and non-consecutive sets of images. The images may be captured and rendered in two or three dimensions. - In operation of
device 102, a user can access a camera (also can be referred to as a capture device) ondevice 102, hold thedevice 102 in a selected orientation and pan or sweep thedevice 102 around to capture images along a capture path. In general,sweeping device 102 around to capture images along a capture path can include having the user move thedevice 102 around a circular capture path (or a capture path of another shape or profile) surrounding her head or body (or part of a body). In such a sweep, thedevice 102 may be directed outward away from the user and pointed toward scenery while the user follows the circular capture path (or a capture path of another shape or profile). In some implementations, the capture path can be disposed within, or aligned along, a single plane, can be curved, can have linear portions, can have one or more discontinuities, and/or so forth. In some implementations, the capture path can have portions that are disposed within, or aligned along, more than one plane. While sweeping a path (e.g., a capture path) and capturing images, themobile device 102 can receive indications to modify particular capture configurations. Capture configurations can include, but are not limited to, mobile device pose/capture positions/angles/pitch/roll, field of view, camera settings for lighting/exposure, horizontal or vertical tilt or twist, speed of camera capture sweep, etc. - Once the images are captured, the image processing system 107 (or mobile device 102) can perform a number of processes on the images to generate panoramic images that can be provided to a head mounted display (HMD)
device 110 for rendering overnetwork 104, for example. In some implementations, theimage processing system 107 can also provide the processed images tomobile device 102 and/or tocomputing device 112 for pose estimation, rendering, storage, or further processing. - As shown in
FIG. 1 , theimage processing system 107 includes apose estimation module 116, abundle adjustment module 118, astitching module 120, and arendering module 122. Thepose estimation module 116 can be configured to estimate a pose that is associated with a capture device on a mobile device that captures panoramic images. In particular, thepose estimation module 116 can be configured to determine, for a particular panoramic capture session, a probable position, orientation, and focal length associated with the capture device on the mobile device. The position, orientation, and focal length can be used as inputs into a pose estimation model carried out by thepose estimation module 116. In some implementations, thebundle adjustment module 118 is part of thepose estimation module 116. - In general, pose information for an object can be expressed by combining three linear displacement coordinates (x, y, z) of any reference point on the object, as well as the three inclination angles (i.e., Euler angles (ϕ, θ, ψ)) that may describe the pitch, yaw and roll of the object. Accordingly, the absolute camera pose for a 3D environment can be measured using an invariant object/feature located in the 3D environment. In some implementations, the absolute pose is expressed with absolute pose data (ϕ, θ, ψ, x, y ,z) that represents Euler rotated object coordinates expressed in world coordinates (Xo, Yo, Zo) with respect to a reference location, such as, for example, the world origin. Other conventions for expressing pose data in 3D space and representing all six degrees of freedom (i.e., three translational degrees of freedom and three rotational degrees of freedom) are also supported. Accordingly, similar conventions for expressing estimated camera pose information will be used throughout this disclosure.
- The
pose estimation module 116 can use one ormore constraints 124 to estimate the pose for particular images.Such constraints 124 can be motivated by the available resources used (e.g., memory resources, computational resources, and capture configuration limitations) when capturing panoramic images with a mobile device. That is, memory and computational resources can be limited for mobile device image capture activities and as such, thepose estimation module 116 can employ one or more constraints and techniques to lessen the computational burden on themobile device 102, while still providing a stereoscopic 3D image. In some implementations, use ofconstraints 124 can be used to reduce the search space for any of the algorithms used to estimate pose so that the estimation can be robust, yet computationally sparing. In one example, the search space for a pose estimation algorithm described herein can be reduced from six degrees of freedom into four degrees of freedom or three degrees of freedom. For example, if N images exist and thepose estimation module 116 is configured to function without constraints, the number of degrees of freedom [DoF] may be 6∗N. If the concentricoptical axis constraint 128 is used alone, the DoF is reduced from 6∗N to 4∗N DoF. If theequidistant orbiting constraint 126 is used, the DoF can be reduced from 6∗N DoF to 3∗N +3 DoF. If both the concentricoptical axis constraint 128 and theequidistant orbiting constraint 126 are used, the DoF can be reduced to 3∗N+1 DoF. - In some implementations, the
system 100 can employ rules, assumptions, and/orconstraints 124 based on two model constraints. For example, thepose estimation module 116 can use a model based on anequidistant orbit constraint 126 to estimate a camera pose formobile device 102. Theequidistant orbit constraint 126 represents a constrained circular-shaped orbit as the user swings an arm around a capture path. Thisconstraint 126 ensures that the orbit the capture path travels is equidistant from a center point because the arm length of the user will not change beyond a maximum limit during capture. Thus, the circular path can be constrained to a maximum of an arm length of the user or slightly less than arm length if, for example, the user swings a bent arm. Constraining the path can provide the advantage of lessening computational requirements used by themobile device 102 to estimate camera pose for particular imagery. - In another example, the
pose estimation module 116 can use a model based on a concentricoptical axis constraint 128 to estimate a camera pose. The concentricoptical axis constraint 128 represents an assumption that a user can swing any mobile device with a camera in a circular path and in doing so the optical axis of the camera will pass through the world origin (Xo, Yo, Zo). Similarly, if multiple cameras are used (i.e., on a camera rig configured to perform the same circular swinging motion), all cameras on the rig may have optical axes that pass through the world origin (Xo, Yo, Zo). Thisconstraint 128 can be further defined because when panoramic images are captured with themobile device 102, thedevice 102 may be rotating around the head of a user and the optical axis passes through the center of the two eyes, approximately. - In operation of
system 100, theequidistant orbiting constraint 126 and the concentricoptical axis constraint 128 can be used together or independently, resulting in the following example combinations. In a non-limiting example, thepose estimation module 116 can use theequidistant orbiting constraint 126 and the concentricoptical axis constraint 128, in combination. In this example, a center location on the mobile device (i.e., capture device onboard mobile device 102) can be manipulated by a user to rotate on a circular (e.g., spherical) path and as such, thepose estimation module 116 can approximate the center ofdevice 102 to be rotating on a sphere with the optical axis ofdevice 102 passing through the world origin (Xo, Yo, Zo). - In another non-limiting example, the
pose estimation module 116 can use theequidistant orbiting constraint 126 without the concentricoptical axis constraint 128. In this example, the approximate center ofdevice 102 can be modeled to rotate on a sphere with an optical axis ofdevice 102 that may not pass through the center of the sphere. - In another non-limiting example, the
pose estimation module 116 can use the concentricoptical axis constraint 128 without theequidistant orbiting constraint 126. In this example, themobile device 102 can be modeled to rotate on a sphere with an optical axis that passes through the world origin (Xo, Yo, Zo), but the distance to the world origin may vary. Using the concentricoptical axis constraint 128 alone may provide image output that can be stitched into stereoscopic panoramic imagery. - In yet another non-limiting example, the
pose estimation module 116 can use neither of theequidistant orbiting constraint 126 or the concentricoptical axis constraint 128. In this example, themobile device 102 can move freely without constrained axes or capture path, however, pose estimation may be time-consuming. - Referring to
FIG. 1 , thebundle adjustment module 118 can be configured to receive captured content (e.g., video or image frames) from one or more cameras (e.g., mobile device 102) and can estimate a camera pose that can be associated with the captured content. Thebundle adjustment module 118 can access a portion (e.g., a bundle) of image frames and adjust the frames for rendering in 3D space. In some implementations, thebundle adjustment module 118 can be used to correct for deviations in camera pose or movement in a number of bundles and the corrected bundles can be stitched together to generate a 3D panoramic image. - In operation, the
bundle adjustment module 118 can retrieve or select a bundle of image frames. The bundle of image frames may represent a set of images that may or may not be in sequence. Themodule 118 can detect, from the bundle of image frames, a first set of feature points in a first frame. Themodule 118 can then track the first set of feature points in subsequent frames during playback, for example. While tracking the first set of feature points throughout the frames, themodule 118 can detect new features that may come into a view as the sequence of frames is scrolled (or played). The new features may be represented by a second set of features in a second frame, for example. Themodule 118 can track the new features (i.e., the second set of features) in addition to tracking the first set of features. - In some implementations,
image processing system 107 may be installed on a mobile computing device. Thesystem 107 may include, among other things, apose estimation module 116 and abundle adjustment module 118. Thepose estimation module 116 may include at least one processor configured to estimate a camera pose for a sequence of images. The pose estimation module may simulate a uniform image capture path based on a non-uniform image capture path associated with capturing the sequence of images with the mobile computing device. - A uniform image capture path may refer to a relatively smooth capture path that an image capture device follows during capture. In some implementations, the uniform image capture path may refer to a path remaining in a single plane around a shape without sharp turns or inflection points. Such examples include a circle, an ellipse, or similar shapes in which the radius remains constant or semi-constant. In some implementations, a uniform image capture path includes a path in a single plane in which the path maintains less than one inflection point in the image capture path.
- A non-uniform image capture path may refer to a combination of jagged and smooth movements along a capture path that an image capture device follows during capture. In some implementations, the non-uniform image capture path may refer to a path in multiple planes. In some implementations, a non-uniform image capture path includes a capture path with two or more inflection points in the image capture path.
- The
bundle adjustment module 118 may include at least one processor configured to access at least a portion of the sequence of images to adjust the portion for rendering in 3D space, for example. In some implementations, thebundle adjustment module 118 is configured to correct for deviations in movement in a plurality of portions of the sequence of images. In some implementations, thesystem 107 may also include a stitching module configured to stitch a plurality of portions of the sequence of images to generate a 3D image of the scene. For example, the stitching module can access estimated poses to recraft scenes into a different pose than the scenes were originally captured. The different pose may correct for any number of things including, but not limited to errors, user configurations, display configurations, user capture mistakes, lighting, and the like. - In some implementations, the
system 100 can track and store gyroscope data from the mobile device, for example. The gyroscope data may pertain to specific image frames and/or feature points and may include raw estimation data of a camera rotation associated with the mobile device during capture of each specific frame. Such data can be used to estimate an initial estimate for a camera pose associated with themobile device 102, for example. Using the initial estimate, thesystem 100 can configure an objective function for an optimization procedure. In addition, thesystem 100 can configure variables as input to the objective function to include a) 3D locations of the tracked feature point, and b) camera positions and orientations for each of the captured frames (e.g., retrieved from the gyroscope data or other sensor or input to device 102). Thesystem 100 can use the objective function and the 3D locations of the tracked feature points to generate an updated estimate. In some implementations, thesystem 100 can use a predefined initial estimate for the camera pose in lieu of gyroscope data. An initial estimate may be calculated using the camera pose with respect to calibrating a mapping of 3D feature points in a scene to 2D feature points in the scene in combination with using geometrical measurements of objects in the scene. The initial estimate can defined bysystem 100 to be used as the actual camera pose until thesystem 100 can perform additional estimation steps to calculate the camera pose. - The objective function can be configured to project frames from a first image plane into a second image plane. The projection may function to determine any errors in the 3D feature points. The objective function can be used to project the 3D location of the feature points in each of the image frames using the estimated initial camera pose information to find one or more hypothetical projection locations that represent the 3D feature points. The hypothetical projection locations can be compared to the feature point locations that the
system 100 has tracked. From this comparison, an error associated with particular feature points can be determined by assessing the differences between the hypothetical projection locations and the tracked feature points. Once the error is determined,system 100 can sum all errors associated with all feature points in a particular bundle to generate the objective function. In general, thesystem 100 can repeat the initial estimate process to attempt to minimize the objective function and in turn, reduce errors associated with the pose corresponding to particular feature points. For example, thesystem 100 can search for one or more camera poses (i.e., position and orientation) for the 3D locations that minimize the objective function. The objective function can be used to determine an accurate camera pose to associate with each bundle of images. The result of applying the objective function with a number of estimated camera poses, for each bundle, can enable generation of an accurate, distortion-free panoramic scene. - In some implementations, the
bundle adjustment module 118 can be used in combination with thepose estimation module 116 to estimate camera pose for a portion of frames in a particular capture path. In general, the frames used inbundle adjustment module 118 may be encoded as video to optimize performance and playback. The bundle adjustment executed by thebundle adjustment module 118 may include selecting a portion (i.e., bundle) of frames within the image content in which to perform optimization adjustments. Thebundle adjustment module 118 can receive estimated camera pose information from thepose estimation module 116, for example, and for each frame in the bundle,module 118 can use the estimated camera pose information to perform a course optimization for the frames in the bundle. The course optimization can be used to stitch the frames in the bundle together (i.e., using stitching module 120). Any number of bundles can be generated from frames within captured image content. Thepose estimation module 116 can estimate pose for one or more frames in each bundle and thebundle adjustment module 118 andstitching module 120 can use the estimates to stitch together a panoramic scene. - In some implementations, the
bundle adjustment module 118 can be configured to correct images post-capture. For example, after images are captured,module 118 can compensate for a non-circular camera trajectory or sweep, a non-parallel principal (camera) axis, and/or an incorrect viewing-direction with respect to camera trajectory, just to name a few examples. - Referring again to
FIG. 1 , thestitching module 120 can be configured to blend or stitch columns of pixels from several image frames to remove artifacts and provide distortion-free stereoscopic images. Example artifacts include artifacts due to poor exposure (or exposure changes from image frame to image frame) and/or artifacts due to misalignment errors based on a pose associated with a mobile device camera. In some implementations, themodule 120 can blend additional content in between two columns of pixels to provide missing content in image frames. In other implementations, themodule 120 can blend additional content in between two columns to remove artifacts in the image frames. - In some implementations, the
stitching module 120 can be configured to adjust captured images for rendering in 3D space by correcting for lateral roll movement that occurred during image capture. In some implementations, thestitching module 120 can be configured to adjust captured images for rendering in 3D space by correcting non-conformal camera arc movement that occurred during image capture. In some implementations, thestitching module 120 can be configured to adjust captured images for rendering in 3D space by correcting inconsistent radius measurements (related to the capture path) that occurred during image capture. - In some implementations, the
stitching module 120 can determine which columns to stitch (e.g., blend) together. For example,module 120 can analyze captured image data to determine at least one overlap between images. The overlap may include matched columns or regions of pixels from the images, for example. For each overlap found, themodule 120 can select a portion of the matched pixels and combine them so that each portion is vertically aligned. The vertically aligned pixels can be combined to generate one segment of a 3D scene, for example. In the event that no overlap is detected, thestitching module 120 can generate additional image content to be blended between the images. The stitched content can be rendered in a head-mounted display to display the content in a VR environment, for example. - In some implementations, the
stitching module 120 can be configured to generate 3D stereoscopic images based on images obtained withmobile device 102. Thestitching module 120 can be configured to blend pixels and/or image-strips from multiple image portions. In some implementations, blending can be based on flow fields as determined by an image interpolation component (not shown). For example, thestitching module 120 can be configured to determine flow fields (and/or flow vectors) between related pixels in adjacent images. Flow fields can be used to compensate for both transformations that images have undergone and for processing images that have undergone transformations. For example, flow fields can be used to compensate for a transformation of a particular pixel grid of a captured image. In some implementations,module 120 can generate, by interpolation of surrounding images, one or more images that are not part of the captured images, and can interleave the generated images into the captured images to generate additional virtual reality content for a scene adapted for display in a VR environment. - In some implementations, the
system 100 can interleave the generated images using depth, alignment, and color correction into account to fill in any visual gaps. For example, thestitching module 120 can be configured to adjust global color correction across all stitched elements in a particular scene. In addition, thesystem 100 can use estimated camera pose information and image pixel data to adjust color when stitching image frames together. Thesystem 100 can also generate image data to color match other image data and can stitch the generated image data to the other image data. In some implementations, thesystem 100 can determine depth data from a number of different images and can stitch the different images together into a stereoscopic virtual reality video using the depth information. - In some implementations, the
stitching module 120 can estimate optical flow by adjusting particular images. The adjustments can include, for example, rectifying a portion of images, determining an estimated camera pose associated with the portion of images, and determining a flow between images in the portion. In some implementations, thestitching module 120 receives images depicting multiple sweeps of a particular capture path to capture images in a scene. The images corresponding to the multiple sweeps of the capture path can be stitched together to form a 3D stereoscopic view of the scene. - Once the images are properly adjusted and stitched, the
rendering module 122 can render the images into a realistic looking scene provided from a virtual viewpoint. For example,module 122 can render a collection of images captured by a mobile device and provide virtual reality content, based on those images, to a user inHMD device 110, for example. Rendered scenes can be based on images captured in a single circular sweep, a partial sweep, multiple sweeps of a scene, and/or at a number of pose positions. - In some implementations, the
mobile device 102 can function as theimage processing system 107. For example, in the event that combinations of particular images captured withdevice 102 do not provide an accurate 3D stereoscopic scene, thesame device 102 can perform processing on such images to improve or correct the accuracy and rendering of any subsequently generated 3D stereoscopic scene. In particular, themobile device 102 can be configured with an image processing system to optimize the captured images using pose estimation and flow fields to provide an accurate rendering of a scene for presentation in a VR environment. - In some implementations, the optimization can include stitching together particular image frames. For example, during capture of images, the
mobile device 102 can determine which columns of pixels in each captured image frame can be stitched together. Themobile device 102 can analyze the image frames to determine feature points/pixel columns in which stitching would provide a cohesive scene. For each point/column that themobile device 102 selects for stitching,device 102 can capture placement data for the points/columns and can linearly stitch appropriate points/columns together into a final image. Such optimizations can function to correct exposure inconsistencies, correct camera misalignment/pose errors, and/or correct for missing image frames. - The implementations described in this disclosure can provide one or more advantages. For example, the methods and systems described herein can use a limited field of view associated with a camera on a mobile device, to produce 3D omnistereo images by stitching together content from multiple capture events of a scene. In addition, the methods and systems described herein can ensure that particular content or angles in a scene are correctly captured by estimating camera pose and adjusting the captured images according to the estimated camera pose.
- In some implementations, multiple mobile devices can be used to capture images of the scene. For example,
mobile device 102 may be one of many mobile devices arranged on a camera rig (not shown) to capture additional images of a particular scene. Such a camera rig can be configured for use as an image capture device and/or processing device to gather image data for rendering content in a VR environment, for example. The rig can include mobile devices configured with video cameras, image sensors, and/or infrared cameras, as well as processing circuitry to process captured images.Image processing system 107 can estimate the camera pose of any or all devices on the camera rig (not shown) for purposes of stitching together portions of images or video captured from the devices. - In the
example system 100,HMD device 110 may represent a virtual reality headset, glasses, eyepiece, or other wearable device capable of displaying virtual reality content. In operation, theHMD device 110 can execute a VR application (not shown) which can playback received and/or processed images to a user. In some implementations, the VR application can be hosted by one or more of thedevices FIG. 1 . In one example, theHMD device 110 can provide a still representation and/or video playback of a scene captured bymobile device 102. In general, the playback may include images stitched into a 3D stereoscopic and panoramic scene that provides a user with a realistic viewing of the scene. - In the
example system 100, thedevices devices HMD device 110. The mobile computing device can include a display device that can be used as the screen for theHMD device 110, for example.Devices devices HMD device 110, when these devices are placed in front of or held within a range of positions relative to theHMD device 110. In some implementations,devices HMD device 110 overnetwork 104. In some implementations,devices network 104. The connection can be wired or wireless. Thenetwork 104 can be a public communications network or a private communications network. - The
system 100 may include electronic storage. The electronic storage can include non-transitory storage media that electronically stores information. The electronic storage may be configured to store captured images, obtained images, preprocessed images, post-processed images, etc. Images captured with any of the disclosed cameras or devices can be processed and stored as one or more streams of video, or stored as individual frames. In some implementations, storage can occur during capture and rendering can occur directly after portions of images are captured to enable access to stereoscopic image content earlier than if capture and processing were concurrent. -
FIG. 2 is a diagram depicting anexample capture path 202 for capturing images using a mobile device 102 (e.g.,mobile device 204a-c). Themobile device 204a-c shown here represents thedevice 102 at three locations around thecapture path 202 as auser 206 swings her arm in a circular motion to followpath 202. Thecapture path 202 can be modeled using a number of constraints described above with reference toFIG. 1 , for example. As shown, thecapture path 202 is about circular in shape. In some implementations, thecapture path 202 can be circular, semi-circular, partially circular, elliptical or other variant within arms-length of theuser 206. - As described above, capture resources can be tied to particular user capturing techniques, and movements. For example, as a user swings her arm to capture a panoramic view of a scene (e.g., images), the trajectory of the mobile device (e.g., mobile device 102) may be approximated by a spherical-shaped trajectory with an optical axis of the camera on
device 102 approximately passing through at a point between the user's eyes during the capture. The user may unintentionally tilt, rotate or otherwise modify her mobile device (e.g., 204a) throughout a trajectory. These modifications can make pose estimation difficult. Accordingly, the constraints associated with the estimation techniques described herein can be used to simulate and/or estimate a stable camera capture path from an unknown or unexpected user-driven camera trajectory. Such constraints may provide the advantage of compact parameterization using minimal computational resources for pose estimation, while enabling faster reconstruction, and stabilizing pose estimation significantly. The constraints can also provide the advantage of ensuring that any subsequent image processing and rendering tasks become computationally less than traditional panoramic capture and pose estimation for mobile devices. -
FIG. 3 is a diagram depicting anexample capture path 302 traversed by amobile device 304 to capture panoramic images of ascene 300. In this example, the user may be actively capturing a view of ascene 300 using an onboard camera ondevice 304. Portions of the scene are displayed on thedevice 304 screen as the capture is occurring. The screen can be updated with new scenery as the user sweeps thedevice 304 across thescene 300. - In the depicted example, the user is holding
mobile device 304 in a vertical (e.g., portrait) orientation and sweeping thedevice 304 along acapture path 302. In some implementations, thedevice 304 can be held horizontally (e.g., in a landscape orientation) or anywhere in thex-y-z plane 306 to capture a scene facing a camera associated with themobile device 304. The user can sweep around a circular path around her body by sweeping her arm or moving her entire body in a portion of a circle, a semi-circle or full circle. The user can move themobile device 304 at varying speeds and can change speeds mid-sweep. In general, the user can sweep the device around anorigin 308, which in some examples may represent the world origin (Xo, Yo, Zo). - In a non-limiting example, the
capture path 302 may be represented as the surface of a sphere-shaped space in whichmobile device 304 can sweep aroundpath 302 while capturing images with the onboard camera. Capturing the images may correspond to recording image content with themobile device 304 by aligning thedevice 304 to traverse thecapture path 302. - The following description includes a number of defined variables and equations that can be used with both the
equidistant orbit constraint 126 and the concentricoptical axis constraint 128 described inFIG. 1 above. The variables include [k] to represent a camera, [X_k] to represent a pixel location for a camera [k], [P] to represent aprojection 310 or point in the world that a camera [k] is capturing, [R_k] to represent a rotation of a camera and [T_k] to represent a translation of a camera transform. The variables also include [O k] to represent the negative inverse of [R_k] ∗ [T_k], which represents a position of the camera [k] at the camera center. The variables additionally include matrix [C] to represent a 3 x 3 camera intrinsic matrix where [X_k] is equal to [x_k; y_k; 1] to represent an x-y pixel location of the projection of [P] 310 in a camera [k]. Matrix [C] can contain camera focal length data, skew factor data, pixel aspect ratio data, and principal points as elements. The matrix [C] can function to map a 3D point position in a scene to a 2D pixel coordinate in an image. In some implementations, the model can be written as [X_k] = [C] ∗ [R_k] ∗ ([P] - [O k]) or alternatively written as [X_k] = [C] ∗ [R_k] ∗ [P] +[T_k]. The model can be applied using a number of combinations of the constraints. The following examples can be selectively implemented by theimage processing system 107 and poseestimation module 116 to estimate camera pose associated with a camera [k]. - In a non-limiting example, the
pose estimation module 116 can use theequidistant orbit constraint 126 to estimate a camera pose. For example, thepose estimation module 116 can define that all cameras [k] share the same translation vector for the translation of a camera transform (i.e., [T_k] = [T_0] for all cameras [k]n. Although the camera translation vectors are the same in this example, the camera center position [O k] (which is equal to the negative inverse of [R_k] ∗ [T 0]) for each of [T_k] can still differ from each other. However, when using the equidistant orbit constraint, all of the camera [k] centers may be considered to be constrained on a spherical surface. This constraint is generally motivated from the user-based motion of capturing a panoramic set of images by sweeping her arm and in doing so the position of themobile device 304 in her hand may be assumed to be approximately rotating on a sphere. - In a non-limiting example, the
pose estimation module 116 can use the concentricoptical axis constraint 128 to estimate a camera pose. For example, when using concentric optical axis, only the z-axis component of each translation [T_k] may be nonzero. That is, [T_k] = [0; 0; t_k]. The concentricoptical axis constraint 128 may define that any or all of the cameras' optical axis pass through theworld origin 308. -
FIG. 4 is a flow chart diagramming one embodiment of a process 400 to estimate a camera pose for a mobile device. In various embodiments, the procedure 400 may be used or produced by the systems such as those ofFIGS. 1 and5 . In some implementations, the process 400 is carried out on a mobile device. In some implementations, the process 400 is carried out on a computing device other than the mobile device. -
Block 402 illustrates that, in one embodiment, thesystem 100 can obtain a sequence of images. The sequence of images may be captured with amobile device 102 and obtained from thedevice 102 or another computing device or storage device. The sequence of images can include a plurality of image frames of any or all portions of a scene. - At
block 404, thesystem 100 can detect a first set of feature points in a first image frame and track the first set of feature points in a plurality of subsequent image frames. For example, as the sequence of images is presented to a computing device (e.g., during playback or editing), thesystem 100 can detect feature points and track the feature points as they appear during playback and after additional frames and feature points are presented on the computing device. The tracking can include indexing object location data, pose data, and/or other image data in such a way thatsystem 100 can retrieve the data at a later time to perform stitching and rendering tasks. - While continuing to track the first set of feature points, the
system 100 can detect a second set of feature points in a second image frame. Thesystem 100 can also track the second set of feature points in the plurality of subsequent image frames, atblock 406. - At
block 408, thesystem 100 can select a first initial camera pose associated with the first image frame and a second initial camera pose associated with the second image frame. The first and second initial camera poses may correspond to a hypothesis of location and pose information for image frames captured withmobile device 102. For example, thesystem 100 can determine that the user is swinging her arm while holdingmobile device 102 to capture images and in doing so, she is looking at the screen of themobile device 102. If the user is looking at the screen of themobile device 102, thesystem 100 can determine specific angles of capture that can be used to calculated camera pose information. Thesystem 100 can use this assumption and/or other constraints to determine a capture path and distances from themobile device 102 to objects in the scene. - In some implementations, selecting a first initial camera pose and a second initial camera pose can include obtaining gyroscope data associated with each image frame in the plurality of image frames and estimating, using the gyroscope data associated with the first image frame, a pose corresponding to the first set of feature points and estimating, using the gyroscope data associated with the second image frame, a pose corresponding to the second set of feature points. In some implementations, the gyroscope information can include at least a raw estimation of a rotation associated with a mobile device that captured the plurality of image frames. In some implementations, the gyroscope information may be used to estimate pose and image orientation associated with each image frame.
- At
block 410, thesystem 100 can determine a plurality of projection locations corresponding to each feature point in the first set of feature points and the second set of feature points. The projection locations, in world coordinates, correspond to the center of projection of the camera onmobile device 102 during capture of particular feature points. For example, the projection locations may correspond to a center of projection associated withdevice 102 for a time point at which the second image frame was captured. Similarly, the projection locations may correspond to a center of projection associated withdevice 102 for a time point at which the second image frame was captured, for example The projection locations can be calculated based at least in part on the first initial camera pose and the second initial camera pose. Calculating the projection may include transforming the projection into a perspective view that can be applied to a 3D coordinate system associated with themobile device 102, for example. - In some implementation, determining a plurality of projection locations corresponding to each feature point in the first and second sets of feature points includes calculating the projections for each feature point by coupling position data and orientation data so that the pose is calculated using the coupled pair as a single representation instead of being calculated with an independent pose metric and an independent orientation metric. The method to determine the plurality of projection locations can also include selecting at least one constraint and applying the constraint to the coupled position data and orientation data for each feature point in the first and second sets of feature points. For example, the
system 100 can apply the equidistant orbit constraint or the concentric optical axis constraint separately. In some implementations, thesystem 100 can apply both the equidistant orbit constraint and the concentric optical axis constraint in a combined fashion. In some implementations, particular constraints are selected as a way to reduce search space for the algorithm described herein. The reduction can include reducing the search space from six degrees of freedom to four degrees of freedom. The method to determine the plurality of projection locations can further include estimating a camera pose for each feature point in the first and second sets of feature points. - The process 400 can also include providing a stitching module, the estimated camera pose associated with each feature point, the stitching module configured to stitch the plurality of image frames to generate a 3D image of a scene. In some implementations, the process 400 can include performing the above on all of the feature points in all of the image frames captured by
device 102. In other implementations, the process 400 can include performing the above steps on a portion of the feature points and/or a portion of the image frames. - At
block 412, thesystem 100 can compare the projection locations corresponding to each feature point in the first set of feature points and each feature point in the second set of feature points to the corresponding tracked first and second set of feature points. The comparison can be used to determine errors in camera pose in order to improve the image content for stitching tasks at a later time. For example, using this comparison, an error associated with particular feature points can be determined by assessing differences between the hypothetical or initial projection locations and the tracked feature points. Once the error is determined, thesystem 100 can sum all errors associated with corresponding feature points in a particular bundle to generate a block of image content that can be stitched. - At
block 414, thesystem 100 can provide an updated camera pose for the first and second image frames based at least in part on determining whether the projection locations meet a predefined threshold associated with the tracked first and second set of feature points. The predefined threshold may pertain to distortion level, parallax requirements, image metrics, etc. For example, thesystem 100 can be configured to analyze the tracked first and second set of feature points to ensure (using a predefined distortion level) that the associated projection locations do not exceed a particular distortion percentage for a particular scene or image. Similarly, thesystem 100 can be configured to ensure (using a predefined parallax threshold) that the projection locations do not exceed parallax discomfort levels for a human being. Such predefined thresholds can function to ensure the updated camera pose provides a scene or image with little or no distortion while providing proper parallax to a human being. -
FIG. 5 shows an example of ageneric computer device 500 and a genericmobile computer device 550, which may be used with the techniques described here.Computing device 500 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers.Computing device 550 is intended to represent various forms of mobile devices, such as personal digital assistants, cellular telephones, smart phones, and other similar computing devices. The components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document. -
Computing device 500 includes aprocessor 502,memory 504, astorage device 506, a high-speed interface 508 connecting tomemory 504 and high-speed expansion ports 510, and alow speed interface 512 connecting tolow speed bus 514 andstorage device 506. Each of thecomponents processor 502 can process instructions for execution within thecomputing device 500, including instructions stored in thememory 504 or on thestorage device 506 to display graphical information for a GUI on an external input/output device, such asdisplay 516 coupled tohigh speed interface 508. In other implementations, multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory. Also,multiple computing devices 500 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system). - The
memory 504 stores information within thecomputing device 500. In one implementation, thememory 504 is a volatile memory unit or units. In another implementation, thememory 504 is a non-volatile memory unit or units. Thememory 504 may also be another form of computer-readable medium, such as a magnetic or optical disk. - The
storage device 506 is capable of providing mass storage for thecomputing device 500. In one implementation, thestorage device 506 may be or contain a computer-readable medium, such as a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations. A computer program product can be tangibly embodied in an information carrier. The computer program product may also contain instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer- or machine-readable medium, such as thememory 504, thestorage device 506, or memory onprocessor 502. - The
high speed controller 508 manages bandwidth-intensive operations for thecomputing device 500, while thelow speed controller 512 manages lower bandwidth-intensive operations. Such allocation of functions is exemplary only. In one implementation, the high-speed controller 508 is coupled tomemory 504, display 516 (e.g., through a graphics processor or accelerator), and to high-speed expansion ports 510, which may accept various expansion cards (not shown). In the implementation, low-speed controller 512 is coupled tostorage device 506 and low-speed expansion port 514. The low-speed expansion port, which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet) may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter. - The
computing device 500 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as astandard server 520, or multiple times in a group of such servers. It may also be implemented as part of arack server system 524. In addition, it may be implemented in a personal computer such as alaptop computer 522. Alternatively, components fromcomputing device 500 may be combined with other components in a mobile device (not shown), such asdevice 550. Each of such devices may contain one or more ofcomputing device multiple computing devices -
Computing device 550 includes aprocessor 552,memory 564, an input/output device such as adisplay 554, acommunication interface 566, and atransceiver 568, among other components. Thedevice 550 may also be provided with a storage device, such as a microdrive or other device, to provide additional storage. Each of thecomponents - The
processor 552 can execute instructions within thecomputing device 550, including instructions stored in thememory 564. The processor may be implemented as a chipset of chips that include separate and multiple analog and digital processors. The processor may provide, for example, for coordination of the other components of thedevice 550, such as control of user interfaces, applications run bydevice 550, and wireless communication bydevice 550. -
Processor 552 may communicate with a user throughcontrol interface 558 anddisplay interface 556 coupled to adisplay 554. Thedisplay 554 may be, for example, a TFT LCD (Thin-Film-Transistor Liquid Crystal Display) or an OLED (Organic Light Emitting Diode) display, or other appropriate display technology. Thedisplay interface 556 may comprise appropriate circuitry for driving thedisplay 554 to present graphical and other information to a user. Thecontrol interface 558 may receive commands from a user and convert them for submission to theprocessor 552. In addition, anexternal interface 562 may be provide in communication withprocessor 552, so as to enable near area communication ofdevice 550 with other devices.External interface 562 may provide, for example, for wired communication in some implementations, or for wireless communication in other implementations, and multiple interfaces may also be used. - The
memory 564 stores information within thecomputing device 550. Thememory 564 can be implemented as one or more of a computer-readable medium or media, a volatile memory unit or units, or a non-volatile memory unit or units.Expansion memory 574 may also be provided and connected todevice 550 throughexpansion interface 572, which may include, for example, a SIMM (Single In Line Memory Module) card interface.Such expansion memory 574 may provide extra storage space fordevice 550, or may also store applications or other information fordevice 550. Specifically,expansion memory 574 may include instructions to carry out or supplement the processes described above, and may include secure information also. Thus, for example,expansion memory 574 may be provide as a security module fordevice 550, and may be programmed with instructions that permit secure use ofdevice 550. In addition, secure applications may be provided via the SIMM cards, along with additional information, such as placing identifying information on the SIMM card in a non-hackable manner. - The memory may include, for example, flash memory and/or NVRAM memory, as discussed below. In one implementation, a computer program product is tangibly embodied in an information carrier. The computer program product contains instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer- or machine-readable medium, such as the
memory 564,expansion memory 574, or memory onprocessor 552, that may be received, for example, overtransceiver 568 orexternal interface 562. -
Device 550 may communicate wirelessly throughcommunication interface 566, which may include digital signal processing circuitry where necessary.Communication interface 566 may provide for communications under various modes or protocols, such as GSM voice calls, SMS, EMS, or MMS messaging, CDMA, TDMA, PDC, WCDMA, CDMA2000, or GPRS, among others. Such communication may occur, for example, through radio-frequency transceiver 568. In addition, short-range communication may occur, such as using a Bluetooth, Wi-Fi, or other such transceiver (not shown). In addition, GPS (Global Positioning System)receiver module 570 may provide additional navigation- and location-related wireless data todevice 550, which may be used as appropriate by applications running ondevice 550. -
Device 550 may also communicate audibly usingaudio codec 560, which may receive spoken information from a user and convert it to usable digital information.Audio codec 560 may likewise generate audible sound for a user, such as through a speaker, e.g., in a handset ofdevice 550. Such sound may include sound from voice telephone calls, may include recorded sound (e.g., voice messages, music files, etc.) and may also include sound generated by applications operating ondevice 550. - The
computing device 550 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as acellular telephone 580. It may also be implemented as part of asmart phone 582, personal digital assistant, or other similar mobile device. - Various implementations of the systems and techniques described here can be realized in digital electronic circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof. These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- These computer programs (also known as programs, software, software applications or code) include machine instructions for a programmable processor, and can be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. As used herein, the terms "machine-readable medium" "computer-readable medium" refers to any computer program product, apparatus and/or device (e.g., magnetic discs, optical disks, memory, Programmable Logic Devices (PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term "machine-readable signal" refers to any signal used to provide machine instructions and/or data to a programmable processor.
- To provide for interaction with a user, the systems and techniques described here can be implemented on a computer having a display device (e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor) for displaying information to the user and a keyboard and a pointing device (e.g., a mouse or a trackball) by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback (e.g., visual feedback, auditory feedback, or tactile feedback); and input from the user can be received in any form, including acoustic, speech, or tactile input.
- The systems and techniques described here can be implemented in a computing system that includes a back end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front end component (e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the systems and techniques described here), or any combination of such back end, middleware, or front end components. The components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include a local area network ("LAN"), a wide area network ("WAN"), and the Internet.
- The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- The invention is defined by the appended claims.
Claims (11)
- A computer-implemented method (400) comprising:obtaining (402) a sequence of images including a plurality of image frames of a scene;detecting (404) a first set of feature points in a first image frame;tracking (404) the first set of feature points in a plurality of subsequent image frames and while continuing to track the first set of feature points:detecting (406) a second set of feature points in a second image frame, wherein new features are represented by the second set of feature points;tracking (406) the second set of feature points in the plurality of subsequent image frames;selecting (408) a first initial camera pose associated with the first image frame and a second initial camera pose associated with the second image frame;determining (410) a plurality of projection locations corresponding to each feature point in the first set of feature points and the second set of feature points, the projection locations based at least in part on the first initial camera pose and the second initial camera pose;comparing (412) the projection locations corresponding to each feature point in the first set of feature points and each feature point in the second set of feature points to the corresponding tracked first set of feature points and the tracked second set of feature points; andin response to determining that the projection locations meet a predefined threshold corresponding to distortion associated with the tracked first and second set of feature points, generating (414) an updated camera pose for the first image frame and the second image frame.
- The computer-implemented method of claim 1, further comprising generating a 3D image of the scene using the updated camera pose for the first image frame and the second image frame based at least in part on determining that the projection locations meet the predefined threshold.
- The computer-implemented method of claim 1, wherein selecting a first initial camera pose and a second initial camera pose includes obtaining gyroscope data associated with each image frame in the plurality of image frames and estimating, using the gyroscope data associated with the first image frame, a pose corresponding to the first set of feature points.
- The computer-implemented method of claim 3, further comprising estimating, using the gyroscope data associated with the second image frame, a pose corresponding to the second set of feature points.
- The computer-implemented method of claim 3, wherein the gyroscope data includes at least a raw estimation of a rotation associated with a mobile device that captured the plurality of image frames, and the gyroscope data is used to estimate pose and image orientation associated with each image frame.
- The computer-implemented method of claim 1, wherein determining a plurality of projection locations corresponding to each feature point in the first and second sets of feature points includes:for each feature point:coupling position data and orientation data;selecting at least one constraint and applying the constraint to the coupled position data and orientation data; andestimating a camera pose; andproviding, to a stitching module, the updated camera pose associated with each feature point, the stitching module configured to stitch the plurality of image frames to generate a 3D image of the scene based on the estimated camera pose.
- The computer-implemented method of claim 6, wherein the at least one constraint includes an equidistant orbit constraint or a concentric optical axis constraint.
- The computer-implemented method of claim 6, wherein the constraint is selected to reduce search space for a pose estimation algorithm from six degrees of freedom to four degrees of freedom.
- The computer-implemented method of claim 1, wherein the sequence of images are captured with a mobile device.
- The computer-implemented method of claim 1, wherein the plurality of projection locations correspond to a center of projection of a mobile device that captured the plurality of image frames.
- A system (500) comprising:at least one processor (502);memory (504) storing instructions that, when executed by the at least one processor (502), cause the system (500) to perform operations including the computer-implemented method (400) of any preceding claim.
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201562193321P | 2015-07-16 | 2015-07-16 | |
PCT/US2016/042612 WO2017011793A1 (en) | 2015-07-16 | 2016-07-15 | Camera pose estimation for mobile devices |
Publications (2)
Publication Number | Publication Date |
---|---|
EP3323109A1 EP3323109A1 (en) | 2018-05-23 |
EP3323109B1 true EP3323109B1 (en) | 2022-03-23 |
Family
ID=56799525
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
EP16756820.3A Active EP3323109B1 (en) | 2015-07-16 | 2016-07-15 | Camera pose estimation for mobile devices |
Country Status (4)
Country | Link |
---|---|
US (1) | US10129527B2 (en) |
EP (1) | EP3323109B1 (en) |
CN (1) | CN107646126B (en) |
WO (1) | WO2017011793A1 (en) |
Families Citing this family (52)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10365711B2 (en) | 2012-05-17 | 2019-07-30 | The University Of North Carolina At Chapel Hill | Methods, systems, and computer readable media for unified scene acquisition and pose tracking in a wearable display |
US8788968B1 (en) * | 2012-06-04 | 2014-07-22 | Google Inc. | Transitioning an interface to a neighboring image |
WO2016073557A1 (en) | 2014-11-04 | 2016-05-12 | The University Of North Carolina At Chapel Hill | Minimal-latency tracking and display for matching real and virtual worlds |
CN107646126B (en) * | 2015-07-16 | 2020-12-08 | 谷歌有限责任公司 | Camera pose estimation for mobile devices |
US10962780B2 (en) * | 2015-10-26 | 2021-03-30 | Microsoft Technology Licensing, Llc | Remote rendering for virtual images |
JP2017129567A (en) * | 2016-01-20 | 2017-07-27 | キヤノン株式会社 | Information processing apparatus, information processing method, and program |
US10546385B2 (en) * | 2016-02-25 | 2020-01-28 | Technion Research & Development Foundation Limited | System and method for image capture device pose estimation |
WO2017161192A1 (en) * | 2016-03-16 | 2017-09-21 | Nils Forsblom | Immersive virtual experience using a mobile communication device |
US11232583B2 (en) * | 2016-03-25 | 2022-01-25 | Samsung Electronics Co., Ltd. | Device for and method of determining a pose of a camera |
CN107993276B (en) * | 2016-10-25 | 2021-11-23 | 杭州海康威视数字技术股份有限公司 | Panoramic image generation method and device |
US10250801B2 (en) * | 2017-04-13 | 2019-04-02 | Institute For Information Industry | Camera system and image-providing method |
US10740972B2 (en) * | 2017-04-28 | 2020-08-11 | Harman International Industries, Incorporated | System and method for presentation and control of augmented vehicle surround views |
WO2019006189A1 (en) | 2017-06-29 | 2019-01-03 | Open Space Labs, Inc. | Automated spatial indexing of images based on floorplan features |
US10699438B2 (en) * | 2017-07-06 | 2020-06-30 | Siemens Healthcare Gmbh | Mobile device localization in complex, three-dimensional scenes |
CN110517319B (en) | 2017-07-07 | 2022-03-15 | 腾讯科技（深圳）有限公司 | Method for determining camera attitude information and related device |
WO2019019157A1 (en) * | 2017-07-28 | 2019-01-31 | Qualcomm Incorporated | Image sensor initialization in a robotic vehicle |
US11170531B2 (en) * | 2017-09-13 | 2021-11-09 | Xiaochun Nie | Systems and methods for calibrating imaging and spatial orientation sensors |
WO2019119410A1 (en) * | 2017-12-22 | 2019-06-27 | 深圳市大疆创新科技有限公司 | Panorama photographing method, photographing device, and machine readable storage medium |
US10742959B1 (en) | 2017-12-29 | 2020-08-11 | Perceive Corporation | Use of machine-trained network for misalignment-insensitive depth perception |
CN110998241A (en) * | 2018-01-23 | 2020-04-10 | 深圳市大疆创新科技有限公司 | System and method for calibrating an optical system of a movable object |
US20190243376A1 (en) * | 2018-02-05 | 2019-08-08 | Qualcomm Incorporated | Actively Complementing Exposure Settings for Autonomous Navigation |
CN110622213B (en) * | 2018-02-09 | 2022-11-15 | 百度时代网络技术（北京）有限公司 | System and method for depth localization and segmentation using 3D semantic maps |
CN113487676B (en) * | 2018-03-19 | 2023-06-20 | 百度在线网络技术（北京）有限公司 | Method and apparatus for determining relative attitude angle between cameras mounted to acquisition entity |
EP3718302B1 (en) * | 2018-04-02 | 2023-12-06 | Samsung Electronics Co., Ltd. | Method and system for handling 360 degree image content |
US10410372B1 (en) * | 2018-06-14 | 2019-09-10 | The University Of North Carolina At Chapel Hill | Methods, systems, and computer-readable media for utilizing radial distortion to estimate a pose configuration |
US10916031B2 (en) | 2018-07-06 | 2021-02-09 | Facebook Technologies, Llc | Systems and methods for offloading image-based tracking operations from a general processing unit to a hardware accelerator unit |
EP3591605A1 (en) * | 2018-07-06 | 2020-01-08 | Facebook Technologies, LLC | Systems and methods for offloading image-based tracking operations from a general processing unit to a hardware accelerator unit |
EP3629585A1 (en) * | 2018-09-25 | 2020-04-01 | Koninklijke Philips N.V. | Image synthesis |
CN109462746B (en) * | 2018-10-26 | 2020-11-06 | 北京双髻鲨科技有限公司 | Image stabilization method and device |
CN113424131A (en) | 2018-11-12 | 2021-09-21 | 开放空间实验室公司 | Automatic spatial indexing of images to video |
CN109660723B (en) * | 2018-12-18 | 2021-01-08 | 维沃移动通信有限公司 | Panoramic shooting method and device |
CN109712249B (en) * | 2018-12-31 | 2023-05-26 | 成都纵横大鹏无人机科技有限公司 | Geographic element augmented reality method and device |
US11080532B2 (en) * | 2019-01-16 | 2021-08-03 | Mediatek Inc. | Highlight processing method using human pose based triggering scheme and associated system |
CN111563840B (en) * | 2019-01-28 | 2023-09-05 | 北京魔门塔科技有限公司 | Training method and device of segmentation model, pose detection method and vehicle-mounted terminal |
CN109977847B (en) * | 2019-03-22 | 2021-07-16 | 北京市商汤科技开发有限公司 | Image generation method and device, electronic equipment and storage medium |
CN109992111B (en) * | 2019-03-25 | 2021-02-19 | 联想(北京)有限公司 | Augmented reality extension method and electronic device |
CN111784769B (en) * | 2019-04-04 | 2023-07-04 | 舜宇光学（浙江）研究院有限公司 | Space positioning method and space positioning device based on template, electronic equipment and computer readable storage medium |
US11024054B2 (en) * | 2019-05-16 | 2021-06-01 | Here Global B.V. | Method, apparatus, and system for estimating the quality of camera pose data using ground control points of known quality |
CN110717936B (en) * | 2019-10-15 | 2023-04-28 | 哈尔滨工业大学 | Image stitching method based on camera attitude estimation |
CN111091621A (en) * | 2019-12-11 | 2020-05-01 | 东南数字经济发展研究院 | Binocular vision synchronous positioning and composition method, device, equipment and storage medium |
EP4111368A4 (en) * | 2020-02-26 | 2023-08-16 | Magic Leap, Inc. | Cross reality system with buffering for localization accuracy |
CN111462179B (en) * | 2020-03-26 | 2023-06-27 | 北京百度网讯科技有限公司 | Three-dimensional object tracking method and device and electronic equipment |
CN111598927B (en) * | 2020-05-18 | 2023-08-01 | 京东方科技集团股份有限公司 | Positioning reconstruction method and device |
CN111569432B (en) * | 2020-05-19 | 2021-01-15 | 北京中科深智科技有限公司 | System and method for capturing 6DoF scene image from game |
US11436812B2 (en) | 2020-05-29 | 2022-09-06 | Open Space Labs, Inc. | Machine learning based object identification using scaled diagram and three-dimensional model |
CN111739071B (en) * | 2020-06-15 | 2023-09-05 | 武汉尺子科技有限公司 | Initial value-based rapid iterative registration method, medium, terminal and device |
US11750815B2 (en) | 2020-09-17 | 2023-09-05 | Lemon, Inc. | Versatile video coding track coding |
US11611752B2 (en) | 2020-10-07 | 2023-03-21 | Lemon Inc. | Adaptation parameter set storage in video coding |
CN112907652B (en) * | 2021-01-25 | 2024-02-02 | 脸萌有限公司 | Camera pose acquisition method, video processing method, display device, and storage medium |
US20220318948A1 (en) * | 2021-03-27 | 2022-10-06 | Mitsubishi Electric Research Laboratories, Inc. | System and Method of Image Stitching using Robust Camera Pose Estimation |
CN113515201B (en) * | 2021-07-27 | 2024-03-19 | 北京字节跳动网络技术有限公司 | Cursor position updating method and device and electronic equipment |
CN114841862B (en) * | 2022-06-07 | 2023-02-03 | 北京拙河科技有限公司 | Image splicing method and system based on hundred million pixel array type camera |
Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8761439B1 (en) * | 2011-08-24 | 2014-06-24 | Sri International | Method and apparatus for generating three-dimensional pose using monocular visual sensor and inertial measurement unit |
US20140285486A1 (en) * | 2013-03-20 | 2014-09-25 | Siemens Product Lifecycle Management Software Inc. | Image-based 3d panorama |
Family Cites Families (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5495576A (en) | 1993-01-11 | 1996-02-27 | Ritchey; Kurtis J. | Panoramic image based virtual reality/telepresence audio-visual system and method |
US6571024B1 (en) * | 1999-06-18 | 2003-05-27 | Sarnoff Corporation | Method and apparatus for multi-view three dimensional estimation |
US6559846B1 (en) * | 2000-07-07 | 2003-05-06 | Microsoft Corporation | System and process for viewing panoramic video |
US7224382B2 (en) | 2002-04-12 | 2007-05-29 | Image Masters, Inc. | Immersive imaging system |
KR101199492B1 (en) * | 2008-12-22 | 2012-11-09 | 한국전자통신연구원 | Apparatus and Method for Real Time Camera Tracking for Large Map |
JP2013147052A (en) * | 2012-01-17 | 2013-08-01 | Jtekt Corp | Wheel rolling bearing device |
US9449230B2 (en) * | 2014-11-26 | 2016-09-20 | Zepp Labs, Inc. | Fast object tracking framework for sports video recognition |
CN107646126B (en) * | 2015-07-16 | 2020-12-08 | 谷歌有限责任公司 | Camera pose estimation for mobile devices |
-
2016
- 2016-07-15 CN CN201680025894.7A patent/CN107646126B/en active Active
- 2016-07-15 EP EP16756820.3A patent/EP3323109B1/en active Active
- 2016-07-15 US US15/211,858 patent/US10129527B2/en active Active
- 2016-07-15 WO PCT/US2016/042612 patent/WO2017011793A1/en active Application Filing
Patent Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8761439B1 (en) * | 2011-08-24 | 2014-06-24 | Sri International | Method and apparatus for generating three-dimensional pose using monocular visual sensor and inertial measurement unit |
US20140285486A1 (en) * | 2013-03-20 | 2014-09-25 | Siemens Product Lifecycle Management Software Inc. | Image-based 3d panorama |
Non-Patent Citations (2)
Title |
---|
MOURAGNON E ET AL: "Real Time Localization and 3D Reconstruction", CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, 2006 IEEE COMPUTER SOCIETY , NEW YORK, NY, USA 17-22 JUNE 2006, IEEE, PISCATAWAY, NJ, USA, 1 January 2006 (2006-01-01), pages 363 - 370, XP031461925, ISBN: 978-0-7695-2597-6 * |
PHILIP F MCLAUCHLAN ET AL: "Image mosaicing using sequential bundle adjustment", IMAGE AND VISION COMPUTING, 1 January 2002 (2002-01-01), pages 751 - 759, XP055313406, Retrieved from the Internet <URL:http://www.sciencedirect.com/science/article/pii/S0262885602000641/pdfft?md5=d92252a42c1c409f4fba45e6954077fe&pid=1-s2.0-S0262885602000641-main.pdf> DOI: 10.1016/S0262-8856(02)00064-1 * |
Also Published As
Publication number | Publication date |
---|---|
US10129527B2 (en) | 2018-11-13 |
CN107646126A (en) | 2018-01-30 |
CN107646126B (en) | 2020-12-08 |
US20170018086A1 (en) | 2017-01-19 |
EP3323109A1 (en) | 2018-05-23 |
WO2017011793A1 (en) | 2017-01-19 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
EP3323109B1 (en) | Camera pose estimation for mobile devices | |
US10334165B2 (en) | Omnistereo capture for mobile devices | |
US10375381B2 (en) | Omnistereo capture and render of panoramic virtual reality content | |
US10453175B2 (en) | Separate time-warping for a scene and an object for display of virtual reality content | |
JP6427688B2 (en) | Omnidirectional stereo capture and rendering of panoramic virtual reality content | |
US10038887B2 (en) | Capture and render of panoramic virtual reality content | |
US8330793B2 (en) | Video conference | |
JP2018523326A (en) | Full spherical capture method | |
US11676292B2 (en) | Machine learning inference on gravity aligned imagery | |
JP2004326179A (en) | Image processing device, image processing method, image processing program, and recording medium storing it | |
US20210037230A1 (en) | Multiview interactive digital media representation inventory verification |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: THE INTERNATIONAL PUBLICATION HAS BEEN MADE |
|
PUAI | Public reference made under article 153(3) epc to a published international application that has entered the european phase |
Free format text: ORIGINAL CODE: 0009012 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: REQUEST FOR EXAMINATION WAS MADE |
|
17P | Request for examination filed |
Effective date: 20171018 |
|
AK | Designated contracting states |
Kind code of ref document: A1Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
AX | Request for extension of the european patent |
Extension state: BA ME |
|
DAV | Request for validation of the european patent (deleted) | ||
DAX | Request for extension of the european patent (deleted) | ||
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: EXAMINATION IS IN PROGRESS |
|
17Q | First examination report despatched |
Effective date: 20200603 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: EXAMINATION IS IN PROGRESS |
|
GRAP | Despatch of communication of intention to grant a patent |
Free format text: ORIGINAL CODE: EPIDOSNIGR1 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: GRANT OF PATENT IS INTENDED |
|
INTG | Intention to grant announced |
Effective date: 20211015 |
|
GRAS | Grant fee paid |
Free format text: ORIGINAL CODE: EPIDOSNIGR3 |
|
GRAA | (expected) grant |
Free format text: ORIGINAL CODE: 0009210 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: THE PATENT HAS BEEN GRANTED |
|
AK | Designated contracting states |
Kind code of ref document: B1Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
REG | Reference to a national code |
Ref country code: GBRef legal event code: FG4D |
|
REG | Reference to a national code |
Ref country code: CHRef legal event code: EP |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R096Ref document number: 602016070276Country of ref document: DE |
|
REG | Reference to a national code |
Ref country code: IERef legal event code: FG4D |
|
REG | Reference to a national code |
Ref country code: ATRef legal event code: REFRef document number: 1477978Country of ref document: ATKind code of ref document: TEffective date: 20220415 |
|
REG | Reference to a national code |
Ref country code: LTRef legal event code: MG9D |
|
REG | Reference to a national code |
Ref country code: NLRef legal event code: MPEffective date: 20220323 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: SEFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20220323Ref country code: RSFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20220323Ref country code: NOFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20220623Ref country code: LTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20220323Ref country code: HRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20220323Ref country code: BGFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20220623 |
|
REG | Reference to a national code |
Ref country code: ATRef legal event code: MK05Ref document number: 1477978Country of ref document: ATKind code of ref document: TEffective date: 20220323 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: LVFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20220323Ref country code: GRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20220624Ref country code: FIFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20220323 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: NLFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20220323 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: SMFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20220323Ref country code: SKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20220323Ref country code: ROFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20220323Ref country code: PTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20220725Ref country code: ESFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20220323Ref country code: EEFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20220323Ref country code: CZFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20220323Ref country code: ATFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20220323 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: PLFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20220323Ref country code: ISFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20220723Ref country code: ALFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20220323 |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R097Ref document number: 602016070276Country of ref document: DE |
|
PLBE | No opposition filed within time limit |
Free format text: ORIGINAL CODE: 0009261 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: NO OPPOSITION FILED WITHIN TIME LIMIT |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: DKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20220323 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: MCFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20220323 |
|
REG | Reference to a national code |
Ref country code: CHRef legal event code: PL |
|
26N | No opposition filed |
Effective date: 20230102 |
|
REG | Reference to a national code |
Ref country code: BERef legal event code: MMEffective date: 20220731 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: LUFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20220715Ref country code: LIFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20220731Ref country code: FRFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20220731Ref country code: CHFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20220731 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: SIFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20220323Ref country code: BEFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20220731 |
|
P01 | Opt-out of the competence of the unified patent court (upc) registered |
Effective date: 20230506 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ITFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20220323Ref country code: IEFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20220715 |
|
PGFP | Annual fee paid to national office [announced via postgrant information from national office to epo] |
Ref country code: GBPayment date: 20230727Year of fee payment: 8 |
|
PGFP | Annual fee paid to national office [announced via postgrant information from national office to epo] |
Ref country code: DEPayment date: 20230727Year of fee payment: 8 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: HUFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMIT; INVALID AB INITIOEffective date: 20160715 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: MKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20220323Ref country code: CYFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20220323 |