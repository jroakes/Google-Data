CN115066690A - Search normalization-activation layer architecture - Google Patents
Search normalization-activation layer architecture Download PDFInfo
- Publication number
- CN115066690A CN115066690A CN202180012815.XA CN202180012815A CN115066690A CN 115066690 A CN115066690 A CN 115066690A CN 202180012815 A CN202180012815 A CN 202180012815A CN 115066690 A CN115066690 A CN 115066690A
- Authority
- CN
- China
- Prior art keywords
- neural network
- architecture
- candidate
- architectures
- layer
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/086—Learning methods using evolutionary algorithms, e.g. genetic algorithms or genetic programming
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/048—Activation functions
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N5/00—Computing arrangements using knowledge-based models
- G06N5/01—Dynamic search techniques; Heuristics; Dynamic trees; Branch-and-bound
Abstract
Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for searching a framework for an activation-normalization layer to be included in a neural network to replace a set of layers that receive a layer input including a plurality of values, applying one or more normalization operations to values in the layer input to generate a normalization layer input, and applying an element-by-element activation function to the normalization layer input to generate a layer output.
Description
Cross Reference to Related Applications
Priority of U.S. provisional patent application No.62/971,887, filed on 7/2/2020, the entire contents of which are hereby incorporated by reference.
Technical Field
The present description relates to determining an architecture for a neural network.
Background
Neural networks are machine learning models that employ one or more layers of nonlinear units to predict output for received inputs. In addition to the output layer, some neural networks include one or more hidden layers. The output of each hidden layer is used as an input to the next layer in the network, i.e. the next hidden layer or output layer. Each layer of the network generates an output from the received input in accordance with the current values of the corresponding set of parameters.
Some neural networks are recurrent neural networks. A recurrent neural network is a neural network that receives an input sequence and generates an output sequence from the input sequence. In particular, the recurrent neural network can use some or all of the internal states of the network from a previous time step in calculating the output for the current time step. An example of a recurrent neural network is a Long Short Term Memory (LSTM) neural network that includes one or more LSTM memory blocks. Each LSTM memory block can include one or more cells, each cell including an input gate, a forgetting gate, and an output gate that allow the cell to store a previous state of the cell, e.g., for use in generating a current activation or to be provided to other components of the LSTM neural network.
Disclosure of Invention
This specification describes a system implemented as a computer program on one or more computers in one or more locations that determines an architecture for a normalized-activated neural network layer (also referred to as an "NA layer"). The determined architecture can then be used in the neural network instead of the existing layer set, first performing normalization and then applying an element-wise activation function.
Particular embodiments of the subject matter described in this specification can be implemented to realize one or more of the following advantages.
By determining the architecture of the normalization-activation layer as described in this specification, the system is able to identify an architecture that outperforms the architecture of conventional human design (e.g., batch norm + ReLu), particularly in image understanding or generation tasks. In other words, including the identified architecture in the neural network can result in the neural network being trained to quickly converge by improving the stability of the training process and result in the trained neural network having improved performance on target interrogation relative to conventional human-designed or searched architectures. Due at least in part to the evaluation of multiple different architectures during a search, the resulting normalized-active layer architecture is generalized to many different architectures. During a search, the system can exploit the "sparsity" of the search space using one or more rejection criteria in order to reduce the amount of computing resources consumed by the search.
The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Drawings
FIG. 1 illustrates an example neural architecture search system.
FIG. 2 illustrates an example computational graph.
FIG. 3 is a flow diagram of an example process for searching the architecture for the NA layer.
FIG. 4 is a flow diagram of an example process for performing an iteration of an evolutionary search process.
Like reference numbers and designations in the various drawings indicate like elements.
Detailed Description
This specification describes a system implemented as a computer program on one or more computers in one or more locations that determines an architecture for a normalized-activated neural network layer. The determined architecture can then be used in the neural network in place of an existing layer set that first performs normalization and then applies element-wise activation functions.
The neural network can be trained to perform any kind of machine learning task, i.e., can be configured to receive any kind of numerical data input and generate any kind of score, classification, or regression output based on the input.
In some cases, the neural network is a convolutional neural network configured to perform image processing tasks, i.e., receive an input image and process the input image to generate a network output for the input image. For example, the task may be an image classification and the output generated by the neural network for a given image may be a score for each class in a set of object classes, where each score represents an estimated likelihood that the image contains an image of an object belonging to that class. As another example, the task can be image embedding generation and the output generated by the neural network can be numerical embedding of the input image. As yet another example, the task can be object detection and the output generated by the neural network can identify a location in the input image at which a particular type of object is depicted. As yet another example, the task can be image segmentation and the output generated by the neural network can assign each pixel of the input image to a category from a set of categories.
In some cases, the neural network is a convolutional neural network configured to generate an image, i.e., unconditionally or conditional on some context (context) input. For example, the neural network can be a producer neural network paired with one or more discriminator neural networks in a producer countermeasure network (GAN) framework.
As another example, if the input to the neural network is an internet resource (e.g., a web page), a document or portion of a document or a feature extracted from an internet resource, document or portion of a document, the task can be to classify the resource or document, i.e., the output generated by the neural network for a given internet resource, document or portion of a document can be a score for each of a set of topics, where each score represents an estimated likelihood that the internet resource, document or portion of a document is about that topic.
As another example, if the input to the neural network is a feature of the impression context of a particular advertisement, the output generated by the neural network may be a score representing an estimated likelihood that the particular advertisement will be clicked.
As another example, if the input to the neural network is a feature of a personalized recommendation for the user, e.g., a feature characterizing the context of the recommendation, e.g., a feature characterizing previous actions taken by the user, the output generated by the neural network may be a score for each of a set of content items, where each score represents an estimated likelihood that the user will preferentially respond to the recommended content item.
As another example, if the input to the neural network is a sequence of text in one language, the output generated by the neural network may be a score for each of a set of text segments in another language, where each score represents an estimated likelihood that a text segment in the other language is an appropriate translation of the input text into the other language.
As another example, the task may be an audio processing task. For example, if the input to the neural network is a sequence representing a spoken utterance, the output generated by the neural network may be a score for each of a set of text segments, each score representing an estimated likelihood that the text segment is a correct transcription for the utterance. As another example, the task may be a keyword recognition task, where if the input to the neural network is a sequence representing a spoken utterance, the output generated by the neural network can indicate whether a particular word or phrase ("hotword") was spoken in the utterance. As another example, if the input to the neural network is a sequence representing a spoken utterance, the output generated by the neural network can identify the natural language in which the utterance was spoken.
As another example, a task can be a natural language processing or understanding task that operates on a sequence of text in some natural language, such as an entailment task, a paraphrase task, a text similarity task, an emotion task, a sentence completion task, a grammar task, and so forth.
As another example, the task can be a text-to-speech task where the input is text in natural language or a feature of text in natural language and the network output is a spectrogram or other data defining audio of the text being spoken in natural language.
As another example, the task can be a health prediction task where the input is electronic health record data for the patient and the output is a prediction related to the patient's future health, e.g., a predicted therapy that should be prescribed to the patient, a likelihood that the patient will have an adverse health event, or a predicted diagnosis for the patient.
As another example, the task can be an agent control task, where the input is an observation characterizing a state of the environment and the output defines an action to be performed by the agent in response to the observation. For example, the agent can be a real-world or simulated robot, a control system of an industrial installation, or a control system controlling a different kind of agent.
Fig. 1 illustrates an example neural architecture search system 100. The neural architecture search system 100 is an example of a system implemented as a computer program on one or more computers in one or more locations, in which the following systems, components, and techniques can be implemented.
The neural architecture search system 100 is a system that searches for an architecture of a normalization-activation (NA) layer to be included in one or more neural networks. That is, rather than searching the architecture for the entire neural network, the system 100 only searches the architecture for the NA layer, which is to be included at one or more fixed locations within an existing neural network having an existing architecture, i.e., an architecture that is designed manually or an architecture that has been generated by a different neural architecture search technique.
At a high level, the system 100 determines an architecture for the NA layer by generating a plurality of candidate architectures for the normalization-activation layer, evaluating how each of the generated candidate architectures behaves when included in a plurality of different neural network architectures, and then selecting an architecture based on the results of the evaluation.
In other words, unlike conventional techniques for architectural searching, rather than being a neural network search architecture capable of performing tasks, the system 100 instead is an NA-level search architecture that performs well when included as part of many different architectures that each perform the same task or each perform different tasks.
In more detail, the system 100 obtains architecture data 110 that specifies a plurality of different neural network architectures.
Each neural network architecture includes at least one NA layer. The NA layer is a component of the neural network and, in the neural network architecture, can be included, for example, after the convolutional layer, the cyclic layer, or the fully-connected layer. As a specific example, the NA layer can be included after the layer performing the linear transformation, e.g., convolution or matrix multiplication, and can receive as input the output of the layer performing the linear transformation.
In general, the NA layer is a component configured to receive a layer input comprising a plurality of values, apply one or more normalization operations to the values in the layer input to generate a normalization layer input, and apply an element-by-element activation function to the normalization layer input to generate a layer output. Examples of normalization operations include operations performed by batch normalization, group normalization, and layer normalization layers to normalize received inputs. Examples of element-wise activation functions include rectified linear units (relus), arctangent, and sigmoid functions.
For each neural network architecture, the system 100 also obtains respective training data 120 for training the neural network having that neural network architecture to perform the corresponding neural network task and validation data 130 for evaluating how well the neural network performed the corresponding neural network task.
The system 100 can receive the training data 120, the validation data 130, and the architecture data 110 in any of a variety of ways. For example, the system 100 can receive data as an upload from a remote user of the system over a data communication network, e.g., using an Application Programming Interface (API) provided by the system 100. As another example, the system 100 can receive input from a user specifying which data that has been maintained by the system 100 should be used as training data 120 and validation data 130 and architecture data 110.
The system 100 then performs an iterative search process using the received data to search the architecture for the NA layer.
In particular, at each iteration of the search process, selection engine 135 generates one or more candidate architectures 140 for the NA layer.
In general, selection engine 135 selects candidate architectures 140 for the NA layer from the architecture's specified search space. The search space of the architecture defines the possible combinations of operations that can be performed by any NA layer to generate layer outputs from a given layer input.
As a specific example, each possible candidate architecture in the search space can be represented as a computed graph of an output tensor that converts one input tensor into the same shape. The computational graph includes a set of initial nodes representing tensors and a set of intermediate nodes representing outputs of primitive operations from the set of primitive operations.
An example of such a computational graph is shown in fig. 2.
In the example of fig. 2, the computational graph is a Directed Acyclic Graph (DAG) having 4 initial nodes, including three auxiliary nodes and the input tensor of the NA layer: a constant zero tensor and two trainable vectors v0 and v1 along the channel dimension initialized to 0 and 1, respectively.
In general, a DAG can have any number of nodes, but in the example of fig. 2, the DAG limits the total number of nodes to fourteen: 4 initial nodes plus 10 intermediate nodes. Each intermediate node in the DAG represents a result of one of a set of primitive operations, e.g., a set including unary operations and binary operations.
A set of example primitive operations that can be used to define a search space is shown in table 1 below:
TABLE 1
In Table 1, the symbol x I Representing a subset of x elements along the dimension indicated by I. In particular, index set I can be taken { (b, w, h); (w, h, c); (w, h); (w, h, c/g) }, where b is the batch dimension, w is the width dimension, h is the height dimension, c is the channel dimension, and c/g indicates that aggregation is performed in a grouped manner across the channel dimension. In any aggregation operation, a small amount of noise may be inserted as necessary to obtain numerical stability. All operations in table 1 preserve the shape of the input tensor. Thus, μ I(x) Is made of x I The first moment of the set element in (1) replaces the mapping of each element in x. Likewise, s 2 I(x) Is to convert each element of x into x I A mapping of second moments among the elements in (1).
The engine 135 can generate the schema 140 by determining which nodes in the computational graph are connected by edges and which operations from the set of primitive operations, e.g., the operations in table 1, correspond to each node in the computational graph.
At each iteration, the engine 135 can generate the candidate architecture 140 in any of a variety of ways. As a particular example, system 100 can perform an evolution-based search.
In an evolution-based search, on the first iteration, the engine 135 can randomly generate a plurality of candidate architectures 140 from the search space. For example, the engine 135 can randomly generate the candidate architectures 140 by generating a stochastic computational graph in a sequential manner. Starting from the initial node in the graph, the engine 135 generates each new node by randomly sampling primitive operations and then randomly sampling its input nodes according to the operation's argument. This process is repeated multiple times and the last node is used as output.
At each subsequent iteration, engine 135 can select a subset of candidate architectures from the candidate architectures that have been generated at the earlier iteration, select a candidate architecture from the subset of candidate architectures based on the overall fitness of the candidate architectures, and generate a new candidate architecture 140 from the selected candidate architecture. The overall fitness of the candidate architectures and the selection of the candidate architecture 140 on subsequent iterations will be described in more detail below.
At each iteration, for each of the one or more candidate architectures 140 generated by the engine 135, the training engine 150 identifies two or more of a plurality of different neural network architectures specified by the architecture data 110. In some embodiments, training engine 150 identifies all of the multiple neural network architectures for all of the architectures 140. In some other embodiments, the training engine 150 identifies a random subset of the multiple neural network architectures for each candidate 140.
For each candidate 140 and for each identified neural network architecture, the training engine 150 trains the neural network on the respective training data 120 for the identified neural network architecture, which has the identified neural network architecture, but where the normalization-activation layer in the identified neural network architecture is replaced with a new normalization-activation layer having the candidate architecture 140.
In general, because the system 100 is searching the architecture for only the NA layer, the search space will be "sparse," i.e., only a very small subset of the generated candidates 140 will yield significantly better performance than random guesses when included in any given network architecture. To take advantage of this sparsity in order to minimize the computational resources consumed by the search process, training engine 150 can apply one or more rejection criteria for rejecting candidates 140. Rejection of a candidate 140 results in early stopping training of the rejected candidate 140 (thereby saving computational resources), and then removing the candidate 140 from consideration.
As a particular example, the training engine 150 can reject any candidate architecture that does not reach at least a threshold degree of fit when included in any of the two or more neural network architectures after a threshold number of training steps. For example, if for a given candidate 140, none of the two or more neural network architectures receives more than a threshold percentage, e.g., twenty percent or thirty percent, of validation accuracy after a threshold number of training steps, e.g., fifty training steps, one hundred training steps, or one hundred fifty training steps, the system can reject the candidate 140 and terminate training.
As another particular example, the training engine 150 can reject any candidate 140 that is subject to numerical instability during training and terminate training of the rejected candidate 140. To determine whether a candidate 140 is subject to numerical instability, the training engine 150 can perform multiple gradient ascent steps at one or more checkpoints during training to opposingly adjust model weights of the network architecture comprising the candidate toward maximizing the gradient norm of the network, i.e., maximizing the norm of the gradient of the network architecture over a collection of training inputs relative to the model parameters. If the gradient norm exceeds the threshold after performing the gradient ascent step, the training engine 150 can reject the candidate.
For each identified architecture, training engine 150 determines a fitness from a measure of the performance of the trained neural network on validation data 130 for the identified neural network architecture. For example, the fitness can be a validation accuracy of the trained neural network on the validation data 130 or any other suitable measure of the performance of the trained neural network, such as a validation loss, a cross-over ratio measure, and so forth.
The training engine 150 then determines an overall fitness 152 of the candidate architecture from the fitness of the two or more different identified neural network architectures.
After termination of the search, i.e., after the last iteration of the iterative search process, the system 100 selects a final architecture for the NA layer based on the overall fitness 152 of the candidate architectures 140 generated during the search.
For example, the system 100 can select the candidate architecture 140 for the NA layer with the best overall fitness.
As another example, the system 100 can select a subset of candidate architectures with the highest overall fitness for an architecture in the architectures 110 and select a final architecture by evaluating the subset of candidate architectures on a target neural network task that is more computationally expensive than the corresponding neural network task.
In some approaches, after the search process has terminated, the system 100 can then output final architecture data 160 specifying the final architecture of the NA layer. For example, the system 100 can output data specifying the final NA-tier architecture to a user who submitted the training data 102. For example, the final architecture data 160 can specify primitive operations performed by the NA layer, the inputs of each of those primitive operations, and which output is used as a layer output by the NA layer.
In some embodiments, after the final architecture has been selected, the system 100 trains a neural network that includes a-N layers with the determined architecture, e.g., determined from scratch or trimming parameter values generated as a result of training during the search, and then uses the trained neural network to process requests received by the user, e.g., through an API provided by the system.
FIG. 3 is a flow diagram of an example process 300 for searching the architecture for the NA layer. For convenience, the process 300 will be described as being performed by a system of one or more computers located in one or more locations. For example, a suitably programmed neural architecture search system, such as neural architecture search system 100 of fig. 1, can perform process 300.
The system receives architecture data specifying a plurality of different neural network architectures (step 302), wherein each neural network architecture includes at least one NA layer, and each NA layer is configured to: receiving a layer input comprising a plurality of values; applying one or more normalization operations to values in the layer input to generate a normalized layer input; and applying the element-by-element activation function to the normalization layer input to generate a layer output.
For each neural network architecture, the system receives respective training data for training a neural network having that neural network architecture to perform the corresponding neural network task and validation data for evaluating how well the neural network performed on the corresponding neural network task (step 304).
The system generates a plurality of candidate architectures for the normalization-activation layer (step 306). For example, the system can repeatedly generate candidate architectures by performing iterations of an evolutionary search process. The iteration of performing the evolutionary search process is described in more detail below with reference to FIG. 4.
For each of a plurality of candidate architectures and for each of two or more of a plurality of different neural network architectures, the system trains a neural network on respective training data for the neural network architecture that has the neural network architecture but in which the NA layer is replaced with a new NA layer having the candidate architecture (step 308) and determines a fitness for the neural network architecture from a measure of execution of the trained neural network on the validation data (step 310).
The system determines an overall fit of the candidate architecture from the fits of two or more different neural network architectures (step 312). In some embodiments, the overall fitness can be an average of the fitness of two or more different architectures. In some other embodiments, the overall fitness comprises all fitness of two or more different architectures, i.e., is a list of overall fitness.
The system selects a final architecture for the normalized-activation layer based on the overall fitness of the candidate architectures (step 314).
FIG. 4 is a flow diagram of an example process 400 for performing an iteration of an evolutionary search process. For convenience, process 400 will be described as being performed by a system of one or more computers located in one or more locations. For example, a suitably programmed neural architecture search system, such as neural architecture search system 100 of fig. 1, can perform process 400.
In general, the system can repeatedly perform the process 400 to determine the final architecture for the NA layer.
More specifically, for a first iteration of the evolutionary search process, the system can randomly generate a plurality of candidate architectures, and at each subsequent iteration, can perform process 400 to generate one or more candidate architectures for the iteration.
The system selects a subset of candidate architectures from the candidate architectures that have been generated (step 402). For example, the system can randomly select a fixed-size subset of candidate architectures that have already been generated. In some cases, the system selects only a fixed-size subset, i.e., the most recently generated candidate architecture, from a sliding window of the most recent portion of the population.
The system selects a candidate architecture from a subset of candidate architectures based on the overall fitness of the candidate architecture (step 404).
In particular, when the overall fitness is an average of the fitness of two or more frameworks, the system is able to select the candidate framework with the best overall fitness among the candidate frameworks in the subset.
When the overall fitness includes all fitness, the system is able to identify each candidate architecture from the subset that has an overall fitness that is not governed by any overall fitness from any other candidate architecture of the subset. The system can then randomly select one of the identified candidate architectures. When the degree of fit of the second architecture is at least as high as the degree of fit of the first architecture for each of the two or more architectures, the overall degree of fit of the first architecture is governed by the overall degree of fit of the second architecture.
The system generates a new candidate architecture from the selected candidate architecture (step 406). In particular, the system can generate new candidate architectures by mutating the selected candidate architectures. For example, the system can randomly select an intermediate node in the computational graph, then randomly select a new operation for the selected node from the set of primitive operations and randomly select a new predecessor node for the selected operation. The system can then modify the selected candidate architecture to replace the corresponding operation and predecessor nodes with randomly selected primitive operations and predecessor nodes.
As a result of performing the described search process using the techniques described above on a search space defined by the set of primitive operations described in table 1, the system is able to identify a high performance NA layer that can be included in any of a variety of network architectures.
As a particular example, the neural network can include a normalization-activation layer between the first neural network layer and the second neural network layer. The first neural network layer, e.g., the layer applying the linear transformation, generates a first layer output having a plurality of elements, the normalization-activation layer configured to: receiving a first layer output generated by a first neural network layer; and generating an activation output having a respective activation value for each element of the first layer output, wherein the activation value for each element of the layer input x satisfies any one of:
Where β is a learning parameter.
As can be seen from the above expressions, these NA layers all have batch dependencies, i.e., each performs at least one operation that aggregates values across batch dimensions. In some cases, the system can constrain the search to not include any such operations in the set of primitive operations. In these cases, the high performance NA layer architecture that can be used without batch dependencies can include those that perform any of the following set of operations. In this case, the NA layer generates an activation output having a corresponding activation value for each element of the first layer output, wherein the activation value for each element of the layer input x satisfies any one of:
The term "configured" is used herein in connection with system and computer program components. To the extent that a system of one or more computers is configured to perform a particular operation or action means that the system has installed thereon software, firmware, hardware, or a combination thereof that, in operation, causes the system to perform the operation or action. To the extent that one or more computer programs are configured to perform particular operations or actions means that the one or more programs include instructions that, when executed by data processing apparatus, cause the apparatus to perform the operations or actions.
Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly embodied computer software or firmware, in computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of instructions encoded on a tangible, non-transitory storage medium for execution by, or to control the operation of, data processing apparatus. The computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them. Alternatively or in addition, the program instructions can be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by data processing apparatus.
The term "data processing apparatus" refers to data processing hardware and encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. The apparatus can also be or further comprise special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). The apparatus can optionally include, in addition to hardware, code that creates a run-time environment for the computer program, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
A computer program, which may also be referred to or described as a program, software application, app, module, software module, script, or code, can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages; and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data, e.g., one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, e.g., files that store one or more modules, sub programs, or portions of code. A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a data communication network.
In this specification, the term "database" is used broadly to refer to any collection of data: the data need not be structured in any particular way, or at all, and it can be stored on a storage device in one or more locations. Thus, for example, an index database can include multiple sets of data, each of which can be organized and accessed differently.
Similarly, the term "engine" is used broadly in this specification to refer to a software-based system, subsystem, or process that is programmed to perform one or more particular functions. Typically, the engine will be implemented as one or more software modules or components installed on one or more computers in one or more locations. In some cases, one or more computers will be dedicated to a particular engine; in other cases, multiple engines can be installed and run on one or more of the same computers.
The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, and in combination with, special purpose logic circuitry, e.g., an FPGA or an ASIC.
A computer adapted to run a computer program can be based on a general purpose microprocessor or a special purpose microprocessor or both, or any other kind of central processing unit. Generally, a central processing unit will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a central processing unit for executing or executing instructions and one or more memory devices for storing instructions and data. The central processing unit and the memory can be supplemented by, or incorporated in, special purpose logic circuitry. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, a computer need not have such a device. Moreover, the computer can be embedded in another device, e.g., a mobile telephone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, e.g., a Universal Serial Bus (USB) flash drive, to name a few.
Computer-readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD ROM and DVD-ROM disks.
To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices can also be used to provide for interaction with the user; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input. In addition, the computer can interact with the user by sending documents to and receiving documents from the device used by the user; for example, by sending a web page to a web browser on the user's device in response to a request received from the web browser. In addition, computers can interact with users by sending text messages or other forms of messages to personal devices, such as smart phones that are running messaging applications, and in turn receiving response messages from the users.
The data processing apparatus for implementing the machine learning model can also comprise, for example, a dedicated hardware accelerator unit for processing common and computationally intensive parts of machine learning training or production, i.e. reasoning, workload.
The machine learning model can be implemented and deployed using a machine learning framework, such as a TensorFlow framework, a Microsoft Cognitive Toolkit framework, an Apache Singa framework, or an Apache MXNet framework.
Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front-end component, e.g., a client computer having a graphical user interface, a web browser, or an app through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a Local Area Network (LAN) and a Wide Area Network (WAN), e.g., the internet.
The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, the server transmits data, e.g., HTML pages, to the user device, e.g., for the purpose of displaying data to a user interacting with the device as a client and receiving user input from the device as a client. Data generated at the user device, e.g., the result of the user interaction, can be received at the server from the device.
While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any inventions or of what may be claimed, but rather as descriptions of features that may be specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Furthermore, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, while operations are depicted in the drawings and are recited in the claims, these should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous. Moreover, the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a software product or packaged into multiple software products.
Specific embodiments of the present subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous.
Claims (15)
1. A method, comprising:
receiving data specifying a plurality of different neural network architectures, each neural network architecture comprising at least one normalization-activation layer, each normalization-activation layer configured to:
receiving a layer input comprising a plurality of values;
applying one or more normalization operations to the values in the layer input to generate a normalized layer input; and is
Applying a element-by-element activation function to the normalization layer input to generate a layer output;
for each neural network architecture, receiving respective training data for training a neural network having the neural network architecture to perform a corresponding neural network task and validation data for evaluating how well the neural network performed the corresponding neural network task;
generating a plurality of candidate architectures for the normalization-activation layer;
for each of the plurality of candidate architectures:
for each of two or more of the plurality of different neural network architectures:
training a neural network having the neural network architecture but the normalization-activation layer replaced with a new normalization-activation layer having the candidate architecture on corresponding training data for the neural network architecture; and is
Determining a fitness from a measure of performance of the trained neural network on the validation data for the neural network architecture; and
determining an overall fitness of the candidate architectures from the fitness of the two or more different neural network architectures; and
selecting a final architecture for the normalized-activation layer based on the overall fitness of the candidate architectures.
2. The method of claim 1, wherein the activation-normalization layer application batch normalization in two or more of the plurality of architectures is followed by a ReLu activation function.
3. The method of any preceding claim, wherein the neural network task, the training data and the validation data are the same for all of the plurality of neural network architectures.
4. The method of any preceding claim, wherein generating a plurality of candidate architectures for the normalization-activation layer comprises repeatedly performing the following operations:
selecting a subset of candidate architectures from the candidate architectures that have been generated;
selecting a candidate architecture from a subset of the candidate architectures based on the overall fitness of the candidate architectures; and
a new candidate architecture is generated from the selected candidate architecture.
5. The method of claim 4, wherein the generating comprises: a plurality of initial candidate architectures are randomly generated.
6. The method of any of claims 4 or 5, wherein the overall fitness is an average of the fitness of the two or more architectures, and wherein selecting a candidate architecture comprises selecting the candidate architecture having the best overall fitness.
7. The method of any of claims 4 or 5, wherein the overall degree of fit comprises all of the degrees of fit of the two or more architectures, and wherein selecting candidate architectures comprises:
identifying, from the subset, one or more candidate architectures having an overall fitness that is not governed by any overall fitness of any other candidate architecture from the subset; and
one of the one or more identified candidate architectures is randomly selected.
8. The method of any preceding claim, wherein the candidate architecture is represented as a computational graph of one input tensor converted to an identically shaped output tensor.
9. The method of claim 8, wherein the computational graph includes a set of initial nodes representing tensors and a set of intermediate nodes representing outputs of primitive operations from the set of primitive operations.
10. The method of any of claims 4 to 7 and claim 9, wherein generating the new architecture comprises:
randomly selecting an intermediate node;
randomly selecting a new operation for the selected node from the set of primitive operations; and
a new predecessor node is randomly selected for the selected node.
11. The method of any preceding claim, wherein the training comprises:
rejecting any candidate architectures that do not meet a threshold fitness when included in any of the two or more neural network architectures after a threshold number of training steps.
12. The method of any preceding claim, further comprising:
any candidate architecture that is subject to numerical instability is rejected.
13. The method of any preceding claim, wherein selecting a final architecture for the normalized-activation layer based on the overall fitness of the candidate architectures comprises:
selecting a subset of candidate architectures having the highest overall fitness; and
selecting a final architecture by evaluating a subset of the candidate architectures on a target neural network task that is more computationally expensive than the corresponding neural network task.
14. A system comprising one or more computers and one or more storage devices storing instructions that, when executed by the one or more computers, cause the one or more computers to perform operations of the respective methods of any of claims 1-13.
15. One or more computer storage media storing instructions that, when executed by one or more computers, cause the one or more computers to perform operations of the respective methods of any of claims 1-13.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202062971887P | 2020-02-07 | 2020-02-07 | |
US62/971,887 | 2020-02-07 | ||
PCT/US2021/017122 WO2021159099A1 (en) | 2020-02-07 | 2021-02-08 | Searching for normalization-activation layer architectures |
Publications (1)
Publication Number | Publication Date |
---|---|
CN115066690A true CN115066690A (en) | 2022-09-16 |
Family
ID=74858779
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202180012815.XA Pending CN115066690A (en) | 2020-02-07 | 2021-02-08 | Search normalization-activation layer architecture |
Country Status (4)
Country | Link |
---|---|
US (1) | US20230121404A1 (en) |
EP (1) | EP4078458A1 (en) |
CN (1) | CN115066690A (en) |
WO (1) | WO2021159099A1 (en) |
Families Citing this family (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2021187305A1 (en) * | 2020-03-17 | 2021-09-23 | 日本電気株式会社 | Information processing system, information processing method, and recording medium |
Family Cites Families (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2019081705A1 (en) * | 2017-10-27 | 2019-05-02 | Deepmind Technologies Limited | Using hierarchical representations for neural network architecture searching |
-
2021
- 2021-02-08 WO PCT/US2021/017122 patent/WO2021159099A1/en unknown
- 2021-02-08 US US17/798,046 patent/US20230121404A1/en active Pending
- 2021-02-08 EP EP21710118.7A patent/EP4078458A1/en active Pending
- 2021-02-08 CN CN202180012815.XA patent/CN115066690A/en active Pending
Also Published As
Publication number | Publication date |
---|---|
US20230121404A1 (en) | 2023-04-20 |
EP4078458A1 (en) | 2022-10-26 |
WO2021159099A1 (en) | 2021-08-12 |
WO2021159099A9 (en) | 2021-11-11 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11568207B2 (en) | Learning observation representations by predicting the future in latent space | |
EP3711000B1 (en) | Regularized neural network architecture search | |
CN107066464B (en) | Semantic natural language vector space | |
US20200104687A1 (en) | Hybrid neural architecture search | |
US20220092416A1 (en) | Neural architecture search through a graph search space | |
US10970493B1 (en) | Systems and methods for slot relation extraction for machine learning task-oriented dialogue systems | |
US11922281B2 (en) | Training machine learning models using teacher annealing | |
CN109766557B (en) | Emotion analysis method and device, storage medium and terminal equipment | |
CN112100377B (en) | Text classification method, apparatus, computer device and storage medium | |
CN110825849A (en) | Text information emotion analysis method, device, medium and electronic equipment | |
CN114860930A (en) | Text classification method and device and storage medium | |
CN113785314A (en) | Semi-supervised training of machine learning models using label guessing | |
CN112560440B (en) | Syntax dependency method for aspect-level emotion analysis based on deep learning | |
CN115066690A (en) | Search normalization-activation layer architecture | |
WO2023158881A1 (en) | Computationally efficient distillation using generative neural networks | |
US20230063686A1 (en) | Fine-grained stochastic neural architecture search | |
US20220108174A1 (en) | Training neural networks using auxiliary task update decomposition | |
CN115485694A (en) | Machine learning algorithm search | |
CN115398446A (en) | Machine learning algorithm search using symbolic programming | |
CN116453702B (en) | Data processing method, device, system and medium for autism behavior feature set | |
US20220129760A1 (en) | Training neural networks with label differential privacy | |
US20240143696A1 (en) | Generating differentiable order statistics using sorting networks | |
US20240152809A1 (en) | Efficient machine learning model architecture selection | |
CN114416970A (en) | Text classification model with role and dialog text classification method and device | |
WO2024015591A1 (en) | Efficient decoding of output sequences using adaptive early exiting |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |