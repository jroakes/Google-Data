US20180061400A1 - Using textual input and user state information to generate reply content to present in response to the textual input - Google Patents
Using textual input and user state information to generate reply content to present in response to the textual input Download PDFInfo
- Publication number
- US20180061400A1 US20180061400A1 US15/252,049 US201615252049A US2018061400A1 US 20180061400 A1 US20180061400 A1 US 20180061400A1 US 201615252049 A US201615252049 A US 201615252049A US 2018061400 A1 US2018061400 A1 US 2018061400A1
- Authority
- US
- United States
- Prior art keywords
- user
- input
- state information
- user state
- textual
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/33—Querying
- G06F16/3331—Query processing
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/953—Querying, e.g. by the use of web search engines
- G06F16/9535—Search customisation based on user profiles and personalisation
-
- G06F17/30657—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06Q—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES; SYSTEMS OR METHODS SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES, NOT OTHERWISE PROVIDED FOR
- G06Q10/00—Administration; Management
- G06Q10/10—Office automation; Time management
- G06Q10/107—Computer-aided management of electronic mailing [e-mailing]
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/06—Creation of reference templates; Training of speech recognition systems, e.g. adaptation to the characteristics of the speaker's voice
- G10L15/063—Training
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/16—Speech classification or search using artificial neural networks
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/1815—Semantic context, e.g. disambiguation of the recognition hypotheses based on word meaning
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/26—Speech to text systems
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0481—Interaction techniques based on graphical user interfaces [GUI] based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance
- G06F3/0482—Interaction with lists of selectable items, e.g. menus
Definitions
- Automated assistants may be interacted with by a user via a variety of computing devices, such as smart phones, tablet computers, wearable devices, automobile systems, standalone automated assistant devices, and so forth.
- the automated assistants receive textual input from the user (e.g., typed and/or spoken) and respond with responsive textual output (e.g., visual and/or audible).
- This specification is directed to methods, apparatus, and computer readable media related to receiving textual input of a user during a dialog between the user and an automated assistant (and optionally one or more additional users), and generating responsive reply content based on the textual input and based on user state information.
- the reply content is provided for inclusion in the dialog.
- the reply content is provided as a reply, by the automated assistant, to the user's textual input and may optionally be automatically incorporated in the dialog between the user and the automated assistant.
- the reply content is suggested by the automated assistant for inclusion in the dialog and is only included in the dialog in response to further user interface input.
- the user state information used to generate the reply content is based at least in part on one or more indicators that are in addition to the textual input itself, such as indicators based on one or more sensors (e.g., camera, microphone, keyboard sensors, touchscreen sensors, heart rate sensor) of a computing device of at least one of the users participating in the dialog that includes the automated assistant.
- the user state information indicates the current sentiment of the user.
- the user state information additionally or alternatively indicates the current sentiment of another user that is also engaged in the dialog that includes the automated assistant and the user (e.g., a dialog that includes two or more users and the automated assistant).
- textual input of “hello” provided by a user to an automated assistant during a dialog that includes the user and the automated assistant.
- textual reply content of “hi, what can I do to help?” may be provided by the automated assistant as a reply to the user's textual input.
- textual reply content of “howdy, can I help?” may instead be provided.
- different reply contents may be provided based on modifying a “default” initial textual output from a text generation engine in view of the user state information.
- the initial textual output of “hi, can I help?” may be modified with additional content that is dependent on user state information as indicated in the preceding examples (e.g., replacing “hi” with “howdy” or adding “what” and “do” to “can I help”).
- textual input of “I'm bored” provided by a user to an automated assistant during a dialog that includes the user and the automated assistant.
- textual reply content of “want to hear some jokes?” may be provided by the automated assistant as a reply to the textual input.
- the user state information indicates the user is sad
- textual reply content of “anything you want to talk about?” may instead be provided.
- the different reply contents may be provided by selecting, based on the user state information, from multiple reply options identified for the textual input. Additional description of the above and additional techniques are provided below.
- Some of the above mentioned and other implementations of the specification may achieve various technical advantages. For example, some implementations of generating reply content based on user state information may enable a more concise dialog to be achieved, while still satisfying informational and/or other needs of the user(s) included in the dialog. This may reduce the use of various computational resources, such as resources of computing device(s), that are required for visually and/or audibly presenting the dialog to the user(s). As yet another example, some implementations that generate reply content that includes interactive user interface elements that are based on user state information may increase the likelihood that such interactive elements are beneficial to one or more user(s) included in the dialog—and lessen the chance that computational resources are expended in association with irrelevant interactive elements. Additional or alternative technical advantages may be achieved, such as one or more described elsewhere in this specification.
- a method performed by one or more processors includes receiving textual input.
- the textual input is based on user interface input generated by a user via one or more user interface input devices of a computing device of the user and the user interface input is generated by the user as part of a dialog that includes the user and an automated assistant implemented by one or more of the processors.
- the method further includes determining user state information for the user, generating reply content based on both the textual input and the user state information, and providing the reply content in response to the user interface input.
- the user state information identifies a state of the user, is in addition to the textual input, and is based on sensor data generated by the computing device or an additional computing device of the user.
- the reply content is provided for inclusion in the dialog in response to the textual input and the reply content is provided for presentation via one or more user interface output devices.
- a transcript of the dialog between the user and the automated assistant is displayed in a graphical user interface rendered by one of the user interface output devices of the computing device of the user.
- providing the reply content includes incorporating the reply content into the transcript for display along with previous content of the dialog.
- Incorporating the reply content into the transcript may include transmitting, via one or more network interfaces, a command to the computing device, where the command causes the computing device to incorporate the reply content into the transcript.
- generating the reply content based on both the textual input and the user state information includes: providing the textual input to a text generation engine; receiving an initial textual output from the text generation engine; and generating the reply content by modifying the initial textual output based on the user state information.
- modifying the initial textual output based on the user state information includes: applying input to a model stored in one or more computer readable media, where the input is based on one or more segments of the initial textual output; and generating, over the model and based on the input, output that indicates one or more terms for modifying the initial textual output.
- the model includes mappings of neutral textual segments to textual segments that are specific to the user state
- the input applied to the model is one or more of the segments of the initial textual output
- the output generated over the model includes at least one of the textual segments that are specific to the user state.
- the method further includes selecting the model based on conformance between the user state information and one or more user state identifiers assigned to the model.
- the method further includes: applying, to the model, additional input that is based on the user state information. Generating the output over the model may be further based on the additional input.
- the model may be a trained machine learning model and the method may further include generating the trained machine learning model based on a plurality of training examples.
- Each of the training examples may include: training example input based on a corresponding neutral text segment and corresponding user state information; and training example output based on a corresponding modified text segment that is a modification of the corresponding neutral text segment in view of the corresponding user state information.
- Generating the trained machine learning model may include training the trained machine learning model based on application of the training example input of the training examples and backpropagation based on the training example output of the training examples.
- the reply content includes at least one selectable graphical element that, when selected via further user interface input, causes the computing device of the user to present additional content to the user.
- generating the reply content based on both the textual input and the user state information includes: issuing a search of one or more databases based on both the textual input and the user state information; receiving one or more search results in response to issuing the search; and incorporating one or more of the search results into the reply content.
- the additional search parameter may be either: a search term used to identify responsive content on which the search results are based, or a ranking parameter used to rank the search results.
- selection of the selectable graphical element may cause the computing device of the user to establish a network connection with a computing device of an additional user.
- the additional content may indicate initiation of a new dialog with the additional user.
- determining the user state information includes: identifying a plurality of user state indicators based on the sensor data; applying the user state indicators as input to at least one user state model stored in one or more computer readable media; and generating, over the at least one user state model and based on the input, output that indicates the user state information.
- the at least one user state model includes a trained sentiment classifier.
- the sensor data includes user interface sensor data that is based on the user interface input and determining the user state information includes determining the user state information based on the user interface input sensor data.
- generating the reply content based on both the textual input and the user state information includes: determining a plurality of reply options based on the textual input; selecting one of the reply options based on conformity of the selected one of the reply options to the user state information; and generating the reply content based on the selected one of the reply options.
- generating the reply content based on both the textual input and the user state information includes: providing the textual input to a text generation engine; receiving at least a first textual output and a second textual output from the text generation engine; and selecting, based on the user state information, one of the first textual output and the second textual output to utilize as the reply content.
- the dialog includes an additional user and the method further includes determining additional user state information for the additional user. Generating the reply content may be further based on the additional user state information.
- generating the reply content based on the user state information includes determining a style feature of the reply content based on the user state information.
- the sensor data on which the user state information is based is from a plurality of sensors of the computing device or the additional computing device.
- a method performed by one or more processors includes receiving textual input.
- the textual input is based on user interface input generated via one or more user interface input devices of a computing device of a user or an additional user.
- the user interface input is generated as part of a dialog that includes the user, the additional user, and an automated assistant implemented by one or more of the processors.
- the method further includes: determining user state information for at least one of: the user and the additional user; generating reply content based on both the textual input and the user state information; and providing the reply content in response to the textual input.
- the reply content is provided for inclusion in the dialog and the reply content is provided for presentation via one or more user interface output devices.
- providing the reply content includes incorporating the reply content in the dialog for presentation to the user and to the additional user.
- determining the user state information includes determining the user state information for both the user and the additional user.
- the user interface input on which the textual input is based is from the computing device of the additional user
- the user state information is for at least the additional user
- providing the reply content includes providing the reply content for presentation to the user as a suggested reply, by the user, to the textual input.
- implementations include one or more processors of one or more computing devices, where the one or more processors are operable to execute instructions stored in associated memory, and where the instructions are configured to cause performance of any of the aforementioned methods. Some implementations also include one or more non-transitory computer readable storage media storing computer instructions executable by one or more processors to perform any of the aforementioned methods.
- FIG. 1 is a block diagram of an example environment in which implementations disclosed herein may be implemented.
- FIGS. 2A, 2B, and 2C each illustrate an example of using components of the example environment of FIG. 1 in generating reply content based on textual input and based on user state information.
- FIG. 3A illustrates an example client computing device with a display screen displaying an example of dialog that may occur between a user of the client computing device and an automated assistant according to user state information of the user.
- FIG. 3B illustrates an example of how the dialog of FIG. 3A may differ according to user state information of the user that differs from that of FIG. 3A .
- FIG. 4A illustrates the example client computing device of FIG. 3A , displaying another example of dialog that may occur between a user of the client computing device and the automated assistant according to user state information of the user.
- FIG. 4B illustrates an example of how the dialog of FIG. 4A may differ according to user state information of the user that differs from that of FIG. 4A .
- FIG. 5 illustrates another example client computing device, and an example of audible dialog that may occur between a user of the client computing device and an automated assistant according to implementations disclosed herein.
- FIGS. 6 and 7 illustrate the example client computing device of FIG. 3A , displaying yet further examples of dialog that may occur between a user of the client computing device and the automated assistant according to user state information of the user.
- FIGS. 8 and 9 illustrate the example client computing device of FIG. 3A , displaying examples of dialog that includes multiple users and the automated assistant.
- FIG. 10 is a flowchart illustrating an example method according to implementations disclosed herein.
- FIG. 11 illustrates an example architecture of a computing device.
- Implementations described herein relate to receiving textual input of a user during a dialog that involves the user, an automated assistant, and optionally one or more additional users—and generating responsive reply content based on the textual input.
- the textual input that is provided by the user may be textual input that initiates the dialog or it may be a continuation of a previously initiated dialog.
- the textual input may be natural language free-form input, such as textual input that is based on user interface input generated by the user via one or more user interface input devices (e.g., based on typed input provided via a physical or virtual keyboard or based on spoken input provided via a microphone).
- free-form input is input that is formulated by a user and that is not constrained to a group of options presented for selection by the user (e.g., not constrained to a group of options presented in a drop-down menu).
- reply content generated by an automated assistant in response to textual input of a user is generated based on the textual input and based on user state information.
- the user state information on which the reply content is generated may be user state information of the user and/or of other users engaged in the dialog that includes the automated assistant.
- the generated reply content is provided, by the automated assistant, for inclusion in the dialog.
- the reply content is automatically included in the dialog by the automated assistant as a reply, by the automated assistant, to the textual input.
- the reply content is suggested by the automated assistant for inclusion in the dialog and is only included in the dialog in response to further user interface input.
- the further user interface input is input of the user that provided the textual input and the inclusion of the reply content in the dialog is as a reply to the textual input by the automated assistant.
- the further user interface input is input of an additional user involved in the dialog and the inclusion of the reply content in the dialog is as a reply to the textual input by the other user involved in the dialog.
- reply content may be provided in response to textual input provided by a user.
- various techniques may be utilized to determine the user state information and/or to determine the reply content based on both the textual input and the user state information. Some of these techniques are described in more detail below with reference to the figures.
- Automated assistants may receive textual input from a user and respond with textual output that is tailored to the received textual input. However, many conventional automated assistants may fail to take into account any user state information that is not explicitly contained in the received textual input. Moreover, many conventional automated assistants may fail to provide responsive non-textual output (e.g., interactive user interface elements, changes to an application state) that is based on user state information.
- responsive non-textual output e.g., interactive user interface elements, changes to an application state
- Such failure to take into account user state information may lead to undue utilization of computational resources and/or other technical problems in various situations.
- failing to take into account user state information may cause excess dialog to occur between a user and the automated assistant to resolve informational or other needs of the user.
- Such excess dialog may consume various computational resources, such as resources required to render a presentation of the dialog, communicate between a computing device of the user and remotely located components of an automated assistant and/or of other user's computing devices, etc.
- failing to take into account user state information may cause the user to prematurely abandon the dialog and seek to resolve his or her needs using a more computationally expensive alternative technique.
- failing to provide responsive interactive user interface elements that are based on user state information may cause interactive user interface elements to be provided and selected by the user—only to have the user ignore the responsive output due to the elements not being tailored to the user state information. This may cause a client computing device to unnecessarily perform action(s) in response to the selection, such as opening a new application, rendering a new interface, retrieving content over a network, etc.
- Some implementations of the specification may address the above-mentioned and/or additional technical problems via one or more technical solutions. For example, some implementations of generating reply content based on user state information may enable a more concise dialog to be achieved, while still satisfying informational and/or other needs of the user(s) included in the dialog. This may reduce the use of various computational resources, such as resources of computing device(s), that are required for visually and/or audibly presenting the dialog to the user(s). As yet another example, some implementations that generate reply content that includes interactive user interface elements that are based on user state information may increase the likelihood that such interactive elements are beneficial to one or more user(s) included in the dialog—and lessen the chance that computational resources are expended in association with irrelevant interactive elements. Additional or alternative technical advantages may be achieved, such as one or more described elsewhere in this specification.
- the example environment includes one or more user interface input devices 102 , one or more user interface output devices 104 , one or more sensors 106 , an automated assistant 120 , a search system 130 , one or more search database(s) 154 , and one or more models 152 .
- the user interface input devices 102 may include, for example, a physical keyboard, a touch screen (e.g., implementing a virtual keyboard or other textual input mechanism), and/or a microphone.
- the user interface output devices 104 may include, for example, a display screen, and/or speaker(s).
- the sensor(s) 106 may include one or more sensors that are in addition to the user interface input devices 102 such as, for example, a camera, a heart rate sensor, a location sensor, a GPS sensor, an accelerometer, a gyroscope, a Wi-Fi chip, etc.
- the user interface input device(s) 102 , user interface output devices 104 , and sensor(s) 106 may be incorporated on one or more computing devices of a user.
- a mobile phone of the user may include the user interface input and output devices 102 , 104 and include one or more of the sensors 106 .
- a standalone personal assistant hardware device may include the user interface input and output devices 102 , 104 and a separate computing device (e.g., in a watch form, or a mobile phone form) may include one or more of the sensors 106 .
- a first computing device may include the user interface output device(s) 104
- a separate second computing device may include the user interface input device(s) 102 .
- sensor data from any other sensor(s) 106 of the first and second computing devices may optionally not be provided to the automated assistant 120 (e.g., sensor data from the user interface input device(s) 102 may be utilized to determine a user state).
- automated assistant 120 is illustrated in FIG. 1 as separate from the sensor(s) 106 and separate from the user interface input and output devices 102 , 104 , in some implementations all or aspects of the automated assistant 120 may be implemented on a computing device that also contains the user interface input device(s) 102 , the user interface output device(s) 104 , and/or the sensor(s) 106 .
- all or aspects of user state engine 122 and/or reply content engine 126 of automated assistant 120 may be implemented on the computing device.
- synthesized versions of user state information may be determined by the computing device and provided to a remote automated assistant 120 , which may enable prevention of certain more granular sensor signals from being transmitted by the computing device to the remote automated assistant 120 .
- all or aspects of the automated assistant 120 may be implemented on computing device(s) that are separate and remote from a computing device that contains the user interface input device(s) 102 , the user interface output device(s) 104 , and/or the sensor(s) 106 (e.g., all or aspects may be implemented “in the cloud”).
- those aspects of automated assistant 120 may communicate with the computing device via one or more networks such as a local area network (LAN) and/or wide area network (WAN) (e.g., the Internet).
- LAN local area network
- WAN wide area network
- client computing device(s) that may include the user interface input device(s) 102 , the user interface output device(s) 104 , and/or the sensor(s) 106 include one or more of: a desktop computing device, a laptop computing device, a standalone hardware device at least in part dedicated to automated assistance, a tablet computing device, a mobile phone computing device, a computing device of a vehicle of the user (e.g., an in-vehicle communications system, an in-vehicle entertainment system, an in-vehicle navigation system), or a wearable apparatus of the user that includes a computing device (e.g., a watch of the user having a computing device, glasses of the user having a computing device, a virtual or augmented reality computing device).
- a desktop computing device e.g., a laptop computing device, a standalone hardware device at least in part dedicated to automated assistance
- a tablet computing device e.g., a mobile phone computing device
- a computing device of a vehicle of the user e
- a given user may communicate with all or aspects of automated assistant 120 utilizing a plurality of client computing devices that collectively form a coordinated “ecosystem” of computing devices.
- client computing devices that collectively form a coordinated “ecosystem” of computing devices.
- some examples described in this disclosure will focus on the user operating a single client computing device.
- multiple users may communicate with all or aspects of automated assistant 120 , and each user may utilize one or more of their computing devices to communicate with the automated assistant 120 .
- the automated assistant 120 may be involved in a dialog between multiple users and may communicate with client computing device(s) of one or more of those users.
- a client computing device and automated assistant 120 may each (when they are separate devices) include one or more memories for storage of data and software applications, one or more processors for accessing data and executing applications, and other components that facilitate communication over a network.
- the operations performed by one or more of the client computing devices and/or by the automated assistant 120 may be distributed across multiple computing devices.
- Automated assistant 120 may be implemented as, for example, computer programs running on one or more computers in one or more locations that are coupled to each other through a network.
- a user provides input to the automated assistant 120 via the user interface input device(s) 102 .
- the automated assistant 120 provides responsive output for presentation to the user and/or to additional user(s) via the user interface output devices(s) 104 (optionally after further processing by one or more components).
- the input is illustrated in FIG. 1 as being provided directly to the automated assistant 120 by the user interface input device(s) 102 and the output is illustrated as being provided by the automated assistant 120 directly to the user interface output device(s) 104 .
- one or more intermediary hardware components may be functionally interposed between the automated assistant 120 and the user interface input and/or output devices 102 , 104 , and may optionally process the input and/or output.
- one or more components may process the output provided by automated assistant 120 and generate, based on the processing of the output, one or more signals for presentation of reply content via the user interface output device(s) 104 .
- a hardware processor and/or other components may process the output and generate, based on the output, signals for driving the user interface output device 104 .
- the input received by the automated assistant 120 includes content that is already in a textual format (and optionally additional content that is not in a textual format, such as images).
- the user interface input device(s) 102 may include a keyboard that generates textual input in response to user interface input directed to the keyboard, and the textual input provided to the automated assistant 120 .
- the user interface input device(s) 102 may include a microphone, a voice-to-text processor that is separate from the automated assistant 120 may convert voice input received at the microphone into textual input, and the textual input may be provided to the automated assistant 120 .
- the input initially received by the automated assistant 120 is not in a textual format, but at least some of the input is converted to a textual format by the automated assistant 120 and provided in the textual format to one or more other components of the automated assistant 120 .
- the user interface input device(s) 102 may include a microphone, voice input may be received at the microphone and provided to the automated assistant 120 , and a voice-to-text processor of the automated assistant 120 may convert the voice input into textual input.
- a client computing device may optionally operate one or more applications that enable dialog with the automated assistant 120 .
- Such applications may come in various forms such as a short messaging service (“SMS”) and/or multimedia messaging service (“MMS”) client, an online chat client (e.g., instant messenger, Internet relay chat, or “IRC,” etc.), a messaging application associated with a social network, an email client, a social blogging client, an automated assistant messaging service dedicated to conversations with automated assistant 120 , and so forth.
- SMS short messaging service
- MMS multimedia messaging service
- IRC Internet relay chat
- one or more of the applications may be implemented via a webpage or other resources rendered by a web browser or other application of a client computing device.
- the automated assistant 120 may generate responsive reply content based on the textual input and based on user state information. The automated assistant 120 may then provide, as output, the reply content for presentation (visual and/or audible) to one or more of the users involved in the dialog.
- the reply content is automatically included in the dialog by the automated assistant 120 as a reply, by the automated assistant 120 , to the textual input.
- the reply content is suggested, by the automated assistant 120 , for inclusion in the dialog and is only included in the dialog in response to further user interface input.
- the further user interface input is input of the user that provided the textual input and the inclusion of the reply content in the dialog is as a reply to the textual input by the automated assistant 120 .
- the further user interface input is input of an additional user involved in the dialog and the inclusion of the reply content in the dialog is as a reply to the textual input by the additional user involved in the dialog.
- the automated assistant 120 may include a user state engine 122 , a text generation engine 124 , and a reply content engine 126 .
- one or more of engines 122 , 124 , and/or 126 may be omitted, combined, and/or implemented in a component that is separate from automated assistant 120 .
- one or more of engines 122 , 124 , and/or 126 , or any operative portion thereof, may be implemented in a component that is executed by a client computing device that includes the user interface input and/or output devices 102 and 104 and that is separate from the automated assistant 120 .
- the user state engine 122 determines user state information for one or more users involved in a dialog with the automated assistant 120 .
- the user state engine 122 provides the user state information to text generation engine 124 and/or reply content engine 126 for use in generating reply content based on the user state information.
- the user state information that is provided and utilized to generate reply content may include one or more classes and/or magnitudes of user state.
- Various classes and/or magnitudes may be utilized. For example, classes that are indicative of happy, sad, neutral, active, tired, stressed, and/or other user state(s) may be utilized.
- the user state engine 122 may also determine a magnitude of one or more of the user states.
- the user state engine 122 may determine a weight of “0.8” for “happy”, a weight of “0.7” for “active”, and a weight of “0.0” for the other classes.
- more granular classes may additionally and/or alternatively be utilized such as slightly happy, medium happy, and very happy (e.g., in lieu of a general “happy” class).
- the user state engine 122 determines class(es) and/or magnitude(s) of a user state based on applying one or more user state indicators to one or more trained classifier model(s) or other trained machine learning model(s). For example, one or more images of a user captured by a camera may be passed through a face detector and further through a facial expression classifier whose output indicates whether the facial expression is happy, tired, sad, and/or other class. The user state engine 122 may determine the user state information based at least in part on the output of the facial expression classifier.
- the user state information that is provided by the user state engine 122 and utilized to generate reply content includes the “raw signals” from one or more sensor(s) and/or discretized versions of those “raw signals”. For example, one or more images of the user captured by a camera may be provided by the user state engine 122 and applied by the reply content engine 126 as direct input to a neural network model or other machine learning model(s) that are utilized to determine one or more aspects of reply content to provide to a user.
- a typing speed of a user determined from sensor data associated with one of the user interface input device(s) may be provided by the user state engine 122 and applied by the reply content engine 126 as direct input to a neural network model or other machine learning model(s) that are utilized to determined one or more aspects of reply content to provide to a user.
- the user state engine 122 determines user state information based on synthesizing multiple user state indicators. For example, the user state engine 122 may apply an image of a user captured by a camera to a first classifier trained to predict class(es) of user state based on images and may apply the heart rate sensor data to a second classifier trained to predict class(es) of user state based on heart rate sensor data. The user state engine 122 may consider the outputs of the two classifiers in determining the user state information. For example, the user state engine 122 may average the outputs to determine the user state information, use the greater magnitude output in determining the user state information, and/or otherwise consider and/or combine both outputs in determining the user state information.
- the user state engine 122 may apply both an image of a user captured by a camera and heart rate sensor data of the user as input to a neural network model trained to predict class(es) of user state based on both images and heart rate sensor data.
- the user state engine 122 may determine the user state information based on output generated over the neural network model based on the applied input.
- the user state engine 122 determines user state information that is indicative of user state of multiple users involved in a dialog.
- the user state engine 122 may determine user state information that is indicative of the user states of both users utilizing various techniques. For example, the user state engine 122 may determine one or more classes and/or magnitudes of user state for a first user, determine one or more classes and/or magnitudes of user state for a second user, and determine the user state information based on the classes and/or magnitudes for both users. For example, the user state engine 122 may average the classes and magnitudes of the two users.
- the user state for the first user indicates a first class of “happy” with a magnitude of “0.8” and a second class of “stressed” with a magnitude of “0.2”; and that the user state for the second user indicates the first class of “happy” with a magnitude of “0.6” and the second class of “stressed” with a magnitude of “0.1”.
- the user state engine 122 may determine an average of the classes that indicates the first class of “happy” with a magnitude of “0.7” and the second class of “stressed” with a magnitude of “0.1”. Additional or alternative techniques of determining user state information that is indicative of the user states of multiple users may be utilized.
- the user state engine 122 determines user state information for a given user based at least in part on one or more user state indicators that are in addition to textual input provided by the given user during a dialog that includes the automated assistant 120 . In some of those implementations, the user state engine 122 may determine the user state information based on sensor data from one or more of the user interface input device(s) 102 used by the given user.
- the user state information may be determined based on a typing speed of the typed input, applied pressure for one or more characters of typed input (e.g., as sensed by a touch screen implementing a virtual keyboard), a “delay time” for starting to provide the typed input (e.g., when the typed input is provided responsive to other content), and/or other sensed features of the typed input.
- user state information that indicates the given user is stressed may be determined based on typing speed of the given user being fast and the pressure applied by the given user during the typing being high.
- a typing speed of “fast” and/or a pressure of “high” may be relative to one or more baselines such as a baseline that is specific to the given user (e.g., based on prior typed input of the user) and/or a baseline that is applicable to multiple users.
- the user state information may be determined based on tone, inflection, intonation, accent, pitch, volume, breathing volume, breathing rate, background noise level, background noise type, and/or other features of the voice input.
- the user state engine 122 may determine user state information for the given user based on audio input from a microphone, even when the microphone isn't used by the given user to provide textual input.
- the user state engine 122 may determine the user state information based on the presence and/or absence of certain types of sound (e.g., laughter, crying, music) in the audio input, background noise level in the audio input, breathing noise level in the audio input, breathing rate in the audio input, aspects of detected speech in the audio input (e.g., intonation, accent), etc.
- certain types of sound e.g., laughter, crying, music
- background noise level in the audio input e.g., breathing noise level in the audio input
- breathing rate in the audio input e.g., aspects of detected speech in the audio input (e.g., intonation, accent), etc.
- the user state indicators that are in addition to textual input provided by the given user include sensor data from one or more of the sensor(s) 106 that are in addition to the user interface input and output devices 102 , 104 .
- the sensor(s) 106 may be part of the computing device utilized by the given user to provide textual input in the dialog and/or may be from another computing device.
- the user state engine 122 may determine user state information for the given user based on one or more images from a camera that is included in the sensor(s) 106 .
- the user state engine 122 may determine the user state information based on: user facial expression(s) in the image(s); user gesture(s) in the image(s); user speed of movement in the image(s); user gaze direction(s) in the image(s); environmental brightness in the image(s); background object(s) in the image(s); environmental color temperature in the image(s); and/or shape, wavelength, and/or other features of infrared radiation in the image(s) (e.g., when the camera is a thermographic camera).
- one or more images of the given user captured by a camera may be passed through a face detector to detect the face of the given user in the image, and the face of the given user in the image passed through a facial expression classifier whose output indicates whether the facial expression is happy, tired, sad, and/or other class of user state.
- the outputted class of user state may be utilized as the user state information, or used along with other user state indicators to determine the user state information.
- the user state engine 122 may determine user state information for the given user based on sensor data indicating a user's heart rate.
- the sensor data may be from a sensor 106 of a watch worn by the given user or other wearable computing device worn by the given user.
- the user state engine 122 may determine user state information based on location information determined based on a GPS sensor, a Wi-Fi chip (e.g., location information determined based on available network(s), IP address, and/or other sensor data from the Wi-Fi chip), and/or other sensor(s) 106 .
- the user state engine 122 may determine the user state information based on whether the user is at home, at work, or at another location.
- the user state engine 122 may determine user state information based on sensor data from an accelerometer and/or gyroscope. For instance, sensor data from an accelerometer and/or gyroscope may be utilized to determine a potential activity being engaged in by the given user, which may indicate the user state information.
- the user state indicators that are in addition to textual input provided by the given user during a dialog include non-textual content provided by the given user in the dialog such as emojis, images, and/or other non-textual content.
- non-textual content provided by the given user in the dialog
- certain emojis and/or images submitted by a user may be directly mapped to certain indications of user state information.
- the user state engine 122 may provide an image submitted by a user to an image analysis engine that recognizes object(s) in images, and one or more objects recognized in the image may be mapped to certain indicators of user state information.
- the user state engine 122 may also utilize one or more features of textual input provided by the given user in a dialog that includes the automated assistant 120 .
- Features of the textual input may include, for example, particular term(s) included in the textual input, syntactic structure(s) of the textual input, and/or length of the textual input (word and/or character length).
- the user state engine 122 may apply one or more terms and/or other features of textual input to a sentiment classifier trained to predict class(es) and/or magnitude(s) of sentiment in textual segments and utilize the output from the sentiment classifier in determining the user state information.
- the user state engine 122 may determine the user state information based on combining the output from the sentiment classifier with output(s) from other classifiers trained to predict user state information based on sensor data and/or other signals that are in addition to textual input itself.
- Text generation engine 124 is optional and may be utilized to generate one or more initial textual outputs that are each responsive to received textual input.
- initial textual output that is generated by text generation engine 124 is provided to reply content engine 126 and reply content engine 126 generates reply content based on modifying the initial textual input in view of the user state information.
- the reply content engine 126 may utilize one or more techniques, such as those described herein, to add term(s) to the initial textual output, remove term(s) from the initial textual output, and/or replace term(s) in the initial textual output, to thereby modify the initial textual output such that it is more tailored to the user state information than is the initial textual output.
- the text generation engine 124 may take various forms such as a machine learning model and/or human created heuristics.
- the text generation engine 124 may utilize a recurrent neural network model that is trained to predict, based on feature(s) of textual input, an initial textual output that is responsive to the textual input.
- the recurrent neural network model may be trained, for example, based on a corpus of past textual inputs and responsive replies that occur between humans, such as in past chat sessions between humans.
- the reply content engine 126 may generate and provide various types of reply content in response to textual input provided by a user. Moreover, various techniques may be utilized by the reply content engine 126 to determine the reply content based on both the textual input and user state information. For example, as referenced above, in some implementations the textual input is provided to the text generation engine 124 , an initial textual output is received from the text generation engine 124 , and the reply content engine 126 generates reply content based on modifying the initial textual input in view of the user state information.
- the reply content engine 126 utilizes the textual input to determine a plurality of candidate reply options (e.g., based on one or more of the model(s) 152 and/or based on multiple initial textual outputs provided by the text generation engine 124 ), uses the user state information to select one of those reply options, and generates the reply content based on the selected reply option. Also, for example, in some implementations, the reply content engine 126 causes the search system 130 to issue a search based on received textual input and user state information, receives search results that are based on the search, and generates the reply content based on one or more of the search results (e.g., one or more of the search results may be provided as all or part of the reply content).
- FIGS. 2A-2C additional description is provided of the components of FIG. 1 and of various examples of generating reply content based on textual input and based on user state information.
- FIG. 2A illustrates an example of the text generation engine 124 generating initial textual output 205 A and the reply content engine 126 generating reply content 209 A by modifying the initial textual output 205 A in view of user state information 207 A.
- textual input 201 A that is based on user interface input generated by one or more user interface input device(s) 102 is provided to the text generation engine 124 as part of a dialog.
- the text generation engine 124 generates initial textual output 205 A and provides the initial textual output 205 A to reply content engine 126 .
- the user state engine 122 receives sensor data 203 A and/or the textual input 201 A and utilizes one or more user state models 152 A (included in the models 152 of FIG. 1 ) to generate user state information 207 A that indicates user state of the user that submitted the textual input 201 A and/or of one or more additional users that are also involved in the dialog.
- the sensor data 203 A may be based on sensor(s) of user interface input device(s) 102 and/or of sensor(s) 106 that are in addition to the user interface input device(s) 102 .
- the user state model(s) 152 A may include a model that is trained to predict one or more classes of sentiment (and optionally magnitude(s)) based on image(s) of a user's face.
- the user state engine 122 may apply, as input to the model, one or more images, captured by a camera of sensor(s) 106 , that each capture a user's face.
- the user state engine 122 may generate, over the model, output that indicates one or more classes (and optionally magnitudes) indicated by the user's face in the image(s). For instance, the user state engine 122 may generate an output that indicates a user is “happy”.
- the user state model(s) 152 A may further include a model that is trained to predict the presence of one or more types of sound in audio input.
- the user state engine 122 may apply, as input to the model, audio input captured by a microphone of the user interface input device(s) 102 (regardless of whether the microphone was utilized to generate the textual input 201 A).
- the user state engine 122 may generate, over the model, output that indicates one or more types of sound that are present in the audio input. For instance, the user state engine 122 may generate an output that indicates laughter is present in the audio input.
- the user state model(s) 152 A may further include a model that is trained to predict one or more classes of sentiment (and optionally magnitude(s)) based on feature(s) of textual input.
- the user state engine 122 may apply, as input to the model, feature(s) of the textual input 201 A and may generate, over the model, output that indicates one or more classes (and optionally magnitudes) indicated by the textual input 201 A. For instance, the user state engine 122 may generate an output that indicates the textual input 201 A is “happy” textual input.
- the user state engine 122 may optionally synthesize the three outputs from the three models and provide an indication of the synthesized output as user state information 207 A. For example, the user state engine 122 may synthesize “happy”, “laughter”, and “happy” outputs to user state information that is indicative of a “happy” or “very happy” user state. In some implementations, the user state engine 122 may optionally provide an indication of the three separate outputs as the user state information 207 A without synthesizing those outputs.
- the user state engine 122 may optionally provide one or more raw or discretized signals as all or part of the user state information 207 A in lieu of one or more of the three separate outputs (e.g., the user state information 207 A may simply pass the signals without application of the signals to the user state model(s) 152 A).
- the user state model(s) 152 A may include a neural network model of other machine learning model that is trained to predict one or more classes of user state (and optionally magnitude(s)) based on multiple heterogeneous inputs.
- the neural network model may be trained to predict one or more classes of user state based on input that includes: image(s) that each capture a user's face, audio input, and textual input.
- the user state engine 122 may apply, as input to the neural network model, image(s) from camera(s) of sensor(s) 106 , audio input(s) from user interface input device(s) 102 , and one or more features of the textual input 201 A (and optionally prior textual input); and may generate, over the model, output that indicates a class of user state indicated by the inputs.
- the reply content engine 126 uses the initial textual output 205 A and the user state information 207 A to generate reply content 209 A.
- the reply content engine 126 provides the reply content 209 A for presentation, to one or more users engaged in the dialog, via the user interface input device(s) 104 of those user(s).
- the reply content 209 A is automatically included in the dialog as a reply to the textual input 201 A by the automated assistant.
- the reply content 209 A is suggested by the reply content engine 126 for inclusion in the dialog and is only included in the dialog in response to further user interface input.
- the further user interface input is input of the user that provided the textual input 201 A and the inclusion of the reply content in the dialog is as a reply to the textual input 201 A by the automated assistant.
- the further user interface input is input of an additional user involved in the dialog and the inclusion of the reply content in the dialog is as a reply to the textual input by the other user involved in the dialog.
- the reply content engine 126 generates the reply content 209 A based on one or more text adaptation models 152 B (included in the models 152 of FIG. 1 ). In some of those implementations, the reply content engine 126 applies one or more (e.g. all) segments of the initial textual output 205 A as input to one of the text adaptation model(s) 152 B, and generates, over the model based on the input, output that indicates content for modifying the initial textual output.
- the reply content 209 A may include at least some of the content indicated by the output generated over the model and optionally one or more terms of the initial textual output 205 A.
- the term(s) indicated by the output may include term(s) for adding to the initial textual output 205 A and/or term(s) for replacing term(s) of the initial textual output 205 A—and the reply content engine 126 may modify the initial textual output 205 A by adding and/or replacing terms as indicated by the output.
- terms indicated by the output may be the reply content to be provided (i.e., the output may indicate the initial textual output 205 A as modified by the user state information 207 A).
- the model may include mappings (data defining an association) of neutral textual segments to segments that are each specific to a user state.
- a given model may be for a specific user state indicated (explicitly or implicitly) by the user state information 207 A.
- a “jovial” model may map neutral textual segments to segments that are each more appropriate for a jovial user state (e.g., neutral segment “hi” to more jovial segments “howdy, ;-)” and/or “helllooooo”).
- the given model for the specific user state indicated by the user state information 207 A may be selected based on the given model being assigned a model identifier that indicates the specific user state.
- a given model may be inclusive of multiple user states, instead of being for a specific user state. For example, a model may map a neutral segment to segment(s) that are each more appropriate for a jovial user state, to segment(s) that are each more appropriate for a stressed user state, and to segment(s) that are each more appropriate for a sad user state.
- the user state specific segments may each be mapped to corresponding user state identifiers and an appropriate user state specific segment selected by the reply content engine 126 based on its user state identifier corresponding to a specific user state indicated by the user state information 207 A.
- a segment that is specific to a user state and that is mapped to a corresponding neutral segment may include various content that differs from the neutral segment.
- the segment that is specific to the user state may include one or more terms that are in addition to terms of the neutral segment, term(s) that replace term(s) of the neutral segment, may omit term(s) that are included in the neutral segment, and/or may include emojis, GIFs, and/or other non-textual content that is not included in the neutral segment.
- a segment that is specific to a user state and that is mapped to a corresponding neutral segment may additionally and/or alternatively include various style features that differ from those of the neutral segment.
- Style features include, for example, font size, font type (e.g., Calibri, Times New Roman), and/or font color. Where reply content 209 A is additionally or alternatively to be provided as audible output, style features may additionally and/or alternatively include volume for providing the reply content 209 A, a tone for providing the reply content 209 A, a pitch for providing the reply content 209 A, and/or a voice (e.g., male/female, female 1/female 2/female 3) for providing the reply content 209 A.
- a voice e.g., male/female, female 1/female 2/female
- a model that maps neutral segments to user state specific segments may be generated by analyzing a corpus of multi-user chat dialogs and/or other communications. For example, to generate mappings between neutral segments and “happy” segments, reply segments of the communications may be analyzed (e.g., using a sentiment classifier) to identify the reply segments that are “happy”. Those reply segments could then be processed to remove particular keywords, symbols, tense of the sentence, and/or other features such that there is no sentiment expressed anymore (i.e., to obtain a neutral segment). The neutral segments may then be mapped to their corresponding original/happy reply segments.
- reply segments of the communications may be analyzed (e.g., using a sentiment classifier) to identify the reply segments that are “happy”. Those reply segments could then be processed to remove particular keywords, symbols, tense of the sentence, and/or other features such that there is no sentiment expressed anymore (i.e., to obtain a neutral segment).
- the neutral segments may then be mapped to their corresponding original/happy reply
- the model may include one or more mappings of additional non-textual content that correspond to various user states, and the reply content engine 126 may generate the reply content 209 A by adding the additional non-textual content to the initial textual output 205 A.
- the text adaptation model(s) 152 B may include one or more emojis, GIFs, and/or other images that are each assigned to one or more user state identifiers that corresponds to one or more user states.
- the reply content engine 126 may select one or more of the emojis, GIFs, and/or other images to add to the initial textual output 205 A based on the selected content being assigned to a user state identifier that corresponds to the user state information 207 A.
- the initial textual output 205 A with the added non-textual content may then be provided as the reply content 209 A.
- the model may be a neural network model that can be utilized to transform a textual segment into a user state specific version of the textual segment.
- the neural network model may be trained for a specific user state and may be selected based on that user state corresponding to the user state information 207 A.
- the neural network model may be trained for multiple user states and the user state information 207 A may be applied as input over the model (e.g., as side input along with the initial textual input).
- the initial textual output 205 A and the user state information 207 A may both be applied as input to the model (optionally at different layers of the model) and a modified textual segment generated as output over the model, where the modified textual segment is specific to the user state information 207 A.
- the user state information 207 A that is applied to the model may be in various forms such as a “synthesized” form or as parameterized sensor data.
- a neural network model that can be utilized to transform a textual segment into a user state specific version of the textual segment may be trained based on training examples that each include: training example input based on a corresponding neutral text segment and corresponding user state information, and training example output based on a corresponding modified text segment that is a modification of the corresponding neutral text segment in view of the corresponding user state information.
- the neural network model may be trained by applying the training example inputs to the neural network model and performing backpropagation on the neural network model based on the training example outputs.
- FIG. 2B illustrates an example of the reply content engine 126 generating reply content 209 B by utilizing user state information 207 B to select from multiple reply options.
- the multiple reply options may be identified based on one or more reply option models 152 C (included in the models 152 of FIG. 1 ) and/or based on output from text generation engine 124 .
- textual input 201 B that is based on user interface input generated by one or more user interface input device(s) 102 is provided to reply content engine 126 as part of a dialog.
- the user state engine 122 receives sensor data 203 B and/or the textual input 201 B.
- the user state engine 122 utilizes one or more user state models 152 A to generate user state information 207 B that indicates user state of the user that submitted the textual input 201 B and/or of one or more additional users that are also involved in the dialog.
- the sensor data 203 B may be based on sensor(s) of user interface input device(s) and/or of sensor(s) 106 that are in addition to the user interface input device(s) 102 .
- the user state engine 122 may utilize various techniques to generate the user state information 207 B, such as one or more techniques described with respect to FIG. 1 , FIG. 2A , and/or elsewhere herein.
- the user state information 207 B may take various forms such as those described herein.
- the reply content engine 126 utilizes the textual input 201 B to determine a plurality of candidate reply options, utilizes the user state information 207 B to select one or more of those options, and generates the reply content 209 B based on the selected reply option(s).
- the reply content engine 126 provides the textual input 201 B to text generation engine 124 , text generation engine 124 provides multiple explicit reply options, reply content engine 126 selects one of those reply options based on the user state information 207 B, and uses the selected option to generate the reply content 209 B (e.g., the reply content 209 B may conform strictly to the selected option).
- the text generation engine 124 may identify multiple explicit reply option outputs for further dialog such as “how can I help you?” and “hope you are having a good day, how can I help?”.
- the reply content engine 126 may select one of those options based on the selected option conforming to the user state information 207 B more closely than the non-selected option(s).
- the reply content engine 126 may pass each of the multiple reply options through a sentiment classifier (e.g., of user state model(s) 152 A) and compare outputs from the classifier to the user state information to select one of the reply options. For example, assume user state information 207 B is indicative of a “happy” user state, that output from the sentiment classifier for a first reply option indicates a “happy” user state, and that output from the sentiment classifier for a second reply option indicates a “neutral” user state. In such a situation, the reply content engine 126 may select the first reply option based on it and the user state information 207 B both indicating a “happy” user state.
- a sentiment classifier e.g., of user state model(s) 152 A
- a selected reply option may conform to the user state information 207 B even when the selected reply option and the user state information are not associated with the same user state. For example, assume user state information 207 B is indicative of a “sad” user state, that output from the sentiment classifier for a first reply option indicates a “sad” user state, and that output from the sentiment classifier for a second reply option indicates an “empathetic” user state. In some of those situations, the reply content engine 126 may select the second reply option based on an “empathetic” reply being more appropriate for a “sad” user state than is a “sad” reply.
- the reply content engine 126 may optionally rely on one or more mappings between user states indicated by user state information 207 B and user states indicated by reply options in selecting a reply option. For instance, for a “sad” user state the mappings may define that “empathetic” reply options are most appropriate, followed by “neutral” reply options, etc.
- the reply content engine 126 additionally or alternatively identifies multiple reply option categories from reply option model(s) 152 C based on the textual input 201 B. The reply content engine 126 may then select one of those reply option categories based on the user state information 207 B. For example, the textual input 201 B may be mapped to multiple reply option categories in reply option model(s) 152 C, such as reply option categories of “tell a joke”, “ask a question”, “divert the user to a human”, “divert the user to another chat bot”, etc. In some implementations, the reply content engine 126 may utilize a rules-based approach to select from the multiple reply option categories based on user state information 207 B.
- a “divert the user to a human” reply option category may be assigned to a user state identifier that causes it to be selected when the user state information 207 B indicates a certain type and/or magnitude of user state (e.g., “very upset”).
- the rules-based approach may cause the “tell a joke” reply option category to be selected for certain user states and the “ask a question” category to be selected for certain other user states.
- the reply option model(s) 152 C may additionally or alternatively include machine learning model(s) trained to select an appropriate reply option based on user state information.
- a machine learning model may be trained to select a reply option category based on training examples generated based on past dialogs with the automated assistant 120 and/or other automated assistants.
- Each of the training examples may include, as training example input, user state information for a corresponding textual input and, as training example output, the corresponding reply option category for the corresponding textual input.
- Each of the training examples may be labeled as a positive or negative training examples (and/or weighted) based on various signals, such as length of further textual input (e.g., if the further textual input is longer than normal it may indicate a positive/desired reply option), whether further textual input was received (e.g., if the conversation was abandoned it may indicate a negative/undesired reply option), explicit user feedback, etc.
- FIG. 2C illustrates an example of the reply content engine 126 causing a search to be issued based on textual input 201 C and based on user state information 207 C.
- the reply content engine 126 receives search result(s) based on the search, and generates reply content 209 C that is based on one or more of the received search results.
- textual input 201 C that is based on user interface input generated by one or more user interface input device(s) 102 is provided to reply content engine 126 as part of a dialog.
- the user state engine 122 receives sensor data 203 C and/or the textual input 201 C and utilizes one or more user state models 152 A to generate user state information 207 C that indicates user state of the user that submitted the textual input 201 C and/or of one or more additional users that are also involved in the dialog.
- the sensor data 203 C may be based on sensor(s) of user interface input device(s) and/or of sensor(s) 106 that are in addition to the user interface input device(s) 102 .
- the user state engine 122 may utilize various techniques to generate the user state information 207 C, such as one or more techniques described with respect to FIG. 1 , FIG. 2A , and/or elsewhere herein.
- the user state information 207 C may take various forms such as those described herein.
- the reply content engine 126 causes a search to be issued based on textual input 201 C and user state information 207 C.
- the reply content engine 126 causes the search system 130 to issue a search of search databases 154 based on one or more search parameters determined based on the textual input 201 C and based on one or more search parameters determined based on the user state information 207 C.
- one or more of the parameters may individually be based on both the textual input 201 C and the user state information 207 C.
- the search system 130 returns responsive search result(s) and the reply content engine 126 selects one or more of the search results for including (in whole or in part) in reply content 209 C.
- Determining search parameters based on the user state information 207 C may include selecting, based on the user state information 207 C, one or more terms and/or other parameters used for identifying content in the search and/or submitting the search with an indication of the user state information 207 C for ranking one or more of the search results based on the user state information 207 C. For example, for received textual input of “I'm bored” with a user state indicative of “stressed”, a search may be issued for “relaxing activities nearby”, where “relaxing” is chosen based on the “stressed” user state.
- search results may be issued and the search results ranked based at least in part on whether they have a “relaxing” or similar attribute. For instance, search results that do not have a “relaxing” attribute may be filtered out of the search results.
- the search system 130 searches one or more search databases 154 to identify content that is responsive to the search parameters provided by the reply content engine 126 .
- the search databases 154 include database(s) that index publicly available content and/or database(s) that index content that is private to the user.
- the search system 130 may utilize the search databases 154 to identify responsive content and may generate search results based on the identified responsive content.
- one or more of the search databases 154 may be remote from the automated assistant 120 and/or any separate client computing device(s), and/or one or more of the search databases 154 may be local to the automated assistant 120 and/or any separate client computing device(s).
- the term “database” is used to refer to any collection of structured or unstructured data stored in one or more computer readable media.
- the search system 130 may utilize various techniques in searching the search databases 154 in response to textual input.
- the search system 130 may search one or more of the search databases 154 based on search parameter(s) that conform strictly to the textual input 201 C and optionally based on search parameter(s) that are added based on the user state information 207 C.
- the search system 130 may search one or more of the search databases 154 based on one or more search parameters that are based on, but that do not necessarily conform strictly to, the textual input 201 C.
- the search system 130 may search one or more of the search databases 154 based on a search parameter that restricts the search databases 154 to “points of interests” databases and/or content to “restaurant” content, and based on a search parameter that restricts content to content that is within a threshold distance of a current location of the user.
- the search system 130 may search one or more of the search databases 154 based on a search parameter that restricts the search databases 154 to databases that are personal to the user and/or content to “image” content.
- the search system 130 may include a ranking engine that ranks identified content using one or more ranking signals, such as popularity of the content, a degree of matching between the search parameters and the content, attributes of the user (e.g., a location of the user, a primary language of the user), and/or based on search attributes that are based on the user state information 207 C.
- Ranking the identified content may include calculating scores or other measures for the identified content and using those measures to indicate which content items to utilize as search results included in reply content 209 C.
- the search system 130 may filter out or otherwise demote one or more content items based on determining those content items don't conform to the user state information 207 C.
- the search system 130 may select or otherwise promote one or more content items based on determining those content items conform to the user state information 207 C.
- the search system 130 uses the identified responsive content ranked by the ranking engine to generate search results.
- the search results include search results corresponding to the content that is responsive to the search issued based on the textual input.
- each of the search results can include a title or other synopsis of a responsive content item, a summary of the content item, a link to the responsive content item, other information related to the responsive content item, and/or even the entirety of the content item.
- the summary of a news story content item may include a particular “snippet” or section of the news story.
- the search result may include a reduced size display of the image, a title associated with the image, and/or a link to the image.
- the search result may include an image from the video, a segment of the video, a title of the video, and/or a link to the video.
- search system 130 is illustrated as separate from the automated assistant 120 in FIG. 1 , in some implementations one or more aspects of the search system 130 may be incorporated in the automated assistant 120 .
- all or aspects of the ranking engine of the search system 130 e.g., at least the aspect that ranks search results based on user state information
- FIGS. 3A-9 various examples of implementations of the automated assistant 120 are described.
- FIGS. 3A, 3B, 4A, 4B, and 6-9 each illustrate a computing device 110 with a display screen 140 displaying graphical user interfaces with examples of dialog that may occur between a user of the computing device 110 , the automated assistant 120 , and/or one or more additional users according to implementations disclosed herein.
- One or more aspects of the automated assistant 120 may be implemented on the computing device 110 and/or on one or more computing devices that are in network communication with the computing device 110 .
- the display screen 140 of FIGS. 3A, 3B, 4A, 4B, and 6-9 further includes a textual reply interface element 188 that the user may select to generate user interface input via a virtual keyboard and a voice reply interface element 189 that the user may select to generate user interface input via a microphone.
- the user may generate user interface input via the microphone without selection of the voice reply interface element 189 .
- active monitoring for audible user interface input via the microphone may occur to obviate the need for the user to select the voice reply interface element 189 .
- the voice reply interface element 189 may be omitted.
- the textual reply interface element 188 may additionally and/or alternatively be omitted (e.g., the user may only provide audible user interface input).
- the display screen 140 of FIGS. 3A, 3B, 4A, 4B, and 6-9 also includes system interface elements 181 , 182 , 183 that may be interacted with by the user to cause the computing device 110 to perform one or more actions.
- FIG. 5 illustrates a computing device 110 that includes one or more microphones and one or more speakers and illustrates examples of dialog that may occur, via the microphone(s) and speaker(s), between a user 101 of the computing device 510 and the automated assistant 120 according to implementations described herein.
- One or more aspects of the automated assistant 120 may be implemented on the computing device 510 and/or on one or more computing devices that are in network communication with the computing device 510 .
- the user provides textual input 321 A as part of a dialog between the user and the automated assistant 120 .
- the automated assistant 120 determines user state information for the user and provides a reply 331 A that is based on the textual input 321 A and the user state information.
- the user state information may be indicative of the user being “sad” and the reply 331 A may be generated based on modifying initial textual output in view of that user state information.
- the text generation engine 124 may generate initial textual output of “Hi, tell me about your day”, and, based on the user state information, the reply content engine 126 may replace “tell me about” with “how was”.
- the user provides textual input 322 A in furtherance of the dialog.
- the automated assistant 120 provides a reply 332 A that is based on the previously determined user state information for the user and/or based on updated user state information (e.g., determined based at least in part on the textual input 322 A and/or sensor data received after the providing of the textual input 321 A).
- the automated assistant 120 generates the reply 332 A based on the textual input 322 A (and optionally based on prior textual inputs 321 A and/or 331 A) and based on the user state information.
- the user state information may still be indicative of the user being “sad” and the reply 332 A may be generated based on modifying initial textual output in view of that user state information.
- the text generation engine 124 may generate initial textual output of “Nice to hear. Anything I can help with?”, and, based on the user state information, the reply content engine 126 may replace “Nice” with “Sorry” and add “Let me know if there is” before “anything”.
- FIG. 3B illustrates an example of how the dialog of FIG. 3A may differ according to user state information of the user that differs from that of FIG. 3A .
- the textual input 321 B is the same as textual input 321 A of FIG. 3A and the textual input 322 B is the same as textual input 322 A of FIG. 3A .
- replies 331 B and 332 B differ based on different user state information being determined for the user. For example, in FIG. 3B the user state information may be indicative of the user being “happy”.
- the replies 331 B and 332 B may be generated based on modifying initial textual outputs in view of that user state information.
- the text generation engine 124 may generate initial textual output of “Hi, tell me about your day”, and, based on the user state information, the reply content engine 126 may replace “Hi” with “howdy” and add the smiley face emoji. Also, for instance, for reply 332 B the text generation engine 124 may generate initial textual output of “Nice to hear. Anything I can help with?”, and, based on the user state information, the reply content engine 126 may replace “Nice” with “Good”.
- the user provides textual input 421 A as part of a dialog between the user and the automated assistant 120 .
- the automated assistant 120 determines user state information for the user and provides a reply 431 A that is based on the textual input 421 A and the user state information.
- the user state information may be indicative of the user being “jovial” and the reply 431 A may be generated based on identifying multiple reply options, and selecting one of the reply options in view of that user state information.
- the reply content engine 126 may identify at least a “tell a joke” option and a “provide information on time management” option based on the textual input 421 A and may select the “tell the joke” option based on the “jovial” user state information.
- the “tell the joke” option may be assigned to a jovial user state identifier in one or more reply option models.
- the user provides affirmative textual input 422 A in furtherance of the dialog and the automated assistant 120 provides a reply 432 A.
- the reply is a joke that is selected from a corpus of jokes based on the reply content engine 126 selecting of the “tell the joke” option.
- FIG. 4B illustrates an example of how the dialog of FIG. 4A may differ according to user state information of the user that differs from that of FIG. 4A .
- the textual input 421 B is the same as textual input 421 A of FIG. 4A and the textual input 422 B is the same as textual input 422 A of FIG. 4A .
- replies 431 B and 432 B differ from the replies 431 A and 432 A of FIG. 4A , and differ based on different user state information being determined for the user.
- the user state information may be indicative of the user being “stressed”.
- the reply 431 B may be generated based on the reply content engine 126 identifying that a “provide information on time management” reply option conforms to the user state information.
- the reply content engine 126 may further provide the reply 432 B based on the affirmative textual input 422 B and based on selection of the “provide information on time management reply option”.
- the textual inputs 521 A and 522 A are the same as the textual inputs 321 A and 321 B of FIG. 3A .
- the replies 531 A and 532 A provided by the automated assistant 120 are the same as the replies 331 A and 332 A of FIG. 3A .
- the textual inputs 521 A and 522 A are provided audibly by the user to the computing device 510 via microphone(s) of the computing device 510 and the computing device provides the replies 531 A and 532 A for audible presentation via speaker(s) of the computing device 510 .
- the replies 531 A and 532 A may be generated in a similar manner as that described with respect to the replies 331 A and 332 A of FIG. 3A .
- the audible presentation of the replies 531 A and 532 A may optionally be based on one or more audible style features determined based on the user state information. For example, based on the user state information indicating the user is “jovial”, the audible tone and/or other audible feature used in providing the audible replies 531 A and 532 A may be more “jovial” than if, for example, the user state information indicated the user was “sad”.
- the user provides textual input 621 as part of a dialog between the user and the automated assistant 120 .
- the automated assistant 120 determines user state information for the user and provides a reply 631 that is based on the textual input 621 and the user state information.
- the user state information may be indicative of the user being “stressed” and the reply 631 may be generated based on issuing a search that includes one or more search parameters that are based on the textual input 621 and that includes one or more search parameters that are based on that user state information.
- the reply content engine 126 may cause the search system 130 to issue a search for “relaxing activities nearby”, where “relaxing” is chosen based on the “stressed” user state and/or may cause the search system 130 to rank the search results based at least in part on whether they have a “relaxing” or similar attribute. For instance, search results that do not have a “relaxing” attribute may be filtered out of the search results.
- the reply 631 includes three of the search results identified in response to the search. Each of the search results relates to a “relaxing” location. Selection of one of the search results by the user may cause additional information about the corresponding location to be presented to the user.
- the additional information may be presented as a continuation of the ongoing dialog (e.g., added in the graphical user interface of FIG. 6 to the transcript of the dialog as further output from the automated assistant 120 ).
- selection of one of the search results may cause the additional information to be presented to the user in a separate application and/or interface. For example, selection of one of the search results may cause a separate application to navigate to a link associated with the search result.
- the user provides textual input 721 as part of the dialog with the automated assistant 120 and the automated assistant 120 provides a reply 731 that is responsive to the textual input 721 .
- the reply 731 may not be determined based on user state information of the user.
- the user then provides textual input 722 as a continuation of the dialog and in response to the reply 731 .
- the automated assistant 120 may determine that the user state information of the user during or before providing the textual input 722 is indicative of the user being “frustrated”, “angry”, and/or “stressed”. In response to such a determination, the automated assistant 120 may provide the user with a reply 732 that includes an interactive element that can be selected to speak with a human.
- the interactive element may be provided in response to determining such user state information and in response to determining that such user state information indicates at least a threshold degree of change (in direction and/or magnitude) from prior (e.g., immediately prior) user state information of the user. Compare this to a situation where the user state information of the user during or before providing the textual input 722 is indicative of the user being “neutral” or “happy”.
- the reply 732 may instead be tailored to helping the user resolve the request of textual input 721 without presenting the option to speak with a human.
- Selection of the “Select to Speak with Human” interactive element may cause a network connection to be established between the computing device 110 and an additional computing device of a human operator, and further communication to occur between the user and the human operator via the network connection.
- the further communication may be via the graphical user interface of FIG. 7 .
- selection of the interactive element may cause a separate application and/or interface to be utilized for the further communication.
- FIGS. 8 and 9 present two examples of dialogs that include the user of the computing device 110 and an additional user (“Joe”). Although FIGS. 8 and 9 only illustrate one additional user, it is understood that multiple additional users may be engaged in a dialog.
- the user of the computing device 110 provides textual input 821 in the dialog and the additional user (“Joe”) provides a responsive reply 841 in the dialog.
- the user of the computing device 110 then provides an additional textual input 822 that is directed to the automated assistant 120 .
- the textual input is directed to the automated assistant 120 by the inclusion of “@AS”.
- additional or alternative techniques for invoking the automated assistant 120 may be utilized and/or the automated assistant 120 may optionally provide responsive content without explicit invocation.
- the automated assistant provides a reply 831 that is based on the textual input 822 and based on the user state information of the user of the computing device 110 and/or of the additional user (“Joe”).
- the user state information may be indicative of both the user of the computing device 110 and the additional user being “giddy”
- the reply 831 may be generated based on issuing a search that includes one or more search parameters that are based on the textual input 822 and that includes one or more search parameters that are based on that user state information.
- the reply content engine 126 may cause a search to be issued for “funny events nearby”, where “funny” is chosen based on the “giddy” user state and/or may cause the search results to be ranked based at least in part on whether they have a “funny” or similar attribute. For instance, search results that do not have a “funny” attribute may be filtered out of the search results.
- the reply 831 includes three of the search results identified in response to the search. Each of the search results relates to a “funny” event. Selection of one of the search results by the user may cause additional information about the corresponding event to be presented to the user. In some implementations, the additional information may be presented as a continuation of the ongoing dialog. In some other implementations, selection of one of the search results may cause the additional information to be presented to the selecting user in a separate application and/or interface.
- the additional user (“Joe”) provides textual input 941 that is a message directed to the user of the computing device 110 .
- the automated assistant 120 generates responsive reply content 931 .
- the automated assistant 120 provides the responsive reply content to the user of the computing device 110 (optionally without separately providing it to the additional user).
- the user of the computing device 110 may optionally select one of the two reply options listed in the reply content 931 to utilize the selected option as a reply, by the user of the computing device 110 , to the textual input 941 . Selection of one of the options may cause the selected option to be automatically included in the transcript of the dialog presented in the graphical user interface of FIG.
- the textual reply interface element 188 may populate the text of the selected option in the textual reply interface element 188 (for optional editing by the user prior to explicit submission by the user), or may otherwise enable editing by the user of the computing device 110 and/or require an additional explicit input of the user of the computing device 110 prior to inclusion in the transcript of the dialog.
- the automated assistant 120 may generate the reply options of reply content 931 based on determined user state information for the additional user (“Joe”). For example, the user state information for the additional user may indicate that he is “sad” and, as a result, the reply content engine 126 may generate reply content that conforms to that user state information. For instance, the text generation engine 124 may generate an initial textual output based on the textual input 941 , and the reply content engine 126 may modify that initial textual input based on the user state information for the additional user.
- FIG. 10 is a flowchart illustrating an example method 1000 according to implementations disclosed herein. For convenience, the operations of the flow chart are described with reference to a system that performs the operations. This system may include various components of various computer systems, such as one or more components of automated assistant 120 and/or search system 130 . Moreover, while operations of method 1000 are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted or added.
- the system receives textual input.
- the system receives the textual input as part of a dialog that includes a user, an automated assistant, and optionally one or more additional users.
- the textual input may be generated by the user based on user interface input generated by one or more user interface input devices, such as a microphone and/or virtual keyboard.
- the system determines user state information for the user that submitted the textual input and/or for one or more additional users.
- the system may utilize various inputs and/or various techniques in determining the user state information. For example, the system may determine the user state information for the user based on sensor data of user interface input device(s) of the user, other sensor(s) of the user, the textual input received at block 1010 , and/or prior textual input received from the user. In some implementations, block 1020 may occur prior to block 1010 .
- the system generates reply content based on both the textual input and the user state information.
- the system may generate various types of reply content and may utilize various techniques to generate the reply content based on both the textual input and the user state information.
- the system may utilize one or more of the techniques described with respect to earlier figures.
- the system provides the reply content for inclusion in the dialog.
- the system provides the reply content in response to the user interface input of block 1010 .
- the system automatically incorporates the reply content in the dialog as a reply, by the system, to the textual input of block 1010 .
- the system may provide one or more commands to one or more computing devices of users engaged in the dialog to cause those computing devices to present the reply content (e.g., to incorporate the reply content in a transcript of the dialog).
- the system suggests the reply content to the user for inclusion in the dialog (e.g., by a general indication that reply content is available), and only includes the reply content in the dialog in response to further user interface input.
- the system suggests the reply content to an additional user involved in the dialog as a reply, by the additional user, to the textual input of block 1010 .
- blocks 1010 , 1020 , 1030 , and 1040 may occur during a dialog to enable reply content to be adapted to various user states and/or textual inputs that may occur throughout the dialog.
- blocks 1010 , 1020 , 1030 , and 1040 are illustrated in a particular order, it is understood the order may be altered, one or more blocks may be performed in parallel, and/or one or more blocks may only be selectively performed.
- FIG. 11 is a block diagram of an example computing device 1110 that may optionally be utilized to perform one or more aspects of techniques described herein.
- one or more of a client computing device, automated assistant 120 , and/or other component(s) may comprise one or more components of the example computing device 1110 .
- Computing device 1110 typically includes at least one processor 1114 which communicates with a number of peripheral devices via bus subsystem 1112 .
- peripheral devices may include a storage subsystem 1124 , including, for example, a memory subsystem 1125 and a file storage subsystem 1126 , user interface output devices 1120 , user interface input devices 1122 , and a network interface subsystem 1116 .
- the input and output devices allow user interaction with computing device 1110 .
- Network interface subsystem 1116 provides an interface to outside networks and is coupled to corresponding interface devices in other computing devices.
- User interface input devices 1122 may include a keyboard, pointing devices such as a mouse, trackball, touchpad, or graphics tablet, a scanner, a touchscreen incorporated into the display, audio input devices such as voice recognition systems, microphones, and/or other types of input devices.
- pointing devices such as a mouse, trackball, touchpad, or graphics tablet
- audio input devices such as voice recognition systems, microphones, and/or other types of input devices.
- use of the term “input device” is intended to include all possible types of devices and ways to input information into computing device 1110 or onto a communication network.
- User interface output devices 1120 may include a display subsystem, a printer, a fax machine, or non-visual displays such as audio output devices.
- the display subsystem may include a cathode ray tube (CRT), a flat-panel device such as a liquid crystal display (LCD), a projection device, or some other mechanism for creating a visible image.
- the display subsystem may also provide non-visual display such as via audio output devices.
- output device is intended to include all possible types of devices and ways to output information from computing device 1110 to the user or to another machine or computing device.
- Storage subsystem 1124 stores programming and data constructs that provide the functionality of some or all of the modules described herein.
- the storage subsystem 1124 may include the logic to perform selected aspects of the method of FIG. 10 .
- Memory 1125 used in the storage subsystem 1124 can include a number of memories including a main random access memory (RAM) 1130 for storage of instructions and data during program execution and a read only memory (ROM) 1132 in which fixed instructions are stored.
- a file storage subsystem 1126 can provide persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive along with associated removable media, a CD-ROM drive, an optical drive, or removable media cartridges.
- the modules implementing the functionality of certain implementations may be stored by file storage subsystem 1126 in the storage subsystem 1124 , or in other machines accessible by the processor(s) 1114 .
- Bus subsystem 1112 provides a mechanism for letting the various components and subsystems of computing device 1110 communicate with each other as intended. Although bus subsystem 1112 is shown schematically as a single bus, alternative implementations of the bus subsystem may use multiple busses.
- Computing device 1110 can be of varying types including a workstation, server, computing cluster, blade server, server farm, or any other data processing system or computing device. Due to the ever-changing nature of computers and networks, the description of computing device 1110 depicted in FIG. 11 is intended only as a specific example for purposes of illustrating some implementations. Many other configurations of computing device 1110 are possible having more or fewer components than the computing device depicted in FIG. 11 .
- users are provided with one or more opportunities to control whether information is collected, whether the personal information is stored, whether the personal information is used, and how the information is collected about the user, stored and used. That is, the systems and methods discussed herein collect, store and/or use user personal information only upon receiving explicit authorization from the relevant users to do so. For example, a user is provided with control over whether programs or features collect user information about that particular user or other users relevant to the program or feature.
- Each user for which personal information is to be collected is presented with one or more options to allow control over the information collection relevant to that user, to provide permission or authorization as to whether the information is collected and as to which portions of the information are to be collected.
- users can be provided with one or more such control options over a communication network.
- certain data may be treated in one or more ways before it is stored or used so that personally identifiable information is removed.
- a user's identity may be treated so that no personally identifiable information can be determined.
- a user's geographic location may be generalized to a larger region so that the user's particular location cannot be determined.
Abstract
Description
- Automated assistants (also known as “personal assistant modules”, “mobile assistants”, or “chat bots”) may be interacted with by a user via a variety of computing devices, such as smart phones, tablet computers, wearable devices, automobile systems, standalone automated assistant devices, and so forth. The automated assistants receive textual input from the user (e.g., typed and/or spoken) and respond with responsive textual output (e.g., visual and/or audible).
- This specification is directed to methods, apparatus, and computer readable media related to receiving textual input of a user during a dialog between the user and an automated assistant (and optionally one or more additional users), and generating responsive reply content based on the textual input and based on user state information. The reply content is provided for inclusion in the dialog. In some implementations, the reply content is provided as a reply, by the automated assistant, to the user's textual input and may optionally be automatically incorporated in the dialog between the user and the automated assistant. In some implementations, the reply content is suggested by the automated assistant for inclusion in the dialog and is only included in the dialog in response to further user interface input.
- In many implementations, the user state information used to generate the reply content is based at least in part on one or more indicators that are in addition to the textual input itself, such as indicators based on one or more sensors (e.g., camera, microphone, keyboard sensors, touchscreen sensors, heart rate sensor) of a computing device of at least one of the users participating in the dialog that includes the automated assistant. In some implementations, the user state information indicates the current sentiment of the user. In some implementations, the user state information additionally or alternatively indicates the current sentiment of another user that is also engaged in the dialog that includes the automated assistant and the user (e.g., a dialog that includes two or more users and the automated assistant).
- As one example, assume textual input of “hello” provided by a user to an automated assistant during a dialog that includes the user and the automated assistant. If user state information of the user indicates the user is stressed, textual reply content of “hi, what can I do to help?” may be provided by the automated assistant as a reply to the user's textual input. If the user state information indicates the user is jovial, textual reply content of “howdy, can I help?” may instead be provided. As described in more detail herein, in some implementations different reply contents may be provided based on modifying a “default” initial textual output from a text generation engine in view of the user state information. For instance, the initial textual output of “hi, can I help?” may be modified with additional content that is dependent on user state information as indicated in the preceding examples (e.g., replacing “hi” with “howdy” or adding “what” and “do” to “can I help”).
- As another example, assume textual input of “I'm bored” provided by a user to an automated assistant during a dialog that includes the user and the automated assistant. If user state information of the user indicates the user is jovial, textual reply content of “want to hear some jokes?” may be provided by the automated assistant as a reply to the textual input. If the user state information indicates the user is sad, textual reply content of “anything you want to talk about?” may instead be provided. As described in more detail herein, in some implementations the different reply contents may be provided by selecting, based on the user state information, from multiple reply options identified for the textual input. Additional description of the above and additional techniques are provided below.
- Some of the above mentioned and other implementations of the specification may achieve various technical advantages. For example, some implementations of generating reply content based on user state information may enable a more concise dialog to be achieved, while still satisfying informational and/or other needs of the user(s) included in the dialog. This may reduce the use of various computational resources, such as resources of computing device(s), that are required for visually and/or audibly presenting the dialog to the user(s). As yet another example, some implementations that generate reply content that includes interactive user interface elements that are based on user state information may increase the likelihood that such interactive elements are beneficial to one or more user(s) included in the dialog—and lessen the chance that computational resources are expended in association with irrelevant interactive elements. Additional or alternative technical advantages may be achieved, such as one or more described elsewhere in this specification.
- In some implementations, a method performed by one or more processors is provided that includes receiving textual input. The textual input is based on user interface input generated by a user via one or more user interface input devices of a computing device of the user and the user interface input is generated by the user as part of a dialog that includes the user and an automated assistant implemented by one or more of the processors. The method further includes determining user state information for the user, generating reply content based on both the textual input and the user state information, and providing the reply content in response to the user interface input. The user state information identifies a state of the user, is in addition to the textual input, and is based on sensor data generated by the computing device or an additional computing device of the user. The reply content is provided for inclusion in the dialog in response to the textual input and the reply content is provided for presentation via one or more user interface output devices.
- These and other implementations of technology disclosed herein may optionally include one or more of the following features.
- In some implementations, a transcript of the dialog between the user and the automated assistant is displayed in a graphical user interface rendered by one of the user interface output devices of the computing device of the user. In some of those implementations, providing the reply content includes incorporating the reply content into the transcript for display along with previous content of the dialog. Incorporating the reply content into the transcript may include transmitting, via one or more network interfaces, a command to the computing device, where the command causes the computing device to incorporate the reply content into the transcript.
- In some implementations, generating the reply content based on both the textual input and the user state information includes: providing the textual input to a text generation engine; receiving an initial textual output from the text generation engine; and generating the reply content by modifying the initial textual output based on the user state information. In some of those implementations, modifying the initial textual output based on the user state information includes: applying input to a model stored in one or more computer readable media, where the input is based on one or more segments of the initial textual output; and generating, over the model and based on the input, output that indicates one or more terms for modifying the initial textual output. In some versions of those implementations, the model includes mappings of neutral textual segments to textual segments that are specific to the user state, the input applied to the model is one or more of the segments of the initial textual output, and the output generated over the model includes at least one of the textual segments that are specific to the user state. In some other version of those implementations, the method further includes selecting the model based on conformance between the user state information and one or more user state identifiers assigned to the model. In yet other versions of those implementations, the method further includes: applying, to the model, additional input that is based on the user state information. Generating the output over the model may be further based on the additional input. The model may be a trained machine learning model and the method may further include generating the trained machine learning model based on a plurality of training examples. Each of the training examples may include: training example input based on a corresponding neutral text segment and corresponding user state information; and training example output based on a corresponding modified text segment that is a modification of the corresponding neutral text segment in view of the corresponding user state information. Generating the trained machine learning model may include training the trained machine learning model based on application of the training example input of the training examples and backpropagation based on the training example output of the training examples.
- In some implementations, the reply content includes at least one selectable graphical element that, when selected via further user interface input, causes the computing device of the user to present additional content to the user. In some versions of those implementations, generating the reply content based on both the textual input and the user state information includes: issuing a search of one or more databases based on both the textual input and the user state information; receiving one or more search results in response to issuing the search; and incorporating one or more of the search results into the reply content. The selectable graphical element may be associated with one of the search results. Issuing the search based on both the textual input and the user state information may include: determining a search parameter based on the textual input; determining an additional search parameter based on the user state information. The additional search parameter may be either: a search term used to identify responsive content on which the search results are based, or a ranking parameter used to rank the search results. In some other version of implementations where the reply content includes at least one selectable graphical element, selection of the selectable graphical element may cause the computing device of the user to establish a network connection with a computing device of an additional user. The additional content may indicate initiation of a new dialog with the additional user.
- In some implementations, determining the user state information includes: identifying a plurality of user state indicators based on the sensor data; applying the user state indicators as input to at least one user state model stored in one or more computer readable media; and generating, over the at least one user state model and based on the input, output that indicates the user state information. In some of those implementations, the at least one user state model includes a trained sentiment classifier.
- In some implementations, the sensor data includes user interface sensor data that is based on the user interface input and determining the user state information includes determining the user state information based on the user interface input sensor data.
- In some implementations, generating the reply content based on both the textual input and the user state information includes: determining a plurality of reply options based on the textual input; selecting one of the reply options based on conformity of the selected one of the reply options to the user state information; and generating the reply content based on the selected one of the reply options.
- In some implementations, generating the reply content based on both the textual input and the user state information includes: providing the textual input to a text generation engine; receiving at least a first textual output and a second textual output from the text generation engine; and selecting, based on the user state information, one of the first textual output and the second textual output to utilize as the reply content.
- In some implementations, the dialog includes an additional user and the method further includes determining additional user state information for the additional user. Generating the reply content may be further based on the additional user state information.
- In some implementations, generating the reply content based on the user state information includes determining a style feature of the reply content based on the user state information.
- In some implementations, the sensor data on which the user state information is based is from a plurality of sensors of the computing device or the additional computing device.
- In some implementations, a method performed by one or more processors is provided that includes receiving textual input. The textual input is based on user interface input generated via one or more user interface input devices of a computing device of a user or an additional user. The user interface input is generated as part of a dialog that includes the user, the additional user, and an automated assistant implemented by one or more of the processors. The method further includes: determining user state information for at least one of: the user and the additional user; generating reply content based on both the textual input and the user state information; and providing the reply content in response to the textual input. The reply content is provided for inclusion in the dialog and the reply content is provided for presentation via one or more user interface output devices.
- These and other implementations of technology disclosed herein may optionally include one or more of the following features.
- In some implementations, providing the reply content includes incorporating the reply content in the dialog for presentation to the user and to the additional user.
- In some implementations, determining the user state information includes determining the user state information for both the user and the additional user.
- In some implementations, the user interface input on which the textual input is based is from the computing device of the additional user, the user state information is for at least the additional user, and providing the reply content includes providing the reply content for presentation to the user as a suggested reply, by the user, to the textual input.
- In addition, some implementations include one or more processors of one or more computing devices, where the one or more processors are operable to execute instructions stored in associated memory, and where the instructions are configured to cause performance of any of the aforementioned methods. Some implementations also include one or more non-transitory computer readable storage media storing computer instructions executable by one or more processors to perform any of the aforementioned methods.
- It should be appreciated that all combinations of the foregoing concepts and additional concepts described in greater detail herein are contemplated as being part of the subject matter disclosed herein. For example, all combinations of claimed subject matter appearing at the end of this disclosure are contemplated as being part of the subject matter disclosed herein.
-
FIG. 1 is a block diagram of an example environment in which implementations disclosed herein may be implemented. -
FIGS. 2A, 2B, and 2C each illustrate an example of using components of the example environment ofFIG. 1 in generating reply content based on textual input and based on user state information. -
FIG. 3A illustrates an example client computing device with a display screen displaying an example of dialog that may occur between a user of the client computing device and an automated assistant according to user state information of the user. -
FIG. 3B illustrates an example of how the dialog ofFIG. 3A may differ according to user state information of the user that differs from that ofFIG. 3A . -
FIG. 4A illustrates the example client computing device ofFIG. 3A , displaying another example of dialog that may occur between a user of the client computing device and the automated assistant according to user state information of the user. -
FIG. 4B illustrates an example of how the dialog ofFIG. 4A may differ according to user state information of the user that differs from that ofFIG. 4A . -
FIG. 5 illustrates another example client computing device, and an example of audible dialog that may occur between a user of the client computing device and an automated assistant according to implementations disclosed herein. -
FIGS. 6 and 7 illustrate the example client computing device ofFIG. 3A , displaying yet further examples of dialog that may occur between a user of the client computing device and the automated assistant according to user state information of the user. -
FIGS. 8 and 9 illustrate the example client computing device ofFIG. 3A , displaying examples of dialog that includes multiple users and the automated assistant. -
FIG. 10 is a flowchart illustrating an example method according to implementations disclosed herein. -
FIG. 11 illustrates an example architecture of a computing device. - Implementations described herein relate to receiving textual input of a user during a dialog that involves the user, an automated assistant, and optionally one or more additional users—and generating responsive reply content based on the textual input. The textual input that is provided by the user may be textual input that initiates the dialog or it may be a continuation of a previously initiated dialog. The textual input may be natural language free-form input, such as textual input that is based on user interface input generated by the user via one or more user interface input devices (e.g., based on typed input provided via a physical or virtual keyboard or based on spoken input provided via a microphone). As used herein, free-form input is input that is formulated by a user and that is not constrained to a group of options presented for selection by the user (e.g., not constrained to a group of options presented in a drop-down menu).
- As described herein, in many situations reply content generated by an automated assistant in response to textual input of a user is generated based on the textual input and based on user state information. The user state information on which the reply content is generated may be user state information of the user and/or of other users engaged in the dialog that includes the automated assistant.
- The generated reply content is provided, by the automated assistant, for inclusion in the dialog. In some implementations, the reply content is automatically included in the dialog by the automated assistant as a reply, by the automated assistant, to the textual input. In some implementations, the reply content is suggested by the automated assistant for inclusion in the dialog and is only included in the dialog in response to further user interface input. In some versions of those implementations, the further user interface input is input of the user that provided the textual input and the inclusion of the reply content in the dialog is as a reply to the textual input by the automated assistant. In some other versions of those implementations, the further user interface input is input of an additional user involved in the dialog and the inclusion of the reply content in the dialog is as a reply to the textual input by the other user involved in the dialog.
- Various types of reply content may be provided in response to textual input provided by a user. Moreover, various techniques may be utilized to determine the user state information and/or to determine the reply content based on both the textual input and the user state information. Some of these techniques are described in more detail below with reference to the figures.
- Automated assistants may receive textual input from a user and respond with textual output that is tailored to the received textual input. However, many conventional automated assistants may fail to take into account any user state information that is not explicitly contained in the received textual input. Moreover, many conventional automated assistants may fail to provide responsive non-textual output (e.g., interactive user interface elements, changes to an application state) that is based on user state information.
- Such failure to take into account user state information may lead to undue utilization of computational resources and/or other technical problems in various situations. For example, in some situations, failing to take into account user state information may cause excess dialog to occur between a user and the automated assistant to resolve informational or other needs of the user. Such excess dialog may consume various computational resources, such as resources required to render a presentation of the dialog, communicate between a computing device of the user and remotely located components of an automated assistant and/or of other user's computing devices, etc. Also, for example, in some situations failing to take into account user state information may cause the user to prematurely abandon the dialog and seek to resolve his or her needs using a more computationally expensive alternative technique. As yet another example, failing to provide responsive interactive user interface elements that are based on user state information may cause interactive user interface elements to be provided and selected by the user—only to have the user ignore the responsive output due to the elements not being tailored to the user state information. This may cause a client computing device to unnecessarily perform action(s) in response to the selection, such as opening a new application, rendering a new interface, retrieving content over a network, etc.
- Some implementations of the specification may address the above-mentioned and/or additional technical problems via one or more technical solutions. For example, some implementations of generating reply content based on user state information may enable a more concise dialog to be achieved, while still satisfying informational and/or other needs of the user(s) included in the dialog. This may reduce the use of various computational resources, such as resources of computing device(s), that are required for visually and/or audibly presenting the dialog to the user(s). As yet another example, some implementations that generate reply content that includes interactive user interface elements that are based on user state information may increase the likelihood that such interactive elements are beneficial to one or more user(s) included in the dialog—and lessen the chance that computational resources are expended in association with irrelevant interactive elements. Additional or alternative technical advantages may be achieved, such as one or more described elsewhere in this specification.
- In
FIG. 1 , an example environment in which techniques disclosed herein may be implemented is illustrated. The example environment includes one or more userinterface input devices 102, one or more userinterface output devices 104, one ormore sensors 106, anautomated assistant 120, asearch system 130, one or more search database(s) 154, and one ormore models 152. - The user
interface input devices 102 may include, for example, a physical keyboard, a touch screen (e.g., implementing a virtual keyboard or other textual input mechanism), and/or a microphone. The userinterface output devices 104 may include, for example, a display screen, and/or speaker(s). The sensor(s) 106 may include one or more sensors that are in addition to the userinterface input devices 102 such as, for example, a camera, a heart rate sensor, a location sensor, a GPS sensor, an accelerometer, a gyroscope, a Wi-Fi chip, etc. - The user interface input device(s) 102, user
interface output devices 104, and sensor(s) 106 may be incorporated on one or more computing devices of a user. For example, a mobile phone of the user may include the user interface input andoutput devices sensors 106. Also, for example, a standalone personal assistant hardware device may include the user interface input andoutput devices sensors 106. As another example, a first computing device may include the user interface output device(s) 104, and a separate second computing device may include the user interface input device(s) 102. In that and/or other examples, sensor data from any other sensor(s) 106 of the first and second computing devices may optionally not be provided to the automated assistant 120 (e.g., sensor data from the user interface input device(s) 102 may be utilized to determine a user state). - Although
automated assistant 120 is illustrated inFIG. 1 as separate from the sensor(s) 106 and separate from the user interface input andoutput devices automated assistant 120 may be implemented on a computing device that also contains the user interface input device(s) 102, the user interface output device(s) 104, and/or the sensor(s) 106. For example, all or aspects ofuser state engine 122 and/orreply content engine 126 ofautomated assistant 120 may be implemented on the computing device. In some situations where all or aspects ofuser state engine 122 are implemented on the computing device, synthesized versions of user state information may be determined by the computing device and provided to a remoteautomated assistant 120, which may enable prevention of certain more granular sensor signals from being transmitted by the computing device to the remoteautomated assistant 120. In some implementations, all or aspects of theautomated assistant 120 may be implemented on computing device(s) that are separate and remote from a computing device that contains the user interface input device(s) 102, the user interface output device(s) 104, and/or the sensor(s) 106 (e.g., all or aspects may be implemented “in the cloud”). In some of those implementations, those aspects ofautomated assistant 120 may communicate with the computing device via one or more networks such as a local area network (LAN) and/or wide area network (WAN) (e.g., the Internet). - Some non-limiting examples of client computing device(s) that may include the user interface input device(s) 102, the user interface output device(s) 104, and/or the sensor(s) 106 include one or more of: a desktop computing device, a laptop computing device, a standalone hardware device at least in part dedicated to automated assistance, a tablet computing device, a mobile phone computing device, a computing device of a vehicle of the user (e.g., an in-vehicle communications system, an in-vehicle entertainment system, an in-vehicle navigation system), or a wearable apparatus of the user that includes a computing device (e.g., a watch of the user having a computing device, glasses of the user having a computing device, a virtual or augmented reality computing device).
- Additional and/or alternative client computing devices may be provided. In some implementations, a given user may communicate with all or aspects of
automated assistant 120 utilizing a plurality of client computing devices that collectively form a coordinated “ecosystem” of computing devices. However, for the sake of brevity, some examples described in this disclosure will focus on the user operating a single client computing device. Also, in many implementations multiple users may communicate with all or aspects ofautomated assistant 120, and each user may utilize one or more of their computing devices to communicate with theautomated assistant 120. For example, as described herein theautomated assistant 120 may be involved in a dialog between multiple users and may communicate with client computing device(s) of one or more of those users. - A client computing device and
automated assistant 120 may each (when they are separate devices) include one or more memories for storage of data and software applications, one or more processors for accessing data and executing applications, and other components that facilitate communication over a network. The operations performed by one or more of the client computing devices and/or by theautomated assistant 120 may be distributed across multiple computing devices.Automated assistant 120 may be implemented as, for example, computer programs running on one or more computers in one or more locations that are coupled to each other through a network. - As illustrated in
FIG. 1 , a user provides input to theautomated assistant 120 via the user interface input device(s) 102. Theautomated assistant 120 provides responsive output for presentation to the user and/or to additional user(s) via the user interface output devices(s) 104 (optionally after further processing by one or more components). For the sake of simplicity, the input is illustrated inFIG. 1 as being provided directly to theautomated assistant 120 by the user interface input device(s) 102 and the output is illustrated as being provided by theautomated assistant 120 directly to the user interface output device(s) 104. However, it is noted that in various implementations one or more intermediary hardware components may be functionally interposed between theautomated assistant 120 and the user interface input and/oroutput devices automated assistant 120 and generate, based on the processing of the output, one or more signals for presentation of reply content via the user interface output device(s) 104. For instance, where a userinterface output device 104 is on a client computing device separate from all or aspects of theautomated assistant 120, a hardware processor and/or other components may process the output and generate, based on the output, signals for driving the userinterface output device 104. - In some implementations, the input received by the
automated assistant 120 includes content that is already in a textual format (and optionally additional content that is not in a textual format, such as images). For example, the user interface input device(s) 102 may include a keyboard that generates textual input in response to user interface input directed to the keyboard, and the textual input provided to theautomated assistant 120. Also, for example, the user interface input device(s) 102 may include a microphone, a voice-to-text processor that is separate from theautomated assistant 120 may convert voice input received at the microphone into textual input, and the textual input may be provided to theautomated assistant 120. In some other implementations, the input initially received by theautomated assistant 120 is not in a textual format, but at least some of the input is converted to a textual format by theautomated assistant 120 and provided in the textual format to one or more other components of theautomated assistant 120. For instance, the user interface input device(s) 102 may include a microphone, voice input may be received at the microphone and provided to theautomated assistant 120, and a voice-to-text processor of theautomated assistant 120 may convert the voice input into textual input. - A client computing device may optionally operate one or more applications that enable dialog with the
automated assistant 120. Such applications may come in various forms such as a short messaging service (“SMS”) and/or multimedia messaging service (“MMS”) client, an online chat client (e.g., instant messenger, Internet relay chat, or “IRC,” etc.), a messaging application associated with a social network, an email client, a social blogging client, an automated assistant messaging service dedicated to conversations withautomated assistant 120, and so forth. In some implementations, one or more of the applications may be implemented via a webpage or other resources rendered by a web browser or other application of a client computing device. - In various implementations, in response to textual input provided to the
automated assistant 120 during a dialog that includes theautomated assistant 120, theautomated assistant 120 may generate responsive reply content based on the textual input and based on user state information. Theautomated assistant 120 may then provide, as output, the reply content for presentation (visual and/or audible) to one or more of the users involved in the dialog. In some implementations, the reply content is automatically included in the dialog by theautomated assistant 120 as a reply, by theautomated assistant 120, to the textual input. In some implementations, the reply content is suggested, by theautomated assistant 120, for inclusion in the dialog and is only included in the dialog in response to further user interface input. In some versions of those implementations, the further user interface input is input of the user that provided the textual input and the inclusion of the reply content in the dialog is as a reply to the textual input by theautomated assistant 120. In some other versions of those implementations, the further user interface input is input of an additional user involved in the dialog and the inclusion of the reply content in the dialog is as a reply to the textual input by the additional user involved in the dialog. - In various implementations, the
automated assistant 120 may include auser state engine 122, atext generation engine 124, and areply content engine 126. In some implementations, one or more ofengines automated assistant 120. For example, one or more ofengines output devices automated assistant 120. - The
user state engine 122 determines user state information for one or more users involved in a dialog with theautomated assistant 120. Theuser state engine 122 provides the user state information to textgeneration engine 124 and/orreply content engine 126 for use in generating reply content based on the user state information. In some implementations, the user state information that is provided and utilized to generate reply content may include one or more classes and/or magnitudes of user state. Various classes and/or magnitudes may be utilized. For example, classes that are indicative of happy, sad, neutral, active, tired, stressed, and/or other user state(s) may be utilized. In some of those and/or other implementations, theuser state engine 122 may also determine a magnitude of one or more of the user states. For instance, theuser state engine 122 may determine a weight of “0.8” for “happy”, a weight of “0.7” for “active”, and a weight of “0.0” for the other classes. Also, for example, more granular classes may additionally and/or alternatively be utilized such as slightly happy, medium happy, and very happy (e.g., in lieu of a general “happy” class). - In various implementations, the
user state engine 122 determines class(es) and/or magnitude(s) of a user state based on applying one or more user state indicators to one or more trained classifier model(s) or other trained machine learning model(s). For example, one or more images of a user captured by a camera may be passed through a face detector and further through a facial expression classifier whose output indicates whether the facial expression is happy, tired, sad, and/or other class. Theuser state engine 122 may determine the user state information based at least in part on the output of the facial expression classifier. - In some implementations, the user state information that is provided by the
user state engine 122 and utilized to generate reply content includes the “raw signals” from one or more sensor(s) and/or discretized versions of those “raw signals”. For example, one or more images of the user captured by a camera may be provided by theuser state engine 122 and applied by thereply content engine 126 as direct input to a neural network model or other machine learning model(s) that are utilized to determine one or more aspects of reply content to provide to a user. Also, for example, a typing speed of a user determined from sensor data associated with one of the user interface input device(s) may be provided by theuser state engine 122 and applied by thereply content engine 126 as direct input to a neural network model or other machine learning model(s) that are utilized to determined one or more aspects of reply content to provide to a user. - In some implementations, the
user state engine 122 determines user state information based on synthesizing multiple user state indicators. For example, theuser state engine 122 may apply an image of a user captured by a camera to a first classifier trained to predict class(es) of user state based on images and may apply the heart rate sensor data to a second classifier trained to predict class(es) of user state based on heart rate sensor data. Theuser state engine 122 may consider the outputs of the two classifiers in determining the user state information. For example, theuser state engine 122 may average the outputs to determine the user state information, use the greater magnitude output in determining the user state information, and/or otherwise consider and/or combine both outputs in determining the user state information. As another example, theuser state engine 122 may apply both an image of a user captured by a camera and heart rate sensor data of the user as input to a neural network model trained to predict class(es) of user state based on both images and heart rate sensor data. Theuser state engine 122 may determine the user state information based on output generated over the neural network model based on the applied input. - In some implementations, the
user state engine 122 determines user state information that is indicative of user state of multiple users involved in a dialog. Theuser state engine 122 may determine user state information that is indicative of the user states of both users utilizing various techniques. For example, theuser state engine 122 may determine one or more classes and/or magnitudes of user state for a first user, determine one or more classes and/or magnitudes of user state for a second user, and determine the user state information based on the classes and/or magnitudes for both users. For example, theuser state engine 122 may average the classes and magnitudes of the two users. For instance, assume the user state for the first user indicates a first class of “happy” with a magnitude of “0.8” and a second class of “stressed” with a magnitude of “0.2”; and that the user state for the second user indicates the first class of “happy” with a magnitude of “0.6” and the second class of “stressed” with a magnitude of “0.1”. Theuser state engine 122 may determine an average of the classes that indicates the first class of “happy” with a magnitude of “0.7” and the second class of “stressed” with a magnitude of “0.1”. Additional or alternative techniques of determining user state information that is indicative of the user states of multiple users may be utilized. - In some implementations, the
user state engine 122 determines user state information for a given user based at least in part on one or more user state indicators that are in addition to textual input provided by the given user during a dialog that includes theautomated assistant 120. In some of those implementations, theuser state engine 122 may determine the user state information based on sensor data from one or more of the user interface input device(s) 102 used by the given user. For example, when the given user has provided typed input in a dialog (e.g., using a physical keyboard or virtual keyboard), the user state information may be determined based on a typing speed of the typed input, applied pressure for one or more characters of typed input (e.g., as sensed by a touch screen implementing a virtual keyboard), a “delay time” for starting to provide the typed input (e.g., when the typed input is provided responsive to other content), and/or other sensed features of the typed input. For instance, user state information that indicates the given user is stressed may be determined based on typing speed of the given user being fast and the pressure applied by the given user during the typing being high. A typing speed of “fast” and/or a pressure of “high” may be relative to one or more baselines such as a baseline that is specific to the given user (e.g., based on prior typed input of the user) and/or a baseline that is applicable to multiple users. - As another example, when the given user has provided voice input in a dialog (e.g., as sensed by a microphone), the user state information may be determined based on tone, inflection, intonation, accent, pitch, volume, breathing volume, breathing rate, background noise level, background noise type, and/or other features of the voice input. As another example, the
user state engine 122 may determine user state information for the given user based on audio input from a microphone, even when the microphone isn't used by the given user to provide textual input. For example, theuser state engine 122 may determine the user state information based on the presence and/or absence of certain types of sound (e.g., laughter, crying, music) in the audio input, background noise level in the audio input, breathing noise level in the audio input, breathing rate in the audio input, aspects of detected speech in the audio input (e.g., intonation, accent), etc. - In some implementations, the user state indicators that are in addition to textual input provided by the given user include sensor data from one or more of the sensor(s) 106 that are in addition to the user interface input and
output devices user state engine 122 may determine user state information for the given user based on one or more images from a camera that is included in the sensor(s) 106. For example, theuser state engine 122 may determine the user state information based on: user facial expression(s) in the image(s); user gesture(s) in the image(s); user speed of movement in the image(s); user gaze direction(s) in the image(s); environmental brightness in the image(s); background object(s) in the image(s); environmental color temperature in the image(s); and/or shape, wavelength, and/or other features of infrared radiation in the image(s) (e.g., when the camera is a thermographic camera). For instance, one or more images of the given user captured by a camera may be passed through a face detector to detect the face of the given user in the image, and the face of the given user in the image passed through a facial expression classifier whose output indicates whether the facial expression is happy, tired, sad, and/or other class of user state. The outputted class of user state may be utilized as the user state information, or used along with other user state indicators to determine the user state information. - As another example, the
user state engine 122 may determine user state information for the given user based on sensor data indicating a user's heart rate. For example, the sensor data may be from asensor 106 of a watch worn by the given user or other wearable computing device worn by the given user. As yet another example, theuser state engine 122 may determine user state information based on location information determined based on a GPS sensor, a Wi-Fi chip (e.g., location information determined based on available network(s), IP address, and/or other sensor data from the Wi-Fi chip), and/or other sensor(s) 106. For instance, theuser state engine 122 may determine the user state information based on whether the user is at home, at work, or at another location. As yet a further example, theuser state engine 122 may determine user state information based on sensor data from an accelerometer and/or gyroscope. For instance, sensor data from an accelerometer and/or gyroscope may be utilized to determine a potential activity being engaged in by the given user, which may indicate the user state information. - In some implementations, the user state indicators that are in addition to textual input provided by the given user during a dialog include non-textual content provided by the given user in the dialog such as emojis, images, and/or other non-textual content. For example, certain emojis and/or images submitted by a user may be directly mapped to certain indications of user state information. Also, for example, the
user state engine 122 may provide an image submitted by a user to an image analysis engine that recognizes object(s) in images, and one or more objects recognized in the image may be mapped to certain indicators of user state information. - In some implementations, in determining user state information for a given user, the
user state engine 122 may also utilize one or more features of textual input provided by the given user in a dialog that includes theautomated assistant 120. Features of the textual input may include, for example, particular term(s) included in the textual input, syntactic structure(s) of the textual input, and/or length of the textual input (word and/or character length). In some implementations, theuser state engine 122 may apply one or more terms and/or other features of textual input to a sentiment classifier trained to predict class(es) and/or magnitude(s) of sentiment in textual segments and utilize the output from the sentiment classifier in determining the user state information. For example, theuser state engine 122 may determine the user state information based on combining the output from the sentiment classifier with output(s) from other classifiers trained to predict user state information based on sensor data and/or other signals that are in addition to textual input itself. -
Text generation engine 124 is optional and may be utilized to generate one or more initial textual outputs that are each responsive to received textual input. In some implementations, initial textual output that is generated bytext generation engine 124 is provided to replycontent engine 126 andreply content engine 126 generates reply content based on modifying the initial textual input in view of the user state information. For example, thereply content engine 126 may utilize one or more techniques, such as those described herein, to add term(s) to the initial textual output, remove term(s) from the initial textual output, and/or replace term(s) in the initial textual output, to thereby modify the initial textual output such that it is more tailored to the user state information than is the initial textual output. Thetext generation engine 124 may take various forms such as a machine learning model and/or human created heuristics. As one non-limiting example, thetext generation engine 124 may utilize a recurrent neural network model that is trained to predict, based on feature(s) of textual input, an initial textual output that is responsive to the textual input. The recurrent neural network model may be trained, for example, based on a corpus of past textual inputs and responsive replies that occur between humans, such as in past chat sessions between humans. - The
reply content engine 126 may generate and provide various types of reply content in response to textual input provided by a user. Moreover, various techniques may be utilized by thereply content engine 126 to determine the reply content based on both the textual input and user state information. For example, as referenced above, in some implementations the textual input is provided to thetext generation engine 124, an initial textual output is received from thetext generation engine 124, and thereply content engine 126 generates reply content based on modifying the initial textual input in view of the user state information. Also, for example, in some implementations, thereply content engine 126 utilizes the textual input to determine a plurality of candidate reply options (e.g., based on one or more of the model(s) 152 and/or based on multiple initial textual outputs provided by the text generation engine 124), uses the user state information to select one of those reply options, and generates the reply content based on the selected reply option. Also, for example, in some implementations, thereply content engine 126 causes thesearch system 130 to issue a search based on received textual input and user state information, receives search results that are based on the search, and generates the reply content based on one or more of the search results (e.g., one or more of the search results may be provided as all or part of the reply content). - Referring now to
FIGS. 2A-2C , additional description is provided of the components ofFIG. 1 and of various examples of generating reply content based on textual input and based on user state information. -
FIG. 2A illustrates an example of thetext generation engine 124 generating initialtextual output 205A and thereply content engine 126generating reply content 209A by modifying the initialtextual output 205A in view ofuser state information 207A. InFIG. 2A ,textual input 201A that is based on user interface input generated by one or more user interface input device(s) 102 is provided to thetext generation engine 124 as part of a dialog. Thetext generation engine 124 generates initialtextual output 205A and provides the initialtextual output 205A to replycontent engine 126. - The
user state engine 122 receivessensor data 203A and/or thetextual input 201A and utilizes one or moreuser state models 152A (included in themodels 152 ofFIG. 1 ) to generateuser state information 207A that indicates user state of the user that submitted thetextual input 201A and/or of one or more additional users that are also involved in the dialog. Thesensor data 203A may be based on sensor(s) of user interface input device(s) 102 and/or of sensor(s) 106 that are in addition to the user interface input device(s) 102. - As one specific example, the user state model(s) 152A may include a model that is trained to predict one or more classes of sentiment (and optionally magnitude(s)) based on image(s) of a user's face. The
user state engine 122 may apply, as input to the model, one or more images, captured by a camera of sensor(s) 106, that each capture a user's face. Theuser state engine 122 may generate, over the model, output that indicates one or more classes (and optionally magnitudes) indicated by the user's face in the image(s). For instance, theuser state engine 122 may generate an output that indicates a user is “happy”. The user state model(s) 152A may further include a model that is trained to predict the presence of one or more types of sound in audio input. Theuser state engine 122 may apply, as input to the model, audio input captured by a microphone of the user interface input device(s) 102 (regardless of whether the microphone was utilized to generate thetextual input 201A). Theuser state engine 122 may generate, over the model, output that indicates one or more types of sound that are present in the audio input. For instance, theuser state engine 122 may generate an output that indicates laughter is present in the audio input. The user state model(s) 152A may further include a model that is trained to predict one or more classes of sentiment (and optionally magnitude(s)) based on feature(s) of textual input. Theuser state engine 122 may apply, as input to the model, feature(s) of thetextual input 201A and may generate, over the model, output that indicates one or more classes (and optionally magnitudes) indicated by thetextual input 201A. For instance, theuser state engine 122 may generate an output that indicates thetextual input 201A is “happy” textual input. - In some implementations, the
user state engine 122 may optionally synthesize the three outputs from the three models and provide an indication of the synthesized output asuser state information 207A. For example, theuser state engine 122 may synthesize “happy”, “laughter”, and “happy” outputs to user state information that is indicative of a “happy” or “very happy” user state. In some implementations, theuser state engine 122 may optionally provide an indication of the three separate outputs as theuser state information 207A without synthesizing those outputs. Moreover, in some implementations, theuser state engine 122 may optionally provide one or more raw or discretized signals as all or part of theuser state information 207A in lieu of one or more of the three separate outputs (e.g., theuser state information 207A may simply pass the signals without application of the signals to the user state model(s) 152A). - As another example, instead of or in addition to the three separate models mentioned above, the user state model(s) 152A may include a neural network model of other machine learning model that is trained to predict one or more classes of user state (and optionally magnitude(s)) based on multiple heterogeneous inputs. For example, the neural network model may be trained to predict one or more classes of user state based on input that includes: image(s) that each capture a user's face, audio input, and textual input. For instance, the
user state engine 122 may apply, as input to the neural network model, image(s) from camera(s) of sensor(s) 106, audio input(s) from user interface input device(s) 102, and one or more features of thetextual input 201A (and optionally prior textual input); and may generate, over the model, output that indicates a class of user state indicated by the inputs. Although particular inputs and models are described with reference toFIG. 2A , additional or alternative inputs and/or models may be utilized, such as those described elsewhere herein. - The
reply content engine 126 uses the initialtextual output 205A and theuser state information 207A to generatereply content 209A. Thereply content engine 126 provides thereply content 209A for presentation, to one or more users engaged in the dialog, via the user interface input device(s) 104 of those user(s). In some implementations, thereply content 209A is automatically included in the dialog as a reply to thetextual input 201A by the automated assistant. In some implementations, thereply content 209A is suggested by thereply content engine 126 for inclusion in the dialog and is only included in the dialog in response to further user interface input. In some versions of those implementations, the further user interface input is input of the user that provided thetextual input 201A and the inclusion of the reply content in the dialog is as a reply to thetextual input 201A by the automated assistant. In some other versions of those implementations, the further user interface input is input of an additional user involved in the dialog and the inclusion of the reply content in the dialog is as a reply to the textual input by the other user involved in the dialog. - In some implementations, the
reply content engine 126 generates thereply content 209A based on one or more text adaptation models 152B (included in themodels 152 ofFIG. 1 ). In some of those implementations, thereply content engine 126 applies one or more (e.g. all) segments of the initialtextual output 205A as input to one of the text adaptation model(s) 152B, and generates, over the model based on the input, output that indicates content for modifying the initial textual output. Thereply content 209A may include at least some of the content indicated by the output generated over the model and optionally one or more terms of the initialtextual output 205A. For example, where term(s) are indicated by the output, the term(s) indicated by the output may include term(s) for adding to the initialtextual output 205A and/or term(s) for replacing term(s) of the initialtextual output 205A—and thereply content engine 126 may modify the initialtextual output 205A by adding and/or replacing terms as indicated by the output. As another example, terms indicated by the output may be the reply content to be provided (i.e., the output may indicate the initialtextual output 205A as modified by theuser state information 207A). - As one example of a model of text adaptation model(s) 152B, the model may include mappings (data defining an association) of neutral textual segments to segments that are each specific to a user state. In some implementations, a given model may be for a specific user state indicated (explicitly or implicitly) by the
user state information 207A. For example, a “jovial” model may map neutral textual segments to segments that are each more appropriate for a jovial user state (e.g., neutral segment “hi” to more jovial segments “howdy, ;-)” and/or “helllooooo”). In some of those implementations, the given model for the specific user state indicated by theuser state information 207A may be selected based on the given model being assigned a model identifier that indicates the specific user state. In some implementations, a given model may be inclusive of multiple user states, instead of being for a specific user state. For example, a model may map a neutral segment to segment(s) that are each more appropriate for a jovial user state, to segment(s) that are each more appropriate for a stressed user state, and to segment(s) that are each more appropriate for a sad user state. In some of those implementations, the user state specific segments may each be mapped to corresponding user state identifiers and an appropriate user state specific segment selected by thereply content engine 126 based on its user state identifier corresponding to a specific user state indicated by theuser state information 207A. - A segment that is specific to a user state and that is mapped to a corresponding neutral segment may include various content that differs from the neutral segment. For example, the segment that is specific to the user state may include one or more terms that are in addition to terms of the neutral segment, term(s) that replace term(s) of the neutral segment, may omit term(s) that are included in the neutral segment, and/or may include emojis, GIFs, and/or other non-textual content that is not included in the neutral segment. A segment that is specific to a user state and that is mapped to a corresponding neutral segment may additionally and/or alternatively include various style features that differ from those of the neutral segment. Style features include, for example, font size, font type (e.g., Calibri, Times New Roman), and/or font color. Where
reply content 209A is additionally or alternatively to be provided as audible output, style features may additionally and/or alternatively include volume for providing thereply content 209A, a tone for providing thereply content 209A, a pitch for providing thereply content 209A, and/or a voice (e.g., male/female, female 1/female 2/female 3) for providing thereply content 209A. - In some implementations, a model that maps neutral segments to user state specific segments may be generated by analyzing a corpus of multi-user chat dialogs and/or other communications. For example, to generate mappings between neutral segments and “happy” segments, reply segments of the communications may be analyzed (e.g., using a sentiment classifier) to identify the reply segments that are “happy”. Those reply segments could then be processed to remove particular keywords, symbols, tense of the sentence, and/or other features such that there is no sentiment expressed anymore (i.e., to obtain a neutral segment). The neutral segments may then be mapped to their corresponding original/happy reply segments.
- As another example of a model of text adaptation model(s) 152B, the model may include one or more mappings of additional non-textual content that correspond to various user states, and the
reply content engine 126 may generate thereply content 209A by adding the additional non-textual content to the initialtextual output 205A. For example, the text adaptation model(s) 152B may include one or more emojis, GIFs, and/or other images that are each assigned to one or more user state identifiers that corresponds to one or more user states. Thereply content engine 126 may select one or more of the emojis, GIFs, and/or other images to add to the initialtextual output 205A based on the selected content being assigned to a user state identifier that corresponds to theuser state information 207A. The initialtextual output 205A with the added non-textual content may then be provided as thereply content 209A. - As another example of a model of text adaptation model(s) 152B, the model may be a neural network model that can be utilized to transform a textual segment into a user state specific version of the textual segment. In some implementations, the neural network model may be trained for a specific user state and may be selected based on that user state corresponding to the
user state information 207A. In some implementations, the neural network model may be trained for multiple user states and theuser state information 207A may be applied as input over the model (e.g., as side input along with the initial textual input). For example, the initialtextual output 205A and theuser state information 207A may both be applied as input to the model (optionally at different layers of the model) and a modified textual segment generated as output over the model, where the modified textual segment is specific to theuser state information 207A. As described herein, theuser state information 207A that is applied to the model may be in various forms such as a “synthesized” form or as parameterized sensor data. - In some implementations, a neural network model that can be utilized to transform a textual segment into a user state specific version of the textual segment may be trained based on training examples that each include: training example input based on a corresponding neutral text segment and corresponding user state information, and training example output based on a corresponding modified text segment that is a modification of the corresponding neutral text segment in view of the corresponding user state information. The neural network model may be trained by applying the training example inputs to the neural network model and performing backpropagation on the neural network model based on the training example outputs.
-
FIG. 2B illustrates an example of thereply content engine 126generating reply content 209B by utilizing user state information 207B to select from multiple reply options. The multiple reply options may be identified based on one or morereply option models 152C (included in themodels 152 ofFIG. 1 ) and/or based on output fromtext generation engine 124. InFIG. 2B ,textual input 201B that is based on user interface input generated by one or more user interface input device(s) 102 is provided to replycontent engine 126 as part of a dialog. - The
user state engine 122 receivessensor data 203B and/or thetextual input 201B. In some implementations, theuser state engine 122 utilizes one or moreuser state models 152A to generate user state information 207B that indicates user state of the user that submitted thetextual input 201B and/or of one or more additional users that are also involved in the dialog. Thesensor data 203B may be based on sensor(s) of user interface input device(s) and/or of sensor(s) 106 that are in addition to the user interface input device(s) 102. Theuser state engine 122 may utilize various techniques to generate the user state information 207B, such as one or more techniques described with respect toFIG. 1 ,FIG. 2A , and/or elsewhere herein. The user state information 207B may take various forms such as those described herein. - In
FIG. 2B , thereply content engine 126 utilizes thetextual input 201B to determine a plurality of candidate reply options, utilizes the user state information 207B to select one or more of those options, and generates thereply content 209B based on the selected reply option(s). - In some implementations, the
reply content engine 126 provides thetextual input 201B to textgeneration engine 124,text generation engine 124 provides multiple explicit reply options,reply content engine 126 selects one of those reply options based on the user state information 207B, and uses the selected option to generate thereply content 209B (e.g., thereply content 209B may conform strictly to the selected option). As one example, based ontextual input 201B, thetext generation engine 124 may identify multiple explicit reply option outputs for further dialog such as “how can I help you?” and “hope you are having a good day, how can I help?”. Thereply content engine 126 may select one of those options based on the selected option conforming to the user state information 207B more closely than the non-selected option(s). In some of those implementations, thereply content engine 126 may pass each of the multiple reply options through a sentiment classifier (e.g., of user state model(s) 152A) and compare outputs from the classifier to the user state information to select one of the reply options. For example, assume user state information 207B is indicative of a “happy” user state, that output from the sentiment classifier for a first reply option indicates a “happy” user state, and that output from the sentiment classifier for a second reply option indicates a “neutral” user state. In such a situation, thereply content engine 126 may select the first reply option based on it and the user state information 207B both indicating a “happy” user state. - In some implementations, a selected reply option may conform to the user state information 207B even when the selected reply option and the user state information are not associated with the same user state. For example, assume user state information 207B is indicative of a “sad” user state, that output from the sentiment classifier for a first reply option indicates a “sad” user state, and that output from the sentiment classifier for a second reply option indicates an “empathetic” user state. In some of those situations, the
reply content engine 126 may select the second reply option based on an “empathetic” reply being more appropriate for a “sad” user state than is a “sad” reply. Thereply content engine 126 may optionally rely on one or more mappings between user states indicated by user state information 207B and user states indicated by reply options in selecting a reply option. For instance, for a “sad” user state the mappings may define that “empathetic” reply options are most appropriate, followed by “neutral” reply options, etc. - In some implementations, the
reply content engine 126 additionally or alternatively identifies multiple reply option categories from reply option model(s) 152C based on thetextual input 201B. Thereply content engine 126 may then select one of those reply option categories based on the user state information 207B. For example, thetextual input 201B may be mapped to multiple reply option categories in reply option model(s) 152C, such as reply option categories of “tell a joke”, “ask a question”, “divert the user to a human”, “divert the user to another chat bot”, etc. In some implementations, thereply content engine 126 may utilize a rules-based approach to select from the multiple reply option categories based on user state information 207B. For example, a “divert the user to a human” reply option category may be assigned to a user state identifier that causes it to be selected when the user state information 207B indicates a certain type and/or magnitude of user state (e.g., “very upset”). Also, for example, when selecting between a “tell a joke” and “ask a question” reply option categories, the rules-based approach may cause the “tell a joke” reply option category to be selected for certain user states and the “ask a question” category to be selected for certain other user states. - In some implementations, the reply option model(s) 152C may additionally or alternatively include machine learning model(s) trained to select an appropriate reply option based on user state information. For example, a machine learning model may be trained to select a reply option category based on training examples generated based on past dialogs with the
automated assistant 120 and/or other automated assistants. Each of the training examples may include, as training example input, user state information for a corresponding textual input and, as training example output, the corresponding reply option category for the corresponding textual input. Each of the training examples may be labeled as a positive or negative training examples (and/or weighted) based on various signals, such as length of further textual input (e.g., if the further textual input is longer than normal it may indicate a positive/desired reply option), whether further textual input was received (e.g., if the conversation was abandoned it may indicate a negative/undesired reply option), explicit user feedback, etc. -
FIG. 2C illustrates an example of thereply content engine 126 causing a search to be issued based on textual input 201C and based onuser state information 207C. Thereply content engine 126 receives search result(s) based on the search, and generatesreply content 209C that is based on one or more of the received search results. InFIG. 2C , textual input 201C that is based on user interface input generated by one or more user interface input device(s) 102 is provided to replycontent engine 126 as part of a dialog. - The
user state engine 122 receivessensor data 203C and/or the textual input 201C and utilizes one or moreuser state models 152A to generateuser state information 207C that indicates user state of the user that submitted the textual input 201C and/or of one or more additional users that are also involved in the dialog. Thesensor data 203C may be based on sensor(s) of user interface input device(s) and/or of sensor(s) 106 that are in addition to the user interface input device(s) 102. Theuser state engine 122 may utilize various techniques to generate theuser state information 207C, such as one or more techniques described with respect toFIG. 1 ,FIG. 2A , and/or elsewhere herein. Theuser state information 207C may take various forms such as those described herein. - In
FIG. 2C , thereply content engine 126 causes a search to be issued based on textual input 201C anduser state information 207C. In particular, thereply content engine 126 causes thesearch system 130 to issue a search ofsearch databases 154 based on one or more search parameters determined based on the textual input 201C and based on one or more search parameters determined based on theuser state information 207C. In some of those implementations, one or more of the parameters may individually be based on both the textual input 201C and theuser state information 207C. Thesearch system 130 returns responsive search result(s) and thereply content engine 126 selects one or more of the search results for including (in whole or in part) inreply content 209C. - Determining search parameters based on the
user state information 207C may include selecting, based on theuser state information 207C, one or more terms and/or other parameters used for identifying content in the search and/or submitting the search with an indication of theuser state information 207C for ranking one or more of the search results based on theuser state information 207C. For example, for received textual input of “I'm bored” with a user state indicative of “stressed”, a search may be issued for “relaxing activities nearby”, where “relaxing” is chosen based on the “stressed” user state. Also, for example, for received textual input of “I'm hungry” with a user state indicative of “stressed”, a search for “restaurants nearby” may be issued and the search results ranked based at least in part on whether they have a “relaxing” or similar attribute. For instance, search results that do not have a “relaxing” attribute may be filtered out of the search results. - The
search system 130 searches one ormore search databases 154 to identify content that is responsive to the search parameters provided by thereply content engine 126. In some implementations, thesearch databases 154 include database(s) that index publicly available content and/or database(s) that index content that is private to the user. Thesearch system 130 may utilize thesearch databases 154 to identify responsive content and may generate search results based on the identified responsive content. In some implementations, one or more of thesearch databases 154 may be remote from theautomated assistant 120 and/or any separate client computing device(s), and/or one or more of thesearch databases 154 may be local to theautomated assistant 120 and/or any separate client computing device(s). In this specification, the term “database” is used to refer to any collection of structured or unstructured data stored in one or more computer readable media. - The
search system 130 may utilize various techniques in searching thesearch databases 154 in response to textual input. In some implementations, thesearch system 130 may search one or more of thesearch databases 154 based on search parameter(s) that conform strictly to the textual input 201C and optionally based on search parameter(s) that are added based on theuser state information 207C. In some implementations, thesearch system 130 may search one or more of thesearch databases 154 based on one or more search parameters that are based on, but that do not necessarily conform strictly to, the textual input 201C. For example, for textual input of “restaurants nearby”, thesearch system 130 may search one or more of thesearch databases 154 based on a search parameter that restricts thesearch databases 154 to “points of interests” databases and/or content to “restaurant” content, and based on a search parameter that restricts content to content that is within a threshold distance of a current location of the user. As yet another example, for textual input of “my photos”, thesearch system 130 may search one or more of thesearch databases 154 based on a search parameter that restricts thesearch databases 154 to databases that are personal to the user and/or content to “image” content. - The
search system 130 may include a ranking engine that ranks identified content using one or more ranking signals, such as popularity of the content, a degree of matching between the search parameters and the content, attributes of the user (e.g., a location of the user, a primary language of the user), and/or based on search attributes that are based on theuser state information 207C. Ranking the identified content may include calculating scores or other measures for the identified content and using those measures to indicate which content items to utilize as search results included inreply content 209C. As one example, thesearch system 130 may filter out or otherwise demote one or more content items based on determining those content items don't conform to theuser state information 207C. As another example, thesearch system 130 may select or otherwise promote one or more content items based on determining those content items conform to theuser state information 207C. - The
search system 130 uses the identified responsive content ranked by the ranking engine to generate search results. The search results include search results corresponding to the content that is responsive to the search issued based on the textual input. For example, each of the search results can include a title or other synopsis of a responsive content item, a summary of the content item, a link to the responsive content item, other information related to the responsive content item, and/or even the entirety of the content item. As one example, the summary of a news story content item may include a particular “snippet” or section of the news story. Also, for example, for a search result associated with an image, the search result may include a reduced size display of the image, a title associated with the image, and/or a link to the image. Also, for example, for a search result associated with a video, the search result may include an image from the video, a segment of the video, a title of the video, and/or a link to the video. - Although the
search system 130 is illustrated as separate from theautomated assistant 120 inFIG. 1 , in some implementations one or more aspects of thesearch system 130 may be incorporated in theautomated assistant 120. For example, all or aspects of the ranking engine of the search system 130 (e.g., at least the aspect that ranks search results based on user state information) may be incorporated in whole or in part by theautomated assistant 120. - Referring now to
FIGS. 3A-9 , various examples of implementations of theautomated assistant 120 are described.FIGS. 3A, 3B, 4A, 4B, and 6-9 each illustrate acomputing device 110 with adisplay screen 140 displaying graphical user interfaces with examples of dialog that may occur between a user of thecomputing device 110, theautomated assistant 120, and/or one or more additional users according to implementations disclosed herein. One or more aspects of theautomated assistant 120 may be implemented on thecomputing device 110 and/or on one or more computing devices that are in network communication with thecomputing device 110. - The
display screen 140 ofFIGS. 3A, 3B, 4A, 4B, and 6-9 further includes a textualreply interface element 188 that the user may select to generate user interface input via a virtual keyboard and a voicereply interface element 189 that the user may select to generate user interface input via a microphone. In some implementations, the user may generate user interface input via the microphone without selection of the voicereply interface element 189. For example, during the dialog, active monitoring for audible user interface input via the microphone may occur to obviate the need for the user to select the voicereply interface element 189. In some of those and/or in other implementations, the voicereply interface element 189 may be omitted. Moreover, in some implementations, the textualreply interface element 188 may additionally and/or alternatively be omitted (e.g., the user may only provide audible user interface input). Thedisplay screen 140 ofFIGS. 3A, 3B, 4A, 4B, and 6-9 also includessystem interface elements computing device 110 to perform one or more actions. -
FIG. 5 illustrates acomputing device 110 that includes one or more microphones and one or more speakers and illustrates examples of dialog that may occur, via the microphone(s) and speaker(s), between auser 101 of thecomputing device 510 and theautomated assistant 120 according to implementations described herein. One or more aspects of theautomated assistant 120 may be implemented on thecomputing device 510 and/or on one or more computing devices that are in network communication with thecomputing device 510. - In
FIG. 3A , the user providestextual input 321A as part of a dialog between the user and theautomated assistant 120. Theautomated assistant 120 determines user state information for the user and provides areply 331A that is based on thetextual input 321A and the user state information. For example, the user state information may be indicative of the user being “sad” and thereply 331A may be generated based on modifying initial textual output in view of that user state information. For instance, thetext generation engine 124 may generate initial textual output of “Hi, tell me about your day”, and, based on the user state information, thereply content engine 126 may replace “tell me about” with “how was”. - The user provides
textual input 322A in furtherance of the dialog. Theautomated assistant 120 provides areply 332A that is based on the previously determined user state information for the user and/or based on updated user state information (e.g., determined based at least in part on thetextual input 322A and/or sensor data received after the providing of thetextual input 321A). Theautomated assistant 120 generates thereply 332A based on thetextual input 322A (and optionally based on priortextual inputs 321A and/or 331A) and based on the user state information. For example, the user state information may still be indicative of the user being “sad” and thereply 332A may be generated based on modifying initial textual output in view of that user state information. For instance, thetext generation engine 124 may generate initial textual output of “Nice to hear. Anything I can help with?”, and, based on the user state information, thereply content engine 126 may replace “Nice” with “Sorry” and add “Let me know if there is” before “anything”. -
FIG. 3B illustrates an example of how the dialog ofFIG. 3A may differ according to user state information of the user that differs from that ofFIG. 3A . InFIG. 3B , thetextual input 321B is the same astextual input 321A ofFIG. 3A and thetextual input 322B is the same astextual input 322A ofFIG. 3A . However, replies 331B and 332B differ based on different user state information being determined for the user. For example, inFIG. 3B the user state information may be indicative of the user being “happy”. Thereplies reply 331B thetext generation engine 124 may generate initial textual output of “Hi, tell me about your day”, and, based on the user state information, thereply content engine 126 may replace “Hi” with “howdy” and add the smiley face emoji. Also, for instance, forreply 332B thetext generation engine 124 may generate initial textual output of “Nice to hear. Anything I can help with?”, and, based on the user state information, thereply content engine 126 may replace “Nice” with “Good”. - In
FIG. 4A , the user providestextual input 421A as part of a dialog between the user and theautomated assistant 120. Theautomated assistant 120 determines user state information for the user and provides areply 431A that is based on thetextual input 421A and the user state information. For example, the user state information may be indicative of the user being “jovial” and thereply 431A may be generated based on identifying multiple reply options, and selecting one of the reply options in view of that user state information. For instance, thereply content engine 126 may identify at least a “tell a joke” option and a “provide information on time management” option based on thetextual input 421A and may select the “tell the joke” option based on the “jovial” user state information. For example, the “tell the joke” option may be assigned to a jovial user state identifier in one or more reply option models. The user provides affirmativetextual input 422A in furtherance of the dialog and theautomated assistant 120 provides areply 432A. The reply is a joke that is selected from a corpus of jokes based on thereply content engine 126 selecting of the “tell the joke” option. -
FIG. 4B illustrates an example of how the dialog ofFIG. 4A may differ according to user state information of the user that differs from that ofFIG. 4A . InFIG. 4B , thetextual input 421B is the same astextual input 421A ofFIG. 4A and thetextual input 422B is the same astextual input 422A ofFIG. 4A . However, replies 431B and 432B differ from thereplies FIG. 4A , and differ based on different user state information being determined for the user. For example, inFIG. 4B the user state information may be indicative of the user being “stressed”. Thereply 431B may be generated based on thereply content engine 126 identifying that a “provide information on time management” reply option conforms to the user state information. Thereply content engine 126 may further provide thereply 432B based on the affirmativetextual input 422B and based on selection of the “provide information on time management reply option”. - In
FIG. 5 , thetextual inputs textual inputs FIG. 3A . Moreover, thereplies automated assistant 120 are the same as thereplies FIG. 3A . InFIG. 5 , thetextual inputs computing device 510 via microphone(s) of thecomputing device 510 and the computing device provides thereplies computing device 510. In some implementations, thereplies replies FIG. 3A . In some implementations, the audible presentation of thereplies audible replies - In
FIG. 6 , the user providestextual input 621 as part of a dialog between the user and theautomated assistant 120. Theautomated assistant 120 determines user state information for the user and provides areply 631 that is based on thetextual input 621 and the user state information. For example, the user state information may be indicative of the user being “stressed” and thereply 631 may be generated based on issuing a search that includes one or more search parameters that are based on thetextual input 621 and that includes one or more search parameters that are based on that user state information. For example, thereply content engine 126 may cause thesearch system 130 to issue a search for “relaxing activities nearby”, where “relaxing” is chosen based on the “stressed” user state and/or may cause thesearch system 130 to rank the search results based at least in part on whether they have a “relaxing” or similar attribute. For instance, search results that do not have a “relaxing” attribute may be filtered out of the search results. Thereply 631 includes three of the search results identified in response to the search. Each of the search results relates to a “relaxing” location. Selection of one of the search results by the user may cause additional information about the corresponding location to be presented to the user. In some implementations, the additional information may be presented as a continuation of the ongoing dialog (e.g., added in the graphical user interface ofFIG. 6 to the transcript of the dialog as further output from the automated assistant 120). In some other implementations, selection of one of the search results may cause the additional information to be presented to the user in a separate application and/or interface. For example, selection of one of the search results may cause a separate application to navigate to a link associated with the search result. - In
FIG. 7 , the user providestextual input 721 as part of the dialog with theautomated assistant 120 and theautomated assistant 120 provides areply 731 that is responsive to thetextual input 721. In some implementations, thereply 731 may not be determined based on user state information of the user. The user then providestextual input 722 as a continuation of the dialog and in response to thereply 731. Theautomated assistant 120 may determine that the user state information of the user during or before providing thetextual input 722 is indicative of the user being “frustrated”, “angry”, and/or “stressed”. In response to such a determination, theautomated assistant 120 may provide the user with areply 732 that includes an interactive element that can be selected to speak with a human. In some implementations, the interactive element may be provided in response to determining such user state information and in response to determining that such user state information indicates at least a threshold degree of change (in direction and/or magnitude) from prior (e.g., immediately prior) user state information of the user. Compare this to a situation where the user state information of the user during or before providing thetextual input 722 is indicative of the user being “neutral” or “happy”. In some of those implementations, thereply 732 may instead be tailored to helping the user resolve the request oftextual input 721 without presenting the option to speak with a human. - Selection of the “Select to Speak with Human” interactive element may cause a network connection to be established between the
computing device 110 and an additional computing device of a human operator, and further communication to occur between the user and the human operator via the network connection. In some implementations, the further communication may be via the graphical user interface ofFIG. 7 . In some other implementations, selection of the interactive element may cause a separate application and/or interface to be utilized for the further communication. -
FIGS. 8 and 9 present two examples of dialogs that include the user of thecomputing device 110 and an additional user (“Joe”). AlthoughFIGS. 8 and 9 only illustrate one additional user, it is understood that multiple additional users may be engaged in a dialog. - In
FIG. 8 , the user of thecomputing device 110 providestextual input 821 in the dialog and the additional user (“Joe”) provides aresponsive reply 841 in the dialog. The user of thecomputing device 110 then provides an additionaltextual input 822 that is directed to theautomated assistant 120. The textual input is directed to theautomated assistant 120 by the inclusion of “@AS”. In some implementations, additional or alternative techniques for invoking theautomated assistant 120 may be utilized and/or theautomated assistant 120 may optionally provide responsive content without explicit invocation. - In response to the
textual input 822, the automated assistant provides areply 831 that is based on thetextual input 822 and based on the user state information of the user of thecomputing device 110 and/or of the additional user (“Joe”). For example, the user state information may be indicative of both the user of thecomputing device 110 and the additional user being “giddy”, and thereply 831 may be generated based on issuing a search that includes one or more search parameters that are based on thetextual input 822 and that includes one or more search parameters that are based on that user state information. For example, thereply content engine 126 may cause a search to be issued for “funny events nearby”, where “funny” is chosen based on the “giddy” user state and/or may cause the search results to be ranked based at least in part on whether they have a “funny” or similar attribute. For instance, search results that do not have a “funny” attribute may be filtered out of the search results. Thereply 831 includes three of the search results identified in response to the search. Each of the search results relates to a “funny” event. Selection of one of the search results by the user may cause additional information about the corresponding event to be presented to the user. In some implementations, the additional information may be presented as a continuation of the ongoing dialog. In some other implementations, selection of one of the search results may cause the additional information to be presented to the selecting user in a separate application and/or interface. - In
FIG. 9 , the additional user (“Joe”) providestextual input 941 that is a message directed to the user of thecomputing device 110. Theautomated assistant 120 generatesresponsive reply content 931. However, instead of automatically incorporating theresponsive reply content 931 into the dialog, theautomated assistant 120 provides the responsive reply content to the user of the computing device 110 (optionally without separately providing it to the additional user). The user of thecomputing device 110 may optionally select one of the two reply options listed in thereply content 931 to utilize the selected option as a reply, by the user of thecomputing device 110, to thetextual input 941. Selection of one of the options may cause the selected option to be automatically included in the transcript of the dialog presented in the graphical user interface ofFIG. 9 , may populate the text of the selected option in the textual reply interface element 188 (for optional editing by the user prior to explicit submission by the user), or may otherwise enable editing by the user of thecomputing device 110 and/or require an additional explicit input of the user of thecomputing device 110 prior to inclusion in the transcript of the dialog. - The
automated assistant 120 may generate the reply options ofreply content 931 based on determined user state information for the additional user (“Joe”). For example, the user state information for the additional user may indicate that he is “sad” and, as a result, thereply content engine 126 may generate reply content that conforms to that user state information. For instance, thetext generation engine 124 may generate an initial textual output based on thetextual input 941, and thereply content engine 126 may modify that initial textual input based on the user state information for the additional user. -
FIG. 10 is a flowchart illustrating anexample method 1000 according to implementations disclosed herein. For convenience, the operations of the flow chart are described with reference to a system that performs the operations. This system may include various components of various computer systems, such as one or more components ofautomated assistant 120 and/orsearch system 130. Moreover, while operations ofmethod 1000 are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted or added. - At block 1010, the system receives textual input. The system receives the textual input as part of a dialog that includes a user, an automated assistant, and optionally one or more additional users. The textual input may be generated by the user based on user interface input generated by one or more user interface input devices, such as a microphone and/or virtual keyboard.
- At block 1020, the system determines user state information for the user that submitted the textual input and/or for one or more additional users. The system may utilize various inputs and/or various techniques in determining the user state information. For example, the system may determine the user state information for the user based on sensor data of user interface input device(s) of the user, other sensor(s) of the user, the textual input received at block 1010, and/or prior textual input received from the user. In some implementations, block 1020 may occur prior to block 1010.
- At block 1030, the system generates reply content based on both the textual input and the user state information. The system may generate various types of reply content and may utilize various techniques to generate the reply content based on both the textual input and the user state information. For example, the system may utilize one or more of the techniques described with respect to earlier figures.
- At
block 1040, the system provides the reply content for inclusion in the dialog. The system provides the reply content in response to the user interface input of block 1010. In some implementations, the system automatically incorporates the reply content in the dialog as a reply, by the system, to the textual input of block 1010. For example, the system may provide one or more commands to one or more computing devices of users engaged in the dialog to cause those computing devices to present the reply content (e.g., to incorporate the reply content in a transcript of the dialog). In some implementations, the system suggests the reply content to the user for inclusion in the dialog (e.g., by a general indication that reply content is available), and only includes the reply content in the dialog in response to further user interface input. In some implementations, the system suggests the reply content to an additional user involved in the dialog as a reply, by the additional user, to the textual input of block 1010. - Multiple iterations of
blocks 1010, 1020, 1030, and 1040 may occur during a dialog to enable reply content to be adapted to various user states and/or textual inputs that may occur throughout the dialog. Althoughblocks 1010, 1020, 1030, and 1040 are illustrated in a particular order, it is understood the order may be altered, one or more blocks may be performed in parallel, and/or one or more blocks may only be selectively performed. -
FIG. 11 is a block diagram of anexample computing device 1110 that may optionally be utilized to perform one or more aspects of techniques described herein. In some implementations, one or more of a client computing device, automatedassistant 120, and/or other component(s) may comprise one or more components of theexample computing device 1110. -
Computing device 1110 typically includes at least oneprocessor 1114 which communicates with a number of peripheral devices viabus subsystem 1112. These peripheral devices may include astorage subsystem 1124, including, for example, amemory subsystem 1125 and afile storage subsystem 1126, userinterface output devices 1120, userinterface input devices 1122, and anetwork interface subsystem 1116. The input and output devices allow user interaction withcomputing device 1110.Network interface subsystem 1116 provides an interface to outside networks and is coupled to corresponding interface devices in other computing devices. - User
interface input devices 1122 may include a keyboard, pointing devices such as a mouse, trackball, touchpad, or graphics tablet, a scanner, a touchscreen incorporated into the display, audio input devices such as voice recognition systems, microphones, and/or other types of input devices. In general, use of the term “input device” is intended to include all possible types of devices and ways to input information intocomputing device 1110 or onto a communication network. - User
interface output devices 1120 may include a display subsystem, a printer, a fax machine, or non-visual displays such as audio output devices. The display subsystem may include a cathode ray tube (CRT), a flat-panel device such as a liquid crystal display (LCD), a projection device, or some other mechanism for creating a visible image. The display subsystem may also provide non-visual display such as via audio output devices. In general, use of the term “output device” is intended to include all possible types of devices and ways to output information fromcomputing device 1110 to the user or to another machine or computing device. -
Storage subsystem 1124 stores programming and data constructs that provide the functionality of some or all of the modules described herein. For example, thestorage subsystem 1124 may include the logic to perform selected aspects of the method ofFIG. 10 . - These software modules are generally executed by
processor 1114 alone or in combination with other processors.Memory 1125 used in thestorage subsystem 1124 can include a number of memories including a main random access memory (RAM) 1130 for storage of instructions and data during program execution and a read only memory (ROM) 1132 in which fixed instructions are stored. Afile storage subsystem 1126 can provide persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive along with associated removable media, a CD-ROM drive, an optical drive, or removable media cartridges. The modules implementing the functionality of certain implementations may be stored byfile storage subsystem 1126 in thestorage subsystem 1124, or in other machines accessible by the processor(s) 1114. -
Bus subsystem 1112 provides a mechanism for letting the various components and subsystems ofcomputing device 1110 communicate with each other as intended. Althoughbus subsystem 1112 is shown schematically as a single bus, alternative implementations of the bus subsystem may use multiple busses. -
Computing device 1110 can be of varying types including a workstation, server, computing cluster, blade server, server farm, or any other data processing system or computing device. Due to the ever-changing nature of computers and networks, the description ofcomputing device 1110 depicted inFIG. 11 is intended only as a specific example for purposes of illustrating some implementations. Many other configurations ofcomputing device 1110 are possible having more or fewer components than the computing device depicted inFIG. 11 . - In situations in which certain implementations discussed herein may collect or use personal information about users (e.g., user data extracted from other electronic messages, information about a user's social network, a user's location, a user's biometric information, and a user's activities and demographic information), users are provided with one or more opportunities to control whether information is collected, whether the personal information is stored, whether the personal information is used, and how the information is collected about the user, stored and used. That is, the systems and methods discussed herein collect, store and/or use user personal information only upon receiving explicit authorization from the relevant users to do so. For example, a user is provided with control over whether programs or features collect user information about that particular user or other users relevant to the program or feature. Each user for which personal information is to be collected is presented with one or more options to allow control over the information collection relevant to that user, to provide permission or authorization as to whether the information is collected and as to which portions of the information are to be collected. For example, users can be provided with one or more such control options over a communication network. In addition, certain data may be treated in one or more ways before it is stored or used so that personally identifiable information is removed. As one example, a user's identity may be treated so that no personally identifiable information can be determined. As another example, a user's geographic location may be generalized to a larger region so that the user's particular location cannot be determined.
- While several implementations have been described and illustrated herein, a variety of other means and/or structures for performing the function and/or obtaining the results and/or one or more of the advantages described herein may be utilized, and each of such variations and/or modifications is deemed to be within the scope of the implementations described herein. More generally, all parameters, dimensions, materials, and configurations described herein are meant to be exemplary and that the actual parameters, dimensions, materials, and/or configurations will depend upon the specific application or applications for which the teachings is/are used. Those skilled in the art will recognize, or be able to ascertain using no more than routine experimentation, many equivalents to the specific implementations described herein. It is, therefore, to be understood that the foregoing implementations are presented by way of example only and that, within the scope of the appended claims and equivalents thereto, implementations may be practiced otherwise than as specifically described and claimed. Implementations of the present disclosure are directed to each individual feature, system, article, material, kit, and/or method described herein. In addition, any combination of two or more such features, systems, articles, materials, kits, and/or methods, if such features, systems, articles, materials, kits, and/or methods are not mutually inconsistent, is included within the scope of the present disclosure.
Claims (26)
Priority Applications (7)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/252,049 US10192551B2 (en) | 2016-08-30 | 2016-08-30 | Using textual input and user state information to generate reply content to present in response to the textual input |
PCT/US2017/049225 WO2018044956A1 (en) | 2016-08-30 | 2017-08-29 | Using textual input and user state information to generate reply content to present in response to the textual input |
CN201780053486.7A CN110023926B (en) | 2016-08-30 | 2017-08-29 | Generating reply content to be presented in response to text input using text input and user state information |
EP17768889.2A EP3507718B1 (en) | 2016-08-30 | 2017-08-29 | Using textual input and user state information to generate reply content to present in response to the textual input |
US16/258,035 US10650820B2 (en) | 2016-08-30 | 2019-01-25 | Using textual input and user state information to generate reply content to present in response to the textual input |
US16/853,143 US11264032B2 (en) | 2016-08-30 | 2020-04-20 | Using textual input and user state information to generate reply content to present in response to the textual input |
US17/592,042 US11929072B2 (en) | 2016-08-30 | 2022-02-03 | Using textual input and user state information to generate reply content to present in response to the textual input |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/252,049 US10192551B2 (en) | 2016-08-30 | 2016-08-30 | Using textual input and user state information to generate reply content to present in response to the textual input |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/258,035 Continuation US10650820B2 (en) | 2016-08-30 | 2019-01-25 | Using textual input and user state information to generate reply content to present in response to the textual input |
Publications (2)
Publication Number | Publication Date |
---|---|
US20180061400A1 true US20180061400A1 (en) | 2018-03-01 |
US10192551B2 US10192551B2 (en) | 2019-01-29 |
Family
ID=59901573
Family Applications (4)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/252,049 Active US10192551B2 (en) | 2016-08-30 | 2016-08-30 | Using textual input and user state information to generate reply content to present in response to the textual input |
US16/258,035 Active US10650820B2 (en) | 2016-08-30 | 2019-01-25 | Using textual input and user state information to generate reply content to present in response to the textual input |
US16/853,143 Active US11264032B2 (en) | 2016-08-30 | 2020-04-20 | Using textual input and user state information to generate reply content to present in response to the textual input |
US17/592,042 Active 2036-12-16 US11929072B2 (en) | 2016-08-30 | 2022-02-03 | Using textual input and user state information to generate reply content to present in response to the textual input |
Family Applications After (3)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/258,035 Active US10650820B2 (en) | 2016-08-30 | 2019-01-25 | Using textual input and user state information to generate reply content to present in response to the textual input |
US16/853,143 Active US11264032B2 (en) | 2016-08-30 | 2020-04-20 | Using textual input and user state information to generate reply content to present in response to the textual input |
US17/592,042 Active 2036-12-16 US11929072B2 (en) | 2016-08-30 | 2022-02-03 | Using textual input and user state information to generate reply content to present in response to the textual input |
Country Status (4)
Country | Link |
---|---|
US (4) | US10192551B2 (en) |
EP (1) | EP3507718B1 (en) |
CN (1) | CN110023926B (en) |
WO (1) | WO2018044956A1 (en) |
Cited By (110)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20180287968A1 (en) * | 2017-04-03 | 2018-10-04 | Microsoft Technology Licensing, Llc | Conversation Bot Discovery and Response Fusion |
US20180357310A1 (en) * | 2017-06-09 | 2018-12-13 | Google Inc. | Validate modification of audio-based computer program output |
CN109032381A (en) * | 2018-08-01 | 2018-12-18 | 平安科技（深圳）有限公司 | A kind of input method based on context, device, storage medium and terminal |
US20190147867A1 (en) * | 2017-11-10 | 2019-05-16 | Hyundai Motor Company | Dialogue system and method for controlling thereof |
US10318871B2 (en) * | 2005-09-08 | 2019-06-11 | Apple Inc. | Method and apparatus for building an intelligent automated assistant |
WO2019172878A1 (en) * | 2018-03-05 | 2019-09-12 | Google Llc | Transitioning between prior dialog contexts with automated assistants |
US20190362568A1 (en) * | 2018-05-23 | 2019-11-28 | Google Llc | Providing a communications channel between instances of automated assistants |
WO2019236372A1 (en) * | 2018-06-03 | 2019-12-12 | Google Llc | Selectively generating expanded responses that guide continuance of a human-to-computer dialog |
CN110895558A (en) * | 2018-08-23 | 2020-03-20 | 北京搜狗科技发展有限公司 | Dialog reply method and related device |
US10614122B2 (en) | 2017-06-09 | 2020-04-07 | Google Llc | Balance modifications of audio-based computer program output using a placeholder field based on content |
WO2020087534A1 (en) * | 2018-11-02 | 2020-05-07 | Microsoft Technology Licensing, Llc | Generating response in conversation |
US10652170B2 (en) | 2017-06-09 | 2020-05-12 | Google Llc | Modification of audio-based computer program output |
CN111414733A (en) * | 2020-03-18 | 2020-07-14 | 联想(北京)有限公司 | Data processing method and device and electronic equipment |
US10720160B2 (en) | 2018-06-01 | 2020-07-21 | Apple Inc. | Voice interaction at a primary device to access call functionality of a companion device |
WO2020157731A1 (en) * | 2019-02-03 | 2020-08-06 | International Business Machines Corporation | Performing multi-objective tasks via primal networks trained with dual networks |
US10741185B2 (en) | 2010-01-18 | 2020-08-11 | Apple Inc. | Intelligent automated assistant |
US10741181B2 (en) | 2017-05-09 | 2020-08-11 | Apple Inc. | User interface for correcting recognition errors |
JP2020123359A (en) * | 2018-05-23 | 2020-08-13 | グーグル エルエルシー | Providing communications channel between instances of automated assistants |
US10748546B2 (en) | 2017-05-16 | 2020-08-18 | Apple Inc. | Digital assistant services based on device capabilities |
US10839159B2 (en) | 2018-09-28 | 2020-11-17 | Apple Inc. | Named entity normalization in a spoken dialog system |
US20200364413A1 (en) * | 2017-09-20 | 2020-11-19 | Samsung Electronics Co., Ltd. | Electronic device and control method thereof |
US10878809B2 (en) | 2014-05-30 | 2020-12-29 | Apple Inc. | Multi-command single utterance input method |
US10909171B2 (en) | 2017-05-16 | 2021-02-02 | Apple Inc. | Intelligent automated assistant for media exploration |
US20210035576A1 (en) * | 2018-02-07 | 2021-02-04 | Samsung Electronics Co., Ltd. | Electronic device for chatting with chatbot and operation method thereof |
US10930282B2 (en) | 2015-03-08 | 2021-02-23 | Apple Inc. | Competing devices responding to voice triggers |
US20210056489A1 (en) * | 2019-08-23 | 2021-02-25 | Nokia Technologies Oy | Controlling submission of content |
US10942703B2 (en) | 2015-12-23 | 2021-03-09 | Apple Inc. | Proactive assistance based on dialog communication between devices |
US10978090B2 (en) | 2013-02-07 | 2021-04-13 | Apple Inc. | Voice trigger for a digital assistant |
US11009970B2 (en) | 2018-06-01 | 2021-05-18 | Apple Inc. | Attention aware virtual assistant dismissal |
US11010127B2 (en) | 2015-06-29 | 2021-05-18 | Apple Inc. | Virtual assistant for media playback |
US11010561B2 (en) | 2018-09-27 | 2021-05-18 | Apple Inc. | Sentiment prediction from textual data |
US11037565B2 (en) | 2016-06-10 | 2021-06-15 | Apple Inc. | Intelligent digital assistant in a multi-tasking environment |
US11070949B2 (en) | 2015-05-27 | 2021-07-20 | Apple Inc. | Systems and methods for proactively identifying and surfacing relevant content on an electronic device with a touch-sensitive display |
US11087759B2 (en) | 2015-03-08 | 2021-08-10 | Apple Inc. | Virtual assistant activation |
CN113378583A (en) * | 2021-07-15 | 2021-09-10 | 北京小米移动软件有限公司 | Dialogue reply method and device, dialogue model training method and device, and storage medium |
US11120372B2 (en) | 2011-06-03 | 2021-09-14 | Apple Inc. | Performing actions associated with task items that represent tasks to perform |
US11126400B2 (en) | 2015-09-08 | 2021-09-21 | Apple Inc. | Zero latency digital assistant |
US11133008B2 (en) | 2014-05-30 | 2021-09-28 | Apple Inc. | Reducing the need for manual start/end-pointing and trigger phrases |
US11140099B2 (en) | 2019-05-21 | 2021-10-05 | Apple Inc. | Providing message response suggestions |
US11151324B2 (en) | 2019-02-03 | 2021-10-19 | International Business Machines Corporation | Generating completed responses via primal networks trained with dual networks |
US11152002B2 (en) | 2016-06-11 | 2021-10-19 | Apple Inc. | Application integration with a digital assistant |
US11169616B2 (en) | 2018-05-07 | 2021-11-09 | Apple Inc. | Raise to speak |
US11170166B2 (en) | 2018-09-28 | 2021-11-09 | Apple Inc. | Neural typographical error modeling via generative adversarial networks |
US20210383809A1 (en) * | 2017-05-03 | 2021-12-09 | Google Llc | Proactive incorporation of unsolicited content into human-to-computer dialogs |
CN113785354A (en) * | 2019-05-06 | 2021-12-10 | 谷歌有限责任公司 | Selectively activating on-device speech recognition and using recognized text in selectively activating NLUs on devices and/or fulfillment on devices |
US11210345B2 (en) * | 2017-11-03 | 2021-12-28 | Google Llc | Using distributed state machines for human-to-computer dialogs with automated assistants to protect private data |
US11217251B2 (en) | 2019-05-06 | 2022-01-04 | Apple Inc. | Spoken notifications |
JP2022501730A (en) * | 2018-09-27 | 2022-01-06 | グーグル エルエルシーGoogle LLC | Web page analysis to facilitate automatic navigation |
US11227589B2 (en) | 2016-06-06 | 2022-01-18 | Apple Inc. | Intelligent list reading |
US20220020365A1 (en) * | 2020-07-15 | 2022-01-20 | Google Llc | Automated assistant with audio presentation interaction |
US11237797B2 (en) | 2019-05-31 | 2022-02-01 | Apple Inc. | User activity shortcut suggestions |
US20220036887A1 (en) * | 2018-12-07 | 2022-02-03 | Sony Group Corporation | Information processing device, information processing method, and program |
US11257504B2 (en) | 2014-05-30 | 2022-02-22 | Apple Inc. | Intelligent assistant for home automation |
US11269678B2 (en) | 2012-05-15 | 2022-03-08 | Apple Inc. | Systems and methods for integrating third party services with a digital assistant |
CN114172856A (en) * | 2021-11-30 | 2022-03-11 | 中国平安财产保险股份有限公司 | Automatic message reply method, device, equipment and storage medium |
US11281867B2 (en) * | 2019-02-03 | 2022-03-22 | International Business Machines Corporation | Performing multi-objective tasks via primal networks trained with dual networks |
US11289073B2 (en) | 2019-05-31 | 2022-03-29 | Apple Inc. | Device text to speech |
US11294784B1 (en) * | 2019-09-26 | 2022-04-05 | Amazon Technologies, Inc. | Techniques for providing predictive interface elements |
US11307752B2 (en) | 2019-05-06 | 2022-04-19 | Apple Inc. | User configurable task triggers |
US11348573B2 (en) | 2019-03-18 | 2022-05-31 | Apple Inc. | Multimodality in digital assistant systems |
US11348582B2 (en) | 2008-10-02 | 2022-05-31 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
US11360641B2 (en) | 2019-06-01 | 2022-06-14 | Apple Inc. | Increasing the relevance of new available information |
US11380310B2 (en) | 2017-05-12 | 2022-07-05 | Apple Inc. | Low-latency intelligent automated assistant |
US11388291B2 (en) | 2013-03-14 | 2022-07-12 | Apple Inc. | System and method for processing voicemail |
US11405466B2 (en) | 2017-05-12 | 2022-08-02 | Apple Inc. | Synchronization and task delegation of a digital assistant |
US11423899B2 (en) * | 2018-11-19 | 2022-08-23 | Google Llc | Controlling device output according to a determined condition of a user |
US11423886B2 (en) | 2010-01-18 | 2022-08-23 | Apple Inc. | Task flow identification based on user intent |
US11423908B2 (en) | 2019-05-06 | 2022-08-23 | Apple Inc. | Interpreting spoken requests |
US11429833B2 (en) * | 2017-06-19 | 2022-08-30 | Kyndryl, Inc. | Cognitive communication assistant services |
US11431642B2 (en) | 2018-06-01 | 2022-08-30 | Apple Inc. | Variable latency device coordination |
US11461398B2 (en) | 2018-08-31 | 2022-10-04 | Accenture Global Solutions Limited | Information platform for a virtual assistant |
US11462215B2 (en) | 2018-09-28 | 2022-10-04 | Apple Inc. | Multi-modal inputs for voice commands |
US11468282B2 (en) | 2015-05-15 | 2022-10-11 | Apple Inc. | Virtual assistant in a communication session |
US11467802B2 (en) | 2017-05-11 | 2022-10-11 | Apple Inc. | Maintaining privacy of personal information |
US11475884B2 (en) | 2019-05-06 | 2022-10-18 | Apple Inc. | Reducing digital assistant latency when a language is incorrectly determined |
US11475898B2 (en) | 2018-10-26 | 2022-10-18 | Apple Inc. | Low-latency multi-speaker speech recognition |
US11488406B2 (en) | 2019-09-25 | 2022-11-01 | Apple Inc. | Text detection using global geometry estimators |
US11493992B2 (en) | 2018-05-04 | 2022-11-08 | Google Llc | Invoking automated assistant function(s) based on detected gesture and gaze |
US11496600B2 (en) | 2019-05-31 | 2022-11-08 | Apple Inc. | Remote execution of machine-learned models |
US11500672B2 (en) | 2015-09-08 | 2022-11-15 | Apple Inc. | Distributed personal assistant |
US11516537B2 (en) | 2014-06-30 | 2022-11-29 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US11526368B2 (en) | 2015-11-06 | 2022-12-13 | Apple Inc. | Intelligent automated assistant in a messaging environment |
US11532306B2 (en) | 2017-05-16 | 2022-12-20 | Apple Inc. | Detecting a trigger of a digital assistant |
US11562744B1 (en) * | 2020-02-13 | 2023-01-24 | Meta Platforms Technologies, Llc | Stylizing text-to-speech (TTS) voice response for assistant systems |
US11580990B2 (en) | 2017-05-12 | 2023-02-14 | Apple Inc. | User-specific acoustic models |
US11599331B2 (en) | 2017-05-11 | 2023-03-07 | Apple Inc. | Maintaining privacy of personal information |
US11614794B2 (en) | 2018-05-04 | 2023-03-28 | Google Llc | Adapting automated assistant based on detected mouth movement and/or gaze |
US11638059B2 (en) | 2019-01-04 | 2023-04-25 | Apple Inc. | Content playback on multiple devices |
US11656884B2 (en) | 2017-01-09 | 2023-05-23 | Apple Inc. | Application integration with a digital assistant |
US11657813B2 (en) | 2019-05-31 | 2023-05-23 | Apple Inc. | Voice identification in digital assistant systems |
US11671920B2 (en) | 2007-04-03 | 2023-06-06 | Apple Inc. | Method and system for operating a multifunction portable electronic device using voice-activation |
US11688417B2 (en) | 2018-05-04 | 2023-06-27 | Google Llc | Hot-word free adaptation of automated assistant function(s) |
US11696060B2 (en) | 2020-07-21 | 2023-07-04 | Apple Inc. | User identification using headphones |
US20230214590A1 (en) * | 2017-07-31 | 2023-07-06 | Ebay Inc. | Emoji Understanding in Online Experiences |
US11710482B2 (en) | 2018-03-26 | 2023-07-25 | Apple Inc. | Natural assistant interaction |
US11727219B2 (en) | 2013-06-09 | 2023-08-15 | Apple Inc. | System and method for inferring user intent from speech inputs |
US11755276B2 (en) | 2020-05-12 | 2023-09-12 | Apple Inc. | Reducing description length based on confidence |
US11765209B2 (en) | 2020-05-11 | 2023-09-19 | Apple Inc. | Digital assistant hardware abstraction |
US11790914B2 (en) | 2019-06-01 | 2023-10-17 | Apple Inc. | Methods and user interfaces for voice-based control of electronic devices |
US11798547B2 (en) | 2013-03-15 | 2023-10-24 | Apple Inc. | Voice activated device for use with a voice-based digital assistant |
US11809483B2 (en) | 2015-09-08 | 2023-11-07 | Apple Inc. | Intelligent automated assistant for media search and playback |
US11809783B2 (en) | 2016-06-11 | 2023-11-07 | Apple Inc. | Intelligent device arbitration and control |
US11838734B2 (en) | 2020-07-20 | 2023-12-05 | Apple Inc. | Multi-device audio adjustment coordination |
US20230396566A1 (en) * | 2022-05-17 | 2023-12-07 | Bank Of America Corporation | Auto-adjust app operation in response to data entry anomalies |
US11853536B2 (en) | 2015-09-08 | 2023-12-26 | Apple Inc. | Intelligent automated assistant in a media environment |
US11854539B2 (en) | 2018-05-07 | 2023-12-26 | Apple Inc. | Intelligent automated assistant for delivering content from user experiences |
US11861263B1 (en) * | 2021-08-24 | 2024-01-02 | X Development Llc | Robust natural language based control of computer applications |
US11886805B2 (en) | 2015-11-09 | 2024-01-30 | Apple Inc. | Unconventional virtual assistant interactions |
US11887594B2 (en) | 2017-03-22 | 2024-01-30 | Google Llc | Proactive incorporation of unsolicited content into human-to-computer dialogs |
US11914848B2 (en) | 2020-05-11 | 2024-02-27 | Apple Inc. | Providing relevant data items based on context |
Families Citing this family (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10192551B2 (en) * | 2016-08-30 | 2019-01-29 | Google Llc | Using textual input and user state information to generate reply content to present in response to the textual input |
EP3577860B1 (en) * | 2017-04-07 | 2023-07-26 | Microsoft Technology Licensing, LLC | Voice forwarding in automated chatting |
US11010180B2 (en) * | 2018-05-29 | 2021-05-18 | Wipro Limited | Method and system for providing real-time guidance to users during troubleshooting of devices |
WO2020226663A1 (en) * | 2019-05-06 | 2020-11-12 | Google Llc | Rendering content using a content agent and/or stored content parameter(s) |
CN110597973A (en) * | 2019-09-12 | 2019-12-20 | 腾讯科技（深圳）有限公司 | Man-machine conversation method, device, terminal equipment and readable storage medium |
CN112652301B (en) * | 2019-10-12 | 2023-05-12 | 阿里巴巴集团控股有限公司 | Voice processing method, distributed system, voice interaction device and voice interaction method |
US11227583B2 (en) | 2019-11-05 | 2022-01-18 | International Business Machines Corporation | Artificial intelligence voice response system having variable modes for interaction with user |
CN111159380B (en) * | 2019-12-31 | 2021-04-27 | 腾讯科技（深圳）有限公司 | Interaction method and device, computer equipment and storage medium |
CN111783422B (en) | 2020-06-24 | 2022-03-04 | 北京字节跳动网络技术有限公司 | Text sequence generation method, device, equipment and medium |
US11455996B2 (en) | 2020-07-27 | 2022-09-27 | Google Llc | Automated assistant adaptation of a response to an utterance and/or of processing of the utterance, based on determined interaction measure |
US20230004833A1 (en) * | 2021-06-30 | 2023-01-05 | On24, Inc. | Methods, Systems, And Apparatuses For Model Selection And Content Recommendations |
Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8660849B2 (en) * | 2010-01-18 | 2014-02-25 | Apple Inc. | Prioritizing selection criteria by automated assistant |
Family Cites Families (18)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7720784B1 (en) | 2005-08-30 | 2010-05-18 | Walt Froloff | Emotive intelligence applied in electronic devices and internet using emotion displacement quantification in pain and pleasure space |
US20090128567A1 (en) | 2007-11-15 | 2009-05-21 | Brian Mark Shuster | Multi-instance, multi-user animation with coordinated chat |
US9858925B2 (en) | 2009-06-05 | 2018-01-02 | Apple Inc. | Using context information to facilitate processing of commands in a virtual assistant |
US10276170B2 (en) * | 2010-01-18 | 2019-04-30 | Apple Inc. | Intelligent automated assistant |
US10474875B2 (en) * | 2010-06-07 | 2019-11-12 | Affectiva, Inc. | Image analysis using a semiconductor processor for facial evaluation |
US9762719B2 (en) | 2011-09-09 | 2017-09-12 | Qualcomm Incorporated | Systems and methods to enhance electronic communications with emotional context |
GB2495222B (en) * | 2011-09-30 | 2016-10-26 | Apple Inc | Using context information to facilitate processing of commands in a virtual assistant |
US9348479B2 (en) | 2011-12-08 | 2016-05-24 | Microsoft Technology Licensing, Llc | Sentiment aware user interface customization |
AU2014236686B2 (en) | 2013-03-15 | 2017-06-15 | Ntt Disruption Us, Inc. | Apparatus and methods for providing a persistent companion device |
AU2014274913B2 (en) * | 2013-06-07 | 2017-05-11 | Apple Inc. | Intelligent automated assistant |
EP3049961A4 (en) * | 2013-09-25 | 2017-03-22 | Intel Corporation | Improving natural language interactions using emotional modulation |
US20150100537A1 (en) * | 2013-10-03 | 2015-04-09 | Microsoft Corporation | Emoji for Text Predictions |
EP2881898A1 (en) | 2013-12-09 | 2015-06-10 | Accenture Global Services Limited | Virtual assistant interactivity platform |
US20160021038A1 (en) | 2014-07-21 | 2016-01-21 | Alcatel-Lucent Usa Inc. | Chat-based support of communications and related functions |
US10924444B2 (en) * | 2014-12-02 | 2021-02-16 | Facebook, Inc. | Device, method, and graphical user interface for managing customer relationships using a lightweight messaging platform |
US9837081B2 (en) | 2014-12-30 | 2017-12-05 | Microsoft Technology Licensing, Llc | Discovering capabilities of third-party voice-enabled resources |
US10594638B2 (en) * | 2015-02-13 | 2020-03-17 | International Business Machines Corporation | Point in time expression of emotion data gathered from a chat session |
US10192551B2 (en) * | 2016-08-30 | 2019-01-29 | Google Llc | Using textual input and user state information to generate reply content to present in response to the textual input |
-
2016
- 2016-08-30 US US15/252,049 patent/US10192551B2/en active Active
-
2017
- 2017-08-29 WO PCT/US2017/049225 patent/WO2018044956A1/en active Search and Examination
- 2017-08-29 CN CN201780053486.7A patent/CN110023926B/en active Active
- 2017-08-29 EP EP17768889.2A patent/EP3507718B1/en active Active
-
2019
- 2019-01-25 US US16/258,035 patent/US10650820B2/en active Active
-
2020
- 2020-04-20 US US16/853,143 patent/US11264032B2/en active Active
-
2022
- 2022-02-03 US US17/592,042 patent/US11929072B2/en active Active
Patent Citations (12)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8930191B2 (en) * | 2006-09-08 | 2015-01-06 | Apple Inc. | Paraphrasing of user requests and results by automated digital assistant |
US8942986B2 (en) * | 2006-09-08 | 2015-01-27 | Apple Inc. | Determining user intent based on ontologies of domains |
US9117447B2 (en) * | 2006-09-08 | 2015-08-25 | Apple Inc. | Using event alert text as input to an automated assistant |
US8660849B2 (en) * | 2010-01-18 | 2014-02-25 | Apple Inc. | Prioritizing selection criteria by automated assistant |
US8670979B2 (en) * | 2010-01-18 | 2014-03-11 | Apple Inc. | Active input elicitation by intelligent automated assistant |
US8706503B2 (en) * | 2010-01-18 | 2014-04-22 | Apple Inc. | Intent deduction based on previous user interactions with voice assistant |
US8731942B2 (en) * | 2010-01-18 | 2014-05-20 | Apple Inc. | Maintaining context information between user interactions with a voice assistant |
US8799000B2 (en) * | 2010-01-18 | 2014-08-05 | Apple Inc. | Disambiguation based on active input elicitation by intelligent automated assistant |
US8892446B2 (en) * | 2010-01-18 | 2014-11-18 | Apple Inc. | Service orchestration for intelligent automated assistant |
US8903716B2 (en) * | 2010-01-18 | 2014-12-02 | Apple Inc. | Personalized vocabulary for digital assistant |
US9318108B2 (en) * | 2010-01-18 | 2016-04-19 | Apple Inc. | Intelligent automated assistant |
US9548050B2 (en) * | 2010-01-18 | 2017-01-17 | Apple Inc. | Intelligent automated assistant |
Cited By (173)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10318871B2 (en) * | 2005-09-08 | 2019-06-11 | Apple Inc. | Method and apparatus for building an intelligent automated assistant |
US11928604B2 (en) | 2005-09-08 | 2024-03-12 | Apple Inc. | Method and apparatus for building an intelligent automated assistant |
US11979836B2 (en) | 2007-04-03 | 2024-05-07 | Apple Inc. | Method and system for operating a multi-function portable electronic device using voice-activation |
US11671920B2 (en) | 2007-04-03 | 2023-06-06 | Apple Inc. | Method and system for operating a multifunction portable electronic device using voice-activation |
US11900936B2 (en) | 2008-10-02 | 2024-02-13 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
US11348582B2 (en) | 2008-10-02 | 2022-05-31 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
US11423886B2 (en) | 2010-01-18 | 2022-08-23 | Apple Inc. | Task flow identification based on user intent |
US10741185B2 (en) | 2010-01-18 | 2020-08-11 | Apple Inc. | Intelligent automated assistant |
US11120372B2 (en) | 2011-06-03 | 2021-09-14 | Apple Inc. | Performing actions associated with task items that represent tasks to perform |
US11321116B2 (en) | 2012-05-15 | 2022-05-03 | Apple Inc. | Systems and methods for integrating third party services with a digital assistant |
US11269678B2 (en) | 2012-05-15 | 2022-03-08 | Apple Inc. | Systems and methods for integrating third party services with a digital assistant |
US11862186B2 (en) | 2013-02-07 | 2024-01-02 | Apple Inc. | Voice trigger for a digital assistant |
US10978090B2 (en) | 2013-02-07 | 2021-04-13 | Apple Inc. | Voice trigger for a digital assistant |
US11557310B2 (en) | 2013-02-07 | 2023-01-17 | Apple Inc. | Voice trigger for a digital assistant |
US11636869B2 (en) | 2013-02-07 | 2023-04-25 | Apple Inc. | Voice trigger for a digital assistant |
US11388291B2 (en) | 2013-03-14 | 2022-07-12 | Apple Inc. | System and method for processing voicemail |
US11798547B2 (en) | 2013-03-15 | 2023-10-24 | Apple Inc. | Voice activated device for use with a voice-based digital assistant |
US11727219B2 (en) | 2013-06-09 | 2023-08-15 | Apple Inc. | System and method for inferring user intent from speech inputs |
US11670289B2 (en) | 2014-05-30 | 2023-06-06 | Apple Inc. | Multi-command single utterance input method |
US11133008B2 (en) | 2014-05-30 | 2021-09-28 | Apple Inc. | Reducing the need for manual start/end-pointing and trigger phrases |
US11257504B2 (en) | 2014-05-30 | 2022-02-22 | Apple Inc. | Intelligent assistant for home automation |
US11699448B2 (en) | 2014-05-30 | 2023-07-11 | Apple Inc. | Intelligent assistant for home automation |
US10878809B2 (en) | 2014-05-30 | 2020-12-29 | Apple Inc. | Multi-command single utterance input method |
US11810562B2 (en) | 2014-05-30 | 2023-11-07 | Apple Inc. | Reducing the need for manual start/end-pointing and trigger phrases |
US11838579B2 (en) | 2014-06-30 | 2023-12-05 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US11516537B2 (en) | 2014-06-30 | 2022-11-29 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US11842734B2 (en) | 2015-03-08 | 2023-12-12 | Apple Inc. | Virtual assistant activation |
US11087759B2 (en) | 2015-03-08 | 2021-08-10 | Apple Inc. | Virtual assistant activation |
US10930282B2 (en) | 2015-03-08 | 2021-02-23 | Apple Inc. | Competing devices responding to voice triggers |
US11468282B2 (en) | 2015-05-15 | 2022-10-11 | Apple Inc. | Virtual assistant in a communication session |
US11070949B2 (en) | 2015-05-27 | 2021-07-20 | Apple Inc. | Systems and methods for proactively identifying and surfacing relevant content on an electronic device with a touch-sensitive display |
US11947873B2 (en) | 2015-06-29 | 2024-04-02 | Apple Inc. | Virtual assistant for media playback |
US11010127B2 (en) | 2015-06-29 | 2021-05-18 | Apple Inc. | Virtual assistant for media playback |
US11500672B2 (en) | 2015-09-08 | 2022-11-15 | Apple Inc. | Distributed personal assistant |
US11126400B2 (en) | 2015-09-08 | 2021-09-21 | Apple Inc. | Zero latency digital assistant |
US11853536B2 (en) | 2015-09-08 | 2023-12-26 | Apple Inc. | Intelligent automated assistant in a media environment |
US11550542B2 (en) | 2015-09-08 | 2023-01-10 | Apple Inc. | Zero latency digital assistant |
US11954405B2 (en) | 2015-09-08 | 2024-04-09 | Apple Inc. | Zero latency digital assistant |
US11809483B2 (en) | 2015-09-08 | 2023-11-07 | Apple Inc. | Intelligent automated assistant for media search and playback |
US11526368B2 (en) | 2015-11-06 | 2022-12-13 | Apple Inc. | Intelligent automated assistant in a messaging environment |
US11809886B2 (en) | 2015-11-06 | 2023-11-07 | Apple Inc. | Intelligent automated assistant in a messaging environment |
US11886805B2 (en) | 2015-11-09 | 2024-01-30 | Apple Inc. | Unconventional virtual assistant interactions |
US10942703B2 (en) | 2015-12-23 | 2021-03-09 | Apple Inc. | Proactive assistance based on dialog communication between devices |
US11853647B2 (en) | 2015-12-23 | 2023-12-26 | Apple Inc. | Proactive assistance based on dialog communication between devices |
US11227589B2 (en) | 2016-06-06 | 2022-01-18 | Apple Inc. | Intelligent list reading |
US11037565B2 (en) | 2016-06-10 | 2021-06-15 | Apple Inc. | Intelligent digital assistant in a multi-tasking environment |
US11657820B2 (en) | 2016-06-10 | 2023-05-23 | Apple Inc. | Intelligent digital assistant in a multi-tasking environment |
US11749275B2 (en) | 2016-06-11 | 2023-09-05 | Apple Inc. | Application integration with a digital assistant |
US11152002B2 (en) | 2016-06-11 | 2021-10-19 | Apple Inc. | Application integration with a digital assistant |
US11809783B2 (en) | 2016-06-11 | 2023-11-07 | Apple Inc. | Intelligent device arbitration and control |
US11656884B2 (en) | 2017-01-09 | 2023-05-23 | Apple Inc. | Application integration with a digital assistant |
US11887594B2 (en) | 2017-03-22 | 2024-01-30 | Google Llc | Proactive incorporation of unsolicited content into human-to-computer dialogs |
US20180287968A1 (en) * | 2017-04-03 | 2018-10-04 | Microsoft Technology Licensing, Llc | Conversation Bot Discovery and Response Fusion |
US10581765B2 (en) * | 2017-04-03 | 2020-03-03 | Microsoft Technology Licensing, Llc | Conversation bot discovery and response fusion |
US20210383809A1 (en) * | 2017-05-03 | 2021-12-09 | Google Llc | Proactive incorporation of unsolicited content into human-to-computer dialogs |
US11929069B2 (en) * | 2017-05-03 | 2024-03-12 | Google Llc | Proactive incorporation of unsolicited content into human-to-computer dialogs |
US10741181B2 (en) | 2017-05-09 | 2020-08-11 | Apple Inc. | User interface for correcting recognition errors |
US11599331B2 (en) | 2017-05-11 | 2023-03-07 | Apple Inc. | Maintaining privacy of personal information |
US11467802B2 (en) | 2017-05-11 | 2022-10-11 | Apple Inc. | Maintaining privacy of personal information |
US11538469B2 (en) | 2017-05-12 | 2022-12-27 | Apple Inc. | Low-latency intelligent automated assistant |
US11380310B2 (en) | 2017-05-12 | 2022-07-05 | Apple Inc. | Low-latency intelligent automated assistant |
US11580990B2 (en) | 2017-05-12 | 2023-02-14 | Apple Inc. | User-specific acoustic models |
US11862151B2 (en) | 2017-05-12 | 2024-01-02 | Apple Inc. | Low-latency intelligent automated assistant |
US11405466B2 (en) | 2017-05-12 | 2022-08-02 | Apple Inc. | Synchronization and task delegation of a digital assistant |
US11837237B2 (en) | 2017-05-12 | 2023-12-05 | Apple Inc. | User-specific acoustic models |
US10748546B2 (en) | 2017-05-16 | 2020-08-18 | Apple Inc. | Digital assistant services based on device capabilities |
US11675829B2 (en) | 2017-05-16 | 2023-06-13 | Apple Inc. | Intelligent automated assistant for media exploration |
US10909171B2 (en) | 2017-05-16 | 2021-02-02 | Apple Inc. | Intelligent automated assistant for media exploration |
US11532306B2 (en) | 2017-05-16 | 2022-12-20 | Apple Inc. | Detecting a trigger of a digital assistant |
US10614122B2 (en) | 2017-06-09 | 2020-04-07 | Google Llc | Balance modifications of audio-based computer program output using a placeholder field based on content |
US10652170B2 (en) | 2017-06-09 | 2020-05-12 | Google Llc | Modification of audio-based computer program output |
US10855627B2 (en) | 2017-06-09 | 2020-12-01 | Google Llc | Modification of audio-based computer program output |
US10657173B2 (en) * | 2017-06-09 | 2020-05-19 | Google Llc | Validate modification of audio-based computer program output |
US20180357310A1 (en) * | 2017-06-09 | 2018-12-13 | Google Inc. | Validate modification of audio-based computer program output |
US11582169B2 (en) | 2017-06-09 | 2023-02-14 | Google Llc | Modification of audio-based computer program output |
US11429833B2 (en) * | 2017-06-19 | 2022-08-30 | Kyndryl, Inc. | Cognitive communication assistant services |
US20230214590A1 (en) * | 2017-07-31 | 2023-07-06 | Ebay Inc. | Emoji Understanding in Online Experiences |
US11928428B2 (en) * | 2017-07-31 | 2024-03-12 | Ebay Inc. | Emoji understanding in online experiences |
US20200364413A1 (en) * | 2017-09-20 | 2020-11-19 | Samsung Electronics Co., Ltd. | Electronic device and control method thereof |
US11210345B2 (en) * | 2017-11-03 | 2021-12-28 | Google Llc | Using distributed state machines for human-to-computer dialogs with automated assistants to protect private data |
US20190147867A1 (en) * | 2017-11-10 | 2019-05-16 | Hyundai Motor Company | Dialogue system and method for controlling thereof |
US10937420B2 (en) * | 2017-11-10 | 2021-03-02 | Hyundai Motor Company | Dialogue system and method to identify service from state and input information |
US11646022B2 (en) * | 2018-02-07 | 2023-05-09 | Samsung Electronics Co., Ltd | Electronic device for chatting with chatbot and operation method thereof |
US20210035576A1 (en) * | 2018-02-07 | 2021-02-04 | Samsung Electronics Co., Ltd. | Electronic device for chatting with chatbot and operation method thereof |
WO2019172878A1 (en) * | 2018-03-05 | 2019-09-12 | Google Llc | Transitioning between prior dialog contexts with automated assistants |
US11727220B2 (en) | 2018-03-05 | 2023-08-15 | Google Llc | Transitioning between prior dialog contexts with automated assistants |
US11314944B2 (en) | 2018-03-05 | 2022-04-26 | Google Llc | Transitioning between prior dialog contexts with automated assistants |
EP4307160A1 (en) * | 2018-03-05 | 2024-01-17 | Google LLC | Transitioning between prior dialog contexts with automated assistants |
US11710482B2 (en) | 2018-03-26 | 2023-07-25 | Apple Inc. | Natural assistant interaction |
US11614794B2 (en) | 2018-05-04 | 2023-03-28 | Google Llc | Adapting automated assistant based on detected mouth movement and/or gaze |
US11688417B2 (en) | 2018-05-04 | 2023-06-27 | Google Llc | Hot-word free adaptation of automated assistant function(s) |
US11493992B2 (en) | 2018-05-04 | 2022-11-08 | Google Llc | Invoking automated assistant function(s) based on detected gesture and gaze |
US11854539B2 (en) | 2018-05-07 | 2023-12-26 | Apple Inc. | Intelligent automated assistant for delivering content from user experiences |
US11900923B2 (en) | 2018-05-07 | 2024-02-13 | Apple Inc. | Intelligent automated assistant for delivering content from user experiences |
US11907436B2 (en) | 2018-05-07 | 2024-02-20 | Apple Inc. | Raise to speak |
US11487364B2 (en) | 2018-05-07 | 2022-11-01 | Apple Inc. | Raise to speak |
US11169616B2 (en) | 2018-05-07 | 2021-11-09 | Apple Inc. | Raise to speak |
US10957126B2 (en) | 2018-05-23 | 2021-03-23 | Google Llc | Providing a communications channel between instances of automated assistants |
US10861254B2 (en) | 2018-05-23 | 2020-12-08 | Google Llc | Providing a communications channel between instances of automated assistants |
JP2020123359A (en) * | 2018-05-23 | 2020-08-13 | グーグル エルエルシー | Providing communications channel between instances of automated assistants |
EP3608783A1 (en) * | 2018-05-23 | 2020-02-12 | Google LLC | Providing a communications channel between instances of automated assistants |
US11721135B2 (en) * | 2018-05-23 | 2023-08-08 | Google Llc | Providing a communications channel between instances of automated assistants |
EP4345599A3 (en) * | 2018-05-23 | 2024-04-17 | Google LLC | Providing a communications channel between instances of automated assistants |
US20190362568A1 (en) * | 2018-05-23 | 2019-11-28 | Google Llc | Providing a communications channel between instances of automated assistants |
US10984798B2 (en) | 2018-06-01 | 2021-04-20 | Apple Inc. | Voice interaction at a primary device to access call functionality of a companion device |
US10720160B2 (en) | 2018-06-01 | 2020-07-21 | Apple Inc. | Voice interaction at a primary device to access call functionality of a companion device |
US11009970B2 (en) | 2018-06-01 | 2021-05-18 | Apple Inc. | Attention aware virtual assistant dismissal |
US11630525B2 (en) | 2018-06-01 | 2023-04-18 | Apple Inc. | Attention aware virtual assistant dismissal |
US11431642B2 (en) | 2018-06-01 | 2022-08-30 | Apple Inc. | Variable latency device coordination |
US11360577B2 (en) | 2018-06-01 | 2022-06-14 | Apple Inc. | Attention aware virtual assistant dismissal |
WO2019236372A1 (en) * | 2018-06-03 | 2019-12-12 | Google Llc | Selectively generating expanded responses that guide continuance of a human-to-computer dialog |
US11238864B2 (en) | 2018-06-03 | 2022-02-01 | Google Llc | Selectively generating expanded responses that guide continuance of a human-to-computer dialog |
US11869506B2 (en) | 2018-06-03 | 2024-01-09 | Google Llc | Selectively generating expanded responses that guide continuance of a human-to-computer dialog |
CN109032381A (en) * | 2018-08-01 | 2018-12-18 | 平安科技（深圳）有限公司 | A kind of input method based on context, device, storage medium and terminal |
CN110895558A (en) * | 2018-08-23 | 2020-03-20 | 北京搜狗科技发展有限公司 | Dialog reply method and related device |
US11461398B2 (en) | 2018-08-31 | 2022-10-04 | Accenture Global Solutions Limited | Information platform for a virtual assistant |
US11487832B2 (en) | 2018-09-27 | 2022-11-01 | Google Llc | Analyzing web pages to facilitate automatic navigation |
US11010561B2 (en) | 2018-09-27 | 2021-05-18 | Apple Inc. | Sentiment prediction from textual data |
US11971936B2 (en) | 2018-09-27 | 2024-04-30 | Google Llc | Analyzing web pages to facilitate automatic navigation |
JP2022501730A (en) * | 2018-09-27 | 2022-01-06 | グーグル エルエルシーGoogle LLC | Web page analysis to facilitate automatic navigation |
JP7209818B2 (en) | 2018-09-27 | 2023-01-20 | グーグル エルエルシー | Analysis of web pages to facilitate automatic navigation |
US11462215B2 (en) | 2018-09-28 | 2022-10-04 | Apple Inc. | Multi-modal inputs for voice commands |
US11170166B2 (en) | 2018-09-28 | 2021-11-09 | Apple Inc. | Neural typographical error modeling via generative adversarial networks |
US10839159B2 (en) | 2018-09-28 | 2020-11-17 | Apple Inc. | Named entity normalization in a spoken dialog system |
US11893992B2 (en) | 2018-09-28 | 2024-02-06 | Apple Inc. | Multi-modal inputs for voice commands |
US11475898B2 (en) | 2018-10-26 | 2022-10-18 | Apple Inc. | Low-latency multi-speaker speech recognition |
WO2020087534A1 (en) * | 2018-11-02 | 2020-05-07 | Microsoft Technology Licensing, Llc | Generating response in conversation |
CN111971670A (en) * | 2018-11-02 | 2020-11-20 | 微软技术许可有限责任公司 | Generating responses in a conversation |
US11423899B2 (en) * | 2018-11-19 | 2022-08-23 | Google Llc | Controlling device output according to a determined condition of a user |
US11942081B2 (en) * | 2018-12-07 | 2024-03-26 | Sony Group Corporation | Information processing device and information processing method for response presentation |
US20220036887A1 (en) * | 2018-12-07 | 2022-02-03 | Sony Group Corporation | Information processing device, information processing method, and program |
US11638059B2 (en) | 2019-01-04 | 2023-04-25 | Apple Inc. | Content playback on multiple devices |
US11151324B2 (en) | 2019-02-03 | 2021-10-19 | International Business Machines Corporation | Generating completed responses via primal networks trained with dual networks |
GB2595123A (en) * | 2019-02-03 | 2021-11-17 | Ibm | Performing multi-objective tasks via primal networks trained with dual networks |
WO2020157731A1 (en) * | 2019-02-03 | 2020-08-06 | International Business Machines Corporation | Performing multi-objective tasks via primal networks trained with dual networks |
US11281867B2 (en) * | 2019-02-03 | 2022-03-22 | International Business Machines Corporation | Performing multi-objective tasks via primal networks trained with dual networks |
US11348573B2 (en) | 2019-03-18 | 2022-05-31 | Apple Inc. | Multimodality in digital assistant systems |
US11783815B2 (en) | 2019-03-18 | 2023-10-10 | Apple Inc. | Multimodality in digital assistant systems |
US11475884B2 (en) | 2019-05-06 | 2022-10-18 | Apple Inc. | Reducing digital assistant latency when a language is incorrectly determined |
US11307752B2 (en) | 2019-05-06 | 2022-04-19 | Apple Inc. | User configurable task triggers |
US11675491B2 (en) | 2019-05-06 | 2023-06-13 | Apple Inc. | User configurable task triggers |
US11217251B2 (en) | 2019-05-06 | 2022-01-04 | Apple Inc. | Spoken notifications |
US11705130B2 (en) | 2019-05-06 | 2023-07-18 | Apple Inc. | Spoken notifications |
US11423908B2 (en) | 2019-05-06 | 2022-08-23 | Apple Inc. | Interpreting spoken requests |
CN113785354A (en) * | 2019-05-06 | 2021-12-10 | 谷歌有限责任公司 | Selectively activating on-device speech recognition and using recognized text in selectively activating NLUs on devices and/or fulfillment on devices |
US11140099B2 (en) | 2019-05-21 | 2021-10-05 | Apple Inc. | Providing message response suggestions |
US11888791B2 (en) | 2019-05-21 | 2024-01-30 | Apple Inc. | Providing message response suggestions |
US11237797B2 (en) | 2019-05-31 | 2022-02-01 | Apple Inc. | User activity shortcut suggestions |
US11360739B2 (en) | 2019-05-31 | 2022-06-14 | Apple Inc. | User activity shortcut suggestions |
US11289073B2 (en) | 2019-05-31 | 2022-03-29 | Apple Inc. | Device text to speech |
US11657813B2 (en) | 2019-05-31 | 2023-05-23 | Apple Inc. | Voice identification in digital assistant systems |
US11496600B2 (en) | 2019-05-31 | 2022-11-08 | Apple Inc. | Remote execution of machine-learned models |
US11790914B2 (en) | 2019-06-01 | 2023-10-17 | Apple Inc. | Methods and user interfaces for voice-based control of electronic devices |
US11360641B2 (en) | 2019-06-01 | 2022-06-14 | Apple Inc. | Increasing the relevance of new available information |
US20210056489A1 (en) * | 2019-08-23 | 2021-02-25 | Nokia Technologies Oy | Controlling submission of content |
US11727338B2 (en) * | 2019-08-23 | 2023-08-15 | Nokia Technologies Oy | Controlling submission of content |
US11488406B2 (en) | 2019-09-25 | 2022-11-01 | Apple Inc. | Text detection using global geometry estimators |
US11294784B1 (en) * | 2019-09-26 | 2022-04-05 | Amazon Technologies, Inc. | Techniques for providing predictive interface elements |
US11562744B1 (en) * | 2020-02-13 | 2023-01-24 | Meta Platforms Technologies, Llc | Stylizing text-to-speech (TTS) voice response for assistant systems |
CN111414733A (en) * | 2020-03-18 | 2020-07-14 | 联想(北京)有限公司 | Data processing method and device and electronic equipment |
US11924254B2 (en) | 2020-05-11 | 2024-03-05 | Apple Inc. | Digital assistant hardware abstraction |
US11914848B2 (en) | 2020-05-11 | 2024-02-27 | Apple Inc. | Providing relevant data items based on context |
US11765209B2 (en) | 2020-05-11 | 2023-09-19 | Apple Inc. | Digital assistant hardware abstraction |
US11755276B2 (en) | 2020-05-12 | 2023-09-12 | Apple Inc. | Reducing description length based on confidence |
US20220020365A1 (en) * | 2020-07-15 | 2022-01-20 | Google Llc | Automated assistant with audio presentation interaction |
US11838734B2 (en) | 2020-07-20 | 2023-12-05 | Apple Inc. | Multi-device audio adjustment coordination |
US11750962B2 (en) | 2020-07-21 | 2023-09-05 | Apple Inc. | User identification using headphones |
US11696060B2 (en) | 2020-07-21 | 2023-07-04 | Apple Inc. | User identification using headphones |
CN113378583A (en) * | 2021-07-15 | 2021-09-10 | 北京小米移动软件有限公司 | Dialogue reply method and device, dialogue model training method and device, and storage medium |
US11861263B1 (en) * | 2021-08-24 | 2024-01-02 | X Development Llc | Robust natural language based control of computer applications |
CN114172856A (en) * | 2021-11-30 | 2022-03-11 | 中国平安财产保险股份有限公司 | Automatic message reply method, device, equipment and storage medium |
US11870739B2 (en) * | 2022-05-17 | 2024-01-09 | Bank Of America Corporation | Auto-adjust app operation in response to data entry anomalies |
US20230396566A1 (en) * | 2022-05-17 | 2023-12-07 | Bank Of America Corporation | Auto-adjust app operation in response to data entry anomalies |
Also Published As
Publication number | Publication date |
---|---|
CN110023926A (en) | 2019-07-16 |
US11929072B2 (en) | 2024-03-12 |
US20220172719A1 (en) | 2022-06-02 |
US20190156831A1 (en) | 2019-05-23 |
WO2018044956A1 (en) | 2018-03-08 |
US10192551B2 (en) | 2019-01-29 |
CN110023926B (en) | 2024-01-30 |
US10650820B2 (en) | 2020-05-12 |
US20200251112A1 (en) | 2020-08-06 |
EP3507718A1 (en) | 2019-07-10 |
EP3507718B1 (en) | 2020-08-05 |
US11264032B2 (en) | 2022-03-01 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11929072B2 (en) | Using textual input and user state information to generate reply content to present in response to the textual input | |
CN109983430B (en) | Determining graphical elements included in an electronic communication | |
JP6710740B2 (en) | Providing suggested voice-based action queries | |
US11322140B2 (en) | Providing prompt in an automated dialog session based on selected content of prior automated dialog session | |
KR102097621B1 (en) | Interpreting automated assistant requests based on images and / or other sensor data | |
KR20200006107A (en) | Obtain response information from multiple corpus | |
EP3593346B1 (en) | Graphical data selection and presentation of digital content | |
US20220215179A1 (en) | Rendering content using a content agent and/or stored content parameter(s) | |
US11842206B2 (en) | Generating content endorsements using machine learning nominator(s) | |
US20230298580A1 (en) | Emotionally Intelligent Responses to Information Seeking Questions |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:CARBUNE, VICTOR;KEYSERS, DANIEL;DESELAERS, THOMAS;SIGNING DATES FROM 20160829 TO 20160830;REEL/FRAME:039606/0713 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044567/0001Effective date: 20170929 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |