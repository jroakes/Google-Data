US9477757B1 - Latent user models for personalized ranking - Google Patents
Latent user models for personalized ranking Download PDFInfo
- Publication number
- US9477757B1 US9477757B1 US13/517,767 US201213517767A US9477757B1 US 9477757 B1 US9477757 B1 US 9477757B1 US 201213517767 A US201213517767 A US 201213517767A US 9477757 B1 US9477757 B1 US 9477757B1
- Authority
- US
- United States
- Prior art keywords
- user
- ranking
- community
- personalized
- mixing weight
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Expired - Fee Related, expires
Links
Images
Classifications
-
- G06F17/30864—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/951—Indexing; Web crawling techniques
Definitions
- Ranking is a very popular machine learning problem that is applied in many contexts. Ranking can be applied to, for example, the restaurants in a city, a list of videos, a list of songs, etc. This problem can be solved by using machine learning technologies to learn a ranking model from a training dataset.
- a training dataset is a set of ground truth pairs (x i (1) , x i (2) ), where x i (1) is “better” than x i (2) , where each sample x i is a restaurant, video, song, etc.
- the ranking model Given a new list of samples, the ranking model can predict a score for each of them, and the list can be ranked according to the ranking score.
- Some ranking problems attempt to make predictions about subjective tastes. For example, user preferences regarding videos tend to be very subjective. A person who owns a dog might prefer to watch funny videos about dogs while a cat owner might prefer to watch funny videos about cats.
- personalized ranking models which attempt to take user preference into consideration when learning the ranking model, may perform better than generalized ranking models.
- Some existing personalized ranking algorithms treat personalized ranking problems in the same manner as general ranking problems, except that they extend the ground truth pairs to (x i (1) +user j , x i (2) +user j ), and learn a global ranking model for all users.
- the model size must be very big to be discriminative enough for each user if the number of users is large-scale; and 2) a large number of training samples are needed if the user information is very limited.
- the disclosure relates to personalized ranking.
- One aspect of the disclosed embodiments is a method that includes accessing a community preference dataset representing the preferences of a community of users regarding a plurality of objects.
- the community preference dataset includes a plurality of paired comparisons regarding the plurality of objects.
- the method also includes generating a ranking model and a baseline mixing weight for each latent user category from a plurality of latent user categories based on the community preference dataset and one or more latent variables that relate the users from the community of users to the latent user categories.
- the method also includes accessing an individual preference dataset representing the preferences of a specified user regarding at least some objects from the plurality of objects.
- the method also includes generating a personalized mixing weight for each latent user category for the specified user based on the individual preference dataset, the ranking models for the latent user category, and one or more latent variables that relate the specified user to the latent user categories.
- the method also includes adjusting the personalized mixing weight for each latent user category for the specified user based on the baseline mixing weight for each latent user category, and generating ranking output for at least some objects from the plurality of objects using the personalized mixing weights and the ranking models for the latent user categories.
- Another aspect of the disclosed embodiments is a method that includes accessing a community preference dataset representing the preferences of a community of users regarding a plurality of objects, and generating a ranking model and a baseline mixing weight for each latent user category from a plurality of latent user categories based on the community preference dataset.
- the method also includes accessing an individual preference dataset representing the preferences of a specified user regarding at least some objects from the plurality of objects, and generating a personalized mixing weight for each latent user category for the specified user based on the individual preference dataset and the ranking model for the latent user categories.
- Another aspect of the disclosed embodiments is an apparatus that includes one or more processors and one or more memory devices for storing program instructions used by the one or more processors.
- the program instructions when executed by the one or more processors, cause the one or more processors to access a community preference dataset representing the preferences of a community of users regarding a plurality of objects, generate a ranking model and a baseline mixing weight for each latent user category from a plurality of latent user categories based on the community preference dataset, access an individual preference dataset representing the preferences of a specified user regarding at least some objects from the plurality of objects, and generate a personalized mixing weight for each latent user category for the specified user based on the individual preference dataset and the ranking model for the latent user categories.
- FIG. 1 is a block diagram of a system according to one implementation of personalized ranking.
- FIG. 2 is a block diagram showing an example of a hardware configuration for a server.
- FIG. 3 is a block diagram showing an example of a baseline modeling stage.
- FIG. 4 is a block diagram showing an example of a personalized modeling stage.
- FIG. 5 is a block diagram showing an example of an implementation of a ranking stage.
- FIG. 6 is a flowchart showing a process for personalized ranking.
- the disclosure herein relates to personalized ranking using latent user models.
- a community of users can be characterized as belonging to a plurality of latent user categories, and a ranking model can be learned for each latent user category.
- Each latent user category represents a specific subjective user preference or combination of subjective user preferences.
- individual user preferences do not generally conform to a single latent user category, but instead resemble a mixture of the latent user categories.
- Latent variables describe data that is treated as being unknown.
- the values of the latent variables are inferred using known data.
- the systems and methods described herein generate a personalized ranking model in two stages.
- a baseline modeling stage ranking models for the latent user categories and baseline mixing weights are determined using a community preference dataset representing the preferences of a community of users regarding a plurality of objects as training data.
- personalized mixing weights for a specified user are determined using an individual preference dataset representing the preferences of the specified user, and also using the previously determined ranking models for the latent user categories.
- the personalized ranking model which is defined by the personalized mixing weights in combination with the ranking models for the latent user categories, is applied in a ranking stage, for example, to generate ranking output, such as ranking scores or a ranking order, in response to a query.
- FIG. 1 is a block diagram of a system 100 in accordance with one implementation.
- the system 100 can include one or more server computers such as a server 110 , and one or more client devices such as client 120 , client 130 , and client 140 .
- the server 110 and the clients 120 , 130 , 140 can be in communication with one another by a network 150 .
- the clients 120 , 130 , 140 are representative, and it is understood that very large numbers (e.g., millions) of clients can be supported and can be in communication with the server 110 at any time.
- the clients 120 , 130 , 140 may include a variety of different computing devices. Examples include personal computers, digital assistants, personal digital assistants, cellular phones, mobile phones, smart phones, laptop computers, and other types of computing devices now-existing or hereafter developed.
- the network 150 is typically the Internet, but may also be any network, including but not limited to a LAN, a MAN, a WAN, a mobile, wired or wireless network, a private network, a virtual private network, and combination of these types of networks and/or other types of networks.
- FIG. 2 is a block diagram of an example of a hardware configuration for the server 110 .
- the clients 120 , 130 , 140 can, in some implementations, use a similar hardware configuration.
- the server 110 can include a CPU 210 .
- the CPU 210 of the server 110 can be a conventional central processing unit.
- the CPU 210 can be any other type of device, or multiple devices, capable of manipulating or processing information now-existing or hereafter developed.
- the disclosed examples can be practiced with a single processor as shown, e.g. CPU 210 , advantages in speed and efficiency can be achieved using more than one processor.
- the server 110 can include memory 220 , such as a random access memory device (RAM). Any other suitable type of storage device can be used as the memory 220 .
- the memory 220 can include code and data 222 that can be accessed by the CPU 210 using a bus 230 .
- the memory 220 can further include one or more application programs 224 and an operating system 226 .
- the application programs 224 can include programs that permit the CPU 210 to perform the methods described here.
- a storage device 240 can be optionally provided in the form of any suitable computer readable medium, such as a hard disc drive, a memory device, a flash drive or an optical drive.
- One or more input devices 250 such as a keyboard, a mouse, or a gesture sensitive input device, receive user inputs and can output signals or data indicative of the user inputs to the CPU 210 .
- One or more output devices can be provided, such as a display device 260 .
- the display device 260 such as liquid crystal display (LCD) or a cathode-ray tube (CRT), allows output to be presented to a user, for example, in response to receiving a video signal.
- LCD liquid crystal display
- CRT cathode-ray tube
- FIG. 1 depicts the CPU 210 and the memory 220 of the server 110 as being integrated into a single unit, other configurations can be utilized.
- the operations of the CPU 210 can be distributed across multiple machines (each machine having one or more of processors) which can be coupled directly or across a local area or other network.
- the memory 220 can be distributed across multiple machines such as network-based memory or memory in multiple machines performing the operations of the server 110 .
- the bus 230 of the server 110 can be composed of multiple buses.
- the storage device 240 can be directly coupled to the other components of the server 110 or can be accessed via a network and can comprise a single integrated unit such as a memory card or multiple units such as multiple memory cards.
- the server 110 can thus be implemented in a wide variety of configurations.
- FIG. 3 is a block diagram showing an example of a baseline modeling stage 300 .
- a baseline modeling component 310 receives a community preference dataset 320 as an input.
- the baseline modeling component 310 is a software component that is executed by the server 110
- the community preference dataset 320 can be received as an input by accessing it at the memory 220 , the storage device 240 , or at any other device, system, or media on which it is stored.
- the baseline modeling component 310 generates ranking models ⁇ and baseline mixing weights ⁇ .
- the ranking models ⁇ and the baseline mixing weights ⁇ are jointly optimized by the baseline modeling component 310 based on the community preference dataset 320 using machine learning techniques.
- the baseline modeling component 310 can be implemented as a software component that is executed by the server 110 .
- the community preference dataset 320 serves as a training dataset for the baseline modeling component 310 , and represents the preferences of a community of users regarding a plurality of objects.
- the community of users can be, for example, the users of the clients 120 , 130 , 140 as well as additional client devices.
- the objects can be any type of objects to which personalized ranking is to be applied.
- the objects can be, for example, video clips, audio files, documents, etc.
- the information in the community preference dataset 320 can be collected based on user behavior or explicit rankings or comparisons, which are collected and scored with permission from the users.
- the identities of the individual users from whom the preferences were derived can be anonymized or omitted from the community preference dataset 320 .
- the community preference dataset 320 can include or be used as a basis for generating paired comparisons.
- Each comparison consists of two samples, where x i (1) is regarded as “better” than x i (2) .
- Each comparison x i was made by an individual user.
- the comparisons could be made explicitly. For example, if a user selects an option from a list of options, this selection can be used to generate a set of paired comparisons, each indicating that the selected option is preferred over one of the other options.
- the comparisons could be made implicitly, based on data that does not represent an explicit comparison of an object to one or more other objects. For example, in a system where a user ranks objects on a scale from 1 to 5, these rankings could be used to generate a comparison for each pair of ranked objects having a non-equal ranking.
- community of users and similar terms refer to the users whose preferences are included in the community preference dataset 320 . Assuming that the community of users includes a total of M users, then u i ⁇ 1, . . . , M ⁇ denotes the user that made the i th comparison.
- the ranking models ⁇ can be machine-learned ranking models. As used herein, ranking model is synonymous with ranking function.
- the baseline modeling component 310 can learn a ranking model ⁇ to represent each category of users, where ranking model ⁇ k is the ranking model that corresponds to the k th category.
- the baseline modeling component 310 assumes that each user behaves as a mixture of the K user categories.
- mixing weight ⁇ jk represents the mixing weight for the j th user on the k th user category.
- the sum of the mixing weights ⁇ jk is equal to one, as in Equation 1:
- user j ) [Equation 2]
- the user category corresponding to each user is treated as a latent variable.
- the categories are treated as latent user categories, meaning that the users are not explicitly grouped into categories, but rather, the relationship between the users and the latent user categories is determined using machine learning techniques, as will be explained further herein.
- a latent variable, z i ⁇ 1, . . . , K ⁇ is introduced.
- a latent variable z i ⁇ 1, . . . , K ⁇ .
- the latent variable z i can be introduced, for example, as described in Equation 4:
- Equation 4 maximizing equation 3 is performed by finding appropriate values for the ranking models ⁇ k and the mixing weights ⁇ jk , which can be performed by the baseline modeling component 310 using machine learning techniques.
- the baseline modeling component 310 can utilize the expectation-maximization (EM) algorithm to find the optimal or suboptimal ranking models ⁇ k and the baseline mixing weights ⁇ jk .
- the EM algorithm is an iterative method for finding maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables.
- the EM iteration alternates between performing an expectation (E) step, which computes the expectation of a log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step.
- E expectation
- M maximization
- ⁇ k ) ⁇ ⁇ u i , k / ⁇ k 1 K ⁇ p ⁇ ( x i
- the baseline modeling component 310 can perform the M step by first re-estimating the mixing weights ⁇ jk . This can be done, for example, per Equation 7:
- Equation 8 any desired type of ranking models can be utilized.
- linear ranking models can be utilized, as in Equation 9, where x i is an object to be ranked, in which case, Equation 8 becomes a weighted version of logistic regression, which can be solved by well-known methods.
- f k ( x i ) w k T ( x i (1) ⁇ x i (2) ) [Equation 9]
- the E step and the M step are repeated iteratively until convergence.
- the ranking models ⁇ and the baseline mixing weights ⁇ can be stored for later use, for example, at the memory 220 of the server 110 .
- FIG. 4 is a block diagram showing an example of a personalized modeling stage 400 .
- a personalized modeling component 410 can receive an individual preference dataset 420 as an input.
- the individual preference dataset 420 corresponds to a specified user, and represent the preferences of the specified user regarding at least some objects from the plurality of objects that form the subject matter of the comparisons in the community preference dataset 320 .
- the specified user can be the user of one of the clients 120 , 130 , 140 .
- the preferences of the specified user can be incorporated both in the individual preference dataset 420 and the community preference dataset 320 .
- the individual preference dataset can be a subset of the community preference dataset 320 .
- the data that is incorporated in the individual preference dataset 420 is collected and stored with permission from the user.
- the individual preference dataset 420 and ranking functions that utilize it can be associated with a password-protected or otherwise secured profile to protect the user's privacy.
- the ranking models ⁇ are held constant.
- the personalized mixing weights ⁇ _user are optimized by the personalized modeling component 410 based on the individual preference dataset 420 using machine learning techniques.
- the personalized modeling component 410 can be implemented as a software component that is executed by the server 110 .
- the personalized modeling component 410 can generate the personalized mixing weights ⁇ _user using the expectation maximization algorithm. Generally, this process is as described in the baseline modeling stage 300 .
- the E step can be performed in the same manner as in the baseline modeling stage 300 , for example, according to Equation 6.
- the personalized modeling component 410 re-estimates the personalized mixing weights ⁇ _user.
- the personalized modeling component 410 does not re-estimate the ranking models ⁇ as part of the M step, and instead, holds them constant.
- the E step and the M step are performed until convergence.
- the personalized mixing weights ⁇ _user can be stored for later use, for example, at the memory 220 of the server 110 .
- FIG. 5 is a block diagram showing an example of a ranking stage 500 .
- a ranking component 510 receives a ranking request regarding the specified user, such as a query 520 , as an input, and generates a ranking output 530 .
- the query 520 can be a stimulus in any form that causes operation of the ranking component 510 .
- the query 520 can be generated in response to user input, or can be generated programmatically.
- the query 520 can specify parameters that are utilized for the purposes of ranking, such as keywords or a list of objects to be ranked.
- the ranking component 510 generates the ranking output 530 in response to the query 520 using the ranking models ⁇ , which are received by the ranking component 510 , such as by accessing them at the memory 220 of the server 110 . If the personalized mixing weights ⁇ _user do not yet exist for the specified user, the ranking component 510 can utilize the baseline ranking model, which is defined by applying the baseline mixing weights ⁇ to the ranking models ⁇ . This can be the case, for example, if the specified user has not previously been seen by the system, or if there is not sufficient data available in the individual preference dataset 420 for the specified user.
- the ranking component 510 can utilize the personalized ranking model, which is defined by applying the personalized mixing weights ⁇ _user to the ranking models ⁇ .
- the ranking output 530 is generated by first computing model-specific ranking scores for a selected object x i using each of the ranking models ⁇ .
- the model specific ranking scores are then weighted according to the baseline mixing weights ⁇ or the personalized mixing weights ⁇ _user, and combined.
- the model specific ranking scores can be combined as a weighted sum or weighted average of either of the baseline mixing weights ⁇ or the personalized mixing weights ⁇ _user.
- the personalized mixing weights ⁇ _user can be used by the ranking component 510 to produce the ranking output 530 by applying smoothing techniques and/or smoothing algorithms. These smoothing techniques or algorithms can be based in part on the baseline mixing weights ⁇ .
- the personalized mixing weight ⁇ _user for each latent user category can be adjusted based on the baseline mixing weights ⁇ . The magnitude of the adjustment can be based on, for example, the size of the individual preference dataset.
- the personalized mixing weights ⁇ _user can be modified based on a weighted average of the baseline mixing weights ⁇ and the personalized mixing weights ⁇ _user, where the weighting is based on the number of comparisons or other data elements in the individual preference dataset 420 relative to a threshold value.
- the baseline ranking model can be adopted for a newly seen user. As data regarding the newly seen user is collected, it is added to the individual preference dataset 420 for that user. The personalized ranking model for that user can then be phased in gradually as the individual preference dataset 420 regarding the user grows.
- FIG. 6 is a flowchart showing an example of a process 600 for personalized ranking.
- the operations described in connection with the process 600 can be performed at one or more server computers, such as the server 110 . When an operation is performed by one or more computers, it is completed when it is performed by one computer.
- the operations described in connection with the process 600 can be embodied as a non-transitory computer readable medium including program instructions executable by one or more processors that, when executed, cause the one or more processors to perform the operations.
- the operations described in connection with the process 600 could be stored at the memory 220 of the server 110 and be executable by the CPU 210 of the server 110 .
- Operation 610 includes accessing a community preference dataset that represents the preferences of a community of users regarding a plurality of objects.
- the community preference dataset can includes a plurality of paired comparisons regarding the plurality of objects. Accessing can occur in any suitable manner, such as accessing by transmission over a network, accessing from memory, or accessing from a disc.
- the CPU 210 of the server 110 accesses the community preference dataset 320 from the memory 220 of the server 110 .
- Operation 620 includes generating a ranking model and a baseline mixing weight for each latent user category from a plurality of latent user categories based on the community preference dataset and one or more latent variables that relate the users from the community of users to the latent user categories.
- the CPU 210 of the server 110 generates the ranking model and a baseline mixing weights in the manner described in connection with the baseline modeling component 310 , which can be implemented in the form of computer program instructions that are executed at the CPU 210 of the server 110 .
- Operation 630 includes accessing an individual preference dataset that represents the preferences of a specified user regarding at least some objects from the plurality of objects.
- the individual preference dataset can includes a plurality of paired comparisons regarding at least some objects from the plurality of objects. Accessing can occur in any suitable manner, such as accessing by transmission over a network, accessing from memory, or accessing from a disc.
- the CPU 210 of the server 110 accesses the individual preference dataset 420 from the memory 220 of the server 110 .
- Operation 640 includes generating a personalized mixing weight for each latent user category for the specified user based on the individual preference dataset, the ranking models for the latent user category, and one or more latent variables that relate the specified user to the latent user categories.
- the CPU 210 of the server 110 generates the personalized mixing weights in the manner described in connection with the personalized modeling component 410 , which can be implemented in the form of computer program instructions that are executed at the CPU 210 of the server 110 .
- Operation 650 includes adjusting the personalized mixing weight for each latent user category for the specified user based on the baseline mixing weight for each latent user category. Operation 650 can be performed, for example, when the individual preference dataset 420 that is used to generate the personalized mixing weights does not include sufficient data from which to generate an accurate personalized ranking model. Operation 650 can be performed, for example, by the CPU 210 of the server 110 in the manner described in connection with the ranking component 510 , which can be implemented in the form of computer program instructions that are executed at the CPU 210 of the server 110 .
- Operation 660 includes generating ranking output for at least some objects from the plurality of objects using the personalized mixing weights and the ranking models for the latent user categories. Operation 660 can be performed, for example, by the CPU 210 of the server 110 in the manner described in connection with the ranking component 510 , which can be implemented in the form of computer program instructions that are executed at the CPU 210 of the server 110 .
- example or “exemplary” are used herein to mean serving as an example, instance, or illustration. Any aspect or design described herein as “example” or “exemplary” is not necessarily to be construed as preferred or advantageous over other aspects or designs. Rather, use of the words “example” or “exemplary” is intended to present concepts in a concrete fashion.
- the term “or” is intended to mean an inclusive “or” rather than an exclusive “or”. That is, unless specified otherwise, or clear from context, “X includes A or B” is intended to mean any of the natural inclusive permutations.
- the implementations of the computer devices can be realized in hardware, software, or any combination thereof.
- the hardware can include, for example, computers, intellectual property (IP) cores, application-specific integrated circuits (ASICs), programmable logic arrays, optical processors, programmable logic controllers, microcode, microcontrollers, servers, microprocessors, digital signal processors or any other suitable circuit.
- IP intellectual property
- ASICs application-specific integrated circuits
- programmable logic arrays optical processors
- programmable logic controllers microcode, microcontrollers, servers, microprocessors, digital signal processors or any other suitable circuit.
- signal processor should be understood as encompassing any of the foregoing hardware, either singly or in combination.
- signals and “data” are used interchangeably. Further, portions of each of the clients and each of the servers described herein do not necessarily have to be implemented in the same manner.
- Operations that are described as being performed by a single processor, computer, or device can be distributed across a number of different processors, computers or devices. Similarly, operations that are described as being performed by different processors, computers, or devices can, in some cases, be performed by a single processor, computer or device.
- the systems described herein can be implemented using general purpose computers/processors with a computer program that, when executed, carries out any of the respective methods, algorithms and/or instructions described herein.
- special purpose computers/processors can be utilized which can contain specialized hardware for carrying out any of the methods, algorithms, or instructions described herein.
- At least one implementation of this disclosure relates to an apparatus for performing the operations herein.
- This apparatus may be specially constructed for the required purposes, or it may comprise a general-purpose computer selectively activated or reconfigured by a computer program stored on a computer readable storage medium that can be accessed by the computer.
- All or a portion of the embodiments of the disclosure can take the form of a computer program product accessible from, for example, a non-transitory computer-usable or computer-readable medium.
- the computer program when executed, can carry out any of the respective techniques, algorithms and/or instructions described herein.
- a non-transitory computer-usable or computer-readable medium can be any device that can, for example, tangibly contain, store, communicate, or transport the program for use by or in connection with any processor.
- the non-transitory medium can be, for example, any type of disk including floppy disks, optical disks, CD-ROMs, magnetic-optical disks, read-only memories (ROMs), random access memories (RAMs), EPROMs, EEPROMs, magnetic or optical cards, application specific integrated circuits (ASICs), or any type of media suitable for tangibly containing, storing, communicating, or transporting electronic instructions.
- any type of disk including floppy disks, optical disks, CD-ROMs, magnetic-optical disks, read-only memories (ROMs), random access memories (RAMs), EPROMs, EEPROMs, magnetic or optical cards, application specific integrated circuits (ASICs), or any type of media suitable for tangibly containing, storing, communicating, or transporting electronic instructions.
Abstract
Description
γjk =p(user category=k|user=j) [Equation 2]
p(z i =k|x i ,u i) [Equation 5]
f k(x i)=w k T(x i (1) −x i (2)) [Equation 9]
Claims (6)
log Πi=1 N p(x i ,u i),
log Πi=1 N p(x i ,u i),
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/517,767 US9477757B1 (en) | 2012-06-14 | 2012-06-14 | Latent user models for personalized ranking |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/517,767 US9477757B1 (en) | 2012-06-14 | 2012-06-14 | Latent user models for personalized ranking |
Publications (1)
Publication Number | Publication Date |
---|---|
US9477757B1 true US9477757B1 (en) | 2016-10-25 |
Family
ID=57137642
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/517,767 Expired - Fee Related US9477757B1 (en) | 2012-06-14 | 2012-06-14 | Latent user models for personalized ranking |
Country Status (1)
Country | Link |
---|---|
US (1) | US9477757B1 (en) |
Cited By (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN109558500A (en) * | 2018-11-21 | 2019-04-02 | 杭州网易云音乐科技有限公司 | Multimedia sequence generation method, medium, device and calculating equipment |
US20190189022A1 (en) * | 2017-12-20 | 2019-06-20 | International Business Machines Corporation | Personalized tutoring with automatic matching of content-modality and learner-preferences |
US11263323B2 (en) | 2018-12-19 | 2022-03-01 | Google Llc | Systems and methods for increasing robustness of machine-learned models and other software systems against adversarial attacks |
US11436661B2 (en) * | 2019-05-14 | 2022-09-06 | Samsung Electronics Co., Ltd. | Electronic device for providing information in response to query and method therefor |
Citations (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20020107853A1 (en) * | 2000-07-26 | 2002-08-08 | Recommind Inc. | System and method for personalized search, information filtering, and for generating recommendations utilizing statistical latent class models |
US20090055139A1 (en) | 2007-08-20 | 2009-02-26 | Yahoo! Inc. | Predictive discrete latent factor models for large scale dyadic data |
US20090083126A1 (en) | 2007-09-26 | 2009-03-26 | At&T Labs, Inc. | Methods and Apparatus for Modeling Relationships at Multiple Scales in Ratings Estimation |
US7587391B1 (en) | 2006-06-13 | 2009-09-08 | Google Inc. | Method and apparatus for generating a preference ranking |
US20100100416A1 (en) | 2008-10-17 | 2010-04-22 | Microsoft Corporation | Recommender System |
US20110035379A1 (en) | 2009-08-10 | 2011-02-10 | Ye Chen | Probabilistic clustering of an item |
US20110179081A1 (en) * | 2010-01-19 | 2011-07-21 | Maksims Ovsjanikov | Personalized recommendation of a volatile item |
US20110184806A1 (en) | 2010-01-27 | 2011-07-28 | Ye Chen | Probabilistic recommendation of an item |
US20110238608A1 (en) * | 2010-03-25 | 2011-09-29 | Nokia Corporation | Method and apparatus for providing personalized information resource recommendation based on group behaviors |
US8037080B2 (en) * | 2008-07-30 | 2011-10-11 | At&T Intellectual Property Ii, Lp | Recommender system utilizing collaborative filtering combining explicit and implicit feedback with both neighborhood and latent factor models |
US8386488B2 (en) * | 2004-04-27 | 2013-02-26 | International Business Machines Corporation | Method and system for matching appropriate content with users by matching content tags and profiles |
-
2012
- 2012-06-14 US US13/517,767 patent/US9477757B1/en not_active Expired - Fee Related
Patent Citations (13)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6687696B2 (en) | 2000-07-26 | 2004-02-03 | Recommind Inc. | System and method for personalized search, information filtering, and for generating recommendations utilizing statistical latent class models |
US7328216B2 (en) | 2000-07-26 | 2008-02-05 | Recommind Inc. | System and method for personalized search, information filtering, and for generating recommendations utilizing statistical latent class models |
US20020107853A1 (en) * | 2000-07-26 | 2002-08-08 | Recommind Inc. | System and method for personalized search, information filtering, and for generating recommendations utilizing statistical latent class models |
US8386488B2 (en) * | 2004-04-27 | 2013-02-26 | International Business Machines Corporation | Method and system for matching appropriate content with users by matching content tags and profiles |
US7587391B1 (en) | 2006-06-13 | 2009-09-08 | Google Inc. | Method and apparatus for generating a preference ranking |
US20090055139A1 (en) | 2007-08-20 | 2009-02-26 | Yahoo! Inc. | Predictive discrete latent factor models for large scale dyadic data |
US20090083126A1 (en) | 2007-09-26 | 2009-03-26 | At&T Labs, Inc. | Methods and Apparatus for Modeling Relationships at Multiple Scales in Ratings Estimation |
US8037080B2 (en) * | 2008-07-30 | 2011-10-11 | At&T Intellectual Property Ii, Lp | Recommender system utilizing collaborative filtering combining explicit and implicit feedback with both neighborhood and latent factor models |
US20100100416A1 (en) | 2008-10-17 | 2010-04-22 | Microsoft Corporation | Recommender System |
US20110035379A1 (en) | 2009-08-10 | 2011-02-10 | Ye Chen | Probabilistic clustering of an item |
US20110179081A1 (en) * | 2010-01-19 | 2011-07-21 | Maksims Ovsjanikov | Personalized recommendation of a volatile item |
US20110184806A1 (en) | 2010-01-27 | 2011-07-28 | Ye Chen | Probabilistic recommendation of an item |
US20110238608A1 (en) * | 2010-03-25 | 2011-09-29 | Nokia Corporation | Method and apparatus for providing personalized information resource recommendation based on group behaviors |
Non-Patent Citations (2)
Title |
---|
Expectation-maximization algorithm, website, May 29, 2012, en.wikipedia.org/wiki/Expectation-maximization-algorithm, Wikipedia Foundation, Inc. |
Sargin, Mehmet Emre, and Aradhye, Hrishikesh, Boosting Video Classification Using Cross-Video Signals, IEEE 2011, pp. 1805-1808, Google Inc. |
Cited By (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20190189022A1 (en) * | 2017-12-20 | 2019-06-20 | International Business Machines Corporation | Personalized tutoring with automatic matching of content-modality and learner-preferences |
US10832584B2 (en) * | 2017-12-20 | 2020-11-10 | International Business Machines Corporation | Personalized tutoring with automatic matching of content-modality and learner-preferences |
CN109558500A (en) * | 2018-11-21 | 2019-04-02 | 杭州网易云音乐科技有限公司 | Multimedia sequence generation method, medium, device and calculating equipment |
US11263323B2 (en) | 2018-12-19 | 2022-03-01 | Google Llc | Systems and methods for increasing robustness of machine-learned models and other software systems against adversarial attacks |
US11436661B2 (en) * | 2019-05-14 | 2022-09-06 | Samsung Electronics Co., Ltd. | Electronic device for providing information in response to query and method therefor |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
Li et al. | Unsupervised streaming feature selection in social media | |
US9811781B2 (en) | Time-series data prediction device of observation value, time-series data prediction method of observation value, and program | |
US9536201B2 (en) | Identifying associations in data and performing data analysis using a normalized highest mutual information score | |
US7853599B2 (en) | Feature selection for ranking | |
US7979426B2 (en) | Clustering-based interest computation | |
Ocepek et al. | Improving matrix factorization recommendations for examples in cold start | |
US11636314B2 (en) | Training neural networks using a clustering loss | |
US9754188B2 (en) | Tagging personal photos with deep networks | |
EP3304350B1 (en) | Column ordering for input/output optimization in tabular data | |
US20090106179A1 (en) | System and method for the longitudinal analysis of education outcomes using cohort life cycles, cluster analytics-based cohort analysis, and probablistic data schemas | |
WO2021135562A1 (en) | Feature validity evaluation method and apparatus, and electronic device and storage medium | |
US20110307542A1 (en) | Active Image Tagging | |
US20110264617A1 (en) | Reducing the dissimilarity between a first multivariate data set and a second multivariate data set | |
CN108021708B (en) | Content recommendation method and device and computer readable storage medium | |
Hornick et al. | Extending recommender systems for disjoint user/item sets: The conference recommendation problem | |
CN106599194B (en) | Label determining method and device | |
JP2021103542A (en) | Information providing device, information providing method, and program | |
US8121967B2 (en) | Structural data classification | |
US10146872B2 (en) | Method and system for predicting search results quality in vertical ranking | |
US20190273789A1 (en) | Establishing and utilizing behavioral data thresholds for deep learning and other models to identify users across digital space | |
US20230195809A1 (en) | Joint personalized search and recommendation with hypergraph convolutional networks | |
US9477757B1 (en) | Latent user models for personalized ranking | |
CN111522886B (en) | Information recommendation method, terminal and storage medium | |
US20220129709A1 (en) | Systems and Methods for Preference and Similarity Learning | |
VanDerwerken et al. | Monitoring joint convergence of MCMC samplers |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:NING, HUAZHONG;LI, ZHEN;ARADHYE, HRISHIKESH;REEL/FRAME:028373/0731Effective date: 20120613 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044097/0658Effective date: 20170929 |
|
FEPP | Fee payment procedure |
Free format text: MAINTENANCE FEE REMINDER MAILED (ORIGINAL EVENT CODE: REM.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
LAPS | Lapse for failure to pay maintenance fees |
Free format text: PATENT EXPIRED FOR FAILURE TO PAY MAINTENANCE FEES (ORIGINAL EVENT CODE: EXP.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
STCH | Information on status: patent discontinuation |
Free format text: PATENT EXPIRED DUE TO NONPAYMENT OF MAINTENANCE FEES UNDER 37 CFR 1.362 |
|
FP | Expired due to failure to pay maintenance fee |
Effective date: 20201025 |