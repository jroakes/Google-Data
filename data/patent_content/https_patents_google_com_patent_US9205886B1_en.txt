US9205886B1 - Systems and methods for inventorying objects - Google Patents
Systems and methods for inventorying objects Download PDFInfo
- Publication number
- US9205886B1 US9205886B1 US13/463,596 US201213463596A US9205886B1 US 9205886 B1 US9205886 B1 US 9205886B1 US 201213463596 A US201213463596 A US 201213463596A US 9205886 B1 US9205886 B1 US 9205886B1
- Authority
- US
- United States
- Prior art keywords
- map
- robot
- location
- objects
- area
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
- 238000000034 method Methods 0.000 title claims abstract description 118
- 230000006870 function Effects 0.000 claims description 51
- 238000013475 authorization Methods 0.000 claims description 3
- 230000004044 response Effects 0.000 abstract description 15
- 235000016213 coffee Nutrition 0.000 description 133
- 235000013353 coffee beverage Nutrition 0.000 description 133
- 238000013507 mapping Methods 0.000 description 43
- 238000003860 storage Methods 0.000 description 27
- 230000008569 process Effects 0.000 description 26
- 230000009471 action Effects 0.000 description 23
- 230000037361 pathway Effects 0.000 description 19
- 235000015114 espresso Nutrition 0.000 description 14
- 238000012545 processing Methods 0.000 description 14
- 238000010586 diagram Methods 0.000 description 11
- 230000003993 interaction Effects 0.000 description 11
- 238000004891 communication Methods 0.000 description 10
- 230000001413 cellular effect Effects 0.000 description 9
- 230000033001 locomotion Effects 0.000 description 8
- 239000000463 material Substances 0.000 description 5
- 230000006399 behavior Effects 0.000 description 4
- CDBYLPFSWZWCQE-UHFFFAOYSA-L Sodium Carbonate Chemical compound [Na+].[Na+].[O-]C([O-])=O CDBYLPFSWZWCQE-UHFFFAOYSA-L 0.000 description 3
- 238000004422 calculation algorithm Methods 0.000 description 3
- 230000003287 optical effect Effects 0.000 description 3
- 241001465754 Metazoa Species 0.000 description 2
- 230000006978 adaptation Effects 0.000 description 2
- 238000004458 analytical method Methods 0.000 description 2
- 210000004556 brain Anatomy 0.000 description 2
- 238000004364 calculation method Methods 0.000 description 2
- 230000007774 longterm Effects 0.000 description 2
- 238000007726 management method Methods 0.000 description 2
- 235000012054 meals Nutrition 0.000 description 2
- 230000000737 periodic effect Effects 0.000 description 2
- 230000002093 peripheral effect Effects 0.000 description 2
- 230000002085 persistent effect Effects 0.000 description 2
- 238000013439 planning Methods 0.000 description 2
- 239000000047 product Substances 0.000 description 2
- 230000003442 weekly effect Effects 0.000 description 2
- 238000012896 Statistical algorithm Methods 0.000 description 1
- 230000002776 aggregation Effects 0.000 description 1
- 238000004220 aggregation Methods 0.000 description 1
- 230000009118 appropriate response Effects 0.000 description 1
- 230000008901 benefit Effects 0.000 description 1
- 230000010267 cellular communication Effects 0.000 description 1
- 235000013339 cereals Nutrition 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 239000003086 colorant Substances 0.000 description 1
- 238000011161 development Methods 0.000 description 1
- 230000000694 effects Effects 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 238000000605 extraction Methods 0.000 description 1
- 238000010191 image analysis Methods 0.000 description 1
- 238000003780 insertion Methods 0.000 description 1
- 230000037431 insertion Effects 0.000 description 1
- 230000002452 interceptive effect Effects 0.000 description 1
- 230000004807 localization Effects 0.000 description 1
- 238000012423 maintenance Methods 0.000 description 1
- 230000007257 malfunction Effects 0.000 description 1
- 238000004519 manufacturing process Methods 0.000 description 1
- 230000007246 mechanism Effects 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 238000012015 optical character recognition Methods 0.000 description 1
- 238000005192 partition Methods 0.000 description 1
- 239000000779 smoke Substances 0.000 description 1
- 235000014214 soft drink Nutrition 0.000 description 1
- 230000003068 static effect Effects 0.000 description 1
- 239000013589 supplement Substances 0.000 description 1
- 230000001360 synchronised effect Effects 0.000 description 1
- 210000003813 thumb Anatomy 0.000 description 1
- 238000012384 transportation and delivery Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Images
Classifications
-
- B—PERFORMING OPERATIONS; TRANSPORTING
- B62—LAND VEHICLES FOR TRAVELLING OTHERWISE THAN ON RAILS
- B62D—MOTOR VEHICLES; TRAILERS
- B62D57/00—Vehicles characterised by having other propulsion or other ground- engaging means than wheels or endless track, alone or in addition to wheels or endless track
- B62D57/02—Vehicles characterised by having other propulsion or other ground- engaging means than wheels or endless track, alone or in addition to wheels or endless track with ground-engaging propulsion means, e.g. walking members
- B62D57/032—Vehicles characterised by having other propulsion or other ground- engaging means than wheels or endless track, alone or in addition to wheels or endless track with ground-engaging propulsion means, e.g. walking members with alternately or sequentially lifted supporting base and legs; with alternately or sequentially lifted feet or skid
-
- B—PERFORMING OPERATIONS; TRANSPORTING
- B25—HAND TOOLS; PORTABLE POWER-DRIVEN TOOLS; MANIPULATORS
- B25J—MANIPULATORS; CHAMBERS PROVIDED WITH MANIPULATION DEVICES
- B25J9/00—Programme-controlled manipulators
- B25J9/16—Programme controls
- B25J9/1656—Programme controls characterised by programming, planning systems for manipulators
-
- B—PERFORMING OPERATIONS; TRANSPORTING
- B25—HAND TOOLS; PORTABLE POWER-DRIVEN TOOLS; MANIPULATORS
- B25J—MANIPULATORS; CHAMBERS PROVIDED WITH MANIPULATION DEVICES
- B25J9/00—Programme-controlled manipulators
- B25J9/16—Programme controls
- B25J9/1694—Programme controls characterised by use of sensors other than normal servo-feedback from position, speed or acceleration sensors, perception control, multi-sensor controlled systems, sensor fusion
- B25J9/1697—Vision controlled systems
-
- G—PHYSICS
- G05—CONTROLLING; REGULATING
- G05D—SYSTEMS FOR CONTROLLING OR REGULATING NON-ELECTRIC VARIABLES
- G05D1/00—Control of position, course or altitude of land, water, air, or space vehicles, e.g. automatic pilot
- G05D1/02—Control of position or course in two dimensions
- G05D1/021—Control of position or course in two dimensions specially adapted to land vehicles
- G05D1/0231—Control of position or course in two dimensions specially adapted to land vehicles using optical position detecting means
- G05D1/0246—Control of position or course in two dimensions specially adapted to land vehicles using optical position detecting means using a video camera in combination with image processing means
-
- G—PHYSICS
- G05—CONTROLLING; REGULATING
- G05D—SYSTEMS FOR CONTROLLING OR REGULATING NON-ELECTRIC VARIABLES
- G05D1/00—Control of position, course or altitude of land, water, air, or space vehicles, e.g. automatic pilot
- G05D1/02—Control of position or course in two dimensions
- G05D1/021—Control of position or course in two dimensions specially adapted to land vehicles
- G05D1/0268—Control of position or course in two dimensions specially adapted to land vehicles using internal positioning means
- G05D1/0274—Control of position or course in two dimensions specially adapted to land vehicles using internal positioning means using mapping information stored in a memory device
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06Q—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES; SYSTEMS OR METHODS SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES, NOT OTHERWISE PROVIDED FOR
- G06Q10/00—Administration; Management
- G06Q10/08—Logistics, e.g. warehousing, loading or distribution; Inventory or stock management
- G06Q10/087—Inventory or stock management, e.g. order filling, procurement or balancing against orders
-
- G—PHYSICS
- G05—CONTROLLING; REGULATING
- G05B—CONTROL OR REGULATING SYSTEMS IN GENERAL; FUNCTIONAL ELEMENTS OF SUCH SYSTEMS; MONITORING OR TESTING ARRANGEMENTS FOR SUCH SYSTEMS OR ELEMENTS
- G05B2219/00—Program-control systems
- G05B2219/30—Nc systems
- G05B2219/40—Robotics, robotics mapping to robotics vision
- G05B2219/40543—Identification and location, position of components, objects
Definitions
- Cloud computing refers to provision of computational resources via a computer network.
- both data and software are fully contained on a user's computer.
- the user's computer may contain relatively little software or data (perhaps a minimal operating system and web browser, for example), and may serve as a display terminal for processes occurring on a network of computers.
- a common shorthand provided for a cloud computing service is “the cloud”.
- Cloud computing has been referred to as “client-server computing”, however, there may be distinctions between general cloud computing and client-server computing.
- client-server computing may include a distributed application structure that partitions tasks or workloads between providers of a resource or service (e.g., servers), and service requesters (e.g., clients).
- Client-server computing generally involves a one-to-one relationship between the server and the client, whereas cloud computing includes generic services that can be accessed by generic clients (e.g., a one-to-one relationship or connection may not be required).
- cloud computing generally includes client-server computing, and additional services and functionality.
- Cloud computing may free users from certain hardware and software installation and maintenance tasks through use of simpler hardware on the user's computer that accesses a vast network of computing resources (e.g., processors, hard drives, etc.). Sharing of resources may reduce cost to individuals.
- any computer connected to the cloud may be connected to the same pool of computing power, applications, and files. Users can store and access personal files such as music, pictures, videos, and bookmarks or play games or use productivity applications on a remote server rather than physically carrying around a storage medium, such as a DVD or thumb drive.
- a user may open a browser and connect to a host of web servers that run user interface software that collect commands from the user and interpret the commands into commands on the servers.
- the servers may handle the computing, and can either store or retrieve information from database servers or file servers and display an updated page to the user.
- cloud computing data across multiple servers can be synchronized around the world allowing for collaborative work on one file or project, from multiple users around the world, for example.
- the present application discloses, inter alia, methods and systems for robot cloud computing.
- Any of the methods described herein may be provided in a form of instructions stored on a non-transitory, computer readable medium, that when executed by a computing device, cause the computing device to perform functions of the method. Further examples may also include articles of manufacture including tangible computer-readable media that have computer-readable instructions encoded thereon, and the instructions may comprise instructions to perform functions of the methods described herein.
- the computer readable medium may include non-transitory computer readable medium, for example, such as computer-readable media that stores data for short periods of time like register memory, processor cache and Random Access Memory (RAM).
- the computer readable medium may also include non-transitory media, such as secondary or persistent long term storage, like read only memory (ROM), optical or magnetic disks, compact-disc read only memory (CD-ROM), for example.
- the computer readable media may also be any other volatile or non-volatile storage systems.
- the computer readable medium may be considered a computer readable storage medium, for example, or a tangible storage medium.
- circuitry may be provided that is wired to perform logical functions in any processes or methods described herein.
- many types of devices may be used or configured as means for performing functions of any of the methods described herein (or any portions of the methods described herein).
- a method may include a cloud computing system receiving, from a robotic device having at least one sensor, identification data corresponding to an object detected by the robotic device.
- the received identification data may be an image of the object, an identifier from an RFID tag or similar type of identifier tag associated with the object, and/or other identification data associated with the object (dimensions, texture, estimated weight, etc.).
- the method may further include the cloud computing system looking up the received identification data in a database system associated with the cloud computing system to identify the object corresponding to the identification data received from the robotic device.
- the cloud computing system may also receive, from the robotic device, a current location of the identified object.
- the cloud computing system may also identify (or alternatively generate) a first map in the database system, where the first map is associated with the current location of the identified object.
- the cloud computing system may also identify a second map in the database system, where the second map is associated with a past location of the identified object.
- the cloud computing system may also compare the first map and the second map to determine if the current location of the identified object in the first map is different than the past location of the identified object in the second map.
- the cloud computing system may then send instructions to the robotic device based on one or more differences between the first map and the second map.
- a device may include a non-transitory computer-readable medium and program instructions stored on the non-transitory computer-readable medium and executable by at least one processor to perform a number of steps.
- the steps may include a cloud computing system receiving, from a robotic device, identification data corresponding to an object detected by the robotic device, and receiving, also from the robotic device, a current location of the object.
- the steps may also include the cloud computing system using the identification data to identify the object in a database system associated with the cloud computing system, and to also identify in the database system, a first map associated with the current location of the identified object and a second map associated with a past location of the identified object.
- the steps may further include the cloud computing system comparing the first map and the second map to determine if the current location of the identified object is different than the past location of the identified object. Instructions may be sent to the robotic device based on differences between the first map and the second map.
- Another example may include a non-transitory computer-readable medium having stored thereon instructions executable by a computing device to cause the computing device to perform a number of functions.
- the functions may include receiving from a robotic device identification data associated with at least one object detected by the robotic device, identifying the at least one object, and receiving from the robotic device a current location of the identified at least one object.
- the functions may also using the identification data to further identify, via a database lookup, a first map associated with the current location of the identified at least one object and a second map associated with a past location of the identified at least one object.
- the functions may further include comparing the first map and the second map to determine if the current location of the identified at least one object is different than the past location of the identified at least one object. Instructions may be sent to the robotic device based on differences between the first map and the second map.
- FIG. 1 is an example system for cloud-based computing
- FIG. 2A illustrates an example client device
- FIG. 2B illustrates a graphical example of a robot
- FIG. 2C illustrates another example of a robot
- FIG. 3 illustrates an example of a conceptual robot-cloud interaction
- FIG. 4 is an example system in which robots may interact with the cloud and share information with other cloud computing devices
- FIG. 5 is a block diagram of an example method of a robot interaction with the cloud to facilitate object recognition
- FIG. 6 is an example conceptual illustration of a robot interacting with a cloud to perform object recognition and interaction
- FIG. 7 is an example conceptual illustration of a mapping function
- FIG. 8 is a block diagram of an example method for mapping of objects in an area or mapping an area
- FIGS. 9A-9B are example interfaces illustrating a map of an area, and objects in the map
- FIG. 10 is a block diagram of an example method for mapping of objects in an area and performing inventory of objects
- FIG. 11 is a block diagram of an example method for performing voice recognition/control by a robot, all arranged in accordance with at least some embodiments described herein;
- FIG. 12 is a block diagram of an example method for processing data from a robot.
- FIG. 13A is an example conceptual illustration of a first map of an area
- FIG. 13B is an example conceptual illustration of a second map of an area
- FIG. 14 is an example conceptual illustration of a robot interacting with an object.
- FIG. 15 is an example conceptual illustration of a robot interacting with an object.
- cloud-based computing generally refers to networked computer architectures in which application execution and storage may be divided, to some extent, between client and server devices.
- a robot may be any device that has a computing ability and interacts with its surroundings with an actuation capability (e.g., electromechanical capabilities).
- a client device may be configured as a robot including various sensors and devices in the forms of modules, and different modules may be added or removed from a robot depending on requirements.
- a robot may be configured to receive a second device, such as mobile phone, that may be configured to function as an accessory or a “brain” of the robot.
- a robot may interact with the cloud to perform any number of actions, such as to share information with other cloud computing devices.
- a robot may interact with the cloud to facilitate object recognition, to perform a mapping function, or to perform navigational functions (i.e., receive a map/navigation pathway previously traversed by another robot).
- a robot may interact with the cloud to perform mapping of objects in an area, to perform inventory of objects, and to perform voice recognition by and/or control of a robot.
- a robot may perform any actions or queries to the cloud as described herein based on contextual or situational information.
- some embodiments enable robots to store and access data at an external location, such as on a server and/or other computing device.
- the external location may receive data and/or or requests from one or more robots.
- a server may store received data from one or more robots and/or distribute all or part of the stored data to one or more robots so as to create a common knowledge base amongst the robots, where robots can obtain instructions and/or data.
- the robot or server may create or obtain a map of an area including a mapping of all objects in the area (locations and identifications of the objects).
- the robot may traverse through the same area at a later time to build the map representing the area at the later time.
- the robot or server may compare the two maps to determine and/or identify any differences. For example, a robot may perform a first mapping of a room to identify a base map (e.g., configuration of the room in a normal situation).
- the robot may perform subsequent mapping to identify new items placed in the room, such as a set of keys, to help a user find their keys.
- the base map may be the benchmark map of the room in a clean state, so that subsequent maps can be performed to determine a differential between the benchmark and a current condition of the room.
- the robot or server may compare a current inventory with the prior benchmark inventory.
- the robot or server may create an inventory of objects in an area, along with a mapping of the objects.
- the robot or server may catalog all objects in the space and store details of the objects (name, shape, color, last known location, etc.).
- the robot may follow a navigation path to inventory a space, so that the same navigation path can be traversed to identify missing and/or new objects.
- the robot may be an inventory robot configured to move about through a store to take inventory of what is in stock.
- all (or most) objects in an area may be tagged with a type of identifier (e.g., RFID, near field communication chip (NFC), Bluetooth transceiver, bar code).
- the robot may execute an application enabling the robot to identify the object based on the identifier.
- the robot may use the identifier to obtain information about the object including an identification of the object, details of the object (mass, color, type, brand, etc.), a location of the object, etc.
- the identifier may enable the localization and/or identification of the objects in the area.
- some objects may emit beacons that can be received by a computing device, sensors, the robot, and/or the server. These beacons may contain an identifier similar to (or the same as) the identifier described above.
- FIG. 1 is an example system 100 for cloud-based computing.
- Cloud-based computing generally refers to networked computer architectures in which application execution and storage may be divided, to some extent, between client and server devices.
- a “cloud” may refer to a service or a group of services accessible over a network (e.g., Internet) by client and server devices, for example.
- any computer connected to the cloud may be connected to the same pool of computing power, applications, and files.
- cloud computing enables a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be provisioned and released with minimal management effort or service provider interaction. Users can store and access personal files such as music, pictures, videos, and bookmarks or play games or use productivity applications on a remote server rather than physically carrying around a storage medium.
- a cloud-based application may store copies of data and/or executable program logic at remote server devices, while allowing client devices to download at least some of this data and program logic as needed for execution at the client devices.
- downloaded data and program logic can be tailored to capabilities of specific client devices (e.g., a personal computer, tablet, or mobile phone, or robot) accessing the cloud based application.
- client devices e.g., a personal computer, tablet, or mobile phone, or robot
- dividing application execution and storage between the client and server devices allows more processing to be performed by the server devices, thereby taking advantage of the server devices' processing power and capability, for example.
- Cloud-based computing can also refer to distributed computing architectures in which data and program logic for a cloud-based application are shared between one or more client devices and/or server devices on a near real-time basis. Parts of this data and program logic may be dynamically delivered, as needed or otherwise, to various clients accessing the cloud-based application. Details of the architecture may be transparent to users of client devices. Thus, a PC user or robot client device accessing a cloud-based application may not be aware that the PC or robot downloads program logic and/or data from the server devices, or that the PC or robot offloads processing or storage functions to the server devices, for example.
- a cloud 102 includes a cloud service 104 , a cloud platform 106 , a cloud infrastructure 108 , and a database 110 .
- the cloud 102 may include more of fewer components, and each of the cloud service 104 , the cloud platform 106 , the cloud infrastructure 108 , and the database 110 may comprise multiple elements as well.
- one or more of the described functions of the system 100 may be divided into additional functional or physical components, or combined into fewer functional or physical components.
- the cloud 102 may represent a networked computer architecture, and in one example, the cloud service 104 represents a queue for handling requests from client devices.
- the cloud platform 106 may include a frontend of the cloud and may be coupled to the cloud service 104 to perform functions to interact with client devices.
- the cloud platform 106 may include applications used to access the cloud 102 via a user interface, such as a web browser.
- the cloud infrastructure 108 may include service application of billing components of the cloud 102 , and thus, may interact with the cloud service 104 .
- the database 110 may represent storage capabilities by the cloud 102 , and thus, may be accessed by any of the cloud service 104 , the cloud platform 106 , and/or the infrastructure 108 .
- the system 100 includes a number of client devices coupled to or configured to be capable of communicating with components of the cloud 102 .
- client devices coupled to or configured to be capable of communicating with components of the cloud 102 .
- a computer 112 a mobile device 114 , a host 116 , and a robot client 118 are shown coupled to the cloud 102 .
- more or fewer client devices may be coupled to the cloud 102 .
- different types of client devices may be coupled to the cloud 102 .
- any of the client devices may generally comprise a display system, memory, and a processor.
- the computer 112 may be any type of computing device (e.g., PC, laptop computer, etc.), and the mobile device 114 may be any type of mobile computing device (e.g., laptop, mobile telephone, cellular telephone, etc.).
- the host 116 may be any type of computing device with a transmitter/receiver including a laptop computer, a mobile telephone, etc., that is configured to transmit/receive data to/from the cloud 102 .
- the robot client 118 may comprise any computing device that has connection abilities to the cloud 102 and that has an actuation capability (e.g., electromechanical capabilities). A robot may further be a combination of computing devices. In some examples, the robot 118 may collect data and upload the data to the cloud 102 . The cloud 102 may be configured to perform calculations or analysis on the data and return processed data to the robot client 118 . In some examples, as shown in FIG. 1 , the cloud 102 may include a computer that is not co-located with the robot client 118 . In other examples, the robot client 118 may send data to a second client (e.g., computer 112 ) for processing.
- a second client e.g., computer 112
- the robot client 118 may include one or more sensors, such as a gyroscope or an accelerometer to measure movement of the robot client 118 .
- Other sensors may further include any of Global Positioning System (GPS) receivers, infrared sensors, sonar, optical sensors, biosensors, Radio Frequency identification (RFID) systems, Near Field Communication (NFC) chip, wireless sensors, and/or compasses, among others, for example.
- GPS Global Positioning System
- RFID Radio Frequency identification
- NFC Near Field Communication
- any of the client devices may include an integrated user-interface (UI) that allows a user to interact with the device.
- UI user-interface
- the robot client 118 may include various buttons and/or a touchscreen interface that allow a user to provide input.
- the robot client device 118 may include a microphone configured to receive voice commands from a user.
- the robot client 118 may include one or more interfaces that allow various types of user-interface devices to be connected to the robot client 118 .
- communication links between client devices and the cloud 102 may include wired connections, such as a serial or parallel bus.
- Communication links may also be wireless links, such as link 120 , which may include Bluetooth, IEEE 802.11 (IEEE 802.11 may refer to IEEE 802.11-2007, IEEE 802.11n-2009, or any other IEEE 802.11 revision), or other wireless based communication links.
- IEEE 802.11 may refer to IEEE 802.11-2007, IEEE 802.11n-2009, or any other IEEE 802.11 revision
- the system 100 may include access points through which the client devices may communicate with the cloud 102 .
- Access points may take various forms, for example, an access point may take the form of a wireless access point (WAP) or wireless router.
- WAP wireless access point
- an access point may be a base station in a cellular network that provides Internet connectivity via the cellular network.
- the client devices may include a wired or wireless network interface through which the client devices can connect to the cloud 102 (or access points).
- the client devices may be configured use one or more protocols such as 802.11, 802.16 (WiMAX), LTE, GSM, GPRS, CDMA, EV-DO, and/or HSPDA, among others.
- the client devices may be configured to use multiple wired and/or wireless protocols, such as “3G” or “4G” data connectivity using a cellular communication protocol (e.g., CDMA, GSM, or WiMAX, as well as for “WiFi” connectivity using 802.11).
- a cellular communication protocol e.g., CDMA, GSM, or WiMAX, as well as for “WiFi” connectivity using 802.11).
- Other examples are also possible.
- FIG. 2A illustrates an example client device 200 .
- the client device 200 is configured as a robot.
- a robot may contain computer hardware, such as a processor 202 , memory or storage 204 , and sensors 206 .
- a robot controller e.g., processor 202 , computing system, and sensors 206
- the robot may have a link by which the link can access cloud servers (as shown in FIG. 1 ).
- a wired link may include, for example, a parallel bus or a serial bus such as a Universal Serial Bus (USB).
- a wireless link may include, for example, Bluetooth, IEEE 802.11, Cellular (such as GSM, CDMA, UMTS, EV-DO, WiMAX, or LTE), or Zigbee, among other possibilities.
- the storage 204 may be used for compiling data from various sensors 206 of the robot and storing program instructions.
- the processor 202 may be coupled to the storage 204 and may be configured to control the robot based on the program instructions.
- the processor 202 may also be able to interpret data from the various sensors 206 on the robot.
- Example sensors may include, smoke sensors, light sensors, radio sensors, infrared sensors, microphones, speakers, gyroscope, accelerometer, a camera, radar, capacitive sensors and touch sensors, etc.
- the client device 200 may also have components or devices that allow the client device 200 to interact with its environment.
- the client device 200 may have mechanical actuators 208 , such as motors, wheels, movable arms, etc., that enable the client device 200 to move or interact with the environment.
- various sensors and devices on the client device 200 may be modules. Different modules may be added or removed from a client device 200 depending on requirements. For example, in a low power situation, a robot may have fewer modules to reduce power usages. However, additional sensors may be added as needed. To increase an amount of data a robot may be able to collect, additional sensors may be added, for example.
- the client device 200 may be configured to receive a device, such as device 210 , that includes the processor 202 , the storage 204 , and the sensors 206 .
- the client device 200 may be a robot that has a number of mechanical actuators (e.g., a movable base), and the robot may be configured to receive a mobile telephone, smartphone, tablet computer, etc. to function as the “brains” or control components of the robot.
- the device 210 may be considered a module of the robot.
- the device 210 may be physically attached to the robot.
- a smartphone may sit on a robot's “chest” and form an interactive display.
- the device 210 may provide a robot with sensors, a wireless link, and processing capabilities, for example.
- the device 210 may allow a user to download new routines for his or her robot from the cloud.
- a laundry folding routine may be stored on the cloud, and a user may be able to select this routine using a smartphone to download the routine from the cloud, and when the smartphone is placed into or coupled to the robot, the robot would be able to perform the downloaded action.
- the client device 200 may be coupled to a mobile or cellular telephone to provide additional sensing capabilities.
- the cellular phone may not be physically attached to the robot, but may be coupled to the robot wirelessly.
- a low cost robot may omit a direct connection to the Internet.
- This robot may be able to connect to a user's cellular phone via a wireless technology (e.g., Bluetooth) to be able to access the Internet.
- the robot may be able to access various sensors and communication means of the cellular phone.
- the robot may not need as many sensors to be physically provided on the robot, however, the robot may be able to keep the same or similar functionality.
- the client device 200 may include mechanical robot features, and may be configured to receive the device 210 (e.g., a mobile phone, smartphone, tablet computer, etc.), which can provide additional peripheral components to the device 200 , such as any of an accelerometer, gyroscope, compass, GPS, camera, WiFi connection, a touch screen, etc., that are included within the device 210 .
- the device 210 e.g., a mobile phone, smartphone, tablet computer, etc.
- FIG. 2B illustrates a graphical example of a robot 212 .
- the robot 212 is shown as a mechanical form of a person including arms, legs, and a head.
- the robot 212 may be configured to receive any number of modules or components, such a mobile phone, which may be configured to operate the robot.
- a device e.g., robot 212
- a mobile phone e.g., device 210
- Other types of devices that have connectivity to the Internet can be coupled to robot 212 to provide additional functions on the robot 212 .
- the device 210 may be separate from the robot 212 and can be attached or coupled to the robot 212 .
- the robot 212 may be a toy with only limited mechanical functionality, and by connecting device 210 to the robot 212 , the toy robot 212 may now be capable of performing a number of functions with the aid of the device 210 and/or the cloud.
- the robot 212 (or components of a robot) can be attached to a mobile phone to transform the mobile phone into a robot (e.g., with legs/arms) that is connected to a server to cause operation/functions of the robot.
- the mountable device 210 may further be configured to maximize runtime usage of the robot 212 (e.g., if the robot 212 could learn what happens to cause the user to turn the toy off or set the toy down, the device 210 may be configured to perform functions to counteract such occurrences).
- FIG. 2C illustrates another example of a robot 214 .
- the robot 214 includes a computing device 216 , sensors 218 , and a mechanical actuator 220 .
- the computing device 216 may be a laptop computer, which may be coupled to the sensors 218 .
- the sensors 218 may include a camera, infrared projectors, and other motion sensing or vision sensing elements.
- the mechanical actuator 220 may include a base, wheels, and a motor upon which the computing device 216 and the sensors 218 can be positioned, for example.
- Any of the robots illustrated in FIGS. 2A-2C may be configured to operate according to a robot operating system (e.g., an operating system designed for specific functions of the robot).
- a robot operating system may provide libraries and tools (e.g., hardware abstraction, device drivers, visualizers, message-passing, package management, etc.) to enable robot applications.
- robot operating systems include open source software such as ROS (robot operating system), DROS, or ARCOS (advanced robotics control operating system); proprietary software such as the robotic development platform ESRP from Evolution Robotics® and MRDS (Microsoft® Robotics Developer Studio), and other examples may also include ROSJAVA.
- a robot operating system may include publish and subscribe functionality, and may also include functionality to control components of the robot, such as head tracking, base movement (e.g., velocity control, navigation framework), etc.
- FIG. 3 illustrates an example of a conceptual robot-cloud interaction.
- a robot such as a robot described and illustrated in FIG. 2 , may connect to a network of computers (e.g., the cloud), and may request data or processing to be performed by the cloud.
- the robot may include a number of sensors and mechanical actuators that may generally provide motor control for the robot.
- Outputs of the sensors such as camera feeds, vision sensors, etc., may be provided to the cloud, which can process the outputs to enable the robot to perform functions.
- the cloud may process a camera feed, for example, to determine a location of a robot, perform object recognition, or to indicate a navigation pathway for the robot.
- any of the modules may be interconnected, and/or may communicate to receive data or instructions from each other so as to provide a specific output or functionality for the robot.
- the robot may send data to a cloud for data processing, and in another example the robot may receive data from the cloud.
- the data received from the cloud may be in many different forms.
- the received data may be a processed form of data the robot sent to the cloud.
- the received data may also come from sources other than the robot.
- the cloud may have access to other sensors, other robots, and the Internet.
- FIG. 4 is an example system 400 in which robots may interact with the cloud and share information with other cloud computing devices.
- the system 400 illustrates robots 402 , 404 , 406 , and 408 (e.g., as conceptual graphical representations) each coupled to a cloud 410 .
- Each robot 402 , 404 , 406 , and 408 may interact with the cloud 410 , and may further interact with each other through the cloud 410 , or through other access points and possibly directly (e.g., as shown between robots 406 and 408 ).
- the cloud 410 may receive input from several robots. Data from each robot may be complied into a larger data set. For example, the robot 402 may take a picture of an object and upload the picture to the cloud 410 .
- An object recognition program on the cloud 410 may be configured to identify the object in the picture and provide data about the recognized object to all the robots connected to the cloud 410 , as well as possibly about other characteristics (e.g., metadata) of the recognized object, such as a location, size, weight, color, etc.
- every robot may be able to know attributes of an object in a photo uploaded by the robot 402 .
- the robots 402 , 404 , 406 and 408 may perform any number of actions within an area, with people, with other robots, etc.
- each robot 402 , 404 , 406 and 408 has WiFi or another network based connectivity and will upload/publish data to the cloud 410 that can then be shared with any other robot.
- each robot 402 , 404 , 406 and 408 shares experiences with each other to enable learned behaviors.
- the robot 402 may traverse a pathway and encounter an obstacle, and can inform the other robots 404 , 406 , and 408 (through the cloud 410 ) of a location of the obstacle.
- Each robot 402 , 404 , 406 , and 408 will have access to real-time up to date data.
- the robot 404 can download data indicating images seen by the other robots 402 , 406 , and 408 to help the robot 404 identify an object using various views (e.g., in instances in which the robots 402 , 406 , and 408 have captured images of the objects from a different perspective).
- the robot 408 may build a map of an area, and the robot 402 can download the map to have knowledge of the area. Similarly, the robot 402 could update the map created by the robot 408 with new information about the area (e.g., the hallway now has boxes or other obstacles), or with new information collected from sensors that the robot 408 may not have had (e.g., the robot 402 may record and add temperature data to the map if the robot 408 did not have a temperature sensor).
- the robots 402 , 404 , 406 , and 408 may be configured to share data that is collected to enable faster adaptation, such that each robot 402 , 404 , 406 , and 408 can build upon a learned experience of a previous robot.
- Sharing and adaptation capabilities enable a variety of applications based on a variety of inputs/data received from the robots 402 , 404 , 406 , and 408 .
- mapping of a physical location such as providing data regarding a history of where a robot has been, can be provided.
- Another number or type of indicators may be recorded to facilitate mapping/navigational functionality of the robots 402 , 404 , 406 , and 408 (e.g., a scuff mark on a wall can be one of many cues that a robot may record and then rely upon later to orient itself).
- the cloud 410 may include, store, or provide access to a database 412 of information related to objects, and the database 412 may be accessible by all the robots 402 , 404 , 406 , and 408 .
- the database 412 may include information identifying objects, and details of the objects (e.g., mass, properties, shape, instructions for use, etc., any detail that may be associated with the object) that can be accessed by the robots 402 , 404 , 406 , and 408 to perform object recognition.
- information regarding use of an object can include, e.g., for a phone, how to pick up a handset, how to answer the phone, location of buttons, how to dial, etc.
- the database 412 may include information about objects that can be used to distinguish objects.
- the database 412 may include general information regarding an object (e.g., such as a computer), and additionally, information regarding a specific computer (e.g., a model number, details or technical specifications of a specific model, etc.).
- Each object may include information in the database 412 including an object name, object details, object distinguishing characteristics, etc., or a tuple space for objects that can be accessed.
- Each object may further include information in the database in an ordered list, for example.
- the database 412 may include a global unique identifier (GUID) for objects identified in the database 412 (e.g., to enable distinguishing between specific objects), and the GUID may be associated with any characteristics or information describing the object.
- GUID global unique identifier
- a robot may be configured to access the database 412 to receive information generally distinguishing objects (e.g., a baseball vs. a computer), and to receive information that may distinguish between specific objects (e.g., two different computers).
- the database 412 may be accessible by all robots through the cloud 410 (or alternatively directly accessible by all robots without communication through the cloud 410 ).
- the database 412 may thus be a shared knowledge-base stored in the cloud 410 .
- robots may share learned behaviors through the cloud 410 .
- the cloud 410 may have a server that stores robot learned activities or behaviors resulting in a shared knowledge-base of behaviors and heuristics for object interactions (e.g., a robot “app store”).
- a given robot may perform actions and build a map of an area, and then the robot can upload the data to the cloud 410 to share this knowledge with all other robots.
- a transportation of the given robot's “consciousness” can be made through the cloud 410 from one robot to another (e.g., robot “Bob” builds a map, and the knowledge of “Bob” can be downloaded onto another robot to receive knowledge of the map).
- the robots 402 , 404 , 406 , and 408 may share information through the cloud 410 , and may access the database 412 .
- robots may interact with the cloud to perform any number of functions.
- Example functions are described below.
- FIG. 5 is a block diagram of an example method of a robot interaction with the cloud to facilitate object recognition, in accordance with at least some embodiments described herein.
- Method 500 shown in FIG. 5 presents an embodiment of a method that, for example, could be used with the systems 100 and 400 , for example, and may be performed by a device, such as another device illustrated in FIGS. 1-4 , or components of the device.
- Method 500 may include one or more operations, functions, or actions as illustrated by one or more of blocks 502 - 512 .
- the blocks are illustrated in a sequential order, these blocks may also be performed in parallel, and/or in a different order than those described herein.
- the various blocks may be combined into fewer blocks, divided into additional blocks, and/or removed based upon the desired implementation.
- each block may represent a module, a segment, or a portion of program code, which includes one or more instructions executable by a processor for implementing specific logical functions or steps in the process.
- the program code may be stored on any type of computer readable medium, for example, such as a storage device including a disk or hard drive.
- the computer readable medium may include a non-transitory computer readable medium, for example, such as computer-readable media that stores data for short periods of time like register memory, processor cache and Random Access Memory (RAM).
- the computer readable medium may also include non-transitory media, such as secondary or persistent long term storage, like read only memory (ROM), optical or magnetic disks, compact-disc read only memory (CD-ROM), for example.
- the computer readable media may also be any other volatile or non-volatile storage systems.
- the computer readable medium may be considered a computer readable storage medium, for example, or a tangible storage device.
- each block in FIG. 5 may represent circuitry that is wired to perform the specific logical functions in the process.
- the method 500 includes capturing an image of an object.
- a robot may capture many images of objects using any number of sensors, such as a camera (still pictures or video feeds), infrared sensors, etc.
- the method 500 includes capturing information associated with characteristics about the object.
- a robot may optionally determine or record a weight, dimensions, a texture, color, or any type of physical attribute of the object.
- the robot may capture additional data of the object, such as by accessing memory of the object. For instance, if the object has communication capabilities (such as WiFi, Bluetooth, infrared or other wireless or wired methods), the robot may communicate with the object to determine any type of data. Additionally, the object may have serial/parallel ports through which the robot may communicate with the object.
- the object may have communication capabilities (such as WiFi, Bluetooth, infrared or other wireless or wired methods)
- the robot may communicate with the object to determine any type of data.
- the object may have serial/parallel ports through which the robot may communicate with the object.
- the method 500 includes querying a computing device with the captured image and/or information.
- the robot may query the cloud with a captured image by sending the captured image to the cloud and requesting information about the image, such as an identity of the image or information associated with characteristics of objects in the image.
- the robot may query another robot directly to request information about the object, such as an image of an alternate view of the object.
- the method 500 includes receiving information associated with the object.
- the robot may receive data from the cloud indicating an identity of an object in the image, or other information related to or associated with characteristics about the object.
- the cloud may perform object recognition on the uploaded image or video. For example, if a picture was taken in a living room, the cloud may be able to identify a television. The cloud may even be able to recognize an exact model of television, and provide information regarding instructions for use of the television.
- the method 500 includes storing the received information for future recognitions. For example, after receiving the information from the cloud, the robot would be able to recognize the object in the future enabling the robot to learn and adapt.
- the method 500 includes performing an action based on the received information.
- the action may vary based on a type of received information, or the query that is presented by the robot.
- a robot may capture an image of a coffee maker and provide the image to the cloud.
- the robot may receive details about the coffee maker including an identity, model number, and instructions for use.
- the robot may then perform actions according to the instructions for use to use the coffee maker.
- a robot may open a refrigerator, take inventory of objects inside (e.g., capture images, identify objects), determine/receive recipes for possible meals based on the determined inventory, and suggest a meal based on items in the refrigerator.
- the robot may query the cloud to identify an object and details of the object to enable the robot to interact with the object. If the received information is not fully accurate (e.g., the robot determines a different weight of the object), the robot can share this information with the cloud to update/modify a shared database in the cloud.
- objects may be tagged with a type of identifier (e.g., radio frequency identification (RFID) chip, near field communication chip (NFC), bar code, etc.), so that a robot may include an application enabling the robot to identify the object based on the identifier.
- RFID radio frequency identification
- NFC near field communication chip
- the identifier may provide information about the object including an identification, details of the object (mass, color, type, brand, etc.), a location of the object, etc.
- objects may emit beacons that can be received by the robots to determine existence/identification/location of the object.
- a physical environment may be instrumented with beacons in forms of NFC, RFID, QR codes, etc. to enable a robot to localize and identify objects.
- Beacons may be stationary beacons or moving beacons (e.g., RFID in an employee's ID badge) to perform tracking of objects.
- FIG. 6 is an example conceptual illustration of a robot 600 interacting with a cloud 602 to perform object recognition and interaction, or other functions as described in FIG. 5 .
- the robot 600 may interact with an object (such as any of objects 604 ), and interact with the cloud 602 as described above to further interact with the object.
- an object such as any of objects 604
- the method 500 to facilitate object recognition may be a higher-level service (e.g., higher in a software stack), such that details and specifics for how to recognize an object may be performed by the cloud.
- the robot may be configured to perform actions/functions based on a result of object recognition, rather than or in addition to, performing functions regarding recognizing an object.
- the robot may execute software to perform function calls, such as GetObject( ) which may return information associated with an object (e.g., a cereal box), or PickUpObject( ) which may cause the robot to pick up the object. Enabling function calls and operation of robots through the cloud facilitates control and operation of the robot without having to control or operate various sensors/mechanical aspects of the robot, for example.
- FIG. 7 is an example conceptual illustration of a mapping function.
- a robot 700 may interact with a cloud 702 to perform functions as described in FIG. 5 , for example, such as to perform object recognition queries of objects 704 .
- the robot 700 may traverse through an area 706 in any number of pathways so as to map the area.
- the robot 700 may be configured to roam around an area in which the robot 700 is located to create a map of the area 706 (e.g., room of a house) with the aid of cloud processing.
- the robot 700 may relay data collected from various sensors to the cloud 702 .
- the robot 700 may use sensors to return still pictures, video, location data, and distance information to the cloud computing system.
- the robot 700 may traverse through the area 706 capturing images using a range camera, video camera, etc., and send the data to the cloud 702 .
- the robot 700 (or servers in the cloud 702 ) may identify objects within the data and provide annotations (such as mass, shape, material, etc.) of the objects.
- a computerized map may be generated to represent the area 706 , and computer graphics (e.g., low resolution graphics) can be used to represent identified objects.
- computer graphics within the generated map of the area 706 may be replaced with high resolution images of the objects.
- the robot 700 may capture an image of a couch, and the cloud 702 may identify a specific model of the couch, and perform a search within a database to locate/identify a retailer that manufactures/sells the couch.
- a server in the cloud 702 may query a retailer's server to receive a high resolution image of the couch that may also include other metadata indicating characteristics of the couch (e.g., style, material, price, availability, etc.).
- the high resolution image of the identified couch may be inserted into the computer generated map to replace the computer graphics representing the couch.
- an annotated mapping of objects in an area can be generated.
- FIG. 8 is a block diagram of an example method for mapping of objects in an area or mapping an area, in accordance with at least some embodiments described herein.
- Method 800 shown in FIG. 8 presents an embodiment of a method that, for example, could be used with the systems 100 and 400 , for example, and may be performed by a device, such as the devices illustrated in FIGS. 1-4 , or components of the device.
- the various blocks of method 800 may be combined into fewer blocks, divided into additional blocks, and/or removed based upon the desired implementation.
- each block may represent a module, a segment, or a portion of program code, which includes one or more instructions executable by a processor for implementing specific logical functions or steps in the process.
- the program code may be stored on any type of computer readable medium, for example, such as a non-transitory storage device including a disk or hard drive, or any other type of non-transitory media described elsewhere herein.
- the method 800 includes receiving data gathered by a computing device associated with objects in an area.
- a robot may traverse an area and capture raw data (e.g., such as point map data) and images (e.g., still or video feed) using a range camera, video camera, still camera, etc.
- the robot may provide the gathered data to the cloud, which may receive the gathered data.
- the cloud may receive gathered metadata associated with the objects in the area, such as a time the data was collected, a location (e.g., GPS location) at which the data was collected, or other characteristics regarding when/how the data was collected.
- the method 800 includes performing object recognition on the gathered data.
- a server on the cloud may interpret objects, and further, shapes can be matched with a three-dimensional warehouse or database of objects to identify representations of the point maps.
- the server may use any type of object recognition methods, such as by matching edges, colors, variances, etc., of the gathered data to known objects within a database.
- the objects within the database may have associated metadata indicating characteristics of the objects.
- the server can perform object extraction to identify and extract objects from the received data.
- the server can further localize the objects precisely in a map and provide annotations for the objects.
- Example annotations include mass, shape, material, etc. of the object.
- Annotated objects may be stored and shared through a global object database, such as, the database 412 in FIG. 4 .
- the method 800 includes generating a map of the area based on the gathered data.
- the server may use locations of the data collected as well as times the data was collected to interpret a path traversed by the robot and to create a conceptual map.
- the map may be further generated based on circumstantial data of the recognized objects, such as size and shape of the object. If an object has a known size and shape, and the location of the object is known, an estimated area at which the object is present can be determined.
- the method 800 includes inserting computer images of the objects into the map.
- the server may retrieve a computer image of the recognized object (e.g., television), and insert the object into the location on the generated map at which the object is present.
- the method 800 includes associating data with the computer images.
- the server may associate any number of metadata with the computer image, or may alternatively, retrieve metadata or other high resolution images representing the object from a retailer's database for insertion into the computer generated map.
- an image of a couch may be captured, and the server may identify a specific model of the couch, and perform a search within a database to locate/identify a retailer that manufactures/sells the couch.
- a server in the cloud may query a retailer's server to receive a high resolution image of the couch that may also include other metadata indicating characteristics of the couch (e.g., style, material, price, availability, etc.).
- the high resolution image of the identified couch may be inserted into the computer generated map to replace the computer graphics representing the couch.
- an annotated mapping of objects in an area can be generated.
- a robot builds shapes and appearances of all objects in a scene and performs object recognition as possible (with the help of the cloud) to provide an annotated map of objects.
- Raw data is gathered (e.g., point map) and used with camera data (e.g., indicating color/texture of objects) to interpret objects, and further, shapes can be matched with a 3D warehouse of objects to identify representations of the point clouds.
- FIGS. 9A-9B are example interfaces illustrating a map of an area, and objects in the map.
- a home has been outlined (e.g., in a blueprint format) to show different rooms in the home. Items in the home may also have been mapped.
- FIG. 9B illustrates an example interface illustrating mapping of objects in a room.
- the interfaces in FIGS. 9A-9B , and methods described herein may enable a user to determine configurations of rooms, and objects in the rooms.
- a user may be in a store and identify a few television stands that the user would like to purchase; however, the user would like to see how the television stands look/fit into the room configuration.
- the user may capture some images of the television stands using a device, cause the device to perform object recognition (e.g., using the method 500 in FIG. 5 ), access the interface in FIG. 9A to select a room in which to place the television stand, and access the interface in FIG. 9B to insert the new television stand in place of an old television stand (e.g., swap out the old television stand with the new television stand and place the television and peripherals into a desired configuration).
- the example interfaces in FIGS. 9A-9B may be used to maneuver computer generated objects in a room, for example.
- the interfaces in FIGS. 9A-9B may provide information (e.g., metadata) regarding rooms or objects in the rooms as determined during a mapping of the room.
- the interface in FIG. 9A may indicate details regarding a ceiling light, such as a brand name, a model number, details regarding light bulbs used by the ceiling light, etc.
- the metadata may provide links to purchase the ceiling light or accessories online, or a link to see the ceiling light in the room, such as a link to the example interface in FIG. 9B .
- the robot 700 may receive instructions to navigate from point A to point B across the area 706 .
- the robot 708 may have completed this navigation previously, and may have uploaded information regarding a possible navigation pathway to the cloud.
- the robot 708 may have documented objects along the pathway, and also, possible obstructions as well.
- the robot 700 may query the cloud requesting navigation instructions to traverse from point A to point B (e.g., a map), and may receive in response, the navigation pathway shown in FIG. 7 as previously traveled by the robot 708 . In this manner, the robots may share information to enable learning of the area 706 .
- the robot 700 may have limited memory, and in one example, to enable and manage updates, a server may include or store data to be provided to the robot 700 .
- the robot 700 may not download “the entire world” of data, but rather, may download data representing immediate surroundings into a local cache to perform actions, such as to traverse through a portion of the area 706 .
- the robot 700 may download additional data when needed.
- downloads can occur from the server to the robot 700 , or through peer-to-peer sharing (e.g., from the robot 708 to the robot 700 ).
- basic instructions for mobility, safety, and general robot operation can be stored on-board the robot 700 , while instructions for higher-level functionality may be stored in the cloud 702 and accessed by the robot 700 as needed.
- the robot 700 can use “Just in Time” downloading where high level data can be downloaded first, followed by lower level data streamed as needed.
- providing business logic stored in the cloud 702 enables fleet-wide upgrades to all robots.
- slug trails may be used for shared information (i.e., information that may be used for object recognition).
- a slug trail may be indicative of previous requests and matching responses.
- the request/response can be cached so that future requests hit an answer sooner in path. For example, if the robot 700 is navigating through the area 706 and notices a hallway is blocked, the robot 700 can publish an update on the pathway so that other robots learn of the obstacle and other problems/constraints, and may request/receive an alternate pathway. Any interaction that the robot 700 experiences can be published to the cloud so that the robot 700 logs interactions to be shared amongst all robots. Specific locations may trigger robots to download new information. For example, when entering a new room, data about the room may be retrieved that was collected by another robot.
- FIG. 10 is a block diagram of an example method for mapping of objects in an area and performing inventory of objects, in accordance with at least some embodiments described herein.
- Method 1000 shown in FIG. 10 presents an embodiment of a method that, for example, could be used with the systems 100 and 400 , for example, and may be performed by a device, such as the devices illustrated in FIGS. 1-4 , or components of the device.
- the various blocks of method 1000 may be combined into fewer blocks, divided into additional blocks, and/or removed based upon the desired implementation.
- each block may represent a module, a segment, or a portion of program code, which includes one or more instructions executable by a processor for implementing specific logical functions or steps in the process.
- the program code may be stored on any type of computer readable medium, for example, such as a non-transitory storage device including a disk or hard drive, or any other type of non-transitory media described elsewhere herein.
- the method 1000 includes determining a mapping of objects in a space according to a navigation pathway.
- a robot may move through an area and perform object recognition and generate a map, as described above using method 500 in FIG. 5 and method 800 in FIG. 8 .
- the robot may be configured to follow a predetermined navigation pathway through an area, or may follow a random navigation pathway through the area.
- the robot may store or associate the navigation pathway with the determined mapping of the area, so that a record is formed of the navigation pathway used to determine the mapping.
- the mapping may be different (e.g., a robot may not map an entirety of an area or take account a full inventory of all objects in an area).
- the mapping of the area may be, in one example, an inventory of objects in the area. As the robot traverses through the area capturing images and performing object recognition, the robot may determine what objects are present, and determine locations of objects in the area.
- the method 1000 includes storing mapping and information indicating characteristics of the objects.
- the robot may store the mapping locally on memory of the robot or within the cloud.
- the robot may further store associated information indicating characteristics of the objects with the mapping of the objects, such as, metadata describing details of the objects (weight, color, model number, size, shape, etc.).
- the method 1000 includes performing a second mapping of the objects according to the previous navigation pathway. For example, at a later time, the robot may perform another mapping of the area using the same navigation pathway so as to take an inventory of the area at the later time. The robot may follow the same (or substantially the same) navigation pathway previously used so that the same (or substantially the same) mapping of the area and objects in the area can be determined.
- the method 1000 includes comparing the second mapping to the stored mapping, and at block 1010 , the method 1000 includes identifying differences between the second mapping and the stored mapping. By comparing the two mappings, differences between the mappings can be identified to determine differences in the inventoried objects.
- a user may configure an area (e.g., bedroom) into a default configuration (e.g., where all clothes are picked up off the ground, items are arranged and the room is cleaned).
- the user may request the robot to perform a mapping and inventory of objects in the bedroom with the bedroom in the default configuration.
- the user may request the robot to perform a new inventory of the room, and the new inventory can be compared to the default inventory to determine what changes have been made to the bedroom (e.g., what objects are not in the default location).
- a user may configure a stock room at a retail store into a default configuration (e.g., all shelves are fully stocked).
- the user may request the robot to perform a mapping and inventory of objects in the stock room with the room in the default configuration.
- the user may request the robot to perform a new inventory of the stock room to determine what items have been sold.
- the new inventory can be compared to the default inventory to determine what changes have been made to the stock room, such as, to indicate a current supply of items in the stock room that can be used to configure future orders.
- the default inventory map may thus be a benchmark map of the room in a clean state, so that subsequent maps can be performed to determine a differential between the benchmark and a current condition of the room.
- a robot may create an inventory of objects in a room or scene, along with a mapping of the objects to catalog all objects in the space and to store details of the objects (e.g., name, shape, color, last known location, etc.).
- the robot may have a navigation path that is followed to inventory a space, so that the same navigation path can be traversed to identify missing/new objects.
- FIG. 11 is a block diagram of an example method for performing voice recognition by and/or control of a robot, in accordance with at least some embodiments described herein.
- Method 1100 shown in FIG. 11 presents an embodiment of a method that, for example, could be used with the systems 100 and 400 , for example, and may be performed by a device, such as the devices illustrated in FIGS. 1-4 , or components of the device.
- the various blocks of method 1100 may be combined into fewer blocks, divided into additional blocks, and/or removed based upon the desired implementation.
- each block may represent a module, a segment, or a portion of program code, which includes one or more instructions executable by a processor for implementing specific logical functions or steps in the process.
- the program code may be stored on any type of computer readable medium, for example, such as a non-transitory storage device including a disk or hard drive or any other type of non-transitory storage media described elsewhere herein.
- the method 1100 includes recording a sample of media.
- a user may interact with a robot by speaking to the robot, and the robot may record the speech of the user.
- the robot may record samples of speech from other areas as well (e.g., televisions, radio, etc.) and of other types of media, such as music, video, live performances, etc.
- the method 1100 includes sending the sample of media to a server.
- the robot may be configured to communicate with a server (e.g., the “cloud”), and may send the sample of media using wired or wireless communication to the server.
- a server e.g., the “cloud”
- the method 1100 includes receiving from the server a voice recognition on the sample of media.
- the server may use any number of known techniques for performing voice recognition, and may provide a response to the robot.
- the voice recognition response may include a textual equivalent of the speech, such as when the server performs speech to text processes.
- the method 1100 includes performing an action based on the received information.
- the voice recognition response may also include a command or instructions indicating actions for the robot to perform.
- the server may perform a voice recognition, and may further identify actions to be performed by the robot in response to the voice recognition.
- the user may speak “call John” to the robot.
- the robot may record the speech and send the speech sample to the server.
- the server may recognize the speech, and return a voice recognition response to the robot with instructions for the robot to call a user named John at the phone number 555 - 1234 .
- the robot may then initialize a phone call to John using internal phone capabilities.
- the voice recognition response may further include instructions to the robot for how to initialize the phone call, such as to provide a map to the robot of where a telephone is located, provide a navigation pathway to the robot to instruct the robot how to traverse to the telephone, provide instructions how to use the telephone to the robot, and provide the phone number of John to the robot.
- a robot may perform any actions or queries to the cloud as described herein based on contextual or situational information.
- a robot may have information relating to a local environment in which the robot operates (e.g., a local map, a location, etc.) and this information can be used as constraints for recognition systems that are used to identify objects within captured data by the robot. For example, if the robot is in an office, the robot may access an “office” database of objects within the cloud to perform object or data recognitions. Thus, the robot may send to the cloud a query to determine or identify an object within gathered data, and the query may include contextual information, such as an indication of a location of the robot. The server may use the contextual information to select a database in which to search for a matching object.
- a local environment in which the robot operates e.g., a local map, a location, etc.
- this information can be used as constraints for recognition systems that are used to identify objects within captured data by the robot. For example, if the robot is in an office, the robot may access an “office” database of objects within the cloud to perform object or data recognitions.
- the robot may send to the
- a robot may use location, or possible nearby objects as constraints into recognition systems to provide a context of the environment enabling object recognition to be performed using a subset or a limited set of nouns/verbs/objects to improve recognition accuracy.
- the robot may operate with situational awareness such that a robot may inventory objects in a scene, and if given a command by a user, the robot can determine the content or interpret the meaning of the command based on a situation of the scene or by comparing with objects in the scene (e.g., command is to retrieve a cola from the refrigerator, and robot can use limited database warehouse of objects associated with “house” to quickly identify refrigerator/cola, etc.).
- the robot may be configured to use contextual as well as situational data to help perform decision making.
- a robot may perform actions using contextual/situational data, such as time of day, weather outside, etc. For example, at night a robot may be configured to move more slowly and make less noise to be quiet than as compared to operations during the day. In another example, in the morning a robot may offer coffee to a person as opposed to a soft drink. Other situational examples that may affect configurations of robot actions include if the weather is rainy, the robot may offer an umbrella to the person, or based on what a person is wearing, the robot may offer suggestions as to whether the person will be hot/cold due to weather. Thus, the robot may take context/situation into account, as well as whom the robot is interacting with when determining an appropriate response/function. Still further, a robot may make a sound of presence, intent, state, based on context/situations.
- contextual/situational data such as time of day, weather outside, etc. For example, at night a robot may be configured to move more slowly and make less noise to be quiet than as compared to operations during the day. In another example,
- a voice recognition database (or information from a voice recognition database) may be provided to a client device (e.g., robot) to enable the robot to perform voice/speech recognition locally.
- the voice recognition database may be provided based on a current state of the robot. As an example, if a robot is in an office setting, an office database for voice recognition may be provided to the robot to enable a voice recognition process to be performed more quickly than having the robot search within a database for all settings. Any situational or contextual information of the robot may be used to select a voice recognition database to provide to the robot. As other examples, a time of day, context of a conversation, location of the robot, etc. may be used individually or in combination to select a voice recognition database to provide to the robot.
- FIG. 12 is a block diagram of an example method for processing data from a robot, in accordance with at least some embodiments described herein.
- Method 1200 shown in FIG. 12 , presents an embodiment of a method that, for example, could be used with the systems 100 and 400 , and may be performed by a device, such as the devices illustrated in FIGS. 1-4 , or components of the device.
- the various blocks of method 1200 may be combined into fewer blocks, divided into additional blocks, and/or removed based upon the desired implementation.
- each block may represent a module, a segment, or a portion of program code, which includes one or more instructions executable by a processor for implementing specific logical functions or steps in the process.
- the program code may be stored on any type of computer readable medium, for example, such as a non-transitory storage device including a disk or hard drive, or any other type of non-transitory media described elsewhere herein.
- the method includes receiving identification data from a robot.
- the identification data corresponds to an object detected by the robot.
- the identification data may be identification information from a tag or beacon associated with the object, an image of the object, any tangible aspect associated with the object (e.g., texture, size, weight, etc.), voice data associated with the object, red green blue plus depth (RGB-D) data, image and/or range data, structure-from-motion data, etc.
- the identification information may be from a signal or beacon, a 1D barcode, a 2D barcode, an RFID, an NFC chip, etc., attached to or otherwise associated with the object.
- the object may be a sofa.
- a RFID may be attached to the sofa and may be used to identify the sofa.
- a robot or other computing device may be able to scan or otherwise read identification information from the RFID, and send the identification information from the RFID or information encoded in the RFID to a server.
- the server may be located in or otherwise associated with a cloud computing system, such as the cloud configurations described elsewhere herein.
- the identification data may be an image of the object.
- the robot, server, cloud, etc. may use the image of the object to identify the object.
- the server or the robot may also use the image of the object to ascertain an identifier that may be associated with the object, for example. Ascertaining the identifier associated with the object may be based on one or more of (i) the image of the object, (ii) the identification information read from the signal, 1D barcode, 2D barcode, RFID, NFC chip, etc., associated with the object as described herein, and/or (iii) any other identification data associated with the object that the robot may collect.
- the identification data may be associated with RGB-D data.
- RGB-D data may be used to build a 3D image or map of the object, an area, etc.
- the robot may obtain RGB-D data of the object using a depth camera sensor or any other sensor that may be capable of combining visual and/or 3D shape information.
- the robot may send the RGB-D data to the server, which may associate the RGB-D data with an object using a database lookup, for example.
- the identification data may be associated with metadata derived from RGB-D data and/or image or range data associated with the object.
- Image or range data may be associated with a 3D image or map of the object, an area, etc., and may be generated from any number of sensors including stereo camera sensors, depth camera sensors, ranging sensors, sonic sensors, etc.
- the robot may send the image or range data to the server, which may associate the image or range data with an object using a database lookup, for example.
- structure-from-motion data from a camera may be post-processed by the robot and/or server, compared to stored object data using a database lookup, and used for purposes of object recognition and/or identification.
- the identification data may be associated with voice data received from the object, user, robot, etc.
- Voice data may include any media including spoken words, vocalizations, background noises, etc.
- the robot may obtain the voice data and perform voice recognition on the voice data, as described in more detail herein.
- the voice recognition may be used to recognize the name of the object, an action associated with the object, etc.
- the robot may recognize the word “table” and identify an object in a database that may be associated with the word “table”.
- the voice recognition and object identification may be performed by the server.
- the method includes receiving current location data.
- the current location data may indicate the current location of the object within an area.
- the robot, a sensor, the object, a third party, etc. may obtain the current location of the object and send the current location to the server.
- the process of obtaining current location data may vary in different embodiments.
- the robot may obtain current location data via one or more sensors, such as GPS, radar, etc., which may be associated with the robot.
- the current location data may be exact or an approximation (e.g., based on calculations from the location of the robot).
- the robot or the server may apply one or more algorithms to the approximate current location to identify a better approximation of the current location.
- the robot may send the current location data, or an approximation thereof, to the server.
- sensors independent from the robot may be used to obtain the object's current location and to send the current location to the server, robot, user, or object.
- the object may have a beacon (or similar construct) located on the object.
- a GPS may be used to identify the current location of the object via the beacon and to send the current location of the object to the robot, for example.
- the GPS may also send an object identifier that allows the robot to associate the current location with the object.
- current location data may be stored at the object using an RFID, NFC chip, etc.
- the process of storing the location data on the RFID, NFC chip, etc. may include the robot, the user, or a computing device, determining the current location of the object and writing the current location of the object to the RFID, NFC chip, etc., for example. This process may occur periodically, such as when the robot performs a scan of an entire area or when the user adds location data to (i) the RFID, NFC chip, etc. or (ii) one or more databases that store an association between location data with a particular RFID, NFC chip, etc.
- the process may be prompted when the object recognizes that the object has been moved.
- an electronic picture frame may include an RFID as an identifier.
- the electronic picture frame may also include a sensor, such as a GPS, a motion detector, or an accelerometer, which may be configured to identify when the picture frame has been moved. Once moved, the sensor may obtain new GPS location data, calculate the new location data, etc., and write the new location data to the RFID, for example.
- the robot may obtain the location data from the object during a scan of the object and send the location data to the server, for example. In some embodiments, the robot may use a partially supervised method for learning objects and locations of objects. Thus, for example, if the robot is not able to identify an object or its location, then the robot may prompt the user to enter the information and use entered information to reinforce one or more learning algorithms.
- the method includes identifying an object.
- the server may identify the object by one or more of (i) analyzing the image of the object, (ii) looking up identification information from a tag or beacon associated with the object in a database, and/or (iii) analyzing any other information data about the object that might be collected by the robot (e.g., weight, dimensions, texture, etc.).
- the server may determine an identifier associated with the object.
- the database may be indexed by the identifier. However, the database may alternatively be indexed by other data as well.
- the identifier in the database corresponding to the object may be associated with a type of identifier (e.g., barcode, RFID, object recognition, etc.), a description of the object, a manufacturer of the object, one or more computer-executable instructions that the object may be capable of responding to, the current location of the object, what time the object was at the current location, one or more past locations of the object, what time(s) the object was at the one or more past locations, a map associated with the current location of the object, a map associated with one or more past locations of the object, the owner of the object, etc.
- the identifier may also be associated with a confidence interval, which may indicate the likelihood that the object has been associated with the correct identifier.
- the method includes associating current location data with a first map.
- a map may include a collection of data associated with an area, for example.
- the collection of data may include data associated with the identifier of the object.
- the area may be a room, office, city, state, country, etc., and may be defined by a coordinate or other location based system.
- the map may be visually represented by a diagram.
- multiple maps may cover the same area.
- a first map may be of a table within a room and another map may be of the room.
- yet another map may be of a house that includes the room and the table.
- the server may store a unique primary map (e.g., of the world) that may be divided into unique smaller maps, such as of a room or of a city.
- the smaller maps may be predefined, such as by the boundaries of a room, or may be created ad hoc.
- Ad hoc maps may include an area having a proximity to the current location of the object, the past location of the object, the robot, the user, etc.
- Maps may be public or private. Public maps may be accessed by any robot, user, etc. Private maps, however, may be accessed by limited robots, users, etc. Access may be limited by various authentication and/or authorization mechanisms.
- the robot may be in a room having confidential material. The robot may scan the room and identify an object within the room, such as a sofa. The robot may also obtain current location data associated with the sofa. The identification data and the current location data may be sent from the robot to the server. Upon receipt of the location data, the server may add the location data to the database. The server may also identify a map associated with the current location of the sofa. The map may include private permissions that may restrict the robot from accessing the map of the room or portions of the room without first being authenticated (e.g., via a digital signature) and authorized access to the map of the room.
- the method includes comparing the first map to a second map.
- the first map may be the map associated with the current location of the object.
- the second map may be a map associated with a past location of the object.
- the first map and the second map may be maps of the same area.
- the first map and the second map may be of the same area if the object has remained in the same room.
- the first map and the second map may be maps of different areas when the object has moved to a different room.
- the method 1200 may include comparing the first map to the map associated with the past location chronologically preceding the current location.
- the method 1200 may include comparing the first map to a map associated with a specific time.
- the user may request the first map be compared to a map associated with a past location that occurred one day, week, month, etc., prior to the current location, for example.
- the determination of which map associated with a past location may be compared to the first map may be determined by the user, robot, server, etc.
- the user, robot, server, etc. may determine a default or preferred map for the server to compare to the first map.
- the default or preferred map may be a map associated with a specific time period or a past location of the object.
- the user may identify a second map having past location data associated with the last time the area was cleaned.
- the first map (e.g., with the current location of the objects) may be compared to the second map (e.g., of the clean area).
- the first map may include an inventory of what objects are currently in a store and the second map may include an inventory of what items were in the store one or two weeks prior.
- the first map (e.g., with the current objects in the store) may be compared to the second map (e.g., of the objects in the store one or two weeks prior.)
- the method includes identifying differences between the first map and the second map.
- the differences may include a new object, a new location of an object, a missing object, etc.
- the method may include identifying all differences between the first map and the second map.
- the method may include identifying specific differences between the first map and the second map.
- An example of a specific difference may include identifying the new location of a specific object located within the first map rather than identifying the new location of each object within the first map.
- the differences between the first map and the second map may be stored in the database having the object identifier or in an alternative database.
- the method includes inventorying objects.
- the inventory of objects may be determined based at least in part on the comparison of the first map to the second map and the identification of differences between the first map and the second map.
- an inventory of an entire map or portion thereof may be maintained in the database.
- the inventory of objects may be maintained within the same database as the object identifier and the maps or in a different database.
- the database may be used to track inventory in any number of ways including maintaining a counter associated with one or more object identifiers, descriptions, etc.
- the counter may be represented in any number of bases. For example, assuming that the same types of objects share the same identifier, the counter may be used to determine how many of the objects having the same identifier are in an area. Thus, the counter may be used to determine how many cans of “soda x” having identifier “abc” are located within the area.
- a binary counter may be used to represent the presence or absence of the object in the area.
- each can of “soda y” may have a unique identifier and the presence of each can of “soda y” may be represented in the database in a binary fashion.
- the server may reconcile the inventory of objects.
- the reconciliation process may use the information determined by comparing the first map to the second map and determining if additional objects may need to be added or removed for the first map and the second map to match.
- the server, robot, user, etc. may add objects to or remove objects from the first map to facilitate the reconciliation.
- the server may send computer executable instructions to the robot to cause the robot to remove an object from the first map if the object is not located in the second map.
- the server may send computer executable instructions to the robot to cause the robot to add or retrieve an object when the object may be associated with the second map, but is not present in the area associated with the first map.
- the server may reconcile inventory by determining what objects are currently present in a storeroom and comparing the identifiers associated with the currently present objects to identifiers associated with objects that were present in the storeroom during the last inventory reconciliation.
- the server may obtain or maintain a list of objects or identifiers that should be present in the storeroom, for example.
- the identifiers associated with the objects that are currently in the storeroom may be compared to the identifiers on the list of objects or identifiers that should be present in the storeroom. The difference may be analyzed by the server, robot, or user to determine whether inventory has been added, removed, adjusted, etc.
- the inventory process may be periodic or event driven. Periodic inventorying may occur on an hourly, daily, weekly, monthly, quarterly, yearly, etc., basis. Event driven inventorying may occur when a user requests an inventory, when a searcher instructs the robot to search for an object, etc. In embodiments, the inventory of objects may be stored on the server or sent to the robot or user for storage.
- the method includes sending the first map to the robot.
- the map that is sent to the robot may include the entire first map or a subset of the first map.
- the robot may receive the first map and store the first map for future usage. For example, the robot may use the stored first map to determine where objects are located within the area. The robot may also use the stored first map as a reference for finding objects within the area.
- FIG. 13A is an example conceptual illustration of a first map of an area, in accordance with at least some embodiments described herein.
- FIG. 13A illustrates a robot 1300 within an area, such as a dining room, that has multiple objects.
- Example objects include a table 1302 , multiple chairs 1304 , 1306 , 1308 , 1310 surrounding the table 1302 , and multiple dishes 1312 on the table 1302 .
- one or more of the objects may include an identifier 1314 , 1316 .
- the identifiers 1314 , 1316 may be an RFID tag or any other type of identifier described elsewhere herein.
- the robot 1300 may scan the area to identify objects within the area.
- the scan may include (i) examining individual objects in the area separately, (ii) examining the entire area (or a sub-region thereof) to identify individual objects in the area (or sub-region), or (iii) a combination of individual object examination and area (or sub-region) examination.
- the robot 1300 may attempt to detect an RFID tag or other beacon associated with the object, such as the RFID tag 1314 associated with chair 1308 .
- the robot may also or alternatively obtain one or more images of the object to identify the object.
- the robot may send the identification information obtained from the RFID tag or other beacon and/or images of the object to the server, and the server may attempt to identify the object and/or determine an identifier associated with the object based on the images or other information received from the robot.
- the robot 1300 may optionally scan an entire area (or sub-regions thereof) for objects.
- the scanning may include the robot accumulating information from a plurality of RFID tags 1314 , 1316 and/or images of the objects and sending the information from the plurality of RFID tags 1314 , 1306 and/or images to the server.
- the server may receive the information from the RFID tags 1314 , 1316 and/or the multiple images from the robot, and then use the information from the RFID tags 1314 , 1316 and/or the images to identify the objects in the area. Identifying the object may include assigning or determining an identifier to the object based on an analysis of the image data or a lookup of the identification information read from the RFID tags.
- the server may send a request to the robot for additional images of one or more objects within the area.
- the server may also determine a likelihood that the object has been associated with the correct identifier. This likelihood may be determined using any number of statistical algorithms. In some embodiments, the likelihood that the object is associated with the correct identifier may need to reach a threshold before the identifier is associated with the object.
- the threshold may be a percent certainty and may vary between objects, robots, users, etc. Thus, the threshold may be higher when the object is a specific object (e.g., a white plate with a blue boarder) compared to when the object is a less specific object (e.g., a plate).
- the robot 1300 may also identify a location associated with identified objects.
- the robot 1300 may include location sensors (e.g., GPS, sonar, etc.) that allow the robot to identify the location of one or more objects in an area. Spatial sensors may also or alternatively be used to determine the location of the object.
- the location data may be obtained periodically or upon the happening of an event, such as a user, server, etc., requesting that the entire area or portion thereof be scanned.
- the robot 1300 may send the location data to the server when the data is obtained, when the robot has completed a scan of the area, upon a specified time, etc.
- the robot 1300 may be performing a weekly scan of objects in the dining room. As part of the scan, the robot 1300 may examine the table 1302 for an RFID tag or other type of tag or beacon. Having found no tag or beacon, the robot 1300 may obtain one or more images of the table 1302 and send the one or more images to the server, and the server may identify the table 1302 and/or determine an identifier associated with the table 1302 based on the one or more images received from the robot. After or while the determination is being performed at the server, the robot 1300 may obtain location data for the table 1302 using a GPS or other sensor, for example. The robot 1300 may send the location data to the server and/or store the location data locally on the robot 1300 until the server has identified the table and/or determined an identifier for the table.
- the robot 1300 may continue the scan and identify a next object in the room.
- the next object may be one of the chairs 1304 , 1306 , 1308 , 1310 or one of the dishes 1312 , for example.
- the robot 1300 may recognize a barcode identifier 1314 on the chair 1308 .
- the robot may send the information from the barcode identifier 1304 to the server along with location data associated the chair 1308 .
- the robot 1300 may continue to scan the area until all (or at least a desired subset of) the objects have been examined and/or identified.
- the robot 1300 may continue scanning an area for objects until a predetermined time period has elapsed, until a specified object is identified, until the robot 1300 receives a command to stop scanning, etc.
- the robot 1300 may obtain location data associated with individual objects during or after the scan and send the obtained location data to the server.
- the server may receive the identification data (e.g., one or more of the images, information from tags or beacons, etc.) and the location data from the robot 1300 and store the identification data and location data in a database.
- the server may also determine identifiers associated with the objects, and associate the determined identifiers and location data with a first map.
- the first map may be of the area (e.g., the dining room) or a superset (e.g., the house) or subset (e.g., the table) of the area.
- the server may identify one or more maps that include the location information associated with all or a majority of the determined identifiers.
- the robot 1300 may send or designate the map that may be associated with the identification data and location information. Once the map is determined, one or more of the determined identifiers and/or the location information associated with the determined identifiers may be added or otherwise associated with the map. The server may store this map as the first map.
- FIG. 13B is an example conceptual illustration of a second map of an area, in accordance with at least some embodiments described herein.
- the second map may represent the same or similar area as is represented by the first map.
- the second map may represent a dining room having table 1302 , multiple chairs 1304 , 1306 , 1308 , 1310 surrounding the table 1302 , and multiple dishes 1312 on the table 1302 .
- the second map may differ from the first map in that the second map may represent the location of one or more of the objects in the area at a desired state or at a different point in time.
- the second map may represent a desired state of the dining room.
- the desired state or configuration may be based on a user programmed map of the dining room or may be based on a past scan of the dining room.
- the second map may be an organized version of the first map.
- the server may compare the first map to the second map to determine if there are differences between the first map and the second map. If differences exist, the server may identify the differences and store information associated with the differences in the database. The server may use the differences to inventory the objects in the area. The inventory may include the movement of an object from a first location to a second location, the addition or removal of an object, etc.
- the server may determine that the second map differs from the first map in that the second map includes straightened and pushed in chairs 1304 , 1306 , 1308 , 1310 , and also include dishes 1312 set in front of the chairs 1304 , 1306 , 1308 , 1310 .
- the second map may also include more dishes 1312 than illustrated in the first map. The server may use this information to inventory the objects in the first and/or second map.
- the server may send the differences to the robot 1300 in addition to the first and/or second map.
- the differences may be in a data format identifying the object, the current location of the object, the location of the object in the second map, etc.
- the data may also include a list or other data construct that identifies what objects and/or identifiers are missing, have been added, or have changed between the first map and the second map.
- the server may further identify where the missing objects are located. This may occur by the server scanning the database for the identifier and sending the most recent location of the object associated with the identifier to the robot 1300 . Embodiments may also include the server sending data to the robot indicating where the added objects may be located.
- the robot 1300 may perform an action in response to the receipt of the difference, first map, and/or second map from the server.
- the actions may include the robot 1300 straightening chairs, 1304 , 1306 , 1308 , 1310 and placing dishes 1312 in front of the chairs 1304 , 1306 , 13080 , 1310 .
- the robot 1300 may recognize or be informed by the server that there are not enough dishes 1312 to complete the place setting, i.e., that the inventory of the dishes 1312 in the area is too low to complete the task.
- the robot 1300 may locate additional dishes 1312 by determining where dishes 1312 having the same identifier, description, etc., are located and by retrieving the dishes 1312 from the identified location. In this manner, the robot 1300 may rearrange objects identified in the first map (perhaps along with additional objects not found in the first map) into desired positions based on a reference configuration defined in the second map.
- FIG. 14 is an example conceptual illustration of a robot interacting with an object, in accordance with at least some embodiments described herein.
- FIG. 14 includes a robot 1400 interacting with a coffee machine 1402 and one or more types of coffee 1404 , 1406 , 1408 .
- the coffee 1404 , 1406 , 1408 may be organized in rows, such that each row has a number of individual packets of coffee. All of the coffee 1404 , 1406 , 1408 may be the same type of coffee. Optionally, each row may represent a different type of coffee, for example. Therefore, coffee 1404 may be espresso, coffee 1406 may be a French roast, and coffee 1408 may be a flavored coffee.
- Each individual packet of coffee 1404 , 1406 , 1408 may have a corresponding identifier.
- the identifier may be associated with a 2D barcode (e.g., a QR code) on the front of each packet of coffee 1404 , 1406 , 1408 , for example. Other types of codes or identifiers could be used as well.
- the corresponding identifier may be a unique identifier for each packet of coffee or a unique identifier for each type of coffee. This may result in a first identifier associated with all packets of coffee 1404 that are espresso, a second identifier associated with all packets of coffee 1406 that are French roast, and a third identifier associated with all packets of coffee 1408 that are flavored coffee.
- the same identifier may be associated with all of the packets of coffee 1404 , 1406 , 1408 , regardless of whether the coffee 1404 , 1406 , 1408 , is an espresso, French roast, or flavored coffee. Identifiers associated with the coffee 1404 , 1406 , 1408 may further vary based on the product, manufacturer, distributer, etc.
- a user may ask the robot 1400 to prepare a cup of coffee.
- the robot 1400 may examine one or more of the coffee packets 1404 , 1406 , 1408 to locate a code such as a 2D barcode, for example. If the user specifies the type of coffee as French roast 1404 , for example, the robot 1400 may read a code on a particular coffee packet, compare the read code to an internal list of codes to determine if the read code matches the requested type of French roast coffee 1404 . Alternatively, the robot may send the code to a server for the server to determine if the code matches the requested type of French roast coffee 1404 . If the code does not match the requested type of French roast coffee 1404 , the robot 1400 may continue reading codes on the coffee packets until the robot 1400 (with or without the aid of the server) identifies a packet of French roast coffee 1404 .
- a code such as a 2D barcode
- the robot 1400 may obtain one or more images of an individual coffee packet and analyze the one or more images (or portions thereof) to determine if the coffee packet is the requested type of French roast coffee 1404 .
- the robot may send the one or more images of the coffee packet to the server for the server to analyze the one or more images (or portions thereof) to identify the type of coffee packet.
- the image analysis could be based on the color of the coffee packet, the shape of the coffee packet, logos or trademarks on the coffee packet, optical character recognition of text appearing on the coffee packet, etc. If the coffee packet is determined to not match the requested type of French roast coffee 1404 , the robot 1400 may continue obtaining images of different coffee packets until the robot identifies a packet of French roast coffee 1404 .
- the robot 1400 may alternatively perform an entire scan of the area (e.g., the coffee bar) to identify all of the objects at the coffee bar.
- the robot may send any codes that it read during the scan (and/or any images of objects obtained during the scan) to the server, and the server may identify which coffee packets correspond to the requested French roast coffee 1404 .
- the robot 1400 may also obtain a location associated with one or more of the packets.
- the associated location may correspond to an individual packet or a group of packets.
- each coffee packet may have a corresponding location, or the set of coffee packets at the coffee bar may all have the same corresponding location.
- the robot may read a code on a packet of espresso coffee 1406 , and use one or more sensors to identify a relative or exact location (or at least substantially exact location) of that particular packet of espresso coffee 1406 .
- the location of the espresso packet may be determined using a GPS, for example, and may be defined by latitude, longitude, and altitude coordinates, for example. Other location systems could be used as well.
- the robot may additionally or alternatively determine a relative position of the particular coffee packet.
- the relative location may be determined by reading codes on other coffee packets 1406 , 1408 around the espresso coffee packet 1406 and defining the location of the espresso coffee packet 1406 as a certain distance to the left of the French roast coffee packet 1404 , an even farther distance to the left of the flavored coffee packet 1408 , a certain height relative to the robot 1400 , etc.
- the robot 1400 may send the location data to the server.
- the server may receive the code read from the coffee packet and the location data associated with the coffee packet. The server may perform a lookup using the code to determine if the code is in the database.
- the code may be a unique key or not.
- a code may be unique if the code is associated with a single identifier stored in the database.
- A may be non-unique if the code is associated with more than one identifier stored in the database. If the code is not a unique key, the server may identify one or more of the objects having the same code and use other stored data to determine the correct identifier (of the multiple identifiers) to be associated with the object based on the code.
- the server may recognize that the robot 1400 is in a coffee bar and, therefore, (i) determine that the object is a packet of espresso coffee 1406 and not the hammer, and (ii) associate the packet of espresso coffee 1406 with its appropriate identifier in the database.
- the database may be indexed by identifiers, and individual identifiers may be associated with (i) image information, (ii) codes or other types of identification information obtained from barcodes, RFID tags, beacons, etc., (iii) location data, (iv) maps (e.g., the first and second map), or (v) other information that might be useful to associate with a particular object.
- the server may assign a confidence level to the identifier determined for an object, such as a confidence level that the object has been correctly identified as the packet of espresso coffee 1406 instead of a hammer.
- the server may assign a confidence interval indicating the likelihood that the code read from the coffee packet correctly corresponds to the identifier associated with the packet of espresso coffee 1404 that is stored in the database.
- the robot may add the location data associated with the object to the database.
- the process of adding the location data to the database may vary in different embodiments. For example, some embodiments may add the location data to the database each time the location data is obtained. This location data may be related to the relative or exact location of the object.
- the server may also add a timestamp to the location data in the database. The timestamp may correspond to the time at which the object was identified, the time at which the location data was entered into the database, etc. In another embodiment, the server may add location data to the database only if the location data has changed since the last timestamp.
- a new location of the object may result in a new entry into the database, whereas a static location of the object may not result in a new entry into the database.
- the server may not enter the location of the object unless the location has changed; however, the server may update the timestamp to indicate the last time the object was identified at the location, for example.
- the server may use the timestamp to determine how much time has passed since the last time that a particular object was identified at a particular location and, if a predefined amount of time has passed, send a message to the robot with instructions to locate the particular object to obtain a current location of the particular object.
- the server may associate the current location of the object, e.g., the coffee 1404 , 1406 , 1408 with a first map.
- the first map may represent the coffee bar illustrated in FIG. 14 , for example.
- the first map may be defined by a coordinate system and include one or more objects currently located within the bounds of a set of coordinates corresponding to the first map.
- the first map may also be defined by a map identifier, which may be used to identify the first map from other maps.
- One or more objects may be associated with the first map.
- the process of determining which objects are associated with the first map may include a database search for all objects currently associated with the first map's map identifier.
- the process may include a database search of all objects that are located within the bounds of the first map. This database search may analyze whether the timestamp associated with the object is current or has been updated. Location data associated with a current timestamp may be associated with the first map, for example.
- the database and/or the first map may be updated at various intervals.
- the intervals may be related to the frequency at which the robot scans for objects, when there is a change in location data associated with the first map, upon a predetermined schedule, etc.
- the process of updating the first map may include updating the entire first map, updating a subset of the first map, or creating a new version of the first map or a subset thereof
- the server may update the first map associated with the coffee bar.
- the update may include identifying the current location of each object in the coffee bar and updating the first map to include the current location of each object in the coffee bar, regardless of whether the current location of an object has changed since the last update.
- the update may include identifying the current location of objects in a subset of the coffee bar and updating the current location of the objects within the subset of the coffee bar, regardless of whether the current location of an object has changed since the last update.
- An example subset may include the section of the coffee bar with the coffee 1404 , 1406 , 1408 .
- the server may update the first map by creating a new version of the first map (or a subset thereof).
- the new version of the first map may include those objects that have moved locations since the last update.
- the server and/or robot 1400 may use the new version of the first map as an update or supplement to the original first map. In this way, the server and/or robot 1400 may limit the number of objects that may be updated at one time, for example.
- the server may compare the first map to a second map. Similar to previous examples, the first map may include the current location of the objects in the coffee bar. The second map, however, may include the prior location of one or more objects in the coffee bar. Both the first map and the second map may be defined by a plurality of coordinates, which may represent the bounds of the area and/or the location of the objects within the area.
- the server may compare the objects that are currently located in a coffee bar to objects that were located in the coffee bar the week prior.
- the server may have already identified the objects in the first map, i.e., the current location of the objects in the coffee bar, by having the robot 1400 perform a scan of the coffee bar and recording the results of the scan.
- the server may then identify the objects in the second map by identifying a range of timestamps that occurred one week prior and determining if the timestamps are associated with objects that were located in the area defined by the second map.
- the server may develop the second map.
- the server may then compare the first map and the second map to determine differences between the two maps. These differences may be represented as data in the database and/or pictorially as a map, chart, or the like, for example.
- the server may use the differences identified in the comparison of the first map to the second map to inventory objects within the area. For example, the server may create an inventory of how many total packets of coffee 1404 , 1406 , 1408 are present in the coffee bar. The server may also determine how many packets of espresso coffee 1404 , French roast coffee 1406 , or flavored coffee 1408 are available. The server may compare one or more of these numbers to a desired or preferred number of coffee packets to determine if the inventory of coffee packets is low and if additional coffee packets may need to be ordered.
- the server may determine that only one French roast coffee packet 1404 is left and take one or more actions, which may include (i) notify the user to order additional French roast coffee packets 1404 , (ii) instruct the robot 1400 to notify the user to order additional French roast coffee 1404 , (iii) order additional French roast coffee packets 1404 , or (iv) instruct the robot 1400 to order additional French roast coffee packets 1404 .
- the server may take any of the previously-listed actions in response to determining that the quantity of French roast coffee packets 1404 has dropped below a predefined level.
- the server may also determine a rate in which objects are being used.
- the rate may be calculated based on how many objects were consumed and/or removed in the first map within a predefined period of time.
- the server may determine this rate by determining how many of the objects were present in a prior inventory, how many of the objects are present in the current inventory, and how much time has elapsed between the prior inventory and the current inventory.
- the server may measure the rate in which the object is being used by the wear and tear on the object. Thus, a coffee machine that is dented, frequently used, malfunctions, etc., may have a higher wear and tear than a new coffee machine.
- the server may measure the rate at which the coffee packets 1404 , 1406 , 1408 are consumed.
- the server may maintain statistics on the rate to determine when the inventory is likely to be depleted.
- the server may use the rate to determine how much and when additional inventory may need to be purchased, for example.
- the server may use the inventory data to determine what objects are being used and what objects are not being used. For example, the server may analyze how much flavored coffee 1408 has been consumed throughout the past week and determine that no flavored coffee 1408 has been consumed. In some embodiments, the server may determine the last time flavored coffee 1408 was consumed by searching one or more previous inventory records on the database. The results may indicate that no flavored coffee 1408 was consumed in the past month. Based on this information, the server may determine if the flavored coffee 1408 has expired by comparing the current date to an expiration date that may be associated with one or more of the flavored coffee 1408 identifiers in the database.
- the server may notify the user and/or may send computer executable instructions to the robot 1400 to cause the robot 1400 to remove one or more of the flavored coffee 1408 from the coffee bar and discard the flavored coffee 1408 , for example.
- the server may also notify the user that the flavored coffee 1408 is not being consumed at a suitable rate (e.g., a rate that would prevent spoilage) and may limit or refrain from ordering additional flavored coffee 1408 in the future.
- the server may use analytics to determine an alternative product that may be consumed at a quicker rate than the flavored coffee 1408 .
- the server or user may determine an optimal inventory. This may be performed by the server analyzing previous purchases (e.g., previously added objects) and consumption rates (e.g., the rate at which the inventory is depleted) to determine an inventory that will last through a purchase cycle.
- the optimal inventory may be created by a user and may be in the form of a list or map of an optimal or ideal amount and type of objects within an area, for example.
- the server may compare the optimal inventory to the inventory of the first map to determine what objects may need to be ordered.
- the server may provide a report of these objects to the robot or to the user (e.g., when the user or robot is shopping) in a list, map, or other data format.
- the user or robot may query the server for the report of the objects so that the user or robot may know what objects to pick up while shopping, for example.
- the user may edit the list if the user determines that one or more of the inventory numbers, object descriptions, etc., are incorrect.
- the server may propagate the user edit to the database and may also use the user edit for analytic purposes, for example.
- the server may track the history and movement of objects throughout one or more areas. The tracking may be performed by identifying which map or maps have been associated with the tracked objects in the past. This may be performed using a database lookup, for example. Once identified, the server may determine any trends in object movements. The server may use this data to determine the location of one or more objects that may have moved from one map to another, for example.
- the server may send the inventory of objects, the first map, the second map, data associated with the differences between the first and the second map, etc., to the robot 1400 .
- the server may also send one or more computer executable instructions to the robot 1400 that are executable to cause the robot 1400 to perform one or more actions such as removing particular objects from the particular object's current location in the first map, adding objects (e.g., when the second map has more objects than the first map), and removing objects (e.g., when the first map has more objects than the second map).
- the server may send the robot 1400 data indicating that there is only one French roast coffee packet 1404 located in the coffee bar.
- the server may also send the robot 1400 data indicating that additional French roast coffee 1404 is located in the pantry, i.e., in a different area associated with a different map.
- the server may have identified the existence of the additional French roast coffee 1404 by tracking the movement of the French roast coffee 1404 in the database from the time it entered into the house, office, or café, up to the current time, for example.
- the server may send the location of the additional French roast coffee 1404 to the robot 1400 along with computer executable instructions for how to go to the pantry, pick up the additional French roast coffee 1404 , and put the additional French roast coffee 1404 in the coffee bar, for example.
- the robot 1400 may use the inventory from the server to determine what object or objects the robot 1400 may need and where the object or objects are located.
- FIG. 15 is an example conceptual illustration of a robot interacting with an object, in accordance with at least some embodiments described below.
- FIG. 15 includes a robot 1500 located in an area, such as a living room 1502 .
- a robot 1500 located in an area, such as a living room 1502 .
- Also located in the living room 1502 may be a number of objects including a book 1504 , a shoe 1506 , a guitar 1508 , and a bookcase 1510 .
- a dog 1512 or other animal, may also be located in the living room 1502 .
- the robot 1500 may examine one or more of the objects to (i) read information from a code, tag, beacon, etc. associated with the object and (ii) determine a current location of the object.
- the robot 1500 may additionally or alternatively obtain images of objects in the living room 1502 as described elsewhere herein.
- the robot may scan an RFID tag located in the dog's 1512 collar, for example.
- the robot 1500 may send identification data associated with the objects (e.g., images of the objects and/or information read from codes, tags, beacons, etc. associated with the objects) and the current locations of the objects to the server.
- more than one robot may identify objects within the living room 1502 .
- the robot 1500 may scan the living room 1502 , or a portion thereof, concurrently with or at a different time as another robot.
- Each robot may send the identification data and current location data to the server, where the identification data and location data associated with the objects may be added to the database.
- the server may apply a conflict resolution algorithm or the like to resolve the conflicting data.
- the current location of the object may be associated with a first map.
- the server may compare the first map having the current location of the objects in the living room 1502 to a second map.
- the second map may be a map having a preferred location for each of the objects in the living room 1502 .
- the preferred location may be entered by the user and may indicate the location that the user would like the object to be positioned.
- the server may determine the preferred location by identifying the location where the object is most frequently located and noting that location as the preferred location. The server may make this determination using database lookups, for example.
- the server may determine that the book 1504 is frequently located on the coffee table, instead of in the bookcase 1510 . From this information, the server may determine that the preferred location of the book 1504 is on the coffee table. Similarly, the server may determine that the user has entered a preferred location for the shoe 1506 and the guitar 1508 . The server may not have a preferred location for the dog 1512 , because the dog's 1512 location may be sporadic. Rather, the server may represent the dog's 1512 location in the map as present or not present. A map having one or more of the preferred locations may be used as the second map.
- the server may identify differences between the first map and the second map. Thus, the server may identify the shoe 1506 as belonging in a bedroom closet located in a different map associated with the bedroom closet. The server may also identify the guitar 1508 as belonging hung up on a wall in the living room 1502 . In some embodiments, the server may also identify similarities between the first map and the second map, such as the location of the book 1504 on the coffee table.
- the server may perform an inventory of the objects in the living room 1502 .
- the server may send the inventory, the first map, and/or the second map to the robot 1500 or another entity.
- An example entity may include an insurance company. For example, if a break-in were to occur at a house, the owner of the house may file an insurance claim for any stolen items. The owner may also grant the insurance company access to an inventory of objects that were located in the house at the time of the break-in. The insurance company may compare the insurance claim against the inventory of one or more maps of the house. Using this inventory, the insurance company may determine whether to accept or reject the insurance claim.
- the entity may be a fire department.
- the fire department may receive a call indicating that the house is on fire.
- the fire department may obtain an inventory of one or more maps associated with the house.
- the fire department may use the information to determine a floor plan of the house as well as the location of objects in the house. This may allow the fire department to safely enter the house and to maneuver around objects, such as a couch or a coffee table.
- the fire department may also use the information from the inventory of one or more of the maps to determine if any animals, such as the dog 1512 , are inside of the house.
Abstract
Description
Claims (15)
Priority Applications (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/463,596 US9205886B1 (en) | 2011-05-06 | 2012-05-03 | Systems and methods for inventorying objects |
US14/946,266 US10391633B1 (en) | 2011-05-06 | 2015-11-19 | Systems and methods for inventorying objects |
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201161483291P | 2011-05-06 | 2011-05-06 | |
US201261596998P | 2012-02-09 | 2012-02-09 | |
US13/463,596 US9205886B1 (en) | 2011-05-06 | 2012-05-03 | Systems and methods for inventorying objects |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US14/946,266 Continuation US10391633B1 (en) | 2011-05-06 | 2015-11-19 | Systems and methods for inventorying objects |
Publications (1)
Publication Number | Publication Date |
---|---|
US9205886B1 true US9205886B1 (en) | 2015-12-08 |
Family
ID=54708199
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/463,596 Active 2033-10-07 US9205886B1 (en) | 2011-05-06 | 2012-05-03 | Systems and methods for inventorying objects |
US14/946,266 Active 2034-01-25 US10391633B1 (en) | 2011-05-06 | 2015-11-19 | Systems and methods for inventorying objects |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US14/946,266 Active 2034-01-25 US10391633B1 (en) | 2011-05-06 | 2015-11-19 | Systems and methods for inventorying objects |
Country Status (1)
Country | Link |
---|---|
US (2) | US9205886B1 (en) |
Cited By (67)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20140058563A1 (en) * | 2012-07-27 | 2014-02-27 | Alberto Daniel Lacaze | Method and system for the directed control of robotic assets |
US20150375398A1 (en) * | 2014-06-26 | 2015-12-31 | Robotex Inc. | Robotic logistics system |
US9486922B2 (en) * | 2012-02-07 | 2016-11-08 | X Development Llc | Systems and methods for determining a status of a component of a device |
US9533773B1 (en) * | 2012-04-30 | 2017-01-03 | The Boeing Company | Methods and systems for automated vehicle asset tracking |
US20170072563A1 (en) * | 2015-09-10 | 2017-03-16 | X Development Llc | Using object observations of mobile robots to generate a spatio-temporal object inventory, and using the inventory to determine monitoring parameters for the mobile robots |
US20170157766A1 (en) * | 2015-12-03 | 2017-06-08 | Intel Corporation | Machine object determination based on human interaction |
US9740895B1 (en) * | 2014-05-30 | 2017-08-22 | Google Inc. | Method and system for identifying and tracking tagged, physical objects |
US9798321B2 (en) | 2016-02-18 | 2017-10-24 | Elwha Llc | Package management system for robotic vehicles |
US9804602B2 (en) * | 2016-02-18 | 2017-10-31 | Elwha Llc | Package management system for robotic vehicles |
WO2017192868A1 (en) * | 2016-05-04 | 2017-11-09 | Wal-Mart Stores, Inc. | Distributed autonomous robot systems and methods |
US20180043542A1 (en) * | 2014-10-24 | 2018-02-15 | Fellow, Inc. | Customer service robot and related systems and methods |
US20180075403A1 (en) * | 2014-10-24 | 2018-03-15 | Fellow, Inc. | Intelligent service robot and related systems and methods |
US9921583B2 (en) | 2016-02-18 | 2018-03-20 | Elwha Llc | Package management system for robotic vehicles |
WO2018081782A1 (en) * | 2016-10-31 | 2018-05-03 | Caliburger Cayman | Devices and systems for remote monitoring of restaurants |
WO2018106719A1 (en) * | 2016-12-05 | 2018-06-14 | Fellow, Inc. | Intelligent service robot and related systems and methods |
WO2018140690A1 (en) * | 2017-01-30 | 2018-08-02 | Walmart Apollo, Llc | Systems and methods for distributed autonomous robot interfacing using live image feeds |
WO2018194903A1 (en) * | 2017-04-17 | 2018-10-25 | Walmart Apollo, Llc | A hybrid remote retrieval system |
US10137567B2 (en) | 2016-09-20 | 2018-11-27 | Toyota Motor Engineering & Manufacturing North America, Inc. | Inventory robot |
US20180349847A1 (en) * | 2017-06-02 | 2018-12-06 | Scott A. Nurick | Inventory Management Method and System |
US20180361584A1 (en) * | 2015-01-06 | 2018-12-20 | Discovery Robotics | Robotic platform with long-term learning |
US20180374036A1 (en) * | 2017-06-21 | 2018-12-27 | Walmart Apollo, Llc | Systems and Methods for Object Replacement |
US10173320B1 (en) | 2016-06-08 | 2019-01-08 | X Development Llc | Robot task optimization based on historical task and location correlated durations |
US20190087772A1 (en) * | 2014-10-24 | 2019-03-21 | Fellow, Inc. | Intelligent inventory management and related systems and methods |
CN110202576A (en) * | 2019-06-14 | 2019-09-06 | 福耀集团(福建)机械制造有限公司 | A kind of workpiece two-dimensional visual guidance crawl detection system and method |
US20190325379A1 (en) * | 2014-10-24 | 2019-10-24 | Fellow, Inc. | Intelligent inventory management using cleaning machines |
US20190329421A1 (en) * | 2016-02-09 | 2019-10-31 | Cobalt Robotics Inc. | Wireless tag detection and localization by a mobile robot |
US10558947B2 (en) * | 2017-03-15 | 2020-02-11 | Walmart Apollo, Llc | System and method for management of perpetual inventory values based upon financial assumptions |
US20200050205A1 (en) * | 2018-08-07 | 2020-02-13 | Cnh Industrial America Llc | System and method for updating a mapped area |
US10586082B1 (en) | 2019-05-29 | 2020-03-10 | Fellow, Inc. | Advanced micro-location of RFID tags in spatial environments |
US10591931B1 (en) * | 2017-10-19 | 2020-03-17 | Amazon Technologies, Inc. | Managing operations of a mobile drive unit within a workspace based on a fire-based policy |
US10614274B2 (en) | 2017-01-30 | 2020-04-07 | Walmart Apollo, Llc | Distributed autonomous robot systems and methods with RFID tracking |
US10612934B2 (en) | 2018-01-12 | 2020-04-07 | General Electric Company | System and methods for robotic autonomous motion planning and navigation |
US10625941B2 (en) | 2017-01-30 | 2020-04-21 | Walmart Apollo, Llc | Distributed autonomous robot systems and methods |
US20200234395A1 (en) * | 2019-01-23 | 2020-07-23 | Qualcomm Incorporated | Methods and apparatus for standardized apis for split rendering |
US10723027B2 (en) * | 2017-09-26 | 2020-07-28 | Toyota Research Institute, Inc. | Robot systems incorporating cloud services systems |
US10728570B2 (en) * | 2014-05-22 | 2020-07-28 | Brain Corporation | Apparatus and methods for real time estimation of differential motion in live video |
US10733004B2 (en) * | 2017-04-26 | 2020-08-04 | At&T Intellectual Property I, L.P. | Intelligent service on-demand robot virtualization |
US10741087B1 (en) * | 2018-08-13 | 2020-08-11 | Alarm.Com Incorporated | Drone digital locker builder |
US10969459B2 (en) * | 2016-05-05 | 2021-04-06 | Ubisense Limited | Object detection |
US20210110137A1 (en) * | 2019-10-15 | 2021-04-15 | Alarm.Com Incorporated | Navigation using selected visual landmarks |
US10997552B2 (en) | 2017-03-15 | 2021-05-04 | Walmart Apollo, Llc | System and method for determination and management of root cause for inventory problems |
US11055662B2 (en) | 2017-03-15 | 2021-07-06 | Walmart Apollo, Llc | System and method for perpetual inventory management |
US20210272225A1 (en) * | 2017-04-19 | 2021-09-02 | Global Tel*Link Corporation | Mobile correctional facility robots |
US11157869B2 (en) | 2016-08-05 | 2021-10-26 | Vocollect, Inc. | Monitoring worker movement in a warehouse setting |
US11179845B2 (en) | 2017-01-30 | 2021-11-23 | Walmart Apollo, Llc | Distributed autonomous robot interfacing systems and methods |
US20220012493A1 (en) * | 2020-07-10 | 2022-01-13 | Alarm.Com Incorporated | Robot localization |
US11282157B2 (en) | 2017-03-15 | 2022-03-22 | Walmart Apollo, Llc | System and method for management of product movement |
CN114596022A (en) * | 2022-01-27 | 2022-06-07 | 上海华能电子商务有限公司 | Intelligent warehouse management method and system based on AOA radio frequency positioning technology |
US11368497B1 (en) * | 2018-09-18 | 2022-06-21 | Amazon Technolgies, Inc. | System for autonomous mobile device assisted communication |
US11400595B2 (en) | 2015-01-06 | 2022-08-02 | Nexus Robotics Llc | Robotic platform with area cleaning mode |
US11412188B2 (en) * | 2017-02-03 | 2022-08-09 | Alarm.Com Incorporated | Asset management monitoring |
US11407111B2 (en) * | 2018-06-27 | 2022-08-09 | Abb Schweiz Ag | Method and system to generate a 3D model for a robot scene |
US11449828B2 (en) | 2017-05-26 | 2022-09-20 | Walmart Apollo, Llc | System and method for management of perpetual inventory values based upon confidence level |
FR3123483A1 (en) * | 2021-05-26 | 2022-12-02 | Laurent BRULÉ | Method for finding an object. |
US11556889B2 (en) * | 2015-08-05 | 2023-01-17 | Whirlpool Corporation | Object recognition system for an appliance and method for managing household inventory of consumables |
US20230080728A1 (en) * | 2009-12-07 | 2023-03-16 | Meps Real-Time, Inc. | Wireless inventory tracking for containers |
US11679969B2 (en) | 2015-03-06 | 2023-06-20 | Walmart Apollo, Llc | Shopping facility assistance systems, devices and methods |
US11715066B2 (en) | 2017-03-15 | 2023-08-01 | Walmart Apollo, Llc | System and method for management of perpetual inventory values based upon customer product purchases |
US11724399B2 (en) | 2017-02-06 | 2023-08-15 | Cobalt Robotics Inc. | Mobile robot with arm for elevator interactions |
US11761160B2 (en) | 2015-03-06 | 2023-09-19 | Walmart Apollo, Llc | Apparatus and method of monitoring product placement within a shopping facility |
US11772270B2 (en) | 2016-02-09 | 2023-10-03 | Cobalt Robotics Inc. | Inventory management by mobile robot |
US11772833B1 (en) * | 2020-03-30 | 2023-10-03 | Amazon Technologies, Inc. | Systems and methods for automated custom shipping containers |
US11809200B1 (en) * | 2019-12-06 | 2023-11-07 | Florida A&M University | Machine learning based reconfigurable mobile agents using swarm system manufacturing |
US11816628B2 (en) | 2017-03-15 | 2023-11-14 | Walmart Apollo, Llc | System and method for management of perpetual inventory values associated with nil picks |
US11868960B2 (en) | 2017-03-15 | 2024-01-09 | Walmart Apollo, Llc | System and method for perpetual inventory management |
US11954551B2 (en) | 2009-12-07 | 2024-04-09 | Meps Real-Time, Inc. | Modular system and method to establish tracking activation field |
US11959733B2 (en) | 2017-04-19 | 2024-04-16 | Global Tel*Link Corporation | Mobile correctional facility robots |
Families Citing this family (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10464206B2 (en) * | 2014-10-31 | 2019-11-05 | Vivint, Inc. | Smart home robot assistant |
WO2016189896A1 (en) * | 2015-05-22 | 2016-12-01 | 富士フイルム株式会社 | Robot device and movement control method for robot device |
CN111387953A (en) * | 2020-04-09 | 2020-07-10 | 泉州市易智通智能设备有限公司 | Intelligent body temperature detection system and method |
Citations (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6286079B1 (en) * | 1999-01-11 | 2001-09-04 | International Business Machines Corporation | Interruptible inventory of a mass data storage library |
US6600418B2 (en) * | 2000-12-12 | 2003-07-29 | 3M Innovative Properties Company | Object tracking and management system and method using radio-frequency identification tags |
US20030154141A1 (en) | 2001-09-18 | 2003-08-14 | Pro Corp Holdings International Ltd. | Image recognition inventory management system |
US20100017407A1 (en) * | 2008-07-16 | 2010-01-21 | Hitachi, Ltd. | Three-dimensional object recognition system and inventory system using the same |
US7693757B2 (en) * | 2006-09-21 | 2010-04-06 | International Business Machines Corporation | System and method for performing inventory using a mobile inventory robot |
US20110288684A1 (en) * | 2010-05-20 | 2011-11-24 | Irobot Corporation | Mobile Robot System |
US8095150B2 (en) * | 2008-06-04 | 2012-01-10 | Sony Ericsson Mobile Communications Ab | Method and apparatus for conveying location of lost or motionless mobile communication devices |
US8140188B2 (en) * | 2008-02-18 | 2012-03-20 | Toyota Motor Engineering & Manufacturing North America, Inc. | Robotic system and method for observing, learning, and supporting human activities |
Family Cites Families (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8073564B2 (en) * | 2006-07-05 | 2011-12-06 | Battelle Energy Alliance, Llc | Multi-robot control interface |
-
2012
- 2012-05-03 US US13/463,596 patent/US9205886B1/en active Active
-
2015
- 2015-11-19 US US14/946,266 patent/US10391633B1/en active Active
Patent Citations (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6286079B1 (en) * | 1999-01-11 | 2001-09-04 | International Business Machines Corporation | Interruptible inventory of a mass data storage library |
US6600418B2 (en) * | 2000-12-12 | 2003-07-29 | 3M Innovative Properties Company | Object tracking and management system and method using radio-frequency identification tags |
US20030154141A1 (en) | 2001-09-18 | 2003-08-14 | Pro Corp Holdings International Ltd. | Image recognition inventory management system |
US7693757B2 (en) * | 2006-09-21 | 2010-04-06 | International Business Machines Corporation | System and method for performing inventory using a mobile inventory robot |
US8140188B2 (en) * | 2008-02-18 | 2012-03-20 | Toyota Motor Engineering & Manufacturing North America, Inc. | Robotic system and method for observing, learning, and supporting human activities |
US8095150B2 (en) * | 2008-06-04 | 2012-01-10 | Sony Ericsson Mobile Communications Ab | Method and apparatus for conveying location of lost or motionless mobile communication devices |
US20100017407A1 (en) * | 2008-07-16 | 2010-01-21 | Hitachi, Ltd. | Three-dimensional object recognition system and inventory system using the same |
US20110288684A1 (en) * | 2010-05-20 | 2011-11-24 | Irobot Corporation | Mobile Robot System |
Cited By (100)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11954551B2 (en) | 2009-12-07 | 2024-04-09 | Meps Real-Time, Inc. | Modular system and method to establish tracking activation field |
US11923059B2 (en) * | 2009-12-07 | 2024-03-05 | Meps Real-Time, Inc. | Wireless inventory tracking for containers |
US20230080728A1 (en) * | 2009-12-07 | 2023-03-16 | Meps Real-Time, Inc. | Wireless inventory tracking for containers |
US9486922B2 (en) * | 2012-02-07 | 2016-11-08 | X Development Llc | Systems and methods for determining a status of a component of a device |
US9533773B1 (en) * | 2012-04-30 | 2017-01-03 | The Boeing Company | Methods and systems for automated vehicle asset tracking |
US9969081B2 (en) * | 2012-07-27 | 2018-05-15 | Alberto Daniel Lacaze | Method and system for the directed control of robotic assets |
US20140058563A1 (en) * | 2012-07-27 | 2014-02-27 | Alberto Daniel Lacaze | Method and system for the directed control of robotic assets |
US10728570B2 (en) * | 2014-05-22 | 2020-07-28 | Brain Corporation | Apparatus and methods for real time estimation of differential motion in live video |
US9740895B1 (en) * | 2014-05-30 | 2017-08-22 | Google Inc. | Method and system for identifying and tracking tagged, physical objects |
US9636825B2 (en) * | 2014-06-26 | 2017-05-02 | Robotex Inc. | Robotic logistics system |
US20150375398A1 (en) * | 2014-06-26 | 2015-12-31 | Robotex Inc. | Robotic logistics system |
US20190087772A1 (en) * | 2014-10-24 | 2019-03-21 | Fellow, Inc. | Intelligent inventory management and related systems and methods |
US20180075403A1 (en) * | 2014-10-24 | 2018-03-15 | Fellow, Inc. | Intelligent service robot and related systems and methods |
US10311400B2 (en) * | 2014-10-24 | 2019-06-04 | Fellow, Inc. | Intelligent service robot and related systems and methods |
US10373116B2 (en) * | 2014-10-24 | 2019-08-06 | Fellow, Inc. | Intelligent inventory management and related systems and methods |
US20190325379A1 (en) * | 2014-10-24 | 2019-10-24 | Fellow, Inc. | Intelligent inventory management using cleaning machines |
US20180043542A1 (en) * | 2014-10-24 | 2018-02-15 | Fellow, Inc. | Customer service robot and related systems and methods |
US11400595B2 (en) | 2015-01-06 | 2022-08-02 | Nexus Robotics Llc | Robotic platform with area cleaning mode |
US20180361584A1 (en) * | 2015-01-06 | 2018-12-20 | Discovery Robotics | Robotic platform with long-term learning |
US11761160B2 (en) | 2015-03-06 | 2023-09-19 | Walmart Apollo, Llc | Apparatus and method of monitoring product placement within a shopping facility |
US11840814B2 (en) | 2015-03-06 | 2023-12-12 | Walmart Apollo, Llc | Overriding control of motorized transport unit systems, devices and methods |
US11679969B2 (en) | 2015-03-06 | 2023-06-20 | Walmart Apollo, Llc | Shopping facility assistance systems, devices and methods |
US11556889B2 (en) * | 2015-08-05 | 2023-01-17 | Whirlpool Corporation | Object recognition system for an appliance and method for managing household inventory of consumables |
US11660749B2 (en) | 2015-09-10 | 2023-05-30 | Boston Dynamics, Inc. | Using object observations of mobile robots to generate a spatio-temporal object inventory, and using the inventory to determine monitoring parameters for the mobile robots |
US11123865B2 (en) | 2015-09-10 | 2021-09-21 | Boston Dynamics, Inc. | Using object observations of mobile robots to generate a spatio-temporal object inventory, and using the inventory to determine monitoring parameters for the mobile robots |
US20170072563A1 (en) * | 2015-09-10 | 2017-03-16 | X Development Llc | Using object observations of mobile robots to generate a spatio-temporal object inventory, and using the inventory to determine monitoring parameters for the mobile robots |
US10195740B2 (en) * | 2015-09-10 | 2019-02-05 | X Development Llc | Using object observations of mobile robots to generate a spatio-temporal object inventory, and using the inventory to determine monitoring parameters for the mobile robots |
US9975241B2 (en) * | 2015-12-03 | 2018-05-22 | Intel Corporation | Machine object determination based on human interaction |
US20170157766A1 (en) * | 2015-12-03 | 2017-06-08 | Intel Corporation | Machine object determination based on human interaction |
US11819997B2 (en) * | 2016-02-09 | 2023-11-21 | Cobalt Robotics Inc. | Mobile robot map generation |
US11772270B2 (en) | 2016-02-09 | 2023-10-03 | Cobalt Robotics Inc. | Inventory management by mobile robot |
US20190329421A1 (en) * | 2016-02-09 | 2019-10-31 | Cobalt Robotics Inc. | Wireless tag detection and localization by a mobile robot |
US9798321B2 (en) | 2016-02-18 | 2017-10-24 | Elwha Llc | Package management system for robotic vehicles |
US9921583B2 (en) | 2016-02-18 | 2018-03-20 | Elwha Llc | Package management system for robotic vehicles |
US9804602B2 (en) * | 2016-02-18 | 2017-10-31 | Elwha Llc | Package management system for robotic vehicles |
US10810544B2 (en) * | 2016-05-04 | 2020-10-20 | Walmart Apollo, Llc | Distributed autonomous robot systems and methods |
WO2017192868A1 (en) * | 2016-05-04 | 2017-11-09 | Wal-Mart Stores, Inc. | Distributed autonomous robot systems and methods |
GB2569698A (en) * | 2016-05-04 | 2019-06-26 | Walmart Apollo Llc | Distributed autonomous robot systems and methods |
US20180365632A1 (en) * | 2016-05-04 | 2018-12-20 | Walmart Apollo, Llc | Distributed Autonomous Robot Systems and Methods |
GB2569698B (en) * | 2016-05-04 | 2021-04-07 | Walmart Apollo Llc | Distributed autonomous robot systems and methods |
US10083418B2 (en) * | 2016-05-04 | 2018-09-25 | Walmart Apollo, Llc | Distributed autonomous robot systems and mehtods |
US10969459B2 (en) * | 2016-05-05 | 2021-04-06 | Ubisense Limited | Object detection |
US11235464B1 (en) | 2016-06-08 | 2022-02-01 | X Development Llc | Robot task optimization based on historical task and location correlated durations |
US10173320B1 (en) | 2016-06-08 | 2019-01-08 | X Development Llc | Robot task optimization based on historical task and location correlated durations |
US11878425B1 (en) | 2016-06-08 | 2024-01-23 | Google Llc | Robot task optimization based on historical task and location correlated durations |
US11157869B2 (en) | 2016-08-05 | 2021-10-26 | Vocollect, Inc. | Monitoring worker movement in a warehouse setting |
US10137567B2 (en) | 2016-09-20 | 2018-11-27 | Toyota Motor Engineering & Manufacturing North America, Inc. | Inventory robot |
WO2018081782A1 (en) * | 2016-10-31 | 2018-05-03 | Caliburger Cayman | Devices and systems for remote monitoring of restaurants |
CN110352116A (en) * | 2016-12-05 | 2019-10-18 | 费洛有限公司 | Intellect service robot and related systems and methods |
WO2018106719A1 (en) * | 2016-12-05 | 2018-06-14 | Fellow, Inc. | Intelligent service robot and related systems and methods |
GB2573902B (en) * | 2017-01-30 | 2021-11-24 | Walmart Apollo Llc | Systems and methods for distributed autonomous robot interfacing using live image feeds |
GB2573902A (en) * | 2017-01-30 | 2019-11-20 | Walmart Apollo Llc | Systems and methods for distributed autonomous robot interfacing using live image feeds |
US10189642B2 (en) | 2017-01-30 | 2019-01-29 | Walmart Apollo, Llc | Systems and methods for distributed autonomous robot interfacing using live image feeds |
US10614274B2 (en) | 2017-01-30 | 2020-04-07 | Walmart Apollo, Llc | Distributed autonomous robot systems and methods with RFID tracking |
US10494180B2 (en) | 2017-01-30 | 2019-12-03 | Walmart Apollo, Llc | Systems and methods for distributed autonomous robot interfacing using live image feeds |
WO2018140690A1 (en) * | 2017-01-30 | 2018-08-02 | Walmart Apollo, Llc | Systems and methods for distributed autonomous robot interfacing using live image feeds |
US10625941B2 (en) | 2017-01-30 | 2020-04-21 | Walmart Apollo, Llc | Distributed autonomous robot systems and methods |
US11707839B2 (en) | 2017-01-30 | 2023-07-25 | Walmart Apollo, Llc | Distributed autonomous robot interfacing systems and methods |
US11179845B2 (en) | 2017-01-30 | 2021-11-23 | Walmart Apollo, Llc | Distributed autonomous robot interfacing systems and methods |
US11412188B2 (en) * | 2017-02-03 | 2022-08-09 | Alarm.Com Incorporated | Asset management monitoring |
US11724399B2 (en) | 2017-02-06 | 2023-08-15 | Cobalt Robotics Inc. | Mobile robot with arm for elevator interactions |
US11715066B2 (en) | 2017-03-15 | 2023-08-01 | Walmart Apollo, Llc | System and method for management of perpetual inventory values based upon customer product purchases |
US11797929B2 (en) | 2017-03-15 | 2023-10-24 | Walmart Apollo, Llc | System and method for perpetual inventory management |
US11868960B2 (en) | 2017-03-15 | 2024-01-09 | Walmart Apollo, Llc | System and method for perpetual inventory management |
US11282157B2 (en) | 2017-03-15 | 2022-03-22 | Walmart Apollo, Llc | System and method for management of product movement |
US11816628B2 (en) | 2017-03-15 | 2023-11-14 | Walmart Apollo, Llc | System and method for management of perpetual inventory values associated with nil picks |
US11055662B2 (en) | 2017-03-15 | 2021-07-06 | Walmart Apollo, Llc | System and method for perpetual inventory management |
US10997552B2 (en) | 2017-03-15 | 2021-05-04 | Walmart Apollo, Llc | System and method for determination and management of root cause for inventory problems |
US11501251B2 (en) | 2017-03-15 | 2022-11-15 | Walmart Apollo, Llc | System and method for determination and management of root cause for inventory problems |
US10558947B2 (en) * | 2017-03-15 | 2020-02-11 | Walmart Apollo, Llc | System and method for management of perpetual inventory values based upon financial assumptions |
WO2018194903A1 (en) * | 2017-04-17 | 2018-10-25 | Walmart Apollo, Llc | A hybrid remote retrieval system |
US11959733B2 (en) | 2017-04-19 | 2024-04-16 | Global Tel*Link Corporation | Mobile correctional facility robots |
US20210272225A1 (en) * | 2017-04-19 | 2021-09-02 | Global Tel*Link Corporation | Mobile correctional facility robots |
US10733004B2 (en) * | 2017-04-26 | 2020-08-04 | At&T Intellectual Property I, L.P. | Intelligent service on-demand robot virtualization |
US11449828B2 (en) | 2017-05-26 | 2022-09-20 | Walmart Apollo, Llc | System and method for management of perpetual inventory values based upon confidence level |
US20180349847A1 (en) * | 2017-06-02 | 2018-12-06 | Scott A. Nurick | Inventory Management Method and System |
US20180374036A1 (en) * | 2017-06-21 | 2018-12-27 | Walmart Apollo, Llc | Systems and Methods for Object Replacement |
WO2018237013A1 (en) * | 2017-06-21 | 2018-12-27 | Walmart Apollo, Llc | Systems and methods for object replacement |
US20210334742A1 (en) * | 2017-06-21 | 2021-10-28 | Walmart Apollo, Llc | Systems and methods for object replacement |
US10723027B2 (en) * | 2017-09-26 | 2020-07-28 | Toyota Research Institute, Inc. | Robot systems incorporating cloud services systems |
US10591931B1 (en) * | 2017-10-19 | 2020-03-17 | Amazon Technologies, Inc. | Managing operations of a mobile drive unit within a workspace based on a fire-based policy |
US10612934B2 (en) | 2018-01-12 | 2020-04-07 | General Electric Company | System and methods for robotic autonomous motion planning and navigation |
US11407111B2 (en) * | 2018-06-27 | 2022-08-09 | Abb Schweiz Ag | Method and system to generate a 3D model for a robot scene |
US20200050205A1 (en) * | 2018-08-07 | 2020-02-13 | Cnh Industrial America Llc | System and method for updating a mapped area |
US11455897B1 (en) | 2018-08-13 | 2022-09-27 | Alarm.Com Incorporated | Drone digital locker builder |
US10741087B1 (en) * | 2018-08-13 | 2020-08-11 | Alarm.Com Incorporated | Drone digital locker builder |
US11368497B1 (en) * | 2018-09-18 | 2022-06-21 | Amazon Technolgies, Inc. | System for autonomous mobile device assisted communication |
US20200234395A1 (en) * | 2019-01-23 | 2020-07-23 | Qualcomm Incorporated | Methods and apparatus for standardized apis for split rendering |
US11625806B2 (en) * | 2019-01-23 | 2023-04-11 | Qualcomm Incorporated | Methods and apparatus for standardized APIs for split rendering |
US10586082B1 (en) | 2019-05-29 | 2020-03-10 | Fellow, Inc. | Advanced micro-location of RFID tags in spatial environments |
CN110202576A (en) * | 2019-06-14 | 2019-09-06 | 福耀集团(福建)机械制造有限公司 | A kind of workpiece two-dimensional visual guidance crawl detection system and method |
US20210110137A1 (en) * | 2019-10-15 | 2021-04-15 | Alarm.Com Incorporated | Navigation using selected visual landmarks |
US11640677B2 (en) * | 2019-10-15 | 2023-05-02 | Alarm.Com Incorporated | Navigation using selected visual landmarks |
US11809200B1 (en) * | 2019-12-06 | 2023-11-07 | Florida A&M University | Machine learning based reconfigurable mobile agents using swarm system manufacturing |
US11772833B1 (en) * | 2020-03-30 | 2023-10-03 | Amazon Technologies, Inc. | Systems and methods for automated custom shipping containers |
US20220012493A1 (en) * | 2020-07-10 | 2022-01-13 | Alarm.Com Incorporated | Robot localization |
US11880212B2 (en) * | 2020-07-10 | 2024-01-23 | Alarm.Com Incorporated | Robot localization |
FR3123483A1 (en) * | 2021-05-26 | 2022-12-02 | Laurent BRULÉ | Method for finding an object. |
CN114596022B (en) * | 2022-01-27 | 2023-12-12 | 上海华能电子商务有限公司 | Intelligent warehouse management method and system based on AOA radio frequency positioning technology |
CN114596022A (en) * | 2022-01-27 | 2022-06-07 | 上海华能电子商务有限公司 | Intelligent warehouse management method and system based on AOA radio frequency positioning technology |
Also Published As
Publication number | Publication date |
---|---|
US10391633B1 (en) | 2019-08-27 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US10391633B1 (en) | Systems and methods for inventorying objects | |
US9473594B1 (en) | Projection of interactive map data | |
US8386078B1 (en) | Methods and systems for providing a data library for robotic devices | |
US8984136B1 (en) | Systems and methods for object recognition | |
US8380349B1 (en) | Methods and systems for providing instructions to a robotic device | |
US10372988B2 (en) | Systems and methods for automatically varying privacy settings of wearable camera systems | |
CN105934760B (en) | It is searched for using the adaptive topography of computer vision auxiliary | |
US11727468B2 (en) | Agent robot control system, agent robot system, agent robot control method, and storage medium | |
US9606992B2 (en) | Personal audio/visual apparatus providing resource management | |
US10026116B2 (en) | Methods and devices for smart shopping | |
US10157324B2 (en) | Systems and methods of updating user identifiers in an image-sharing environment | |
US8406926B1 (en) | Methods and systems for robotic analysis of environmental conditions and response thereto | |
US11049344B2 (en) | Dual-mode commercial messaging systems | |
US8525836B1 (en) | Systems and methods for representing information associated with objects in an area | |
US11018939B1 (en) | Determining product compatibility and demand | |
KR20210156283A (en) | Prompt information processing apparatus and method | |
TWI719412B (en) | Computer-readable medium storing instructions associated with displaying data related to objects in images | |
WO2020029663A1 (en) | Commodity information query method and system | |
US20230069541A1 (en) | System and method for providing an automated virtual closet | |
US11727737B2 (en) | Retail light-based sensor-driven messaging systems | |
CN109672798A (en) | Information processing unit and storage media | |
WO2020244578A1 (en) | Interaction method employing optical communication apparatus, and electronic device | |
KR20130025996A (en) | Goods connected social network service | |
KR20150071593A (en) | Object tracking |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:HICKMAN, RYAN;KUFFNER, JAMES J., JR.;FRANCIS, ANTHONY G., JR.;AND OTHERS;REEL/FRAME:028186/0609Effective date: 20120502 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: X DEVELOPMENT LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:GOOGLE INC.;REEL/FRAME:039900/0610Effective date: 20160901 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044142/0357Effective date: 20170929 |
|
AS | Assignment |
Owner name: X DEVELOPMENT LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:GOOGLE INC.;REEL/FRAME:047631/0671Effective date: 20160901 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CORRECTIVE ASSIGNMENT TO CORRECT THE CORRECTIVE BY NULLIFICATIONTO CORRECT INCORRECTLY RECORDED APPLICATION NUMBERS PREVIOUSLY RECORDED ON REEL 044142 FRAME 0357. ASSIGNOR(S) HEREBY CONFIRMS THE CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:047837/0678Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 8 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:X DEVELOPMENT LLC;REEL/FRAME:064658/0001Effective date: 20230401 |