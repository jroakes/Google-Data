CN104021150A - Facial recognition with social network aiding - Google Patents
Facial recognition with social network aiding Download PDFInfo
- Publication number
- CN104021150A CN104021150A CN201410211070.1A CN201410211070A CN104021150A CN 104021150 A CN104021150 A CN 104021150A CN 201410211070 A CN201410211070 A CN 201410211070A CN 104021150 A CN104021150 A CN 104021150A
- Authority
- CN
- China
- Prior art keywords
- image
- people
- vision
- result
- search
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V40/00—Recognition of biometric, human-related or animal-related patterns in image or video data
- G06V40/10—Human or animal bodies, e.g. vehicle occupants or pedestrians; Body parts, e.g. hands
- G06V40/16—Human faces, e.g. facial parts, sketches or expressions
- G06V40/172—Classification, e.g. identification
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/58—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/5866—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using information manually generated, e.g. tags, keywords, comments, manually generated location and time information
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/24—Querying
- G06F16/245—Query processing
- G06F16/2457—Query processing with adaptation to user needs
- G06F16/24578—Query processing with adaptation to user needs using ranking
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/53—Querying
- G06F16/532—Query formulation, e.g. graphical querying
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/58—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/583—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
- G06F16/5838—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content using colour
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/58—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/583—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
- G06F16/5854—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content using shape and object relationship
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/953—Querying, e.g. by the use of web search engines
- G06F16/9535—Search customisation based on user profiles and personalisation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/953—Querying, e.g. by the use of web search engines
- G06F16/9538—Presentation of query results
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/30—Scenes; Scene-specific elements in albums, collections or shared content, e.g. social network photos or video
Abstract
The invention relates to facial recognition with social network aiding. A method of processing a visual query including a facial image comprises: obtaining a query image that was submitted to an imgae-based search system by a requesting user; obtaining one or more images that are identified as matching the query image; identifying people that are associated with at least one of the one or more images that are identified as matching the query image; for each person that is associated with at least one of the one or more images that are identified as matching the query image, obtaining a score that reflects a level of social connectivity between the person and the requesting user; selecting one or more of the people based on the scores of the people; and providing a respective identifier of one or more of the selected people to the requesting user.
Description
Division explanation
The application belongs to the divisional application that the applying date is the Chinese patent application 201080045193.2 on August 6th, 2010.
Technical field
The disclosed embodiments relate generally to helps identify the individual of optimum matching by the information of utilizing social network information and obtain from identified other individual pictures, identify one or more people of the face in potential matching image inquiry.
Background technology
Based on text or the search based on word, wherein user is input to word or phrase in search engine and receives various results, is the useful tool for searching for.But the search request user based on word can inputting related term language.Sometimes, user may like to know that the information about image.For example, user may want to know the name of the people in photo.Individual also may like to know that other information about the people in photo, such as contact details.Therefore, expectation, can receive facial image querying and provide with face-image inquiry in the system of multiple Search Results of the individual relating to persons identifying.
Summary of the invention
According to some embodiment, a kind of processing comprises that the computer implemented method of the vision inquiry of face-image is performed on server system, and described server system has one or more processors and stores the storer that one or more programs are carried out for these one or more processors.The method is included in the process of summarizing below.Receive the vision inquiry that comprises the one or more face-images that comprise corresponding face-image from requestor.Identify the latent image coupling of the corresponding face-image of potential coupling according to visual similarity standard.Latent image coupling comprises the image of the one or more image sources that identify about requestor's data from basis.Mark mates with latent image the one or more people that are associated.The individual who identifies for each, retrieval is specific to individual data, and it comprises social networks tolerance that obtain from multiple application and social contiguity requestor.The freely group of following composition of multiple application choosings: communications applications, social networks application, calendar application and collaboration applications.By according to one or more tolerance of the visual similarity between corresponding face-image and latent image coupling and also according to comprising that at least the ranking information of social networks tolerance carries out rank to one or more identified individuals, generate individual sorted lists.Then, at least one personal identifier from sorted lists is sent to requestor.Such method can also comprise the programmed instruction for carrying out the other option of discussing in part below.
According to some embodiment, provide a kind of server system of inquiring about for the treatment of the vision that comprises face-image.This server system comprises for one or more processors of executive routine and stores the storer of one or more programs of being carried out by these one or more processors.These one or more programs comprise for as the instruction of process of general introduction below.Receive the vision inquiry that comprises the one or more face-images that comprise corresponding face-image from requestor.Identify the latent image coupling of the corresponding face-image of potential coupling according to visual similarity standard.Latent image coupling comprises the image of the one or more image sources that identify about requestor's data from basis.Mark mates with latent image the one or more people that are associated.The individual who identifies for each, retrieval is specific to individual data, and it comprises social networks tolerance that obtain from multiple application and social contiguity requestor.The freely group of following composition of multiple application choosings: communications applications, social networks application, calendar application and collaboration applications.By according to one or more tolerance of the visual similarity between corresponding face-image and latent image coupling and also according to comprising that at least the ranking information of social networks tolerance carries out rank to one or more identified individuals, generate individual sorted lists.Then, at least one personal identifier from sorted lists is sent to requestor.Such system can also comprise the programmed instruction for carrying out the other option of discussing in part below.
According to some embodiment, provide a kind of nonvolatile computer-readable recording medium of inquiring about for the treatment of the vision that comprises face-image.One or more programs that this computer-readable recording medium storage is configured to be carried out by computing machine, these one or more programs comprise for carrying out following instruction.Receive the vision inquiry that comprises the one or more face-images that comprise corresponding face-image from requestor.Identify the latent image coupling of the corresponding face-image of potential coupling according to visual similarity standard.Latent image coupling comprises the image of the one or more image sources that identify about requestor's data from basis.Mark mates with latent image the one or more people that are associated.The individual who identifies for each, retrieval is specific to individual data, and it comprises social networks tolerance that obtain from multiple application and social contiguity requestor.The freely group of following composition of multiple application choosings: communications applications, social networks application, calendar application and collaboration applications.By according to one or more tolerance of the visual similarity between corresponding face-image and latent image coupling and also according to comprising that at least the ranking information of social networks tolerance carries out rank to one or more identified individuals, generate individual sorted lists.Then, at least one personal identifier from sorted lists is sent to requestor.Such computer-readable recording medium can also comprise the programmed instruction for carrying out the other option of discussing in part below.
Brief description of the drawings
Fig. 1 is the block diagram that diagram comprises the computer network of vision querying server system.
Fig. 2 is that diagram is consistent with some embodiment vision being inquired about to the process flow diagram of the process responding.
Fig. 3 is that diagram is consistent with some embodiment vision being inquired about by interactive result document the process flow diagram of the process responding.
Fig. 4 is the process flow diagram of the communication between client and vision querying server system that diagram is consistent with some embodiment.
Fig. 5 is the block diagram of the client that diagram is consistent with some embodiment.
Fig. 6 is the block diagram of the front-end vision query processing server system that diagram is consistent with some embodiment.
Fig. 7 is that the block diagram of universal search system in the parallel search system that vision inquires about is processed in be used to consistent with some embodiment of diagram.
Fig. 8 is the block diagram that the OCR search system of vision inquiry is processed in be used to consistent with some embodiment of diagram.
Fig. 9 is the block diagram that the face recognition search system of vision inquiry is processed in be used to consistent with some embodiment of diagram.
Figure 10 is that be used to consistent with some embodiment of diagram processed the image of vision inquiry to the block diagram of word search system.
Figure 11 illustrates the client of the screenshotss with Exemplary Visual inquiry consistent with some embodiment.
The client of the screenshotss with the interactive result document with bounding box that each diagram of Figure 12 A and 12B is consistent with some embodiment.
What Figure 13 diagram was consistent with some embodiment has the client of the screenshotss of the interactive result document of coding by type.
What Figure 14 diagram was consistent with some embodiment have with the client of the screenshotss of the interactive result document of label.
The screenshotss that the interactive result document that Figure 15 diagram is consistent with some embodiment and vision inquiry and the results list show simultaneously.
Figure 16 A-16B is that diagram is consistent with some embodiment comprising that the vision of face-image inquires about the process flow diagram of the process responding.
Figure 17 is consistent with some embodiment various factors using in the time generating the individual sorted lists of the face-image of potential coupling vision in inquiring about of diagram and the process flow diagram of feature.
Figure 18 A is the block diagram of a part for the data structure in the face image data storehouse that utilizes of the diagram face recognition search system consistent with some embodiment.
Figure 18 B illustrates consistent with some embodiment the relation between the people of the multiple application such as social networks and communications applications.
Figure 18 C is the block diagram of the feature that obtains of diagram some images consistent with some embodiment.
Whole accompanying drawing, identical reference number refers to corresponding part.
Embodiment
With detailed reference to embodiment, illustrate in the accompanying drawings the example of described embodiment.In the following detailed description, many details have been set forth to provide overall understanding of the present invention.But, will be it is evident that to those of ordinary skill in the art, in the situation that there is no these details, can put into practice the present invention.In other cases, do not describe well-known method, program, assembly, circuits and networks in detail, in order to avoid unnecessarily make the aspect of embodiment smudgy.
It will also be understood that, although first, second grade of word can be for describing various elements at this, these elements should not limited by these words.These words are only for distinguishing element.For example, in the situation that not deviating from scope of the present invention, the first contact person can be called as the second contact person, and similarly, the second contact person can be called as the first contact person.The first contact person and the second contact person are contact persons, but it is not same contact person.
The term using in this description of this invention is only for describing the object of specific embodiment, and to be not intended to be limitation of the present invention.Unless linguistic context is clearly instruction in addition, as used in description of the invention and claims, singulative " ", " one " and " described " are also intended to comprise plural form.It will also be understood that, word "and/or" refers to and contains one or more any in the item of listing of being associated and likely combine as used in this.What will be further understood that is, word " comprises " existence of feature, complete thing, step, operation, element and/or the assembly of instruction statement in the time using in this manual, but does not get rid of existence or the interpolation of one or more other features, complete thing, step, operation, element, assembly and/or its cohort.
Depend on linguistic context, as used in this, word " if " can be construed as meaning " ... time " or " when ... " or " in response to determining " or " in response to detecting ".Similarly, depend on linguistic context, phrase " if determining " or " if detecting (the conditioned disjunction event of statement) " can be construed as meaning " when definite " or " in response to determining " or " in the time detecting (the conditioned disjunction event of statement) " or " in response to detecting (the conditioned disjunction event of statement) ".
Fig. 1 is that diagram is according to the block diagram of the computer network that comprises vision querying server system of some embodiment.Computer network 100 comprises one or more client 102 and vision querying server system 106.One or more communication networks 104 make these assembly interconnects.Communication network 104 can be any network in multiple network, comprises the combination of LAN (Local Area Network) (LAN), wide area network (WAN), wireless network, cable network, the Internet or such network.
Client 102 comprises for example, client application 108 for receiving vision inquiry (, the vision inquiry 1102 of Figure 11), and it is carried out by client.Vision inquiry is the image of submitting to search engine or search system as inquiry.Unrestricted document and image and the picture that comprises photo, scanning of example of vision inquiry.In certain embodiments, the set that client application 108 choosings are freely searched for application, formed for the search engine plug-in unit of browser application with for the search engine expansion of browser application.In certain embodiments, client application 108 is " widely " search boxes, and it allows user that the image of any form is dragged and dropped into this search box to be used as vision inquiry.
Client 102 sends to inquiry vision querying server system 106 and receives data from vision querying server system 106.Client 102 can be any computing machine or other equipment that can communicate with vision querying server system 106.Example is unrestricted comprises desk-top and notebook computer, mainframe computer, server computer, mobile device, the network terminal and Set Top Box such as mobile phone and personal digital assistant.
Vision querying server system 106 comprises front-end vision query processing server 110.Front-end server 110 receives vision inquiry from client 102, and sends to multiple parallel search systems 112 for process simultaneously this vision inquiry.Each realizes different vision query search processes search system 112, and access if desired database 114 that it is corresponding with by its different search procedure to vision inquiry process.For example, face recognition search system 112-A by the facial image data base 114-A of access to search and the facial match of image querying.As being described in more detail with reference to Fig. 9, if vision inquiry packet containing face, face recognition search system 112-A will return to one or more Search Results from face image data storehouse 114-A (for example, the face of name, coupling etc.).In another example, optical character identification (OCR) search system 112-B becomes text to return as one or more Search Results any discernible text-converted in vision inquiry.In optical character identification (OCR) search system 112-B, as being described in more detail with reference to Fig. 8, can access OCR database 114-B with identification specific font or patterns of text.
Can use any amount of parallel search system 112.Some examples comprise face recognition search system 112-A, OCR search system 112-B, image is to word search system 112-C (its can identifying object or object type), (it can be configured to the two dimensional image of identification such as book cover and CD to product identification search system, and can also be configured to identification such as the 3-D view of furniture), bar code recognition search system (it identifies a peacekeeping two-dimensional pattern bar code), named entity recognition search system, (it can be configured to the specific famous landmark of identification as Eiffel Tower in terrestrial reference identification, and can also be configured to the corpus of identification such as the specific image of billboard), the auxiliary place of geographical location information being provided by the gps receiver in client 102 or mobile telephone network is identified, color identification search system and similarly image search system (its search mark and vision are inquired about similar image).More search system can be added in Fig. 1 by represented, the other parallel search system of system 112-N.Except OCR search system, all search systems are defined as the search system of carries out image matching process jointly at this.The all search systems that comprise OCR search system are collectively referred to as by image querying search system.In certain embodiments, vision querying server system 106 comprise face recognition search system 112-A, OCR search system 112-B and at least one other by image querying search system 112.
Parallel search system 112 each respectively to visual search inquiry process, and its result is returned to front-end server system 110.In certain embodiments, front-end server 100 can be carried out one or more analyses to Search Results, such as following one or more: the subset that result is aggregated into compound document, selection result shows and result is carried out to rank, as being described in more detail with reference to Fig. 6.Front-end server 110 communicates by letter Search Results to client 102.
Client 102 is shown one or more Search Results to user.Result can be on display, show by audio tweeter or for any other device to telex network information.User can carry out with Search Results in many ways alternately.In certain embodiments, user's selection, annotation and be transmitted to alternately vision querying server system 106 with other of Search Results, and be recorded in inquiry and annotations database 116 together with vision inquiry.Information in inquiry and annotations database can be for improvement of vision Query Result.In certain embodiments, will be pushed to parallel search system 112 from property information cycle of inquiry and annotations database 116, its any relevant portion by information is integrated with its independent database 114 separately.
Computer network 100 comprises the word querying server system 118 for carry out search in response to word inquiry alternatively.With respect to the vision inquiry that comprises image, word inquiry is the inquiry that comprises one or more words.Word querying server system 118 can be carried out supplementary Search Results for generating the information that the search engine separately in vision querying server system 106 is produced.The result of returning from word querying server system 118 can comprise any form.Word querying server system 118 can comprise text document, image, video etc.Although word querying server system 118 is shown as autonomous system in Fig. 1, alternatively, vision querying server system 106 can comprise word querying server system 118.
Provide the other information about the operation of vision querying server system 106 with reference to the process flow diagram in figure 2-4 below.
Fig. 2 be diagram according to some embodiment of the present invention for vision being inquired about to the process flow diagram of the vision querying server systems approach responding.Each in the operation shown in Fig. 2 can be corresponding to the instruction being stored in computer memory or computer-readable recording medium.
Vision querying server system receives vision inquiry (202) from client.Client can be for example desk-top computing equipment, mobile device or another similar devices (204), as illustrated with reference to figure 1.Figure 11 illustrates the example vision inquiry in example client systems.
Vision inquiry is the image document of any appropriate format.For example, vision inquiry can be the image of photo, screenshotss, scanning or the sequence (206) of frame or multiple frame of video.In certain embodiments, vision inquiry is the picture that content creation program (Fig. 5 736) produces.So, in certain embodiments, the inquiry of user's " drafting " vision, and in other embodiments, scanning input or the inquiry of shooting vision.Some vision inquiries are used such as image generation application, photo editing program, plotter program or the image editor of Acrobat and are created.For example, vision inquiry can be from: user is at its friend's of its mobile phone photographs photo, and then using this photo as vision, server system is submitted in inquiry.Vision inquiry can also be from: user is scanned magazine page, or obtains the screenshotss of the webpage on desk-top computer, then scanning or screenshotss is inquired about and is submitted to server system as vision.In certain embodiments, vision inquiry is submitted to server system 106 by the search engine expansion of browser application, the search application of carrying out by the plug-in unit for browser application or by client 102.Vision inquiry can also or generate other application programs that can send the image that is positioned at long-range server to by client by (client is carried out) support and submit to.
Vision inquiry can be the combination (208) of text and non-text element.For example, inquiry can be to comprise image and text, stands in road sign side such as a people, the scanning of magazine page.Vision inquiry can comprise no matter being by the image that is embedded in document camera or that scanned or received by the client face that obtain, people in client.Vision inquiry can also be the scanning that only comprises the document of text.Vision inquiry can also be the image of multiple different themes, for example, for example, such as several birds, people and object in forest (, automobile, park bench etc.), humans and animals (, pet, farm-animals, butterfly etc.).Vision inquiry can have two or more different elements.For example, vision inquiry can be included in bar code and product image or the name of product in the packing of product.For example, vision inquiry can be the picture that comprises the book cover of books title, Album Cover Art and bar code.As be discussed in more detail below, in some cases, a vision inquiry will produce the two or more different Search Results corresponding from the different piece of this vision inquiry.
Server system is inquired about and is processed vision as follows.Front-end server system sends to multiple parallel search systems for process (210) simultaneously vision inquiry.Each search system realizes different vision query search processes, separate searches system by himself processing scheme to vision inquiry process.
In certain embodiments, to be sent to it be optical character identification (OCR) search system for one in the search system of processing for vision inquiry.In certain embodiments, to be sent to it be face recognition search system for one in the search system of processing for vision inquiry.In certain embodiments, multiple search systems of moving different vision query search processes at least comprise: optical character identification (OCR), face recognition and be different from OCR and another of face recognition by image querying process (212).Another is selected from and includes but not limited to following process collection by image querying process: product identification, bar code recognition, object or object type identification, named entity recognition and color identification (212).
In certain embodiments, named entity recognition occurs as the later stage process of OCR search system, wherein the text results of OCR is analyzed to famous people, place, object etc., then in word querying server system (Fig. 1 118) search to be identified as be the word of named entity.In other embodiments, the image of famous terrestrial reference, mark, people, album cover, trade mark etc. by image to word search system identification.In other embodiments, utilize from image and press image querying process to the different named entities of word search system separation.Object or the identification of object type recognition system are as the general result type of " automobile ".In certain embodiments, this system is also identified product brand, specific products model etc., and description is more specifically provided, as " Porsche ".Part in search system can be the search system specific to special user.For example, the particular version of color identification and face recognition can be the special search system being used by blind person.
Front-end server system is from parallel search system reception result (214).In certain embodiments, result is with search score value.For some vision inquiries, the part in search system can not find correlated results.For example, if vision inquiry is the picture of flower, face recognition search system and bar code search system can not find any correlated results.In certain embodiments, if do not have correlated results found, receive sky or zero searching score value (216) from this search system.In certain embodiments, if front-end server the predefine period (for example, 0.2,0.5,1,2 or 5 second) do not receive result from search system afterwards, it has produced this overtime server seemingly as empty search score value received result has been processed, and will the result receiving from other search systems be processed.
Alternatively, in the time meeting predefine standard at least two in received Search Results, it is carried out to rank (218).In certain embodiments, in predefine standard gets rid of null result.Predefine standard is that result is not invalid.In certain embodiments, an eliminating in predefine standard has the result of (for example, about the relevance factors) numerical score under the minimum score value of the predefine of dropping on.Alternatively, multiple Search Results are filtered to (220).In certain embodiments, only exceed predefine threshold value at the sum of result, result is filtered.In certain embodiments, the result under dropping on the minimum score value of predefine is excluded, all results are carried out to rank.For some vision inquiries, the content of result is filtered.For example, if the information that the part in result comprises personal information or individual's protection, these result of filtering.
Alternatively, vision querying server system creation compound searching result (222).An one embodiment is: as illustrated with reference to figure 3, in the time that more than one search system result is embedded in interactive result document.Word querying server system (Fig. 1 118) can be used from the result of word search and expand from the result of in parallel search system, text and/or image that wherein result is linked to document or information source or comprises other information that may be relevant to vision inquiry in addition.Therefore, for example, compound searching result can comprise OCR result and the link (224) to the named entity in OCR document.
In certain embodiments, in OCR search system (112-B of Fig. 1) or front-end vision query processing server (Fig. 1 110) identification text may be relevant word.For example, it can identify the named entity such as famous person or place.Submit to word querying server system (Fig. 1 118) using named entity as query terms.In certain embodiments, word Query Result word querying server system being produced is embedded in vision Query Result as " link ".In certain embodiments, word Query Result is returned as independent link.For example, if the picture of book cover is vision inquiry, likely, object identification search system will produce hitting of higher scoring for these books.So, the word inquiry by operation in word querying server system 118 about the title of these books, and word Query Result is returned together with vision Query Result.In certain embodiments, in group, show that word Query Result is to distinguish itself and vision Query Result tagging.Can distinguish Search Results, maybe can carry out search to produce relevant especially other Search Results with the named entity of all identifications in search inquiry.For example, if vision inquiry is the tourism pamphlet about Paris of scanning, the result of returning can be included in the link of the search to word inquiry " Notre Dame de Paris " for initiation of word querying server system 118.Similarly, compound searching result comprises the result from the text search of the famous image about identified.For example, in same tourism pamphlet situation, can also illustrate about the famous destination that is shown as picture in pamphlet, as " Eiffel Tower " and " Louvre Palace ", the live link (even word " Eiffel Tower " and " Louvre Palace " in pamphlet itself not occur) of word Query Result.
Then vision querying server system sends at least one result client (226).Typically, if at least part of reception multiple Search Results of vision query processing server from multiple search systems, then it send to client by least one in multiple Search Results.For some vision inquiries, only a search system can be returned to correlated results.For example, in the vision inquiry of image that only comprises text, only the possibility of result of OCR server is correlated with.For some vision inquiries, be correlated with from an only the possibility of result of a search system.For example, only may be correlated with the relevant product of bar code of scanning.In these cases, front-end vision processing server will only return to relevant search result.For some vision inquiries, multiple Search Results are sent to client, and the plurality of Search Results comprises from the more than one Search Results (228) in parallel search system.This can occur in the time that more than one different images is in vision inquiry.For example, if vision inquiry is the picture that people rides, can show the result about this people's face recognition with together with object recognition result about this horse.In certain embodiments, make the poly-group of all results about ad hoc inquiry and the displaying together via image search system.For example, in the highest N face recognition result of the lower demonstration of title " face recognition result ", and under title " object recognition result ", show together the highest N object recognition result.As an alternative, as described below, can make to gather group from the Search Results of specific image search system by image-region.For example, if vision inquiry comprises two faces, both all produce face recognition result its, will be shown as not on the same group about the result of each face.For example, for some vision inquiries (, comprising the vision inquiry of the image of text and one or more objects), Search Results can comprise OCR result and one or more images match result (230).
In certain embodiments, user may wish to understand more information about particular search result.For example, if vision inquiry is the picture of dolphin, and " image is to word " search system is returned to following word " water ", " dolphin ", " blueness " and " flipper ", user may wish the text based query terms search of operation to " flipper ".For example, when user wishes that operation to the search of word inquiry (, as clicked or select the correspondence link in Search Results indicated by user) time, query terms server system (Fig. 1 118) is conducted interviews, and the search of operation to selected word.Independent or show corresponding search terms result (232) in conjunction with vision Query Result in client.In certain embodiments, front-end vision query processing server (Fig. 1 110) is automatically (except initial visual inquiry, do not receive any user command) be vision query selection the highest one or more potential text results, in word querying server system 118, move those text results, then those word Query Results are returned to client together with vision Query Result, as a part (232) that at least one Search Results is returned to client.In the above example, if " flipper " is the first word result of the vision inquiry picture of dolphin, front-end server is inquired about for " flipper " operation word, and those word Query Results are returned to client together with vision Query Result.This embodiment, wherein thinks that the word result that may be selected by user was automatically performed before the Search Results of inquiring about from vision is sent to user, has saved user time.In certain embodiments, as mentioned above, these results are shown as to compound searching result (222).In other embodiments, alternative composite Search Results or except compound searching result, described result is a part for search result list.
Fig. 3 is that diagram is for inquiring about the process flow diagram of the process responding to vision by interactive result document.With reference to figure 2, first three operation (202,210,214) is described in the above.From receive the Search Results of (214) from parallel search system, create interactive result document (302).
Now will describe in detail and create interactive result document (302).For some vision inquiries, interactive result document comprises one or more visual identifier of each subdivision of vision inquiry.Each visual identifier has at least one at least one the at user option link in Search Results.The corresponding subdivision of visual identifier mark vision inquiry.For some vision inquiries, interactive result document only has a visual identifier with an at user option link to one or more results.In certain embodiments, the selectable link of one or more relative users in Search Results has active region, and active region is corresponding to the subdivision of the vision inquiry being associated with corresponding visual identifier.
In certain embodiments, visual identifier is bounding box (304).In certain embodiments, as shown in Figure 12 A, bounding box is around the subdivision of vision inquiry.Bounding box needs not to be square or rectangular shaped as frame shape, but can be the shape of any style, comprise circular, oval-shaped, (for example, region with object, entity or vision inquiry in vision inquiry) isogonism, irregular or any other shape, as shown in Figure 12B.For the inquiry of some visions, bounding box is sketched the contours of the border (306) of the entity identifying in the subdivision of vision inquiry.In certain embodiments, each bounding box is included in the at user option link of one or more Search Results, wherein at user option link have with bounding box around the corresponding active region of the subdivision of vision inquiry.In the time that the space in bounding box (active region of at user option link) selected by user, return to the Search Results corresponding with image in the subdivision of sketching the contours of.
In certain embodiments, as shown in Figure 14, visual identifier is label (307).In certain embodiments, label comprises at least one word of the image correlation connection in the corresponding subdivision of inquiring about with vision.Each label is formatd in interactive result document, in corresponding subdivision or near corresponding subdivision, show.In certain embodiments, label is coloud coding.
In certain embodiments, each corresponding visual identifier is formatd, with the type of the entity of being identified according in the corresponding subdivision of vision inquiry, show in visually different modes.For example, as shown in Figure 13, around product, people, trade mark and two text filed bounding boxes, each is shown with different cross hatch patterns, represents the transparent bounding box of different colours.In certain embodiments, visual identifier is formatd, show in visually different modes, such as overlapping color, overlapping pattern, label background color, label background patterns, label font color and border color.
In certain embodiments, the at user option link in interactive result document is link (308) to the document that comprises the one or more results relevant with the corresponding subdivision of vision inquiry or object.In certain embodiments, at least one Search Results comprises the data relevant with the corresponding subdivision of vision inquiry.So, in the time of selectable link that user selects to be associated with corresponding subdivision, this user is directed to the Search Results corresponding with the entity of identifying in the corresponding subdivision of vision inquiry.
For example, if vision inquiry is the photo of bar code, may have such photo part, it is the uncorrelated part of the packaging on bar code invests.Interactive result document can comprise the only bounding box around bar code.In the time selecting in the bar code bounding box that user is sketching the contours of, show bar shape code Search Results.Bar code Search Results can comprise a result, and the name of product corresponding with this bar code, or barcode results can comprise several results, such as wherein can buying, the multiple place of this product such as comment.
In certain embodiments, in the time that the subdivision of the vision inquiry corresponding with corresponding visual identifier comprises the text that comprises one or more words, the corresponding Search Results of visual identifier corresponding to this comprises the result from the word query search of at least one of the word in text.In certain embodiments, the face that the subdivision of inquiring about when the vision corresponding with corresponding visual identifier comprises people, while wherein finding for this face at least one coupling (being Search Results) that meets predefine reliability (or other) standard, the corresponding Search Results of visual identifier corresponding to this comprises following one or more: name, address, contact details, account information, address information, be included in the current location of the relevant mobile device that the people in selectable subdivision is associated to its face, its face is included in the latent image coupling of other images of the people in selectable subdivision and this people's face.In certain embodiments, when the subdivision of the vision inquiry corresponding with corresponding visual identifier comprises product, while wherein finding for this product at least one coupling (being Search Results) that meets predefine reliability (or other) standard, the corresponding Search Results of visual identifier corresponding to this comprises following one or more: the option of product information, product review, the purchase of initiation to product, initiate option, similar products list and the Related product list of the bid to product.
Alternatively, the selectable link of relative users in interactive result document comprises anchor text, and it shows, and needn't activate link in document.Anchor text provide with when linking the information-related information of obtaining while being activated, such as keyword or word.Can be by anchor text display a part for label (307) in a part for bounding box (304), show or be shown as when user by cursor hovers in the at user option other information shown while determining the period such as 1 second pre-that reaches that chains.
Alternatively, the selectable link of relative users in interactive result document is the link to search engine, and it is for search information or the document corresponding with text based inquiry (being sometimes referred to as word inquiry at this).The activation of this link impels search engine to carry out search, wherein inquiry and search engine (are for example specified by this link, search engine is specified by the URL in this link, and text based search inquiry is specified by the URL parameter of this link), result is returned to client simultaneously.Alternatively, link in this example can comprise the text of specifying in search inquiry or the anchor text of word.
In certain embodiments, in response to vision inquiry and the interactive result document producing can comprise multiple link corresponding with result from same search system.For example, vision inquiry can be group's image or picture.Interactive result document can comprise the bounding box around everyone, and it is that each face in crowd returns results from face recognition search system in the time being activated.For some vision inquiries, the multiple links in interactive result document are corresponding to the Search Results from more than one search system (310).For example, if the picture of people and dog is submitted as vision inquiry, the bounding box in interactive result document can be sketched the contours of respectively this people and dog.In the time that (in interactive result document) this people is selected, return to the Search Results from face recognition search system, and in the time that (in interactive result document) this dog is selected, return to the result to word search system from image.For some vision inquiries, interactive result document comprises OCR result and images match result (312).For example, submitted as vision inquiry if people stands in the picture on mark side, interactive result document can comprise for this people with for the visual identifier of the text of this mark.Similarly, if the scanning of magazine is used as vision inquiry, interactive result document can comprise for the photo of the advertisement on the page or the visual identifier of trade mark and for the visual identifier of the text of same article on this page.
After having created interactive result document, send it to client (314).In certain embodiments, as discussed with reference to figure 2 in the above, for example, in conjunction with send interactive result document (, the document 1200 of Figure 15) from the search result list of one or more parallel search systems.In certain embodiments, as shown in Figure 15, on client is in from the search result list of one or more parallel search systems or contiguous described search result list show interactive result document (315).
Alternatively, user comes the visual identifier by selection result document to carry out alternately with result document.Server system receives from client the information (316) of selecting about the user of the visual identifier interactive result document.As mentioned above, in certain embodiments, by selecting the active region in bounding box to activate link.In other embodiments, the user of the visual identifier of the subdivision of inquiring about by vision selects to activate link, and described visual identifier is not bounding box.In certain embodiments, the visual identifier of link is hot button, is arranged in the underlined word of label, text or the object of vision inquiry or other expressions of theme near subdivision.
Search result list is shown in the embodiment of (315) together with interactive result document, in the time that user selects at user option link (316), mark links corresponding Search Results with selected in search result list.In certain embodiments, cursor is by redirect or be automatically moved to selected and link the first corresponding result.Too little and can not show in some embodiment of interactive result document and whole search result list at the display of client 102, select link in interactive result document to impel search result list to roll or redirect, link at least the first corresponding result to show with selected.In some other embodiment, select in response to the user of the link in interactive result document, to the results list rearrangement, make to show and link the first corresponding result with this at the top of the results list.
In certain embodiments, in the time that user selects at user option link (316), vision querying server system sends to client to show (318) to user at least subset of the result relevant with the corresponding subdivision of vision inquiry.In certain embodiments, user can select multiple visual identifier simultaneously, and by the result subset receiving about all selected visual identifier simultaneously.In other embodiments, before the user of any link at user option link selects, to be preloaded in client corresponding to the Search Results of at user option link, select and almost at once provide Search Results to user with the user of the one or more links in response to in interactive result document.
Fig. 4 is the process flow diagram that is shown in the communication between client and vision querying server system.Client 102 receives vision inquiry (402) from user/inquiry/requestor.In certain embodiments, can be only from registered or " selection adds (opt-in) " accept vision inquiry to the user of visual query system.In certain embodiments, be only that the user of registered face recognition visual query system carries out the search to face recognition coupling, and be anyone vision inquiry of carrying out other types, no matter its whether " selection adds " to face recognition part.
As mentioned above, the form of vision inquiry can be taked many forms.Vision inquiry may comprise one or more themes of the subdivision that is arranged in vision inquiry document.For some vision inquiries, client 102 is to vision query execution type identification pre-service (404).In certain embodiments, client 102 is searched for specific discernible pattern in this pretreatment system.For example, for some vision inquiries, client can identification colors.For the inquiry of some visions, client can identify specific subdivision may comprise text (because this region by with light color space etc. around less dark-coloured character form).Client can comprise any amount of pre-service type identifier or type identification module.In certain embodiments, client is by the type identification module (bar code recognition 406) having for identifying bar code.Can do so by the unique candy strip in identification rectangular region.In certain embodiments, client will have the type identification module (face detects 408) that may comprise face for the particular topic of recognition visible sensation inquiry or subdivision.
In certain embodiments, identified " type " returned to user for checking.For example, client 102 can be returned to statement and " in your vision inquiry, find bar code, is it interested that you dock receipt shape code Query Result? " message.In certain embodiments, message even can be indicated the type subdivision of found vision inquiry therein.In certain embodiments, this displaying is similar to the interactive result document of discussing with reference to figure 3.For example, it can sketch the contours of the subdivision of vision inquiry, and indicates this subdivision may comprise face, and inquires user that whether it is interested in receiving face recognition result.
After client 102 is carried out the optional pre-service of vision inquiry, this vision inquiry is sent to vision querying server system 106 by client, specifically sends to front-end vision query processing server 110.In certain embodiments, if pre-service has produced correlated results, an if result having produced higher than a certain threshold value in type identification module, the subdivision of instruction inquiry or inquiry may be particular type (face, text, bar code etc.), and client is by the information about pretreated result to front transfer.For example, client can indicate face recognition module to comprise face to the specific subdivision of vision inquiry 75% assurance.More generally, pre-service result, if any, comprises one or more theme class offsets (for example, bar code, face, text etc.).Alternatively, send to the pre-service result of vision querying server system to comprise following one or more: for each the theme class offset in pre-service result, identify the information of the subdivision of the vision inquiry corresponding with this theme class offset, and for each the theme class offset in pre-service result, the value of the confidence of the confidence level of the mark of the corresponding subdivision of instruction to this theme class offset and/or vision inquiry.
Front-end server 110 receives vision inquiry (202) from client.Received vision inquiry can comprise above-mentioned pretreatment information.As mentioned above, vision inquiry is sent to multiple parallel search systems (210) by front-end server.If front-end server 110 has received the pretreatment information of the possibility of the theme that has comprised a certain type about subdivision, front-end server can pass to this information forward one or more in parallel search system.For example, it can transmit the information that specific subdivision may be face, and first face recognition search system 112-A can be processed this branch of vision inquiry.Similarly, sending (specific subdivision may be face) identical information can be made for ignoring this subdivision or first other subdivisions being analyzed by other parallel search systems.In certain embodiments, front-end server can not pass to pretreatment information parallel search system, but alternatively expands by this information the mode that it is processed the result receiving from parallel search system.
As illustrated with reference to figure 2, for some vision inquiries, front-end server 110 receives multiple Search Results (214) from parallel search system.Then front-end server can carry out multiple rank and filtration, and can create interactive search result document, as with reference to figure 2 and 3 explanations.If front-end server 110 has received the pretreatment information of the possibility of the theme that has comprised a certain type about subdivision, it can be filtered and be sorted by those result preferences of mating the type of theme of identifying through pre-service.If user has indicated the result of request particular type, front-end server will be considered user's request when the result.For example, if user has only asked bar code information, front-end server can the every other result of filtering, or front-end server will be listed all results relevant with asked type before listing other results.If interactive visual inquiry document is returned, server can be to indicated linking that interested result type is associated to carry out pre-search with user, and be only provided for carrying out the link of the relevant search to other themes indicated in interactive result document.Then, Search Results is sent to client (226) by front-end server 110.
Client 102 is from server system reception result (412).When in place, these results will comprise the result of the result type that coupling finds in pretreatment stage.For example, in certain embodiments, it will comprise one or more barcode results (414) or one or more face recognition result (416).If it is possible that the pretreatment module of client has been indicated the result of particular type, and this result is found, by the outstanding result finding of listing the type.
Alternatively, user will select or annotate (418) to one or more in result.User can select a Search Results, can select the Search Results of particular type and/or can select the part (420) of interactive result document.Returned result implicit feedback associated with the query to the selection of result.Such feedback information can be utilized in following query processing operation.Annotation provide also can in following query processing operation, be utilized, about the demonstration feedback of returned result.Annotation is taked following form: the correction (as the correction of the word to wrong OCRization) of the part to returned result or independent annotation (free form or structurized).
User's the selection to a Search Results, generally selects " correct " result (for example, selecting the correct result from facial recognition server) from several results of same type, is the process that is called as the selection in explanation.The selection of user's the Search Results to particular type, the general result from the several dissimilar results of returning selections interested " type " (for example, select the text through OCRization of the article in magazine, instead of about the visual results of same advertisement on the same page), be the process that is called as the disambiguation to intention.As described in detail with reference to figure 8, user can select the word (such as identified named entity) of the specific link in the document of OCRization similarly.
As an alternative or additionally, user may wish particular search result to annotate.Can complete this annotation (422) with free form style or structured format.Annotation can be can be maybe the comment to result to the description of result.For example, it can indicate the title of the theme in result, or it can indicate " this is this good book " or " this product damages in buying 1 year ".Another example of annotation is the text that the user of object in bounding box and this bounding box of mark of drawing around the user of the subdivision of vision inquiry or theme provides.Be described in more detail user comment with reference to figure 5.
The user of Search Results is selected to send to server system (424) with other annotations.Front-end server 110 receives this selection and annotation, and it is further processed to (426).If information is the selection to the object in interactive result document, subregion or word, as suitably, can ask the further information about this selection.For example, if selected a visual results, the more information about this visual results by request.If selecting is (arriving word server from OCR server or from image) word, will the text search of this word be sent to word querying server system 118.If selecting is the people from face-image identification search system, by this people's of request profile.If selecting is the specific part about interactive search result document, by vision Query Result potential request.
Illustrate with reference to figure 5, if server system receive annotation, by this annotation storage inquiry and annotations database 116 in.Then, by the one or more independent annotations database copying to from property information cycle of annotations database 116 in parallel server system, as discussed with reference to figure 7-10 below.
Fig. 5 is the block diagram of the client 102 that diagram is consistent with one embodiment of the present of invention.Client 102 typically comprises one or more processing units (CPU) 702, one or more network or other communication interfaces 704, storer 712 and for making one or more communication buss 714 of these assembly interconnects.Client 102 comprises user interface 705.User interface 705 comprises display device 706, and comprises alternatively the input media 708 such as keyboard, mouse or other load buttons.As an alternative or additionally, display device 706 comprises touch-sensitive surperficial 709, and in this case, display 706/709 is touch-sensitive display.Having in the client of touch-sensitive display 706/709, physical keyboard is optional (for example,, in the time that needs keyboard is inputted, can show soft keyboard).In addition, some client are supplemented or alternative keyboard with microphone and speech recognition.Alternatively, client 102 comprises GPS (HA Global Positioning Satellite) receiver or for determining other position detecting devices 707 of position of client 102.In certain embodiments, provide the service of vision query search, it requires client 102 to support vision querying server system to receive the positional information of the position of instruction client 102.
Client 102 also comprises image-capturing apparatus 710, such as camera or scanner.Storer 712 comprises high-speed random access memory, such as DRAM, SRAM, DDR RAM or other random access solid-state memory device; And can comprise nonvolatile memory, such as one or more disk storage devices, optical disc memory apparatus, flash memory device or other non-volatile solid-state memory devices.Storer 712 can comprise the local one or more memory devices that are positioned at away from CPU702 alternatively.Non-volatile memory devices in storer 712 or as an alternative storer 712 comprises nonvolatile computer-readable recording medium.In certain embodiments, following program, module and data structure or its subset of the computer-readable recording medium storage of storer 712 or storer 712:
Operating system 716, it comprises for the treatment of various basic system services with for carrying out the program of the task of relying on hardware;
Network communication module 718, it is used to via one or more communications network interfaces 704 (wired or wireless) and such as one or more communication networks of the Internet, other wide area networks, LAN (Local Area Network), Metropolitan Area Network (MAN) etc., client computer 102 is connected to other computing machines;
Image capture module 720, the respective image that it captures for the treatment of image-capturing apparatus/camera 710, wherein this respective image can be used as vision inquiry (for example,, by client application module) and sends to vision querying server system;
One or more client application modules 722, it,, for the treatment of the various aspects of query-by-image, includes but not limited to: submit module 724 to by image querying, it is for submitting to vision querying server system by vision inquiry; Alternatively, area-of-interest is selected module 725, and it detects selection to the area-of-interest in image (such as, the gesture on touch-sensitive display 706/709), and this area-of-interest is prepared as to vision inquires about; Result browsing device 726, it is for showing the result of vision inquiry; And alternatively, annotations module 728, its with: for the optional module 730 of structuring narrative text input, such as filling with a kind of form; Or the optional module 732 of inputting for free form narrative text, it can accept the annotation from multiple format; And image-region selection module 734 (be sometimes called as result at this and select module), it allows user to select the specific subdivision of image to annotate;
Optional content creation application 736, it allows user by creating or edited image, instead of only catches one via image-capturing apparatus 710 and create vision inquiry; Alternatively, one or such application 736 can comprise that the subdivision that makes user can select image is with the instruction as vision inquiry;
Optional local image analysis module 738, it carried out pre-service to this vision inquiry before vision inquiry is sent to vision querying server system.Local graphical analysis can recognition image particular type or the subregion in image.The example of the image type that can be identified by such module 738 comprises following one or more: facial types (face-image of identification in vision inquiry), bar code type (bar code of identification in vision inquiry) and text (text of identifying in vision is inquired about); And
Optional client application 740 in addition, such as e-mail applications, phone application, browser application, map application, instant message application, social networks application etc.In certain embodiments, in the time that suitable moved Search Results is selected, can starts or access with this and can move the corresponding application of Search Results.
Alternatively, allow user to select the specific subdivision of image to select module 734 also to allow user to select Search Results to hit as " correct " using the image-region annotating, and needn't further annotate it.For example, user can be demonstrated the highest N face recognition coupling, and can select correct people from this results list.For some search inquiries, by the result of showing more than a type, and user is by the result of selecting one type.For example, image querying can comprise that a people stands in tree side, but only concerning user, is only interested about this people's result.Therefore, image is selected module 734 to allow user to indicate which kind of image type is " correct " type---and, it is interested type in reception.User also may wish, by using (for what fill with a kind of form) narrative text load module 730 or free form narrative text load module 732 to add individual's notes and commentary or descriptive word, Search Results to be annotated.
In certain embodiments, optional local image analysis module 738 is parts of client application (Fig. 1 108).In addition, in certain embodiments, optional local image analysis module 738 comprises for carrying out local graphical analysis with the one or more programs to vision is inquired about or its part is carried out pre-service or classification.For example, client application 722 can comprise bar code, face or text by recognition image before vision inquiry is submitted to search engine.In certain embodiments, in the time that local image analysis module 738 detects vision inquiry packet containing the image of particular type, whether it interested in the Search Results of corresponding types for this module queries user.For example, local image analysis module 738 can the general features (that is, need not determine which people's face) based on face detect face, and provides immediate feedback to user before inquiry being sent in vision querying server system.It can return as " face detected, you mate interested to the face recognition that obtains this face? " result.This can save time for vision querying server system (Fig. 1 106).For the inquiry of some visions, front-end vision query processing server (Fig. 1 110) only sends to by vision inquiry the corresponding search system 112 of image type of identifying with local image analysis module 738.In other embodiments, vision inquiry can be sent to all search system 112A-N to the vision inquiry of search system 112, but the result of the search system 112 corresponding image type to from identifying with local image analysis module 738 is carried out to rank.In certain embodiments, configuration or processing parameter that the mode that local graphical analysis exerts an influence to the operation of vision querying server system depends on the configuration of client or is associated with user or client.In addition the actual content that, any particular visual is inquired about and the result being produced by local graphical analysis can impel different visions inquiries in client and vision querying server system is arbitrary or both places are differently processed.
In certain embodiments, carry out bar code recognition with two steps, wherein to vision, whether inquiry comprises that analysis local image analysis module 738 places in client of bar code carry out.Then, only, in the time that client determines that vision inquiry may comprise bar code, just this vision inquiry is passed to bar code search system.In other embodiments, bar code search system is processed each vision inquiry.
Alternatively, client 102 comprises other client application 740.
Fig. 6 is the block diagram of the front-end vision query processing server system 110 that diagram is consistent with one embodiment of the present of invention.Front-end server 110 typically comprises one or more processing units (CPU) 802, one or more network or other communication interfaces 804, storer 812 and for making one or more communication buss 814 of these assembly interconnects.Storer 812 comprises high-speed random access memory, such as DRAM, SRAM, DDR RAM or other random access solid-state memory device; And can comprise nonvolatile memory, such as one or more disk storage devices, optical disc memory apparatus, flash memory device or other non-volatile solid-state memory devices.Storer 812 can comprise the local one or more memory devices that are positioned at away from CPU802 alternatively.Non-volatile memory devices in storer 812 or as an alternative storer 812 comprises nonvolatile computer-readable recording medium.In certain embodiments, following program, module and data structure or its subset of the computer-readable recording medium storage of storer 812 or storer 812:
Operating system 816, it comprises for the treatment of various basic system services with for carrying out the program of the task of relying on hardware;
Network communication module 818, it is used to via one or more communications network interfaces 804 (wired or wireless) and such as one or more communication networks of the Internet, other wide area networks, LAN (Local Area Network), Metropolitan Area Network (MAN) etc., front-end server system 110 is connected to other computing machines;
Inquiry manager 820, it is inquired about for the treatment of the vision entering from client 102, and sends it to two or more parallel search systems; As described in other places in this document, at some in particular cases, vision inquiry can be for only in search system, for example, such as in the time that vision inquiry comprises the instruction (, " only face recognition search ") of client generation;
Result filtering module 822, its for alternatively to filtering from the result of one or more parallel search systems, and will be the highest or " being correlated with " result send to client 102 for displaying;
Result rank and formatting module 824, it is for alternatively the result from one or more parallel search systems being carried out to rank, and for result is formatd for displaying;
Result document creation module 826, it is used to create interactive search result document in due course; Module 826 can comprise submodule, includes but not limited to: bounding box creation module 828 and link creation module 830;
Label creation module 831, it is for being created as the label of visual identifier of corresponding subdivision of vision inquiry;
Annotations module 832, it is for receiving annotation from user, and sends it to annotations database 116;
Can move Search Results module 838, it,, in response to vision inquiry, generates one or more Search Results elements that move, and each is configured to start client-side action; The example of Search Results element of can moving is for telephone calling, initiate email message, the address that is mapped out, to carry out restaurant predetermined and the button of the option of buying product is provided; And
Inquiry and annotations database 116, it comprises database itself 834 and the index 836 to database.
Result rank and formatting module 824 carry out rank to the result of returning from one or more parallel search systems (112-A-112-N of Fig. 1).As pointed out in the above, for some vision inquiries, be only correlated with from the possibility of result of a search system.Under these circumstances, only the relevant search result from this search system is carried out to rank.For some vision inquiries, the Search Results of a few types may be correlated with.In these cases, in certain embodiments, result rank and formatting module 824 make from all result ranks of search system for example, with correlated results (, having the result of high correlation score value) higher than the result about more incoherent search system.In other embodiments, result rank and formatting module 824 make from the highest result rank of each related search system higher than residue result.In certain embodiments, result rank and formatting module 824 carry out rank according to the relevance score of each calculating in Search Results to result.For some vision inquiries, except the enterprising line search of parallel visual search system, carry out the text query expanding.In certain embodiments, in the time that text query is performed equally, show its result to be visually different from the mode of visual search system result.
Result rank and formatting module 824 also format result.In certain embodiments, show result with listings format.In certain embodiments, show result by interactive result document.In certain embodiments, show interactive result document and the results list.In certain embodiments, how query type instruction result is demonstrated.For example, if the more than one theme of searching for detected in vision inquiry, produce interactive result document, and iff the theme that can search for being detected, will only show result with listings format.
Result document creation module 826 is for creating interactive search result document.The theme that interactive search result document can have one or more detections and search.Bounding box creation module 828 creates around the one or more bounding box in the theme searching.Bounding box can be rectangle frame, maybe can sketch the contours of the shape of theme.Link creation module 830 is created to the link of Search Results, described Search Results and its corresponding Topic relative connection in interactive search result document.In certain embodiments, in bounding box region, click and activate the correspondence link that link creation module is inserted.
Inquiry and annotations database 116 comprise the information that can be used for improving vision Query Result.In certain embodiments, user can annotate image after vision Query Result has been demonstrated.In addition, in certain embodiments, user can annotate image before image being sent to vision query search system.Pre-annotation can be by making to help vision query processing in result set or with the text based search of the word of vision query search parallel running to annotation.In certain embodiments, can make the version through annotation of picture open (for example, when user is not for example private and has permitted when open by image and annotation are indicated as being), to be returned as latent image match hit.For example, if user has taken the picture of flower, and by providing about detailed genus and the kind information of this flower, this image is annotated, this user may want this image to search anyone displaying of the vision inquiry research of this flower to execution.In certain embodiments, will be pushed to parallel search system 112 from property information cycle of inquiry and annotations database 116, it merges to the relevant portion of information (if any) in its independent database 114 separately.
Fig. 7 is the block diagram of illustrating in the parallel search system that is used to process vision inquiry, and Fig. 7 illustrates " general " search system 112-N consistent with one embodiment of the present of invention.This server system is general, only because it represents any one in vision query search server 112-N.Generic server system 112-N typically comprises one or more processing units (CPU) 502, one or more network or other communication interfaces 504, storer 512 and for making one or more communication buss 514 of these assembly interconnects.Storer 512 comprises high-speed random access memory, such as DRAM, SRAM, DDR RAM or other random access solid-state memory device; And can comprise nonvolatile memory, such as one or more disk storage devices, optical disc memory apparatus, flash memory device or other non-volatile solid-state memory devices.Storer 512 can comprise the local one or more memory devices that are positioned at away from CPU502 alternatively.Non-volatile memory devices in storer 512 or as an alternative storer 512 comprises nonvolatile computer-readable recording medium.In certain embodiments, following program, module and data structure or its subset of the computer-readable recording medium storage of storer 512 or storer 512:
Operating system 516, it comprises for the treatment of various basic system services with for carrying out the program of the task of relying on hardware;
Network communication module 518, it is used to via one or more communications network interfaces 504 (wired or wireless) and such as one or more communication networks of the Internet, other wide area networks, LAN (Local Area Network), Metropolitan Area Network (MAN) etc., generic server system 112-N is connected to other computing machines;
Specific to the search application 520 of particular server system, it can be for example bar code search application, color identification search application, product identification search application and object or object type search application etc.;
If specific search applications exploiting index, optional index 522;
Optional image data base 524, it applies relevant image for storing to specific search, and the view data wherein stored if any, depends on search procedure type;
Optional result ranking module 526 (being sometimes called as relevance score module), it is for carrying out rank to the result of applying from search, ranking module can be each the result assigned relevance score value from search application, if and come to nothing and reach predefined minimum score value, can return to result incoherent sky or the null value score value of instruction from this server system by forward end vision query processing server; And
Annotations module 528, it is for receiving annotation information from annotations database (Fig. 1 116), determining that whether any information of annotation information is relevant to specific search application, and any definite relevant portion of annotation information is integrated with to corresponding annotations database 530.
Fig. 8 is the block diagram that the OCR search system 112-B of vision inquiry is processed in be used to consistent with one embodiment of the present of invention of diagram.OCR search system 112-B typically comprises one or more processing units (CPU) 602, one or more network or other communication interfaces 604, storer 612 and for making one or more communication buss 614 of these assembly interconnects.Storer 612 comprises high-speed random access memory, such as DRAM, SRAM, DDR RAM or other random access solid-state memory device; And can comprise nonvolatile memory, such as one or more disk storage devices, optical disc memory apparatus, flash memory device or other non-volatile solid-state memory devices.Storer 612 can comprise the local one or more memory devices that are positioned at away from CPU602 alternatively.Non-volatile memory devices in storer 612 or as an alternative storer 612 comprises nonvolatile computer-readable recording medium.In certain embodiments, following program, module and data structure or its subset of the computer-readable recording medium storage of storer 612 or storer 612:
Operating system 616, it comprises for the treatment of various basic system services with for carrying out the program of the task of relying on hardware;
Network communication module 618, it is used to via one or more communications network interfaces 604 (wired or wireless) and such as one or more communication networks of the Internet, other wide area networks, LAN (Local Area Network), Metropolitan Area Network (MAN) etc., OCR search system 112-B is connected to other computing machines;
Optical character identification (OCR) module 620, it attempts the text in recognition visible sensation inquiry, and by letter image changed;
Optional OCR database 114-B, it is used to identify specific font, patterns of text and letter is identified to distinctive other features by OCR module 620;
Optional spell check module 622, it passes through the word through conversion for dictionary inspection, and the letter of the latent fault conversion in the word of other coupling dictionary word is replaced, and improves the conversion of letter image to character;
Optional named entity recognition module 624, the named entity of its search in the text of conversion, the word using identified named entity as word in inquiry send to word querying server system (Fig. 1 118) and provide explicitly the result from word querying server system as the named entity that is embedded in the link in the text of OCRization and identify;
Optional text matches application 632, it is by the fragment (such as sentence and paragraph through conversion) through conversion for text fragments database auditing, and the letter of the latent fault conversion in the text fragments of OCRization to other matched text coupling applicating text fragment is replaced, improve the conversion of letter image to character, in certain embodiments, text matches is applied to the text fragments finding and (for example offer user as link, if scanning input a page of the New York Times, text matches application can be provided to the link of the whole article of delivering on New York Times website),
Result rank and formatting module 626, it is for the result through OCRization is formatd for displaying, and the optional link to named entity is formatd, and also alternatively to carrying out rank from any correlated results of text matches application; And
Optional annotations module 628, it is for receiving annotation information, determine that whether any information of annotation information is relevant to OCR search system from annotations database (Fig. 1 116), and any definite relevant portion of annotation information is integrated with to corresponding annotations database 630.
Fig. 9 is the block diagram that the face recognition search system 112-A inquiring about with the vision of at least one face-image is processed in be used to consistent with one embodiment of the present of invention of diagram.Face recognition search system 112-A typically comprises one or more processing units (CPU) 902, one or more network or other communication interfaces 904, storer 912 and for making one or more communication buss 914 of these assembly interconnects.Storer 912 comprises high-speed random access memory, such as DRAM, SRAM, DDR RAM or other random access solid-state memory device; And can comprise nonvolatile memory, such as one or more disk storage devices, optical disc memory apparatus, flash memory device or other non-volatile solid-state memory devices.Storer 912 can comprise the local one or more memory devices that are positioned at away from CPU902 alternatively.Non-volatile memory devices in storer 912 or as an alternative storer 912 comprises nonvolatile computer-readable recording medium.In certain embodiments, following program, module and data structure or its subset of the computer-readable recording medium storage of storer 912 or storer 912:
Operating system 916, it comprises for the treatment of various basic system services with for carrying out the program of the task of relying on hardware;
Network communication module 918, it is used to via one or more communications network interfaces 904 (wired or wireless) and such as one or more communication networks of the Internet, other wide area networks, LAN (Local Area Network), Metropolitan Area Network (MAN) etc., face recognition search system 112-A is connected to other computing machines;
Face recognition search application 920, it comprises: visual identifier module 924, it is for identifying the latent image coupling of face-image of potential matching inquiry; Personal identifier's module 926, it mates for identifying with latent image the individual who is associated; And social networks metric module 928, its for retrieval comprise with the tolerance of requestor's (and/or another person of image) social contiguity specific to individual data; And ranking module 930, it,, for according to the tolerance of the visual similarity between face-image and potential coupling and according to social networks tolerance, generates identified individual ranked list.
Face image data storehouse 114-A, its searched image that finds the face-image in potential matching inquiry; Comprise one or more image sources, share image 936 and previous query image 938 such as social networks image 932, web photograph album image 934, photo.In response to ad hoc inquiry, the image source of use is according to identifying about requestor's data.In certain embodiments, it only comprises the image in the account who belongs to requestor or be associated with requestor, described account such as requestor's social networks account, requestor's web photograph album etc.In other embodiments, described source comprise belong to requestor with it aspect social associated other people or with the described image that other people are associated, described other people for example have the people of direct relation on socialgram with requestor.Alternatively, face image data storehouse 114-A comprises famous person's image 940.In certain embodiments, face image data storehouse comprises the face-image obtaining from external source, and described external source is such as in the legal face-image supplier of PD;
Image characteristics extraction device 942, it extracts the feature that the image from the 114-A of face image data storehouse obtains, and information is stored in the database 964 of individual data.In certain embodiments, extract visual signature with Visual Feature Retrieval Process device 944, such as indoor living environment factor, outdoor living environment factor, sex factor, ethnic factor, glasses factor, facial hair factor, head hair factor, headwear factor, eye color factor, occur information and co-occurrence information.In certain embodiments, extract metadata feature with metadata feature extractor 946, such as data message, temporal information and positional information.
For the public database 948 in the source of the data specific to individual, it is included in the relation tolerance of mating the social contiguity between individual and the requestor who is associated with latent image.Described data are obtained from multiple application, and described multiple application include but not limited to social network data storehouse 922, social microblogging database 950, blog data storehouse 952, email database 954, IM database 956, calendar database 958, contacts list 960 and/or public URL962.
Specific to the database 964 of individual data, its storage is specific to unique individual's information.Specific to partly or entirely obtaining from public database in individual data.Describe in more detail specific to individual data with reference to figure 18A-C.
Result formatting module 966, it is for formaing for displaying result; In certain embodiments, comprise latent image coupling and the information subset from the database 964 of the data specific to individual through the result of format.
Annotations module 968, it is for receiving annotation information, determine that whether any information of annotation information is relevant to face recognition search system from annotations database (Fig. 1 116), and any definite relevant portion of annotation information is stored in corresponding annotations database 970.
Individual position's module 972, it obtains with requestor and is identified as the relevant positional information of one or more people's the current location of the potential coupling to the face-image in vision inquiry.To improve the individual's coupling to face-image with reference to the application 920 use location information of obtaining and search for of figure 16A, 17,18A and 18C discussion individual position module 972 location information below.
Figure 10 is that be used to consistent with one embodiment of the present of invention of diagram processed the image of vision inquiry to the block diagram of word search system 112-C.In certain embodiments, image is to the object (example recognition) in word search system identification vision inquiry.In other embodiments, image is to the object type (type identification) in word search system identification vision inquiry.In certain embodiments, image is to word system identification object and object type.Image is that image in vision inquiry returns to potential word match to word search system.Image typically comprises one or more processing units (CPU) 1002, one or more network or other communication interfaces 1004, storer 1012 and for making one or more communication buss 1014 of these assembly interconnects to word search system 112-C.Storer 1012 comprises high-speed random access memory, such as DRAM, SRAM, DDR RAM or other random access solid-state memory device; And can comprise nonvolatile memory, such as one or more disk storage devices, optical disc memory apparatus, flash memory device or other non-volatile solid-state memory devices.Storer 1012 can comprise the local one or more memory devices that are positioned at away from CPU1002 alternatively.Non-volatile memory devices in storer 1012 or as an alternative storer 1012 comprises nonvolatile computer-readable recording medium.In certain embodiments, following program, module and data structure or its subset of the computer-readable recording medium storage of storer 1012 or storer 1012:
Operating system 1016, it comprises for the treatment of various basic system services with for carrying out the program of the task of relying on hardware;
Network communication module 1018, it is used to via one or more communications network interfaces 1004 (wired or wireless) and such as one or more communication networks of the Internet, other wide area networks, LAN (Local Area Network), Metropolitan Area Network (MAN) etc., image is connected to other computing machines to word search system 112-C;
Image is to word search application 1020, and it searches for the image of the theme in the inquiry of coupling vision in picture search database 114-C;
Picture search database 114-C, it can searched application 1020 searches for to find the image of the theme that is similar to vision inquiry;
Word is to image reversal index 1022, and it stores the text word that user uses in the time carrying out searching image by text based query search engine 1006;
Result rank and formatting module 1024, it is for carrying out rank and/or to the mating with latent image the word being associated and carry out rank to image back index 1022 marks at word to latent image coupling; And
Annotations module 1026, it is for receiving annotation information, determine that whether any information of annotation information is relevant to word search system 112-C to image from annotations database (Fig. 1 116), and any definite relevant portion of annotation information is stored in corresponding annotations database 1028.
Be intended to more using Fig. 5-10 as the functional descriptions of the various features that can exist in one group of computer system, instead of as the structural signal of embodiment described here.In fact, and as of ordinary skill in the art recognize, the item shown in can making to separate is combined and can make some separation.For example, can on individual server, realize some shown in separating in these figure, and can realize single by one or more servers.The actual quantity of the system that is used for realizing vision query processing and between them how assigned characteristics because of the difference of embodiment different.
Each in method described here can be by being stored in the instruction control of carrying out in nonvolatile computer-readable recording medium and by one or more processors of one or more servers or client.Module or the program (, instruction set) of mark needn't be implemented as stand alone software program, routine or module in the above, therefore, in various embodiments, can combine or rearrange in addition each subset of these modules.Each in the operation shown in Fig. 5-10 can be corresponding to the instruction being stored in computer memory or nonvolatile computer-readable recording medium.
Figure 11 illustrates the client 102 with the screenshotss of Exemplary Visual inquiry 1102.Mobile devices in the client 102 shown in Figure 11, such as cell phone, portable music player or portable email equipment.Client 102 comprises display 706 and one or more input media 708, such as at the button shown in this accompanying drawing.In certain embodiments, display 706 is touch-sensitive displays 709.Having in the embodiment of touch-sensitive display 709, the soft key showing on display 709 can substitute part or all of in electromechanical push-button 708 alternatively.As explained in more detail below, touch-sensitive display is also helpful carrying out when mutual with vision Query Result.Client 102 also comprises picture catching mechanism, such as camera 710.
Figure 11 illustrates vision inquiry 1102, and it is photo or the frame of video of the packaging in shop shelf.In the embodiment of inferior description, vision inquiry is the two dimensional image in pixel in each in bidimensional with the resolution corresponding with the size of vision inquiry.Vision inquiry 1102 is in this example two dimensional images of three dimensional object.Vision inquiry 1102 comprises the polytype entity in background element, the packing of product 1104 and packaging, comprises people's image 1106, trademark image 1108, product image 1110 and multiple text element 1112.
As illustrated with reference to figure 3, vision inquiry 1102 is sent to front-end server 110, and it sends to vision inquiry 1102 multiple parallel search systems (112A-N), reception result and create interactive result document.
Each illustrates the client 102 with the screenshotss of the embodiment of interactive result document 1200 Figure 12 A and 12B.Interactive result document 1200 comprises one or more visual identifier 1202 of the corresponding subdivision of vision inquiry 1102, its each be included in the at user option link of Search Results subset.Figure 12 A and 12B illustrate the interactive result document 1200 of the visual identifier of being with promising bounding box 1202 (for example, bounding box 1202-1,1202-2,1202-3).In the embodiment shown in Figure 12 A and 12B, user activates the demonstration to the Search Results corresponding with this specific subdivision by the active region touching in the space of sketching the contours of at the bounding box 1202 by specific subdivision.For example, user will activate the Search Results corresponding with this people's image by touching around the bounding box 1306 (Figure 13) of people's image.In other embodiments, select selectable link with mouse or keyboard instead of touch-sensitive display.In certain embodiments, in the time of user's preview bounding box 1202 when touching once or pointer being hovered on bounding box (, when user click), show the Search Results of the first correspondence.In the time that user selects bounding box (, in the time that user double-clicks, touches twice or indicates selection by another mechanism), user activates the demonstration of the Search Results to multiple correspondences.
In Figure 12 A and 12B, visual identifier is the bounding box 1202 around the subdivision of vision inquiry.Figure 12 A illustrates the bounding box 1202 into square or rectangular.Figure 12 B illustrates the bounding box 1202 on the border of the entity identifying in the subdivision of sketching the contours of vision inquiry, such as the bounding box 1202-3 for beverage bottle.In certain embodiments, each bounding box 1202 comprises less bounding box 1202 therein.For example, in Figure 12 A and 12B, the bounding box 1202-1 of mark packaging is around bounding box 1202-2 and the every other bounding box 1202 of mark trade mark.In some embodiment that comprise text, also comprise the movable hot link 1204 for the part of text word.Figure 12 B shows example, and wherein " Active Drink " and " United States " is shown as hot link 1204.The Search Results corresponding with these words is the result receiving from word querying server system 118, and the result corresponding with bounding box is from by the result of image querying search system.
Figure 13 illustrates the client 102 with the screenshotss of the interactive result document 1200 of the type coding of the entity of identifying by vision inquiry.The vision inquiry packet of Figure 11 is containing people's image 1106, trademark image 1108, product image 1110 and multiple text element 1112.So, the interactive result document 1200 showing in Figure 13 comprises the bounding box 1202 around people 1306, label 1308, product 1310 and two text filed 1312.Each shows the bounding box of Figure 13 with different cross hatches, and it represents the transparent bounding box 1202 of different colours.In certain embodiments, the visual identifier of bounding box (and/or label or other visual identifier) in interactive result document 1200 is formatd in visually different modes and shown, such as overlapping color, overlapping pattern, label background color, label background patterns, label font color and bounding box border color.Show the type coding for the entity of specific identification with reference to the bounding box in Figure 13, but also can be applied to the visual identifier into label by encoding by type.
Figure 14 illustrates the client device 102 having with the screenshotss of the interactive result document 1200 of label 1402, and label 1402 is visual identifier of the corresponding subdivision of the vision inquiry 1102 of Figure 11.Each is included in the at user option link of the subset of corresponding Search Results label visual identifier 1402.In certain embodiments, identify selectable link by descriptive text shown in the region of label 1402.Some embodiment are included in the multiple links in a label 1402.For example, in Figure 14, hovering over label on the woman's who drinks water image is included in about the link of this woman's face recognition result with for example, to the link of the image recognition result (, using other products of identical picture or the image of advertisement) about this particular picture.
In Figure 14, label 1402 is shown as the region with the partially transparent of text, and it is positioned in its corresponding subdivision of interactive result document.In other embodiments, respective labels is placed near not still being positioned in its corresponding subdivision of interactive result document.In certain embodiments, with by type label is encoded with reference to the identical mode that Figure 13 was discussed.In certain embodiments, user by touch active region in the edge by label 1302 or peripheral space of sketching the contours of activate to demonstration corresponding to the corresponding Search Results of the specific subdivision of label 1302.Identical preview and the selection function discussed with reference to the bounding box of figure 12A and 12B in the above are also applicable to the visual identifier into label 1402.
Figure 15 illustrates the screenshotss that interactive result document 1200 and the inquiry 1102 of original vision and the results list 1500 show simultaneously.In certain embodiments, as shown in Figure 12-14, interactive result document 1200 shows alone.In other embodiments, as shown in Figure 15, interactive result document 1200 shows with original vision inquiry simultaneously.In certain embodiments, vision Query Result list 1500 shows together with original vision inquiry 1102 and/or interactive result document 1200 simultaneously.The type of client and the amount of space on display 706 can determine whether the results list 1500 shows with interactive result document 1200 simultaneously.In certain embodiments, client 102 (in response to the vision inquiry of submitting to vision querying server system) reception result list 1500 and interactive result document 1200 both, but when below user is rolled to interactive result document 1200, only show the results list 1500.In in these embodiments some, client 102 shows the result corresponding with the visual identifier 1202/1402 of user's selection in the situation that of querying server again, because the results list 1500 is received in response to vision inquiry by client 102, be then stored locally on client 102 places.
In certain embodiments, the results list 1500 is organized into classification 1502.Each classification comprises at least one result 1503.In certain embodiments, make classification title highlighted so that itself and result 1503 are distinguished.Classification 1502 sorts according to the classification weight of its calculating.In certain embodiments, classification weight is the combination of the weight of the highest N result in this classification.So, first show and may produce the more classification of correlated results.At the entity for same identified, to return in the embodiment of more than one classification 1502 (all face-image identification coupling and images match as shown in Figure 15), the classification first showing has higher classification weight.
As illustrated with reference to figure 3, in certain embodiments, in the time that the selectable link in interactive result document 1200 is selected by the user of client 102, cursor is by the first result 1503 being automatically moved in suitable classification 1502 or this classification.As an alternative, in the time that the selectable link in interactive result document is selected by the user of client 102, the results list 1500 is resequenced, make first to show to selected to link relevant classification.This is for example by making selectable concatenated coding have the information of Search Results corresponding to identification, or indicates corresponding selectable link or indicate corresponding result classification to complete by Search Results is encoded.
In certain embodiments, the classification of Search Results is pressed image querying search system corresponding to what produce those Search Results.For example, in Figure 15, the part in classification is product coupling 1506, tag match 1508, face recognition coupling 1510, images match 1512.Original vision inquiry 1102 and/or interactive result document 1200 can be used similarly such as the classification title of inquiry 1504 and show.Similarly, can also be by being shown as independent classification from the result of the performed any word search of word querying server, such as web result 1514.In other embodiments, the more than one entity in vision inquiry will bear results by image querying search system from same.For example, vision inquiry can comprise two different faces, and it will return to Different Results from face recognition search system 112-A.So, in certain embodiments, classification 1502 is divided by identified entity instead of search system.In certain embodiments, in identified entity class head 1502, show the image of the entity of identifying, making about the result of this entity of identifying is differentiable with the result of the entity of identifying about another, even if both results are produced by image querying search system by same.For example, in Figure 15, product coupling classification 1506 comprises two entity products entities and same two entity classes 1502---box-packed product 1516 and bottled product 1518, and wherein each has the Search Results 1503 of multiple correspondences.In certain embodiments, classification can be divided by identified entity with by the type of image query systems.For example, in Figure 15, under product coupling classification product, there are two different entities having returned to correlated results.
In certain embodiments, 1503 comprise thumbnail image as a result.For example, as in Figure 15 about as shown in face recognition matching result, with the less version (also referred to as thumbnail image) having shown together with some textual descriptions of the name of the people such as in image about the picture of the facial match of " Actress X (actress X) " and " Social Network Friend Y (social networks friend Y) ".
Figure 16 A-16B is that diagram is consistent with some embodiment comprising that the vision of face-image inquires about the process flow diagram of the process responding.Each in operation shown in these figure can be corresponding to the instruction being stored in computer memory or nonvolatile computer-readable recording medium.Face recognition search system 112-A receives and wherein has the vision of one or more face-images inquiry (1602) from requestor.In certain embodiments, vision inquiry packet is determined by front-end vision query processing server 110 containing the fact of at least one face.In other words,, in the time that face recognition search system 112-A processes vision inquiry, at least a portion of vision query image has been confirmed as comprising potential face.In some cases, vision inquiry packet is containing multiple faces, such as two or more friends' picture or several people's group picture.In vision inquiry packet, containing under the certain situation of multiple face-images, requestor may be only interested in one in face.So, in certain embodiments, in the time that vision inquiry comprises at least corresponding face-image and the second face-image, before mark latent image coupling, system receives the selection to corresponding face-image from requestor.For example, in certain embodiments, each potential face of system banner, and which or which the face request of wishing to identify in inquiry about requestor is confirmed.
Identify the image (1604) of the corresponding face-image of potential coupling.These images are called as latent image coupling.Latent image coupling identifies according to visual similarity standard.And, mate (1606) from the one or more image source mark latent images according to the Data Identification about requestor.In certain embodiments, obtain the data about requestor from requestor's profile information.In certain embodiments, directly obtain requestor's profile information from requestor.As an alternative or additionally, receive requestor's profile information from social networks.Latent image coupling comprises by tagged image, comprises the image for the people's of image personal identifier.In certain embodiments, one or more image sources comprise the image from requestor's social network data storehouse, web photograph album, photo shared data bank and other image sources of being associated with requestor.In addition, in certain embodiments, the database of famous person's image (Fig. 9 940) is also included in the image source of search latent image coupling.In certain embodiments, the image source of search latent image coupling also comprises the image from requestor's friend or contact person's social network data storehouse, web photograph album, photo shared data bank and other image sources of being associated with requestor.In the embodiment comprising from the image of requestor's friend or contact person's database, make to be included the determining of which database.For example, in certain embodiments, comprise the pre-friend of maximum quantity of determining or contact person's database.In other embodiments, only comprise directly social networks friend's database.
Then, mark mates with latent image the one or more people (1608) that are associated.In certain embodiments, the one or more people of personal identifier's tag identifier from being associated with identified images match.For example, system can identify Bob Smith, Qiao Qiongsi and Peter Johnson and mate with the latent image of inquiry of the image that comprises the friend male sex people who is associated, because these three people are tagged in other images that are associated with requestor, and these three people are visually similar to the face-image in inquiry.
The individual who identifies for each, retrieval is specific to individual data, and it comprises the social networks tolerance (1610) of obtaining from multiple application.Multiple application comprise communications applications, social networks application, calendar application and collaboration applications (1612).For example, application can comprise such as following application: Facebook, Twitter, Buzz, G-mail (Email and IM), web calendar, the blog such as " LiveJournal ", individual public URL and any contacts list associated with it.In certain embodiments, the acquisition of information data that only " the disclosing " from these application issued.In other embodiments, if data belong to requestor or clearly and requestor share, obtain these data.In certain embodiments, comprise individual name, address, occupation, group membership, interest, age, local, individual's statistics and the job information (as discussed more in detail with reference to figure 18A) of respective identification specific to individual data.In certain embodiments, this information of one or more collections from above-mentioned application.
Comprise social networks tolerance specific to individual data, it is the tolerance (1614) of the social contiguity between individual and the requestor of respective identification.In certain embodiments, social contiguity tolerance is included in the tolerance of the social contiguity on one or more in above-mentioned application.For example, social contiguity tolerance can be considered following one or more: whether the individual of respective identification and requestor are whether the Email that exchanges of the individual of friend, requestor and respective identification and/or the quantity (if any) of IM message and the individual of requestor and respective identification pay close attention to mutual social microblogging model etc. in social networking website.
In certain embodiments, also comprise specific to individual data the feature (1616) obtaining from other images of this corresponding individual about the individual of respective identification.In certain embodiments, these features comprise the metadata information from image, such as date and time information, temporal information and positional information.In other embodiments, the feature obtaining from other images of corresponding individual comprises sense of vision factor, such as indoor living environment factor, outdoor living environment factor, sex factor, ethnic factor, glasses factor, facial hair factor, head hair factor, headwear factor and eye color factor.In other embodiment again, the feature obtaining from other images of corresponding individual comprises with the appearance of corresponding individual one or more image sources measures relevant appearance information and/or the information about corresponding individual co-occurrence amount in the image from one or more image sources together with the second people.
Alternatively, in certain embodiments, individual position's module 972 (Fig. 9) is obtained (1618) requestor's current location information and the individual current location information of respective identification.For example, can or obtain the individual current location of requestor or respective identification from the position (such as " I am at present on a Bostonian meeting ") of individual issue from the IP address of the bench device that is arranged in the gps receiver of mobile device, use from individual, from individual home address or work address.
Then, by according to one or more tolerance of the visual similarity between corresponding face-image and latent image coupling and also according to comprising that at least the ranking information of social networks tolerance carries out rank to one or more identified individuals, generate individual sorted lists (1620).To discuss in more detail with reference to Figure 17 these and other of the rank factor that affects below.
As shown in Figure 16 B, process continues.Alternatively, check and select to add list, and whether be releasable make definite (1622) about one or more personal identifiers to requestor.In certain embodiments, when the image of potential coupling is during from source except self account of requestor, or in the time that self account of requestor does not comprise the individual tagged image of respective identification, carry out this inspection.
Then, will send to requestor (1624) from least one personal identifier of sorted lists, thereby identify one or more people.In certain embodiments, personal identifier is name.In other embodiments, personal identifier is address, e-mail address, the pet name etc.In certain embodiments, by representational picture, such as the individual image identifying of profile picture, the inquiry of optimum matching vision, send together with personal identifier.In such embodiments, in the time that more than one people is identified as potential coupling, the individual representative picture that each is identified with together with the response of image querying, send.In certain embodiments, also by other information, send together with personal identifier such as the extracts of contact details or the recent model of issuing.In other embodiments, except personal identifier, also return to the relation finding between the individual in requestor and image.For example, tall Smith's ranking result may comprise statement " tall Smith is listed as contact person in your account more than one " or " you and tall Smith are the members of Palo Alto tennis club " or " you and tall Smith make friends with card human relations Jones ".Further information, such as individual contact details, group be subordinate to, the name of people in the middle of individual according to socialgram in the image of requestor and coupling, can be included in the result that returns to requestor.The information of the amplification of showing to requestor in certain embodiments, clearly or is impliedly indicated (for example,, by the type of the parameter in the Configuration Values in its profile or vision inquiry or vision inquiry) by requestor.In certain embodiments, in the time that more than one personal identifier is sent to requestor, for the more information providing than the individual of the mark for lower rank is provided for the individual of the mark of high rank.
In certain embodiments, also the copy of the vision inquiry query portion of corresponding face-image (or have) is sent to (1626) together with one or more personal identifiers.When more than one face-image is in the inquiry of original vision and when one or more face-image is clearly identified, in certain embodiments, also the copy of vision inquiry is sent to one or more in the people who identifies in vision inquiry.Therefore, if taken group picture, and multiple people wants its copy, and requestor needn't find their contact details, and sends by hand the copy of photo to them.In certain embodiments, first requestor verifies that copy should be sent to one or more in identified people before must one or more transmission the in the people to identified.
In certain embodiments, receive the selection (1628) to personal identifier from requestor.Then,, in response to selection, send the data (1630) corresponding with selected personal identifier to requestor.In certain embodiments, these data comprise the one or more images that are associated with personal identifier, the contact details that are associated with personal identifier, the public profiles information that is associated with personal identifier etc.In certain embodiments, give requestor's option with by the contacts list that is partly or entirely stored in requestor of this information or for the individual update request person's that identified associated person information.In certain embodiments, this information and requestor's vision inquiry is carried out associated, maybe query portion and the contact list information with the face-image corresponding with personal identifier are stored together.
In addition, in certain embodiments, the face-image of vision inquiry is stored as to the corresponding individual's corresponding with selected personal identifier other image (1632).In certain embodiments, this image is stored in the previous query portion (Fig. 9 938) of image source.In certain embodiments, give chance that requestor annotates image to comprise other data.In the situation that requestor inputs annotation data, face recognition search system 112-A receives and stores (1634) this annotation data.Annotations module (Fig. 9 968) is accepted annotation to improve following face recognition search.For example, if the name of user's employment annotates this people's picture, this picture can be used for identifying this people in following face recognition inquiry.In certain embodiments, because privacy reason, the picture of individual's other annotation can be made for the face-recognition procedure that increases by face recognition search system 112-A, but does not return to anyone except original requestor as image result.In certain embodiments, only allow the actual individual of mark in vision inquiry to make image open (or can obtain) concerning the people except requestor.In certain embodiments, once individual is clearly identified, just send following request to this people: inquire that its future Query that whether is the people in its social networks by permission image is returned as result.
In certain embodiments, in step 1604, can retrieve same people's more than one image.Once retrieve the image of potential coupling and determine that described image is same people, it can be by noticing that described image all has identical individual ID, same or analogous specific to individual data (name, address etc.) or have same or analogous social networks and complete, just described image and identical data are carried out associatedly, and for the remainder for the treatment of step, described image is treated as individual unit.Alternatively, if in step 1624, two or more images are returned with identical personal identifier, in response to image querying, return to the more than one image retrieving about this identical personal identifier.
Figure 17 is the factor that uses while being shown in the individual sorted lists of the face-image that generates in the inquiry of potential coupling vision and the process flow diagram of feature.This process flow diagram provides the more information about above-mentioned steps 1620.
In certain embodiments, in the time determining rank score value according to social network relationships tolerance for the corresponding individual in individual sorted lists, use various factors (1702).In certain embodiments, determine the traffic between corresponding individual and requestor in one or more communications applications, then determine corresponding individual's rank score value, wherein the factor in the time determining rank score value for corresponding individual is the definite traffic (1704) between corresponding individual and requestor in one or more communications applications.Communications applications can comprise social networks application, social microblogging, e-mail applications and/or instant message application.For example, for example, if corresponding individual (has carried out mass communication by one or more communications applications and requestor, by the mass communication of Email and social networks model), requestor may be very familiar to this corresponding individual, and therefore the face-image in vision inquiry may be more this corresponding individual.In certain embodiments, while only working as the traffic for example, higher than pre-definite threshold value (, the number percent of the number of communications of setting, number of communications in a certain amount of time or total communication), just use this factor.In certain embodiments, face recognition search system 112-A determines whether the traffic between corresponding individual and requestor exceedes threshold value in one or more communications applications, and factor in the time determining rank score value for corresponding individual is that whether the traffic between corresponding individual and requestor exceedes determining of threshold value in one or more communications applications.
In certain embodiments, make definite that whether requestor and corresponding individual contacted directly in corresponding social networks application, then determine this corresponding individual's rank score value, wherein the factor in the time determining rank score value for this corresponding individual is definite (1706) whether requestor and corresponding individual contacted directly in corresponding social networks application.For example, if requestor and corresponding personal accomplishment friend contact directly, requestor may be very familiar to this corresponding individual, and therefore, the face-image in vision inquiry may be more this corresponding individual.
Comprise multiple features in corresponding individual's the data specific to individual, described multiple feature is such as following two or more: this corresponding individual's name, address, occupation, group membership, interest, age, local, individual's statistics and/or job information, also, for requestor retrieves identical information, reach such degree: such information can obtain concerning face recognition search system 112-A.Then, according to requestor specific to the individual data degree similar to the individual data specific to individual of respective identification, determine one or more individual similarity measurements.Determine the individual rank score value of respective identification, wherein the one or more factors in the time determining rank score value for the individual of respective identification are one or more individual similarity measurements (1708).For example, if requestor is similar age, similar occupation with corresponding individual, and be the member of similar group, it may be more friend, therefore the face-image in vision inquiry may be more this corresponding individual.
In the case of individual's the current location information that successfully obtains requestor and mark, determine the individual rank score value of respective identification, wherein the factor in the time determining rank score value for the individual of respective identification is whether requestor's current location information mates the individual current location information (1710) of respective identification.For example, in the time that requestor and corresponding individual are all determined to be in same position, the face-image that this proximity has increased in vision inquiry is corresponding individual's possibility.Even more like this, in the time that requestor and corresponding individual are determined not in same position, the face-image that the shortage of proximity has greatly reduced in vision inquiry is corresponding individual's possibility.In addition, in certain embodiments, history or the daily record of retrieval request person and the individual's who identifies position, and for mating, it is compared mutually.In certain embodiments, requestor and the individual position log identifying and position (and/or date and time) feature obtaining from query image self are further compared.For example, if inquiring position information indicating image was taken in Santa Cruz, California July 2, and the daily record of requestor and the individual's that identifies position also indicate its July 2 in Santa Cruz, California, to have increased the face-image in vision inquiry be this corresponding individual's possibility to this location matches.
The embodiment of the feature (its refer step 1616 is discussed) obtaining from other images of this corresponding individual in also comprising specific to individual data of corresponding individual, rank is further according to the similarity (1712) between the feature obtaining in received inquiry with from other images of corresponding individual.In the time determining rank score value for corresponding individual, use various factors, it is according to these features (1714) that obtain from other images of corresponding individual.
In certain embodiments, the feature obtaining from other images of corresponding individual comprises picture catching date (for example, Zhou Tian, day or the moon and/or long date fomat) and temporal information.Then the degree that, has the picture catching date and time information similar to the date and time information of one or more other images of corresponding individual according to received inquiry is determined one or more similarity measurements.Determine corresponding individual's rank score value, wherein the one or more factors in the time determining rank score value for corresponding individual are one or more similarity measurements (1716).In certain embodiments, similarity measurement is Boolean (for example, Yes/No or 1/0).In other embodiments, similarity measurement be Boolean vector (for example, phase same date Yes/No, in 1 hour Yes/No, in 5 hours Yes/No etc.).It can be the numerical value (for example,, between 0 and 1) of measuring similarity.In certain embodiments, for each other image of corresponding individual are determined similarity measurement, and in certain embodiments, determine the class value of all images of corresponding individual.In certain embodiments, another feature obtaining from image is place/positional information, and it can be as other or alternative similarity measurement as mentioned above.For example, if vision inquiry has and date, time and/or the positional information of one or more other image similarities, to have increased the face-image in vision inquiry be the possibility of the corresponding individual in one or more other images with similar date, time and/or positional information to this similarity.
In certain embodiments, the feature obtaining from other images of corresponding individual comprises that the appearance of individual corresponding to this image from one or more image sources measure relevant appearance information.In in these embodiments some, the factor in the time determining rank score value for corresponding individual is the appearance information (1718) about this corresponding individual.For example, if multiple other images comprise corresponding individual, requestor may be very familiar to this corresponding individual, and its face-image that has increased in vision inquiry is this corresponding individual's possibility.
In certain embodiments, the feature obtaining from other images of corresponding individual comprises sense of vision factor, and it comprises following one or more: indoor living environment factor, outdoor living environment factor, sex factor, ethnic factor, glasses factor, facial hair factor, head hair factor, headwear factor, clothes element and eye color factor.In in these embodiments some, the one or more factors in the time determining rank score value for corresponding individual comprise the sense of vision factor (1720) about this corresponding individual.
In some cases, vision inquiry comprises multiple face-images.In the time that more than one face-image is in vision inquiry, between it to connect each other in correct mark can be helpful at that time.For example, if it has stronger social networks tolerance or occurs together in other images, those facts have increased its also common possibility in query image.In certain embodiments, vision inquiry comprises at least corresponding face-image and the second face-image.Mark is according to the image of potential coupling the second face-image of visual similarity standard (being referred to herein as potential the second images match).Potential the second images match is the image about one or more image sources of requestor's Data Identification from basis.Then the second people that, mark is associated with potential the second images match.For this definite object, suppose to identify the second people with height determinacy.For each individual who identifies of the potential coupling as to corresponding face-image, from multiple application obtain comprise with the second social networks tolerance of the second people's social contiguity specific to individual data.Then, generate individual sorted lists by further one or more identified individuals being carried out to rank according to the ranking information that comprises at least the second social networks tolerance.So, corresponding individual's rank further according to comprise with inquiry in the second people's the second social networks tolerance (1722) of tolerance of social contiguity.In other words, in certain embodiments, in the time generating individual sorted lists, use with requestor's social networks and with the second people's social networks.
In other embodiments, the second people be identified as between everyone of potential coupling, one or more in other factors of discussing are compared to find optimum matching in the above.For example, if the second people is hired by same company with corresponding individual, appearance or mass communication mutually in other images with similar date/time information, in the time correctly identifying them, can use these factors.In another example, the feature obtaining from other images of corresponding individual comprises the information relevant with the co-occurrence amount of the second people the image from one or more image sources with corresponding individual; And in the time determining corresponding individual's rank score value, the factor in the time determining rank score value for this corresponding individual is this individual and the co-occurrence amount (1724) of the second people in the image from one or more image sources.
Figure 18 A is the block diagram of a part for the data structure of the face image data storehouse 114-A that utilizes of diagram face recognition search system 112-A.In certain embodiments, face image data storehouse comprises the individual one or more images 1802 from obtaining according to the one or more image sources that identify about requestor's data.In certain embodiments, face image data storehouse 114-A also comprises unique ID1804 or the personal identifier for this individual.Other information about this individual and personal identifier 1804 are carried out associated, and be stored in the database 964 of individual data.Then, in the time determining potential coupling for the face-image in vision inquiry, use the part or all of of this other information.For example, by according to the tolerance of requestor's social contiguity, such as the group membership 1812 of coupling or stronger social networks 1814, individual is carried out to rank and generates with latent image and mate the individual sorted lists identifying being associated.In the time determining the individual sorted lists identifying, except visually similar to face-image in vision inquiry latent image, use the data from the database 964 of the data specific to individual.In can include but not limited to specific to the database 964 of individual data that unique ID1804 identifies individual following any: name 1806, address 1808, occupation 1810, group membership 1812, social network relationships 1814 (being described in more detail with reference to figure 18B), current location 1816, share preference 1818, interest 1820, age 1822, local 1824, individually add up 1826, job information 1828.This information is from obtaining such as multiple application of communications applications, social networks application, calendar application and collaboration applications.As discussed with reference to figure 18C, in certain embodiments, also comprise the feature 1830 obtaining from one or more images of individual specific to individual data.
The example of Figure 18 B diagram social network relationships 1814.In certain embodiments, the individual data specific to individual that identify comprise and the social networks tolerance of requestor's's (being identified as inquiry in Figure 18 B) social contiguity, and it obtains from multiple application.Line between people represents one or more (such as by the relation of Email, instant message and social networking website) in its mutual social networks in the figure.In certain embodiments, by the factor being used as in two person-to-person sociodistances in the time determining rank score value for latent image coupling.For example, if a potential matching image is the image of individual C, and another potential matching image image that is individual Y, in certain embodiments, the potential matching image of individual C will receive the social contiguity rank factor (when at calculated for rank score value use) higher than individual Y, because ignore every other factor, compared with the picture of taking someone (the individual Y) that leave three social networks " distance jumps " with requestor, more likely, requestor takes the picture of someone (the individual C) that contact directly with this requestor.Similarly, individual W will receive the social contiguity rank factor higher than individual A, because individual W leaves two social networks of requestor " distance jumps ", and individual A leaves three social networks of requestor " distance jumps ".In certain embodiments, also determine that with requestor's social network relationships to inquire about which image source in response to requestor's vision to be searched.For example, in certain embodiments, image in the account who belongs to the people with direct social network relationships is included in the image source of image of the face-image in the inquiry of search coupling vision, and does not have image in the individual account of direct social network relationships to be included in search with requestor mate in the image source of image of the face-image of vision in inquiring about by belonging to.
For some vision inquiries, use other information from the database 964 specific to individual data of Figure 18 A in conjunction with the distance on the social network relationships figure of Figure 18 B or " distance jumps ".For example, if requestor and corresponding individual live very closely each other, if they are in same industry work, in same social networks " group ", if and both there is the current mobile device of locating at same position (as measured by the gps receiver in its mobile device for example), even if this corresponding individual leaves requestor's several " distance jumps " on social network relationships figure, this corresponding individual's rank score value still may be very high.In another example, if the corresponding individual in potential matching image leaves only one of requestor " distance jumps " on social network relationships figure, even if there is the weak relation (member who is all larger group membership such as two people who determines by the database 964 of the data specific to individual, as shared religion or political party), this corresponding individual may be very high by rank.
In certain embodiments, requestor can be recently more important from other information of the database 964 specific to individual data by some message identification of the database of the data from specific to individual 964.For example, requestor may specify give the relevant information of the industry of working therein with individual than other specific to the higher weight of individual data, the relevant kickup because requestor is just working, therefore query image may be included in other people face-image of the industry work identical with requestor.In another example, requestor may specify give the information relevant with the age than other specific to the higher weight of individual data, because requestor is just submitting to from the query image that is all or mainly the party (or other kickups) participated in of people of the same age.
Figure 18 C is the block diagram of the feature 1830 that obtains of some images of diagram, and the feature 1830 that this image obtains obtains from everyone image being associated with requestor.In certain embodiments, the feature (obtaining from least one image of individual) these being obtained is stored in database according to personal identifier.These features that obtain comprise following one or more (and typically following two or more): indoor living environment factor 1832, outdoor living environment factor 1834, sex factor 1836, ethnic factor 1838, glasses factor 1840, facial hair factor 1842, head hair factor 1844, headwear factor 1846, clothes factor 1847, eye color factor 1848 and measure relevant appearance information 1850 with the appearance of corresponding individual in one or more image sources, and the information 1852 relevant with corresponding individual co-occurrence amount in the image from one or more image sources together with various other people.In certain embodiments, the feature obtaining also comprises the metadata information from image, such as date and time information 1854, temporal information 1856 and the positional information 1858 of each image.Each feature obtaining 1830 that gives to obtain from other images of corresponding individual is worth and weight, and it is to use during corresponding individual determines rank score value in the time of the feature that this obtains when use.
For illustration purpose, with reference to specific embodiment, description is above described.But, superincumbent illustrative discussion be not intended to be limit or limit the present invention to disclosed precise forms.According to instruction above, many amendments and distortion are possible.For best illustration principle of the present invention and its practical application, select and described embodiment, thereby make those skilled in the art can be to be suitable for various amendment optimum utilization the present invention and the various embodiment of special-purpose of expection.
Claims (16)
1. processing comprises a method for the vision inquiry of face-image, and described method comprises:
Obtain the query image that is submitted to the search system based on image by request user;
Obtain and be identified as the one or more images that mate with described query image;
Mark and the multiple people that are identified as at least one the image correlation connection in the described one or more images that mate with described query image;
For with described one or more images in everyone of at least one image correlation connection, obtain the score value of the social contiguity degree between this people of reflection and described request user;
Based on described multiple people's corresponding score value, from described multiple people, select one or more people; And
The respective identifier of selected described one or more people in described multiple people is offered to described request user.
2. method according to claim 1, further comprises:
At least a portion of determining described query image comprises face.
3. method according to claim 1, further comprises:
Determine that described query image comprises multiple faces; And
Receive the selection to a face described multiple faces from described request user,
Wherein obtain and be identified as the described one or more images that mate with described query image and further comprise based on selected face and obtain described one or more image.
4. method according to claim 1, further comprises:
The user profile that mark is associated with described request user; And
Based on the identified user profile being associated with described request user, identify one or more image sources,
Wherein obtain and be identified as the described one or more images that mate with described query image and further comprise from described one or more image sources and obtain described one or more image.
5. method according to claim 4, the user profile that wherein mark is associated with described request user further comprises the one or more social profile that mark is associated with described request user.
6. method according to claim 2, wherein identifies with the multiple people that are identified as at least one the image correlation connection in the described one or more images that mate with described query image and further comprises:
The one or more appended drawings pictures that comprise one or more additional faces that mark is associated with described request user;
Described one or more additional faces in the described face of described query image and described one or more appended drawings picture are compared;
Based on described comparison, determine that at least one face in the described one or more additional faces in described face and described one or more appended drawings picture of described query image matches; And
Identify the attach identifier of described at least one face in the described one or more additional faces in described one or more appended drawings picture.
7. method according to claim 1, further comprises:
For everyone in selected described one or more people, the mark social networks profile that is associated with this people; And
For everyone in selected described one or more people, a part for the social networks profile being associated with this people is offered to described request user.
8. method according to claim 7, further comprises:
Based on the score value being associated with everyone in selected described one or more people, this people is carried out to rank,
Wherein, for everyone in selected described one or more people, a part for the social networks profile being associated with this people being offered to described request user further comprises: for everyone in selected described one or more people, a described part for the social networks profile being associated with this people is offered described request user by the rank based on this people.
9. method according to claim 8, wherein for everyone in selected described one or more people, rank based on this people offers described request user by a described part for the social networks profile being associated with this people and further comprises: based on everyone rank in selected described one or more people is risen, increase this people's who is provided to described request user the content quantity of social networks profile.
10. method according to claim 1, further comprises:
For everyone in selected described one or more people, the social networks profile information that mark is associated with this people;
For everyone in selected described one or more people, the social networks profile information being associated by the social networks profile information being associated with this people with described request user compares;
For everyone in selected described one or more people, based on described comparison, mark common social networks profile information for this people and described request user; And
For everyone in selected described one or more people, provide the identified described common social networks profile information being associated with this people and described request user.
11. methods according to claim 1, further comprise: described query image is offered to everyone in selected described one or more people.
12. 1 kinds of processing comprise the system of the vision inquiry of face-image, and described system comprises:
For obtaining the device that is submitted to the query image of the search system based on image by request user;
For obtaining the device that is identified as the one or more images that mate with described query image;
For identifying the multiple people's that join with at least one image correlation that is identified as the described one or more images that mate with described query image device;
For for everyone of at least one image correlation connection of described one or more images, obtain the device of the score value of the social contiguity degree between this people of reflection and described request user;
For the corresponding score value based on described multiple people, from described multiple people, select one or more people's device; And
For selected described one or more people's of described multiple people respective identifier being offered to described request user's device.
13. systems according to claim 12, further comprise:
For determining that at least a portion of described query image comprises the device of face.
14. systems according to claim 12, further comprise:
For determining that described query image comprises the device of multiple faces; And
For receive the device of the selection to described multiple faces face from described request user,
The described device that is wherein identified as the described one or more images that mate with described query image for obtaining further comprises the device for obtain described one or more images based on selected face.
15. systems according to claim 12, further comprise:
For identifying the device of the user profile being associated with described request user; And
For the user profile being associated with described request user based on identified, identify the device of one or more image sources,
The described device that is wherein identified as the described one or more images that mate with described query image for obtaining further comprises the device for obtain described one or more images from described one or more image sources.
16. systems according to claim 15, wherein further comprise the device for identifying the one or more social profiles that are associated with described request user for the described device that identifies the user profile being associated with described request user.
Applications Claiming Priority (7)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US23239709P | 2009-08-07 | 2009-08-07 | |
US61/232,397 | 2009-08-07 | ||
US37078410P | 2010-08-04 | 2010-08-04 | |
US61/370,784 | 2010-08-04 | ||
US12/851,473 | 2010-08-05 | ||
US12/851,473 US8670597B2 (en) | 2009-08-07 | 2010-08-05 | Facial recognition with social network aiding |
CN2010800451932A CN102667763A (en) | 2009-08-07 | 2010-08-06 | Facial recognition with social network aiding |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN2010800451932A Division CN102667763A (en) | 2009-08-07 | 2010-08-06 | Facial recognition with social network aiding |
Publications (2)
Publication Number | Publication Date |
---|---|
CN104021150A true CN104021150A (en) | 2014-09-03 |
CN104021150B CN104021150B (en) | 2017-12-19 |
Family
ID=42964240
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN2010800451932A Pending CN102667763A (en) | 2009-08-07 | 2010-08-06 | Facial recognition with social network aiding |
CN201410211070.1A Active CN104021150B (en) | 2009-08-07 | 2010-08-06 | Face recognition with social networks auxiliary |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN2010800451932A Pending CN102667763A (en) | 2009-08-07 | 2010-08-06 | Facial recognition with social network aiding |
Country Status (9)
Country | Link |
---|---|
US (4) | US8670597B2 (en) |
EP (1) | EP2462522A1 (en) |
JP (4) | JP5557911B2 (en) |
KR (3) | KR101760855B1 (en) |
CN (2) | CN102667763A (en) |
AU (1) | AU2010279248B2 (en) |
BR (1) | BR112012002823B1 (en) |
CA (1) | CA2770239C (en) |
WO (1) | WO2011017653A1 (en) |
Cited By (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN105069083A (en) * | 2015-07-31 | 2015-11-18 | 小米科技有限责任公司 | Determination method and device of associated user |
CN105787023A (en) * | 2016-02-24 | 2016-07-20 | 北京橙鑫数据科技有限公司 | Multi-media file release method and device |
CN109388722A (en) * | 2018-09-30 | 2019-02-26 | 上海碳蓝网络科技有限公司 | It is a kind of for adding or searching the method and apparatus of social connections people |
CN111506825A (en) * | 2020-03-12 | 2020-08-07 | 浙江工业大学 | Visual analysis method for character relationship based on social photos |
CN112270297A (en) * | 2020-11-13 | 2021-01-26 | 杭州睿琪软件有限公司 | Method and computer system for displaying recognition result |
Families Citing this family (285)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7801864B2 (en) | 2005-11-28 | 2010-09-21 | Commvault Systems, Inc. | Systems and methods for using metadata to enhance data identification operations |
US20200257596A1 (en) | 2005-12-19 | 2020-08-13 | Commvault Systems, Inc. | Systems and methods of unified reconstruction in storage systems |
US8106856B2 (en) | 2006-09-06 | 2012-01-31 | Apple Inc. | Portable electronic device for photo management |
KR20090102827A (en) * | 2007-01-23 | 2009-09-30 | 조스텐즈 인코포레이팃드 | Method and system for creating customized output |
WO2009116049A2 (en) | 2008-03-20 | 2009-09-24 | Vizi Labs | Relationship mapping employing multi-dimensional context including facial recognition |
US9143573B2 (en) | 2008-03-20 | 2015-09-22 | Facebook, Inc. | Tag suggestions for images on online social networks |
CA2728497A1 (en) * | 2008-06-17 | 2009-12-23 | Jostens, Inc. | System and method for yearbook creation |
US8457366B2 (en) | 2008-12-12 | 2013-06-04 | At&T Intellectual Property I, L.P. | System and method for matching faces |
US9727312B1 (en) * | 2009-02-17 | 2017-08-08 | Ikorongo Technology, LLC | Providing subject information regarding upcoming images on a display |
US10706601B2 (en) | 2009-02-17 | 2020-07-07 | Ikorongo Technology, LLC | Interface for receiving subject affinity information |
US9210313B1 (en) | 2009-02-17 | 2015-12-08 | Ikorongo Technology, LLC | Display device content selection through viewer identification and affinity prediction |
US8670597B2 (en) * | 2009-08-07 | 2014-03-11 | Google Inc. | Facial recognition with social network aiding |
US9135277B2 (en) | 2009-08-07 | 2015-09-15 | Google Inc. | Architecture for responding to a visual query |
JP5436104B2 (en) * | 2009-09-04 | 2014-03-05 | キヤノン株式会社 | Image search apparatus and image search method |
US20110099199A1 (en) * | 2009-10-27 | 2011-04-28 | Thijs Stalenhoef | Method and System of Detecting Events in Image Collections |
US9197736B2 (en) | 2009-12-31 | 2015-11-24 | Digimarc Corporation | Intuitive computing methods and systems |
US8121618B2 (en) | 2009-10-28 | 2012-02-21 | Digimarc Corporation | Intuitive computing methods and systems |
US8819172B2 (en) * | 2010-11-04 | 2014-08-26 | Digimarc Corporation | Smartphone-based methods and systems |
US20110119297A1 (en) * | 2009-11-18 | 2011-05-19 | Robert Rango | System and method for providing a personal characteristic-based contact list |
US9405772B2 (en) | 2009-12-02 | 2016-08-02 | Google Inc. | Actionable search results for street view visual queries |
US9176986B2 (en) | 2009-12-02 | 2015-11-03 | Google Inc. | Generating a combination of a visual query and matching canonical document |
US9852156B2 (en) | 2009-12-03 | 2017-12-26 | Google Inc. | Hybrid use of location sensor data and visual query to return local listings for visual query |
US8644563B2 (en) * | 2009-12-14 | 2014-02-04 | Microsoft Corporation | Recognition of faces using prior behavior |
US8526684B2 (en) * | 2009-12-14 | 2013-09-03 | Microsoft Corporation | Flexible image comparison and face matching application |
WO2011082332A1 (en) | 2009-12-31 | 2011-07-07 | Digimarc Corporation | Methods and arrangements employing sensor-equipped smart phones |
US8698762B2 (en) | 2010-01-06 | 2014-04-15 | Apple Inc. | Device, method, and graphical user interface for navigating and displaying content in context |
US9182596B2 (en) | 2010-02-28 | 2015-11-10 | Microsoft Technology Licensing, Llc | See-through near-eye display glasses with the optical assembly including absorptive polarizers or anti-reflective coatings to reduce stray light |
US9341843B2 (en) | 2010-02-28 | 2016-05-17 | Microsoft Technology Licensing, Llc | See-through near-eye display glasses with a small scale image source |
US20150309316A1 (en) | 2011-04-06 | 2015-10-29 | Microsoft Technology Licensing, Llc | Ar glasses with predictive control of external device based on event input |
US9366862B2 (en) | 2010-02-28 | 2016-06-14 | Microsoft Technology Licensing, Llc | System and method for delivering content to a group of see-through near eye display eyepieces |
US10180572B2 (en) | 2010-02-28 | 2019-01-15 | Microsoft Technology Licensing, Llc | AR glasses with event and user action control of external applications |
US9229227B2 (en) | 2010-02-28 | 2016-01-05 | Microsoft Technology Licensing, Llc | See-through near-eye display glasses with a light transmissive wedge shaped illumination system |
US9128281B2 (en) | 2010-09-14 | 2015-09-08 | Microsoft Technology Licensing, Llc | Eyepiece with uniformly illuminated reflective display |
US9129295B2 (en) | 2010-02-28 | 2015-09-08 | Microsoft Technology Licensing, Llc | See-through near-eye display glasses with a fast response photochromic film system for quick transition from dark to clear |
US9223134B2 (en) | 2010-02-28 | 2015-12-29 | Microsoft Technology Licensing, Llc | Optical imperfections in a light transmissive illumination system for see-through near-eye display glasses |
US20120249797A1 (en) | 2010-02-28 | 2012-10-04 | Osterhout Group, Inc. | Head-worn adaptive display |
US9097891B2 (en) | 2010-02-28 | 2015-08-04 | Microsoft Technology Licensing, Llc | See-through near-eye display glasses including an auto-brightness control for the display brightness based on the brightness in the environment |
US20120206335A1 (en) * | 2010-02-28 | 2012-08-16 | Osterhout Group, Inc. | Ar glasses with event, sensor, and user action based direct control of external devices with feedback |
US9759917B2 (en) | 2010-02-28 | 2017-09-12 | Microsoft Technology Licensing, Llc | AR glasses with event and sensor triggered AR eyepiece interface to external devices |
US9091851B2 (en) | 2010-02-28 | 2015-07-28 | Microsoft Technology Licensing, Llc | Light control in head mounted displays |
US9285589B2 (en) | 2010-02-28 | 2016-03-15 | Microsoft Technology Licensing, Llc | AR glasses with event and sensor triggered control of AR eyepiece applications |
US9097890B2 (en) | 2010-02-28 | 2015-08-04 | Microsoft Technology Licensing, Llc | Grating in a light transmissive illumination system for see-through near-eye display glasses |
US9134534B2 (en) | 2010-02-28 | 2015-09-15 | Microsoft Technology Licensing, Llc | See-through near-eye display glasses including a modular image source |
AU2011220382A1 (en) | 2010-02-28 | 2012-10-18 | Microsoft Corporation | Local advertising content on an interactive head-mounted eyepiece |
US8903798B2 (en) | 2010-05-28 | 2014-12-02 | Microsoft Corporation | Real-time annotation and enrichment of captured video |
KR101317401B1 (en) * | 2010-08-25 | 2013-10-10 | 주식회사 팬택 | Terminal device and method for object storing |
KR20120021057A (en) * | 2010-08-31 | 2012-03-08 | 삼성전자주식회사 | Method for providing search service to extract keywords in specific region and display apparatus applying the same |
KR20120021061A (en) * | 2010-08-31 | 2012-03-08 | 삼성전자주식회사 | Method for providing search service to extract keywords in specific region and display apparatus applying the same |
US8630494B1 (en) | 2010-09-01 | 2014-01-14 | Ikorongo Technology, LLC | Method and system for sharing image content based on collection proximity |
US8824748B2 (en) * | 2010-09-24 | 2014-09-02 | Facebook, Inc. | Auto tagging in geo-social networking system |
WO2012050251A1 (en) * | 2010-10-14 | 2012-04-19 | 엘지전자 주식회사 | Mobile terminal and method for controlling same |
WO2012061824A1 (en) * | 2010-11-05 | 2012-05-10 | Myspace, Inc. | Image auto tagging method and application |
US8559682B2 (en) * | 2010-11-09 | 2013-10-15 | Microsoft Corporation | Building a person profile database |
KR101429962B1 (en) * | 2010-11-22 | 2014-08-14 | 한국전자통신연구원 | System and method for processing data for recalling memory |
US9984157B2 (en) * | 2010-12-01 | 2018-05-29 | Aware Inc. | Relationship detection within biometric match results candidates |
US8526686B2 (en) * | 2010-12-24 | 2013-09-03 | Telefonaktiebolaget L M Ericsson (Publ) | Dynamic profile creation in response to facial recognition |
AU2011358100B2 (en) * | 2011-02-03 | 2016-07-07 | Facebook, Inc. | Systems and methods for image-to-text and text-to-image association |
JP5795650B2 (en) | 2011-02-18 | 2015-10-14 | グーグル・インク | Face recognition |
US20120213404A1 (en) | 2011-02-18 | 2012-08-23 | Google Inc. | Automatic event recognition and cross-user photo clustering |
US9483751B2 (en) | 2011-02-18 | 2016-11-01 | Google Inc. | Label privileges |
US9251854B2 (en) | 2011-02-18 | 2016-02-02 | Google Inc. | Facial detection, recognition and bookmarking in videos |
US8606776B2 (en) | 2011-02-18 | 2013-12-10 | Google Inc. | Affinity based ranked for search and display |
US9317530B2 (en) * | 2011-03-29 | 2016-04-19 | Facebook, Inc. | Face recognition based on spatial and temporal proximity |
RU2011115292A (en) * | 2011-04-18 | 2012-10-27 | Валерий Леонидович Сериков (RU) | METHOD FOR COMMUNICATION FOR THE PURPOSE OF CARRYING OUT ELECTORAL ACQUAINTANCE |
US20120278176A1 (en) * | 2011-04-27 | 2012-11-01 | Amir Naor | Systems and methods utilizing facial recognition and social network information associated with potential customers |
US8631084B2 (en) | 2011-04-29 | 2014-01-14 | Facebook, Inc. | Dynamic tagging recommendation |
US8818049B2 (en) * | 2011-05-18 | 2014-08-26 | Google Inc. | Retrieving contact information based on image recognition searches |
US9678992B2 (en) | 2011-05-18 | 2017-06-13 | Microsoft Technology Licensing, Llc | Text to image translation |
US8891832B2 (en) * | 2011-06-03 | 2014-11-18 | Facebook, Inc. | Computer-vision-assisted location check-in |
US8755610B2 (en) * | 2011-06-10 | 2014-06-17 | Apple Inc. | Auto-recognition for noteworthy objects |
US8935259B2 (en) | 2011-06-20 | 2015-01-13 | Google Inc | Text suggestions for images |
JP5830784B2 (en) * | 2011-06-23 | 2015-12-09 | サイバーアイ・エンタテインメント株式会社 | Interest graph collection system by relevance search with image recognition system |
US9159324B2 (en) * | 2011-07-01 | 2015-10-13 | Qualcomm Incorporated | Identifying people that are proximate to a mobile device user via social graphs, speech models, and user context |
US9143889B2 (en) * | 2011-07-05 | 2015-09-22 | Htc Corporation | Method of establishing application-related communication between mobile electronic devices, mobile electronic device, non-transitory machine readable media thereof, and media sharing method |
TWI452527B (en) * | 2011-07-06 | 2014-09-11 | Univ Nat Chiao Tung | Method and system for application program execution based on augmented reality and cloud computing |
EP2731072A4 (en) * | 2011-07-07 | 2015-03-25 | Kao Corp | Face impression analysis method, cosmetic counseling method, and face image generation method |
US8725796B2 (en) | 2011-07-07 | 2014-05-13 | F. David Serena | Relationship networks having link quality metrics with inference and concomitant digital value exchange |
US9195679B1 (en) | 2011-08-11 | 2015-11-24 | Ikorongo Technology, LLC | Method and system for the contextual display of image tags in a social network |
US20130054631A1 (en) * | 2011-08-30 | 2013-02-28 | Microsoft Corporation | Adding social network data to search suggestions |
US8533204B2 (en) * | 2011-09-02 | 2013-09-10 | Xerox Corporation | Text-based searching of image data |
US10074113B2 (en) * | 2011-09-07 | 2018-09-11 | Elwha Llc | Computational systems and methods for disambiguating search terms corresponding to network members |
US8953889B1 (en) * | 2011-09-14 | 2015-02-10 | Rawles Llc | Object datastore in an augmented reality environment |
US8873813B2 (en) * | 2012-09-17 | 2014-10-28 | Z Advanced Computing, Inc. | Application of Z-webs and Z-factors to analytics, search engine, learning, recognition, natural language, and other utilities |
US9906927B2 (en) | 2011-09-28 | 2018-02-27 | Elwha Llc | Multi-modality communication initiation |
US9477943B2 (en) | 2011-09-28 | 2016-10-25 | Elwha Llc | Multi-modality communication |
US9503550B2 (en) | 2011-09-28 | 2016-11-22 | Elwha Llc | Multi-modality communication modification |
US9002937B2 (en) | 2011-09-28 | 2015-04-07 | Elwha Llc | Multi-party multi-modality communication |
US9788349B2 (en) | 2011-09-28 | 2017-10-10 | Elwha Llc | Multi-modality communication auto-activation |
US9699632B2 (en) | 2011-09-28 | 2017-07-04 | Elwha Llc | Multi-modality communication with interceptive conversion |
US9794209B2 (en) | 2011-09-28 | 2017-10-17 | Elwha Llc | User interface for multi-modality communication |
US20130109302A1 (en) * | 2011-10-31 | 2013-05-02 | Royce A. Levien | Multi-modality communication with conversion offloading |
US9165017B2 (en) | 2011-09-29 | 2015-10-20 | Google Inc. | Retrieving images |
US8885960B2 (en) | 2011-10-05 | 2014-11-11 | Microsoft Corporation | Linking photographs via face, time, and location |
US8782042B1 (en) * | 2011-10-14 | 2014-07-15 | Firstrain, Inc. | Method and system for identifying entities |
CN102624534A (en) * | 2011-10-18 | 2012-08-01 | 北京小米科技有限责任公司 | Method for creating group |
US9177062B2 (en) | 2012-10-31 | 2015-11-03 | Google Inc. | Sorting social profile search results based on computing personal similarity scores |
WO2013067513A1 (en) | 2011-11-04 | 2013-05-10 | Massachusetts Eye & Ear Infirmary | Contextual image stabilization |
US9087273B2 (en) * | 2011-11-15 | 2015-07-21 | Facebook, Inc. | Facial recognition using social networking information |
US9280708B2 (en) * | 2011-11-30 | 2016-03-08 | Nokia Technologies Oy | Method and apparatus for providing collaborative recognition using media segments |
WO2013088994A1 (en) | 2011-12-14 | 2013-06-20 | 日本電気株式会社 | Video processing system, video processing method, and video processing device for portable terminal or for server and method for controlling and program for controlling same |
US10115127B2 (en) | 2011-12-16 | 2018-10-30 | Nec Corporation | Information processing system, information processing method, communications terminals and control method and control program thereof |
US20130156274A1 (en) * | 2011-12-19 | 2013-06-20 | Microsoft Corporation | Using photograph to initiate and perform action |
US9256620B2 (en) * | 2011-12-20 | 2016-02-09 | Amazon Technologies, Inc. | Techniques for grouping images |
WO2013100980A1 (en) * | 2011-12-28 | 2013-07-04 | Empire Technology Development Llc | Preventing classification of object contextual information |
US8924890B2 (en) * | 2012-01-10 | 2014-12-30 | At&T Intellectual Property I, L.P. | Dynamic glyph-based search |
KR102007840B1 (en) * | 2012-04-13 | 2019-08-06 | 엘지전자 주식회사 | A Method for Image Searching and a Digital Device Operating the Same |
US8422747B1 (en) | 2012-04-16 | 2013-04-16 | Google Inc. | Finding untagged images of a social network member |
US8925106B1 (en) | 2012-04-20 | 2014-12-30 | Google Inc. | System and method of ownership of an online collection |
US8666123B2 (en) * | 2012-04-26 | 2014-03-04 | Google Inc. | Creating social network groups |
US9047376B2 (en) * | 2012-05-01 | 2015-06-02 | Hulu, LLC | Augmenting video with facial recognition |
US20130294594A1 (en) * | 2012-05-04 | 2013-11-07 | Steven Chervets | Automating the identification of meeting attendees |
US8897484B1 (en) | 2012-05-18 | 2014-11-25 | Google Inc. | Image theft detector |
US8892523B2 (en) | 2012-06-08 | 2014-11-18 | Commvault Systems, Inc. | Auto summarization of content |
US8861804B1 (en) | 2012-06-15 | 2014-10-14 | Shutterfly, Inc. | Assisted photo-tagging with facial recognition models |
US20140015967A1 (en) * | 2012-07-16 | 2014-01-16 | Shaun Moore | Social intelligence, tracking and monitoring system and methods |
US9098584B1 (en) * | 2012-07-19 | 2015-08-04 | Google Inc. | Image search privacy protection techniques |
US8868598B2 (en) * | 2012-08-15 | 2014-10-21 | Microsoft Corporation | Smart user-centric information aggregation |
KR20140027826A (en) * | 2012-08-27 | 2014-03-07 | 삼성전자주식회사 | Apparatus and method for displaying a content in a portabel terminal |
US9471838B2 (en) * | 2012-09-05 | 2016-10-18 | Motorola Solutions, Inc. | Method, apparatus and system for performing facial recognition |
AU2012101375A4 (en) * | 2012-09-06 | 2012-10-18 | Oracle Recording's | FutureNetID |
US9514536B2 (en) * | 2012-10-10 | 2016-12-06 | Broadbandtv, Corp. | Intelligent video thumbnail selection and generation |
US20140108501A1 (en) * | 2012-10-17 | 2014-04-17 | Matthew Nicholas Papakipos | Presence Granularity with Augmented Reality |
US10038885B2 (en) | 2012-10-17 | 2018-07-31 | Facebook, Inc. | Continuous capture with augmented reality |
US10032233B2 (en) * | 2012-10-17 | 2018-07-24 | Facebook, Inc. | Social context in augmented reality |
TWI528186B (en) * | 2012-11-09 | 2016-04-01 | 財團法人資訊工業策進會 | System and method for posting messages by audio signals |
US20140160157A1 (en) * | 2012-12-11 | 2014-06-12 | Adam G. Poulos | People-triggered holographic reminders |
CN103076879A (en) * | 2012-12-28 | 2013-05-01 | 中兴通讯股份有限公司 | Multimedia interaction method and device based on face information, and terminal |
US8824751B2 (en) * | 2013-01-07 | 2014-09-02 | MTN Satellite Communications | Digital photograph group editing and access |
US9311640B2 (en) | 2014-02-11 | 2016-04-12 | Digimarc Corporation | Methods and arrangements for smartphone payments and transactions |
JP2014164697A (en) * | 2013-02-27 | 2014-09-08 | Canon Inc | Image processing apparatus, image processing method, program, and storage medium |
US9514197B2 (en) * | 2013-03-01 | 2016-12-06 | Ebay Inc. | System and method of selecting events or locations based on content |
US20140280267A1 (en) * | 2013-03-14 | 2014-09-18 | Fotofad, Inc. | Creating real-time association interaction throughout digital media |
SG11201507312RA (en) | 2013-03-15 | 2015-10-29 | Socure Inc | Risk assessment using social networking data |
US10296933B2 (en) * | 2013-04-12 | 2019-05-21 | Facebook, Inc. | Identifying content in electronic images |
EP2990920B1 (en) | 2013-04-22 | 2019-04-03 | Fujitsu Limited | Information terminal control method |
WO2014172827A1 (en) * | 2013-04-22 | 2014-10-30 | Nokia Corporation | A method and apparatus for acquaintance management and privacy protection |
US9922052B1 (en) * | 2013-04-26 | 2018-03-20 | A9.Com, Inc. | Custom image data store |
KR20140130331A (en) * | 2013-04-30 | 2014-11-10 | (주)세이엔 | Wearable electronic device and method for controlling the same |
US9646208B2 (en) * | 2013-05-07 | 2017-05-09 | Htc Corporation | Method for computerized grouping contact list, electronic device using the same and computer program product |
US10176500B1 (en) * | 2013-05-29 | 2019-01-08 | A9.Com, Inc. | Content classification based on data recognition |
US10645127B1 (en) * | 2013-05-30 | 2020-05-05 | Jpmorgan Chase Bank, N.A. | System and method for virtual briefing books |
CA2913461C (en) * | 2013-05-30 | 2016-05-24 | Facebook, Inc. | Tag suggestions for images on online social networks |
EP3008425B1 (en) | 2013-06-13 | 2018-08-08 | Intuitive Surgical Operations, Inc. | An overlapped chirped fiber bragg grating sensing fiber and methods and apparatus for parameter measurement using same |
KR102099400B1 (en) * | 2013-06-20 | 2020-04-09 | 삼성전자주식회사 | Apparatus and method for displaying an image in a portable terminal |
US20150006669A1 (en) * | 2013-07-01 | 2015-01-01 | Google Inc. | Systems and methods for directing information flow |
CN104346370B (en) * | 2013-07-31 | 2018-10-23 | 阿里巴巴集团控股有限公司 | Picture search, the method and device for obtaining image text information |
US9798813B2 (en) * | 2013-07-31 | 2017-10-24 | Salesforce.Com, Inc. | Extensible person container |
WO2015016784A1 (en) * | 2013-08-01 | 2015-02-05 | National University Of Singapore | A method and apparatus for tracking microblog messages for relevancy to an entity identifiable by an associated text and an image |
CN103347032A (en) * | 2013-08-01 | 2013-10-09 | 赵频 | Method and system for making friends |
US10152495B2 (en) | 2013-08-19 | 2018-12-11 | Qualcomm Incorporated | Visual search in real world using optical see-through head mounted display with augmented reality and user interaction tracking |
WO2015053604A1 (en) * | 2013-10-08 | 2015-04-16 | Data Calibre Sdn Bhd | A face retrieval method |
US9531722B1 (en) | 2013-10-31 | 2016-12-27 | Google Inc. | Methods for generating an activity stream |
US9542457B1 (en) | 2013-11-07 | 2017-01-10 | Google Inc. | Methods for displaying object history information |
US9614880B1 (en) | 2013-11-12 | 2017-04-04 | Google Inc. | Methods for real-time notifications in an activity stream |
US20150131868A1 (en) * | 2013-11-14 | 2015-05-14 | VISAGE The Global Pet Recognition Company Inc. | System and method for matching an animal to existing animal profiles |
WO2015094370A1 (en) * | 2013-12-20 | 2015-06-25 | Intel Corporation | Social circle and relationship identification |
US9972324B2 (en) | 2014-01-10 | 2018-05-15 | Verizon Patent And Licensing Inc. | Personal assistant application |
IN2014MU00227A (en) * | 2014-01-22 | 2015-09-04 | Reliance Jio Infocomm Ltd | |
US9177194B2 (en) | 2014-01-29 | 2015-11-03 | Sony Corporation | System and method for visually distinguishing faces in a digital image |
US20150213010A1 (en) * | 2014-01-30 | 2015-07-30 | Sage Microelectronics Corp. | Storage system with distributed data searching |
US9509772B1 (en) | 2014-02-13 | 2016-11-29 | Google Inc. | Visualization and control of ongoing ingress actions |
US10121060B2 (en) | 2014-02-13 | 2018-11-06 | Oath Inc. | Automatic group formation and group detection through media recognition |
US9710447B2 (en) * | 2014-03-17 | 2017-07-18 | Yahoo! Inc. | Visual recognition using social links |
GB201406594D0 (en) | 2014-04-11 | 2014-05-28 | Idscan Biometric Ltd | Method, system and computer program for validating a facial image-bearing identity document |
US9910479B2 (en) | 2014-04-16 | 2018-03-06 | Facebook, Inc. | Location based content promotion on online social networks |
US9495617B2 (en) | 2014-05-08 | 2016-11-15 | Shutterfly, Inc. | Image product creation based on face images grouped using image product statistics |
US9594946B2 (en) | 2014-05-08 | 2017-03-14 | Shutterfly, Inc. | Image product creation based on face images grouped using image product statistics |
US9280701B2 (en) | 2014-05-08 | 2016-03-08 | Shutterfly, Inc. | Grouping face images using statistic distribution estimate |
US9519826B2 (en) | 2014-05-08 | 2016-12-13 | Shutterfly, Inc. | Automatic image product creation for user accounts comprising large number of images |
US20150356180A1 (en) * | 2014-06-04 | 2015-12-10 | Facebook, Inc. | Inferring relationship statuses of users of a social networking system |
US9536199B1 (en) | 2014-06-09 | 2017-01-03 | Google Inc. | Recommendations based on device usage |
JP5664813B1 (en) * | 2014-06-10 | 2015-02-04 | 富士ゼロックス株式会社 | Design management apparatus and program |
US9147117B1 (en) | 2014-06-11 | 2015-09-29 | Socure Inc. | Analyzing facial recognition data and social network data for user authentication |
US9507791B2 (en) | 2014-06-12 | 2016-11-29 | Google Inc. | Storage system user interface with floating file collection |
US10078781B2 (en) | 2014-06-13 | 2018-09-18 | Google Llc | Automatically organizing images |
US9811592B1 (en) | 2014-06-24 | 2017-11-07 | Google Inc. | Query modification based on textual resource context |
US9830391B1 (en) | 2014-06-24 | 2017-11-28 | Google Inc. | Query modification based on non-textual resource context |
US10049477B1 (en) | 2014-06-27 | 2018-08-14 | Google Llc | Computer-assisted text and visual styling for images |
CN104143213B (en) * | 2014-07-16 | 2017-05-31 | 北京卫星制造厂 | A kind of conduit automatic identifying method of view-based access control model detection |
US20160019284A1 (en) * | 2014-07-18 | 2016-01-21 | Linkedln Corporation | Search engine using name clustering |
CN104091164A (en) * | 2014-07-28 | 2014-10-08 | 北京奇虎科技有限公司 | Face picture name recognition method and system |
KR102366677B1 (en) * | 2014-08-02 | 2022-02-23 | 삼성전자주식회사 | Apparatus and Method for User Interaction thereof |
US9251427B1 (en) * | 2014-08-12 | 2016-02-02 | Microsoft Technology Licensing, Llc | False face representation identification |
GB201415938D0 (en) * | 2014-09-09 | 2014-10-22 | Idscan Biometrics Ltd | Distributed Identity Validation Method System And Computer Program |
US10277588B2 (en) * | 2014-11-03 | 2019-04-30 | Facebook, Inc. | Systems and methods for authenticating a user based on self-portrait media content |
US10104345B2 (en) * | 2014-12-16 | 2018-10-16 | Sighthound, Inc. | Data-enhanced video viewing system and methods for computer vision processing |
US10489637B2 (en) | 2014-12-23 | 2019-11-26 | Beijing Qihoo Technology Company Limited | Method and device for obtaining similar face images and face image information |
CN104537341B (en) * | 2014-12-23 | 2016-10-05 | 北京奇虎科技有限公司 | Face picture information getting method and device |
US9870420B2 (en) | 2015-01-19 | 2018-01-16 | Google Llc | Classification and storage of documents |
US9953151B2 (en) | 2015-02-03 | 2018-04-24 | Chon Hock LEOW | System and method identifying a user to an associated device |
JP6589300B2 (en) * | 2015-03-09 | 2019-10-16 | フリュー株式会社 | Image generating apparatus and control method thereof |
JP6522779B2 (en) * | 2015-03-27 | 2019-05-29 | 華為技術有限公司Ｈｕａｗｅｉ Ｔｅｃｈｎｏｌｏｇｉｅｓ Ｃｏ．，Ｌｔｄ． | Method and apparatus for displaying electronic photographs and mobile device |
US10445391B2 (en) | 2015-03-27 | 2019-10-15 | Jostens, Inc. | Yearbook publishing system |
GB2537139A (en) * | 2015-04-08 | 2016-10-12 | Edward Henderson Charles | System and method for processing and retrieving digital content |
CN106156144A (en) * | 2015-04-13 | 2016-11-23 | 腾讯科技（深圳）有限公司 | Information-pushing method and device |
US10691314B1 (en) | 2015-05-05 | 2020-06-23 | State Farm Mutual Automobile Insurance Company | Connecting users to entities based on recognized objects |
US9704020B2 (en) | 2015-06-16 | 2017-07-11 | Microsoft Technology Licensing, Llc | Automatic recognition of entities in media-captured events |
US9872061B2 (en) | 2015-06-20 | 2018-01-16 | Ikorongo Technology, LLC | System and device for interacting with a remote presentation |
AU2016277553B2 (en) * | 2015-06-26 | 2022-02-17 | Rovi Guides, Inc. | Systems and methods for automatic formatting of images for media assets based on user profile |
US9591359B2 (en) | 2015-06-26 | 2017-03-07 | Rovi Guides, Inc. | Systems and methods for automatic formatting of images for media assets based on prevalence |
US10628009B2 (en) | 2015-06-26 | 2020-04-21 | Rovi Guides, Inc. | Systems and methods for automatic formatting of images for media assets based on user profile |
US20160378308A1 (en) * | 2015-06-26 | 2016-12-29 | Rovi Guides, Inc. | Systems and methods for identifying an optimal image for a media asset representation |
KR20170004450A (en) * | 2015-07-02 | 2017-01-11 | 엘지전자 주식회사 | Mobile terminal and method for controlling the same |
US10094655B2 (en) | 2015-07-15 | 2018-10-09 | 15 Seconds of Fame, Inc. | Apparatus and methods for facial recognition and video analytics to identify individuals in contextual video streams |
US10154071B2 (en) * | 2015-07-29 | 2018-12-11 | International Business Machines Corporation | Group chat with dynamic background images and content from social media |
CN105095873B (en) * | 2015-07-31 | 2018-12-18 | 小米科技有限责任公司 | Photo be shared method, apparatus |
US10521099B2 (en) * | 2015-08-28 | 2019-12-31 | Facebook, Inc. | Systems and methods for providing interactivity for panoramic media content |
US10521100B2 (en) | 2015-08-28 | 2019-12-31 | Facebook, Inc. | Systems and methods for providing interactivity for panoramic media content |
US20170060404A1 (en) * | 2015-08-28 | 2017-03-02 | Facebook, Inc. | Systems and methods for providing interactivity for panoramic media content |
EP3365838A4 (en) | 2015-10-21 | 2019-08-28 | 15 Seconds Of Fame, Inc. | Methods and apparatus for false positive minimization in facial recognition applications |
EP3371742A4 (en) | 2015-11-04 | 2019-06-26 | Shutterfly, Inc. | Automatic image product creation for user accounts comprising large number of images |
US9904872B2 (en) | 2015-11-13 | 2018-02-27 | Microsoft Technology Licensing, Llc | Visual representations of photo albums |
US10002313B2 (en) | 2015-12-15 | 2018-06-19 | Sighthound, Inc. | Deeply learned convolutional neural networks (CNNS) for object localization and classification |
US10291610B2 (en) | 2015-12-15 | 2019-05-14 | Visa International Service Association | System and method for biometric authentication using social network |
CN107533566A (en) * | 2016-02-25 | 2018-01-02 | 华为技术有限公司 | Method, portable electric appts and the graphic user interface retrieved to the content of picture |
US10306315B2 (en) | 2016-03-29 | 2019-05-28 | International Business Machines Corporation | Video streaming augmenting |
US10740385B1 (en) * | 2016-04-21 | 2020-08-11 | Shutterstock, Inc. | Identifying visual portions of visual media files responsive to search queries |
US11003667B1 (en) | 2016-05-27 | 2021-05-11 | Google Llc | Contextual information for a displayed resource |
AU2017100670C4 (en) | 2016-06-12 | 2019-11-21 | Apple Inc. | User interfaces for retrieving contextually relevant media content |
DK201670608A1 (en) * | 2016-06-12 | 2018-01-02 | Apple Inc | User interfaces for retrieving contextually relevant media content |
US10318812B2 (en) | 2016-06-21 | 2019-06-11 | International Business Machines Corporation | Automatic digital image correlation and distribution |
US10152521B2 (en) | 2016-06-22 | 2018-12-11 | Google Llc | Resource recommendations for a displayed resource |
CN106096009A (en) * | 2016-06-23 | 2016-11-09 | 北京小米移动软件有限公司 | Method for generating message and device |
CN106096011A (en) | 2016-06-23 | 2016-11-09 | 北京小米移动软件有限公司 | Method for picture sharing and device |
US10802671B2 (en) | 2016-07-11 | 2020-10-13 | Google Llc | Contextual information for a displayed resource that includes an image |
US10467300B1 (en) | 2016-07-21 | 2019-11-05 | Google Llc | Topical resource recommendations for a displayed resource |
US10489459B1 (en) | 2016-07-21 | 2019-11-26 | Google Llc | Query recommendations for a displayed resource |
US10051108B2 (en) | 2016-07-21 | 2018-08-14 | Google Llc | Contextual information for a notification |
US10169649B2 (en) * | 2016-07-28 | 2019-01-01 | International Business Machines Corporation | Smart image filtering method with domain rules application |
US10212113B2 (en) | 2016-09-19 | 2019-02-19 | Google Llc | Uniform resource identifier and image sharing for contextual information display |
CN106446831B (en) * | 2016-09-24 | 2021-06-25 | 江西欧迈斯微电子有限公司 | Face recognition method and device |
US10540516B2 (en) | 2016-10-13 | 2020-01-21 | Commvault Systems, Inc. | Data protection within an unsecured storage environment |
US20180247310A1 (en) * | 2017-02-28 | 2018-08-30 | Mastercard International Incorporated | System and method for validating a cashless transaction |
US10282598B2 (en) | 2017-03-07 | 2019-05-07 | Bank Of America Corporation | Performing image analysis for dynamic personnel identification based on a combination of biometric features |
US10311308B2 (en) * | 2017-03-31 | 2019-06-04 | International Business Machines Corporation | Image processing to identify selected individuals in a field of view |
US10552471B1 (en) * | 2017-04-21 | 2020-02-04 | Stripe, Inc. | Determining identities of multiple people in a digital image |
EP3410330B1 (en) | 2017-05-31 | 2021-07-21 | Mastercard International Incorporated | Improvements in biometric authentication |
US10679068B2 (en) | 2017-06-13 | 2020-06-09 | Google Llc | Media contextual information from buffered media data |
US20180365268A1 (en) * | 2017-06-15 | 2018-12-20 | WindowLykr Inc. | Data structure, system and method for interactive media |
WO2019008553A1 (en) * | 2017-07-07 | 2019-01-10 | Bhushan Fani | System and method for establishing a communication session |
US10607082B2 (en) * | 2017-09-09 | 2020-03-31 | Google Llc | Systems, methods, and apparatus for image-responsive automated assistants |
US10824329B2 (en) * | 2017-09-25 | 2020-11-03 | Motorola Solutions, Inc. | Methods and systems for displaying query status information on a graphical user interface |
US10803297B2 (en) | 2017-09-27 | 2020-10-13 | International Business Machines Corporation | Determining quality of images for user identification |
US10776467B2 (en) | 2017-09-27 | 2020-09-15 | International Business Machines Corporation | Establishing personal identity using real time contextual data |
US10795979B2 (en) | 2017-09-27 | 2020-10-06 | International Business Machines Corporation | Establishing personal identity and user behavior based on identity patterns |
US10839003B2 (en) | 2017-09-27 | 2020-11-17 | International Business Machines Corporation | Passively managed loyalty program using customer images and behaviors |
GB2581657A (en) | 2017-10-10 | 2020-08-26 | Laurie Cal Llc | Online identity verification platform and process |
WO2019083508A1 (en) * | 2017-10-24 | 2019-05-02 | Hewlett-Packard Development Company, L.P. | Facial recognitions based on contextual information |
WO2019083509A1 (en) * | 2017-10-24 | 2019-05-02 | Hewlett-Packard Development Company, L.P. | Person segmentations for background replacements |
US10565432B2 (en) | 2017-11-29 | 2020-02-18 | International Business Machines Corporation | Establishing personal identity based on multiple sub-optimal images |
KR102061787B1 (en) | 2017-11-29 | 2020-01-03 | 삼성전자주식회사 | The Electronic Device Shooting Image and the Method for Displaying the Image |
US10387487B1 (en) | 2018-01-25 | 2019-08-20 | Ikorongo Technology, LLC | Determining images of interest based on a geographical location |
CN108270794B (en) * | 2018-02-06 | 2020-10-09 | 腾讯科技（深圳）有限公司 | Content distribution method, device and readable medium |
US10642886B2 (en) * | 2018-02-14 | 2020-05-05 | Commvault Systems, Inc. | Targeted search of backup data using facial recognition |
US11145294B2 (en) | 2018-05-07 | 2021-10-12 | Apple Inc. | Intelligent automated assistant for delivering content from user experiences |
DK180171B1 (en) | 2018-05-07 | 2020-07-14 | Apple Inc | USER INTERFACES FOR SHARING CONTEXTUALLY RELEVANT MEDIA CONTENT |
US10810457B2 (en) * | 2018-05-09 | 2020-10-20 | Fuji Xerox Co., Ltd. | System for searching documents and people based on detecting documents and people around a table |
US10511763B1 (en) * | 2018-06-19 | 2019-12-17 | Microsoft Technology Licensing, Llc | Starting electronic communication based on captured image |
CA3050456C (en) * | 2018-07-24 | 2023-01-03 | Sultan A. Alrasheed | Facial modelling and matching systems and methods |
US10402553B1 (en) * | 2018-07-31 | 2019-09-03 | Capital One Services, Llc | System and method for using images to authenticate a user |
KR102077093B1 (en) * | 2018-08-23 | 2020-02-13 | 엔에이치엔 주식회사 | Device and method to share image received from user terminal with other user terminals |
US10936856B2 (en) | 2018-08-31 | 2021-03-02 | 15 Seconds of Fame, Inc. | Methods and apparatus for reducing false positives in facial recognition |
US10891480B2 (en) * | 2018-09-27 | 2021-01-12 | Ncr Corporation | Image zone processing |
US11012387B2 (en) | 2018-10-30 | 2021-05-18 | AMI Holdings Limited | Apparatus and method for matching individuals during an ephemeral time period based upon voting and matching criteria |
US10810403B2 (en) * | 2018-10-30 | 2020-10-20 | AMI Holdings Limited | Apparatus and method for coordinating the matching and initial communications between individuals in a dating application |
KR102605451B1 (en) * | 2018-11-14 | 2023-11-24 | 삼성전자주식회사 | Electronic device and method for providing multiple services respectively corresponding to multiple external objects included in image |
KR102581146B1 (en) * | 2018-11-23 | 2023-09-21 | 삼성전자주식회사 | Display apparatus and control method thereof |
US10936178B2 (en) | 2019-01-07 | 2021-03-02 | MemoryWeb, LLC | Systems and methods for analyzing and organizing digital photos and videos |
JP6635208B1 (en) * | 2019-02-22 | 2020-01-22 | 日本電気株式会社 | Search device, search method, and program |
US11010596B2 (en) | 2019-03-07 | 2021-05-18 | 15 Seconds of Fame, Inc. | Apparatus and methods for facial recognition systems to identify proximity-based connections |
US11093715B2 (en) * | 2019-03-29 | 2021-08-17 | Samsung Electronics Co., Ltd. | Method and system for learning and enabling commands via user demonstration |
US11468881B2 (en) | 2019-03-29 | 2022-10-11 | Samsung Electronics Co., Ltd. | Method and system for semantic intelligent task learning and adaptive execution |
DK201970535A1 (en) | 2019-05-06 | 2020-12-21 | Apple Inc | Media browsing user interface with intelligently selected representative media items |
CN110457602A (en) * | 2019-08-15 | 2019-11-15 | 张学志 | A kind of making friends method and device based on recognition of face |
US11283937B1 (en) | 2019-08-15 | 2022-03-22 | Ikorongo Technology, LLC | Sharing images based on face matching in a network |
US11341351B2 (en) | 2020-01-03 | 2022-05-24 | 15 Seconds of Fame, Inc. | Methods and apparatus for facial recognition on a user device |
JP6842136B1 (en) * | 2020-01-10 | 2021-03-17 | クェスタ株式会社 | Program, display control method, display control device and signage system |
US20210248562A1 (en) * | 2020-02-10 | 2021-08-12 | The Boeing Company | Method and system for communicating social network scheduling between devices |
CN111539438B (en) * | 2020-04-28 | 2024-01-12 | 北京百度网讯科技有限公司 | Text content identification method and device and electronic equipment |
US10990166B1 (en) * | 2020-05-10 | 2021-04-27 | Truthify, LLC | Remote reaction capture and analysis system |
US11108996B1 (en) | 2020-07-28 | 2021-08-31 | Bank Of America Corporation | Two-way intercept using coordinate tracking and video classification |
US20220092105A1 (en) * | 2020-09-18 | 2022-03-24 | Google Llc | Intelligent Systems and Methods for Visual Search Queries |
US11907521B2 (en) | 2021-01-28 | 2024-02-20 | Samsung Electronics Co., Ltd. | Augmented reality calling interface |
US20230245127A1 (en) * | 2022-02-02 | 2023-08-03 | Kyndryl, Inc. | Augmented user authentication |
US20230403268A1 (en) * | 2022-05-25 | 2023-12-14 | Paypal, Inc. | Reducing false positives in entity matching based on image-linking graphs |
Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20050097131A1 (en) * | 2003-10-30 | 2005-05-05 | Lucent Technologies Inc. | Network support for caller identification based on biometric measurement |
US20060020630A1 (en) * | 2004-07-23 | 2006-01-26 | Stager Reed R | Facial database methods and systems |
US20090060289A1 (en) * | 2005-09-28 | 2009-03-05 | Alex Shah | Digital Image Search System And Method |
KR20090073294A (en) * | 2007-12-31 | 2009-07-03 | 인하대학교 산학협력단 | Method for social network analysis based on face recognition in an image or image sequences |
Family Cites Families (81)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2813728B2 (en) | 1993-11-01 | 1998-10-22 | インターナショナル・ビジネス・マシーンズ・コーポレイション | Personal communication device with zoom / pan function |
US5764799A (en) | 1995-06-26 | 1998-06-09 | Research Foundation Of State Of State Of New York | OCR method and apparatus using image equivalents |
US5987448A (en) | 1997-07-25 | 1999-11-16 | Claritech Corporation | Methodology for displaying search results using character recognition |
US6269188B1 (en) | 1998-03-12 | 2001-07-31 | Canon Kabushiki Kaisha | Word grouping accuracy value generation |
US6137907A (en) | 1998-09-23 | 2000-10-24 | Xerox Corporation | Method and apparatus for pixel-level override of halftone detection within classification blocks to reduce rectangular artifacts |
GB9903451D0 (en) | 1999-02-16 | 1999-04-07 | Hewlett Packard Co | Similarity searching for documents |
US6408293B1 (en) | 1999-06-09 | 2002-06-18 | International Business Machines Corporation | Interactive framework for understanding user's perception of multimedia data |
EP1312039B1 (en) | 2000-08-24 | 2006-03-29 | Olive Software, Inc. | System and method for automatic preparation and searching of scanned documents |
US20020103920A1 (en) | 2000-11-21 | 2002-08-01 | Berkun Ken Alan | Interpretive stream metadata extraction |
JP2002189724A (en) | 2000-12-20 | 2002-07-05 | Victor Co Of Japan Ltd | Image data retrieval device |
US6748398B2 (en) | 2001-03-30 | 2004-06-08 | Microsoft Corporation | Relevance maximizing, iteration minimizing, relevance-feedback, content-based image retrieval (CBIR) |
US7313617B2 (en) | 2001-09-28 | 2007-12-25 | Dale Malik | Methods and systems for a communications and information resource manager |
JP2004086625A (en) * | 2002-08-27 | 2004-03-18 | Hitoshi Hongo | Customer information managing device |
DE10245900A1 (en) | 2002-09-30 | 2004-04-08 | Neven jun., Hartmut, Prof.Dr. | Image based query system for search engines or databases of mobile telephone, portable computer uses image recognition to access more information about objects in image |
US7298931B2 (en) | 2002-10-14 | 2007-11-20 | Samsung Electronics Co., Ltd. | Image retrieval method and apparatus using iterative matching |
US7472110B2 (en) * | 2003-01-29 | 2008-12-30 | Microsoft Corporation | System and method for employing social networks for information discovery |
US7370034B2 (en) | 2003-10-15 | 2008-05-06 | Xerox Corporation | System and method for performing electronic information retrieval using keywords |
US20050083413A1 (en) | 2003-10-20 | 2005-04-21 | Logicalis | Method, system, apparatus, and machine-readable medium for use in connection with a server that uses images or audio for initiating remote function calls |
US7872669B2 (en) | 2004-01-22 | 2011-01-18 | Massachusetts Institute Of Technology | Photo-based mobile deixis system and related techniques |
US7707039B2 (en) | 2004-02-15 | 2010-04-27 | Exbiblio B.V. | Automatic modification of web pages |
JP2005092854A (en) * | 2004-04-16 | 2005-04-07 | Penpower Technology Ltd | Face model production method and face identification method |
WO2005114476A1 (en) | 2004-05-13 | 2005-12-01 | Nevengineering, Inc. | Mobile image-based information retrieval system |
US7890871B2 (en) * | 2004-08-26 | 2011-02-15 | Redlands Technology, Llc | System and method for dynamically generating, maintaining, and growing an online social network |
JP2006079460A (en) | 2004-09-10 | 2006-03-23 | Fuji Photo Film Co Ltd | System, method and program for displaying electronic album and device, method, and program for classifying image |
US8489583B2 (en) | 2004-10-01 | 2013-07-16 | Ricoh Company, Ltd. | Techniques for retrieving documents using an image capture device |
US8320641B2 (en) | 2004-10-28 | 2012-11-27 | DigitalOptics Corporation Europe Limited | Method and apparatus for red-eye detection using preview or other reference images |
JP4380524B2 (en) * | 2004-12-17 | 2009-12-09 | ソニー株式会社 | Information processing apparatus and information processing method |
US8503800B2 (en) * | 2007-03-05 | 2013-08-06 | DigitalOptics Corporation Europe Limited | Illumination detection using classifier chains |
US9451219B2 (en) | 2004-12-31 | 2016-09-20 | Nokia Technologies Oy | Provision of target specific information |
US20060150119A1 (en) | 2004-12-31 | 2006-07-06 | France Telecom | Method for interacting with automated information agents using conversational queries |
US20070201749A1 (en) | 2005-02-07 | 2007-08-30 | Masaki Yamauchi | Image Processing Device And Image Processing Method |
JP4739062B2 (en) * | 2005-02-28 | 2011-08-03 | 富士フイルム株式会社 | Image output apparatus, image output method, and program |
JP4267584B2 (en) | 2005-02-28 | 2009-05-27 | 株式会社東芝 | Device control apparatus and method |
US7765231B2 (en) * | 2005-04-08 | 2010-07-27 | Rathus Spencer A | System and method for accessing electronic data via an image search engine |
US7956669B2 (en) | 2005-04-15 | 2011-06-07 | International Business Machines Corporation | High-density low-power data retention power gating with double-gate devices |
US7773822B2 (en) | 2005-05-02 | 2010-08-10 | Colormax, Inc. | Apparatus and methods for management of electronic images |
US7760917B2 (en) * | 2005-05-09 | 2010-07-20 | Like.Com | Computer-implemented method for performing similarity searches |
US7809722B2 (en) | 2005-05-09 | 2010-10-05 | Like.Com | System and method for enabling search and retrieval from image files based on recognized information |
JP2007026419A (en) | 2005-06-17 | 2007-02-01 | Hitachi Ltd | Method for managing social network information and system therefor |
KR100754656B1 (en) | 2005-06-20 | 2007-09-03 | 삼성전자주식회사 | Method and system for providing user with image related information and mobile communication system |
JP2007026316A (en) * | 2005-07-20 | 2007-02-01 | Yamaha Motor Co Ltd | Image management device, image-managing computer program and recording medium recording the same |
US8095551B2 (en) * | 2005-08-18 | 2012-01-10 | Microsoft Corporation | Annotating shared contacts with public descriptors |
US7450740B2 (en) | 2005-09-28 | 2008-11-11 | Facedouble, Inc. | Image classification and information retrieval over wireless digital networks and the internet |
US7876978B2 (en) | 2005-10-13 | 2011-01-25 | Penthera Technologies, Inc. | Regions of interest in video frames |
US20070098303A1 (en) * | 2005-10-31 | 2007-05-03 | Eastman Kodak Company | Determining a particular person from a collection |
US8849821B2 (en) | 2005-11-04 | 2014-09-30 | Nokia Corporation | Scalable visual search system simplifying access to network and device functionality |
US7826665B2 (en) | 2005-12-12 | 2010-11-02 | Xerox Corporation | Personal information retrieval using knowledge bases for optical character recognition correction |
US7725477B2 (en) | 2005-12-19 | 2010-05-25 | Microsoft Corporation | Power filter for online listing service |
US8874591B2 (en) | 2006-01-31 | 2014-10-28 | Microsoft Corporation | Using user feedback to improve search results |
US9336333B2 (en) | 2006-02-13 | 2016-05-10 | Linkedin Corporation | Searching and reference checking within social networks |
JP2007249394A (en) * | 2006-03-14 | 2007-09-27 | Nippon Hoso Kyokai <Nhk> | Face image recognition device and face image recognition program |
US20070245045A1 (en) * | 2006-03-27 | 2007-10-18 | Sidney Wu | Wireless data transceiver |
US7668405B2 (en) * | 2006-04-07 | 2010-02-23 | Eastman Kodak Company | Forming connections between image collections |
JP2007316939A (en) | 2006-05-25 | 2007-12-06 | Fujifilm Corp | Electronic album providing device and image network system |
US7917514B2 (en) | 2006-06-28 | 2011-03-29 | Microsoft Corporation | Visual and multi-dimensional search |
US9176984B2 (en) | 2006-07-31 | 2015-11-03 | Ricoh Co., Ltd | Mixed media reality retrieval of differentially-weighted links |
JP4891691B2 (en) * | 2006-07-31 | 2012-03-07 | ヤフー株式会社 | Method and system for retrieving data with location information added |
US20080031506A1 (en) | 2006-08-07 | 2008-02-07 | Anuradha Agatheeswaran | Texture analysis for mammography computer aided diagnosis |
US7934156B2 (en) | 2006-09-06 | 2011-04-26 | Apple Inc. | Deletion gestures on a portable multifunction device |
JP4914778B2 (en) | 2006-09-14 | 2012-04-11 | オリンパスイメージング株式会社 | camera |
US8599251B2 (en) | 2006-09-14 | 2013-12-03 | Olympus Imaging Corp. | Camera |
JP2008165701A (en) | 2007-01-05 | 2008-07-17 | Seiko Epson Corp | Image processing device, electronics equipment, image processing method, and program |
KR100865973B1 (en) | 2007-02-08 | 2008-10-30 | (주)올라웍스 | Method for searching certain person and method and system for generating copyright report for the certain person |
US8861898B2 (en) | 2007-03-16 | 2014-10-14 | Sony Corporation | Content image search |
KR100768127B1 (en) * | 2007-04-10 | 2007-10-17 | (주)올라웍스 | Method for inferring personal relations by using readable data and method and system for tagging person identification information to digital data by using readable data |
CN104866469B (en) | 2007-04-11 | 2018-10-02 | 谷歌有限责任公司 | Input Method Editor with secondary language mode |
US20080267504A1 (en) | 2007-04-24 | 2008-10-30 | Nokia Corporation | Method, device and computer program product for integrating code-based and optical character recognition technologies into a mobile visual search |
US10069924B2 (en) | 2007-07-25 | 2018-09-04 | Oath Inc. | Application programming interfaces for communication systems |
JP5128880B2 (en) | 2007-08-30 | 2013-01-23 | オリンパスイメージング株式会社 | Image handling device |
KR101435140B1 (en) | 2007-10-16 | 2014-09-02 | 삼성전자 주식회사 | Display apparatus and method |
WO2009050741A2 (en) * | 2007-10-19 | 2009-04-23 | Amsoft Systems Private Limited | Method and system of ranking transaction channels associated with real world identities, based on their attributes and preferences |
JP5459527B2 (en) | 2007-10-29 | 2014-04-02 | 株式会社Ｊｖｃケンウッド | Image processing apparatus and method |
GB2454213A (en) | 2007-10-31 | 2009-05-06 | Sony Corp | Analyzing a Plurality of Stored Images to Allow Searching |
US20090132264A1 (en) | 2007-11-16 | 2009-05-21 | Wood Mark D | Media asset evaluation based on social relationships |
US9237213B2 (en) | 2007-11-20 | 2016-01-12 | Yellowpages.Com Llc | Methods and apparatuses to initiate telephone connections |
US20090237546A1 (en) | 2008-03-24 | 2009-09-24 | Sony Ericsson Mobile Communications Ab | Mobile Device with Image Recognition Processing Capability |
US8190604B2 (en) | 2008-04-03 | 2012-05-29 | Microsoft Corporation | User intention modeling for interactive image retrieval |
JP4939480B2 (en) * | 2008-05-19 | 2012-05-23 | 富士フイルム株式会社 | Display device, imaging device, image search device, and program |
JP5109836B2 (en) | 2008-07-01 | 2012-12-26 | 株式会社ニコン | Imaging device |
US8520979B2 (en) * | 2008-08-19 | 2013-08-27 | Digimarc Corporation | Methods and systems for content processing |
US8670597B2 (en) * | 2009-08-07 | 2014-03-11 | Google Inc. | Facial recognition with social network aiding |
-
2010
- 2010-08-05 US US12/851,473 patent/US8670597B2/en active Active
- 2010-08-06 JP JP2012523986A patent/JP5557911B2/en not_active Expired - Fee Related
- 2010-08-06 CN CN2010800451932A patent/CN102667763A/en active Pending
- 2010-08-06 EP EP10748165A patent/EP2462522A1/en not_active Ceased
- 2010-08-06 BR BR112012002823-5A patent/BR112012002823B1/en active IP Right Grant
- 2010-08-06 CA CA2770239A patent/CA2770239C/en active Active
- 2010-08-06 KR KR1020167023080A patent/KR101760855B1/en active IP Right Grant
- 2010-08-06 KR KR1020127006118A patent/KR101760853B1/en active IP Right Grant
- 2010-08-06 WO PCT/US2010/044771 patent/WO2011017653A1/en active Application Filing
- 2010-08-06 CN CN201410211070.1A patent/CN104021150B/en active Active
- 2010-08-06 AU AU2010279248A patent/AU2010279248B2/en not_active Ceased
- 2010-08-06 KR KR1020167023079A patent/KR101686613B1/en active IP Right Grant
-
2014
- 2014-02-20 US US14/185,392 patent/US9208177B2/en active Active
- 2014-06-03 JP JP2014114528A patent/JP5985535B2/en active Active
-
2015
- 2015-11-02 US US14/929,958 patent/US10031927B2/en active Active
-
2016
- 2016-08-03 JP JP2016152728A patent/JP6470713B2/en active Active
-
2018
- 2018-07-09 US US16/030,316 patent/US10515114B2/en active Active
- 2018-10-18 JP JP2018196565A patent/JP2019023923A/en active Pending
Patent Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20050097131A1 (en) * | 2003-10-30 | 2005-05-05 | Lucent Technologies Inc. | Network support for caller identification based on biometric measurement |
US20060020630A1 (en) * | 2004-07-23 | 2006-01-26 | Stager Reed R | Facial database methods and systems |
US20090060289A1 (en) * | 2005-09-28 | 2009-03-05 | Alex Shah | Digital Image Search System And Method |
KR20090073294A (en) * | 2007-12-31 | 2009-07-03 | 인하대학교 산학협력단 | Method for social network analysis based on face recognition in an image or image sequences |
Non-Patent Citations (1)
Title |
---|
ZAK STONE ET AL: "Autotagging Facebook:Social Network Context Improves Photo Annotation", 《IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION WORKSHOPS》 * |
Cited By (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN105069083A (en) * | 2015-07-31 | 2015-11-18 | 小米科技有限责任公司 | Determination method and device of associated user |
WO2017020476A1 (en) * | 2015-07-31 | 2017-02-09 | 小米科技有限责任公司 | Method and apparatus for determining associated user |
US9892314B2 (en) | 2015-07-31 | 2018-02-13 | Xiaomi Inc. | Method and device for determining associated user |
CN105069083B (en) * | 2015-07-31 | 2019-03-08 | 小米科技有限责任公司 | The determination method and device of association user |
CN105787023A (en) * | 2016-02-24 | 2016-07-20 | 北京橙鑫数据科技有限公司 | Multi-media file release method and device |
CN105787023B (en) * | 2016-02-24 | 2019-03-26 | 北京橙鑫数据科技有限公司 | The dissemination method and device of multimedia file |
CN109388722A (en) * | 2018-09-30 | 2019-02-26 | 上海碳蓝网络科技有限公司 | It is a kind of for adding or searching the method and apparatus of social connections people |
CN111506825A (en) * | 2020-03-12 | 2020-08-07 | 浙江工业大学 | Visual analysis method for character relationship based on social photos |
CN112270297A (en) * | 2020-11-13 | 2021-01-26 | 杭州睿琪软件有限公司 | Method and computer system for displaying recognition result |
WO2022100352A1 (en) * | 2020-11-13 | 2022-05-19 | 杭州睿琪软件有限公司 | Method and computer system for displaying identification result |
Also Published As
Publication number | Publication date |
---|---|
JP5557911B2 (en) | 2014-07-23 |
EP2462522A1 (en) | 2012-06-13 |
KR101760853B1 (en) | 2017-07-24 |
US20140172881A1 (en) | 2014-06-19 |
US9208177B2 (en) | 2015-12-08 |
CN104021150B (en) | 2017-12-19 |
JP2016201135A (en) | 2016-12-01 |
CA2770239C (en) | 2019-01-22 |
CN102667763A (en) | 2012-09-12 |
KR101760855B1 (en) | 2017-07-24 |
JP5985535B2 (en) | 2016-09-06 |
KR101686613B1 (en) | 2016-12-14 |
US20180322147A1 (en) | 2018-11-08 |
US20160055182A1 (en) | 2016-02-25 |
US20110038512A1 (en) | 2011-02-17 |
KR20160108832A (en) | 2016-09-20 |
US10031927B2 (en) | 2018-07-24 |
BR112012002823B1 (en) | 2021-06-22 |
JP2014194810A (en) | 2014-10-09 |
WO2011017653A1 (en) | 2011-02-10 |
AU2010279248A1 (en) | 2012-03-15 |
US10515114B2 (en) | 2019-12-24 |
CA2770239A1 (en) | 2011-02-10 |
AU2010279248B2 (en) | 2013-07-25 |
KR20120058539A (en) | 2012-06-07 |
JP2019023923A (en) | 2019-02-14 |
KR20160108833A (en) | 2016-09-20 |
JP2013501978A (en) | 2013-01-17 |
JP6470713B2 (en) | 2019-02-13 |
US8670597B2 (en) | 2014-03-11 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US10515114B2 (en) | Facial recognition with social network aiding | |
CN102625937B (en) | Architecture for responding to visual query | |
CN102822817B (en) | For the Search Results of the action taked of virtual query | |
CA2770186C (en) | User interface for presenting search results for multiple regions of a visual query | |
CN107018486B (en) | Method and system for processing visual query | |
CN103493069A (en) | Identifying matching canonical documents in response to a visual query | |
AU2016201546B2 (en) | Facial recognition with social network aiding | |
AU2013245488B2 (en) | Facial recognition with social network aiding |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
C06 | Publication | ||
PB01 | Publication | ||
C10 | Entry into substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant | ||
CP01 | Change in the name or title of a patent holder |
Address after: American CaliforniaPatentee after: Google limited liability companyAddress before: American CaliforniaPatentee before: Google Inc. |
|
CP01 | Change in the name or title of a patent holder |