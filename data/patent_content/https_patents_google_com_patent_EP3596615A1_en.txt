EP3596615A1 - Adaptive interface in a voice-activated network - Google Patents
Adaptive interface in a voice-activated networkInfo
- Publication number
- EP3596615A1 EP3596615A1 EP19712877.0A EP19712877A EP3596615A1 EP 3596615 A1 EP3596615 A1 EP 3596615A1 EP 19712877 A EP19712877 A EP 19712877A EP 3596615 A1 EP3596615 A1 EP 3596615A1
- Authority
- EP
- European Patent Office
- Prior art keywords
- audio signal
- input audio
- interface
- request
- client device
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Withdrawn
Links
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/1815—Semantic context, e.g. disambiguation of the recognition hypotheses based on word meaning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/903—Querying
- G06F16/9032—Query formulation
- G06F16/90332—Natural language query formulation or dialogue systems
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
- G06F3/167—Audio in a user interface, e.g. using voice commands for navigating, audio feedback
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/30—Semantic analysis
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/1822—Parsing for meaning understanding
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/26—Speech to text systems
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/268—Morphological analysis
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/279—Recognition of textual entities
- G06F40/289—Phrasal analysis, e.g. finite state techniques or chunking
- G06F40/295—Named entity recognition
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/223—Execution procedure of a spoken command
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/226—Procedures used during a speech recognition process, e.g. man-machine dialogue using non-speech characteristics
Definitions
- Excessive network transmissions, packet-based or otherwise, of network traffic data between computing devices can prevent a computing device from properly processing the network traffic data, completing an operation related to the network traffic data, or timely responding to the network traffic data.
- the excessive network transmissions of network traffic data can also complicate data routing or degrade the quality of the response if the responding computing device is at or above its processing capacity, which may result in inefficient bandwidth utilization.
- the network transmissions corresponding ambiguous requests can generate numerous unnecessary network traffic between computing devices.
- a system to generate voice-based interfaces in a networked system can include a data processing system.
- the data processing system can execute a natural language processor (NLP) component, an interface management component, and a direct action application programming interface (API).
- NLP natural language processor
- API direct action application programming interface
- the data processing system can receive, at an interface of the data processing system, an input audio signal detected by a sensor of a client device.
- the data processing system can parse the input audio signal to identify a plurality of candidate requests based on the input audio signal.
- the data processing system can determine an interface type of the client device.
- the data processing system can select a portion the plurality of candidate requests based on the interface type of the client device.
- the data processing system can generate an action data structure for each of the portion of the plurality of the candidate requests based on the interface type of the client device.
- the data processing system can transmit, to the client device, the action data structure for each of the portion of the plurality of candidate requests based on the interface type of the client device.
- the interface type may indicate at least one of a display format of the client device, a display size of the client device, a display availability of the client device, or a client device type.
- the interface may transmit, to the client device, the second action data structure for the second candidate request to be rendered after the first action data structure.
- the natural language processor component may parse the input audio signal to identify a request in the input audio signal, determine a confidence score for a semantic meaning of the request, and select the plurality of candidate requests based on the confidence score being below a predetermined threshold.
- the natural language processor component may identify a term in the input audio signal having a plurality of interpretations, and the interface management component may select the subset of the plurality of candidate requests based on the term in the input audio signal having a plurality of interpretations.
- the natural language processor component may parse the input audio signal to identify a primary request in the input audio signal, and the natural language processor component may determine the plurality of candidate requests based on a semantic similarity between the primary request and each of the plurality of candidate requests.
- the natural language processor component may parse the input audio signal to identify a primary request in the input audio signal, and the natural language processor component may select the plurality of candidate requests from a log of previously received input audio signal and based at least on a semantic similarity between each of the plurality of candidate requests and the primary request.
- the direct action application programming interface may include an indication of the plurality of candidate requests in the first action data structure.
- the natural language processor component may parse the input audio signal to identify a request in the input audio signal, and a content selection component, executed by the data processing system, may select a digital component based on the request in the input audio signal.
- the natural language processor component my parse the input audio signal to identify a primary request in the input audio signal, the direct action application programming interface may determine a response to the primary request, the direct action application programming interface may determine a response to each of the plurality of candidate requests, and the interface management component may select the subset of the plurality of candidate requests based on a comparison between the response to the request and the response to each of the plurality of candidate requests.
- the natural language processor component may parse the input audio signal to identify a primary request in the input audio signal, the direct action application programming interface my determine a response to the primary request, the natural language processor component may determine a generality score for the response to the primary request in the input audio signal, the natural language processor component may determine a generality score for a response to each of the plurality of candidate requests, and the interface management component may select the subset of the plurality of candidate requests based on a comparison of the generality score for the response to the request in the input audio signal and the generality score for the response to each of the plurality of candidate requests.
- the interface may transmit an audio signal request comprising a prompt
- the natural language processor component may receive a second input audio signal from the client device, the second input audio signal generated in response to the prompt, and the interface management component may select the subset of the plurality of candidate requests based the second input audio signal.
- the interface management component may select a second client device associated with the client device, the second client device having an interface with an interface type different than the interface type of the client device, and the interface may transmit the second action data structure to the second client device.
- the natural language processor component may receive a second input audio signal detected by the sensor of the client device, the natural language processor component to parse the second input audio signal to identify a candidate request based on the second input audio signal, and the interface may transmit the candidate request to the client device.
- a method to generate voice-based interfaces in a networked system can include receiving, by a natural language processor component executed by a data processing system via an interface, an input audio signal detected by a sensor of a client device.
- the method can include parsing, by the natural language processor component, the input audio signal to identify a plurality of candidate requests based on the input audio signal.
- the method can include determining, by an interface management component executed by the data processing system, an interface type of the client device.
- the method can include selecting, by the interface management component, a number of the plurality of candidate requests based on the interface type of the client device.
- the method can include generating, by a direct action application programming interface of the data processing system, an action data structure for each of the number of the plurality of candidate requests based on the interface type of the client device.
- the method can include transmitting, by the interface to the client device, the action data structure for each of the number of plurality of candidate requests based on the interface type of the client device.
- the interface type may indicate at least one of a display format of the client device, a display size of the client device, a display availability of the client device, or a client device type.
- the method may further comprise transmitting, by the interface, to the client device, the second action data structure for the second candidate request to be rendered after the first action data structure.
- the method may further comprise parsing, by the natural language processor component, the input audio signal to identify a request in the input audio signal, determining, by the natural language processor component, a confidence score for a semantic meaning of the request, and selecting, by the natural language processor component, the plurality of candidate requests based on the confidence score being below a predetermined threshold.
- the method may further comprise identifying, by the natural language processor component, a term in the input audio signal having a plurality of interpretations, and selecting, by the interface management component, the portion of the plurality of candidate requests based on the term in the input audio signal having a plurality of interpretations.
- the method may further comprise parsing, by the natural language processor component, the input audio signal to identify a primary request in the input audio signal, and determining, by the natural language processor component, the plurality of candidate requests based on a semantic similarity between the primary request and each of the plurality of candidate requests.
- the method may further comprise parsing, by the natural language processor component, the input audio signal to identify a primary request in the input audio signal, and selecting, by the natural language processor component, the plurality of candidate requests from a log of previously received input audio signal and based at least on a semantic similarity between each of the plurality of candidate requests and the primary request.
- the method may further comprise including, by the direct action application programming interface, in the first action data structure an indication of the candidate request for which the first action data structure was generated.
- the method may further comprise parsing, by the natural language processor component, the input audio signal to identify a request in the input audio signal, and selecting, by a content selection component, executed by the data processing system, a digital component based on the request in the input audio signal.
- the method may further comprise parsing, by the natural language processor component, the input audio signal to identify a primary request in the input audio signal, determining, by the direct action application programming interface, a response to the primary request, determining, by the direct action application programming interface, a response to each of the plurality of candidate requests, and selecting, by the interface management component, the portion of the plurality of candidate requests based on a comparison between the response to the primary request and the response to each of the plurality of candidate requests.
- the method may further comprise parsing, by the natural language processor component, the input audio signal to identify a primary request in the input audio signal, determining, by the direct action application programming interface, a response to the primary request, determining, by the natural language processor component, a generality score for the response to the primary request in the input audio signal, determining, by the natural language processor component, a generality score for a response to each of the plurality of candidate requests, and selecting, by the interface
- the subset of the plurality of candidate requests based on a comparison of the generality score for the response to the request in the input audio signal and the generality score for the response to each of the plurality of candidate requests.
- the method may further comprise transmitting, by the interface, an audio signal request comprising a prompt, receiving, by the natural language processor component, a second input audio signal from the client device, the second input audio signal generated in response to the prompt, and selecting, by the interface management component, the subset of the plurality of candidate requests based the second input audio signal.
- the method may further comprise selecting, by the interface management component, a second client device associated with the client device, the second client device having an interface with an interface type different than the interface type of the client device, and transmitting, by the interface, the second action data structure to the second client device.
- the method may further comprise receiving, by the natural language processor component, a second input audio signal detected by the sensor of the client device, parsing, by the natural language processor component, the second input audio signal to identify a candidate request based on the second input audio signal, and transmitting, by the interface, the candidate request to the client device.
- FIG. 1 illustrates an example system to generate interfaces in a voice-activated system, in accordance with an example of the present disclosure.
- FIG. 2 illustrates a block diagram of an example method to generate a voice-based interface in a voice-activated system, in accordance with an example of the present disclosure.
- FIGS. 3 and 4 illustrate diagrams of example voice-based interfaces for presenting responses to candidate requests, in accordance with an example of the present disclosure.
- FIG. 5 illustrates a block diagram of an example computer system, in accordance with an example of the present disclosure.
- the systems and methods of the present disclosure generally relate to a data processing system that can identify and generate or surface alternative requests when presented with ambiguous, unclear, or other requests to which a data processing system may not be able to respond.
- the data processing system can improve the efficiency of network transmissions to reduce network bandwidth usage and processor utilization by selecting alternative requests that are responsive to the intent of the original request. Selecting and responding to the alternative requests can save bandwidth by not having to transmit error messages or follow up messages to client devices requesting additional information or data about the original request.
- the data processing system can select which of the additional responses for which the data processing system will generate a response based on an interface type of the client device that transmitted the request to the data processing system.
- the data processing system can select one or a subset of the alternative requests for which it generates responses. Selecting and responding to only a portion of the possible alternative requests can save bandwidth by not transmitting to the client device responses that were generated in response to all the possible interpretations of the original request.
- FIG. 1 illustrates an example system 100 to generate interfaces in a voice-activated system.
- the system 100 can include a digital component selection infrastructure.
- the system 100 can include a data processing system 102.
- the data processing system 102 can communicate with one or more digital component provider devices 106 (e.g., a content provider device) or client computing devices 104 via a network 105.
- the network 105 can include computer networks such as the Internet, local, wide, metro, or other area networks, intranets, satellite networks, and other communication networks such as voice or data mobile telephone networks.
- the network 105 can be used to access information resources such as web pages, web sites, domain names, or uniform resource locators that can be presented, output, rendered, or displayed on at least one computing device 104, such as a laptop, desktop, tablet, digital assistant, personal digital assistant, smartwatch, wearable device, smart phone, portable computers, or speaker.
- a user of the client computing device 104 can access information or data provided by a digital component provider device 106.
- the client computing device 104 may or may not include a display.
- the client computing device 104 may include limited types of user interfaces, such as a microphone and speaker (e.g., the client computing device 104 can include a voice-drive or audio-based interface).
- the primary user interface of the computing device 104 may be a microphone and speaker.
- the client computing device 104 can be speaker-based digital assistant device.
- the network 105 can include or constitute a display network, e.g., a subset of information resources available on the internet that are associated with a content placement or search engine results system, or that are eligible to include third party digital components.
- the network 105 can be used by the data processing system 102 to access information resources such as web pages, web sites, domain names, or uniform resource locators that can be presented, output, rendered, or displayed by the client computing device 104.
- information resources such as web pages, web sites, domain names, or uniform resource locators that can be presented, output, rendered, or displayed by the client computing device 104.
- a user of the client computing device 104 can access information or data provided by the digital component provider device 106.
- the network 105 may be any type or form of network and may include any of the following: a point-to-point network, a broadcast network, a wide area network, a local area network, a telecommunications network, a data communication network, a computer network, an ATM (Asynchronous Transfer Mode) network, a SONET (Synchronous Optical Network) network, a SDH (Synchronous Digital Hierarchy) network, a wireless network and a wire line network.
- the network 105 may include a wireless link, such as an infrared channel or satellite band.
- the topology of the network 105 may include a bus, star, or ring network topology.
- the network may include mobile telephone networks using any protocol or protocols used to communicate among mobile devices, including advanced mobile phone protocol (“AMPS”), time division multiple access (“TDMA”), code-division multiple access (“CDMA”), global system for mobile communication (“GSM”), general packet radio services (“GPRS”) or universal mobile telecommunications system (“UMTS”).
- AMPS advanced mobile phone protocol
- TDMA time division multiple access
- CDMA code-division multiple access
- GSM global system for mobile communication
- GPRS general packet radio services
- UMTS universal mobile telecommunications system
- Different types of data may be transmitted via different protocols, or the same types of data may be transmitted via different protocols.
- the system 100 can include at least one data processing system 102.
- the data processing system 102 can include at least one logic device such as a computing device having a processor to communicate via the network 105, for example, with the computing device 104 or the digital component provider device 106.
- the data processing system 102 can include at least one computation resource, server, processor or memory.
- the data processing system 102 can include a plurality of computation resources or servers located in at least one data center.
- the data processing system 102 can include multiple, logically -grouped servers and facilitate distributed computing techniques.
- the logical group of servers may be referred to as a data center, server farm or a machine farm.
- the servers can also be geographically dispersed.
- a data center or machine farm may be administered as a single entity, or the machine farm can include a plurality of machine farms.
- the servers within each machine farm can be heterogeneous - one or more of the servers or machines can operate according to one or more type of operating system platform.
- Servers in the machine farm can be stored in high-density rack systems, along with associated storage systems, and located in an enterprise data center. For example, consolidating the servers in this way may improve system manageability, data security, the physical security of the system, and system performance by locating servers and high-performance storage systems on localized high-performance networks. Centralization of all or some of the data processing system 102 components, including servers and storage systems, and coupling them with advanced system management tools allows more efficient use of server resources, which saves power and processing requirements and reduces bandwidth usage.
- the client computing device 104 can include, execute, interface, or otherwise communicate with one or more of at least one local digital assistant 134, at least one sensor 138, at least one transducer 140, at least one audio driver 142, or at least one display 144.
- the sensor 138 can include, for example, a camera, an ambient light sensor, proximity sensor, temperature sensor, accelerometer, gyroscope, motion detector, GPS sensor, location sensor, microphone, video, image detection, or touch sensor.
- the transducer 140 can include or be part of a speaker or a microphone.
- the audio driver 142 can provide a software interface to the hardware transducer 140.
- the audio driver 142 can execute the audio file or other instructions provided by the data processing system 102 to control the transducer 140 to generate a corresponding acoustic wave or sound wave.
- the display 144 can include one or more hardware or software component configured to provide a visual indication or optical output, such as a light emitting diode, organic light emitting diode, liquid crystal display, laser, or display.
- the local digital assistant 134 can include or be executed by one or more processors, logic array, or memory.
- the local digital assistant 134 can be a pre-processor.
- the local digital assistant 134 can execute any of the components of the data processing system 102.
- the local digital assistant 134 can detect a keyword and perform an action based on the keyword.
- the local digital assistance 134 can execute instances of the components executed by the data processing system 102 or can perform any of the functions of the data processing system 102.
- the local digital assistant 134 can pre-process input audio signals received by the client computing device 104. For example, the local digital assistant 134 can filter out one or more terms or modify the terms prior to transmitting the terms as data to the data processing system 102 for further processing.
- the local digital assistant 134 can convert the analog audio signals detected by the transducer 140 into a digital audio signal and transmit one or more data packets carrying the digital audio signal to the data processing system 102 via the network 105.
- the local digital assistant 134 can transmit data packets carrying some or all the input audio signal responsive to detecting an instruction to perform such transmission.
- the instruction can include, for example, a trigger keyword or other keyword or approval to transmit data packets comprising the input audio signal to the data processing system 102.
- the local digital assistant 134 can perform a pre-filtering or pre-processing on the input audio signal to remove certain frequencies of audio.
- the pre-filtering can include filters such as a low-pass filter, high-pass filter or a bandpass filter.
- the filters can be applied in the frequency domain.
- the filters can be applied using digital signal processing techniques.
- the filter can be configured to keep frequencies that correspond to a human voice or human speech, while eliminating frequencies that fall outside the typical frequencies of human speech.
- a bandpass filter can be configured to remove frequencies below a first threshold (e.g., 70 Hz, 75 Hz, 80 Hz, 85 Hz, 90 Hz, 95 Hz, 100 Hz, or 105 Hz) and above a second threshold (e.g., 200 Hz, 205 Hz, 210 Hz, 225 Hz, 235 Hz, 245 Hz, or 255 Hz).
- Applying a bandpass filter can reduce computing resource utilization in downstream processing.
- the local digital assistant 134 on the computing device 104 can apply the bandpass filter prior to transmitting the input audio signal to the data processing system 102, thereby reducing network bandwidth utilization.
- the local digital assistant 134 can apply additional pre-processing or pre-filtering techniques such as noise reduction techniques to reduce ambient noise levels that can interfere with natural language processor. Noise reduction techniques can improve accuracy and speed of natural language processor, thereby improving the performance of the data processing system 102 and manage rendering of a graphical user interface provided via the display 144.
- the client computing device 104 can be associated with an end user that enters voice queries as audio input into the client computing device 104 (via the sensor 138 or transducer 140) and receives audio (or other) output from the data processing system 102 or digital component provider device 106 to present, display, or render to the end user of the client computing device 104.
- the digital component can include a computer-generated voice that can be provided from the data processing system 102 or digital component provider device 106 to the client computing device 104.
- the client computing device 104 can render the computer generated voice to the end user via the transducer 140 (e.g., a speaker).
- the computer-generated voice can include recordings from a real person or computer generated language.
- the client computing device 104 can provide visual output via a display device 144 communicatively coupled to the computing device 104.
- the end user that enters the voice queries to the client computing device 104 can be associated with multiple client computing devices 104.
- the end user can be associated with a first client computing device 104 that can be a speaker-based digital assistant device, a second client computing device 104 that can be a mobile device (e.g., a smartphone), and a third client computing device 104 that can be a desktop computer.
- the data processing system 102 can associate each of the client computing devices 104 through a common login, location, network, or other linking data.
- the end user may log into each of the client computing devices 104 with the same account user name and password.
- the client computing device 104 can receive an input audio signal detected by a sensor 138 (e.g., microphone) of the computing device 104.
- the input audio signal can include, for example, a query, question, command, instructions, request or other statement provided in a language.
- the input audio signal can include an identifier or name of a third-party (e.g., a digital component provider device 106) to which the question or request is directed.
- the request can be for content provided by a specific digital component provider device 106.
- the client computing device 104 can include, execute, or be referred to as a digital assistant device.
- the digital assistant device can include one or more components of the computing device 104.
- the digital assistant device can include a graphics driver that can receive display output from the data processing system 102 and render the display output on display 144.
- the graphics driver can include hardware or software components that control or enhance or how graphics or visual output is displayed on the display 144.
- the graphics driver can include, for example, a program that controls how the graphic components work with the rest of the computing device 104 (or digital assistant).
- the local digital assistant 134 can filter the input audio signal to create a filtered input audio signal, convert the filtered input audio signal to data packets, and transmit the data packets to a data processing system comprising one or more processors and memory.
- the digital assistant device can include an audio driver 142 and a speaker component (e.g., transducer 140).
- the pre-processor component may receive an indication of the display output and instruct the audio driver 142 to generate an output audio signal to cause the speaker component (e.g., transducer 140) to transmit an audio output corresponding to the indication of the display output.
- the system 100 can include, access, or otherwise interact with at least digital component provider device 106.
- the digital component provider device 106 can include one or more servers that can provide digital components to the client computing device 104 or data processing system 102.
- the digital component provider device 106 or components thereof can be integrated with the data processing system 102 or executed at least partially by the data processing system 102.
- the digital component provider device 106 can include at least one logic device such as a computing device having a processor to communicate via the network 105, for example with the computing device 104, the data processing system 102, or the digital component provider device 106.
- the digital component provider device 106 can include at least one computation resource, server, processor or memory.
- the digital component provider device 106 can include a plurality of computation resources or servers located in at least one data center.
- a digital component provider device 106 can provide audio, visual, or multimedia based digital components for presentation by the client computing device 104 as an audio output digital component, visual output digital components, or a mix thereof.
- the digital components can be incorporated into action data structures that are transmitted to the client computing device 104 and rendered by the client computing device 104.
- the digital component can be or include a digital content.
- the digital component can be or include a digital object.
- the digital component can include subscription-based content or pay-for content.
- a digital component can include a plurality of digital components. For example, a digital component can include the text answering a question present by the user in a request.
- the client computing device 104 can process the text into an audio output signal.
- the digital components can include or can be digital movies, websites, songs, applications (e.g., smartphone or other client device applications), or other text- based, audio-based, image-based, or video-based content.
- the digital content provider device 106 can provide digital components generated by the digital content provider device 106, uploaded by users, or sources from other digital content provider devices 106.
- the digital component provider device 106 can provide the digital components to the client computing device 104 via the network 105 and bypass the data processing system 102.
- the digital component provider device 106 can provide the digital component to the client computing device 104 via the network 105 and data processing system 102.
- the digital component provider device 106 can provide the digital components to the data processing system 102, which can store the digital components and provide the digital components to the client computing device 104 when requested by the client computing device 104.
- the data processing system 102 can include at least one computation resource or server.
- the data processing system 102 can include, interface, or otherwise communicate with at least one interface 110.
- the data processing system 102 can include, interface, or otherwise communicate with at least one natural language processor component 114.
- the data processing system 102 can include, interface, or otherwise communicate with at least one digital component selector 120.
- the data processing system 102 can include, interface, or otherwise communicate with at least one interface management component 135.
- the data processing system 102 can include, interface, or otherwise communicate with at least one data repository 124.
- the at least one data repository 124 can include or store, in one or more data structures or databases, logs of past requests 128, templates 130, and content data 132.
- the data repository 124 can include one or more local or distributed databases.
- the interface 110, the natural language processor component 114, the digital component selector 120, and the interface management component 135 can each include at least one processing unit or other logic device such as programmable logic array engine, or module configured to communicate with the database repository or database 124.
- the interface 110, the natural language processor component 114, the digital component selector 120, interface management component 135, and the data repository 124 can be separate components, a single component, or part of multiple data processing systems 102.
- the system 100 and its components, such as a data processing system 102 can include hardware elements, such as one or more processors, logic devices, or circuits.
- the data processing system 102 can include an interface 110.
- the interface 110 can be configured, constructed, or operational to receive and transmit information using, for example, data packets.
- the interface 110 can receive and transmit information using one or more protocols, such as a network protocol.
- the interface 110 can include a hardware interface, software interface, wired interface, or wireless interface.
- the interface 110 can facilitate translating or formatting data from one format to another format.
- the interface 110 can include an application programming interface that includes definitions for communicating between various components, such as software components.
- the data processing system 102 can include an application, script or program installed at the client computing device 104, such as a local digital assistant 134 to communicate input audio signals to the interface 110 of the data processing system 102 and to drive components of the client computing device to render output audio signals or visual output.
- the data processing system 102 can receive data packets, a digital file, or other signals that include or identify an input audio signal (or input audio signals).
- the computing device 104 can detect the audio signal via the transducer 140 and convert the analog audio signal to a digital file via an analog-to-digital converter.
- the audio driver 142 can include an analog-to-digital converter component.
- the pre-processor component can convert the audio signals to a digital file that can be transmitted via data packets over network 105.
- the data processing system 102 can execute or run an NLP component 114 to receive or obtain the data packets including the input audio signal detected by the sensor 138 of the computing device 104.
- the client computing device 104 can also execute an instance of the client computing device 104 to process language and text at the client computing device 104.
- the data packets can provide a digital file.
- the NLP component 114 can receive or obtain the digital file or data packets comprising the audio signal and parse the audio signal.
- the NLP component 114 can provide for interactions between a human and a computer.
- the NLP component 114 can be configured with techniques for understanding natural language and enabling the data processing system 102 to derive meaning from human or natural language input.
- the NLP component 114 can include or be configured with techniques based on machine learning, such as statistical machine learning.
- the NLP component 114 can utilize decision trees, statistical models, or probabilistic models to parse the input audio signal.
- the NLP component 114 can perform, for example, functions such as named entity recognition (e.g., given a stream of text, determine which items in the text map to proper names, such as people or places, and what the type of each such name is, such as person, location, or organization), natural language generation (e.g., convert information from computer databases or semantic intents into understandable human language), natural language understanding (e.g., convert text into more formal representations such as first-order logic structures that a computer module can manipulate), machine translation (e.g., automatically translate text from one human language to another), morphological segmentation (e.g., separating words into individual morphemes and identify the class of the morphemes, which can be challenging based on the complexity of the morphology or structure of the words of the language being considered), question answering (e.g., determining an answer to a human-language question, which can be specific or open-ended), semantic processing (e.g., processing that can occur after identifying a word and encoding its meaning in order
- the NLP component 114 can identify semantic representations of the identified words. By identifying semantic representations, the data processing system can match words or phrases based on their similar semantic meanings rather than specific word matches. For example, a search of an input request based on semantic representations can return the related requests.
- the NLP component 114 can convert the input audio signal into recognized text by comparing the input signal against a stored, representative set of audio waveforms (e.g., in the data repository 124) and choosing the closest matches.
- the set of audio waveforms can be stored in data repository 124 or other database accessible to the data processing system 102.
- the representative waveforms are generated across a large set of users, and then may be augmented with speech samples from the user.
- the NLP component 114 matches the text to words that are associated, for example via training across users or through manual specification, with actions that the data processing system 102 can serve.
- the NLP component 114 can convert image or video input to text or digital files.
- the NLP component 114 can detect the speech in a video file, convert the speech into text, and then process the text.
- the NLP component 114 can convert
- the NLP component 114 can process, analyze or interpret image or video input to perform actions, generate requests, or select or identify data structures.
- the data processing system 102 can receive image or video input signals, in addition to, or instead of, input audio signals.
- the data processing system 102 can process the image or video input signals using, for example, image interpretation techniques, computer vision, a machine learning engine, or other techniques to recognize or interpret the image or video to convert the image or video to a digital file.
- image interpretation techniques, computer vision techniques, machine learning techniques can be collectively referred to as imaging techniques.
- the data processing system 102 e.g., the NLP component 114) can be configured with the imaging techniques, in addition to, or instead of, audio processing techniques.
- the NLP component 114 can obtain the input audio signal. From the input audio signal, the NLP component 114 can identify at least one request or at least one trigger keyword corresponding to the request.
- the request can indicate intent, digital components, or subject matter of the input audio signal.
- the trigger keyword can indicate a type of action likely to be taken.
- the NLP component 114 can parse the input audio signal to identify at least one request for the current weather in at a specific location.
- the request may be an express request or an implied request. For example, the request“is it going to rain today” can be an express request for an indication of whether it will rain. The request“do I need an umbrella” can be an implied request for an indication of whether it will rain.
- the NLP component 114 can parse the input audio signal to identify, determine, retrieve, or otherwise obtain a primary request from the input audio signal. For instance, the NLP component 114 can apply a semantic processing technique to the input audio signal to identify requests in the input audio signal.
- the natural language processor component 114 can identify candidate requests based on the input audio signal. For example, the natural language processor component 114 can identify a primary request in the input audio signal. The primary request can be ambiguous or unclear. The primary request can be ambiguous or unclear if the primary request has a plurality of possible responses. The primary request can be ambiguous or unclear if the quality of the input audio signal is poor and the natural language processor component 114 cannot process one or more of the terms in the input audio signal.
- the NLP component 114 can determine the candidate requests based on a log of previously received input audio signals.
- the data processing system 102 can log previously identified requests from the previously received input audio signals.
- the candidate requests can be logged requests that are semantically similar to the primary request identified in the input audio signal.
- the NLP component 114 can rank the logged requests based on the semantic similarity between the primary request and the logged requests. For example, each of the logged requests can be one-hot encoded and converted into a vector space.
- the primary request can be one-hot encoded and converted into the vector space.
- the similarity between the primary request and the log requests can be based on the distance between the primary request and the logged requests in the vector space.
- the similarity between the primary request and the log requests can be based on a Pearson correlation between the primary request and the logged requests.
- the data processing system 102 can execute or run an instance of the direct action API 112.
- the direct action API 112 can identify, select, or generate an action data structure for fulfilling a request (or candidate request) identified in the input audio signal. From the request or the trigger keyword the direct action API 112 predicts, estimates, or otherwise determines subject matter for the action data structures.
- the action data structures can include digital components, text, video, images, or other content that can be rendered by the client computing device 104 in response to transmitting an input audio signal to the data processing system 102.
- the action data structures and content items can correspond to subject matter of the input audio signal.
- the direct action API 112 can generate a specified action to satisfy the end user’s intention, primary request, or candidate requests, as determined by the NLP component 114.
- the direct action API 112 can execute code or a dialog script that identifies the parameters required to fulfill a user request. Such code can look up additional information, in the data repository 124 or transmit the action data structure (or a request generated therefrom) to a third party device to provide data to the data processing system 102 for inclusion in the action data structure.
- the direct action API 112 can generate a search phrase that is transmitted to a search engine.
- the response from the search engine can be included in a response field of the action data structure.
- the search phrase or the request from the input audio signal can be included in an input field of the action data structure.
- the direct action API 112 can determine necessary parameters and can package the information into an action data structure, which can then be sent to another component such as the digital component selector component 120 or to the agent of a service provider computing device to be fulfilled.
- the direct action API 112 can transmit the primary request and the candidate requests to a service provider or third-party, which can return a populated action data structure in response to receiving the request.
- a weather agent when provided with a location, can return an action data structure that indicates the weather, such as ⁇ loc:94035;
- the data processing system 102 can execute or run an instance of the interface management component 135.
- the interface management component 135 can poll, determine, identify, or select interfaces for rendering of the action data structures and of the digital components.
- the interface management component 135 can identify the interface of the client computing device 104.
- the interface management component 135 can identify one or more interfaces associated with the client computing device 104 or associated client computing devices 104.
- the client computing device 104 can be associated with one or more additional client computing devices 104.
- the client computing device 104 and additional client computing devices 104 can be associated with one another through a common application login, login, end user, or other identifier.
- the end user of the client computing device 104 can log into an application installed on the client computing device 104 and the additional client computing devices 104.
- the application can be associated with or provided by the data processing system 102. Logging into the application with the same credentials can enable the data processing system 102 to link the client computing devices 104 together in the data repository 124.
- the interface management component 135 can identify the interface type of the client computing device 104. Identifying the interface type can include determining capabilities of the client computing device’s interfaces. For example, the interface management component 135 can determine whether the client computing device 104 includes a display 144, an audio driver 142 (e.g., a speaker), or a combination thereof. Identifying or determining the interface type can include determining a device type of the client computing device 104. For example, the interface management component 135 can determine if the client computing device 104 is a smartphone, laptop, desktop computer, speaker-based assistant device, or other type of computing device. Identifying interface type can include determining display screen parameters (e.g., size of the display, orientation of the display, resolution of the display); applications, user agent, content, or digital components displayed on the interface or executed by the client computing device 104; or audio parameters.
- display screen parameters e.g., size of the display, orientation of the display, resolution of the display
- the interface management component 135 can poll the client computing device 104 to determine the interface type of the client computing device 104.
- the interface management component 135 can poll the client computing device 104 by transmitting a message to the client computing device 104 that determines the display screen parameters and returns the data to the interface management component 135.
- the interface management component 135 can transmit a digital component to the client computing device 104 that includes a client- executable script (e.g., JavaScript) that can determine the resolution of the display 144 and transmit the resolution data to the interface management component 135.
- the interface management component 135 can poll the client computing device 104 at regular intervals (e.g., responsive to receiving an input audio signal from the client computing device 104) to determine the interface type of the client computing device 104.
- the interface management component 135 can poll the client computing device 104 once during a registration phase. Once registered with the data processing system 102, the data processing system 102 can save the interface type associated with the client computing device 104 in the data repository 124.
- the interface management component 135 can retrieve the interface type from the input audio signal.
- the client computing device 104 can include metadata with the input audio signal that indicates the interface type of the client computing device 104.
- the interface management component 135 can process the metadata to extract the interface type of the client computing device 104.
- the interface management component 135 can determine which of the plurality of candidate requests to respond to.
- the data processing system can respond to a portion or subset of the identified plurality of candidate requests.
- the interface management component 135 can select a number (e.g., a portion or subset) of the plurality of candidate requests in response to the input audio signal.
- the interface management component 135 can select the number of candidate requests based on the interface type of the client computing device 104. For example, the interface management component 135 can select relatively more candidate requests for a client computing device 104 with a relatively larger display.
- the interface management component 135 can determine the available space on the client computing device’ display 144 for displaying responses (e.g., rendered action data structures). The interface management component 135 can select how many candidate requests to respond to based on the number of responses that will fit within available space of the display 144.
- the interface management component 135 can select the number of the plurality of candidate requests based on the natural language processor component 114 determining that a term or phrase in the input audio signal can have multiple interpretations or possible responses. For example, an input audio signal that includes the phrase“Ok, how long will it take to get to work” can have multiple interpretations.
- the data processing system 102 can determine a first response based on whether the user intends to drive, a second response based on whether the user intends to take public transit, and a third response based on whether the user intends to walk.
- a request can have multiple interpretations when the request generates multiple responses or when one or more fields of the request’s action data structure are left empty enabling multiple responses.
- the action data structure for the above phrase can be (start location:
- the data processing system can generate candidate requests and associated action data structures.
- Each action data structure for the different candidate requests can include a different method of transportation in the method field.
- the interface management component 135 can determine the number of the plurality of candidate requests based on a comparison between a response to a request parsed from the input audio signal and the responses to each of the plurality of candidate requests generated based on the input audio signal. For example, the natural language processor component 114 can identify from the input audio signal the request“Ok, how long will it take to get to work by car.” The natural language processor component 114 can determine candidate requests that are related to the identified request.
- the natural language processor component 114 can determine candidate requests, such as“how long will it take to get to work by public transit” and“how long will it take to get to work by bike.”
- the response to the request may be“35 minutes by car.”
- the response to the candidate requests can be“15 minutes by the subway” and“40 minutes by bike.”
- the interface management component 135 can select the responses that are substantially different from the response to the request identified in the input audio signal.
- the interface management component 135 can select, in addition to the response“35 minutes by car,” the response“15 minutes by subway” because the response (e.g., the transit time) is substantially different.
- the difference or similarity between the responses can be determined by machine learning or with a neural network. For example, the text of each response can be converted into a vector.
- the distance between the responses in the vector space can indicate the similarity (or difference) between each of the responses.
- Vectors closer to one another can be ranked as more similar when compared to vectors spaced farther apart.
- the natural language processor component 114 can generate similarity scores by generating a word vector for each of the words within a response.
- the natural language processor component 114 can use a continuous bag-of-words neural network model or a skip-gram neural network model to generate vector representations of the words in the response.
- the natural language processor component 114 can use Word2Vec to generate the word vectors.
- the interface management component 135 can determine the number of the plurality of candidate requests based on a generality score for each candidate requests (or a response thereto). For example, the natural language processor component 114 can identify a request with a generality score above a predetermined threshold. The generality score can be based on the number of possible responses the data processing system 102 can return in response to an input audio signal. For example, the request“what is the top speed of a 2015 Brand A car?” has a low generality score because there is substantially only one result for this request. The request“what is the top speed of a car?” can have a relatively high generality score because multiple responses could be returned. For example, the data processing system 102 could return different responses for different car categories, manufactures, or car configurations.
- the interface management component 135 can select a greater number of candidate requests to which the data processing system 102 provides responses to the client computing device 104.
- the interface management component 135 can determine the number of the candidate requests to which the data processing system 102 provides responses based on the interface type of a second client computing device 104.
- the second client computing device 104 can be related to the client computing device 104 that transmitted the input audio signal to the data processing system 102.
- the client computing device 104 can be the end user’s smartphone and the second client computing device 104 can be the end user’s laptop computer.
- the second client computing device 104 can have a different interface type than the client computing device 104 that transmitted the input audio signal to the data processing system 102.
- the user’s smartphone can have a display of a first size and resolution and the user’s laptop can have a display of a second size and resolution.
- the interface management component 135 can transmit a portion of the responses to the candidate requests to the client computing device 104 and the remaining portion of the responses to the candidate request to the second client computing device 104.
- the digital component selector 120 can select a digital component that includes text, strings, characters, video files, image files, or audio files that can be processed by the client computing device 104 and presented to the user via the display 144 or the transducer 140 (e.g., speaker).
- the digital component selector 120 can select a digital component that is responsive to the request identified by the NLP component 114 in the input audio signal.
- the digital component selector 120 can select supplemental digital components that can also be provided with a primary digital component.
- the primary digital component can be digital component directly selected responsive to a request or candidate request.
- the primary digital component can include an answer to a question presented in the request.
- the supplemental digital components can be additional digital components that provide additional information or are related to the primary digital component.
- the digital component selector 120 can select which digital component provider device 106 should or can fulfill the request and can forward the request to the digital component provider device 106.
- the data processing system 102 can initiate a session between the digital component provider device 106 and the client computing device 104 to enable the digital component provider device 106 to transmit the digital component to the client computing device 104.
- the digital component selector 120 can request digital component from the digital component provider device 106.
- the digital component provider device 106 can provide digital components to the data processing system 102, which can store the digital components in the data repository 124. Responsive to a request for a digital component, the digital component selector 120 can retrieve the digital component from the data repository 124. In response to a request for a digital component, the digital component selector 120 can select a portion or all of a digital component to provide the client computing device 104 in response to the request.
- the digital component selector 120 can select multiple digital components via a real- time content selection process.
- the digital component selector 120 can score and rank the digital components and provide multiple digital components to inclusion in an action data structure, or more generally, for transmission to the client computing device 104.
- the digital component selector 120 can select one or more additional digital components that are transmitted to a client computing device 104 based on an input audio signal (or keywords and requests contained therein).
- the input audio signal can include a request to start a streaming how-to video.
- the digital component selector 120 can select additional digital components (e.g., ads).
- the additional digital components can inform an end user of additional or related digital component provider devices 106 that could fulfill the request from the first client computing device 104.
- the digital component selector 120 can provide the selected digital component selected in response to the request identified in the input audio signal to the computing device 104 or local digital assistant 134 or application executing on the computing device 104 for presentation.
- the digital component selector 120 can receive the content request from the client computing device 104, select, responsive to the content request, a digital component, and transmit, to the client computing device 104, the digital component for presentation.
- the digital component selector 120 can transmit, to the local digital assistant 134, the selected digital component for presentation by the local digital assistant 134 itself or a third-party application executed by the client computing device 104.
- the local digital assistant 134 can play or output an audio signal corresponding to the selected digital component.
- the data repository 124 stores content data 132 that can include, for example, digital components provided by a digital component provider device 106 or obtained or determined by the data processing system 102 to facilitate content selection.
- the content data 132 can include, for example, digital components (or digital component object) that can include, for example, a digital component, an online document, audio, images, video, multimedia content, or third-party content.
- the digital component provider device 106 can provide full-length digital components to the data processing system 102 to store as content data 132.
- the digital component provider device 106 can provide portions of the digital components to the data processing system 102.
- the data repository 124 can store templates 130.
- the templates 130 can be templates of action data structures.
- the templates 130 can include fields that the direct action API 112 can populate when fulfilling a request.
- the templates can include standardized fields that the direct action API 112 or third party can populate when completing or responding to a request.
- the data repository 124 can store past requests 128.
- the past requests 128 can be past requests that are received by the data processing system 102 in input audio signal or other input signals.
- the past requests can be parsed from the input signals by the natural language processor component 114.
- the past requests 128 can be a log of past requests.
- the past requests 128 can be a database of requests.
- the database can include the text of the past requests.
- the database can include a vectorization of the past requests.
- the vector of each past request can be one-hot encoded. The vectors can be used to determine the semantic similarity between the past request and a current request.
- FIG. 2 illustrates a block diagram of an example method 200 to generate a voice-based interface in a voice-activated system.
- the method 200 can include receiving an input signal (ACT 202).
- the method 200 can include parsing the input signal (ACT 204).
- the method 200 can include determining an interface type (ACT 206).
- the method 200 can include selecting candidate requests (ACT 208).
- the method 200 can include generating action data structures (ACT 210).
- the method 200 can include transmitting the action data structures (ACT 212).
- the method 200 can include receiving an input signal (ACT 202).
- the method 200 can include receiving, by a natural language processor component executed by a data processing system, the input signal.
- the input signal can be an input audio signal that is detected by a sensor at a first client device.
- the sensor can be a microphone of the first client device.
- a digital assistant component executed at least partially by a data processing system that includes one or more processors and memory can receive the input audio signal.
- the input audio signal can include a conversation facilitated by a digital assistant.
- the conversation can include one or more inputs and outputs.
- the conversation can be audio based, text based, or a combination of audio and text.
- the input audio signal can include text input, or other types of input that can provide conversational information.
- the data processing system can receive the audio input for a session corresponding to the conversation.
- the data processing system can receive the audio input in one or more portions or as a bulk or batch upload (e.g., multiple portions of the conversations uploaded in a single transmission to reduce the number of transmissions).
- the method 200 can include parsing the input signal (ACT 204).
- the NLP component of the data processing system can parse the input signal to identify a plurality of candidate requests based on the input audio signal. Each of the plurality of candidate requests can be based on a different semantic meaning or interpretation of the input signal.
- the data processing system can identify a primary request in the input audio signal.
- the candidate requests can be based on the primary request.
- the candidate requests can be based on a term or phrase identified by the NLP component in the primary request having a plurality of interpretations. Each of the candidate requests can correspond with the primary request with the term or phrase interpreted in each of the possible interpretations.
- the data processing system can generate the candidate requests“what’s the time in Greenville, NH” and“what’s the time in Greenville, SC”.
- the data processing system can generate or select the candidate requests based on a semantic similarity between the primary request and each of the candidate requests. For example, the data processing system can search a log file of past requests. The data processing system can calculate a distance between the past requests and the primary request in a vector space. The data processing system can select the top 1, 3, 5, 10, or more past responses that are the most similar with the primary response (e.g., closest to the primary response in the vector space).
- the NLP component can determine a confidence score of the primary request identified in the input audio signal.
- the NLP can determine a confidence score based on a semantic meaning of the primary request.
- the confidence score can indicate if the primary request is ambiguous or unclear. For example, if the primary request (or a term therein) has a plurality of interpretations the confidence score of the primary request’s semantic meaning can be low.
- a low confidence score can indicate that the primary request can have multiple interpretations, multiple possible responses, is a broad or generic request, or that the request does not include enough information to provide a response.
- a request can have multiple or different semantic meanings if a term in the request has multiple interpretations.
- the term can have multiple interpretations because the term has multiple definitions or because the term is a homonym or homophone (e.g., different words that have the same pronunciation). For example, the user requests information on“genes,” but in an audio-based interface, the data processing system can interpret“genes” as“genes” or“jeans.” A request can have multiple or different semantic meanings because the request lacks information, context, or other data. For example, the user can request the top speed of a bird without specifying what type of bird. A term can be broad when the term is a genus (e.g., a higher hierarchy) term rather than a species term. For example,
- Corolla (c) is a species of the broader genus term, car.
- the NLP component can determine to select a plurality of candidate requests when the confidence score is below a predetermined threshold, which indicates that the primary request has multiple semantic meanings.
- the method 200 can include determining an interface type (ACT 206).
- the data processing system can determine the interface type of the client computing device that transmitted the input audio signal to the data processing system.
- the interface type can indicate at least one of a display format of the client device, a display size of the client device, a display availability of the client device, or a client device type.
- the data processing system can poll the client computing device to determine the interface type.
- the data processing system can transmit a message to the client computing device that includes processor executable instructions that, when executed by the client computing device, determine the interface type of the client computing device.
- the client computing device can transmit an indication of the interface type to the data processing system when transmitting the input audio signal to the data processing system.
- the client computing device can include an indication of the client computing device’s interface type in the metadata or as a parameter of the input audio signal.
- the method 200 can include selecting candidate requests (ACT 208).
- the data processing system can select a subset of the candidate requests that the data processing system will transmit responses or action data structures to the client computing device.
- the subset of the candidate requests can be one or more of the candidate requests.
- the data processing system can select all the candidate requests.
- the number of candidate requests selected can be based on the interface type of the client computing device.
- the number of candidate requests selected can be based on a term or phrase in a primary request having a plurality of interpretations. For example, the data processing system can select the number of candidate requests that corresponds to the number of interpretations the term or phrase has.
- the data processing system can select the number of candidate requests based on a semantic similarity between a primary request and one or more past requests that are stored in a log of past requests.
- the data processing system can select the number of candidate requests (or the portion of the candidate requests to which the data processing system will respond) based on the interface type of the client computing device.
- FIG. 3 illustrates an example client computing device 104 displaying a response to a single candidate request.
- FIG. 4 illustrates an example client computing device 104 displaying a response to a plurality of candidate requests.
- the client computing device 104 is a smartphone.
- the client computing device 104 provided an input signal.
- the input signal can be an audio-based signal or a text-based signal.
- the text 302 of the input signal can be rendered on the display 144 of the client computing device 104.
- the data processing system 102 can parse the text 302 of the input signal to determine the input signal includes the primary request“how long does it take to get to work.”
- the data processing system can determine the primary request can include a plurality of candidate requests.
- the candidate requests can be“how long does it take to get to work by car,”“how long does it take to get to work by public transit,” or“how long does it take to get to work by bike.”
- the interface type can indicate that the display 144 is a relatively small display.
- the data processing system can select a single candidate request.
- the data processing system can generate an action data structure 304 that includes a text-based response (or portion) 306 and an image-based response (or portion) 308.
- the text-based response 306 can include text that is displayed or spoken to the user.
- the image-based response 308 can include images, videos, or other digital components that are displayed to the user.
- the action data structure can include an indication of the candidate request for which the action data structure was generated. For example, the text-based response 306 indicates that the data processing system returned a response to the candidate request“how long does it take to get to work by public transit.”
- FIG. 4 illustrates an example where the client computing device 104 displays a plurality of responses to different candidate requests.
- the client computing device 104 is a tablet and can have a relatively larger display 144 when compared to the client computing device 104 illustrated in FIG. 3.
- the client computing device 104 provided an input signal.
- the input signal can be an audio-based signal or a text-based signal.
- the text 302 of the input signal can be rendered on the display 144 of the client computing device 104.
- the NLP component can determine that the primary request“how fast do birds fly” is generic because the request can return a plurality of possible responses. For example, the speed of the bird can depend on the type of bird on which the user intended to request the information.
- the data processing system can determine the primary request is generic or broad. In response to receiving a generic or broad primary request, the data processing system can transmit a message comprising a prompt.
- the message can be an audio signal request.
- the user can respond to the prompt.
- the client computing device 104 can capture the response as a second input audio signal that is transmitted to the data processing system 102.
- the data processing system can select one or more of the candidate responses (or generate new candidate responses) based on the response identified in the second input audio signal.
- the data processing system can generate a plurality of candidate responses, which can include“how fast can bird A fly,”“how fast can bird B fly,” and“how fast can bird C fly.”
- the data processing system can generate additional candidate requests.
- the interface type e.g., the client computing device 104 having a relatively larger screen
- the data processing system can select to generate responses for three of the candidate requests.
- the data processing system can generate an action data structure for each of the selected candidate requests.
- the action data structures can include an indication of the candidate request in response to which they were generated.
- FIG. 4 illustrates that data processing system generates three action data structures, which the client computing device 104 renders as cards 404(1), 404(2), and 404(3), which can generally be referred to as cards 404.
- the cards 404 can include individual responses to each of the candidate requests.
- the cards 404 can include text, images, videos, audio, or other forms of content or digital components.
- the cards 404 can be arranged in a carousel such that the user can swipe or navigate between the cards 404 to view each of the cards 404.
- the user can select one or more of the cards 404 to activate the card 404.
- Activating the card 404 can cause the user interface to display additional information associated with the card 404 or begin to play or render media (e.g., video or audio files) associated with the card 404.
- the method 200 can include generating action data structures (ACT 210).
- the data processing system can generate an action data structure for each of selected candidate requests.
- the action data structure for each candidate request can include responses that include digital components, video-based content, text-based content, audio-based content, or other types of content items.
- the action data structure can be sent to a third-party server or the data processing system can receive data from the third-party server to fill one or more fields of the action data structure before the action data structure is transmitted to the client device.
- the content of the action data structure can be rendered by the client computing device and displayed to the user.
- Each action data structure can include a response to a respective one of the candidate requests.
- the method 200 can include transmitting the action data structures (ACT 212).
- the data processing system can transmit the generated action data structures to the client computing device via an interface of the data processing system.
- the data processing system can transmit action data structures to a client device related to the client computing device.
- the data processing system can generate action data structures for one or more of the unselect candidate requests, which can be referred to as additional requests.
- the action data structures for the additional requests can be transmitted to the second client device.
- the client device can be a smart phone.
- the data processing system can select one candidate request (e.g., the highest ranked candidate request) for which an action data structure is generated as a response and transmitted to the client device.
- the data processing system can also generate action data structures for the next five (or other number) of candidate requests.
- the action data structures can be transmitted to the user’s laptop computer.
- a notification can be sent to the smartphone indicating that additional information and possible responses are viewable on the user’s laptop.
- the data processing system can transmit the action data structures to the client computing device to be displayed in series or in parallel with one another. For example, the data processing system can select a first and a second candidate request from a plurality of candidate request. The data processing system can generate a first action data structure for the first candidate request and a second action data structure for the second candidate request. The data processing system can transmit the first and second action data structures so that the action data structures are rendered together on the display as responses. The data processing system can transmit the first and the second action data structures to the client computing device such that the action data structures are rendered in series. For example, the first action data structure can be rendered as a first possible response to the user’s input signal.
- Rendering the first action data structure can cause the client computing device to display the result associated with the first candidate request and an indication of the first candidate request (e.g., the text of the first candidate request).
- the user can clear the rendered first action data structure. For example, the user can swipe the card including the response off the screen.
- the client computing device can then render the second action data structure.
- the data processing system can cause the action data structures to be rendered in series based on the interface type of the client computing device. For example, on the relatively smaller screen of a smart phone, the action data structures can be rendered in series.
- the data processing system can also transmit the first and the second action data structures to the client computing device separately.
- FIG. 5 illustrates a block diagram of an example computer system 500.
- the computer system or computing device 500 can include or be used to implement the system 100, or its components such as the data processing system 102.
- the data processing system 102 can include an intelligent personal assistant or voice-based digital assistant.
- the computing system 500 includes a bus 505 or other communication component for communicating information and a processor 510 or processing circuit coupled to the bus 505 for processing information.
- the computing system 500 can also include one or more processors 510 or processing circuits coupled to the bus for processing information.
- the computing system 500 also includes main memory 515, such as a random access memory (RAM) or other dynamic storage device, coupled to the bus 505 for storing information, and instructions to be executed by the processor 510.
- the main memory 515 can be or include the data repository 124.
- the main memory 515 can also be used for storing position information, temporary variables, or other intermediate information during execution of instructions by the processor 510.
- the computing system 500 may further include a read-only memory (ROM) 520 or other static storage device coupled to the bus 505 for storing static information and instructions for the processor 510.
- ROM read-only memory
- a storage device 525 such as a solid-state device, magnetic disk or optical disk, can be coupled to the bus 505 to persistently store information and instructions.
- the storage device 525 can include or be part of the data repository 124.
- the computing system 500 may be coupled via the bus 505 to a display 535, such as a liquid crystal display, or active matrix display, for displaying information to a user.
- a display 535 such as a liquid crystal display, or active matrix display
- An input device 530 such as a keyboard including alphanumeric and other keys, may be coupled to the bus 505 for communicating information and command selections to the processor 510.
- the input device 530 can include a touch screen display 535.
- the input device 530 can also include a cursor control, such as a mouse, a trackball, or cursor direction keys, for communicating direction information and command selections to the processor 510 and for controlling cursor movement on the display 535.
- the display 535 can be part of the data processing system 102, the client computing device 104 or other component of FIG. 1, for example.
- the processes, systems and methods described herein can be implemented by the computing system 500 in response to the processor 510 executing an arrangement of instructions contained in main memory 515. Such instructions can be read into main memory 515 from another computer-readable medium, such as the storage device 525. Execution of the arrangement of instructions contained in main memory 515 causes the computing system 500 to perform the illustrative processes described herein. One or more processors in a multi-processing arrangement may also be employed to execute the instructions contained in main memory 515. Hard-wired circuitry can be used in place of or in combination with software instructions together with the systems and methods described herein. Systems and methods described herein are not limited to any specific combination of hardware circuitry and software.
- the users may be provided with an opportunity to control whether programs or features that may collect personal information (e.g., information about a user’s social network, social actions or activities, a user’s preferences, or a user’s location), or to control whether or how to receive content from a content server or other data processing system that may be more relevant to the user.
- personal information e.g., information about a user’s social network, social actions or activities, a user’s preferences, or a user’s location
- certain data may be anonymized in one or more ways before it is stored or used, so that personally identifiable information is removed when generating parameters.
- a user’s identity may be anonymized so that no personally identifiable information can be determined for the user, or a user’s geographic location may be generalized where location information is obtained (such as to a city, postal code, or state level), so that a particular location of a user cannot be determined.
- location information such as to a city, postal code, or state level
- the user may have control over how information is collected about him or her and used by the content server.
- the subject matter described in this specification can be implemented as one or more computer programs, e.g., one or more circuits of computer program instructions, encoded on one or more computer storage media for execution by, or to control the operation of, data processing apparatuses.
- the program instructions can be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.
- a computer storage medium can be, or be included in, a computer-readable storage device, a computer-readable storage substrate, a random or serial access memory array or device, or a combination of one or more of them. While a computer storage medium is not a propagated signal, a computer storage medium can be a source or destination of computer program instructions encoded in an artificially generated propagated signal. The computer storage medium can also be, or be included in, one or more separate components or media (e.g., multiple CDs, disks, or other storage devices).
- the operations described in this specification can be implemented as operations performed by a data processing apparatus on data stored on one or more computer-readable storage devices or received from other sources.
- the terms“data processing system”“computing device”“component” or“data processing apparatus” encompass various apparatuses, devices, and machines for processing data, including by way of example a programmable processor, a computer, a system on a chip, or multiple ones, or combinations of the foregoing.
- the apparatus can include special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- the apparatus can also include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, a cross-platform run time environment, a virtual machine, or a combination of one or more of them.
- the apparatus and execution environment can realize various computing model infrastructures, such as web services, distributed computing and grid computing infrastructures.
- the interface 110, digital component selector 120, NLP component 114, the interface management component 135, and other data processing system components can include or share one or more data processing apparatuses, systems, computing devices, or processors.
- a computer program also known as a program, software, software application, app, script, or code
- a computer program can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment.
- a computer program can correspond to a file in a file system.
- a computer program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code).
- a computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- the processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs (e.g., components of the data processing system 102) to perform actions by operating on input data and generating output.
- the processes and logic flows can also be performed by, and apparatuses can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- Devices suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks.
- semiconductor memory devices e.g., EPROM, EEPROM, and flash memory devices
- magnetic disks e.g., internal hard disks or removable disks
- magneto optical disks e.g., CD ROM and DVD-ROM disks.
- the processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- the subject matter described herein can be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface or a web browser through which a user can interact with an
- the components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network.
- Examples of communication networks include a local area network (“LAN”) and a wide area network (“WAN”), an inter-network (e.g., the Internet), and peer-to-peer networks (e.g., ad hoc peer-to-peer networks).
- the computing system such as system 100 or system 500 can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network (e.g., the network 105).
- the relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- a server transmits data (e.g., data packets representing a digital component) to a client device (e.g., for purposes of displaying data to and receiving user input from a user interacting with the client device).
- Data generated at the client device e.g., a result of the user interaction
- can be received from the client device at the server e.g., received by the data processing system 102 from the client computing device 104).
- the separation of various system components does not require separation in all implementations, and the described program components can be included in a single hardware or software product.
- the NLP component 114 or interface management component 135, can be a single component, app, or program, or a logic device having one or more processing circuits, or part of one or more servers of the data processing system 102.
- systems and methods described herein consist of one, each combination of more than one, or all the described elements, acts, or components.
- references to implementations or elements or acts of the systems and methods herein referred to in the singular may also embrace implementations including a plurality of these elements, and any references in plural to any implementation or element or act herein may also embrace implementations including only a single element.
- References in the singular or plural form are not intended to limit the presently disclosed systems or methods, their components, acts, or elements to single or plural configurations.
- References to any act or element being based on any information, act or element may include implementations where the act or element is based at least in part on any information, act, or element.
- implementation may be included in at least one implementation or embodiment. Such terms as used herein are not necessarily all referring to the same implementation. Any implementation may be combined with any other implementation, inclusively or exclusively, in any manner consistent with the aspects and implementations disclosed herein.
- references to“or” may be construed as inclusive so that any terms described using“or” may indicate any of a single, more than one, and all the described terms.
- a reference to“at least one of‘A’ and‘B’” can include only‘A’, only‘B’, as well as both‘A’ and ‘B’.
- Such references used in conjunction with“comprising” or other open terminology can include additional items.
- the computing device 104 can generate the packaged data object and forward it to the third-party application when launching the application.
- the foregoing implementations are illustrative rather than limiting of the described systems and methods. Scope of the systems and methods described herein is thus indicated by the appended claims, rather than the foregoing description, and changes that come within the meaning and range of equivalency of the claims are embraced therein.
Abstract
Description
Claims
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/977,699 US11087748B2 (en) | 2018-05-11 | 2018-05-11 | Adaptive interface in a voice-activated network |
PCT/US2019/021387 WO2019216980A1 (en) | 2018-05-11 | 2019-03-08 | Adaptive interface in a voice-activated network |
Publications (1)
Publication Number | Publication Date |
---|---|
EP3596615A1 true EP3596615A1 (en) | 2020-01-22 |
Family
ID=65895059
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
EP19712877.0A Withdrawn EP3596615A1 (en) | 2018-05-11 | 2019-03-08 | Adaptive interface in a voice-activated network |
Country Status (3)
Country | Link |
---|---|
US (5) | US11087748B2 (en) |
EP (1) | EP3596615A1 (en) |
WO (1) | WO2019216980A1 (en) |
Families Citing this family (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
KR102375800B1 (en) * | 2017-04-28 | 2022-03-17 | 삼성전자주식회사 | electronic device providing speech recognition service and method thereof |
US20200067760A1 (en) * | 2018-08-21 | 2020-02-27 | Vocollect, Inc. | Methods, systems, and apparatuses for identifying connected electronic devices |
US10831999B2 (en) * | 2019-02-26 | 2020-11-10 | International Business Machines Corporation | Translation of ticket for resolution |
US11922193B2 (en) | 2020-02-28 | 2024-03-05 | Google Llc | Interface and mode selection for digital action execution |
US11907676B1 (en) * | 2020-08-28 | 2024-02-20 | Amazon Technologies, Inc. | Processing orchestration for systems including distributed components |
Family Cites Families (37)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5583761A (en) * | 1993-10-13 | 1996-12-10 | Kt International, Inc. | Method for automatic displaying program presentations in different languages |
US6839667B2 (en) * | 2001-05-16 | 2005-01-04 | International Business Machines Corporation | Method of speech recognition by presenting N-best word candidates |
US6963834B2 (en) * | 2001-05-29 | 2005-11-08 | International Business Machines Corporation | Method of speech recognition using empirically determined word candidates |
US8010357B2 (en) * | 2004-03-02 | 2011-08-30 | At&T Intellectual Property Ii, L.P. | Combining active and semi-supervised learning for spoken language understanding |
KR100679044B1 (en) * | 2005-03-07 | 2007-02-06 | 삼성전자주식회사 | Method and apparatus for speech recognition |
GB0505941D0 (en) * | 2005-03-23 | 2005-04-27 | Patel Sanjay | Human-to-mobile interfaces |
US8041570B2 (en) * | 2005-05-31 | 2011-10-18 | Robert Bosch Corporation | Dialogue management using scripts |
US20110060587A1 (en) * | 2007-03-07 | 2011-03-10 | Phillips Michael S | Command and control utilizing ancillary information in a mobile voice-to-speech application |
US8600152B2 (en) * | 2009-10-26 | 2013-12-03 | Ancestry.Com Operations Inc. | Devices, systems and methods for transcription suggestions and completions |
US8626511B2 (en) | 2010-01-22 | 2014-01-07 | Google Inc. | Multi-dimensional disambiguation of voice commands |
US9292203B2 (en) * | 2012-05-10 | 2016-03-22 | Apple Inc. | Providing a vertical candidate bar with an on-screen keyboard |
US20130317805A1 (en) * | 2012-05-24 | 2013-11-28 | Google Inc. | Systems and methods for detecting real names in different languages |
BR112014028434A2 (en) * | 2012-12-07 | 2017-06-27 | Yota Devices Ipr Ltd | Method for authenticating data entry for a device driver and electronic device |
US10735552B2 (en) | 2013-01-31 | 2020-08-04 | Google Llc | Secondary transmissions of packetized data |
US9110889B2 (en) * | 2013-04-23 | 2015-08-18 | Facebook, Inc. | Methods and systems for generation of flexible sentences in a social networking system |
US11218434B2 (en) | 2013-06-12 | 2022-01-04 | Google Llc | Audio data packet status determination |
CN105474743B (en) * | 2013-09-18 | 2019-05-10 | 华为技术有限公司 | The method for building up and device of the interface of access point |
US10713416B2 (en) * | 2013-09-30 | 2020-07-14 | Echostar Ukraine, L.L.C. | Systems, devices and methods for font size selection |
US10741182B2 (en) * | 2014-02-18 | 2020-08-11 | Lenovo (Singapore) Pte. Ltd. | Voice input correction using non-audio based input |
US10216855B2 (en) * | 2014-06-26 | 2019-02-26 | International Business Machines Corporation | Mobilizing an existing web application |
US9741342B2 (en) * | 2014-11-26 | 2017-08-22 | Panasonic Intellectual Property Corporation Of America | Method and apparatus for recognizing speech by lip reading |
US10255101B2 (en) * | 2014-12-11 | 2019-04-09 | Sap Se | Device emulator |
CN107111648A (en) * | 2015-01-09 | 2017-08-29 | 索尼公司 | Information processing system, information processor, control method and program |
US10002449B2 (en) * | 2015-04-16 | 2018-06-19 | Sap Se | Responsive and adaptive chart controls |
US20170092278A1 (en) | 2015-09-30 | 2017-03-30 | Apple Inc. | Speaker recognition |
US9747926B2 (en) | 2015-10-16 | 2017-08-29 | Google Inc. | Hotword recognition |
US9928840B2 (en) | 2015-10-16 | 2018-03-27 | Google Llc | Hotword recognition |
US10691473B2 (en) | 2015-11-06 | 2020-06-23 | Apple Inc. | Intelligent automated assistant in a messaging environment |
US20170285765A1 (en) * | 2016-03-29 | 2017-10-05 | Seiko Epson Corporation | Input apparatus, input method, and computer program |
US10192552B2 (en) | 2016-06-10 | 2019-01-29 | Apple Inc. | Digital assistant providing whispered speech |
US10438264B1 (en) * | 2016-08-31 | 2019-10-08 | Amazon Technologies, Inc. | Artificial intelligence feature extraction service for products |
GB201619724D0 (en) * | 2016-11-22 | 2017-01-04 | Microsoft Technology Licensing Llc | Trained data input system |
CN108153800B (en) * | 2016-12-06 | 2023-05-23 | 松下知识产权经营株式会社 | Information processing method, information processing apparatus, and recording medium |
US11030515B2 (en) * | 2016-12-30 | 2021-06-08 | Google Llc | Determining semantically diverse responses for providing as suggestions for inclusion in electronic communications |
US11204787B2 (en) * | 2017-01-09 | 2021-12-21 | Apple Inc. | Application integration with a digital assistant |
US10754441B2 (en) * | 2017-04-26 | 2020-08-25 | Microsoft Technology Licensing, Llc | Text input system using evidence from corrections |
US10997258B2 (en) * | 2018-02-28 | 2021-05-04 | Fujitsu Limited | Bot networks |
-
2018
- 2018-05-11 US US15/977,699 patent/US11087748B2/en active Active
-
2019
- 2019-03-08 EP EP19712877.0A patent/EP3596615A1/en not_active Withdrawn
- 2019-03-08 WO PCT/US2019/021387 patent/WO2019216980A1/en unknown
-
2020
- 2020-03-25 US US16/829,786 patent/US11282510B2/en active Active
-
2021
- 2021-08-09 US US17/397,533 patent/US11848009B2/en active Active
-
2022
- 2022-03-21 US US17/699,580 patent/US11908462B2/en active Active
-
2023
- 2023-11-16 US US18/511,517 patent/US20240087560A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
US11282510B2 (en) | 2022-03-22 |
US11087748B2 (en) | 2021-08-10 |
CN110720098A (en) | 2020-01-21 |
US20240087560A1 (en) | 2024-03-14 |
US20190348028A1 (en) | 2019-11-14 |
US11848009B2 (en) | 2023-12-19 |
US20220208183A1 (en) | 2022-06-30 |
US20210366469A1 (en) | 2021-11-25 |
US20200227031A1 (en) | 2020-07-16 |
US11908462B2 (en) | 2024-02-20 |
WO2019216980A1 (en) | 2019-11-14 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11908462B2 (en) | Adaptive interface in a voice-activated network | |
KR102603717B1 (en) | Generation of domain-specific models in networked systems | |
US11514907B2 (en) | Activation of remote devices in a networked system | |
US11776536B2 (en) | Multi-modal interface in a voice-activated network | |
CN110720098B (en) | Adaptive interface in voice activated networks | |
CN111213136B (en) | Generation of domain-specific models in networked systems | |
WO2023003537A1 (en) | Bit vector-based content matching for third-party digital assistant actions |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: UNKNOWN |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: THE INTERNATIONAL PUBLICATION HAS BEEN MADE |
|
PUAI | Public reference made under article 153(3) epc to a published international application that has entered the european phase |
Free format text: ORIGINAL CODE: 0009012 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: REQUEST FOR EXAMINATION WAS MADE |
|
17P | Request for examination filed |
Effective date: 20191015 |
|
AK | Designated contracting states |
Kind code of ref document: A1Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
AX | Request for extension of the european patent |
Extension state: BA ME |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: EXAMINATION IS IN PROGRESS |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: EXAMINATION IS IN PROGRESS |
|
17Q | First examination report despatched |
Effective date: 20201214 |
|
DAV | Request for validation of the european patent (deleted) | ||
DAX | Request for extension of the european patent (deleted) | ||
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: THE APPLICATION HAS BEEN WITHDRAWN |
|
18W | Application withdrawn |
Effective date: 20230124 |
|
P01 | Opt-out of the competence of the unified patent court (upc) registered |
Effective date: 20230519 |