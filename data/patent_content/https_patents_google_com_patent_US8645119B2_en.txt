US8645119B2 - Minimum error rate training with a large number of features for machine learning - Google Patents
Minimum error rate training with a large number of features for machine learning Download PDFInfo
- Publication number
- US8645119B2 US8645119B2 US12/056,083 US5608308A US8645119B2 US 8645119 B2 US8645119 B2 US 8645119B2 US 5608308 A US5608308 A US 5608308A US 8645119 B2 US8645119 B2 US 8645119B2
- Authority
- US
- United States
- Prior art keywords
- feature
- function
- feature functions
- source sentence
- functions
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/40—Processing or translation of natural language
- G06F40/42—Data-driven translation
- G06F40/49—Data-driven translation using very large corpora, e.g. the web
Definitions
- This specification relates to machine learning.
- Machine translation attempts to identify a most probable translation in a target language given a particular input in a source language. For example, when translating a sentence from French to English, statistical machine translation identifies the most probable English sentence given the French sentence.
- MERT Minimum Error Rate Training
- MERT Machine translation
- NIST National Institute of Standards and Technology
- BLEU Bilingual Evaluation Understudy
- the MERT technique directly optimizes the objective function of interest and thereby avoids making approximations of other objective functions for example, likelihood or margin.
- the MERT technique is generally efficient for training model parameters (i.e., weights) for only a relatively small number of feature functions (e.g., less than 20 or 30 feature functions).
- the MERT technique is slow if a large number of feature functions are considered, because only one feature function is updated at a time and the computation involves iterating over the complete training corpus.
- the MERT technique tends to assign most of the weight to one of the correlated features, causing instability. Instability in the MERT technique occurs when different values of the initial weights result in very different final weights.
- a method includes determining model parameters for a plurality of feature functions for a linear machine learning model, ranking the plurality of feature functions according to a quality criterion, and selecting, using the ranking, a group of feature functions from the plurality of feature functions to update with the determined model parameters.
- Implementations can include one or more of the following features. Determining model parameters for the plurality of feature functions can further include, for each feature function in the plurality of feature functions: calculating a source sentence error surface for each source sentence of a plurality of source sentences as a function of feature function model parameter, merging the source sentence error surfaces into an aggregate error surface for the feature function, and identifying an optimal model parameter for the feature function that minimizes the aggregate error surface for the feature function.
- the quality criterion can be BLEU score gain.
- Selecting, using the ranking, the group of feature functions to update with the determined model parameters can further include, for each source sentence in a plurality of source sentences, calculating a source sentence error surface as a function of number of updates for ranked feature functions, merging all source sentence error surfaces into an aggregate error surface, and identifying an optimal number of updates for ranked feature functions that minimizes the aggregate error surface. Selecting, using the ranking, the group of feature functions to update with the determined model parameters can further include selecting the group of feature functions to include a particular feature function if updating the particular feature function with the respective optimal model parameter does not increase an error count.
- the linear machine learning model can be a linear statistical machine translation model.
- a method in one aspect, includes determining a group of candidate translations for each source sentence in a plurality of source sentences, and, for one or more iterations: calculating a first aggregate error surface and an optimal model parameter for each feature function in a plurality of feature functions for a linear statistical machine translation model, ranking the plurality of feature functions according to a quality criterion, calculating a second aggregate error surface and an optimal number of updates for ranked feature functions, determining a group of feature functions from the plurality of feature functions using the optimal number of updates for ranked feature functions, where the group of feature functions includes a particular feature function if updating the particular feature function with the respective optimal model parameter does not increase an error count, and updating each feature function of the group of feature functions with the corresponding optimal model parameter.
- Calculating the first aggregate error surface and the optimal model parameter for each feature function can further include, for each feature function in the plurality of feature functions: for each source sentence in the plurality of source sentences: calculating a minimum cost surface as a function of feature function model parameter, and calculating a source sentence error surface using the minimum cost surface, merging the source sentence error surfaces for each source sentence into the first aggregate error surface for the feature function, and identifying the optimal model parameter for the feature function that minimizes the first aggregate error surface for the feature function.
- the quality criterion can be BLEU score gain.
- Calculating the second aggregate error surface and the optimal number of updates for ranked feature functions can further include, for each source sentence in the plurality of source sentences: calculating a minimum cost surface as a function of number of updates for ranked feature functions, and calculating a source sentence error surface using the minimum cost surface, merging all source sentence error surfaces into the second aggregate error surface, and identifying the optimal number of updates for ranked feature functions that minimizes the second aggregate error surface.
- the aspect can further include recalculating the second aggregate error surface and the optimal number of updates for ranked feature functions using the determined group of feature functions. Updating with the optimal model parameters a group of feature functions can further include updating with the optimal model parameters reduced in step size.
- the first aggregate error surface and the optimal model parameter for each feature function can be calculated using a first training corpus, and the second aggregate error surface and the optimal number of updates for ranked feature functions can be calculated using a second training corpus.
- Calculating the first aggregate error surface and the optimal model parameter for each feature function can further include calculating the first aggregate error surface and the optimal model parameter for each feature function in parallel across a plurality of machines.
- Calculating the second aggregate error surface and the optimal number of updates for ranked feature functions can further include calculating the second aggregate error surface and the optimal number of updates for ranked feature functions in parallel across a plurality of machines.
- the MERT technique is extended to scale to an arbitrary number (e.g., millions) of features and an arbitrary number (e.g., millions) of training examples.
- a translation system can efficiently calculate the effect of updating increasing groups of model parameters essentially simultaneously. Modifying step size in updates to model parameters can reduce overfitting to training data. This technique is easy to parallelize efficiently over many machines and provides solid improvements in BLEU score over previous techniques.
- FIG. 1 is an example of a minimum cost surface for a model parameter for a single source sentence.
- FIG. 2 is an example of a source sentence error surface for a single source sentence corresponding to the example minimum cost surface of FIG. 1 .
- FIG. 3 is an example of an aggregate error surface over all source sentences, showing the optimal model parameter.
- FIG. 4 is an example of a minimum cost surface for the number of updates for a single source sentence.
- FIG. 5 is an example of a source sentence error surface for a single source sentence corresponding to the example minimum cost surface of FIG. 4 .
- FIG. 6 is an example of an aggregate error surface over all source sentences, showing the optimal number of updates.
- FIG. 7 is an example of training corpus BLEU scores for the top ranked 50 features for three iterations of batch updating, where feature decorrelation filtering follow batch updating in iterations 1 and 2.
- FIG. 8 is an example of the training corpus BLEU scores of FIG. 7 for the top ranked million features for five iterations of batch updating, where feature decorrelation filtering follow batch updating in iterations 1 through 4.
- FIG. 9 is an example of training corpus BLEU scores for one through five iterations of the MERT technique, each with five iterations of batch updating and feature decorrelation filtering.
- FIG. 10 shows an example process for selecting a group of feature functions to update with determined model parameters.
- FIG. 11 shows an example process for minimum error rate training with batch updating and feature decorrelation filtering.
- FIG. 12 is a schematic diagram of an example computer system.
- MERT machine translation
- the MERT technique and an extension to the MERT technique can be used in speech recognition, optical character recognition, search ranking, and advertisement targeting, for example.
- the application determines the type of objective function for which parameter training is needed. For example, word error rate (e.g., how many words have been recognized correctly) can be used for speech recognition, and dialog success rate (e.g., how many dialogs have been handled successfully) can be used for dialog systems.
- word error rate e.g., how many words have been recognized correctly
- dialog success rate e.g., how many dialogs have been handled successfully
- the MERT technique and an extension to the MERT technique will be described below as applied to statistical machine translation.
- An extension to the MERT technique allows a large number of features of a linear statistical machine translation model to be trained on a large number of training examples by optimizing the linear model with respect to an arbitrary error function.
- a phrase-based statistical machine translation system can use the technique to train millions of lexicalized language model features (e.g., lexical n-gram features) to improve the BLEU score.
- BLEU is a method for evaluating the quality of text which has been translated from one natural language to another using machine translation. The BLEU score provides a measure of the statistical closeness of machine translations to reference translations.
- An n-gram is a sequence of n consecutive words.
- An n-gram has an order, which is the number of words in the n-gram. For example, a 1-gram (or unigram) includes one word; a 2-gram (or bigram) includes two words.
- a translation system uses the technique to train other forms of language model features, e.g., long-distance language model features, phrase table features, or syntactic features.
- the model parameter ⁇ m has a default value of zero.
- the translation system identifies as ê(f; ⁇ 1 M ) the target sentence e (e.g., where e is in a group C of multiple target sentences) for which the cost, as defined by Eqn. 1, has the smallest value.
- the modeling problem includes developing suitable feature functions h 1 M that capture the relevant properties of the translation task.
- the training problem focuses on identifying suitable model parameter values ⁇ 1 M .
- a single error count is typically insufficient to calculate corpus-wide scores (i.e., scores calculated across a representative corpus of source sentences) for common metrics including, for example, a BLEU score or F-Measure.
- corpus-wide scores i.e., scores calculated across a representative corpus of source sentences
- common metrics including, for example, a BLEU score or F-Measure.
- the BLEU score is described, for example, in Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu, “BLEU: a Method for Automatic Evaluation of Machine Translation,” Proceedings of the 40th Annual Meeting on the Association for Computational Linguistics, pages 311-318, July 2002.
- the BLEU score provides a geometric mean of the ratio of matching n-grams of length one to four between a candidate translation and a group of reference translations, along with a length term penalizing short sentences.
- the sufficient statistics of the BLEU score are the number of matching n-grams (i.e., n-gram precisions for the group of reference translations), the candidate translation length, and the effective length of the reference translations of the group.
- the ⁇ (ê(f s ; ⁇ 1 M ), e s,k function of Eqn. 2 is the Kronecker delta function, which is equal to 1 when ê(f s ; ⁇ 1 M ) is equal to e s,k and 0 otherwise.
- the translation system obtains the optimal parameter values by minimizing the sum of the errors over all source sentences in the representative corpus:
- This optimization criterion is computationally difficult as the objective function has a large number of local optima, is piecewise constant, and does not allow the computation of a gradient.
- the MERT technique can be the basis for the extension technique described in further detail below.
- the MERT technique trains parameters for a linear statistical machine translation model directly with respect to an automatic evaluation criterion (e.g., the BLEU score) that measures translation quality.
- an automatic evaluation criterion e.g., the BLEU score
- a globally optimal value for each model parameter ⁇ m is identified while holding all other model parameters fixed.
- Each corresponding feature function h m (e,f) is updated greedily in turn (i.e., by applying the optimal value as a model parameter for the particular feature function h m (e,f) without regard to other feature functions).
- the MERT technique includes several steps: calculating a minimum cost surface function for each source sentence f s of a representative corpus; calculating an error surface E s ( ⁇ m ) for each source sentence f s of the representative corpus; calculating an aggregate error surface E( ⁇ m ) across all source sentences f 1 S of the representative corpus; and identifying a globally optimal model parameter ⁇ circumflex over ( ⁇ ) ⁇ m , which minimizes the aggregate error surface E( ⁇ m ).
- K(e,f) ⁇ m′ ⁇ m ⁇ m′ h m′ , (e,f), which corresponds to the weighted feature function sum excluding the feature m that is being optimized. Therefore, K(e,f) is a constant with respect to ⁇ m .
- Every candidate translation e ⁇ C corresponds to a line with slope h m (e,f), as illustrated in the example 100 of FIG. 1 .
- the minimum cost surface f(f; ⁇ m ) 120 for the model parameter ⁇ m for a source sentence f s has the following functional form:
- f ⁇ ( f ; ⁇ m ) min e ⁇ C ⁇ ⁇ K ⁇ ( e , f ) + ⁇ m ⁇ h m ⁇ ( e , f ) ⁇ ( Eqn . ⁇ 5 )
- the minimum cost surface f(f; ⁇ m ) 120 illustrated as the bold line in FIG. 1 , is piecewise linear, where each piece corresponds to the particular candidate translation e with the lowest cost at the corresponding value of the model parameter ⁇ m .
- the example 100 of FIG. 1 represents a stack of candidate translation cost plots 110 , where each cost plot 110 is for a particular source sentence f s from the group of source sentences f 1 s of the representative corpus.
- the translation system calculates the respective minimum cost surface f(f; ⁇ m ) 120 for each source sentence f s .
- each candidate translation e ⁇ C has an associated error count function defined by E s (r,e).
- the translation system calculates the error surface (i.e., an error count) for each candidate translation e in the minimum cost surface f(f; ⁇ m ) 120 .
- This error surface is the source sentence error surface E s ( ⁇ m ) as a function of ⁇ m , which defines the error count of the minimum cost surface f(f; ⁇ m ) 120 at every possible value of ⁇ m .
- the example 200 of FIG. 2 represents a stack of source sentence error surface E s ( ⁇ m ) plots 210 , where each error surface plot 210 is for a particular source sentence f s from the group of source sentences f 1 s .
- the translation system aggregates error counts while traversing in parallel the source sentence error surfaces E s ( ⁇ m )
- the translation system identifies the globally optimal model parameter ⁇ m , which minimizes the aggregate error surface E( ⁇ m ). That is,
- ⁇ ⁇ m arg ⁇ ⁇ min ⁇ m ⁇ ⁇ E ⁇ ( ⁇ m ) ⁇ .
- the model parameter ⁇ m for the feature function h m (e,f) can then be updated to the identified optimal parameter value ⁇ circumflex over ( ⁇ ) ⁇ m .
- the overall MERT optimization technique therefore includes the following steps:
- ERROR SURFACE CALCULATION For each source sentence f s , calculate the piecewise linear minimum cost surface f(f; ⁇ m ) and its associated source sentence error surface E s ( ⁇ m ) as functions of ⁇ m .
- the MERT technique is generally efficient for only a relatively small number of features and does not scale well to a large number of features, because only one feature is updated at a time and the computation involves iterating over the complete training corpus.
- an extension of the MERT technique allows the effect of updating increasing groups of features (i.e., batch updates) to be efficiently calculated at once. Further efficiencies are gained by parallelizing the MERT technique and the extension technique, including the efficient batch updates, over many machines.
- FIG. 10 shows an example process 1000 for selecting a group of feature functions to update with determined model parameters.
- the example process 1000 illustrates one technique for extending the MERT technique for batch updating a large number of feature functions. For convenience, the example process 1000 will be described with reference to a translation system that performs the process 1000 .
- a translation system determines model parameters for multiple feature functions for a linear machine learning model (step 1010 ).
- the linear machine learning model is a linear statistical machine translation model.
- the model parameters can be determined using the MERT technique described above.
- the translation system ranks the multiple feature functions according to a quality criterion (step 1020 ).
- the translation system selects, using the ranking, a group of feature functions from the multiple feature functions to update with the determined model parameters (step 1030 ).
- the group of feature functions does not include all of the multiple feature functions. Ranking of the feature functions and selection of the group of feature functions will be described in more detail below.
- FIG. 11 shows an example process 1100 for minimum error rate training with batch updating and feature decorrelation filtering.
- the example process 1100 will be described with reference to FIGS. 4-6 and a translation system that performs the process 1100 .
- the translation system determines a group of candidate translations for each source sentence of multiple source sentences (step 1110 ).
- the translation system can use a decoder to apply a language model (e.g., a syntactic language model) and a translation model (e.g., word alignment or phrase-based translation) to the respective source sentence in order to determine each candidate translation in the group of candidate translations.
- a language model e.g., a syntactic language model
- a translation model e.g., word alignment or phrase-based translation
- the decoder can determine the candidate sentence e, that maximizes the product of P(e) (i.e., the probability of e) determined by the language model and P(f
- the translation system calculates a first aggregate error surface and an optimal model parameter for each feature function of multiple feature functions for a linear statistical machine translation model (step 1120 ).
- the first aggregate error surfaces and the optimal model parameters can be calculated using the MERT technique.
- the translation system ranks the plurality of feature functions according to a quality criterion (step 1130 ).
- the MERT technique identifies an optimal parameter value ⁇ circumflex over ( ⁇ ) ⁇ m for each feature function h m .
- the aggregate error surface E( ⁇ circumflex over ( ⁇ ) ⁇ m ) at the optimal parameter value ⁇ circumflex over ( ⁇ ) ⁇ m is a measure of the quality of the corresponding feature function h m .
- the translation system can rank the feature functions h 1 M by quality, for example, by the gain in the evaluation metric (e.g., a gain in BLEU score). For the following analysis, it is assumed that the feature functions h 1 M are sorted according to quality, such that E( ⁇ circumflex over ( ⁇ ) ⁇ m ) ⁇ E( ⁇ circumflex over ( ⁇ ) ⁇ m ⁇ 1 ).
- the translation system determines which subgroup of feature function updates results in a minimal error count.
- the problem can be simplified by using the quality ranking of the feature functions and restricting the considered subgroups to the M subgroups ordered by quality: ⁇ h 1 ⁇ , ⁇ h 1 , h 2 ⁇ , . . . , ⁇ h 1 , . . . , h M ⁇
- each candidate translation e ⁇ C corresponds to a line when cost is plotted as a function of ⁇ m .
- each candidate translation e ⁇ C corresponds to a piecewise constant surface, as illustrated in the example 400 of FIG. 4 , when cost is plotted as a function of m.
- the cost of a particular candidate translation e for a number of updates m is the cost if the feature functions h 1 m are updated by applying the corresponding optimal model parameters ⁇ circumflex over ( ⁇ ) ⁇ 1 m . This allows the effect of updating multiple subgroups of feature functions (i.e., ⁇ h 1 ⁇ , ⁇ h 1 , h 2 ⁇ , . . . , ⁇ h 1 , . . . , h M ⁇ ) to be efficiently calculated at once for comparison.
- the minimum cost surface f(f; m) 420 for the number of updates m for a source sentence f s is defined by the function
- Each section of the piecewise constant minimum cost surface f (f; m) 420 corresponds to the cost of the candidate translation e with the lowest cost for the particular number of updates m.
- the example 400 of FIG. 4 represents a stack of candidate translation cost plots 410 , where each cost plot 410 is for a particular source sentence f s from the group of source sentences f 1 s of the representative corpus. The translation system calculates the respective minimum cost surface f (f; m) 420 for each source sentence f s .
- Each candidate translation e ⁇ C has an associated error count function defined by E s (r,e).
- E s (r,e) the error count function
- the translation system can obtain the error surface (i.e., an error count) for each candidate translation e ⁇ C in the minimum cost surface f (f; m) 420 , as illustrated in the example 500 of FIG. 5 .
- This error surface is the source sentence error surface E s (m), which defines the error count of the minimum cost surface f (f; m) 420 at the values of m.
- the example 500 of FIG. 5 represents a stack of source sentence error surface E s (m) plots 510 , where each error surface plot 510 is for a particular source sentence f s from the group of source sentences f 1 s .
- the translation system calculates source sentence error surfaces E s (m) for all source sentences f 1 s of the representative corpus.
- the translation system calculates a second aggregate error surface and an optimal number of ranked feature function updates (step 1140 ).
- the example 600 of FIG. 6 illustrates the aggregate error surface E(m) as a function of m calculated across the stack of source sentence error surface E s (m) plots 510 .
- the translation system identifies the optimal number of updates m (i.e., updates 1, . . . , ⁇ circumflex over (m) ⁇ ), which minimizes the aggregate error surface E(m). That is,
- m ⁇ arg ⁇ ⁇ min m ⁇ ⁇ E ⁇ ( m ) ⁇ .
- the translation system determines a group of feature functions from the multiple feature functions using the optimal number of ranked feature function updates (step 1150 ).
- the translation system updates each feature function of the group of feature functions with the corresponding optimal model parameter (step 1160 ).
- the translation system applies the optimal parameter values ⁇ circumflex over ( ⁇ ) ⁇ 1 ⁇ circumflex over (m) ⁇ to update the corresponding feature functions in the determined subgroup ⁇ h 1 , . . . , h ⁇ circumflex over (m) ⁇ ⁇ while retaining the present values ⁇ m for all feature functions not in the subgroup ⁇ h 1 , . . . , h ⁇ circumflex over (m) ⁇ .
- the translation system repeats step 1120 through step 1160 of example process 1100 if multiple iterations are to be performed (decision 1170 ).
- the number of iterations can be determined using a threshold, e.g., a convergence criterion or a minimum gain in the evaluation metric.
- the efficient batch update technique therefore includes the following steps:
- ERROR SURFACE CALCULATION For each source sentence f s , calculate the piecewise constant minimum cost surface f(f; m) 420 and its associated source sentence error surface E s m) as functions of m.
- step 1 the translation system only processes the complete group of N ⁇ S candidate translations once. Additionally, for each candidate translation e ⁇ C, the translation system only iterates through all non-zero optimal parameter values ⁇ circumflex over ( ⁇ ) ⁇ m . In step 2, the translation system iterates through the non-trivial decision boundaries of the S sentence-specific error surfaces E s (m).
- the translation system can efficiently calculate the impact of updating millions of feature functions, problems can exist if there are correlated features. Correlated features are common in machine learning problems. For example, strong correlations can occur in translation systems using individual n-grams as features. Strong correlations can be expected between n-grams that subsume or are subsumed by each other. For example, the effects of updating the features “of” and “of the” by applying the identified optimal parameters values ⁇ circumflex over ( ⁇ ) ⁇ m are expected to be highly correlated, which suggests that it might be better not to update the features together.
- the translation system avoids applying the optimal parameter value ⁇ circumflex over ( ⁇ ) ⁇ m (i.e., the feature weight) to update a feature if the update leads to an increase in the error count. Instead, the optimal parameter value ⁇ circumflex over ( ⁇ ) ⁇ m for the detrimental feature is not applied (i.e., the feature model parameter remains at its present value ⁇ m ).
- the translation system can then repeat the steps (i.e., run another iteration) of the batch update technique to produce a new aggregate error surface E(m) without including the updates to the detrimental features. Each iteration of this filtering step typically reduces the resulting error count.
- Table 1 and FIGS. 7-8 illustrate an example of this feature decorrelation filtering for a translation system using the BLEU score as an evaluation metric. Because the BLEU score is the evaluation metric, the goal is to maximize the score as opposed to minimizing an error count (e.g., an aggregate error surface).
- an error count e.g., an aggregate error surface
- Table 1 illustrates an example of the top seven n-gram features ranked by gain in the BLEU score, where the gain in the BLEU score is calculated under the assumption that the translation system updates each feature individually. As mentioned above, the effects of updating the second feature (“of”) and the third feature (“of the”) are expected to be highly correlated.
- FIG. 7 illustrates an example 700 of the training corpus BLEU scores for the top ranked 50 features for three iterations of batch updating, where feature decorrelation filtering follows batch updating in iterations 1 and 2.
- iteration 1 there is a sharp drop in the BLEU score after applying the update for the third feature (“of the”), confirming the expected high correlation with the update for the second feature (“of”).
- the translation system does not include the third feature update and, as a result, the drop does not appear in the BLEU score in the next iteration (i.e., iteration 2).
- the translation system does not include updates to all features identified in iteration 2 as reducing the BLEU score.
- FIG. 8 illustrates an example 800 of the training corpus BLEU scores of FIG. 7 for the top ranked million features for five iterations of batch updating, where feature decorrelation filtering follows batch updating in iterations 1 through 4.
- the final BLEU score and number of remaining updated features are displayed for each iteration.
- the baseline score represents a phrase-based statistical machine translation system with twenty different baseline features.
- FIG. 8 illustrates that the feature decorrelation filtering is also effective for a large number of features, improving the BLEU score with each iteration.
- An N-best list of candidate translations C s is a list of the top N candidate translations for the respective source sentence f s as determined by, for example, translation scores or confidence estimations.
- the remaining steps can be implemented as described above.
- the numbers of iterations I and J are fixed.
- the numbers of iterations I and J are determined using a threshold, e.g., a convergence criterion or a minimum gain in the evaluation metric.
- the training corpus BLEU score increases with each iteration of the combined technique and quickly approaches the oracle score.
- the oracle score represents the BLEU score that would be achieved if the translation system picked the optimal candidate translation for each source sentence, using a known translation score for each candidate translation.
- the translation system processes the training corpus (i.e., the corpus of source sentences f 1 s ) I ⁇ (J+1) times.
- the batch updating and feature decorrelation filtering can be used with other machine learning techniques for linear models and not just the MERT technique.
- a translation system can use conditional-random fields or the Perceptron algorithm to learn feature function model parameters for a linear model, rank the features according to different quality criteria, and use the batch updating and feature decorrelation filtering to select an optimal group of features according to the BLEU score, the NIST score, or another automatic evaluation metric.
- the combined technique performs well on training data. However, in some cases, the performance of the combined technique may suffer due to overfitting to training data. In one example of overfitting, the combined technique adapts to certain stylistic preferences in the training corpus. When the test data are very different from the training data, the overfit feature model parameters might not improve and might even deteriorate the quality of the translations.
- the translation system can calculate the aggregate error surface E(m) and hence, the optimal number of updates ⁇ circumflex over (m) ⁇ , using training data that is different from the training data used to determine the optimal feature model parameters ⁇ circumflex over ( ⁇ ) ⁇ m .
- a much smaller training corpus is needed to determine the optimal number of updates ⁇ circumflex over (m) ⁇ than is needed to determine the optimal feature model parameters ⁇ circumflex over ( ⁇ ) ⁇ m , because only one parameter, ⁇ circumflex over (m) ⁇ , is being optimized.
- the translation system can limit the number of considered features by retaining, after ranking features by quality, a determined number of features for the batch updating. Alternatively, the translation system only uses those features that occur in a determined number of sentences.
- MapReduce programming model is described, for example, in Jeffrey Dean and Sanjay Ghemawat, “MapReduce: Simplified Data Processing on Large Clusters,” Proceedings of the 6th Symposium on Operating Systems Design and Implementation, pages 137-150, December 2004.
- MapReduce is a powerful programming model for processing large data sets.
- the basic paradigm is borrowed from functional programming and requires the user to provide two functions: map and reduce.
- the map function processes a single key and value pair from the input and produces a set of intermediate key and value pairs.
- the reduce function merges all values associated with a single intermediate key, producing a set of output values.
- MapReduce's run-time system handles the details of coordination among thousands of machines.
- the strength of MapReduce lies in parallel execution.
- the map function is replicated on different machines operating on different parts of the input data.
- the intermediate data is partitioned by the intermediate key so that the reduce function can also be executed in parallel. Because there are no dependencies among map workers or among reduce workers, the parallelization of each step is highly efficient.
- Each of the three components (i.e., DECODE, MERT, and BATCH) of the combined technique described above corresponds to one MapReduce.
- MapReduce is used solely to handle parallelization.
- the input to this step includes the training corpus sentences f 1 s .
- the map function implements the actual translation process and outputs an N-best list of candidate translations C s for each input sentence f s .
- the reduce function outputs these N-best lists into a distributed file representation.
- the input to the MERT step includes the group of all N-best lists output from the DECODE step.
- the map function calculates the error surface E s ( ⁇ m ) for a single sentence. For example, each machine can independently calculate the error surface for a respective sentence E s ( ⁇ m ).
- the reduce function merges all error surfaces E s ( ⁇ m ) for a single feature h m , producing the aggregate error surface E( ⁇ m ) and identifying the optimal value ⁇ circumflex over ( ⁇ ) ⁇ m for the feature over the entire corpus.
- the input to the BATCH step includes the output from the DECODE and MERT steps.
- the map function reads the output of the MERT step and calculates the error surface for a single sentence E s (m).
- the reduce function merges all error surfaces E s (m) to produce an aggregate error surface E(m). This reduce function can be effectively identical to the reduce function for the MERT step.
- Combining the MERT technique with batch updating and feature decorrelation filtering results in a technique that can efficiently train a large number of features and improve the BLEU score over conventional statistical machine translation systems.
- This technique can improve translation quality for many applications of statistical machine translation, including automatic translation of text content on the Internet and military applications.
- the technique can also be applied to other problems where parameters of a log-linear model need to be trained.
- FIG. 12 is a schematic diagram of an example computer system 1200 .
- the system 1200 can be used for performing the actions and methods described above.
- the system 1200 can include a processor 1218 , a memory 1216 , a storage device 1252 , and input/output devices 1254 .
- Each of the components 1218 , 1216 , 1252 , and 1254 are interconnected using a system bus 1256 .
- the processor 1218 is capable of processing instructions within the system 1200 . These instructions can implement one or more aspects of the systems, components, and techniques described above.
- the processor 1218 is a single-threaded processor.
- the processor 1218 is a multi-threaded processor.
- the processor 1218 can include multiple processing cores and is capable of processing instructions stored in the memory 1216 or on the storage device 1252 to display graphical information for a user interface on the input/output device 1254 .
- the memory 1216 is a computer readable medium such as volatile or non-volatile that stores information within the system 1200 .
- the memory 1216 can store processes related to the functionality of a machine translation engine, for example.
- the storage device 1252 is capable of providing persistent storage for the system 1200 .
- the storage device 1252 can include a floppy disk device, a hard disk device, an optical disk device, or a tape device, or other suitable persistent storage mediums.
- the storage device 1252 can store the various databases described above.
- the input/output device 1254 provides input/output operations for the system 1200 .
- the input/output device 1254 can include a keyboard, a pointing device, and a display unit for displaying graphical user interfaces.
- FIG. 12 The computer system shown in FIG. 12 is but one example.
- embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- Embodiments of the subject matter described in this specification can be implemented as one or more computer program products, i.e., one or more modules of computer program instructions encoded on a tangible program carrier for execution by, or to control the operation of, data processing apparatus.
- the tangible program carrier can be a propagated signal or a computer readable medium.
- the propagated signal is an artificially generated signal, e.g., a machine generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a computer.
- the computer readable medium can be a machine readable storage device, a machine readable storage substrate, a memory device, a composition of matter effecting a machine readable propagated signal, or a combination of one or more of them.
- data processing apparatus encompasses all apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers.
- the apparatus can include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- a computer program (also known as a program, software, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.
- a computer program does not necessarily correspond to a file in a file system.
- a program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub-programs, or portions of code).
- a computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- the processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
- processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer.
- a processor will receive instructions and data from a read-only memory or a random access memory or both.
- the essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks.
- mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks.
- a computer need not have such devices.
- a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, to name just a few.
- PDA personal digital assistant
- GPS Global Positioning System
- Computer-readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks.
- semiconductor memory devices e.g., EPROM, EEPROM, and flash memory devices
- magnetic disks e.g., internal hard disks or removable disks
- magneto-optical disks e.g., CD-ROM and DVD-ROM disks.
- the processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front-end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described is this specification, or any combination of one or more such back-end, middleware, or front-end components.
- the components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (“LAN”) and a wide area network (“WAN”), e.g., the Internet.
- LAN local area network
- WAN wide area network
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network.
- the relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
Abstract
Description
The translation system identifies as ê(f; λ1 M) the target sentence e (e.g., where e is in a group C of multiple target sentences) for which the cost, as defined by Eqn. 1, has the smallest value. The modeling problem includes developing suitable feature functions h1 M that capture the relevant properties of the translation task. The training problem focuses on identifying suitable model parameter values λ1 M.
E s(λ1 M)=Σk=1 N(r s ,e s,k)δ(ê(f s; λ1 M),e s,k) (Eqn. 2)
This optimization criterion is computationally difficult as the objective function has a large number of local optima, is piecewise constant, and does not allow the computation of a gradient.
K(e,f)=Σm′≠mλm′hm′, (e,f), which corresponds to the weighted feature function sum excluding the feature m that is being optimized. Therefore, K(e,f) is a constant with respect to λm. If cost is plotted as a function of λm, every candidate translation eεC corresponds to a line with slope hm(e,f), as illustrated in the example 100 of
The minimum cost surface f(f; λm) 120, illustrated as the bold line in
E s(λm)=Σk=1 N E(r s ,e s,k)δ({circumflex over (e)}(f s; λm),e s,k) (Eqn. 6)
The example 200 of
The model parameter λm for the feature function hm(e,f) can then be updated to the identified optimal parameter value {circumflex over (λ)}m.
{{h1}, {h1, h2}, . . . , {h1, . . . , hM}
forming the lower boundary, illustrated as the bold line, of
E s(m)=Σk=1 N E(r s ,e s,k)δ({circumflex over (e)})δ(ê(f s ; m),e s,k) (Eqn. 9)
TABLE 1 |
Example top seven n-gram features |
BLEU | Calculated optimal | ||
Index | Feature | score gain | parameter {circumflex over (λ)}m |
1 | “the” | 0.237304 | −6.57 |
2 | “of” | 0.234246 | −7.93 |
3 | “of the” | 0.231457 | −10.10 |
4 | “to” | 0.225307 | −4.65 |
5 | “in” | 0.225246 | −6.92 |
6 | “The” | 0.224727 | −7.80 |
7 | “in the” | 0.224697 | −6.38 |
DECODE: determine candidate translations Cs = {es,1, ..., es,N} for all |
source sentences f1 S |
FOR i = 1, ..., I iterations: |
MERT: calculate for each feature hm the aggregate error surface |
E(λm) |
RANK: sort features by quality, such that E({circumflex over (λ)}m) ≦ E({circumflex over (λ)}m+1) |
FOR j = 1, ..., J iterations: |
BATCH: calculate the aggregate error surface E(m) |
FILTER: remove detrimental features |
Update optimal parameter values {circumflex over (λ)}m for non-detrimental features |
In the DECODE step, the translation system translates the training corpus, which potentially includes millions of source sentences, and produces an N-best list of candidate translations Cs for each source sentence fs. An N-best list of candidate translations Cs is a list of the top N candidate translations for the respective source sentence fs as determined by, for example, translation scores or confidence estimations. The remaining steps can be implemented as described above. In some implementations, the numbers of iterations I and J are fixed. In other implementations, the numbers of iterations I and J are determined using a threshold, e.g., a convergence criterion or a minimum gain in the evaluation metric.
Claims (20)
E s(λm)=Σk=1 N E(r s ,e s,k)δ({circumflex over (e)}(f s;λm),e s,k),
E s(λm)=Σk=1 N E(r s ,e s,k)δ({circumflex over (e)}(f s;λm),e s,k),
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US12/056,083 US8645119B2 (en) | 2007-03-26 | 2008-03-26 | Minimum error rate training with a large number of features for machine learning |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US92024207P | 2007-03-26 | 2007-03-26 | |
US12/056,083 US8645119B2 (en) | 2007-03-26 | 2008-03-26 | Minimum error rate training with a large number of features for machine learning |
Publications (2)
Publication Number | Publication Date |
---|---|
US20130144593A1 US20130144593A1 (en) | 2013-06-06 |
US8645119B2 true US8645119B2 (en) | 2014-02-04 |
Family
ID=48524620
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US12/056,083 Active 2032-03-27 US8645119B2 (en) | 2007-03-26 | 2008-03-26 | Minimum error rate training with a large number of features for machine learning |
Country Status (1)
Country | Link |
---|---|
US (1) | US8645119B2 (en) |
Cited By (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20110202330A1 (en) * | 2010-02-12 | 2011-08-18 | Google Inc. | Compound Splitting |
US9318102B2 (en) * | 2012-12-07 | 2016-04-19 | Postech Academy—Industry Foundation | Method and apparatus for correcting speech recognition error |
US20170208207A1 (en) * | 2016-01-20 | 2017-07-20 | Fujitsu Limited | Method and device for correcting document image captured by image pick-up device |
US20180068195A1 (en) * | 2016-09-07 | 2018-03-08 | Apple, Inc. | Multi-Dimensional Objective Metric Concentering |
US11176327B2 (en) * | 2016-10-04 | 2021-11-16 | Fujitsu Limited | Information processing device, learning method, and storage medium |
US11373760B2 (en) | 2019-10-12 | 2022-06-28 | International Business Machines Corporation | False detection rate control with null-hypothesis |
US11645555B2 (en) | 2019-10-12 | 2023-05-09 | International Business Machines Corporation | Feature selection using Sobolev Independence Criterion |
Families Citing this family (17)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8953885B1 (en) * | 2011-09-16 | 2015-02-10 | Google Inc. | Optical character recognition |
US20130325590A1 (en) * | 2012-05-31 | 2013-12-05 | Yahoo! Inc. | Centralized and aggregated tracking in online advertising performance prediction |
US9032416B2 (en) * | 2012-07-30 | 2015-05-12 | Oracle International Corporation | Load balancing using progressive sampling based on load balancing quality targets |
CN105446896B (en) * | 2014-08-29 | 2018-05-04 | 国际商业机器公司 | The buffer memory management method and device of MapReduce application |
US10083295B2 (en) * | 2014-12-23 | 2018-09-25 | Mcafee, Llc | System and method to combine multiple reputations |
US10067936B2 (en) * | 2014-12-30 | 2018-09-04 | Facebook, Inc. | Machine translation output reranking |
US10397043B2 (en) * | 2015-07-15 | 2019-08-27 | TUPL, Inc. | Wireless carrier network performance analysis and troubleshooting |
US10268684B1 (en) * | 2015-09-28 | 2019-04-23 | Amazon Technologies, Inc. | Optimized statistical machine translation system with rapid adaptation capability |
US10185713B1 (en) | 2015-09-28 | 2019-01-22 | Amazon Technologies, Inc. | Optimized statistical machine translation system with rapid adaptation capability |
US9959271B1 (en) | 2015-09-28 | 2018-05-01 | Amazon Technologies, Inc. | Optimized statistical machine translation system with rapid adaptation capability |
US10521415B2 (en) | 2016-05-06 | 2019-12-31 | Facebook, Inc. | Method and system for providing weighted evaluation |
US10318640B2 (en) * | 2016-06-24 | 2019-06-11 | Facebook, Inc. | Identifying risky translations |
CN107193807B (en) * | 2017-05-12 | 2021-05-28 | 北京百度网讯科技有限公司 | Artificial intelligence-based language conversion processing method and device and terminal |
CN108363704A (en) * | 2018-03-02 | 2018-08-03 | 北京理工大学 | A kind of neural network machine translation corpus expansion method based on statistics phrase table |
US11281995B2 (en) * | 2018-05-21 | 2022-03-22 | International Business Machines Corporation | Finding optimal surface for hierarchical classification task on an ontology |
US11676043B2 (en) | 2019-03-04 | 2023-06-13 | International Business Machines Corporation | Optimizing hierarchical classification with adaptive node collapses |
CN111611293B (en) * | 2020-04-24 | 2023-09-29 | 太原太工天宇教育科技有限公司 | Outlier data mining method based on feature weighting and MapReduce |
Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20090048822A1 (en) * | 2006-05-10 | 2009-02-19 | Stanley Chen | Systems and methods for fast and memory efficient machine translation using statistical integrated phase lattice |
-
2008
- 2008-03-26 US US12/056,083 patent/US8645119B2/en active Active
Patent Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20090048822A1 (en) * | 2006-05-10 | 2009-02-19 | Stanley Chen | Systems and methods for fast and memory efficient machine translation using statistical integrated phase lattice |
Non-Patent Citations (22)
Title |
---|
A. Lopez et al., Word-Based Alignment, Phrase-Based Translation: What's the Link?, Aug. 2006, Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pp. 90-99. * |
Alon Levie et al., The significance of recall in automatic metrics for MT evaluation, 2004, Language Technologies Institute, pp. 134-143. * |
Cettolo, M., and Marcello, F. Minimum Error Training of Log-Linear Translation Models. In Proceedings of the International Workshop on Spoken Language Translation (IWSLT), pp. 103-106, Kyoto, Japan. Oct. 2004. |
Chiang, D. A Hierarchical Phrase-based Model for Statistical Machine Translation. In Proceedings of the 43rd Annual Meeting of the Association of Computational Linguistics (ACL), pp. 263-270, Ann Arbor, MI, USA. Jun. 2005. |
Dean, J., and Ghemawat, S. MapReduce: Simplified Data Processing on Large Clusters. In Proceedings of the OSDI '04: 6th Symposium on Operating Systems Design and Implementation, pp. 137-149, San Francisco, CA, USA. Dec. 2004. |
Deborah Coughlin, Correlating automated and human assessments of machine translation quality, 2003, Proceedings of MT Summit XI, pages, entire document. * |
Gao, J., et al. Minimum Sample Risk Methods for Language Modeling. In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT-EMNLP), pp. 209-216, Vancouver, BC, Canada. Oct. 2005. |
Joseph Turian et al., Evaluation of machine translation and its evaluation, 2003, In Proceedings of MT Summit IX, entire document. * |
Koehn, P., et al. Statistical Phrase-based Translation. In Proceedings of the Human Language Technology Conference/North American Chapter of the Association for Computational Linguistics Annual Meeting (HLT-NAACL), pp. 48-54, Edmonton, AB, Canada. May-Jun. 2003. |
Liang, P., et al. An End-to-End Discriminative Approach to Machine Translation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association of Computational Linguistics (ACL), pp. 761-768, Sydney, Australia. Jul. 2006. |
Marcu, D., et al. SPMT: Statistical Machine Translation with Syntactified Target Language Phrases. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 44-52, Sydney, Australia. Jul. 2006. |
Och, F.J. Minimum Error Rate Training in Statistical Machine Translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL), pp. 160-167, Sapporo, Japan. Jul. 2003. |
Och, F.J., and Ney, H. Discriminative Training and Maximum Entropy Models for Statistical Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pp. 295-302, Philadelphia, PA, USA. Jul. 2002. |
Och, F.J., and Ney, H. The Alignment Template Approach to Statistical Machine Translation. Computational Linguistics, 30(4):417-449. 2004. |
Papineni, K., et al. BLEU: A Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pp. 311-318, Philadelphia, PA, USA. Jul. 2002. |
Roark, B., et al. Discriminative Language Modeling with Conditional Random Fields and the Perceptron Algorithm. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL), Barcelona, Spain. Jul. 2004. |
Santanjeev Banerjee et al., METEOR: An automatic metric for MT evaluation with improved correlation with human judgment, Jun. 2005, Proceedings of the ACL workshop on Intrinsic and Extrinsic evalutation measures for machine translation and/or summarization, pp. 65-72. * |
Shen, L., et al. Discriminative Reranking for Machine Translation. In Proceedings of the Human Language Technology Conference/North American Chapter of the Association for Computational Linguistics Annual Meeting (HLT-NAACL), Boston, MA, USA. May 2004. |
Stefan Riezlar et al, On some pitfalls in automatic evaluation and significance testing for MT, Jun. 2005, Proceedings of the ACL workshop on Intrinsic and Extrinsic evalutation measures for machine translation and/or summarization, pp. 57-64. * |
Tadashi Nomoto, Multi-Engine Machine Translation with Voted Language Model, 2004, Proceedings of the 42nd Annual Meeting of the Association of Computational Linguistics, entire document. * |
Ueffing, N., et al. Generation of Word Graphs in Statistical Machine Translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 156-163, Philadelphia, PA, USA. Jul. 2002. |
Zens, R., and Ney, H. N-Gram Posterior Probabilities for Statistical Machine Translation. In Proceedings of the Human Language Technology Conference/North American Chapter of the Association for Computational Linguistics Annual Meeting (HLT-NAACL), pp. 72-77, New York, NY, USA. Jun. 2006. |
Cited By (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20110202330A1 (en) * | 2010-02-12 | 2011-08-18 | Google Inc. | Compound Splitting |
US9075792B2 (en) * | 2010-02-12 | 2015-07-07 | Google Inc. | Compound splitting |
US9318102B2 (en) * | 2012-12-07 | 2016-04-19 | Postech Academy—Industry Foundation | Method and apparatus for correcting speech recognition error |
US20170208207A1 (en) * | 2016-01-20 | 2017-07-20 | Fujitsu Limited | Method and device for correcting document image captured by image pick-up device |
US10187546B2 (en) * | 2016-01-20 | 2019-01-22 | Fujitsu Limited | Method and device for correcting document image captured by image pick-up device |
US20180068195A1 (en) * | 2016-09-07 | 2018-03-08 | Apple, Inc. | Multi-Dimensional Objective Metric Concentering |
US10185884B2 (en) * | 2016-09-07 | 2019-01-22 | Apple Inc. | Multi-dimensional objective metric concentering |
US11176327B2 (en) * | 2016-10-04 | 2021-11-16 | Fujitsu Limited | Information processing device, learning method, and storage medium |
US11373760B2 (en) | 2019-10-12 | 2022-06-28 | International Business Machines Corporation | False detection rate control with null-hypothesis |
US11645555B2 (en) | 2019-10-12 | 2023-05-09 | International Business Machines Corporation | Feature selection using Sobolev Independence Criterion |
Also Published As
Publication number | Publication date |
---|---|
US20130144593A1 (en) | 2013-06-06 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US8645119B2 (en) | Minimum error rate training with a large number of features for machine learning | |
US8326598B1 (en) | Consensus translations from multiple machine translation systems | |
US8626488B1 (en) | Word alignment with bridge languages | |
US8812291B2 (en) | Large language models in machine translation | |
US8027938B1 (en) | Discriminative training in machine learning | |
Gao et al. | A large scale ranker-based system for search query spelling correction | |
JP5774751B2 (en) | Extracting treelet translation pairs | |
US9092483B2 (en) | User query reformulation using random walks | |
US8660836B2 (en) | Optimization of natural language processing system based on conditional output quality at risk | |
TWI512502B (en) | Method and system for generating custom language models and related computer program product | |
US20110295897A1 (en) | Query correction probability based on query-correction pairs | |
US20110282643A1 (en) | Statistical machine translation employing efficient parameter training | |
JP5586817B2 (en) | Extracting treelet translation pairs | |
US20080120092A1 (en) | Phrase pair extraction for statistical machine translation | |
US20090063130A1 (en) | Fast beam-search decoding for phrasal statistical machine translation | |
US9298693B2 (en) | Rule-based generation of candidate string transformations | |
Le et al. | Measuring the influence of long range dependencies with neural network language models | |
Setiawan et al. | Statistical machine translation features with multitask tensor networks | |
Mondal et al. | Machine translation and its evaluation: a study | |
US20220004712A1 (en) | Systems and methods for diverse keyphrase generation with neural unlikelihood training | |
Saluja et al. | Machine translation with binary feedback: a large-margin approach | |
Galley et al. | Regularized minimum error rate training | |
Farzi et al. | A swarm-inspired re-ranker system for statistical machine translation | |
JP2016058003A (en) | Translation device | |
Farzi et al. | A neural reordering model based on phrasal dependency tree for statistical machine translation |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:OCH, FRANZ JOSEF;JAHR, MICHAEL E.;THAYER, IGNACIO E.;REEL/FRAME:021089/0575Effective date: 20080610 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
FPAY | Fee payment |
Year of fee payment: 4 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044101/0299Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 8 |