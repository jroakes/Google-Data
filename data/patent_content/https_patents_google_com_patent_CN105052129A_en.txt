CN105052129A - Cascaded camera motion estimation, rolling shutter detection, and camera shake detection for video stabilization - Google Patents
Cascaded camera motion estimation, rolling shutter detection, and camera shake detection for video stabilization Download PDFInfo
- Publication number
- CN105052129A CN105052129A CN201480016067.2A CN201480016067A CN105052129A CN 105052129 A CN105052129 A CN 105052129A CN 201480016067 A CN201480016067 A CN 201480016067A CN 105052129 A CN105052129 A CN 105052129A
- Authority
- CN
- China
- Prior art keywords
- video
- model
- frame
- singly
- motion
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/68—Control of cameras or camera modules for stable pick-up of the scene, e.g. compensating for camera body vibrations
- H04N23/682—Vibration or motion blur correction
- H04N23/683—Vibration or motion blur correction performed by a processor, e.g. controlling the readout of an image memory
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/20—Analysis of motion
- G06T7/246—Analysis of motion using feature-based methods, e.g. the tracking of corners or segments
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/40—Scenes; Scene-specific elements in video content
- G06V20/41—Higher-level, semantic clustering, classification or understanding of video scenes, e.g. detection, labelling or Markovian modelling of sport events or news items
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/40—Scenes; Scene-specific elements in video content
- G06V20/46—Extracting features or characteristics from the video content, e.g. video fingerprints, representative shots or key frames
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/68—Control of cameras or camera modules for stable pick-up of the scene, e.g. compensating for camera body vibrations
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/68—Control of cameras or camera modules for stable pick-up of the scene, e.g. compensating for camera body vibrations
- H04N23/681—Motion detection
- H04N23/6811—Motion detection based on the image signal
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/68—Control of cameras or camera modules for stable pick-up of the scene, e.g. compensating for camera body vibrations
- H04N23/689—Motion occurring during a rolling shutter mode
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/10—Image acquisition modality
- G06T2207/10016—Video; Image sequence
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/20—Special algorithmic details
- G06T2207/20021—Dividing image into blocks, subimages or windows
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/30—Subject of image; Context of image processing
- G06T2207/30244—Camera pose
Abstract
An easy-to-use online video stabilization system and methods for its use are described. Videos are stabilized after capture, and therefore the stabilization works on all forms of video footage including both legacy video and freshly captured video. In one implementation, the video stabilization system is fully automatic, requiring no input or parameter settings by the user other than the video itself. The video stabilization system uses a cascaded motion model to choose the correction that is applied to different frames of a video. In various implementations, the video stabilization system is capable of detecting and correcting high frequency jitter artifacts, low frequency shake artifacts, rolling shutter artifacts, significant foreground motion, poor lighting, scene cuts, and both long and short videos.
Description
Technical field
Present disclosure is usually directed to operate video content, and more particularly, relates to the camera motion in stable video content.
Background technology
Website is shared video content and developed into the global phenomenon supported by many websites.Issue thousands of video every day, and this number is easier to use and more universal and increase along with the instrument and chance for catching video becomes.People up to a million watch the video issued.
The video that usual needs process is issued is to improve image and audio quality.Visible shake in the video that this process can comprise correcting video to reduce undesirably the moving of physics camera owing to being used for catching video and cause.Such as, along with the growth of mobile telephone camera, to upload and the video of sharing the daily experience being caught them by casual user by their mobile device has remarkable increase.A big chunks of these videos is easy to shake because be difficult to keep handheld camera to stablize, if especially when catching mobile object or recording limit, limit move.
Although the camera in many modern times is equipped with the image stabilizer for still image usually, but there is serious camera shake or low frequency movement, such as during shooting, the shake caused when user is just walking or running, the stability provided by image stabilizer is usually not enough.Only the prediction provisional video amount with significantly shake increases along with the growth of wearable and first person view camera, especially welcome in motion and other activities.Most of casual user does not use or is inclined to use the stable equipment of specialty (such as, tripod, photographic car, stablize cam).In addition, by comparatively, camera was taken or also can be benefited from stability from the conventional video of film digitization morning.
In addition, most of casual user did not use specialty to stablize software yet.In addition, usually exist although these specialties stablize software, these software programs are by variation quality degree correcting video.In addition, these software programs many when the metadata do not obtained from physics camera, or do not have from user about working when how should perform the input of stabilisation.
Summary of the invention
A kind of wieldy Online Video systems stabilisation and computer implemented video stabilizing method are described.Stable video after seizure video, and therefore, the stable video length to comprising conventional video and the new video form of ownership caught works.In one implementation, video stabilization system is completely automatically, except video itself, does not require input or the optimum configurations of user.Video stabilization system uses cascade motion model to carry out selective gist in the correction of the different frame of video.In different implementation, video stabilization system can detect and correct high dither pseudomorphism, low-frequency jitter pseudomorphism, rolling shutter pseudomorphism, significantly foreground moving, poor lighting, scene switching and length video.
In one embodiment, generate camera path by the multiple tracking characteristics of each of at least two consecutive frames of accessing video and generating video at least in part, the interframe movement of the tracking characteristics instruction camera of consecutive frame.The interframe movement multiple motion model being applied to tracking characteristics estimates that multiple characteristics of each of applied motion model, motion model represent the dissimilar camera motion of the degree of freedom (DOF) comprising different number respectively.By the characteristic of motion model being compared with corresponding threshold value, it is effective for determining in motion model one or more.Based on effective exercise model, generate the camera path between consecutive frame.
In one embodiment, correct rolling shutter by the multiple tracking characteristics of each of at least two consecutive frames of accessing video and generating video at least in part, the interframe movement of the tracking characteristics instruction camera of consecutive frame.Using singly answer models applying in interframe movement determine as with multiple tracking characteristics of singly to answer in Model Matching point.Using singly answer mixed model be applied to interframe movement to determine as with singly answer mixed model mate in point multiple tracking characteristics.If the number that should mix interior point single exceeds threshold value than the number of point in single answering, and by the consecutive frame of will singly answer mixed model to be applied to video, generates stable video.
In one embodiment, correct rolling shutter by the multiple tracking characteristics of each of at least two consecutive frames of accessing video and generating video at least in part, the interframe movement of the tracking characteristics instruction camera of consecutive frame.Using singly answer models applying in interframe movement determine as with multiple tracking characteristics of singly to answer in Model Matching point.Multiple mixed model of singly answering is applied to tracking characteristics to singly answering each of mixed model, determine as with each singly answer mixed model mate in multiple tracking characteristics of point, singly answer mixed model to have different rigidity.Determine single number that should mix interior point than single should in the number of point exceed and be exclusively used in that and singly answer the minimum rigidity list of the threshold value of mixed model to answer mixed model.By answering mixed model to be applied to the consecutive frame of video minimum rigidity list, generate stable video.
In one embodiment, at least in part by accessing video and the multiple frames to video, estimate the value of multiple degrees of freedom (DOF) of similitude motion model, every single-degree-of-freedom represents that the different cameral for the original camera of capturing video is moved, and the value of DOF represents the size that different cameral is moved.Generate the spectrogram of each being used for DOF, each spectrogram is based on the value of the DOF on the time window of multiple consecutive frames comprising video.Based on spectrogram, generate multiple jitter feature.Based on jitter feature, classification video.Then, based on classification stable video.
Accompanying drawing explanation
Fig. 1 is according to an embodiment, comprises the high level block diagram of the computing environment of video stabilization system.
Fig. 2 is the high level block diagram that diagram is used as the example of the computer of video stabilization system, video server and/or client.
Fig. 3 is according to an embodiment, the high level block diagram of the module in diagram video stabilization system.
Fig. 4 is according to an embodiment, illustrates the flow chart of the process of the camera path for determining video.
Fig. 5 is according to an embodiment, illustrates for detecting the flow chart with the process of the rolling shutter in correcting video.
Fig. 6 illustrates according to an embodiment, the motion of example tracking characteristics and their intraframe motion.
Fig. 7 diagram, according to an embodiment, has multiple motion models of the degree of freedom of different number respectively.
Fig. 8 is according to an embodiment, illustrates the flow chart of the process for detecting the camera shake in video.
Fig. 9 A and 9B is according to an embodiment, diagram is used for multiple time window of Similarity Model and multiple spectrograms of the different degree of freedom, wherein, Fig. 9 A illustrates first, the spectrogram of video 12 windows of the short length in length aspect, and Fig. 9 B illustrates second, the spectrogram of 40 windows of the longer length video in length aspect.
Accompanying drawing is only example object, describes embodiments of the invention.Those skilled in the art recognizes when not deviating from principle of the present invention described herein by being easy to from following description, can adopt structure shown here and the alternate embodiment of method.
Embodiment
I. summarize
Fig. 1 is according to an embodiment, comprises the high level block diagram of the computing environment of video stabilization system.Fig. 1 illustrate connected by network 116 video server 110, video stabilization system 112 (" systems stabilisation ") and client 114.A client 114 is only shown in FIG to simplify and to make to be described clearly.The embodiment of computing environment 100 can have client 114 thousands of or up to a million, and multiple video server 110 and systems stabilisation 112.
Video server 110, via network 116, provides video content (referred to here as " video ") to client 114.In one embodiment, video server 110 is positioned at by YOUTUBE
tMon the website provided, although this video server also can be provided by another entity.Video server 110 comprises the database that stores multiple video and for the Web server mutual with client 114.Video server 110 receives the request for the video database from the user of client 114 and responsively, provides video.In addition, video server 110 can receive, stores, process (such as stablizing) and provide the user by client 114 and the video by other entity issued.
Client 114 makes computer for executed activity or other electronic equipments by one or more user, and described activity comprises uploaded videos, uses systems stabilisation 112 to start the stabilisation of video and to browse the video and other guide that receive from video server 110.Client 114 can be such as perform the personal computer allowing user to browse and search for the Web browser 118 of the video that can obtain in video server website.In other embodiments, client 114 be except computer can networking gear, such as personal digital assistant (PDA), mobile phone, beep-pager, TV " Set Top Box " etc.
Network 116 realizes communicating between the entity that is attached thereto.In one embodiment, network 116 is the Internet and uses standard communication techniques and/or agreement.Thus, network 116 can comprise operation technique, such as Ethernet, 802.11, World Interoperability for Microwave Access, WiMax (WiMAX), 3G, digital subscriber line (DSL), asynchronous transfer mode (ATM), the senior exchange of InfiniBand, PCI etc. link.Similarly, multiprotocol label switching (MPLS), TCP/IP (TCP/IP), User Datagram Protoco (UDP) (UDP), HTML (Hypertext Markup Language) (HTTP), Simple Mail Transfer protocol (SMTP), file transfer protocol (FTP) (FTP) etc. can be comprised with networking protocol over a network 116.The data exchanged over a network 116 can use the technology that comprises HTML (HTML), extend markup language (XML) etc. and/or form to represent.In addition, conventional encryption technology can be used, such as all or some links of SSL (SSL), Transport Layer Security (TLS) and VPN (virtual private network) (VPN), the Internet protocol security (IPsec) etc. encryption.In another embodiment, replace or except above-mentioned points, entity use customization and/or the exclusive data communication technology.
Systems stabilisation 112 is configured to receive input video and stablizes this input video by the pixel content changing frame of video.Systems stabilisation 112 stable output video.As a part for stabilization procedures, systems stabilisation 112 determines to describe the camera path that the two dimension (2D) that is initially used for the camera recording this video is moved.Systems stabilisation 112 also only can stablize this video separation with use camera path, exports this camera path.In order to stable video, camera path is used to get rid of the motion of the pixel in the frame of video caused due to the motion of camera as much as possible.In one embodiment, stable output video is the copy of original video, wherein, adjusts the position of the pixel of each frame according to determined camera path, to offset the motion of interframe.
Fig. 2 is the high level block diagram that diagram is used as the example of the computer 200 of video server 110, systems stabilisation 112 and/or client 114.At least one processor 202 being couple to chipset 204 is shown.Chipset 204 comprises memory control hub 220 and I/O (I/O) control centre 222.Memory 206 and graphics adapter 212 are couple to memory control hub 220, and display device 218 is couple to graphics adapter 212.Memory device 208, keyboard 210, indicating equipment 214 and network adapter 216 are couple to I/O control centre 222.Other embodiments of computer 200 have different architecture.Such as, in certain embodiments, memory 206 is directly coupled to processor 202.
Memory device 208 is non-transitory computer-readable storage medium, such as hard-drive, compact disc-ROM (CD-ROM), DVD or solid storage device.The instruction and data used by processor 202 preserved by memory 206.Indicating equipment 214 is indicating equipments of mouse, tracking ball or other types, and makes for entering data into computer system 200 in conjunction with keyboard 210.Graphics adapter 212 shows image and other information on display device 218.Computer system 200 is couple to network 116 by network adapter 216.Some embodiments of computer 200 have and/or other parts different from shown in Fig. 2.
Computer 200 is adapted to and performs for providing functional computer program module described herein.As used in this, term " module " refers to the computer program instructions and other logics that are used to provide particular functionality.Thus, can with hardware, firmware and/or software-implemented module.In one embodiment, the program module formed by executable computer program instruction is stored on memory device 208, is loaded in memory 206 and is performed by processor 202.
The type of the computer 200 used by the entity of Fig. 1 can depend on that the disposal ability that embodiment and entity use changes.Such as, as the client 114 of mobile phone, there is limited disposal ability, small displays 218 and may lack indicating equipment 214 usually.On the contrary, systems stabilisation 112 can comprise and works together to provide functional multiple server described herein.As apparent from following description, with regard to requiring to be realized by computer, the operation of systems stabilisation 112 stable video is very complicated, and can not be performed by human thought in intelligence completely thus.
II. video stabilization
Fig. 3 is according to an embodiment, the high level block diagram of the module in diagram video stabilization system 112.As described above, systems stabilisation 112 is configured to receive input video 302 to stablize this video and stable output video 304 and/or camera path 306.
In one embodiment, systems stabilisation 112 comprises movement estimation system module 310, camera path analysis module 320, camera path stable module 330, stable video generation module 340, data memory module 350 and shaking detection module 360.Some embodiments of systems stabilisation 112 have and said different and/or other module.Similarly, function can with from said different mode, be distributed among module.Certain module and function can be incorporated in other modules of systems stabilisation 112 and/or other entities of network 116, comprise video server 110 and client 114.
Data memory module 350 stores the data used by the various modules of systems stabilisation 112.Other intermediate data items that the data stored comprise such as operated frame and/or other parts of video, the feature of following the tracks of, estimated motion model, the characteristic relevant with stabilization process and threshold value, camera path and produce during stabilization process.This inventory is intended to exemplary, instead of exhaustive.
II.A. estimation
The frame that movement estimation system module 310 analyzes input video 302 carrys out characterization and is used for the original 2D camera motion of the camera catching input video 302, and the output of that characterization is provided as camera path, and is a kind of device for performing this function.To a pair of time t and t+1 represented respectively in video adjacent frame I
tand I
t+1, motion estimation module 310 is based on from frame I
tits initial position to frame I
t+1the tracking characteristics T of its rearmost position
t, T
t+1the motion of set, characterization camera path.Generate tracking characteristics T by the base pixel of each frame to gather, and be interframe movement M by the movement representation between consecutive frame
tset.Use this interframe movement M
t, motion estimation module 310 is estimated to be used for frame I
tmotion model F
tset, wherein, by estimated motion model F
tbe applied to frame I
tpixel descriptor frame I
tand I
t+1between the motion of pixel.
But, and the motion model estimated by not all is by the interframe movement of characterization pixel effectively, and thus, motion estimation module 310 is configured to determine which estimates motion model by effective at frame I further
tand I
t+1between camera path in.Only be used in camera path for the effective exercise model that each frame is right.In one embodiment, in order to determine camera path, motion estimation module 310 comprises tracking module 312 and cascade motion module 314.When performing estimation at frame to level, by parallelization estimation on some computers of the upper parallel running of different piece (or fragment) that estimation is distributed in video.
II.A.i follows the tracks of
Tracking module 312 is configured to each frame I generating input video 312
ttracking characteristics T
tset.Tracking characteristics serves as the mark for there is object in the video frame.Tracking module 312 tracking frame between the object followed the tracks of in video of the motion of single tracking characteristics how to move between frames.Generally speaking, the motion M of the tracking characteristics between a pair consecutive frame can be analyzed
tobject motion in frame is separated with the motion of shooting camera.
Tracking module 312 is by being applied to frame pixel (such as Harris angular measurement), the tracking characteristics T of delta frame by angular measurement
t.Angular measurement occurs at " angle ", and each pixel in the frame of the vertical and horizontal line intersection of the remarkable gradient namely in pixel color generates tracking characteristics.More particularly, the smallest eigen that the feature of tracking is positioned at the self-incidence matrix of the gradient of frame is greater than the pixel place of threshold value after non-maximum suppresses.Tracking characteristics can be stored as the set of two dimension (2D) point, and each tracking characteristics has x in the cartesian coordinate system of frame of video and y-axis coordinate.Thus, can by i-th of a frame tracking characteristics T
t,iand to frame I
t+1motion M
t,ifollowing expression:
In addition, when generating tracking characteristics, frame can be divided into the multi-layer net (such as, 4 × 4 or 16 grids altogether, 8 × 8 grids and 16 × 16 grids) with different size by tracking module 312.Can on the basis of each grid, the number of the tracking characteristics that the Grads threshold that setting can regard as tracking characteristics generates to each unit of the grid that standardizes.This contributes to the number being equilibrated at the tracking characteristics produced in every part of frame, makes tracking characteristics can not exceedingly represent some unit relative to other unit.Like that, in relative short distance, have number of colors change unit will not necessarily have than color evenly the more tracking characteristics of unit.The similar district that absolute minimum threshold solves frame can be implemented.Absolute minimum threshold can infer that specific region may lack tracking characteristics.Can gather or filter and the hand-to-hand tracking characteristics of other tracking characteristics (such as 5 pixels in) to guarantee that tracking characteristics is distributed in unit, and in frame as a whole.
Fig. 6 illustrates according to an embodiment, the motion of example tracking characteristics and their intraframe motion M
t.Usually, at least some tracking characteristics will present the inconsistent motion of the motion that closes on tracking characteristics with other.Analyze the tracking characteristics that tracking characteristics identified and filtered out those exceptions.Inconsistent motion can comprise such as tracking characteristics T
t,itracking characteristics different (such as contrary) square M that moves up substantially is being closed on from other
t,i.Some not at the same level, several times are determined represent different directions and represent the threshold value of closing on.As mentioned above, use some grades of grids (such as 4 × 4 or altogether 16 grids, 8 × 8 grids and 16 × 16 grids), wherein, each grid level has the different threshold values forming and close on tracking characteristics and form basic different directions.
Usually, the tracking characteristics in a unit of grid is considered to adjacent.Based on the gathering (such as, mean value) of the direction of motion of each of the tracking characteristics of that unit, the direction of motion of the tracking characteristics of determining unit.For the threshold tolerance of basic different directions, to comparatively macrolattice (such as, 16 × 16) can be set between tracking characteristics very high (such as, require a large amount of uniformity), and to comparatively small grid (such as, 4 × 4) can be set between tracking characteristics relatively low (such as, not too requiring uniformity).Abandon the tracking characteristics of the directivity threshold value not meeting one or more grades.In one embodiment, use stochastical sampling consistency (RANSAC) algorithm, filter tracking characteristics.
Such as, in example grid, all tracking characteristics except a tracking characteristics all present to left, and remaining tracking characteristics presents to right translation.Therefore, can filter move right tracking characteristics and its be not considered in further processing.
II.A.ii cascade estimation
Cascade motion module 314 be configured to use consecutive frame between tracking characteristics T
tthe interframe movement M of set
toriginal camera path is determined in set, and is a kind of device for performing this function.For completing this operation, cascade motion module 314 makes interframe movement M
tset and linear movement model F
tset matching.Each of motion model represents the dissimilar motion with the degree of freedom (DOF) of different number.The output camera path of cascade motion module 314, to each frame pair, is the estimation motion model of the effective expression being confirmed as interframe movement.
Conveniently, determine consecutive frame I
tand I
t+1effective motion model set is assigned at this centering second frame I
t+1.Usually, the effective exercise model set that each frame except the first frame in video distributes is defined as, because do not have analyzable motion in the first frame of video.To the first frame of video, identity motion model is used for initialization.As mentioned below, application of frame I
teffective exercise model be used to generate as stablize original camera move frame I
t+1stable video frame J
t+1process at least partially.In a further embodiment, alternatively effective exercise model set can be assigned to the first frame of this centering.
II.A.ii.a estimates motion model
Fig. 7 diagram, according to an embodiment, has multiple motion models of the degree of freedom of different number respectively.In one embodiment, at least four motion model F are considered
t (k).First motion model F
t (0)it is the translation model had for detecting move two degrees of freedom along the x of frame and y-axis.Second motion model F
t (1)have for detecting rotation and even convergent-divergent (size of such as frame) and the Similarity Model for four degrees of freedom detecting translation.3rd motion model F
t (2)have for detecting transparent effect, inclination, non-homogeneous convergent-divergent and singly answering model for eight degrees of freedom detecting similitude and translation.4th motion model F
t (3)be there is 8 × n the degree of freedom singly answer mixed model, wherein, n is the number of the mixing of singly answering in mixed model.In one embodiment, n=10.Singly answer mixed model except detection homography, similitude and translation, also detect rolling shutter distortion (such as, rocking).Thus, along with the number of the DOF of motion model increases, the new degree of freedom that each motion model comprises the camera representing newtype and the DOF comprised for the motion represented by lower DOF motion model.Hereinafter, exemplary motion model will be further described.
Motion model respectively can by their parameter configuration, wherein, and one in the DOF of each Parametric Representation motion model.Thus, two different frames between two different translations by matching two different translation model F
(0), there is their parameter (or DOF) configuration respectively.Cascade motion module 314 estimates that the parameter of motion model is to determine best fitted interframe movement M
tthe configuration of each motion model.As long as have estimated the parameter for motion model, the motion model estimated by evaluating determines that whether they be the model of " correctly " to be applied.That is, their validity is evaluated to determine whether they represent the motion M of the tracking characteristics of interframe
t.In next chapters and sections, will be described further.
For estimating relative to interframe movement M
tthe parameter of each motion model, determine the parameter of given motion model to minimize:
∑
i||(T
t,i+M
t,i)-F(T
t,i)||
p(1)
Wherein, each i represents the interframe movement between two corresponding tracking characteristics that frame is right, and wherein p is the rank (such as, to euclideam norm, p=2) of normalization factor.More particularly, in one embodiment, use iteration weighted least-squares function again, make motion model be fitted to interframe movement:
And wherein, w
iit is inverse error of fitting weight.W
ivalue larger, matching is better.Interior point (inlier) tracking characteristics of this model of matching has the value much larger than 1, and exterior point (outlier) has close to or is less than the little weight of value of 1.By minimizing the summation of equation 2, estimate each motion model F
tparameter.
The parameter (DOF) of each motion model is as follows.Translational motion model F
t (0)be estimated as and there is weight w
ithe weighted average translation of tracking characteristics, make:
Wherein, t
xand t
yrepresent the size of the translation of the camera along x and y-axis respectively.Translation size represents with pixel or is expressed as the percentage of the frame width before pruning.As above, also can by t
xand t
yvalue think and represent for the value of the DOF of the translation model of that frame.
Estimate similitude motion model F
t (1)make:
Wherein, a is the constant zooming parameter of frame, and b represents rotation, and t represents the translation of x and y.
Use 3 × 3 matrixes, estimate sheet answers model F
t (2), wherein, by normalization matrix element, solve because the fuzzy of convergent-divergent makes h
33equal 1.Use the weighted version of non-homogeneous direct linear transformation (DLT) algorithm decomposing answer via QR, estimate sheet answers the matrix element of model.
Wherein, assuming that little interframe Rotation and Zoom, w
t=(w
1, w
2)
tbe frame constant perspective partial, a and d is frame constant zooming parameter, and t is the translation of x and y, and c and b rotates and tilt.
Use multiple (such as, 10) difference singly to answer mixing of model, and can between difference realizes the regularization matrix of changes values, estimate sheet answers mixed model F
t (3).Mixed model is singly answered difference singly to be answered models applying in every part of frame.More particularly, block is the continuous sweep line set in frame, and wherein, the sum of the scan line in frame is divided into 10 blocks of scan line.Thus, difference singly answers model to be applied to each block.The rigidity of mixed model is singly answered in regularization matrix impact.Such as, the regularization matrix value of enough high level (such as 1) makes singly to answer mixed model rigidity, makes it and singly answers model F
t (2)identical.Less regularization matrix value (such as between 0 and 1) increases the effect of other mixture/blocks, and the rolling shutter of singly answering mixed model better to simulate interframe movement is rocked.
Mixed model F is singly answered by following expression
t (3):
Wherein: w
t=(w
1, w
2)
tbe frame constant perspective portion, a and b is frame constant zooming parameter, t
kthe block change translation of x and y, and c
kand b
kthat block change rotates and tilts.To 10 blocks (k), F
t (3)there is 4x10+4=44 the degree of freedom.
Singly answering F
t (2)singly answer mixed model F
t (3)before the estimation of parameter, filter and be used for interframe movement M
ttracking characteristics T
t.For performing this filtration, first estimate to be used for Similarity Model F
t (1)parameter.Determine one or more tracking characteristics set of not mating with estimation Similarity Model.At least to the first iteration of equation 2, from be used in estimate sheet should with singly answer mixed model parameter, filter these non-matching tracking characteristics.This can such as pass through their weight w
ibe set as that 0 has come.This contributes to relative to remarkable foreground moving the motion of camera (such as closely), and delimitation order should with the parameter of singly answering mixed model.
When the parameter of Confirming model, in a further embodiment, the tracking characteristics that can be biased near to frame border gives larger weight or gives less weight to the tracking characteristics near frame center in the weighting of tracking characteristics.This can, such as along x and the y reference axis of frame, use inverse Gaussian function.This tends to relative to the prediction centered by frame with other objects based on the face close to camera frame.
II.A.ii.b determines effectively to estimate motion model
Any to framing between tracking characteristics T
tinterframe movement M
tcan seem estimate motion model any, some, all or neither one.Such as, if scene be strict on-plane surface (such as, due to different depth layer or remarkable foreground moving), translational motion model is deficiency (relative to other motion models, translation model generates the stabilized pseudo picture of minimal number) in Describing Motion.Correct (or effective) motion model set is applied to interframe movement and will stablizes those frames and eliminate at least some instability, produces residue shake.Apply incorrect model and camera path and stable video are introduced in initial non-existent distortion.
More particularly, if translation model is effective, the result of its application will be reduce shake.If translation model is invalid, the result of its application will be other shake distortion.If Similarity Model is effective, the result of its application will be introduce high frequency rigidity to rock residue shake (in fact great majority perspective).If Similarity Model is invalid, the result of its application will be other shake distortion.If singly answer model effective, if existed without rolling shutter, result of its application will close to without residue shake, and if rolling shutter exists, will be to rock to remain to shake.If singly answer model invalid, the result of its application will be perspective distort error.If singly answer mixed model effective, the result of its application will be shaken close to without residue.If singly answer mixed model invalid, the result of its application will be non-rigid wavy distortion distortion.
As long as cascade motion module 314 is as calculated for motion model F
tthe parameter of set, makes motion model and tracking characteristics T
t, T
t+1set and interframe movement M
twhich motion model F matching determines
teffectively mate interframe movement.Usually, if relative to one or more characteristic, the type of sports represented by motion model mates with presented interframe movement, and relative to interframe movement, this motion model is considered as effectively.These characteristics represent the degree of fitting between motion model and interframe movement.Characteristic is different between motion model.
Table 1 illustrates according to an embodiment, for the example collection of the characteristic of efficiency evaluation.Table 1 comprises motion model, the characteristic relevant with each motion model and threshold value.Some characteristics are simply the parameters of the motion model for inter frame motion estimation.Other characteristics can by estimated motion model and tracking characteristics T
t, T
t+1with interframe movement M
tmatching draw.Interior point (inlier) can be called with the tracking characteristics of Model Matching, and can not be called exterior point (outlier) with the tracking characteristics of Model Matching.If the motion model estimated by tracking characteristics matching is in threshold tolerance, this tracking characteristics is interior point.Such as, if motion model predicting tracing feature T
t,imotion M
t,iin the precision of 1.5 pixels, then this tracking characteristics is considered as interior point.
In one embodiment, if single characteristic does not meet the threshold value of its correspondence, this motion model is invalid.In other embodiments, for determining that whether motion model is effective, other characteristics, threshold value and demand can be defined.
About translation feature, the number of tracking characteristics is the total number of point in tracking characteristics.Translation size is the amount of the interframe movement estimated by translation model.This can such as be determined by the translation size parameter of motion model.Based on the single translation of the tracking characteristics between frame, the standard deviation of translation can be determined.Mean pixel based on the tracking characteristics between a pair frame by one or more intermediate value offset at the mean pixel of front frame to (such as 5 front frame to) offsets, and can determine to accelerate.
About similitude characteristic, the number of tracking characteristics is identical with translation model.There is the box of fixed size by placement around each feature and by making all boxes combine, determine the feature coverage of the percentage as frame area.Area in the associating of box determines feature coverage compared with total frame area.Respectively based on convergent-divergent change and the rotation parameter of Similarity Model, convergent-divergent change and revolving property can be determined.
About singly answering characteristic, convergent-divergent change is identical with Similarity Model with the change of revolving property.Singly answer characteristic also can comprise can based on singly answer the change of the perspective parameter of model and the see-through property determined.For the threshold value of see-through property based on every normalization, without unit and can be such as be worth 4 × 10
-4.Grid coverage characteristic represents the calculating of the amount of the frame covered by interior some tracking characteristics.By grid coverage on the tracking characteristics that frame is right (such as, 10 × 10), determine grid coverage characteristic.To each unit (or storehouse) of grid, determine that this storehouse is in interior point or the score of exterior point.Storehouse score is interior point or exterior point based on the tracking characteristics in storehouse relative to singly answering model, and based on the weight w of the tracking characteristics in storehouse
i, especially based on the intermediate value b of the characteristic weighing of the tracking characteristics in storehouse
j.In one embodiment, the score of storehouse j is determined based on following:
Wherein, a and b
jit is the zoom factor for logistic regression score function.Grid coverage G
tbe the mean value of all storehouses score, make:
If G
ttoo low, grid coverage characteristic too low (such as, lower than 30% of storehouse), and thus this singly to answer model to be considered as invalid.
About singly answering mixed characteristic, block coverage characteristic and above-mentioned grid coverage property class are seemingly.Wherein, replace single every frame grid coverage score, each block of mixture specifies its oneself block coverage score.Particularly, to 1 × 10 grid be positioned on tracking characteristics, each storehouse (altogether 10 storehouses) are corresponding in block.Each block covers the multiple scan lines in frame thus.Based on tracking characteristics weighted sum they whether be interior point, to each storehouse/block, determine score.If its coverage is lower than threshold value, such as 40%, this block is considered as exterior point block.Adjacent exterior point block characteristic instruction is as the number of the adjacent block of exterior point.If be exterior point too much, this characteristic is invalid.Empty block characteristic instruction has the number of seldom (such as, lower than threshold value) or the block without tracking characteristics.If too many block has tracking characteristics very little, not enough data can be used to verify single should mixing completely, and therefore, singly answer mixed model to be considered as this invalid.
For the estimation of streamlined motion model, estimate motion model, and by orderly order, from translation model, increase the number of DOF, evaluate validity relative to interframe movement.If translation model is confirmed as effectively, consider Similarity Model.If Similarity Model is confirmed as effectively, consider singly to answer model etc.At any point, if model is confirmed as invalid, stops this process and will think the effective part being used as the camera path of that frame at front model.If effective without motion model, use supposition camera path not movement identity motion model (such as, not performing stabilisation).This streamline is effective, if because lower DOF motion model is invalid usually, probably higher DOF motion model is also by invalid.
II.B camera path is analyzed
Camera path analysis module 320 receives tracking characteristics from motion estimation module 310 and effectively estimates motion model.Usually, camera path analysis module 320 uses these inputs to solve than the stable problem occurred on the time span that inter frame temporal span is longer, such as, the milliseconds up to a hundred of video to several seconds on the stable problem that occurs.Camera path analysis module 320 is thought effectively estimate motion model by being changed frame by frame, and presents the frame of particular characteristics by mark, performs correction.In one embodiment, camera path analysis module 320 comprises invalid propagation module 322, rolling shutter correction module 324 and covers and ambiguity correction module 326.
II.B.i is without effect spread
Invalid propagation module 322 is configured to interim stabilization, in order to the camera path of time stability smoothly in the longer extension of frame, and is a kind of device for performing this function.This usually occurs in the supposition between multipair frame instead of two frames based on instability.Such as, if be singly answer mixed model at the highest DOF effective exercise model of t-1, at t, it is Similarity Model, and be singly answer mixed model at t+1, in two frame time spans between its invalid frame occurring over just frame time t and time t+1 unlikely causing the higher DOF model of t.
For smooth camera path, multiplely close on frame pair propagating into the number of the DOF of the right the highest DOF valid model of framing.Using above-mentioned example, can be Similarity Model at the highest DOF valid model of time t.To multiple (such as 3) front and at rear frame pair, (such as t ± 1, t ± 2 and t ± 3), that number of DOF at the highest DOF valid model of front or rear continuous time compares with the number of the DOF of the highest DOF valid model of time t by invalid propagation module 322.If the number of the DOF of time t is lower, demotes and carry out the number of the DOF of t match time the highest effective DOF model (with regard to DOF) of front or rear continuous time.Continue example presented hereinbefore, using without effect spread, Similarity Model will be downgraded to from singly answering mixed model at the highest DOF valid model of time t-1 and time t+1.
In this propagation of execution, only the number of DOF propagates into previous frame, is previously to the motion model that frame with that DOF number is estimated at those actual motion models continuing time use in the front and back.This is because expect each frame between motion different, usually significantly different, and thus, the parameter of the motion model calculated at a time point will shall not be applied to another frame pair usually.In addition, multiple exercise is not without effect spread usually, otherwise all frames terminate with still motion model that is effective, the frame with less DOF number.
The output of invalid propagation module 322 is that the setting being different from the effective exercise model set received from motion estimation module 310 effectively estimates motion model.
II.B.ii rolling shutter corrects
Rolling shutter correction module 324 is configured to analyze tracking characteristics T
t, T
t+1with interframe movement M
tdetect and correct rolling shutter distortion, and be a kind of device for performing this function.Rolling shutter correction module 324 does not require to catch the relevant of camera how to catch video from original, or during taking, how to move any information of camera.When also non-frame is all partly recorded by the camera catching this video simultaneously, rolling shutter occurs.Although this can be the effect deliberately generated under single image catches use situation, normally less desirable in video.Rolling shutter will cause some different-effects, comprise rock, tilt, light leak and partial exposure.Usual rolling shutter effect is due to during catching at frame, and object fast moving in frame and occurring, makes object seem to rock, seem to tilt etc.
II.B.ii.a detects rolling shutter
For detect frame between rolling shutter effect, rolling shutter correction module 324 be configured to by for that frame to estimation singly answer model F
t (2)be applied to the tracking characteristics that frame is right.Determine multiple list should in point, wherein, single should in point be corresponding motion M
t,iwith by singly answering model to the tracking characteristics i of that frame to the motion match estimated so that in the threshold number of pixel.Such as, if threshold value is in 1.5 pixels, if between two of this centering frames, as the M that moved by the tracking characteristics in estimated x and y singly answering model to expect
t,iin 1.5 pixels of precision, tracking characteristics i is interior point.In this example, the weight w of feature
ito be 1/1.5=0.66667.To determine that multiple list should mix interior point in an identical manner, except replacing singly answering model F
t (2), use and singly answer mixed model F
t (3)outward.
Point poly-composition grid in tracking characteristics is determined point and single independent grid coverage that should mix interior point in single answering.The determination of grid coverage is similar to the above, but repeats for the purpose of hereafter knowing.Grid coverage (such as, 10 × 10) on frame.Based on the border in the coordinate position in frame and single storehouse, each tracking characteristics is arranged in a storehouse, because storehouse is not overlapping.To each unit (or storehouse) of grid, determine two scores, Dan Yingcang score and singly answer mixing bunker score.Dan Yingcang score determines that this storehouse is single should interiorly putting or exterior point.Similarly, this storehouse is that list should mix interior point or exterior point singly to answer mixing bunker score to determine.Each score based on relative to singly answering model or single should mixing, as the number of the tracking characteristics in the storehouse of interior point or exterior point.Based on the weight w of the tracking characteristics in storehouse
i, especially based on the intermediate value b of the characteristic weighing of the tracking characteristics in storehouse
j, further weight score.In one embodiment, based on hereafter, determine the score of the storehouse j of arbitrary situation:
Wherein, a and b
jit is the zoom factor for logistic regression score function.Grid coverage G
tbe the mean value of all storehouses score, make:
Determine two grid coverages, singly answer grid coverage G
t (2)singly answer hybrid grid coverage G
t (3), respectively based on they corresponding storehouse scores.
Usually, mixed model is singly answered than singly answering model modeling rolling shutter better.Therefore, usually, when there is rolling shutter effect, list should mix the grid coverage G had than singly answering model
t (2)higher grid coverage G
t (3).In one embodiment, rolling shutter correction module uses rolling shutter to enhance (boost) and estimate rse
tdetect rolling shutter effect, wherein, enhance rse
tfollowing ratio:
Be greater than 1 enhance rse
tsome motions (such as rolling shutter) that ordinary representation list answers mixed model just detecting singly to answer model not catch.Thus, singly answer mixed model to be considered to response that " enhancing " singly answers model.In one embodiment, rolling shutter correction module 324 is configured to determine to respond and enhances rse higher than what enhance threshold value (such as 1.1,1.3,1.4,1.9 etc.)
t, there is rolling shutter at time t.
Usually, rolling shutter effect occurs over a plurality of frames (such as, millisecond about up to a hundred was to several seconds).Rolling shutter correction module 324 determine multiple time/frame t (such as, 10% of the frame of 6 seconds fragments, hundreds of millisecond, or some other duration) enhance rse
t.If the threshold percentage of frame (such as, 30-100%) presents be greater than enhancing of specific threshold, so rolling shutter correction module 324 infers to there is rolling shutter effect to frame set.If do not meet the threshold percentage of frame, rolling shutter correction module 324 infers there is not rolling shutter effect to this frame set.
In another implementation, rolling shutter correction module 324 uses multiple difference singly to answer mixed model F
t (2, λ), detect rolling shutter effect, wherein, each singly answers mixed model to change relative to regularization matrix λ.As above, sufficiently high regularization matrix makes singly to answer mixed model rigidity, and thus, and singly answers model identical.Relatively low regularization matrix value (such as, 3 × 10
-5) better modeling is derived from the serious distortion of fast vibration image center (being such as arranged on helicopter or motorcycle by camera).Relatively high value regularization matrix value (such as 4.7x10
-4) the relatively slow mobile distortion of modeling (such as, with the people of camera walking, the video from ship shooting).
According to this implementation, be used for the right rolling shutter effect of frame for detecting, motion estimation module 310 estimates that multiple (such as 4) being used for this frame right singly answer mixed model, and wherein each singly answers mixed model to have different regularization matrix.Rolling shutter correction module 324 singly answers model F relative to estimated
t (2), determine the grid coverage G that each singly answers mixed model
t (2, λ)with enhance rse
t, λ.Due to the difference of regularization matrix, each singly answers mixed model F
t (2, λ)different rse is enhanced by having
t, λ.
Different rolling shutter effect of singly answering mixed model to allow more Accurate Model dissimilar.For determining whether there is rolling shutter effect for frame and determine which is applied singly answers mixed model, each singly answers enhancing of mixed model to compare from the different threshold value that enhances.Larger rigidity list should mix have with relatively low enhance compared with threshold value (such as enhancing threshold value 1.1-1.3) its enhance rse
t, λ.Not too rigidity list should mix and has and higher its rse enhanced compared with threshold value (such as 1.5-1.9)
t, λ.In one embodiment, have and meet the list enhancing the minimum rigidity regularization matrix of threshold value should to mix (or in other words, meeting the highest list enhancing threshold value should mix) be should mix for the list of that frame.In one embodiment, configure and variously enhance threshold value, if make to meet lower regularization matrix to enhance threshold value, also will meet the threshold value of all higher regularization matrix threshold values.
For determining whether there is rolling shutter effect in frame set, relatively meet various enhance threshold value singly answer mixed model.In one embodiment, if the percentage of the frame of this set (such as, 5-15% or higher) meets one that enhances in threshold value, so determine to there is rolling shutter effect.
II.B.ii.b corrects rolling shutter
For correcting rolling shutter effect, rolling shutter correction module 324 is configured to change the effective estimation motion model set received from motion estimation module 310.In one embodiment, if determine to there is rolling shutter effect in frame set, the effective exercise model being allowed for that frame set comprises answers mixed model F for those frames at the effective estimate sheet front determined
t (3).If determine there is not rolling shutter effect in frame set, the effective estimation motion model being used for that frame set is tied to and there are 8 DOF (such as, singly answers model F
t (2)) or lower motion model.In another embodiment, if determine to there is rolling shutter effect in frame set, upgrading is used for the effective exercise model of that frame set, makes, to all frames in this set, to think and singly answer mixed model F
t (3)effectively.
For determining that multiple singly answering in the implementation of mixed model corrects rolling shutter effect, first rolling shutter correction module 324 is determining whether there is rolling shutter effect, and if rolling shutter effect exists, which singly answers mixed model will be used for frame set.As above, if determine there is not rolling shutter effect in frame set, the effective estimation motion model being used for that frame set is tied to and there are 8 DOF (such as, singly answers model F
t (2)) or lower motion model.If determine to there is rolling shutter effect, what use singly answers mixed model F
t (2, λ)be meet for the frame in gathering particular percentile enhance threshold value singly answer mixed model.Singly answer mixed model to meet this condition if more than one, rolling shutter correction module 324 use have for the frame in this set most weak regularization matrix singly answer mixed model.As above, this implementation is depended on, all frames during this singly answers mixed model to may be used for gathering or only for being wherein confirmed as those frames effective for the mixed model of singly answering estimated by that frame.
II.B.iii covers and ambiguity correction
To cover and ambiguity correction module 326 mark presents the frame (or frame to) of fuzzy and remarkable static covering in a large number, and be a kind of device for performing this function.Mark is used for restriction to be arranged on camera path originally and/or be used in generation stable video with it.
By identifying the motion M presented close to 0
t,ithose tracking characteristics T of (being such as less than 0.2 pixel) and the quite little relative motion (such as <20%) relative to obvious camera translation
t,i, identify static covering.These tracking characteristics are indicated as static state.It is that static judgement determines whether frame on the whole has static covering that covering and ambiguity correction module 326 gather single tracking characteristics.For completing this operation, using grid as above, frame is divided into unit.If more than 30% of the tracking characteristics of unit is all indicated as static state, this unit is confirmed as static state.If preset time, the unit of t was indicated as covering, that instruction is propagated into and multiplely closes on frame (such as, 30) cover because static usually exist exceed one second some points several more than.If enough multiple unit of grid are indicated as have covering, by whole frame flag for comprising covering.To frame can be closed on by other of similar mark, repeat this process.There is static covering in these mark instructions, it can consider in generation stable video, as mentioned below.
Based on the angular measurement by pixel in the detection, detect motion blur or simple fuzzy control.Here angular measurement is similar with the angular measurement be used in above-mentioned tracking.But fuzzy for detecting, angular measurement can be different from parameter for following the tracks of and threshold value performs.Use angular measurement, to each frame, determine fuzzy score.
Covering and ambiguity correction module 324 are configured to the fuzzy score based on frame, are fuzzy by single frame flag.For being fuzzy by frame flag, the fuzzy score of frame compares the ratio between the fuzzy score of determining described frame and the fuzzy score that each closes on frame with multiple fuzzy score of each of closing on frame (such as 50 are closed on frame).Each of frame is closed on to those, determines this ratio individually.Based on multiple factor, comprise such as described frame and based on this than the time (or frame count) closed between frame poor, and described frame and close on frame area between frame overlapping/intersection, this ratio of weighting.If one or more ratio is higher than threshold value (such as, 2.5), described frame is marked as fuzzy.
II.C camera path is stablized
Camera path stable module 330 generates smooth camera path and editing conversion (or simple editing), and is a kind of device for performing this function.Camera path stable module 330 is by the tracking characteristics T generated by estimation 310 and motion M, as generated by motion estimation module 310 and the effective estimation motion model F improved by camera path analysis module 320 gather, and any mark generated by camera path analysis module 320 receives as inputting.
As mentioned above, camera path 306 can be exported individually.The camera path 306 of this output can comprise estimated motion model and/or the smooth paths that generated by camera path stable module 330 and editing.The input that smooth camera path and editing can also be used as stable video module 340 generates stable video 304.Camera path stable module 330 comprises camera path Leveling Block 332 and editing module 334.
II.C.i camera path is level and smooth
Camera path Leveling Block 332 is eliminated due to the shake of similitude (4DOF) and the smooth paths P of lower DOF camera motion by generating, smooth camera path.Smooth paths P does not consider or corrects higher DOF (being such as greater than 4) motion.Camera path Leveling Block 332 uses L1 path to stablize and estimated effective translation F
t (0), similitude F
t (1)with identity motion model, be created on the smooth paths P of the frame of time
t, and be a kind of device for performing this function.Use following formulae discovery at the camera path P of time t
t:
With
P
t＝C
tB
t(13)
P
tcomprise a series of segmentation, each segmentation is constant, linear and/or in parabolic motion one.For realizing this segmentation, by using constraint L1 to optimize, estimate P
t:
O(P)＝α
1|DP(t)|
1|α
2D
2P(t)|
1|α
3|D
3P(t)｜
1(14)
Wherein, D is differentiating operator.The result that camera path is level and smooth is two dimension (such as, along x and y-axis) the function P minimizing O (P)
t.As above, P is worked as
tonly based on Similarity Model F
t (1)time, P
tincomplete expression camera path.Usually, smooth camera path P
tcamera path 306 is represented on the whole in conjunction with higher DOF motion model (translation, Dan Ying, list should mix).
If be along smooth paths P at the frame of time t
tshooting, B
trepresent be applied to its editing conversion come use camera path generate stable video 304 occur, stable video thus.In one embodiment, CLP (computing basic facility Research on Logistic Operation (COIN-OR) linear programming) simplex (simplix) solver is used to determine B
t.Hereinafter, relative to editing module 334, further describe editing and determine.
Thus, camera path Leveling Block 332 is configured to based on the editing B from editing module 334
twith based on the similitude motion model estimated by right from each frame, output smoothing camera path P
t.If for giving the Similarity Model of framing invalid, determining in smooth paths and editing, translation or identity model can be used to replace Similarity Model.
II.C.ii editing
Editing module 334 is configured to the editing B determining each frame
t, and be a kind of device for performing this function.The size of editing management frames.For completely automatic video stabilization, by being present in the camera motion in video, determine editing B
t.Usually, editing module 334 is configured to the editing B of the content finding each frame of editing
t, make the remainder of frame have what part that each frame is shown by adjustment, compensate the freedom of undesirably moving.Although larger editing more easily completes usually, very large editing has elimination content frame, and does not provide the effect of other stable advantage.
Editing module uses equation (14) and determines to minimize the O in equation (14) at least approx by testing some different editing window sizes
i(P
t) editing B
t, determine editing B
t, wherein, i represents i-th editing test.In one embodiment, the editing tested comprises 95% editing, 90% editing, 85% editing etc., until comparatively Low threshold, such as 70% editing.In one embodiment, the best achievement in film editing c
optbased on absolute threshold a
swith relative threshold r
s.Example value comprises a
s=0.002 and r
s=0.8.
The best achievement in film editing size c
optthe percentage of frame rectangle.As above, editing conversion Bt and editing size have nothing to do.Although to each frame, may accurately determine the accurate editing minimizing O (P), from process viewpoint, finding out accurate desirable editing is poor efficiency.Even if only test some editings, as mentioned above, also can be quite intensive in calculating.In one embodiment, for improving the best achievement in film editing c being used for each frame
optthe efficiency of determination, determine the best achievement in film editing c to performing interim sub sampling (such as, if k=3, performing interim sub sampling every three frames) every k frame
opt.The best achievement in film editing c is determined in this reduction
optthe number of times altogether needed, reduces the total process determined needed for camera path thus.In one embodiment, for the best achievement in film editing c of the frame of interim sub sampling
optdetermination be not based on equation (14) but based on following equation:
O(P)＝α
1k|DP(kt)|
1|α
2k
2|D
2P(t)|
1|α
3k
3|D
3P(kt)|
1(16)
Determine editing conversion B
tcomprise multiple constraint.First, four angle c of editing window
kthere is the pre-sizing being less than frame sign.After the size at angle is defined in conversion, remain in frame to all four angles, such as [0,0]≤B
tc
k< [wide, high].C
kvalue based on editing conversion and by c
optthe size of the editing determined.This prevents at application editing B
tafterwards undefined go out interfacial area, alleviate the needs for the high motion of cost in repairing.Secondly, convergent-divergent change (such as, 90%) that the swing (such as, 15 °) allowed for editing and editing allow arranges boundary.Which has limited and camera path P
tabsolute deviation.3rd, if frame is marked as present enough large covering, by editing B
tbe tied to identity conversion.4th, if frame is marked as motion blur, inequality constraints is set, makes P
tpreserve the motion of a part (60%) original camera, be dithered as cost with more thus, suppress the fuzzy of the institute's perception in result.This can isolate a frame, or is dispersed on some consecutive frames.5th, by little negative weight, by the c of editing
optconvergent-divergent be increased to as in the target described in equation (14), effectively will be applied to against elastic force and editing window make result be partial to less editing.
In one embodiment, the fragment (or part) of time (such as, 6 second time) video is determined editing conversion B
t.Other constraint can be set to individual chip, but not necessarily can be applicable to other fragment.First, the axle of editing window deflection alignment and the frame (such as, null transformation, convergent-divergent 1 and zero rotate) centered by the first frame of fragment.This constraint is used for the initial orientation of the editing of fragment.Secondly, if the translation model of the first frame of fragment is considered to invalid, identity model is embedded in Similarity Model, and editing conversion is centered by the first frame of fragment.3rd, if the Similarity Model of the first frame of fragment is considered to invalid, identity model is embedded in similitude and the convergent-divergent of the rotation change on the frame of that fragment and editing is set as zero (such as only allowing translation DOF).
II.D stable video generates
Stable video generation module 340 is configured to use effective exercise model F
t (k)gather and convert B from the editing that each frame is right
t, generate stable video, and be a kind of device for performing this function.In one embodiment, for generating stable video 304, stable video module 340 generates each incoming frame I from original input video 302
tstabilizer frame J
t.In one embodiment, stable video module 340 is passed through according to editing B according to following equation
t, resampling primitive frame I
t, and compensate any residual movement by correcting resampling, generate each stable video frame J
t:
Wherein, x is the cartesian coordinate axes of the pixel of frame, wherein, and R
trepresent residual movement:
And wherein, H
t=F
t (k*), and wherein, k* equals 2 or 3, as long as effectively estimate motion model for the highest DOF of that frame.
According to B
tresampling I
tcorrecting camera motion has similitude (such as, DOF=4) or lower (such as, translation) DOF.But this resampling does not consider higher DOF camera motion, such as should with those camera motions of singly answering mixed model to catch by list.If do not perform further correction, the high frequency looked like in final stabilizer frame residue is rocked distortion by these higher DOF motions.Item H in addition
tand R
ton a frame by frame basis these higher DOF motions are tackled.They affect output frame J
t, wherein exist and be confirmed as effectively singly and/or singly to answer mixed model to that frame.
In fact, in order to solution comprises residue R
tequation (17), equation (17) is recursively expanded into:
Until some comparatively early time/frame p.Two key frames of set time t=p and t=n.Use simple resampling, that is, J
t(x)=I
t(B
tx) make
to intermediate frame t:p<t<n, use equation (19) recursively calculates the resampling position y from p to t
t (p)with the resampling position y of the chain backward used from n to t
t (n).Then, mix (or interpolation) these two resampling positions linearly and determine J
tx the end value of (), makes:
More generally, stable video module 340 is passed through editing B
twith the effective exercise model F estimated
tdirectly apply to each frame I
tpixel, delta frame J
t.The position that estimation motion model specifies each pixel from each frame will occur after stabilization, as truly having, is specified by this editing.This process can be completed to generate stable video 304 to all available frame.
II.E. camera shake detects
II.E.i summarizes
Shaking detection module 360 is configured to analyze video to determine whether video 302 will be benefited from stable, because and not all video all will be benefited.Determine benefiting from stable process, whether video is called that camera shake detects, or be called for short shaking detection.Shaking detection module 360 is configured to by generating multiple jitter feature, the amount of jitter in quantitation video.This jitter feature is used for determining whether stable video 302.
Automatically or through received request can perform shaking detection.In response to execution shaking detection, obtain this video relevant and whether there is enough shakes relative to threshold value with worth stable conclusion.Can, when reaching this threshold value, automatically perform stable, or in addition, based on the conclusion of shaking detection module 360, by performing stable option, the user of input video 302 can be pointed out.
For determining that video can change benefiting from stable threshold value between implementation.To the video with very little camera motion (or shake), stablely in fact make video than not performing and stablize even worse (such as, spectators are more difficult to watch).Can setting threshold, make only to perform when improving this video stable.Performing the stable processing cost related to also is a factor.All right setting threshold, makes only just to perform when sufficiently improving video to prove that processing cost is reasonable to stablize.Thus, for determining whether that the stable threshold value of application can change between implementation.
II.E.ii generates jitter feature
As mentioned above, for determining whether to stablize Video Applications, shaking detection module 360 is configured to by generating multiple jitter feature, quantizes the shake be present in video.For generating jitter feature, shaking detection module 360 is based on for the Similarity Model C estimated by frame of video
t(square journey 12), multiple spectrogram S of generating video 302.
Each spectrogram S describes frequency (or energy) component of the value of the single DOF of the Similarity Model on multiple consecutive frame.Thus, each spectrogram represents along x translation of axes t
xdOF, translation t along y-axis
ydOF, convergent-divergent change DOF or rotate change DOF.As mentioned above, for the value of each DOF of frame by the Parametric Representation in motion model, thus, the value of each similitude DOF is for the similitude motion model F estimated by that frame
t (1)in the value of parameter of correspondence.
Each spectrogram S also covers the finite time window (such as 128 frames, or about 5 seconds videos) of frame.Spectrogram is also partly overlapped in time, makes two spectrograms to share frame.Such as, the first spectrogram can based on frame 0-128, and the second spectrogram can based on frame 64-196, and the 3rd spectrogram can based on frame 128-256.
DOF value on the frame using window and use Fourier transform, such as discrete cosine transform (DCT)-II algorithm, generates the spectrogram S being used for each frame k
ka part frequency coordinate system in, generate spectrogram S:
Each spectrogram is used to the implementation of 128 frames, wherein, d
nrepresent the actuating quantity to the characteristic frequency/energy of the DOF value of the frame of window.Spectrogram S
ksingle part can be stored in data and store in 350, as the histogram comprising 128 storehouses, each storehouse represents characteristic frequency/energy range.Each storehouse has d
nheight, represent the effect in the storehouse of the DOF value of the frame to window.Thus, at S
kin, relatively high storehouse indicates the frequency/energy scope in that storehouse compared with another relatively Short Position, acts on the value of the DOF in window more consumingly.Usually, the higher histogram bin of upper frequency/energy represents stronger camera motion, the fast jitter of such as camera.Therefore, the higher histogram bin of lower frequency/energy represents slower camera motion.The DOF value of the frame of time window is aggregated into the histogram with multiple storehouse by histogram S, and wherein, each storehouse represents the effect of different frequency (or energy) scope to the DOF value of the frame in window.
Spectrogram can be compressed help save memory space.In one embodiment, convergent-divergent 2 is used to compress, because usually expect to find that the most of energy in most of video 302 spectrogram is in more low-yield.Convergent-divergent 2 is with interval [2
n, 2
n+1] gather all frequencies, cause 8 storehouses (2 altogether for spectrogram
8=128) instead of 128 storehouses of above-mentioned example.Thus, in execution compression, by the effect d of similar energies scope
nbe summarised in together.Using above-mentioned example, upon compression, is not 128 d
nvalue, but, spectrogram S
kevery part there are 8 d
nvalue, one for each energy bin.
Fig. 9 is according to an embodiment, for the diagram of multiple spectrograms of the different degrees of freedom of multiple time window and Similarity Model.Fig. 9 A illustrates first, length is the spectrogram of short length video 12 window, and the spectrogram that Fig. 9 B illustrates second, length is longer length video 40 window.To each DOF of the Similarity Model for each video, illustrate independent figure.The exemplary plot of Fig. 9 supposes each frequency spectrum Figure 128 frame, and convergent-divergent 2 compresses, and each spectrogram 8 energy bin thus, and in the frame of each spectrogram, about 50% windows overlay.The y-axis of each figure illustrates 8 energy bin, and storehouse number increases relative to energy.The x-axis of each figure presses the spectrogram of window diagram video.The color of each pixel of figure represents the energy fluence (namely moving) in the particular frequency range in each window of frame.
The spectrogram of comparison diagram 9A and 9B, more short-sighted frequency has the very little shake of DOF at higher-energy place, and longer video has remarkable amount of jitter at higher-energy place.Thus, can infer relative to more short-sighted frequency, longer video will be benefited from stable larger.
Any one of some distinct methods can be used, comprise such as based on fenestrate on the mean value of spectrogram histogram bin height, intermediate value and/or maximum and based on the independent histogram of energy gathering group spectrogram according to hundredths, generate jitter feature by spectrogram.Each of these methods is described now successively.
By adopting the one or more of the mean value in each storehouse on the window of video, maximum and intermediate value spectrogram height, one or more jitter feature set can be generated by spectrogram.As mentioned above, the altimeter in the storehouse of spectrogram is shown on the basis by window, and the energy/frequency of particular range is to the effect of the DOF value of window.Thus, fenestrate mean value represents on the window of video, and by window, the frequency/energy in that storehouse is as a whole to the mean effort of video.Similarly, maximum on all windows represents on the window of video, by window, the frequency/energy in that storehouse is as a whole to the maximum effect of video, and the intermediate value on all windows represents on the window of video, by window, the frequency/energy in that storehouse is as a whole to the intermediate value effect of video.Use above-mentioned example condition, if, there are 8 energy bin in every DOF, and supposition exists 4 DOF in Similarity Model, therefore, shaking detection module 360 generates 32 mean value jitter features, 32 maximum jitter features and 32 intermediate value jitter features for video.Note, the number of the jitter feature generated and the length (such as window number) of video have nothing to do.
Can by producing the histogrammic independent set in spectrogram territory, another jitter feature set is generated by spectrogram, a territory histogram is used for each energy bin of each DOF, and use above-mentioned exemplary condition thus, 32 territory histograms (such as, 8 energy bin are multiplied by 4 DOF) altogether.Each territory histogram has multiple storehouse (be called storehouse, territory to avoid obscure with the energy bin of spectrogram below).Each storehouse, territory has its oneself jitter feature.Continue the example presented above, if each territory histogram has storehouse, 10 territories, the jitter feature so generated by this technology amounts to 320.
Territory histogram is by the height/effect d of the single window of the single energy bin (in such as 0-7) of spectrogram
npoly-composition is relative to the hundredths scope of the effect of all energy bin of the spectrogram of all windows.Such as, on the convergent-divergent of [0,1], normalization territory histogram, wherein, 0 be expressed as 0 threshold d
n, or the least action d in histogram
n, min, and the highest actuating quantity d in 1 expression spectrogram
n, max.
Each storehouse, territory covers the percentage range of the restriction of threshold.The height in each storehouse, territory is the threshold d had within the scope of that hundredths
nenergy bin in the number of window.Such as, if each of storehouse, 10 territories covers 10% scope, the instruction of the height in the first storehouse, territory have the effect of the maximum effect in the arbitrary storehouse in spectrogram 0 to 10% between threshold d
nthe number of window of energy bin (the spectrogram storehouse 0 such as, in the 0-7 of storehouse).The height instruction in the second storehouse, territory has the threshold d between the 11-20% of the effect of the maximum effect in arbitrary storehouse in spectrogram
nthe number of window in identical energy storehouse (such as or spectrogram storehouse 0).
The height in storehouse, territory can, by the window sum normalization in video, make storehouse, territory constant relative to the length of video.Although the video of various length has different windows number, this allows to compare storehouse, the territory jitter feature from video.
II.E.ii determines whether stable video
Analyze jitter feature to determine whether stable application in video.In one implementation, shaking detection module 360 uses machine learning algorithm to train shake grader to determine whether that application is stable.For training shake grader, whether shaking detection module 360 uses the jitter feature from known video really to have made to order stable as training input with these known video.By will be stablized about these known video or do not carried out training classifier by stable judgement, this shake grader be trained to understand the video 302 whether should stablized and receive after a while.
Be used for training the jitter feature of shake grader can change between implementation.In one embodiment, 32 mean value jitter features, 32 maximum jitter features and 320 territory jitter features are used to carry out training classifier.In other embodiments, the combination in any of mean value, maximum, intermediate value and territory jitter feature can be used for training classifier.In other embodiments, the other feature of video also can be used to carry out training classifier.These features can comprise such as from the feature of the fuzzy inference be present in video, and non-jitter feature, the scene content of such as video and the audio frequency of video.
As long as trained shake grader, video 302 can be analyzed to determine whether stable video.Shaking detection module 360 processes this video to generate jitter feature as above.Jitter feature (with any other feature) is input to shake grader.Whether then shake grader exports should the determination of stable video.Then, automatically implement, or response user input implement stable.
III. illustrative methods
Fig. 4 is according to an embodiment, illustrates the flow chart of the process of the camera path for determining video.Stablize server 112 access 402 videos and generate 404 for the two-dimensional tracking feature of at least two consecutive frames of received video.The interframe movement of the tracking characteristics instruction camera of consecutive frame.Multiple different motion model is separately applied 406 in the tracking characteristics of frame to determine the characteristic of motion model.Each motion model has the DOF of different number.Based on this characteristic, make related activities model which effectively determine 408.Based on to the effective motion model of the interframe movement between consecutive frame, generate to describe and be used for the camera path 410 of motion of the camera catching video.
Fig. 5 is according to an embodiment, illustrates for detecting the flow chart with the process of the rolling shutter in correcting video.Stablize server access 502 video and generate the two-dimensional tracking feature of at least two consecutive frames of the video that 504 receive.The interframe movement of the tracking characteristics instruction camera of consecutive frame.Using singly answer models applying 506 in interframe movement determine as with multiple tracking characteristics of singly to answer in Model Matching point.Singly answer mixed model be employed 508 in interframe movement determine as with singly answer mixed model mate in point multiple tracking characteristics.In response to determining that 510 single numbers that should mix interior point exceed threshold value than the number of point in single answering, the determination that this video presents rolling shutter effect can be made.By the consecutive frame of will singly answer mixed model to be applied to video, generate stable video.
Fig. 8 is according to an embodiment, illustrates the flow chart of the process for detecting the camera shake in video.Systems stabilisation 112 accesses 802 videos and multiple frames to video, estimates the value (or parameter) of the DOF of 804 similitude motion models, as mentioned above.Systems stabilisation 112, to each DOF and time window, generates 806 spectrograms, makes each spectrogram based on the value of the DOF on the time window of multiple consecutive frames comprising video.Use spectrogram, systems stabilisation 112 generates 808 jitter features based on spectrogram.Systems stabilisation 112 based on jitter feature and the shake grader in front training, 810 videos of classifying.Shake grader by the one in visual classification 810 one-tenth two type, video that should be stable and video that should be unstable.Based on classification, systems stabilisation inputs automatically or in response to user, stablizes 812 videos based on classification.
IV. other consideration
Comprise the operation that foregoing description carrys out example embodiment, and foregoing description does not intend to limit the scope of the invention.Scope of the present invention is only limited by following claims.From above-mentioned discussion, many distortion will be apparent to persons skilled in the relevant art and be comprised by the spirit and scope of the present invention.
Claims (27)
1. a computer implemented method, comprising:
Accessing video;
Generate multiple tracking characteristics of each of at least two consecutive frames of described video, the interframe movement of the described tracking characteristics instruction camera of described consecutive frame;
The described interframe movement multiple motion model being applied to described tracking characteristics estimates that multiple characteristics of each of applied motion model, described motion model represent the dissimilar camera motion of the degree of freedom (DOF) comprising different number respectively;
By the described characteristic of described motion model being compared with corresponding threshold value, it is effective for determining in described motion model one or more;
Based on described effective exercise model, generate the camera path between described consecutive frame.
2. the method for claim 1, comprises further:
By described camera path being applied to the described consecutive frame of described video, generate stable video.
3. be the method for claim 1, wherein used for the data of motion of the original camera catching described video and independently generate described camera path with describing.
4. the described multiple tracking characteristics the method for claim 1, wherein generating in described frame comprises:
Angular measurement is applied to described frame, generates described tracking characteristics in multiple positions along two reference axis of described frame with high color gradient.
5. the method for claim 1, wherein, described motion model comprises the translation model with two degrees of freedom, and wherein, the described kinetic characteristic for described translation model comprises by least one in the following group formed: the standard deviation of multiple interior some kinetic characteristic, translation size and described translation size.
6. the method for claim 1, wherein, described motion model comprises the similitude motion model with 4 degrees of freedom, and wherein, comprise by least one of the following group formed for the described kinetic characteristic of described Similarity Model: multiple interior some kinetic characteristic, to change and frame rotation amount relative to the feature coverage of frame area, convergent-divergent.
7. the method for claim 1, wherein, described motion model comprise there are 8 degrees of freedom singly answer model, and wherein, the described described kinetic characteristic of model of singly answering comprises by least one of the following group formed: convergent-divergent change, frame rotation amount, perspective variable quantity and grid coverage amount.
8. the method for claim 1, wherein, described motion model comprises having singly answers mixed model more than 8 degrees of freedom, and wherein, described at least one composition for singly answering the described kinetic characteristic of mixed model to comprise the free group of following composition of origin: interior some block coverage, multiple adjacent exterior point block and do not have multiple pieces of interior some tracking characteristics.
9. the method for claim 1, comprises further:
To at least one in described frame, the highest DOF effective exercise model is propagated into and multiple frames of closing on described frame time, make those any motion models closing on frame of having than the higher DOF of the highest DOF effective exercise model invalid thus.
10. the method for claim 1, comprises further:
Determine that described video presents rolling shutter effect; And
In response to determining that described video presents described rolling shutter effect,
The described mixed model of singly answering being used in described consecutive frame is effective, and
Singly answer mixed model based on described, generate the stabilized camera path between described consecutive frame.
11. the method for claim 1, comprise further:
Determine that described video does not present rolling shutter effect; And
In response to determining that described video does not present described rolling shutter effect,
The described mixed model of singly answering being used in described consecutive frame is invalid, and
Singly answer mixed model without the need to described, generate the stabilized camera path between described consecutive frame.
12. 1 kinds of computer implemented methods, comprising:
Accessing video;
Generate multiple tracking characteristics of each of at least two consecutive frames of described video, the interframe movement of the described tracking characteristics instruction camera of described consecutive frame;
Determine singly to answer as with described the multiple tracking characteristics put in Model Matching in described interframe movement using singly answering models applying;
Using singly answer mixed model be applied to described interframe movement to determine as with described singly answer mixed model mate in point multiple tracking characteristics;
Determine that single number that should mix interior point exceeds threshold value than the number of point in single answering; And
By by described described consecutive frame of singly answering mixed model to be applied to described video, generate stable video.
13. methods as claimed in claim 12, wherein, describedly singly answer model and described dissimilar motion of singly answering mixed model to represent the degree of freedom with different number respectively.
14. methods as claimed in claim 12, wherein, one that determines in described tracking characteristics be single should in point comprise and determine describedly singly to answer the described interframe movement of of tracking characteristics described in model following in the pixel of threshold number.
15. methods as claimed in claim 12, comprise the most of frames pair in a part for described video, determine that single number that should mix interior point exceeds threshold value than the number of point in single answering.
16. methods as claimed in claim 12, wherein, determine that single number that should mix interior point exceeds threshold value than the number of point in single answering and comprises:
In described consecutive frame one is divided into the grid comprising multiple storehouse;
Determine to indicate at least have threshold number list should in point described grid in the first number storehouse singly answer grid coverage;
That determines to indicate the list at least with threshold number should mix the storehouse of the second number in the described grid of interior point singly answers hybrid grid coverage; And
Determine that described hybrid grid coverage of singly answering singly answers grid coverage to exceed to enhance threshold value than described.
17. methods as claimed in claim 16, wherein, determine that a list at least with threshold number in described storehouse should comprise by interior point:
Based on the subset in the described storehouse of the described grid be positioned at relative to described frame, determine the subset of the described tracking characteristics being arranged in described storehouse; And
Singly answer model based on applied, determine which tracking characteristics of described subset is interior point.
18. methods as claimed in claim 12, wherein, determine that single number that should mix interior point exceeds threshold value than the number of point in single answering and comprises:
In described consecutive frame one is divided into the grid comprising multiple storehouse;
Based on multiple Dan Yingcang scores in the described storehouse for described grid, determine singly to answer grid coverage;
Based on multiple Dan Yingcang scores in the described storehouse for described grid, determine singly to answer hybrid grid coverage; And
Determine that described hybrid grid coverage of singly answering singly answers grid coverage to exceed to enhance threshold value than described.
19. methods as claimed in claim 18, comprise and determine described multiple Dan Yingcang score, wherein, determine that in described Dan Yingcang score comprises:
Based on the subset in the described storehouse of the described grid be positioned at relative to described frame, determine the subset of the described tracking characteristics being arranged in described storehouse;
Singly answer model based on applied, determine which tracking characteristics of described subset is interior point;
Utilize each putting interior described in Weight; And
Based on the described weight of described interior point and described interior point, determine described storehouse score.
20. 1 kinds of computer implemented methods, comprising:
Accessing video;
Generate multiple tracking characteristics of each of at least two consecutive frames of described video, the interframe movement of the described tracking characteristics instruction camera of described consecutive frame;
Determine singly to answer as with described the multiple tracking characteristics put in Model Matching in described interframe movement using singly answering models applying;
Multiple mixed model of singly answering is applied to described tracking characteristics to described each of mixed model of singly answering, determine as with each singly answer mixed model mate in multiple tracking characteristics of point, described mixed model of singly answering has different rigidity;
Determine single number that should mix interior point than single should in the number of point exceed and be exclusively used in the described minimum rigidity list of the threshold value of mixed model of singly answering and answer mixed model; And
By answering mixed model to be applied to the described consecutive frame of described video described minimum rigidity list, generate stable video.
21. 1 kinds of methods, comprising:
Accessing video;
To multiple frames of described video, estimate the value of multiple degrees of freedom (DOF) of similitude motion model, every single-degree-of-freedom represents the different cameral motion of the original camera for catching described video, and the described value of described DOF represents the size of different cameral motion;
Generate the spectrogram of each being used for described DOF, each spectrogram is based on the described value of the described DOF on the time window of multiple consecutive frames comprising described video;
Based on described spectrogram, generate multiple jitter feature;
Based on described jitter feature, described video of classifying; And
Based on described classification, stablize described video.
22. methods as claimed in claim 21, wherein, described similitude motion model comprises transverse translation DOF, longitudinal translation DOF, convergent-divergent change DOF and rotates change DOF.
23. methods as claimed in claim 21, wherein, described jitter feature is constant relative to the length of described video.
24. methods as claimed in claim 21, wherein, the described time window of described spectrogram has the superimposed coverage area at least partially in the frame of continuous spectrogram in the front and back.
25. methods as claimed in claim 21, wherein, one of described spectrogram comprises the energy of multiple energy range to the effect of the value of a DOF in the described DOF on the frame of the time window of in described time window and represents, described spectrogram comprises the threshold of each for described energy range further.
26. methods as claimed in claim 25, wherein, described jitter feature is included in the mean effort value of an energy range in the described energy range on the described window of described video.
27. methods as claimed in claim 25, wherein, generate multiple jitter feature based on described spectrogram and comprise:
Generate the territory histogram of of described energy range, described territory histogram comprises storehouse, multiple territory,
Each storehouse, territory indicates the hundredths scope of the threshold defined by the maximum effect value of described spectrogram,
The height instruction in each storehouse, territory has the window number of the described energy range of the threshold within the scope of described hundredths; And
Based on the described height in each storehouse, territory, generate jitter feature.
Applications Claiming Priority (5)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201361799985P | 2013-03-15 | 2013-03-15 | |
US61/799,985 | 2013-03-15 | ||
US13/854,819 US9374532B2 (en) | 2013-03-15 | 2013-04-01 | Cascaded camera motion estimation, rolling shutter detection, and camera shake detection for video stabilization |
US13/854,819 | 2013-04-01 | ||
PCT/US2014/023211 WO2014150421A1 (en) | 2013-03-15 | 2014-03-11 | Cascaded camera motion estimation, rolling shutter detection, and camera shake detection for video stabilization |
Publications (2)
Publication Number | Publication Date |
---|---|
CN105052129A true CN105052129A (en) | 2015-11-11 |
CN105052129B CN105052129B (en) | 2019-04-23 |
Family
ID=51525713
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201480016067.2A Active CN105052129B (en) | 2013-03-15 | 2014-03-11 | Camera motion estimation, rolling shutter detection and the camera shake for video stabilisation is cascaded to detect |
Country Status (5)
Country | Link |
---|---|
US (3) | US9374532B2 (en) |
EP (2) | EP3800878B1 (en) |
KR (2) | KR102185963B1 (en) |
CN (1) | CN105052129B (en) |
WO (1) | WO2014150421A1 (en) |
Cited By (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN108156441A (en) * | 2016-12-06 | 2018-06-12 | 谷歌有限责任公司 | Visual is stablized |
CN109348125A (en) * | 2018-10-31 | 2019-02-15 | Oppo广东移动通信有限公司 | Video correction method, apparatus, electronic equipment and computer readable storage medium |
CN111031200A (en) * | 2018-10-10 | 2020-04-17 | 三星电子株式会社 | Electronic device, camera and image stabilization method |
CN111709979A (en) * | 2020-05-15 | 2020-09-25 | 北京百度网讯科技有限公司 | Image alignment method and device, electronic equipment and storage medium |
Families Citing this family (45)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8896715B2 (en) | 2010-02-11 | 2014-11-25 | Microsoft Corporation | Generic platform video image stabilization |
US9824426B2 (en) | 2011-08-01 | 2017-11-21 | Microsoft Technology Licensing, Llc | Reduced latency video stabilization |
US9374532B2 (en) | 2013-03-15 | 2016-06-21 | Google Inc. | Cascaded camera motion estimation, rolling shutter detection, and camera shake detection for video stabilization |
US9336460B2 (en) * | 2013-05-31 | 2016-05-10 | Intel Corporation | Adaptive motion instability detection in video |
US9277129B2 (en) * | 2013-06-07 | 2016-03-01 | Apple Inc. | Robust image feature based video stabilization and smoothing |
US9210327B2 (en) * | 2013-12-02 | 2015-12-08 | Yahoo! Inc. | Blur aware photo feedback |
US9939253B2 (en) * | 2014-05-22 | 2018-04-10 | Brain Corporation | Apparatus and methods for distance estimation using multiple image sensors |
CN105493496B (en) | 2014-12-14 | 2019-01-18 | 深圳市大疆创新科技有限公司 | A kind of method for processing video frequency, device and picture system |
US9418396B2 (en) | 2015-01-15 | 2016-08-16 | Gopro, Inc. | Watermarking digital images to increase bit depth |
US9877036B2 (en) | 2015-01-15 | 2018-01-23 | Gopro, Inc. | Inter frame watermark in a digital video |
GB2523253B (en) | 2015-01-23 | 2017-04-12 | Visidon Oy | Image processing method |
US9525821B2 (en) | 2015-03-09 | 2016-12-20 | Microsoft Technology Licensing, Llc | Video stabilization |
US10708571B2 (en) | 2015-06-29 | 2020-07-07 | Microsoft Technology Licensing, Llc | Video frame processing |
US20170006219A1 (en) | 2015-06-30 | 2017-01-05 | Gopro, Inc. | Image stitching in a multi-camera array |
CN105163046B (en) * | 2015-08-17 | 2018-11-06 | 成都泛视微星科技有限公司 | A kind of video anti-fluttering method for inhaling point imparametrization motion model based on grid |
US10341561B2 (en) * | 2015-09-11 | 2019-07-02 | Facebook, Inc. | Distributed image stabilization |
US10602153B2 (en) | 2015-09-11 | 2020-03-24 | Facebook, Inc. | Ultra-high video compression |
US10499070B2 (en) | 2015-09-11 | 2019-12-03 | Facebook, Inc. | Key frame placement for distributed video encoding |
US10063872B2 (en) | 2015-09-11 | 2018-08-28 | Facebook, Inc. | Segment based encoding of video |
US10375156B2 (en) | 2015-09-11 | 2019-08-06 | Facebook, Inc. | Using worker nodes in a distributed video encoding system |
US10602157B2 (en) | 2015-09-11 | 2020-03-24 | Facebook, Inc. | Variable bitrate control for distributed video encoding |
US10506235B2 (en) | 2015-09-11 | 2019-12-10 | Facebook, Inc. | Distributed control of video encoding speeds |
US9930271B2 (en) | 2015-09-28 | 2018-03-27 | Gopro, Inc. | Automatic composition of video with dynamic background and composite frames selected based on frame criteria |
GB2549074B (en) | 2016-03-24 | 2019-07-17 | Imagination Tech Ltd | Learned feature motion detection |
US9749738B1 (en) | 2016-06-20 | 2017-08-29 | Gopro, Inc. | Synthesizing audio corresponding to a virtual microphone location |
US10045120B2 (en) | 2016-06-20 | 2018-08-07 | Gopro, Inc. | Associating audio with three-dimensional objects in videos |
US9922398B1 (en) | 2016-06-30 | 2018-03-20 | Gopro, Inc. | Systems and methods for generating stabilized visual content using spherical visual content |
US10134114B2 (en) | 2016-09-20 | 2018-11-20 | Gopro, Inc. | Apparatus and methods for video image post-processing for segmentation-based interpolation |
US10313686B2 (en) | 2016-09-20 | 2019-06-04 | Gopro, Inc. | Apparatus and methods for compressing video content using adaptive projection selection |
US10003768B2 (en) | 2016-09-28 | 2018-06-19 | Gopro, Inc. | Apparatus and methods for frame interpolation based on spatial considerations |
US10489897B2 (en) | 2017-05-01 | 2019-11-26 | Gopro, Inc. | Apparatus and methods for artifact detection and removal using frame interpolation techniques |
KR102330264B1 (en) * | 2017-08-04 | 2021-11-23 | 삼성전자주식회사 | Electronic device for playing movie based on movment information and operating mehtod thereof |
AU2017245322A1 (en) * | 2017-10-10 | 2019-05-02 | Canon Kabushiki Kaisha | Method, system and apparatus for selecting frames of a video sequence |
US10740620B2 (en) | 2017-10-12 | 2020-08-11 | Google Llc | Generating a video segment of an action from a video |
US10534837B2 (en) | 2017-11-13 | 2020-01-14 | Samsung Electronics Co., Ltd | Apparatus and method of low complexity optimization solver for path smoothing with constraint variation |
US10587807B2 (en) | 2018-05-18 | 2020-03-10 | Gopro, Inc. | Systems and methods for stabilizing videos |
US10432864B1 (en) | 2018-09-19 | 2019-10-01 | Gopro, Inc. | Systems and methods for stabilizing videos |
US10482584B1 (en) * | 2019-01-31 | 2019-11-19 | StradVision, Inc. | Learning method and learning device for removing jittering on video acquired through shaking camera by using a plurality of neural networks for fault tolerance and fluctuation robustness in extreme situations, and testing method and testing device using the same |
US11089220B2 (en) | 2019-05-02 | 2021-08-10 | Samsung Electronics Co., Ltd. | Electronic test device, method and computer-readable medium |
CN114303364B (en) * | 2019-06-21 | 2024-02-23 | 高途乐公司 | System and method for stabilizing video |
KR102176273B1 (en) * | 2019-07-04 | 2020-11-09 | 재단법인대구경북과학기술원 | Method, system and computer program for video upright adjustment |
US11599974B2 (en) * | 2019-11-22 | 2023-03-07 | Nec Corporation | Joint rolling shutter correction and image deblurring |
US11694311B2 (en) * | 2020-03-04 | 2023-07-04 | Nec Corporation | Joint rolling shutter image stitching and rectification |
CN114095659B (en) * | 2021-11-29 | 2024-01-23 | 厦门美图之家科技有限公司 | Video anti-shake method, device, equipment and storage medium |
CN115439501B (en) * | 2022-11-09 | 2023-04-07 | 慧视云创(北京)科技有限公司 | Video stream dynamic background construction method and device and moving object detection method |
Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20080246848A1 (en) * | 2007-04-06 | 2008-10-09 | Canon Kabushiki Kaisha | Image stabilizing apparatus, image-pickup apparatus and image stabilizing method |
US20090066800A1 (en) * | 2007-09-06 | 2009-03-12 | Texas Instruments Incorporated | Method and apparatus for image or video stabilization |
US20120105654A1 (en) * | 2010-10-28 | 2012-05-03 | Google Inc. | Methods and Systems for Processing a Video for Stabilization and Retargeting |
CN102742260A (en) * | 2010-02-11 | 2012-10-17 | 微软公司 | Generic platform video image stabilization |
Family Cites Families (75)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5903454A (en) * | 1991-12-23 | 1999-05-11 | Hoffberg; Linda Irene | Human-factored interface corporating adaptive pattern recognition based controller apparatus |
JP3626218B2 (en) * | 1993-08-23 | 2005-03-02 | ソニー株式会社 | Motion amount detection apparatus and motion amount detection method |
AP9701084A0 (en) * | 1995-03-22 | 1997-10-31 | Idt International Digital Tech Deutschland Gmbh | Method and apparatus for coordination of motion determination over multiple frames. |
AU2001240100A1 (en) * | 2000-03-10 | 2001-09-24 | Sensormatic Electronics Corporation | Method and apparatus for video surveillance with defined zones |
US6965645B2 (en) * | 2001-09-25 | 2005-11-15 | Microsoft Corporation | Content-based characterization of video frame sequences |
US7433497B2 (en) | 2004-01-23 | 2008-10-07 | Hewlett-Packard Development Company, L.P. | Stabilizing a sequence of image frames |
US7847823B2 (en) * | 2005-01-14 | 2010-12-07 | Morpho, Inc. | Motion vector calculation method and hand-movement correction device, imaging device and moving picture generation device |
US20060215036A1 (en) * | 2005-03-25 | 2006-09-28 | Multivision Intelligent Surveillance (Hk) Ltd. | Method and apparatus for video stabilization |
US7558405B2 (en) * | 2005-06-30 | 2009-07-07 | Nokia Corporation | Motion filtering for video stabilization |
US8666661B2 (en) * | 2006-03-31 | 2014-03-04 | The Boeing Company | Video navigation |
JP4961850B2 (en) * | 2006-06-15 | 2012-06-27 | ソニー株式会社 | Motion detection method, motion detection method program, recording medium recording motion detection method program, and motion detection apparatus |
US8073196B2 (en) * | 2006-10-16 | 2011-12-06 | University Of Southern California | Detection and tracking of moving objects from a moving platform in presence of strong parallax |
US8019180B2 (en) * | 2006-10-31 | 2011-09-13 | Hewlett-Packard Development Company, L.P. | Constructing arbitrary-plane and multi-arbitrary-plane mosaic composite images from a multi-imager |
JP4209938B2 (en) * | 2007-03-06 | 2009-01-14 | パナソニック株式会社 | Image processing apparatus and method, image processing program, and image processor |
KR100866963B1 (en) * | 2007-03-12 | 2008-11-05 | 삼성전자주식회사 | Method for stabilizing digital image which can correct the horizontal shear distortion and vertical scale distortion |
WO2008131520A1 (en) * | 2007-04-25 | 2008-11-06 | Miovision Technologies Incorporated | Method and system for analyzing multimedia content |
KR101392732B1 (en) * | 2007-08-20 | 2014-05-08 | 삼성전자주식회사 | Apparatus and method for estimating motion by hand trembling, and image pickup device using the same |
JP2009077362A (en) * | 2007-08-24 | 2009-04-09 | Sony Corp | Image processing device, dynamic image reproduction device, and processing method and program for them |
TWI381719B (en) * | 2008-02-18 | 2013-01-01 | Univ Nat Taiwan | Full-frame video stabilization with a polyline-fitted camcorder path |
US8494058B2 (en) * | 2008-06-23 | 2013-07-23 | Mediatek Inc. | Video/image processing apparatus with motion estimation sharing, and related method and machine readable medium |
EP2157800B1 (en) * | 2008-08-21 | 2012-04-18 | Vestel Elektronik Sanayi ve Ticaret A.S. | Method and apparatus for increasing the frame rate of a video signal |
US8102428B2 (en) * | 2008-08-28 | 2012-01-24 | Adobe Systems Incorporated | Content-aware video stabilization |
GB0818561D0 (en) * | 2008-10-09 | 2008-11-19 | Isis Innovation | Visual tracking of objects in images, and segmentation of images |
JP2010097056A (en) * | 2008-10-17 | 2010-04-30 | Seiko Epson Corp | Display apparatus |
JP5284048B2 (en) * | 2008-11-12 | 2013-09-11 | キヤノン株式会社 | Image processing apparatus, imaging apparatus, and image processing method |
CA2687913A1 (en) * | 2009-03-10 | 2010-09-10 | Her Majesty The Queen In Right Of Canada, As Represented By The Minister Of Industry Through The Communications Research Centre Canada | Estimation of image relations from point correspondences between images |
US8749662B2 (en) | 2009-04-16 | 2014-06-10 | Nvidia Corporation | System and method for lens shading image correction |
KR101614914B1 (en) * | 2009-07-23 | 2016-04-25 | 삼성전자주식회사 | Motion adaptive high dynamic range image pickup apparatus and method |
US9626769B2 (en) * | 2009-09-04 | 2017-04-18 | Stmicroelectronics International N.V. | Digital video encoder system, method, and non-transitory computer-readable medium for tracking object regions |
WO2011036625A2 (en) * | 2009-09-23 | 2011-03-31 | Ramot At Tel-Aviv University Ltd. | System, method and computer program product for motion detection |
US8135221B2 (en) * | 2009-10-07 | 2012-03-13 | Eastman Kodak Company | Video concept classification using audio-visual atoms |
US9667887B2 (en) * | 2009-11-21 | 2017-05-30 | Disney Enterprises, Inc. | Lens distortion method for broadcast video |
US8553982B2 (en) * | 2009-12-23 | 2013-10-08 | Intel Corporation | Model-based play field registration |
US8179446B2 (en) * | 2010-01-18 | 2012-05-15 | Texas Instruments Incorporated | Video stabilization and reduction of rolling shutter distortion |
US8358359B2 (en) * | 2010-01-21 | 2013-01-22 | Microsoft Corporation | Reducing motion-related artifacts in rolling shutter video information |
US8350922B2 (en) * | 2010-04-30 | 2013-01-08 | Ecole Polytechnique Federale De Lausanne | Method to compensate the effect of the rolling shutter effect |
US20120010513A1 (en) * | 2010-07-08 | 2012-01-12 | Wong Stephen T C | Chemically-selective, label free, microendoscopic system based on coherent anti-stokes raman scattering and microelectromechanical fiber optic probe |
US8571328B2 (en) * | 2010-08-16 | 2013-10-29 | Adobe Systems Incorporated | Determining correspondence between image regions |
US8872928B2 (en) * | 2010-09-14 | 2014-10-28 | Adobe Systems Incorporated | Methods and apparatus for subspace video stabilization |
US8810692B2 (en) * | 2010-10-19 | 2014-08-19 | Apple Inc. | Rolling shutter distortion correction |
US9538982B2 (en) * | 2010-12-18 | 2017-01-10 | Massachusetts Institute Of Technology | User interface for ultrasound scanning system |
US20120182442A1 (en) * | 2011-01-14 | 2012-07-19 | Graham Kirsch | Hardware generation of image descriptors |
US8964041B2 (en) * | 2011-04-07 | 2015-02-24 | Fr Vision Ab | System and method for video stabilization of rolling shutter cameras |
US8724854B2 (en) * | 2011-04-08 | 2014-05-13 | Adobe Systems Incorporated | Methods and apparatus for robust video stabilization |
WO2013005316A1 (en) * | 2011-07-06 | 2013-01-10 | 株式会社モルフォ | Image processing device, image processing method, and image processing program |
US8718378B2 (en) * | 2011-07-11 | 2014-05-06 | Futurewei Technologies, Inc. | Image topological coding for visual search |
US8913140B2 (en) * | 2011-08-15 | 2014-12-16 | Apple Inc. | Rolling shutter reduction based on motion sensors |
TWI478833B (en) * | 2011-08-31 | 2015-04-01 | Autoequips Tech Co Ltd | Method of adjusting the vehicle image device and system thereof |
JP5729237B2 (en) * | 2011-09-26 | 2015-06-03 | カシオ計算機株式会社 | Image processing apparatus, image processing method, and program |
US8699852B2 (en) * | 2011-10-10 | 2014-04-15 | Intellectual Ventures Fund 83 Llc | Video concept classification using video similarity scores |
US8903043B2 (en) * | 2011-10-24 | 2014-12-02 | Bruker Axs, Inc. | Method for correcting timing skew in X-ray data read out of an X-ray detector in a rolling shutter mode |
US8457357B2 (en) * | 2011-11-09 | 2013-06-04 | Disney Enterprises, Inc. | Relative pose estimation of non-overlapping cameras using the motion of subjects in the camera fields of view |
TWI469062B (en) * | 2011-11-11 | 2015-01-11 | Ind Tech Res Inst | Image stabilization method and image stabilization device |
US8842883B2 (en) * | 2011-11-21 | 2014-09-23 | Seiko Epson Corporation | Global classifier with local adaption for objection detection |
US9003289B2 (en) * | 2012-02-23 | 2015-04-07 | Google Inc. | Automatic detection of suggested video edits |
US9232230B2 (en) * | 2012-03-21 | 2016-01-05 | Vixs Systems, Inc. | Method and device to identify motion vector candidates using a scaled motion search |
US20130251340A1 (en) * | 2012-03-21 | 2013-09-26 | Wei Jiang | Video concept classification using temporally-correlated grouplets |
US9129524B2 (en) * | 2012-03-29 | 2015-09-08 | Xerox Corporation | Method of determining parking lot occupancy from digital camera images |
ITVI20120087A1 (en) * | 2012-04-17 | 2013-10-18 | St Microelectronics Srl | DIGITAL VIDEO STABILIZATION |
US8948497B2 (en) * | 2012-09-04 | 2015-02-03 | Digital Signal Corporation | System and method for increasing resolution of images obtained from a three-dimensional measurement system |
US8860825B2 (en) * | 2012-09-12 | 2014-10-14 | Google Inc. | Methods and systems for removal of rolling shutter effects |
US9684941B2 (en) * | 2012-10-29 | 2017-06-20 | Digimarc Corporation | Determining pose for use with digital watermarking, fingerprinting and augmented reality |
EP2739044B1 (en) * | 2012-11-29 | 2015-08-12 | Alcatel Lucent | A video conferencing server with camera shake detection |
TWI602152B (en) * | 2013-02-06 | 2017-10-11 | 聚晶半導體股份有限公司 | Image capturing device nd image processing method thereof |
US9084531B2 (en) * | 2013-02-27 | 2015-07-21 | Siemens Aktiengesellschaft | Providing real-time marker detection for a stent in medical imaging |
US9317781B2 (en) * | 2013-03-14 | 2016-04-19 | Microsoft Technology Licensing, Llc | Multiple cluster instance learning for image classification |
US9374532B2 (en) * | 2013-03-15 | 2016-06-21 | Google Inc. | Cascaded camera motion estimation, rolling shutter detection, and camera shake detection for video stabilization |
US20140313325A1 (en) * | 2013-04-18 | 2014-10-23 | Ge Aviation Systems Llc | Method of generating a spatial and spectral object model |
JP6209002B2 (en) * | 2013-07-16 | 2017-10-04 | キヤノン株式会社 | Imaging apparatus and control method thereof |
US9554048B2 (en) * | 2013-09-26 | 2017-01-24 | Apple Inc. | In-stream rolling shutter compensation |
EP3068301A4 (en) * | 2013-11-12 | 2017-07-12 | Highland Instruments, Inc. | Analysis suite |
US9854168B2 (en) * | 2014-03-07 | 2017-12-26 | Futurewei Technologies, Inc. | One-pass video stabilization |
CN105095900B (en) * | 2014-05-04 | 2020-12-08 | 斑马智行网络(香港)有限公司 | Method and device for extracting specific information in standard card |
FR3027144B1 (en) * | 2014-10-09 | 2016-11-04 | St Microelectronics Sa | METHOD AND DEVICE FOR DETERMINING MOVEMENT BETWEEN SUCCESSIVE VIDEO IMAGES |
US20170213576A1 (en) * | 2016-01-22 | 2017-07-27 | Artur Nugumanov | Live Comics Capturing Camera |
-
2013
- 2013-04-01 US US13/854,819 patent/US9374532B2/en active Active
-
2014
- 2014-03-11 KR KR1020157029569A patent/KR102185963B1/en active IP Right Grant
- 2014-03-11 CN CN201480016067.2A patent/CN105052129B/en active Active
- 2014-03-11 KR KR1020207034211A patent/KR102225410B1/en active IP Right Grant
- 2014-03-11 EP EP20209572.5A patent/EP3800878B1/en active Active
- 2014-03-11 WO PCT/US2014/023211 patent/WO2014150421A1/en active Application Filing
- 2014-03-11 EP EP14768016.9A patent/EP2974274B1/en active Active
-
2016
- 2016-05-09 US US15/150,069 patent/US9635261B2/en active Active
-
2017
- 2017-03-20 US US15/463,127 patent/US9888180B2/en active Active
Patent Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20080246848A1 (en) * | 2007-04-06 | 2008-10-09 | Canon Kabushiki Kaisha | Image stabilizing apparatus, image-pickup apparatus and image stabilizing method |
US20090066800A1 (en) * | 2007-09-06 | 2009-03-12 | Texas Instruments Incorporated | Method and apparatus for image or video stabilization |
CN102742260A (en) * | 2010-02-11 | 2012-10-17 | 微软公司 | Generic platform video image stabilization |
US20120105654A1 (en) * | 2010-10-28 | 2012-05-03 | Google Inc. | Methods and Systems for Processing a Video for Stabilization and Retargeting |
Cited By (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN108156441A (en) * | 2016-12-06 | 2018-06-12 | 谷歌有限责任公司 | Visual is stablized |
CN108156441B (en) * | 2016-12-06 | 2021-07-30 | 谷歌有限责任公司 | Head-mounted device and computer-implemented method therein |
CN111031200A (en) * | 2018-10-10 | 2020-04-17 | 三星电子株式会社 | Electronic device, camera and image stabilization method |
CN111031200B (en) * | 2018-10-10 | 2023-05-23 | 三星电子株式会社 | Electronic device, camera, and image stabilization method |
CN109348125A (en) * | 2018-10-31 | 2019-02-15 | Oppo广东移动通信有限公司 | Video correction method, apparatus, electronic equipment and computer readable storage medium |
CN111709979A (en) * | 2020-05-15 | 2020-09-25 | 北京百度网讯科技有限公司 | Image alignment method and device, electronic equipment and storage medium |
CN111709979B (en) * | 2020-05-15 | 2023-08-25 | 北京百度网讯科技有限公司 | Image alignment method, image alignment device, electronic equipment and storage medium |
Also Published As
Publication number | Publication date |
---|---|
US20170195575A1 (en) | 2017-07-06 |
KR102185963B1 (en) | 2020-12-03 |
US9888180B2 (en) | 2018-02-06 |
EP2974274A1 (en) | 2016-01-20 |
EP3800878A1 (en) | 2021-04-07 |
KR20200137039A (en) | 2020-12-08 |
US20140267801A1 (en) | 2014-09-18 |
CN105052129B (en) | 2019-04-23 |
KR20150132846A (en) | 2015-11-26 |
EP3800878B1 (en) | 2024-01-31 |
EP2974274B1 (en) | 2020-12-09 |
US9635261B2 (en) | 2017-04-25 |
WO2014150421A1 (en) | 2014-09-25 |
EP2974274A4 (en) | 2016-11-09 |
US9374532B2 (en) | 2016-06-21 |
US20160269642A1 (en) | 2016-09-15 |
KR102225410B1 (en) | 2021-03-08 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN105052129A (en) | Cascaded camera motion estimation, rolling shutter detection, and camera shake detection for video stabilization | |
CN102742260B (en) | Generic platform video image stabilization | |
Kopf | 360 video stabilization | |
CN111178125A (en) | Intelligent identification of replacement areas for mixing and replacement of people in group portraits | |
US20170024852A1 (en) | Image Processing System for Downscaling Images Using Perceptual Downscaling Method | |
US9697592B1 (en) | Computational-complexity adaptive method and system for transferring low dynamic range image to high dynamic range image | |
CN113066017B (en) | Image enhancement method, model training method and equipment | |
Liu et al. | Survey of natural image enhancement techniques: Classification, evaluation, challenges, and perspectives | |
US9177406B2 (en) | Image mosaicing utilizing motion of scene content between successive images | |
CN115606174A (en) | Video quality evaluation method and device | |
US9098110B2 (en) | Head rotation tracking from depth-based center of mass | |
US20090136131A1 (en) | Method of Extracting an Object on a Projected Backdrop | |
Singh et al. | Visibility enhancement and dehazing: Research contribution challenges and direction | |
JP2012238932A (en) | 3d automatic color correction device and color correction method and color correction program | |
TW201528208A (en) | Image mastering systems and methods | |
Soma et al. | An efficient and contrast-enhanced video de-hazing based on transmission estimation using HSL color model | |
CN113556600B (en) | Drive control method and device based on time sequence information, electronic equipment and readable storage medium | |
CN113012254B (en) | Underwater image synthesis method based on pixel-level self-supervision training | |
CN111355881B (en) | Video stabilization method for simultaneously eliminating rolling artifacts and jitters | |
Bhavani et al. | Robust U‐Net: Development of robust image enhancement model using modified U‐Net architecture | |
Peterlevitz et al. | Sim-to-Real Transfer for Object Detection in Aerial Inspections of Transmission Towers | |
Tang et al. | An adaptive interpolation and 3D reconstruction algorithm for underwater images | |
Zhang et al. | See Blue Sky: Deep Image Dehaze Using Paired and Unpaired Training Images | |
Ito | Video Stabilisation Based on Spatial Transformer Networks | |
CN116977194A (en) | Video processing method and device, computer equipment and storage medium |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
C06 | Publication | ||
PB01 | Publication | ||
C10 | Entry into substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
CB02 | Change of applicant information |
Address after: American CaliforniaApplicant after: Google limited liability companyAddress before: American CaliforniaApplicant before: Google Inc. |
|
CB02 | Change of applicant information | ||
GR01 | Patent grant | ||
GR01 | Patent grant |