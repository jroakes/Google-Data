CN107077595A - Selection and presentation representative frame are for video preview - Google Patents
Selection and presentation representative frame are for video preview Download PDFInfo
- Publication number
- CN107077595A CN107077595A CN201580034616.3A CN201580034616A CN107077595A CN 107077595 A CN107077595 A CN 107077595A CN 201580034616 A CN201580034616 A CN 201580034616A CN 107077595 A CN107077595 A CN 107077595A
- Authority
- CN
- China
- Prior art keywords
- frame
- video
- semantic
- feature
- representative
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000000034 method Methods 0.000 claims abstract description 40
- 239000012634 fragment Substances 0.000 claims description 107
- 238000004590 computer program Methods 0.000 claims description 13
- 230000033001 locomotion Effects 0.000 claims description 10
- 230000004044 response Effects 0.000 claims description 5
- 238000004458 analytical method Methods 0.000 claims description 4
- 238000012549 training Methods 0.000 claims description 4
- 230000008859 change Effects 0.000 claims description 2
- 230000000007 visual effect Effects 0.000 claims description 2
- 230000004075 alteration Effects 0.000 claims 2
- 238000013442 quality metrics Methods 0.000 claims 2
- 238000010276 construction Methods 0.000 claims 1
- 238000001228 spectrum Methods 0.000 claims 1
- 230000011218 segmentation Effects 0.000 description 20
- 241000282320 Panthera leo Species 0.000 description 16
- 241000283899 Gazella Species 0.000 description 13
- 230000006870 function Effects 0.000 description 10
- 241001465754 Metazoa Species 0.000 description 5
- 238000004422 calculation algorithm Methods 0.000 description 5
- 238000005516 engineering process Methods 0.000 description 5
- 239000002131 composite material Substances 0.000 description 4
- 241000406668 Loxodonta cyclotis Species 0.000 description 3
- 230000009471 action Effects 0.000 description 3
- 241000282326 Felis catus Species 0.000 description 2
- 241001596291 Namibia Species 0.000 description 2
- 230000008901 benefit Effects 0.000 description 2
- 239000003795 chemical substances by application Substances 0.000 description 2
- 238000010586 diagram Methods 0.000 description 2
- 238000000605 extraction Methods 0.000 description 2
- 238000010606 normalization Methods 0.000 description 2
- 238000012545 processing Methods 0.000 description 2
- 241000218691 Cupressaceae Species 0.000 description 1
- 241000282313 Hyaenidae Species 0.000 description 1
- 230000002776 aggregation Effects 0.000 description 1
- 238000004220 aggregation Methods 0.000 description 1
- 238000013528 artificial neural network Methods 0.000 description 1
- 230000000712 assembly Effects 0.000 description 1
- 238000000429 assembly Methods 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 238000004364 calculation method Methods 0.000 description 1
- 238000005094 computer simulation Methods 0.000 description 1
- 238000013461 design Methods 0.000 description 1
- 238000001035 drying Methods 0.000 description 1
- 230000000694 effects Effects 0.000 description 1
- 230000001815 facial effect Effects 0.000 description 1
- 238000009432 framing Methods 0.000 description 1
- 210000003127 knee Anatomy 0.000 description 1
- 238000010801 machine learning Methods 0.000 description 1
- 238000012423 maintenance Methods 0.000 description 1
- 238000005259 measurement Methods 0.000 description 1
- 230000007246 mechanism Effects 0.000 description 1
- 230000005012 migration Effects 0.000 description 1
- 238000013508 migration Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 238000012544 monitoring process Methods 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 238000000513 principal component analysis Methods 0.000 description 1
- 230000008569 process Effects 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 238000000638 solvent extraction Methods 0.000 description 1
- 241000894007 species Species 0.000 description 1
- 230000003595 spectral effect Effects 0.000 description 1
- 230000003068 static effect Effects 0.000 description 1
- 238000013519 translation Methods 0.000 description 1
- 238000010626 work up procedure Methods 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/40—Scenes; Scene-specific elements in video content
- G06V20/41—Higher-level, semantic clustering, classification or understanding of video scenes, e.g. detection, labelling or Markovian modelling of sport events or news items
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/40—Scenes; Scene-specific elements in video content
- G06V20/46—Extracting features or characteristics from the video content, e.g. video fingerprints, representative shots or key frames
- G06V20/47—Detecting features for summarising video content
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/70—Information retrieval; Database structures therefor; File system structures therefor of video data
- G06F16/78—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/783—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
- G06F16/7834—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content using audio features
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/40—Scenes; Scene-specific elements in video content
- G06V20/46—Extracting features or characteristics from the video content, e.g. video fingerprints, representative shots or key frames
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/40—Scenes; Scene-specific elements in video content
- G06V20/49—Segmenting video sequences, i.e. computational techniques such as parsing or cutting the sequence, low-level clustering or determining units such as shots or scenes
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/70—Labelling scene content, e.g. deriving syntactic or semantic representations
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V2201/00—Indexing scheme relating to image or video recognition or understanding
- G06V2201/10—Recognition assisted with metadata
Abstract
There is provided a kind of computer implemented method for being used to select the representative frame of video.This method includes each frame identification feature set for receiving video and being video.The feature includes feature and semantic feature based on frame.The likelihood that the semantic feature identification semantic concept exists in the frame of video as content.Subsequently generate the video segment set for video.Each video segment includes the subset of the chronological frame of the frame from video, and each frame is associated with least one in the semantic feature.This method at least generates fraction based on semantic feature for each frame of the subset of the frame of each video segment, and the fraction based on the frame in video segment is that each video segment selects representative frame.Representative frame represents and summarizes video segment.
Description
Technical field
The disclosure relates generally to that representative video summaries are presented to user, and is selected more particularly to using semantic feature
Select representative video summaries.
Background technology
Video mandatory system stores and provides video to client device.As these video mandatory systems become increasingly
Prevalence, video mandatory system stores the video compared with long form more and more, sometimes more than the length of a few houres.These are more elongated
The video of formula can show various themes and setting, and describe many different scenes and object in video.Example
Such as, the wild animal video of entitled " Serengeti animal " may show many different animals, such as lion, gazelle, elephant
And hyena.These animals can show in various backgrounds, such as when herding, migrating or during chasing.Work as user
When browsing video, video trusteeship service provides some parts of video as the preview of video, the list such as since video
Individual frame.For the video compared with long form, the selection of preview generally can not represent the full content of video exactly, and user is not
It can rapidly distinguish whether particular video frequency has desired content in the case where not watching video in itself." Serengeti move
In thing " example, the preview can show the frame of lion rest, but user will not be able to really in the case of no viewing video
Determining video also includes migration gazelle.
The content of the invention
Representative frame from video is presented to user by video trusteeship service with the preview of video.This allows user to receive
On the additional context of video, and determine whether that selection will watch that video.Video trusteeship service is analyzed by video support
The video that pipe service is received describes the feature of each frame in video to generate.Such feature includes：The low level of frame is described
Information, such as color, motion and audio frequency characteristics；And the semantic feature of the presence of prediction each conception of species of frame in.Recognized in frame
Such concept include for example frame include certain types of object (" lion ") or action (" going hunting ").
Feature of the video trusteeship service based on video recognizes the fragment in video.Each fragment identification will be summarised in one
A part for the successive frame of the video risen.In one embodiment, each piece is determined by recognizing the shot boundary in video
Section.After identification set of segments, video mandatory system is analyzed each fragment and recognizing and can be used for the user profile fragment
Representative frame.In order to recognize representative frame, video mandatory system determines which semantic concept is wrapped in fragment and according in frame
The likelihood of semantic concept containing the fragment is given a mark to each frame in fragment.In one embodiment, fraction combination comes
From the fraction of multiple semantic concepts of frame, this can allow the frame for the multiple concepts for including fragment than the single concept including fragment
Frame receive higher fraction.The fraction of each frame can also include the aesthetic score for indicating the frame of its photographic quality.Selection
The frame in fragment with highest score is used as the representative frame of the fragment.Photo matter can be measured by acutance and contrast etc.
Amount.In various embodiments, semantic fraction combines to determine the gross score of the frame with aesthetic score.Select that there is highest in fragment
The frame of gross score as the fragment representative frame.
In one embodiment, the fragment of video is recognized by one or more different cutting techniques.By every kind of skill
The fragment that the fragment of art identification is referred to as in set of segments, and set of segments can be the lap of original video.Therefore,
Video can be split in a number of different manners by various set of segments.It is determined that the generation of each fragment of each set of segments
Table frame.As the result of multiple set of segments, video mandatory system is based on recognizing video for splitting the technology of video
Representative frame, the likelihood of the replacement part of increase representative frame capture video.Fragment and associated representative frame are stored
For the entry in piece segment table.Entry indicate the part of video of fragment, the representative frame of fragment and with the representative frame phase
The concept of association.
Video mandatory system receives the request for summarizing video.The request of general introduction video can browse video based on user
Video in mandatory system, or can be based on the search inquiry associated with request.Video mandatory system is by by fragment
The semantic concept semantic concept associated to request is compared to recognize fragment related with request in piece segment table.By right
Search inquiry, the analysis of user interest information are determined with asking by recognizing the semantic concept associated with the metadata of video
Seek associated semantic concept.When not searching for, in certain embodiments, all fragments in piece segment table are considered as related
's.
Next, from associated clip, selecting representative segment set.One or more representative segments can be selected.
Matching based on associated clip between the semantic concept associated with inquiry is given a mark to associated clip.In marking
Afterwards, representative segment set is selected to summarize video from associated clip.The selection of video mandatory system has highest score and anti-
The multifarious fragment reflected in the semantic concept associated with selected fragment.The representativeness associated with selected fragment
Frame is used for the general introduction for generating video.The general introduction combines representative frame and series of frames is presented to user in chronological order.Video
General introduction is provided to the user that may determine whether to watch whole video.
During video is watched, the instruction of associated clip or representative frame can also be presented to user in video playback.
Representative frame can be selected in a variety of ways, such as by by the fragment of search inquiry or user profiles and the video watched
In semantic concept match.In one example, the representative frame of additional clip is illustrated as adjacent with the video played.
In another example, one or more marks are shown on the timeline for the video played.These marks indicate various
When the representative frame of section occurs.Because representative frame it is associated with associated clip and based on user context (for example, letter
Shelves or search inquiry) select, so representative frame may indicate that user's frame of special interest.By playing the same of video
When show these frames, video mandatory system allow user readily recognize video user especially may be interested part, and
Part without finding video manually.
The feature and advantage described in the description are not entirely inclusive, and especially, it is contemplated that accompanying drawing, explanation
Book and claim, many additional feature and advantage will be apparent to those of ordinary skill in the art.In addition, should
Work as attention, the language used in the description primarily to readable and instruct purpose and select, and may not be by
Select to describe or limit subject of the present invention.
Brief description of the drawings
Fig. 1 is the block diagram of the example video trusteeship service according to one embodiment, wherein special using the semanteme of video segment
Levy to generate video preview.
Fig. 2 illustrates the segmentation and the selection of representative frame of the video according to one embodiment.
Fig. 3 illustrates the generation of the piece segment table of the representative frame of the instruction video segment according to one embodiment.
Fig. 4 illustrates the method for recognizing representative frame according to one embodiment.
Fig. 5 shows being used for from method of the piece segment table selection representative frame to be shown to user according to one embodiment.
Fig. 6 shows the video preview interface of the representative frame including video according to one embodiment.
Fig. 7 A show another video preview interface of the representative frame including video according to one embodiment.
Fig. 7 B-7D show the other interface for being used to the representative frame of video is presented according to various embodiments.
Fig. 8 shows the interface for being used to provide representative frame in player interface according to one embodiment.
Fig. 9 shows the interface for being used to provide representative frame for video using player interface according to one embodiment.
Accompanying drawing depicts embodiment of the disclosure for illustration purposes only.Those skilled in the art will hold from following description
Change places and recognize, the alternative embodiment of structures and methods shown in this article can be used, without departing from described herein public affairs
The principle opened.
Embodiment
System architecture
Fig. 1 is the block diagram of video trusteeship service 100, wherein generating video preview using the semantic feature of video segment.
Video preview is a part for video, such as frame, frame set, animation or other can be displayed for user to user and determines video
Content general introduction.User can use preview to determine whether request video to watch.Video trusteeship service 100 stores video
And provide video to the client of such as client device 135.Video host site 100 is carried via network 140 with multiple contents
Communicated for business 130 and client device 135, to promote the shared of the video content between user.In Fig. 1, in order to clear
Chu Qijian, illustrate only an example of content supplier 130 and client 135, although can have any amount of each.Depending on
Frequency trusteeship service 100 includes front end interface 102, video and provides module 104, video search module 106, upload server 108, use
User data storehouse 114, video storage storehouse 116 and feature repository 118.
Video trusteeship service 100 also includes the component for being used to selecting and providing the representative preview of video, and such as feature is carried
Modulus block 120, Video segmentation module 122, frame selecting module 124 and video summaries module 126.Not shown video trusteeship service
100 other general characteristics (such as fire wall, load balancer, certificate server, application server, failover services device
And site management tool) to be shown more clearly that the feature of video host site 100.The shown component of video trustship website 100
It can be implemented as the single or multiple components of software or hardware.Generally, it is described as being performed by a component in one embodiment
Function can also be performed by other components in other embodiments or by the combination of component.In addition, in one embodiment
Being described as if appropriate can also be in other embodiments by one or many by the function of the component execution of video trustship website 100
Individual client device 135 is performed.
Client device 135 is to perform client software (for example, web browser or built in client application) with via net
Network 140 is connected to the front end interface 102 of video trusteeship service 100 and shows the computing device of video.Make in these embodiments
Client device 135 includes such as personal computer, personal digital assistant, honeycomb, movement or smart phone or on knee
Computer.
Network 140 is typically internet, but can be any network, and including but not limited to LAN, MAN, WAN, movement are wired
Or wireless network, system for cloud computing, dedicated network or virtual network dedicated network.Client device 135 can include personal meter
Calculation machine or other equipment with network capabilities, such as personal digital assistant (PDA), mobile phone, pager, TV " machine top
Box " etc..
Conceptually, content supplier 130 provides video content to video trusteeship service 100, and client 135 is watched
The content.In practice, content supplier can also be content viewers.In addition, content supplier 130 can be operation video
The identical entity of host site 100.
Content supplier 130 operates client device to perform various content supplier's functions.Content supplier's function can
Including video file for example is uploaded into video trustship website 100, to edit the video text stored by video trustship website 100
Part or editor's content supplier preference associated with video file.
Client device 135 is equipment of the operation to watch the video content stored by video host site 100.Client
Equipment 135 can be also used for configuring the beholder preference relevant with video content.In certain embodiments, client device 135
Including embedded video player, such as from Adobe Systems, Inc FLASH players or suitable for video trustship net
Stand any other player of the video file format used in 100.Note, " client " used herein and " content is provided
Business " can be related to the software for providing both client and content offer function and be related to the hardware that the software is performed thereon.
" content supplier " also includes the entity of operation software and/or hardware, such as from obvious using the context of term.
The upload server 108 of video trusteeship service 100 receives video content from client device 135.What is received is interior
Appearance is stored in video storage storehouse 116.In response to the request from client device 135, video provides module 104 and considered oneself as future
The video data of frequency repository 116, which is provided, arrives client device 135.Client device 135 can also use video search module
106, the sense being stored in video storage storehouse 116 is for example searched for by inputting the text query comprising keyword interested emerging
The video of interest.Video search module 106 can from video summaries module 126 ask search result in any video preview,
As further described herein.Front end interface 102 is provided between the various assemblies of client 135 and video host site 100
Interface.Especially, front end interface 102 provides a user video preview interface, to allow user checking display complete video
Before the interface of itself video is checked to summarize form.
In certain embodiments, customer data base 114 is responsible for safeguarding all registered users' of video Entrust Server 100
Record.Registered user includes content supplier 130 and/or the user of video is only watched on video trustship website 100.Each
Content supplier 130 and/or personal user include login name, Email (e-mail) to the registration of video Entrust Server 100
Address and the accounts information of password, and it is provided unique ID.Account information is stored in customer data base 114.
Customer data base 114 can also store the user interest associated with user.The previous video that can be watched by user or user
The interest of input or User Activity on other websites in addition to video trusteeship service 100 determine user interest.
Video storage storehouse 116 includes the set for the video 117 submitted by user.Video storage storehouse 116 can include any
The video 117 of quantity, such as it is tens thousand of or several hundred million.Each in video 117 has itself and each area in other videos
Separated unique video identifier, such as text title (for example, character string " a91qrx8 "), integer uniquely name video
Any other mode.Video 117 can be encapsulated in such as AVI, MP4 or MOV various containers, and can be used all
Such as MPEG-2, MPEG-4, WebM, WMV, Video Codec H.263 and H.264 are encoded.Except in their audiovisual
Outside appearance, video 117 also has associated metadata 117A, such as such as title, description and/or label text element number
According to.The piece segment table of the video metadata 117A also identifications of the fragment of storage system maintenance video.Each fragment indicates to belong to same video
The successive frame set of camera lens.In addition to the representative frame of fragment, also by the instruction of the start and stop time with fragment
Fragment is stored in piece segment table.Representative frame is from being chosen so as to the shown frame with the fragment of summary clip in preview.
For example, the fragment can be identified as from 4:25 start and 8:05 terminates, with 4:The representative frame of 45 identification.When
When this fragment is used to summarize video, 4 are used:45 representative frame carrys out summary clip, as further described herein.In addition, fragment
Each fragment in table is identified as including one or more semantic concepts.
Feature repository 118 stores the vision or audio on one or more types for the video in video storage storehouse 116
Information (such as color, motion and audio-frequency information) characterizes the linked character set of video.The feature of video 117 and video are in itself
Original contents are different, and are derived from by characteristic extracting module 120.In one embodiment, feature be stored as value to
Amount, for consistency, vector have identical dimensional for each video 117.
Extracted in one embodiment using characteristic extracting module 120 be characterized in vision low level the spy based on frame
Levy.For example, one embodiment uses color histogram, the histogram of orientation gradient, color distortion, motion feature with consecutive frame
And tracking features, but other features based on frame can be used.It is characterized in collect on a per frame basis to be extracted, and
And other features based on frame, the facial quantity of such as identification or the histogram for orienting light stream can be included, and can include
The combination of the feature of extraction.Further feature, such as Laplacian (LoG) or Scale invariant are extracted in other embodiments
Eigentransformation (SIFT) feature extractor, use the form and aspect and the color histogram of saturation computation in hsv color space, motion
Rigidity characteristic, textural characteristics, wave filter response (for example, being exported from gal cypress (Gabor) small echo), including 3D wave filters are responded, made
With the edge feature at the edge detected by Tuscany (Canny) edge detector, gradient locations and orientation histogram (GLOH), it is based on
The shape histogram (LESH) of local energy accelerates robust features (SURF).Additional audio frequency characteristics can also be used, for example
Volume, audio spectrogram, speech-non-voice designator or the sense of hearing image of tranquilization.Feature can also include being directed to various images
The intermediate layer output for the deep-neural-network trained with video identification, classification or ranking task.Alternatively, it is special in order to reduce these
In terms of the dimension levied is while keep discrimination, feature is reduced.In one embodiment, used using the linear projection of study
The dimension of characteristic vector is reduced to 50 or some other suitable quantity less than 100 by principal component analysis, to perform feature
Reduce.Other embodiments can use supplementary technology to reduce the quantity of the dimension in characteristic vector when needed.
Characteristic extracting module 120 can also include multiple semantic classifiers to determine the semanteme related to semantic concept set
Feature.Semantic concept is allocated to the label of the content of video or frame, and can correspond to the entity of such as " dog " or " cat "
Or the free text of such as " dog chases cat ".The set of semantic concept changes according to embodiment, and can include for example
25,000 concepts.Semantic classifiers are specifying and exporting the frame and certain semantic conceptual dependency or retouch for receiving frame and its feature
State the computer model of the likelihood of certain semantic concept.For example, the semantic classifiers for semantic concept " dog " determine frame bag
Likelihood containing semantic concept " dog ".Likelihood can be determined within the scope of one, for example between zero and one.Frame is comprising semantic general
The likelihood of thought is stored as the semantic feature of frame.Each semantic concept is associated with semantic classifiers, and feature extraction mould
The application semantics grader of block 120 is to determine the semantic feature of semantic concept set.In this embodiment, it is using semantic classifiers
Each semantic concept generative semantics characteristic set, and semantic feature set is associated with each frame in video and is stored in
In feature repository 118.Semantic classifiers can be also used for being determined as whole video or the specific fragment for video or portion
The semantic concept being present in video divided.Semantic classifiers are trained by classifier training module (not shown), and training module makes
With monitoring data (for example, the specific mankind that frame or video belong to semantic concept specify) or by from the data associated with video
(for example, metadata of video) inference tag trains semantic classifiers.
Video segmentation module 122 recognizes the fragment of video.In order to recognize the fragment in video, 122 points of Video segmentation module
The vision and audio frequency characteristics of frame in analysis video.Video segmentation module 122 can be applied for determining the shot boundary in video
One kind or its combination in different technologies.In certain embodiments, using multiple many in video to recognize in these methods
In a set of segments.
Video segmentation module 122 can be used grader to recognize video segment.Shot boundary using mark is used as positivity
Characteristic set and it will be close to the frame on border and train as hard negativity gather, to train grader.The frame analyzed by the grader
Feature can include and the color distortion of consecutive frame, motion feature, audio volume and audio speech detect.Video segmentation module
Grader is applied to the frame of video to determine whether each frame is shot boundary by 122.
In one embodiment, Video segmentation module 122 recognizes video segment by using the coherence of frame feature.Phase
The similitude of feature in dryness measurement scheduled time fragment.Scheduled time fragment is for measuring similitude between frames
The short-movie section of video.This similitude provides the distance metric to Unsupervised clustering/partitioning algorithm, for example aggregation cluster, parent
Propagated with power or spectral clustering.The fragment of the output identification video of the algorithm.
Video segmentation module 122 can recognize video segment by across frame tracking visual signature.When more than number of thresholds or
Partial feature between those of frame frame when including changing, and the frame is identified as segment boundaries by Video segmentation module 122.Depending on
Frequency division cuts module 122 and one kind in technique described above or its combination can be used to recognize video segment.Then, video point
Cut module 122 and the fragment recognized is supplied to frame selecting module 124.
Frame selecting module 124 recognizes representative frame to represent and summarize video segment for each video segment.It is representative
Frame is most to represent the frame of the concept in video segment.When recognizing representative frame, frame selecting module 124 is special according to the semanteme of frame
Levy and the frame of fragment is given a mark, and the semantic feature of the semantic feature of frame and video segment is compared.Frame selecting module
124 can also generate the aesthetic score associated with frame and generate the fraction of the combination for frame.The fraction of the combination of frame is examined
Consider semantic fraction and aesthetic score.From the fraction of the combination of the frame of fragment, the selection of frame selecting module 124 has highest score
Frame as video segment representative frame.
For the semantic fraction of delta frame, frame selecting module 124 is by recognizing that the semantic concept in each frame regards to recognize
Semantic concept present in frequency fragment.When the semantic feature of the concept in frame is higher than threshold value, (for example semantic concept is present in frame
40,50 or 60% likelihood when), the semantic concept in frame is added to the semantic concept set of video segment.For
The each semantic concept recognized in fragment, frame selecting module 124 is by determining that concept is present in the amount in frame compared with reference value
To determine the fraction of the concept in frame.Reference value can be the average value of the concept in the frame of fragment, intermediate value, minimum value or
Maximum semantic feature, or can be zero.The each concept of 124 pairs of frame selecting module sums fraction, to generate each frame
Semantic fraction.By being summed to the fraction of each concept present in fragment, including multiple concepts in fragment frame more likely
It is selected as the representative frame of the fragment.For example, describing lion chases frame of the fragment including some description lions of gazelle, one
The frame of the combination of a little frames for only describing gazelle and some description lions and gazelle.In this example, lion and gazelle are described
Frame receives the semantic fraction for the presence for considering both lion and gazelle.
In one embodiment, calculate frame semantic fraction include by tag representation semantic concept linear combination and
Likelihood to the semantic concept in framing.As an example, frame f semantic fraction S is determined according to equation (1)：
S (f)=sum_c (concept_segment (c) * likelihood (c, f)) (1)
Wherein sum_c represents the sum of each semantic concept in fragment, and concept_segment (c) represents semantic concept pair
In fragment how significantly (for example, average likelihood on all frames in fragment), and likelihood (c, f) is that semantic concept c exists
Likelihood (concept scores of the particular frame) in frame f.Therefore, for each semantic concept in fragment, semantic fraction S is to piece
The generality of semantic concept in section is multiplied by the likelihood summation of the semantic concept in frame.Therefore, the semantic fraction of frame is highlighted
Semantic concept (being represented by likelihood (c, f)) in frame is dominant (by concept_segment (c) in whole video segment
Represent) frame.
In addition to semantic fraction, in one embodiment, fraction also includes aesthetic score to help to select also in aesthetics
Upper pleasant representative frame.Aesthetic score is determined for each frame, and using such as amount of exercise, acutance, away from fragment side
The distance (for example, first of fragment and last frame) and the Individual Quality of photographic quality on boundary determines aesthetic score.This
Each in a little aesthetic qualities is combined to determine the aesthetic score of frame, and machine learning model can be used to pass through summation
Or combined by another means.
Frame selecting module 124 combines semantic fraction and aesthetic score to generate the composite score for each frame, and it is used for
Identification is selected as the representational frame for the fragment.Fraction can be normalized before the combination, and the combination
Can the model based on computer learning, or can be the summation of fraction.
In order to normalize fraction, function can be calculated for semantic and aesthetic score, for example average value, maximum, minimum value,
Noise or (noisy-or) or k- noises or.Normalization or non-normalized value that can be to signal calculate these functions.Can be
In fragment, in the window of the frame around the frame being scored, either using the fraction across video or using in database
The fraction of Sample video calculates normalization (such as map fraction be 0-1).
In one embodiment, frame selecting module 124 receives aesthetic score by application and semantic fraction is used as input
Computer learning model come determine combination fraction.The model of computer learning can be trained in a variety of ways, for example using into
To data, (better than frame y) or using returning, (frame x has fraction s) to frame x.Normalized fraction not as described above can also be used
Perform model.
It is determined that after the composite score of each frame in fragment, frame selecting module 124 is according to the fraction of combination to fragment
In frame carry out ranking.The frame (that is, the frame of the fraction combined with highest) of the selection top ranked of frame selecting module 124 is used as should
The representative frame of fragment.In one embodiment, frame selecting module 124 selects to represent using only the semantic fraction of highest in frame
Property frame.Frame selecting module 124 it is also possible to use similar techniques and select the representative audio of frame, and select across the audio of some frames
A part.Representative audio can be selected from the audio at the frame around selected representative frame.Selected in frame selecting module 124
After representative frame, representative frame is specified with fragment and is collectively stored in the piece segment table associated with video.With representative frame phase
The semantic concept of association can also be stored in piece segment table.In one embodiment, frame selecting module 124 is from Video segmentation module
122 receive multiple set of segments.Multiple set of segments are determined by using the method for different segmentation videos.These fragment collection
Each in conjunction can be stored together with the associated representative frame of piece segment table and each fragment.
In one embodiment, before video to be supplied to client terminal device 135 for viewing, by frame selecting module
124 and the Executive Agent's property frame of Video segmentation module 122 selection and Video segmentation., can when upload service 108 receives new video
To recognize representative frame.By recognizing representative frame when uploading (or before video is watched in request) and generating piece segment table, then
Piece segment table can obtain to recognize that representative frame is used to show before user asks.
Fig. 2 illustrates the segmentation and the selection of representative frame of the video according to one embodiment.As described above, by video trustship
The segmentation and selection of the component Executive Agent property frame of service 100.Video 200 is divided into fragment 210 by Video segmentation module 120
Set.Each fragment includes chronological frame set 220, here depicted as frame F1-F7.Each frame with by characteristic extracting module
The semantic feature set of 120 identifications is associated.In this example, shown fragment is to show that lion chases the fragment of gazelle.
In the fragment, initially, frame depicts lion, then in frame F3And F4Place shows gazelle, in F5Place lion starts to follow the trail of gazelle,
And in F6In all in frame in and be identified, and in F7In identify single lion.As described above, in one embodiment
In, these semantic features identification semantic concept is present in the likelihood in frame, although and shown here as " presence ", it is semantic
Concept can only indicate that specific concept (such as " lion ") may or be likely present in frame, or can include concept in frame
The floating-point likelihood or probability of middle appearance.After the semantic concept marking in frame, the selection frame of frame selecting module 124 F6As
Representative frame in the fragment.When being given a mark to frame, frame selecting module 124 recognizes the semantic concept associated with the fragment
It is " lion " and " gazelle ".Frame F6(due to including lion and gazelle) receives the fraction of each concept and considers the total of each concept
Semantic fraction.After aesthetic score generation composite score is alternatively considered, frame F is selected6It is used as representative frame 230.In practice
In, multiple frames potentially include concept " lion " and " gazelle ".It can help to recognize which in these frames with reference to aesthetic score
It is aesthetically most pleasant for user.
Fig. 3 shows the generation of the piece segment table of the representative frame of the video segment of the instruction video according to one embodiment.
In the example, video 300 includes various animals.If Video segmentation module 122 is analyzed using the drying method of identification video segment
Video, it produces identified video segment set 310A-310C.For each video segment in the set, as described above,
Representative frame 315 is recognized by frame selecting module 124.Because various dividing methods can recognize that the different boundary in video 300, therefore
The different representative frame of various Piece Selections can be directed to, as shown in the figure.Fragment and representative frame are stored in piece segment table 320,
The identification of piece segment table 320 fragment, the representative frame of each fragment and the semantic concept set associated with representative frame.
Fig. 4 shows the method for recognizing representative frame according to one embodiment.This method is by the reality with reference to Fig. 1 descriptions
The characteristic extracting module 120 in example, Video segmentation module 122 and frame selecting module 124 is applied to perform.Initially, being received 400 is used for
Recognize the video of representative frame.Video trusteeship service 100 can be uploaded in response to video to receive video to recognize representativeness
Frame, or another time that can be after upload receive video.It is video identification feature as described above, including base 410
The semantic feature of semantic concept present in the feature and identification frame of frame.Semantic feature can be determined from the feature based on frame,
For example by the feature based on frame for being applied to recognize for frame by semantic classifiers, to determine the one or more semantic special of the frame
Levy.Video features are analyzed by Video segmentation module 122 to generate video segment 420, video segment 420 can be included by multiple videos
Multiple set of segments that dividing method is determined.For the fragment recognized, the frame in 430 pairs of fragments is given a mark to generate language
Adopted fraction.In one embodiment, semantic fraction includes the composite score for incorporating the aesthetic score of frame.Using with fragment
The associated fraction of frame, the representative frame of fragment is selected 440 from the frame with highest score.The fragment recognized and representative
Property frame can be added to the piece segment table of video.
Fig. 1 is returned to, video summaries module 126 uses the preview that piece segment table is that user generates video.Video preview is used for
Generate video " Storyboard (storyboard) " to describe the representative frame of video, and can select with it is customer-furnished
The representative frame that search inquiry is related or interest of to user is related.
Video summaries module 126 receives the request of the preview of generation video.User can be worked as and browse video trusteeship service 100
On video when provide request from front end interface, or can be provided for the search result of video from video search module 106
The request is to generate preview.The video of preview will be generated for it by generating the request instruction of preview, and can be in customer data base
114 include search inquiry or ask the mark of user.
After the request of general introduction video is received, video summaries module 126 recognizes the fragment of the video related to request.
When being not received by search inquiry, all fragments are considered correlation.As an alternative, the metadata associated with video
(for example, title and any label for being associated to video) can be chosen as related lexical item to determine fragment and representative frame
Correlation.When receiving search inquiry, search inquiry is translated into related lexical item to recognize related lexical item to analyze video simultaneously
Recognize which semantic concept is described by search inquiry.In addition, the request user recognized can with it is emerging in customer data base 114
Interest is associated.Various related lexical items are translated into semantic concept with the correlation for the fragment for determining video.By the related term of translation
Item and the semantic concept associated with the representative frame of the fragment of video are compared.Video summaries module 126 will include and phase
The representative frame for closing the concept of the semantic concept matching of lexical item is identified as associated clip, and uses these fragments as potential fragment
To generate the preview of video.
Identification video associated clip after, video summaries module 126 identification will use which associated clip (and represent
Property frame) generate the preview of video.In order to select the representative frame for preview, video summaries module 126 generates the generation of fragment
The relevance scores of table frame.Phase is calculated using the metadata of the semantic feature on representative frame, inquiry or user interest
Closing property fraction.The relevance scores match the semantic feature of metadata, inquiry or user interest with the semantic feature.Phase
Closing property fraction is ranked, and selects the relevance scores of top ranked as representative frame.In addition, the semanteme of selected frame
Concept can be used for selecting other representative frames.In one application, the semantic concept in selected frame is emphasized in the selection of frame
Diversity.For example, can be preferable over similar language as the frame with different semantic concepts of chosen frame
The frame of adopted concept.The representative frame of specified quantity is selected to represent video, such as 3 or 5.
Selected frame can also be organized to be shown to user in chronological order.In one embodiment, video summaries
Module 126 generates animation to generate video summaries by using selected representative frame.The animation provides a user video
The brief overview of representative frame, and allow user quickly to determine user whether to video interested.
In one embodiment, video summaries module determines whether to replace video also based on selected representative frame
Default thumbnail.Each video can be associated with default thumbnail, and the default thumbnail can be referred to by the user of uploaded videos
It is fixed, or can be selected based on the semanteme of the aesthetic features of video.In certain embodiments, video summaries module 126 passes through
The relevance scores of selected representative frame and the relevance scores on default thumbnail calculating are compared to determine
Whether default thumbnail is replaced.Correlation can be calculated on video metadata, search inquiry or user interest as described above
Fraction.When representative frame correlation score ratio default thumbnail is higher by threshold value, representative frame is selected for use in display
Replace thumbnail.
In one embodiment, inquiry lexical item or user interest is not based on to determine representative segment, but will inquiry
Lexical item and user interest are incorporated in the marking of the selection of the representative frame for preview, and are increased for the representativeness in preview
The marking of frame, and do not influence selection to be related representative segment.That is, the language associated with inquiry or user interest
Adopted concept be used to increasing with inquire about or user interest the fraction of representative frame that matches of semantic concept.
Fig. 5 shows being used for from method of the piece segment table selection representative frame to be shown to user according to one embodiment.
In embodiment shown in Fig. 1, this method is performed by video summaries module 126.Initially, 500 receive for summarize video and to
User provides the request of preview.The request can specify search for inquiring about and/or asking the user of video.Next, in 510, base
The piece related with request is recognized in search inquiry, the user interest for the user for asking video or the metadata associated to video
Section.Can by by the semantic concept associated with fragment and the semantic concept associated with request be compared to recognize piece
Section.For example, can be recognized from the piece segment table of the associated semantic concept including video segment, the representative frame of fragment and fragment
The semantic concept associated with fragment.Can be by analyzing search inquiry or user interest information, or pass through identification and video
The associated semantic concept of metadata, to determine the semantic concept associated with request.
520 representative segment is selected from the fragment related to request is confirmed as.Based on video metadata and use
The correlation of the context (for example, search inquiry or user interest of user) at family is carried out a pair fragment related to request and given a mark
And selection.For example, the matching between the semantic concept being associated based on fragment and to inquiry is entered come a pair fragment related with request
Row marking.Selection has highest score and reflects the multifarious fragment of semantic concept.It can be determined and selected generation from piece segment table
The associated representative frame of table fragment.Video summaries module 126 is regarded using the representative frame of selected representative segment to generate
Frequency general introduction 530.Video summaries combine representative frame in chronological order, and for example in static " Storyboard " or can pass through
Frame is synthesized to the animation changed from a frame to another frame sequential, a series of representative frames are presented to user.Video summaries
It is provided to the user for determining whether to watch whole video.
Fig. 6 shows the video preview interface 600 of the representative frame including video according to one embodiment.Video preview
Interface 600 is provided to client device 135 to browse video and determine whether intactly to watch based on video preview
Video.In this example, the search inquiry of user's input " corvette unveiling (Ke Erweite displayings) ", and responding
Multiple videos are determined in the request.Search inquiry and obtained video are supplied to video summaries module 126, for selecting generation
The preview of table frame and video.
In this example, in the Part I of display, three video 610A-610C set is selected as correlation.
Each associated video is analyzed to determine the relevance scores of representative frame and each representative frame.Correlation can be determined as described above
Property fraction is to recognize the frame related to search inquiry or user profiles.In this example, when representative frame exceedes threshold correlation
During fraction, video summaries module 126 selects representative frame 620 with the video in display.In this example, video summaries
Frame of the selection of module 126 with the highest relevance scores more than threshold correlation fraction is presented on video preview interface 600.
Also show in this example, video 610B and 610C do not have the representativeness of the relevance scores higher than threshold correlation fraction
Frame, and do not shown in the preview interface of representative frame 620.
In the another part at video preview interface 600, scene preview 630 is shown to user.Except associated video 610A-
Outside 610C, scene preview 630 can also be shown, or scene preview 630 can be shown on single interface or display.
In this example, scene preview 630 shows the thumbnail 640 of relevant search result.In this example, will for each video
Default thumbnail replaces with representative frame.Therefore, each shown thumbnail 640A-640C is having for each search result
The representative frame of highest relevance scores.It is every in thumbnail 640, the generation associated video of video summaries module 126 in order to generate
The relevance scores of individual fragment, and select the representative frame of highest score.Representative frame replace default thumbnail image so as to
Shown in scene preview 630.By this way, scene preview 630 presents most preferably general by the search inquiry inputted on user
State each video that the representative frame of video is summarized.When user selects representative frame, video can be shown to user, and
The playback of video starts in representative frame, it is allowed to the representative frame that user is jumped in video.In modification, representative frame is selected
Cause to start playback at the beginning of the fragment comprising representative frame.As described above, relevance scores are also conceivable to user's letter
Shelves other information is for determination relevance scores.Although in addition, scene preview 630 is herein shown as video preview interface
600 part, but in the present embodiment, interface element 650 allows user to watch additional to be regarded by what representative frame was summarized
Frequently.The interface element 650 is provided a user also with the default thumbnail replaced with inquiry or the specific representative frame of user
Additional search results.
Fig. 7 A show another video preview interface 700 of the representative frame including video according to one embodiment.Depending on
Frequency preview interface 700 is provided to client device 135, for browsing video and being determined whether intactly based on video preview
Watch video.In this example, user have input the search inquiry of " bulldog skateboarding (bulldag slide plate) ",
And several videos are determined in response to the request.Search inquiry and obtained video are supplied to video summaries module 126, used
In the preview of selection representative frame and video.In this embodiment, representative frame 710A, 710B and 710C collection are provided a user
Cooperate the preview for each video.That is, be not to select single representative frame as shown in Figure 6, in this embodiment it is possible to
Select multiple frames of video and be presented to user.This allows user to determine that user wants to watch in which video and video
Particular representative frame.When user selects representative frame, video can be shown to user, and video playback in representative frame
Start, it is allowed to the representative frame that user is jumped in video or the fragment comprising representative frame.By showing the request with user
Related representative frame, user can quickly determine which in watching these videos user want from video preview interface 700
It is individual.Further, since representative video segment is determined before searching request in one embodiment, so video mandatory system
100 can determine the representative frame at video preview interface 700 in search inquiry in the case of significant processing frame by frame.
Fig. 7 B-7D show the other interface for being used to the representative frame of video is presented according to various embodiments.As schemed
Shown in 7B, representative frame 710 can be specified or highlighted as the search with user or user by video trusteeship service 100
(being in this example " elephant " or " Namibia (Namibia) elephant ") is especially relevant.In this example, representative frame 710D
Highlighted with 710E by the underscore in the profile and 710E in representative frame 710D.In order to generate these interfaces, depending on
Frequency general introduction module 126 determines the relevance scores associated with representative frame for the representative frame set and generation of user.
Representative frame is identified and is presented to highlighting come ranking, and top ranked representative frame by relevance scores
User.Representative frame is herein shown as according to time sequence, but it is also possible to sorted according to the relevance scores of representative frame.Figure
7C shows the selection of the representative frame of video.As seen in figure 7 c, video preview interface 700 includes how indicating particular representative frame
When there is timeline 720 or progress bar 730 in video.Fig. 7 D show another video preview interface 700, wherein with grid
Configuration shows representative frame 710.
Fig. 8 shows the interface for being used to provide representative frame in player interface 800 according to one embodiment.Player
Interface 800 be user interact with play video and adjust video control (for example, volume, beginning, stopping, search and its
It act) interface.Player interface 800 also includes the progress bar of the part for the length and video watched for indicating video
805.One or more representative frames that the identification of video summaries module 126 can be indicated in player interface 800.In the example
In, mark 810 of the time that representative frame occurs in video on progress bar 805 is indicated.When user interacts with mark 810,
Representative frame 815 is shown to user, it may also include the description of the semantic concept recognized for representative frame or action.For showing
The user mutual of representative frame changes in different embodiments, and can be when exceeding threshold value at the position of mark 810
Between the user that detects of section cursor (for example, hovering), or can be that user clicks on mark 810.
Fig. 9 shows to provide the interface of the representative frame of video according to the utilization player interface 900 of one embodiment.At this
It is not that mark is provided in the progress bar of video, but representative frame is shown as list 910 in example.The row of representative frame
Table can also classify according to the relevance scores of frame.As set forth above, it is possible to which profile, search or user based on user may be felt
Other instructions of the frame of interest select the list of representative frame.The list of representative frame allows user to look back and selection representativeness
Frame, the viewing areas without influenceing video.In addition, when user selects representative frame, video trusteeship service 100 is in representative frame
Or start the playback of video at the time of associated clip, it is allowed to the part of user fast searching user video interested.Use this
Various technologies described by text, user can efficiently identify user it is interested and be it is peculiar in inquiry or user video
Part.To allow user to determine representative frame of the user to one or more videos mode whether interested, by this of video
Partly it is presented to user a bit.
The disclosure is described in particular detail with a possible embodiment.It will be understood by those skilled in the art that
The disclosure can be put into practice in other embodiments.First, the specific name of component and variable, the capitalization of term, attribute, data knot
Structure or any other programming or configuration aspects are not enforceable or important, and the mechanism for realizing the disclosure or its feature can
With with different titles, form or agreement.In addition, the particular division of functionality between various system components described herein is only
Merely to the purpose of example, rather than it is enforceable；The function of being performed by individual system assembly can be alternatively by multiple groups
Part is performed, and the function of being performed by multiple components can be performed alternatively by single component.It is as described herein, term " set "
Refer to one or more.
The spy of the disclosure is presented in some parts described above in terms of the algorithm of the operation on information and symbol expression
Levy.These arthmetic statements and expression are that the technical staff of data processing field is used for most effectively passing the essence of their work
Up to the means to others skilled in the art.These operations functionally or in logic described are interpreted as by computer journey
Sequence is realized.Also, it has proven that these arrangements of operations are referred to as module or function title is sometimes easily, it is general without losing
Property.
Unless specifically stated otherwise, or from that discussed above it is apparent in, it will be appreciated that in whole description, using such as
" it is determined that " or the discussion of the term such as " display " refer to computer system or the action of similar electronic computing device and process, should
Computer system or similar electronic computing device manipulate and convert computer system memory or register or it is other this
The data of physics (electronics) amount are expressed as in information storage, transmission or the display device of sample.
Some aspects of the disclosure include process steps described here and instruction in the form of algorithm.It should be noted that this
Disclosed process steps and instruction can be realized with software, firmware or hardware, and when implemented in software, can be downloaded
To reside in the different platform used by real-time network operating system and be operated from the different platform.
The disclosure further relates to the device for performing operation here.The device can be special structure for required purpose
Make, or it can include selecting by the computer program being stored on the computer-readable medium that can be accessed by computer
The all-purpose computer being activated or reconfigured by property.Such computer program can be stored in computer-readable recording medium
In, it is such as, but not limited to any kind of disk including floppy disk, CD, CD-ROM, magneto-optic disk, read-only storage (ROM), random
Access memory (RAM), EPROM, EEPROM, magnetic or optical card, application specific integrated circuit (ASIC) or be suitable for storage electronics refer to
Any kind of non-transitory computer-readable storage media of order.In addition, the computer referred in specification can include list
Individual processor can use multiprocessor design to improve the framework of computing capability.
Algorithm and operation presented herein is not inherently related to any certain computer or other devices.It is various general
System can also be used together with the program according to teaching herein, or can prove to construct more special device to perform
The method and step needed is convenient.Structure and equivalence changes needed for these various systems will be for those skilled in the art
Obviously.In addition, describing the disclosure without reference to any specific programming language.It should be appreciated that various programming languages can
For realizing the teaching of the disclosure as described herein, and any reference provided to language-specific is used to disclose this public affairs
The realization opened and optimal mode.
The disclosure is perfectly suitable for the various computer network systems in a variety of topologys.In the field, greatly
The configuration of type network and management include being communicably coupled to different computer and storage device by network (such as internet)
Storage device and computer.
Finally, it is to be noted that the language used in the description is primarily to readable and instruct purpose and select,
And may not be to be selected to describe or limit subject of the present invention.Therefore, the disclosure of the disclosure is intended to illustrative and not limiting
The scope of the present disclosure illustrated in the following claims.
Claims (27)
1. a kind of computer implemented method for being used to select the representative frame of video, including：
Reception includes the video of frame set；
The characteristic set of each frame of the frame of the video is recognized, it is special that the feature includes feature and semanteme based on frame
Levy, the likelihood that the semantic feature identification semantic concept exists in the frame of the video as content；
The video segment set of the video is generated, each video segment includes the chronological frame from the video
Subset, and each frame is associated with least one semantic feature in the semantic feature；
Fraction is at least generated for each frame of the subset of the frame of each video segment based on the semantic feature；With
The fraction based on the frame in the video segment is that each video segment selects representative frame, the representativeness
Frame represents and summarized the video segment.
2. according to the method described in claim 1, including the piece segment table of the generation video, regarded described in described segment table storage
The representative frame of the video segment of frequency and the semantic concept associated with each of the representative frame
Set.
3. computer implemented method according to claim 1, wherein, the feature based on frame include it is following in extremely
It is few one：
Visual signature, including by color histogram, orient the histogram of gradient, the color difference of frame and consecutive frame, motion feature,
Or at least one in the group of tracking features composition, and；
Audio frequency characteristics, including by volume, audible spectrum figure, speech-non-voice designator or the sense of hearing image construction of tranquilization
At least one in group.
4. computer implemented method according to claim 1, wherein, recognize the step bag of the characteristic set
Include：
Using multiple semantic classifiers to determine the semantic feature,
Wherein, semantic classifiers receive specifying for the frame associated with the feature based on frame, and export the frame and semantic concept
The likelihood of related or descriptive semantics concept, and
Wherein, the semantic concept is allocated to the label of the content of the video, and corresponding to entity or freely literary
This.
5. computer implemented method according to claim 1, wherein, generate the step of the video segment set
Including：
The feature based on frame is analyzed to determine the shot boundary set in the video；
Wherein, camera lens includes successive frame set, and shot boundary indicates the frame between adjacent camera lens.
6. computer implemented method according to claim 5, wherein it is determined that the step of the set of the shot boundary
Suddenly include：
Grader is applied to the frame associated with the feature based on frame to determine whether frame is shot boundary；
Wherein, using the shot boundary for having label as positivity characteristic set, and the frame conduct near the shot boundary is used
Hard negativity training set, to train the grader, and
Wherein, the feature based on frame includes detecting with the aberration of consecutive frame, motion feature, audio volume and audio speech.
7. computer implemented method according to claim 5, wherein determining the step of the shot boundary set
Including：
The coherence of the analysis feature based on frame；
Wherein, the similitude of coherency measure feature based on frame in scheduled time fragment, and
Wherein, the similitude provides the distance metric for splitting the video.
8. computer implemented method according to claim 5, wherein determining the step of the shot boundary set
Including：
The feature based on frame is followed the trail of in multiple series of the frame of the video；And
Wherein, when the change of the feature based on frame between frame and consecutive frame is more than threshold value, the frame is defined as camera lens side
Boundary.
9. computer implemented method according to claim 1, wherein the fraction includes semantic fraction, and is described
The step that frame generates the semantic fraction includes：
The each semantic feature generated by the subset of the chronological frame being included within the video segment with
Threshold value is compared, to recognize semantic concept set comprising the video segment of the frame, wherein the set is each semantic general
Read the correspondence semantic feature with more than the threshold value；With
It is described semantic general in the frame by determining to be present in compared with reference value for each semantic concept of the set
The amount of thought, come the frame ranking score of each frame of the subset that determines the chronological frame in the video segment；With
And
By adding up to the frame ranking score of the frame in the fragment to determine the semantic fraction of the frame.
10. computer implemented method according to claim 1, wherein, generate the institute of the fraction of each frame
Stating step includes combining the semantic concept and corresponding likelihood in the frame.
11. computer implemented method according to claim 1, wherein the fraction set closes semantic fraction and aesthetics point
Number, and include for the step that each frame generates the fraction：
The semantic fraction is calculated based on identified semantic feature；
Use quality metric set calculates the aesthetic score, and the quality metric includes coming free acutance, amount of exercise and piece
Segment boundary distance and photographic quality composition group at least one；And
Combine the semantic fraction and the aesthetic score.
12. computer implemented method according to claim 1, in addition in response to receiving the request to video segment
And the representative frame of the video segment is presented.
13. it is a kind of including can by the computer program instructions of computing device non-transitory computer-readable storage media,
The computer program instructions include：
Reception includes the video of frame set；
The characteristic set of each frame of the frame of the video is recognized, it is special that the feature includes feature and semanteme based on frame
Levy, the likelihood that the semantic feature identification semantic concept exists in the frame of the video as content；
The video segment set of the video is generated, each video segment includes the chronological frame from the video
Subset, and each frame is associated with least one semantic feature in the semantic feature；
Fraction is at least generated for each frame of the subset of the frame of each video segment based on the semantic feature；With
The fraction based on the frame in the video segment is that each video segment selects representative frame, the representativeness
Frame represents and summarized the video segment.
14. non-transitory computer-readable storage media according to claim 13, wherein the computer program instructions
Also include the piece segment table for generating the video, described segment table stores the representative frame of the video segment of the video
The set of the associated semantic concept with each representative frame of the representative frame.
15. computer program product according to claim 13, wherein recognizing the step of the characteristic set includes：
Using multiple semantic classifiers to determine the semantic feature,
Wherein, semantic classifiers receive specifying for the frame associated with the feature based on frame, and export the frame and semantic concept
The likelihood of related or descriptive semantics concept, and
Wherein, the semantic concept is allocated to the label of the content of the video, and corresponding to entity or freely literary
This.
16. computer program product according to claim 13, wherein generating the step of the video segment set
Including：
The feature based on frame is analyzed to determine the shot boundary set in the video；
Wherein, camera lens includes successive frame set, and shot boundary indicates the frame between adjacent camera lens.
17. computer program product according to claim 16, wherein determining the step of the shot boundary set
Including：
Grader is applied to the frame associated with the feature based on frame to determine whether frame is shot boundary；
Wherein, using the shot boundary for having label as positivity characteristic set, and the frame conduct near the shot boundary is used
Hard negativity training set, to train the grader, and
Wherein, the feature based on frame includes detecting with the aberration of consecutive frame, motion feature, audio volume and audio speech.
18. computer program product according to claim 16, wherein it is determined that the step of the shot boundary set
Including：
The coherence of the analysis feature based on frame；
Wherein, the similitude of coherency measure feature based on frame in scheduled time fragment, and
Wherein, the similitude provides the distance metric for splitting the video.
19. computer program product according to claim 13, wherein the fraction includes semantic fraction, and is described
The step that frame generates the semantic fraction includes：
By by each semantic feature to be generated including the subset of the chronological frame in the video segment
It is compared with threshold value, to recognize semantic concept set comprising the video segment of the frame, wherein each semanteme of the set
Concept has the correspondence semantic feature more than the threshold value；With
It is described semantic general in the frame by determining to be present in compared with reference value for each semantic concept of the set
The amount of thought, come the frame ranking score of each frame of the subset that determines the chronological frame in the video segment；With
And
By adding up to the frame ranking score of the frame in the fragment to determine the semantic fraction of the frame.
20. a kind of system, including：
Processor for performing computer program instructions；With
Including can by the computer program instructions of the computing device non-transitory computer-readable storage media, it is described
Computer program instructions include：
Reception includes the video of frame set；
The characteristic set of each frame of the frame of the video is recognized, it is special that the feature includes feature and semanteme based on frame
Levy, the likelihood that the semantic feature identification semantic concept exists in the frame of the video as content；
The video segment set of the video is generated, each video segment includes the chronological frame from the video
Subset, and each frame is associated with least one semantic feature in the semantic feature；
Fraction is at least generated for each frame of the subset of the frame of each video segment based on the semantic feature；With
The fraction based on the frame in the video segment is that each video segment selects representative frame, the representativeness
Frame represents and summarized the video segment.
21. a kind of computer implemented method for being used to select the representative frame for showing to user, including：
The request for summarizing video is received, the request specifies at least search inquiry or asks the user of video；
Context or the metadata associated with the video at least based on the user, recognize regard related to the request
Frequency set of segments；
Based on the video segment set and the correlation of the video metadata and the diversity of semantic concept, asked from described
Ask and representative segment set is selected in the video segment set of correlation；
The representative frame associated with the representative segment set is determined from the one or more segment tables for multiple videos
Set, piece segment table indicates the representative frame and semantic concept set for video segment；With
Video summaries are generated using the representative frame set, the video summaries combine the representative frame in chronological order.
22. computer implemented method according to claim 21, being additionally included in present in user interface includes being recognized
Video segment set video collection, only in the relevance scores associated with the representative frame more than threshold correlation point
In the case of number, the video accompaniment in the video collection the representative frame for the correspondence video segment selection.
23. computer implemented method according to claim 22, in addition to：
For each video in the video collection, the representative frame of each video segment of the video is recognized, and is selected
The representative frame of the video is summarized, the representative frame has its of the remainder of the video segment than the video
The bigger associated associated score of its representative frame；With
The selected representative frame of the general introduction video collection is presented.
24. computer implemented method according to claim 21, being additionally included in present in user interface includes being recognized
Video segment set video collection, each video accompaniment in the video collection to include in the video one
The representative frame set of individual or multiple video segment selections.
25. computer implemented method according to claim 24, in addition to by associated relevance scores to institute
State representative frame set and carry out ranking, and highlight the representative frame of top ranked.
26. computer implemented method according to claim 21, in addition to：
Receive selection of the user to the video including one or more of video segment set video segment；
Recognize the representative frame set of at least one video segment of the video；And
The video is played, and remembers the time point set associated with the representative frame set playing the video timestamp.
27. computer implemented method according to claim 21, in addition to：
Receive selection of the user to the video including one or more of video segment set video segment；
Recognize the representative frame set of at least one video segment of the video；And
The video is played, and the representative frame set is presented when playing the video.
Applications Claiming Priority (5)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201462047639P | 2014-09-08 | 2014-09-08 | |
US62/047,639 | 2014-09-08 | ||
US201562120107P | 2015-02-24 | 2015-02-24 | |
US62/120,107 | 2015-02-24 | ||
PCT/IB2015/056783 WO2016038522A1 (en) | 2014-09-08 | 2015-09-05 | Selecting and presenting representative frames for video previews |
Publications (1)
Publication Number | Publication Date |
---|---|
CN107077595A true CN107077595A (en) | 2017-08-18 |
Family
ID=55437782
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201580034616.3A Pending CN107077595A (en) | 2014-09-08 | 2015-09-05 | Selection and presentation representative frame are for video preview |
Country Status (4)
Country | Link |
---|---|
US (3) | US9953222B2 (en) |
EP (1) | EP3192273A4 (en) |
CN (1) | CN107077595A (en) |
WO (1) | WO2016038522A1 (en) |
Cited By (18)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN107613373A (en) * | 2017-09-12 | 2018-01-19 | 中广热点云科技有限公司 | A kind of method that multi-screen continuously watches TV programme |
CN107832725A (en) * | 2017-11-17 | 2018-03-23 | 北京奇虎科技有限公司 | Video front cover extracting method and device based on evaluation index |
CN107918656A (en) * | 2017-11-17 | 2018-04-17 | 北京奇虎科技有限公司 | Video front cover extracting method and device based on video title |
CN107958030A (en) * | 2017-11-17 | 2018-04-24 | 北京奇虎科技有限公司 | Video front cover recommended models optimization method and device |
CN108307229A (en) * | 2018-02-02 | 2018-07-20 | 新华智云科技有限公司 | A kind of processing method and equipment of video-audio data |
CN108377417A (en) * | 2018-01-17 | 2018-08-07 | 百度在线网络技术（北京）有限公司 | Video reviewing method, device, computer equipment and storage medium |
CN109040823A (en) * | 2018-08-20 | 2018-12-18 | 青岛海信传媒网络技术有限公司 | A kind of method and device that bookmark is shown |
CN109756767A (en) * | 2017-11-06 | 2019-05-14 | 腾讯科技（深圳）有限公司 | Preview data playback method, device and storage medium |
CN110347872A (en) * | 2019-07-04 | 2019-10-18 | 腾讯科技（深圳）有限公司 | Video cover image extracting method and device, storage medium and electronic equipment |
CN110868630A (en) * | 2018-08-27 | 2020-03-06 | 北京优酷科技有限公司 | Method and device for generating forecast report |
CN111553185A (en) * | 2019-01-16 | 2020-08-18 | 联发科技股份有限公司 | Highlight display processing method and related system thereof |
CN111818363A (en) * | 2020-07-10 | 2020-10-23 | 携程计算机技术（上海）有限公司 | Short video extraction method, system, device and storage medium |
CN111836072A (en) * | 2020-05-21 | 2020-10-27 | 北京嘀嘀无限科技发展有限公司 | Video processing method, device, equipment and storage medium |
CN111836118A (en) * | 2019-04-19 | 2020-10-27 | 百度在线网络技术（北京）有限公司 | Video processing method, device, server and storage medium |
CN113132752A (en) * | 2019-12-30 | 2021-07-16 | 阿里巴巴集团控股有限公司 | Video processing method and device |
CN113302915A (en) * | 2019-01-14 | 2021-08-24 | 杜比实验室特许公司 | Sharing a physical writing surface in a video conference |
TWI741550B (en) * | 2020-03-31 | 2021-10-01 | 國立雲林科技大學 | Method for bookmark frame generation, and video player device with automatic generation of bookmark and user interface thereof |
CN115278355A (en) * | 2022-06-20 | 2022-11-01 | 北京字跳网络技术有限公司 | Video editing method, device, equipment, computer readable storage medium and product |
Families Citing this family (68)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8923607B1 (en) | 2010-12-08 | 2014-12-30 | Google Inc. | Learning sports highlights using event detection |
KR102306538B1 (en) * | 2015-01-20 | 2021-09-29 | 삼성전자주식회사 | Apparatus and method for editing content |
US10440443B2 (en) * | 2015-02-04 | 2019-10-08 | Mobitv, Inc. | Intermediate key frame selection and animation |
US10440076B2 (en) * | 2015-03-10 | 2019-10-08 | Mobitv, Inc. | Media seek mechanisms |
US9449248B1 (en) * | 2015-03-12 | 2016-09-20 | Adobe Systems Incorporated | Generation of salient contours using live video |
US9659218B1 (en) | 2015-04-29 | 2017-05-23 | Google Inc. | Predicting video start times for maximizing user engagement |
US20160378863A1 (en) * | 2015-06-24 | 2016-12-29 | Google Inc. | Selecting representative video frames for videos |
US11158344B1 (en) * | 2015-09-30 | 2021-10-26 | Amazon Technologies, Inc. | Video ingestion and clip creation |
US10230866B1 (en) | 2015-09-30 | 2019-03-12 | Amazon Technologies, Inc. | Video ingestion and clip creation |
US10204273B2 (en) * | 2015-10-20 | 2019-02-12 | Gopro, Inc. | System and method of providing recommendations of moments of interest within video clips post capture |
US10229324B2 (en) * | 2015-12-24 | 2019-03-12 | Intel Corporation | Video summarization using semantic information |
JP6731178B2 (en) * | 2016-03-07 | 2020-07-29 | 富士ゼロックス株式会社 | Video search device and program |
US9866887B2 (en) * | 2016-03-08 | 2018-01-09 | Flipboard, Inc. | Auto video preview within a digital magazine |
US10049279B2 (en) | 2016-03-11 | 2018-08-14 | Qualcomm Incorporated | Recurrent networks with motion-based attention for video understanding |
JP2017204753A (en) * | 2016-05-11 | 2017-11-16 | 富士通株式会社 | Frame extraction method, video reproduction control method, program, frame extraction device and video reproduction controller |
US20170337273A1 (en) * | 2016-05-17 | 2017-11-23 | Opentv, Inc | Media file summarizer |
US11523188B2 (en) * | 2016-06-30 | 2022-12-06 | Disney Enterprises, Inc. | Systems and methods for intelligent media content segmentation and analysis |
US10347294B2 (en) * | 2016-06-30 | 2019-07-09 | Google Llc | Generating moving thumbnails for videos |
US10645142B2 (en) * | 2016-09-20 | 2020-05-05 | Facebook, Inc. | Video keyframes display on online social networks |
KR20180036153A (en) * | 2016-09-30 | 2018-04-09 | 주식회사 요쿠스 | Video editing system and method |
CN108605165A (en) | 2016-10-31 | 2018-09-28 | 华为技术有限公司 | The method and electronic equipment of video thumbnails are generated in the electronic device |
KR20180058019A (en) * | 2016-11-23 | 2018-05-31 | 한화에어로스페이스 주식회사 | The Apparatus For Searching Image And The Method For Storing Data And The Apparatus For Storing Data |
US10482126B2 (en) * | 2016-11-30 | 2019-11-19 | Google Llc | Determination of similarity between videos using shot duration correlation |
CN106528884B (en) * | 2016-12-15 | 2019-01-11 | 腾讯科技（深圳）有限公司 | A kind of information exhibiting pictures generation method and device |
US10631028B2 (en) | 2016-12-19 | 2020-04-21 | Sony Interactive Entertainment LLC | Delivery of third party content on a first party portal |
US10430661B2 (en) * | 2016-12-20 | 2019-10-01 | Adobe Inc. | Generating a compact video feature representation in a digital medium environment |
US10366132B2 (en) | 2016-12-28 | 2019-07-30 | Sony Interactive Entertainment LLC | Delivering customized content using a first party portal service |
US10419384B2 (en) | 2017-01-06 | 2019-09-17 | Sony Interactive Entertainment LLC | Social network-defined video events |
US10671852B1 (en) | 2017-03-01 | 2020-06-02 | Matroid, Inc. | Machine learning in video classification |
US10268897B2 (en) | 2017-03-24 | 2019-04-23 | International Business Machines Corporation | Determining most representative still image of a video for specific user |
US10409859B2 (en) * | 2017-05-15 | 2019-09-10 | Facebook, Inc. | Video heat maps personalized for online system users |
IT201700053345A1 (en) * | 2017-05-17 | 2018-11-17 | Metaliquid S R L | METHOD AND EQUIPMENT FOR THE ANALYSIS OF VIDEO CONTENTS IN DIGITAL FORMAT |
US11822591B2 (en) * | 2017-09-06 | 2023-11-21 | International Business Machines Corporation | Query-based granularity selection for partitioning recordings |
CN107872724A (en) * | 2017-09-26 | 2018-04-03 | 五八有限公司 | A kind of preview video generation method and device |
US10521705B2 (en) * | 2017-11-14 | 2019-12-31 | Adobe Inc. | Automatically selecting images using multicontext aware ratings |
US10445586B2 (en) | 2017-12-12 | 2019-10-15 | Microsoft Technology Licensing, Llc | Deep learning on image frames to generate a summary |
US20190199763A1 (en) * | 2017-12-22 | 2019-06-27 | mindHIVE Inc. | Systems and methods for previewing content |
US10932006B2 (en) * | 2017-12-22 | 2021-02-23 | Facebook, Inc. | Systems and methods for previewing content |
US10474903B2 (en) * | 2018-01-25 | 2019-11-12 | Adobe Inc. | Video segmentation using predictive models trained to provide aesthetic scores |
US10679069B2 (en) | 2018-03-27 | 2020-06-09 | International Business Machines Corporation | Automatic video summary generation |
KR102464907B1 (en) * | 2018-04-10 | 2022-11-09 | 삼성전자주식회사 | Electronic apparatus and operating method for the same |
US10733984B2 (en) * | 2018-05-07 | 2020-08-04 | Google Llc | Multi-modal interface in a voice-activated network |
US11430312B2 (en) * | 2018-07-05 | 2022-08-30 | Movidius Limited | Video surveillance with neural networks |
WO2020060538A1 (en) * | 2018-09-18 | 2020-03-26 | Google Llc | Methods and systems for processing imagery |
CN109740499B (en) * | 2018-12-28 | 2021-06-11 | 北京旷视科技有限公司 | Video segmentation method, video motion recognition method, device, equipment and medium |
KR102613328B1 (en) | 2019-01-17 | 2023-12-14 | 삼성전자주식회사 | Display apparatus and control method thereof |
US20200380030A1 (en) * | 2019-05-31 | 2020-12-03 | Adobe Inc. | In-application video navigation system |
CN110381339B (en) * | 2019-08-07 | 2021-08-27 | 腾讯科技（深圳）有限公司 | Picture transmission method and device |
EP3798866A1 (en) * | 2019-09-24 | 2021-03-31 | Facebook Inc. | Customized thumbnail image generation and selection for digital content using computer vision and machine learning |
CN110704681B (en) * | 2019-09-26 | 2023-03-24 | 三星电子（中国）研发中心 | Method and system for generating video |
CN110650379B (en) * | 2019-09-26 | 2022-04-01 | 北京达佳互联信息技术有限公司 | Video abstract generation method and device, electronic equipment and storage medium |
US10998007B2 (en) * | 2019-09-30 | 2021-05-04 | Adobe Inc. | Providing context aware video searching |
US11500927B2 (en) | 2019-10-03 | 2022-11-15 | Adobe Inc. | Adaptive search results for multimedia search queries |
CN110856037B (en) * | 2019-11-22 | 2021-06-22 | 北京金山云网络技术有限公司 | Video cover determination method and device, electronic equipment and readable storage medium |
CN110909205B (en) * | 2019-11-22 | 2023-04-07 | 北京金山云网络技术有限公司 | Video cover determination method and device, electronic equipment and readable storage medium |
KR102285039B1 (en) * | 2019-12-12 | 2021-08-03 | 한국과학기술원 | Shot boundary detection method and apparatus using multi-classing |
EP3852059A1 (en) | 2020-01-15 | 2021-07-21 | General Electric Company | System and method for assessing the health of an asset |
CN111277892B (en) * | 2020-01-20 | 2022-03-22 | 北京百度网讯科技有限公司 | Method, apparatus, server and medium for selecting video clip |
CN113438500B (en) * | 2020-03-23 | 2023-03-24 | 阿里巴巴集团控股有限公司 | Video processing method and device, electronic equipment and computer storage medium |
CN111464833B (en) * | 2020-03-23 | 2023-08-04 | 腾讯科技（深圳）有限公司 | Target image generation method, target image generation device, medium and electronic device |
CN113453055B (en) * | 2020-03-25 | 2022-12-27 | 华为技术有限公司 | Method and device for generating video thumbnail and electronic equipment |
US11636677B2 (en) * | 2021-01-08 | 2023-04-25 | Huawei Technologies Co., Ltd. | Systems, devices and methods for distributed hierarchical video analysis |
US11711579B1 (en) * | 2021-01-25 | 2023-07-25 | Amazon Technologies, Inc. | Navigation integrated content stream |
US11893792B2 (en) * | 2021-03-25 | 2024-02-06 | Adobe Inc. | Integrating video content into online product listings to demonstrate product features |
US11678030B2 (en) * | 2021-07-16 | 2023-06-13 | Rovi Guides, Inc. | Personalized screencaps for trickplay slider |
WO2023130326A1 (en) * | 2022-01-06 | 2023-07-13 | Huawei Technologies Co., Ltd. | Methods and devices for generating customized video segment based on content features |
CN114445754A (en) * | 2022-01-29 | 2022-05-06 | 北京有竹居网络技术有限公司 | Video processing method and device, readable medium and electronic equipment |
US20240012555A1 (en) * | 2022-07-07 | 2024-01-11 | Google Llc | Identifying and navigating to a visual item on a web page |
Citations (21)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2001015017A1 (en) * | 1999-08-26 | 2001-03-01 | Lg Electronics Inc. | Video data structure for video browsing based on event |
US6697523B1 (en) * | 2000-08-09 | 2004-02-24 | Mitsubishi Electric Research Laboratories, Inc. | Method for summarizing a video using motion and color descriptors |
US6721454B1 (en) * | 1998-10-09 | 2004-04-13 | Sharp Laboratories Of America, Inc. | Method for automatic extraction of semantically significant events from video |
US20090208106A1 (en) * | 2008-02-15 | 2009-08-20 | Digitalsmiths Corporation | Systems and methods for semantically classifying shots in video |
US7627823B2 (en) * | 1998-12-28 | 2009-12-01 | Sony Corporation | Video information editing method and editing device |
CN101778257A (en) * | 2010-03-05 | 2010-07-14 | 北京邮电大学 | Generation method of video abstract fragments for digital video on demand |
CN102057433A (en) * | 2008-06-09 | 2011-05-11 | 皇家飞利浦电子股份有限公司 | Method and apparatus for generating a summary of an audio/visual data stream |
CN102414680A (en) * | 2009-03-20 | 2012-04-11 | 伊斯曼柯达公司 | Semantic event detection using cross-domain knowledge |
CN102549603A (en) * | 2009-08-24 | 2012-07-04 | 谷歌公司 | Relevance-based image selection |
US20120206567A1 (en) * | 2010-09-13 | 2012-08-16 | Trident Microsystems (Far East) Ltd. | Subtitle detection system and method to television video |
CN102663015A (en) * | 2012-03-21 | 2012-09-12 | 上海大学 | Video semantic labeling method based on characteristics bag models and supervised learning |
US20120281969A1 (en) * | 2011-05-03 | 2012-11-08 | Wei Jiang | Video summarization using audio and visual cues |
US20130089303A1 (en) * | 2011-10-10 | 2013-04-11 | Wei Jiang | Video concept classification using audio-visual grouplets |
US20130114902A1 (en) * | 2011-11-04 | 2013-05-09 | Google Inc. | High-Confidence Labeling of Video Volumes in a Video Sharing Service |
US8473981B1 (en) * | 2011-06-30 | 2013-06-25 | Google Inc. | Augmenting metadata of digital media objects using per object classifiers |
CN103262096A (en) * | 2010-12-09 | 2013-08-21 | 诺基亚公司 | Limited-context-ased identifying key frame from video sequence |
CN103299324A (en) * | 2010-11-11 | 2013-09-11 | 谷歌公司 | Learning tags for video annotation using latent subtags |
US20130300939A1 (en) * | 2012-05-11 | 2013-11-14 | Cisco Technology, Inc. | System and method for joint speaker and scene recognition in a video/audio processing environment |
WO2014043438A1 (en) * | 2012-09-13 | 2014-03-20 | Google Inc. | Identifying a thumbnail image to represent a video |
CN103761284A (en) * | 2014-01-13 | 2014-04-30 | 中国农业大学 | Video retrieval method and video retrieval system |
CN103905824A (en) * | 2014-03-26 | 2014-07-02 | 深圳先进技术研究院 | Video semantic retrieval and compression synchronization camera system and method |
Family Cites Families (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6097853A (en) * | 1996-09-11 | 2000-08-01 | Da Vinci Systems, Inc. | User definable windows for selecting image processing regions |
US6535639B1 (en) * | 1999-03-12 | 2003-03-18 | Fuji Xerox Co., Ltd. | Automatic video summarization using a measure of shot importance and a frame-packing method |
US7325199B1 (en) * | 2000-10-04 | 2008-01-29 | Apple Inc. | Integrated time line for editing |
JP4595263B2 (en) * | 2001-07-31 | 2010-12-08 | アイシン精機株式会社 | Valve timing control device |
US20080155627A1 (en) * | 2006-12-04 | 2008-06-26 | O'connor Daniel | Systems and methods of searching for and presenting video and audio |
US20110262103A1 (en) * | 2009-09-14 | 2011-10-27 | Kumar Ramachandran | Systems and methods for updating video content with linked tagging information |
US9628673B2 (en) * | 2010-04-28 | 2017-04-18 | Microsoft Technology Licensing, Llc | Near-lossless video summarization |
EP2641401B1 (en) * | 2010-11-15 | 2017-04-05 | Huawei Technologies Co., Ltd. | Method and system for video summarization |
KR101976178B1 (en) * | 2012-06-05 | 2019-05-08 | 엘지전자 주식회사 | Mobile terminal and method for controlling of the same |
US8989503B2 (en) * | 2012-08-03 | 2015-03-24 | Kodak Alaris Inc. | Identifying scene boundaries using group sparsity analysis |
US10229324B2 (en) * | 2015-12-24 | 2019-03-12 | Intel Corporation | Video summarization using semantic information |
-
2015
- 2015-09-05 EP EP15839919.6A patent/EP3192273A4/en active Pending
- 2015-09-05 WO PCT/IB2015/056783 patent/WO2016038522A1/en active Application Filing
- 2015-09-05 CN CN201580034616.3A patent/CN107077595A/en active Pending
- 2015-09-08 US US14/848,216 patent/US9953222B2/en active Active
-
2018
- 2018-04-23 US US15/959,858 patent/US10867183B2/en active Active
-
2020
- 2020-12-14 US US17/120,525 patent/US20210166035A1/en active Pending
Patent Citations (21)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6721454B1 (en) * | 1998-10-09 | 2004-04-13 | Sharp Laboratories Of America, Inc. | Method for automatic extraction of semantically significant events from video |
US7627823B2 (en) * | 1998-12-28 | 2009-12-01 | Sony Corporation | Video information editing method and editing device |
WO2001015017A1 (en) * | 1999-08-26 | 2001-03-01 | Lg Electronics Inc. | Video data structure for video browsing based on event |
US6697523B1 (en) * | 2000-08-09 | 2004-02-24 | Mitsubishi Electric Research Laboratories, Inc. | Method for summarizing a video using motion and color descriptors |
US20090208106A1 (en) * | 2008-02-15 | 2009-08-20 | Digitalsmiths Corporation | Systems and methods for semantically classifying shots in video |
CN102057433A (en) * | 2008-06-09 | 2011-05-11 | 皇家飞利浦电子股份有限公司 | Method and apparatus for generating a summary of an audio/visual data stream |
CN102414680A (en) * | 2009-03-20 | 2012-04-11 | 伊斯曼柯达公司 | Semantic event detection using cross-domain knowledge |
CN102549603A (en) * | 2009-08-24 | 2012-07-04 | 谷歌公司 | Relevance-based image selection |
CN101778257A (en) * | 2010-03-05 | 2010-07-14 | 北京邮电大学 | Generation method of video abstract fragments for digital video on demand |
US20120206567A1 (en) * | 2010-09-13 | 2012-08-16 | Trident Microsystems (Far East) Ltd. | Subtitle detection system and method to television video |
CN103299324A (en) * | 2010-11-11 | 2013-09-11 | 谷歌公司 | Learning tags for video annotation using latent subtags |
CN103262096A (en) * | 2010-12-09 | 2013-08-21 | 诺基亚公司 | Limited-context-ased identifying key frame from video sequence |
US20120281969A1 (en) * | 2011-05-03 | 2012-11-08 | Wei Jiang | Video summarization using audio and visual cues |
US8473981B1 (en) * | 2011-06-30 | 2013-06-25 | Google Inc. | Augmenting metadata of digital media objects using per object classifiers |
US20130089303A1 (en) * | 2011-10-10 | 2013-04-11 | Wei Jiang | Video concept classification using audio-visual grouplets |
US20130114902A1 (en) * | 2011-11-04 | 2013-05-09 | Google Inc. | High-Confidence Labeling of Video Volumes in a Video Sharing Service |
CN102663015A (en) * | 2012-03-21 | 2012-09-12 | 上海大学 | Video semantic labeling method based on characteristics bag models and supervised learning |
US20130300939A1 (en) * | 2012-05-11 | 2013-11-14 | Cisco Technology, Inc. | System and method for joint speaker and scene recognition in a video/audio processing environment |
WO2014043438A1 (en) * | 2012-09-13 | 2014-03-20 | Google Inc. | Identifying a thumbnail image to represent a video |
CN103761284A (en) * | 2014-01-13 | 2014-04-30 | 中国农业大学 | Video retrieval method and video retrieval system |
CN103905824A (en) * | 2014-03-26 | 2014-07-02 | 深圳先进技术研究院 | Video semantic retrieval and compression synchronization camera system and method |
Non-Patent Citations (2)
Title |
---|
YONG JAE LEE ET AL.: "Discovering Important People and Objects for Egocentric Video Summarization", 《2012 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION》 * |
YONG JAE LEE ET AL.: "Discovering Important People and Objects for Egocentric Video Summarization", 《2012 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION》, 26 July 2012 (2012-07-26), pages 1349 - 1351 * |
Cited By (29)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN107613373B (en) * | 2017-09-12 | 2019-07-30 | 中广热点云科技有限公司 | A kind of method that multi-screen continuously watches TV programme |
CN107613373A (en) * | 2017-09-12 | 2018-01-19 | 中广热点云科技有限公司 | A kind of method that multi-screen continuously watches TV programme |
CN109756767B (en) * | 2017-11-06 | 2021-12-14 | 腾讯科技（深圳）有限公司 | Preview data playing method, device and storage medium |
CN109756767A (en) * | 2017-11-06 | 2019-05-14 | 腾讯科技（深圳）有限公司 | Preview data playback method, device and storage medium |
CN107958030B (en) * | 2017-11-17 | 2021-08-24 | 北京奇虎科技有限公司 | Video cover recommendation model optimization method and device |
CN107918656A (en) * | 2017-11-17 | 2018-04-17 | 北京奇虎科技有限公司 | Video front cover extracting method and device based on video title |
CN107958030A (en) * | 2017-11-17 | 2018-04-24 | 北京奇虎科技有限公司 | Video front cover recommended models optimization method and device |
CN107832725A (en) * | 2017-11-17 | 2018-03-23 | 北京奇虎科技有限公司 | Video front cover extracting method and device based on evaluation index |
CN108377417A (en) * | 2018-01-17 | 2018-08-07 | 百度在线网络技术（北京）有限公司 | Video reviewing method, device, computer equipment and storage medium |
CN108377417B (en) * | 2018-01-17 | 2019-11-26 | 百度在线网络技术（北京）有限公司 | Video reviewing method, device, computer equipment and storage medium |
CN108307229A (en) * | 2018-02-02 | 2018-07-20 | 新华智云科技有限公司 | A kind of processing method and equipment of video-audio data |
CN108307229B (en) * | 2018-02-02 | 2023-12-22 | 新华智云科技有限公司 | Video and audio data processing method and device |
CN109040823A (en) * | 2018-08-20 | 2018-12-18 | 青岛海信传媒网络技术有限公司 | A kind of method and device that bookmark is shown |
CN109040823B (en) * | 2018-08-20 | 2021-06-04 | 青岛海信传媒网络技术有限公司 | Bookmark display method and device |
CN110868630A (en) * | 2018-08-27 | 2020-03-06 | 北京优酷科技有限公司 | Method and device for generating forecast report |
CN113302915A (en) * | 2019-01-14 | 2021-08-24 | 杜比实验室特许公司 | Sharing a physical writing surface in a video conference |
CN111553185B (en) * | 2019-01-16 | 2023-10-24 | 联发科技股份有限公司 | Highlighting processing method and associated system |
CN111553185A (en) * | 2019-01-16 | 2020-08-18 | 联发科技股份有限公司 | Highlight display processing method and related system thereof |
CN111836118B (en) * | 2019-04-19 | 2022-09-06 | 百度在线网络技术（北京）有限公司 | Video processing method, device, server and storage medium |
CN111836118A (en) * | 2019-04-19 | 2020-10-27 | 百度在线网络技术（北京）有限公司 | Video processing method, device, server and storage medium |
CN110347872A (en) * | 2019-07-04 | 2019-10-18 | 腾讯科技（深圳）有限公司 | Video cover image extracting method and device, storage medium and electronic equipment |
CN110347872B (en) * | 2019-07-04 | 2023-10-24 | 腾讯科技（深圳）有限公司 | Video cover image extraction method and device, storage medium and electronic equipment |
CN113132752A (en) * | 2019-12-30 | 2021-07-16 | 阿里巴巴集团控股有限公司 | Video processing method and device |
CN113132752B (en) * | 2019-12-30 | 2023-02-24 | 阿里巴巴集团控股有限公司 | Video processing method and device |
TWI741550B (en) * | 2020-03-31 | 2021-10-01 | 國立雲林科技大學 | Method for bookmark frame generation, and video player device with automatic generation of bookmark and user interface thereof |
CN111836072A (en) * | 2020-05-21 | 2020-10-27 | 北京嘀嘀无限科技发展有限公司 | Video processing method, device, equipment and storage medium |
CN111818363A (en) * | 2020-07-10 | 2020-10-23 | 携程计算机技术（上海）有限公司 | Short video extraction method, system, device and storage medium |
CN115278355A (en) * | 2022-06-20 | 2022-11-01 | 北京字跳网络技术有限公司 | Video editing method, device, equipment, computer readable storage medium and product |
CN115278355B (en) * | 2022-06-20 | 2024-02-13 | 北京字跳网络技术有限公司 | Video editing method, device, equipment, computer readable storage medium and product |
Also Published As
Publication number | Publication date |
---|---|
US10867183B2 (en) | 2020-12-15 |
EP3192273A1 (en) | 2017-07-19 |
US20160070962A1 (en) | 2016-03-10 |
US20210166035A1 (en) | 2021-06-03 |
EP3192273A4 (en) | 2018-05-23 |
WO2016038522A1 (en) | 2016-03-17 |
US20180239964A1 (en) | 2018-08-23 |
US9953222B2 (en) | 2018-04-24 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN107077595A (en) | Selection and presentation representative frame are for video preview | |
US11556743B2 (en) | Learning highlights using event detection | |
US7707162B2 (en) | Method and apparatus for classifying multimedia artifacts using ontology selection and semantic classification | |
US8948515B2 (en) | Method and system for classifying one or more images | |
US8804999B2 (en) | Video recommendation system and method thereof | |
US20180082127A1 (en) | Video segmentation techniques | |
US20160004911A1 (en) | Recognizing salient video events through learning-based multimodal analysis of visual features and audio-based analytics | |
WO2019245781A1 (en) | Video summarization and collaboration systems and methods | |
CN103200463A (en) | Method and device for generating video summary | |
Baraldi et al. | Recognizing and presenting the storytelling video structure with deep multimodal networks | |
US20130117780A1 (en) | Video synthesis using video volumes | |
Sreeja et al. | Towards genre-specific frameworks for video summarisation: A survey | |
US20210117471A1 (en) | Method and system for automatically generating a video from an online product representation | |
Choi et al. | A spatio-temporal pyramid matching for video retrieval | |
US20230140369A1 (en) | Customizable framework to extract moments of interest | |
Xu et al. | Fast summarization of user-generated videos: Exploiting semantic, emotional, and quality clues | |
Fei et al. | Creating memorable video summaries that satisfy the user’s intention for taking the videos | |
Li et al. | Multimedia maximal marginal relevance for multi-video summarization | |
Ivanov et al. | Object-based tag propagation for semi-automatic annotation of images | |
Otani et al. | Video summarization using textual descriptions for authoring video blogs | |
Feng et al. | Multiple style exploration for story unit segmentation of broadcast news video | |
Nixon et al. | Multimodal video annotation for retrieval and discovery of newsworthy video in a news verification scenario | |
Priya et al. | A comprehensive review of significant researches on content based indexing and retrieval of visual information | |
JP4995770B2 (en) | Image dictionary generation device, image dictionary generation method, and image dictionary generation program | |
Darabi et al. | User-centred personalised video abstraction approach adopting SIFT features |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
CB02 | Change of applicant information | ||
CB02 | Change of applicant information |
Address after: American CaliforniaApplicant after: Google limited liability companyAddress before: American CaliforniaApplicant before: Google Inc. |