CN112955911A - Digital image classification and annotation - Google Patents
Digital image classification and annotation Download PDFInfo
- Publication number
- CN112955911A CN112955911A CN201980039726.7A CN201980039726A CN112955911A CN 112955911 A CN112955911 A CN 112955911A CN 201980039726 A CN201980039726 A CN 201980039726A CN 112955911 A CN112955911 A CN 112955911A
- Authority
- CN
- China
- Prior art keywords
- digital images
- event
- user
- natural language
- image
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000000034 method Methods 0.000 claims abstract description 86
- 238000010801 machine learning Methods 0.000 claims description 28
- 230000003993 interaction Effects 0.000 claims description 21
- 238000012545 processing Methods 0.000 claims description 20
- 230000004044 response Effects 0.000 claims description 17
- 238000012549 training Methods 0.000 claims description 11
- 230000008569 process Effects 0.000 claims description 10
- 230000001186 cumulative effect Effects 0.000 claims description 8
- 230000000007 visual effect Effects 0.000 description 34
- 241001414890 Delia Species 0.000 description 26
- 230000015654 memory Effects 0.000 description 7
- 230000009471 action Effects 0.000 description 6
- 238000013527 convolutional neural network Methods 0.000 description 6
- 230000002452 interceptive effect Effects 0.000 description 6
- 239000000463 material Substances 0.000 description 5
- 238000003780 insertion Methods 0.000 description 4
- 230000037431 insertion Effects 0.000 description 4
- 235000013550 pizza Nutrition 0.000 description 4
- 241000723377 Coffea Species 0.000 description 3
- 241001310793 Podium Species 0.000 description 3
- 230000008901 benefit Effects 0.000 description 3
- 235000016213 coffee Nutrition 0.000 description 3
- 235000013353 coffee beverage Nutrition 0.000 description 3
- 238000004891 communication Methods 0.000 description 3
- 238000010586 diagram Methods 0.000 description 3
- 238000013473 artificial intelligence Methods 0.000 description 2
- 230000001419 dependent effect Effects 0.000 description 2
- 235000013305 food Nutrition 0.000 description 2
- 230000006872 improvement Effects 0.000 description 2
- 230000007246 mechanism Effects 0.000 description 2
- 230000002093 peripheral effect Effects 0.000 description 2
- 241000251468 Actinopterygii Species 0.000 description 1
- 244000257790 Brassica carinata Species 0.000 description 1
- 235000005156 Brassica carinata Nutrition 0.000 description 1
- 241000282412 Homo Species 0.000 description 1
- 240000008790 Musa x paradisiaca Species 0.000 description 1
- 235000018290 Musa x paradisiaca Nutrition 0.000 description 1
- 206010037742 Rabies Diseases 0.000 description 1
- 238000004458 analytical method Methods 0.000 description 1
- 230000003190 augmentative effect Effects 0.000 description 1
- 230000009286 beneficial effect Effects 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 239000003795 chemical substances by application Substances 0.000 description 1
- 238000004590 computer program Methods 0.000 description 1
- 238000012790 confirmation Methods 0.000 description 1
- 238000001514 detection method Methods 0.000 description 1
- 238000011496 digital image analysis Methods 0.000 description 1
- 235000013399 edible fruits Nutrition 0.000 description 1
- 230000001815 facial effect Effects 0.000 description 1
- 230000008921 facial expression Effects 0.000 description 1
- 230000006870 function Effects 0.000 description 1
- 239000011521 glass Substances 0.000 description 1
- 230000006266 hibernation Effects 0.000 description 1
- 230000000977 initiatory effect Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000013507 mapping Methods 0.000 description 1
- 239000003550 marker Substances 0.000 description 1
- 235000012054 meals Nutrition 0.000 description 1
- 230000003278 mimic effect Effects 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 238000012544 monitoring process Methods 0.000 description 1
- 238000003058 natural language processing Methods 0.000 description 1
- 230000006855 networking Effects 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 230000008520 organization Effects 0.000 description 1
- 230000000737 periodic effect Effects 0.000 description 1
- 230000002085 persistent effect Effects 0.000 description 1
- 238000000513 principal component analysis Methods 0.000 description 1
- 230000009467 reduction Effects 0.000 description 1
- 230000033764 rhythmic process Effects 0.000 description 1
- 230000011664 signaling Effects 0.000 description 1
- 239000004984 smart glass Substances 0.000 description 1
- 238000013518 transcription Methods 0.000 description 1
- 230000035897 transcription Effects 0.000 description 1
- 230000001960 triggered effect Effects 0.000 description 1
- 235000013311 vegetables Nutrition 0.000 description 1
- 230000001755 vocal effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/53—Querying
- G06F16/535—Filtering based on additional data, e.g. user or group profiles
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/55—Clustering; Classification
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/58—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/58—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/5866—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using information manually generated, e.g. tags, keywords, comments, manually generated location and time information
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/21—Design or setup of recognition systems or techniques; Extraction of features in feature space; Blind source separation
- G06F18/214—Generating training patterns; Bootstrap methods, e.g. bagging or boosting
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/10—Text processing
- G06F40/166—Editing, e.g. inserting or deleting
- G06F40/169—Annotation, e.g. comment data or footnotes
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N5/00—Computing arrangements using knowledge-based models
- G06N5/02—Knowledge representation; Symbolic representation
- G06N5/022—Knowledge engineering; Knowledge acquisition
Abstract
Embodiments are described herein for automatically annotating or curating digital images using various signals generated by individual users in addition to or in lieu of the content of the digital images themselves, thereby enabling digital images to be retrieved from a searchable database based on their annotation. In particular, techniques are described herein for identifying events associated with a user, e.g., based on natural language input provided by the user, and automatically classifying/annotating images inferred to be relevant to those events.
Description
Background
Humans may participate in human-machine conversations with interactive software applications, also referred to herein as "automated assistants" (also referred to as "chat robots," "interactive personal assistants," "intelligent personal assistants," "personal voice assistants," "conversation agents," "virtual assistants," etc.). For example, a human being (which may be referred to as a "user" when interacting with an automated assistant) may provide commands, queries, and/or requests (collectively referred to herein as "queries") using free-form natural language input, which may include voiced utterances and/or typed free-form natural language input that are converted to text and then processed. The automated assistant may perform various types of processing on the natural language input, such as natural language processing, syntactic processing, semantic processing, and so forth, to identify and respond to the user's intent.
With the increasing popularity of camera-equipped mobile computing devices, most individuals (also referred to herein as "users") acquire an unmanageable number of digital images and/or digital videos (as used herein, "digital images" will include both digital still images and sequences of digital images forming digital videos). As the number of digital images acquired increases, methods for efficiently identifying and retrieving images are increasingly beneficial.
Techniques exist for performing object and/or entity recognition on digital images using various types of artificial intelligence and enabling those digital images to be searched in a database based on identified objects or entities, thereby enabling efficient retrieval of digital images. In some cases, the identified objects and/or entities may also be used to automatically generate subtitles for digital images. However, performing object and/or entity identification on digital images acquired by individuals is challenging given the large and ever changing search space of all potential objects and/or entities that may be captured in digital images. Furthermore, general object recognition may only be able to annotate digital images with relatively non-personal and/or general and context-lacking information.
Disclosure of Invention
Techniques are described herein for automatically annotating or curating (curing) digital images using various signals generated by an individual user in addition to or in lieu of the content of the digital images themselves, thereby enabling retrieval of digital images from a searchable database based on their annotation. In particular, techniques are described herein for identifying events associated with a user and automatically classifying/annotating images that are inferred to be relevant to those events. For example, in various implementations, a user may provide natural language input to an automated assistant, e.g., executing at least in part on one or more computing devices operated by the user, that identifies an event associated with the user and a date associated with the event. The content of the natural language input may then be used to annotate at least some of the digital images acquired on that date (which, as noted above, may include digital video), for example, for storage in a searchable database. In some implementations, the annotations and associated digital images may then be used to train a machine-learned image classifier to classify other digital images.
The natural language input provided by the user does not necessarily have to be directed to or even directly related to any given digital image. In fact, the user's natural language input and the acquisition of digital images need not even occur simultaneously, but may be completely non-simultaneous (e.g., days, weeks, months, years apart). Rather, the user may simply wish to identify (mark) a particular date in the past or in the future. By signaling that a particular date occurs (or will occur) on that day, the user invokes the techniques described herein to facilitate automatic "do not interfere" annotation of digital images that may be related to the event. This, in turn, may enable additional use cases such as automatic subtitling of images, search improvement of images, organization improvement of images, automatic identification and/or tagging of individuals in digital images, and so forth.
Additionally, digital images annotated using the techniques described herein may be used downstream to train and/or improve machine learning models. For example, a machine learning classifier such as a convolutional neural network may be trained using digital images labeled with annotations generated using the techniques described herein. In addition to or instead of identifying objects that are typically associated with a particular event (e.g., balloons and cakes are often found in birthday photographs), these trained models may be able to identify events in a more holistic manner (e.g., based on overall similarity to other similarly annotated photographs).
As an example, a user may state (verbally or by typed input) content such as "Tomorrow, my daughter Delia turns tree (Tomorrow, My daughter Delia becomes three years). This natural language input may be analyzed, for example, using a natural language and/or semantic processor, to identify an event ("Delia/daughter's birthday"), a date associated with the event ("tomorrow," which may be parsed into a more standard date format), and one or more words or phrases describing the event (e.g., "Delia", "birthday", "turn" etc.). Natural language inputs such as these may be obtained from various computing interactions, such as a user making a statement to an automated assistant, a user updating his/her social network status, and so forth.
Once the event and associated data associated with the user are identified, various signals relating to digital images acquired or otherwise obtained by the user at or near a determined date may be analyzed, and/or aspects of the digital images themselves may be analyzed to automatically determine (i.e., without explicit user input) which digital images are associated with the event and which digital images are unrelated to the event. The digital images determined to be associated with the event may be tagged or annotated, for example, in a searchable database and/or their metadata, with one or more words (or "tokens") and/or phrases determined from the natural language input to be associated with the event.
Various signals may be analyzed to determine whether a given digital image captured on a date associated with an event should be classified/annotated as being relevant to the event. In some implementations, a record of user-controlled computing interactions with a digital image can be examined to determine whether it should be classified as related to the event. For example, in some implementations, an amount of time for each of a plurality of digital images displayed via one or more graphical user interfaces may be determined. Images displayed to the user and/or other users over a longer period of time may be classified as relevant to the event than other images captured on the same day. Intuitively, if a user attempts to provide natural language input to his or her automated assistant to mark a particular event, the digital images viewed by the user and/or others are more likely to be relevant to the event than other digital images captured during the same day, for example, that may not be of significant importance.
Additionally or alternatively, in some embodiments, metrics of image manipulation applied to each of the plurality of digital images via one or more digital image manipulation applications may be considered. For reasons similar to those described above, users are more likely to manipulate (e.g., apply filters, crop, zoom in/out, etc.) digital images associated with events that have been specifically identified (e.g., to an automated assistant) than they would to manipulate other images. Additionally or alternatively, in some implementations, a shared metric associated with each of the plurality of digital images may be determined. If a user shares one or more particular digital images captured during a day that has been identified as an event more than other digital images captured during the same day, the one or more particular digital images are more likely to be associated with the identified event. Likewise, if other users share digital images among themselves, the shared digital images are more likely to be associated with the identified events in the life of the originally sharing user.
Additionally or alternatively, in some implementations, various types of image recognition processing may be performed on the plurality of digital images captured on the date to identify one or more objects or entities associated with the event and depicted in the subset of the plurality of digital images. Various types of image processing may be employed, such as object recognition, facial recognition, entity recognition, and the like. For example, if the identified event associated with the user is a graduation of a child, a digital image describing objects known to be associated with a graduation (such as a hat, a dress, a large number of people, a podium, etc.) may be automatically classified/annotated as being related to the graduation. Other images captured on the same date describing objects related to non-graduation ceremonies, such as images taken on farmer markets, may not be so classified/annotated.
In various implementations, image processing may be biased toward identifying one or more tokens related to an event and/or identifying one or more tokens related to other objects/entities known to be related to a token by way of a knowledge graph. For example, objects detected in an image may be segmented and applied individually as input to a trained machine learning model (such as a convolutional neural network). The output may be the embedding of the individual objects in the latent/semantic space. These embeddings can then be checked for semantic similarity (e.g., distance in the potential space) of semantic embeddings related to birthdays, such as "cake", "presents", "clowns", etc. The embedding of objects generated from digital images that are most similar in concept in the underlying space with respect to birthdays may be enhanced over the embedding of other birthday-independent objects generated from digital images. Additionally or alternatively, contextual information determined from natural language input by the user may be used to break the impasse if a particular object is similarly a distance away from two different embeddings related to two different types of events.
In various embodiments, a combination of the foregoing signals may be used to automatically classify/annotate digital images as relating to events associated with a user. In some such implementations, each contributing signal may increase an aggregate measure of confidence that a digital image should be classified as relating to an event associated with the user. A given digital image may be classified/annotated as being related to an event if the aggregate metric for the confidence of the given digital image satisfies some threshold (e.g., manually set, learned over time, etc.).
For example, assume that the user identifies (e.g., identifies to their automated assistant) a kindergarten graduation ceremony. Typically, a kindergarten graduation may not characterize (feature) many objects typically associated with a graduation, such as a hat and/or dress. However, the images of the kindergarten graduation may include other weaker graduation signals, such as crowd, podium, and the like. These signals are not sufficient by themselves to classify/annotate digital images as being related to kindergarten graduation ceremonies. However, assume that the digital image is displayed on the parent photographer's graphical user interface for longer than other digital images captured that day, or that the parent shares the digital image with more users (e.g., grandparents) than other digital images captured that day. In some implementations, an aggregate measure of confidence that a digital image is associated with a kindergarten graduation, calculated based on multiple signal sets (e.g., detected crowd and podium, extended display time, user-provided natural language input), may satisfy a threshold, resulting in the digital image being classified/annotated as being related to the kindergarten graduation.
As another example, assume that other digital images are captured within a relatively short time window as the particular digital image classified/annotated as being related to the kindergarten graduation. These other photographs themselves do not necessarily have a strong signal relating to the kindergarten graduation. For example, if several family members are present at the kindergarten graduation, they may take a group photo in the hallway or outside (even not including the kindergarten graduation). However, these other digital images may also be annotated as being related to the kindergarten graduation ceremony because the capture of these other digital images is close in time to the capture of the annotated digital image.
Other signals may be utilized in addition to or in lieu of temporally proximate to the classified/annotated digital image to classify an otherwise ambiguous digital image as being related to a particular event. Continuing with the example of the kindergarten graduation, location coordinates (e.g., geotags) associated with other digital images captured on the same day may be associated with the kindergarten graduation (even though the location coordinates themselves do not provide a strong signal related to graduation), based on the location coordinates being spatially close to other digital images that are more strongly associated with the kindergarten graduation.
The techniques described herein may be used to annotate digital images with information other than about events associated with a user. For example, in some embodiments, the techniques described herein may be used to automatically identify and/or tag individuals in digital images. Many digital image libraries may undergo face clustering to identify clusters of the same face depicted on multiple digital images acquired by a user. However, unless the user labels the individual's face in one or more images, these face clusters may remain anonymous. Using the techniques described herein, a user-provided signal (such as a natural language input identifying an event related to a particular individual) may be used to automatically assign an identity to a hitherto anonymous face cluster. Assume that there is a father, for example, for an automated assistant: "Redmond look his first steps last Saturday (Redmond takes the first step on Saturday)". Digital images (or digital videos) acquired on that date may be classified/annotated using the techniques described herein. Children repeatedly depicted in these classified/annotated images may be matched with the anonymous face clusters so far. Then, because "Redmond" is identified by the user, the identifier "Redmond" may be assigned to the face cluster so that it is no longer anonymous.
The techniques described herein result in various technical advantages. For example, automatically classifying/annotating digital images using the techniques described herein may make a digital image library more robustly searchable because digital images may now be searched based on events associated with a user that may otherwise be unrecognizable based on objects/entities identified in the images themselves. In addition, the techniques described herein may reduce the amount of user input (or more generally, effort) required to organize a digital image library.
As another example, automatically determining the identity of an individual associated with a face cluster may facilitate automatic tagging of the individual in past and/or future digital images. By tagging the personal image with the identity of the individual, a database of digital images can be searched by the identity of the individual and subtitles characterizing the identity of the individual are automatically generated. Moreover, once an individual is tagged in a particular digital image, it may become easier to classify/annotate the digital image using the techniques described herein. For example, and using the example above in connection with the Delia birthday, assume that Delia is depicted in a digital image captured on the day of her birthday (a fact that can be determined by the parent's natural language input, "Tomorrow is Delia's third birthday"). The fact that Delia was identified in a digital image captured that day of her birthday can be used as a strong signal that the digital image should be classified/annotated as being relevant to her birthday.
As another example, and as previously described, digital images annotated using the techniques described herein may be used as training data to train (e.g., start over or improve) downstream machine learning models/classifiers. For example, a classifier may be trained to comprehensively identify events depicted in a digital image, e.g., based on the overall similarity of the digital image to other similar digital images having the same context, rather than (uniquely) based on identifying individual objects/entities in the image. Once trained, these machine learning models can automatically annotate subsequent digital images even without the benefit of user-provided input identifying the event. For example, digital images of different users captured at different kindergarten graduations may be annotated as generally related to the graduations, even if the different users do not provide any sort of signal as to the occurrence or imminence of the kindergarten graduations.
In some implementations, there is provided a method performed by one or more processors, the method comprising: obtaining, via one or more input components of a computing device, natural language input provided by a user, wherein the natural language input is spoken by the user to an automated assistant executing at least in part on the computing device; analyzing the natural language input to determine an event associated with the user, determine one or more tokens of the natural language input describing the event, and determine a date associated with the event; identifying a plurality of digital images captured on the date; examining a record of user-controlled computing interactions associated with the identified plurality of digital images; classifying a subset of the plurality of digital images captured on the date as being related to the event based on the examining, wherein other digital images of the plurality of digital images captured on the date are not classified as being related to the event; and storing, in a searchable database, data indicative of one or more of the tokens describing the event in association with a subset of the plurality of images classified as relevant to the event, wherein the digital images in the searchable database can be searched based on the one or more tokens stored in association with the digital images.
In various embodiments, the obtaining may include: receiving, by the automatic assistant via the one or more input components of the computing device, an audio recording of an utterance spoken by the user into the automatic assistant; and performing speech-to-text processing on the audio recording to generate the natural language input.
In various implementations, the examining may include determining an amount of time to display each of the plurality of digital images via one or more graphical user interfaces. In various implementations, the amount of time to display a given digital image of the plurality of digital images may include an accumulated amount of time to display the given digital image across the one or more graphical user interfaces. In various implementations, the one or more graphical user interfaces may include multiple graphical user interfaces rendered on multiple different displays, and the cumulative amount of time to display the given image may include a cumulative amount of time to display the given image across the multiple graphical user interfaces.
In various embodiments, the examining may include determining a metric of image manipulation applied to each of the plurality of digital images via one or more digital image manipulation applications. In various implementations, the examining may include determining a shared metric associated with each of the plurality of digital images. In various implementations, the metric of sharing associated with a given digital image of the plurality of digital images may include a count of sharing of the given digital image by a user capturing the given digital image to a plurality of other users. In various implementations, the metric of sharing associated with a given digital image of the plurality of digital images may include a count of sharing the given digital image across a plurality of other users.
In various implementations, the method may further include formulating natural language captions for each subset of the digital images based on the one or more tokens describing the event. In various embodiments, the method may further include applying one or more digital images of the subset of digital images as input to a machine learning classifier to generate an output; comparing the output to the one or more tokens describing the event to generate an error; and training the machine learning classifier based on the error, wherein the training configures the machine learning classifier to classify subsequent digital images as being related to the one or more tokens describing the event.
In various embodiments, the method may further comprise: performing an image recognition process on the identified plurality of images captured at the date to identify one or more objects or entities depicted in the subset of the plurality of digital images that are associated with the event, wherein the performing comprises biasing the image recognition process towards identifying one or more tokens related to the event; wherein the classification is further based on the identified one or more objects or entities.
In another aspect, a method implemented using one or more processors may comprise: obtaining, via one or more input components of a computing device, natural language input provided by a user; analyzing the natural language input to determine an event associated with the user, determine one or more tokens describing the event, and determine a date associated with the event; identifying a plurality of digital images captured on the date; performing image recognition processing on the identified plurality of images captured at the date to identify one or more objects or entities depicted in the subset of the plurality of digital images that are associated with the event, wherein the performing comprises biasing image recognition processing towards identifying one or more tokens related to the event; classifying the subset of the plurality of digital images as related to the event, wherein other digital images of the plurality of digital images captured on the date that do not describe the one or more objects or entities are not classified as related to the event; and storing, in a searchable database, data indicative of one or more of the tokens describing the event in association with a subset of the plurality of digital images classified as related to the event.
In various embodiments, the obtaining may include: receiving, by the automatic assistant via the one or more input components of the computing device, an audio recording of an utterance spoken by a user to the automatic assistant; and performing speech-to-text processing on the audio recording to generate the natural language input.
In various implementations, the method may further include formulating natural language captions for each subset of the digital images based on the one or more tokens describing the event.
Additionally, some embodiments include one or more processors of one or more computing devices, wherein the one or more processors are operable to execute instructions stored in an associated memory, and wherein the instructions are configured to perform any of the foregoing methods. Some embodiments also include one or more non-transitory computer-readable storage media storing computer instructions executable by one or more processors to perform any of the foregoing methods.
It should be appreciated that all combinations of the above concepts and additional concepts described in greater detail herein are contemplated as being part of the subject matter disclosed herein. For example, all combinations of the subject matter claimed in the claims appended to this disclosure are contemplated as being part of the subject matter disclosed herein.
Drawings
FIG. 1 depicts an example environment in which selected aspects of the present disclosure may be implemented.
Fig. 2A and 2B schematically depict examples of how the techniques described herein may be employed to annotate a digital image based on natural language input provided by a user.
Fig. 3A and 3B schematically depict another example of how the techniques described herein may be employed to annotate a digital image based on natural language input provided by a user.
Fig. 4 schematically depicts an example of how images are annotated using the techniques described herein and then used to train downstream machine learning models for comprehensive context/topic classification.
Fig. 5 depicts a flowchart illustrating an example method according to embodiments disclosed herein.
FIG. 6 illustrates an example architecture of a computing device.
Detailed Description
In fig. 1, an example environment is illustrated in which the techniques disclosed herein may be implemented. The example environment includes one or more client computing devices 106. Each client device 106 may execute a respective instance of an automated assistant client 108, which automated assistant client 108 may also be referred to herein as the "client portion" of the automated assistant. One or more cloud-based automated assistant components 119 (which may also be collectively referred to herein as the "server portion" of the automated assistant) may be implemented on one or more computing systems (collectively referred to as "cloud" computing systems) that are communicatively coupled to the client device 106 via one or more local and/or wide area networks (e.g., the internet), indicated generally at 114.
In various embodiments, the manner by which an instance of the automated assistant client 108 interacts with one or more cloud-based automated assistant components 119 may form content that appears from the user's perspective to be a logical instance of the automated assistant 120, which the user may engage in a man-machine conversation with the automated assistant 120. An example of such an automated assistant 120 is shown in dashed lines in fig. 1. Thus, it should be understood that each user engaged with the automated assistant client 108 executing on the client device 106 may actually engage with his or her own logical instance of the automated assistant 120. For the sake of brevity and simplicity, "automatic assistant (term)" as used herein to "service" a particular user will refer to a combination of an automatic assistant client 108 and one or more cloud-based automatic assistant components 119 (which cloud-based automatic assistant component(s) 119 may be shared among multiple automatic assistant clients 108) executing on a client device 106 operated by the user. It should also be understood that in some implementations, the automated assistant 120 may respond to requests from any user, regardless of whether the user is "served" by a particular instance of the automated assistant 120.
The one or more client devices 106 may include, for example, one or more of the following: a desktop computing device, a laptop computing device, a tablet computing device, a mobile phone computing device, a computing device of a user vehicle (e.g., an in-vehicle communication system, an in-vehicle entertainment system, an in-vehicle navigation system), a standalone interactive speaker (which may include a visual sensor in some cases), a smart device such as a smart television (or a standard television equipped with a network dongle with automatic assistant functionality), and/or a user wearable apparatus including a computing device (e.g., a watch of a user with a computing device, glasses of a user with a computing device, a virtual or augmented reality computing device). Additional and/or alternative client computing devices may be provided. Some client devices 106, such as a stand-alone interactive speaker (or "smart speaker"), may take the form of an assistant device that is primarily designed to facilitate a conversation between a user and the automated assistant 120. Some such assistant devices may take the form of a stand-alone interactive speaker with a display that may or may not be a touch screen.
In some implementations, the client device 106 may be equipped with one or more visual sensors 107 having one or more fields of view, but this is not required. The one or more vision sensors 107 may take various forms, such as a digital camera, a passive infrared ("PIR") sensor, a stereo camera, an RGBd camera, and so forth. The one or more vision sensors 107 may be used to capture image frames (still images or video) of the environment in which the client device 106 is deployed. May then be presented, for example, by visual cue module 1121The image frames are analyzed to detect user-provided visual cues contained in the image frames. These visual cues may include, but are not limited to, gestures, gaze at a particular reference point, facial expressions, predefined movements of the user, and the like. These detected visual cues may be used for various purposes, such as invoking the automated assistant 120 and/or causing the automated assistant 120 to perform various actions.
The automated assistant 120 may participate in a human-machine conversation session with one or more users via user interface input and output devices of one or more client devices 106. In some implementations, the automated assistant 120 can engage in a human-machine conversation session with the user in response to user interface input provided by the user via one or more user interface input devices of one of the client devices 106. In some of those implementations, the user interface input is explicitly directed to the automated assistant 120. For example, a user can verbally provide (e.g., type, say) a predetermined invocation phrase, such as "OK, asistat (OK, Assistant)" or "Hey, asistat (Hey, Assistant)". When such spoken input is spoken, the spoken input may be captured by microphone 109, and automated assistant 120 may be caused to actively begin listening for or monitoring the typed text. Additionally or alternatively, in some implementations, the automated assistant 120 may be invoked based on one or more detected visual cues, alone or in combination with a spoken invocation phrase.
In some implementations, the automated assistant 120 can participate in a human-machine conversation session in response to user interface inputs even when the user interface inputs are not explicitly directed to the automated assistant 120. For example, the automated assistant 120 may examine the content of the user interface input and participate in the conversation session in response to the presence of certain terms in the user interface input and/or based on other prompts. In many implementations, the automated assistant 120 can utilize speech recognition to convert utterances from the user into text and thus respond to the text, for example, by providing search results, general information, and/or taking one or more responsive actions (e.g., playing media, starting a game, ordering a dish, etc.). In some implementations, the automated assistant 120 can additionally or alternatively respond to the utterance without converting the utterance to text. For example, the automated assistant 120 may convert the speech input into an embedding, into an entity representation (indicating one or more entities present in the speech input), and/or other "non-text" representations, and operate on such non-text representations. Thus, embodiments described herein as operating based on text converted from speech input may additionally and/or alternatively operate directly on speech input and/or other non-text representations of speech input.
Each of the client computing device 106 and the computing devices operating the cloud-based automated assistant component 119 may include one or more memories for storing data and software applications, one or more processors for accessing data and executing applications, and other components that facilitate communication over a network. The operations performed by the client computing device 106 and/or the automated assistant 120 may be distributed across multiple computer systems. The automated assistant 120 may be implemented, for example, as a computer program running on one or more computers in one or more locations coupled to each other over a network.
As described above, in various implementations, the client computing device 106 may operate the automated assistant client 108 or a "client portion" of the automated assistant 120. In various implementations, the automated assistant client 108 may include a voice capture module 110 and/or a visual cue module 1121. In other implementations, one or more aspects of the voice capture module 110 and/or the visual cue module 112 may be implemented separately from the automated assistant client 108, e.g., by one or more cloud-based automated assistant components 119. For example, in FIG. 1, there is also a cloud-based visual cue module 112 that can detect visual cues in image data2。
In various implementations, the voice capture module 110, which may be implemented using any combination of hardware and software, may interface with hardware such as a microphone 109 or other pressure sensor to capture an audio recording of a user utterance. Various types of processing may be performed for this audio recording for various purposes. .
In various implementations, the visual cue module 1121(and/or cloud-based visual cue module 112)2) May be implemented using any combination of hardware or software and may be configured to analyze one or more image frames provided by the vision sensor 107 to detect one or more visual cues captured in and/or across the one or more image frames. Visual cue module 1121Visual cues may be detected using a variety of techniques. For example, visual cue module 1121(or 112)2) One or more artificial intelligence (or machine learning) models may be used that are trained to generate an output indicative of user-provided visual cues detected in the image frames.
The client device 106 may also have other applications installed, such as a Web browser 111 and/or a message exchange client 113. The message exchange client 113 may be in various forms. In some embodiments, the message exchange client 113 may be of the form: a short messaging service ("SMS") client and/or a multimedia messaging service ("MMS") client, an online chat client (e.g., instant messenger, internet relay chat, or "IRC," etc.), a messaging application associated with a social network, and the like. In some implementations, the message exchange client 113 can be implemented within a web page rendered by the web browser 111. In various implementations, message exchange client 113 may provide an interface for a user to participate in a typed or spoken human-machine conversation with automated assistant 120, as a one-to-one conversation or as a multi-participant conversation in which automated assistant 120 may "participate".
As previously described, the voice capture module 110 may be configured to capture the voice of the user, for example, via the microphone 109. Additionally or alternatively, in some implementations, the speech capture module 110 may be further configured to convert the captured audio to text and/or other representations or embeddings, for example using speech-to-text ("STT") processing techniques. Additionally or alternatively, in some implementations, the speech capture module 110 may be configured to convert text to computer-synthesized speech, for example, using one or more speech synthesizers. However, in some cases, because the client device 106 may be relatively limited in computing resources (e.g., processor cycles, memory, battery, etc.), the speech capture module 110 local to the client device 106 may be configured to convert a limited number of different spoken phrases (particularly phrases that invoke the automated assistant 120) into text (or other forms, such as lower-dimensional embedding). Other speech inputs may be sent to the cloud-based automatic assistant component 119, which may include a cloud-based text-to-speech ("TTS") module 116 and/or a cloud-based STT module 117.
Cloud-based TTS module 116 may be configured to convert text data (e.g., natural language responses formulated by automated assistant 120) into computer-generated speech output using the virtually unlimited resources of the cloud. In some implementations, the TTS module 116 can provide the computer-generated speech output to the client device 106 for direct output, e.g., using one or more speakers. In other implementations, text data (e.g., natural language responses) generated by the automated assistant 120 can be provided to the speech capture module 110, and the speech capture module 110 can then convert the text data into locally output computer-generated speech.
The cloud-based STT module 117 may be configured to convert audio data captured by the speech capture module 110 into text using the virtually unlimited resources of the cloud, which may then be provided to the intent matcher 135. In some implementations, the cloud-based STT module 117 can convert an audio recording of speech into one or more phonemes and then convert the one or more phonemes into text. Additionally or alternatively, in some embodiments, the STT module 117 may employ a state decoding graph. In some implementations, the STT module 117 can generate a plurality of candidate text interpretations of the user utterance. In some implementations, the STT module 117 may weight or bias particular candidate text interpretations higher than other candidate text interpretations depending on whether there are simultaneously detected visual cues.
The automatic assistant 120, and in particular the cloud-based automatic assistant component 119, can include an intent matcher 135, the aforementioned TTS module 116, the aforementioned STT module 117, as well as other components described in more detail below. In some implementations, one or more modules and/or modules of the automated assistant 120 may be omitted, combined, and/or implemented in a component separate from the automated assistant 120. In some implementations, to protect privacy, one or more components of the automated assistant 120 (such as the natural language processor 122, the TTS module 116, the STT module 117, etc.) may be implemented at least partially on the client device 106 (e.g., excluded from the cloud).
In some implementations, the automated assistant 120 generates the response content in response to various inputs generated by a user of one of the client devices 106 during a human-to-machine conversation session with the automated assistant 120. The automated assistant 120 can provide response content (e.g., over one or more networks when separate from the user's client device) for presentation to the user as part of a conversation session. For example, the automated assistant 120 may generate response content in response to free-form natural language input provided via the client device 106. As used herein, free-form input is input that is formulated by a user and is not limited to a set of options presented to the user for selection. Free-form natural language input may be spoken (and captured by microphone 109) and/or typed (e.g., typed into one or more interfaces provided by one or more applications, such as message exchange client 113).
As used herein, a "conversation session" may include a logically self-consistent conversation of one or more messages between a user and the automated assistant 120 (and, in some cases, other human participants). The automated assistant 120 may distinguish between multiple dialog sessions with the user based on various signals, such as the passage of time between sessions, changes in the user context between sessions (e.g., location, before/during/after a scheduled meeting, etc.), detection of one or more intervening interactions (e.g., user switching application for a period of time, user leaving, then later returning to a separate voice-activated product) between the user and the client device other than the dialog between the user and the automated assistant, locking/hibernation of the client device between sessions, and changes in the client device for interfacing with one or more instances of the automated assistant 120, and so forth.
The intent matcher 135 may be configured to determine the intent of the user based on input provided by the user (e.g., vocal utterances, visual cues, etc.) and/or based on other signals (e.g., sensor signals and online signals (e.g., data obtained from a Web service), etc.). In some implementations, the intent matcher 135 can include the natural language processor 122 and the cloud-based visual cue module 112 described above2. In various embodiments, in addition to cloud-based visual cue module 1122Beyond having more resources available for disposal, the cloud-based visual cue module 1122May be similar to visual cue module 1121To operate. In particular, cloud-based visual cue module 1122Visual cues that may be used by the intent matcher 135 alone or in combination with other signals may be detected to determine the intent of the user.
The natural language processor 122 may be configured to process natural language input generated by a user through the client device 106 and may generate annotation output (e.g., in textual form) for use by one or more other components of the automated assistant 120. For example, the natural language processor 122 may process natural language free-form input generated by a user via one or more user interface input devices of the client device 106. The generated annotation output includes one or more annotations of the natural language input and/or one or more (e.g., all) words in the natural language input.
In some implementations, the natural language processor 122 is configured to recognize and annotate various types of grammatical information in the natural language input. For example, the natural language processor 122 may include a morphology module that may separate individual words into morphemes and/or annotate the morphemes, for example, in a category of morphemes. The natural language processor 122 may also include a part-of-speech tagger configured to annotate a word with a grammatical role for the word. For example, a part-of-speech tagger may tag each word with a part-of-speech of the word (such as "noun", "verb", "adjective", "pronoun", etc.). Further, for example, in some implementations, the natural language processor 122 may additionally and/or alternatively include a dependency parser (not shown) configured to determine syntactic relationships between words in the natural language input. For example, the dependency parser may determine which words modify other words, as well as the subject and verbs of the sentences, etc. (e.g., parse trees), and may annotate such dependencies.
In some implementations, the natural language processor 122 can additionally and/or alternatively include an entity tagger (not shown) configured to annotate entity references in one or more segments, such as references to people (including, for example, literary characters, celebrities, public characters, etc.), organizations and locations (real and fictional), and so forth. In some implementations, data about the entities may be stored in one or more databases, such as in a knowledge graph (not shown). In some implementations, the knowledge graph can include nodes (in some cases, entity attributes) that represent known entities and edges that connect the nodes and represent relationships between the entities. For example, a "banana" node may be connected (e.g., as a child) to a "fruit" node, which may in turn be connected (e.g., as a child) to a "product" and/or "food" node. As another example, a restaurant called a "virtual cafe" may be represented by a node that also includes attributes such as its address, the type of food served, time, contact information, and the like. In some implementations, the "financial Caf" node may be connected to one or more other nodes (e.g., a "restaurant" node, a "business" node, and a node representing a city and/or state in which the restaurant is located, etc.) by an edge (e.g., representing a relationship of a child to a parent).
The entity tagger of the natural language processor 122 may annotate references to entities at a higher level of granularity (e.g., to enable identification of all references to a category of entities, such as a person) and/or a lower level of granularity (e.g., to enable identification of all references to a particular entity (e.g., a particular person)). The entity tagger may rely on the content of the natural language input to resolve a particular entity and/or may optionally communicate with a knowledge graph or other entity database to resolve a particular entity.
In some implementations, the natural language processor 122 may additionally and/or alternatively include a coreference resolver (not shown) configured to group or "cluster" references of the same entity based on one or more contextual cues. For example, the term "there" may be parsed into "synthetic Caf é" using a coreference parser in the natural language input "I liked synthetic Caf time we ate there where I liked to have a meal last time".
In some implementations, one or more components of the natural language processor 122 may rely on annotations from one or more other components of the natural language processor 122. For example, in some implementations, a named entity tagger can rely on annotations from co-referred resolvers and/or dependent resolvers to annotate all mentions of a particular entity. Also, for example, in some implementations, the coreference resolver may rely on annotations from dependent resolvers to cluster references to the same entity. In some implementations, in processing a particular natural language input, one or more components of the natural language processor 122 may determine one or more annotations using related previous inputs and/or other related data in addition to the particular natural language input.
The intent matcher 135 may be based on, for example, output from the natural language processor 122 (which may include annotations and words of the natural language input) and/or based on information from a visual cue module (e.g., 112)1And/or 1122) Using various techniques to determine the user's intent. In some implementations, the intent matcher 135 can access one or more databases (not shown) that include, for example, a plurality of mappings between grammars, visual cues, and responsive actions (or, more generally, intents). In many cases, these grammars may be selected and/or learned over time and may represent the most common intent of the user. For example, a grammar "play<artist>(Play)<Artist (R & D)>) "can be mapped to an intent that invokes a response action that causes the intent to be<artist>(<Artist (R & D)>) Is played on the user-operated client device 106. Another grammar "[ weather | forecast]today ([ weather | weather forecast)]Today) "may be associated with, for example," what's the weather today "and" what's the weather for today? (what is today's weather forecast.
In addition to or instead of grammars, in some implementations, the intent matcher 135 may employ one or more trained machine learning models, either alone or in combination with one or more grammars and/or visual cues. These trained machine learning models may also be stored in one or more databases and may be trained to recognize intent by: for example, by embedding data indicative of the user utterance and/or any detected user-provided visual cues into the potential space, and then determining which other insertions (and therefore intentions) are closest, for example, using techniques such as euclidean distance, cosine similarity, and the like.
As in "play<artist>As seen in the example syntax, some syntaxes have slots that can be filled with slot values (or "parameters") (e.g.,<artist>). The slot value may be determined in various ways. The user will typically actively provide the slot value. For example, for the grammar "Order me a<topping>pizza (order me)<topping>Pizza) ", the user might say" order me a usage pizza "in which case the slot is automatically filled<topping>. Additionally or alternatively, if the user invokes a grammar that includes slots to be filled with slot values, without the user having to actively provide the slot values, the automated assistant 120 may request these slot values from the user (e.g., "what type of crust is you on your pizza. In some implementations, the module 112 can be based on visual cues1-2The detected visual cue fills the slot with the slot value. For example, while the user says "On this day Delia turns this many" (Delia becomes so large today), the user or Delia may hold up three fingers to the visual sensor 107 of the client device 106. Alternatively, the user may say "Find me movies like this" while holding the DVD box for a particular movie.
Fulfillment (or "resolution") information may take various forms, as the intent may be fulfilled (or "resolved") in a number of ways. Suppose that The user requests to provide pure information, such as "Where powers The outdoor notes of The Shining' filtered? (where was the outdoor shot of the "flash") "the user's intent can be determined as a search query, for example, by the intent matcher 135. The intent and content of the search query may be provided to a fulfillment module 124, which, as shown in FIG. 1, may communicate with one or more search modules 150 configured to search a corpus of documents and/or other data sources (e.g., knowledge graphs, etc.) to obtain responsive information. The fulfillment module 124 may provide data indicative of the search query (e.g., text of the query, dimension reduction embedding, etc.) to the search module 150. The search module 150 may provide: response information, such as GPS coordinates; or other more specific information such as "Timberline Lodge, Mt. hood, Oregon (mountain Loge, Huideshan, Oregon). "the response information may form part of the fulfillment information generated by fulfillment module 124.
Additionally or alternatively, fulfillment module 124 may be configured to receive the user's intent and any slot values provided by the user or otherwise determined (e.g., the user's GPS coordinates, user preferences, etc.), e.g., from intent matcher 135, and trigger a responsive action. The responsive action may include, for example, ordering goods/services, starting a timer, setting a reminder, initiating a phone call, playing media, sending a message, etc. In some such embodiments, the fulfillment information may include slot values associated with fulfillment, confirmation responses (which may be selected from predetermined responses in some cases), and so forth. In some implementations, fulfillment module 124 may, for example, generate data indicative of events associated with the user as, for example, fulfillment information, e.g., for storage in automated assistant history database 166.
The natural language generator 126 may be configured to generate and/or select natural language output (e.g., words/phrases designed to mimic human speech) based on data obtained from various sources. In some implementations, the natural language generator 126 may be configured to receive as input fulfillment information associated with fulfillment of the intent and generate a natural language output based on the fulfillment information. Additionally or alternatively, the natural language generator 126 may receive information (e.g., required slots) from other sources, such as third party applications, which may be used to write natural language output for the user.
Various aspects of the disclosure may be implemented in whole or in part by the image annotation engine 128. In general, the image annotation engine 128, alone or in combination with other components of the automated assistant 120, may be configured to obtain language input provided by a user via one or more input components of the client device 106. The natural language input may be spoken by the user to the automated assistant 120 or the natural language input may be targeted elsewhere, such as for social media status updates. The image annotation engine 128, alone or in combination with other components of the automated assistant 120, may be further configured to analyze the natural language input to determine an event associated with the user, determine one or more tokens describing the natural language input of the event, and determine a date associated with the event.
The image annotation engine 128, alone or in combination with other components of the automated assistant 120, may be further configured to identify, for example, in an image repository 172 (e.g., a searchable database of digital images acquired by users) a plurality of digital images captured at that date. The image annotation engine 128, alone or in combination with other components of the automated assistant 120, may be further configured to examine a record of user-controlled computing interactions having the identified plurality of digital images, for example, stored in the interaction records database 168. Based on the examination, the image annotation engine 128, alone or in combination with other components of the automated assistant 120, may be configured to classify the subset of the plurality of digital images captured at the date as being related to the event. Other digital images of the plurality of digital images captured at the date may not be classified as being related to the event. The image annotation engine 128, alone or in combination with other components of the automated assistant 120, may be further configured to store, in the searchable images repository 172 (also referred to herein as a "database"), data indicative of one or more of the tokens describing the event in association with the subset of the plurality of images classified as relevant to the event.
As described above, the image annotation engine 128 may access a plurality of databases and/or indices, such as the automated assistant history database 166, the interaction records database 168, and/or the searchable images repository 172. The automated assistant history database 166 may store logs and/or other data (e.g., transcriptions, topics, etc.) that indicate past conversations with the automated assistant 120. In some implementations, the techniques described herein may utilize information stored in the database 166 to identify events associated with a user and/or to trigger digital image analysis for annotation. The interaction record database 168 may store (at least temporarily) a record of user-controlled interactions with digital images. User-controlled interaction with an image may include, for example, user manipulation of the image, user sharing of the image, user "surfacing" (e.g., displaying the image in a graphical user interface) of the image, and so forth. The image annotation engine 128 may also access a searchable images repository 172, which searchable images repository 172 may store digital images acquired by one or more users. For example, many users log on to their online accounts on their mobile devices, which provide, among other things, backup functionality for photos.
Fig. 2A and 2B depict examples of how the techniques described herein may be employed to annotate a digital image with context information based on natural language input provided by a user. In fig. 2A, a user 101 is interacting with an automated assistant 120 executing at least in part on a client device 206. In fig. 2A, the client device 206 is in the form of a stand-alone interactive speaker. However, this is not meant to be limiting, and any other type of client device operable to participate in a natural language conversation with the automated assistant 120 may be employed, such as a smart phone, a smart appliance (e.g., television), a laptop computer, a tablet computer, a vehicle computing system, a smart watch, smart glasses, and so forth.
In FIG. 2A, the user 101 invokes the automated Assistant by speaking "OK, Assistant" and then provides the natural language input, "Delia's third birthday is tomorrow (tomorrow is the third birthday of Delia)". While the user 101 provides the statement verbally, this is not meant to be limiting, and it should be understood that the user 101 may also provide natural language input to the automated assistant using typed input.
In some embodiments, the audio recording of the user's 101 statement may be processed, for example using STT processing, to generate a textual natural language input, in this case the input is "Delia's third birthday is tomorrow". The natural language input may then be analyzed to determine an event associated with the user 101 and a date associated with the event, as well as one or more other word(s) or phrase(s) describing the event, if provided by the user 101, for example, as described above with respect to fig. 1. In the example of fig. 2A, the event is "Delia's birthday", the date is "tomorrow" (which may be resolved to a more standardized date/time), and the word/phrase or words describing the event include "third birthday".
Now, assume that tomorrow arrives, and that user 101 operates one or more client devices (e.g., mobile phone, smart watch), even non-smart devices (such as digital camera), to capture a plurality of digital images 1701-5. The techniques described herein may be employed to selectively classify and annotate the image 170 as being related to an event. Obtained on the same day and does not look likeRelated images may not be classified and annotated as related.
For example, in FIG. 2B, image 1701And 1703Includes objects commonly known to be associated with a birthday, including a balloon (170)1) And cake (170)3). Rather, the other digital images depict birds (170)2) Fish (170)4) And a vehicle (170)5) These objects are generally unrelated to birthdays. Thus, the image 1701And 1703Stored in a database 172 (e.g., a cloud-based photo store, a photo store on a device, etc.) in association with data indicative of one or more tokens ("Delia's third birthday") describing an event. Other images 1702、1704And 1705May be stored in the database 172 but without any comments or accompanying data indicating that they are relevant to the event.
Assume that the user 101 indicates that tomorrow is a 16 year old birthday of Delia, rather than a 3 year old birthday of Delia. In the united states, many states permit the issuance of driver licenses at 16 years of age. In some embodiments, the fact that tomorrow is a Delia sixteen year birthday, rather than her three year birthday, may affect which digital images are classified as being relevant to her birthday. For example, image 1705A vehicle is depicted. It may be that the Delia receives the depicted vehicle as a birthday present, or her parent/friend may capture an image of the vehicle because the Delia is driving. In any case, the digital image 170 may be taken because the event "sixteenth birthday" is generally of driving-related importance, at least in the United states5Categorizing/annotating as relating to the sixteen year old birthday of Delia.
In various implementations, the image processing performed by the image annotation engine 128 may be biased towards identifying one or more tokens related to the event. For example, in FIG. 2B, image processing may be biased toward identifying various objects and/or entities known to be related to a birthday, e.g., by way of a knowledge map. For example, the digital image may be segmented based on the detected objects. Each detected object may be used to generate potential spatial (i.e., semantic space) embeddings, for example, using a trained machine learning model, such as a convolutional neural network. Then, the embedding distances (e.g., euclidean distances) to other insertions in the potential space can be determined, for example, to identify the N nearest insertions (N being a positive integer). The N nearest embeddings may then be ranked based on their distance to a birthday-related embeddings, such as tokens contained in a natural language input (birthday) or other tokens/topics determined from a knowledge graph as being related to birthday.
Continuing with the above example, assume that the digital image captured on the day of the Delia birthday depicts some balloons, chairs, tables, and coffee pots. These objects can be segmented from the image and used to generate corresponding embeddings in the underlying space. These embeddings can be used to identify objects. For example, a chair inlay will be embedded relatively close to other chairs, a coffee maker inlay will be embedded relatively close to other coffees, and so on. Only balloons are particularly relevant to the birthday. Thus, the embedding may be ranked based on their association with the birthday (or concepts related to the birthday as determined from the knowledge graph) and/or their relationship may be enhanced based on concepts associated with the birthday. For example, a balloon embedded in a potential space may be a similar distance from the birthday and the narrative (an event common to balloons). However, the token "birthday" obtained from the natural language input of the user 101 may be used to enhance the balloon's relationship to the birthday over the balloon's relationship to the rabies rhythm. More generally, contextual information collected from a user's natural language input may be used to "break the impasse" (break a tie) between insertions that are similar in distance but semantically different in the underlying space.
Fig. 3A-3B depict another example in which the techniques described herein may be employed to automatically classify/annotate digital images as relating to user-identified events. Although the same client device 106 and user 101 are depicted in fig. 3A-3B and in fig. 2A-2B, this is not meant to be limiting. In fig. 3A, user 101 identifies a past event, stating: "Yesterday, Delia took her first steps (Yesterday, Delia has gone out of her first step)". This is clearly different from fig. 2A to 2B, where the user 101 identifies a future event. Because the event occurred in the past, the techniques described herein may be employed immediately (or periodically or at any other time) to determine which digital images that user 101 captured yesterday are relevant to this event.
In FIG. 3B, the image annotation engine 128 can identify five digital images 170 taken yesterday in the image repository6-10. These images 1706-10Depicting a bird, a baby walking, another bird, a fish, and a vehicle, respectively. Although object/entity recognition techniques similar to those previously described may be employed to recognize the image 170 depicting the infant walking7But other techniques may alternatively (or additionally) be used. For example, in FIG. 3B, the image annotation engine 128 can examine (e.g., in the database 168) a record of user-controlled computing interactions with the digital image to determine that the user 101 manipulated (e.g., zoomed in or out, cropped, applied filters, etc.) the image 170, for example at 1747. Additionally, at 176, the user 101 has shared the image 170 with a plurality of other users7For example, using text messaging, social media, etc. Based on this examination, the image annotation engine 128 may classify the digital image 1707 as event-related (first step of Delia), with other digital images 1706And 1708-10Not classified as related to the event.
In various embodiments, various different user-controlled computing interactions with a digital image may be considered alone or in combination with other user-controlled computing interactions when determining whether a digital image should be classified as related to an event identified by a user (e.g., to an automated assistant). In some implementations, the amount of time for each digital image displayed via one or more graphical user interfaces may be considered. For example, if the user or one or more other users who acquired an image loads and views an image (i.e., displays an image) longer than other images acquired the same day, the displayed image may be indicated as being very important. If an event is identified on the same day that the image was acquired, it may be sufficient to classify/annotate the digital image as relating to the event, either alone or in combination with other signals described herein. The amount of time for image display may be calculated in various ways, such as the highest cumulative amount of time a given digital image is displayed on any given graphical user interface, or the cumulative amount of time a given image is displayed across multiple graphical user interfaces.
As mentioned above, another user-controlled interaction metric may be a metric of image manipulation applied to a digital image via a digital image manipulation application. The large number of manipulations that a user makes with a particular digital image may indicate that the digital image may be more important to the user than other images. A user may manipulate digital images in various ways using various types of digital image manipulation applications. The user may apply one or more filters to change the overall appearance of the image. Additionally or alternatively, the user may zoom in and/or out on the digital image simply to see the image more closely and/or create a modified image. Additionally or alternatively, the user may crop the image, make various tints to the image (e.g., make uncluttered, blurred, color swapped, etc.).
Yet another user-controlled interaction metric may include determining a shared metric associated with each of the plurality of digital images. As shown in FIG. 3B, at 176, the particular digital image being shared with others to a greater extent than the other digital images may indicate that the particular digital image is relatively important. Various sharing metrics may be employed in various embodiments. For example, a number of times or count that a given digital image is shared by a user capturing the given digital image to a plurality of other users may be determined. Additionally or alternatively, in some implementations, the measure of sharing associated with a given digital image may include, for example, a count of the sharing of the given digital image across a plurality of other users that is unrelated to the user that acquired the image. For example, images that "go red" (goes "visual") in a family member may be classified as related to a particular event.
In various embodiments, a combination of the foregoing signals may be used to automatically classify/annotate digital images as relating to events associated with a user. In some such implementations, each contributing signal may increase an aggregate measure of confidence that a digital image should be classified as relevant to an event. A given digital image may be classified/annotated as being related to an event if the aggregate metric for the confidence of the given digital image satisfies some threshold (e.g., manually set, learned over time, etc.). For example, both object recognition and user-controlled interaction may be examined together to determine that a particular image may be important to a user, and thus may be relevant to a user-identified event.
As another example, assume that the user provides a natural language statement, such as "Tomorrow, I'm going to begin rebuilding an engine". When the day comes, the user may capture a digital image of the engine prior to disassembly, and capture digital images of the components that make up the engine as the user disassembles the engine. Using the techniques described herein, digital images captured today may be analyzed, for example, using a record of user-controlled interactions with the digital images and/or using object recognition, to classify and annotate digital images associated with engine teardown. For example, an image depicting a single engine part may be used to generate an embedding in the underlying space, e.g., using a convolutional neural network. These embeddings may be closer to (and thus semantically similar to) embeddings associated with the engine, part, etc. Conversely, other images captured that day, e.g., vegetables on a farmer market, birthday party of children, etc., may generate embeddings that are remote from engine-related embeddings. Thus, a digital image of the entire engine and its component parts may be automatically annotated as being relevant to the event "engine rebuild". These images may be later searched in association with "engine reconstruction," for example, so that the user may use the images to assist in reassembling the engine.
The techniques described herein are not limited to classifying/annotating images based on natural language input provided by a user to the automated assistant 120. Other sources of natural language input may be used in addition to or in place of the dialog for which the assistant is directed. For example, a user may send an email (e.g., a text message, an email, etc.) to another user that describes a past or future event. Alternatively, the user may post natural language input describing the event to their social networking page, e.g., with or without uploading digital images related to the event. In some implementations, if the user posts a single picture on social media that is related to an event, other images captured that day (or within a small time interval concurrent with the event) may also be classified/annotated as related to the event.
Fig. 4 schematically depicts an example of how a downstream machine learning model may be trained to identify a context of a digital image using training data that includes images annotated using the techniques described herein. First, the user provides some natural language input that identifies an event associated with the user and a date associated with the event. For example, the user may speak a phrase such as "On this day, I not my black belt in karate" (I got an empty lane black band On this day). The token "On this day" may mean that an event has occurred or is about to occur that may be used to annotate the digital image. Alternatively, the user may have identified a past or future date. The remainder of the user's natural language input identifies the event as receipt of a clear-hand black band. At step 1, this natural language input (or other data indicative thereof, such as an embedded) may be provided to the automated assistant 120 and/or the image annotation engine 128.
At step 2 of fig. 4, the automatic assistant 120 and/or the image annotation engine 128 may store information about the event, such as its date and/or one or more user-provided tokens describing the event, in the automatic assistant history database 166. At step 3, one or more images taken on the date associated with the event may be retrieved, for example, from searchable images repository 172. At step 4, the techniques described herein may be performed to identify which of the images retrieved at step 3 are relevant to the event, and which may not. Digital images identified as being relevant to the event may be similarly classified. At step 4, an annotation relating to the event may be generated and stored in searchable images repository 172 (either in another database or as metadata). These annotations may include, for example, one or more tokens spoken by the user, natural language outputs generated by natural language generator 126 based on these tokens, and so forth.
At step 5 of FIG. 4, which may be performed at any point in time after annotation of at least some of the digital images in searchable images repository 172 using the techniques described herein, one or more of the digital images and/or their accompanying annotations may be provided as training data. As shown in step 6 of fig. 4, the digital image itself may be used as an input across one or more machine learning models 450 (such as one or more convolutional neural networks) to train those models. The one or more machine learning models 450 may generate an output at step 7. The output may be provided to a training engine 452, which may be implemented using any combination of hardware or software. At step 9, the training engine 452 may be configured to compare the output of step 7 with the label provided at step 8. The marker may be an annotation generated using the techniques described herein. In various embodiments, the difference (or "error") between the output and the label may be used to train the machine learning model (e.g., using various training techniques such as gradient descent, back propagation, etc.).
Once one or more machine learning models 450 are trained, they can be configured to generate an output indicative of the overall context of the digital image. In particular, non-labeled digital images may be applied as input to one or more trained machine learning models 450 to generate output. The output may indicate a context or other topic or theme of the image. In summary, one or more machine learning models 450 are trained using annotations generated from user-provided natural language input, including natural language input that does not necessarily target a particular digital image when the input is provided. As a result, the trained machine learning model or models 450 may be configured to "comprehensively" classify digital images based on their semantic, topic, and/or topic similarities to other digital images captured in similar contexts, rather than strictly based on the similarity of objects sharing the depiction.
Fig. 5 is a flow chart illustrating an example method 500 in accordance with embodiments disclosed herein. For convenience, the operations of the flow diagram are described with reference to a system that executes the flow diagram. The system may include various components of various computer systems, such as one or more components of the automated assistant 120 and/or the image annotation engine 128. Further, while the operations of method 500 are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted, or added.
At block 502, the system may obtain natural language input provided by a user via one or more input components of a computing device. In various implementations, the natural language input may be spoken by a user to an automated assistant executing at least in part at a computing device. At block 504, the system may analyze the natural language input to determine an event associated with the user, determine one or more tokens describing the natural language input of the event, and determine a date associated with the event.
At block 506, the system may identify a plurality of digital images captured on that date. As previously mentioned, this date need not occur simultaneously with providing the natural language input. In some cases, the natural language input may identify a past date, which may immediately trigger the techniques described herein. Additionally or alternatively, the natural language input may identify a future date, in which case the techniques described herein may be triggered at that date, the end of the day of that date, the second day, and so forth.
At block 508, the system may analyze the plurality of digital images in order to determine which digital images are related to the event determined at block 504. This can be done in various ways. For example, at block 510, the system may examine a record (e.g., 168) of user-controlled computing interactions associated with the identified plurality of digital images. As previously described, various types of user-controlled computing interactions may be considered, such as a measure of time, a measure of sharing, a measure of image manipulation, and so forth, of digital images displayed through one or more graphical user interfaces. At block 512, based on the examination, the system may classify a subset of the plurality of digital images captured on the date as being related to the event. Other digital images of the plurality of digital images captured on the date may not be classified as related to the event.
Additionally or alternatively, at block 514 (which block 514 may or may not be performed in parallel with blocks 510-512), the system may perform an image recognition process on the plurality of images captured at the date to identify one or more objects or entities associated with the event and depicted in the subset of the plurality of digital images. Various types of object/entity recognition may be employed, including but not limited to one or more convolutional neural networks, appearance-based methods, feature-based methods, interpretation trees, pattern matching, scale invariant feature transforms ("SIFTs"), accelerated robust features ("SURFs"), principal component analysis, linear discriminant analysis, and the like, as previously described. Also, as previously described, in some implementations, the image recognition process may be biased towards identifying one or more tokens related to the event. At block 516, similar to block 512, the system may classify the subset of digital images as being related to the event. Additionally or alternatively, in some implementations, the system may identify overlaps between the subset of digital images classified in block 512 and the subset of images classified in block 516. The digital images identified in the two sets may be classified as being related to an event. Additionally or alternatively, the confidence measures associated with each digital image may be accumulated based on two paths (510-512 and 514-516).
At block 518, whether one or both of the operational paths 510 and 514 and 516 are performed, the system may store, for example, in the searchable images repository 172, data indicative of one or more of the tokens describing the event in association with the subset of the plurality of images classified as relevant to the event at blocks 512 and/or 516. Thereafter, searchable images store 172 may be indexed and/or searchable by the tokens.
Although not depicted in fig. 5, various downstream operations may be performed once the digital image is annotated using the techniques described herein. For example, in some implementations, the system can formulate natural language subtitles for one or more of the subset of digital images based on the one or more tokens describing the event. This may be performed, for example, by the natural language generator 126 or another similar component in a periodic manner, etc., upon request by a user (e.g., when creating a digital scrapbook). Additionally or alternatively, in some implementations, the annotation stored in association with the classified digital image may itself be a full subtitle, but this is not required.
As another example, and as depicted in fig. 5, the system may apply one or more annotated digital images as input across a machine learning classifier (or machine learning model) to generate an output, compare the output to the one or more tokens describing the event to generate an error, and train the machine learning classifier based on the error, wherein the training configures the machine learning classifier to classify subsequent digital images as being related to the one or more tokens describing the event.
FIG. 6 is a block diagram of an example computing device 610, which example computing device 610 may optionally be used to perform one or more aspects of the techniques described herein. In some implementations, one or more of the client computing device, the image annotation engine 128, the automated assistant 120, and/or other components can include one or more components of the example computing device 610.
User interface input devices 622 may include: a keyboard; a pointing device (such as a mouse, trackball, touch pad, or graphics tablet); a scanner; a touch screen incorporated into the display; audio input devices (such as voice recognition systems, microphones); and/or other types of input devices. In general, use of the term "input device" is intended to include all possible types of devices and ways to input information into computing device 610 or onto a communication network.
User interface output devices 620 may include a display subsystem, a printer, a fax machine, or a non-visual display (such as an audio output device). The display subsystem may include: cathode Ray Tubes (CRTs); flat panel devices such as Liquid Crystal Displays (LCDs); a projection device; or some other mechanism for creating a visible image. The display subsystem may also provide non-visual displays, for example, via an audio output device. In general, use of the term "output device" is intended to include all possible types of devices and ways to output information from computing device 610 to a user or to another machine or computing device.
These software modules are typically executed by the processor 614 alone or in combination with other processors. Memory 625 used in storage subsystem 624 may include a number of memories including a main Random Access Memory (RAM)630 for storing instructions and data during program execution and a Read Only Memory (ROM)632 in which fixed instructions are stored. File storage subsystem 626 may provide persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive along with associated removable media, a CD-ROM drive, an optical disk drive, or removable media cartridges. Modules implementing the functionality of certain embodiments may be stored by file storage subsystem 626 in storage subsystem 624, or in other machines accessible by processor 614.
Although several embodiments have been described and illustrated herein, one or more of a variety of other means and/or structures for performing the function and/or obtaining the result and/or the advantages described herein may be utilized and each such variation and/or modification is considered to be within the scope of the embodiments described herein. More generally, all parameters, dimensions, materials, and configurations described herein are meant to be exemplary, and the actual parameters, dimensions, materials, and/or configurations will depend upon the specific application for which the present teachings are used. Those skilled in the art will recognize, or be able to ascertain using no more than routine experimentation, many equivalents to the specific embodiments described herein. It is, therefore, to be understood that the foregoing embodiments are presented by way of example only and that, within the scope of the appended claims and equivalents thereto, the embodiments may be practiced otherwise than as specifically described and claimed. Embodiments of the present disclosure are directed to each individual feature, system, article, material, kit, and/or method described herein. In addition, the scope of the present disclosure includes any combination of two or more such features, systems, articles, materials, kits, and/or methods, if such features, systems, articles, materials, kits, and/or methods are not mutually inconsistent.
Claims (20)
1. A method implemented using one or more processors, comprising:
obtaining, via one or more input components of a computing device, natural language input provided by a user, wherein the natural language input is spoken by the user to an automated assistant executing at least in part on the computing device;
analyzing the natural language input to determine an event associated with the user, determine one or more tokens of the natural language input describing the event, and determine a date associated with the event;
identifying a plurality of digital images captured on the date;
examining a record of user-controlled computing interactions associated with the identified plurality of digital images;
classifying a subset of the plurality of digital images captured on the date as being related to the event based on the examining, wherein other digital images of the plurality of digital images captured on the date are not classified as being related to the event; and
storing data indicative of one or more of the tokens describing the event in association with a subset of the plurality of digital images classified as related to the event in a searchable database, wherein the digital images in the searchable database are searchable based on the one or more tokens stored in association with the digital images.
2. The method of claim 1, wherein the obtaining comprises:
receiving, by the automatic assistant via the one or more input components of the computing device, an audio recording of an utterance spoken by the user to the automatic assistant; and
performing speech-to-text processing on the audio recording to generate the natural language input.
3. The method of claim 1 or 2, wherein the examining comprises determining an amount of time to display each of the plurality of digital images via one or more graphical user interfaces.
4. The method of claim 3, wherein the amount of time to display a given digital image of the plurality of digital images comprises a cumulative amount of time to display the given digital image across the one or more graphical user interfaces.
5. The method of claim 4, wherein the one or more graphical user interfaces include a plurality of graphical user interfaces rendered on a plurality of different displays, and the cumulative amount of time to display the given image includes a cumulative amount of time to display the given image across the plurality of graphical user interfaces.
6. The method of any of the preceding claims, wherein the examining comprises determining a metric of image manipulation applied to each of the plurality of digital images via one or more digital image manipulation applications.
7. The method of any preceding claim, wherein the examining comprises determining a shared metric associated with each digital image of the plurality of digital images.
8. The method of claim 7, wherein the metric of sharing associated with a given digital image of the plurality of digital images comprises a count of sharing the given digital image to a plurality of other users by a user capturing the given digital image.
9. The method of claim 7 or 8, wherein the metric of sharing associated with a given digital image of the plurality of digital images comprises a count of sharing the given digital image across a plurality of other users.
10. The method of any one of the preceding claims, further comprising formulating a natural language caption for each of the subset of digital images based on the one or more tokens describing the event.
11. The method of any of the preceding claims, further comprising:
applying one or more of the subset of digital images as input to a machine learning classifier to generate an output;
comparing the output to the one or more tokens describing the event to generate an error; and
training the machine learning classifier based on the error, wherein the training configures the machine learning classifier to classify subsequent digital images as being related to the one or more tokens describing the event.
12. The method of any of the preceding claims, further comprising:
performing an image recognition process on the identified plurality of images captured at the date to identify one or more objects or entities depicted in the subset of the plurality of digital images associated with the event, wherein the performing comprises biasing the image recognition process towards identifying one or more of the tokens related to the event;
wherein the classification is further based on the identified one or more objects or entities.
13. A method implemented using one or more processors, comprising:
obtaining, via one or more input components of a computing device, natural language input provided by a user;
analyzing the natural language input to determine an event associated with the user, determine one or more tokens describing the event, and determine a date associated with the event;
identifying a plurality of digital images captured on the date;
performing an image recognition process on the identified plurality of images captured at the date to identify one or more objects or entities depicted in the subset of the plurality of digital images associated with the event, wherein the performing comprises biasing the image recognition process towards identifying one or more of the tokens related to the event;
classifying the subset of the plurality of digital images as related to the event, wherein other digital images of the plurality of digital images captured on the date that do not describe the one or more objects or entities are not classified as related to the event; and
storing data indicative of one or more of the tokens describing the event in association with a subset of the plurality of digital images classified as related to the event in a searchable database.
14. The method of claim 13, wherein the obtaining comprises:
receiving, by an automatic assistant executing at least in part on the computing device, via the one or more input components of the computing device, an audio recording of an utterance spoken by the user to the automatic assistant; and
performing speech-to-text processing on the audio recording to generate the natural language input.
15. The method of claim 13 or 14, further comprising formulating a natural language caption for each digital image of the subset of digital images based on the one or more tokens describing the event.
16. At least one non-transitory computer-readable medium comprising instructions that, in response to execution by one or more processors, cause the one or more processors to:
obtaining, via one or more input components of a computing device, natural language input provided by a user;
analyzing the natural language input to determine an event associated with the user, determine one or more tokens of the natural language input describing the event, and determine a date associated with the event;
identifying a plurality of digital images captured on the date;
examining a record of user-controlled computing interactions associated with the identified plurality of digital images;
classifying a subset of the plurality of digital images captured on the date as being related to the event based on the examining, wherein other digital images of the plurality of digital images captured on the date are not classified as being related to the event; and
storing data indicative of one or more of the tokens describing the event in association with a subset of the plurality of digital images classified as related to the event in a searchable database, wherein the digital images in the searchable database are searchable based on the one or more tokens stored in association with the digital images.
17. The at least one non-transitory computer-readable medium of claim 16, wherein the natural language input is spoken by the user to an automated assistant executing at least in part on the computing device.
18. The at least one non-transitory computer-readable medium of claim 17, wherein the obtaining comprises:
receiving, by the automatic assistant via the one or more input components of the computing device, an audio recording of an utterance spoken by the user to the automatic assistant; and
performing speech-to-text processing on the audio recording to generate the natural language input.
19. The at least one non-transitory computer-readable medium of any one of claims 16 to 18, wherein the examining comprises determining an amount of time to display each of the plurality of digital images via one or more graphical user interfaces.
20. The at least one non-transitory computer-readable medium of any one of claims 16 to 19, wherein the amount of time to display a given digital image of the plurality of digital images includes a cumulative amount of time to display the given digital image across the one or more graphical user interfaces.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201862742866P | 2018-10-08 | 2018-10-08 | |
US62/742,866 | 2018-10-08 | ||
PCT/US2019/025099 WO2020076362A1 (en) | 2018-10-08 | 2019-04-01 | Digital image classification and annotation |
Publications (1)
Publication Number | Publication Date |
---|---|
CN112955911A true CN112955911A (en) | 2021-06-11 |
Family
ID=66476813
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201980039726.7A Pending CN112955911A (en) | 2018-10-08 | 2019-04-01 | Digital image classification and annotation |
Country Status (4)
Country | Link |
---|---|
US (2) | US11567991B2 (en) |
EP (1) | EP3662417A1 (en) |
CN (1) | CN112955911A (en) |
WO (1) | WO2020076362A1 (en) |
Families Citing this family (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
EP3662417A1 (en) | 2018-10-08 | 2020-06-10 | Google LLC. | Digital image classification and annotation |
CN110162644B (en) * | 2018-10-10 | 2022-12-20 | 腾讯科技（深圳）有限公司 | Image set establishing method, device and storage medium |
US11163826B2 (en) * | 2020-03-01 | 2021-11-02 | Daniel Joseph Qualiano | Method and system for generating elements of recorded information in response to a secondary user's natural language input |
CN113688265B (en) * | 2020-05-19 | 2023-12-29 | 杭州海康威视数字技术股份有限公司 | Picture duplicate checking method, device and computer readable storage medium |
US11928146B2 (en) * | 2020-09-17 | 2024-03-12 | Memorythium Corporation | Perpetual system for capturing, curating, preserving, storing, positioning, reliving, and sharing memories |
CN116806343A (en) | 2020-10-01 | 2023-09-26 | 克劳德斯玛特有限公司 | Probability graph network |
US20220230180A1 (en) * | 2021-01-21 | 2022-07-21 | Dell Products L.P. | Image-Based Search and Prediction System for Physical Defect Investigations |
KR20220106499A (en) * | 2021-01-22 | 2022-07-29 | 삼성전자주식회사 | Method for providing personalized media content and electronic device using the same |
EP4184347A1 (en) * | 2021-11-23 | 2023-05-24 | Popescu, Calin-Laurentiu | Computer-implemented method and system of collecting, organizing and storing data relating to the memories of a human |
US11922550B1 (en) | 2023-03-30 | 2024-03-05 | OpenAI Opco, LLC | Systems and methods for hierarchical text-conditional image generation |
Citations (15)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20030059112A1 (en) * | 2001-06-01 | 2003-03-27 | Eastman Kodak Company | Method and system for segmenting and identifying events in images using spoken annotations |
WO2006122164A2 (en) * | 2005-05-09 | 2006-11-16 | Riya, Inc. | System and method for enabling the use of captured images through recognition |
US20090232417A1 (en) * | 2008-03-14 | 2009-09-17 | Sony Ericsson Mobile Communications Ab | Method and Apparatus of Annotating Digital Images with Data |
US20130346068A1 (en) * | 2012-06-25 | 2013-12-26 | Apple Inc. | Voice-Based Image Tagging and Searching |
CN103561652A (en) * | 2011-06-01 | 2014-02-05 | 皇家飞利浦有限公司 | Method and system for assisting patients |
CN104657423A (en) * | 2015-01-16 | 2015-05-27 | 北京合辉信息技术有限公司 | Method and device thereof for sharing contents of applications |
US20150379005A1 (en) * | 2014-06-26 | 2015-12-31 | Amazon Technologies, Inc. | Identifying data from keyword searches of color palettes |
US20160110355A1 (en) * | 2014-10-17 | 2016-04-21 | Verizon Patent And Licensing Inc. | Automated image organization techniques |
CN106255968A (en) * | 2014-05-16 | 2016-12-21 | 微软技术许可有限责任公司 | Natural language picture search |
US20170154314A1 (en) * | 2015-11-30 | 2017-06-01 | FAMA Technologies, Inc. | System for searching and correlating online activity with individual classification factors |
CN107209762A (en) * | 2014-05-15 | 2017-09-26 | 思腾科技（巴巴多斯）有限公司 | Visual interactive formula is searched for |
CN107766399A (en) * | 2016-08-23 | 2018-03-06 | 百度（美国）有限责任公司 | For the method and system and machine readable media for image is matched with content item |
CN108108102A (en) * | 2018-01-02 | 2018-06-01 | 联想(北京)有限公司 | Image recommendation method and electronic equipment |
CN108369799A (en) * | 2015-09-29 | 2018-08-03 | 安泊音乐有限公司 | Using machine, system and the process of the automatic music synthesis and generation of the music experience descriptor based on linguistics and/or based on graphic icons |
US20180267994A1 (en) * | 2017-03-20 | 2018-09-20 | Google Inc. | Contextually disambiguating queries |
Family Cites Families (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9009163B2 (en) * | 2009-12-08 | 2015-04-14 | Intellectual Ventures Fund 83 Llc | Lazy evaluation of semantic indexing |
US20140046765A1 (en) * | 2011-05-05 | 2014-02-13 | Celtra Inc. | Rich media mobile advertising development platform |
US9710763B2 (en) * | 2012-06-27 | 2017-07-18 | Sheldon O. Linker | Method and system for robot understanding, knowledge, conversation, volition, planning, and actuation |
US9405771B2 (en) * | 2013-03-14 | 2016-08-02 | Microsoft Technology Licensing, Llc | Associating metadata with images in a personal image collection |
US20150006545A1 (en) * | 2013-06-27 | 2015-01-01 | Kodak Alaris Inc. | System for ranking and selecting events in media collections |
KR102067057B1 (en) * | 2013-07-24 | 2020-01-16 | 엘지전자 주식회사 | A digital device and method of controlling thereof |
EP3662417A1 (en) | 2018-10-08 | 2020-06-10 | Google LLC. | Digital image classification and annotation |
-
2019
- 2019-04-01 EP EP19723234.1A patent/EP3662417A1/en not_active Withdrawn
- 2019-04-01 US US16/619,006 patent/US11567991B2/en active Active
- 2019-04-01 CN CN201980039726.7A patent/CN112955911A/en active Pending
- 2019-04-01 WO PCT/US2019/025099 patent/WO2020076362A1/en unknown
-
2023
- 2023-01-05 US US18/093,541 patent/US11836183B2/en active Active
Patent Citations (15)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20030059112A1 (en) * | 2001-06-01 | 2003-03-27 | Eastman Kodak Company | Method and system for segmenting and identifying events in images using spoken annotations |
WO2006122164A2 (en) * | 2005-05-09 | 2006-11-16 | Riya, Inc. | System and method for enabling the use of captured images through recognition |
US20090232417A1 (en) * | 2008-03-14 | 2009-09-17 | Sony Ericsson Mobile Communications Ab | Method and Apparatus of Annotating Digital Images with Data |
CN103561652A (en) * | 2011-06-01 | 2014-02-05 | 皇家飞利浦有限公司 | Method and system for assisting patients |
US20130346068A1 (en) * | 2012-06-25 | 2013-12-26 | Apple Inc. | Voice-Based Image Tagging and Searching |
CN107209762A (en) * | 2014-05-15 | 2017-09-26 | 思腾科技（巴巴多斯）有限公司 | Visual interactive formula is searched for |
CN106255968A (en) * | 2014-05-16 | 2016-12-21 | 微软技术许可有限责任公司 | Natural language picture search |
US20150379005A1 (en) * | 2014-06-26 | 2015-12-31 | Amazon Technologies, Inc. | Identifying data from keyword searches of color palettes |
US20160110355A1 (en) * | 2014-10-17 | 2016-04-21 | Verizon Patent And Licensing Inc. | Automated image organization techniques |
CN104657423A (en) * | 2015-01-16 | 2015-05-27 | 北京合辉信息技术有限公司 | Method and device thereof for sharing contents of applications |
CN108369799A (en) * | 2015-09-29 | 2018-08-03 | 安泊音乐有限公司 | Using machine, system and the process of the automatic music synthesis and generation of the music experience descriptor based on linguistics and/or based on graphic icons |
US20170154314A1 (en) * | 2015-11-30 | 2017-06-01 | FAMA Technologies, Inc. | System for searching and correlating online activity with individual classification factors |
CN107766399A (en) * | 2016-08-23 | 2018-03-06 | 百度（美国）有限责任公司 | For the method and system and machine readable media for image is matched with content item |
US20180267994A1 (en) * | 2017-03-20 | 2018-09-20 | Google Inc. | Contextually disambiguating queries |
CN108108102A (en) * | 2018-01-02 | 2018-06-01 | 联想(北京)有限公司 | Image recommendation method and electronic equipment |
Also Published As
Publication number | Publication date |
---|---|
EP3662417A1 (en) | 2020-06-10 |
WO2020076362A1 (en) | 2020-04-16 |
US11567991B2 (en) | 2023-01-31 |
US20210073272A1 (en) | 2021-03-11 |
US20230146144A1 (en) | 2023-05-11 |
US11836183B2 (en) | 2023-12-05 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11836183B2 (en) | Digital image classification and annotation | |
JP7064018B2 (en) | Automated assistant dealing with multiple age groups and / or vocabulary levels | |
US11971936B2 (en) | Analyzing web pages to facilitate automatic navigation | |
US11734375B2 (en) | Automatic navigation of interactive web documents | |
US11775254B2 (en) | Analyzing graphical user interfaces to facilitate automatic interaction | |
KR20200006107A (en) | Obtain response information from multiple corpus | |
US11392213B2 (en) | Selective detection of visual cues for automated assistants | |
US20230274090A1 (en) | Recommending action(s) based on entity or entity type | |
JP7486540B2 (en) | Automated assistants that address multiple age groups and/or vocabulary levels | |
US20220405478A1 (en) | Using Video Clips as Dictionary Usage Examples | |
Kebe | Speaker-Based Variability in Robotic Spoken Language Grounding |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |