JP2018517967A - Dynamic switching and merging of head, gesture, and touch input in virtual reality - Google Patents
Dynamic switching and merging of head, gesture, and touch input in virtual reality Download PDFInfo
- Publication number
- JP2018517967A JP2018517967A JP2017555336A JP2017555336A JP2018517967A JP 2018517967 A JP2018517967 A JP 2018517967A JP 2017555336 A JP2017555336 A JP 2017555336A JP 2017555336 A JP2017555336 A JP 2017555336A JP 2018517967 A JP2018517967 A JP 2018517967A
- Authority
- JP
- Japan
- Prior art keywords
- input
- virtual
- virtual object
- touch
- electronic device
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 230000004044 response Effects 0.000 claims abstract description 51
- 238000000034 method Methods 0.000 claims description 67
- 230000008859 change Effects 0.000 claims description 9
- 230000007704 transition Effects 0.000 claims description 5
- 230000008569 process Effects 0.000 claims description 4
- 230000000977 initiatory effect Effects 0.000 claims description 2
- 230000009467 reduction Effects 0.000 claims 1
- 230000003993 interaction Effects 0.000 abstract description 49
- 230000015654 memory Effects 0.000 description 40
- 238000004891 communication Methods 0.000 description 27
- 210000003128 head Anatomy 0.000 description 21
- 230000009471 action Effects 0.000 description 16
- 238000004590 computer program Methods 0.000 description 16
- 230000003287 optical effect Effects 0.000 description 10
- 238000012545 processing Methods 0.000 description 6
- 230000006870 function Effects 0.000 description 5
- 238000012913 prioritisation Methods 0.000 description 5
- 230000003044 adaptive effect Effects 0.000 description 4
- 238000013459 approach Methods 0.000 description 4
- 230000000875 corresponding effect Effects 0.000 description 4
- 230000000007 visual effect Effects 0.000 description 4
- 230000000694 effects Effects 0.000 description 3
- 230000004886 head movement Effects 0.000 description 3
- 239000004973 liquid crystal related substance Substances 0.000 description 3
- 230000007246 mechanism Effects 0.000 description 3
- 238000012986 modification Methods 0.000 description 3
- 230000004048 modification Effects 0.000 description 3
- 230000001413 cellular effect Effects 0.000 description 2
- 238000010586 diagram Methods 0.000 description 2
- 239000004065 semiconductor Substances 0.000 description 2
- 230000001953 sensory effect Effects 0.000 description 2
- 238000012800 visualization Methods 0.000 description 2
- 239000000969 carrier Substances 0.000 description 1
- 238000006243 chemical reaction Methods 0.000 description 1
- 230000001276 controlling effect Effects 0.000 description 1
- 230000008878 coupling Effects 0.000 description 1
- 238000010168 coupling process Methods 0.000 description 1
- 238000005859 coupling reaction Methods 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 239000011521 glass Substances 0.000 description 1
- 238000005259 measurement Methods 0.000 description 1
- 230000006855 networking Effects 0.000 description 1
- 230000010399 physical interaction Effects 0.000 description 1
- 210000001747 pupil Anatomy 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 238000006467 substitution reaction Methods 0.000 description 1
- 230000001502 supplementing effect Effects 0.000 description 1
- 239000010409 thin film Substances 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000001960 triggered effect Effects 0.000 description 1
- 230000001755 vocal effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/011—Arrangements for interaction with the human body, e.g. for user immersion in virtual reality
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/017—Head mounted
- G02B27/0172—Head mounted characterised by optical features
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/011—Arrangements for interaction with the human body, e.g. for user immersion in virtual reality
- G06F3/012—Head tracking input arrangements
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/011—Arrangements for interaction with the human body, e.g. for user immersion in virtual reality
- G06F3/013—Eye tracking input arrangements
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/017—Gesture based interaction, e.g. based on a set of recognized hand gestures
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/03—Arrangements for converting the position or the displacement of a member into a coded form
- G06F3/041—Digitisers, e.g. for touch screens or touch pads, characterised by the transducing means
- G06F3/0416—Control or interface arrangements specially adapted for digitisers
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0481—Interaction techniques based on graphical user interfaces [GUI] based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance
- G06F3/04815—Interaction with a metaphor-based environment or interaction object displayed as three-dimensional, e.g. changing the user viewpoint with respect to the environment or object
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0484—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range
- G06F3/04842—Selection of displayed objects or displayed text elements
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0484—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range
- G06F3/04847—Interaction techniques to control parameter settings, e.g. interaction with sliders or dials
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0487—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser
- G06F3/0488—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures
- G06F3/04883—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures for inputting data by handwriting, e.g. gesture or text
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T19/00—Manipulating 3D models or images for computer graphics
- G06T19/006—Mixed reality
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L67/00—Network arrangements or protocols for supporting network services or applications
- H04L67/2866—Architectures; Arrangements
- H04L67/30—Profiles
- H04L67/306—User profiles
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/017—Head mounted
- G02B2027/0178—Eyeglass type
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2203/00—Indexing scheme relating to G06F3/00 - G06F3/048
- G06F2203/038—Indexing scheme relating to G06F3/038
- G06F2203/0381—Multimodal input, i.e. interface arrangements enabling the user to issue commands by simultaneous use of input devices of different nature, e.g. voice plus gesture on digitizer
Abstract
仮想現実における、頭部、ジェスチャおよびタッチ入力の動的な切り替えおよびマージのためのシステムでは、第１の入力が多数の異なる入力モードのうちの１つを実施することに応答してユーザが仮想オブジェクトを選択することができる。一旦選択されると、第１の入力によって第１のオブジェクトにフォーカスが確立された状態で、第１のオブジェクトは、第２の入力が異なる入力モードのうちの別のものを実施すること応答して仮想世界で操作されてもよい。第３の入力に応答して、別のオブジェクトが選択され、例えば、第３の入力の優先順位値が第１のオブジェクトにフォーカスを確立した第１の入力の優先順位値よりも高い場合には、第３の入力に応答して、フォーカスが第１のオブジェクトから第２のオブジェクトにシフトされてもよい。第３の入力の優先順位値が第１のオブジェクトにフォーカスを確立した第１の入力の優先順位値よりも小さい場合、第１のオブジェクトにフォーカスが残っていてもよい。特定のトリガ入力に応答して、仮想オブジェクトの表示は、仮想オブジェクトとの相互作用およびそれらの操作の特定のモードに適応するために、ファーフィールドディスプレイとニアフィールドディスプレイとの間でシフトされてもよい。In a system for dynamic switching and merging of head, gesture and touch inputs in virtual reality, the user is virtual in response to the first input performing one of a number of different input modes. You can select an object. Once selected, with the first input focused on the first object, the first object responds that the second input implements another of the different input modes. May be operated in a virtual world. In response to the third input, another object is selected, for example, if the priority value of the third input is higher than the priority value of the first input that has established focus on the first object. In response to the third input, the focus may be shifted from the first object to the second object. If the priority value of the third input is smaller than the priority value of the first input that has established focus on the first object, the focus may remain on the first object. In response to a specific trigger input, the display of the virtual object may be shifted between the far field display and the near field display to adapt to the specific mode of interaction with the virtual object and their operation. Good.
Description
関連出願への相互参照
本出願は、２０１５年８月２６日に出願された「仮想現実における、頭部、ジェスチャ、およびタッチ入力の動的な切り替えならびにマージ（DYNAMIC SWITCHING AND MERGING OF HEAD, GESTURE AND TOUCH INPUT IN VIRTUAL REALITY）」と題される米国非仮特許出願第１４／８３６，３１１号の優先権を主張し、その継続出願であり、その開示全体をここに引用により援用する。
Cross-reference to related applications This application was filed on August 26, 2015, “DYNAMIC SWITCHING AND MERGING OF HEAD, GESTURE AND Claims non-provisional US patent application Ser. No. 14 / 836,311 entitled “TOUCH INPUT IN VIRTUAL REALITY” and is a continuation of that application, the entire disclosure of which is hereby incorporated by reference.
分野
この文書は、一般的に、没入型仮想現実システムにおける入力の処理に関する。
Field This document generally relates to processing input in immersive virtual reality systems.
背景
仮想現実（ＶＲ）システムは、３次元（３Ｄ）の没入型環境を生成することができる。ユーザは、例えば、表示装置を見るときに通して見るディスプレイ、眼鏡またはゴーグルを含むヘルメットまたは他の頭部装着型装置、センサとフィットされる手袋、センサを含む外部ハンドヘルドデバイス、および他のそのような電子デバイスなどの様々な電子デバイスとの相互作用を通じてこの仮想３Ｄ没入環境を体験することができる。一旦仮想３Ｄ環境に没入すると、ユーザの３Ｄ環境との相互作用は、仮想３Ｄ環境と相互に作用し、それを個人的なものにし、制御するよう、目での注視、頭部での注視、物理的な移動および／または電子デバイスの操作など、様々な形態をとり得る。
Background Virtual reality (VR) systems can generate a three-dimensional (3D) immersive environment. The user may, for example, display through when viewing a display device, a helmet or other head-mounted device that includes glasses or goggles, a glove that is fitted with a sensor, an external handheld device that includes the sensor, and other such This virtual 3D immersive environment can be experienced through interaction with various electronic devices such as electronic devices. Once immersed in the virtual 3D environment, the user's interaction with the 3D environment interacts with the virtual 3D environment, making it personal and controlled, gaze with the eyes, gaze with the head, Various forms may be taken, such as physical movement and / or manipulation of electronic devices.
概要
１つの局面において、方法は、複数の入力モードに応答して各々が選択可能かつ操作可能である複数の仮想オブジェクトを含む没入型仮想体験を開始することと、複数の入力モードのうちの第１の入力モードを実施する第１の入力を受信することと、第１の入力に応答して複数の仮想オブジェクトのうちの第１の仮想オブジェクトを選択することと、複数の入力モードのうちの第２の入力モードを実施する第２の入力を受信することと、第２の入力モードの優先順位値と第１の入力モードの優先順位値とを比較することと、比較に基づいて、第２の入力に応答して、第１の仮想オブジェクトの選択を解放し、選択を複数の仮想オブジェクトのうちの第２の仮想オブジェクトにシフトするか、または第１の仮想オブジェクトの選択を維持することとを含んでもよい。
Overview In one aspect, a method initiates an immersive virtual experience that includes a plurality of virtual objects that are each selectable and operable in response to a plurality of input modes; Receiving a first input that implements one input mode; selecting a first virtual object of the plurality of virtual objects in response to the first input; and Receiving a second input implementing the second input mode, comparing the priority value of the second input mode with the priority value of the first input mode, and based on the comparison, In response to the input of 2, the selection of the first virtual object is released, and the selection is shifted to the second virtual object of the plurality of virtual objects, or the selection of the first virtual object is maintained. May include.
別の局面において、方法は、没入型仮想体験を開始することと、複数の仮想オブジェクトのファーフィールドディスプレイを生成することと、第１の入力を受信することと、第１の入力に応答して複数の仮想オブジェクトを含むファーフィールドディスプレイを選択することと、第２の入力を受信することと、第２の入力に応答して複数の仮想オブジェクトのニアフィールド表示のために複数の仮想オブジェクトの外観をモーフィングすることと、モーフィングされた複数の仮想オブジェクトを含むニアフィールドディスプレイを生成することと、第３の入力を受信することと、第３の入力に応答して、モーフィングされた複数の仮想オブジェクトのうちの１つを選択することとを含んでもよい。 In another aspect, a method initiates an immersive virtual experience, generates a far field display of a plurality of virtual objects, receives a first input, and in response to the first input Selecting a far field display including a plurality of virtual objects, receiving a second input, and appearance of the plurality of virtual objects for near field display of the plurality of virtual objects in response to the second input , Generating a near-field display including a plurality of morphed virtual objects, receiving a third input, and responsive to the third input, a plurality of morphed virtual objects Selecting one of them.
１つ以上の実現例の詳細は、添付の図面および以下の記載において述べられる。他の特徴は記載および図面、ならびに特許請求の範囲から明らかになる。 The details of one or more implementations are set forth in the accompanying drawings and the description below. Other features will be apparent from the description and drawings, and from the claims.
詳細な記載
例えばＨＭＤ（頭部装着型ディスプレイ）デバイスを装着した、３Ｄ仮想環境に没入したユーザは、３Ｄ仮想環境を探索し、様々な異なるタイプの入力を介して３Ｄ仮想環境と相互に作用することができる。これらの入力は、例えば、手／腕のジェスチャ、頭部の動き、および／または頭部の方向による注視、目の方向による注視など、および／またはＨＭＤの操作、および／またはＨＭＤとは別個の電子デバイスの操作、例えば、電子デバイスのタッチセンシティブ面上に加えられるタッチ、電子デバイスの移動、電子デバイスによって放射される光線またはビームなどを含む物理的相互作用を含み得る。例えば、いくつかの実現例では、ＨＭＤは、例えば、スマートフォンまたは他のそのようなハンドヘルド電子デバイスのようなハンドヘルド電子デバイスと対にされてもよい。ＨＭＤと対にされるハンドヘルド電子デバイスのユーザ操作により、ユーザは、ＨＭＤによって生成される３Ｄ仮想没入体験と相互に作用してもよい。いくつかのシステムは、ユーザの目での注視を検出および追跡し、ならびに／またはユーザの頭部の方向による注視を検出して、検出された目のおよび／または頭部での注視を３Ｄ仮想没入体験における対応する相互作用に変換するように構成されてもよい。これらの複数の異なる入力モード、特に複数の同時入力に直面した際に、複数の入力を実行するための設定された優先順位のシーケンスまたは階層は、検出された入力および設定された優先順位のシーケンスまたは階層に基づいて仮想世界で取られるべきオブジェクトおよび／またはアクションの正確な選択を容易にすることができる。
Detailed Description For example, a user who is immersed in a 3D virtual environment wearing an HMD (head-mounted display) device searches the 3D virtual environment and interacts with the 3D virtual environment via various different types of inputs. be able to. These inputs may be separate from, for example, hand / arm gestures, head movements, and / or gaze by head direction, gaze by eye direction, and / or HMD manipulation and / or HMD Manipulation of the electronic device may include physical interactions including, for example, touch applied on the touch-sensitive surface of the electronic device, movement of the electronic device, light rays or beams emitted by the electronic device, and the like. For example, in some implementations, the HMD may be paired with a handheld electronic device such as, for example, a smartphone or other such handheld electronic device. The user operation of the handheld electronic device paired with the HMD may allow the user to interact with the 3D virtual immersive experience generated by the HMD. Some systems detect and track gaze in the user's eyes and / or detect gaze according to the direction of the user's head to 3D virtualize gaze in the detected eyes and / or head It may be configured to translate into a corresponding interaction in an immersive experience. When faced with these multiple different input modes, especially multiple simultaneous inputs, the set priority sequence or hierarchy for performing the multiple inputs is the detected input and the set priority sequence Or it can facilitate the precise selection of objects and / or actions to be taken in the virtual world based on the hierarchy.
図１に示す例示的実現例において、ＨＭＤ１００を装着しているユーザは、ＨＭＤ１００によって生成される没入型仮想体験における相互作用のために、ＨＭＤ１００と対になりそれと通信することができる例えばスマートフォンなどの携帯用ハンドヘルド電子デバイス１０２または他の携帯用ハンドヘルド電子デバイスを手１４２に保持している。図１に示される例では、ユーザは、ハンドヘルド電子デバイス１０２を右手で保持している。しかしながら、ユーザは、ハンドヘルド電子デバイス１０２を左手で、または左手と右手の両方で保持し、それでいて、ＨＭＤ１００によって生成される没入型仮想体験と相互に作用してもよい。
In the exemplary implementation shown in FIG. 1, a user wearing the
上述したようなハンドヘルド電子デバイス１０２は、例えば、有線接続、または例えばＷｉＦｉもしくはＢｌｕｅｔｏｏｔｈ（登録商標）接続などの無線接続を介して、ＨＭＤ１００と動作可能に結合されるか、または対になることができる。ハンドヘルド電子デバイス１０２とＨＭＤ１００とのこのペアリングまたは動作可能な結合は、ハンドヘルド電子デバイス１０２とＨＭＤ１００との間の通信、およびハンドヘルド電子デバイス１０２とＨＭＤ１００との間のデータの交換を提供することができ、ハンドヘルド電子デバイス１０２は、ＨＭＤ１００によって生成される仮想没入型体験において相互に作用するために、ＨＭＤ１００との通信においてコントローラとして機能することができる。すなわち、ハンドヘルド電子デバイス１０２の操作、例えばハンドヘルド電子デバイス１０２のタッチ面で受信される入力、またはハンドヘルド電子デバイス１０２の移動、またはユーザによって向き付けられるハンドヘルド電子デバイス１０２によって放射される光線もしくはビームなどは、ＨＭＤ１００によって生成される仮想没入型体験において、対応する相互作用または移動に変換され得る。
The handheld
様々な異なるタイプのユーザ入力が、ＨＭＤ１００によって生成される仮想没入型体験においてこのタイプの相互作用に変換されてもよい。例えば、ハンドヘルド電子デバイス１０２のタッチ面上のタッチ入力は、仮想世界内のオブジェクトの選択に対応してもよく、ハンドヘルド電子デバイス１０２のタッチ面上でのタッチおよびドラッグ、またはタッチおよび移動ハンドヘルド電子デバイス１０２自体のタッチおよび次いでのその移動は、オブジェクトの選択および移動に対応してもよい。いくつかの実施形態では、ハンドヘルド電子デバイス１０２によって放射されたビームまたは光線は、仮想世界内の特定のオブジェクトを指し示してオブジェクトを選択し、ビームのその後の移動により、選択されたオブジェクトを移動させてもよい。いくつかの実施形態では、ユーザの目での注視を監視し追跡することができ、仮想世界でオブジェクトに向けられたユーザの注視は、そのオブジェクトの選択に対応してもよい。いくつかの実施形態では、ハンドヘルド電子デバイス１０２のタッチ面上の次のドラッグおよび／またはハンドヘルド電子デバイス１０２自体の移動、および／または頭部の動き、および／または手／腕の動きは、例えば、目での注視によって選択されたオブジェクトの移動などの、後のアクションを検出してもよい。いくつかの実施形態では、オブジェクトの初期選択を含むユーザ入力は、頭部の動き、手／腕のジェスチャ、ハンドヘルド電子機器１０２の移動などから検出されてもよい。これらの複数の入力モードが可能にされる実現例では、複数の入力を例えば同時に受信される場合に実行するための設定された優先順位のシーケンスまたは階層は、検出された入力および設定された優先順位のシーケンスまたは階層に基づいて仮想世界で取られるべきオブジェクトおよび／またはアクションの正確な選択を容易にすることができる。
Various different types of user input may be translated into this type of interaction in a virtual immersive experience generated by the
図２Ａおよび図２Ｂは、没入型仮想体験を生成するよう、例えば図１においてユーザによって装着されるＨＭＤ１００のような例示的なＨＭＤの斜視図である。ＨＭＤ１００は、フレーム１２０に、例えば回転可能に結合され、および／または取り外し可能に取付け可能であるハウジング１１０を含むことができる。例えば、ヘッドホンに取り付けられるスピーカを含む音声出力装置１３０も、フレーム１２０に結合されてもよい。図２Ｂにおいて、ハウジング１１０の前面１１０ａは、ハウジング１１０のベース部分１１０ｂから離れるように回転され、ハウジング１１０に収容されたコンポーネントのいくつかが見える。ディスプレイ１４０は、ハウジング１１０の前面１１０ａに取り付けられてもよい。前面１１０ａがハウジング１１０のベース部分１１０ｂに対して閉位置にあるときに、レンズ１５０は、ハウジング１１０においてユーザの目とディスプレイ１４０との間に取り付けられてもよい。レンズ１５０の位置は、相対的広い視野および相対的短い焦点距離を与えるよう、ユーザの目のそれぞれの光軸と整列されてもよい。いくつかの実施形態では、ＨＭＤ１００は、ＨＭＤ１００の動作を容易にするよう、様々なセンサを含む検知システム１６０と、プロセッサ１９０および様々な制御システムデバイスを含む制御システム１７０とを含んでもよい。
2A and 2B are perspective views of an exemplary HMD, such as the
いくつかの実施形態では、ＨＭＤ１００は、ＨＭＤ１００の外部の現実世界環境の静止画像および動画像をキャプチャするようカメラ１８０を含んでもよい。カメラ１８０によってキャプチャされた画像は、ディスプレイ１４０上においてパススルーモードでユーザに表示され、ＨＭＤ１００を取り外したり、またはそうでなければＨＭＤ１００の構成を変更してハウジング１１０をユーザの視線から移動させることなく、ユーザが一時的に仮想世界を離れて現実世界に戻ることを可能にする。
In some embodiments, the
いくつかの実施形態では、ＨＭＤ１００は、ユーザの目での注視を検出および追跡するためのデバイス、または注視追跡装置１６５を含んでもよい。注視追跡装置１６５は、例えば、ユーザの目の画像、およびいくつかの実施形態では、例えば、瞳孔のようなユーザの目の特定の部分の画像をキャプチャするよう、画像センサ１６５Ａを含んでもよい。いくつかの実施形態では、注視追跡装置１６５は、ユーザの注視の動きを検出して追跡するように位置決めされた複数の画像センサ１６５Ａを含んでもよい。画像センサ１６５Ａによってキャプチャされた画像から、ユーザの注視の方向が検出されてもよい。検出された注視は、本質的に、ユーザの目からオブジェクトへの、例えばＨＭＤ１００によって生成される３Ｄ仮想没入型体験におけるオブジェクトへの視線を規定することができる。いくつかの実施形態では、ＨＭＤ１００は、検出された注視が仮想没入体験における対応する相互作用に変換されるべきユーザ入力として処理されるように構成されてもよい。
In some embodiments, the
仮想現実環境における、頭部、ジェスチャおよびタッチ入力の動的な切り替えおよびマージのためのシステムのブロック図を図３に示す。システムは、第２のユーザ電子デバイス３０２と通信する第１のユーザ電子デバイス３００を含んでもよい。第１のユーザ電子デバイス３００は、例えば、図１および図２Ａおよび図２Ｂに関して上述したＨＭＤであってもよく、没入型の仮想没入型体験を生成し、第２のユーザ電子デバイス３０２は、例えば、ＨＭＤによって生成される仮想没入型体験とのユーザ相互作用を容易にするために、第１のユーザ電子デバイスと通信している、図１に関して上述したスマートフォンであってもよい。
A block diagram of a system for dynamic switching and merging of head, gesture and touch inputs in a virtual reality environment is shown in FIG. The system may include a first user
第１の電子デバイス３００は、図２Ａおよび図２Ｂに示す検知システム１６０および制御システム１７０とそれぞれ同様であってもよい検知システム３６０および制御システム３７０を含んでもよい。図３の例示的実施例において、検知システム３６０は、例えば、光センサ、音声センサ、画像センサ、距離／近接度センサ、および／もしくは他のセンサならびに／または異なるセンサの組み合わせを含む多くの異なるタイプのセンサを含んでもよい。いくつかの実施形態では、光センサ、画像センサおよび音声センサは、例えば図２Ａおよび図２Ｂに示すＨＭＤ１００のカメラ１８０のようなカメラなどの１つのコンポーネントに含まれてもよい。いくつかの実施形態では、検知システム３６０は、例えば、図２Ｂに示す注視追跡装置１６５と同様のデバイスのような、ユーザの目での注視を検出して追跡するように位置決めされた画像センサを含んでもよい。制御システム３７０は、例えば、電力／休止制御装置、音声および映像制御装置、光学制御装置、遷移制御装置、および／もしくは他のこのような装置ならびに／または異なる装置の組み合わせを含む数多くの異なるタイプの装置を含んでもよい。いくつかの実施形態では、特定の実現例に応じて、検知システム３６０および／または制御システム３７０は、より多くの、またはより少ない装置を含んでもよい。検知システム３６０および／または制御システム３７０に含まれる要素は、例えば、図２Ａおよび図２Ｂに示すＨＭＤ１００以外のＨＭＤ内に異なる物理的配置（例えば、異なる物理的位置）を有することができる。
The first
第１の電子デバイス３００はまた、検知システム３６０および制御システム３７０と通信するプロセッサ３９０と、例えば制御システム３７０のモジュールによってアクセス可能なメモリ３８０と、第１の電子デバイス３００と例えば第１の電子デバイス４００と対になっている第２の電子デバイス３０２のような別の外部装置との間の通信を提供する通信モジュール３５０とを含んでもよい。
The first
第２の電子デバイス３０２は、第２の電子デバイス３０２と、例えば第２の電子デバイス３０２と対になっている第１の電子デバイス３００などの別の外部装置との間の通信を提供する通信モジュール３０６を含んでもよい。第１の電子デバイス３００と第２の電子デバイス３０２との間で、例えば電子データの交換を提供することに加えて、いくつかの実施形態では、通信モジュール３０６は、さらに、上述のように光線またはビームを放射するように構成されてもよい。第２の電子デバイス３０２は、例えば、カメラおよびマイクロフォンに含まれるような画像センサおよび音声センサ、慣性測定ユニット、ハンドヘルド電子デバイスまたはスマートフォンのタッチセンシティブ面に含まれるようなタッチセンサ、ならびに他のそのようなセンサおよび／またはセンサの異なる組み合わせを含む検知システム３０４を含んでもよい。プロセッサ３０９は、第２の電子デバイス３０２の検知システム３０４およびコントローラ３０５と通信することができ、コントローラ３０５はメモリ３０８にアクセスし、第２の電子デバイス３０２の全体的な動作を制御する。
The second
上述したように、様々なタイプのユーザ入力が、仮想没入型体験において対応するアクションのトリガとなることができる。ユーザ相互作用のための様々な異なる機構が、図４Ａ〜図１０Ｄに示される。図４Ａ〜図４Ｇおよび図５Ａ〜図５Ｄに示す注視およびタッチ（ＧＴ）相互作用の概念、図６Ａ〜図６Ｇ、図７Ａ〜図７Ｃおよび図８Ａ〜図８Ｄに示されているポイントおよびタッチ（ＰＴ）相互作用の概念、図９Ａ〜図９Ｅおよび図１０Ａ〜図１０Ｄに示されているリーチおよびタッチ（ＲＴ）相互作用の概念は、すべて、目による注視、頭部による注視、タッチ（例えば、ハンドヘルド電子デバイス１０２のタッチ面上のタッチおよび／またはハンドヘルド電子デバイス１０２の移動の追跡）、およびハンドヘルド電子デバイス１０２によって放射される光線またはビームを含む、４つの異なる入力機構の何らかの組合わせを利用する。これらの４つの異なる入力機構が与えられると、どのアクションがシステムのアクティブなフォーカスを保持するかを明確にするシステムによって、仮想世界における所望の相互作用への入力の正確な変換を容易にし得る。
As described above, various types of user input can trigger corresponding actions in a virtual immersive experience. Various different mechanisms for user interaction are shown in FIGS. 4A-10D. The gaze and touch (GT) interaction concept shown in FIGS. 4A-4G and 5A-5D, the points and touches shown in FIGS. 6A-6G, 7A-7C, and 8A-8D ( The concept of PT) interaction, the reach and touch (RT) interaction concepts shown in FIGS. 9A-9E and 10A-10D are all the same as eye gaze, head gaze, touch (e.g., Utilizing any combination of four different input mechanisms, including touching on the touch surface of the handheld
ＧＴタイプの相互作用は、仮想世界においてオブジェクトを見て、選択および操作するよう、ハンドヘルドコントローラを介して入力される入力に関連して、目での注視方向または頭部の方向を含む光線ベースの相互作用を含んでもよい。以下では、「注視（ｇａｚｅ）」という用語は、ＨＭＤ１００の前方を向いている外側面と本質的に直交する、目の方向による注視および／または頭部の方向による注視を指すために使用される。
GT-type interactions are ray-based including eye gaze direction or head direction in relation to input entered via the handheld controller to view, select and manipulate objects in the virtual world. Interactions may be included. In the following, the term “gaze” is used to refer to a gaze by eye direction and / or a gaze by head direction, essentially orthogonal to the anterior surface facing forward of
図４Ａ〜図４Ｇに示す例示的実現例は、例えば仮想現実において図２Ａおよび図２Ｂに示すＨＭＤ１００のディスプレイ１４０に表示されるオブジェクトＸ、ＹおよびＺのスクロール可能なリストに向けられる、ホバー状態、選択状態およびスクロール状態のＧＴ相互作用を示す。この例示的な実現例では、単に例示および説明を容易にするために、ＨＭＤ１００の完全な構造は示されていない。しかしながら、この例示的な実現例では、例えば、注視は、上で図２Ａおよび図２Ｂに関して論じた１つ以上の画像センサ１６５Ａを含む注視追跡装置１６５によって検出され、追跡されてもよい。いくつかの実施形態では、注視、特に頭部の方向による注視は、例えば、ＨＭＤ１００に含まれる加速度計および／またはジャイロスコープのような、ＨＭＤ１００の検知システム１６０によって検出されてもよい。
The exemplary implementation shown in FIGS. 4A-4G is a hover state, for example directed to a scrollable list of objects X, Y and Z displayed on the
図４Ａから図４Ｂから進む際に、ユーザは、オブジェクトＸ、Ｙ、Ｚのスクロール可能リストに向かって注視をシフトし、ユーザの注視は、オブジェクトＸ、Ｙ、Ｚのスクロール可能リストを囲むヒット領域Ｈと交差し、図４Ｂに示すようにオブジェクトＸに留まるに至る。ユーザの注視がオブジェクトＸに留まると、ユーザは、次いで、図４Ｃに示すように、例えば、スマートフォン１０２などの、ＨＭＤ１００と対にされたハンドヘルド電子デバイス１０２のタッチセンシティブ面上のタッチ入力によってオブジェクトＸを選択してもよい。一旦オブジェクトＸが図４Ｃで選択されると、ユーザの注視がオブジェクトＸから離れても、フォーカスはオブジェクトＸ上に留まってもよい。すなわち、一旦オブジェクトＸがハンドヘルド電子デバイス１０２のタッチセンシティブ面上のタッチによって選択されると、図４Ｄに示すようにユーザの注視がわずかにシフトしても、タッチがハンドヘルド電子デバイス１０２のタッチセンシティブ面上に保持されるかまたは維持される限り、選択されたオブジェクトＸは選択されイネーブルのままである。この時点で、スクロール可能リストのヒット領域Ｈ内を注視しながら、ハンドヘルド電子デバイスのタッチセンシティブ面上のドラッグ入力で、図４Ｅに示すようにリストをスクロールさせてもよい。リストは、たとえユーザの注視がもはやスクロール可能なリストに向けられていなくても、図４Ｇに示すように、ユーザがハンドヘルド電子デバイス１０２のタッチセンシティブ面上のタッチ／ドラッグ入力を解放するまで、図４Ｆに示すように、スクロールし続けることができる。
As the user proceeds from FIG. 4A to FIG. 4B, the user shifts his gaze toward the scrollable list of objects X, Y and Z, and the user's gaze is a hit area surrounding the scrollable list of objects X, Y and Z. Crosses H and stays on object X as shown in FIG. 4B. When the user's gaze stays at object X, the user then touches object X by touch input on the touch-sensitive surface of handheld
例えば、図２Ａおよび図２Ｂに示すＨＭＤ１００のディスプレイ１４０に表示されるスクロール可能なリストにおけるスクロール可能および／または操作可能なオブジェクトＸ、ＹおよびＺのリストを通るスクロールするアクションが、図５Ａ〜図５Ｄに示されている。図５Ａ〜図５Ｄに示される実施例では、複数のスクロール可能なオブジェクトおよび／または操作可能なオブジェクトは、ユーザの視野に関係なく、および／またはオブジェクトに対するユーザの向きもしくは角度にかかわらず、一様スクロールスケールを維持してもよい。すなわち、オブジェクトは、図５Ａに示すように、ユーザの視野内において相対的に遠くの位置にあってもよく、またはオブジェクトは、図５Ｂに示すように、ユーザの視野内において相対的近くの位置にあってもよい。例えば仮想世界においてＨＭＤ１００のディスプレイ１４０に表示されるオブジェクトリスト、例えばオブジェクトＸ、Ｙ、Ｚのリストの移動は、ユーザの視野内のオブジェクトの位置にかかわらず、一様なスクロールスケールを維持し、ハンドヘルド電子デバイス１０２のタッチセンシティブ面上のドラッグ入力と整合し得る。同様に、オブジェクトは、図５Ｃに示すように、ユーザの光軸または視線と整列されてもよく、またはオブジェクトは、ユーザの光軸または視線から、ある角度で、またはオフセットされて位置決めされてもよい。例えば仮想世界においてＨＭＤ１００のディスプレイ１４０に表示されるオブジェクトリスト、例えばオブジェクトＸ、Ｙ、Ｚのリストの移動は、ユーザの光軸または視線に対するオブジェクトの角度にかかわらず、一様なスクロールスケールを維持し、ハンドヘルド電子デバイス１０２のタッチセンシティブ面上のドラッグ入力と整合し得る。
For example, the scrolling action through the list of scrollable and / or manipulable objects X, Y and Z in the scrollable list displayed on the
ポイントおよびタッチ（ＰＴ）タイプの入力を図６Ａ〜図６Ｆ、図７Ａ〜図７Ｃおよび図８Ａ〜図８Ｄに示す。図６Ａ〜図６Ｇに示されるＰＴ入力は、ＰＴタイプの入力を使用して、オブジェクトまたはリストを選択すること、およびリストにおいてオブジェクトをスクロールすることを含むことができ、図７Ａ〜図７ＣにおけるＰＴ入力は、オブジェクトの選択をキャンセルするためのスナップビームまたは光線の使用を含むことができ、図８Ａ〜図８Ｄに示されるＰＴタイプ入力は、動的プッシュプルビームを使用してオブジェクトを選択して移動させることを含むことができる。これらのＰＴタイプ入力は、例えば、上述したスマートフォン１０２などのハンドヘルド電子デバイス１０２、またはＨＭＤ１００と対になって通信する他のハンドヘルド電子デバイス、および／または仮想現実システムの様々な他のコンポーネントとの光線ベースの相互作用を含んでもよい。
Point and touch (PT) type inputs are shown in FIGS. 6A-6F, 7A-7C, and 8A-8D. The PT input shown in FIGS. 6A-6G can include selecting an object or list using the PT type input and scrolling the object in the list, PT in FIGS. 7A-7C. Inputs can include the use of snap beams or rays to cancel object selection, and the PT type inputs shown in FIGS. 8A-8D select objects using dynamic push-pull beams. Can be included. These PT-type inputs are, for example, rays of handheld
図６Ａから図６Ｂに進む際、ユーザは、ハンドヘルド電子デバイス１０２によって放射される光線のフォーカスをシフトし、ハンドヘルド電子デバイスによって放射される光線が、オブジェクトＸ、ＹおよびＺのスクロール可能なリストを囲むヒット領域Ｈと交差し、光線は、図６Ｂに示すように、オブジェクトＸに留まりフォーカスを合わせる。一旦オブジェクトＸがハンドヘルド電子デバイス１０２によって放射される光線のフォーカスとなると、ユーザは、次いで、図６Ｃに示すように、例えば、上述したようなスマートフォン１０２のような、ＨＭＤ１００と対になるハンドヘルド電子デバイス１０２のタッチセンシティブ面上のタッチ入力によって、オブジェクトＸを選択してもよい。一旦オブジェクトＸが図６Ｃにおいて選択されると、ハンドヘルド電子デバイス１０２によって放射される光線がもはやリストまたはオブジェクトＸに向けられなくても、フォーカスはオブジェクトＸ上に残ってもよい。すなわち、図６Ｃに示すように、一旦オブジェクトＸがハンドヘルド電子デバイス１０２のタッチセンシティブ面上のタッチによって選択されると、図６Ｄに示すようにハンドヘルド電子デバイス１０２によって放射される光線の方向がシフトしても、タッチがハンドヘルド電子デバイス１０２のタッチセンシティブ面上に保持されるかまたは維持される限り、選択されたオブジェクトＸは選択されイネーブルのままである。この時点で、ハンドヘルド電子デバイス１０２のタッチセンシティブ面上のドラッグ入力で、図６Ｅに示すようにリストをスクロールさせてもよい。いくつかの実施形態では、光線の視覚化は、ドラッグ入力に応答してリストがスクロールすると、外観を変えるか、または消えてもよい。リストは、図６Ｇに示すように、ユーザがハンドヘルド電子デバイス１０２のタッチセンシティブ面上のタッチ／ドラッグ入力を解放するまで、図６Ｆに示すように、スクロールし続けることができる。
In proceeding from FIG. 6A to FIG. 6B, the user shifts the focus of light rays emitted by the handheld
選択されたオブジェクトＸから光線をスナップして、オブジェクトＸの選択を解放またはキャンセルするためのＰＴジェスチャを図７Ａ〜図７Ｃに示す。ハンドヘルド電子デバイス１０２によって放射される光線が図７Ａに示すようにオブジェクトＸにフォーカスされる状態で、ユーザは、図７Ｂに示すように、光線のフォーカスをオブジェクトＡから充分に離れるよう、およびスクロール可能リストのヒット領域Ｄの充分外側にシフトさせるような方法でハンドヘルド電子デバイス１０２を移動させるかまたは向きを変更してもよく、次いで、図７Ｃに示すように、ハンドヘルド電子デバイス１０２のタッチセンシティブ面上のタッチを解放してもよい。このようにして、ユーザがフォーカスを確立し、ハンドヘルド電子デバイスのタッチセンシティブ面にタッチしてオブジェクトＸにフォーカスを確立すると、ハンドヘルド電子デバイス１０２をオブジェクトＸから十分離して角度付けることで、必ずしもタッチを解放する必要なく、フォーカスを解放し、および／または選択をキャンセルしてもよい。
A PT gesture for snapping a ray from a selected object X to release or cancel the selection of object X is shown in FIGS. 7A-7C. With the light beam emitted by the handheld
図８Ａ〜図８Ｄは、図２Ａおよび２Ｂに示すＨＭＤ１００のディスプレイ１４０上に表示されるオブジェクト、例えばオブジェクトＸ、Ｙ、Ｚおよび／または仮想現実システムの様々な他のコンポーネントの動的プッシュプルロッド操作を可能にするＰＴモードを示す。図８Ａに示すように、ハンドヘルド電子デバイス１０２によって生成される光線またはポインタビームは、オブジェクトＸに延在してもよく、またはそれを指し示してもよい。ポインタビームがオブジェクトＸにフォーカスされた状態で、ハンドヘルド電子デバイス１０２のタッチセンシティブ面を押すことまたはそれにタッチすることで、オブジェクトＸをつかんでいる間に（オブジェクトＸに取り付けられる）一定の長さでポインタビームを確立してもよい。ポインタビームによってつかまれたオブジェクトＸは、次いで、ハンドヘルド電子デバイス１０２が移動されユーザのタッチがハンドヘルド電子デバイス１０２のタッチセンシティブ面上に維持される際に、移動されてもよく、それは図８Ｃに示すように、あたかもプッシュプルロッドの遠位端に取り付けられているかのようである。オブジェクトＸは、ユーザのタッチがハンドヘルド電子デバイス１０２のタッチセンシティブ面から解放されると、解放されてもよい。
8A-8D illustrate dynamic push-pull rod manipulation of objects displayed on the
リーチおよびタッチ（ＲＴ）タイプの入力を図９Ａ〜図９Ｅおよび図１０Ａ〜図１０Ｅに示す。これらのＲＴタイプの入力は、例えば、ユーザの片手もしくは両手および／またはハンドヘルド電子デバイスを使用してオブジェクトを選択および操作する近接度ベースの相互作用を含むことができる。特に、図９Ａ〜図９Ｅに示すＲＴタイプ入力は、オブジェクトまたはリストを選択し、リストにおいてオブジェクトをスクロールし、図１０Ａ〜図１０Ｄに示すＲＴタイプ入力は、オブジェクトを選択すること、および選択されたオブジェクトを移動することを示す。 Reach and touch (RT) type inputs are shown in FIGS. 9A-9E and 10A-10E. These RT-type inputs can include, for example, proximity-based interactions that select and manipulate objects using a user's one or both hands and / or handheld electronic devices. In particular, the RT type input shown in FIGS. 9A-9E selects an object or list and scrolls the object in the list, and the RT type input shown in FIGS. 10A-10D selects an object and is selected. Indicates to move the object.
図９Ａ〜図９Ｅは、オブジェクトＸ、ＹおよびＺのネスト化されたスクロール可能リストのホバー、選択、およびスクロールを含むＲＴ相互作用を示す。図９Ａから図９Ｂに進む際に、ユーザが手および／またはハンドヘルド電子デバイス１０２をスクロール可能なオブジェクトＸ、ＹおよびＺのリストに向かって動かすと、図９Ｂに示すように、例えばユーザの手またはハンドヘルド電子デバイスの端部に最も近接したオブジェクト（この場合、オブジェクトＸ）が強調表示される。いくつかの実施形態では、電子ハンドヘルド装置１０２によって放射されるビームの長さを視覚的に減少させて、手またはハンドヘルド電子デバイス１０２へのオブジェクトの潜在的な接続の付加的な視覚的指示を提供することができる。次いで、図９Ｃに示すように、把持動作などのようなユーザの手による予め設定されたジェスチャ、またはハンドヘルド電子デバイス１０２のタッチセンシティブ面上のタッチで、最も近接した強調表示されるオブジェクト（この例ではオブジェクトＸ）を選択してもよい。次いで、図９Ｅに示すように、把持および／またはハンドヘルド電子デバイスのタッチセンシティブ面上のタッチが解放されるまで、図９Ｄに示すように、スクロール可能なリスト上のプル動作が、リストのトリガとなって、オブジェクトＸ、ＹおよびＺをスクロールしてもよい。
9A-9E illustrate RT interactions involving hovering, selection, and scrolling of nested scrollable lists of objects X, Y, and Z. In proceeding from FIG. 9A to FIG. 9B, when the user moves the hand and / or handheld
図１０Ａ〜図１０Ｄは、移動可能なオブジェクトを選択し移動するためのＲＴタイプの入力を示す。図１０Ａから図１０Ｂに進む際に、ユーザが手および／またはハンドヘルド電子デバイス１０２をスクロール可能なオブジェクトＸ、ＹおよびＺのリストに向かって動かすと、図１０Ｂに示すように、例えばユーザの手またはハンドヘルド電子デバイス１０２の端部に最も近接したオブジェクト（この場合、オブジェクトＸ）が強調表示される。次いで、例えば、把持動作などのようなユーザの手による予め設定されたジェスチャ、またはハンドヘルド電子デバイス１０２のタッチセンシティブ面上のタッチで、最も近接した強調表示されるオブジェクト（この例ではオブジェクトＸ）を選択してもよく、一旦把持されると、オブジェクトは、図１０Ｃに示すように、ユーザが手および／またはハンドヘルド電子デバイス１０２を動かすとともに、移動されてもよい。いくつかの実施形態では、選択されたオブジェクトは、ユーザの手またはハンドヘルド電子デバイス１０２の端部に対して固定された平行移動オフセットに留まってもよく、オブジェクトが選択されたままである間ユーザの手および／またはハンドヘルド電子デバイス１０２が移動するにつれてオブジェクトは移動する。ユーザの把持の解放、および／またはハンドヘルド電子デバイス１０２のタッチセンシティブ面上でのユーザのタッチの解放で、オブジェクトは、図１０Ｄに示すように、解放された位置に配置変えされてもよい。いくつかの実施形態では、解放されたオブジェクトは、確立されたグリッドまたは近くのオブジェクトと整列されてもよい。
10A-10D illustrate RT type input for selecting and moving a movable object. In proceeding from FIG. 10A to FIG. 10B, when the user moves the hand and / or handheld
仮想世界において第１のオブジェクトにフォーカスを確立し、第１のオブジェクトからフォーカスを解放し、第２のオブジェクトにフォーカスをシフトするとき、様々な異なるタイプの入力がどのように優先されるかを区別するために、様々な異なるアプローチが取られてもよい。すなわち、上述したように、いくつかの実施形態では、図４Ａ〜図５Ｄに示す注視の例のように、例えば目での注視または頭部での注視によって選択されるべき特定のオブジェクトに初期フォーカスを確立してもよい。いくつかの実施形態では、初期フォーカスは、例えば、図６Ａ〜図８Ｄに示されるポイント例でのように、ハンドヘルド電子デバイス１０２によって放射され、フォーカスのために選択されるべきオブジェクトに向けられる光線またはビームの点によって選択されるべき特定のオブジェクト上に確立してもよい。いくつかの実施形態では、初期フォーカスは、例えば、図９Ａ〜図１０Ｄに示されるリーチ例でのように、ハンドヘルド電子デバイス１０２および／またはユーザの手でのようなリーチアクションによって選択されるべき特定のオブジェクト上に確立されてもよい。フォーカスを解放してシフトするよう特定のアクションがとられるまで、フォーカスは解放されず、別のオブジェクトにシフトされない。いくつかの実施形態では、上述した入力モードのうちの１つを実施するアクションに基づいて、フォーカスが別のオブジェクトにシフトしたと判定されると、フォーカスを解放してもよい。単に議論および説明を容易にするために、オブジェクトにフォーカスを確立するための４つの入力モードの例（頭部での注視、目での注視、ポイント、およびリーチ）を、単に４つの異なる入力モードＡ、Ｂ、ＣおよびＤと呼ぶことにする。
Distinguish how different types of input are prioritized when establishing focus on the first object in the virtual world, releasing focus from the first object, and shifting focus to the second object Various different approaches may be taken to do so. That is, as described above, in some embodiments, the initial focus is on a particular object to be selected, for example, by eye gaze or head gaze, as in the gaze example shown in FIGS. 4A-5D. May be established. In some embodiments, the initial focus is a light ray emitted by the handheld
現在フォーカスを有するオブジェクトを判定するために複数の入力を区別する際に、フォーカスがつい最近第１の入力モード（例えば、頭部での注視、目での注視、ハンドヘルド電子デバイス１０２によって放射される光線、ハンドヘルド電子デバイスのタッチセンシティブ面上のタッチ、リーチ／タッチジェスチャなど）を用いて達成された場合、その第１の入力モードが他の入力モードよりも優先されてもよい。複数の入力モードの中で優先順位を確立するこのモードは、最新性と呼ばれることがある。例えば、ハンドヘルド電子デバイス１０２がフォーカスを確立しオブジェクトを選択するために使用され、ユーザがディスプレイ上に提示される他のオブジェクトを見るために注視をシフトし続ける場合、ハンドヘルド電子デバイス１０２を介する入力は、ハンドヘルド電子デバイス１０２がつい最近使用されていたため、ユーザの注視よりも優先される。しかしながら、ハンドヘルド電子デバイス１０２が何らかの方法で係合解除される（離れて置かれ、オフにされ、範囲外に持ち出されるなど）場合、ハンドヘルド電子デバイス１０２の最近の使用はもはやないので、ユーザの注視（または別の入力モード）は、次いで、フォーカスを別のオブジェクトにシフトしてもよい。
In distinguishing multiple inputs to determine the object that currently has focus, the focus is on the first input mode (e.g., gaze at the head, gaze at the eye, emitted by the handheld electronic device 102). The first input mode may take precedence over other input modes if achieved using light, touch on touch-sensitive surface of handheld electronic device, reach / touch gesture, etc. This mode of establishing priority among multiple input modes is sometimes referred to as up-to-date. For example, if the handheld
いくつかの実施形態では、様々な異なるタイプの入力Ａ、Ｂ、ＣおよびＤを区別し、それらの間で優先順位を確立するために、固定された優先順位アプローチを用いてもよい。例えば、いくつかの実施形態では、入力モードＡ、Ｂ、ＣおよびＤの各々は優先順位値を割当てられてもよい。例えば、４つの入力モードＡ、Ｂ、Ｃ、Ｄに優先順位値、例えば１，２，３または４が割り当てられ、優先順位値４が割り当てられる入力モードは最高優先順位入力モードであり、優先順位値１が割り当てられる入力モードは最低優先順位入力モードであり、いずれの２つの入力モードにも同じ優先順位値は割り当てられていなくてもよい。いくつかの実施形態では、異なる入力モードＡ、Ｂ、ＣおよびＤに対するこれらの優先順位値は工場設定として確立されてもよい。いくつかの実施形態では、異なる入力モードＡ、Ｂ、ＣおよびＤのこれらの優先順位値は、例えば特定のＨＭＤ１００およびハンドヘルド電子デバイス１０２のペアリングに対してユーザにより設定されてもよく、または特定のユーザに特定のユーザプロファイルで設定されてもよい。いくつかの実施形態では、これらの優先順位値は、例えば特定の仮想環境などのために、システムによって、またはユーザによって、変更および／または再設定されてもよい。
In some embodiments, a fixed priority approach may be used to distinguish various different types of inputs A, B, C and D and establish a priority among them. For example, in some embodiments, each of input modes A, B, C, and D may be assigned a priority value. For example, a priority value, for example, 1, 2, 3 or 4, is assigned to the four input modes A, B, C, D, and an input mode to which a priority value of 4 is assigned is the highest priority input mode. The input mode to which the
図１１に示すように、ブロック１１１０において、第１の入力を受信することに基づいて、ブロック１１２０において、第１のオブジェクト上にフォーカスをセットし維持してもよい。ブロック１１３０において、第２のオブジェクトにフォーカスされる第２の入力が受信されると、ブロック１１４０において、第２の入力の優先順位値が、第１のオブジェクトにフォーカスを確立した第１の入力の優先順位値より大きいかどうかが判定される。第２の入力の優先順位値が第１の入力の優先順位値よりも大きい場合、第２の入力に関連付けられる入力モードを実施することによって、ユーザは第２のオブジェクトにフォーカスをシフトしようとしていると判断され、フォーカスは第１のオブジェクトから解放され、第２のオブジェクトにシフトされる。第２の入力の優先順位値が第１の入力の優先順位値よりも小さい場合、第１のオブジェクトに対してフォーカスが維持される。この方法は、上述したような関連付けられる優先順位を各々が有するモードＡ、Ｂ、Ｃ、Ｄなどの複数の入力モードがユーザに利用可能である場合に、複数の入力のうちのどの入力がフォーカスを解放、シフトおよび確立することにおいて優先するかを判定するときに適用されてもよい。
As shown in FIG. 11, based on receiving a first input at
このようにして、第１のオブジェクトＸからフォーカスを解放して第２のオブジェクトＹに移動させることを、例えば図４Ｇおよび図６Ｇに示すようにハンドヘルド電子デバイス１０２のタッチ面上でのタッチ／ドラッグを解放すること、図７Ｃに示すようにスナップホールドを解放すること、図８Ｄに示されているようにプッシュプルロッドを解放すること、または図９Ｅおよび図１０Ｄに示されているように把持／リーチを解放すること、などの特定の解放アクションなしに行なってもよい。
In this way, releasing the focus from the first object X and moving it to the second object Y is performed by touch / drag on the touch surface of the handheld
複数の入力モードを互いに対して優先順位付ける１つの技術的効果は、依然としてフォーカスをシフトすることができながら、特定のオブジェクトからフォーカスを明示的に解放する必要がない態様で、複数の入力モードをユーザに提供できることである。それは、仮想現実相互作用に対する「さらなる次元」をある意味で可能にし、なぜならば、以前に選択されたオブジェクトを明示的に解放することのみによってではなく、より高い優先順位を有する入力モードを用いて異なるオブジェクトにフォーカスを変更することによっても、オブジェクトの選択の変更が可能になるからである。これにより、必要な相互作用数が削減される（積極的な解放が不要）と同時に、ユーザ相互作用の次元も増加する。しかしながら、これは優先順位付けによって達成される単に１つの技術的効果であり、他のものも存在する。一般に、入力モードの優先順位付けは、相互作用の次元を増大させ、それにより、仮想世界とのユーザ相互作用の柔軟性を増大させ、それによってユーザが以前には提供されなかった方法で仮想世界においてオブジェクトに影響を及ぼし、それを選択し、制御することを可能にする。これらの仮想現実オブジェクトは、「物理的エンティティ」に対応しているため、仮想現実で表示されているが、それのためのアプローチは、単なるデータの表現だけではなく、実際に物理的エンティティの制御およびそれらとの相互作用にも関連し、それに向けられる。 One technical effect of prioritizing multiple input modes with respect to each other is to allow multiple input modes in a manner that can still shift focus but does not require explicit release of focus from a particular object. It can be provided to the user. It allows in a sense a “further dimension” for virtual reality interaction, not only by explicitly releasing previously selected objects, but using input modes with higher priority This is because the selection of an object can be changed by changing the focus to a different object. This reduces the number of required interactions (no aggressive release is required) and at the same time increases the user interaction dimension. However, this is just one technical effect achieved by prioritization, others exist. In general, input mode prioritization increases the dimension of interaction, thereby increasing the flexibility of user interaction with the virtual world, thereby enabling the virtual world in a way that the user has not previously provided. Affects objects, allowing them to be selected and controlled. Since these virtual reality objects correspond to “physical entities”, they are displayed in virtual reality, but the approach for that is not just a representation of data, but actually a control of physical entities And related to and interaction with them.
いくつかの実施形態では、様々な異なる入力モードＡ、Ｂ、ＣおよびＤの間で優先順位を区別し確立するために、適応的優先順位アプローチを使用することができる。例えば、いくつかの実施形態では、実際の使用に基づいて入力モードＡ、Ｂ、ＣおよびＤの各々について優先順位値を決定してもよく、オブジェクトにフォーカスを確立するために最も頻繁に実施される入力モードには、最も高い優先順位値が割り当てられ、オブジェクトにフォーカスを確立するために最も頻繁に実施されない入力モードには、最も低い優先順位値が割り当てられる。いくつかの実施形態では、入力モードの各々に割り当てられる優先順位値は、使用に基づいて変更されてもよい。例えば、いくつかの実施形態では、優先順位値は、特定の期間中の使用、または特定の仮想体験もしくはゲームなどに基づいて、自動的または手動のいずれかで周期的に更新されてもよい。図１１に示される方法を、これらの適応的優先順位値を使用して、固定された優先順位値に関して上述したものと同様の方法でフォーカスを解放しシフトするために実現してもよい。 In some embodiments, an adaptive priority approach can be used to distinguish and establish priorities among a variety of different input modes A, B, C, and D. For example, in some embodiments, a priority value may be determined for each of input modes A, B, C, and D based on actual use, most often performed to establish focus on an object. The input mode that is assigned the highest priority value and the input mode that is not implemented most frequently to establish focus on the object is assigned the lowest priority value. In some embodiments, the priority value assigned to each of the input modes may be changed based on usage. For example, in some embodiments, the priority value may be periodically updated either automatically or manually based on usage during a specific time period, or on a specific virtual experience or game, etc. The method shown in FIG. 11 may be implemented to release and shift focus in a manner similar to that described above with respect to fixed priority values using these adaptive priority values.
いくつかの実施形態では、ターゲットサイズ、例えばフォーカスオブジェクトのサイズが、異なる入力モードＡ、Ｂ、ＣおよびＤの優先順位を決定してもよい。例えば、いくつかの実施形態では、仮想世界における相対的に小さいターゲットオブジェクトは、ターゲットオブジェクトに対してフォーカスを確立するために、頭部での注視または目での注視入力よりも、ポイント入力（図６Ｂのような）またはリーチ／タッチ入力（図９Ｂのような）に対して、より高い優先順位を割当ててもよい。いくつかの実施形態では、仮想世界における相対的により大きなターゲットオブジェクトは、ターゲットオブジェクトに対してフォーカスを確立するために、それがポイント入力またはリーチ／タッチ入力に割り当てるよりも、（図４Ｂでのような）注視入力（頭部での注視または目での注視）に対して、より高い優先順位を割当ててもよい。したがって、いくつかの実施形態では、特定の仮想没入型環境の異なるオブジェクトまたはオブジェクトのグループの各々は、オブジェクトのサイズなど、オブジェクト（またはオブジェクトのグループ）の特性に基づいて、異なる入力モードに対してそれ自身の優先順位のセットを割り当てることができる。図１１に示される方法を、これらの適応的優先順位値を使用して、固定された優先順位値に関して上述したものと同様の方法でフォーカスを解放しシフトするために実現してもよい。 In some embodiments, the target size, eg, the size of the focus object, may determine the priority of different input modes A, B, C, and D. For example, in some embodiments, a relatively small target object in the virtual world may have a point input (see FIG. 5) rather than a gaze at the head or an eye gaze input to establish focus on the target object. Higher priority may be assigned to reach / touch inputs (such as 6B) or reach / touch inputs (such as FIG. 9B). In some embodiments, a relatively larger target object in the virtual world may be assigned to a point input or reach / touch input (as in FIG. 4B) to establish focus on the target object. A higher priority may be assigned to gaze input (gaze at the head or gaze at the eyes). Thus, in some embodiments, each different object or group of objects in a particular virtual immersive environment is subject to different input modes based on the characteristics of the object (or group of objects), such as the size of the object. You can assign your own set of priorities. The method shown in FIG. 11 may be implemented to release and shift focus in a manner similar to that described above with respect to fixed priority values using these adaptive priority values.
いくつかの実施形態では、距離、例えばオブジェクトが仮想世界においてユーザから見える距離が、異なる入力モードＡ、Ｂ、ＣおよびＤの優先順位を決定してもよい。例えば、いくつかの実施形態では、仮想世界でユーザに相対的に近く見えるオブジェクトは、相対的に近いオブジェクトにフォーカスを確立するために、それがポイント入力または注視入力に割り当てるよりも、（図９Ｂのように）リーチ／タッチ入力に対してより高い優先順位を割り当ててもよい。いくつかの実施形態では、仮想世界においてユーザから相対的に遠くにあるように見えるオブジェクトは、その相対的に遠いオブジェクトに対してフォーカスを確立するために、それがリーチ／タッチ入力に割り当てるよりも、ポイント入力（図６Ｂにおけるように）または注視入力（図４Ｂにおけるように頭部での注視または目での注視）に対して、より高い優先順位を割当ててもよい。したがって、いくつかの実施形態では、特定の仮想没入型環境の異なるオブジェクトまたはオブジェクトのグループの各々は、ユーザからのオブジェクトの知覚される距離など、オブジェクト（またはオブジェクトのグループ）の特性に基づいて、異なる入力モードに対してそれ自身の優先順位のセットを割り当ててもよい。図１１に示される方法を、これらの適応的優先順位値を使用して、固定された優先順位値に関して上述したものと同様の方法でフォーカスを解放しシフトするために実現してもよい。 In some embodiments, the distance, eg, the distance at which an object is visible to the user in the virtual world, may determine the priority of different input modes A, B, C, and D. For example, in some embodiments, an object that appears relatively close to the user in the virtual world is assigned to a point input or gaze input in order to establish focus on the relatively close object (FIG. 9B). Higher priority may be assigned to reach / touch input. In some embodiments, an object that appears to be relatively far from the user in the virtual world is more than assigned to reach / touch input to establish focus on that relatively far object. Higher priority may be assigned to point inputs (as in FIG. 6B) or gaze inputs (gaze at the head or gaze at the eye as in FIG. 4B). Thus, in some embodiments, each different object or group of objects in a particular virtual immersive environment is based on the characteristics of the object (or group of objects), such as the perceived distance of the object from the user, It may assign its own set of priorities for different input modes. The method shown in FIG. 11 may be implemented to release and shift focus in a manner similar to that described above with respect to fixed priority values using these adaptive priority values.
いくつかの実施形態では、仮想世界においてユーザから相対的に遠くに見えるオブジェクトまたはオブジェクトのセットは、例えば、上述したような目での注視または頭部での注視によって選択されてもよい。仮想世界においてユーザから相対的に遠く離れたオブジェクトとのユーザ相互作用を容易にするために、ユーザは、例えば、ユーザの手がＨＭＤ１００によって追跡され、検出された手の動きがユーザの手および／またはハンドヘルド電子デバイス１０２で変換される、リーチジェスチャなどである、他の入力モードのうちの１つを使用して相互作用および／または操作のためにオブジェクトをより近くに引き寄せたいかもしれない。より長い距離の注視入力相互作用とより短い距離のタッチまたはリーチ相互作用との間のこの動的な切り替えは、様々な異なるユーザ入力、アクション、または信号がトリガとなり得る。例えば、いくつかの実施形態では、注視入力でオブジェクトにフォーカスが確立された後、オブジェクトは、例えば、リーチ／タッチ相互作用のために、ハンドヘルド電子デバイス１０２のタッチセンシティブ面上のタッチ入力によって、システムが動作する６自由度（６ＤＯＦ）空間内でのハンドヘルド電子デバイス１０２の移動によって、ＨＭＤ１００の制御コンポーネントとの相互作用などによって、より近くに引き寄せられてもよい。
In some embodiments, an object or set of objects that appears relatively far from the user in the virtual world may be selected, for example, by eye gaze or head gaze as described above. In order to facilitate user interaction with objects that are relatively far away from the user in the virtual world, the user may, for example, have the user's hand tracked by the
いくつかの実施形態では、注視入力でオブジェクトにフォーカスが確立された後、オブジェクトは、例えば、リーチ／タッチ相互作用ならびに／またはより細分化および／もしくは拡張された視覚化のために、オブジェクトを視覚的リーチおよび／またはタッチ範囲に引き込むようコマンドとしてＨＭＤ１００によって認識されるジェスチャによって、より近くに引き寄せられてもよい。いくつかの実施形態では、ＨＭＤ１００によって認識されるジェスチャは、例えば、ＨＭＤ１００の例えばカメラ１８０などのセンサによって検出され、ＨＭＤ１００のプロセッサ１９０および／もしくはコントローラ１７０によって処理される手ならびに／または腕のジェスチャであってもよい。議論および説明を簡単にするために、以下では、仮想世界においてユーザから相対的に遠くにあるように見えるフォーカスのオブジェクトに向かって伸ばす手／腕のジェスチャを含むユーザジェスチャを、フォーカスを確立するためのより長い距離の目の注視入力、およびフォーカスのオブジェクトに基づいて仮想世界で操作および／または相互に作用するためのより短い距離のリーチ／タッチ入力からのこの動的切り替えのためのトリガとして論ずる。しかしながら、他のジェスチャを含む他のトリガもまた、このタイプの動的切り替えを引き起こし得る。
In some embodiments, after focus is established on the object with gaze input, the object may view the object, eg, for reach / touch interaction and / or for more granular and / or expanded visualization. May be drawn closer by gestures recognized by the
このタイプの動的切り替えの一例が、図１２Ａ〜図１２Ｇに示されている。図１２Ａでは、ユーザインタフェースがユーザに提示される。図１２Ａに示す例示的なユーザインタフェースは、ファーフィールドディスプレイに提示されるオブジェクト２０Ａのリスト２０またはグループである。すなわち、この実現例では、仮想世界においてユーザから相対的に遠く離れた壁にあるかのように、ユーザインタフェースをユーザに提示することができる。いくつかの実施形態では、オブジェクトのリスト２０のファーフィールドディスプレイは、仮想世界においてユーザから、例えば、８フィートの距離、または８フィートを超えるように見えてもよい。いくつかの実施形態では、オブジェクトのリスト２０のファーフィールドディスプレイは、それらが、例えば、仮想世界においてユーザから８フィート未満の距離にあるように見えてもよい。ユーザの注視（頭部での注視または目での注視）がユーザインタフェース、特に、図１２Ｂに示すように、オブジェクト２０Ａのリスト２０と交差すると、ユーザのフォーカスは、上述したようにオブジェクト２０Ａのリスト２０上に確立されてもよい。オブジェクト２０Ａのリスト２０にフォーカスがセットされると、例えば、図１２Ｃに示すようなユーザの手／腕のジェスチャのようなトリガアクションが、より長い距離の注視入力モード（オブジェクトのファーフィールドディスプレイに向けられる）からより短い距離のリーチ／タッチ入力モードへの動的切り替えのトリガとなって、ニアフィールドディスプレイにおいて、オブジェクト２０Ａのリスト２０をユーザにより近く引き付けて、図１２Ｄに示されるように、リーチおよび／またはタッチ入力を容易にしてもよい。次いで、ユーザは、図１２Ｅに示すように、オブジェクト２０Ａのリスト２０からオブジェクト２０Ａを手に取って選択してもよく、図１２Ｆに示されているように、ユーザは、オブジェクト２０Ａとの相互作用および／またはそれの操作をリーチ／タッチ入力で行なってもよい。いくつかの実施形態では、オブジェクトのリスト２０のニアフィールドディスプレイは、それらが例えば仮想世界においてユーザから約１．５フィートまたは１．５フィート未満の距離にあるかのように見えてもよい。いくつかの実施形態では、オブジェクトのリスト２０のニアフィールドディスプレイは、それらが、例えば、仮想世界においてユーザから約１．５フィートよりも大きな距離にあるかのように見えてもよい。
An example of this type of dynamic switching is shown in FIGS. 12A-12G. In FIG. 12A, a user interface is presented to the user. The exemplary user interface shown in FIG. 12A is a
いくつかの実施形態では、動的切替トリガに応答してオブジェクト２０Ａがユーザにより近く引き付けられ、オブジェクト２０Ａは、図１２Ａ〜図１２Ｃに示したファーフィールドディスプレイから図１２Ｄ〜図１２Ｆに示したニアフィールドディスプレイに遷移し、オブジェクト２０Ａの１つ以上を、ユーザのオブジェクト選択、相互作用および操作を容易にするために、追加の特徴、情報、粒度などを含むようにモーフィングまたは形態を変更することができる。図１２Ａ〜図１２Ｆに示す例では、図１２Ａ〜図１２Ｃに示されるリスト２０の例示的なオブジェクト２０Ａは、例えば、映画タイトルおよび／または書籍タイトルおよび／またはゲームタイトルのリストに関連付けられる画像であってもよい。リーチおよび／またはタッチの相互作用のためにリスト２０がユーザの近くに引き付けられると、リスト２０内のオブジェクト２０Ａは、図１２Ｄ〜図１２Ｆに示すように、ユーザの選択および相互作用を容易にするようモーフィングしてもよい。オブジェクトが映画タイトルおよび／または書籍タイトルおよび／またはゲームタイトルを含むこの例では、オブジェクト２０は、例えば、タイトル全体、タイトルに関連付けられる静止画像および／または動画、タイトルに関連付けられるジャンル、タイトルに関連付けられる長さ（ランタイム、ページ数など）、ならびにユーザの選択および／または相互作用を容易にすることができる他の特徴を含むようモーフィングしてもよい。
In some embodiments, the
いくつかの実施形態では、動的切替トリガに応答してオブジェクト２０Ａがユーザにより近く引きつけられ、オブジェクト２０Ａが、図１２Ａ〜図１２Ｃに示したファーフィールドディスプレイから図１２Ｄ〜図１２Ｆに示したニアフィールドディスプレイに遷移し、オブジェクト２０Ａが上で論じたようにモーフィングすると、オブジェクト２０Ａは、さらに、アニメーション化され、視覚的にファーフィールドディスプレイからニアフィールドディスプレイに移動してもよい。ファーフィールドディスプレイとニアフィールドディスプレイとの間で移動しモーフィングするにつれてのオブジェクトのアニメーションは、オブジェクト２０Ａのリスト２０において進行中の変化の視覚的な表示をユーザに対して与え、ユーザの関与を維持してもよい。
In some embodiments, the
図１２Ａ〜図１２Ｆに示す例示的な実現例に含まれるオブジェクト２０Ａのリスト２０には、単に議論および説明を容易にするために、タイトル（映画タイトル、書籍タイトル、ゲームタイトルなど）が含まれている。しかしながら、上述した動的切り替えは、例えば、仮想ゲームの部分および機器、仮想ショッピング環境内のアイテム、ならびにより長い距離の注視入力モードからより短い距離のリーチ／タッチ入力モードへの動的切替が仮想世界におけるオブジェクトとのユーザの相互作用を容易にし得る数多くの他の例などの、多数の他の異なるタイプのオブジェクトを含むリストにも適用することができる。
The
さらに、図１２Ａ〜図１２Ｆに示す実現例では、オブジェクト２０Ａのリスト２０からのオブジェクト２０Ａの選択、ならびに選択されたオブジェクト２０Ａとの相互作用およびその操作は、ユーザの手によって行われる。しかしながら、選択されたオブジェクト２０Ａとの相互作用およびその操作は、図９Ａ〜図１０Ｄに関して上述したように、ハンドヘルド電子デバイス１０２を用いて行うこともできる。
Furthermore, in the implementation examples shown in FIGS. 12A to 12F, the selection of the
本明細書で広く説明されている実施形態による動的切り替えの例を図１３に示す。ブロック１３１０で没入型仮想体験に入った後、ブロック１３２０で第１の入力モードを使用して第１の入力が受信されると、ブロック１３３０でオブジェクトのリストにフォーカスが確立される。いくつかの実施形態では、第１の入力モードを使用する第１の入力は、例えば、仮想世界においてユーザから相対的に遠く見える、オブジェクトのリストのような、ユーザインタフェースにおける注視入力（頭部での注視および／または目での注視）であってもよい。一旦オブジェクトのリストにフォーカスが確立されると、動的切り替えのトリガとなる入力が受信される場合、オブジェクトのリストは、第１の入力モードとは異なる第２の入力モードを使用して相互作用を容易にするようにモーフィングされてもよい。いくつかの実施形態では、動的切り替えのトリガとなる入力は、例えば、上述のようにＨＭＤの画像センサによってキャプチャされる手および／もしくは腕のジェスチャ、または他の入力アクションであってもよい。いくつかの実施形態では、動的切り替えのトリガとなる入力アクションは、入力モードを、（オブジェクトのリストにフォーカスを確立した）注視入力モードから、例えば、オブジェクトのリストの個々のオブジェクトとの相互作用を容易にするリーチおよび／またはタッチ入力モードに切り替えさせてもよい。
An example of dynamic switching according to an embodiment broadly described herein is shown in FIG. After entering the immersive virtual experience at block 1310, when a first input is received using the first input mode at
動的切り替えトリガに応答して、オブジェクトのリストは、ブロック１３５０で、例えば上述のリーチおよび／またはタッチ入力モードのような第２の入力モードを使用してユーザ相互作用を容易にするようにモーフィングされてもよい。リスト内のオブジェクトのモーフィングには、例えば、オブジェクトを視覚的にモーフィングして、それらがより近く見えるようにすること、ならびに仮想世界内でユーザの手の届く範囲内で、オブジェクトと関連付けられる画像および／またはテキストおよび／または他の特徴を追加および／または変更することを含んで、リーチおよび／またはタッチ入力モードを使用してオブジェクトとのユーザの相互作用を容易にしてもよい。ブロック１３６０で、第２の入力モードを使用して第２の入力を受信することに応答して、第２の入力モードを使用してオブジェクトをオブジェクトのリストから選択し、操作してもよい。例えば、ユーザは、リストに含まれるオブジェクトの１つ（モーフィング後に、今や仮想世界においてユーザの手の届く範囲内にある）にタッチしてオブジェクトを選択し、次いで、リーチまたはタッチ入力を使用してオブジェクトを操作してもよい。
In response to the dynamic switch trigger, the list of objects is morphed at
このようにして、オブジェクトおよびオブジェクトのリストは、ユーザによって使用される入力モードに基づいて、オブジェクトの選択、それとの相互作用およびそれの操作を容易にする、仮想世界におけるユーザからの距離に視覚的に位置決めされてもよい。 In this way, the object and the list of objects are visually displayed at a distance from the user in the virtual world that facilitates the selection, interaction with and manipulation of the object based on the input mode used by the user. May be positioned.
これにより、モーフィングは、優先順位付けと同様に、優先順位付けに加えて、場合によっては優先順位付けとは独立さえして、ユーザ相互作用の次元をさらに向上させるという技術的効果を有する。 Thereby, morphing has the technical effect of further improving the dimension of user interaction, in addition to prioritization, possibly even independent of prioritization, as well as prioritization.
図１４は、本明細書に記載の技術とともに用いられ得る汎用コンピュータデバイス１４００および汎用モバイルコンピュータデバイス１４５０の例を示す。コンピューティングデバイス１４００は、ラップトップ、デスクトップ、タブレット、ワークステーション、携帯情報端末、テレビ、サーバ、ブレードサーバ、メインフレーム、および他の適切なコンピューティングデバイスといった、様々な形態のデジタルコンピュータを表わすことを意図している。コンピューティングデバイス１４５０は、携帯情報端末、セルラー電話、スマートフォン、および他の同様のコンピューティングデバイスといった、様々な形態のモバイルデバイスを表わすことを意図している。ここに示すコンポーネント、それらの接続および関係、ならびにそれらの機能は例示であることが意図されているに過ぎず、本文書に記載のおよび／または請求項に記載の本発明の実現例を限定することを意図していない。
FIG. 14 illustrates an example of a general
コンピューティングデバイス１４００は、プロセッサ１４０２、メモリ１４０４、記憶装置１４０６、メモリ１４０４および高速拡張ポート１４１０に接続している高速インターフェイス１４０８、ならびに低速バス１４１４および記憶装置１４０６に接続している低速インターフェイス１４１２を含む。プロセッサ１４０２は、半導体ベースのプロセッサとすることができる。メモリ１４０４は、半導体ベースのメモリとすることができる。コンポーネント１４０２，１４０４，１４０６，１４０８，１４１０および１４１２の各々は様々なバスを用いて相互に接続されており、共通のマザーボード上にまたは他の態様で適宜搭載され得る。プロセッサ１４０２は、コンピューティングデバイス１４００内で実行される命令を処理可能であり、この命令には、ＧＵＩのためのグラフィック情報を高速インターフェイス１４０８に結合されているディスプレイ１４１６などの外部入出力デバイス上に表示するためにメモリ１４０４内または記憶装置１４０６上に記憶されている命令が含まれる。他の実現例では、複数のプロセッサおよび／または複数のバスが、複数のメモリおよび複数種類のメモリとともに必要に応じて用いられ得る。また、複数のコンピューティングデバイス１４００が接続され得、各デバイスは（例えばサーババンク、ブレードサーバのグループ、またはマルチプロセッサシステムとして）必要な動作の一部を提供する。
The
メモリ１４０４は情報をコンピューティングデバイス１４００内に記憶する。一実現例では、メモリ１４０４は１つまたは複数の揮発性メモリユニットである。別の実現例では、メモリ１４０４は１つまたは複数の不揮発性メモリユニットである。また、メモリ１４０４は、磁気ディスクまたは光ディスクといった別の形態のコンピュータ読取可能媒体であってもよい。
記憶装置１４０６は、コンピューティングデバイス１４００に大容量記憶を提供可能である。一実現例では、記憶装置１４０６は、フロッピー（登録商標）ディスクデバイス、ハードディスクデバイス、光ディスクデバイス、またはテープデバイス、フラッシュメモリもしくは他の同様のソリッドステートメモリデバイス、またはストレージエリアネットワークもしくは他のコンフィギュレーションにおけるデバイスを含む多数のデバイスといった、コンピュータ読取可能媒体であってもよく、または当該コンピュータ読取可能媒体を含んでいてもよい。コンピュータプログラムプロダクトが情報媒体内に有形に具体化され得る。また、コンピュータプログラムプロダクトは、実行されると上述のような１つ以上の方法を実行する命令を含み得る。情報媒体は、メモリ１４０４、記憶装置１４０６、またはプロセッサ１４０２上のメモリといった、コンピュータ読取可能媒体または機械読取可能媒体である。
Storage device 1406 can provide mass storage to
高速コントローラ１４０８はコンピューティングデバイス１４００のための帯域幅集約的な動作を管理するのに対して、低速コントローラ１４１２はより低い帯域幅集約的な動作を管理する。そのような機能の割当ては例示に過ぎない。一実現例では、高速コントローラ１４０８はメモリ１４０４、ディスプレイ１４１６に（例えばグラフィックスプロセッサまたはアクセラレータを介して）、および様々な拡張カード（図示せず）を受付け得る高速拡張ポート１４１０に結合される。当該実現例では、低速コントローラ１４１２は記憶装置１４０６および低速拡張ポート１４１４に結合される。様々な通信ポート（例えばＵＳＢ、ブルートゥース（登録商標）、イーサネット（登録商標）、無線イーサネット）を含み得る低速拡張ポートは、キーボード、ポインティングデバイス、スキャナ、またはスイッチもしくはルータといったネットワーキングデバイスなどの１つ以上の入出力デバイスに、例えばネットワークアダプタを介して結合され得る。
The
コンピューティングデバイス１４００は、図に示すように多数の異なる形態で実現されてもよい。例えば、コンピューティングデバイス１４００は標準的なサーバ１４２０として、またはそのようなサーバのグループ内で複数回実現されてもよい。また、コンピューティングデバイス１４００はラックサーバシステム１４２４の一部として実現されてもよい。さらに、コンピューティングデバイス１４００はラップトップコンピュータ１４２２などのパーソナルコンピュータにおいて実現されてもよい。あるいは、コンピューティングデバイス１４００からのコンポーネントは、デバイス１４５０などのモバイルデバイス（図示せず）内の他のコンポーネントと組合されてもよい。そのようなデバイスの各々がコンピューティングデバイス１４００，１４５０の１つ以上を含んでいてもよく、システム全体が、互いに通信する複数のコンピューティングデバイス１４００，１４５０で構成されてもよい。
The
コンピューティングデバイス１４５０は、数あるコンポーネントの中でも特に、プロセッサ１４５２、メモリ１４６４、ディスプレイ１４５４などの入出力デバイス、通信インターフェイス１４６６、およびトランシーバ１４６８を含む。また、デバイス１４５０には、マイクロドライブまたは他のデバイスなどの記憶装置が提供されて付加的なストレージが提供されてもよい。コンポーネント１４５０，１４５２，１４６４，１４５４，１４６６および１４６８の各々は様々なバスを用いて相互に接続されており、当該コンポーネントのいくつかは共通のマザーボード上にまたは他の態様で適宜搭載され得る。
The
プロセッサ１４５２は、メモリ１４６４に記憶されている命令を含む、コンピューティングデバイス１４５０内の命令を実行可能である。プロセッサは、別個の複数のアナログおよびデジタルプロセッサを含むチップのチップセットとして実現されてもよい。プロセッサは、例えば、ユーザインターフェイス、デバイス１４５０が実行するアプリケーション、およびデバイス１４５０による無線通信の制御といった、デバイス１４５０の他のコンポーネントの協調を提供し得る。
The
プロセッサ１４５２は、ディスプレイ１４５４に結合された制御インターフェイス１４５８およびディスプレイインターフェイス１４５６を介してユーザと通信し得る。ディスプレイ１４５４は、例えば、ＴＦＴ ＬＣＤ（薄膜トランジスタ液晶ディスプレイ）もしくはＯＬＥＤ（有機発光ダイオード）ディスプレイ、または他の適切なディスプレイ技術であり得る。ディスプレイインターフェイス１４５６は、ディスプレイ１４５４を駆動してグラフィックおよび他の情報をユーザに提示するための適切な回路を含み得る。制御インターフェイス１４５８はユーザからコマンドを受信し、当該コマンドをプロセッサ１４５２に提出するために変換し得る。さらに、外部インターフェイス１４６２が、デバイス１４５０と他のデバイスとの隣接通信を可能にするために、プロセッサ１４５２と通信した状態で提供されてもよい。外部インターフェイス１４６２は、例えば、ある実現例では有線通信を提供し、他の実現例では無線通信を提供してもよく、また、複数のインターフェイスが用いられてもよい。
メモリ１４６４は情報をコンピューティングデバイス１４５０内に記憶する。メモリ１４６４は、１つもしくは複数のコンピュータ読取可能媒体、１つもしくは複数の揮発性メモリユニット、または１つもしくは複数の不揮発性メモリユニットの１つ以上として実現され得る。さらに、拡張メモリ１４７４が提供され、例えばＳＩＭＭ（Single In Line Memory Module）カードインターフェイスを含み得る拡張インターフェイス１４７２を介してデバイス１４５０に接続されてもよい。このような拡張メモリ１４７４はデバイス１４５０に余分のストレージスペースを提供し得るか、またはデバイス１４５０のためのアプリケーションもしくは他の情報をさらに記憶し得る。具体的には、拡張メモリ１４７４は上述のプロセスを実行または補足するための命令を含み得、さらにセキュア情報を含み得る。ゆえに、例えば、拡張メモリ１４７４はデバイス１４５０のためのセキュリティモジュールとして提供されてもよく、デバイス１４５０のセキュアな使用を許可する命令でプログラムされてもよい。さらに、ハッキング不可能なようにＳＩＭＭカード上に識別情報を置くといったように、セキュアなアプリケーションが付加的な情報とともにＳＩＭＭカードを介して提供されてもよい。
Memory 1464 stores information within
メモリは、以下に記載のように、例えばフラッシュメモリおよび／またはＮＶＲＡＭメモリを含み得る。一実現例では、コンピュータプログラムプロダクトが情報媒体内に有形に具体化される。コンピュータプログラムプロダクトは、実行されると上述のような１つ以上の方法を実行する命令を含む。情報媒体は、メモリ１４６４、拡張メモリ１４７４、またはプロセッサ１４５２上のメモリといった、コンピュータ読取可能媒体または機械読取可能媒体であり、これは、例えばトランシーバ１４６８または外部インターフェイス１４６２上で受信され得る。
The memory may include, for example, flash memory and / or NVRAM memory, as described below. In one implementation, a computer program product is tangibly embodied in an information medium. The computer program product includes instructions that, when executed, perform one or more methods as described above. The information medium is a computer-readable or machine-readable medium, such as memory 1464, expansion memory 1474, or memory on
デバイス１４５０は、必要に応じてデジタル信号処理回路を含み得る通信インターフェイス１４６６を介して無線通信し得る。通信インターフェイス１４６６は、とりわけ、ＧＳＭ（登録商標）音声通話、ＳＭＳ、ＥＭＳ、またはＭＭＳメッセージング、ＣＤＭＡ、ＴＤＭＡ、ＰＤＣ、ＷＣＤＭＡ（登録商標）、ＣＤＭＡ２０００、またはＧＰＲＳといった、様々なモードまたはプロトコル下の通信を提供し得る。そのような通信は、例えば無線周波数トランシーバ１４６８を介して起こり得る。さらに、ブルートゥース、Ｗｉ−Ｆｉ、または他のそのようなトランシーバ（図示せず）を用いるなどして、短距離通信が起こり得る。さらに、ＧＰＳ（全地球測位システム）レシーバモジュール１４７０が付加的なナビゲーション関連および位置関連の無線データをデバイス１４５０に提供し得、当該データはデバイス１４５０上で実行されるアプリケーションによって適宜用いられ得る。
また、デバイス１４５０は、ユーザから口頭情報を受信して当該情報を使用可能なデジタル情報に変換し得る音声コーデック１４６０を用いて可聴的に通信し得る。音声コーデック１４６０も同様に、例えばデバイス１４５０のハンドセット内で、スピーカを介すなどしてユーザに可聴音を生成し得る。そのような音は音声電話からの音を含んでいてもよく、録音された音（例えば音声メッセージ、音楽ファイル等）を含んでいてもよく、さらに、デバイス１４５０上で実行されるアプリケーションが生成する音を含んでいてもよい。
コンピューティングデバイス１４５０は、図に示すように多数の異なる形態で実現されてもよい。例えば、コンピューティングデバイス１４５０はセルラー電話１４８０として実現されてもよい。また、コンピューティングデバイス１４５０は、スマートフォン１４８２、携帯情報端末、または他の同様のモバイルデバイスの一部として実現されてもよい。
本明細書に記載のシステムおよび技術の様々な実現例は、デジタル電子回路、集積回路、特別に設計されたＡＳＩＣ（特定用途向け集積回路）、コンピュータハードウェア、ファームウェア、ソフトウェア、および／またはそれらの組合せで実現され得る。これらの様々な実現例は、少なくとも１つのプログラマブルプロセッサを含むプログラマブルシステム上で実行可能および／または解釈可能な１つ以上のコンピュータプログラムにおける実現例を含んでいてもよく、当該プロセッサは専用であっても汎用であってもよく、ストレージシステム、少なくとも１つの入力デバイス、および少なくとも１つの出力デバイスからデータおよび命令を受信するように、かつこれらにデータおよび命令を送信するように結合されている。 Various implementations of the systems and techniques described herein include digital electronic circuits, integrated circuits, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and / or their It can be realized in combination. These various implementations may include implementations in one or more computer programs that are executable and / or interpretable on a programmable system that includes at least one programmable processor, the processor being dedicated. And may be general purpose and is coupled to receive and send data and instructions to and from the storage system, at least one input device, and at least one output device.
これらのコンピュータプログラム（プログラム、ソフトウェア、ソフトウェアアプリケーションまたはコードとしても公知）はプログラマブルプロセッサのための機械命令を含んでおり、高レベル手続きおよび／もしくはオブジェクト指向プログラミング言語で、ならびに／またはアセンブリ／機械言語で実現され得る。本明細書において使用する「機械読取可能媒体」「コンピュータ読取可能媒体」という用語は、機械命令および／またはデータをプログラマブルプロセッサに提供するために用いられる任意のコンピュータプログラムプロダクト、装置および／またはデバイス（例えば磁気ディスク、光ディスク、メモリ、プログラマブルロジックデバイス（ＰＬＤ））を指し、機械命令を機械読取可能信号として受信する機械読取可能媒体を含む。「機械読取可能信号」という用語は、機械命令および／またはデータをプログラマブルプロセッサに提供するために用いられる任意の信号を指す。 These computer programs (also known as programs, software, software applications or code) contain machine instructions for programmable processors, in high-level procedural and / or object-oriented programming languages, and / or in assembly / machine languages. Can be realized. As used herein, the terms “machine-readable medium” and “computer-readable medium” refer to any computer program product, apparatus and / or device used to provide machine instructions and / or data to a programmable processor ( For example, a magnetic disk, an optical disk, a memory, a programmable logic device (PLD)), including a machine-readable medium that receives machine instructions as a machine-readable signal. The term “machine-readable signal” refers to any signal used to provide machine instructions and / or data to a programmable processor.
ユーザとの相互作用を提供するために、本明細書に記載のシステムおよび技術は、情報をユーザに表示するためのディスプレイデバイス（例えばＣＲＴ（陰極線管）またはＬＣＤ（液晶ディスプレイ）モニタ）と、ユーザが入力をコンピュータに提供する際に使用可能なキーボードおよびポインティングデバイス（例えばマウスまたはトラックボール）とを有するコンピュータ上で実現され得る。他の種類のデバイスを用いてユーザとの相互作用を提供することもでき、例えば、ユーザに提供されるフィードバックは任意の形態の感覚フィードバック（例えば視覚フィードバック、聴覚フィードバック、または触覚フィードバック）であり得、ユーザからの入力は、音響、スピーチ、または触覚入力を含む任意の形態で受信され得る。 In order to provide user interaction, the systems and techniques described herein provide a display device (eg, a CRT (cathode ray tube) or LCD (liquid crystal display) monitor) for displaying information to the user, and a user. Can be implemented on a computer having a keyboard and pointing device (eg, a mouse or trackball) that can be used in providing input to the computer. Other types of devices can also be used to provide interaction with the user, for example, the feedback provided to the user can be any form of sensory feedback (eg, visual feedback, audio feedback, or tactile feedback). The input from the user may be received in any form including acoustic, speech, or haptic input.
本明細書に記載のシステムおよび技術は、バックエンドコンポーネントを（例えばデータサーバとして）含む、またはミドルウェアコンポーネントを（例えばアプリケーションサーバとして）含む、またはフロントエンドコンポーネント（例えば、ユーザが上記のシステムおよび技術の実現例と相互に作用する際に使用可能なグラフィックユーザインターフェイスもしくはウェブブラウザを有するクライアントコンピュータ）、またはそのようなバックエンド、ミドルウェア、もしくはフロントエンドコンポーネントの任意の組合せを含むコンピューティングシステムにおいて実現され得る。システムのコンポーネントは、任意の形態または媒体のデジタルデータ通信（例えば通信ネットワーク）によって相互に接続され得る。通信ネットワークの例として、ローカルエリアネットワーク（「ＬＡＮ」）、ワイドエリアネットワーク（「ＷＡＮ」）、およびインターネットが挙げられる。 The systems and techniques described herein include a back-end component (eg, as a data server), or include a middleware component (eg, as an application server), or a front-end component (eg, a user of the systems and techniques described above). Can be implemented in a computing system including a graphic user interface or a client computer with a web browser that can be used to interact with an implementation), or any combination of such back-end, middleware, or front-end components . The components of the system can be interconnected by any form or medium of digital data communication (eg, a communication network). Examples of communication networks include a local area network (“LAN”), a wide area network (“WAN”), and the Internet.
コンピューティングシステムはクライアントおよびサーバを含み得る。クライアントおよびサーバは一般に互いにリモートであり、典型的に通信ネットワークを介して相互に作用する。クライアントとサーバとの関係は、それぞれのコンピュータ上で実行されて互いにクライアント−サーバ関係を有するコンピュータプログラムによって生じる。 The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship between the client and the server is caused by computer programs that are executed on the respective computers and have a client-server relationship with each other.
多数の実施形態を説明した。しかしながら、本発明の精神および範囲から逸脱することなく様々な変更がなされ得ることが理解されるであろう。 A number of embodiments have been described. However, it will be understood that various modifications can be made without departing from the spirit and scope of the invention.
また、図面に示す論理フローは、所望の結果を達成するために、示されている特定の順序、または起こる順序を必要としない。また、記載のフローとは他のステップが提供されてもよく、または当該フローからステップが除去されてもよく、記載のシステムに他のコンポーネントが追加されてもよく、または当該システムからコンポーネントが除去されてもよい。したがって、他の実施形態も以下の請求項の範囲内にある。 Also, the logic flows shown in the drawings do not require the particular order shown or the order in which they occur to achieve the desired result. Also, other steps may be provided with the described flow, or steps may be removed from the flow, other components may be added to the described system, or components may be removed from the system. May be. Accordingly, other embodiments are within the scope of the following claims.
本明細書で説明される様々な技術の実現は、デジタル電子回路、もしくはコンピュータハードウェア、ファームウェア、ソフトウェア、またはそれらの組み合わせで実現されてもよい。実現例は、データ処理装置、例えばプログラム可能なプロセッサ、コンピュータ、または複数のコンピュータによる処理のために、またはそれらの動作を制御するために、コンピュータプログラム製品、すなわち例えば機械可読記憶装置（コンピュータ可読媒体）などの情報担体に有形で実施されるコンピュータプログラムとして実現されてもよい。したがって、コンピュータ可読記憶媒体は、実行されるとプロセッサ（例えば、ホストデバイスのプロセッサ、クライアントデバイスのプロセッサ）にプロセスを実行させる命令を格納するように構成することができる。 Implementations of the various techniques described herein may be implemented in digital electronic circuitry, or computer hardware, firmware, software, or combinations thereof. An implementation is a computer program product, eg, a machine-readable storage device (computer-readable medium), for processing by a data processing device, eg, a programmable processor, computer, or multiple computers, or for controlling the operation thereof. ) And the like may be implemented as a computer program tangibly implemented on an information carrier. Accordingly, a computer-readable storage medium may be configured to store instructions that, when executed, cause a processor (eg, a host device processor, a client device processor) to perform a process.
上述したコンピュータプログラムのようなコンピュータプログラムは、コンパイルまたは解釈された言語を含む任意の形式のプログラミング言語で書かれ得、スタンドアロンプログラムとして、または、モジュール、コンポーネント、サブルーチン、もしくは、コンピューティング環境で使用するのに好適な他のユニットとして任意の形態で展開され得る。コンピュータプログラムは、１つのコンピュータまたは１つの場所に位置するかもしくは複数の場所にわたって分散され通信ネットワークによって相互接続される複数のコンピュータ上で処理されるように展開され得る。 A computer program, such as the computer program described above, can be written in any form of programming language, including a compiled or interpreted language, and used as a stand-alone program or in a module, component, subroutine, or computing environment. It can be deployed in any form as other units suitable for the above. A computer program can be deployed to be processed on one computer or on multiple computers located at one location or distributed across multiple locations and interconnected by a communications network.
方法ステップは、入力データ上で動作し出力を生成することにより機能を実行するよう１つ以上のプログラマブルプロセッサがコンピュータプログラムを実行することによって実行されてもよい。方法ステップの実行および装置の実施は、例えばＦＰＧＡ（フィールドプログラマブルゲートアレイ）またはＡＳＩＣ（特定用途向け集積回路）といった特殊目的論理回路系で行われてもよい。 Method steps may be performed by one or more programmable processors executing a computer program to perform functions by operating on input data and generating output. The execution of the method steps and the implementation of the apparatus may be performed in a special purpose logic circuit system, for example an FPGA (Field Programmable Gate Array) or an ASIC (Application Specific Integrated Circuit).
コンピュータプログラムの処理に好適であるプロセッサは、例として、汎用マイクロプロセッサおよび特殊目的マイクロプロセッサの両方、ならびに任意の種類のデジタルコンピュータの任意の１つ以上のプロセッサを含んでもよい。一般に、プロセッサは、リードオンリメモリもしくはランダムアクセスメモリまたはその両方から命令およびデータを受取ることになる。コンピュータの要素は、命令を実行するための少なくとも１つのプロセッサと、命令およびデータを格納するための１つ以上のメモリデバイスとを含んでもよい。一般に、コンピュータはさらに、例えば磁気ディスク、光磁気ディスクまたは光ディスクといった、データを格納するための１つ以上の大容量記憶装置を含むか、当該１つ以上の大容量記憶装置からデータを受取るかもしくは当該１つ以上の大容量記憶装置にデータを転送するよう動作可能に結合されるか、またはその両方を行ってもよい。コンピュータプログラム命令およびデータを実施するのに好適である情報担体は、例として、例えばＥＰＲＯＭ、ＥＥＰＲＯＭおよびフラッシュメモリデバイスを含む全ての形態の不揮発性メモリと；例えば内部ハードディスクまたはリムーバブルディスクといった磁気ディスクと；光磁気ディスクと；ＣＤ−ＲＯＭおよびＤＶＤ−ＲＯＭディスクとを含む。プロセッサおよびメモリは、特殊目的論理回路系によって補足され得るか、または特殊目的論理回路系に組み込まれ得る。 Processors suitable for processing computer programs may include, by way of example, both general and special purpose microprocessors, as well as any one or more processors of any type of digital computer. Generally, a processor will receive instructions and data from a read-only memory or a random access memory or both. The computer elements may include at least one processor for executing instructions and one or more memory devices for storing instructions and data. Generally, a computer further includes, receives data from, or receives data from, one or more mass storage devices, such as a magnetic disk, a magneto-optical disk, or an optical disk, for example. It may be operatively coupled to transfer data to the one or more mass storage devices, or both. Information carriers suitable for implementing computer program instructions and data include, by way of example, all forms of non-volatile memory including, for example, EPROM, EEPROM and flash memory devices; and magnetic disks such as, for example, internal hard disks or removable disks; Includes magneto-optical disks; CD-ROM and DVD-ROM disks. The processor and memory can be supplemented by, or incorporated in, special purpose logic circuitry.
ユーザとの相互作用を提供するために、実現例は、例えば陰極線管（ＣＲＴ）、発光ダイオード（ＬＥＤ）または液晶ディスプレイ（ＬＣＤ）モニタといったユーザに対して情報を表示するための表示デバイスと、ユーザがコンピュータに入力を提供可能であるキーボードおよびポインティングデバイス、例えばマウス、トラックボールとを有するコンピュータ上で実現されてもよい。他の種類のデバイスが同様に、ユーザとの相互作用を提供するために使用され得；例えば、ユーザに提供されるフィードバックは、例えば視覚フィードバック、聴覚フィードバックまたは触覚フィードバックといった任意の形態の感覚フィードバックであり得；ユーザからの入力は、音響入力、発話入力、または触覚入力を含む任意の形態で受取られ得る。 In order to provide user interaction, implementations include a display device for displaying information to a user, such as a cathode ray tube (CRT), a light emitting diode (LED) or a liquid crystal display (LCD) monitor, and a user May be implemented on a computer having a keyboard and pointing device that can provide input to the computer, such as a mouse, trackball. Other types of devices can be used to provide interaction with the user as well; for example, the feedback provided to the user is any form of sensory feedback, such as visual feedback, audio feedback or tactile feedback, for example. Possible; input from the user may be received in any form including acoustic input, speech input, or haptic input.
実現例は、例えばデータサーバなどのバックエンドコンポーネントを含むコンピューティングシステム、またはアプリケーションサーバなどのミドルウェアコンポーネントを含むコンピューティングシステム、または例えばユーザが実現例と相互に作用できるグラフィカルユーザインターフェイスもしくはウェブブラウザを有するクライアントコンピュータなどのフロントエンドコンポーネント含むコンピューティングシステム、またはそのようなバックエンド、ミドルウェア、もしくはフロントエンドコンポーネントの任意の組合わせにおいて実現されてもよい。コンポーネントは、例えば通信ネットワークといったデジタルデータ通信の任意の形態または媒体によって相互接続されてもよい。通信ネットワークの例は、ローカルエリアネットワーク（ＬＡＮ）およびワイドエリアネットワーク（ＷＡＮ）、例えばインターネットを含む。 An implementation has a computing system that includes a back-end component, such as a data server, or a middleware component, such as an application server, or a graphical user interface or web browser that allows a user to interact with the implementation, for example. It may be implemented in a computing system that includes a front-end component, such as a client computer, or any combination of such back-end, middleware, or front-end components. The components may be interconnected by any form or medium of digital data communication such as a communication network. Examples of communication networks include a local area network (LAN) and a wide area network (WAN), such as the Internet.
さらなる例が以下に開示される。
例１：方法は、複数の入力モードに応答して各々が選択可能かつ操作可能である複数の仮想オブジェクトを含む没入型仮想体験を開始することと、複数の入力モードのうちの第１の入力モードを実施する第１の入力を受信することと、第１の入力に応答して複数の仮想オブジェクトのうちの第１の仮想オブジェクトを選択することと、複数の入力モードのうちの第２の入力モードを実施する第２の入力を受信することと、第２の入力モードの優先順位値と第１の入力モードの優先順位値とを比較することと、比較に基づいて、第２の入力に応答して、第１の仮想オブジェクトの選択を解放し、選択を複数の仮想オブジェクトのうちの第２の仮想オブジェクトにシフトするか、または第１の仮想オブジェクトの選択を維持することとを含む。
Further examples are disclosed below.
Example 1: The method initiates an immersive virtual experience including a plurality of virtual objects, each selectable and operable in response to a plurality of input modes, and a first input of the plurality of input modes Receiving a first input that implements the mode; selecting a first virtual object of the plurality of virtual objects in response to the first input; and a second of the plurality of input modes. Receiving a second input implementing the input mode, comparing the priority value of the second input mode with the priority value of the first input mode, and based on the comparison, the second input In response to releasing the selection of the first virtual object and shifting the selection to a second virtual object of the plurality of virtual objects or maintaining the selection of the first virtual object. .
例２：第２の入力に応答して、第１の仮想オブジェクトの選択を解放し、選択を第２の仮想オブジェクトにシフトすることは、第２の入力の優先順位値が第１の入力の優先順位値よりも大きい場合に、第２の入力に応答して、第１の仮想オブジェクトの選択を解放し、選択を第２の仮想オブジェクトにシフトすることを含む、例１の方法。 Example 2: In response to a second input, releasing the selection of the first virtual object and shifting the selection to the second virtual object means that the priority value of the second input is that of the first input. The method of example 1, comprising releasing the selection of the first virtual object and shifting the selection to the second virtual object in response to the second input if greater than the priority value.
例３：第２の入力の優先順位値が第１の入力の優先順位値よりも小さい場合に、第１の仮想オブジェクトの選択を維持することをさらに備える、例１または２の方法。 Example 3: The method of example 1 or 2, further comprising maintaining the selection of the first virtual object when the priority value of the second input is less than the priority value of the first input.
例４：複数の入力モードの各々は、関連付けられる優先順位値を有し、複数の入力モードの各々に関連付けられるそれぞれの優先順位値は、予め設定されるかまたはユーザプロファイルに従ってユーザによって設定される、例１、２または３の方法。 Example 4: Each of the plurality of input modes has an associated priority value, and each priority value associated with each of the plurality of input modes is preset or set by a user according to a user profile The method of Examples 1, 2 or 3.
例５：複数の入力モードは、頭部での注視入力モード、目での注視入力モード、ポイント入力モード、リーチ入力モード、およびジェスチャ入力モードのうちの少なくとも２つを含む、例１〜４の１つの方法。 Example 5: The plurality of input modes includes at least two of a gaze input mode with a head, a gaze input mode with an eye, a point input mode, a reach input mode, and a gesture input mode. One way.
例６：複数の入力モードのうちの第３の入力モードを実施する第３の入力を受信することと、第３の入力に応答して、選択された第１の仮想オブジェクトを操作することとをさらに備える、例１〜５の１つの方法。 Example 6: receiving a third input that implements a third input mode of the plurality of input modes; manipulating the selected first virtual object in response to the third input; The method of Examples 1-5, further comprising:
例７：第１の入力を受信し、第１の仮想オブジェクトを選択することは、操作のために第１の仮想オブジェクトを選択するよう第１の仮想オブジェクトに向けられる注視入力を受信することを含む、例１〜６の１つの方法。 Example 7: Receiving a first input and selecting a first virtual object includes receiving a gaze input directed to the first virtual object to select the first virtual object for manipulation. One method of Examples 1-6.
例８：第３の入力を受信することは、ハンドヘルド電子デバイスのタッチセンシティブ面上でタッチ入力を受信することと、タッチ入力に応答して、選択された第１の仮想オブジェクトを移動させることとを含む、例１〜７の１つの方法。 Example 8: Receiving a third input includes receiving a touch input on a touch-sensitive surface of a handheld electronic device, and moving a selected first virtual object in response to the touch input. One method of Examples 1-7, comprising
例９：ハンドヘルド電子デバイスのタッチセンシティブ面上のタッチ入力はタッチおよびドラッグ入力であり、選択されたオブジェクトの移動は、タッチセンシティブ面上のタッチおよびドラッグ入力の移動に対応する、例１〜８の１つの方法。 Example 9: The touch input on the touch sensitive surface of the handheld electronic device is a touch and drag input, and the movement of the selected object corresponds to the movement of the touch and drag input on the touch sensitive surface. One way.
例１０：第１の仮想オブジェクトを選択するよう第１の仮想オブジェクトに向けられる注視入力を受信することは、第１の仮想オブジェクトを含む複数の仮想オブジェクトのファーフィールドディスプレイで注視入力を受信することを含み、第３の入力を受信することは、ジェスチャ入力を受信することと、ファーフィールドディスプレイからニアフィールドディスプレイへの複数の仮想オブジェクトの動的切り替えのためのトリガとしてジェスチャ入力を認識することと、複数の仮想オブジェクトのニアフィールド表示のために複数の仮想オブジェクトの外観をモーフィングすることと、モーフィングされた複数の仮想オブジェクトをニアフィールドディスプレイに表示することとを含む、例１〜９の１つの方法。 Example 10: Receiving a gaze input directed to a first virtual object to select a first virtual object receives a gaze input on a far field display of a plurality of virtual objects including the first virtual object. Receiving the third input includes receiving the gesture input, recognizing the gesture input as a trigger for dynamic switching of the plurality of virtual objects from the far field display to the near field display; One of examples 1-9, including morphing the appearance of the plurality of virtual objects for near field display of the plurality of virtual objects and displaying the plurality of morphed virtual objects on a near field display Method.
例１１：複数の仮想オブジェクトのニアフィールド表示のために複数の仮想オブジェクトの外観をモーフィングすることは、複数の仮想オブジェクトがファーフィールドディスプレイからニアフィールドディスプレイに移動する際に、ユーザから複数の仮想オブジェクトへの仮想距離を減少させることと、複数の仮想オブジェクトのサイズを変更すること、複数の仮想オブジェクトに関連付けられる画像を変更すること、複数の仮想オブジェクトに関連付けられるテキストを変更すること、または複数の仮想オブジェクトに関連付けられるメニューアイテムを変更することのうちの少なくとも１つとを含む、例１０の方法。 Example 11: Morphing the appearance of multiple virtual objects for near field display of multiple virtual objects means that when multiple virtual objects move from a far field display to a near field display, multiple virtual objects from the user Reducing the virtual distance to, changing the size of multiple virtual objects, changing the image associated with multiple virtual objects, changing the text associated with multiple virtual objects, or multiple 11. The method of example 10, comprising at least one of changing a menu item associated with the virtual object.
例１２：第４の入力を受信することをさらに備え、第４の入力は、ニアフィールドディスプレイに表示される複数の仮想オブジェクトの仮想オブジェクトを選択し操作するためのリーチジェスチャを含む、例１〜１１の１つの方法。 Example 12: further comprising receiving a fourth input, wherein the fourth input includes a reach gesture for selecting and manipulating virtual objects of the plurality of virtual objects displayed on the near field display. 11 one method.
例１３：第１の入力を受信し、第１の仮想オブジェクトを選択することは、操作のために第１の仮想オブジェクトを選択するよう第１の仮想オブジェクトに向けられるハンドヘルド電子デバイスによって生成されるビームを受信することを含む、例１〜１２の１つの方法。 Example 13: Receiving a first input and selecting a first virtual object is generated by a handheld electronic device that is directed to the first virtual object to select the first virtual object for manipulation One method of Examples 1-12, comprising receiving a beam.
例１４：第３の入力を受信することは、ハンドヘルド電子デバイスのタッチセンシティブ面上でタッチ入力を受信することと、タッチ入力をタッチセンシティブ面に沿ってドラッグすることと、タッチおよびドラッグ入力に応答して、選択された第１の仮想オブジェクトを移動させることとを含み、選択された第１の仮想オブジェクトの移動は、ハンドヘルド電子装置のタッチセンシティブ面に沿ったドラッグ入力の移動に対応する、例１〜１３の１つの方法。 Example 14: Receiving a third input is receiving a touch input on a touch-sensitive surface of a handheld electronic device, dragging the touch input along the touch-sensitive surface, and responding to the touch and drag input Moving the selected first virtual object, wherein the movement of the selected first virtual object corresponds to the movement of a drag input along the touch-sensitive surface of the handheld electronic device. One method of 1-13.
例１５：第３の入力は、ハンドヘルド電子デバイスによって生成されるビームをハンドヘルド電子デバイスと選択された第１の仮想オブジェクトとの間で接続するために、ハンドヘルド電子デバイスのタッチセンシティブ面上でタッチ入力を受信することと、ハンドヘルド電子デバイスを移動させることと、ハンドヘルド電子デバイスを移動させることと、ハンドヘルド電子デバイスの移動に応答して、選択された第１の仮想オブジェクトを移動させることとを含み、選択された第１の仮想オブジェクトの移動は、ハンドヘルド電子デバイスの移動に対応し、第３の入力はさらに、ハンドヘルド電子デバイスのタッチセンシティブ面からタッチ入力を解放することと、選択された第１の仮想オブジェクトをタッチ入力の解放ポイントに再配置するよう、タッチ入力の解放に応答して、選択された第１の仮想オブジェクトを解放することとを含む、例１〜１３の１つの方法。 Example 15: The third input is a touch input on the touch-sensitive surface of the handheld electronic device to connect the beam generated by the handheld electronic device between the handheld electronic device and the selected first virtual object. , Moving the handheld electronic device, moving the handheld electronic device, and moving the selected first virtual object in response to the movement of the handheld electronic device; The movement of the selected first virtual object corresponds to the movement of the handheld electronic device, and the third input further releases the touch input from the touch sensitive surface of the handheld electronic device, and the selected first virtual object. Relocate virtual object to touch input release point So that, in response to release of the touch input, and a releasing the first virtual object selected, one method of Example 1-13.
例１６：没入型仮想体験を開始することと、複数の仮想オブジェクトのファーフィールドディスプレイを生成することと、第１の入力を受信することと、第１の入力に応答して複数の仮想オブジェクトを含むファーフィールドディスプレイを選択することと、第２の入力を受信することと、第２の入力に応答して複数の仮想オブジェクトのニアフィールド表示のために複数の仮想オブジェクトの外観をモーフィングすることと、モーフィングされた複数の仮想オブジェクトを含むニアフィールドディスプレイを生成することと、第３の入力を受信することと、第３の入力に応答してモーフィングされた複数の仮想オブジェクトのうちの１つを選択することとを備える、例１６の方法。 Example 16: Initiating an immersive virtual experience, generating a far field display of a plurality of virtual objects, receiving a first input, and responding to the first input with a plurality of virtual objects Selecting a far field display to include; receiving a second input; morphing the appearance of the plurality of virtual objects for near field display of the plurality of virtual objects in response to the second input; Generating a near field display including a plurality of morphed virtual objects; receiving a third input; and selecting one of the plurality of morphed virtual objects in response to the third input. The method of example 16, comprising selecting.
例１７：第１の入力を受信することは、ファーフィールド注視入力またはハンドヘルド電子デバイスによって生成されるファーフィールドビーム入力を受信することを含み、第２の入力を受信することは、複数の仮想オブジェクトのファーフィールドディスプレイからモーフィングされた複数の仮想オブジェクトのニアフィールドディスプレイへの遷移のトリガとなる手または腕ジェスチャ入力を受信することを含み、第３の入力を受信することは、モーフィングされた仮想オブジェクトのニアフィールドディスプレイから仮想オブジェクトを選択するニアフィールド手ジェスチャまたはニアフィールドリーチジェスチャを受信することを含む、例１６の方法。 Example 17: Receiving a first input includes receiving a far field gaze input or a far field beam input generated by a handheld electronic device, and receiving a second input includes a plurality of virtual objects Receiving a hand or arm gesture input that triggers a transition of a plurality of virtual objects morphed from a far field display to a near field display, wherein receiving a third input is a morphed virtual object 17. The method of example 16, comprising receiving a near-field hand gesture or near-field reach gesture selecting a virtual object from a near-field display.
例１８：第４の入力を受信することと、第４の入力に応答して、選択された仮想オブジェクトを操作することとをさらに備える、例１６または１７の方法。 Example 18: The method of example 16 or 17, further comprising receiving a fourth input and manipulating the selected virtual object in response to the fourth input.
例１９：第４の入力を受信することは、ハンドヘルド電子デバイスのタッチセンシティブ面上のタッチおよびドラッグ入力、タッチセンシティブ面上のタッチおよびハンドヘルド電子デバイスの移動、または選択された仮想オブジェクトの特徴に向けられる手のジェスチャのうちの少なくとも１つを受信することを含む、例１６〜１８の１つの方法。 Example 19: Receiving a fourth input is directed to touch and drag input on a touch-sensitive surface of a handheld electronic device, movement of a touch and handheld electronic device on a touch-sensitive surface, or a feature of a selected virtual object One method of Examples 16-18, comprising receiving at least one of the hand gestures to be made.
例２０：第４の入力に応答して選択された仮想オブジェクトを操作することは、タッチおよびドラッグ入力の軌跡に基づいて選択された仮想オブジェクトを移動させること、ハンドヘルド電子デバイスの移動に基づいて選択された仮想オブジェクトを移動させること、または手のジェスチャに基づいて選択された仮想オブジェクトをさらにモーフィングすることのうちの少なくとも１つを含む、例１６〜１９の１つの方法。 Example 20: Manipulating a selected virtual object in response to a fourth input moves a selected virtual object based on a trajectory of touch and drag input, selects based on movement of a handheld electronic device The method of Examples 16-19, comprising at least one of moving a selected virtual object or further morphing a selected virtual object based on a hand gesture.
例２１：第５の入力を受信することと、第５の入力の優先順位値と第４の入力の優先順位値とを比較することと、第５の入力の優先順位値が第４の入力の優先順位値より大きいとき、現在選択されている仮想オブジェクトの選択を解放し、選択を、第５の入力に関連付けられる新たな仮想オブジェクトにシフトすることと、第５の入力の優先順位値が第４の入力の優先順位値より小さいとき、現在選択されている仮想オブジェクトの選択を維持することとをさらに備える、例１６〜１８の１つの方法。 Example 21: Receiving the fifth input, comparing the priority value of the fifth input with the priority value of the fourth input, and the priority value of the fifth input being the fourth input Release the currently selected virtual object selection, shift the selection to a new virtual object associated with the fifth input, and the priority value of the fifth input is The method of Examples 16-18, further comprising maintaining a selection of the currently selected virtual object when less than a priority value of the fourth input.
例２２：各入力は、複数の入力モードのうちの１つを介して受信され、複数の入力モードの各々は、関連付けられる優先順位値を有し、複数の入力モードの各々に関連付けられるそれぞれの優先順位値は、予め設定されるかまたはユーザプロファイルに従ってユーザによって設定される、例１６〜２１の１つの方法。 Example 22: Each input is received via one of a plurality of input modes, each of the plurality of input modes having a priority value associated therewith, each associated with each of the plurality of input modes. The method of Examples 16-21, wherein the priority value is preset or set by the user according to a user profile.
例２３：複数の入力モードは、頭部での注視入力モード、目での注視入力モード、ポイント入力モード、リーチ入力モード、およびジェスチャ入力モードを含む、例１６〜２２の１つの方法。 Example 23: The method of Examples 16-22, wherein the plurality of input modes include a gaze input mode with a head, a gaze input mode with an eye, a point input mode, a reach input mode, and a gesture input mode.
例２４：ハンドヘルド電子デバイスと動作可能に結合される頭部装着型電子デバイスと、頭部装着型電子デバイスおよびハンドヘルド電子デバイスの動作を制御する制御システムとを備え、制御システムは、頭部装着型電子デバイスおよびハンドヘルド電子デバイスを制御して、複数の入力モードに応答して各々が選択可能かつ操作可能である複数の仮想オブジェクトを含む没入型仮想体験を開始し、複数の入力モードのうちの第１の入力モードを実施する第１の入力を受信し、第１の入力に応答して複数の仮想オブジェクトのうちの第１の仮想オブジェクトを選択し、複数の入力モードのうちの第２の入力モードを実施する第２の入力を受信し、第２の入力モードの優先順位値と第１の入力モードの優先順位値とを比較し、比較に基づいて、第２の入力に応答して、第１の仮想オブジェクトの選択を解放し、選択を複数の仮想オブジェクトのうちの第２の仮想オブジェクトにシフトするか、または第１の仮想オブジェクトの選択を維持するよう構成される、例２４に従う装置。 Example 24: A head mounted electronic device operably coupled to a handheld electronic device, and a control system that controls the operation of the head mounted electronic device and the handheld electronic device, the control system being a head mounted electronic device Control the electronic device and the handheld electronic device to initiate an immersive virtual experience that includes a plurality of virtual objects that are each selectable and operable in response to a plurality of input modes. Receiving a first input that implements one input mode, selecting a first virtual object of the plurality of virtual objects in response to the first input, and selecting a second input of the plurality of input modes; Receiving a second input implementing the mode, comparing the priority value of the second input mode with the priority value of the first input mode, and based on the comparison Responsive to the second input, releases the selection of the first virtual object and shifts the selection to a second virtual object of the plurality of virtual objects or maintains the selection of the first virtual object An apparatus according to Example 24, configured as follows:
例２５：複数の入力モードの各々は、関連付けられる優先順位値を有し、複数の入力モードの各々に関連付けられるそれぞれの優先順位値は、予め設定されるかまたはユーザプロファイルに従ってユーザによって設定され、制御システムはさらに、第２の入力の優先順位値が第１の入力の優先順位値よりも大きい場合に、第１の仮想オブジェクトの選択を解放し、選択を第２の仮想オブジェクトにシフトし、第２の入力の優先順位値が第１の入力の優先順位値よりも小さい場合に、第１の仮想オブジェクトの選択を維持するよう構成される、例２４の装置。 Example 25: Each of the plurality of input modes has an associated priority value, and each priority value associated with each of the plurality of input modes is preset or set by a user according to a user profile, The control system further releases the selection of the first virtual object and shifts the selection to the second virtual object if the priority value of the second input is greater than the priority value of the first input; The apparatus of example 24, configured to maintain selection of the first virtual object when the priority value of the second input is less than the priority value of the first input.
例２６：制御システムはさらに、複数の入力モードのうちの第３の入力モードを実施する第３の入力を受信するよう構成され、第３の入力は、第１の仮想オブジェクトを含む複数の仮想オブジェクトのファーフィールドディスプレイに向けられる注視入力を含み、制御システムはさらに、複数の入力モードのうちの第４の入力モードを実施する第４の入力を受信するよう構成され、第４の入力は、ファーフィールドディスプレイからニアフィールドディスプレイへの複数の仮想オブジェクトの動的切り替えのトリガとなるジェスチャを含み、制御システムはさらに、複数の仮想オブジェクトのニアフィールド表示のために複数の仮想オブジェクトの外観をモーフィングし、モーフィングされた複数の仮想オブジェクトをニアフィールドディスプレイに表示するよう構成される、例２４または２５の装置。 Example 26: The control system is further configured to receive a third input that implements a third input mode of the plurality of input modes, the third input including a plurality of virtual objects including the first virtual object. Including a gaze input directed to the far field display of the object, the control system is further configured to receive a fourth input that implements a fourth input mode of the plurality of input modes, the fourth input comprising: Includes gestures that trigger dynamic switching of multiple virtual objects from a far-field display to a near-field display, and the control system further morphs the appearance of multiple virtual objects for near-field display of multiple virtual objects. Multiple near morphed virtual objects Configured to display a ray apparatus of Example 24 or 25.
例２７：実行時にコンピューティングデバイスにプロセスを実行させる命令を格納するコンピュータ可読記憶媒体であって、命令は、複数の入力モードに応答して各々が選択可能かつ操作可能である複数の仮想オブジェクトを含む没入型仮想体験を開始し、複数の入力モードのうちの第１の入力モードを実施する第１の入力を受信し、第１の入力に応答して複数の仮想オブジェクトのうちの第１の仮想オブジェクトを選択し、複数の入力モードのうちの第２の入力モードを実施する第２の入力を受信し、第２の入力モードの優先順位値と第１の入力モードの優先順位値とを比較し、比較に基づいて、第２の入力に応答して、第１の仮想オブジェクトの選択を解放し、選択を複数の仮想オブジェクトのうちの第２の仮想オブジェクトにシフトするか、または第１の仮想オブジェクトの選択を維持するための命令を含む、例２７に従うコンピュータ可読記憶媒体。 Example 27: A computer readable storage medium that stores instructions that cause a computing device to perform a process at runtime, the instructions comprising a plurality of virtual objects each selectable and operable in response to a plurality of input modes. Start an immersive virtual experience, receive a first input that implements a first input mode of the plurality of input modes, and respond to the first input with a first of the plurality of virtual objects A virtual object is selected, a second input that implements a second input mode of the plurality of input modes is received, and a priority value of the second input mode and a priority value of the first input mode are obtained. Compare, and based on the comparison, in response to the second input, releases the selection of the first virtual object and shifts the selection to the second virtual object of the plurality of virtual objects Or it includes instructions for maintaining the selection of the first virtual object, a computer readable storage medium according to Example 27.
例２８：複数の入力モードの各々は、関連付けられる優先順位値を有し、複数の入力モードの各々に関連付けられるそれぞれの優先順位値は、予め設定されるかまたはユーザプロファイルに従ってユーザによって設定される、例２６または２７のコンピュータ可読記憶媒体。 Example 28: Each of the plurality of input modes has a priority value associated therewith, and each priority value associated with each of the plurality of input modes is preset or set by the user according to a user profile The computer-readable storage medium of Example 26 or 27.
例２９：第２の入力の優先順位値が第１の入力の優先順位値よりも大きい場合に、第１の仮想オブジェクトの選択を解放し、第２の仮想オブジェクトにシフトし、第２の入力の優先順位値が第１の入力の優先順位値よりも小さい場合に、第１の仮想オブジェクトの選択を維持する、例２６、２７または２８のコンピュータ可読記憶媒体。 Example 29: When the priority value of the second input is greater than the priority value of the first input, the selection of the first virtual object is released, shifted to the second virtual object, and the second input The computer-readable storage medium of example 26, 27, or 28, wherein the selection of the first virtual object is maintained when the priority value of is lower than the priority value of the first input.
例３０：命令はさらに、複数の入力モードのうちの第３の入力モードを実施する第３の入力を受信するための命令を含み、第３の入力は、複数の仮想オブジェクトのファーフィールドディスプレイに向けられ、命令はさらに、複数の入力モードのうちの第４の入力モードを実施する第４の入力を受信するための命令を含み、第４の入力は、ファーフィールドディスプレイからニアフィールドディスプレイへの複数の仮想オブジェクトの動的切り替えのトリガとなり、命令はさらに、複数の仮想オブジェクトのニアフィールド表示のために複数の仮想オブジェクトの外観をモーフィングし、モーフィングされた複数の仮想オブジェクトをニアフィールドディスプレイに表示するための命令を含む、例２６、２７、２８または２９のコンピュータ可読記憶媒体。 Example 30: The instructions further include an instruction for receiving a third input that implements a third input mode of the plurality of input modes, the third input being in a far field display of the plurality of virtual objects. The instructions further include instructions for receiving a fourth input that implements a fourth input mode of the plurality of input modes, the fourth input from the far field display to the near field display. Triggers the dynamic switching of multiple virtual objects, and the instruction further morphs the appearance of multiple virtual objects for near field display of multiple virtual objects and displays the morphed virtual objects on the near field display The computer of example 26, 27, 28, or 29, including instructions for Loaded storage medium.
例３１：複数の仮想オブジェクトのニアフィールド表示のために複数の仮想オブジェクトのモーフィングされた外観は、複数の仮想オブジェクトがファーフィールドディスプレイからニアフィールドディスプレイに移動する際に、ユーザから複数の仮想オブジェクトへの仮想距離の減少と、複数の仮想オブジェクトのサイズの変更、複数の仮想オブジェクトに関連付けられる画像の変更、複数の仮想オブジェクトに関連付けられるテキストの変更、または複数の仮想オブジェクトに関連付けられるメニューアイテムの変更のうちの少なくとも１つとを含む、例２６〜３０の１つのコンピュータ可読記憶媒体。 Example 31: The morphed appearance of multiple virtual objects for near field display of multiple virtual objects is from a user to multiple virtual objects as they move from a far field display to a near field display. Decrease the virtual distance and change the size of multiple virtual objects, change the image associated with multiple virtual objects, change the text associated with multiple virtual objects, or change the menu items associated with multiple virtual objects One of the computer readable storage media of Examples 26-30, including at least one of:
記載された実現例の特定の特徴が本明細書に記載されているように示されているが、当業者には多くの修正、置換、変更および均等物がここで思い浮かぶであろう。したがって、特許請求の範囲は、実現例の範囲内にあるすべてのそのような修正および変更を包含するように意図されていることを理解されたい。それらは限定ではなく単なる例として提示されたものであり、形態および細部の様々な変更がなされ得ることを理解されたい。相互排他的な組み合わせを除いて、本明細書に記載の装置および／または方法の任意の部分を、任意の組み合わせで組み合わせることができる。本明細書で説明される実現例は、説明される異なる実現例の機能、構成要素、および／または特徴の様々な組合せおよび／または下位組合せを含むことができる。 While specific features of the described implementation are shown as described herein, many modifications, substitutions, changes and equivalents will occur to those skilled in the art. Therefore, it is to be understood that the claims are intended to cover all such modifications and changes that fall within the scope of the implementation. It should be understood that they are presented by way of example only and not limitation, and that various changes in form and detail may be made. Except for mutually exclusive combinations, any part of the devices and / or methods described herein may be combined in any combination. Implementations described herein can include various combinations and / or subcombinations of functions, components, and / or features of the different implementations described.
Claims (31)
前記複数の入力モードのうちの第１の入力モードを実施する第１の入力を受信することと、
前記第１の入力に応答して前記複数の仮想オブジェクトのうちの第１の仮想オブジェクトを選択することと、
前記複数の入力モードのうちの第２の入力モードを実施する第２の入力を受信することと、
前記第２の入力モードの優先順位値と前記第１の入力モードの優先順位値とを比較することと、
前記比較に基づいて、前記第２の入力に応答して、前記第１の仮想オブジェクトの選択を解放し、選択を前記複数の仮想オブジェクトのうちの第２の仮想オブジェクトにシフトするか、または前記第１の仮想オブジェクトの選択を維持することとを備える、方法。 Initiating an immersive virtual experience including a plurality of virtual objects, each of the plurality of virtual objects being selectable and operable in response to a plurality of input modes;
Receiving a first input implementing a first input mode of the plurality of input modes;
Selecting a first virtual object of the plurality of virtual objects in response to the first input;
Receiving a second input implementing a second input mode of the plurality of input modes;
Comparing the priority value of the second input mode with the priority value of the first input mode;
Based on the comparison, in response to the second input, release the selection of the first virtual object and shift the selection to a second virtual object of the plurality of virtual objects, or Maintaining the selection of the first virtual object.
前記第３の入力に応答して、前記選択された第１の仮想オブジェクトを操作することとをさらに備える、請求項１に記載の方法。 Receiving a third input implementing a third input mode of the plurality of input modes;
The method of claim 1, further comprising manipulating the selected first virtual object in response to the third input.
ハンドヘルド電子デバイスのタッチセンシティブ面上でタッチ入力を受信することと、
前記タッチ入力に応答して、前記選択された第１の仮想オブジェクトを移動させることとを含む、請求項７に記載の方法。 Receiving the third input is
Receiving touch input on a touch-sensitive surface of a handheld electronic device;
The method of claim 7, comprising moving the selected first virtual object in response to the touch input.
前記第１の仮想オブジェクトを含む前記複数の仮想オブジェクトのファーフィールドディスプレイで注視入力を受信することを含み、
第３の入力を受信することは、
ジェスチャ入力を受信することと、
前記ファーフィールドディスプレイからニアフィールドディスプレイへの前記複数の仮想オブジェクトの動的切り替えのためのトリガとして前記ジェスチャ入力を認識することと、
前記複数の仮想オブジェクトのニアフィールド表示のために前記複数の仮想オブジェクトの外観をモーフィングすることと、
前記モーフィングされた複数の仮想オブジェクトを前記ニアフィールドディスプレイに表示することとを含む、請求項７に記載の方法。 Receiving a gaze input directed to the first virtual object to select the first virtual object;
Receiving a gaze input at a far field display of the plurality of virtual objects including the first virtual object;
Receiving the third input is
Receiving gesture input;
Recognizing the gesture input as a trigger for dynamic switching of the plurality of virtual objects from the far field display to a near field display;
Morphing the appearance of the plurality of virtual objects for near-field display of the plurality of virtual objects;
8. The method of claim 7, comprising displaying the morphed virtual objects on the near field display.
ハンドヘルド電子デバイスのタッチセンシティブ面上でタッチ入力を受信することと、
前記タッチ入力を前記タッチセンシティブ面に沿ってドラッグすることと、
前記タッチおよびドラッグ入力に応答して、前記選択された第１の仮想オブジェクトを移動させることとを含み、前記選択された第１の仮想オブジェクトの移動は、前記ハンドヘルド電子装置の前記タッチセンシティブ面に沿った前記ドラッグ入力の移動に対応する、請求項１３に記載の方法。 Receiving the third input is
Receiving touch input on a touch-sensitive surface of a handheld electronic device;
Dragging the touch input along the touch-sensitive surface;
Moving the selected first virtual object in response to the touch and drag input, the movement of the selected first virtual object on the touch sensitive surface of the handheld electronic device. The method of claim 13, corresponding to movement of the drag input along.
ハンドヘルド電子デバイスによって生成される前記ビームを前記ハンドヘルド電子デバイスと前記選択された第１の仮想オブジェクトとの間で接続するために、前記ハンドヘルド電子デバイスのタッチセンシティブ面上でタッチ入力を受信することと、
前記ハンドヘルド電子デバイスを移動させることと、
前記ハンドヘルド電子デバイスの前記移動に応答して、前記選択された第１の仮想オブジェクトを移動させることとを含み、前記選択された第１の仮想オブジェクトの移動は、前記ハンドヘルド電子デバイスの移動に対応し、前記第３の入力はさらに、
前記ハンドヘルド電子デバイスの前記タッチセンシティブ面から前記タッチ入力を解放することと、
前記選択された第１の仮想オブジェクトを前記タッチ入力の解放ポイントに再配置するよう、前記タッチ入力の解放に応答して、前記選択された第１の仮想オブジェクトを解放することとを含む、請求項１３に記載の方法。 The third input is
Receiving touch input on a touch-sensitive surface of the handheld electronic device to connect the beam generated by the handheld electronic device between the handheld electronic device and the selected first virtual object; ,
Moving the handheld electronic device;
Moving the selected first virtual object in response to the movement of the handheld electronic device, wherein movement of the selected first virtual object corresponds to movement of the handheld electronic device And the third input is further
Releasing the touch input from the touch-sensitive surface of the handheld electronic device;
Releasing the selected first virtual object in response to releasing the touch input to relocate the selected first virtual object to the release point of the touch input. Item 14. The method according to Item 13.
複数の仮想オブジェクトのファーフィールドディスプレイを生成することと、
第１の入力を受信することと、
前記第１の入力に応答して前記複数の仮想オブジェクトを含むファーフィールドディスプレイを選択することと、
第２の入力を受信することと、
前記第２の入力に応答して前記複数の仮想オブジェクトのニアフィールド表示のために前記複数の仮想オブジェクトの外観をモーフィングすることと、
前記モーフィングされた複数の仮想オブジェクトを含む前記ニアフィールドディスプレイを生成することと、
第３の入力を受信することと、
前記第３の入力に応答して前記モーフィングされた複数の仮想オブジェクトのうちの１つを選択することとを備える、方法。 Starting an immersive virtual experience,
Creating a far-field display of multiple virtual objects;
Receiving a first input;
Selecting a far field display including the plurality of virtual objects in response to the first input;
Receiving a second input;
Morphing the appearance of the plurality of virtual objects for near field display of the plurality of virtual objects in response to the second input;
Generating the near field display including the plurality of morphed virtual objects;
Receiving a third input;
Selecting one of the morphed virtual objects in response to the third input.
第２の入力を受信することは、前記複数の仮想オブジェクトの前記ファーフィールドディスプレイから前記モーフィングされた複数の仮想オブジェクトの前記ニアフィールドディスプレイへの遷移のトリガとなる手または腕ジェスチャ入力を受信することを含み、
第３の入力を受信することは、モーフィングされた仮想オブジェクトの前記ニアフィールドディスプレイから仮想オブジェクトを選択するニアフィールド手ジェスチャまたはニアフィールドリーチジェスチャを受信することを含む、請求項１６に記載の方法。 Receiving a first input includes receiving a far-field gaze input or a far-field beam input generated by a handheld electronic device;
Receiving a second input includes receiving a hand or arm gesture input that triggers a transition from the far field display of the plurality of virtual objects to the near field display of the morphed virtual objects. Including
The method of claim 16, wherein receiving a third input comprises receiving a near field hand gesture or a near field reach gesture that selects a virtual object from the near field display of a morphed virtual object.
前記第４の入力に応答して、前記選択された仮想オブジェクトを操作することとをさらに備える、請求項１７に記載の方法。 Receiving a fourth input;
The method of claim 17, further comprising manipulating the selected virtual object in response to the fourth input.
前記第５の入力モードの優先順位値と前記第４の入力モードの優先順位値とを比較することと、
前記第５の入力の優先順位値が前記第４の入力の優先順位値より大きいとき、現在選択されている仮想オブジェクトの選択を解放し、選択を、前記第５の入力に関連付けられる新たな仮想オブジェクトにシフトすることと、
前記第５の入力の優先順位値が前記第４の入力の優先順位値より小さいとき、現在選択されている仮想オブジェクトの選択を維持することとをさらに備える、請求項１８に記載の方法。 Receiving a fifth input;
Comparing the priority value of the fifth input mode with the priority value of the fourth input mode;
When the priority value of the fifth input is greater than the priority value of the fourth input, the selection of the currently selected virtual object is released and a new virtual object associated with the fifth input is released. Shifting to objects,
19. The method of claim 18, further comprising maintaining a selection of a currently selected virtual object when the fifth input priority value is less than the fourth input priority value.
前記頭部装着型電子デバイスおよび前記ハンドヘルド電子デバイスの動作を制御する制御システムとを備え、前記制御システムは、前記頭部装着型電子デバイスおよび前記ハンドヘルド電子デバイスを制御して、
複数の入力モードに応答して各々が選択可能かつ操作可能である複数の仮想オブジェクトを含む没入型仮想体験を開始し、
前記複数の入力モードのうちの第１の入力モードを実施する第１の入力を受信し、
前記第１の入力に応答して前記複数の仮想オブジェクトのうちの第１の仮想オブジェクトを選択し、
前記複数の入力モードのうちの第２の入力モードを実施する第２の入力を受信し、
前記第２の入力モードの優先順位値と前記第１の入力モードの優先順位値とを比較し、
前記比較に基づいて、前記第２の入力に応答して、前記第１の仮想オブジェクトの選択を解放し、選択を前記複数の仮想オブジェクトのうちの第２の仮想オブジェクトにシフトするか、または前記第１の仮想オブジェクトの選択を維持するよう構成される、装置。 A head mounted electronic device operably coupled to the handheld electronic device;
A control system that controls operations of the head-mounted electronic device and the handheld electronic device, the control system controlling the head mounted electronic device and the handheld electronic device,
Initiate an immersive virtual experience that includes multiple virtual objects, each selectable and operable in response to multiple input modes,
Receiving a first input that implements a first input mode of the plurality of input modes;
Selecting a first virtual object of the plurality of virtual objects in response to the first input;
Receiving a second input implementing a second input mode of the plurality of input modes;
Comparing the priority value of the second input mode with the priority value of the first input mode;
Based on the comparison, in response to the second input, release the selection of the first virtual object and shift the selection to a second virtual object of the plurality of virtual objects, or An apparatus configured to maintain selection of a first virtual object.
前記第２の入力の優先順位値が前記第１の入力の優先順位値よりも大きい場合に、前記第１の仮想オブジェクトの選択を解放し、選択を前記第２の仮想オブジェクトにシフトし、
前記第２の入力の優先順位値が前記第１の入力の優先順位値よりも小さい場合に、前記第１の仮想オブジェクトの選択を維持するよう構成される、請求項２４に記載の装置。 Each of the plurality of input modes has a priority value associated therewith, and each priority value associated with each of the plurality of input modes is preset or set by a user according to a user profile, The control system further
If the priority value of the second input is greater than the priority value of the first input, release the selection of the first virtual object and shift the selection to the second virtual object;
25. The apparatus of claim 24, configured to maintain a selection of the first virtual object when a priority value of the second input is less than a priority value of the first input.
前記複数の入力モードのうちの第３の入力モードを実施する第３の入力を受信するよう構成され、前記第３の入力は、前記第１の仮想オブジェクトを含む前記複数の仮想オブジェクトのファーフィールドディスプレイに向けられる注視入力を含み、前記制御システムはさらに、
前記複数の入力モードのうちの第４の入力モードを実施する第４の入力を受信するよう構成され、前記第４の入力は、前記ファーフィールドディスプレイからニアフィールドディスプレイへの前記複数の仮想オブジェクトの動的切り替えのトリガとなるジェスチャを含み、前記制御システムはさらに、
前記複数の仮想オブジェクトのニアフィールド表示のために前記複数の仮想オブジェクトの外観をモーフィングし、
前記モーフィングされた複数の仮想オブジェクトを前記ニアフィールドディスプレイに表示するよう構成される、請求項２４に記載の装置。 The control system further includes
The third input is configured to receive a third input that implements a third input mode of the plurality of input modes, and the third input is a far field of the plurality of virtual objects including the first virtual object. Including a gaze input directed at a display, the control system further comprising:
The fourth input is configured to receive a fourth input that implements a fourth input mode of the plurality of input modes, the fourth input of the plurality of virtual objects from the far field display to the near field display. Including a gesture that triggers dynamic switching, the control system further comprising:
Morphing the appearance of the plurality of virtual objects for near field display of the plurality of virtual objects;
25. The apparatus of claim 24, configured to display the morphed virtual objects on the near field display.
複数の入力モードに応答して各々が選択可能かつ操作可能である複数の仮想オブジェクトを含む没入型仮想体験を開始し、
前記複数の入力モードのうちの第１の入力モードを実施する第１の入力を受信し、
前記第１の入力に応答して前記複数の仮想オブジェクトのうちの第１の仮想オブジェクトを選択し、
前記複数の入力モードのうちの第２の入力モードを実施する第２の入力を受信し、
前記第２の入力モードの優先順位値と前記第１の入力モードの優先順位値とを比較し、
前記比較に基づいて、前記第２の入力に応答して、前記第１の仮想オブジェクトの選択を解放し、選択を前記複数の仮想オブジェクトのうちの第２の仮想オブジェクトにシフトするか、または前記第１の仮想オブジェクトの選択を維持するための命令を含む、コンピュータ可読記憶媒体。 A computer readable storage medium that stores instructions that, when executed, cause a computing device to perform a process, the instructions comprising:
Initiate an immersive virtual experience that includes multiple virtual objects, each selectable and operable in response to multiple input modes,
Receiving a first input that implements a first input mode of the plurality of input modes;
Selecting a first virtual object of the plurality of virtual objects in response to the first input;
Receiving a second input implementing a second input mode of the plurality of input modes;
Comparing the priority value of the second input mode with the priority value of the first input mode;
Based on the comparison, in response to the second input, release the selection of the first virtual object and shift the selection to a second virtual object of the plurality of virtual objects, or A computer readable storage medium comprising instructions for maintaining a selection of a first virtual object.
前記複数の入力モードのうちの第３の入力モードを実施する第３の入力を受信するための命令を含み、前記第３の入力は、前記複数の仮想オブジェクトのファーフィールドディスプレイに向けられ、前記命令はさらに、
前記複数の入力モードのうちの第４の入力モードを実施する第４の入力を受信するための命令を含み、前記第４の入力は、前記ファーフィールドディスプレイからニアフィールドディスプレイへの前記複数の仮想オブジェクトの動的切り替えのトリガとなり、前記命令はさらに、
前記複数の仮想オブジェクトのニアフィールド表示のために前記複数の仮想オブジェクトの外観をモーフィングし、
前記モーフィングされた複数の仮想オブジェクトを前記ニアフィールドディスプレイに表示するための命令を含む、請求項２６に記載のコンピュータ可読記憶媒体。 The instructions further include
Including instructions for receiving a third input that implements a third input mode of the plurality of input modes, the third input directed to a far field display of the plurality of virtual objects; The instruction is further
Instructions for receiving a fourth input that implements a fourth input mode of the plurality of input modes, wherein the fourth input is the plurality of virtual from the far field display to the near field display; Triggering dynamic switching of objects, the instruction further
Morphing the appearance of the plurality of virtual objects for near field display of the plurality of virtual objects;
27. The computer readable storage medium of claim 26, comprising instructions for displaying the morphed virtual objects on the near field display.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/836,311 US10101803B2 (en) | 2015-08-26 | 2015-08-26 | Dynamic switching and merging of head, gesture and touch input in virtual reality |
US14/836,311 | 2015-08-26 | ||
PCT/US2016/039806 WO2017034667A1 (en) | 2015-08-26 | 2016-06-28 | Dynamic switching and merging of head, gesture and touch input in virtual reality |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2019218129A Division JP7008681B2 (en) | 2015-08-26 | 2019-12-02 | Dynamic switching and merging of heads, gestures, and touch inputs in virtual reality |
Publications (1)
Publication Number | Publication Date |
---|---|
JP2018517967A true JP2018517967A (en) | 2018-07-05 |
Family
ID=56409218
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2017555336A Pending JP2018517967A (en) | 2015-08-26 | 2016-06-28 | Dynamic switching and merging of head, gesture, and touch input in virtual reality |
JP2019218129A Active JP7008681B2 (en) | 2015-08-26 | 2019-12-02 | Dynamic switching and merging of heads, gestures, and touch inputs in virtual reality |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2019218129A Active JP7008681B2 (en) | 2015-08-26 | 2019-12-02 | Dynamic switching and merging of heads, gestures, and touch inputs in virtual reality |
Country Status (6)
Country | Link |
---|---|
US (2) | US10101803B2 (en) |
EP (1) | EP3341815B1 (en) |
JP (2) | JP2018517967A (en) |
KR (1) | KR102003493B1 (en) |
CN (1) | CN107533374B (en) |
WO (1) | WO2017034667A1 (en) |
Cited By (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2020031795A1 (en) * | 2018-08-07 | 2020-02-13 | ソニー株式会社 | Information processing device, information processing method, and program |
JP2020187668A (en) * | 2019-05-17 | 2020-11-19 | 株式会社電通グループ | Display control method, display control apparatus, display control program, and display control system |
JP2022121592A (en) * | 2017-04-19 | 2022-08-19 | マジック リープ， インコーポレイテッド | Multimode execution and text editing for wearable system |
Families Citing this family (107)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20130339859A1 (en) | 2012-06-15 | 2013-12-19 | Muzik LLC | Interactive networked headphones |
JP6308214B2 (en) * | 2013-05-15 | 2018-04-11 | ソニー株式会社 | Display control device, display control method, and recording medium |
US10514755B2 (en) * | 2015-05-08 | 2019-12-24 | Lg Electronics Inc. | Glasses-type terminal and control method therefor |
US10101803B2 (en) * | 2015-08-26 | 2018-10-16 | Google Llc | Dynamic switching and merging of head, gesture and touch input in virtual reality |
JP6174646B2 (en) * | 2015-09-24 | 2017-08-02 | 株式会社コロプラ | Computer program for 3-axis operation of objects in virtual space |
KR102641655B1 (en) * | 2015-10-20 | 2024-02-27 | 매직 립, 인코포레이티드 | Select virtual objects in 3D space |
JP6597235B2 (en) * | 2015-11-30 | 2019-10-30 | 富士通株式会社 | Image processing apparatus, image processing method, and image processing program |
US10133407B2 (en) * | 2015-12-28 | 2018-11-20 | Seiko Epson Corporation | Display apparatus, display system, method for controlling display apparatus, and program |
US20170185261A1 (en) * | 2015-12-28 | 2017-06-29 | Htc Corporation | Virtual reality device, method for virtual reality |
US10908694B2 (en) * | 2016-02-01 | 2021-02-02 | Microsoft Technology Licensing, Llc | Object motion tracking with remote device |
US9983697B1 (en) * | 2016-05-18 | 2018-05-29 | Meta Company | System and method for facilitating virtual interactions with a three-dimensional virtual environment in response to sensor input into a control device having sensors |
US10303323B2 (en) | 2016-05-18 | 2019-05-28 | Meta Company | System and method for facilitating user interaction with a three-dimensional virtual environment in response to user input into a control device having a graphical interface |
US10255658B2 (en) | 2016-08-09 | 2019-04-09 | Colopl, Inc. | Information processing method and program for executing the information processing method on computer |
RU2677566C1 (en) * | 2016-08-30 | 2019-01-17 | Бейдзин Сяоми Мобайл Софтвэр Ко., Лтд. | Method, device and electronic equipment for virtual reality managing |
US10888773B2 (en) | 2016-10-11 | 2021-01-12 | Valve Corporation | Force sensing resistor (FSR) with polyimide substrate, systems, and methods thereof |
US11625898B2 (en) | 2016-10-11 | 2023-04-11 | Valve Corporation | Holding and releasing virtual objects |
US10307669B2 (en) | 2016-10-11 | 2019-06-04 | Valve Corporation | Electronic controller with finger sensing and an adjustable hand retainer |
US20180129274A1 (en) * | 2016-10-18 | 2018-05-10 | Colopl, Inc. | Information processing method and apparatus, and program for executing the information processing method on computer |
JP6653489B2 (en) * | 2016-12-16 | 2020-02-26 | パナソニックＩｐマネジメント株式会社 | Input device and input method |
US10936872B2 (en) | 2016-12-23 | 2021-03-02 | Realwear, Inc. | Hands-free contextually aware object interaction for wearable display |
US10393312B2 (en) | 2016-12-23 | 2019-08-27 | Realwear, Inc. | Articulating components for a head-mounted display |
US10620910B2 (en) | 2016-12-23 | 2020-04-14 | Realwear, Inc. | Hands-free navigation of touch-based operating systems |
US11099716B2 (en) | 2016-12-23 | 2021-08-24 | Realwear, Inc. | Context based content navigation for wearable display |
US10437070B2 (en) | 2016-12-23 | 2019-10-08 | Realwear, Inc. | Interchangeable optics for a head-mounted display |
US11507216B2 (en) | 2016-12-23 | 2022-11-22 | Realwear, Inc. | Customizing user interfaces of binary applications |
US10168789B1 (en) | 2017-05-31 | 2019-01-01 | Meta Company | Systems and methods to facilitate user interactions with virtual content having two-dimensional representations and/or three-dimensional representations |
US10817128B2 (en) * | 2017-07-11 | 2020-10-27 | Logitech Europe S.A. | Input device for VR/AR applications |
US10782793B2 (en) * | 2017-08-10 | 2020-09-22 | Google Llc | Context-sensitive hand interaction |
CN111052042B (en) | 2017-09-29 | 2022-06-07 | 苹果公司 | Gaze-based user interaction |
US10930075B2 (en) * | 2017-10-16 | 2021-02-23 | Microsoft Technology Licensing, Llc | User interface discovery and interaction for three-dimensional virtual environments |
KR102138412B1 (en) * | 2017-10-20 | 2020-07-28 | 한국과학기술원 | Method for managing 3d windows in augmented reality and virtual reality using projective geometry |
KR102029906B1 (en) | 2017-11-10 | 2019-11-08 | 전자부품연구원 | Apparatus and method for providing virtual reality contents of moving means |
US11282133B2 (en) | 2017-11-21 | 2022-03-22 | International Business Machines Corporation | Augmented reality product comparison |
US10565761B2 (en) * | 2017-12-07 | 2020-02-18 | Wayfair Llc | Augmented reality z-stack prioritization |
US20190253700A1 (en) * | 2018-02-15 | 2019-08-15 | Tobii Ab | Systems and methods for calibrating image sensors in wearable apparatuses |
US11488602B2 (en) | 2018-02-20 | 2022-11-01 | Dropbox, Inc. | Meeting transcription using custom lexicons based on document history |
US10467335B2 (en) | 2018-02-20 | 2019-11-05 | Dropbox, Inc. | Automated outline generation of captured meeting audio in a collaborative document context |
WO2019172678A1 (en) * | 2018-03-07 | 2019-09-12 | Samsung Electronics Co., Ltd. | System and method for augmented reality interaction |
US11145096B2 (en) * | 2018-03-07 | 2021-10-12 | Samsung Electronics Co., Ltd. | System and method for augmented reality interaction |
US10831265B2 (en) * | 2018-04-20 | 2020-11-10 | Microsoft Technology Licensing, Llc | Systems and methods for gaze-informed target manipulation |
WO2019204161A1 (en) | 2018-04-20 | 2019-10-24 | Pcms Holdings, Inc. | Method and system for gaze-based control of mixed reality content |
CN112041788B (en) | 2018-05-09 | 2024-05-03 | 苹果公司 | Selecting text input fields using eye gaze |
EP3797345A4 (en) | 2018-05-22 | 2022-03-09 | Magic Leap, Inc. | Transmodal input fusion for a wearable system |
US20190361521A1 (en) * | 2018-05-22 | 2019-11-28 | Microsoft Technology Licensing, Llc | Accelerated gaze-supported manual cursor control |
EP3807746B1 (en) * | 2018-06-20 | 2023-07-26 | Valve Corporation | Holding and releasing virtual objects |
CN112512648A (en) * | 2018-06-20 | 2021-03-16 | 威尔乌集团 | Holding and releasing virtual objects |
US10665206B2 (en) | 2018-07-30 | 2020-05-26 | Honeywell International Inc. | Method and system for user-related multi-screen solution for augmented reality for use in performing maintenance |
US10692299B2 (en) * | 2018-07-31 | 2020-06-23 | Splunk Inc. | Precise manipulation of virtual object position in an extended reality environment |
US10909772B2 (en) | 2018-07-31 | 2021-02-02 | Splunk Inc. | Precise scaling of virtual objects in an extended reality environment |
CN109101110A (en) * | 2018-08-10 | 2018-12-28 | 北京七鑫易维信息技术有限公司 | A kind of method for executing operating instructions, device, user terminal and storage medium |
CN111124236B (en) | 2018-10-30 | 2023-04-28 | 斑马智行网络(香港)有限公司 | Data processing method, device and machine-readable medium |
CN111309142A (en) | 2018-12-11 | 2020-06-19 | 托比股份公司 | Method and device for switching input modality of display device |
US11320911B2 (en) * | 2019-01-11 | 2022-05-03 | Microsoft Technology Licensing, Llc | Hand motion and orientation-aware buttons and grabbable objects in mixed reality |
KR20200091988A (en) | 2019-01-23 | 2020-08-03 | 삼성전자주식회사 | Method for controlling device and electronic device thereof |
US11853533B1 (en) * | 2019-01-31 | 2023-12-26 | Splunk Inc. | Data visualization workspace in an extended reality environment |
US11644940B1 (en) | 2019-01-31 | 2023-05-09 | Splunk Inc. | Data visualization in an extended reality environment |
AU2019428009A1 (en) * | 2019-02-04 | 2021-09-02 | Razer (Asia-Pacific) Pte. Ltd. | Method and apparatus of using a computer touchpad or digitizer stylus pad as a mousepad |
US11297366B2 (en) | 2019-05-22 | 2022-04-05 | Google Llc | Methods, systems, and media for object grouping and manipulation in immersive environments |
US11689379B2 (en) * | 2019-06-24 | 2023-06-27 | Dropbox, Inc. | Generating customized meeting insights based on user interactions and meeting media |
US11151794B1 (en) * | 2019-06-28 | 2021-10-19 | Snap Inc. | Messaging system with augmented reality messages |
CN110308794A (en) * | 2019-07-04 | 2019-10-08 | 郑州大学 | There are two types of the virtual implementing helmet of display pattern and the control methods of display pattern for tool |
CN110502120A (en) * | 2019-08-29 | 2019-11-26 | 广州创幻数码科技有限公司 | It is dynamic to catch the virtual newscaster's system and realization side that data and deliberate action data combine |
US11189099B2 (en) * | 2019-09-20 | 2021-11-30 | Facebook Technologies, Llc | Global and local mode virtual object interactions |
US11176745B2 (en) * | 2019-09-20 | 2021-11-16 | Facebook Technologies, Llc | Projection casting in virtual environments |
US10991163B2 (en) | 2019-09-20 | 2021-04-27 | Facebook Technologies, Llc | Projection casting in virtual environments |
US11170576B2 (en) | 2019-09-20 | 2021-11-09 | Facebook Technologies, Llc | Progressive display of virtual objects |
US11086406B1 (en) | 2019-09-20 | 2021-08-10 | Facebook Technologies, Llc | Three-state gesture virtual controls |
US11086476B2 (en) * | 2019-10-23 | 2021-08-10 | Facebook Technologies, Llc | 3D interactions with web content |
KR20210063928A (en) * | 2019-11-25 | 2021-06-02 | 삼성전자주식회사 | Electronic device for providing augmented reality service and operating method thereof |
JP7433860B2 (en) * | 2019-11-26 | 2024-02-20 | キヤノン株式会社 | Electronic devices and control methods for electronic devices |
US11175730B2 (en) | 2019-12-06 | 2021-11-16 | Facebook Technologies, Llc | Posture-based virtual space configurations |
US11475639B2 (en) | 2020-01-03 | 2022-10-18 | Meta Platforms Technologies, Llc | Self presence in artificial reality |
EP3851939A1 (en) * | 2020-01-14 | 2021-07-21 | Apple Inc. | Positioning a user-controlled spatial selector based on extremity tracking information and eye tracking information |
US11257280B1 (en) | 2020-05-28 | 2022-02-22 | Facebook Technologies, Llc | Element-based switching of ray casting rules |
US11256336B2 (en) | 2020-06-29 | 2022-02-22 | Facebook Technologies, Llc | Integration of artificial reality interaction modes |
US11481177B2 (en) | 2020-06-30 | 2022-10-25 | Snap Inc. | Eyewear including multi-user, shared interactive experiences |
US11954268B2 (en) * | 2020-06-30 | 2024-04-09 | Snap Inc. | Augmented reality eyewear 3D painting |
CN111773658B (en) * | 2020-07-03 | 2024-02-23 | 珠海金山数字网络科技有限公司 | Game interaction method and device based on computer vision library |
CN111598273B (en) * | 2020-07-20 | 2020-10-20 | 中国人民解放军国防科技大学 | VR (virtual reality) technology-based maintenance detection method and device for environment-friendly life protection system |
US11249314B1 (en) * | 2020-08-04 | 2022-02-15 | Htc Corporation | Method for switching input devices, head-mounted display and computer readable storage medium |
US11467403B2 (en) * | 2020-08-20 | 2022-10-11 | Htc Corporation | Operating method and electronic system |
US11227445B1 (en) | 2020-08-31 | 2022-01-18 | Facebook Technologies, Llc | Artificial reality augments and surfaces |
US11176755B1 (en) | 2020-08-31 | 2021-11-16 | Facebook Technologies, Llc | Artificial reality augments and surfaces |
US11178376B1 (en) | 2020-09-04 | 2021-11-16 | Facebook Technologies, Llc | Metering for display modes in artificial reality |
US20220148268A1 (en) * | 2020-11-10 | 2022-05-12 | Noderix Teknoloji Sanayi Ticaret Anonim Sirketi | Systems and methods for personalized and interactive extended reality experiences |
US11113893B1 (en) | 2020-11-17 | 2021-09-07 | Facebook Technologies, Llc | Artificial reality environment with glints displayed by an extra reality device |
CN116438503A (en) * | 2020-12-17 | 2023-07-14 | 三星电子株式会社 | Electronic device and operation method thereof |
CN112631424A (en) * | 2020-12-18 | 2021-04-09 | 上海影创信息科技有限公司 | Gesture priority control method and system and VR glasses thereof |
US11461973B2 (en) | 2020-12-22 | 2022-10-04 | Meta Platforms Technologies, Llc | Virtual reality locomotion via hand gesture |
US11409405B1 (en) | 2020-12-22 | 2022-08-09 | Facebook Technologies, Llc | Augment orchestration in an artificial reality environment |
US11327630B1 (en) * | 2021-02-04 | 2022-05-10 | Huawei Technologies Co., Ltd. | Devices, methods, systems, and media for selecting virtual objects for extended reality interaction |
US11294475B1 (en) | 2021-02-08 | 2022-04-05 | Facebook Technologies, Llc | Artificial reality multi-modal input switching model |
KR20220136776A (en) * | 2021-04-01 | 2022-10-11 | 삼성전자주식회사 | Method for providing augmented reality image and head mounted display device supporting the same |
US11278810B1 (en) * | 2021-04-01 | 2022-03-22 | Sony Interactive Entertainment Inc. | Menu placement dictated by user ability and modes of feedback |
US11927756B2 (en) | 2021-04-01 | 2024-03-12 | Samsung Electronics Co., Ltd. | Method for providing augmented reality image and head mounted display device supporting the same |
JP2022163813A (en) * | 2021-04-15 | 2022-10-27 | キヤノン株式会社 | Wearable information terminal, control method for the same, and program |
US11295503B1 (en) | 2021-06-28 | 2022-04-05 | Facebook Technologies, Llc | Interactive avatars in artificial reality |
US11762952B2 (en) | 2021-06-28 | 2023-09-19 | Meta Platforms Technologies, Llc | Artificial reality application lifecycle |
DE102021003490A1 (en) | 2021-07-06 | 2023-01-12 | Mercedes-Benz Group AG | Method for presenting information with a head-up display device, head-up display device and vehicle |
US11748944B2 (en) | 2021-10-27 | 2023-09-05 | Meta Platforms Technologies, Llc | Virtual object structures and interrelationships |
US11798247B2 (en) | 2021-10-27 | 2023-10-24 | Meta Platforms Technologies, Llc | Virtual object structures and interrelationships |
RU210426U1 (en) * | 2021-12-15 | 2022-04-15 | Общество с ограниченной ответственностью "ДАР" | DEVICE FOR AUGMENTED REALITY BROADCASTING |
US20230334808A1 (en) * | 2022-01-12 | 2023-10-19 | Apple Inc. | Methods for displaying, selecting and moving objects and containers in an environment |
US11620000B1 (en) * | 2022-03-31 | 2023-04-04 | Microsoft Technology Licensing, Llc | Controlled invocation of a precision input mode |
US11531448B1 (en) * | 2022-06-01 | 2022-12-20 | VR-EDU, Inc. | Hand control interfaces and methods in virtual reality environments |
JP2024048680A (en) * | 2022-09-28 | 2024-04-09 | キヤノン株式会社 | Control device, control method, and program |
US11947862B1 (en) | 2022-12-30 | 2024-04-02 | Meta Platforms Technologies, Llc | Streaming native application content to artificial reality devices |
Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2000200125A (en) * | 1998-12-30 | 2000-07-18 | Fuji Xerox Co Ltd | Interface |
JP2012108842A (en) * | 2010-11-19 | 2012-06-07 | Konica Minolta Holdings Inc | Display system, display processing device, display method, and display program |
WO2013118373A1 (en) * | 2012-02-10 | 2013-08-15 | ソニー株式会社 | Image processing apparatus, image processing method, and program |
WO2014093608A1 (en) * | 2012-12-13 | 2014-06-19 | Microsoft Corporation | Direct interaction system for mixed reality environments |
JP2015133088A (en) * | 2014-01-16 | 2015-07-23 | カシオ計算機株式会社 | Gui system, display processing device, input processing device, and program |
Family Cites Families (32)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JPH08241163A (en) * | 1995-03-01 | 1996-09-17 | Canon Inc | Information processor |
US6720949B1 (en) * | 1997-08-22 | 2004-04-13 | Timothy R. Pryor | Man machine interfaces and applications |
WO2000008547A1 (en) | 1998-08-05 | 2000-02-17 | British Telecommunications Public Limited Company | Multimodal user interface |
US6243093B1 (en) * | 1998-09-14 | 2001-06-05 | Microsoft Corporation | Methods, apparatus and data structures for providing a user interface, which exploits spatial memory in three-dimensions, to objects and which visually groups matching objects |
US6611253B1 (en) | 2000-09-19 | 2003-08-26 | Harel Cohen | Virtual input environment |
US7095401B2 (en) | 2000-11-02 | 2006-08-22 | Siemens Corporate Research, Inc. | System and method for gesture interface |
US7646394B1 (en) | 2004-03-05 | 2010-01-12 | Hrl Laboratories, Llc | System and method for operating in a virtual environment |
US9772689B2 (en) | 2008-03-04 | 2017-09-26 | Qualcomm Incorporated | Enhanced gesture-based image manipulation |
US20100241955A1 (en) * | 2009-03-23 | 2010-09-23 | Microsoft Corporation | Organization and manipulation of content items on a touch-sensitive display |
US20120113223A1 (en) | 2010-11-05 | 2012-05-10 | Microsoft Corporation | User Interaction in Augmented Reality |
US20130154913A1 (en) | 2010-12-16 | 2013-06-20 | Siemens Corporation | Systems and methods for a gaze and gesture interface |
US9104239B2 (en) * | 2011-03-09 | 2015-08-11 | Lg Electronics Inc. | Display device and method for controlling gesture functions using different depth ranges |
CN102226880A (en) * | 2011-06-03 | 2011-10-26 | 北京新岸线网络技术有限公司 | Somatosensory operation method and system based on virtual reality |
US20130082928A1 (en) * | 2011-09-30 | 2013-04-04 | Seung Wook Kim | Keyboard-based multi-touch input system using a displayed representation of a users hand |
US8990682B1 (en) * | 2011-10-05 | 2015-03-24 | Google Inc. | Methods and devices for rendering interactions between virtual and physical objects on a substantially transparent display |
US9142182B2 (en) | 2011-10-07 | 2015-09-22 | Lg Electronics Inc. | Device and control method thereof |
US9152376B2 (en) | 2011-12-01 | 2015-10-06 | At&T Intellectual Property I, L.P. | System and method for continuous multimodal speech and gesture interaction |
AU2013239179B2 (en) * | 2012-03-26 | 2015-08-20 | Apple Inc. | Enhanced virtual touchpad and touchscreen |
US9552673B2 (en) | 2012-10-17 | 2017-01-24 | Microsoft Technology Licensing, Llc | Grasping virtual objects in augmented reality |
US9791921B2 (en) * | 2013-02-19 | 2017-10-17 | Microsoft Technology Licensing, Llc | Context-aware augmented reality object commands |
KR20140112910A (en) | 2013-03-14 | 2014-09-24 | 삼성전자주식회사 | Input controlling Method and Electronic Device supporting the same |
US20140282275A1 (en) * | 2013-03-15 | 2014-09-18 | Qualcomm Incorporated | Detection of a zooming gesture |
US9443354B2 (en) * | 2013-04-29 | 2016-09-13 | Microsoft Technology Licensing, Llc | Mixed reality interactions |
US10905943B2 (en) | 2013-06-07 | 2021-02-02 | Sony Interactive Entertainment LLC | Systems and methods for reducing hops associated with a head mounted system |
US20140362110A1 (en) | 2013-06-08 | 2014-12-11 | Sony Computer Entertainment Inc. | Systems and methods for customizing optical representation of views provided by a head mounted display based on optical prescription of a user |
US20140368537A1 (en) | 2013-06-18 | 2014-12-18 | Tom G. Salter | Shared and private holographic objects |
KR102039427B1 (en) | 2013-07-01 | 2019-11-27 | 엘지전자 주식회사 | Smart glass |
KR20150024247A (en) * | 2013-08-26 | 2015-03-06 | 삼성전자주식회사 | Method and apparatus for executing application using multiple input tools on touchscreen device |
US9244539B2 (en) | 2014-01-07 | 2016-01-26 | Microsoft Technology Licensing, Llc | Target positioning with gaze tracking |
US9552060B2 (en) | 2014-01-28 | 2017-01-24 | Microsoft Technology Licensing, Llc | Radial selection by vestibulo-ocular reflex fixation |
JP2015149634A (en) * | 2014-02-07 | 2015-08-20 | ソニー株式会社 | Image display device and method |
US10101803B2 (en) * | 2015-08-26 | 2018-10-16 | Google Llc | Dynamic switching and merging of head, gesture and touch input in virtual reality |
-
2015
- 2015-08-26 US US14/836,311 patent/US10101803B2/en active Active
-
2016
- 2016-06-28 KR KR1020177031644A patent/KR102003493B1/en active IP Right Grant
- 2016-06-28 EP EP16738324.9A patent/EP3341815B1/en active Active
- 2016-06-28 WO PCT/US2016/039806 patent/WO2017034667A1/en unknown
- 2016-06-28 CN CN201680025102.6A patent/CN107533374B/en active Active
- 2016-06-28 JP JP2017555336A patent/JP2018517967A/en active Pending
-
2018
- 2018-09-13 US US16/130,040 patent/US10606344B2/en active Active
-
2019
- 2019-12-02 JP JP2019218129A patent/JP7008681B2/en active Active
Patent Citations (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2000200125A (en) * | 1998-12-30 | 2000-07-18 | Fuji Xerox Co Ltd | Interface |
US6570555B1 (en) * | 1998-12-30 | 2003-05-27 | Fuji Xerox Co., Ltd. | Method and apparatus for embodied conversational characters with multimodal input/output in an interface device |
JP2012108842A (en) * | 2010-11-19 | 2012-06-07 | Konica Minolta Holdings Inc | Display system, display processing device, display method, and display program |
WO2013118373A1 (en) * | 2012-02-10 | 2013-08-15 | ソニー株式会社 | Image processing apparatus, image processing method, and program |
WO2014093608A1 (en) * | 2012-12-13 | 2014-06-19 | Microsoft Corporation | Direct interaction system for mixed reality environments |
JP2015133088A (en) * | 2014-01-16 | 2015-07-23 | カシオ計算機株式会社 | Gui system, display processing device, input processing device, and program |
Cited By (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2022121592A (en) * | 2017-04-19 | 2022-08-19 | マジック リープ， インコーポレイテッド | Multimode execution and text editing for wearable system |
JP7336005B2 (en) | 2017-04-19 | 2023-08-30 | マジック リープ， インコーポレイテッド | Multimode execution and text editing for wearable systems |
US11960636B2 (en) | 2017-04-19 | 2024-04-16 | Magic Leap, Inc. | Multimodal task execution and text editing for a wearable system |
WO2020031795A1 (en) * | 2018-08-07 | 2020-02-13 | ソニー株式会社 | Information processing device, information processing method, and program |
JP2020187668A (en) * | 2019-05-17 | 2020-11-19 | 株式会社電通グループ | Display control method, display control apparatus, display control program, and display control system |
JP7291878B2 (en) | 2019-05-17 | 2023-06-16 | 株式会社電通 | Display control method, display control device, display control program and display control system |
Also Published As
Publication number | Publication date |
---|---|
US10101803B2 (en) | 2018-10-16 |
US20190011979A1 (en) | 2019-01-10 |
WO2017034667A1 (en) | 2017-03-02 |
CN107533374B (en) | 2020-12-01 |
JP7008681B2 (en) | 2022-01-25 |
CN107533374A (en) | 2018-01-02 |
EP3341815A1 (en) | 2018-07-04 |
US20170060230A1 (en) | 2017-03-02 |
JP2020061169A (en) | 2020-04-16 |
KR102003493B1 (en) | 2019-07-24 |
US10606344B2 (en) | 2020-03-31 |
EP3341815B1 (en) | 2022-04-13 |
KR20170126508A (en) | 2017-11-17 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
JP7008681B2 (en) | Dynamic switching and merging of heads, gestures, and touch inputs in virtual reality | |
US10509487B2 (en) | Combining gyromouse input and touch input for navigation in an augmented and/or virtual reality environment | |
JP6682623B2 (en) | Teleportation in augmented and / or virtual reality environments | |
CN108292146B (en) | Laser pointer interaction and scaling in virtual reality | |
CN109906424B (en) | Input controller stabilization techniques for virtual reality systems | |
US10545584B2 (en) | Virtual/augmented reality input device | |
US8643951B1 (en) | Graphical menu and interaction therewith through a viewing window | |
JP6535819B2 (en) | Control system for navigation in virtual reality environment | |
KR20170130582A (en) | Hover behavior for gaze interaction in virtual reality | |
KR102251252B1 (en) | Location globe in virtual reality | |
US20230325003A1 (en) | Method of displaying selectable options | |
EP3850468B1 (en) | Snapping range for augmented reality objects | |
JP2024018909A (en) | XR operation function using smart watch |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20171219 |
|
A621 | Written request for application examination |
Free format text: JAPANESE INTERMEDIATE CODE: A621Effective date: 20171219 |
|
A977 | Report on retrieval |
Free format text: JAPANESE INTERMEDIATE CODE: A971007Effective date: 20181015 |
|
A131 | Notification of reasons for refusal |
Free format text: JAPANESE INTERMEDIATE CODE: A131Effective date: 20181127 |
|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20190225 |
|
A02 | Decision of refusal |
Free format text: JAPANESE INTERMEDIATE CODE: A02Effective date: 20190806 |