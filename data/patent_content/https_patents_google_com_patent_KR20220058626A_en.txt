KR20220058626A - Multi-horizontal forecast processing for time series data - Google Patents
Multi-horizontal forecast processing for time series data Download PDFInfo
- Publication number
- KR20220058626A KR20220058626A KR1020227011809A KR20227011809A KR20220058626A KR 20220058626 A KR20220058626 A KR 20220058626A KR 1020227011809 A KR1020227011809 A KR 1020227011809A KR 20227011809 A KR20227011809 A KR 20227011809A KR 20220058626 A KR20220058626 A KR 20220058626A
- Authority
- KR
- South Korea
- Prior art keywords
- time
- layer
- static
- covariates
- sequence
- Prior art date
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G06N3/0445—
-
- G06N3/0454—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/049—Temporal neural networks, e.g. delay elements, oscillating neurons or pulsed inputs
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
Abstract
시계열 데이터에 관해 다중 수평 예보를 수행하기 위한 컴퓨터 저장 매체를 포함하는 방법들, 시스템들, 및 장치들. 방법은 하나 이상의 시간 단계들의 각각의 예보 수평들에 대한 단기 시간 특성들을 결정하는 단계를 포함한다. 결정하는 단계는 RNN 인코더들을 사용하여, 정적 공변량들, 및 시변 입력 데이터에 기초하여 인코더 벡터들을 생성하는 단계; 및 하나 이상의 RNN 디코더들을 사용하여, 각각의 미래의 시간 기간에 대한 단기 패턴을 예측하는 단계를 포함할 수 있다. 방법은 또한, 정적 공변량들, 각각의 과거의 시간 기간들 동안에 캡처된 시변 입력 데이터, 및 시변 공지 미래 입력 데이터에 기초하여, 각각의 예보 수평들에 대한 장기 시간 특성들을 캡처하는 단계를 포함할 수 있다.Methods, systems, and apparatus comprising a computer storage medium for performing multiple horizontal forecasting on time series data. The method includes determining short-term temporal characteristics for respective forecast levels of one or more time steps. Determining may include using RNN encoders to generate encoder vectors based on static covariates, and time-varying input data; and predicting, using one or more RNN decoders, a short-term pattern for each future time period. The method may also include capturing long-term temporal characteristics for each of the forecast horizons based on the static covariates, the time-varying input data captured during each past time periods, and the time-varying known future input data. there is.
Description
[1] 본 출원은, 2019년 12월 18일자로 출원되었고 발명의 명칭이 "TEMPORAL FUSION TRANSFORMERS FOR INTERPRETABLE MULTI-HORIZON TIME SERIES FORECASTING"인 미국 가출원 제62/949,904호의 출원일의 이익을 주장하며, 이로써 개시내용은 본원에 인용에 의해 포함된다. [1] This application claims the benefit of filing date of U.S. Provisional Application No. 62/949,904, filed December 18, 2019 and entitled "TEMPORAL FUSION TRANSFORMERS FOR INTERPRETABLE MULTI-HORIZON TIME SERIES FORECASTING", hereby disclosed The content is incorporated herein by reference.
[2] 신경망들은, 각각이 하나 이상의 입력을 처리하고 하나 이상의 출력을 생성하도록 구성된 더 작은 모델들의 2 개 이상의 계층(또는 "계층")으로 조직화된 기계 학습 모델들이다. 입력들은 이전 계층, 신경망 외부의 어딘가 또는 이들 양쪽에서 올 수 있다. 각각의 계층은 들어오는 입력을 가중치 값 및 옵션으로서 바이어스로 처리할 수 있는 하나 이상의 활성화 함수를 포함할 수 있다. 신경망은, 신경망으로 하여금 주어진 입력들에 대한 정확한 출력들을 생성하게 하는 가중치 값들을 학습하기 위해 학습 알고리즘에 따라 훈련될 수 있다.[2] Neural networks are machine learning models organized into two or more layers (or “layers”) of smaller models, each configured to process one or more inputs and produce one or more outputs. Inputs can come from the previous layer, somewhere outside the neural network, or both. Each layer may include one or more activation functions that may treat incoming inputs as weight values and optionally biases. A neural network may be trained according to a learning algorithm to learn weight values that cause the neural network to produce correct outputs for given inputs.
[3] 인코더-디코더 순환 신경망(RNN)은 데이터의 입력 시퀀스를 고정형 표현으로 변환함으로써 데이터의 입력 시퀀스와 출력 시퀀스 사이의 패턴들을 식별하도록 훈련될 수 있는 신경망 아키텍처의 유형이다. 그 후, 제2 모델("디코더")은 인코딩된 시퀀스를 입력으로서 수신하고, 인코딩된 시퀀스를 디코딩하여 예측된 시퀀스, 예를 들어 프랑스어 문장으로부터 번역된 영어 문장을 생성한다.[3] An encoder-decoder recurrent neural network (RNN) is a type of neural network architecture that can be trained to identify patterns between an input sequence of data and an output sequence by transforming the input sequence of data into a fixed representation. A second model (“decoder”) then receives the encoded sequence as input and decodes the encoded sequence to produce a predicted sequence, eg, a translated English sentence from a French sentence.
[4] 인코더-디코더 장단기 메모리(LSTM) 네트워크는 인코더 및 디코더 모델들로서 LSTM 네트워크들을 사용하는 인코더-디코더 RNN의 유형이다. LSTM 네트워크는 하나 이상의 셀 상태, 입력 게이트 및 망각 게이트를 포함한다. 셀 상태들은 LSTM 네트워크가 이전 시간 단계들에서 데이터를 처리하는 것으로부터 "기억"한다고 하는 정보를 저장하는 반면, 게이트들은 LSTM 네트워크가 셀 상태로부터 그리고 주어진 시간 단계 동안 어떤 정보를 획득 또는 폐기하는지를 조절한다.[4] An encoder-decoder long-term memory (LSTM) network is a type of encoder-decoder RNN that uses LSTM networks as encoder and decoder models. An LSTM network includes one or more cell states, an input gate and a forget gate. Cell states store information that the LSTM network "remembers" from processing data in previous time steps, whereas gates control what information the LSTM network acquires or discards from the cell state and during a given time step. .
[5] 수평 예보는 주어진 시간 단계 또는 시간 단계들에서의 상이한 변수들의 예측("예보 수평")을 지칭한다. 다중 수평 예보는 하나보다 많은 수평에 대한 상이한 변수들의 예측을 지칭한다.[5] Horizontal forecast refers to the prediction of different variables at a given time step or time steps (“forecast horizontal”). Multiple horizontal forecasting refers to the prediction of different variables for more than one horizontal.
[6] 본 명세서는 시계열 데이터를 처리하여 다중 수평 예보들을 생성하기 위한 기술들을 설명한다. 이 기술들은 일반적으로 시계열 데이터로부터 단기 및 장기 시간 특성들 둘 다를 처리하는 시스템을 수반한다. 시간 특성들은 시계열 데이터의 시간 독립 변수들에 대응하는 정적 메타데이터로 증강될 수 있다. 시간 의존 데이터를 대응하는 시간 독립 데이터로 처리함으로써, 본 명세서의 설명에 따라 구현된 시스템은 상이한 시간 단계들 또는 시간 단계들의 그룹들에서 정확하고 해석가능한 예보들("예보 수평들")을 생성할 수 있다.[6] This specification describes techniques for processing time series data to generate multiple horizontal forecasts. These techniques generally involve systems that process both short-term and long-term temporal characteristics from time-series data. Time characteristics may be augmented with static metadata corresponding to time independent variables of time series data. By processing time-dependent data into corresponding time-independent data, a system implemented in accordance with the description herein can produce accurate and interpretable forecasts (“forecast horizons”) at different time steps or groups of time steps. can
[7] 본 명세서에서 일부 구현들에서 설명된 바와 같은 시스템은 상이한 시간 단계들에서 소정의 입력을 선택적으로 필터링하거나 게이팅하는 반면, 다른 입력에는 상대적 중요도의 가중치들을 할당할 수 있다. 이를 통해, 입력 데이터는 시계열 데이터의 고도의 가변성과 일관되게, 시스템에 의해 더 높은 입도로 처리될 수 있다.[7] A system as described in some implementations herein may selectively filter or gate certain inputs at different time steps, while assigning weights of relative importance to other inputs. This allows the input data to be processed at a higher granularity by the system, consistent with the high variability of the time series data.
[8] 본 명세서에서 설명된 바와 같은 다중 수평 예보는, 시계열 데이터로 표현된 엔티티들의 특성들이 시간 경과에 따라 변할 수 있는 다수의 응용에서 유익할 수 있다.[8] Multi-horizontal forecasting as described herein can be beneficial in many applications where the properties of entities represented by time-series data may change over time.
[9] 예로서, 엔티티는 전력 그리드일 수 있고, 특성들은 상이한 위치들 및 상이한 시점들에서의 그리드의 소비를 포함할 수 있다. 시스템은 일부 구현들에서 상이한 예보 수평들에서 미래의 전기 소비율을 예측할 수 있다. 예측된 미래의 전기 소비율은 전력 그리드 부하의 시프트들을 예상하기 위해 사용될 수 있고, 자동화된 시스템은 예측된 미래의 소비율에 대해 전력 그리드를 평가하여, 전력 그리드에 대한 기반구조가 어느 위치들에서 개선되거나 대체될 필요가 있는지를 결정할 수 있다.[9] By way of example, an entity may be a power grid, and characteristics may include consumption of the grid at different locations and at different points in time. The system can predict future electricity consumption rates at different forecast levels in some implementations. The predicted future electricity consumption rate can be used to anticipate shifts in power grid load, and the automated system evaluates the power grid for the predicted future consumption rate, in which locations the infrastructure for the power grid is improved or You can decide if it needs to be replaced.
[10] 다른 예로서, 엔티티는 다수의 상이한 상호 연결 도로들 및 고속도로들에 의해 정의된 고속도로 교통 시스템일 수 있다. 고속도로 교통 시스템에 대한 특성들은 상이한 시점들에서의 그리고 상이한 위치들에서의 교통 흐름 및/또는 교통 혼잡을 포함할 수 있다. 일부 구현들에서, 시스템은 상이한 예보 수평들에서 미래의 교통 패턴들을 예측할 수 있다. 예측된 교통 패턴들은, 예를 들어, 자동 교통 신호들 및 다른 교통 제어 메커니즘들로 하여금 교통 흐름을 개선하기 위해 그리고/또는 혼잡을 감소시키기 위해 상이한 스케줄들에 따라 동작하게 하는 것의 일부로서 사용될 수 있다.[10] As another example, the entity may be a highway traffic system defined by a number of different interconnecting roads and highways. Characteristics for a highway traffic system may include traffic flow and/or traffic congestion at different points in time and at different locations. In some implementations, the system can predict future traffic patterns at different forecast levels. Predicted traffic patterns may be used, for example, as part of causing automatic traffic signals and other traffic control mechanisms to operate according to different schedules to improve traffic flow and/or reduce congestion. .
[11] 다른 예로서, 표현된 엔티티가 상업적 소매상인 경우, 특성들은 일년에 걸친 또는 휴가철 동안과 같은 일정 시간 기간에 걸친 재고의 변화를 포함할 수 있다. 다른 특성들은 재고 내의 상이한 아이템들에 대한 비용의 계절적 변화들, 및 일부 제품들이 전자기기와 같은 유사한 유형의 다른 제품들보다 더 판매되는 비율을 포함할 수 있다. 일부 구현들에서, 시스템은 상이한 예보 수평들에서 예상된 재고의 변동을 예보할 수 있다. 자동 재고 제어 시스템은, 예를 들어, 재고가 낮은 것으로 예측되지만 대응하는 공급도 낮은 시간을 예상하여 더 이른 시간에 공급이 더 높을 때 구매함으로써, 재고 내의 상이한 아이템들이 구매되는 방법을 조정할 수 있다.[11] As another example, if the entity represented is a commercial retailer, the characteristics may include changes in inventory over a period of time, such as over a year or during a holiday season. Other characteristics may include seasonal changes in the cost of different items in the inventory, and the rate at which some products are sold more than others of a similar type, such as electronics. In some implementations, the system can predict an expected change in inventory at different forecast levels. An automatic inventory control system may adjust how different items in the inventory are purchased, for example, by anticipating a time when the stock is expected to be low but a corresponding low supply and buying when the supply is higher at an earlier time.
[12] 다른 예로서, 표현된 엔티티가 의료 치료 시설인 경우, 특성들은 미예약 환자들의 입원율 또는 환자들의 퇴원율을 포함할 수 있다. 특성들은 또한, 환자들에 걸쳐 공통인 조건들의 유형들, 및 그러한 조건들의 유형들로부터의 그들의 회복을 위한 평균 시간과 같이 종합적으로 측정되는 특성들을 포함하는, 의료 치료 시설에서 치료받는 환자들에 대한 상이한 의료 조건들을 포함할 수 있다. 일부 구현들에서, 시스템은 상이한 예보 수평들에서 미래의 환자들의 예상된 입원을 예보할 수 있다. 미래의 환자들의 예상된 입원에 응답하여, 자동화된 시스템은 예를 들어 추가의 의료 장비의 획득 및/또는 시설에서의 추가 인원의 스케줄링에 의해 필요한 배열들을 준비할 수 있다.[12] As another example, if the represented entity is a medical treatment facility, the characteristics may include an admission rate of unbooked patients or a discharge rate of patients. Characteristics also include characteristics that are measured collectively, such as the types of conditions common across patients, and the mean time for their recovery from those types of conditions, for patients being treated in a medical treatment facility. may include different medical conditions. In some implementations, the system can predict the expected hospitalization of future patients at different forecast levels. In response to anticipated hospitalization of future patients, the automated system may prepare the necessary arrangements, for example, by acquisition of additional medical equipment and/or scheduling of additional personnel at the facility.
[13] 다른 예로서, 특성들은 또한, 시간 경과에 따라 변하고 엔티티의 재정적 또는 경제적 조건에 관련되는 특성들을 포함할 수 있다. 이러한 유형의 특성들은 일정 시간 기간에 걸친 엔티티에 대한 매일 또는 매주의 수익을 포함할 수 있다. 특성들은 또한, 매시간 및 매일 대비 매월 또는 매년 측정될 때의 수익에 대한 단기 또는 장기 변화들과 같은 단기 또는 장기 패턴들을 포함할 수 있다. 일부 구현들에서, 시스템은 상이한 예보 지평선들에서 예상 변동 수익을 예보할 수 있다.[13] As another example, characteristics may also include characteristics that change over time and relate to the financial or economic condition of an entity. These types of characteristics may include daily or weekly returns for an entity over a period of time. Characteristics may also include short-term or long-term patterns, such as short-term or long-term changes in revenue as measured monthly or annually versus hourly and daily. In some implementations, the system can forecast expected variable return at different forecast horizons.
[14] 예보된 출력들은 많은 정적 및 시변 공변량들 사이의 복잡한 관계들에 의존할 수 있다. 예를 들어, 상업용 소매상들은 소비자 수요가 한 해의 상이한 시간들에서 어떻게 증가하거나 감소하는지에 기초하여 그들의 이용가능 재고를 증가 또는 감소시키는 것에 주목할 수 있다. 다른 예로서, 환자 치료 계획이 치료를 위한 시간 기간의 코스에 걸쳐 생성되고 조정되어, 치료 계획에 응답하여 또는 기존의 조건들과 같은 다른 특성들에 기초하여 환자의 건강의 변화들을 조정할 수 있다.[14] The predicted outputs can depend on complex relationships between many static and time-varying covariates. For example, commercial retailers may note increasing or decreasing their available inventory based on how consumer demand increases or decreases at different times of the year. As another example, a patient treatment plan may be created and adjusted over the course of a period of time for treatment to accommodate changes in the patient's health in response to the treatment plan or based on other characteristics, such as existing conditions.
[15] 첨부된 청구항들의 구현들 및 전술한 구현들에 더하여, 아래의 구현들도 혁신적이다.[15] In addition to the implementations of the appended claims and the implementations described above, the implementations below are also innovative.
[16] 시스템은 하나 이상의 시간 단계의 각각의 예보 수평들에 대한 단기 및 장기 시간 특성들을 결정하기 위한 시퀀스-시퀀스 계층 및 시간 자기-주의 계층을 포함할 수 있다. 시퀀스-시퀀스 계층은 하나 이상의 컴퓨터에 의해 구현될 수 있고, 하나 이상의 시간 단계의 각각의 예보 수평들에 대한 단기 시간 특성들을 결정하도록 구성될 수 있다. 시퀀스-시퀀스 계층은 각각의 과거의 시간 기간들 동안 캡처된 정적 공변량들 및 시변 입력 데이터에 기초하여 인코더 벡터들을 생성하기 위한 하나 이상의 순환 신경망(RNN) 인코더, 및 하나 이상의 RNN 디코더를 포함할 수 있고, 각각의 RNN 디코더는 인코더 벡터들, 정적 공변량들, 및 시변 공지 미래 입력 데이터에 기초하여 각각의 미래의 시간 기간에 대한 단기 패턴을 예측하도록 구성된다. 시간 자기-주의 계층은 하나 이상의 컴퓨터에 의해 구현될 수 있고, 각각의 예보 수평들에 대한 장기 시간 특성들을 캡처하도록 구성될 수 있다. 시간 자기-주의 계층은 정적 공변량들, 각각의 과거의 시간 기간들 동안 캡처된 시변 입력 데이터, 및 시변 공지 미래 입력 데이터에 기초하여 각각의 수평에 대한 예보를 생성하도록 구성된 멀티-헤드 주의 계층을 포함할 수 있다.[16] The system may include a sequence-sequence layer and a temporal self-attention layer for determining short-term and long-term temporal characteristics for respective forecast horizontals of one or more temporal steps. A sequence-to-sequence layer may be implemented by one or more computers and may be configured to determine short-term temporal characteristics for respective forecast levels in one or more time steps. The sequence-sequence layer may include one or more recurrent neural network (RNN) encoders for generating encoder vectors based on time-varying input data and static covariates captured during respective past time periods, and one or more RNN decoders, and , each RNN decoder is configured to predict a short-term pattern for each future time period based on encoder vectors, static covariates, and time-varying known future input data. The temporal self-attention layer may be implemented by one or more computers and may be configured to capture long term temporal characteristics for each of the forecast levels. The temporal self-attention layer comprises a multi-head attention layer configured to generate a forecast for each horizontal based on static covariates, time-varying input data captured during each past time periods, and time-varying known future input data. can do.
[17] 시스템은 변수 선택 계층을 더 포함할 수 있으며, 변수 선택 계층은 정적 공변량들, 각각의 과거의 시간 기간들 동안 캡처된 시변 입력 데이터 또는 시변 공지 미래 입력 데이터 중 하나 이상을 포함하는 각각의 입력 변수에 대한 변수 선택 가중치들을 생성하도록 구성될 수 있다.[17] The system may further comprise a variable selection layer, wherein each variable selection layer comprises one or more of static covariates, time-varying input data captured during respective past time periods, or time-varying known future input data. may be configured to generate variable selection weights for an input variable.
[18] 변수 선택 계층은 복수의 변수 선택기를 포함할 수 있으며, 복수의 변수 선택기의 각각의 변수 선택기는 각각의 미래 또는 과거의 시간 기간에 입력 변수들에 대한 변수 선택 가중치들을 생성하도록 구성된다.[18] The variable selection layer may include a plurality of variable selectors, each variable selector of the plurality of variable selectors being configured to generate variable selection weights for input variables in respective future or past time periods.
[19] 변수 선택 계층은 정적 공변량들의 선택을 위한 변수 선택 가중치들을 생성하도록 구성된 변수 선택기를 포함할 수 있다.[19] The variable selection layer may include a variable selector configured to generate variable selection weights for selection of static covariates.
[20] 시스템은 정적 공변량들의 선택을 위한 변수 선택 가중치들에 기초하여 콘텍스트 벡터들을 인코딩하도록 구성되는 하나 이상의 정적 공변량 인코더들을 더 포함할 수 있다.[20] The system may further include one or more static covariate encoders configured to encode context vectors based on variable selection weights for selection of static covariates.
[21] 인코딩된 콘텍스트 벡터들은 변수 선택 계층 내의 복수의 변수 선택기 각각으로 전달될 수 있다.[21] The encoded context vectors may be passed to each of a plurality of variable selectors in the variable selection layer.
[22] 인코딩된 콘텍스트 벡터들은 정적 보강 계층에 전달될 수 있고, 정적 보강 계층은 복수의 게이팅된 잔류 네트워크(GRN)를 포함할 수 있다. 각각의 GRN에는 각각의 미래 또는 과거의 시간 기간이 할당될 수 있고, 각각의 GRN은 그의 각각의 시간 기간에 대응하는 시간 역학에 영향을 미치는 인코딩된 콘텍스트 벡터들 내의 정적 공변량들의 가중치를 증가시키도록 구성될 수 있다.[22] The encoded context vectors may be passed to a static enhancement layer, which may include a plurality of gated residual networks (GRNs). Each GRN may be assigned a respective future or past time period, each GRN increasing the weight of the static covariates in the encoded context vectors affecting the time dynamics corresponding to its respective time period. can be configured.
[23] 정적 보강 계층에서의 각각의 GRN의 출력은 멀티-헤드 주의 계층에 입력되는 데이터세트를 형성할 수 있다.[23] The output of each GRN in the static reinforcement layer may form a dataset input to the multi-head attention layer.
[24] 각각의 GRN의 출력은 멀티-헤드 주의 계층의 각각의 마스크에 입력될 수 있으며, 각각의 마스크는 인과적 예측을 위한 각각의 시간 기간에 대응한다.[24] The output of each GRN may be input to each mask of the multi-head attention layer, each mask corresponding to a respective time period for causal prediction.
[25] 각각의 미래의 시간 기간에 대한 예보는 변위치 예보(quantile forecast)로 변환될 수 있다.[25] The forecast for each future time period can be converted into a quantile forecast.
[26] 시스템은 복수의 게이팅 네트워크를 더 포함할 수 있으며, 각각의 게이팅 네트워크는 공유 가중치들을 갖는 복수의 게이팅된 선형 유닛(GLU)을 포함한다.[26] The system may further comprise a plurality of gating networks, each gating network comprising a plurality of gated linear units (GLUs) having shared weights.
[27] 복수의 GLU는 다중 수평 예보의 출력 예측에 덜 영향을 미치는 변수들의 기여들을 감소시키도록 구성될 수 있다.[27] A plurality of GLUs may be configured to reduce the contributions of variables that less influence the output prediction of the multi-horizontal forecast.
[28] 전술한 양상의 다른 구현들은 컴퓨터 구현 방법, 장치, 및 하나 이상의 컴퓨터 판독가능 저장 매체에 기록된 컴퓨터 프로그램들을 포함할 수 있다.[28] Other implementations of the above aspect may include a computer-implemented method, apparatus, and computer programs recorded on one or more computer-readable storage media.
[29] 본 명세서에 설명된 주제는 다음의 이점들 또는 기술적 효과들 중 하나 이상을 실현하도록 구현될 수 있다. 시계열 데이터의 다중 수평 예보는 복수의 시간 단계에 걸친 이종 입력들에 대해 더 높은 정확도로 생성될 수 있다. 상이한 엔티티들의 시간 독립적 및 시간 의존적 특징들을 특성화하는 데이터로부터, 예보를 위해 덜 관련성 있는 특징들보다는 가장 관련성 있는 특징들이 선택될 수 있다. 관련성 있는 특징들의 자동 선택은 관심 있는 상이한 예보 수평들 및 처리된 시계열 데이터의 세트들 사이의 차이들에 따라 달라질 수 있다. 또한, 유리하게, 시스템은 시계열 데이터에서 상이한 시간 단계들에서 입력 데이터에 영향을 미치는 시간 독립적 정적 공변량들을 무시하거나 일반화하지 않는다.[29] The subject matter described in this specification may be implemented to realize one or more of the following advantages or technical effects. Multiple horizontal forecasts of time series data can be generated with higher accuracy for heterogeneous inputs over multiple time steps. From data characterizing time-independent and time-dependent characteristics of different entities, the most relevant rather than less relevant features can be selected for prediction. The automatic selection of relevant features may depend on differences between different forecast horizons of interest and sets of processed time series data. Further, advantageously, the system does not generalize or ignore time-independent static covariates that affect the input data at different time steps in the time series data.
[30] 시스템은 예측된 예보에 대한 특정 입력 변수들의 상대적 중요도를 자동으로 해석할 수 있고, 시스템이 게이팅된 부분들에 의해 생성된 특성들의 상대적 중요도가 낮다고 결정할 때 시스템의 부분들을 입력 데이터를 처리하는 것으로부터 게이팅할 수 있다. 입력 변수들의 학습된 중요도는 미래의 예보의 정확도를 향상시키기 위해 사용될 수 있는데, 예를 들어 그 이유는 중요도가 이전에 입력 변수들에 할당된 중요도에 기초하여 그리고 그러한 동일한 특징들과 함께 더 관련이 있는 것으로 밝혀진 입력 데이터로부터 특징들을 엔지니어링 및/또는 선택하기 위해 사용될 수 있기 때문이다.[30] The system can automatically interpret the relative importance of certain input variables to the predicted forecast, and process the input data into parts of the system when the system determines that the relative importance of the features generated by the gated parts is low. can be gated from The learned importance of input variables can be used to improve the accuracy of future forecasts, for example because the importance is based on the importance previously assigned to the input variables and is more relevant with those same characteristics. It can be used to engineer and/or select features from input data that are found to be present.
[31] 예를 들어, 시스템은 일정 시간 기간에 걸쳐 상당한 장기 시간 패턴들을 갖거나 갖지 않는 시계열 데이터를 구별하고, 장기 시간 특성 처리에 전용화되는 시스템의 부분들을 자동으로 게이팅할 수 있다.[31] For example, the system can distinguish between time series data with or without significant long-term temporal patterns over a period of time, and automatically gate parts of the system dedicated to processing long-term temporal characteristics.
[32] 장기 시간 패턴들은 분석된 시간 윈도우 전체에 걸쳐 관찰된 특성들의 패턴들을 포함할 수 있다. 본 명세서에 설명된 바와 같은 시간 윈도우는 시스템이 예보를 생성하기 위해 데이터를 분석할 수 있는 과거 및 미래의 기간을 지칭한다. 장기 시간 패턴들은 매년의 엔티티에 대한 수익의 패턴들을 포함할 수 있다. 다른 예로서, 의료 치료 시설에 대한 장기 시간 패턴은 관찰된 시간 윈도우의 상이한 시간들에서 특정의 의료 조건들에 대한 환자들의 입원에서의 계절적 스파이크(spike) 또는 딥(dip)과 관련될 수 있다. 관찰된 시간 윈도우는 시스템이 장기 시간 패턴들을 식별할 수 있는 범위를 증가시키기 위해 몇 달 또는 몇 년에 걸칠 수 있다.[32] Long-term temporal patterns may include patterns of properties observed throughout the analyzed time window. Time windows as described herein refer to past and future periods over which the system may analyze data to generate a forecast. Long-term time patterns may include patterns of revenue for the entity on an annual basis. As another example, a long-term time pattern for a medical treatment facility may be associated with a seasonal spike or dip in hospitalization of patients for particular medical conditions at different times of the observed time window. The observed time window can span months or years to increase the range over which the system can identify long-term temporal patterns.
[33] 본 개시에서 여기서 설명되는 바와 같이, 단기 패턴들은 시스템에 의해 데이터가 처리되는 현재 시간 단계에 대한 미리 결정된 거리 내의 패턴들일 수 있다. 예를 들어, 시스템은 현재 처리된 시간 단계에 시간상 가까운 데이터로부터 단기 패턴들을 생성할 수 있다. 시계열 데이터가 엔티티에 대응하는 매일 데이터를 지정하는 경우, 예로서, 시스템은 시계열 데이터 내에서 매일 또는 매주 나타나는 단기 패턴들을 생성할 수 있다. 예를 들어, 단기 패턴은 아침 입원 대 야간 입원과 같은, 의료 치료 시설에 대한 환자 입원에 대한 매일 변동들을 포함할 수 있다.[33] As described herein in this disclosure, short-term patterns may be patterns within a predetermined distance to a current time step for which data is processed by the system. For example, the system may generate short-term patterns from data close in time to the currently processed time step. If the time series data specifies daily data corresponding to an entity, as an example, the system may generate short-term patterns appearing daily or weekly within the time series data. For example, a short-term pattern may include daily variations for patient admissions to a medical treatment facility, such as morning versus night admissions.
[34] 시스템은 서로 시간상 관련된 이벤트들에서 단기 패턴들을 식별할 수 있다. 예를 들어, 단기 패턴은 2 개의 이벤트 사이의 상관관계를 특성화할 수 있다. 예를 들어, 엔티티가 상업적 소매상인 경우, 단기 패턴은 전체의 관찰된 시간 윈도우에 비해 짧은 시간 기간에 걸쳐 판매되고 측정된 특정 유형들의 제품들의 구매 및 후속 반환일 수 있다.[34] The system can identify short-term patterns in events that are temporally related to each other. For example, a short-term pattern may characterize a correlation between two events. For example, if the entity is a commercial retailer, the short-term pattern may be the purchase and subsequent return of certain types of products sold and measured over a short period of time compared to the overall observed time window.
[35] 한편, 시스템은 시스템이 예보들을 생성하는 동작 동안 더 관련이 있는 것으로 결정하는 다른 단기 또는 장기 특성들을 더 무겁게 가중할 수 있고, 덜 관련이 있거나 전혀 관련이 없는 다른 특성들을 억제할 수 있다. 시스템의 적응성은 고차원 데이터에 대한 오버피팅(overfitting)을 완화하는 데 도움을 주면서도, 시계열 데이터의 상이한 소스들에 대해 일반화 가능한 기술들을 통한 고도로 정확한 예보를 가능하게 할 수 있다.[35] On the other hand, the system may weight more heavily other short-term or long-term characteristics that the system determines to be more relevant during operation of generating forecasts, and may suppress other characteristics that are less relevant or not at all. . The adaptability of the system can enable highly accurate forecasting through generalizable techniques to different sources of time-series data, while helping to mitigate overfitting for high-dimensional data.
[36] 시스템은 주어진 시간 단계에 대해 시간적으로 가까운, 예를 들어 이웃 시간 단계 또는 먼, 예를 들어 분석된 시간 윈도우 내에 있는 주변 입력으로부터 주어진 시간 단계에서의 입력에 대한 특성들을 공동으로 생성할 수 있다. 상이한 특성들을 공동으로 생성함으로써, 시스템은 단기 및 장기 특성들이 순차적으로 생성되는 접근법들에서 발생할 수 있는 에러 전파를 회피하거나 완화할 수 있다.[36] A system can jointly generate characteristics for an input at a given time step from a nearby input that is temporally close, e.g., a neighboring time step, or distant, e.g., within an analyzed time window for a given time step. there is. By jointly creating different characteristics, the system can avoid or mitigate error propagation that can occur in approaches where short-term and long-term characteristics are generated sequentially.
[37] 본 명세서에서 설명되는 기술들의 구현은 또한 더 직관적이고 덜 복잡한 모델들을 가능하게 할 수 있으며, 이는 더 양호한 설명 가능성 및 더 적은 "블랙박스" 처리를 가능하게 할 수 있다. 적어도 시스템이 상이한 시간 단계들에서 관련 입력에 대한 통찰을 제공하면서도 그러한 시간 단계들에서 존재하는 상이한 특성들의 중요도를 구별할 수 있으므로, 시스템의 출력들은 더 효율적으로 분석될 수 있다. 제안된 시스템은 블랙박스 모델들에 대한 사후 접근법들을 통한 결과적인 예보들의 더 양호한 분석을 유발할 수 있다.[37] Implementation of the techniques described herein may also enable more intuitive and less complex models, which may enable better explainability and less “black box” processing. The outputs of the system can be analyzed more efficiently, at least because the system can discriminate the importance of different properties present at different time steps while providing insight into the relevant input at those time steps. The proposed system can lead to a better analysis of the resulting forecasts through ex post approaches to black box models.
[38] 시스템은 예보를 생성하기 위한 상이한 특성들의 상대적 중요도를 설명하는 더 정확한 데이터를 제공할 수 있다. 예를 들어, 시스템은 (i) 전역적으로 중요한 변수들, (ii) 지속적인 시간 패턴들, 및 (iii) 시간 패턴들로부터의 편차들과 같은 중요 이벤트들을 식별할 수 있다. 이러한 통찰력은 샘플별로 또는 전역적으로 이루어질 수 있으며, 이는 예보들의 해석 가능성이 수평별로 또는 예보된 수평들 전부에 걸쳐 검사되는지에 관계없이 더 정확하고 명확할 수 있다는 것을 의미한다. 예측된 예보들에 대한 강한 인과적 연결들을 갖는 용이하게 해석가능한 데이터를 제공하는 데 있어서, 시스템은, 예컨대, 변수 중요도를 학습하는 것에 의해 그리고/또는 앞서 설명된 바와 같은 시스템의 부분들을 게이팅하는 것에 의해, 주어진 예측 문제 및 시계열 데이터에 대해 더 정확하게 그리고 더 적은 동작들로 동작하도록 자동으로 적응할 수 있다.[38] The system can provide more accurate data describing the relative importance of different characteristics for generating forecasts. For example, the system can identify significant events such as (i) globally significant variables, (ii) persistent temporal patterns, and (iii) deviations from temporal patterns. This insight can be made on a sample-by-sample or global basis, meaning that the interpretability of forecasts can be more accurate and explicit, whether they are checked horizontally or across all forecast horizontals. In providing readily interpretable data with strong causal connections to predicted forecasts, the system may be adapted to, for example, learn variable importance and/or gating parts of the system as described above. can automatically adapt to operate more accurately and with fewer operations for a given prediction problem and time series data.
[39] 도 1은 다중 수평 예보 시스템의 예의 기능도이다.
[40] 도 2는 도 1에 도시된 시스템의 기능도이며, 본 개시의 일부 양상들에 따른 시퀀스 처리 계층 및 원격 의존 계층의 컴포넌트들을 도시한다.
[41] 도 3은 다중 수평 예보 프로세스의 예의 흐름도이다.
[42] 도 4는 게이팅된 잔류 네트워크(GRN)의 예의 기능도이다.
[43] 도 5는 변수 선택기의 예의 기능도이다.
[44] 도 6a는 예보된 시계열 데이터에 대한 변수 중요도를 결정하는 프로세스의 예의 흐름도이다.
[45] 도 6b는 예보된 시계열 데이터에 대한 지속적인 시간 패턴들을 결정하는 프로세스의 예의 흐름도이다.
[46] 도 6c는 시계열 데이터에서 시간적 거동의 시프트들을 결정하는 프로세스의 예의 흐름도이다.
[47] 도 7a 및 도 7b는 하나 이상의 컴퓨팅 디바이스 상에 구현된 다중 수평 예보 시스템의 예를 도시한다.
[48] 위의 도면들 내의 동일한 참조 번호들은 동일한 요소들을 나타낸다.[39] Figure 1 is a functional diagram of an example of a multi-horizontal forecasting system.
[40] FIG. 2 is a functional diagram of the system shown in FIG. 1 , illustrating components of a sequence processing layer and a remote dependency layer in accordance with some aspects of the present disclosure.
[41] Figure 3 is a flow chart of an example of a multi-horizontal forecasting process.
[42] Figure 4 is a functional diagram of an example of a Gated Residual Network (GRN).
[43] Fig. 5 is a functional diagram of an example of a variable selector.
[44] FIG. 6A is a flowchart of an example of a process for determining variable importance for forecasted time series data.
[45] FIG. 6B is a flow diagram of an example of a process for determining persistent time patterns for predicted time series data.
[46] FIG. 6C is a flow diagram of an example of a process for determining shifts in temporal behavior in time series data.
[47] Figures 7A and 7B show an example of a multiple horizontal forecasting system implemented on one or more computing devices.
[48] Like reference numbers in the drawings above indicate like elements.
[49] 도 1은 다중 수평 예보 시스템(100)의 예의 기능도이다. 시스템(100)은 입력(105)을 획득하고, 상이한 예보 수평들에서의 상이한 변수들에 대한 예보들(120)을 생성하도록 구성된다. 시스템(100)은 시계열 데이터베이스 또는 시계열 데이터의 임의의 소스로부터 입력(105)을 획득할 수 있다. 입력(105)은 시계열 데이터를 통해 표현된 적어도 하나의 엔티티에 각각 대응하는 상이한 변수들을 포함한다. 구체적으로, 입력(105)은, 각각이 하나 이상의 엔티티를 적어도 부분적으로 표현하는 정적 공변량들(125), 시간 의존적 특징들(130) 및 타겟들(140)을 포함한다.[49] FIG. 1 is a functional diagram of an example of a
[50] 엔티티는 적어도 부분적으로 하나 이상의 정적 공변량들을 통해 표현될 수 있다. 정적 공변량들은 시간 독립적인, 즉 시간 단계마다 시간상 변하지 않는 변수들이다. 한편, 시간 의존적 특징들(130)은 시간에 따라, 즉 시간 단계마다 변하는 변수들이다. 시간 의존적 특징들은 주어진 시간 단계에 대한 관찰된 입력들(132) 및 공지 입력들(134)을 포함한다. 관찰된 입력들(132)은 시간 단계 t에서 또는 그 이전에 측정되는 변수들이고, 공지 입력들(134)은 시간 단계 t 후에 상이한 시간 단계들에 대해 변하지만 미리 공지된 변수들이다. 예를 들어, 시간 단계들이 매일 측정되는 경우, 일(day) t에 대한 그리고 엔티티 소매점 i에 대한 관찰된 입력은 일 t의 소매점에 대한 수익일 수 있고, 공지 입력은 일 t의 요일 또는 달력 날짜일 수 있다.[50] An entity may be represented, at least in part, via one or more static covariates. Static covariates are time-independent variables that do not change in time from time step to time step. Meanwhile, the time-
[51] 다른 예로서, 관찰된 입력은 고속도로 교통 시스템의 특정 위치 및 시간에서 분 단위로 측정된 교통 지연을 포함할 수 있고, 공지 입력은 고속도로 교통 시스템의 도로의 섹션들이 관찰된 시간 윈도우 내의 다양한 시간들에서 건설을 위해 폐쇄되는 시간들을 포함할 수 있다.[51] As another example, the observed input may include traffic delays measured in minutes at a particular location and time of the highway traffic system, and the known input may include various within the time window over which sections of the road of the highway traffic system were observed. Hours may include times that are closed for construction.
[52] 다른 예로서, 관찰된 입력은 전력 그리드에 대한 특정일 동안의 전기 소비 또는 특정일의 평균 온도를 포함할 수 있다. 전력 그리드에 대한 공지 입력은 전력 그리드의 특정 섹션들을 수리하거나 유지하기 위한 계획된 유지 스케줄일 수 있다.[52] As another example, the observed input may include electricity consumption for a particular day or average temperature for a particular day to the power grid. The announcement input to the power grid may be a planned maintenance schedule for repairing or maintaining specific sections of the power grid.
[53] 본 명세서에서, 예측 간격 예보들(120)은 상이한 관심 시간 단계들(또는 "예보 수평들")에서의 변수들의 값들의 예측을 지칭한다. 예를 들어, 고객들 및 그들의 소비 거동에 대한 시계열 데이터가 주어지면, 시스템(100)은 상이한 예보 수평들(A-T)에서의 판매 수익(관심 변수)을 예측하는 예보들(120A-T)을 생성할 수 있다. 예보들(120A-T)에 대응하는 수평들은 미리 결정될 수 있거나, 입력(105)의 일부로서 지정될 수 있다.[53] As used herein, prediction interval forecasts 120 refer to prediction of values of variables at different time steps of interest (or “prediction horizons”). For example, given time series data about customers and their consumption behavior, the
[54] 시스템(100)은 이종 시계열 데이터를 처리하도록 구성된다. 이것은 시스템(100)이 특정한 유형의 입력, 예를 들어 주어진 시간 단계에서 주어진 엔티티에 대한 소정의 시간 의존적 또는 시간 독립적 변수들의 존재를 가정하지 않는다는 것을 의미한다. 엔티티에 대한 정적 공변량들이 누락되거나, 주어진 시간 단계 t에서 엔티티에 대한 공지 또는 관찰된 변수들이 누락되는 경우, 시스템(100)은 시간 단계에 대응하는 시간 특성들을 생성하기 위해 이용가능한 입력들을 처리할 수 있다. 시스템(100)은 엔티티를 나타내는 이용가능한 정적 공변량들을 대응하는 시간 의존적 특징들의 처리와 통합하도록 구성된다. 이 통합은 입력 변수들의 포맷 또는 성질에 대한 가정을 제한하거나, 예보 동안에 시간 독립적 변수들을 완전히 무시하는 종래의 기술들에 비해 더 효율적이고 정확한 예보들을 유발할 수 있다.[54]
[55] 시간 단계, 및 시간 단계에 대응하는 입력 데이터에 의해 특징들이 표현되는 엔티티들과 관련된 시간 특성들을 생성하기 위해, 시스템(100)은 시퀀스 처리 계층(110) 및 원격 의존 계층(115)을 구현하도록 구성된다. 이 2 개의 계층(110, 115)의 조합은 시스템(100)이 정적 공변량들(125) 및 시간 의존 특징들(130) 양자로부터 장기 및 단기 시간 특성들 양자를 학습하는 것을 가능하게 할 수 있다.[55] To generate temporal characteristics associated with a time step and entities whose features are represented by input data corresponding to the time step, the
[56] 단기 시간 특성들은 현재 처리되는 시간 단계 t에 대한 이웃 시간 단계들, 예를 들어 t+1, t-1에서의 입력으로부터 생성된 특성들을 포함할 수 있다. 시퀀스 처리 계층(110)은 후술하는 시퀀스-시퀀스 계층(220)을 사용하여 단기 시간 특성들을 생성할 수 있다.[56] Short-term temporal characteristics may include characteristics generated from input at neighboring time steps for the currently processed time step t, eg, t+1, t-1. The
[57] 시간 특성들은 주어진 예보 수평에 대한 예보를 예측하는 것에 대한 입력 변수의 상대적 중요도를 포함할 수 있다. 다른 예로서, 시간 특성들은 짧은(시간 단계 대 시간 단계) 또는 긴(예를 들어, 수십 또는 수백 개의 시간 단계에 걸침) 시간 기간에 걸친 시간 패턴들을 나타낼 수 있다. 또한, 시스템(100)은 식별된 시간 패턴들로부터의 편차들뿐만 아니라, 도 6a 내지 도 6c를 참조하여 아래에 더 상세히 설명되는 다른 유형의 중요한 이벤트들을 나타내는 시간 특성들을 생성할 수 있다.[57] Temporal characteristics may include the relative importance of the input variable to predicting the forecast for a given forecast level. As another example, temporal characteristics may represent time patterns over a short (time step versus time step) or long (eg, spanning tens or hundreds of time steps) time periods. In addition, the
[58] 일반적으로, 시퀀스 처리 계층(110)은 입력(105)을 수신하고 예보들(120)을 생성하는 것과 관련된 입력(105)의 시간 특성들을 생성하도록 구성된다. 주의 처리 계층(115)은 시퀀스 처리 계층(110)으로부터의 시간 특성들 및 정적 공변량들(125)을 획득하고, 예보들(120)을 생성하도록 구성된다.[58] Generally, the
[59] 도 2-5와 관련하여 아래에 더 상세히 설명되는 바와 같이, 시스템(100)은 정적 공변량들(125)을 계층들(110, 115)을 통한 입력(105)의 처리에 걸쳐 뒤얽히게 하며, 이는 적어도 정적 공변량들이 무시되거나 별개로 처리되지 않기 때문에 더 효율적이고 정확한 예보들을 유발할 수 있다. 예를 들어, 주의 처리 계층(115)은 정적 공변량들(125)을 이용하여 시퀀스 처리 계층(110)으로부터의 시간 특성들을 보강하여, 시간 의존적 및 시간 독립적 데이터가 별개로 처리된 경우에 누락될 수 있는 입력(105)의 특성들을 더 식별할 수 있다. 주의 처리 계층(115)은 또한 식별된 장기 시간 패턴들 및 시간 패턴들로부터의 편차들과 같은 중요한 이벤트들에 따라 추가적인 단기 또는 장기 시간 특성들을 생성하거나 기존의 시간 특성들을 증대시키도록 구성된다.[59] As described in greater detail below with respect to FIGS. 2-5 ,
[60] 시스템(100)은 각각의 예보 수평에 대한 가능성 있는 타겟 값들의 다수의 변위치들을 통해 예측 간격 예보들(120)을 생성할 수 있다. 예를 들어, 시스템(100)은 타겟 내에서 그리고 특정 예보 수평 A에서 예측 값들의 10 번째, 50 번째 및 90 번째 변위치를 표현하기 위해 예보(120A)를 생성할 수 있고, 50 번째 변위치는 포인트 예보에 대응할 것이다. T 개의 예보 수평들에 대해, 시스템(100)은 예보들(120A-T)을 생성할 수 있고, 각각의 예보는 A로부터 T까지의 각각의 하나 이상의 시간 단계들에 대응한다. 예보들(120)을 생성하기 위해 사용되는 특정 변위치 절단 포인트들은 미리 결정될 수 있거나, 시스템(100)은 (도 1에 도시되지 않은) 입력(105)의 일부로서 하나 이상의 변위치 절단 포인트들을 수신할 수 있다.[60]
[61] 시스템(100)은 입력(105)의 일부로서 타겟들(140)을 수신할 수 있다. 본 명세서에서의 설명의 목적을 위해, 시스템(100)은 예보들(120)을 각각의 예보 수평에 대한 예측 값들의 하나 이상의 변위치로서 생성한다고 가정하지만, 일부 구현들에서, 시스템(100)은 예보들(120)을 포인트별로, 예를 들어 입력(105)으로부터 직접 생성된 예보 수평에 대한 예측 변수들 중 일부 또는 전부로서 생성한다는 것이 이해된다.[61]
[62] 도 2는 도 1에 도시된 시스템(100)의 기능도이며, 본 개시의 일부 양상들에 따른 시퀀스 처리 계층(110) 및 주의 처리 계층(115)의 컴포넌트들을 도시한다. 설명의 편의를 위해, 때때로 시간 단계 t를 참조할 것이지만, 시스템(100)은 복수의 시간 단계에 걸쳐 입력을 예를 들어 병렬로 또는 순차적으로 처리하여 대응하는 예보들을 생성하도록 구성된다는 것이 이해된다. 시스템(100)의 예시적인 구현들이 도 2-8을 참조하여 아래에 설명되고, 시스템(100)이 수행하도록 구성될 수 있는 다중 수평 예보를 위한 예시적인 프로세스가 도 3을 참조하여 아래에 설명된다.[62] FIG. 2 is a functional diagram of the
[63] 도 3은 다중 수평 예보를 위한 프로세스(300)의 예의 흐름도이다. 편의상, 프로세스(300)는 하나 이상의 컴퓨터 상에 구현되고, 하나 이상의 위치에 위치되고, 본 명세서에 따라 적절하게 프로그래밍된 시스템에 의해 수행되는 것으로 설명된다. 예를 들어, 적절하게 구성된 다중 수평 예보 시스템, 예를 들어 도 1-2의 다중 수평 예보 시스템(100)이 프로세스(300)를 수행할 수 있다.[63] FIG. 3 is a flow diagram of an example of a
[64] 시스템은 입력 데이터를 수신한다(310). 도 2를 참조하면, 시스템(100)은 범위 [t-k, t] 내의 관찰된 입력들(205)의 서브세트로서 입력(105)을 수신할 수 있고, 여기서 t는 처리되고 있는 현재의 시간 단계이고, k는 시스템(100)이 현재의 시간 단계로부터 (t-k) 번째 시간 단계까지 관찰된 변수들을 처리할 것임을 나타내는 룩-비하인드 파라미터(look-behind parameter)이다. k는 범위 [0, t] 내의 임의의 정수 값일 수 있고, k에 대한 0의 값은 시스템(100)이 현재의 시간 단계 t로부터의 관찰된 변수들만을 처리하는 것에 대응하고, k에 대한 t의 값은 시스템(100)이 제1 시간 단계로부터 시작하여 모든 관찰된 변수를 처리하는 것에 대응한다. 시스템(100)은 [-k, t] 내의 과거의 시간 기간들에 대한 그리고 시간 단계 t에 비해 과거에서의 관찰 입력들을 처리한다. 과거의 시간 기간들은 하나 이상의 시간 단계를 가질 수 있다.[64] The system receives input data (310). Referring to FIG. 2 ,
[65] 시스템(100)은 또한 범위 [65]
[66] 이와 함께, [66] Along with this,
[67] 공지 입력 및/또는 관찰된 입력이 없는 시간 단계들에 대해, 시스템(100)은 상수 값, 예를 들어 0을 할당할 수 있다. 도 1을 참조하여 위에 설명된 바와 같이, 정적 공변량들(125)은 또한 시스템(100)에 대한 수신된 입력의 일부를 형성할 수 있다.[67] For time steps with no known input and/or observed input, the
[68] 도 3을 참조하면, 시스템은 하나 이상의 시간 단계의 각각의 예보 수평들에 대한 단기 및 장기 시간 특성들을 결정한다(320). 이를 위해, 시스템(100)은 시퀀스 처리 계층(110) 및 주의 처리 계층(115)을 통해 입력(105)을 공동으로 처리할 수 있다. 즉, 양 계층들(110, 115)을 통한 처리에 의해, 시스템(100)은, 아래에 더 상세히 설명되는 바와 같이, 양 계층들(110, 115)의 각각의 단기 및 장기 식별 능력에 의해 보강되거나 증강될 수 있는 단기 및 장기 특성들을 생성할 수 있다. 도 2에 도시된 바와 같이, 시퀀스 처리 계층(110)은 시퀀스-시퀀스 계층(220), 정적 공변량 인코더(235), 및 일부 구현들에서 변수 선택 계층(230)을 포함한다.[68] Referring to FIG. 3, the system determines 320 short-term and long-term time characteristics for respective forecast levels of one or more time steps. To this end,
[69] 도 3을 참조하면, 결정(320)은 시퀀스-시퀀스 계층(220) 내의 하나 이상의 인코더들을 사용하여, 각각의 과거의 시간 기간 동안 캡처된 정적 공변량들 및 시변 입력 데이터에 기초하여 인코더 벡터들을 생성하는 단계(322)를 포함할 수 있다. 시변 입력 데이터는 관찰 입력들(205)을 포함한다. 인코더 벡터들은 적어도 정적 공변량들(125)과 관찰된 입력(205)의 조합의 인코딩된 표현들이고, 그것의 처리는 아래에 설명된다. 인코딩된 벡터는 시퀀스 처리 계층(110) 및/또는 주의 처리 계층(115)이 수신하도록 구성되는 포맷으로 입력 데이터의 표현을 저장하는 벡터(또는 일부 구현들에서, 다차원 어레이 또는 텐서)를 지칭한다. 인코딩은 원시 입력 데이터로부터 처리에 적합한 포맷팅된 표현으로의 자동 매핑일 수 있다. 인코딩은 또한 아래에 설명되는 바와 같이 인코더(222)와 디코더(224) 사이에서 시퀀스-시퀀스 처리를 수행함에 있어서 중간 표현을 생성하는 것의 일부로서 수행될 수 있다.[69] Referring to FIG. 3 , a
[70] 인코더 벡터들을 생성하는 시퀀스-시퀀스 계층(220)에 대한 입력이 정적 공변량 인코더(235)에서 시작하여 설명된다. 시스템(100)은 정적 공변량 인코더(235)를 통해 정적 공변량들(125)을 처리하도록 구성된다. 일반적으로, 정적 공변량 인코더(235)는 정적 공변량들(125)의 인코딩된 표현들인 콘텍스트 벡터들을 생성하도록 구성된다. 본 명세서에서 [70] The input to the sequence-
[71] 본 명세서에서 설명되는 방식으로 콘텍스트 벡터들을 통합함으로써, 시스템(100)은 정적 공변량들(125)에 의해 표현되는 시간 독립 데이터로 시간 의존 입력(205, 210)의 시간 특성들을 컨디셔닝할 수 있다.[71] By integrating the context vectors in the manner described herein, the
[72] 시계열 데이터에서, 중요한 포인트들 또는 기간들은 종종 시간상 가까운 다른 포인트들을 표현하는 변수들의 값들과 관련하여 식별된다. 이 중요한 포인트들은 시스템(100)이 더 정확한 예보들을 생성하기 위해 식별하는 데 중요할 수 있는 이상들, 변화 포인트들 또는 주기적 패턴들을 포함할 수 있다. 그러나, 일부 경우들에서, 공지 입력들에 대한 관찰 입력들의 수가 변할 수 있기 때문에, 관찰된 입력들의 포함은 이 중요한 포인트들의 검출을 더 어렵게 만들 수 있다.[72] In time series data, important points or periods are often identified with respect to values of variables representing other points that are close in time. These important points may include anomalies, change points, or periodic patterns that may be important for the
[73] 따라서, 시퀀스-시퀀스 계층(220)은 관찰 및 공지 입력들을 각각 처리하도록 개별적으로 구성되는 인코더(222) 및 디코더(224)를 포함할 수 있다. 시퀀스-시퀀스 계층(220)은 원격 처리 계층(115)에 대한 입력의 일부로서 사용될 수 있는 단기 시간 특성들을 생성할 수 있다.[73] Accordingly, the sequence-
[74] 시퀀스-시퀀스 계층(220)은 인코더(222) 및 디코더(224)를 포함하고, 이들 양자는 룩-비하인드 파라미터 k 및 룩-어헤드 파라미터 [74] The sequence-
[75] 그러나, 계층(220)은 구현마다 구성되며, 시퀀스-시퀀스 계층(220)은 관찰된 입력(205), 공지 입력(210) 양자로부터의 입력뿐만 아니라 정적 공변량 인코더(235)에 의해 인코딩된 콘텍스트 벡터들도 처리할 수 있다.[75] However, the
[76] 인코더(222)는 과거 관찰 입력 및 콘텍스트 벡터들을 수신하도록 구성되는 입력 계층; 하나 이상의 숨겨진 계층; 및 처리된 데이터에 대한 숨겨진 상태를 전파하는 출력 계층을 포함하는 복수의 계층을 포함할 수 있다. 인코더(222)는 정적 공변량 인코더(235)에 의해 생성된 콘텍스트 벡터들([76] The
[77] 인코더(222)는 [t-k, t] 범위 내의 각각의 시간 단계에서 콘텍스트 벡터들([77] The
[78] 도 3을 다시 참조하면, 시스템(100)은 인코더 벡터들, 정적 공변량들 및 시변 공지 미래 입력에 기초하여 각각의 예보 수평에 대한 단기 패턴을 예측한다(324). 각각의 시간 단계에 걸친 인코더(222) 및 디코더(224)로부터의 출력은 분석된 시간 윈도 내의 단기 패턴을 나타낼 수 있는 시간 특성들의 세트를 형성한다. 시간 특성들은 다음과 같이 표시된다.[78] Referring back to FIG. 3, the
여기서, here,
[79] 포지션 인덱스 [79] position index
[80] 또한, 각각의 시간 단계에서, 인코더(222) 및 디코더(224)는 각각의 시간 단계로부터 게이트들의 게이팅 계층(240) 및 정규화 계층들을 통해, 인코딩/디코딩을 수행하는 동안 생성된 출력, 예를 들어 중간 인코딩된/디코딩된 표현을 추가로 전달한다. 게이팅 계층(240)의 예시적인 구성은 다음과 같이 표시된다.[80] Also, at each time step, the
여기서, here,
[81] 중간 인코딩된/디코딩된 표현은 인코더/디코더(222, 224)에 의해 생성된 값들의 벡터 또는 다차원 어레이일 수 있고, 인코더/디코더(222, 224)의 활성화 함수들에 의해 생성된 출력을 표현할 수 있다. 중간 인코딩된/디코딩된 표현은 인코더/디코더(222, 224)의 가중치들뿐만 아니라, 하나 이상의 이전 시간 단계에서의 인코더/디코더(222, 224)에 대한 입력의 함수로서 변할 수 있다. 이전 시간 단계(들)에서의 입력은 그 자체가 이전 시간 단계(들)에서 인코더/디코더(222, 224)에 의해 생성된 중간 인코딩된/디코딩된 표현의 함수일 수 있다.[81] The intermediate encoded/decoded representation may be a vector or multidimensional array of values generated by the encoder/
[82] 게이팅된 선형 유닛은 신경망의 적어도 일부를 게이팅하기 위한 게이팅 메커니즘의 한 유형이다. 게이팅된 선형 유닛들에 관한 더 상세한 사항은 Dauphin, et al., Language Modeling with Gated Convolutional Neural Networks, Proceedings of the 34th Int. Conf. on Machine Learning (Sep. 8, 2017)에서 발견될 수 있다. [82] A gated linear unit is a type of gating mechanism for gating at least a portion of a neural network. For more details on gated linear units, see Dauphin, et al., Language Modeling with Gated Convolutional Neural Networks, Proceedings of the 34th Int. Conf. on Machine Learning (Sep. 8, 2017).
[83] GLU로서 표현되지만, 일부 구현들에서, 시스템(100)은 게이팅 계층(240)의 일부로서 다른 게이팅 메커니즘들을 사용한다. 일부 구현들에서, 위의 예시적인 구성에서의 GLU는 지수 선형 유닛 활성화 함수("ELU")가 뒤따르는 선형 계층으로 대체되지만, 다른 활성화 함수들 및 게이팅 메커니즘들이 사용될 수 있다.[83] Although represented as a GLU, in some implementations, the
[84] 시스템(100)이 변수 선택 계층(230)을 통해 입력들(205, 210)을 처리한 후에, 시스템(100)은 시퀀스-시퀀스 계층(220)을 통해 관찰 입력들(205) 및 공지 입력들(210)을 처리할 수 있다. 도 5를 참조하여 아래에 더 상세하게 설명되는 바와 같이, 변수 선택 계층(230)은 학습된 관련성 척도에 따라, 예를 들어 입력 변수들의 존재 또는 부재와 대응하는 예보의 값 사이의 상관관계들에 기초하여 입력(105)의 상이한 변수들을 가중할 수 있다. 일부 구현들에서, 시퀀스-시퀀스 계층(220)은 변수 선택 계층(230) 없이 직접 입력들(205, 210)을 처리할 수 있다.[84] After the
[85] 게이팅 계층(240)의 출력은 입력으로서 주의 처리 계층(115)에 전달된다. 도 2에서 별개의 계층으로서 도시되지만, 일부 구현들에서, 게이팅 계층(240)은 시퀀스-시퀀스 계층(220)의 일부를 형성한다. 일부 구현들에서, 게이팅 계층(240)은 완전히 생략되고, 인코더(222)로부터의 출력은 주의 처리 계층(115)에 의해 직접 처리된다.[85] The output of the
[86] 도 3을 다시 참조하면, 시스템(100)은 정적 공변량들 및 시간 의존적 입력, 즉 시간 윈도우 내의 관찰 입력들 및 공지 입력들에 기초하여, 예보 수평에 대한 장기 시간 특성들을 캡처한다(326). 도 2에서, 주의 처리 계층(115)은 시스템(100)으로의 입력(105)의 장기 시간 특성들을 학습하는 것을 적어도 부분적으로 담당하고, 적어도 3 개의 컴포넌트들: 정적 보강 계층(250), 시간 자기 주의 계층(260), 및 포지션별 피드포워드 계층(270)으로 분할된다. 주의 처리 계층(115)은 시퀀스 처리 계층(110)에 의해 생성되는 출력으로부터 장기 특성들을 생성할 수 있다. 각각의 계층이 차례로 설명된다.[86] Referring back to FIG. 3, the
[87] 정적 보강 계층(250)은 시스템(100)이 정적 공변량 인코더(235)를 사용하여 생성하는 콘텍스트 벡터들로 시퀀스-시퀀스 계층(220)로부터 수신된 시간 특성들을 향상시킨다. 정적 공변량들이 시간 데이터에 상당한 영향을 미치기 때문에, 예컨대, 환자에 대한 유전 정보(각각의 시간 단계를 통해 일반적으로 정적임)가 상이한 예보 수평들에서 환자에 대한 질병 위험에 중요한 영향을 미치기 때문에, 시간 특성들을 보강하는 것이 중요할 수 있다.[87] The
[88] 이러한 정적 변수들에 대한 가정들을 무시하거나 일반화하기보다는, 시스템(100)은 그 대신에 정적 공변량들(125)을 통합하고, 그 후 본 명세서에서 게이팅된 잔류 네트워크들(GRN들)(250A-N)이라고 지칭되는 복수의 게이팅 메커니즘들을 사용하여 시퀀스 처리 계층(110)에 의해 생성된 시간 특성들에 대한 그들의 상대적 기여를 게이팅할 수 있다. 일반적으로, 시스템(100)은 주어진 시간 단계에서 더 또는 덜 "관련 있는" 것으로서 시퀀스 처리 계층(110)에 의해 처리된 특정 특성들을 촉진하기 위해 각각의 GRN(250A-N)에 대한 공유 가중치들의 세트를 학습하도톡 구성된다. 관련성은, 예를 들어, 주어진 시간 단계에서의 특정 시간 특성의 존재, 부재, 또는 값과, 시스템(100)이 주어진 시간 단계를 사용하여 생성한 예측된 예보의 값 사이의 통계적 상관관계에 기초하여 측정될 수 있다.[88] Rather than disregard or generalize assumptions about these static variables, the
[89] 도 4는 게이팅된 잔류 네트워크(GRN)(400)의 예를 도시한다. GRN(400)은 이종 시계열 데이터에 대한 입력 처리를 적응시키기 위해 시스템(100)에서 사용된다. 즉, 시스템(100)은, GRN들 및 다른 게이팅 메커니즘들을 통행, 상이한 시간 단계들에서의 특정 입력들의 존재 또는 부재, 또는 예측된 예보들에 대한 그러한 특정 입력들의 학습된 중요도에 기초하여, 예측된 예보들에 대한 입력의 처리 흐름을 조정할 수 있다. 입력들과 타겟들 사이의 정확한 관계가 항상 미리 알려져 있는 것은 아니기 때문에, 입력에서 무엇이 관련이 있는지를 결정하는 것이 어려울 수 있다. 다른 문제는 처리된 시계열 데이터의 상대적 크기 또는 잡음이다. 작은 또는 잡음이 있는 시계열 데이터에 대해, 아키텍처가 덜 복잡한 모델들이 다른 것들보다 더 높은 정확도로 동작할 수 있다. 따라서, 이 문제들을 해결하기 위해, 시스템은 게이팅된 잔류 네트워크(400)와 같은 게이팅 메커니즘을 구현할 수 있다.[89] FIG. 4 shows an example of a gated residual network (GRN) 400 .
[90] GRN들은 시스템(100) 내의 논리적으로 복수의 위치들에서 구현될 수 있다. 예를 들어, 도 2에 도시된 바와 같이, 정적 보강 계층(250)은 복수의 GRN들(250A-N)을 구현할 수 있다. 시스템(100)은 범위 [90] GRNs may be implemented at a plurality of logical locations within the
여기서, here,
[91] 조밀 계층(430A)은, [91] The
[92] 주어진 입력(105)을 처리하는 데 필요하지 않은 시스템(100)의 부분들을 억제하기 위한 유연성의 적어도 일부를 제공하기 위하여, GRN(400)은 컴포넌트 게이팅 계층(440)을 포함할 수 있다. 예를 들어, 컴포넌트 게이팅 계층(440)은 아래와 같이 정의될 수 있는 GLU를 구현할 수 있다.[92] To provide at least some of the flexibility for suppressing portions of the
여기서, here,
[93] 시그모이드 활성화 함수가 사용되지만, 일부 구현들에서는 다른 비선형 함수들로 대체될 수 있다. GLU는 컴포넌트 게이팅 계층(440)으로 하여금 GRN(400)이 GRN 입력(410)에 기여하는 정도를 제어하는 것을 허용하여, 필요한 경우에 잠재적으로 계층을 완전히 생략한다. 예를 들어, GLU 출력들이 모두 0에 가까운 경우, GRN은 GRN 입력(410)의 (무시할만한) 비선형 기여를 억제할 수 있다. 콘텍스트 벡터가 없는 경우, GRN은 콘텍스트 입력을 0 또는 임의의 상수 값으로 취급한다. 훈련 동안, 예를 들어 조밀 계층 [93] A sigmoid activation function is used, but in some implementations it may be replaced by other non-linear functions. The GLU allows the
[94] 정적 보강 계층(250)은 이하의 형태를 취할 수 있다.[94] The
여기서, 가중치들 Here, the weights
[95] 정적 보강 계층(250)에 후속하여, 시스템(100)은 시간 자기 주의 계층(260)을 통해 시퀀스 처리 계층(110)으로부터의 보강된 시간 특성들을 처리한다. 시스템은 해석 가능한 멀티-헤드 주의 계층(262)을 통해 정적 보강 계층(250)으로부터의 보강된 시간 특성들을 적용한다.[95] Following the
[96] 해석 가능한 멀티-헤드 주의 계층(262)은 시스템(100)이 상이한 시간 단계들에서 표현된 변수들 사이의 장기 관계들을 학습하는 메커니즘이다. 본 명세서에서 설명된 바와 같은 멀티-헤드 주의 계층(262)은 시스템(100)에 의해 생성된 예보들의 더 큰 해석 가능성을 허용할 수 있다. 일반적으로, 주의 메커니즘들은 아래와 같이 키들(K)과 질의들(Q) 사이의 관계들에 기초하여 값들(V)을 스케일링한다.[96] The interpretable multi-head attention layer 262 is a mechanism by which the
여기서, here,
[97] 멀티-헤드 주의 계층(262)은 상이한 "헤드들"이 상이한 가중치 값들에 따라 [97] Multi-head attention layer 262 is different "heads" according to different weight values
여기서, here,
[98] 그러나, 멀티-헤드 주의 메커니즘들의 한 가지 문제점은, 적어도 각각의 헤드에서 상이한 값들이 사용되기 때문에, 주의 가중치들만으로는 특정 특성의 중요도를 나타내지 못할 수 있다는 것이다. 따라서, 멀티-헤드 주의 계층(262)은 대신에 각각의 헤드 사이에서 가중치 값들을 공유할 수 있고, 또한 헤드들을 가산적으로 집계할 수 있으며, 이는 다음에 의해 주어질 수 있다.[98] However, one problem with multi-head attention mechanisms is that attention weights alone may not indicate the importance of a particular characteristic, at least because different values are used in each head. Thus, the multi-head attention layer 262 may instead share weight values between each head, and may also aggregate the heads additively, which may be given by:
여기서, here,
[99] 멀티-헤드 주의 계층(262)은, 결합된 행렬 [99] Multi-head attention layer 262 is a combined matrix
[100] 구체적으로, 멀티-헤드 주의 계층(262)은 다음에 의해 주어지는 보강된 시간 특성들을 처리한다.[100] Specifically, the multi-head attention layer 262 processes the augmented temporal characteristics given by
여기서, here,
여기서, here,
[101] 일부 구현들에서, 시간 자기 주의 계층(260)은 또한 게이팅 계층(264)을 포함할 수 있다. 게이팅 계층(264)은 게이팅 계층(240)과 유사하게, 시스템(100)에 의해 하나 이상의 예보를 생성하는 데 관련없는 것으로 간주되는 일부 시간 특성들을 필터링 또는 게이팅할 수 있다. 훈련된 가중치들에 따라, 게이팅 계층(240)은 처리 동안 상이한 시간 특성들의 기여를 증폭 또는 무효화할 수 있다. 일부 경우들에서, 게이팅 계층(240)에 의해 게이팅되지 않은 시간 특성들은 게이팅 계층(264)에 의해 나중에 게이팅될 수 있다. 예를 들어, 그 이유는 시스템(100)이 멀티-헤드 주의 계층(262)을 통해 시간 특성을 처리한 후에, 시간 특성은 예보에 덜 관련있는 것으로 발견되기 때문이다. 일례에서, 게이팅 계층(264)은 다음에 의해 주어질 수 있다.[101] In some implementations, the temporal self-
여기서, here,
[102] 시스템(100)은 피드-포워드 계층(270)을 통해 시간 자기 주의 계층(260)으로부터의 출력을 처리할 수 있다. 피드-포워드 계층(270)은 GRN들(270A-N)을 포함할 수 있다. 도 2 및 도 4를 참조하여 위에 설명된 바와 같이, GRN들(270A-N)은 학습된 가중치들의 세트에 따라 일부 시간 특성들을 게이팅할 수 있다. 피드-포워드 계층(270)의 하나의 예시적인 구성은 다음과 같을 수 있다.[102] The
여기서, here,
[103] 시스템(100)은 옵션으로서 원격 처리 계층(115)으로부터의 출력을 전적으로 게이팅하기 위해 추가의 게이팅 계층(266)을 적용할 수 있다. 이러한 방식으로, 시스템(100)은 입력(105)에 따라 더 간단한 아키텍처에 적응할 수 있다. 예를 들어, 입력(105)은 서로 시간상 비교적 가까운 시계열 데이터를 나타낼 수 있고, 시스템(100)은 원격 처리 계층(115)의 추가의 복잡성을 보장하기에 시간상 충분히 앞으로 멀지 않은 시간 단계들에서 예보들을 생성하도록 구성될 수 있다.[103] The
[104] 게이팅 계층(266)은 다음과 같이 주어질 수 있다.[104] The
여기서, here,
[105] 게이팅 계층(266)에 후속하여(그것이 시스템(100)의 일부로서 구현되는 구현들에서), 시스템(100)은 (시퀀스 처리 계층(110), 원격 처리 계층(115), 및 게이팅 계층에 의해 생략되지 않은 임의의 개재 계층들을 통한 처리 후에) 시간 특징들을 처리하여, 관심 있는 수평들에 대응하는 예보들(120)을 획득할 수 있다. 시스템(100)은 다음과 같이 주어지는 조밀 계층(268)을 통해 출력을 처리함으로써, 관심 있는 각각의 시간 단계에 대한 다양한 백분위수(예를 들어, 10 번째, 50 번째, 및 90 번째 퍼센트) 예보 범위들을 생성할 수 있다.[105] Following gating layer 266 (in implementations where it is implemented as part of system 100 ), system 100 (
여기서, W, b는 지정된 변위치 q에 대한 선형 계수들이고, where W, b are the linear coefficients for the specified displacement q,
[106] 도 5는 변수 선택기(500)의 예를 도시한다. 도 1 및 도 2를 참조하여 전술한 바와 같이, 시스템(100)은 정적 변수 선택기(230A) 및 변수 선택기들(230B-N) 각각을 통해 정적 공변량들(125), 관찰 입력들(205) 및 공지 입력들(210)을 처리할 수 있다. 변수 선택기(500)는 시스템(100)이 변수 선택기들(230A-N) 각각을 어떻게 구현할 수 있는지의 예이다.[106] FIG. 5 shows an example of a
[107] 변수 선택 계층(230)을 통해, 시스템(100)은 정적 공변량들 및 시간 의존 변수들 둘 다의 인스턴스별 변수 선택을 제공하도록 구성된다. 도 6a 내지 도 6c를 참조하여 아래에 더 상세히 설명되는 바와 같이, 정확한 예보를 위해 어느 변수들이 더 중요한지에 대한 통찰력을 제공하는 것을 넘어서, 시스템(100)은 성능에 잠재적으로 부정적인 영향을 줄 수 있는 불필요한 잡음을 제거하기 위해 변수 선택 계층(230)을 통해 입력을 더 처리할 수 있다. 또한, 시스템(100)은 시계열 데이터의 이종 소스들을 처리하도톡 구성되기 때문에, 변수 선택 계층(230)은 시스템(100)이 불필요한 또는 덜 관련된 정보를 강건하게 필터링하는 것을 허용하고, 이는 결국 다중 수평 예보에서 전체 시스템 성능을 개선할 수 있다.[107] Via the
[108] 변수 선택기(500)는 입력, 즉 각각의 정적 공변량에 대해 관찰 입력 및 공지 입력으로부터 도출된 실제 값들의 [108] The
[109] 일반성을 잃지 않고, 변수 선택기(500)는 관찰 입력들(205)을 수신하도록 구성된 선택기로서 설명될 것이다. [109] Without loss of generality, the
여기서, here,
여기서, here,
[110] GRN들(510A-N)을 통해 처리된 관찰 및 공지 입력은 게이팅 계층(505)에 의해 생성된 가중치들에 의해 가중되어(520), 다음을 산출한다.[110] Observation and notification input processed through
여기서, here,
[111] 정적 변수 선택기(230A) 자체는 입력으로서 개별적으로 콘텍스트 벡터([111] The static variable selector 230A itself as an input is individually a context vector (
[112] 앞서 언급한 바와 같이, 본 명세서에 설명된 기술들은 시계열 데이터와 시스템(100)에 의해 생성된 결과적인 예보들 간의 관계의 개선된 해석가능성을 가능하게 할 수 있다. 해석가능성은 일반적으로 입력 데이터의 부분들을 식별하고 이해하는 것 및 개별 변수들이 어떻게 서로 패턴화되거나 상호 관련되는지를 지칭한다. 시스템(100)은 해석가능한 방식으로 예보들을 생성할 수 있으며, 이는 시스템(100)이 특정 특성들을 갖는 시계열 데이터에 더 적합한 시스템(100)의 상이한 구현들에 대한 동기를 부여할 수 있는 특정의 입력 변수들의 상대적 중요도 또는 비중요도에 대한 통찰력을 제공할 수 있다는 것을 의미한다.[112] As noted above, the techniques described herein may enable improved interpretability of the relationship between time series data and the resulting forecasts generated by the
[113] 상이한 입력 변수들의 관계 및 결과적인 예보와의 관계에 대한 새로 발견된 통찰력으로, 시스템(100)은 이러한 관계들을 학습하고, 상이한 구현들에서 설명되는 다양한 게이팅 계층들에 대한 대응하는 가중치 값들을 업데이트할 수 있다. 전술한 바와 같이, 게이팅 계층들은 시스템(100)의 특정 부분들이 억제되는 것을 가능하게 할 수 있다. 예를 들어, 주의 처리 계층(115)은 입력(105)이 장기 시간 처리를 수용하지 않는 것으로 발견되는 상황들에서 억제될 수 있다. 시스템(100)은 그렇게 할 수 있는데, 그 이유는 시스템(100)이 장기가 아니라 단기로 시간 특성들을 식별할 때 관련 입력 변수들 또는 관련 입력 변수들 간의 관계가 가장 두드러지기 때문이다. 게다가, 변수 선택 계층(230)은 상이한 사용 사례들 또는 상이한 유형의 시계열 데이터 하에서 상이한 입력 변수들을 억제 또는 상승시킬 수도 있다.[113] With newly discovered insights into the relationship of different input variables and their relationship to the resulting forecast, the
[114] 이하는 입력들과 대응하는 예보들 사이에서와 같은 시스템(100)의 성능의 해석가능성에 대한 3 가지 사용 사례들: (1) 예측에서 각각의 입력 변수의 중요도를 검사하는 것, (2) 시간 패턴들을 시각화하는 것, 및 (3) 시간 역학의 상당한 변화를 가져오는 임의의 체제들 또는 이벤트들을 식별하는 것을 설명한다. 이 사용 사례들은 시스템(100)이 시계열 데이터 세트 전체에 걸쳐 패턴들을 집계하여 시간 특성들에 관한 일반화가능한 통찰력을 추출할 수 있다는 것을 보여준다.[114] The following are three use cases for the interpretability of the performance of the
[115] 도 6a는 예보된 시계열 데이터에 대한 변수 중요도를 결정하기 위한 프로세스(600A)의 예의 흐름도이다. 편의상, 프로세스(600A)는 하나 이상의 컴퓨터 상에서 구현되고, 하나 이상의 위치에 위치되고, 본 명세서에 따라 적절하게 프로그래밍된 시스템에 의해 수행되는 것으로 설명된다. 예를 들어, 적절하게 구성된 다중 수평 예보 시스템, 예를 들어 도 1-2의 다중 수평 예보 시스템(100)이 프로세스(600A)를 수행할 수 있다.[115] FIG. 6A is a flow diagram of an example of a
[116] 시스템은 각각의 입력 변수에 대해 생성된 변수 선택 가중치들을 집계한다(610). 시스템(100)은 입력 변수들 각각과 연관된 샘플링 분포의 하나 이상의 백분위수를 결정한다(620). 예를 들어, 시스템은 시스템(100)을 훈련하기 위해 사용되는 훈련 데이터의 전체 테스트 세트에 걸쳐 선택 가중치들을 집계하고, 각각의 샘플링 분포의 10 번째, 50 번째, 및 90 번째 백분위수들을 기록할 수 있다. 일부 구현들에서 시스템(100)은 변수 선택 계층(230)을 통해 입력(105)을 자동으로 처리하도록 구성되기 때문에, 그러한 구현들에서 시스템(100)은 또한 변수 선택 가중치들을 집계하고(610), 입력 변수들 각각과 연관된 샘플링 분포를 결정할 수 있다(620). 일부 구현들에서, 변위치들은 예보된 시계열 데이터에 대한 시스템의 출력 분포의, 예를 들어 다른 시스템에 의한 또는 사용자에 의한 추가 처리 및 분석을 위해 확률 밀도 함수에 매핑된다.[116] The system aggregates the variable selection weights generated for each input variable (610). The
[117] 추가로 또는 대안으로서, 시스템(100)은 원격 처리 계층(115)과 관련하여 상이한 입력 변수들에 대한 변수 중요도를 결정하기 위해 멀티-헤드 주의 계층(262)의 가중치들을 획득할 수 있다. 일부 경우들에서, 장기 시간 특성들을 결정하기 위해 변수들의 중요도를 이해하는 것은 시퀀스 처리 계층(110)에서의 변수 선택 계층(230)으로부터의 변수 선택 가중치들만큼 또는 그보다 더 중요할 수 있다. 예를 들어, 장기 시간 특성들은 단기 시간 특성들만이 해석되는 경우에 이용 가능하지 않거나 덜 검출 가능한 지속적인 시간 패턴들을 나타낼 수 있다.[117] Additionally or alternatively, the
[118] 도 6b는 예보된 시계열 데이터에 대한 지속적인 시간 패턴들을 결정하기 위한 프로세스(600B)의 예의 흐름도이다. 편의상, 프로세스(600B)는 하나 이상의 컴퓨터 상에서 구현되고, 하나 이상의 위치에 위치되고, 본 명세서에 따라 적절하게 프로그래밍된 시스템에 의해 수행되는 것으로 설명된다. 예를 들어, 적절하게 구성된 다중 수평 예보 시스템, 예를 들어 도 1-2의 다중 수평 예보 시스템(100)이 프로세스(600B)를 수행할 수 있다.[118] FIG. 6B is a flow diagram of an example of a
[119] 시스템은 복수의 시간 기간들 각각에서 하위 레벨 특징들의 주의 가중 합을 생성한다(630). 하위 레벨 특징들은 각각의 시간 단계 t에서 시퀀스 처리 계층(110)의 출력으로부터의 처리된 표현, 예를 들어 시퀀스 처리 계층(110)에 의해 생성된 시간 특성들 또는 시퀀스-시퀀스 계층(220)에서의 입력의 처리 동안 생성되고 시간 단계 t 및 수평 [119] The system generates an attention-weighted sum of the lower-level features in each of the plurality of time periods ( 630 ). The lower level features are the processed representation from the output of the
여기서, here,
[120] 시스템은 시간 기간들에 걸쳐 주의 가중 합의 분포들을 결정한다(640). 시스템은 특정 수평에 대한 예보를 생성하기 위해 가장 중요한 과거 단계들을 조명하기 위해 주의 가중 패턴들을 사용할 수 있다. (특히, 일화적 경험에 기초하여 수동 튜닝되는 경우에 부정확할 수 있는) 계절성 및 지체 분석을 위한 모델 기반 사양들에 의존하는 다른 기술들과 대조적으로, 시스템(100)은 원시 데이터로부터 시간 패턴들을 학습할 수 있다. 이에 따라, 시스템(100)은, 예를 들어, 특정 특징 엔지니어링 또는 데이터 수집에 의해 모델 개선들을 위해 의존될 수 있다.[120] The system determines ( 640 ) distributions of the weighted sum of the week over time periods. The system can use attention-weighted patterns to highlight the most important past steps to generate a forecast for a particular horizontal. In contrast to other techniques that rely on model-based specifications for seasonality and lag analysis (which can be inaccurate especially when manually tuned based on anecdotal experience), the
[121] 시퀀스 처리 계층(110) 및 원격 처리 계층(115)에 대응하는 가중치들의 이산화는 시스템(100)이 단기 및 장기 시간 특성들과 관련된 파라미터 값들을 추적하여, 최소의 추가 처리로 주어진 예보를 유발하는 것을 가능하게 할 수 있다. 즉, (장기 시간 특성들을 담당하는) 원격 의존성(115)에 대응하는 가중치들은 (단기 시간 특성들을 담당하는) 시퀀스 처리 계층(110)에 대응하는 가중치들과 다르다.[121] Discretization of the weights corresponding to the
[122] 패턴들을 식별하는 것에 더하여, 그러한 패턴들의 갑작스런 변화들을 식별하는 것도 매우 유용할 수 있는데, 그 이유는 중요한 체제들 또는 이벤트들의 존재로 인해 시간적 시프트들이 발생할 수 있기 때문이다. 예를 들어, 체제 스위칭 거동은 시간 패턴들 및 그러한 패턴들로부터의 편차들에 의해 종종 특성화되는 금융 시장들을 나타내는 시계열 데이터에서 널리 문서화되었다.[122] In addition to identifying patterns, it can also be very useful to identify sudden changes in such patterns, since temporal shifts may occur due to the presence of significant regimes or events. For example, regime switching behavior has been widely documented in time series data representing financial markets that are often characterized by time patterns and deviations from those patterns.
[123] 도 6c는 시계열 데이터에서 시간적 거동의 시프트들을 결정하기 위한 프로세스(600C)의 예의 흐름도이다. 편의상, 프로세스(600C)는 하나 이상의 컴퓨터 상에서 구현되고, 하나 이상의 위치에 위치되고, 본 명세서에 따라 적절하게 프로그래밍된 시스템에 의해 수행되는 것으로 설명된다. 예를 들어, 적절하게 구성된 다중 수평 예보 시스템, 예를 들어 도 1-2의 다중 수평 예보 시스템(100)이 프로세스(600C)를 수행할 수 있다.[123] FIG. 6C is a flow diagram of an example of a
[124] 시스템은 평균 주의 패턴을 결정한다(650). 시스템에 대한 입력에서 표현된 주어진 엔티티에 대해, 예보 수평당 평균 주의 패턴은 다음과 같이 정의될 수 있다.[124] The system determines the average attention pattern (650). For a given entity represented in the input to the system, the average attention pattern per forecast horizon can be defined as
여기서, here,
[125] 그 후, 시스템은 [125] After that, the system
[126] 시스템은 임계값을 충족시키는 평균 주의 패턴으로부터의 거리의 변화들을 식별한다(660). 특히, 시스템은 시간 윈도우의 상이한 시간 단계들에 걸쳐 [126] The system identifies ( 660 ) changes in distance from the average attention pattern that meets the threshold. In particular, the system spans different time steps of a time window.
여기서, here,
[127] 각각의 엔티티에 대해, 시스템은 (모든 수평들에 대해 집계된) 다음과 같이 주어지는 평균 패턴을 갖는 각각의 포인트에서의 자기 주의 가중치 벡터들 사이의 거리를 계산함으로써 시간 역학에서의 상당한 시프트들을 결정한다.[127] For each entity, the system computes a significant shift in time dynamics by calculating the distance between the weight vectors of its own attention at each point with an average pattern (aggregated for all horizontals) given as decide them
여기서, here,
[128] 체제 변화 또는 중요한 이벤트를 표현할 가능성이 없는 사소한 시프트들에 의해 야기되는 잡음을 완화하기 위해, 시스템은 거리가 미리 결정된 임계치를 충족시키지 않으면 시간 단계 t에 대한 [128] In order to mitigate noise caused by regime changes or minor shifts that are not likely to represent significant events, the system calculates for a time step t if the distance does not meet a predetermined threshold.
[129] 시스템(100)은 예를 들어 시스템(100)을 훈련하도록 구성된 모델 트레이너를 사용하여 다양한 기계 학습 훈련 기술에 따라 훈련될 수 있다. 시스템(100)은 라벨링된 시계열 데이터의 훈련 세트에 관한 감독 학습 기술에 따라 훈련될 수 있다. 시스템(100)의 계층들 중 일부 또는 전부가 신경망들로서 구현될 때, 모델 트레이너는 훈련 입력에 대응하는 예보를 획득하기 위해 훈련 입력을 시스템(100)을 통해 전달할 수 있다. 모델 트레이너는 훈련 입력에 대응하는 예보들의 실측 세트와 시스템(100)의 출력 예보 사이의 손실을 계산할 수 있다. 그 다음, 손실에 관한 기울기들이 모든 모델 파라미터 값들, 예를 들어 가중치들에 대해 계산되고, 시스템(100)의 다양한 계층들에 걸쳐 설명되고, 업데이트될 수 있다. 일부 구현들에서, 시스템(100)의 일부 부분들, 예를 들어 시퀀스 처리 계층(110) 및 주의 처리 계층(115)은 서로 독립적으로 훈련된다.[129]
[130] 모델 트레이너는 시스템(100)을 훈련하기 위한 다양한 상이한 손실 함수들 중 하나를 구현할 수 있다. 사용될 수 있는 손실 함수들의 하나의 예시적인 클래스는 예보들(120)을 구성하는 변위치 출력들에 걸쳐 합산되는 변위치 손실을 최소화하는 함수들의 클래스이다.[130] The model trainer may implement one of a variety of different loss functions for training the
[131] 모델 트레이너는 데이터를 3 개의 부분, 즉 학습을 위한 훈련 세트, 하이퍼파라미터 튜닝을 위한 확인 세트, 및 성능 평가를 위한 홀드-아웃 테스트 세트로 먼저 분할함으로써 훈련 데이터세트에 대해 시스템을 훈련할 수 있다. 모델 트레이너는 하나 이상의 하이퍼파라미터에 따라 시스템(100)을 훈련할 수 있다. 모델 트레이너는 시스템(100)의 일부일 수 있거나, 시스템(100)으로부터 원격인 위치 내의 하나 이상의 위치에 있는 하나 이상의 컴퓨터 상에 구현될 수 있다. 모델 트레이너는 시스템(100)을 시스템(100)을 구현하는 하나 이상의 컴퓨터에 결합된 메모리 내로 로딩하기 전에 시스템을 오프라인으로 훈련할 수 있다. 추가적으로 또는 대안적으로, 모델 트레이너는 시스템(100)을 온라인으로 훈련할 수 있고, 이 경우에는 시스템(100)이 동작하는 동안 모델 파라미터 값들이 조정되며, 이는 라이브 데이터에 응답하여 시스템(100)의 개선된 정확도를 제공할 수 있다.[131] The model trainer can train the system on the training dataset by first splitting the data into three parts: a training set for learning, a validation set for hyperparameter tuning, and a hold-out test set for performance evaluation. can The model trainer may train the
[132] 훈련 세트는 다수의 시간 단계들에 걸친 복수의 정적 공변량들, 관찰 입력들, 및 공지 입력들을 포함할 수 있다. 각각의 시간 단계 또는 시간 단계들의 그룹은 시스템(100)이 예측할 실측 값을 나타내는 예보들의 대응하는 세트로 라벨링된다. 훈련 세트는 시계열 데이터에서 표현된 다양한 상이한 엔티티 수들, 예를 들어 41 내지 130,000을 포함할 수 있다. 또한, 모델 트레이너에 의한 훈련을 위한 훈련 입력들의 수는 예를 들어 100,000과 500,000 사이에서 변할 수 있지만, 더 많거나 더 적은 수의 입력들이 사용될 수 있다.[132] The training set may include a plurality of static covariates over multiple time steps, observation inputs, and known inputs. Each time step, or group of time steps, is labeled with a corresponding set of forecasts representing the ground truth for the
[133] 네트워크 파라미터 값들과 관련하여, 룩-비하인드 파라미터 k에 대한 일부 예들은 90, 168 및 252 개의 시간 단계를 포함한다. 한편, 예시적인 룩-포워드 파라미터 값들 [133] Regarding network parameter values, some examples for the look-behind parameter k include 90, 168 and 252 time steps. On the other hand, exemplary look-forward parameter values
[134] 모델 트레이너는 하이퍼파라미터 최적화를 허용하도록 구성되며, 이는 다양한 상이한 기술들, 예를 들어 무작위 검색에 따라 행해질 수 있다. 모델 트레이너는 다양한 학습률, 예를 들어 0.0001-0.01에 따라 시스템(100)을 훈련하도록 구성된다. 모델 트레이너가 시스템을 훈련하는 것의 일부로서 역전파를 수행하는 일부 구현들에서, 대응하는 기울기들은 또한 상이한 인자들, 예를 들어 0.01-100에 따라 정규화될 수 있다.[134] The model trainer is configured to allow for hyperparameter optimization, which can be done according to a variety of different techniques, eg, random search. The model trainer is configured to train the
[135] 도 7a 및 도 7b는 하나 이상의 컴퓨팅 디바이스 상에 구현된 다중 수평 예보 시스템(700)의 예를 도시한다. 이 예에서, 시스템(700)은 컴퓨팅 디바이스들(710, 720, 730 및 740) 중 하나 이상뿐만 아니라 저장 시스템(750) 상에 구현될 수 있다. 컴퓨팅 디바이스(710)는 범용 컴퓨팅 디바이스들에 통상적으로 존재하는 하나 이상의 프로세서(712), 메모리(714) 및 다른 컴포넌트들을 포함할 수 있다. 컴퓨팅 디바이스(710)의 메모리(714)는 컴퓨팅 디바이스(710)로 하여금 본 개시의 양상들에 따른 다중 수평 예보를 위한 동작들을 수행하게 하기 위해 하나 이상의 프로세서(712)에 의해 실행될 수 있는 명령들(716)을 포함하여, 하나 이상의 프로세서(712)에 의해 액세스가능한 정보를 저장할 수 있다.[135] FIGS. 7A and 7B show an example of a multi-horizontal forecasting system 700 implemented on one or more computing devices. In this example, system 700 may be implemented on
[136] 메모리는 또한 프로세서에 의해 검색, 조작 또는 저장될 수 있는 데이터(718)를 포함할 수 있다. 메모리는 하드 드라이브, 메모리 카드, ROM, RAM, DVD, CD-ROM, 기입 가능 및 판독 전용 메모리들과 같이, 프로세서에 의해 액세스 가능한 정보를 저장할 수 있는 임의의 비일시적 유형일 수 있다.[136] Memory may also include data 718 that may be retrieved, manipulated, or stored by the processor. Memory may be of any non-transitory type capable of storing information accessible by a processor, such as a hard drive, memory card, ROM, RAM, DVD, CD-ROM, writable and read-only memories.
[137] 명령들(716)은 하나 이상의 프로세서에 의해 기계 코드와 같이 직접 또는 스크립트들과 같이 간접적으로 실행될 명령들의 임의의 세트일 수 있다. 그와 관련하여, 용어 "명령들", "애플리케이션", "단계들", 및 "프로그램들"은 본 명세서에서 교환가능하게 사용될 수 있다. 명령들은 프로세서에 의한 직접 처리를 위해 객체 코드 포맷으로, 또는 요구에 따라 해석되거나 미리 컴파일되는 독립 소스 코드 모듈들의 스크립트들 또는 집합들을 포함하는 임의의 다른 컴퓨팅 디바이스 언어로 저장될 수 있다. 명령들의 함수들, 메소드들, 및 루틴들은 아래에 더 상세히 설명된다. 예시적인 명령들은 웹 애플리케이션 및/또는 특징 애플리케이션들을 포함할 수 있다.[137] Instructions 716 may be any set of instructions to be executed either directly, such as machine code, or indirectly, such as scripts, by one or more processors. In that regard, the terms “instructions”, “application”, “steps”, and “programs” may be used interchangeably herein. The instructions may be stored in object code format for direct processing by a processor, or in any other computing device language, including scripts or sets of independent source code modules that are interpreted or precompiled on demand. The functions, methods, and routines of the instructions are described in more detail below. Example instructions may include a web application and/or feature applications.
[138] 데이터(718)는 명령들(716)에 따라 하나 이상의 프로세서(712)에 의해 검색, 저장 또는 수정될 수 있다. 예를 들어, 본 명세서에 설명된 주제는 임의의 특정 데이터 구조에 의해 제한되지 않지만, 데이터는 컴퓨터 레지스터들에, 많은 상이한 필드들 및 레코드들, 또는 XML 문서들을 갖는 테이블로서 관계형 데이터베이스에 저장될 수 있다. 데이터는 또한 이진 값들, ASCII 또는 유니코드와 같은, 그러나 이에 제한되지 않는 임의의 컴퓨팅 디바이스 판독가능 포맷으로 포맷팅될 수 있다. 더욱이, 데이터는 숫자들, 설명 텍스트, 독점 코드들, 포인터들, 다른 네트워크 위치들에서와 같이 다른 메모리들에 저장된 데이터에 대한 참조들, 또는 관련 데이터를 계산하기 위해 함수에 의해 사용되는 정보와 같은, 관련 정보를 식별하기에 충분한 임의의 정보를 포함할 수 있다.[138] Data 718 may be retrieved, stored, or modified by one or more processors 712 in accordance with instructions 716 . For example, while the subject matter described herein is not limited by any particular data structure, data may be stored in computer registers, in a relational database as a table with many different fields and records, or XML documents. there is. The data may also be formatted in any computing device readable format, such as, but not limited to, binary values, ASCII or Unicode. Moreover, data may include numbers, descriptive text, proprietary codes, pointers, references to data stored in other memories, such as at other network locations, or information used by functions to calculate related data. , and may contain any information sufficient to identify relevant information.
[139] 하나 이상의 프로세서들(712)은 상업적으로 이용가능한 CPU와 같은 임의의 종래의 프로세서들일 수 있다. 대안적으로, 프로세서들은 (텐서 처리 유닛들("TPU들")을 포함하는) 주문형 집적 회로("ASIC") 또는 다른 하드웨어-기반 프로세서와 같은 전용 컴포넌트들일 수 있다. 필요하지는 않지만, 컴퓨팅 디바이스들(710, 720, 730, 및 740) 중 하나 이상은 병렬 처리와 같은 특정 컴퓨팅 프로세스들을 수행하기 위한 특수화된 하드웨어 컴포넌트들을 포함할 수 있다. 예를 들어, 하나 이상의 프로세서들(712)은 그래픽 처리 유닛들(713)("GPU")일 수 있다. 추가적으로, 하나 이상의 GPU들은 단일 명령 다중 데이터("SIMD") 디바이스들 및/또는 단일 명령 다중 스레드 디바이스들("SIMT")일 수 있다.[139] The one or more processors 712 may be any conventional processors, such as a commercially available CPU. Alternatively, processors may be dedicated components such as application specific integrated circuits (“ASICs”) (including tensor processing units (“TPUs”)) or other hardware-based processors. Although not required, one or more of
[140] 도 7a는 컴퓨팅 디바이스(710)의 프로세서, 메모리, 및 다른 요소들을 동일한 블록 내에 있는 것으로서 기능적으로 도시하지만, 프로세서, 컴퓨터, 컴퓨팅 디바이스, 또는 메모리는 실제로는 동일한 물리적 하우징 내에 저장될 수 있거나 저장되지 않을 수 있는 다수의 프로세서들, 컴퓨터들, 컴퓨팅 디바이스들, 또는 메모리들을 포함할 수 있다. 예를 들어, 메모리는 컴퓨팅 디바이스들(710-740)의 하우징과는 상이한 하우징들에 위치된 하드 드라이브 또는 다른 저장 매체일 수 있다. 따라서, 프로세서, 컴퓨터, 컴퓨팅 디바이스, 또는 메모리에 대한 참조들은 병렬로 동작할 수 있거나 동작하지 않을 수 있는 프로세서들, 컴퓨터들, 컴퓨팅 디바이스들, 또는 메모리들의 집합에 대한 참조들을 포함하는 것으로 이해될 것이다. 예를 들어, 컴퓨팅 디바이스들(710-740)은 부하 균형화된 서버 팜, 분산 시스템 등으로서 동작하는 서버 컴퓨팅 디바이스들을 포함할 수 있다. 또한, 이하에서 설명된 일부 기능들은 단일 프로세서를 갖는 단일 컴퓨팅 디바이스 상에서 발생하는 것으로서 표시되지만, 본 명세서에 설명된 주제의 다양한 양상들은 예를 들어 네트워크(760) 상에서 정보를 통신하는 복수의 컴퓨팅 디바이스들에 의해 구현될 수 있다.[140] Although FIG. 7A functionally depicts the processor, memory, and other elements of
[141] 컴퓨팅 디바이스들(710) 각각은 네트워크(760)의 상이한 노드들에 있을 수 있고 네트워크(760)의 다른 노드들과 직접 및 간접적으로 통신할 수 있다. 도 7a 및 도 7b에는 단지 수 개의 컴퓨팅 디바이스만이 도시되지만, 통상적인 시스템은 많은 수의 접속된 컴퓨팅 디바이스들을 포함할 수 있고, 각각의 상이한 컴퓨팅 디바이스는 네트워크(760)의 상이한 노드에 있다는 것을 이해해야 한다. 본 명세서에 설명된 네트워크(760) 및 개재 노드들은 다양한 프로토콜들 및 시스템들을 사용하여 상호접속될 수 있고, 따라서 네트워크(760)는 인터넷, 월드 와이드 웹, 특정 인트라넷들, 광역 네트워크들, 또는 로컬 네트워크들의 일부일 수 있다. 네트워크는 하나 이상의 회사에 독점적인 이더넷, Wi-Fi 및 HTTP 프로토콜들과 같은 표준 통신 프로토콜들, 및 전술한 것의 다양한 조합들을 이용할 수 있다. 위에서 언급된 바와 같이 정보가 송신되거나 수신될 때 특정 이점들이 획득되지만, 본 명세서에 설명된 주제의 다른 양상들은 정보의 임의의 특정 방식의 송신으로 제한되지 않는다.[141] Each of the
[142] 예로서, 컴퓨팅 디바이스(710)는 네트워크를 통해 컴퓨팅 디바이스들(720, 730, 및 740)뿐만 아니라 저장 시스템(750)과 통신할 수 있는 웹 서버들을 포함할 수 있다. 예를 들어, 서버 컴퓨팅 디바이스들(710) 중 하나 이상은 정보, 웹 애플리케이션들 등을 송신하고 컴퓨팅 디바이스(720)의 디스플레이들(722)과 같은 디스플레이 상에 제시하기 위해 네트워크(760)를 사용할 수 있다. 이와 관련하여, 컴퓨팅 디바이스들(720, 730, 및 740)은 클라이언트 컴퓨팅 디바이스들로 간주될 수 있고 본 명세서에 설명된 특징들의 전부 또는 일부를 수행할 수 있다.[142] By way of example,
[143] 클라이언트 컴퓨팅 디바이스들(720, 730, 및 740) 각각은 위에서 설명된 바와 같은 하나 이상의 프로세서들, 메모리, 및 명령들을 갖는 서버 컴퓨팅 디바이스들(710)과 유사하게 구성될 수 있다. 각각의 클라이언트 컴퓨팅 디바이스(720, 730, 또는 740)는 사용자에 의한 사용을 위하여 의도된 개인용 컴퓨팅 디바이스일 수 있고, 중앙 처리 유닛(CPU), 데이터 및 명령들을 저장하는 메모리(예컨대, RAM 및 내부 하드 드라이브들), 디스플레이(722)와 같은 디스플레이(예컨대, 스크린을 갖는 모니터, 터치스크린, 프로젝터, 텔레비전, 또는 정보를 디스플레이하도록 동작가능한 다른 디바이스), 및 사용자 입력(724)(예컨대, 마우스, 키보드, 터치스크린, 또는 마이크로폰)과 같이 개인용 컴퓨팅 디바이스와 관련하여 정상적으로 사용되는 컴포넌트들 전부를 가질 수 있다. 클라이언트 컴퓨팅 디바이스는 비디오 스트림들을 기록하고/하거나 이미지들을 캡처하기 위한 카메라, 스피커들, 네트워크 인터페이스 디바이스, 및 이러한 요소들을 서로 접속하기 위하여 사용되는 컴포넌트들 전부를 또한 포함할 수 있다.[143] Each of the
[144] 본 명세서에서, "~하도록 구성된"이라는 문구는 컴퓨터 시스템들, 하드웨어, 또는 컴퓨터 프로그램의 일부와 관련된 상이한 맥락들에서 사용된다. 시스템이 하나 이상의 동작을 수행하도록 구성된다고 할 때, 이는 시스템이 동작시에 시스템으로 하여금 하나 이상의 동작을 수행하게 하는, 시스템 상에 설치된 적절한 소프트웨어, 펌웨어 및/또는 하드웨어를 갖는다는 것을 의미한다. 일부 하드웨어가 하나 이상의 동작을 수행하도록 구성된다고 할 때, 이는 하드웨어가 동작시에 입력을 수신하고 입력에 따라 하나 이상의 동작에 대응하는 출력을 생성하는 하나 이상의 회로를 포함한다는 것을 의미한다. 컴퓨터 프로그램이 하나 이상의 동작을 수행하도록 구성된다고 할 때, 이는 컴퓨터 프로그램이 하나 이상의 컴퓨터에 의해 실행될 때 하나 이상의 컴퓨터로 하여금 하나 이상의 동작을 수행하게 하는 하나 이상의 프로그램 명령들을 포함한다는 것을 의미한다.[144] In this specification, the phrase "configured to" is used in different contexts relating to computer systems, hardware, or a portion of a computer program. When a system is configured to perform one or more operations, it is meant that the system has appropriate software, firmware and/or hardware installed on the system that, when operated, causes the system to perform one or more operations. When some hardware is said to be configured to perform one or more operations, it is meant that the hardware includes one or more circuitry that, in operation, receives inputs and generates outputs corresponding to the one or more operations in accordance with the inputs. When a computer program is configured to perform one or more operations, it is meant that the computer program includes one or more program instructions that, when executed by one or more computers, cause the one or more computers to perform the one or more operations.
[145] 달리 언급하지 않는 한, 전술한 대안 예들은 상호 배타적이지 않고, 고유의 장점들을 달성하기 위해 다양한 조합들로 구현될 수 있다. 앞서 논의된 특징들의 이들 및 다른 변형들 및 조합들이 청구항들에 의해 정의되는 주제를 벗어나지 않고 이용될 수 있기 때문에, 실시예들의 전술한 설명은 청구항들에 의해 정의되는 주제의 제한이 아니라 예시로서 간주되어야 한다. 게다가, 본 명세서에 설명된 예들의 제공은 물론, "~와 같은", "~를 포함하는" 등으로서 표현된 절들은 청구항들의 주제를 특정 예들로 제한하는 것으로 해석되어서는 안 되며; 오히려, 예들은 많은 가능한 실시예들 중 하나만을 예시하기 위한 것이다. 또한, 상이한 도면들 내의 동일한 참조 번호들은 동일하거나 유사한 요소들을 식별할 수 있다.[145] Unless otherwise noted, the alternative examples described above are not mutually exclusive and may be implemented in various combinations to achieve the inherent advantages. Since these and other variations and combinations of features discussed above may be used without departing from the subject matter defined by the claims, the foregoing description of embodiments is to be regarded as illustrative and not restrictive of the subject matter defined by the claims. should be Moreover, as well as the provision of examples described herein, clauses expressed as “such as”, “comprising”, etc. are not to be construed as limiting the subject matter of the claims to specific examples; Rather, the examples are intended to illustrate only one of many possible embodiments. Also, like reference numbers in different drawings may identify the same or similar elements.
Claims (20)
하나 이상의 시간 단계들의 각각의 예보 수평(forecasting horizon)들에 대한 단기(short-term) 및 장기(long-term) 시간 특성들을 결정하기 위한 시퀀스-시퀀스 계층(sequence-to-sequence layer) 및 시간 자기 주의 계층(temporal self-attention layer)을 포함하고,
상기 시퀀스-시퀀스 계층은 하나 이상의 컴퓨터들에 의해 구현되고, 하나 이상의 시간 단계들의 상기 각각의 예보 수평들에 대한 단기 시간 특성들을 결정하도록 구성되며, 상기 시퀀스-시퀀스 계층은,
각각의 과거의 시간 기간들 동안에 캡처된 정적 공변량(static covariate)들 및 시변 입력 데이터에 기초하여 인코더 벡터들을 생성하기 위한 하나 이상의 RNN(recurrent neural network) 인코더들, 및
하나 이상의 RNN 디코더를 포함하고, 각각의 RNN 디코더는 상기 인코더 벡터들, 상기 정적 공변량들, 및 시변 공지 미래 입력 데이터에 기초하여 각각의 미래의 시간 기간에 대한 단기 패턴을 예측하도록 구성되며;
상기 시간 자기 주의 계층은 상기 하나 이상의 컴퓨터들에 의해 구현되고, 상기 각각의 예보 수평들에 대한 장기 시간 특성들을 캡처하도록 구성되며, 상기 시간 자기 주의 계층은,
상기 정적 공변량들, 상기 각각의 과거의 시간 기간들 동안 캡처된 상기 시변 입력 데이터, 및 상기 시변 공지 미래 입력 데이터에 기초하여 각각의 수평에 대한 예보를 생성하도록 구성된 멀티-헤드 주의 계층(multi-head attention layer)을 포함하는, 다중 수평 예보를 위한 시스템.A system for multi-horizon forecasting, comprising:
A sequence-to-sequence layer and temporal magnetism for determining short-term and long-term temporal characteristics for respective forecasting horizons of one or more time steps. including a temporal self-attention layer,
wherein the sequence-sequence layer is implemented by one or more computers and is configured to determine short-term temporal characteristics for the respective forecast horizontals of one or more time steps, the sequence-sequence layer comprising:
one or more recurrent neural network (RNN) encoders for generating encoder vectors based on time-varying input data and static covariates captured during respective past time periods, and
one or more RNN decoders, each RNN decoder configured to predict a short-term pattern for each future time period based on the encoder vectors, the static covariates, and time-varying known future input data;
wherein the temporal self-attention layer is implemented by the one or more computers and is configured to capture long-term temporal characteristics for each of the forecast horizontals, the temporal self-attention layer comprising:
a multi-head attention layer configured to generate a forecast for each horizontal based on the static covariates, the time-varying input data captured during each of the past time periods, and the time-varying known future input data. A system for multi-horizontal forecasting, including an attention layer.
변수 선택 계층을 더 포함하고, 상기 변수 선택 계층은 상기 정적 공변량들, 각각의 과거의 시간 기간들 동안 캡처된 상기 시변 입력 데이터 또는 상기 시변 공지 미래 입력 데이터 중 하나 이상을 포함하는 각각의 입력 변수에 대한 변수 선택 가중치들을 생성하도록 구성되는, 다중 수평 예보를 위한 시스템.According to claim 1,
further comprising a variable selection layer, wherein the variable selection layer is configured for each input variable comprising one or more of the static covariates, the time-varying input data captured during respective past time periods, or the time-varying known future input data. A system for multi-horizontal forecasting, configured to generate variable selection weights for
상기 변수 선택 계층은 복수의 변수 선택기들을 포함하고, 상기 복수의 변수 선택기들 중 각각의 변수 선택기는 각각의 미래 또는 과거의 시간 기간에 입력 변수들에 대한 변수 선택 가중치들을 생성하도록 구성되는, 다중 수평 예보를 위한 시스템.3. The method of claim 2,
wherein the variable selection layer comprises a plurality of variable selectors, each variable selector of the plurality of variable selectors configured to generate variable selection weights for input variables in a respective future or past time period. system for forecasting.
상기 변수 선택 계층은 상기 정적 공변량들의 선택을 위한 변수 선택 가중치들을 생성하도록 구성되는 변수 선택기를 포함하는, 다중 수평 예보를 위한 시스템.4. The method of claim 2 or 3,
wherein the variable selection layer comprises a variable selector configured to generate variable selection weights for selection of the static covariates.
상기 정적 공변량들의 선택을 위한 상기 변수 선택 가중치들에 기초하여 콘텍스트 벡터(context vector)들을 인코딩하도록 구성되는 하나 이상의 정적 공변량 인코더들을 더 포함하는, 다중 수평 예보를 위한 시스템.5. The method of claim 4,
and one or more static covariate encoders configured to encode context vectors based on the variable selection weights for selection of the static covariates.
상기 인코딩된 콘텍스트 벡터들은 상기 변수 선택 계층 내의 상기 복수의 변수 선택기들 각각에 전달되는, 다중 수평 예보를 위한 시스템.6. The method of claim 5,
and the encoded context vectors are passed to each of the plurality of variable selectors in the variable selection layer.
상기 인코딩된 콘텍스트 벡터들은 정적 보강 계층(static enrichment layer)에 전달되고, 상기 정적 보강 계층은 복수의 GRN(gated residual network)들을 포함하고,
각각의 GRN은 각각의 미래 또는 과거의 시간 기간을 할당받고, 각각의 GRN은 자신의 각각의 시간 기간에 대응하는 시간 역학에 영향을 미치는 상기 인코딩된 콘텍스트 벡터들 내의 정적 공변량들의 가중치를 증가시키도록 구성되는, 다중 수평 예보를 위한 시스템.7. The method according to claim 5 or 6,
The encoded context vectors are passed to a static enrichment layer, the static enrichment layer comprising a plurality of gated residual networks (GRNs),
Each GRN is assigned a respective future or past time period, each GRN increasing the weight of the static covariates in the encoded context vectors that affect the time dynamics corresponding to its respective time period A system for multi-level forecasting, which is constructed.
상기 정적 보강 계층 내의 각각의 GRN의 출력은 상기 멀티-헤드 주의 계층에 입력되는 데이터세트를 형성하는, 다중 수평 예보를 위한 시스템.8. The method of claim 7,
and the output of each GRN in the static reinforcement layer forms a dataset that is input to the multi-head attention layer.
각각의 GRN의 출력은 상기 멀티-헤드 주의 계층의 각각의 마스크에 입력되고, 각각의 마스크는 인과적 예측(causal prediction)을 위한 각각의 시간 기간에 대응하는, 다중 수평 예보를 위한 시스템.9. The method of claim 8,
The output of each GRN is input to a respective mask of the multi-head attention layer, each mask corresponding to a respective time period for causal prediction.
각각의 미래의 시간 기간에 대한 상기 예보는 변위치 예보(quantile forecast)로 변환되는, 다중 수평 예보를 위한 시스템.According to claim 1,
wherein the forecast for each future time period is converted into a quantile forecast.
복수의 게이팅 네트워크들을 더 포함하고, 각각의 게이팅 네트워크는 공유 가중치들을 갖는 복수의 GLU(gated linear unit)들을 포함하는, 다중 수평 예보를 위한 시스템.According to claim 1,
A system for multiple horizontal forecasting, further comprising a plurality of gating networks, each gating network comprising a plurality of gated linear units (GLUs) having shared weights.
상기 복수의 GLU들은 상기 다중 수평 예보의 출력 예측에 영향을 덜 미치는 변수들의 기여들을 감소시키도록 구성되는, 다중 수평 예보를 위한 시스템.12. The method of claim 11,
and the plurality of GLUs are configured to reduce contributions of variables less influencing the output prediction of the multi-horizontal forecast.
시퀀스-시퀀스 계층 및 시간 자기 주의 계층을 실행하는 하나 이상의 컴퓨터들에 의해, 하나 이상의 시간 단계들의 각각의 예보 수평들에 대한 단기 및 장기 시간 특성들을 결정하는 단계 － 상기 결정하는 단계는,
상기 시퀀스-시퀀스 계층 내의 하나 이상의 RNN(recurrent neural network) 인코더들을 사용하여, 각각의 과거의 시간 기간들 동안에 캡처된 정적 공변량들 및 시변 입력 데이터에 기초하여 인코더 벡터들을 생성하는 단계 － 정적 공변량들은 상기 시변 입력 데이터에서 그리고 각각의 미래의 시간 기간들에서 표현된 시간에 걸쳐 일정한 데이터임 －; 및
상기 시퀀스-시퀀스 계층 내의 하나 이상의 RNN 디코더들을 사용하여, 상기 인코더 벡터들, 상기 정적 공변량들 및 시변 공지 미래 입력 데이터에 기초하여 각각의 미래의 시간 기간에 대한 단기 패턴을 예측하는 단계를 포함함 －; 및
멀티-헤드 주의 계층을 갖는 시간 자기 주의 계층을 구현하는 상기 하나 이상의 컴퓨터들에 의해, 상기 정적 공변량들, 상기 각각의 과거의 시간 기간들 동안 캡처된 상기 시변 입력 데이터, 및 상기 시변 공지 미래 입력 데이터에 기초하여 상기 각각의 예보 수평들에 대한 장기 시간 특성들을 캡처하는 단계를 포함하는, 다중 수평 예보를 위한 방법.A method for multi-level forecasting, comprising:
determining, by one or more computers executing a sequence-sequence layer and a temporal self-attention layer, short-term and long-term temporal characteristics for respective forecast horizontals of one or more temporal steps, said determining comprising:
generating encoder vectors based on time-varying input data and static covariates captured during respective past time periods, using one or more recurrent neural network (RNN) encoders in the sequence-sequence layer, wherein the static covariates are data that is constant over time expressed in time-varying input data and in respective future time periods; and
predicting a short-term pattern for each future time period based on the encoder vectors, the static covariates and time-varying known future input data, using one or more RNN decoders in the sequence-sequence layer - ; and
the static covariates, the time-varying input data captured during each of the past time periods, and the time-varying known future input data by the one or more computers implementing a temporal self-attention layer having a multi-headed attention layer capturing long-term temporal characteristics for each of the forecast levels based on
변수 선택 계층에 의해, 상기 정적 공변량들, 각각의 과거의 시간 기간들 동안 캡처된 상기 시변 입력 데이터, 또는 상기 시변 공지 미래 입력 데이터 중 하나 이상을 포함하는 각각의 입력 변수에 대한 변수 선택 가중치들을 생성하는 단계를 더 포함하는, 다중 수평 예보를 위한 방법.14. The method of claim 13,
generate variable selection weights for each input variable comprising one or more of the static covariates, the time-varying input data captured during respective past time periods, or the time-varying known future input data, by a variable selection layer A method for multi-level forecasting, further comprising the step of:
상기 변수 선택 계층은 복수의 변수 선택기들을 포함하고, 상기 복수의 변수 선택기들 중 각각의 변수 선택기는 각각의 미래 또는 과거의 시간 기간에 입력 변수들에 대한 변수 선택 가중치들을 생성하도록 구성되는, 다중 수평 예보를 위한 방법.15. The method of claim 14,
wherein the variable selection layer comprises a plurality of variable selectors, each variable selector of the plurality of variable selectors configured to generate variable selection weights for input variables in a respective future or past time period. Methods for forecasting.
상기 변수 선택 계층은 상기 정적 공변량들의 선택을 위한 변수 선택 가중치들을 생성하도록 구성되는 변수 선택기를 포함하는, 다중 수평 예보를 위한 방법.16. The method of claim 14 or 15,
wherein the variable selection layer comprises a variable selector configured to generate variable selection weights for selection of the static covariates.
하나 이상의 정적 공변량 인코더들에 의해, 상기 정적 공변량들의 선택을 위한 상기 변수 선택 가중치들에 기초하여 콘텍스트 벡터들을 인코딩하는 단계를 더 포함하는, 다중 수평 예보를 위한 방법.14. The method of claim 13,
and encoding, by one or more static covariate encoders, context vectors based on the variable selection weights for selection of the static covariates.
상기 인코딩된 콘텍스트 벡터들을 상기 변수 선택 계층 내의 상기 복수의 변수 선택기들 각각에 전달하는 단계를 더 포함하는, 다중 수평 예보를 위한 방법.18. The method of claim 17,
and passing the encoded context vectors to each of the plurality of variable selectors in the variable selection layer.
상기 인코딩된 콘텍스트 벡터들을 정적 보강 계층에 전달하는 단계를 더 포함하고, 상기 정적 보강 계층은 복수의 GRN(gated residual network)들을 포함하고,
각각의 GRN은 각각의 미래 또는 과거의 시간 기간을 할당받고, 각각의 GRN은 자신의 각각의 시간 기간에 대응하는 시간 역학에 영향을 미치는 상기 인코딩된 콘텍스트 벡터들 내의 정적 공변량들의 가중치를 증가시키도록 구성되는, 다중 수평 예보를 위한 방법.19. The method according to claim 17 or 18,
further comprising passing the encoded context vectors to a static enhancement layer, wherein the static enhancement layer includes a plurality of gated residual networks (GRNs);
Each GRN is assigned a respective future or past time period, each GRN increasing the weight of the static covariates in the encoded context vectors that affect the time dynamics corresponding to its respective time period Constructed, method for multi-level forecasting.
상기 동작들은:
시퀀스-시퀀스 계층, 및 멀티-헤드 자기 계층을 갖는 시간 자기 주의 계층을 실행하는 상기 하나 이상의 컴퓨터들 중의 하나 이상의 제1 컴퓨터들에 의해, 각각의 미래의 시간 기간들에 대한 단기 및 장기 시간 특성들을 결정하는 동작 － 상기 결정하는 동작은,
상기 시퀀스-시퀀스 계층 내의 하나 이상의 RNN(recurrent neural network) 인코더들을 사용하여, 각각의 과거의 시간 기간들 동안에 캡처된 정적 공변량들 및 시변 입력 데이터에 기초하여 인코더 벡터들을 생성하는 동작 － 정적 공변량들은 상기 시변 입력 데이터에서 그리고 상기 각각의 미래의 시간 기간들에서 표현된 시간에 걸쳐 일정한 데이터임 －; 및
상기 시퀀스-시퀀스 계층 내의 하나 이상의 RNN 디코더들을 사용하여, 상기 인코더 벡터들, 상기 정적 공변량들 및 시변 공지 미래 입력 데이터에 기초하여 각각의 미래의 시간 기간에 대한 단기 패턴을 예측하는 동작을 포함함 －; 및
멀티-헤드 주의 계층을 갖는 상기 시간 자기 주의 계층을 구현하는 상기 하나 이상의 컴퓨터들 중의 하나 이상의 제2 컴퓨터들에 의해, 상기 정적 공변량들, 상기 각각의 과거의 시간 기간들 동안 캡처된 상기 시변 입력 데이터, 및 상기 시변 공지 미래 입력 데이터에 기초하여 각각의 예보 수평들에 대한 장기 시간 특성들을 캡처하는 동작을 포함하는, 컴퓨터 판독가능 저장 매체.One or more computer-readable storage media storing instructions operable when performed by one or more computers to cause the one or more computers to perform operations, comprising:
The operations are:
short- and long-term temporal characteristics for respective future time periods by one or more first computers executing a sequence-sequence layer, and a temporal self-attention layer having a multi-head magnetic layer; Determining action - The determining action is
generating encoder vectors based on time-varying input data and static covariates captured during respective past time periods, using one or more recurrent neural network (RNN) encoders in the sequence-sequence layer, wherein the static covariates are data that is constant over time expressed in time-varying input data and in said respective future time periods; and
predicting a short-term pattern for each future time period based on the encoder vectors, the static covariates and time-varying known future input data, using one or more RNN decoders in the sequence-sequence layer; ; and
the static covariates, the time-varying input data captured during the respective past time periods, by one or more second computers of the one or more computers implementing the temporal self-attention layer having a multi-headed attention layer. and capturing long-term temporal characteristics for respective forecast levels based on the time-varying known future input data.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201962949904P | 2019-12-18 | 2019-12-18 | |
US62/949,904 | 2019-12-18 | ||
PCT/US2020/062130 WO2021126500A1 (en) | 2019-12-18 | 2020-11-25 | Processing multi-horizon forecasts for time series data |
Publications (1)
Publication Number | Publication Date |
---|---|
KR20220058626A true KR20220058626A (en) | 2022-05-09 |
Family
ID=73854953
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
KR1020227011809A KR20220058626A (en) | 2019-12-18 | 2020-11-25 | Multi-horizontal forecast processing for time series data |
Country Status (5)
Country | Link |
---|---|
US (1) | US20230018125A1 (en) |
EP (1) | EP4022516A1 (en) |
KR (1) | KR20220058626A (en) |
CN (1) | CN114503124A (en) |
WO (1) | WO2021126500A1 (en) |
Families Citing this family (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN114124734B (en) * | 2021-11-20 | 2023-04-07 | 四川大学 | Network traffic prediction method based on GCN-Transformer integration model |
CN114374616B (en) * | 2021-12-30 | 2024-04-26 | 中国电信股份有限公司 | Energy consumption evaluation method, device, equipment and medium |
CN115118365B (en) * | 2022-06-22 | 2023-07-07 | 中国人民解放军国防科技大学 | Spectrum prediction interpretation method and system based on mask |
CN115204470A (en) * | 2022-06-24 | 2022-10-18 | 中远海运科技股份有限公司 | Travel time prediction method and system |
CN116151459A (en) * | 2023-02-28 | 2023-05-23 | 国网河南省电力公司电力科学研究院 | Power grid flood prevention risk probability prediction method and system based on improved Transformer |
Family Cites Families (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11080595B2 (en) * | 2016-11-04 | 2021-08-03 | Salesforce.Com, Inc. | Quasi-recurrent neural network based encoder-decoder model |
-
2020
- 2020-11-25 KR KR1020227011809A patent/KR20220058626A/en unknown
- 2020-11-25 WO PCT/US2020/062130 patent/WO2021126500A1/en unknown
- 2020-11-25 US US17/782,865 patent/US20230018125A1/en active Pending
- 2020-11-25 EP EP20825361.7A patent/EP4022516A1/en active Pending
- 2020-11-25 CN CN202080070290.0A patent/CN114503124A/en active Pending
Also Published As
Publication number | Publication date |
---|---|
EP4022516A1 (en) | 2022-07-06 |
WO2021126500A1 (en) | 2021-06-24 |
US20230018125A1 (en) | 2023-01-19 |
CN114503124A (en) | 2022-05-13 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
Chen et al. | Learning graph structures with transformer for multivariate time-series anomaly detection in IoT | |
KR20220058626A (en) | Multi-horizontal forecast processing for time series data | |
Ouyang et al. | Agricultural commodity futures prices prediction via long-and short-term time series network | |
Ke et al. | Empirical analysis of optimal hidden neurons in neural network modeling for stock prediction | |
Li et al. | A study of the non-linear adjustment for analogy based software cost estimation | |
Guthke | Defensible model complexity: A call for data‐based and goal‐oriented model choice | |
Jiang et al. | Intermittent demand forecasting for spare parts in the heavy-duty vehicle industry: a support vector machine model | |
Grinblat et al. | Solving nonstationary classification problems with coupled support vector machines | |
Liu et al. | Multivariate time-series forecasting with temporal polynomial graph neural networks | |
Festag et al. | Generative adversarial networks for biomedical time series forecasting and imputation | |
Mohtasham Khani et al. | A deep learning-based method for forecasting gold price with respect to pandemics | |
Saravagi et al. | Indian stock market analysis and prediction using LSTM model during COVID-19 | |
CN117422181B (en) | Fuzzy label-based method and system for early warning loss of issuing clients | |
Precioso et al. | Thresholding methods in non-intrusive load monitoring | |
Faustryjak et al. | Forward forecast of stock prices using LSTM neural networks with statistical analysis of published messages | |
Wei et al. | Sequential recommendation based on long-term and short-term user behavior with self-attention | |
Pang et al. | A robust approach for multivariate time series forecasting | |
Siddiqa et al. | Most recent changepoint detection in censored panel data | |
Abdelaziz et al. | An Epsilon constraint method for selecting Indicators for use in Neural Networks for stock market forecasting | |
Ghosh et al. | Panic Selling Analysis in Stock Market | |
Shen et al. | Bitcoin Return Volatility Forecasting: A Comparative Study of GARCH Model and Machine Learning Model | |
Kasabov et al. | Spiking Neural Networks for Predictive and Explainable Modelling of Multimodal Streaming Data with a Case Study on Financial Time-series and Online News | |
CN117649209B (en) | Enterprise revenue auditing method, system, equipment and storage medium | |
KR102596740B1 (en) | Method for predicting macroeconomic factors and stock returns in the context of economic uncertainty news sentiment using machine learning | |
Xu | The Research on Stock Price Prediction Based on Machine Learning Model |