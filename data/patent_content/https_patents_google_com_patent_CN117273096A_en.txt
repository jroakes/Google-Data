CN117273096A - Performing matrix multiplication in hardware - Google Patents
Performing matrix multiplication in hardware Download PDFInfo
- Publication number
- CN117273096A CN117273096A CN202311134221.3A CN202311134221A CN117273096A CN 117273096 A CN117273096 A CN 117273096A CN 202311134221 A CN202311134221 A CN 202311134221A CN 117273096 A CN117273096 A CN 117273096A
- Authority
- CN
- China
- Prior art keywords
- value
- floating point
- point format
- weight
- input value
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 239000011159 matrix material Substances 0.000 title claims abstract description 149
- 230000004913 activation Effects 0.000 claims abstract description 169
- 238000000034 method Methods 0.000 claims abstract description 64
- 238000004364 calculation method Methods 0.000 claims description 68
- 238000013528 artificial neural network Methods 0.000 claims description 44
- 230000008569 process Effects 0.000 claims description 34
- 239000013598 vector Substances 0.000 description 123
- 239000000178 monomer Substances 0.000 description 47
- 238000012545 processing Methods 0.000 description 33
- 238000004590 computer program Methods 0.000 description 14
- 230000009471 action Effects 0.000 description 11
- 230000009467 reduction Effects 0.000 description 11
- 238000004891 communication Methods 0.000 description 7
- 238000011176 pooling Methods 0.000 description 5
- 230000006870 function Effects 0.000 description 4
- 238000012549 training Methods 0.000 description 4
- 238000012546 transfer Methods 0.000 description 4
- 238000006243 chemical reaction Methods 0.000 description 3
- 238000009825 accumulation Methods 0.000 description 2
- 238000003491 array Methods 0.000 description 2
- 230000008901 benefit Effects 0.000 description 2
- 238000010586 diagram Methods 0.000 description 2
- 230000003993 interaction Effects 0.000 description 2
- 230000003287 optical effect Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 230000007704 transition Effects 0.000 description 2
- 230000003213 activating effect Effects 0.000 description 1
- 230000006399 behavior Effects 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000010801 machine learning Methods 0.000 description 1
- 238000013507 mapping Methods 0.000 description 1
- 230000000873 masking effect Effects 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 230000004044 response Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 230000001960 triggered effect Effects 0.000 description 1
- 238000010977 unit operation Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F17/00—Digital computing or data processing equipment or methods, specially adapted for specific functions
- G06F17/10—Complex mathematical operations
- G06F17/16—Matrix or vector computation, e.g. matrix-matrix or matrix-vector multiplication, matrix factorization
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F7/00—Methods or arrangements for processing data by operating upon the order or content of the data handled
- G06F7/38—Methods or arrangements for performing computations using exclusively denominational number representation, e.g. using binary, ternary, decimal representation
- G06F7/48—Methods or arrangements for performing computations using exclusively denominational number representation, e.g. using binary, ternary, decimal representation using non-contact-making devices, e.g. tube, solid state device; using unspecified devices
- G06F7/483—Computations with numbers represented by a non-linear combination of denominational numbers, e.g. rational numbers, logarithmic number system or floating-point numbers
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F7/00—Methods or arrangements for processing data by operating upon the order or content of the data handled
- G06F7/38—Methods or arrangements for performing computations using exclusively denominational number representation, e.g. using binary, ternary, decimal representation
- G06F7/48—Methods or arrangements for performing computations using exclusively denominational number representation, e.g. using binary, ternary, decimal representation using non-contact-making devices, e.g. tube, solid state device; using unspecified devices
- G06F7/483—Computations with numbers represented by a non-linear combination of denominational numbers, e.g. rational numbers, logarithmic number system or floating-point numbers
- G06F7/487—Multiplying; Dividing
- G06F7/4876—Multiplying
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/30—Arrangements for executing machine instructions, e.g. instruction decode
- G06F9/30003—Arrangements for executing specific machine instructions
- G06F9/30007—Arrangements for executing specific machine instructions to perform operations on data operands
- G06F9/3001—Arithmetic instructions
- G06F9/30014—Arithmetic instructions with variable precision
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/048—Activation functions
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/06—Physical realisation, i.e. hardware implementation of neural networks, neurons or parts of neurons
- G06N3/063—Physical realisation, i.e. hardware implementation of neural networks, neurons or parts of neurons using electronic means
Abstract
Methods, systems, and apparatus for performing matrix multiplication using hardware circuitry are described. An example method begins by obtaining an input activation value and a weight input value in a first floating point format. The input activation value is multiplied by the weight input value to generate a product value in a second floating point format having a higher precision than the first floating point format. A partial sum value in a third floating point format having a higher precision than the first floating point format is obtained. The partial sum value and the product value are combined to generate an updated partial sum value having a third floating point format.
Description
Description of the division
The present application belongs to the divisional application of the Chinese patent application 201880019014.4 with the application date of 2018, 05 and 17.
Technical Field
The present description relates to performing neural network computations in hardware.
Background
Neural networks are machine learning models that employ one or more layers to generate output, e.g., classification, for received inputs. In addition to the output layer, some neural networks include one or more hidden layers. The output of each hidden layer is used as an input to another layer in the network (e.g., the next hidden layer or output layer of the network). Each layer of the network generates an output from the received inputs according to the current values of the respective parameter sets.
Disclosure of Invention
In general, this specification describes a special purpose hardware circuit to calculate neural network inferences.
In general, one innovative aspect of the subject matter described in this specification can be embodied in methods of performing matrix multiplication using hardware circuitry, the methods including the acts of: acquiring an input activation value and a weight input value by a matrix calculation unit of a hardware circuit, wherein the input activation value and the weight input value have a first floating point format; multiplying, by a multiplication circuit of the matrix calculation unit, the input activation value with the weight input value to generate a product value, the product value having a second floating point format having a higher precision than the first floating point format and having a dynamic range at least as large as the dynamic range of the first floating point format; obtaining, by the matrix computing unit, a portion and a value of a third floating point format, the third floating point format having a higher precision than the first floating point format and having a dynamic range at least as large as a dynamic range of the first floating point format; and combining, by a summing circuit of the hardware circuit, at least the partial sum value and the product value to generate an updated partial sum value having a third floating point format.
Embodiments of this aspect may include one or more of the following optional features. The precision of the floating point format may be determined based on a count of available bits of the significand in the floating point format and the dynamic range of the floating point format may be determined based on a count of available bits of the exponent in the floating point format. The second floating point format has the same dynamic range as the first floating point format and the third floating point format has the same dynamic range as the first floating point format. The third floating point format has a higher precision than the second floating point format.
The hardware circuitry may be configured to perform a calculation of a neural network having a plurality of layers, and the input activation value and the weight input value may be associated with one of the plurality of layers.
The method may include the acts of: acquiring an original activation value and an original weight value of a first matrix calculation monomer with a third floating point format; converting the original activation value to a first floating point format to generate an input activation value; and converting the original weight values to a first floating point format to generate weight input values. The method may further comprise the acts of: receiving a request to process the original activation value with enhanced accuracy; generating an activation enhancement precision value of the input value, the activation enhancement precision value being a measure of a difference between the activation input value and the original activation value; and generating a weight enhancement precision value of the weight input value, the weight enhancement precision value being a measure of the difference between the weight input value and the original weight value. Multiplying the activation input value with the weight input value to generate a product value may include the acts of: the multiplication circuit of the matrix calculation unit multiplies the input value by the weight input value, multiplies the input value by the weight enhancement precision value, multiplies the weight input value by the activation enhancement precision value, and multiplies the activation enhancement precision value by the weight enhancement precision value, and combines the multiplied products to generate a product value.
Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs, each configured to perform the actions of the methods, recorded on one or more computer storage devices. A system of one or more computers may be configured to perform particular operations or actions by way of software, firmware, hardware, or any combination thereof installed on the system, which in operation may cause the system to perform the actions. One or more computer programs may be configured to perform particular operations or actions by virtue of including instructions that, when executed by data processing apparatus, cause the apparatus to perform the actions.
Another innovative aspect of the subject matter described in this specification can be embodied in hardware circuits including a matrix computing unit configured to perform a first set of operations including: acquiring an activation input value and a weight input value, wherein the activation input value and the weight input value both have a first floating point format; storing the weight input value in a weight register configured to store a value having a first floating point format; multiplying the activation input value with the weight input value using a multiplication circuit of the hardware circuit to generate a product value, the product value having a second floating point format, the second floating point format having a higher precision than the first floating point format and having a dynamic range at least as large as a dynamic range of the first floating point format; obtaining a partial sum value of a third floating point format, the third floating point format having a higher precision than the first floating point format and having a dynamic range at least as large as the dynamic range of the first floating point format; storing the partial sum value in a sum in a register, the sum in the register configured to store a value having a third floating point format; and combining the partial sum value with the product value using a summing circuit of the matrix calculation unit to generate an updated partial sum value having a third floating point format.
Embodiments of this aspect may include one or more of the following optional features. The first set of operations may include storing an activation input value in an activation register configured to store a value having a first floating point format. The first set of operations may include storing weight input values in weight registers configured to store values having a first floating point format. The first set of operations may include storing the partial sum value in a sum in a register, the sum in the register configured to store the value having the third floating point format. The hardware circuit may comprise an external summing circuit external to the matrix calculation unit. The first set of operations may include receiving a request to process an original activation value with enhanced accuracy; generating an activation enhancement precision value of the input value, the activation enhancement precision value being a measure of a difference between the activation input value and the original activation value; and generating a weight enhancement precision value of the weight input value, the weight enhancement precision value being a measure of the difference between the weight input value and the original weight value. Multiplying the activation input value with the weight input value to generate a product value may include the acts of: the multiplication circuit of the matrix calculation unit multiplies the input value by the weight input value, multiplies the input value by the weight enhancement precision value, multiplies the weight input value by the activation enhancement precision value, and multiplies the activation enhancement precision value by the weight enhancement precision value. The external summing circuit may be configured to perform a second set of operations including combining the multiplied products to generate a product value.
Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs, each configured to perform the actions of the first and second sets of operations, recorded on one or more computer storage devices. A system of one or more computers may be configured to perform particular operations or actions by way of software, firmware, hardware, or any combination thereof installed on the system, which in operation may cause the system to perform the actions. One or more computer programs may be configured to perform particular operations or actions by virtue of including instructions that, when executed by data processing apparatus, cause the apparatus to perform the actions.
Particular embodiments of the subject matter described in this specification can be implemented to realize one or more of the following advantages. The hardware circuitry may perform matrix multiplication with reduced overflow and/or loss of precision. The hardware circuit may perform matrix multiplication with enhanced precision beyond that provided by the floating point format of the input registers in the hardware circuit. The hardware circuitry may perform matrix multiplication with reduced overflow on values using an input matrix stored in an IEEE single precision floating point format, even though the hardware circuitry stores the input matrix values in a floating point format with 16 bits.
The details of one or more embodiments of the subject matter of this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Drawings
FIG. 1A illustrates a high-level diagram of an example dedicated hardware chip for training a neural network.
FIG. 1B illustrates a high-level example of a computing core.
FIG. 1C illustrates an example neural network processing system.
Fig. 2 illustrates an example architecture including a matrix multiplication unit. The matrix multiplication unit is a two-dimensional systolic array.
Fig. 3 illustrates an example architecture of multiple monomers within a systolic array.
Fig. 4 shows an example architecture of a monomer in a vector calculation unit.
Fig. 5 shows an example architecture of a vector calculation unit.
FIG. 6 illustrates an example format of a floating point value.
Fig. 7 shows an example architecture of a multiplication circuit of matrix computation monomers.
Fig. 8 is a flowchart of an example process of performing matrix multiplication using a matrix calculation unit.
FIG. 9 is a flowchart of an example process for performing an activation input value multiplied by a weight input value.
Reference numerals and signs in the various figures refer to the same elements.
Detailed Description
A neural network with multiple layers may be used to perform the calculations. For example, given an input, the neural network may calculate an inference for that input. The neural network calculates this inference by processing the input through each layer in the neural network. Each layer receives an input and processes the input according to a set of weights for that layer to generate an output.
Thus, to calculate an inference from the received inputs, the neural network receives the inputs and processes the inputs through each of the neural network layers to generate an inference, wherein the output from one neural network layer is provided as an input to the next neural network layer. Data input to a neural network layer (e.g., input to the neural network or output to layers of the neural network layer that are sequentially below the layer) may be referred to as an activation input to the layer.
In some implementations, the layers in the neural network are arranged in a sequence. In some other implementations, the layers are arranged as directed graphs. That is, any particular layer may receive multiple inputs, multiple outputs, or both. The layers in the neural network may also be arranged such that the output of a layer may be sent back to the previous layer as input.
The neural network may also be trained to determine trained values of weights for layers in the neural network. Typically, during training, inputs are processed using a neural network and weights are adjusted based on outputs generated by the neural network for these inputs.
This specification describes dedicated hardware circuits that perform neural network computations, i.e., reasoning or training operations, including matrix multiplication operations performed by the neural network layer.
FIG. 1A illustrates a high-level diagram of an example dedicated hardware chip for training a neural network. As shown, a single dedicated hardware chip includes two independent processors (102 a, 102 b). Each processor (102 a, 102 b) contains two distinct cores: (1) Computing cores, i.e., very Long Instruction Word (VLIW) machines (103 a, 103 b) and (2) sparse computing cores, i.e., embedded layer accelerators (105 a, 105 b).
Each computational core (103 a, 103 b) is optimized for dense linear algebra problems. Each compute core executes its own stream of very long instruction word instructions.
An example sparse compute core (105 a, 105 b) maps ultra-sparse, high-dimensional data into dense, low-dimensional data such that the remaining layers process densely packed input data. For example, the sparse computational core may perform computations for any embedded layer in the neural network being trained.
To perform such sparse to dense mapping, the sparse computational core uses a pre-built look-up table, i.e., an embedded table. For example, when there is a series of query words entered as a user, each query word is converted into a hash identifier or a vector that is unithermally encoded. Using the identifier as a table index, the embedded table returns a corresponding dense vector, which may be an input activation vector for the next layer. The sparse computing core may also perform a reduction operation across search query terms to create a dense activation vector. Because the embedded tables can be large and unsuitable for limited capacity high bandwidth memory of one of the dedicated hardware chips, the sparse compute core performs an efficient sparse, distributed lookup. More details about sparse computing core functionality can be found in U.S. patent application Ser. No.15/016,486, entitled MATRIX PROCESSING APPARATUS, filed on day 2016, 2, 5.
FIG. 1B illustrates a high-level example of a computing core (101). The computational core may be a machine, i.e. a VLIW machine, that controls several computational units in parallel. Each computing core (101) comprises: scalar memory (104), vector memory (108), scalar processor (107), vector registers (106), and extended vector units (i.e., matrix multiplication units (MXU) (113), transpose units (XU) (114), and reduction and permutation units (reduction and permutation unit, RPU) (116)).
An example scalar processor executes a VLIW instruction fetch/execute loop and controls a compute core. After fetching and decoding the instruction bundles, the scalar processor itself executes instructions that are present in the scalar slots of the bundles using only a plurality of, multi-bit registers, i.e., 32-bit registers of the scalar processor (107) and scalar memory (104). The scalar instruction set includes normal arithmetic operations, such as those used in address calculation, load/store instructions, and branch instructions. The remaining instruction slots encode instructions for a vector processor or other extended vector unit (113, 114, 116). The decoded vector instruction is forwarded to a vector processor.
Scalar processor (107) may forward up to three scalar register values along with vector instructions to other processors and units to perform operations. The scalar processor may also retrieve the computation results directly from the vector processor. However, in some implementations, the example chip has a low bandwidth communication path from the vector processor to the scalar processor.
The vector instruction scheduler is located between the scalar processor and the vector processor. This scheduler receives decoded instructions from the non-scalar VLIW slots and broadcasts these instructions to the vector processor. The vector processor is described in detail with reference to fig. 1C.
An example scalar processor (107) accesses a small, fast private scalar memory (104), the scalar memory (104) being backed up by a larger but slower High Bandwidth Memory (HBM) (110). Similarly, the example scalar processor accesses a small, fast private vector memory (108), which vector memory (108) is also backed up by the HBM (110). Word-granularity access occurs between a scalar processor (107) and a scalar memory (104) or a vector processor and a vector memory (108). The granularity of the load and store between the vector processor and vector memory is a vector of 128 32-bit words. Direct memory access occurs between scalar memory (104) and HBM (110) and vector memory (108) and HBM (110). In some implementations, the transfer from HBM (110) to memory of processor (107) may only be accomplished through scalar or vector memory. In addition, there may not be a direct memory transfer between the scalar memory and the vector memory.
An instruction may specify an extended vector unit operation. There are two dimensions, 128 by 8 vector units, each of which can send a register value to the extended vector unit as an input operand along with each vector unit instruction being executed. Each expansion vector unit takes an input operand, performs a corresponding operation, and returns the result to the vector processor (306). The extension vector unit will be described with reference to fig. 4.
Fig. 1C illustrates an example application specific integrated circuit 100 for performing neural network computations. As shown, the chip contains two compute cores (103 a, 103 b) and two sparse compute cores (152 a, 152 b).
The chip has a shared area that includes a host interface (150) to a host computer, four stacks of high bandwidth memory (156 a-156 d) along the bottom, and an inter-chip interconnect (148) that connects the interface and memory together with data from other chips. Two stacks of high bandwidth memory (156 a-b, 156 c-d) are associated with each compute core (103 a, 103 b).
The chip stores the data in high bandwidth memory (156 c-d), reads the data into and out of vector memory (108) and processes the data. The computing core (103 b) itself includes vector memory (108) as on-chip S-RAM that is divided into two dimensions. The vector memory has an address space in which addresses hold floating point numbers, i.e., 128 numbers of 32 bits each. The computation core (103 b) further includes a computation unit that computes values and a scalar unit that controls the computation unit.
Vector processors consist of a two-dimensional array, 128 x 8, of vector processing units that all execute the same instruction in a single instruction, multiple data (SIMD) fashion. The vector processor has lanes and sub-lanes, i.e. 128 lanes and 8 sub-lanes. Within the channel, vector units communicate with each other through load and store instructions. Each vector unit may access one 4 byte value at a time. Vector units that do not belong to the same lane cannot communicate directly. These vector units must use a reduction/permutation unit described below.
The calculation unit includes vector registers, i.e. 32 vector registers, in a vector processing unit (106) that can be used for floating point operations and integer operations. The computation unit includes two Arithmetic Logic Units (ALUs) (126 c-d) to perform computations. One ALU (126 c) performs floating point addition and the other ALU (126 d) performs floating point multiplication. Various other operations may be performed by both ALUs (126 c-d), such as shifting, masking, and comparing. For example, the compute core (103 b) may want to register vector V 1 And a second vector register V 2 Add and put the result into the third vector register V 3 Is a kind of medium. To calculate the addition, the calculation core (103 b) performs operations a plurality of times, i.e., 1024 times, in one clock cycle. Using these registers as operands, each of the vector units can execute two ALU instructions and one load and one at the same time per clock cycle The instructions are stored. The base address for a load or store instruction may be calculated in a scalar processor and forwarded to a vector processor. Each of the vector units in each sub-channel may calculate its own offset address using various methods such as stride and address registers that are specifically indexed.
The computation unit also includes an Extended Unary Pipeline (EUP) that performs operations such as square root and reciprocal (116). Since these operations receive one operand at a time, the compute core (103 b) takes three clock cycles to perform these operations. Since the EUP process takes more than one clock cycle, there is a first-in, first-out data store to store the results. After the operation is completed, the result is stored in the FIFO. The compute core may later use a separate instruction to pull the data from the FIFO and place the data into the vector registers. The random number generator (120) allows the computational core (103 b) to generate random numbers per cycle, i.e. 128 random numbers per cycle.
As described above, each processor has three expansion vector units: a matrix multiplication unit (113) that performs a matrix multiplication operation; a cross-channel unit (XLU) including a transpose unit (XU) (114) that performs the transpose operation of the matrix (i.e., 128 by 128 matrix) and reduction and permutation units (illustrated in fig. 1C as separate units, about simple element 115 and permutation unit 116).
The matrix multiplication unit performs matrix multiplication between two matrices. Since the computation core needs to load a set of numbers as a matrix to be multiplied, a matrix multiplication unit (113) receives data. As shown, the data comes from vector registers (106). Each vector register contains 128 x 8 digits, i.e., 32-bit digits. However, when data is sent to the matrix multiplication unit (113), floating point conversion may occur to change the number to a smaller bit size, i.e., from 32 bits to 16 bits. The serializer (130) ensures when the digits are read out of the vector register, a two-dimensional array, i.e. a 128 by 8 matrix, is read as a set of 128 digits that are sent to the matrix multiplication unit (113) in each of the next eight clock cycles. After the matrix multiplication has completed its computation, the results are parallelized (132 a, 132 b), meaning that the result matrix is held for several clock cycles. For example, for a 128 x 8 array, 128 digits are held in each of 8 clock cycles and then pushed into a FIFO (transpose result FIFO (TRF) 134 or Multiplication Result FIFO (MRF) 136) so that a two-dimensional array of 128 x 8 digits can be grabbed in one clock cycle and stored in vector register (106).
In a period of a cycle (i.e., 128 cycles), the weight is shifted into the matrix multiplication unit (113) as a number for multiplication with a matrix. Once the matrix and weights are loaded, the computational core (103 b) may send a set of numbers, i.e., 128 x 8 numbers, to the matrix multiplication unit (113). Each row of the set may be multiplied by a matrix to produce a plurality of results, i.e., 128 results per clock cycle. When the computing core performs a matrix multiplication operation, the computing core also shifts the new set of numbers in the background to the next matrix that the computing core will multiply so that the next matrix is available when the computing process of the previous matrix is completed. Matrix multiplication unit (113) may process the weight inputs and the activation inputs and provide vectors of outputs to vector registers 106. The vector processing unit may process the output vector and store the processed output vector to the vector memory. For example, the vector processing unit may apply a nonlinear function to the output of the matrix multiplication unit to generate the activation value. In some implementations, the vector calculation unit 114 generates a normalized value, a combined value, or both. The vector of processed outputs may be used as an activation input to the matrix multiplication unit 112, for example, for use in subsequent layers in a neural network.
The transpose unit transposes the matrix. A transpose unit (114) receives the numbers and transposes the numbers such that the numbers on the channels are transposed using the numbers in the other dimension. In some implementations, the vector processor includes 128 x 8 vector units. Thus, to transpose a 128×128 matrix, a full matrix transpose requires 16 separate transpose instructions. Once transposed is complete, the transposed matrix will be available. However, explicit instructions for moving the transposed matrix into the vector register file are required.
The reduction/permutation unit (or units 115, 116) solves the problem of cross-channel communication by supporting various operations such as permutation, channel rotation, rotation permutation, channel reduction, permuted channel reduction, and segmented permuted channel reduction. As shown, these computations are separate, however the computational core may use one computation or another computation or one computation linked to another computation. A reduction unit (115) reduces each row of digits and feeds the digits into a permutation unit (116). The permutation unit changes the data between the different channels. The transpose unit, the reduction unit, the permutation unit, and the matrix multiplication unit all take more than one clock cycle to complete. Thus, each element has a FIFO associated with it so that the result of the computation can be pushed into the FIFO and later a separate instruction can be executed to pull the data out of the FIFO and into the vector register. By using FIFOs, the compute core does not need to hold multiple vector registers for long duration of operation. As shown, each of the cells fetches data from a vector register (106).
The compute core controls the compute units using scalar units. Scalar units have two main functions: (1) performing cycle counting and addressing; (2) Direct Memory Address (DMA) requests are generated such that the DMA controller moves data in the background between the high bandwidth memory (156 c-d) and the vector memory (108) and then to the inter-chip connection (148) to other chips in the example system. The scalar unit includes an instruction memory (104), instruction decode and issue (102), a scalar processing unit (107) including scalar registers (i.e., 32 bits), a scalar memory (104), and two ALUs (126 a, b) for performing operations twice per clock cycle. Scalar units may feed operands and immediate values to vector operations. Each instruction may be decoded and issued (102) from the instruction as an instruction bundle containing instructions executing on vector registers (106). Each instruction bundle is a Very Long Instruction Word (VLIW), where each instruction is multiple bit-widths divided into multiple instruction fields.
Fig. 2 illustrates an example architecture 200 including matrix multiplication units (MXU) 201a and 201 b. Each MXU is a two-dimensional systolic array. The array is wired to perform matrix multiplication operations. MXU multiplies a vector of 128 elements with a preloaded 128x128 matrix, where one multiplication per clock cycle has a constant throughput.
Each MXU can have 128 rows and 128 columns. MXU may be divided into identical blocks called tiles (tiles). For example, MXU may be divided into 32 tiles, each of which contains 32 rows by 16 columns. Each tile may be further divided into multiply-add subunit cells (cells). Each cell takes a vector data input operand, multiplies the operand with stored weights to obtain a result and adds the result to a partial sum to produce a new partial sum. In some implementations, the subunit monomers may be grouped into larger multi-monomers, i.e., 2 x 2 arrays of multiply-add subunit monomers or 4 x 4 arrays of multiply-add subunit monomers referred to as setecim monomers. Instead of moving the input data from one multiply-add subunit monomer to the next multiply-add subunit monomer at a rate of one per clock cycle, the data may be moved across the systolic array at one multi-monomer per clock cycle.
The matrix needs to be preloaded into MXU before starting a series of vector matrix multiplications. The data of this matrix is referred to as "weight" data. The weight matrix is transferred to the MXU by a bus connected to the MXU through a source bus and shifted into a weight shift register. The contents of the weight shift register are then loaded into the weight matrix register so that matrix multiplication can begin.
As shown in fig. 2, each MXU (e.g., 113a and 113 b) is connected to three buses, a first source bus (230 a, 240 b) for non-transposed weights, a second source bus (220 a, 220 b) for transposed weights, and a left-hand side bus (210 a, 210 b) for vector data to be multiplied by a matrix stored in the MXU. The MXU is connected to the bus by wires attached to the MXU edge. Each transpose unit (XU) (e.g., 114a and 114 b) is also connected to the first source bus and the second source bus.
The first source bus and the second source bus are multipurpose buses containing data sent from the vector processing unit to be consumed by XU or MXU. Data processing occurs in a vector processing data path that includes a vector register 206, a serialization processing unit 202, and a selection unit 204. There are several ways in which the vector processing unit may send the weights on the bus. The weights may be sent normally, "hi" or "low". Eight 32-bit floating point numbers per channel (one 32-bit floating point number per sub-channel) are rounded to bfoats, 16-bit floating point numbers. These values are packed into four pairs and sent to the MXU every other cycle over the course of 8 cycles. The difference between normal, "hi" and "low" is how the vector processing unit performs the floating point 32 to bfoat conversion. The weights may be packed, meaning that each of the eight 32-bit values for each lane contains a packed pair of bfoats. Sixteen values, instead of eight values, are sent to the MXU using the source bus in each of eight consecutive cycles. During the odd cycles, the lower 16 bits of each subchannel are sent to MXU, and during the even cycles, the upper 16 bits of each subchannel are sent to MXU. Additionally or alternatively, the weights may be sent in bytes. Each 32-bit operand contains a packed set of four 8-bit signed 2-complement integers. Each byte is converted into a modified signed magnitude. These values are sent to the MXU over the source bus in eight consecutive cycles.
The weights may be sent as non-transposed or transposed instructions using the first source bus or the second source bus and shifted into the weight shift registers. As described below, the contents of the weight shift register are loaded into the weight matrix when triggered using a load operation. The load path from the weight shift register to the weight matrix register is also the process of using byte pattern data to complete the conversion from modified signed amounts to bfoat. The load control bus indicates whether this transition is to be made.
Depending on the instruction being executed, the 32-bit value from the source bus may include a packed pair of 16-bit floating point values, with bits [15: the value in 0 represents (in time) an earlier value or a set of four 8-bit integers packed in modified by symbol value format, with bit [7: the value in 0 represents the earliest (in time) value and the other values that follow in turn. When the MXU receives data from the bus, the data values are evenly distributed across the MXU with value 0 on the left and value 127 on the right.
The LHS data bus carries 128 16-bit floating point numbers in a particular format, such as bfoat, to be multiplied by a matrix stored in the connected MXU. The data of the LHS data bus comes from the vector processing unit and passes through the transpose units, e.g., 114a and 114 b. When the LHS input reaches MXU, these values are evenly distributed over MXU, with value 0 to the left and value 127 to the right.
The results of the matrix multiplication are evenly distributed across the MXU and sent from the MXU to a Matrix Result FIFO (MRF), e.g., 136a and 136b. The results from the XU are sent to corresponding Transposed Result FIFOs (TRFs), e.g., 134a and 134b.
Fig. 3 illustrates an example architecture of multiple monomers within a matrix multiplication unit. As described above, the matrix multiplication unit is a two-dimensional systolic array. The array includes a plurality of multiply-add subunits that may be grouped into multiple monomers. In some implementations, a first dimension of the systolic array corresponds to a column of cells and a second dimension of the systolic array corresponds to a row of cells. The systolic array may have more rows than columns, more columns than rows, or an equal number of columns and rows. This specification describes certain processes for columns or vertically. However, different designs may be implemented for rows or horizontally.
In the illustrated example, the data registers 315a, 315b on the left hand side send vector data inputs to the rows of the array. Weight shift chains 301A and 301B send weight input values to columns of the array and weight shift chains 302a and 302B send weight input values to rows of the array. The shift chain is a wired path along which values may be transferred, for example, from the source bus and to each of a plurality of registers within the matrix multiplication unit.
Each weight shift register 305 is designed to shift weight content values from the source bus along the chain of weight shift registers 305. After all data is shifted in, the parallel copy operation ensures that all data is copied from the weight shift register 305 to the corresponding weight matrix register 325. While the data is in the weight matrix register 325, the data is used for multiplication for any number of cycles. During this time, more weights may (and typically are) shifted into the weight register 305 in the background to prepare for the next multiplication set.
The left hand side data registers 315a, 315b may receive vector data inputs. Each left hand side data register holds one LHS data item per clock cycle in one clock cycle. Each vector data input received by a plurality of monomers may be free flowing in a corresponding left-hand side register of the multi-monomers, such as left-hand side data registers 315a, 315 b. The left hand side data register stores vector data inputs, which may be provided by a vector register or an adjacent multi-cell located to the left of a given multi-cell, depending on the location of the multi-cell within the array. For example, if the multi-monomer 300 is located at the leftmost position within the systolic array of matrix multiplication units, then the vector data input is provided by a vector register. The vector registers may provide multiple different vector data inputs to the multi-cell 300, each of which may then be stored by a different one of the left-hand side data registers 315. Each row receives a value at each clock cycle regardless of the number of rows grouped into multiple cells.
Each left hand side register may be coupled to a cell along a first dimension of the multi-cell array. The connection of the left hand side register to the monomer is indicated by the dashed line in fig. 3. For example, left hand side data register 315a (left hand side data register) in the multi-cell is coupled to cells 350a and 350c of the first row. Similarly, left hand side data register 315b (second left hand side register) in the multi-cell is coupled to cells 350b and 350d of the second row. Each left hand side register 315 communicates stored vector data to the cell 350 to which the left hand side register is coupled. Thus, for a given number of cells extending along a first dimension (e.g., along a given row or along a given column), vector data inputs may be transferred to all of the cells in the multi-cell rather than just a single cell, thereby allowing the activation inputs to diffuse rapidly in the cell array, improving the efficiency of operation of the multi-cell.
Multiple vector data inputs may also be sent to adjacent left-hand registers so that multiple vector data inputs may be used at another multi-monomer of the array. This process allows shift vector input to be used in another particular multi-cell of the array.
Each cell 350 in the multi-cell 300 contains a stored weight value. The weights are loaded by shifting the weights into the monomers of the systolic array before starting the matrix multiplication process. Dedicated chains and weight shift registers are provided for weight shifting so that new weights can be shifted simultaneously with the execution of the previous matrix multiplication process. The weight inputs may be loaded into the multi-monomer in a manner that reduces the delay of the overall matrix multiplication process.
As described above, the weight shift chains 301, 302 may receive weight inputs from the source bus. The shift chain may send a plurality of corresponding weight inputs to the weight registers 325 associated with the multi-cell 300.
In some implementations, the weight shift register shifts the vector data input along one dimension (e.g., to the right) throughout the array, while shifting the weight input along one or two dimensions (e.g., to the right or downward) throughout the array. For example, within one clock cycle, each vector data input of multiple activation inputs at a multi-cell 300 may be shifted to a corresponding left-hand side data register in the next multi-cell in the same row. Each clock cycle, both horizontal data (left hand side data) and vertical data (partial sum) are shifted by one multi-monomer per clock cycle. The weights are shifted only when instructed by the system and may be shifted by 1, 2 or 4 rows (or columns) depending on the implementation and the instruction being executed.
Multiplexer 330 selects weights from weight shift registers 305 of first shift chain 301 or second shift chain 302 and forwards the selected inputs into a single row into weight matrix register 325. Although multiplexer 330 is shown outside the boundary line of cell 350, in some implementations multiplexer 330 resides within cell 350.
During a clock cycle, each multi-monomer may process a plurality of given weight inputs and a plurality of given activation inputs to generate a plurality of accumulated outputs. Typically, the processing includes a multiplication operation for multiplying the activation input with the stored weights. The accumulated output may also be passed down to adjacent multi-monomers along the same dimension as the given weight input. In some implementations, during a given clock cycle, weights are shifted by more than one multi-monomer to transition from one convolution calculation to another.
The accumulated output may be passed along the same column as the weight input, e.g., toward the bottom of the column in the array. In some implementations, the partial sum registers 310a, 311A transfer partial sum values from a previous multi-monomer into the multi-monomer. The array may include a portion storing the accumulated output from each column of the multi-cell and registers 310b, 311b. For each column of the multi-monomer, the product generated by the subunit monomers in the column is combined with the incoming portion and sum from the multi-monomer described above and then sent on as the next portion and sum. For some multi-cells, such as the multi-cells in the bottom column of the systolic array, the accumulated output may include a final accumulated value that may be transferred to the vector calculation unit. In some implementations, the final accumulation values are transferred directly from the bottom multiple monomers of the array to the vector calculation unit, while in other implementations, the final accumulation values are first stored in memory or processed by a different component before being sent to the vector calculation unit.
In some implementations, each monomer multiplies a given weight input with a left-hand side data input to generate a product value on each clock cycle. The monomer may then combine the product value with the partial sum value received from another monomer to generate an updated partial sum value. The monomer may then transmit the partial sum value to another monomer in the matrix calculation unit.
Fig. 4 illustrates an example architecture 400 of monomers within a systolic array.
The monomer may include an activation register 406 that stores an activation input. Depending on the location of the cells within the systolic array, the activation register may receive activation inputs from the left neighbor cell (i.e., the neighbor cell to the left of a given cell) or from a unified buffer. The monomer may include a weight register 402 that stores weight inputs. Depending on the location of the cells within the systolic array, the weight input may be transmitted from the top adjacent cell or from the weight fetcher interface. The monomers may also include a sum in register 404. The sum in register 404 may store the accumulated value from the top adjacent cells. The activation register 406 and the weight register 402 may be registers configured to store a value of a particular size, such as a floating point value having a particular format.
Multiplication circuit 408 may be used to multiply the weight input from weight register 402 with the activation input from activation register 406. Multiplication circuit 408 may output the product to summation circuit 410. In some implementations, the input and output values of multiplication circuit 408 may have different sizes and/or formats.
The summing circuit 410 may sum the product and the accumulated value from the sums in the registers 404 to generate a new accumulated value. The summing circuit 410 may then send the new accumulated value to another one of the registers located in the bottom adjacent cells. The new accumulated value may be used as an operand for the summation in the bottom neighbor cells. Summing circuit 410 may also accept the summed value from register 404 and send the summed value from register 404 to the bottom neighbor cell without adding the summed value from register 404 to the product from multiplication circuit 408. In some implementations, the input values of summing circuit 410 may have different sizes and/or formats. In some embodiments, some of the input and output values of summing circuit 410 may have different sizes and/or formats.
The cells may also shift the weight inputs and the activation inputs to neighboring cells for processing. For example, the weight path register 412 may send the weight input to another weight register in the bottom neighbor cell. The activation register 406 may send an activation input to another activation register in the right neighbor cell. Thus, both the weight input and the activation input may be reused by other monomers in the array at a subsequent clock cycle.
In some implementations, the monomer further includes a control register. The control register may store a control signal that determines whether a cell should shift a weight input or an activation input to an adjacent cell. In some implementations, shifting the weight input or activating the input takes one or more clock cycles. The control signal may also determine whether the activation input or the weight input is communicated to the multiplication circuit 408, or may determine whether the multiplication circuit 408 operates on the activation input and the weight input. The control signal may also be transferred to one or more adjacent cells, for example using wires.
In some implementations, the weights are pre-shifted into the weight path registers 412. The weight path register 412 may, for example, receive weight inputs from the top neighbor cells and communicate the weight inputs to the weight register 402 based on control signals. The weight register 402 may statically store the weight inputs such that when the activation inputs are transferred to a cell over multiple clock cycles, e.g., through the activation register 406, the weight inputs remain within the cell and are not transferred to an adjacent cell. Thus, the weight inputs may be applied to multiple activation inputs and the corresponding accumulated values may be transferred to adjacent cells, for example, using multiplication circuit 408.
Fig. 5 illustrates an example architecture 500 of a vector computation unit 502. The vector calculation unit 502 may receive a vector of accumulated values from a matrix calculation unit (e.g., the matrix calculation unit 312 described with reference to fig. 3).
Control signal 510 may be transmitted and may adjust how vector calculation unit 502 processes the vector of accumulated values. That is, control signal 510 may adjust the processing of whether the left-hand data value is pooled, whether the left-hand data value is stored in, for example, ve, or may otherwise adjust the activation value. The control signal 510 may also specify an activation or pooling function as well as other parameters for processing the activation or pooling values, such as a step size value.
The vector calculation unit 502 may send a value, such as a left-hand data value or a pooled value. In some implementations, the pooling circuit 508 receives the activation value or the pooling value and stores the activation value or the pooling value in a unified buffer.
FIG. 6 illustrates an example format 600 of a floating point value. Each of the values processed by the matrix calculation unit (e.g., the values stored by the registers of the monomers of the matrix calculation unit) may be stored as floating point values having a particular format.
Format 600 is characterized by a sign bit 601, a set of bits called significand 602, and another set of bits called exponent 603.
The sign bit 601 indicates whether the value stored using the format 600 is positive or negative. The significances 602 comprise one or more significances of values stored using format 600. Thus, the size of the significand 602 of the format 600, i.e., the number of bits, represents the maximum possible accuracy of any value stored using the format 600. The exponent 603 represents the power of the range radix used to convert the stored value to a normalized form. Thus, the size of the exponent 603 of the format 600 represents the maximum possible dynamic range of any value stored using the format 600.
In some cases, the standardized form used by the system to interpret floating point numbers contains one or more constant values. For example, the normalized version may always be the version 1.XXXXXXX2 XXXX, where the integer part of the first value is always constant, e.g. equal to 1. In some such cases, the significand 602 may include only non-constant bits of the normalized value and not constant bits. In these cases, the bits that are constant in the normalized form and thus do not appear in the significand 602 are referred to as "hidden bits". A computer system interpreting a binary floating point value with a hidden bit will add the hidden bit to the significand 602 in accordance with the standardized form underlying the format 600 of the value.
The manner in which a computer system stores binary numbers as floating point values depends on the standardized form in which the system interprets floating point numbers, as well as the size of the significand 602 and the exponent 603 of the particular floating point format 600 used. For example, floating point format 600 may include sign bit 601, 4-bit significand 602, and 3-bit exponent 603, and a computer system may interpret a binary number having this format 600 by assuming that the number has the standardized form X.XXX 2 XXX, where X is a single binary number, i.e., "0" or "1". Furthermore, the computer system may assume that the binary value preceding the decimal point in the first value in normalized form is always a hidden bit that is not present in the significand 602. Thus, such a computer system may interpret binary number +11.111 using a sign bit 601 having a sign bit of "0" (because the binary number is positive), an exponent 603 of "001", and a significand 602 of "1111".
If the computer system is unable to properly store the number in the format used, attempting to store the number may cause overflow and result in unpredictable or undesirable behavior. The above example of a computer system may store binary numbers whose number exceeds the maximum accuracy allowed in the significances 602 of the formats 600 employed by the system without overflow by rounding the numbers. Even though such rounding results in reduced accuracy, it does not result in overflow.
On the other hand, if the dynamic range of the binary number exceeds the maximum range allowed in the index 603 of the format 600, the computer system cannot round the dynamic range. Continuing with the example given above, the computer system is unable to store and interpret the numbers 111111111.01 2 Because the standardized form of the number has 1000 2 And this dynamic range cannot be represented using an example exponent width of 3 bits within the range of values allowed in exponent 603 of format 600.
As another example, the system may use a bfoat format with an 8-bit exponent and 7-bit significand. In order to reduce the likelihood of overflow, if an operation in the computer system transforms a first floating point value having a first format into a second floating point value having a second format in any way, it is important that the dynamic range of the second format is greater than or equal to the dynamic range of the first format. This includes the case where the system converts a first floating point value to a second floating point value and the case where the system applies an operation to the first floating point value to generate the second floating point value. For example, if a computer system multiplies two values having a first format to generate a result value having a second format, it is important that the dynamic range of the second format is greater than or equal to the dynamic range of the first format to reduce the likelihood of overflow. If the two values being multiplied have different formats, it is important that the dynamic range of the second format is greater than or equal to the dynamic range of the format with the greater dynamic range to reduce the likelihood of overflow.
Examples of floating point format 600 include an IEEE single precision format, a bFLoat format, and an extended bFLoat format.
The IEEE single precision format is a 32-bit format, which includes a 1-bit sign bit 601, an 8-bit exponent 603, and a 23-bit significand 602. The bfoat format is a 16-bit format having a 1-bit sign bit 601, an 8-bit exponent 603, and a 7-bit significand 602. The extended bfoat format is a 20-bit format including a 1-bit sign bit 601, an 8-bit exponent 603, and an 11-bit significand 602.
Importantly, all three formats mentioned above have the same exponent 603 size and therefore the same dynamic range. However, single precision formats allow more precision than extended bfoat formats and extended bfoat formats allow more precision than bfoat formats. To reduce the possibility of overflow but improve the precision, the matrix calculation unit may store the activation input and weight input values in a register holding the value in the bfoat format, store the product of the input values in a register holding the value in the extended bfoat format, and store the sum of the product value and the partial sum value in a register holding the value in the IEEE single precision format.
Fig. 7 illustrates an example architecture 700 of a multiplication circuit for matrix computing monomers. The matrix computation monomer depicted in fig. 7 multiplies two input values (e.g., an activation input value 701 and a weight input value 702) to generate a result value, e.g., a product value 705.
Architecture 700 includes a multiplier 703 and an adder 704, the multiplier 703 multiplies the significand and sign bits of two input values by the significand and sign bits that generate a result value, and the adder 704 adds the exponents of the two input values to generate an exponent of the result value. The combination of the significand and sign bits of the result value generates the result value.
Fig. 8 is a flow chart of an example process 800 for performing matrix multiplication using a matrix calculation unit. Process 800 may be performed by a matrix computing unit (e.g., matrix computing unit 113 of fig. 2) to perform matrix multiplication according to the architecture of the matrix computing unit.
The matrix calculation unit may perform process 800 multiple times in parallel to calculate a vector output that is the product of a vector and a matrix (e.g., an input vector comprising a plurality of activation input values and a weight matrix comprising a plurality of activation weight values).
The matrix calculation unit obtains an activation input value and a weight input value each having a first floating point format (802). The acquisition of activation inputs and weight input values is described in more detail below with reference to fig. 9.
The matrix calculation unit multiplies the activation input values with the weight input values using multiplication circuitry of the matrix calculation unit to generate product values having a second floating point format (804). The second floating point format has a higher precision than the first floating point format and has a dynamic range at least as large as the dynamic range of the first floating point format. By storing the result of multiplying the activation input value by the weight input value in a format having higher accuracy than the format of those input values, the matrix calculation unit reduces the likelihood of losing accuracy in storing the multiplication result. By storing the result of multiplying the activation input value by the weight input value in a format having a dynamic range at least as large as the dynamic range of the format of those input values, the matrix calculation unit also reduces the possibility of overflow in storing the multiplication result.
Multiplying the activation input value by the weight input value is described in more detail below with reference to fig. 8-9.
In some implementations, the first floating point format is a 16-bit format having 1-bit sign bits, 8-bit exponent and 7-bit significand, the 7-bit significand optionally excluding hidden bits in the standardized form of the corresponding binary number, and the second floating point format is a 20-bit format having 1-bit sign bits, 8-bit exponent and 11-bit significand optionally excluding hidden bits in the standardized form of the corresponding binary number.
The matrix calculation unit obtains the partial sum values in a third floating point format (806). The matrix calculation unit may acquire the partial sum value from the monomers in the matrix calculation unit.
The third floating point format has a higher precision than the first floating point format and has a dynamic range at least as large as the dynamic range of the first floating point format. Thus, the partial sum value has a format that allows for higher precision than the format of the input value and a dynamic range that is at least as large as the dynamic range of the format of the input value.
In some implementations, the third floating point format has a higher precision than the second floating point format. In other words, the three floating point formats may be ordered in their precision, starting with the format with the highest precision, in the following order: a third floating point format, a second floating point format, and a first floating point format. In some implementations, the third floating point format has a dynamic range that is at least as large as the dynamic range of the second floating point format.
In some implementations, the third floating point format is an IEEE standard 754 single precision format or other 32-bit format having 1-bit sign bits, 8-bit exponents, and 23-bit significands that do not include hidden bits in the standardized form of the corresponding binary numbers.
The matrix calculation unit combines the partial sum values with the product values using a summing circuit of the matrix calculation unit to generate updated partial sum values having a third format (808). By storing the result of the combination of the product value and the partial sum value in the same format as the partial sum value, the matrix calculation unit reduces the possibility of overflow or loss of accuracy. This is particularly true in implementations where the format of the product value (i.e., the second format) has a lower accuracy than the format of the updated partial sum value (i.e., the third format). In such an implementation, the matrix calculation unit reduces the likelihood of losing precision by storing the combined result in a format having a higher precision than the format of the product value. Similarly, in an implementation in which the third format has a dynamic range at least as large as that of the second format, the matrix calculation unit reduces the possibility of overflow by storing the combined result in a format having a larger dynamic range than that of the product value.
In some implementations, the matrix computing unit transmits the updated partial sum to another component of the matrix computing unit, such as a cell in the matrix computing unit or a multi-cell structure in the matrix computing unit.
Fig. 9 is a flow chart of an example process 900 for performing multiplying an activation input value with a weight input value. For convenience, process 900 will be described as being performed by a system of one or more computers located at one or more locations. For example, a neural network processing system (e.g., the neural network processing system 100 of fig. 1) suitably programmed in accordance with the present description may perform process 900.
The neural network system may perform process 900 multiple times in parallel to calculate a vector output including a higher precision portion of the product of the vector and a matrix (e.g., an input vector including a plurality of activation input values and a weight matrix including a plurality of activation weight values) and a vector output including a lower precision portion of the product of the vector and the matrix.
The system obtains an original activation value and an original weight value (902). The system may obtain the raw values from a neural network implementation engine of the system (e.g., neural network implementation engine 150 of fig. 1). The original value may be in any format, such as an IEEE single precision floating point format.
The system converts the raw activation values to a first format to generate activation input values (904) and converts the raw weight values to the first format to generate weight input values (906). The system may store the number represented by the original activation value as a new value having a new format.
The system determines if it has received a request to multiply the original activation value with an activation input value with enhanced accuracy (908). The system may receive this enhanced accuracy request from an end user of the system and/or by a neural network implementation engine of the system (e.g., neural network implementation engine 150 of fig. 1). The request indicates that the system must store the result of multiplying the original activation value with the activation input value with reduced loss of precision.
In some implementations, the system receives the enhanced precision request through a software instruction (i.e., a VLIW instruction bundle). These instruction bundles may include matrix multiplication instructions with a number of different possible opcodes used to express various options, such as components of enhanced precision matrix multiplication. If the system determines that it has not received an enhanced accuracy request, the system multiplies the activation input value and the original activation value into separate values using a matrix calculation unit implementing hardware circuitry of the system to generate a product value having a second format (910).
Otherwise, if the system determines that it has received an enhanced accuracy request, the system generates an activation enhanced accuracy value that is the difference between the original activation value and the activation input value (912) and generates a weight enhanced accuracy value that is the difference between the original weight value and the weight input value (914). The system generates a difference between the activation input value and the original input value by subtracting the activation input value from the original activation value and generates a difference between the weight input value and the original weight value by subtracting the weight input value from the original weight value. The system may use appropriate circuitry external to the matrix calculation unit to perform the subtraction of the two values, such as by adding the first value to the negative of the second value using summing circuitry external to the matrix calculation unit. The activation enhancement precision value and the weight enhancement precision value are both values in the first floating point format.
The system performs a set of multiplications between the activation input values, the weight input values, the activation enhancement precision values, and the weight enhancement precision values using a matrix computing unit (916). To perform multiplication between two values using a matrix calculation unit, the system provides the two values to the matrix calculation unit to cause the matrix calculation unit to perform multiplication of the two values.
In some implementations, the set of multiplications includes: the method comprises multiplying an activation input value by a weight input value, multiplying the activation input value by a weight enhancement precision value, multiplying the weight input value by an activation enhancement precision value, and multiplying the activation enhancement precision value by a weight enhancement precision value.
The software instructions determine a subset of possible multiplications to include. In some implementations, the set of multiplications includes only multiplying the activation input value with the weight input value and multiplying the activation enhancement precision value with the weight enhancement precision value. This technique may be used to reduce the number of multiplications required when the system determines that at least some of the individual input values and the enhanced precision values have the required level of precision.
The enhanced precision value indicates at least some of the lower precision portions of the original value that were lost in rounding when the original value was stored as an input value having the first format. By using these enhanced precision values in the multiplication, the system can perform the multiplication involving higher precision portions of the original value and thus produce multiplication results with higher precision.
The system then combines the products of the four multiplications to generate (918) a first value in a first format comprising a lower precision portion of the result of multiplying the original values and a second value in a first format comprising a higher precision portion of the result of multiplying the original data. In some implementations, the system performs the summation using external summation circuitry external to the matrix computation unit.
Embodiments of the subject matter and the functional operations described in this specification can be implemented in: in digital electronic circuitry, in tangibly embodied computer software or firmware, in computer hardware, including the structures disclosed in this specification and structural equivalents thereof, or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible, non-transitory program carrier for execution by, or to control the operation of, data processing apparatus. Alternatively or additionally, the program instructions may be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by data processing apparatus. The computer storage medium may be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them.
The term "data processing apparatus" includes all types of apparatus, devices, and machines for processing data, including for example a programmable processor, a computer, or multiple processors or computers. The apparatus may comprise a dedicated logic circuit, such as an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). In addition to hardware, an apparatus may include code that creates an execution environment for the computer program, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
A computer program (which may also be referred to or described as a program, software application, module, software module, script, or code) can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. The computer program may, but need not, correspond to a file in a file system. A program may be stored in a portion of a file that holds other programs or data, such as one or more scripts stored in a markup language document, in a single file dedicated to the program, or in multiple files, such as files that store portions of one or more modules, sub-programs, or code. A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
Computers suitable for executing computer programs include, for example, central processing units which may be based on general purpose or special purpose microprocessors or both or any other kind of central processing unit. Typically, a central processing unit will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a central processing unit for carrying out or executing instructions and one or more memory devices for storing instructions and data. Typically, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, a computer does not require such a device. Furthermore, a computer may be embedded in another device, such as a mobile phone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, such as a Universal Serial Bus (USB), flash drive, to name a few.
Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disk; and CD ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
To send interactions with users, embodiments of the subject matter described in this specification can be implemented on a computer having: display devices for displaying information to a user, such as CRT (cathode ray tube), LCD (liquid crystal display) monitors; and a keyboard and pointing device, such as a mouse or trackball, by which a user can provide input to the computer. Other types of devices may also be used to send interactions with the user; for example, feedback provided to the user may be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including auditory, speech, or tactile input. In addition, the computer may interact with the user by sending and receiving documents to and from devices used by the user; for example, by sending a web page to a web browser on a user's client device in response to a request received from the web browser.
Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front-end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include local area networks ("LANs") and wide area networks ("WANs"), such as the internet.
The computing system may include clients and servers. The client and server are typically remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
Embodiment 1 is a method of performing matrix multiplication using hardware circuitry, the method comprising: acquiring an input activation value and a weight input value by a matrix calculation unit of the hardware circuit, wherein the input activation value and the weight input value are in a first floating point format; multiplying, by multiplication circuitry of the matrix calculation unit, the input activation value with the weight input value to generate a product value, the product value having a second floating point format, the second floating point format having a higher precision than the first floating point format and having a dynamic range at least as large as a dynamic range of the first floating point format; obtaining, by the matrix computing unit, a portion and a value of a third floating point format, the third floating point format having a higher precision than the first floating point format and having a dynamic range at least as large as the dynamic range of the first floating point format; and combining, by a summing circuit of the hardware circuit, at least the partial sum value and the product value to generate an updated partial sum value having the third floating point format.
Embodiment 2 is the method of embodiment 1, wherein the precision of the floating point format is determined based on a count of available bits of significands in the floating point format and the dynamic range of the floating point format is determined based on a count of available bits of exponents in the floating point format.
Embodiment 3 is the method of any of embodiments 1-2, wherein the second floating point format has the same dynamic range as the first floating point format and the third floating point format has the same dynamic range as the first floating point format.
Embodiment 4 is the method of any of embodiments 1-3, wherein the third floating point format has a higher precision than the second floating point format.
Embodiment 5 is the method of any one of embodiments 1-4, wherein the hardware circuitry is configured to perform a calculation of a neural network having a plurality of layers, and the input activation value and the weight input value are associated with one of the plurality of layers.
Embodiment 6 is the method of any one of embodiments 1-5, further comprising: acquiring an original activation value and an original weight value of a first matrix calculation monomer with the third floating point format; converting the original activation value to the first floating point format to generate the input activation value; and converting the original weight value to the first floating point format to generate the weight input value.
Embodiment 7 is the method of any one of embodiments 1-6, further comprising: receiving a request to process the original activation value with enhanced accuracy; generating an activation enhancement precision value for the input value, the activation enhancement precision value being a measure of a difference between the activation input value and the original activation value; and generating a weight enhancement precision value of the weight input value, the weight enhancement precision value being a measure of a difference between the weight input value and the original weight value; and wherein multiplying the activation input value with the weight input value to generate the product value comprises: multiplying the input value by the weight input value, multiplying the input value by the weight enhancement precision value, multiplying the weight input value by the activation enhancement precision value, and multiplying the activation enhancement precision value by the weight enhancement precision value by the multiplication circuit of the matrix calculation unit, and combining the multiplied products to generate the product value.
Embodiment 8 is a hardware circuit comprising: a matrix computing unit configured to perform a first set of operations comprising: acquiring an activation input value and a weight input value, wherein the activation input value and the weight input value both have a first floating point format; storing the weight input value in a weight register configured to store a value having the first floating point format; multiplying the activation input value with the weight input value using a multiplication circuit of the hardware circuit to generate a product value, the product value having a second floating point format, the second floating point format having a higher precision than the first floating point format and having the dynamic range at least as large as a dynamic range of the first floating point format; obtaining a portion and a value of a third floating point format, the third floating point format having a higher precision than the first floating point format and having a dynamic range at least as large as the dynamic range of the first floating point format; storing the partial sum value in a sum in a register, the sum in the register configured to store a value having the third floating point format; and combining the partial sum value with the product value using a summing circuit of the matrix calculation unit to generate an updated partial sum value having the third floating point format.
Embodiment 9 is the hardware circuit of embodiment 8, the first set of operations further comprising: the activation input value is stored in an activation register configured to store a value having the first floating point format.
Embodiment 10 is the hardware circuit of any of embodiments 8-9, the first set of operations further comprising: the weight input value is stored in a weight register configured to store a value having the first floating point format.
Embodiment 11 is the hardware circuit of any of embodiments 8-10, the first set of operations further comprising: the partial sum value is stored in a sum in a register, the sum in the register being configured to store a value having the third floating point format.
Embodiment 12 is the hardware circuit of any one of embodiments 8-11, further comprising an external summing circuit external to the matrix computing unit, and wherein the first set of operations further comprises: receiving a request to process the original activation value with enhanced accuracy; generating an activation enhancement precision value for the input value, the activation enhancement precision value being a measure of a difference between the activation input value and the original activation value; and generating a weight enhancement precision value of the weight input value, the weight enhancement precision value being a measure of a difference between the weight input value and the original weight value; and wherein multiplying the activation input value with the weight input value to generate the product value comprises: multiplying the input value by the weight input value, the input value by the weight enhancement precision value, the weight input value by the activation enhancement precision value, and the activation enhancement precision value by the multiplication circuit of the matrix calculation unit, and wherein the external summation circuit is configured to perform a second set of operations comprising: the products of the multiplications are combined to generate the product value.
Embodiment 13 is a system comprising one or more computers and one or more storage devices storing instructions that, when executed by the one or more computers, are operable to cause the one or more computers to perform the operations of the respective methods of any one of embodiments 1-7.
Embodiment 14 is a computer storage medium encoded with instructions that, when executed by one or more computers, cause the one or more computers to perform the operations of the respective methods of any of embodiments 1-7.
Embodiment 15 is a system comprising one or more computers and one or more storage devices storing instructions that, when executed by the one or more computers, are operable to cause the one or more computers to perform the operations of the respective hardware circuits of any one of embodiments 8-12.
Embodiment 16 is a computer storage medium encoded with instructions that, when executed by one or more computers, cause the one or more computers to perform the operations of the respective hardware circuits of any of embodiments 8-12.
While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any invention or of what may be claimed, but rather as descriptions of features specific to particular embodiments of particular inventions. In this specification, certain features that are described in the context of separate embodiments may also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Furthermore, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, although operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous. Moreover, the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated in a single software product or packaged into multiple software products.
Specific embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some implementations, multitasking and parallel processing may be advantageous.
Claims (21)
1. A method of performing matrix multiplication using hardware circuitry, the method comprising:
obtaining, by a matrix computing unit of the hardware circuit, an activation input value and a weight input value, each having a first floating point format, wherein the hardware circuit is configured to perform a computation of a neural network having a plurality of layers, wherein the activation input value and the weight input value are associated with one of the plurality of layers;
wherein the first floating point format is a 16-bit format comprising: one available bit for a sign, eight available bits for an exponent, and seven available bits for a significand to represent a floating point in the first floating point format;
Multiplying, by multiplication circuitry of the matrix calculation unit, the activation input values with the weight input values to generate product values, the product values having a second floating point format;
obtaining, by the matrix calculation unit, a partial sum value in a third floating point format; and
at least the partial sum value is combined with the product value by a summing circuit of the hardware circuit to generate an updated partial sum value having the third floating point format.
2. The method of claim 1, wherein the precision of a floating point format is determined based on a count of available bits of significands in the floating point format and the dynamic range of a floating point format is determined based on a count of available bits of exponents in the floating point format.
3. The method according to claim 1, wherein:
the second floating point format has a higher precision than the first floating point format, and
the second floating point format has a dynamic range at least as large as the dynamic range of the first floating point format.
4. The method of claim 1, wherein the second floating point format has the same dynamic range as the first floating point format and the third floating point format has the same dynamic range as the first floating point format.
5. The method of claim 1, wherein the third floating point format has a higher precision than the second floating point format.
6. The method according to claim 1, wherein:
the third floating point format has a higher precision than the first floating point format; and is also provided with
The third floating point format has a dynamic range at least as large as the dynamic range of the first floating point format.
7. The method of claim 1, further comprising:
acquiring an original activation value and an original weight value of the first matrix calculation unit with the third floating point format;
converting the original activation value to the first floating point format to generate the activation input value; and is also provided with
The original weight value is converted to the first floating point format to generate the weight input value.
8. The method of claim 7, further comprising:
receiving a request to process the original activation value with enhanced accuracy;
generating an activation enhancement precision value for the input value, the activation enhancement precision value being a measure of a difference between the activation input value and the original activation value; and
generating a weight enhancement precision value of the weight input value, the weight enhancement precision value being a measure of a difference between the weight input value and the original weight value; and
Wherein multiplying the activation input value with the weight input value to generate the product value comprises:
by the multiplication circuit of the matrix calculation unit,
multiplying the input value with the weight input value,
multiplying the input value with the weight enhancement precision value,
multiplying the weight input value with the activation enhancement precision value, and
multiplying the activation enhancement precision value by the weight enhancement precision value, and
the products of the multiplications are combined to generate the product value.
9. A hardware circuit, comprising:
a matrix computing unit configured to perform a first set of operations comprising:
obtaining an activation input value and a weight input value, both of which have a first floating point format, wherein the hardware circuitry is configured to perform a calculation of a neural network having a plurality of layers, wherein the activation input value and the weight input value are associated with one of the plurality of layers;
wherein the first floating point format is a 16-bit format comprising: one available bit for a sign, eight available bits for an exponent, and seven available bits for a significand to represent a floating point in the first floating point format;
Storing the weight input value in a weight register configured to store a value having the first floating point format;
multiplying the activation input value with the weight input value using a multiplication circuit of the hardware circuit to generate a product value, the product value having a second floating point format;
obtaining a partial sum value of a third floating point format;
storing the partial sum value in a sum in a register, the sum in the register configured to store a value having the third floating point format; and
the partial sum value is combined with the product value using a summing circuit of the matrix calculation unit to generate an updated partial sum value having the third floating point format.
10. The hardware circuit of claim 9, wherein:
the second floating point format has a higher precision than the first floating point format; and is also provided with
The second floating point format has a dynamic range at least as large as the dynamic range of the first floating point format.
11. The hardware circuit of claim 9, wherein:
the third floating point format has a higher precision than the first floating point format; and is also provided with
The third floating point format has a dynamic range at least as large as the dynamic range of the first floating point format.
12. The hardware circuit of claim 9, the first set of operations further comprising:
the activation input value is stored in an activation register configured to store a value having the first floating point format.
13. The hardware circuit of claim 9, the first set of operations further comprising:
the weight input value is stored in a weight register configured to store a value having the first floating point format.
14. The hardware circuit of claim 9, the first set of operations further comprising:
the partial sum value is stored in a sum in a register, the sum in the register being configured to store a value having the third floating point format.
15. The hardware circuit of claim 9, further comprising an external summing circuit external to the matrix computing unit, and wherein the first set of operations further comprises:
receiving a request to process the original activation value with enhanced accuracy;
generating an activation enhancement precision value for the input value, the activation enhancement precision value being a measure of a difference between the activation input value and the original activation value; and
Generating a weight enhancement precision value of the weight input value, the weight enhancement precision value being a measure of a difference between the weight input value and an original weight value; and
wherein multiplying the activation input value with the weight input value to generate the product value comprises:
by the multiplication circuit of the matrix calculation unit,
multiplying the input value with the weight input value,
multiplying the input value with the weight enhancement precision value,
multiplying the weight input value with the activation enhancement precision value, and
multiplying the activation enhancement precision value by the weight enhancement precision value, and
wherein the external summing circuit is configured to perform a second set of operations comprising:
the products of the multiplications are combined to generate the product value.
16. A system comprising one or more computers and one or more storage devices storing instructions that when executed by the one or more computers are operable to cause the one or more computers to perform operations comprising:
obtaining an activation input value and a weight input value, each having a first floating point format, wherein the one or more computers are configured to perform a calculation of a neural network having a plurality of layers, wherein the activation input value and the weight input value are associated with one of the plurality of layers;
Wherein the first floating point format is a 16-bit format comprising: one available bit for a sign, eight available bits for an exponent, and seven available bits for a significand to represent a floating point in the first floating point format;
multiplying the activation input value with the weight input value to generate a product value, the product value having a second floating point format;
obtaining a partial sum value of a third floating point format; and
at least the partial sum value is combined with the product value to generate an updated partial sum value having the third floating point format.
17. The system of claim 16, wherein:
the second floating point format has a higher precision than the first floating point format;
the second floating point format has a dynamic range at least as large as the dynamic range of the first floating point format;
the third floating point format has a higher precision than the first floating point format; and is also provided with
The third floating point format has a dynamic range at least as large as the dynamic range of the first floating point format.
18. A computer storage medium encoded with instructions that, when executed by one or more computers, cause the one or more computers to perform operations of respective methods of:
Obtaining an activation input value and a weight input value, each having a first floating point format, wherein the one or more computers are configured to perform a calculation of a neural network having a plurality of layers, wherein the activation input value and the weight input value are associated with one of the plurality of layers;
wherein the first floating point format is a 16-bit format comprising: one available bit for a sign, eight available bits for an exponent, and seven available bits for a significand to represent a floating point in the first floating point format;
multiplying the activation input value with the weight input value to generate a product value, the product value having a second floating point format;
obtaining a partial sum value of a third floating point format; and
at least the partial sum value is combined with the product value to generate an updated partial sum value having the third floating point format.
19. The computer storage medium of claim 18, wherein:
the second floating point format has a higher precision than the first floating point format;
the second floating point format has a dynamic range at least as large as the dynamic range of the first floating point format;
The third floating point format has a higher precision than the first floating point format; and is also provided with
The third floating point format has a dynamic range at least as large as the dynamic range of the first floating point format.
20. A system comprising one or more computers and one or more storage devices storing instructions that when executed by the one or more computers are operable to cause the one or more computers to perform operations of respective hardware circuitry to:
obtaining an activation input value and a weight input value, both of which have a first floating point format, wherein the respective hardware circuitry is configured to perform a calculation of a neural network having a plurality of layers, wherein the activation input value and the weight input value are associated with one of the plurality of layers;
wherein the first floating point format is a 16-bit format comprising: one available bit for a sign, eight available bits for an exponent, and seven available bits for a significand to represent a floating point in the first floating point format;
storing the weight input value in a weight register configured to store a value having the first floating point format;
Multiplying the activation input value with the weight input value to generate a product value, the product value having a second floating point format;
obtaining a partial sum value of a third floating point format;
storing the partial sum value in a sum in a register, the sum in the register configured to store a value having the third floating point format; and
the partial sum value is combined with the product value to generate an updated partial sum value having the third floating point format.
21. A computer storage medium encoded with instructions that, when executed by one or more computers, cause the one or more computers to perform the operations of the following respective hardware circuits:
obtaining an activation input value and a weight input value, both of which have a first floating point format, wherein the hardware circuitry is configured to perform a calculation of a neural network having a plurality of layers, wherein the activation input value and the weight input value are associated with one of the plurality of layers;
wherein the first floating point format is a 16-bit format comprising: one available bit for a sign, eight available bits for an exponent, and seven available bits for a significand to represent a floating point in the first floating point format;
Storing the weight input value in a weight register configured to store a value having the first floating point format;
multiplying the activation input value with the weight input value to generate a product value, the product value having a second floating point format;
obtaining a partial sum value of a third floating point format;
storing the partial sum value in a sum in a register, the sum in the register configured to store a value having the third floating point format; and
the partial sum value is combined with the product value to generate an updated partial sum value having the third floating point format.
Applications Claiming Priority (4)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201762507748P | 2017-05-17 | 2017-05-17 | |
US62/507,748 | 2017-05-17 | ||
CN201880019014.4A CN110447010B (en) | 2017-05-17 | 2018-05-17 | Performing matrix multiplication in hardware |
PCT/US2018/033271 WO2018213636A1 (en) | 2017-05-17 | 2018-05-17 | Performing matrix multiplication in hardware |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201880019014.4A Division CN110447010B (en) | 2017-05-17 | 2018-05-17 | Performing matrix multiplication in hardware |
Publications (1)
Publication Number | Publication Date |
---|---|
CN117273096A true CN117273096A (en) | 2023-12-22 |
Family
ID=62784216
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202311134221.3A Pending CN117273096A (en) | 2017-05-17 | 2018-05-17 | Performing matrix multiplication in hardware |
CN201880019014.4A Active CN110447010B (en) | 2017-05-17 | 2018-05-17 | Performing matrix multiplication in hardware |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201880019014.4A Active CN110447010B (en) | 2017-05-17 | 2018-05-17 | Performing matrix multiplication in hardware |
Country Status (7)
Country | Link |
---|---|
US (3) | US10621269B2 (en) |
EP (1) | EP3625670B1 (en) |
JP (2) | JP7114622B2 (en) |
KR (2) | KR102477516B1 (en) |
CN (2) | CN117273096A (en) |
TW (3) | TWI751500B (en) |
WO (1) | WO2018213636A1 (en) |
Families Citing this family (34)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10521488B1 (en) * | 2016-12-30 | 2019-12-31 | X Development Llc | Dynamic partitioning |
US10879904B1 (en) | 2017-07-21 | 2020-12-29 | X Development Llc | Application specific integrated circuit accelerators |
US10790828B1 (en) | 2017-07-21 | 2020-09-29 | X Development Llc | Application specific integrated circuit accelerators |
US11294626B2 (en) * | 2018-09-27 | 2022-04-05 | Intel Corporation | Floating-point dynamic range expansion |
US11494645B2 (en) | 2018-12-06 | 2022-11-08 | Egis Technology Inc. | Convolutional neural network processor and data processing method thereof |
TWI766193B (en) * | 2018-12-06 | 2022-06-01 | 神盾股份有限公司 | Convolutional neural network processor and data processing method thereof |
CN111338974A (en) | 2018-12-19 | 2020-06-26 | 超威半导体公司 | Tiling algorithm for matrix math instruction set |
US10831496B2 (en) * | 2019-02-28 | 2020-11-10 | International Business Machines Corporation | Method to execute successive dependent instructions from an instruction stream in a processor |
US20220171829A1 (en) * | 2019-03-11 | 2022-06-02 | Untether Ai Corporation | Computational memory |
JP7408671B2 (en) | 2019-03-15 | 2024-01-05 | インテル コーポレイション | Architecture for block sparse operations on systolic arrays |
US20220179787A1 (en) | 2019-03-15 | 2022-06-09 | Intel Corporation | Systems and methods for improving cache efficiency and utilization |
US11934342B2 (en) | 2019-03-15 | 2024-03-19 | Intel Corporation | Assistance for hardware prefetch in cache access |
US11556615B2 (en) * | 2019-05-03 | 2023-01-17 | Tesla, Inc. | Data path for scalable matrix node engine with mixed data formats |
CN112149049A (en) * | 2019-06-26 | 2020-12-29 | 北京百度网讯科技有限公司 | Apparatus and method for transforming matrix, data processing system |
US11379555B2 (en) * | 2019-06-28 | 2022-07-05 | Amazon Technologies, Inc. | Dilated convolution using systolic array |
US11842169B1 (en) | 2019-09-25 | 2023-12-12 | Amazon Technologies, Inc. | Systolic multiply delayed accumulate processor architecture |
US11531729B2 (en) * | 2019-10-04 | 2022-12-20 | Stmicroelectronics S.R.L. | Bitwise digital circuit and method for performing approximate operations |
CN110929861B (en) * | 2019-11-15 | 2023-04-18 | 中国人民解放军国防科技大学 | Hardware accelerator for interlayer flowing water of deep neural network of multilayer perceptron |
US11861761B2 (en) | 2019-11-15 | 2024-01-02 | Intel Corporation | Graphics processing unit processing and caching improvements |
US11467806B2 (en) | 2019-11-27 | 2022-10-11 | Amazon Technologies, Inc. | Systolic array including fused multiply accumulate with efficient prenormalization and extended dynamic range |
US11816446B2 (en) * | 2019-11-27 | 2023-11-14 | Amazon Technologies, Inc. | Systolic array component combining multiple integer and floating-point data types |
US11625453B1 (en) * | 2019-12-12 | 2023-04-11 | Amazon Technologies, Inc. | Using shared data bus to support systolic array tiling |
US11922292B2 (en) * | 2020-01-27 | 2024-03-05 | Google Llc | Shared scratchpad memory with parallel load-store |
KR102404388B1 (en) * | 2020-03-09 | 2022-06-02 | (주)그린파워 | Matrix multiplier structure and multiplying method capable of transpose matrix multiplication |
JP7391774B2 (en) | 2020-06-11 | 2023-12-05 | 株式会社東芝 | Arithmetic processing device, information processing device, and arithmetic processing method |
US11308027B1 (en) | 2020-06-29 | 2022-04-19 | Amazon Technologies, Inc. | Multiple accumulate busses in a systolic array |
TWI746126B (en) | 2020-08-25 | 2021-11-11 | 創鑫智慧股份有限公司 | Matrix multiplication device and operation method thereof |
US11681776B2 (en) * | 2020-10-08 | 2023-06-20 | Applied Materials, Inc. | Adaptive settling time control for binary-weighted charge redistribution circuits |
CN112434256B (en) * | 2020-12-03 | 2022-09-13 | 海光信息技术股份有限公司 | Matrix multiplier and processor |
US11880682B2 (en) | 2021-06-30 | 2024-01-23 | Amazon Technologies, Inc. | Systolic array with efficient input reduction and extended array performance |
US20230010897A1 (en) | 2021-07-06 | 2023-01-12 | Google Llc | In situ sparse matrix expansion |
US20230083270A1 (en) * | 2021-09-14 | 2023-03-16 | International Business Machines Corporation | Mixed signal circuitry for bitwise multiplication with different accuracies |
US20230289139A1 (en) * | 2022-03-08 | 2023-09-14 | International Business Machines Corporation | Hardware device to execute instruction to convert input value from one data format to another data format |
KR20240033565A (en) * | 2022-09-05 | 2024-03-12 | 리벨리온 주식회사 | Neural processing device, processing element included therein and Method for operating various format of neural processing device |
Family Cites Families (18)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JPS6280764A (en) * | 1985-10-04 | 1987-04-14 | Nec Corp | Floating point arithmetic unit for sum of product |
US5548545A (en) | 1995-01-19 | 1996-08-20 | Exponential Technology, Inc. | Floating point exception prediction for compound operations and variable precision using an intermediate exponent bus |
US7301541B2 (en) * | 1995-08-16 | 2007-11-27 | Microunity Systems Engineering, Inc. | Programmable processor and method with wide operations |
US6088715A (en) * | 1997-10-23 | 2000-07-11 | Advanced Micro Devices, Inc. | Close path selection unit for performing effective subtraction within a floating point arithmetic unit |
CN1283428C (en) * | 2000-03-31 | 2006-11-08 | 索尼公司 | Robot device, robot device action control method, external force detecting device and method |
US7219085B2 (en) * | 2003-12-09 | 2007-05-15 | Microsoft Corporation | System and method for accelerating and optimizing the processing of machine learning techniques using a graphics processing unit |
US8280941B2 (en) * | 2007-12-19 | 2012-10-02 | HGST Netherlands B.V. | Method and system for performing calculations using fixed point microprocessor hardware |
CN101782893B (en) | 2009-01-21 | 2014-12-24 | 上海芯豪微电子有限公司 | Reconfigurable data processing platform |
US9104510B1 (en) * | 2009-07-21 | 2015-08-11 | Audience, Inc. | Multi-function floating point unit |
WO2013095504A1 (en) * | 2011-12-22 | 2013-06-27 | Intel Corporation | Matrix multiply accumulate instruction |
US8984042B2 (en) * | 2012-02-09 | 2015-03-17 | International Business Machines Corporation | Mixed precision estimate instruction computing narrow precision result for wide precision inputs |
US9104474B2 (en) * | 2012-12-28 | 2015-08-11 | Intel Corporation | Variable precision floating point multiply-add circuit |
EP2949047B1 (en) * | 2013-01-22 | 2021-03-31 | Altera Corporation | Data compression and decompression using simd instructions |
US9384168B2 (en) * | 2013-06-11 | 2016-07-05 | Analog Devices Global | Vector matrix product accelerator for microprocessor integration |
US9978014B2 (en) * | 2013-12-18 | 2018-05-22 | Intel Corporation | Reconfigurable processing unit |
US10192162B2 (en) * | 2015-05-21 | 2019-01-29 | Google Llc | Vector computation unit in a neural network processor |
US11244225B2 (en) * | 2015-07-10 | 2022-02-08 | Samsung Electronics Co., Ltd. | Neural network processor configurable using macro instructions |
US10726514B2 (en) | 2017-04-28 | 2020-07-28 | Intel Corporation | Compute optimizations for low precision machine learning operations |
-
2018
- 2018-05-17 US US15/983,047 patent/US10621269B2/en active Active
- 2018-05-17 EP EP18735434.5A patent/EP3625670B1/en active Active
- 2018-05-17 CN CN202311134221.3A patent/CN117273096A/en active Pending
- 2018-05-17 TW TW109105926A patent/TWI751500B/en active
- 2018-05-17 JP JP2019551610A patent/JP7114622B2/en active Active
- 2018-05-17 TW TW110146923A patent/TWI807539B/en active
- 2018-05-17 WO PCT/US2018/033271 patent/WO2018213636A1/en unknown
- 2018-05-17 KR KR1020217015439A patent/KR102477516B1/en active IP Right Grant
- 2018-05-17 TW TW107116873A patent/TWI689873B/en active
- 2018-05-17 KR KR1020197027658A patent/KR102258120B1/en active IP Right Grant
- 2018-05-17 CN CN201880019014.4A patent/CN110447010B/en active Active
-
2020
- 2020-03-20 US US16/826,075 patent/US10831862B2/en active Active
- 2020-11-09 US US17/093,439 patent/US20210124795A1/en active Pending
-
2022
- 2022-04-06 JP JP2022063463A patent/JP7312879B2/en active Active
Also Published As
Publication number | Publication date |
---|---|
KR102477516B1 (en) | 2022-12-14 |
TWI751500B (en) | 2022-01-01 |
KR102258120B1 (en) | 2021-05-28 |
TW202046181A (en) | 2020-12-16 |
TW201905768A (en) | 2019-02-01 |
TWI689873B (en) | 2020-04-01 |
JP2020521192A (en) | 2020-07-16 |
KR20210062739A (en) | 2021-05-31 |
EP3625670B1 (en) | 2022-02-23 |
US10621269B2 (en) | 2020-04-14 |
KR20190117714A (en) | 2019-10-16 |
CN110447010B (en) | 2023-09-26 |
US20200257754A1 (en) | 2020-08-13 |
CN110447010A (en) | 2019-11-12 |
JP2022106737A (en) | 2022-07-20 |
TWI807539B (en) | 2023-07-01 |
US20210124795A1 (en) | 2021-04-29 |
US20180336165A1 (en) | 2018-11-22 |
EP3625670A1 (en) | 2020-03-25 |
TW202213189A (en) | 2022-04-01 |
JP7312879B2 (en) | 2023-07-21 |
JP7114622B2 (en) | 2022-08-08 |
US10831862B2 (en) | 2020-11-10 |
WO2018213636A1 (en) | 2018-11-22 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN110447010B (en) | Performing matrix multiplication in hardware | |
CN109997132B (en) | Low-delay matrix multiplication component | |
CN110622134B (en) | Special neural network training chip | |
JP2018055677A (en) | Processor and method for outer product accumulate operations | |
US20240094986A1 (en) | Method and apparatus for matrix computation using data conversion in a compute accelerator | |
CN117280341A (en) | In-situ sparse matrix expansion | |
TW202414199A (en) | Method, system, and non-transitory computer-readable storage media for training neural networks |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |