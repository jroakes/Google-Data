CN117121021A - Machine-learned model for user interface prediction and generation - Google Patents
Machine-learned model for user interface prediction and generation Download PDFInfo
- Publication number
- CN117121021A CN117121021A CN202180096561.4A CN202180096561A CN117121021A CN 117121021 A CN117121021 A CN 117121021A CN 202180096561 A CN202180096561 A CN 202180096561A CN 117121021 A CN117121021 A CN 117121021A
- Authority
- CN
- China
- Prior art keywords
- interface
- embeddings
- training
- user interface
- learned
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000012549 training Methods 0.000 claims abstract description 127
- 238000000034 method Methods 0.000 claims description 52
- 238000012545 processing Methods 0.000 claims description 25
- 230000000873 masking effect Effects 0.000 claims description 11
- 230000000295 complement effect Effects 0.000 claims 3
- 238000000547 structure data Methods 0.000 description 17
- 238000010586 diagram Methods 0.000 description 14
- 238000013528 artificial neural network Methods 0.000 description 12
- 230000006870 function Effects 0.000 description 11
- 230000015654 memory Effects 0.000 description 11
- 238000012015 optical character recognition Methods 0.000 description 11
- 230000008569 process Effects 0.000 description 10
- 239000008186 active pharmaceutical agent Substances 0.000 description 7
- 230000008901 benefit Effects 0.000 description 5
- 238000004891 communication Methods 0.000 description 4
- 238000010801 machine learning Methods 0.000 description 3
- 230000007246 mechanism Effects 0.000 description 3
- 238000012986 modification Methods 0.000 description 3
- 230000004048 modification Effects 0.000 description 3
- 238000007781 pre-processing Methods 0.000 description 3
- 230000000306 recurrent effect Effects 0.000 description 3
- 238000012935 Averaging Methods 0.000 description 2
- 238000013459 approach Methods 0.000 description 2
- 230000008859 change Effects 0.000 description 2
- 238000013527 convolutional neural network Methods 0.000 description 2
- 230000000694 effects Effects 0.000 description 2
- 238000011176 pooling Methods 0.000 description 2
- 239000013598 vector Substances 0.000 description 2
- 241000282836 Camelus dromedarius Species 0.000 description 1
- 206010033799 Paralysis Diseases 0.000 description 1
- 230000004913 activation Effects 0.000 description 1
- 238000007792 addition Methods 0.000 description 1
- 230000006735 deficit Effects 0.000 description 1
- 230000001419 dependent effect Effects 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 230000003993 interaction Effects 0.000 description 1
- 230000007787 long-term memory Effects 0.000 description 1
- 239000003550 marker Substances 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 230000006403 short-term memory Effects 0.000 description 1
- 230000003068 static effect Effects 0.000 description 1
- 230000001360 synchronised effect Effects 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
Abstract
In general, the present disclosure relates to a user interface understanding. More particularly, the present disclosure relates to training and utilizing machine-learned models for user interface predictions and/or generation. Various pre-training tasks can be used to pre-train the machine-learned interface prediction model for final downstream task training and utilization (e.g., interface prediction, interface generation, etc.).
Description
Technical Field
The present disclosure relates generally to user interface understanding. More particularly, the present disclosure relates to training and utilizing machine-learned models for user interface predictions and/or generation.
Background
In order to increase the accessibility of the smart device and simplify its use, it is important to build an intuitive, efficient user interface that can assist the user in completing its tasks. However, interface-specific properties have conventionally made machine learning techniques difficult to apply. For example, conventional machine learning techniques have difficulty effectively utilizing multimodal interface features that involve image, text, and/or structural metadata. For another example, machine-learned models have traditionally been difficult to achieve powerful performance when high quality marker data is not available, as is common in user interfaces. Thus, machine-learned models that enable efficient and accurate user interface predictions and/or generation are highly desirable.
Disclosure of Invention
Aspects and advantages of embodiments of the disclosure will be set forth in part in the description which follows, or may be learned by practice of the embodiments.
One example aspect of the present disclosure relates to a computer-implemented method for training and user interface prediction using a machine-learned model. The method comprises the following steps: interface data describing a single user interface comprising a plurality of interface elements is obtained by a computing system comprising one or more computing devices, wherein the interface data comprises one or more interface images depicting the single user interface. The method comprises the following steps: a plurality of intermediate embeddings is determined by the computing system based at least in part on one or more of the one or more interface images or text content depicted in the one or more interface images. The method comprises the following steps: the method includes processing, by a computing system, a plurality of intermediate embeddings using a machine-learned interface prediction model to obtain one or more user interface embeddings. The method comprises the following steps: the pre-training tasks are performed by the computing system based at least in part on the one or more user interface embeddings to obtain a pre-training output.
Another example aspect of the disclosure relates to a computing system including one or more processors and one or more tangible, non-transitory computer-readable media storing computer-readable instructions configured to generate a machine-learned interface prediction model of a learned representation of a user interface. The machine-learned interface prediction model has been trained by performing operations. The operation includes: interface data is obtained that depicts a single user interface comprising a plurality of interface elements, wherein the interface data comprises an interface image that depicts the single user interface. The operation includes: a plurality of intermediate embeddings is determined based at least in part on one or more of the one or more interface images or text content depicted in the one or more interface images. The operation includes: the plurality of intermediate embeddings are processed using the machine-learned interface prediction model to obtain one or more user interface embeddings. The operation includes: the pre-training task is performed based at least in part on the one or more user interface embeddings to obtain a pre-training output.
Another example aspect of the disclosure relates to one or more tangible, non-transitory computer-readable media storing computer-readable instructions that, when executed by one or more processors, cause the one or more processors to perform operations. The operation includes: interface data is obtained that depicts a single user interface comprising a plurality of interface elements, wherein the interface data comprises structural data and an interface image that depicts the single user interface, wherein the structural data indicates one or more locations of one or more respective ones of the plurality of interface elements. The operation includes: a plurality of intermediate embeddings is determined based at least in part on one or more of the structural data, the one or more interface images, or the textual content depicted in the one or more interface images. The operation includes: the plurality of intermediate embeddings are processed using the machine-learned interface prediction model to obtain one or more user interface embeddings. The operation includes: the pre-training task is performed based at least in part on the one or more user interface embeddings to obtain a pre-training output.
Other aspects of the disclosure relate to various systems, apparatuses, non-transitory computer-readable media, user interfaces, and electronic devices.
These and other features, aspects, and advantages of the various embodiments of the present disclosure will become better understood with reference to the following description and appended claims. The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate exemplary embodiments of the disclosure and together with the description, serve to explain the principles of interest.
Drawings
A detailed discussion of the embodiments directed to one of ordinary skill in the art is set forth in the specification in view of the accompanying drawings, wherein:
FIG. 1A depicts a block diagram of an example computing system performing training and utilization of a machine-learned interface prediction model, according to an example embodiment of the disclosure.
FIG. 1B depicts a block diagram of an example computing device performing pre-training of a machine-learned interface prediction model, according to an example embodiment of the disclosure.
FIG. 1C depicts a block diagram of an example computing device that performs interface prediction using a machine-learned interface prediction model, according to an example embodiment of the disclosure.
FIG. 2 depicts a block diagram of an example machine-learned interface prediction model, according to an example embodiment of the present disclosure.
FIG. 3 depicts a block diagram of an example machine-learned interface prediction model, according to an example embodiment of the present disclosure.
Fig. 4 depicts an example diagram of a user interface according to an example embodiment of the present disclosure.
FIG. 5 depicts a dataflow diagram for performing a pre-training task using a machine-learned interface prediction model.
FIG. 6 depicts a flowchart of an example method of performing pre-training of a machine-learned interface prediction model, according to an example embodiment of the disclosure.
Repeated reference characters on the drawings are intended to represent like features in the various embodiments.
Detailed Description
SUMMARY
In general, the present disclosure relates to user interface understanding. More particularly, the present disclosure relates to training and utilizing machine-learned models for user interface predictions and/or generation. As an example, interface data describing a user interface (e.g., a user interface presented by an application and/or operating system, etc.) can be obtained. The user interface can include a plurality of user interface elements (e.g., icons, interactable buttons, images, text content, etc.). The interface data can include structural data (e.g., metadata indicating the location of the interface element, etc.) and an interface image depicting the user interface. The plurality of intermediate embeddings can be determined based on the structural data, the one or more interface images, and/or text content depicted in the one or more interface images (e.g., using text recognition models (OCR), etc.). These intermediate embeddings can be processed using a machine-learned interface prediction model to obtain one or more user interface embeddings. Based on the one or more user interface embeddings, a pre-training task can be performed to obtain a pre-training output. In this way, the machine-learned interface prediction model can be pre-trained for final downstream task training and utilization (e.g., interface prediction, interface generation, etc.) using various pre-training tasks.
More specifically, interface data describing a user interface can be obtained. The user interface can be a user interface associated with an application and/or an operating system of the computing device. As an example, the user interface may be a main menu interface for a food delivery application. As another example, the user interface may be a lock screen interface for a smart phone device. As yet another example, the user interface may be a home screen interface for a virtual assistant device or video game console. Thus, it should be widely understood that the user interface may be any type of interface for any kind of device and/or application.
The user interface can include a plurality of interface elements. The interface elements can include icons, interactable elements (e.g., buttons, etc.), indicators, and the like. As an example, the interface element can be or otherwise include an interactable element that upon selection by a user (e.g., using a touch gesture on a touch screen device, etc.) navigates to a second user interface. As another example, the interface element can be or otherwise include an input field (input field) configured to accept user input (e.g., via a virtual screen keyboard, etc.). As yet another example, the interface element can be or otherwise include an icon (e.g., a connectivity indication icon, a battery life icon, etc.) describing the functionality of the smartphone device displaying the user interface. Thus, it should be widely understood that the plurality of interface elements can comprise any discrete functional unit or portion of the user interface.
The interface data can include structural data. The structure data can indicate one or more locations of one or more interface elements of the plurality of interface elements. As an example, when rendered, the structure data can indicate the size and location of the icon interface element within the user interface. As another example, the structural data can indicate or otherwise specify various characteristics of the input field interface element (e.g., font, text size, field position, feedback characteristics (e.g., initiate force feedback actions upon receiving input from a user, play sound upon receiving input from a user, etc.), functionality between other applications (e.g., allow use of a virtual keyboard application, etc.), etc.
In some implementations, the structure data can be or otherwise include view level data. As used herein, the term "view level data" can refer to data describing a view level and/or data describing a document object model. The view hierarchy data can include a tree representation of the UI elements. Each node of the tree can describe certain attributes (e.g., bounding box location, functionality, etc.) of the interface element. As an example, the view hierarchy tree of structural data can include text content data associated with visible text of text interface elements included in the user interface. As another example, the view hierarchy tree of structural data can include content descriptors and/or can describe resource ids of functionality (e.g., interface navigation paths, shared functionality, etc.) that are not typically provided to the user. As another example, the view hierarchy tree of structural data can include class name data describing functional classes of the application programming interface and/or software tool associated with an implementation of the corresponding interface element. As another example, the boundary data can represent a bounding box location of the interface element within the user interface. It should be noted that in some embodiments, various types of data (e.g., text content data, etc.) can be empty within the view-level data.
More specifically, in some implementations, the structure data can be or otherwise include view level leaf nodes of view level tree data. For each leaf node, the contents of the text field of the node can be encoded as feature vectors (e.g., text, content descriptors, resource IDs, class names, etc.). In some embodiments, as a preprocessing step, the content of class name data can be normalized by heuristics to one of a discrete number of classes. Additionally or alternatively, in some embodiments, as a preprocessing step, the content of the resource ID data can be split by underlining (underscore) and humping naming (camel case). Normalized class name data can be encoded as a one-hot embedding, while the contents of other fields can be processed to obtain their sentence-level embedding.
Additionally, the interface data can include an interface image depicting the user interface. For example, the one or more interface images can be images captured when the user interface is displayed on the display device (e.g., captured using a camera device, a screen capture application, etc.). Additionally, one or more interface images can depict text content. As an example, the user interface can be a home screen interface of a smart phone device having text content including text. Text can be identified (e.g., using an optical character recognition model, etc.) to obtain text content.
In some implementations, the interface data can describe only a single user interface (e.g., as opposed to multiple user interfaces such as a sequence of user interfaces). By providing data from only a single user interface, the model described herein can be forced to learn a representation of the user interface of a static nature (e.g., without the benefit or context of a change (e.g., visual change) between user interfaces). This can result in a more powerful model that can understand the functionality of a user interface simply by looking at data from a single instance or image, thus eliminating the need to iterate through different interfaces to expose multiple instances or images of functionality.
Based at least in part on the structural data, the one or more interface images, and/or text content depicted in the one or more interface images, a plurality of intermediate embeddings can be determined. In some implementations, the intermediate embeddings can be or otherwise include one or more image embeddings, one or more text embeddings, one or more location embeddings, and/or one or more content embeddings. As an example, features extracted from interface data can be linearly projected to obtain a plurality of intermediate embeddings -for each ith input of type (i) ∈ { IMG, OCR, VH } and for other types of inputs 0 is used.
More specifically, in some embodiments, one or more location embeddings can be determined from the structural data. The one or more location embeddings can correspond to one or more locations of one or more respective interface elements. As an example, the location features of each interface element can be encoded using its bounding box (e.g., as described by structural data, etc.), which can include normalized top-left point coordinates, bottom-right point coordinates, width, height, and/or area of the bounding box. For example, a linear layer may be used to project location features into a location embedmentFor the ith component (for CLS and SEP, P i ＝0)。
In some implementations, one or more image embeddings can be determined by one or more interface images. One or more image embeddings can be associated with at least one interface element of the plurality of interface elements, respectively. As an example, one or more portions of one or more interface images can be determined by the one or more interface images (e.g., based on a bounding box described by the structural data, etc.). The machine-learned model (e.g., machine-learned interface prediction model, etc.) can process portions of one or more interface images to obtain corresponding one or more image embeddings (e.g., using a final spatial averaging pooling layer, etc.).
In some implementations, the plurality of intermediate embeddings can include one or more types of embeddings. The one or more types of embeddings can indicate an embedding type of each of the other embeddings of the plurality of intermediate embeddings, respectively. As an example, to distinguish between portions of interface data, six types of tokens can be utilized: IMG, OCR, VH, CLS, SEP and MASK. In some implementations, MASK primitives can be of a type used to improve the pretraining accuracy of machine-learned interface prediction models. For example, type embedding can be obtained using one-hot encoding followed by linear projection-for the i-th component in the sequence, where d is the dimension size.
In some implementations, the plurality of intermediate embeddings can be determined by processing the structural data, the one or more interface images, and/or text content depicted in the one or more interface images using an embedded portion of the machine-learned interface prediction model to obtain the plurality of intermediate embeddings. For example, interface data (e.g., structural data, one or more interface images, etc.) can be input to the embedding portion to obtain an intermediate embedding, which can then be processed using a separate portion of the machine-learned interface prediction model (e.g., a transformer portion, etc.) to obtain one or more user interface embeddings.
The plurality of intermediate embeddings can be processed using a machine-learned interface prediction model to obtain one or more user interface embeddings. More specifically, each of the intermediate embedding types can be summed and can be processed by a machine-learned interface prediction model. In some implementations, the transformer portion of the machine-learned interface prediction model can process the intermediate embeddings to obtain one or more user interface embeddings. For example, a machine-learned interface prediction model (e.g., a transformer portion of the machine-learned interface prediction model, etc.) can process the summed intermediate embeddings to obtain one or more user interface embeddingsAs indicated below:
U＝TransformerEncoder(T+P+C), (1)
wherein,and n is the sequence length.
Based at least in part on the one or more user interface embeddings, a pre-training task can be performed. In some implementations, a loss function can be evaluated that evaluates the difference between the true value data and the pre-training output. As an example, the true value data can describe an optimal prediction of masking input based on a machine-learned interface prediction model. In some implementations, one or more parameters of the machine-learned interface prediction model (e.g., parameters of the embedded portion of the model and/or the transformer function) can be adjusted based at least in part on the loss function. In this way, the pre-training task can be used to train machine-learned interface predictions to provide a better or more useful representation (e.g., user interface embedding) for a given input interface data.
In some implementations, the pre-training tasks can be or otherwise include interface prediction tasks. For example, one or more of the plurality of interface elements can be replaced with one or more corresponding second interface elements of a second user interface different from the user interface prior to determining the plurality of intermediate embeddings. More specifically, given an original interface a, a "pseudo" version of interface a' can be generated by replacing 20% of interface elements of interface a with components from interface B, which can be a randomly selected interface from a plurality of user interfaces included in a batch of training data. In some implementations, for each interface, the input to be replaced (e.g., one or more interface images, structural data, text content from one or more interface images, etc.) can be randomly selected. As an example, two portions of one or more interface images from interface a can be replaced with two portions of an interface image from interface B to obtain a "pseudo" interface a'. It should be noted that in this example, the structural data and text content are not replaced before being input to the transformer portion of the machine-learned interface prediction model to minimize the difference between the original interface a and the "pseudo" interface a', thereby increasing the difficulty of the task.
To follow the previous example, the machine-learned interface prediction model may be used to perform the interface prediction tasks to obtain the pre-training output. The pre-training output can be configured to indicate whether user interfaces a and a' are real interfaces. For example, the pre-training output can predict whether each interface is real by minimizing Cross Entropy (CE) targets:
where y is the binary label of UI x (y=1 if x is true), andis the predictive probability. U (U) CLS The output corresponding to the CLS lemma can be embedded, and FC can represent a fully connected layer.
In some implementations, the pre-training output can be further configured to indicate whether each interface element of the plurality of interface elements is an unmodified interface element. As an example, the pre-training output can be configured to predict, for each "false" interface, whether the interface element of the interface is a "true" element of the respective interface. To follow the previous example, for a "false" interface a', two portions of one or more interface images can be replaced with a portion of one or more interface images from interface B, while the structural data remains the same as the original interface a. Intuitively, the contents of a "pseudo" interface element will not align with the rest of the interface elements. Thus, the machine-learned interface prediction model only needs to learn from the context to make the correct predictions. For example, the goal of the pre-training task can be the sum of weighted cross entropy losses of all UI components in the pseudo UI:
Wherein y is i Is the label of the i-th component, andis embedded by connecting to UI i Is used for prediction by the linear layer. The weight λ can be multiplied by the loss of the "pseudo" interface element to account for any type of tag imbalance (e.g., λ=2, etc.).
In some implementations, the pre-training task can be an image prediction task. As an example, one or more portions of one or more interface images can be masked before determining the plurality of intermediate embeddings. The pre-training tasks can be performed by processing the one or more user interface embeddings using the machine-learned interface prediction model or a separate pre-training prediction head to obtain a pre-training output, which can include predictions of one or more portions of the one or more interface images. The individual pre-training pre-heads can be small predictive components such as linear models, multi-layer perceptrons, or the like.
More specifically, as an example, a portion of one or more interface images (e.g., 15% of the image, etc.) can be masked (e.g., replacing the associated intermediate embeddings with 0 and replacing its type features with MASK)Etc.). The pre-training task can be configured to infer a masked portion of one or more interface images from surrounding inputs of the user interface. Conventionally, the method of interface image prediction relies on predicting object classes (e.g., tree, sky, car, etc.) or object features of the masked image portion, which can be obtained by a pre-trained object detector. However, this approach is highly dependent on the accuracy of the pre-trained object detector and is therefore not suitable for training of machine-learned interface prediction models. Accordingly, the systems and methods of the present disclosure instead focus on predicting masking image portions in a contrast learning manner. For example, given the embedding of one or more interface image portions and the additional embedding of some negative image portions (e.g., different portions, etc.) sampled from the same user interface, it can be expected that the output embedding of the masked positive example is closest to its embedding in terms of the remaining chord similarity scores. For example, let Is a set of masked image indices in a "real" user interface. Losses (e.g., noise Contrast Estimation (NCE) lost softmax version, etc.) can be employed as targets:
wherein,a set of negative IMG components of i can be represented. In some embodiments, the k closest image portions of the image to the masking portion i can be used as "negative" image portions.
In some implementations, one or more portions of text content depicted in one or more interface images can be masked before determining the plurality of intermediate embeddings. Additionally, in some implementations, performing the pre-training task can include processing one or more user interface embeddings using a machine-learned interface prediction model or a separate pre-training pre-head to obtain the pre-training output. As a more specific example, when masking one or more portions of text content, predictions of the masked text content can be formulated as generating questions because each OCR component is a word sequence. For example, a layer 1 GRU decoder can obtain a user interface embedding associated with masking a text content portion as input to generate a prediction of an unmasked portion of text content. Since the goal of the pre-training task is to learn a powerful interface embedding, in some embodiments, a simple decoder model or model portion can be utilized.
Additionally or alternatively, in some implementations, the tokens associated with the masked portion of the text content can be masked with a certain probability (e.g., 15% chance, etc.), as it may be difficult to generate the entire sequence. For example, when the complete text content includes the word "restaurants for families (restaurant for home)", only a portion of the text content including the word "restaurants" may be masked. By way of example, a designationA term as text content part i, wherein +.>Is the one-hot encoding of the j-th element and is markedAs a predicted probability of a generated token, the MOG target is formulated as the sum of the multi-class cross entropy losses between the masked token and the generated token:
wherein,set of (component id, word id) pairs representing masked OCR
In some implementations, one or more portions of the structural data can be masked before determining the plurality of intermediate embeddings. As an example, content description fields included in the structure data associated with the interface element may be masked. As another example, class name fields included in the structure data associated with the interface element may be masked.
In some implementations, one or more portions of the masked structural data can describe one or more class labels of one or more respective interface elements. Performing the pre-training task can include processing one or more user interface embeddings using a machine-learned interface prediction model or a separate pre-training pre-head to obtain a pre-training output. The pre-training output can include one or more predicted class labels for one or more corresponding class interfaces.
Alternatively or additionally, in some implementations, the one or more masked portions of the structural data can further include one or more content descriptors of one or more respective interface elements of the plurality of interface elements. Performing the pre-training task can include processing one or more user interface embeddings using a machine-learned interface prediction model or a separate pre-training pre-head to obtain a pre-training output. The pre-training output can include one or more predicted content descriptors for one or more respective interface elements.
As a more specific example, masking of content descriptors and class labels is generally observed to be particularly effective for portions of structural data (e.g., view level data, etc.). For each masking portion of the structure data that includes the content descriptor, a simple decoder can be used to generate the predicted content descriptor. Additionally, for each masked portion of the structural data that includes class labels, the predicted class labels can be predicted using a fully connected layer with softmax activation. For example:
Wherein,a set of masking portions capable of representing structural data, c i One-time thermal coding capable of representing class label i, < >>Probability vector capable of representing prediction, and t i，j ，/>Original and predicted content descriptors (e.g., content descriptor primitives, etc.) can be represented. In some implementations, this can be performed in a substantially similar manner as previously described with respect to the prediction of text content. Thus, in some embodiments, the pre-training loss function for all tasks can be defined as:
L＝L RUI +1 {y＝0} L RCP +1 {y＝1) (L MIP +L MOG +L MVG )， (8)
wherein 1 is {.} Is an indicator function.
In some implementations, one or more predictive tasks can be performed using the machine-learned interface prediction model based at least in part on the one or more user interface embeddings to obtain one or more respective interface prediction outputs. In some implementations, the one or more prediction tasks can include a search task, and the one or more prediction outputs can include a search retrieval output describing one or more retrieved interface elements similar to a query interface element of the plurality of interface elements. As an example, given a user interface and an interface component of the user interface that is a query, and a separate search user interface with a set of candidate interface elements, one or more retrieved elements that are closest to the query interface element can be selected based on various characteristics of the interface element (e.g., location, functionality, class labels, content descriptors, appearance, dimensions, etc.).
In some implementations, the one or more prediction tasks can include a relationship prediction task, and the corresponding prediction output can indicate a relationship between a portion of the structural data and an interface element of the plurality of interface elements. As an example, given a portion of the structural data (e.g., descriptive text describing the functionality of the interface element, etc., that may be presented to a user) and one or more interface images (e.g., and/or an embedding associated with the one or more interface images, etc.), a prediction output can be obtained that indicates a relationship between the portion of the structural data and the interface element of the plurality of interface elements. As an example, the portion of the structural data may include descriptive text for presentation to the user, including the word "click this button to go back (clicking this button to return)", and the interface element can be a conventional "return" arrow. The prediction element can be obtained using a machine-learned interfacial prediction model. For example, the portion of the structural data can be processed as OCR components (e.g., recognized text content, etc.), and the plurality of candidate interface elements can be processed as image portions for which a machine-learned interface prediction model can be an input. The dot product of the output embedding of the portion of the structural data and the output embedding of the candidate interface element can be calculated as their similarity score to obtain a predicted output indicative of the relationship between the portion of the structural data and the portion of the image data.
In some implementations, the one or more prediction tasks can include a structural image synchronization prediction task, and the prediction output can include correspondence values of the structural data and the one or more interface images. As an example, the machine-learned interface prediction model can process one or more interface images and structural data to obtain a prediction output. The prediction output can indicate whether the structural data matches one or more interface images (e.g., whether the structural data describes a location of an interface element included in the one or more interface images, etc.). More specifically, the user interface embedding associated with the CLS lemma can be followed by a layer of projection to predict a correspondence value that indicates whether the image and structural data of the user interface are synchronized. In this way, in some embodiments, the predictive task can act as a preprocessing step to filter out unwanted user interfaces.
In some implementations, the one or more prediction tasks can include an application classification task, and the corresponding prediction output can indicate an application category of an application associated with the user interface. As an example, the application classification task can predict a class of an application (e.g., music, finance, etc.) of the user interface. For example, there can be multiple candidate application categories available for selection. A projection layer can be used to project one or more user interface embeddings into one of a plurality of application categories (e.g., output of CLS components and concatenation of one or more user interface embeddings, etc.). It should be noted that the predictive power of applying classification tasks is typically found to be very accurate compared to conventional approaches due to the attention mechanism of the transducer portion of the machine-learned interfacial prediction model and extensive pre-training performed as described previously.
In some implementations, the one or more prediction tasks can include an interface element classification task, and the corresponding interface prediction output can include a classification output that indicates an interface element category of interface elements (e.g., navigation elements, interactable elements, descriptive elements, element types, etc.) in the plurality of interface elements. As an example, the interface element classification task can identify a category (e.g., menu, backward, search, etc.) of the icon interface element, which can be used for applications such as screen readers. To categorize the categories of interface elements, user interface embeddings associated with corresponding interface image portions and structure data portions of the interface elements can be joined and processed using a fully connected layer.
The systems and methods of the present disclosure provide several technical effects and benefits. As one example technical effect and benefit, the ability to quickly and efficiently navigate a user interface is essential to the operation of many modern computing devices. However, a subset of users with certain disabilities (e.g., vision impairment, paralysis, etc.) cannot routinely navigate a user interface, but instead rely on unobstructed (accessibility) solutions (e.g., screen readers, etc.). Conventionally, these unobstructed schemes lack the ability to understand or otherwise infer the functionality of the user interface. Thus, by training machine-learned models for user interface prediction, generation, and general understanding, the systems and methods of the present disclosure provide a significant increase in the efficiency and accuracy of an unobstructed solution for disabled users.
Referring now to the drawings, example embodiments of the present disclosure will be discussed in further detail.
Example devices and systems
FIG. 1A depicts a block diagram of an example computing system 100 that performs training and utilization of a machine-learned interface prediction model, according to an example embodiment of the disclosure. The system 100 includes a user computing device 102, a server computing system 130, and a training computing system 150 communicatively coupled by a network 180.
The user computing device 102 can be any type of computing device, such as, for example, a personal computing device (e.g., a laptop computer or desktop computer), a mobile computing device (e.g., a smart phone or tablet computer), a game console or controller, a wearable computing device, an embedded computing device, or any other type of computing device.
The user computing device 102 includes one or more processors 112 and memory 114. The one or more processors 112 can be any suitable processing device (e.g., processor core, microprocessor, ASIC, FPGA, controller, microcontroller, etc.), and can be one processor or multiple processors operatively connected. The memory 114 can include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, and the like, as well as combinations thereof. The memory 114 is capable of storing data 116 and instructions 118, the data 116 and instructions 118 being executed by the processor 112 to cause the user computing device 102 to perform operations.
In some implementations, the user computing device 102 can store or include one or more machine-learned interface prediction models 120. For example, the machine-learned interface prediction model 120 can be or otherwise include various machine-learned models, such as a neural network (e.g., deep neural network) or other types of machine-learned models, including nonlinear models and/or linear models. The neural network can include a feed forward neural network, a recurrent neural network (e.g., a long and short term memory recurrent neural network), a convolutional neural network, or other form of neural network. Some example machine-learned models can utilize an attention mechanism, such as self-attention. For example, some example machine-learned models can include a multi-headed self-attention model (e.g., a transformer model). An example machine-learned interface prediction model 120 is discussed with reference to fig. 2-3 and 5.
In some implementations, one or more machine-learned interface prediction models 120 can be received from the server computing system 130 over the network 180, stored in the user computing device memory 114, and then used or otherwise implemented by the one or more processors 112. In some implementations, the user computing device 102 is capable of implementing multiple parallel instances of a single machine-learned interface prediction model 120 (e.g., to perform parallel interface predictions across multiple instances of the machine-learned interface prediction model).
More specifically, interface data describing the user interface (e.g., a user interface presented by an application and/or operating system of the user computing device 102, etc.) can be obtained at the user computing device 102. The user interface can include a plurality of user interface elements (e.g., icons, interactable buttons, images, text content, etc.). The interface data can include structural data (e.g., metadata indicating the location of the interface element, etc.) and an interface image depicting the user interface. The plurality of intermediate embeddings can be determined based on the structural data, the one or more interface images, and/or text content depicted in the one or more interface images (e.g., using text recognition models (OCR), etc.). These intermediate embeddings can be processed using a machine-learned interface prediction model to obtain one or more user interface embeddings. Based on the one or more user interface embeddings, a pre-training task can be performed to obtain a pre-training output.
Additionally or alternatively, one or more machine-learned interface prediction models 140 can be included or otherwise stored in the server computing system 130 and implemented by the server computing system 130, the server computing system 130 in communication with the user computing device 102 according to client server relationships. For example, the machine-learned interface prediction model 140 can be implemented by the server computing system 130 as part of a web service (e.g., an interface prediction service). Accordingly, one or more models 120 can be stored and implemented at the user computing device 102 and/or one or more models 140 can be stored and implemented at the server computing system 130.
The user computing device 102 can also include one or more user input components 122 that receive user input. For example, the user input component 122 can be a touch-sensitive component (e.g., a touch-sensitive display screen or touchpad) that is sensitive to touch by a user input object (e.g., a finger or stylus). The touch sensitive component can be used to implement a virtual keyboard. Other example user input components include a microphone, a conventional keyboard, or other device by which a user can provide user input.
The server computing system 130 includes one or more processors 132 and memory 134. The one or more processors 132 can be any suitable processing device (e.g., a processor core, microprocessor, ASIC, FPGA, controller, microcontroller, etc.), and can be one processor or multiple processors operatively connected. The memory 134 can include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, and the like, and combinations thereof. The memory 134 is capable of storing data 136 and instructions 138, the data 136 and instructions 138 being executed by the processor 132 to cause the server computing system 130 to perform operations.
In some implementations, the server computing system 130 includes or is otherwise implemented by one or more server computing devices. In instances where the server computing system 130 includes multiple server computing devices, such server computing devices can operate in accordance with a sequential computing architecture, a parallel computing architecture, or some combination thereof.
As described above, the server computing system 130 can store or otherwise include one or more machine-learned interface prediction models 140. For example, model 140 can be or otherwise include various machine-learned models. Example machine-learned models include neural networks or other multi-layer nonlinear models. Example neural networks include feed forward neural networks, deep neural networks, recurrent neural networks, and convolutional neural networks. Some example machine-learned models can utilize an attention mechanism, such as self-attention. For example, some example machine-learned models can include a multi-headed self-attention model (e.g., a transformer model). Example model 140 is discussed with reference to fig. 2-3 and 5.
The user computing device 102 and/or the server computing system 130 can train the models 120 and/or 140 via interaction with a training computing system 150, the training computing system 150 being communicatively coupled by a network 180. The training computing system 150 can be separate from the server computing system 130 or can be part of the server computing system 130.
The training computing system 150 includes one or more processors 152 and memory 154. The one or more processors 152 can be any suitable processing device (e.g., processor core, microprocessor, ASIC, FPGA, controller, microcontroller, etc.), and can be one processor or multiple processors operatively connected. The memory 154 can include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, and the like, as well as combinations thereof. The memory 154 is capable of storing data 156 and instructions 158, which data 156 and instructions 158 are executed by the processor 152 to cause the training computing system 150 to perform operations. In some implementations, the training computing system 150 includes or is otherwise implemented by one or more server computing devices.
Training computing system 150 can include a model trainer 160, which model trainer 160 uses various training or learning techniques (such as, for example, error back propagation) to train machine-learned models 120 and/or 140 stored at user computing device 102 and/or server computing system 130. For example, the loss function can be counter-propagated through the model to update one or more parameters of the model (e.g., a gradient based on the loss function). Various loss functions can be used, such as mean square error, likelihood loss, cross entropy loss, hinge loss, and/or various other loss functions. Gradient descent techniques can be used to iteratively update parameters through several training iterations.
In some implementations, performing error back-propagation can include performing truncated back-propagation over time. Model trainer 160 can perform several generalization techniques (e.g., weight decay, fall-off (dropout), etc.) to improve the generalization ability of the trained model.
In particular, model trainer 160 can train machine-learned interface prediction models 120 and/or 140 based on a set of training data 162. The training data 162 can include, for example, a plurality of labeled and/or unlabeled user interfaces (e.g., interface data, etc.).
In some implementations, the training examples can be provided by the user computing device 102 if the user has provided consent. Thus, in such an embodiment, the model 120 provided to the user computing device 102 can be trained by the training computing system 150 on user-specific data received from the user computing device 102. In some instances, this process can be referred to as personalizing the model.
Model trainer 160 includes computer logic for providing the desired functionality. Model trainer 160 can be implemented in hardware, firmware, and/or software that controls a general purpose processor. For example, in some implementations, model trainer 160 includes program files stored on a storage device, loaded into memory, and executed by one or more processors. In other implementations, model trainer 160 includes one or more sets of computer-executable instructions stored in a tangible computer-readable storage medium (such as RAM, a hard disk, or an optical or magnetic medium).
The network 180 can be any type of communication network, such as a local area network (e.g., an intranet), a wide area network (e.g., the internet), or some combination thereof, and can include any number of wired or wireless links. In general, communications over network 180 can be carried via any type of wired and/or wireless connection using various communication protocols (e.g., TCP/IP, HTTP, SMTP, FTP), coding or formatting (e.g., HTML, XML), and/or protection schemes (e.g., VPN, secure HTTP, SSL).
FIG. 1A illustrates one example computing system that can be used to implement the present disclosure. Other computing systems can also be used. For example, in some implementations, the user computing device 102 can include a set of model trainers 160 and training data 162. In such an implementation, the model 120 can be trained and used locally at the user computing device 102. In some of such implementations, the user computing device 102 can implement the model trainer 160 to personalize the model 120 based on user-specific data.
FIG. 1B depicts a block diagram of an example computing device 10 performing pre-training of a machine-learned interface prediction model, according to an example embodiment of the disclosure. Computing device 10 can be a user computing device or a server computing device.
Computing device 10 includes several applications (e.g., applications 1 through N). Each application contains its own machine learning library and machine-learned models. For example, each application can include a machine-learned model. Example applications include text messaging applications, email applications, dictation applications, virtual keyboard applications, browser applications, and the like.
As illustrated in fig. 1B, each application is capable of communicating with several other components of the computing device, such as, for example, one or more sensors, a context manager, a device status component, and/or additional components. In some implementations, each application can communicate with each device component using an API (e.g., public API). In some implementations, the APIs used by each application are specific to that application.
FIG. 1C depicts a block diagram of an example computing device 50 that performs interface prediction using a machine-learned interface prediction model, according to an example embodiment of the disclosure. The computing device 50 can be a user computing device or a server computing device.
Computing device 50 includes several applications (e.g., applications 1 through N). Each application communicates with a central intelligent layer. Example applications include text messaging applications, email applications, dictation applications, virtual keyboard applications, browser applications, and the like. In some implementations, each application can communicate with the central intelligence layer (and models stored therein) using APIs (e.g., public APIs across all applications).
The central intelligence layer includes a number of machine-learned models. For example, as illustrated in fig. 1C, a respective machine-learned model can be provided for each application and managed by a central intelligent layer. In other implementations, two or more applications can share a single machine-learned model. For example, in some embodiments, the central intelligence layer can provide a single model for all applications. In some implementations, the central intelligence layer is included within or otherwise implemented by the operating system of computing device 50.
The central intelligence layer is capable of communicating with the central device data layer. The central device data layer can be a centralized repository of data for computing devices 50. As illustrated in fig. 1C, the central device data layer can communicate with several other components of the computing device, such as, for example, one or more sensors, a context manager, a device status component, and/or additional components. In some implementations, the central device data layer can communicate with each device component using an API (e.g., a proprietary API).
Example model arrangement
FIG. 2 depicts a block diagram of an example machine-learned interface prediction model 200, according to an example embodiment of the disclosure. In some implementations, the machine-learned interface prediction model 200 is trained to receive a set of input data 204 describing a user interface and, as a result of receiving the input data 204, provide output data 206 comprising one or more interface prediction outputs.
More specifically, the input data 204 can include interface data describing a user interface (e.g., a user interface presented by an application and/or operating system, etc.). The input data 204 can include a plurality of user interface elements (e.g., icons, interactable buttons, images, text content, etc.). The interface data can include structural data (e.g., metadata indicating the location of the interface element, etc.) and an interface image depicting the user interface. The machine-learned interface prediction model can process the input data 204 to obtain output data 206. The output data 206 can include one or more prediction outputs (e.g., search results, classification outputs, etc.).
FIG. 3 depicts a block diagram of an example machine-learned interface prediction model 300, according to an example embodiment of the disclosure. The machine-learned interfacial prediction model 300 is similar to the machine-learned interfacial prediction model 200 of fig. 2, except that the machine-learned interfacial prediction model 300 further includes an embedding portion 302 and a transformer portion 305.
More specifically, the input data 204 can be first processed by the embedded portion 302 of the machine-learned interfacial prediction model 300. As an example, the embedding portion 302 can process interface data (e.g., structural data, one or more interface images, etc.) of the input data 204 to obtain a plurality of intermediate embeddings 304. The plurality of intermediate embeddings 304 can be processed using the transformer portion 305 of the machine-learned interfacial prediction model 300 to obtain the output data 206. For example, multiple intermediate embeddings 304 can be summed. The summed plurality of intermediate embeddings 304 can be processed using the transformer portion 305 of the machine-learned interface prediction model 300 to obtain the output data 206, which output data 206 can include one or more user interface embeddingsAs indicated below:
U＝TransformerEncoder(T+P+C), (1)
wherein,and n is the sequence length. Alternatively or additionally, in some implementations, the output data 206 can include one or more predicted outputs and/or one or more pre-trains And outputting.
Fig. 4 depicts an example diagram 400 of a user interface according to an example embodiment of the present disclosure. More specifically, as depicted, the user interface 402 can be a user interface of an application presented on a display device. The user interface can include a plurality of interface elements 404. The interface elements can include icons, interactable elements (e.g., buttons, etc.), indicators, and the like. For example, the plurality of interface elements 404 can include a "return" navigation element 404A. As another example, the plurality of interface elements 404 can include descriptive elements 404B. As yet another example, the plurality of interface elements 404 can include an input field element 404C.
The user interface 402 can include structural data 406. The structure data 406 can indicate the location of the interface element 404. As an example, the structure data 406 can indicate the size and location of the navigation element 404A within the presented user interface 402. As another example, the structure data 406 can indicate or otherwise specify various characteristics of the input field interface element 404C (e.g., font, text size, field position, feedback characteristics (e.g., initiate force feedback actions upon receiving input from a user, play sound upon receiving input from a user, etc.), functionality between other applications (e.g., allow use of a virtual keyboard application, etc.), etc.
In some implementations, the structure data 406 can be or otherwise include view hierarchy data 406A. The view hierarchy data 406A can include a tree representation of a plurality of interface elements 404. Each node of the tree of view hierarchy data 406A can describe certain attributes (e.g., bounding box location, functionality, etc.) of interface element 404. As an example, the view level data 406A of the structure data 406 can include text content data associated with the visible text included in the input field element 404C included in the user interface 402. As another example, the view hierarchy tree 406A of the structure data 406 can include content descriptors and/or resource ids that can describe functionality (e.g., interface navigation paths, shared functionality, etc.) that is not typically provided to the user. As another example, the view hierarchy tree 406A of the structure data 406 can include class name data describing functional classes of the application programming interface and/or software tool associated with an implementation of the corresponding interface element.
The user interface 402 can include an interface image 408 depicting at least a portion of the user interface. For example, the one or more interface images 408 can be images captured when the user interface 402 is displayed on a display device (e.g., captured using a camera device, a screen capture application, etc.). As depicted, the one or more interface images 408 can include portions of the one or more interface images 408. For example, one or more interface images 408 can be segmented into portions corresponding to particular interface elements (e.g., element 402C, etc.) of the user interface 402. Additionally, one or more interface images 408 can additionally depict text content 410. Text content 410 can be identified from one or more interface images using text recognition techniques (e.g., optical character recognition, etc.).
FIG. 5 depicts a dataflow diagram of performing a pre-training task using a machine-learned interface prediction model. More specifically, the user interface 502 (e.g., interface elements, structure data, interface images, etc.) can be processed using the embedded portion 504 of the machine-learned interface prediction model to obtain the intermediate embeddings 506. The intermediate embedding 506 can be or otherwise include a location embedding 506A, a type embedding 506B, and an image/text embedding 506C. As an example, using the embedding portion 504 of the machine-learned interface prediction model, features extracted from the user interface 502 can be linearly projected to obtain a plurality of intermediate embeddings-for the ith input, where type (i) ∈ { IMG, OCR, VH }.
More specifically, in some implementations, the location embedding 506A can be determined from the structural data of the user interface 502 using the embedding portion 504 of the machine-learned interface prediction model. The location embedment 506A can correspond to one or more locations of one or more respective interface elements of the user interface 502.
The image/text embedding 506C can be determined from one or more interface images. The image/text embedding 506C embedding can be associated with at least one interface element of the plurality of interface elements, respectively. As an example, one or more portions of one or more interface images can be determined from one or more interface images of the user interface 502 (e.g., based on a bounding box described by the structural data, etc.). The embedded portion 504 of the machine-learned interface prediction model is capable of processing portions of one or more interface images of the user interface 502 to obtain a corresponding image/text embedding 506C (e.g., using a final spatial averaging pooling layer, etc.).
The plurality of intermediate embeddings 506 can include a type embedment 502B. The embeddings 502B can indicate an embedment type of each of the other embeddings in the plurality of intermediate embeddings 506, respectively. As an example, to distinguish portions of interface data of user interface 502, six types of tokens 502B can be utilized: IMG, OCR, VH, CLS, SEP and MASK. In some implementations, MASK primitives can be of a type used to improve the pretraining accuracy of machine-learned interface prediction models. For example, type embedding 502B can be obtained using one-hot encoding followed by linear projection,-for the i-th component in the sequence, where d is the dimension size.
The plurality of intermediate embeddings 506 can be summed and processed using a transformer portion 508 of the machine-learned interface prediction model to obtain one or more user interface embeddings 510. For example, the transformer portion 508 of the machine-learned interface prediction model can process the summed intermediate embeddings 506 to obtain one or more user interface embeddings 510As indicated below:
U＝TransformerEncoder(T+P+C), (1)
wherein,and n is the sequence length.
Based at least in part on the one or more user interface embeddings 510, one or more pre-training tasks 512 can be performed using the machine-learned interface prediction model (e.g., the transformer portion 508, etc.). As an example, the pre-training task 512 can be or otherwise include an interface prediction task. For example, before determining the plurality of intermediate embeddings 506, one or more of the plurality of interface elements of the user interface 502 can be replaced with one or more corresponding second interface elements of a second user interface different from the user interface. As another example, the pre-training task 512 can be or otherwise include an interface element prediction task. As another example, the pre-training task 512 can be or otherwise include an image prediction task. As another example, the pre-training task 512 can be or otherwise include a search retrieval task. As another example, the pre-training task 512 can be or otherwise include an application category classification task. As another example, the pre-training task 512 can be or otherwise include a correspondence prediction task for determining correspondence between structural data and one or more interface images. As another example, the pre-training task 512 can be or otherwise include a relationship prediction task for determining a relationship between a portion of the structural data and an interface element of the plurality of interface elements. As yet another example, the pre-training task 512 can be or otherwise include an interface element category classification task.
Example method
FIG. 6 depicts a flowchart of an example method of performing pre-training of a machine-learned interface prediction model, according to an example embodiment of the disclosure. Although fig. 6 depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particular illustrated order or arrangement. The various steps of method 600 may be omitted, rearranged, combined, and/or accommodated in various ways without departing from the scope of the present disclosure.
At 602, a computing system can obtain interface data. More specifically, a computing system is capable of obtaining interface data describing a user interface including a plurality of interface elements. The interface data can include structural data and an interface image depicting a user interface. The structure data can indicate one or more locations of one or more interface elements of the plurality of interface elements.
At 604, the computing system can determine a plurality of intermediate embeddings. More specifically, the computing system can determine a plurality of intermediate embeddings based at least in part on one or more of the structural data, the one or more interface images, or the textual content depicted in the one or more interface images.
At 606, the computing system can process the plurality of intermediate embeddings to obtain one or more user interface embeddings. More specifically, the computing system is capable of processing the plurality of intermediate embeddings using the machine-learned interface prediction model to obtain one or more user interface embeddings.
At 608, the computing system can perform a pre-training task. More specifically, the computing system can perform a pre-training task based at least in part on one or more user interface embeddings to obtain a pre-training output.
At 608, one or more pre-training tasks can be performed. The pre-training tasks can be performed in parallel (i.e., jointly) or in series.
In some embodiments, the method can further comprise: evaluating, by the computing system, a loss function that evaluates differences between the true value data and the pre-training output; and adjusting, by the computing system, one or more parameters of the machine-learned interface prediction model based at least in part on the loss function.
Additional disclosure
The technology discussed herein refers to servers, databases, software applications, and other computer-based systems, as well as actions taken and information sent to and from such systems. The flexibility inherent in computer-based systems allows for a variety of possible configurations, combinations, and divisions of tasks and functionality between components. For example, the processes discussed herein can be implemented using a single device or component or multiple devices or components working in combination. The database and applications can be implemented on a single system or distributed across multiple systems. The distributed components can operate sequentially or in parallel.
While the present subject matter has been described in detail with respect to various specific example embodiments thereof, each example is provided by way of explanation and not limitation of the disclosure. Modifications, variations and equivalents of such embodiments will occur to those skilled in the art upon attaining an understanding of the foregoing. Accordingly, the subject disclosure does not preclude inclusion of such modifications, variations and/or additions to the present subject matter as would be readily apparent to one of ordinary skill in the art. For instance, features illustrated or described as part of one embodiment, can be used with another embodiment to yield a still further embodiment. Accordingly, this disclosure is intended to cover such alternatives, modifications, and equivalents.
Claims (20)
1. A computer-implemented method for training and utilizing a machine-learned model for user interface prediction, comprising:
obtaining, by a computing system comprising one or more computing devices, interface data describing a single user interface comprising a plurality of interface elements, wherein the interface data comprises one or more interface images depicting the single user interface;
determining, by the computing system, a plurality of intermediate embeddings based at least in part on the one or more interface images or one or more of the text content depicted in the one or more interface images;
Processing, by the computing system, the plurality of intermediate embeddings using a machine-learned interface prediction model to obtain one or more user interface embeddings; and
a pre-training task is performed by the computing system based at least in part on the one or more user interface embeddings to obtain a pre-training output.
2. The computer-implemented method of any preceding claim, wherein the method further comprises:
evaluating, by the computing system, a loss function that evaluates differences between real value data and the pre-training output; and
one or more parameters of the machine-learned interface prediction model are adjusted by the computing system based at least in part on the loss function.
3. The computer-implemented method of any preceding claim, wherein:
before determining the plurality of intermediate embeddings, the method includes: replacing, by the computing system, one or more interface elements of the plurality of interface elements with one or more corresponding second interface elements of a second user interface different from the single user interface; and
performing the one or more pre-training tasks includes: processing, by the computing system, the one or more user interface embeddings using the machine-learned interface prediction model or a separate pre-training pre-header to obtain the pre-training output, wherein the pre-training output is configured to indicate whether the single user interface is an unmodified user interface.
4. The computer-implemented method of claim 3, wherein the pre-training output is further configured to indicate whether each interface element of the plurality of interface elements is an unmodified interface element.
5. The computer-implemented method of any preceding claim, wherein:
before determining the plurality of intermediate embeddings, the method includes: masking, by the computing system, one or more portions of the one or more interface images; and
performing the one or more pre-training tasks includes: processing, by the computing system, the one or more user interface embeddings using the machine-learned interface prediction model or a separate pre-training pre-header to obtain the pre-training output, wherein the pre-training output includes a predicted complement of the one or more portions of the one or more interface images that have been masked, the predicted complement selected from a pool of candidate images.
6. The computer-implemented method of any preceding claim, wherein:
before determining the plurality of intermediate embeddings, the method includes: masking, by the computing system, one or more portions of the text content depicted in the one or more interface images; and
Performing the pre-training task includes: processing, by the computing system, the one or more user interface embeddings using the machine-learned interface prediction model or a separate pre-training pre-header to obtain the pre-training output, wherein the pre-training output includes a predicted text complement of the masked one or more portions of the text content depicted in the one or more interface images.
7. The computer-implemented method of any preceding claim, wherein prior to determining the plurality of intermediate embeddings, the method comprises: one or more portions of the structural data indicating one or more locations of one or more respective interface elements of the plurality of interface elements are masked by the computing system.
8. The computer-implemented method of claim 7, wherein:
the one or more portions of the structural data further depict one or more class labels of one or more respective interface elements of the plurality of interface elements; and
performing the one or more pre-training tasks includes: processing, by the computing system, the one or more user interface embeddings using the machine-learned interface prediction model or a separate pre-training pre-header to obtain the pre-training output, wherein the pre-training output includes one or more predicted class labels for the one or more corresponding interface elements.
9. The computer-implemented method of any of claims 7 to 8, wherein:
the one or more portions of the structural data further include one or more content descriptors of one or more respective interface elements of the plurality of interface elements; and
performing the one or more pre-training tasks includes: processing, by the computing system, the one or more user interface embeddings using the machine-learned interface prediction model or a separate pre-training pre-header to obtain the pre-training output, wherein the pre-training output includes one or more predicted content descriptors of the one or more respective interface elements.
10. The computer-implemented method of any preceding claim, wherein the method further comprises:
performing, by the computing system, one or more prediction tasks based at least in part on the one or more user interface embeddings using the machine-learned interface prediction model to obtain one or more respective interface prediction outputs.
11. The computer-implemented method of claim 10, wherein the respective one or more interface prediction outputs comprise at least one of:
A search retrieval output describing one or more retrieved interface elements that are similar to a query interface element of the plurality of interface elements;
a predictive output indicating a relationship between a portion of structural data and an interface element of the plurality of interface elements, the portion of structural data indicating one or more locations of one or more respective interface elements of the plurality of interface elements;
a prediction output comprising correspondence values of the structural data and the one or more interface images;
a classification output indicating an application category of an application associated with the single user interface; or alternatively
A classification output indicating an interface element category of an interface element of the plurality of interface elements.
12. The computer-implemented method of any preceding claim, wherein the plurality of intermediate embeddings includes one or more image embeddings, one or more text embeddings, and one or more location embeddings.
13. The computer-implemented method of claim 12, wherein determining the plurality of intermediate embeddings comprises:
Determining, by the computing system, the one or more image embeddings from the one or more interface images, wherein the one or more image embeddings are respectively associated with at least one interface element of the plurality of interface elements; and
determining, by the computing system, the one or more text embeddings from the text content depicted in the one or more interface images based at least in part on the interface data.
14. The computer-implemented method of any preceding claim, wherein:
determining the plurality of intermediate embeddings includes: processing, by the computing system, the one or more interface images or text content depicted in the one or more interface images using an embedded portion of the machine-learned interface prediction model to obtain the plurality of intermediate embeddings; and
processing the plurality of intermediate embeddings using the machine-learned interfacial prediction model includes: the plurality of intermediate embeddings are processed by the computing system using a transformer portion of the machine-learned interface prediction model to obtain the one or more user interface embeddings.
15. A computing system, comprising:
One or more processors;
one or more tangible, non-transitory computer-readable media storing computer-readable instructions, the one or more tangible, non-transitory computer-readable media storing a machine-learned interface prediction model configured to generate a learned representation of a user interface, the machine-learned interface prediction model having been trained by performing operations comprising:
obtaining interface data, the interface data depicting a single user interface comprising a plurality of interface elements, wherein the interface data comprises an interface image depicting the single user interface;
determining a plurality of intermediate embeddings based at least in part on the one or more interface images or one or more of the text content depicted in the one or more interface images;
processing the plurality of intermediate embeddings using a machine-learned interface prediction model to obtain one or more user interface embeddings; and
a pre-training task is performed based at least in part on the one or more user interface embeddings to obtain a pre-training output.
16. The computing system of claim 15, wherein:
prior to determining the plurality of intermediate embeddings, the operations further comprise: replacing one or more interface elements of the plurality of interface elements with one or more corresponding second interface elements of a second user interface different from the single user interface; and
Performing the pre-training task includes: processing the one or more user interface embeddings using the machine-learned interface prediction model or a separate pre-training pre-header to obtain the pre-training output, wherein the pre-training output is configured to indicate whether the single user interface is an unmodified user interface; and
the pre-training output is further configured to indicate whether each interface element of the plurality of interface elements is an unmodified interface element.
17. The computing system of claim 15 or 16, wherein:
prior to determining the plurality of intermediate embeddings, the operations further comprise: masking one or more portions of the one or more interface images; and
performing the one or more pre-training tasks includes: processing the one or more user interface embeddings using the machine-learned interface prediction model or a separate pre-training prediction head to obtain the pre-training output, wherein the pre-training output includes predictions of the one or more portions of the one or more interface images.
18. The computing system of any of claims 15 to 17, wherein:
The operations further comprise: performing one or more prediction tasks based at least in part on the one or more user interface embeddings using the machine-learned interface prediction model to obtain one or more respective interface prediction outputs; and
the one or more interface prediction outputs include at least one of:
a search retrieval output describing one or more retrieved interface elements that are similar to a query interface element of the plurality of interface elements;
a predictive output indicating a relationship between a portion of structural data and an interface element of the plurality of interface elements, the portion of structural data indicating one or more locations of one or more respective interface elements of the plurality of interface elements;
a prediction output comprising correspondence values of the structural data and the one or more interface images;
a classification output indicating an application category of an application associated with the single user interface; or alternatively
A classification output indicating an interface element category of an interface element of the plurality of interface elements.
19. The computing system of any of claims 15 to 18, wherein:
determining the plurality of intermediate embeddings includes: processing one or more of the one or more interface images or text content depicted in the one or more interface images using an embedded portion of the machine-learned interface prediction model to obtain the plurality of intermediate embeddings; and
processing the plurality of intermediate embeddings using the machine-learned interfacial prediction model includes: the plurality of intermediate embeddings are processed using a transformer portion of the machine-learned interface prediction model to obtain the one or more user interface embeddings.
20. One or more tangible, non-transitory computer-readable media storing computer-readable instructions that, when executed by one or more processors, cause the one or more processors to perform operations comprising:
obtaining interface data, the interface data depicting a single user interface comprising a plurality of interface elements, wherein the interface data comprises structural data and an interface image depicting the single user interface, wherein the structural data indicates one or more locations of one or more respective ones of the plurality of interface elements;
Determining a plurality of intermediate embeddings based at least in part on one or more of: the structural data, the one or more interface images, or text content depicted in the one or more interface images;
processing the plurality of intermediate embeddings using a machine-learned interface prediction model to obtain one or more user interface embeddings; and
a pre-training task is performed based at least in part on the one or more user interface embeddings to obtain a pre-training output.
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2021/035510 WO2022256007A1 (en) | 2021-06-02 | 2021-06-02 | Machine-learned models for user interface prediction and generation |
Publications (1)
Publication Number | Publication Date |
---|---|
CN117121021A true CN117121021A (en) | 2023-11-24 |
Family
ID=76601841
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202180096561.4A Pending CN117121021A (en) | 2021-06-02 | 2021-06-02 | Machine-learned model for user interface prediction and generation |
Country Status (3)
Country | Link |
---|---|
EP (1) | EP4298564A1 (en) |
CN (1) | CN117121021A (en) |
WO (1) | WO2022256007A1 (en) |
-
2021
- 2021-06-02 CN CN202180096561.4A patent/CN117121021A/en active Pending
- 2021-06-02 EP EP21734731.9A patent/EP4298564A1/en active Pending
- 2021-06-02 WO PCT/US2021/035510 patent/WO2022256007A1/en active Application Filing
Also Published As
Publication number | Publication date |
---|---|
WO2022256007A1 (en) | 2022-12-08 |
EP4298564A1 (en) | 2024-01-03 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
Raschka et al. | Python machine learning: Machine learning and deep learning with Python, scikit-learn, and TensorFlow 2 | |
US11734375B2 (en) | Automatic navigation of interactive web documents | |
CN107066464B (en) | Semantic natural language vector space | |
AU2016256753B2 (en) | Image captioning using weak supervision and semantic natural language vector space | |
GB2547068B (en) | Semantic natural language vector space | |
KR20210040319A (en) | Method, apparatus, device, storage medium and computer program for entity linking | |
EP3411835B1 (en) | Augmenting neural networks with hierarchical external memory | |
CN112667816B (en) | Deep learning-based aspect level emotion analysis method and system | |
CN113010702A (en) | Interactive processing method and device for multimedia information, electronic equipment and storage medium | |
CN112507090A (en) | Method, apparatus, device and storage medium for outputting information | |
KR20190075277A (en) | Method for searching content and electronic device thereof | |
CN111902812A (en) | Electronic device and control method thereof | |
He et al. | Deep learning in natural language generation from images | |
US20240004677A1 (en) | Machine-Learned Models for User Interface Prediction, Generation, and Interaction Understanding | |
Yuan et al. | Deep learning from a statistical perspective | |
US11558471B1 (en) | Multimedia content differentiation | |
CN117121021A (en) | Machine-learned model for user interface prediction and generation | |
Windiatmoko et al. | Mi-Botway: A deep learning-based intelligent university enquiries chatbot | |
Lo et al. | CNERVis: a visual diagnosis tool for Chinese named entity recognition | |
US11645451B2 (en) | Managing relationships among original, modified, and related messages using significance-level analysis and change-relevancy annotations | |
CN116628179B (en) | User operation data visualization and man-machine interaction recommendation method | |
US20230222236A1 (en) | Composite analysis content privacy | |
Gandhi et al. | Sign Language Recognition Using Convolutional Neural Network | |
Agrawal et al. | Analogous sign language communication using gesture detection | |
WO2024035416A1 (en) | Machine-learned models for multimodal searching and retrieval of images |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |