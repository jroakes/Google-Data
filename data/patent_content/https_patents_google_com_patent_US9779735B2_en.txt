US9779735B2 - Methods and systems for detecting and processing speech signals - Google Patents
Methods and systems for detecting and processing speech signals Download PDFInfo
- Publication number
- US9779735B2 US9779735B2 US15/052,426 US201615052426A US9779735B2 US 9779735 B2 US9779735 B2 US 9779735B2 US 201615052426 A US201615052426 A US 201615052426A US 9779735 B2 US9779735 B2 US 9779735B2
- Authority
- US
- United States
- Prior art keywords
- computing device
- confidence score
- hotword
- audio data
- hotword confidence
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/28—Constructional details of speech recognition systems
- G10L15/30—Distributed recognition, e.g. in client-server systems, for mobile phones or network applications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/28—Constructional details of speech recognition systems
- G10L15/32—Multiple recognisers used in sequence or in parallel; Score combination systems therefor, e.g. voting systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/02—Feature extraction for speech recognition; Selection of recognition unit
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L2015/088—Word spotting
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/223—Execution procedure of a spoken command
Definitions
- Media data (e.g., audio/video content) is sometimes shared between multiple modules on a network. To get the most out of such media sharing arrangements, it is desirous to have a platform that is capable of processing such media data from the multiple modules simultaneously.
- the present disclosure generally relates to methods and systems for processing audio signals. More specifically, aspects of the present disclosure relate to detecting and processing speech signals from multiple end points simultaneously.
- One embodiment of the present disclosure relates to a method comprising: detecting, at one or more data modules in a group of data modules in communication with one another over a network, an activation command; computing, for each of the one or more data modules, a score for the detected activation command; receiving audio data from each detecting data module having a computed score above a threshold; sending a request to a server in communication with the group of data modules over the network, wherein the request includes the audio data received from each of the detecting data modules having a computed score above the threshold; receiving from the server, in response to the sent request, audio data associated with a requested action; and communicating the requested action to each of the data modules in the group of data modules.
- the method further comprises: combining the audio data received from each of the detecting data modules having a computed score above the threshold; and generating the request to the server based on the combined audio data.
- the method further comprises, in response to detecting the activation command, muting a loudspeaker of each data module in the group.
- the method further comprises activating a microphone of each detecting data module having a computed score above the threshold.
- the method further comprises causing each of the data modules in the group to playout an audible confirmation of the requested action communicated to each of the data modules.
- Another embodiment of the present disclosure relates to a system comprising a group of data modules in communication with one another over a network, where each of the data modules is configured to: in response to detecting an activation command, compute a score for the detected activation command; determine whether the computed score for the activation command is higher than a threshold number of computed scores for the activation command received from other detecting data modules in the group; in response to determining that the computed score for the activation command is higher than the threshold number of computed scores received from the other detecting data modules, send audio data recorded by the data module to a server in communication with the group of data modules over the network; receive from the server, in response to the sent audio data, a requested action; determine a confidence level for the requested action received from the server; and perform the requested action based on a determination that the confidence level determined by the data module is higher than confidence levels determined by a threshold number of other data modules that received the requested action from the server.
- each of the data modules in the system is configured to, in response to computing the score for the detected activation command, send the computed score to each of the other data modules in the group.
- each of the data modules in the system is configured to receive, from other detecting data modules in the group, scores for the activation command computed by the other detecting data modules.
- each of the data modules in the system is configured to broadcast the determined confidence level to the other data modules in the group that received the requested action from the server.
- each of the data modules in the system is configured to: compare the confidence level determined by the data module to confidence levels broadcasted by the other data modules in the group that received the requested action from the server; and determine, based on the comparison, that the confidence level determined by the data module is higher than the confidence levels determined by the threshold number of other data modules that received the requested action from the server.
- each of the data modules in the system is configured to, in response to determining that the confidence level determined by the data module is higher than the confidence levels determined by the threshold number of other data modules, playout an audible confirmation of the request action received from the server.
- each of the data modules in the system is configured to compute a score for the detected activation command based on one or more of the following: a power of a signal received at the data module for the activation command; a determined location of a source of the activation command relative to the data module; and whether the detected activation command corresponds to a previously stored activation command.
- the methods and systems described herein may optionally include one or more of the following additional features: the computed score for the activation command detected at a data module is based on one or more of a power of a signal received at the data module for the activation command, a determined location of a source of the activation command relative to the data module, and whether the detected activation command corresponds to a previously stored activation command; the audio data received from each detecting data module having a computed score above the threshold includes speech data captured and recorded by the data module; the speech data captured and recorded by the data module is associated with a speech command generated by a user; the speech data captured by each data module with an activated microphone is associated with a portion of a speech command generated by a user, the audio data recorded by the data module includes speech data recorded by the data module; the speech data recorded by the data module is associated with a speech command generated by a user; the confidence level for the requested action is determined based on an audio quality measurement for the audio data recorded by the data module and sent to
- processor and memory systems disclosed herein may also be configured to perform some or all of the method embodiments disclosed above.
- embodiments of some or all of the methods disclosed above may also be represented as instructions embodied on transitory or non-transitory processor-readable storage media such as optical or magnetic memory or represented as a propagated signal provided to a processor or data processing device via a communication network such as an Internet or telephone connection.
- FIG. 1 is a block diagram illustrating an example content management system and surrounding network environment according to one or more embodiments described herein.
- FIG. 2 is a flowchart illustrating an example method for detecting, processing, and responding to speech signals from multiple end points according to one or more embodiments described herein.
- FIG. 3 is a block diagram illustrating example components and data flows for detecting a speech command in a multi-device content management system according to one or more embodiments described herein.
- FIG. 4 is a block diagram illustrating example components and data flows for assessing quality of a detected speech command in a multi-device content management system according to one or more embodiments described herein.
- FIG. 5 is a block diagram illustrating example components and data flows for activating a device based on a detected speech command in a multi-device content management system according to one or more embodiments described herein.
- FIG. 6 is a block diagram illustrating example components and data flows for processing a detected speech command in a multi-device content management system according to one or more embodiments described herein.
- FIG. 7 is a block diagram illustrating example components and data flows for responding to a speech command in a multi-device content management system according to one or more embodiments described herein.
- FIG. 8 is a block diagram illustrating an example computing device arranged for detecting and processing speech signals from multiple end points simultaneously according to one or more embodiments described herein.
- Embodiments of the present disclosure relate to methods, systems, and apparatuses for detecting, processing, and responding to audio (e.g., speech) within an area or space (e.g., a room).
- a platform for multiple media devices connected via a network may be configured to process speech (e.g., voice commands) detected at the media devices, and respond to the detected speech by causing the media devices to simultaneously perform one or more requested actions.
- the methods and systems of the present disclosure use a distributive approach for handling voice commands by considering input from multiple end points of the platform.
- Such end points may be, for example, independent data modules (e.g., media and/or audio devices such as, for example, loudspeakers) connected to one another via a wired or wireless network (e.g., Wi-Fi, Ethernet, etc.).
- independent data modules e.g., media and/or audio devices such as, for example, loudspeakers
- a wired or wireless network e.g., Wi-Fi, Ethernet, etc.
- each data module e.g., loudspeaker
- a unique role e.g., has particular responsibilities, privileges, and/or capabilities
- speech commands e.g., generated by a user.
- the flexibility of the architecture is partly based on the ability of the data modules to dynamically switch between different roles (e.g., operating roles) while the system is in active operation.
- the methods and systems of the present disclosure are capable of scoring the quality of a speech request (e.g., voice command, speech command, etc.), handling speech requests from multiple end points using a centralized processing approach, a de-centralized processing approach, or a combination thereof, and also manipulating partial processing of speech requests from multiple end points into a coherent whole when necessary.
- a speech request e.g., voice command, speech command, etc.
- each data module may compute (e.g., determine) a score for audio data (e.g., speech command, activation command, etc.) it records.
- the score computed by a data module may be referred to as a “Hot Word” score for the data module.
- the computed Hot Word scores may then be used by the system to evaluate which of the data modules received the best signal.
- the Hot Word score computed by each of the data modules may be based on, for example, one or more of the following:
- Power of the signal For example, the power of the signal received at the data module for the speech command may be compared to the power of the signal received prior to the speech command.
- the audio data received or recorded at a given data module may be fed to a Hot Word detector, which may be configured to determine whether the audio data corresponds to a known (e.g., stored) Hot Word.
- the Hot Word detector may utilize a neural network (NN) or a deep neural network (DNN), which takes features of the input audio data and determines (e.g., identifies, assesses, evaluates, etc.) whether there are any occurrences of a Hot Word.
- NN neural network
- DNN deep neural network
- the detector may, for example, set a flag.
- the Hot Word detector may be configured to generate a score for any detection of a Hot Word that is made by the detector.
- the score may, for example, reflect a confidence of the NN or DNN with regard to the detection. For example, the higher the score, the more confident the network is that a Hot Word is present in the audio data.
- the output of the DNN may be a likelihood (e.g., probability) of the Hot Word being present in the audio data recorded at the data module.
- the determined likelihood may be compared to a threshold (e.g., a likelihood threshold, which may be predetermined and/or dynamically adaptable or adjustable based on, for example, network conditions, scores calculated for other nearby data modules, some combination thereof, and the like), and if the determined likelihood is at or above the threshold then a flag may be set to indicate the detection of a Hot Word.
- the threshold may be set so as to achieve or maintain, for example, a target false-detection versus miss-detection rate. As will be described in greater detail herein, if a Hot Word detection confidence is higher for a particular one of the data modules, it intuitively follows that the module in question will likely have a higher chance of correctly recognizing the command query that follows the detected Hot Word.
- (iii) Location of the user relative to the data module For example, by using a localizer (which, for example, may be part of a beamformer, or may be a standalone module) the angle of the sound source may be obtained. In another example, the angles provided by different data modules may be triangulated to estimate the position of the user (this is based on the assumption that the positions of the data modules are known).
- a localizer which, for example, may be part of a beamformer, or may be a standalone module
- the angles provided by different data modules may be triangulated to estimate the position of the user (this is based on the assumption that the positions of the data modules are known).
- Additional processing performed on the audio e.g., combining all microphone array outputs using a beamformer, applying noise suppression/cancellation, gain control, echo suppression/cancellation, etc.
- the system of the present disclosure may be configured to handle speech requests from multiple end points (e.g., data modules) using a centralized processing approach, a de-centralized processing approach, or an approach based on a combination thereof.
- audio data e.g., speech data
- one centralized processor e.g., which may be one of the data modules in a group of data modules, as will be further described below.
- the centralized processor may determine (e.g., identify, select, etc.), based on scores associated with the audio data received from each of the sources, one or more of the sources that recorded the highest quality audio data (e.g., the processor may determine the sources that have scores higher than the scores associated with a threshold number of other sources).
- the centralized processor may send the audio data received from the sources having the highest scores to a server (e.g., a server external to the system of data modules) for further processing.
- the centralized processor may then receive a response from the server and take appropriate action in accordance with the response.
- each data module in a group of data modules may determine its own Hot Word score and broadcast its score to the other data modules in the group. If a data module in the group determines, based on the broadcasted scores, that the data module has one of the best (e.g., highest quality) signals, then the data module may send/upload its recorded audio data (e.g., speech data relating to a command from the user) to the server (e.g., the Voice Search Back-End, further details of which will be provided below). Upon receiving a response from the server, the data module may then broadcast its confidence level of the response and wait for similar broadcasts from other data modules in the group. If the data module determines that it has one of the highest confidence levels for the response, the data module may act on the response accordingly.
- the server e.g., the Voice Search Back-End, further details of which will be provided below.
- a data module when a data module detects a Hot Word, the data module generates a score for the detected Hot Word, broadcasts the score to the other data modules in the group (e.g., an Ethernet broadcast), and waits for some period of time (which may be a predetermined period of time, a period of time based on a setting that may or may not be adjustable, or the like) to receive similar broadcasts from other modules. After the designated period of time has passed, the data module has access to the scores generated by all of the other data modules in the group that have also detected the Hot Word.
- the data module (as well as each of the other detecting data modules in the group) can then determine (e.g., rank) how well it scored with respect to the other detecting data modules. For example, if the data module determines that it has one of the top (e.g., two, three, etc.) scores for the Hot Word, the data module can decide to take action.
- rank e.g., how well it scored with respect to the other detecting data modules. For example, if the data module determines that it has one of the top (e.g., two, three, etc.) scores for the Hot Word, the data module can decide to take action.
- the system may also be capable of performing partial processing of speech commands by utilizing portions of audio data received from multiple data modules. For example, in accordance with one or more embodiments, the system may capture each part of a sentence spoken by the user from the “best” loudspeaker for that particular part. Such partial processing may be applicable, for example, when a user speaks a command while moving around within a room. A per-segment-score may be created for each data module and each word processed independently. It should be noted that because the clocks of the data modules in a given group are synchronized, the system is able to compare signal-to-noise ratio (SNR) values between speech segments.
- SNR signal-to-noise ratio
- users are given the ability to play audio content available from an audio source (e.g., audio content stored on a user device, audio content associated with a URL and accessible through the user device, etc.) to any combination of audio devices that share a common wireless or wired network.
- an audio source e.g., audio content stored on a user device, audio content associated with a URL and accessible through the user device, etc.
- a system of speakers may be located in each room (e.g., living room, dining room, bedroom, etc.) of the house, and the speakers forming a system for a given room may be at various locations throughout the room.
- audio will be played out synchronously across all of the audio devices selected by the user. It should be understood, however, that the methods and systems described herein may be applicable to any system that requires time synchronization of any data type between different modules on a network, and thus the scope of the present disclosure is not in any way limited by the example application described above.
- FIG. 1 is an example content management system 100 in which one or more embodiments described herein may be implemented.
- Data Source 110 e.g., a content source such as an audio source (e.g., an online streaming music or video service, a particular URL, etc.)
- Data Module 115 may be connected to Data Module 115 over a Network 105 (e.g., any kind of network including, for example, Ethernet, wireless LAN, cellular network, etc.).
- Network 105 e.g., any kind of network including, for example, Ethernet, wireless LAN, cellular network, etc.
- Content obtained from Data Source 110 may be played out by Data Module 115 and/or transported by Data Module 115 to one or more of Data Modules 120 a - 120 n (where “n” is an arbitrary number) over Network 125 (e.g., a wireless LAN or Ethernet).
- Network 125 e.g., a wireless LAN or Ethernet
- content obtained at Data Modules 120 a - 120 n may be played out by Data Modules 120 a - 120 n and/or transported over Network 135 to corresponding Data Modules 130 a - 130 m , Data Modules 140 a - 140 p , or some combination thereof (where “m” and “p” are both arbitrary numbers).
- Networks 125 and 135 may be the same or different networks (e.g., different WLANs within a house, one wireless network and the other a wired network, etc.).
- Control Client 150 may be in communication with Data Module 115 over Network 105 .
- Control Client 150 may act as a data source (e.g., Data Source 110 ) by mirroring local data from the Control Client to Data Module 115 .
- the data modules (e.g., Data Module 115 , Data Modules 120 a - 120 n , and Data Modules 130 a - 130 m ) in the content management system 100 may be divided into groups of data modules. Each group of data modules may be divided into one or more systems, which, in turn, may include one or more individual data modules. In accordance with at least one embodiment, group and system configurations may be set by the user.
- Data modules within a group may operate in accordance with different roles.
- data modules within a group may be divided into Player Modules, Follower Modules, and Renderer Modules (sometimes referred to herein simply as “Players,” “Followers,” and “Renderers,” respectively).
- Player Modules sometimes referred to herein simply as “Players,” “Followers,” and “Renderers,” respectively.
- Example features and functionalities of the Players, Followers, and Renderers will be described in greater detail below.
- the methods and systems of the present disclosure allow for multiple configurations and Player/Follower/Renderer combinations, and further allow such configurations and/or combinations to be modified on-the-fly (e.g., adaptable or adjustable by the user and/or system while the system is in operation).
- the resulting configuration (Player/Follower/Renderer) is determined based on the grouping, audio source/type, network conditions, etc.
- the Player acts as “master” or a “leader” of a group of data modules (e.g., Data Module 115 may be the Player in the example group comprising Data Module 115 , Data Modules 120 a - 120 n , and Data Modules 130 a - 130 m in the example content management system 100 shown in FIG. 1 ).
- the Player may fetch (e.g., retrieve, obtain, etc.) the data (e.g., audio) from the source (e.g., Data Source 110 ) and forward the data out to the other data modules (e.g., loudspeakers) in the group.
- the data e.g., audio
- the source e.g., Data Source 110
- the other data modules e.g., loudspeakers
- the source of the data obtained by the Player may be, for example, an online audio/video streaming service or website, a portable user device (e.g., cellular telephone, smartphone, personal digital assistant, tablet computer, laptop computer, smart television, etc.), a storage device containing memory for storing audio/video data (e.g., a standalone hard drive), and the like.
- the Player may also be configured to packetize the data obtained from the source and send raw or coded data packets over the network to the data modules in the group.
- the Player may be configured to determine whether to send raw or coded data packets to the other data modules in the group based on available bandwidth of the network and/or the capabilities of each particular data module (e.g., each Bearer or Renderer's capabilities).
- one or more devices e.g., loudspeakers
- the Player may instead send coded data to the other modules.
- the system may be configured to re-encode the data following the initial decoding by the Player.
- the data originally received by the Player is not necessarily coded in all instances (and thus there may not be an initial decoding performed by the Player).
- the Player may also act as a centralized processor in detecting, processing, and responding to speech commands (e.g., generated by a user). For example, as will be described in greater detail below with respect to the example arrangements illustrated in FIGS.
- the Player may be configured to receive (e.g., retrieve, collect, or otherwise obtain) “Hot Word” command scores from each of the other data modules in the group, determine the data modules with the highest scores, activate or cause to activate the microphones on the data modules with the highest scores, receive audio data containing a speech command of the user from the data modules with the activated microphones, and combine the received audio data into a request that is sent to an external server for processing (e.g., interpretation).
- “Hot Word” command scores e.g., retrieve, collect, or otherwise obtain
- the Player may receive from the server a response containing a requested action corresponding to the speech command, which the Player may then fan out (e.g., distribute) to the other data modules in the group so that the requested action is performed.
- the response received at the Player from the server may also include audio data corresponding to the requested action, which the Player may also fan out to the other data modules in the group.
- Such audio data may, for example, be played out by each of the data modules in the group as an audible confirmation to the user that the user's command was received and is being acted on.
- a Player may also be a follower and/or a Renderer, depending on the particulars of the group configuration.
- the follower is the head of a local system of data modules (e.g., Data Modules 120 a - 120 n may be Followers in different systems of data modules made up of certain Data Modules 130 a - 130 m in the example content management system 100 shown in FIG. 1 ).
- the followers may receive data from a Player and fan (e.g., forward) out the data to the connected Renderers in their respective systems.
- the Follower may receive over the network raw or coded data packets from the Player and send the data packets to the Renderers in the system.
- the follower may send the packets to the Renderers in the same format as the packets are received from the Player, or the Follower may parse the packets and perform various operations (e.g., transcoding, audio processing, etc.) on the received data before re-packeting the data for sending to the connected Renderers. It should be noted that a Follower may also be a Renderer.
- the Renderer is the endpoint of the data pipeline in the content management system (e.g., Data Modules 130 a - 130 m in the example content management system 100 shown in FIG. 1 ).
- the Renderer may be configured to playout the data received from the Follower that heads its respective system.
- the Renderer may perform additional local processing (e.g., fade in/fade out in the context of audio) on the data received from the Follower prior to playing out the data.
- one or more of the data modules in the content management system may be in communication with and/or receive control commands from a control client connected to the network (e.g., Control Client 150 may be in communication with Data Module 115 over Network 105 in the example content management system 100 shown in FIG. 1 ).
- the control client is not a physical data module (e.g., not a physical loudspeaker), but instead may be a device (e.g., cellular telephone, smartphone, personal digital assistant, tablet computer, laptop computer, smart television, etc.) that can control and send messages to the Player.
- the control client may be used to relay various control messages (e.g., play, pause, stop, volume updates, etc.) to the Player.
- control client may also act as a data source (e.g., Data Source 110 ) for the content management system, for example, by mirroring local data from the control client to the Player.
- Data Source 110 e.g., Data Source 110
- the control client may use the same communication protocol as the data modules in the content management system.
- platform, architecture, and system of the present disclosure are extremely dynamic.
- a user of the system and/or the system itself may modify the unique roles of the data modules, the specific data modules targeted for playout, the grouping of data modules, the designation of an “active” group of data modules, or some combination thereof while the system is in active operation.
- the selection of a group leader may be performed using a system in which each data module advertises its capabilities to a common system service, which then determines roles for each of the modules, including the election of the group leader, based on the advertised capabilities.
- the leader selection process may be based on a unique score computed (e.g., by the common system service) for each of the data modules (e.g., loudspeakers).
- this score may be computed based on one or more of the following non-limiting parameters: (i) CPU capabilities; (ii) codec availability (e.g., a select or limited number of codecs may be implemented in particular data modules); and (iii) bandwidth/latency.
- FIG. 2 illustrates an example process 200 for detecting, processing, and responding to speech signals (e.g., speech commands) from multiple end points.
- speech signals e.g., speech commands
- one or more of blocks 205 - 240 in the example process 200 may be performed by one or more of the components in the example content management system shown in FIG. 1 , and described in detail above.
- one or more of Control Client 150 , Data Module 115 , Data Modules 120 a - 120 n , and Data Modules 130 a - 130 m in the example content management system 100 may be configured to perform one or more of the operations associated with blocks 205 - 240 in the example process 200 for detecting, processing, and responding to speech commands, further details of which are provided below.
- the example process 200 for detecting, processing, and responding to speech commands may be performed without one or more of blocks 205 - 240 , and/or performed with one or more of blocks 205 - 240 being combined together.
- a Hot Word command (which may sometimes be referred to herein as an “activation command,” “initialization command,” or the like) may be generated (e.g., by a user) during audio playback by data modules in a group of data modules (e.g., a group of data modules comprising Data Module 115 , Data Modules 120 a - 120 n , and Data Modules 130 a - 130 m in the example content management system 100 shown in FIG. 1 ).
- a Hot Word command (which may sometimes be referred to herein as an “activation command,” “initialization command,” or the like) may be generated (e.g., by a user) during audio playback by data modules in a group of data modules (e.g., a group of data modules comprising Data Module 115 , Data Modules 120 a - 120 n , and Data Modules 130 a - 130 m in the example content management system 100 shown in FIG. 1 ).
- the data modules in the group that detect the generated Hot Word command may determine (e.g., compute, calculate, etc.) a score for the detected command (a “Hot Word” score).
- the Hot Word score that may be determined by each of the data modules may be based on, for example, one or more of the following non-exhaustive and non-limiting factors: (i) power of the signal (e.g., the power of the signal received at the data module for the speech command may be compared to the power of the signal received prior to the speech command); (ii) score of a Hot Word recognizer/detector module (the details of which are described above); (iii) location of the user relative to the data module. For example, by using the localizer of a beamformer, the angle of the sound source may be obtained.
- power of the signal e.g., the power of the signal received at the data module for the speech command may be compared to the power of the signal received prior to the speech command
- score of a Hot Word recognizer/detector module the details of which are described above
- location of the user relative to the data module For example, by using the localizer of a beamformer, the angle of the sound source may be obtained.
- angles provided by different data modules may be triangulated to estimate the position of the user (this is based on the assumption that the positions of the data modules are known); and (iv) additional processing performed on the audio (e.g., combining all microphone array outputs using a beamformer, applying noise suppression/cancellation, gain control, echo suppression/cancellation, etc.).
- each of the data modules in the group may send its computed “Hot Word” score to a group leader data module (e.g., a Player Module, as described above).
- the group leader data module may act as a centralized processor of sorts in that the group leader collects (e.g., receives) the computed Hot Word scores from the other data modules in the group.
- the group leader data module may pause or mute audio playback by the other data modules in the group and determine (e.g., identify), based on the computed Hot Word scores received from the data modules at block 215 , those data modules having the highest computed Hot Word scores for the Hot Word command generated at block 205 .
- the group leader data module may utilize the received Hot Word scores (at block 215 ) to rank or order the data modules in the group according to their corresponding scores.
- the group leader data module may then determine the data modules that have one of the top (e.g., two, three, etc.) scores for the Hot Word command generated at block 205 .
- the group leader data module may determine the data modules that have Hot Word scores higher than the scores of some threshold number of the detecting data modules.
- the group leader data module may activate microphone(s) at the data module(s) in the group determined to have the highest computed scores for the Hot Word command.
- the data modules with activated microphones may record a generated command/request (e.g., a command/request generated by the user) and send audio data containing the recorded command/request to the group leader data module.
- a generated command/request e.g., a command/request generated by the user
- the group leader data module may generate a request based on the audio data containing the recorded command/request received from the data modules with activated microphones (at block 230 ), and send the generated request to an external server for processing (e.g., interpretation).
- the group leader data module may generate the request sent to the external server by combining the audio data received from the data modules.
- the external server may be a back-end server (e.g., Voice Search Back-End 660 or 760 as shown in the example component and data flows in FIGS. 6 and 7 , respectively) that receives the request from the group leader and is configured to interpret the combined audio data (e.g., the audio data containing the recorded command/request from, for example, the user).
- the group leader data module may receive from the external (e.g., back-end) server a response to the request sent by the group leader data module (e.g., at block 235 ).
- the group leader data module may process the received response and take appropriate control action based on the response, and/or the group leader module may distribute (e.g., fan out, transmit, etc.) the response to the other data modules in the group so that the requested action is performed.
- the response received at the group leader data module at block 240 may contain a requested action corresponding to the generated command/request (e.g., speech command) recorded by the data modules with activated microphones (at block 230 ).
- the response received at the group leader data module from the server may also include audio data corresponding to the requested action, which the group leader data module may also fan out to the other data modules in the group.
- audio data may, for example, be played out by each of the data modules in the group as an audible confirmation to the user that the user's command was received and is being acted on.
- each data module in the group of data modules may determine (e.g., calculate, compute, etc.) its own Hot Word score and broadcast its score to the other data modules in the group.
- a data module in the group determines, based on the broadcasted scores, that the data module has one of the best (e.g., highest quality) signals, then the data module may send/upload its recorded audio data (e.g., speech data relating to a command from the user) to the external server for processing/interpretation (e.g., to Voice Search Back-End 660 or 760 as shown in the example component and data flows in FIGS. 6 and 7 , respectively, further details of which are provided below).
- the data module may then broadcast its confidence level of the response and wait for similar broadcasts from other data modules in the group. If the data module determines that it has one of the highest confidence levels for the response, the data module may act on the response accordingly (e.g., perform a requested action contained in the response received from the server).
- the data module when a data module detects a Hot Word, the data module may generate a score for the detected Hot Word, broadcast the score to the other data modules in the group (e.g., an Ethernet broadcast), and wait for some period of time (which may be, for example, a predetermined period of time, a period of time based on a setting that may or may not be adjustable, or the like) to receive similar broadcasts from other data modules. After the designated period of time has passed, the data module has access to the scores generated by the other data modules in the group that have also detected the Hot Word.
- a score for the detected Hot Word broadcast the score to the other data modules in the group (e.g., an Ethernet broadcast), and wait for some period of time (which may be, for example, a predetermined period of time, a period of time based on a setting that may or may not be adjustable, or the like) to receive similar broadcasts from other data modules.
- some period of time which may be, for example, a predetermined period of time, a period of time
- the data module (as well as each of the other detecting data modules in the group) can then determine (e.g., rank) how well it scored with respect to the other detecting data modules. For example, if the data module determines that it has one of the top (e.g., two, three, etc.) scores for the Hot Word, the data module can decide to take action (e.g., send/upload its recorded audio data (e.g., speech data relating to a command from the user) to the external server for processing/interpretation).
- the data module can decide to take action (e.g., send/upload its recorded audio data (e.g., speech data relating to a command from the user) to the external server for processing/interpretation).
- the system of the present disclosure may also be capable of performing partial processing of speech commands by utilizing portions of audio data received from multiple data modules.
- the system may capture each part of a sentence spoken by the user from the “best” loudspeaker for that particular part.
- Such partial processing may be applicable, for example, when a user speaks a command while moving around within a room.
- a per-segment-score may be created for each data module and each word processed independently. It should be noted that because the clocks of the data modules in a given group are synchronized, the system is able to compare signal-to-noise ratio (SNR) values between speech segments.
- SNR signal-to-noise ratio
- FIGS. 3-7 illustrate example components and data flows for various operations that may be performed by the content management system 100 shown in FIG. 1 (and described in detail above).
- a Player data module (or “Group Leader Module”) may also act as a centralized processor in detecting, processing, and responding to speech commands (e.g., generated by a user).
- the Player e.g., 315 , 415 , 515 , 615 , and 715 in the example arrangements shown in FIGS.
- 3-7 may be configured to receive “Hot Word” command scores from each of the other data modules in the group (e.g., data modules 320 a - 320 n , 420 a - 420 n , 520 a - 520 n , 620 a - 620 n , and 720 a - 720 n in the example arrangements shown in FIGS.
- data modules 320 a - 320 n e.g., data modules 320 a - 320 n , 420 a - 420 n , 520 a - 520 n , 620 a - 620 n , and 720 a - 720 n in the example arrangements shown in FIGS.
- the Player in response to sending the audio data containing the user's speech command, the Player (e.g., 715 in FIG. 7 ) may receive from the server ( 760 ) a response containing a requested action corresponding to the speech command, which the Player may then fan out (e.g., distribute) to the other data modules in the group ( 720 a - 720 n ) so that the requested action is performed.
- the response received at the Player from the server may also include audio data corresponding to the requested action, which the Player may also fan out to the other data modules in the group.
- Such audio data may, for example, be played out by each of the data modules in the group as an audible confirmation to the user that the user's command was received and is being acted on.
- FIG. 8 is a high-level block diagram of an exemplary computer ( 800 ) that is arranged for detecting, processing, and responding to speech commands in a multi-device content management system in accordance with one or more embodiments described herein.
- the computing device ( 800 ) typically includes one or more processors ( 810 ) and system memory ( 820 ).
- a memory bus ( 830 ) can be used for communicating between the processor ( 810 ) and the system memory ( 820 ).
- the processor ( 810 ) can be of any type including but not limited to a microprocessor ( ⁇ P), a microcontroller ( ⁇ C), a digital signal processor (DSP), or any combination thereof.
- the processor ( 810 ) can include one more levels of caching, such as a level one cache ( 811 ) and a level two cache ( 812 ), a processor core ( 813 ), and registers ( 814 ).
- the processor core ( 813 ) can include an arithmetic logic unit (ALU), a floating point unit (FPU), a digital signal processing core (DSP Core), or any combination thereof.
- a memory controller ( 815 ) can also be used with the processor ( 810 ), or in some implementations the memory controller ( 815 ) can be an internal part of the processor ( 810 ).
- system memory ( 820 ) can be of any type including but not limited to volatile memory (such as RAM), non-volatile memory (such as ROM, flash memory, etc.) or any combination thereof.
- System memory ( 820 ) typically includes an operating system ( 821 ), one or more applications ( 822 ), and program data ( 824 ).
- the application ( 822 ) may include a system for detecting and processing speech commands ( 823 ).
- the system for detecting and processing speech commands ( 823 ) is further designed to perform partial processing of speech commands by utilizing portions of audio data received from multiple data modules in a content management system (e.g., Data Module 115 , Data Modules 120 a - 120 n , and/or Data Modules 130 a - 130 m in the example content management system 100 shown in FIG. 1 and described in detail above).
- a content management system e.g., Data Module 115 , Data Modules 120 a - 120 n , and/or Data Modules 130 a - 130 m in the example content management system 100 shown in FIG. 1 and described in detail above.
- Program Data ( 824 ) may include storing instructions that, when executed by the one or more processing devices, implement a system ( 823 ) and method for detecting and processing speech commands using multiple data modules operating on a network. Additionally, in accordance with at least one embodiment, program data ( 824 ) may include network, Hot Words, and module data ( 825 ), which may relate to various statistics routinely collected from the local network on which the system ( 823 ) is operating, certain voice/speech commands that activate scoring an processing operations, as well as one or more characteristics of data modules included in a group of modules. In accordance with at least some embodiments, the application ( 822 ) can be arranged to operate with program data ( 824 ) on an operating system ( 821 ).
- the computing device ( 800 ) can have additional features or functionality, and additional interfaces to facilitate communications between the basic configuration ( 801 ) and any required devices and interfaces.
- System memory ( 820 ) is an example of computer storage media.
- Computer storage media includes, but is not limited to, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can be accessed by computing device 800 . Any such computer storage media can be part of the device ( 800 ).
- the computing device ( 800 ) can be implemented as a portion of a small-form factor portable (or mobile) electronic device such as a cell phone, a smartphone, a personal data assistant (PDA), a personal media player device, a tablet computer (tablet), a wireless web-watch device, a personal headset device, an application-specific device, or a hybrid device that include any of the above functions.
- a small-form factor portable (or mobile) electronic device such as a cell phone, a smartphone, a personal data assistant (PDA), a personal media player device, a tablet computer (tablet), a wireless web-watch device, a personal headset device, an application-specific device, or a hybrid device that include any of the above functions.
- PDA personal data assistant
- the computing device ( 800 ) can also be implemented as a personal computer including both laptop computer and non-laptop computer configurations.
- non-transitory signal bearing medium examples include, but are not limited to, the following: a recordable type medium such as a floppy disk, a hard disk drive, a Compact Disc (CD), a Digital Video Disk (DVD), a digital tape, a computer memory, etc.; and a transmission type medium such as a digital and/or an analog communication medium (e.g., a fiber optic cable, a waveguide, a wired communications link, a wireless communication link, etc.).
- the users may be provided with an opportunity to control whether programs or features associated with the systems and/or methods collect user information (e.g., information about a user's preferences).
- user information e.g., information about a user's preferences
- certain data may be treated in one or more ways before it is stored or used, so that personally identifiable information is removed.
- a user's identity may be treated so that no personally identifiable information can be determined for the user.
- the user may have control over how information is collected about the user and used by a server.
Abstract
Description
Claims (20)
Priority Applications (8)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/052,426 US9779735B2 (en) | 2016-02-24 | 2016-02-24 | Methods and systems for detecting and processing speech signals |
US15/597,249 US10163442B2 (en) | 2016-02-24 | 2017-05-17 | Methods and systems for detecting and processing speech signals |
US15/622,170 US10249303B2 (en) | 2016-02-24 | 2017-06-14 | Methods and systems for detecting and processing speech signals |
US15/625,685 US10255920B2 (en) | 2016-02-24 | 2017-06-16 | Methods and systems for detecting and processing speech signals |
US15/624,935 US10163443B2 (en) | 2016-02-24 | 2017-06-16 | Methods and systems for detecting and processing speech signals |
US16/280,642 US10878820B2 (en) | 2016-02-24 | 2019-02-20 | Methods and systems for detecting and processing speech signals |
US17/114,621 US11568874B2 (en) | 2016-02-24 | 2020-12-08 | Methods and systems for detecting and processing speech signals |
US18/159,076 US20230169979A1 (en) | 2016-02-24 | 2023-01-24 | Methods And Systems For Detecting And Processing Speech Signals |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/052,426 US9779735B2 (en) | 2016-02-24 | 2016-02-24 | Methods and systems for detecting and processing speech signals |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/597,249 Continuation US10163442B2 (en) | 2016-02-24 | 2017-05-17 | Methods and systems for detecting and processing speech signals |
Publications (2)
Publication Number | Publication Date |
---|---|
US20170243586A1 US20170243586A1 (en) | 2017-08-24 |
US9779735B2 true US9779735B2 (en) | 2017-10-03 |
Family
ID=59630112
Family Applications (8)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/052,426 Active US9779735B2 (en) | 2016-02-24 | 2016-02-24 | Methods and systems for detecting and processing speech signals |
US15/597,249 Active US10163442B2 (en) | 2016-02-24 | 2017-05-17 | Methods and systems for detecting and processing speech signals |
US15/622,170 Active US10249303B2 (en) | 2016-02-24 | 2017-06-14 | Methods and systems for detecting and processing speech signals |
US15/624,935 Active US10163443B2 (en) | 2016-02-24 | 2017-06-16 | Methods and systems for detecting and processing speech signals |
US15/625,685 Active US10255920B2 (en) | 2016-02-24 | 2017-06-16 | Methods and systems for detecting and processing speech signals |
US16/280,642 Active 2036-06-11 US10878820B2 (en) | 2016-02-24 | 2019-02-20 | Methods and systems for detecting and processing speech signals |
US17/114,621 Active 2036-09-07 US11568874B2 (en) | 2016-02-24 | 2020-12-08 | Methods and systems for detecting and processing speech signals |
US18/159,076 Pending US20230169979A1 (en) | 2016-02-24 | 2023-01-24 | Methods And Systems For Detecting And Processing Speech Signals |
Family Applications After (7)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/597,249 Active US10163442B2 (en) | 2016-02-24 | 2017-05-17 | Methods and systems for detecting and processing speech signals |
US15/622,170 Active US10249303B2 (en) | 2016-02-24 | 2017-06-14 | Methods and systems for detecting and processing speech signals |
US15/624,935 Active US10163443B2 (en) | 2016-02-24 | 2017-06-16 | Methods and systems for detecting and processing speech signals |
US15/625,685 Active US10255920B2 (en) | 2016-02-24 | 2017-06-16 | Methods and systems for detecting and processing speech signals |
US16/280,642 Active 2036-06-11 US10878820B2 (en) | 2016-02-24 | 2019-02-20 | Methods and systems for detecting and processing speech signals |
US17/114,621 Active 2036-09-07 US11568874B2 (en) | 2016-02-24 | 2020-12-08 | Methods and systems for detecting and processing speech signals |
US18/159,076 Pending US20230169979A1 (en) | 2016-02-24 | 2023-01-24 | Methods And Systems For Detecting And Processing Speech Signals |
Country Status (1)
Country | Link |
---|---|
US (8) | US9779735B2 (en) |
Cited By (66)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20190251960A1 (en) * | 2018-02-13 | 2019-08-15 | Roku, Inc. | Trigger Word Detection With Multiple Digital Assistants |
US10777197B2 (en) | 2017-08-28 | 2020-09-15 | Roku, Inc. | Audio responsive device with play/stop and tell me something buttons |
US10878811B2 (en) | 2018-09-14 | 2020-12-29 | Sonos, Inc. | Networked devices, systems, and methods for intelligently deactivating wake-word engines |
US10970035B2 (en) | 2016-02-22 | 2021-04-06 | Sonos, Inc. | Audio response playback |
US10971139B2 (en) | 2016-02-22 | 2021-04-06 | Sonos, Inc. | Voice control of a media playback system |
US11006214B2 (en) | 2016-02-22 | 2021-05-11 | Sonos, Inc. | Default playback device designation |
US11024331B2 (en) | 2018-09-21 | 2021-06-01 | Sonos, Inc. | Voice detection optimization using sound metadata |
US11062710B2 (en) | 2017-08-28 | 2021-07-13 | Roku, Inc. | Local and cloud speech recognition |
US11062702B2 (en) | 2017-08-28 | 2021-07-13 | Roku, Inc. | Media system with multiple digital assistants |
US11080005B2 (en) | 2017-09-08 | 2021-08-03 | Sonos, Inc. | Dynamic computation of system response volume |
US11100923B2 (en) | 2018-09-28 | 2021-08-24 | Sonos, Inc. | Systems and methods for selective wake word detection using neural network models |
US11126389B2 (en) | 2017-07-11 | 2021-09-21 | Roku, Inc. | Controlling visual indicators in an audio responsive electronic device, and capturing and providing audio using an API, by native and non-native computing devices and services |
US11132989B2 (en) | 2018-12-13 | 2021-09-28 | Sonos, Inc. | Networked microphone devices, systems, and methods of localized arbitration |
US11175888B2 (en) | 2017-09-29 | 2021-11-16 | Sonos, Inc. | Media playback system with concurrent voice assistance |
US11175880B2 (en) | 2018-05-10 | 2021-11-16 | Sonos, Inc. | Systems and methods for voice-assisted media content selection |
US11183183B2 (en) | 2018-12-07 | 2021-11-23 | Sonos, Inc. | Systems and methods of operating media playback systems having multiple voice assistant services |
US11189286B2 (en) | 2019-10-22 | 2021-11-30 | Sonos, Inc. | VAS toggle based on device orientation |
US11200889B2 (en) | 2018-11-15 | 2021-12-14 | Sonos, Inc. | Dilated convolutions and gating for efficient keyword spotting |
US11200894B2 (en) | 2019-06-12 | 2021-12-14 | Sonos, Inc. | Network microphone device with command keyword eventing |
US11200900B2 (en) | 2019-12-20 | 2021-12-14 | Sonos, Inc. | Offline voice control |
US11302326B2 (en) | 2017-09-28 | 2022-04-12 | Sonos, Inc. | Tone interference cancellation |
US11308961B2 (en) | 2016-10-19 | 2022-04-19 | Sonos, Inc. | Arbitration-based voice recognition |
US11308958B2 (en) | 2020-02-07 | 2022-04-19 | Sonos, Inc. | Localized wakeword verification |
US11308962B2 (en) | 2020-05-20 | 2022-04-19 | Sonos, Inc. | Input detection windowing |
US11315556B2 (en) | 2019-02-08 | 2022-04-26 | Sonos, Inc. | Devices, systems, and methods for distributed voice processing by transmitting sound data associated with a wake word to an appropriate device for identification |
US11343614B2 (en) | 2018-01-31 | 2022-05-24 | Sonos, Inc. | Device designation of playback and network microphone device arrangements |
US11354092B2 (en) | 2019-07-31 | 2022-06-07 | Sonos, Inc. | Noise classification for event detection |
US11361756B2 (en) | 2019-06-12 | 2022-06-14 | Sonos, Inc. | Conditional wake word eventing based on environment |
US11380322B2 (en) | 2017-08-07 | 2022-07-05 | Sonos, Inc. | Wake-word detection suppression |
US11405430B2 (en) | 2016-02-22 | 2022-08-02 | Sonos, Inc. | Networked microphone device control |
US11432030B2 (en) | 2018-09-14 | 2022-08-30 | Sonos, Inc. | Networked devices, systems, and methods for associating playback devices based on sound codes |
US11451908B2 (en) | 2017-12-10 | 2022-09-20 | Sonos, Inc. | Network microphone devices with automatic do not disturb actuation capabilities |
US11482978B2 (en) | 2018-08-28 | 2022-10-25 | Sonos, Inc. | Audio notifications |
US11482224B2 (en) | 2020-05-20 | 2022-10-25 | Sonos, Inc. | Command keywords with input detection windowing |
US11501795B2 (en) | 2018-09-29 | 2022-11-15 | Sonos, Inc. | Linear filtering for noise-suppressed speech detection via multiple network microphone devices |
US11501773B2 (en) | 2019-06-12 | 2022-11-15 | Sonos, Inc. | Network microphone device with command keyword conditioning |
US20220366906A1 (en) * | 2019-07-15 | 2022-11-17 | Huawei Technologies Co., Ltd. | Voice wake-up method and electronic device |
US11516610B2 (en) | 2016-09-30 | 2022-11-29 | Sonos, Inc. | Orientation-based playback device microphone selection |
US11531520B2 (en) | 2016-08-05 | 2022-12-20 | Sonos, Inc. | Playback device supporting concurrent voice assistants |
US11538451B2 (en) | 2017-09-28 | 2022-12-27 | Sonos, Inc. | Multi-channel acoustic echo cancellation |
US11540047B2 (en) | 2018-12-20 | 2022-12-27 | Sonos, Inc. | Optimization of network microphone devices using noise classification |
US11545169B2 (en) | 2016-06-09 | 2023-01-03 | Sonos, Inc. | Dynamic player selection for audio signal processing |
US11551700B2 (en) | 2021-01-25 | 2023-01-10 | Sonos, Inc. | Systems and methods for power-efficient keyword detection |
US11551669B2 (en) | 2019-07-31 | 2023-01-10 | Sonos, Inc. | Locally distributed keyword detection |
US11556307B2 (en) | 2020-01-31 | 2023-01-17 | Sonos, Inc. | Local voice data processing |
US11556306B2 (en) | 2016-02-22 | 2023-01-17 | Sonos, Inc. | Voice controlled media playback system |
US11562740B2 (en) | 2020-01-07 | 2023-01-24 | Sonos, Inc. | Voice verification for media playback |
US11563842B2 (en) | 2018-08-28 | 2023-01-24 | Sonos, Inc. | Do not disturb feature for audio notifications |
US11568874B2 (en) * | 2016-02-24 | 2023-01-31 | Google Llc | Methods and systems for detecting and processing speech signals |
US11641559B2 (en) | 2016-09-27 | 2023-05-02 | Sonos, Inc. | Audio playback settings for voice interaction |
US11646023B2 (en) | 2019-02-08 | 2023-05-09 | Sonos, Inc. | Devices, systems, and methods for distributed voice processing |
US11646045B2 (en) | 2017-09-27 | 2023-05-09 | Sonos, Inc. | Robust short-time fourier transform acoustic echo cancellation during audio playback |
US11664023B2 (en) | 2016-07-15 | 2023-05-30 | Sonos, Inc. | Voice detection by multiple devices |
US11676590B2 (en) | 2017-12-11 | 2023-06-13 | Sonos, Inc. | Home graph |
US11696074B2 (en) | 2018-06-28 | 2023-07-04 | Sonos, Inc. | Systems and methods for associating playback devices with voice assistant services |
US11698771B2 (en) | 2020-08-25 | 2023-07-11 | Sonos, Inc. | Vocal guidance engines for playback devices |
US11710487B2 (en) | 2019-07-31 | 2023-07-25 | Sonos, Inc. | Locally distributed keyword detection |
US11715489B2 (en) | 2018-05-18 | 2023-08-01 | Sonos, Inc. | Linear filtering for noise-suppressed speech detection |
US11727936B2 (en) | 2018-09-25 | 2023-08-15 | Sonos, Inc. | Voice detection optimization based on selected voice assistant service |
US11727919B2 (en) | 2020-05-20 | 2023-08-15 | Sonos, Inc. | Memory allocation for keyword spotting engines |
US11726742B2 (en) | 2016-02-22 | 2023-08-15 | Sonos, Inc. | Handling of loss of pairing between networked devices |
US11792590B2 (en) | 2018-05-25 | 2023-10-17 | Sonos, Inc. | Determining and adapting to changes in microphone performance of playback devices |
US11798553B2 (en) | 2019-05-03 | 2023-10-24 | Sonos, Inc. | Voice assistant persistence across multiple network microphone devices |
US11899519B2 (en) * | 2018-10-23 | 2024-02-13 | Sonos, Inc. | Multiple stage network microphone device with reduced power consumption and processing load |
US11979960B2 (en) | 2016-07-15 | 2024-05-07 | Sonos, Inc. | Contextualization of voice inputs |
US11984123B2 (en) | 2021-11-11 | 2024-05-14 | Sonos, Inc. | Network device interaction by range |
Families Citing this family (72)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9318108B2 (en) | 2010-01-18 | 2016-04-19 | Apple Inc. | Intelligent automated assistant |
US8977255B2 (en) | 2007-04-03 | 2015-03-10 | Apple Inc. | Method and system for operating a multi-function portable electronic device using voice-activation |
US8676904B2 (en) | 2008-10-02 | 2014-03-18 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
US10706373B2 (en) | 2011-06-03 | 2020-07-07 | Apple Inc. | Performing actions associated with task items that represent tasks to perform |
US10417037B2 (en) | 2012-05-15 | 2019-09-17 | Apple Inc. | Systems and methods for integrating third party services with a digital assistant |
CN104969289B (en) | 2013-02-07 | 2021-05-28 | 苹果公司 | Voice trigger of digital assistant |
US10652394B2 (en) | 2013-03-14 | 2020-05-12 | Apple Inc. | System and method for processing voicemail |
US10748529B1 (en) | 2013-03-15 | 2020-08-18 | Apple Inc. | Voice activated device for use with a voice-based digital assistant |
US10176167B2 (en) | 2013-06-09 | 2019-01-08 | Apple Inc. | System and method for inferring user intent from speech inputs |
US9966065B2 (en) | 2014-05-30 | 2018-05-08 | Apple Inc. | Multi-command single utterance input method |
US10170123B2 (en) | 2014-05-30 | 2019-01-01 | Apple Inc. | Intelligent assistant for home automation |
US9715875B2 (en) | 2014-05-30 | 2017-07-25 | Apple Inc. | Reducing the need for manual start/end-pointing and trigger phrases |
US9338493B2 (en) | 2014-06-30 | 2016-05-10 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US9886953B2 (en) | 2015-03-08 | 2018-02-06 | Apple Inc. | Virtual assistant activation |
US10460227B2 (en) | 2015-05-15 | 2019-10-29 | Apple Inc. | Virtual assistant in a communication session |
US10200824B2 (en) | 2015-05-27 | 2019-02-05 | Apple Inc. | Systems and methods for proactively identifying and surfacing relevant content on a touch-sensitive device |
US20160378747A1 (en) | 2015-06-29 | 2016-12-29 | Apple Inc. | Virtual assistant for media playback |
US10747498B2 (en) | 2015-09-08 | 2020-08-18 | Apple Inc. | Zero latency digital assistant |
US10331312B2 (en) | 2015-09-08 | 2019-06-25 | Apple Inc. | Intelligent automated assistant in a media environment |
US10671428B2 (en) | 2015-09-08 | 2020-06-02 | Apple Inc. | Distributed personal assistant |
US10740384B2 (en) | 2015-09-08 | 2020-08-11 | Apple Inc. | Intelligent automated assistant for media search and playback |
US10691473B2 (en) | 2015-11-06 | 2020-06-23 | Apple Inc. | Intelligent automated assistant in a messaging environment |
US10956666B2 (en) | 2015-11-09 | 2021-03-23 | Apple Inc. | Unconventional virtual assistant interactions |
US10223066B2 (en) | 2015-12-23 | 2019-03-05 | Apple Inc. | Proactive assistance based on dialog communication between devices |
US10586535B2 (en) | 2016-06-10 | 2020-03-10 | Apple Inc. | Intelligent digital assistant in a multi-tasking environment |
DK201670540A1 (en) | 2016-06-11 | 2018-01-08 | Apple Inc | Application integration with a digital assistant |
DK179415B1 (en) | 2016-06-11 | 2018-06-14 | Apple Inc | Intelligent device arbitration and control |
US10091545B1 (en) * | 2016-06-27 | 2018-10-02 | Amazon Technologies, Inc. | Methods and systems for detecting audio output of associated device |
US10559309B2 (en) * | 2016-12-22 | 2020-02-11 | Google Llc | Collaborative voice controlled devices |
US11204787B2 (en) | 2017-01-09 | 2021-12-21 | Apple Inc. | Application integration with a digital assistant |
US11183181B2 (en) | 2017-03-27 | 2021-11-23 | Sonos, Inc. | Systems and methods of multiple voice services |
DK201770383A1 (en) | 2017-05-09 | 2018-12-14 | Apple Inc. | User interface for correcting recognition errors |
US10726832B2 (en) | 2017-05-11 | 2020-07-28 | Apple Inc. | Maintaining privacy of personal information |
DK180048B1 (en) | 2017-05-11 | 2020-02-04 | Apple Inc. | MAINTAINING THE DATA PROTECTION OF PERSONAL INFORMATION |
DK201770427A1 (en) | 2017-05-12 | 2018-12-20 | Apple Inc. | Low-latency intelligent automated assistant |
DK179496B1 (en) | 2017-05-12 | 2019-01-15 | Apple Inc. | USER-SPECIFIC Acoustic Models |
DK179745B1 (en) | 2017-05-12 | 2019-05-01 | Apple Inc. | SYNCHRONIZATION AND TASK DELEGATION OF A DIGITAL ASSISTANT |
US20180336892A1 (en) | 2017-05-16 | 2018-11-22 | Apple Inc. | Detecting a trigger of a digital assistant |
US10303715B2 (en) | 2017-05-16 | 2019-05-28 | Apple Inc. | Intelligent automated assistant for media exploration |
US10789949B2 (en) * | 2017-06-20 | 2020-09-29 | Bose Corporation | Audio device with wakeup word detection |
US10051366B1 (en) | 2017-09-28 | 2018-08-14 | Sonos, Inc. | Three-dimensional beam forming with a microphone array |
US10482878B2 (en) * | 2017-11-29 | 2019-11-19 | Nuance Communications, Inc. | System and method for speech enhancement in multisource environments |
CN107945797B (en) * | 2017-12-07 | 2021-12-31 | 携程旅游信息技术（上海）有限公司 | Monitoring system based on speech recognition |
WO2019112625A1 (en) * | 2017-12-08 | 2019-06-13 | Google Llc | Signal processing coordination among digital voice assistant computing devices |
US10818288B2 (en) | 2018-03-26 | 2020-10-27 | Apple Inc. | Natural assistant interaction |
US11145294B2 (en) | 2018-05-07 | 2021-10-12 | Apple Inc. | Intelligent automated assistant for delivering content from user experiences |
US10928918B2 (en) | 2018-05-07 | 2021-02-23 | Apple Inc. | Raise to speak |
DK180639B1 (en) | 2018-06-01 | 2021-11-04 | Apple Inc | DISABILITY OF ATTENTION-ATTENTIVE VIRTUAL ASSISTANT |
US10892996B2 (en) | 2018-06-01 | 2021-01-12 | Apple Inc. | Variable latency device coordination |
DK179822B1 (en) | 2018-06-01 | 2019-07-12 | Apple Inc. | Voice interaction at a primary device to access call functionality of a companion device |
US11462215B2 (en) | 2018-09-28 | 2022-10-04 | Apple Inc. | Multi-modal inputs for voice commands |
US11475898B2 (en) | 2018-10-26 | 2022-10-18 | Apple Inc. | Low-latency multi-speaker speech recognition |
US11348573B2 (en) | 2019-03-18 | 2022-05-31 | Apple Inc. | Multimodality in digital assistant systems |
US11475884B2 (en) | 2019-05-06 | 2022-10-18 | Apple Inc. | Reducing digital assistant latency when a language is incorrectly determined |
US11307752B2 (en) | 2019-05-06 | 2022-04-19 | Apple Inc. | User configurable task triggers |
DK201970509A1 (en) | 2019-05-06 | 2021-01-15 | Apple Inc | Spoken notifications |
US11423908B2 (en) | 2019-05-06 | 2022-08-23 | Apple Inc. | Interpreting spoken requests |
US11140099B2 (en) | 2019-05-21 | 2021-10-05 | Apple Inc. | Providing message response suggestions |
DK180129B1 (en) | 2019-05-31 | 2020-06-02 | Apple Inc. | User activity shortcut suggestions |
DK201970510A1 (en) | 2019-05-31 | 2021-02-11 | Apple Inc | Voice identification in digital assistant systems |
US11289073B2 (en) | 2019-05-31 | 2022-03-29 | Apple Inc. | Device text to speech |
US11496600B2 (en) | 2019-05-31 | 2022-11-08 | Apple Inc. | Remote execution of machine-learned models |
US11227599B2 (en) | 2019-06-01 | 2022-01-18 | Apple Inc. | Methods and user interfaces for voice-based control of electronic devices |
US11360641B2 (en) | 2019-06-01 | 2022-06-14 | Apple Inc. | Increasing the relevance of new available information |
US11380312B1 (en) * | 2019-06-20 | 2022-07-05 | Amazon Technologies, Inc. | Residual echo suppression for keyword detection |
US11488406B2 (en) | 2019-09-25 | 2022-11-01 | Apple Inc. | Text detection using global geometry estimators |
US11043220B1 (en) | 2020-05-11 | 2021-06-22 | Apple Inc. | Digital assistant hardware abstraction |
US11061543B1 (en) | 2020-05-11 | 2021-07-13 | Apple Inc. | Providing relevant data items based on context |
US11755276B2 (en) | 2020-05-12 | 2023-09-12 | Apple Inc. | Reducing description length based on confidence |
US11490204B2 (en) | 2020-07-20 | 2022-11-01 | Apple Inc. | Multi-device audio adjustment coordination |
US11438683B2 (en) | 2020-07-21 | 2022-09-06 | Apple Inc. | User identification using headphones |
US20230215459A1 (en) * | 2021-12-30 | 2023-07-06 | Comcast Cable Communication, Llc | Methods and systems for voice control |
Citations (84)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US4363102A (en) | 1981-03-27 | 1982-12-07 | Bell Telephone Laboratories, Incorporated | Speaker identification system using word recognition templates |
US5659665A (en) | 1994-12-08 | 1997-08-19 | Lucent Technologies Inc. | Method and apparatus for including speech recognition capabilities in a computer system |
WO1998040875A1 (en) | 1997-03-13 | 1998-09-17 | Telia Ab (Publ) | Speaker verification system |
US5897616A (en) | 1997-06-11 | 1999-04-27 | International Business Machines Corporation | Apparatus and methods for speaker verification/identification/classification employing non-acoustic and/or acoustic models and databases |
US5983186A (en) | 1995-08-21 | 1999-11-09 | Seiko Epson Corporation | Voice-activated interactive speech recognition device and method |
US6141644A (en) | 1998-09-04 | 2000-10-31 | Matsushita Electric Industrial Co., Ltd. | Speaker verification and speaker identification based on eigenvoices |
US20020049596A1 (en) | 2000-03-30 | 2002-04-25 | Bernd Burchard | Speech recognition apparatus and method |
US20020072905A1 (en) | 1999-04-12 | 2002-06-13 | White George M. | Distributed voice user interface |
US20020123890A1 (en) | 2000-06-30 | 2002-09-05 | Dieter Kopp | Telecommunications system, and terminal, and network, and detector system, and speech recognizer, and method |
US20020193991A1 (en) | 2001-06-13 | 2002-12-19 | Intel Corporation | Combining N-best lists from multiple speech recognizers |
US6567775B1 (en) | 2000-04-26 | 2003-05-20 | International Business Machines Corporation | Fusion of audio and video based speaker identification for multimedia information access |
US20030200090A1 (en) | 2002-04-17 | 2003-10-23 | Pioneer Corporation | Speech recognition apparatus, speech recognition method, and computer-readable recording medium in which speech recognition program is recorded |
US20030231746A1 (en) | 2002-06-14 | 2003-12-18 | Hunter Karla Rae | Teleconference speaker identification |
US6671672B1 (en) | 1999-03-30 | 2003-12-30 | Nuance Communications | Voice authentication system having cognitive recall mechanism for password verification |
US20040101112A1 (en) | 2002-11-26 | 2004-05-27 | Lite-On Technology Corporation | Voice identification method for cellular phone and cellular phone with voiceprint password |
US6744860B1 (en) | 1998-12-31 | 2004-06-01 | Bell Atlantic Network Services | Methods and apparatus for initiating a voice-dialing operation |
US6826159B1 (en) | 2000-05-24 | 2004-11-30 | Cisco Technology, Inc. | System and method for providing speaker identification in a conference call |
US20050165607A1 (en) | 2004-01-22 | 2005-07-28 | At&T Corp. | System and method to disambiguate and clarify user intention in a spoken dialog system |
US6931375B1 (en) | 1997-05-27 | 2005-08-16 | Sbc Properties, Lp | Speaker verification method |
US6973426B1 (en) | 2000-12-29 | 2005-12-06 | Cisco Technology, Inc. | Method and apparatus for performing speaker verification based on speaker independent recognition of commands |
US7016833B2 (en) | 2000-11-21 | 2006-03-21 | The Regents Of The University Of California | Speaker verification system using acoustic data and non-acoustic data |
US20060074656A1 (en) | 2004-08-20 | 2006-04-06 | Lambert Mathias | Discriminative training of document transcription system |
US20060085186A1 (en) | 2004-10-19 | 2006-04-20 | Ma Changxue C | Tailored speaker-independent voice recognition system |
US20060184370A1 (en) | 2005-02-15 | 2006-08-17 | Samsung Electronics Co., Ltd. | Spoken dialogue interface apparatus and method |
US20070100620A1 (en) | 2005-10-31 | 2007-05-03 | Hitachi, Ltd. | System, method and computer program product for verifying an identity using voiced to unvoiced classifiers |
US7222072B2 (en) | 2003-02-13 | 2007-05-22 | Sbc Properties, L.P. | Bio-phonetic multi-phrase speaker identity verification |
US20070198262A1 (en) | 2003-08-20 | 2007-08-23 | Mindlin Bernardo G | Topological voiceprints for speaker identification |
US20080252595A1 (en) | 2007-04-11 | 2008-10-16 | Marc Boillot | Method and Device for Virtual Navigation and Voice Processing |
US7571014B1 (en) | 2004-04-01 | 2009-08-04 | Sonos, Inc. | Method and apparatus for controlling multimedia players in a multi-zone system |
US20090258333A1 (en) | 2008-03-17 | 2009-10-15 | Kai Yu | Spoken language learning systems |
US20090292541A1 (en) | 2008-05-25 | 2009-11-26 | Nice Systems Ltd. | Methods and apparatus for enhancing speech analytics |
US20100070276A1 (en) | 2008-09-16 | 2010-03-18 | Nice Systems Ltd. | Method and apparatus for interaction or discourse analytics |
US20100110834A1 (en) | 2008-10-30 | 2010-05-06 | Kim Kyu-Hong | Apparatus and method of detecting target sound |
US7720012B1 (en) | 2004-07-09 | 2010-05-18 | Arrowhead Center, Inc. | Speaker identification in the presence of packet losses |
US20110026722A1 (en) | 2007-05-25 | 2011-02-03 | Zhinian Jing | Vibration Sensor and Acoustic Voice Activity Detection System (VADS) for use with Electronic Systems |
US20110054892A1 (en) | 2008-05-28 | 2011-03-03 | Koreapowervoice Co., Ltd. | System for detecting speech interval and recognizing continuous speech in a noisy environment through real-time recognition of call commands |
US7904297B2 (en) | 2005-05-31 | 2011-03-08 | Robert Bosch Gmbh | Dialogue management using scripts and combined confidence scores |
US20110060587A1 (en) | 2007-03-07 | 2011-03-10 | Phillips Michael S | Command and control utilizing ancillary information in a mobile voice-to-speech application |
US20110066429A1 (en) | 2007-07-10 | 2011-03-17 | Motorola, Inc. | Voice activity detector and a method of operation |
US20110184730A1 (en) | 2010-01-22 | 2011-07-28 | Google Inc. | Multi-dimensional disambiguation of voice commands |
US20110304648A1 (en) | 2010-06-15 | 2011-12-15 | Lg Electronics Inc. | Mobile terminal and method for operating the mobile terminal |
US8099288B2 (en) | 2007-02-12 | 2012-01-17 | Microsoft Corp. | Text-dependent speaker verification |
US20120084087A1 (en) | 2009-06-12 | 2012-04-05 | Huawei Technologies Co., Ltd. | Method, device, and system for speaker recognition |
US8194624B2 (en) | 2005-11-29 | 2012-06-05 | Samsung Electronics Co., Ltd. | Resource allocating method among mobile-stations in distribution communication network |
US8200488B2 (en) | 2002-12-13 | 2012-06-12 | Sony Deutschland Gmbh | Method for processing speech using absolute loudness |
US8209174B2 (en) | 2009-04-17 | 2012-06-26 | Saudi Arabian Oil Company | Speaker verification system |
US8214447B2 (en) | 2004-06-08 | 2012-07-03 | Bose Corporation | Managing an audio network |
US20120232896A1 (en) | 2010-12-24 | 2012-09-13 | Huawei Technologies Co., Ltd. | Method and an apparatus for voice activity detection |
US20120265528A1 (en) | 2009-06-05 | 2012-10-18 | Apple Inc. | Using Context Information To Facilitate Processing Of Commands In A Virtual Assistant |
US20120316471A1 (en) * | 2011-06-10 | 2012-12-13 | Aliphcom | Power management in a data-capable strapband |
US8340975B1 (en) * | 2011-10-04 | 2012-12-25 | Theodore Alfred Rosenberger | Interactive speech recognition device and system for hands-free building control |
US20130060571A1 (en) | 2011-09-02 | 2013-03-07 | Microsoft Corporation | Integrated local and cloud based speech recognition |
US20130110521A1 (en) * | 2011-11-01 | 2013-05-02 | Qualcomm Incorporated | Extraction and analysis of audio feature data |
US20130124207A1 (en) | 2011-11-15 | 2013-05-16 | Microsoft Corporation | Voice-controlled camera operations |
US20130132086A1 (en) | 2011-11-21 | 2013-05-23 | Robert Bosch Gmbh | Methods and systems for adapting grammars in hybrid speech recognition engines for enhancing local sr performance |
US20130183944A1 (en) | 2012-01-12 | 2013-07-18 | Sensory, Incorporated | Information Access and Device Control Using Mobile Phones and Audio in the Home Environment |
US8588949B2 (en) | 2003-07-28 | 2013-11-19 | Sonos, Inc. | Method and apparatus for adjusting volume levels in a multi-zone system |
WO2014008194A1 (en) | 2012-07-03 | 2014-01-09 | Google Inc. | Determining hotword suitability |
US20140012573A1 (en) | 2012-07-06 | 2014-01-09 | Chia-Yu Hung | Signal processing apparatus having voice activity detection unit and related signal processing methods |
US20140012578A1 (en) | 2012-07-04 | 2014-01-09 | Seiko Epson Corporation | Speech-recognition system, storage medium, and method of speech recognition |
US8670985B2 (en) | 2010-01-13 | 2014-03-11 | Apple Inc. | Devices and methods for identifying a prompt corresponding to a voice input in a sequence of prompts |
US20140088961A1 (en) | 2012-09-26 | 2014-03-27 | International Business Machines Corporation | Captioning Using Socially Derived Acoustic Profiles |
US8713119B2 (en) | 2008-10-02 | 2014-04-29 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
US8719009B2 (en) | 2009-02-20 | 2014-05-06 | Voicebox Technologies Corporation | System and method for processing multi-modal device interactions in a natural language voice services environment |
US8719018B2 (en) | 2010-10-25 | 2014-05-06 | Lockheed Martin Corporation | Biometric speaker identification |
US8717949B2 (en) | 2007-06-12 | 2014-05-06 | Microsoft Corporation | Active speaker identification |
US8768687B1 (en) | 2013-04-29 | 2014-07-01 | Google Inc. | Machine translation of indirect speech |
US8775191B1 (en) | 2013-11-13 | 2014-07-08 | Google Inc. | Efficient utterance-specific endpointer triggering for always-on hotwording |
US20140222430A1 (en) | 2008-10-17 | 2014-08-07 | Ashwin P. Rao | System and Method for Multimodal Utterance Detection |
US8805690B1 (en) | 2010-08-05 | 2014-08-12 | Google Inc. | Audio notifications |
US20140257821A1 (en) | 2013-03-07 | 2014-09-11 | Analog Devices Technology | System and method for processor wake-up based on sensor data |
US8838457B2 (en) | 2007-03-07 | 2014-09-16 | Vlingo Corporation | Using results of unstructured language model based speech recognition to control a system-level function of a mobile communications facility |
US20140278383A1 (en) | 2013-03-13 | 2014-09-18 | Kopin Corporation | Apparatuses and methods for multi-channel signal compression during desired voice activity detection |
US20140278435A1 (en) | 2013-03-12 | 2014-09-18 | Nuance Communications, Inc. | Methods and apparatus for detecting a voice command |
US8938394B1 (en) | 2014-01-09 | 2015-01-20 | Google Inc. | Audio triggers based on context |
WO2015025330A1 (en) | 2013-08-21 | 2015-02-26 | Kale Aaditya Kishore | A system to enable user to interact with an electronic processing device using voice of the user |
US20150061969A1 (en) * | 2013-08-30 | 2015-03-05 | Lg Electronics Inc. | Wearable glass-type terminal, system having the same and method of controlling the terminal |
US8996372B1 (en) | 2012-10-30 | 2015-03-31 | Amazon Technologies, Inc. | Using adaptation data with cloud-based speech recognition |
US20150154953A1 (en) | 2013-12-02 | 2015-06-04 | Spansion Llc | Generation of wake-up words |
US9129602B1 (en) * | 2012-12-14 | 2015-09-08 | Amazon Technologies, Inc. | Mimicking user speech patterns |
US20150262577A1 (en) | 2013-08-29 | 2015-09-17 | Panasonic Intellectual Property Corporation Of America | Speech recognition method and speech recognition apparatus |
US9142218B2 (en) | 2008-04-11 | 2015-09-22 | At&T Intellectual Property I, L.P. | System and method for detecting synthetic speaker verification |
US20160104483A1 (en) | 2014-10-09 | 2016-04-14 | Google Inc. | Hotword detection on multiple devices |
US20160260431A1 (en) | 2015-03-08 | 2016-09-08 | Apple Inc. | Competing devices responding to voice triggers |
Family Cites Families (24)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6718308B1 (en) * | 2000-02-22 | 2004-04-06 | Daniel L. Nolting | Media presentation system controlled by voice to text commands |
US7668718B2 (en) * | 2001-07-17 | 2010-02-23 | Custom Speech Usa, Inc. | Synchronized pattern recognition source data processed by manual or automatic means for creation of shared speaker-dependent speech user profile |
US8521529B2 (en) | 2004-10-18 | 2013-08-27 | Creative Technology Ltd | Method for segmenting audio signals |
US7640160B2 (en) * | 2005-08-05 | 2009-12-29 | Voicebox Technologies, Inc. | Systems and methods for responding to natural language speech utterance |
US20070133437A1 (en) * | 2005-12-13 | 2007-06-14 | Wengrovitz Michael S | System and methods for enabling applications of who-is-speaking (WIS) signals |
CN1996847B (en) | 2006-12-27 | 2010-05-19 | 中国科学院上海技术物理研究所 | Cooperative network based image and multi-media data communication and storage system |
US9502025B2 (en) * | 2009-11-10 | 2016-11-22 | Voicebox Technologies Corporation | System and method for providing a natural language content dedication service |
US9171541B2 (en) * | 2009-11-10 | 2015-10-27 | Voicebox Technologies Corporation | System and method for hybrid processing in a natural language voice services environment |
US9292487B1 (en) * | 2012-08-16 | 2016-03-22 | Amazon Technologies, Inc. | Discriminative language model pruning |
US8612211B1 (en) * | 2012-09-10 | 2013-12-17 | Google Inc. | Speech recognition and summarization |
US9842489B2 (en) * | 2013-02-14 | 2017-12-12 | Google Llc | Waking other devices for additional data |
US9202462B2 (en) * | 2013-09-30 | 2015-12-01 | Google Inc. | Key phrase detection |
US20150106091A1 (en) * | 2013-10-14 | 2015-04-16 | Spence Wetjen | Conference transcription system and method |
US8768712B1 (en) * | 2013-12-04 | 2014-07-01 | Google Inc. | Initiating actions based on partial hotwords |
US9589564B2 (en) * | 2014-02-05 | 2017-03-07 | Google Inc. | Multiple speech locale-specific hotword classifiers for selection of a speech locale |
WO2015118672A1 (en) * | 2014-02-07 | 2015-08-13 | 株式会社ナンシン | Double-lock caster |
US9646634B2 (en) * | 2014-09-30 | 2017-05-09 | Google Inc. | Low-rank hidden input layer for speech recognition neural network |
US9318107B1 (en) * | 2014-10-09 | 2016-04-19 | Google Inc. | Hotword detection on multiple devices |
US9812128B2 (en) | 2014-10-09 | 2017-11-07 | Google Inc. | Device leadership negotiation among voice interface devices |
US9842789B2 (en) * | 2015-05-11 | 2017-12-12 | Samsung Electro-Mechanics Co., Ltd. | Electronic component package and method of manufacturing the same |
US10404280B2 (en) * | 2015-11-19 | 2019-09-03 | Westhold Corporation | Error correction using cyclic code-based LDPC codes |
US9792907B2 (en) * | 2015-11-24 | 2017-10-17 | Intel IP Corporation | Low resource key phrase detection for wake on voice |
US9779735B2 (en) * | 2016-02-24 | 2017-10-03 | Google Inc. | Methods and systems for detecting and processing speech signals |
US9972320B2 (en) * | 2016-08-24 | 2018-05-15 | Google Llc | Hotword detection on multiple devices |
-
2016
- 2016-02-24 US US15/052,426 patent/US9779735B2/en active Active
-
2017
- 2017-05-17 US US15/597,249 patent/US10163442B2/en active Active
- 2017-06-14 US US15/622,170 patent/US10249303B2/en active Active
- 2017-06-16 US US15/624,935 patent/US10163443B2/en active Active
- 2017-06-16 US US15/625,685 patent/US10255920B2/en active Active
-
2019
- 2019-02-20 US US16/280,642 patent/US10878820B2/en active Active
-
2020
- 2020-12-08 US US17/114,621 patent/US11568874B2/en active Active
-
2023
- 2023-01-24 US US18/159,076 patent/US20230169979A1/en active Pending
Patent Citations (84)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US4363102A (en) | 1981-03-27 | 1982-12-07 | Bell Telephone Laboratories, Incorporated | Speaker identification system using word recognition templates |
US5659665A (en) | 1994-12-08 | 1997-08-19 | Lucent Technologies Inc. | Method and apparatus for including speech recognition capabilities in a computer system |
US5983186A (en) | 1995-08-21 | 1999-11-09 | Seiko Epson Corporation | Voice-activated interactive speech recognition device and method |
WO1998040875A1 (en) | 1997-03-13 | 1998-09-17 | Telia Ab (Publ) | Speaker verification system |
US6931375B1 (en) | 1997-05-27 | 2005-08-16 | Sbc Properties, Lp | Speaker verification method |
US5897616A (en) | 1997-06-11 | 1999-04-27 | International Business Machines Corporation | Apparatus and methods for speaker verification/identification/classification employing non-acoustic and/or acoustic models and databases |
US6141644A (en) | 1998-09-04 | 2000-10-31 | Matsushita Electric Industrial Co., Ltd. | Speaker verification and speaker identification based on eigenvoices |
US6744860B1 (en) | 1998-12-31 | 2004-06-01 | Bell Atlantic Network Services | Methods and apparatus for initiating a voice-dialing operation |
US6671672B1 (en) | 1999-03-30 | 2003-12-30 | Nuance Communications | Voice authentication system having cognitive recall mechanism for password verification |
US20020072905A1 (en) | 1999-04-12 | 2002-06-13 | White George M. | Distributed voice user interface |
US20020049596A1 (en) | 2000-03-30 | 2002-04-25 | Bernd Burchard | Speech recognition apparatus and method |
US6567775B1 (en) | 2000-04-26 | 2003-05-20 | International Business Machines Corporation | Fusion of audio and video based speaker identification for multimedia information access |
US6826159B1 (en) | 2000-05-24 | 2004-11-30 | Cisco Technology, Inc. | System and method for providing speaker identification in a conference call |
US20020123890A1 (en) | 2000-06-30 | 2002-09-05 | Dieter Kopp | Telecommunications system, and terminal, and network, and detector system, and speech recognizer, and method |
US7016833B2 (en) | 2000-11-21 | 2006-03-21 | The Regents Of The University Of California | Speaker verification system using acoustic data and non-acoustic data |
US6973426B1 (en) | 2000-12-29 | 2005-12-06 | Cisco Technology, Inc. | Method and apparatus for performing speaker verification based on speaker independent recognition of commands |
US20020193991A1 (en) | 2001-06-13 | 2002-12-19 | Intel Corporation | Combining N-best lists from multiple speech recognizers |
US20030200090A1 (en) | 2002-04-17 | 2003-10-23 | Pioneer Corporation | Speech recognition apparatus, speech recognition method, and computer-readable recording medium in which speech recognition program is recorded |
US20030231746A1 (en) | 2002-06-14 | 2003-12-18 | Hunter Karla Rae | Teleconference speaker identification |
US20040101112A1 (en) | 2002-11-26 | 2004-05-27 | Lite-On Technology Corporation | Voice identification method for cellular phone and cellular phone with voiceprint password |
US8200488B2 (en) | 2002-12-13 | 2012-06-12 | Sony Deutschland Gmbh | Method for processing speech using absolute loudness |
US7222072B2 (en) | 2003-02-13 | 2007-05-22 | Sbc Properties, L.P. | Bio-phonetic multi-phrase speaker identity verification |
US8588949B2 (en) | 2003-07-28 | 2013-11-19 | Sonos, Inc. | Method and apparatus for adjusting volume levels in a multi-zone system |
US20070198262A1 (en) | 2003-08-20 | 2007-08-23 | Mindlin Bernardo G | Topological voiceprints for speaker identification |
US20050165607A1 (en) | 2004-01-22 | 2005-07-28 | At&T Corp. | System and method to disambiguate and clarify user intention in a spoken dialog system |
US7571014B1 (en) | 2004-04-01 | 2009-08-04 | Sonos, Inc. | Method and apparatus for controlling multimedia players in a multi-zone system |
US8214447B2 (en) | 2004-06-08 | 2012-07-03 | Bose Corporation | Managing an audio network |
US7720012B1 (en) | 2004-07-09 | 2010-05-18 | Arrowhead Center, Inc. | Speaker identification in the presence of packet losses |
US20060074656A1 (en) | 2004-08-20 | 2006-04-06 | Lambert Mathias | Discriminative training of document transcription system |
US20060085186A1 (en) | 2004-10-19 | 2006-04-20 | Ma Changxue C | Tailored speaker-independent voice recognition system |
US20060184370A1 (en) | 2005-02-15 | 2006-08-17 | Samsung Electronics Co., Ltd. | Spoken dialogue interface apparatus and method |
US7904297B2 (en) | 2005-05-31 | 2011-03-08 | Robert Bosch Gmbh | Dialogue management using scripts and combined confidence scores |
US20070100620A1 (en) | 2005-10-31 | 2007-05-03 | Hitachi, Ltd. | System, method and computer program product for verifying an identity using voiced to unvoiced classifiers |
US8194624B2 (en) | 2005-11-29 | 2012-06-05 | Samsung Electronics Co., Ltd. | Resource allocating method among mobile-stations in distribution communication network |
US8099288B2 (en) | 2007-02-12 | 2012-01-17 | Microsoft Corp. | Text-dependent speaker verification |
US8838457B2 (en) | 2007-03-07 | 2014-09-16 | Vlingo Corporation | Using results of unstructured language model based speech recognition to control a system-level function of a mobile communications facility |
US20110060587A1 (en) | 2007-03-07 | 2011-03-10 | Phillips Michael S | Command and control utilizing ancillary information in a mobile voice-to-speech application |
US20080252595A1 (en) | 2007-04-11 | 2008-10-16 | Marc Boillot | Method and Device for Virtual Navigation and Voice Processing |
US20110026722A1 (en) | 2007-05-25 | 2011-02-03 | Zhinian Jing | Vibration Sensor and Acoustic Voice Activity Detection System (VADS) for use with Electronic Systems |
US8717949B2 (en) | 2007-06-12 | 2014-05-06 | Microsoft Corporation | Active speaker identification |
US20110066429A1 (en) | 2007-07-10 | 2011-03-17 | Motorola, Inc. | Voice activity detector and a method of operation |
US20090258333A1 (en) | 2008-03-17 | 2009-10-15 | Kai Yu | Spoken language learning systems |
US9142218B2 (en) | 2008-04-11 | 2015-09-22 | At&T Intellectual Property I, L.P. | System and method for detecting synthetic speaker verification |
US20090292541A1 (en) | 2008-05-25 | 2009-11-26 | Nice Systems Ltd. | Methods and apparatus for enhancing speech analytics |
US20110054892A1 (en) | 2008-05-28 | 2011-03-03 | Koreapowervoice Co., Ltd. | System for detecting speech interval and recognizing continuous speech in a noisy environment through real-time recognition of call commands |
US20100070276A1 (en) | 2008-09-16 | 2010-03-18 | Nice Systems Ltd. | Method and apparatus for interaction or discourse analytics |
US8713119B2 (en) | 2008-10-02 | 2014-04-29 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
US20140222430A1 (en) | 2008-10-17 | 2014-08-07 | Ashwin P. Rao | System and Method for Multimodal Utterance Detection |
US20100110834A1 (en) | 2008-10-30 | 2010-05-06 | Kim Kyu-Hong | Apparatus and method of detecting target sound |
US8719009B2 (en) | 2009-02-20 | 2014-05-06 | Voicebox Technologies Corporation | System and method for processing multi-modal device interactions in a natural language voice services environment |
US8209174B2 (en) | 2009-04-17 | 2012-06-26 | Saudi Arabian Oil Company | Speaker verification system |
US20120265528A1 (en) | 2009-06-05 | 2012-10-18 | Apple Inc. | Using Context Information To Facilitate Processing Of Commands In A Virtual Assistant |
US20120084087A1 (en) | 2009-06-12 | 2012-04-05 | Huawei Technologies Co., Ltd. | Method, device, and system for speaker recognition |
US8670985B2 (en) | 2010-01-13 | 2014-03-11 | Apple Inc. | Devices and methods for identifying a prompt corresponding to a voice input in a sequence of prompts |
US20110184730A1 (en) | 2010-01-22 | 2011-07-28 | Google Inc. | Multi-dimensional disambiguation of voice commands |
US20110304648A1 (en) | 2010-06-15 | 2011-12-15 | Lg Electronics Inc. | Mobile terminal and method for operating the mobile terminal |
US8805690B1 (en) | 2010-08-05 | 2014-08-12 | Google Inc. | Audio notifications |
US8719018B2 (en) | 2010-10-25 | 2014-05-06 | Lockheed Martin Corporation | Biometric speaker identification |
US20120232896A1 (en) | 2010-12-24 | 2012-09-13 | Huawei Technologies Co., Ltd. | Method and an apparatus for voice activity detection |
US20120316471A1 (en) * | 2011-06-10 | 2012-12-13 | Aliphcom | Power management in a data-capable strapband |
US20130060571A1 (en) | 2011-09-02 | 2013-03-07 | Microsoft Corporation | Integrated local and cloud based speech recognition |
US8340975B1 (en) * | 2011-10-04 | 2012-12-25 | Theodore Alfred Rosenberger | Interactive speech recognition device and system for hands-free building control |
US20130110521A1 (en) * | 2011-11-01 | 2013-05-02 | Qualcomm Incorporated | Extraction and analysis of audio feature data |
US20130124207A1 (en) | 2011-11-15 | 2013-05-16 | Microsoft Corporation | Voice-controlled camera operations |
US20130132086A1 (en) | 2011-11-21 | 2013-05-23 | Robert Bosch Gmbh | Methods and systems for adapting grammars in hybrid speech recognition engines for enhancing local sr performance |
US20130183944A1 (en) | 2012-01-12 | 2013-07-18 | Sensory, Incorporated | Information Access and Device Control Using Mobile Phones and Audio in the Home Environment |
WO2014008194A1 (en) | 2012-07-03 | 2014-01-09 | Google Inc. | Determining hotword suitability |
US20140012578A1 (en) | 2012-07-04 | 2014-01-09 | Seiko Epson Corporation | Speech-recognition system, storage medium, and method of speech recognition |
US20140012573A1 (en) | 2012-07-06 | 2014-01-09 | Chia-Yu Hung | Signal processing apparatus having voice activity detection unit and related signal processing methods |
US20140088961A1 (en) | 2012-09-26 | 2014-03-27 | International Business Machines Corporation | Captioning Using Socially Derived Acoustic Profiles |
US8996372B1 (en) | 2012-10-30 | 2015-03-31 | Amazon Technologies, Inc. | Using adaptation data with cloud-based speech recognition |
US9129602B1 (en) * | 2012-12-14 | 2015-09-08 | Amazon Technologies, Inc. | Mimicking user speech patterns |
US20140257821A1 (en) | 2013-03-07 | 2014-09-11 | Analog Devices Technology | System and method for processor wake-up based on sensor data |
US20140278435A1 (en) | 2013-03-12 | 2014-09-18 | Nuance Communications, Inc. | Methods and apparatus for detecting a voice command |
US20140278383A1 (en) | 2013-03-13 | 2014-09-18 | Kopin Corporation | Apparatuses and methods for multi-channel signal compression during desired voice activity detection |
US8768687B1 (en) | 2013-04-29 | 2014-07-01 | Google Inc. | Machine translation of indirect speech |
WO2015025330A1 (en) | 2013-08-21 | 2015-02-26 | Kale Aaditya Kishore | A system to enable user to interact with an electronic processing device using voice of the user |
US20150262577A1 (en) | 2013-08-29 | 2015-09-17 | Panasonic Intellectual Property Corporation Of America | Speech recognition method and speech recognition apparatus |
US20150061969A1 (en) * | 2013-08-30 | 2015-03-05 | Lg Electronics Inc. | Wearable glass-type terminal, system having the same and method of controlling the terminal |
US8775191B1 (en) | 2013-11-13 | 2014-07-08 | Google Inc. | Efficient utterance-specific endpointer triggering for always-on hotwording |
US20150154953A1 (en) | 2013-12-02 | 2015-06-04 | Spansion Llc | Generation of wake-up words |
US8938394B1 (en) | 2014-01-09 | 2015-01-20 | Google Inc. | Audio triggers based on context |
US20160104483A1 (en) | 2014-10-09 | 2016-04-14 | Google Inc. | Hotword detection on multiple devices |
US20160260431A1 (en) | 2015-03-08 | 2016-09-08 | Apple Inc. | Competing devices responding to voice triggers |
Non-Patent Citations (15)
Title |
---|
Extended European Search Report issued in Application No. 16195834.3-1910, dated Nov. 23, 2016, 9 pages. |
International Preliminary Report on Patentability in International application No. PCT/US2015/030569, dated Jan. 24, 2017, 8 pages. |
International Search Report and Written Opinion in International Application No. PCT/US2015/052860, dated Dec. 8, 2015, 12 pages. |
International Search Report and Written Opinion in International Application No. PCT/US2015/052870, dated Dec. 4, 2015, 11 pages. |
Notice of Allowance in U.S. Appl. No. 14/659,861, dated Jun. 8, 2016, 8 pages. |
Notice of Allowance issued in U.S. Appl. No. 14/675,932, dated Jan. 25, 2016, 12 pages. |
Notice of Allowance issued in U.S. Appl. No. 15/088,477, dated Sep. 9, 2016, 17 pages. |
Office Action in Korean Application No. 10-2016-7021778, dated Dec. 13, 2016, 4 pages. |
Office Action in U.S. Appl. No. 14/659,861, dated Dec. 30, 2015, 10 pages. |
Office Action in U.S. Appl. No. 15/190,739, dated Aug. 12, 2016, 9 pages. |
Office Action in U.S. Appl. No. 15/190,739, dated Feb. 24, 2017, 13 pages. |
Office Action issued in U.S. Appl. No. 14/675,932, dated Jun. 12, 2015, 9 pages. |
Office Action issued in U.S. Appl. No. 14/675,932, dated Oct. 1, 2015, 19 pages. |
Office Action issued in U.S. Appl. No. 15/088,477, dated May 25, 2016, 12 pages. |
Office Action issued in U.S. Appl. No. 15/346,914, dated Feb. 3, 2017, 22 pages. |
Cited By (104)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11750969B2 (en) | 2016-02-22 | 2023-09-05 | Sonos, Inc. | Default playback device designation |
US11513763B2 (en) | 2016-02-22 | 2022-11-29 | Sonos, Inc. | Audio response playback |
US11405430B2 (en) | 2016-02-22 | 2022-08-02 | Sonos, Inc. | Networked microphone device control |
US10970035B2 (en) | 2016-02-22 | 2021-04-06 | Sonos, Inc. | Audio response playback |
US10971139B2 (en) | 2016-02-22 | 2021-04-06 | Sonos, Inc. | Voice control of a media playback system |
US11006214B2 (en) | 2016-02-22 | 2021-05-11 | Sonos, Inc. | Default playback device designation |
US11514898B2 (en) | 2016-02-22 | 2022-11-29 | Sonos, Inc. | Voice control of a media playback system |
US11556306B2 (en) | 2016-02-22 | 2023-01-17 | Sonos, Inc. | Voice controlled media playback system |
US11212612B2 (en) | 2016-02-22 | 2021-12-28 | Sonos, Inc. | Voice control of a media playback system |
US11726742B2 (en) | 2016-02-22 | 2023-08-15 | Sonos, Inc. | Handling of loss of pairing between networked devices |
US11184704B2 (en) | 2016-02-22 | 2021-11-23 | Sonos, Inc. | Music service selection |
US11736860B2 (en) | 2016-02-22 | 2023-08-22 | Sonos, Inc. | Voice control of a media playback system |
US11863593B2 (en) | 2016-02-22 | 2024-01-02 | Sonos, Inc. | Networked microphone device control |
US11832068B2 (en) | 2016-02-22 | 2023-11-28 | Sonos, Inc. | Music service selection |
US11568874B2 (en) * | 2016-02-24 | 2023-01-31 | Google Llc | Methods and systems for detecting and processing speech signals |
US20230169979A1 (en) * | 2016-02-24 | 2023-06-01 | Google Llc | Methods And Systems For Detecting And Processing Speech Signals |
US11545169B2 (en) | 2016-06-09 | 2023-01-03 | Sonos, Inc. | Dynamic player selection for audio signal processing |
US11979960B2 (en) | 2016-07-15 | 2024-05-07 | Sonos, Inc. | Contextualization of voice inputs |
US11664023B2 (en) | 2016-07-15 | 2023-05-30 | Sonos, Inc. | Voice detection by multiple devices |
US11531520B2 (en) | 2016-08-05 | 2022-12-20 | Sonos, Inc. | Playback device supporting concurrent voice assistants |
US11641559B2 (en) | 2016-09-27 | 2023-05-02 | Sonos, Inc. | Audio playback settings for voice interaction |
US11516610B2 (en) | 2016-09-30 | 2022-11-29 | Sonos, Inc. | Orientation-based playback device microphone selection |
US11727933B2 (en) | 2016-10-19 | 2023-08-15 | Sonos, Inc. | Arbitration-based voice recognition |
US11308961B2 (en) | 2016-10-19 | 2022-04-19 | Sonos, Inc. | Arbitration-based voice recognition |
US11126389B2 (en) | 2017-07-11 | 2021-09-21 | Roku, Inc. | Controlling visual indicators in an audio responsive electronic device, and capturing and providing audio using an API, by native and non-native computing devices and services |
US11380322B2 (en) | 2017-08-07 | 2022-07-05 | Sonos, Inc. | Wake-word detection suppression |
US11900937B2 (en) | 2017-08-07 | 2024-02-13 | Sonos, Inc. | Wake-word detection suppression |
US11062710B2 (en) | 2017-08-28 | 2021-07-13 | Roku, Inc. | Local and cloud speech recognition |
US11646025B2 (en) | 2017-08-28 | 2023-05-09 | Roku, Inc. | Media system with multiple digital assistants |
US11804227B2 (en) | 2017-08-28 | 2023-10-31 | Roku, Inc. | Local and cloud speech recognition |
US11062702B2 (en) | 2017-08-28 | 2021-07-13 | Roku, Inc. | Media system with multiple digital assistants |
US11961521B2 (en) | 2017-08-28 | 2024-04-16 | Roku, Inc. | Media system with multiple digital assistants |
US10777197B2 (en) | 2017-08-28 | 2020-09-15 | Roku, Inc. | Audio responsive device with play/stop and tell me something buttons |
US11500611B2 (en) | 2017-09-08 | 2022-11-15 | Sonos, Inc. | Dynamic computation of system response volume |
US11080005B2 (en) | 2017-09-08 | 2021-08-03 | Sonos, Inc. | Dynamic computation of system response volume |
US11646045B2 (en) | 2017-09-27 | 2023-05-09 | Sonos, Inc. | Robust short-time fourier transform acoustic echo cancellation during audio playback |
US11302326B2 (en) | 2017-09-28 | 2022-04-12 | Sonos, Inc. | Tone interference cancellation |
US11538451B2 (en) | 2017-09-28 | 2022-12-27 | Sonos, Inc. | Multi-channel acoustic echo cancellation |
US11769505B2 (en) | 2017-09-28 | 2023-09-26 | Sonos, Inc. | Echo of tone interferance cancellation using two acoustic echo cancellers |
US11288039B2 (en) | 2017-09-29 | 2022-03-29 | Sonos, Inc. | Media playback system with concurrent voice assistance |
US11893308B2 (en) | 2017-09-29 | 2024-02-06 | Sonos, Inc. | Media playback system with concurrent voice assistance |
US11175888B2 (en) | 2017-09-29 | 2021-11-16 | Sonos, Inc. | Media playback system with concurrent voice assistance |
US11451908B2 (en) | 2017-12-10 | 2022-09-20 | Sonos, Inc. | Network microphone devices with automatic do not disturb actuation capabilities |
US11676590B2 (en) | 2017-12-11 | 2023-06-13 | Sonos, Inc. | Home graph |
US11343614B2 (en) | 2018-01-31 | 2022-05-24 | Sonos, Inc. | Device designation of playback and network microphone device arrangements |
US11689858B2 (en) | 2018-01-31 | 2023-06-27 | Sonos, Inc. | Device designation of playback and network microphone device arrangements |
US20190251960A1 (en) * | 2018-02-13 | 2019-08-15 | Roku, Inc. | Trigger Word Detection With Multiple Digital Assistants |
US11935537B2 (en) | 2018-02-13 | 2024-03-19 | Roku, Inc. | Trigger word detection with multiple digital assistants |
US20210383807A1 (en) * | 2018-02-13 | 2021-12-09 | Roku, Inc. | Trigger Word Detection with Multiple Digital Assistants |
US11145298B2 (en) * | 2018-02-13 | 2021-10-12 | Roku, Inc. | Trigger word detection with multiple digital assistants |
US11664026B2 (en) * | 2018-02-13 | 2023-05-30 | Roku, Inc. | Trigger word detection with multiple digital assistants |
US11797263B2 (en) | 2018-05-10 | 2023-10-24 | Sonos, Inc. | Systems and methods for voice-assisted media content selection |
US11175880B2 (en) | 2018-05-10 | 2021-11-16 | Sonos, Inc. | Systems and methods for voice-assisted media content selection |
US11715489B2 (en) | 2018-05-18 | 2023-08-01 | Sonos, Inc. | Linear filtering for noise-suppressed speech detection |
US11792590B2 (en) | 2018-05-25 | 2023-10-17 | Sonos, Inc. | Determining and adapting to changes in microphone performance of playback devices |
US11696074B2 (en) | 2018-06-28 | 2023-07-04 | Sonos, Inc. | Systems and methods for associating playback devices with voice assistant services |
US11563842B2 (en) | 2018-08-28 | 2023-01-24 | Sonos, Inc. | Do not disturb feature for audio notifications |
US11482978B2 (en) | 2018-08-28 | 2022-10-25 | Sonos, Inc. | Audio notifications |
US11551690B2 (en) | 2018-09-14 | 2023-01-10 | Sonos, Inc. | Networked devices, systems, and methods for intelligently deactivating wake-word engines |
US11778259B2 (en) | 2018-09-14 | 2023-10-03 | Sonos, Inc. | Networked devices, systems and methods for associating playback devices based on sound codes |
US11432030B2 (en) | 2018-09-14 | 2022-08-30 | Sonos, Inc. | Networked devices, systems, and methods for associating playback devices based on sound codes |
US10878811B2 (en) | 2018-09-14 | 2020-12-29 | Sonos, Inc. | Networked devices, systems, and methods for intelligently deactivating wake-word engines |
US11790937B2 (en) | 2018-09-21 | 2023-10-17 | Sonos, Inc. | Voice detection optimization using sound metadata |
US11024331B2 (en) | 2018-09-21 | 2021-06-01 | Sonos, Inc. | Voice detection optimization using sound metadata |
US11727936B2 (en) | 2018-09-25 | 2023-08-15 | Sonos, Inc. | Voice detection optimization based on selected voice assistant service |
US11790911B2 (en) | 2018-09-28 | 2023-10-17 | Sonos, Inc. | Systems and methods for selective wake word detection using neural network models |
US11100923B2 (en) | 2018-09-28 | 2021-08-24 | Sonos, Inc. | Systems and methods for selective wake word detection using neural network models |
US11501795B2 (en) | 2018-09-29 | 2022-11-15 | Sonos, Inc. | Linear filtering for noise-suppressed speech detection via multiple network microphone devices |
US11899519B2 (en) * | 2018-10-23 | 2024-02-13 | Sonos, Inc. | Multiple stage network microphone device with reduced power consumption and processing load |
US11200889B2 (en) | 2018-11-15 | 2021-12-14 | Sonos, Inc. | Dilated convolutions and gating for efficient keyword spotting |
US11741948B2 (en) | 2018-11-15 | 2023-08-29 | Sonos Vox France Sas | Dilated convolutions and gating for efficient keyword spotting |
US11557294B2 (en) | 2018-12-07 | 2023-01-17 | Sonos, Inc. | Systems and methods of operating media playback systems having multiple voice assistant services |
US11183183B2 (en) | 2018-12-07 | 2021-11-23 | Sonos, Inc. | Systems and methods of operating media playback systems having multiple voice assistant services |
US11132989B2 (en) | 2018-12-13 | 2021-09-28 | Sonos, Inc. | Networked microphone devices, systems, and methods of localized arbitration |
US11538460B2 (en) | 2018-12-13 | 2022-12-27 | Sonos, Inc. | Networked microphone devices, systems, and methods of localized arbitration |
US11540047B2 (en) | 2018-12-20 | 2022-12-27 | Sonos, Inc. | Optimization of network microphone devices using noise classification |
US11315556B2 (en) | 2019-02-08 | 2022-04-26 | Sonos, Inc. | Devices, systems, and methods for distributed voice processing by transmitting sound data associated with a wake word to an appropriate device for identification |
US11646023B2 (en) | 2019-02-08 | 2023-05-09 | Sonos, Inc. | Devices, systems, and methods for distributed voice processing |
US11798553B2 (en) | 2019-05-03 | 2023-10-24 | Sonos, Inc. | Voice assistant persistence across multiple network microphone devices |
US11501773B2 (en) | 2019-06-12 | 2022-11-15 | Sonos, Inc. | Network microphone device with command keyword conditioning |
US11854547B2 (en) | 2019-06-12 | 2023-12-26 | Sonos, Inc. | Network microphone device with command keyword eventing |
US11200894B2 (en) | 2019-06-12 | 2021-12-14 | Sonos, Inc. | Network microphone device with command keyword eventing |
US11361756B2 (en) | 2019-06-12 | 2022-06-14 | Sonos, Inc. | Conditional wake word eventing based on environment |
US20220366906A1 (en) * | 2019-07-15 | 2022-11-17 | Huawei Technologies Co., Ltd. | Voice wake-up method and electronic device |
US11710487B2 (en) | 2019-07-31 | 2023-07-25 | Sonos, Inc. | Locally distributed keyword detection |
US11714600B2 (en) | 2019-07-31 | 2023-08-01 | Sonos, Inc. | Noise classification for event detection |
US11354092B2 (en) | 2019-07-31 | 2022-06-07 | Sonos, Inc. | Noise classification for event detection |
US11551669B2 (en) | 2019-07-31 | 2023-01-10 | Sonos, Inc. | Locally distributed keyword detection |
US11189286B2 (en) | 2019-10-22 | 2021-11-30 | Sonos, Inc. | VAS toggle based on device orientation |
US11862161B2 (en) | 2019-10-22 | 2024-01-02 | Sonos, Inc. | VAS toggle based on device orientation |
US11200900B2 (en) | 2019-12-20 | 2021-12-14 | Sonos, Inc. | Offline voice control |
US11869503B2 (en) | 2019-12-20 | 2024-01-09 | Sonos, Inc. | Offline voice control |
US11562740B2 (en) | 2020-01-07 | 2023-01-24 | Sonos, Inc. | Voice verification for media playback |
US11556307B2 (en) | 2020-01-31 | 2023-01-17 | Sonos, Inc. | Local voice data processing |
US11308958B2 (en) | 2020-02-07 | 2022-04-19 | Sonos, Inc. | Localized wakeword verification |
US11961519B2 (en) | 2020-02-07 | 2024-04-16 | Sonos, Inc. | Localized wakeword verification |
US11694689B2 (en) | 2020-05-20 | 2023-07-04 | Sonos, Inc. | Input detection windowing |
US11727919B2 (en) | 2020-05-20 | 2023-08-15 | Sonos, Inc. | Memory allocation for keyword spotting engines |
US11308962B2 (en) | 2020-05-20 | 2022-04-19 | Sonos, Inc. | Input detection windowing |
US11482224B2 (en) | 2020-05-20 | 2022-10-25 | Sonos, Inc. | Command keywords with input detection windowing |
US11698771B2 (en) | 2020-08-25 | 2023-07-11 | Sonos, Inc. | Vocal guidance engines for playback devices |
US11551700B2 (en) | 2021-01-25 | 2023-01-10 | Sonos, Inc. | Systems and methods for power-efficient keyword detection |
US11983463B2 (en) | 2021-10-04 | 2024-05-14 | Sonos, Inc. | Metadata exchange involving a networked playback system and a networked microphone system |
US11984123B2 (en) | 2021-11-11 | 2024-05-14 | Sonos, Inc. | Network device interaction by range |
Also Published As
Publication number | Publication date |
---|---|
US20170287484A1 (en) | 2017-10-05 |
US11568874B2 (en) | 2023-01-31 |
US10255920B2 (en) | 2019-04-09 |
US20210090574A1 (en) | 2021-03-25 |
US20170287486A1 (en) | 2017-10-05 |
US20230169979A1 (en) | 2023-06-01 |
US10249303B2 (en) | 2019-04-02 |
US10878820B2 (en) | 2020-12-29 |
US20170249943A1 (en) | 2017-08-31 |
US20190189128A1 (en) | 2019-06-20 |
US10163442B2 (en) | 2018-12-25 |
US20170243586A1 (en) | 2017-08-24 |
US10163443B2 (en) | 2018-12-25 |
US20170287485A1 (en) | 2017-10-05 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11568874B2 (en) | Methods and systems for detecting and processing speech signals | |
JP7351937B2 (en) | Network microphone device with command keyword adjustment | |
JP7397920B2 (en) | System and method for selective wake word detection using neural network model | |
US11854547B2 (en) | Network microphone device with command keyword eventing | |
US11361756B2 (en) | Conditional wake word eventing based on environment | |
US10097902B2 (en) | System and method for using multiple audio input devices for synchronized and position-based audio | |
WO2021081076A1 (en) | Vas toggle based on device orientation | |
EP3753015A1 (en) | Trigger word detection with multiple digital assistants | |
US20230169956A1 (en) | Locally distributed keyword detection | |
US20240105167A1 (en) | Memory allocation for keyword spotting engines | |
US9195740B2 (en) | Audio scene selection apparatus | |
US20230289132A1 (en) | Concurrency rules for network microphone devices having multiple voice assistant services | |
US9654891B2 (en) | System and method for determining proximity of a controller to a media rendering device |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:CIVELLI, JAY PIERRE;SHEMER, MIKHAL;SHABESTARY, TURAJ ZAKIZADEH;AND OTHERS;SIGNING DATES FROM 20160219 TO 20160223;REEL/FRAME:041219/0219 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044129/0001Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |