US9565114B1 - Weighted load balancing using scaled parallel hashing - Google Patents
Weighted load balancing using scaled parallel hashing Download PDFInfo
- Publication number
- US9565114B1 US9565114B1 US14/217,921 US201414217921A US9565114B1 US 9565114 B1 US9565114 B1 US 9565114B1 US 201414217921 A US201414217921 A US 201414217921A US 9565114 B1 US9565114 B1 US 9565114B1
- Authority
- US
- United States
- Prior art keywords
- egress
- data packet
- egress port
- given
- egress ports
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L67/00—Network arrangements or protocols for supporting network services or applications
- H04L67/01—Protocols
- H04L67/10—Protocols in which an application is distributed across nodes in the network
- H04L67/1001—Protocols in which an application is distributed across nodes in the network for accessing one among a plurality of replicated servers
- H04L67/1004—Server selection for load balancing
- H04L67/1023—Server selection for load balancing based on a hash applied to IP addresses or costs
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L47/00—Traffic control in data switching networks
- H04L47/10—Flow control; Congestion control
- H04L47/12—Avoiding congestion; Recovering from congestion
- H04L47/125—Avoiding congestion; Recovering from congestion by balancing the load, e.g. traffic engineering
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L45/00—Routing or path finding of packets in data switching networks
- H04L45/24—Multipath
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L45/00—Routing or path finding of packets in data switching networks
- H04L45/44—Distributed routing
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L45/00—Routing or path finding of packets in data switching networks
- H04L45/74—Address processing for routing
- H04L45/745—Address table lookup; Address filtering
- H04L45/7453—Address table lookup; Address filtering using hashing
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L47/00—Traffic control in data switching networks
- H04L47/50—Queue scheduling
- H04L47/62—Queue scheduling characterised by scheduling criteria
- H04L47/622—Queue service order
- H04L47/623—Weighted service order
Definitions
- This description relates to systems and techniques for weighted load balancing in a multistage network.
- a network may include a multistage network, which may include a wired and/or wireless network.
- a multistage network may be used in the context of telecommunication and data centers to realize large-scale networks.
- a multistage network in a data center may have a large shared infrastructure of shared resources.
- current approaches for weighted may be complicated to implement in large multistage networks, such as, for example, data center networks.
- a method for routing packets in a network includes receiving, at a data switch, a data packet, the data switch including a plurality of egress ports.
- the method includes, for each of the egress ports, generating, by the data switch, an independent hash value based on one or more fields of the data packet and generating a weighted hash value by scaling the hash value using a scaling factor, the scaling factor being based on at least two traffic routing weights of a plurality of respective traffic routing weights associated with the plurality of egress ports.
- the method also includes selecting an egress port of the plurality of egress ports based on the weighted hash value for each of the egress ports and transmitting, by the data switch, the data packet using the selected egress port.
- Implementations can include one or more of the following features.
- the one or more fields of the data packet can include one or more fields of a header of the data packet, where the one or more fields of the header of the data packet can have fixed values for each data packet of a data flow associated with the data packet.
- Generating independent hash values for the plurality of egress ports can include generating a first hash value using a first hash function, where the first hash value is associated with a first egress port of the plurality of egress ports, and generating a second hash value using a second hash function, where the second hash value is associated with a second egress port of the plurality of egress ports,
- the second hash function can be different than, and independent of, the first hash function.
- Generating independent hash values for the plurality of egress ports further can include generating a third hash value using a third hash function.
- the third hash value can be associated with a third egress port of the plurality of egress ports.
- the third hash function can be different than, and independent of, the first hash function and the second hash function.
- a scaling factor for a given egress port of the plurality of egress ports can be a ratio of a probability of the given egress port being selected in a joint probability distribution for the plurality of egress ports with a probability of an egress port with a lowest routing weight of the respective traffic routing weights being selected in the joint probability distribution.
- the probability of the given egress port being selected in the joint probability distribution can be proportional with a routing weight associated with the given egress port.
- the plurality of routing weights can be normalized such that a smallest routing weight of the plurality of routing weights has a normalized value of 1.
- Generating the independent hash value for each of egress ports can include normalizing a plurality of respective independent hash values for the plurality of egress ports to respective values in a range of 0 to 1.
- Selecting the egress port of the plurality of egress ports can include selecting an egress port of the plurality of egress ports having a highest respective scaled hash value.
- a data switch including a plurality of egress ports includes at least one memory that is configured to store instructions and at least one processor that is operably coupled to the at least one memory and that is configured to process the instructions to cause the data switch to receive a data packet.
- the instructions further cause the data switch, for each of the egress ports, to generate an independent hash value based on one or more fields of the data packet and generate a weighted hash value by scaling the hash value using a scaling factor.
- the scaling factor is based on at least two traffic routing weights of a plurality of respective traffic routing weights associated with the plurality of egress ports.
- the instructions further cause the data switch to select an egress port of the plurality of egress ports based on the weighted hash value for each of the egress ports and transmit the data packet using the selected egress port.
- generating independent hash values for the plurality of egress ports can include generating a first hash value using a first hash function, where the first hash value is associated with a first egress port of the plurality of egress ports, and generating a second hash value using a second hash function, where the second hash value is associated with a second egress port of the plurality of egress ports,
- the second hash function can be different than, and independent of, the first hash function.
- Generating independent hash values for the plurality of egress ports further can include generating a third hash value using a third hash function.
- the third hash value can be associated with a third egress port of the plurality of egress ports.
- the third hash function can be different than, and independent of, the first hash function and the second hash function.
- a scaling factor for a given egress port of the plurality of egress ports can be a ratio of a probability of the given egress port being selected in a joint probability distribution for the plurality of egress ports with a probability of an egress port with a lowest routing weight of the respective traffic routing weights being selected in the joint probability distribution.
- the probability of the given egress port being selected in the joint probability distribution can be proportional with a routing weight associated with the given egress port.
- Selecting the egress port of the plurality of egress ports can include selecting an egress port of the plurality of egress ports having a highest respective scaled hash value.
- the one or more fields of the data packet can include one or more fields of a header of the data packet, the one or more fields of the header of the data packet having fixed values for each data packet of a data flow associated with the data packet.
- a method in another general aspect, includes receiving, at a data switch, a set of respective traffic routing weights for a plurality of egress ports of the data switch, each egress port of the plurality of egress ports being associated with a respective independent hash function. The method also includes determining, for the plurality of egress ports, a plurality of respective hash function scaling factors, the plurality of respective hash function scaling factors being based on the set of routing weights and a joint probability distribution for the plurality of egress ports. The method also includes routing a data packet using the respective independent hash functions and the respective hash function scaling factors.
- a scaling factor for a given egress port of the plurality of egress ports can be a ratio of a probability of the given egress port being selected in a joint probability distribution for the plurality of egress ports with a probability of an egress port with a lowest routing weight of the respective traffic routing weights being selected in the joint probability distribution.
- the probability of the given egress port being selected can be proportional with a routing weight associated with the given egress port.
- a data switch in another general aspect, includes at least one memory that is configured to store instructions and at least one processor that is operably coupled to the at least one memory and that is configured to process the instructions to cause the data switch to receive a set of respective traffic routing weights for a plurality of egress ports of the data switch, each egress port of the plurality of egress ports being associated with a respective independent hash function.
- the instructions further cause the data switch to determine, for the plurality of egress ports, a plurality of respective hash function scaling factors, the plurality of respective hash function scaling factors being based on the set of routing weights and a joint probability distribution for the plurality of egress ports.
- the instructions also cause the data switch to route a data packet using the respective independent hash functions and the respective hash function scaling factors.
- a scaling factor for a given egress port of the plurality of egress ports can be a ratio of a probability of the given egress port being selected in a joint probability distribution for the plurality of egress ports with a probability of an egress port with a lowest routing weight of the respective traffic routing weights being selected in the joint probability distribution.
- the probability of the given egress port being selected can be proportional with a routing weight associated with the given egress port
- the plurality of routing weights can be normalized such that a smallest routing weight of the plurality of routing weights has a normalized value of 1.
- the plurality of respective hash function scaling factors can include iteratively determining the plurality of hash function scaling factors.
- FIG. 1 is a block diagram illustrating a multi-level, multi-path network, in accordance with an implementation.
- FIG. 2 is a flowchart illustrating a method for routing data traffic in a multi-level, multipath network, according to an example implementation.
- FIG. 3 is a block diagram illustrating elements of a data switch, according to an implementation.
- FIG. 4 is a flowchart illustrating a method for implementing weighted cost multi-path (WCMP) routing in a data switch, according to an implementation.
- WCMP weighted cost multi-path
- FIG. 5 is a flowchart illustrating a method for implementing WCMP routing in a data switch, according to an implementation.
- FIG. 6 is a block diagram illustrating a data switch, according to an implementation.
- FIG. 7 is a diagram illustrating a joint probability distribution that can be used to determine weighted traffic routing scaling factors, according to an implementation.
- FIG. 8 is a diagram illustrating another joint probability distribution that can be used to determine weighted traffic routing scaling factors, according to an implementation.
- a source device may transmit packets to a destination device using a multi-level network, where multiple data paths (links) may be available (used) to transmit data between the source device and the destination device.
- Data switches (or other data routing devices) in the multi-level network may use a weighted cost multi-path (WCMP) routing module to balance data traffic (between the source device and the destination device) over the multiple paths between the source device and the destination device.
- WCMP weighted cost multi-path
- FIG. 1 is a block diagram illustrating a multi-level, multi-path network 100 , in accordance with an implementation.
- the network 100 includes a first data switch 110 , a second data switch 120 , a third data switch 130 and a fourth data switch 140 .
- the network 100 also includes data communication links 112 , 114 , 116 , 122 , 132 , 134 , which are used to communicate data (e.g., packet data) between the data switches 110 , 120 , 130 , 140 .
- data e.g., packet data
- the data switches 110 , 120 , 130 , 140 may include a number of different devices, such as a network data switch, a router, or other device capable of communicating (steering, routing, switching) data (such as packet data) from a source device to a destination device.
- the network 100 illustrated in FIG. 1 (which may be referred to as a multistage network) is shown for purposes of illustration.
- such multistage networks may include a large number of data communication devices (data switching devices).
- data switching devices data switching devices
- multistage networks may be used to implement large-scale commercial networks, such as commercial data center networks (DCNs). Scalability of such networks may be achieved by using relatively inexpensive and power-efficient commodity data switch devices as the building block at each network stage, instead of using fewer relatively expensive, high-end, large and complex switches.
- DCNs commercial data center networks
- data traffic may be routed using equal cost multi-path (ECMP) routing for load-balancing data traffic across the different switches at each stage.
- ECMP equal cost multi-path
- ECMP approaches may provide substantially equally balanced traffic distribution in multistage networks that are of uniform topology (e.g., there is a same number of data links from a given data switch to each neighboring switch along parallel data paths).
- ECMP approaches would not (e.g., presuming the use of statistically sufficient random selection) provide such evenly balanced traffic distribution in multistage networks that have uneven inter-stage connectivity topologies.
- the ports (ingress ports and egress ports, which may also be referred to as communication links, or links) that connect a sending switch with its neighboring switches on parallel paths are not evenly distributed.
- the network 100 includes such an uneven inter-stage topology (connectivity).
- the data switch 110 may communicate data traffic to the data switch 140 via the data switch 120 , or via the data switch 130 .
- there are two links 112 , 114 connecting the data switch 110 and the data switch 120 while there is only one link 116 connecting the data switch 110 and the data switch 130 .
- Such uneven inter-stage connectivity in such a multistage network may occur, for instance, by design. This design choice may occur because of the number of data switches present at each stage (e.g., due to the configuration of the particular network), or may occur because the multistage network was intentionally configured to leverage certain traffic locality patterns with uneven inter-stage connectivity. In other instances, uneven inter-stage connectivity in a multi-stage network may occur as a result of link failures between data switches in the network.
- Weighted cost multi-path (WCMP) data routing may be used to overcome, at least some of, the shortcomings of ECMP to evenly balance data traffic in such multi-stage networks with uneven inter-stage topologies. Because ECMP, which inherently assumes that all paths to a certain destination have a same capacity, balancing data traffic across unevenly distributed links with equal probability results in unequal traffic loading.
- WCMP Weighted cost multi-path
- ECMP For example and purposes of illustration, with reference to FIG. 1 , consider using ECMP for routing data traffic from the data switch 110 to the data switch 140 (where that traffic can go through either the data switch 120 or the data switch 130 ). Because ECMP routes data traffic using an equal-probability hashing function (e.g., and a corresponding modulo function), that is equally like to select each of the egress ports (links) 112 , 114 , 116 of the data switch 110 (which, in this example, are presumed to have equal capacity) for a arbitrary data flow, using ECMP would (over a population of data flows) result in twice as much data traffic being sent to data switch 120 (e.g., over the links 112 , 114 ) as compared to the amount of data traffic sent to the data switch 130 , despite the fact that the overall capacities of the two parallel paths from the switch 110 to the switch 140 (i.e., respectively through the switch 120 and the switch 130 ) are substantially the same (e.g., each
- WCMP traffic routing may accomplished for data traffic from the data switch 110 to the data switch 140 by using an EMCP table with four entries, where each of the links 112 , 114 is listed once and the link 116 has a replicated listing (is listed twice), for a total of four entries.
- EMCP table with four entries, where each of the links 112 , 114 is listed once and the link 116 has a replicated listing (is listed twice), for a total of four entries.
- Such an approach represents a routing weight of “2” for the link 116 (by virtue of being listed twice) and a routing weight of “1” for each of the links 112 , 114 (by virtue of them each being listed once).
- WCMP may easily implemented by replicating the egress port listing for the link 116 in an EMCP table
- Scalability of WCMP in such networks has multiple considerations. For example, one consideration is the ability to support an arbitrary mix of weights with sufficient resolution in the switches of such networks.
- using ECMP tables with replicated entries to implement WCMP would typically require a prohibitive degree of replication (i.e., the required table sizes would likely far exceed the capacity of memory structures currently used to implement ECMP tables).
- WCMP in large scale multi-level networks
- DCNs large scale multi-level networks
- relatively simple changes in routing weights may become large complex operations. For example, consider changing a routing weight of “5” to a routing weight of “4” in for a group with current weights 5, 30, 75, 40. Because current implementation do not take into account previous table configurations when altering routing weights, such a change could require rewriting, potentially, 149 table entries (e.g., 4+30+75+40). Accordingly, the amount of work can be arbitrarily large for even a small delta in weights.
- FIG. 2 is a flowchart illustrating a method 200 for WCMP data traffic routing in a multi-level, multipath network, according to an example implementation.
- the method 200 is provided by way of illustration and may be implemented using a number of approaches, such as those described herein. In other implementations, other approaches may be used to implement WCMP data traffic routing.
- the method 200 includes, at block 210 , receiving a set of routing weights for a group of egress ports of a data switch.
- the routing weights received at block 210 may correspond with a group of egress ports that can be used to send data traffic to a particular destination.
- the routing weights of block 210 may be used to implement WCMP data traffic routing for data traffic sent from the data switch to that particular destination.
- the routing weights may be provided to the data switch using a number of approaches.
- the routing weights may be provided to the data switch from a network configuration system.
- the data switch, at block 210 may receive the routing weights via a management port, via a configuration packet, or by a number of other approaches.
- the method 200 includes processing (which may also be referred to as pre-processing) the routing weights received at block 210 , such as using hardware or software (or firmware) for use in a WCMP table (or WCMP tables).
- the processed (pre-processed) weights of block 220 may be used (further processed) to create one or more WCMP data routing tables, where the WCMP table(s) created at block 230 may be used to route data traffic from the data switch of block 210 to the particular destination with which the received routing weights are associated.
- the one more WCMP tables of block 230 may be stored in the data switch using hardware, software, or a combination thereof.
- a data packet may be received at the data switch of block 210 .
- the data switch may determine a destination address of the data packet. The destination address may then be looked up in a forwarding table to determine a corresponding routing table (or tables) to use to determine an egress port to use to send the received data packet along to its destination. If the destination address of the data packet received at block 240 corresponds with the destination address associated with the routing weights received at block 210 , the forwarding table lookup may return a pointer to the WCMP table (or tables) generated at block 230 .
- the method 200 includes, at block 250 , generating at least one hash value from a header of the received data packet.
- the hash value may be generated using one or more fields of the data packet header that, for a given data flow, have fixed values. For instance a flow identification field, a source address field, a destination address field, a protocol identification field and/or a number of other possible fields with fixed values for a given data flow.
- the specific header field (or fields) that are used for generating the one or more hash values may depend on the particular network implementation.
- the method 200 includes determining, by using the one or more hash values as lookup value(s) for the WCMP tables of block 230 .
- the lookup at block 260 returns (determines) an egress port of the data switch to use to forward the data packet onto its destination.
- the method 200 includes transmitting the data packet using the determined egress port.
- a number of different hash functions may be used. For instance, a CRC16 hash function, a CRC32 hash function, an AES hash function, an SHA hash function and/or a number of other hash functions may be used.
- the hash function(s) used should be selected such that the hash values generated are sufficiently random for data routing purposes in a multi-level network, such as those described herein.
- a modulus (remainder) of the hash value generated by the selected hash functions may be taken before performing a lookup in the WCMP table(s).
- the result of the modulus function may be used directly, or indirectly, depending on the implementation, as an index (or lookup value) for the WCMP table(s) of block 230 .
- a modulus function that is applied may be based on the number of entries present in a WCMP table(s) on which the lookup will be performed.
- the lookup value may be a function of the result of the modulus operation (e.g., an encoded version of the result of the modulus function).
- the specific structure and arrangement of the WCMP table(s) (and the associated lookup values) used in the method 200 will depend on the specific implementation.
- FIG. 3 is a block diagram illustrating a data switch 300 , according to an implementation.
- the data switch 300 of FIG. 3 includes an ingress port 310 , a forwarding table 320 , an ECMP routing module 330 , a WCMP routing module 340 and a plurality of egress ports 350 .
- the data switch 300 may also include other elements, such as a switch management port (management port), additional ingress ports, a (network) processor, memory structures, and so forth.
- the data switch 300 may be used, for example, to implement the techniques for WCMP routing described herein.
- the data switch 300 may receive a set of routing weights for a particular destination, e.g., via the ingress port 310 or a management port (not shown) and use those weights to implement one or more WCMP routing tables that may be included in the WCMP routing module 340 .
- the data switch 300 may also use the ECMP routing module 330 to implement ECMP data traffic routing for one or more destinations in a data network in which the data switch 300 is included, where each parallel path (the multi-paths) between the data switch 300 and a given destination have uniform inter-level topologies.
- the data switch 300 when a data packet is received at the ingress port 310 , the data switch 300 may determine a destination address for that data packet from its header. If the destination address of the received data packet corresponds with a destination address that is associated with a WCMP table (or tables) in the WCMP module 340 (e.g., corresponds with the received routing weights), the forwarding table 320 (when the destination address is used to perform a lookup in the forwarding table 320 ) may return a pointer to the corresponding WCMP routing tables in the WCMP routing module 340 .
- a WCMP table or tables in the WCMP module 340
- An egress port of the egress ports 350 to use to communicate the data packet to its destination may then be determined from the corresponding WCMP table(s) or scaled hash values, such as by using the approaches described herein. After the particular egress port to use is determined, the data packet may be sent on to its destination using the determined egress port.
- a set of normalized routing weights may be determined from a given set of routing weights such that at least one routing weight of the normalized set of routing weights has an integer value of “1” and that the minimum routing weight value for the normalized set of routing weights is also an integer value of “1.”
- a set of data traffic routing weights of [2, 6, 14, 8] for a WCMP group of four egress ports may be normalized to a set of routing weights of [1, 3, 7, 4].
- hash values produced by the independent hash functions associated with the egress ports of a WCMP group may be normalized to be in a fixed range of values, e.g., in a range from 0 to 1.
- the initial (raw) hash values (before being normalized) may be produced using a modulus operation, such as described herein.
- the hash values that are scaled using respective hash function scaling factors can be the “raw” hash values, such as 16 bit hash values, for example.
- the specific approach will depend on the particular hash functions used for the egress ports of a given WCMP group.
- the independent hash functions may produce raw hash values with different numbers of bits.
- a modulus operation may be used to produce hash values within a fixed integer range (e.g., based on a number of egress ports in an associated WCMP group) before normalizing the hash values.
- groups of egress ports are described as having respective independent hash functions associated with each egress port of the given group.
- independent hash functions may be defined as hash functions that are not dependent on one another and have equal probability of producing any particular value in a given, fixed range of values for a statistically random population of data packets.
- the specific hash functions used in a particular implementation will depend on a number of factors, such as the configuration of a particular network in which they are used and/or the configuration of a data switch that is used to implement weighted (WCMP) data traffic routing using the approaches described herein.
- FIG. 4 is a flow chart illustrating a method 400 for implementing a weighted cost multi-path (WCMP) data traffic routing in a data switching device, according to an implementation.
- the method 400 may be used to implement a relatively simple (as compared to current approaches) and scalable approach for WCMP data traffic routing, as changes in data traffic routing weights can be implemented by changing a single hash function scaling factor for each egress port in a WCMP group as compared to writing a large, overloaded ECMP table.
- the method 400 includes, at block 410 , receiving (e.g., at a data switch) a set of routing weights (WCMP weights) for a plurality of egress ports (e.g., of a given WCMP group).
- the routing weights may be a set of routing weights for traffic being sent to a particular destination device.
- the routing weights may be for routing data traffic (e.g., using WCMP) to multiple destinations, or to a particular group of destinations.
- the routing weights may be a normalized set of routing weights.
- the data switch may be configured to normalize the set of routing weights.
- the routing weights may an un-normalized set of routing weights.
- Each routing weight of the set of routing weights of block 410 may be associated, respectively, with a specific egress port of the data switch.
- Each of the egress ports of the plurality of egress ports may be associated with an independent hash function, which will be used to generate, in parallel, independent hash values for each data packet (data frame, or other data unit used to transmit data from a source to a destination) received at the data switch.
- the independent hash functions may each be of a same class of hash functions, or can be of different classes.
- the independent hash functions may all be independent CRC16 hash function variants.
- the independent hash functions may be a mix of CRC16, CRC32, AES, SHA and/or other types of hash functions.
- the method 400 includes determining, for each egress port of the plurality of egress ports, a respective hash function scaling factor, where the hash function scaling factor is based on the set of routing weights (e.g., a ratio of routing weights) and a joint probability distribution for routing data traffic with the plurality of egress ports according to their assigned routing weights.
- a respective hash function scaling factor is based on the set of routing weights (e.g., a ratio of routing weights) and a joint probability distribution for routing data traffic with the plurality of egress ports according to their assigned routing weights.
- the scaling factor for a given egress port of the plurality of egress ports may be a ratio of a probability of the given egress port being selected in a joint probability distribution for the plurality of egress ports with a probability of an egress port with a lowest routing weight of the respective traffic routing weights being selected in the joint probability distribution, where the probability of the given egress port being selected is proportional with a routing weight associated with the given egress port.
- the scaling factors may be stored in a WCMP traffic routing module (e.g., the WCMP routing module 340 ) of the data switch. Example approaches for determining such hash function scaling factors are described below with respect to FIGS. 7 and 8 .
- the method 400 includes routing a data packet using the respective independent hash functions and the respective hash function scaling factors. For instance, respective independent hash values based on one or more fields of the packet (e.g., one or more packet header fields) may be produced by each independent hash function (each being associated with an egress port a WCMP group). Depending on the specific implementation, the independent (parallel) hash values for the data packet may be normalized to a fixed range (e.g., 0 to 1). The independent hash values may then be scaled using their corresponding scaling factors determined at block 420 . The scaled hash values may then be examined to select an egress port on which to transmit the data packet towards its destination.
- respective independent hash values based on one or more fields of the packet e.g., one or more packet header fields
- each independent hash function each being associated with an egress port a WCMP group.
- the independent (parallel) hash values for the data packet may be normalized to a fixed range (e
- an egress port may be selected by determining the highest scaled hash value and selecting the corresponding egress port for transmitting the data packet towards its destination.
- an egress port with a lowest port index may be selected from the egress port with the same highest scaled hash function value.
- an egress port for that packet can be randomly selected. For instance, for the first data packet in such a flow, a random selection can be made, with all subsequent packets in the data flow being forwarded on that randomly selected egress port.
- FIG. 5 is a flowchart illustrating a method 500 for implementing WCMP routing in a data switch, according to an implementation.
- the method 500 uses respective independent hash function and scaling factors to generate respective scaled hash values, where the scaling factors are based on routing weights associated with egress ports of a WCMP group.
- the scaling factors may be iteratively determined (e.g., using approaches such as those described below with respect to FIGS. 7 and 8 ) based on a weighted joint probability distribution for the plurality of egress ports of a given ECMP group.
- a given scaling factor for a given egress port of a WCMP group that has a normalized routing weight of greater than 1 may be determined by equating a ratio of 1 (e.g., the normalized routing weight of an egress port with the lowest routing weight) and the normalized routing weight of a given egress port with a ratio of the probability of the egress port with the lowest routing weight being selected (in the weighted joint probability distribution) and a probability of the given egress port being selected (in the weighted joint probability distribution).
- a ratio of 1 e.g., the normalized routing weight of an egress port with the lowest routing weight
- the normalized routing weight of a given egress port with a ratio of the probability of the egress port with the lowest routing weight being selected (in the weighted joint probability distribution) and a probability of the given egress port being selected (in the weighted joint probability distribution).
- the method 500 includes, at block 510 , receiving, at a data switch with a plurality of egress ports (e.g., in a WCMP group), a data packet.
- the method 500 also includes, at block 520 , for each egress port of the plurality of egress ports, generating, by the data switch, an independent hash value based on one or more fields of the data packet.
- the one or more fields of the data packet can include one or more fields of a header of the data packet, where the one or more fields of the header of the data packet having fixed values for each data packet of a data flow associated with the data packet.
- the respective independent hash values for each of the egress ports may be generated using respective independent hash functions associated with each of the egress ports.
- the respective independent hash functions can be predetermined (coded in software, hardware and/or firmware), assigned by the data switch, assigned by a network administration system or may be determined using a number of other approaches, such as random selection from a pool of available hash function options.
- the method 500 includes, for each egress port of the plurality of egress ports, generating a weighted hash value by scaling the independent hash value produced at block 520 using a scaling factor, such as a scaling factor that is determined using the approaches described herein.
- the method 500 includes selecting an egress port of the plurality of egress ports based on the respective weighted hash values for the plurality of egress ports. For instance, an egress port associated with the largest scaled hash value for the data packet can be selected. In a case where two or more egress ports “tie” for the largest scaled hash value, the egress port with the smallest index can be selected. In other implementations, different criteria may be used to break “ties” for the largest scaled hash value.
- the method 500 includes transmitting the data packet using the egress port selected at block 540 .
- FIG. 6 is a block diagram schematically illustrating a data switch 600 , according to an implementation.
- the data switch 600 which, in this example, has N total egress ports in a WCMP group, can be used to implement the methods 400 and 500 described above.
- the data switch 600 can receive a packet 610 (with a packet header).
- the data switch 600 can then generate N independent hash values (e.g., from one or more header fields of the data packets 610 ) using the independent hash functions 620 , 630 , 640 , where each independent hash value is associated with a specific egress port of the N egress ports of WCMP group.
- the data switch 600 can then scale the independent hash values using respective scaling factors 650 , 660 , 670 for each of the N egress ports.
- the data switch 600 can then select an egress port of the N egress ports on which to transmit the data packet 610 based on the scaled hash values generated from the data packet.
- the data switch 600 can generate the respective independent hash values for the plurality of egress ports by generating a first hash value using the hash function_1 620 , where the first hash value is associated with an egress port 1 (index of 1) of the plurality of egress ports (the WCMP group).
- Generating the respective independent hash values for the plurality of egress ports in the data switch 600 can further include generating a second hash value using the hash function_2 630 , where the second hash value is associated with an egress port 2 (index of 2) of the plurality of egress ports (the WCMP group).
- generating the respective independent hash values for the plurality of egress ports in the data switch 600 can further include generating a third hash value using the hash function_N 640 , where the third hash value is associated with an egress port N (index of N) of the plurality of egress ports.
- the third hash function is different than, and independent of, the first hash function and the second hash function
- the second hash function is different than, and independent of, the first hash function.
- FIG. 7 is a diagram illustrating a joint probability distribution 700 that can be used to determine weighted traffic routing scaling factors, according to an implementation.
- the joint probability distribution 700 can be used to illustrate an approach for iteratively determining scaling factors for a data switch with a WCMP group of two egress ports, P1 and P2 in this example.
- the joint probability distribution 700 illustrates a weighted probability distribution using variables that can be iteratively solved for in order to determine respective scaling factors for each of the egress ports P1 and P2 that will achieve the desired traffic routing weights for a given set of routing weights.
- normalized routing weights of [1, 2] for the ports P1 and P2 will be used, respectively. In other implementations, other routing weights may be used. Also in the following discussion, it is presumed that hash values produced by respective independent hashing functions for each of the egress ports P1, P2 are normalized to have values in a range of 0 to 1, such as has been described herein. In FIG. 2 , the scaling factors for the egress ports P1 and P2 are represented on axes that are designated with their corresponding port numbers. The diagonal line in FIG. 7 divides the joint probability distribution 700 into a first region 710 and a second region 720 .
- the first region 710 (the area of the first region 710 ) represents the probability that a given data packet will be routed on P1 (with a WCMP weight of 1) and the region 720 (the area of the region 720 ) represents the probability that a given data packet will be routed on P2 (with a WCMP weight of 2).
- the scaling factor for egress port P1 can be used as a multiplier for independent hash values generated by an associated hash function.
- the scaling factor for the egress port P1 will be referenced as m[1].
- the area of the first region 710 in the joint probability distribution 700 would be 1 ⁇ 2, the area of the right triangle of the first region 710 . Therefore, the area of the second region 720 and the joint probability distribution 700 would be 11 ⁇ 2 (e.g., the total area of 2 minus the area of the first region 710 ). Therefore, in this situation, the probability of forwarding on the egress port P1 is the probability that the scaled hash value for the egress port P1 is greater than the scaled hash value for the egress port P2. That probability is equal to the ratio of the area of 1 ⁇ 2 of the first region 710 to the total area of the joint probability distribution 700 of 2, thus that probability equals 1 ⁇ 4 (1 ⁇ 2 divided by 2).
- the probability of forwarding on the egress port P2 is the probability that the scaled hash value for the egress port P2 is greater than the scaled hash value for the egress port P1. That probability is equal to the ratio of the area of 11 ⁇ 2 of the second region 720 to the total area of the joint probability distribution 700 of 2, thus that probability equals 3 ⁇ 4 (11 ⁇ 2 divided by 2). Accordingly, the resulting forwarding weights ratio would be 1:3 instead of the desired 1:2 ratio for the normalized desired WCMP routing weights for the ports P1, P2 and result in inaccurate traffic weighting.
- the joint probability distribution 700 may be used to determine scaling factors that accurately achieve the desired WCMP routing weights of [1, 2] using the approach described below.
- the scaled hash values for the egress ports P1, P2 for a given data packet will be referred to, respectively, as H[1] and H[2].
- FIG. 8 is a diagram illustrating another joint probability distribution 800 that can be used to determine weighted traffic routing scaling factors, according to an implementation.
- the joint probability distribution 800 can be used to illustrate an approach for iteratively determining scaling factors for a data switch with a WCMP group of three egress ports, P1, P2 and P3 in this example.
- the joint probability distribution 800 illustrates a weighted probability distribution using variables that can be iteratively solved for in order to determine respective scaling factors for each of the egress ports P1, P2, P3 that will achieve desired traffic routing weights for a given set of routing weights.
- the scaling factor m[1] could be set to a different value and/or egress port P1 could have a WCMP routing weight that is different than 1.
- the joint probability distribution 800 is a rectangular box with edges of 1, 1+a[2], and 1+a[2]+a[3]. Again, it is presumed that the joint probability distribution 800 has a uniform probability density.
- the overall joint probability distribution 800 may be referred to as having a volume of V. As illustrated in FIG. 8 , the volume V includes a first region 810 , a second region 820 , and a third region 830 that make up the total volume V of the joint probability distribution 800 .
- the scaled hash values for the ports P1, P2, P3 corresponding with the joint probability distribution 800 will be respectively referred to as H[1], H[2], H[3].
- the region 810 can be defined as a volume (V[1]) of the joint probability distribution 800 where H[1], H[2], and H[3] are all less than or equal to 1
- the region 820 can be defined as a volume (V[2]) of the joint probability distribution 800 where H[2] and H[3] are both less than or equal to 1+a[2] minus the volume V[1] of the region 810 .
- the region 830 can be defined as the volume (V[3]) of the joint probability distribution 800 where H[3] is less than or equal to 1+a[2]+a[3] minus the volume V[1] and the volume V[2].
- the probability of each of the scaled hash values H[1], H[2], H[3] being the maximum of the three values is equal, or 1 ⁇ 3.
- the probability of H[1] being the maximum is 0, while the probability of H[2] or H[3] being the maximum is equal or 1 ⁇ 2.
- the probability of H[1] or H[2] being the maximum is 0, while the probability of H[3] being the maximum is 1, as H[3] has the only scaled hash values in the region 830 .
- the union of the regions 810 , 820 , 830 covers the probability space of volume V. Accordingly the volumes of each region may be used to iteratively determine the scaling factors for each of the associated egress ports. Again, in this example, the scaling factor m[1] for the egress port P1 can be fixed at a value of 1.
- the concepts discussed above may be extended to implementations where a WCMP group includes more than three egress ports.
- the WCMP routing weights may be normalized to a have the lowest routing weight of 1 and to also have a routing weight of 1 be the lowest routing weight for an associated WCMP group. For instance, for a data switch that has N egress ports in a given ECMP groups, the joint probability distribution would be N-dimensional.
- the volumes for each region of an associated joint (uniform) probability distribution may be determined by the following equations, where i is the port index for an egress port that is associated with the volume being calculated and the values of i range from 1 to N (the total number of egress ports in the ECMP group) and R i is the region of the joint probability distribution associated with the ith egress port.
- V ( R 1 ) a[ 1] N
- V ( R 2 ) a[ 1] ⁇ ( a[ 1]+ a[ 2]) N-1 ⁇ V ( R 1 )
- V ( R 3 ) a[ 1] ⁇ ( a[ 1]+ a[ 2]) ⁇ ( a[ 1]+ a[ 2]+ a[ 3]) N-2 ⁇ V ( R 2 ) ⁇ V ( R 1 ), etc.
- variables a[i] can be iteratively determined in succession, with each successive calculation solving one equation for one unknown by equating the ratio of desired routing weights for two given egress ports with a corresponding ratio of the volumes in the joint probability distribution associated with the given egress ports, such as discussed above with respect to FIG. 8 .
- probability space volumes for each region of a joint probability distribution for a WCMP group with N members may be expressed as, for the ith egress port:
- the variables a[k] can be determined by iteratively solving the foregoing equations for each egress port in order of their indices, e.g., P1, then P2, then P3, and so forth by equating them with corresponding ratios of the desired WCMP routing weights for the corresponding egress ports.
- Implementations of the various techniques described herein may be implemented in digital electronic circuitry, or in computer hardware, firmware, software, or in combinations of them. Implementations may be implemented as a computer program product, i.e., a computer program tangibly embodied in an information carrier, e.g., in a machine-readable storage device, for execution by, or to control the operation of, data processing apparatus, e.g., a programmable processor, a computer, or multiple computers.
- a computer program such as the computer program(s) described above, can be written in any form of programming language, including compiled or interpreted languages, and can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.
- a computer program can be deployed to be executed on one computer (or device) or on multiple computers (or devices) at one site or distributed across multiple sites and interconnected by a communication network.
- Method steps may be performed by one or more programmable processors executing a computer program to perform functions by operating on input data and generating output. Method steps also may be performed by, and an apparatus may be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
- FPGA field programmable gate array
- ASIC application-specific integrated circuit
- processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer or device.
- a processor will receive instructions and data from a read-only memory or a random access memory or both.
- Elements of a computer or device may include at least one processor for executing instructions and one or more memory devices for storing instructions and data.
- a computer or device also may include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks.
- Information carriers suitable for embodying computer program instructions and data include all forms of non-volatile memory, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks.
- semiconductor memory devices e.g., EPROM, EEPROM, and flash memory devices
- magnetic disks e.g., internal hard disks or removable disks
- magneto-optical disks e.g., CD-ROM and DVD-ROM disks.
- the processor and the memory may be supplemented by, or incorporated in special purpose logic circuitry.
- implementations may be implemented on a computer having a display device, e.g., a cathode ray tube (CRT) or liquid crystal display (LCD) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a cathode ray tube (CRT) or liquid crystal display (LCD) monitor
- keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.
- Implementations may be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front-end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation, or any combination of such back-end, middleware, or front-end components.
- Components may be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (LAN) and a wide area network (WAN), e.g., the Internet.
- LAN local area network
- WAN wide area network
Abstract
Description
Lookup value=Hash_value mod 9
Area=a[1]·(1+a[2])=m[1]·m[2]=1·2=2
-
- MAX1: max(H[1],H[2],H[3])=H(1)
- MAX2: max(H[1],H[2],H[3])=H(2)
- MAX3: max(H[1],H[2],H[3])=H(3).
Volume(region 810)=V[1]=1,
Volume(region 820)=V[2]=(1)(1+a[2])(1+a[2])−V[1]=(1+a[2])2−1,
Volume(region 830)=V[3]=(1)(1+a[2])(1+a[1]+a[2])−V[1]−V[2]=(1+a[2])(a[3]).
V(R 1)=a[1]N,
V(R 2)=a[1]·(a[1]+a[2])N-1 −V(R 1),
V(R 3)=a[1]·(a[1]+a[2])·(a[1]+a[2]+a[3])N-2 −V(R 2)−V(R 1), etc.
The variables a[i] can be iteratively determined in succession, with each successive calculation solving one equation for one unknown by equating the ratio of desired routing weights for two given egress ports with a corresponding ratio of the volumes in the joint probability distribution associated with the given egress ports, such as discussed above with respect to
V(R i)=Πj=1 i(Σk=1 j a[k]))×(Σk=1 i a[k])N-i−(Σk=1 i-1(R K))
And for the Nth egress port:
V(R N)=(Πj=1 N(Σk=1 j a[k]))−(Σk=1 N-1 V(R K))
As in the previous examples, the variables a[k] can be determined by iteratively solving the foregoing equations for each egress port in order of their indices, e.g., P1, then P2, then P3, and so forth by equating them with corresponding ratios of the desired WCMP routing weights for the corresponding egress ports. Once the variables a[k] are determined, the scaling factors for scaling the independent hash values for each egress port may be readily determined by the general equation:
m[i]=Σ j=1 i a[j]
Further, egress ports that have equal routing weights for a given set of routing weights will have equal scaling factors.
Claims (10)
Priority Applications (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/217,921 US9565114B1 (en) | 2014-03-08 | 2014-03-18 | Weighted load balancing using scaled parallel hashing |
US15/396,512 US11075986B2 (en) | 2014-03-08 | 2016-12-31 | Weighted load balancing using scaled parallel hashing |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201461950024P | 2014-03-08 | 2014-03-08 | |
US14/217,921 US9565114B1 (en) | 2014-03-08 | 2014-03-18 | Weighted load balancing using scaled parallel hashing |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/396,512 Continuation US11075986B2 (en) | 2014-03-08 | 2016-12-31 | Weighted load balancing using scaled parallel hashing |
Publications (1)
Publication Number | Publication Date |
---|---|
US9565114B1 true US9565114B1 (en) | 2017-02-07 |
Family
ID=57908905
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US14/217,921 Active 2034-09-22 US9565114B1 (en) | 2014-03-08 | 2014-03-18 | Weighted load balancing using scaled parallel hashing |
US15/396,512 Active US11075986B2 (en) | 2014-03-08 | 2016-12-31 | Weighted load balancing using scaled parallel hashing |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/396,512 Active US11075986B2 (en) | 2014-03-08 | 2016-12-31 | Weighted load balancing using scaled parallel hashing |
Country Status (1)
Country | Link |
---|---|
US (2) | US9565114B1 (en) |
Cited By (35)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20170149877A1 (en) * | 2014-03-08 | 2017-05-25 | Google Inc. | Weighted load balancing using scaled parallel hashing |
US10063479B2 (en) | 2014-10-06 | 2018-08-28 | Barefoot Networks, Inc. | Fast adjusting load balancer |
US10063407B1 (en) | 2016-02-08 | 2018-08-28 | Barefoot Networks, Inc. | Identifying and marking failed egress links in data plane |
US10084687B1 (en) * | 2016-11-17 | 2018-09-25 | Barefoot Networks, Inc. | Weighted-cost multi-pathing using range lookups |
US10158573B1 (en) | 2017-05-01 | 2018-12-18 | Barefoot Networks, Inc. | Forwarding element with a data plane load balancer |
US10237206B1 (en) | 2017-03-05 | 2019-03-19 | Barefoot Networks, Inc. | Equal cost multiple path group failover for multicast |
US10268634B1 (en) | 2014-10-06 | 2019-04-23 | Barefoot Networks, Inc. | Proxy hash table |
US20190140956A1 (en) * | 2016-07-19 | 2019-05-09 | Huawei Technologies Co., Ltd. | Load balancing method, apparatus, and device |
US10313231B1 (en) * | 2016-02-08 | 2019-06-04 | Barefoot Networks, Inc. | Resilient hashing for forwarding packets |
US20190222251A1 (en) * | 2014-08-12 | 2019-07-18 | Qorvo Us, Inc. | Switchable rf transmit/receive multiplexer |
US10374956B1 (en) * | 2015-09-25 | 2019-08-06 | Amazon Technologies, Inc. | Managing a hierarchical network |
US10404619B1 (en) | 2017-03-05 | 2019-09-03 | Barefoot Networks, Inc. | Link aggregation group failover for multicast |
CN110518921A (en) * | 2018-05-21 | 2019-11-29 | 北京松果电子有限公司 | The method, apparatus and storage medium and electronic equipment of multi-path combing |
CN110731070A (en) * | 2017-03-29 | 2020-01-24 | 芬基波尔有限责任公司 | Non-blocking arbitrary to arbitrary data center networks with grouped injection via multiple alternate data paths |
US10666552B2 (en) * | 2016-02-12 | 2020-05-26 | Univeristy-Industry Cooperation Group Of Kyung Hee University | Apparatus for forwarding interest in parallel using multipath in content-centric networking and method thereof |
CN111726299A (en) * | 2019-03-18 | 2020-09-29 | 华为技术有限公司 | Flow balancing method and device |
US10863009B2 (en) * | 2013-09-16 | 2020-12-08 | Amazon Technologies, Inc. | Generic data integrity check |
US11044204B1 (en) | 2016-01-30 | 2021-06-22 | Innovium, Inc. | Visibility packets with inflated latency |
US11057307B1 (en) * | 2016-03-02 | 2021-07-06 | Innovium, Inc. | Load balancing path assignments techniques |
CN113111070A (en) * | 2021-05-08 | 2021-07-13 | 福建天晴数码有限公司 | Method and system for performing database and table division based on grouping |
US11075847B1 (en) | 2017-01-16 | 2021-07-27 | Innovium, Inc. | Visibility sampling |
CN113206783A (en) * | 2020-01-31 | 2021-08-03 | 安华高科技股份有限公司 | Method and apparatus for weighted cost multi-path packet processing |
US11178262B2 (en) | 2017-09-29 | 2021-11-16 | Fungible, Inc. | Fabric control protocol for data center networks with packet spraying over multiple alternate data paths |
US11303472B2 (en) | 2017-07-10 | 2022-04-12 | Fungible, Inc. | Data processing unit for compute nodes and storage nodes |
US11334546B2 (en) * | 2019-05-31 | 2022-05-17 | Cisco Technology, Inc. | Selecting interfaces for device-group identifiers |
US11360895B2 (en) | 2017-04-10 | 2022-06-14 | Fungible, Inc. | Relay consistent memory management in a multiple processor system |
US11469922B2 (en) | 2017-03-29 | 2022-10-11 | Fungible, Inc. | Data center network with multiplexed communication of data packets across servers |
US11601359B2 (en) | 2017-09-29 | 2023-03-07 | Fungible, Inc. | Resilient network communication using selective multipath packet flow spraying |
US11621904B1 (en) | 2020-11-06 | 2023-04-04 | Innovium, Inc. | Path telemetry data collection |
US11632606B2 (en) | 2017-03-29 | 2023-04-18 | Fungible, Inc. | Data center network having optical permutors |
US11665104B1 (en) | 2017-01-16 | 2023-05-30 | Innovium, Inc. | Delay-based tagging in a network switch |
US11734179B2 (en) | 2018-02-02 | 2023-08-22 | Fungible, Inc. | Efficient work unit processing in a multicore system |
US11784932B2 (en) | 2020-11-06 | 2023-10-10 | Innovium, Inc. | Delay-based automatic queue management and tail drop |
US11842216B2 (en) | 2017-07-10 | 2023-12-12 | Microsoft Technology Licensing, Llc | Data processing unit for stream processing |
US11968129B1 (en) | 2023-04-28 | 2024-04-23 | Innovium, Inc. | Delay-based tagging in a network switch |
Families Citing this family (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10182017B2 (en) | 2016-06-30 | 2019-01-15 | Mellanox Technologies Tlv Ltd. | Estimating multiple distinct-flow counts in parallel |
US10218642B2 (en) * | 2017-03-27 | 2019-02-26 | Mellanox Technologies Tlv Ltd. | Switch arbitration based on distinct-flow counts |
GB2573573B (en) * | 2018-05-11 | 2022-08-17 | Cambridge Broadband Networks Group Ltd | A system and method for distributing packets in a network |
US11863613B1 (en) * | 2021-03-22 | 2024-01-02 | Amazon Technologies, Inc. | Allocation of workloads in dynamic worker fleet |
US11956144B1 (en) | 2023-08-15 | 2024-04-09 | Bank Of America Corporation | Systems and methods for network traffic routing and load balancing in an electronic network |
Citations (40)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6356546B1 (en) * | 1998-08-11 | 2002-03-12 | Nortel Networks Limited | Universal transfer method and network with distributed switch |
US20030016666A1 (en) * | 2001-07-18 | 2003-01-23 | Nec Corporation | Packet distributing apparatus and packet distributing method |
US20030105976A1 (en) * | 2000-11-30 | 2003-06-05 | Copeland John A. | Flow-based detection of network intrusions |
US6580721B1 (en) * | 1998-08-11 | 2003-06-17 | Nortel Networks Limited | Routing and rate control in a universal transfer mode network |
US6721800B1 (en) * | 2000-04-10 | 2004-04-13 | International Business Machines Corporation | System using weighted next hop option in routing table to include probability of routing a packet for providing equal cost multipath forwarding packets |
US6765866B1 (en) * | 2000-02-29 | 2004-07-20 | Mosaid Technologies, Inc. | Link aggregation |
US20060178153A1 (en) * | 2005-02-08 | 2006-08-10 | Tenny Nathan E | Method and apparatus for allocating resources in a multicast/broadcast communications system |
US20070288760A1 (en) * | 2003-12-15 | 2007-12-13 | Pitney Bowes Inc. | Method For Mail Address Block Image Information Encoding, Protection And Recovery In Postal Payment Applications |
US7523218B1 (en) * | 2002-04-30 | 2009-04-21 | University Of Florida Research Foundation, Inc. | O(log n) dynamic router tables for prefixes and ranges |
US20090116488A1 (en) * | 2003-06-03 | 2009-05-07 | Nokia Siemens Networks Gmbh & Co Kg | Method for distributing traffic by means of hash codes according to a nominal traffic distribution scheme in a packet-oriented network employing multi-path routing |
US20090268674A1 (en) * | 2008-04-23 | 2009-10-29 | Honeywell International Inc. | Apparatus and method for medium access control in wireless communication networks |
US7697432B2 (en) * | 2003-06-27 | 2010-04-13 | Broadcom Corporation | Equal and weighted cost multipath load balancing in a network device |
US7843926B1 (en) * | 2005-04-05 | 2010-11-30 | Oracle America, Inc. | System for providing virtualization of network interfaces at various layers |
US20100302935A1 (en) * | 2009-05-27 | 2010-12-02 | Yin Zhang | Method and system for resilient routing reconfiguration |
US20110013638A1 (en) * | 2009-07-14 | 2011-01-20 | Broadcom Corporation | Node based path selection randomization |
US20110013639A1 (en) * | 2009-07-14 | 2011-01-20 | Broadcom Corporation | Flow based path selection randomization using parallel hash functions |
US7898959B1 (en) * | 2007-06-28 | 2011-03-01 | Marvell Israel (Misl) Ltd. | Method for weighted load-balancing among network interfaces |
US20110161657A1 (en) * | 2009-12-31 | 2011-06-30 | Verizon Patent And Licensing Inc. | Method and system for providing traffic hashing and network level security |
US20110188503A1 (en) * | 2008-08-13 | 2011-08-04 | Gnodal Limited | Ethernet Forwarding Database Method |
US20110202612A1 (en) * | 2010-02-12 | 2011-08-18 | Jeffrey Alan Craig | Methods, systems, and computer readable media for providing origin routing at a diameter node |
US20110310739A1 (en) * | 2010-06-22 | 2011-12-22 | Gunes Aybay | Methods and apparatus for virtual channel flow control associated with a switch fabric |
US20120170575A1 (en) * | 2010-12-29 | 2012-07-05 | Juniper Networks, Inc. | Methods and apparatus for validation of equal cost multi path (ecmp) paths in a switch fabric system |
US20130201989A1 (en) * | 2012-02-08 | 2013-08-08 | Radisys Corporation | Stateless load balancer in a multi-node system for transparent processing with packet preservation |
US20130232104A1 (en) * | 2011-08-02 | 2013-09-05 | Cavium, Inc. | Duplication in decision trees |
US20130308455A1 (en) * | 2012-05-21 | 2013-11-21 | Cisco Technology, Inc. | Methods and apparatus for load balancing across member ports for traffic egressing out of a port channel |
US20130329584A1 (en) * | 2012-06-06 | 2013-12-12 | Tirthankar Ghose | Finding latency through a physical network in a virtualized network |
US20140032156A1 (en) * | 2012-07-30 | 2014-01-30 | Synopsys, Inc. | Layout-aware test pattern generation and fault detection |
US20140040433A1 (en) * | 2012-08-02 | 2014-02-06 | Talksum, Inc. | Mechanism for facilitating real-time streaming, filtering and routing of data |
US20140181298A1 (en) * | 2012-12-20 | 2014-06-26 | International Business Machines Corporation | Method and apparatus for managing a plurality of sessions in a multi-path routing based network |
US20140198656A1 (en) * | 2013-01-15 | 2014-07-17 | Brocade Communications Systems, Inc. | Adaptive link aggregation and virtual link aggregation |
US20140301394A1 (en) * | 2013-04-04 | 2014-10-09 | Marvell Israel (M.I.S.L) Ltd. | Exact match hash lookup databases in network switch devices |
US8982700B1 (en) * | 2012-01-27 | 2015-03-17 | Google Inc. | System and method for minimizing hardware resources for given performance using weighted cost multi-path flow distribution |
US20150078375A1 (en) * | 2013-09-13 | 2015-03-19 | Broadcom Corporation | Mutable Hash for Network Hash Polarization |
US20150163140A1 (en) * | 2013-12-09 | 2015-06-11 | Edward J. Rovner | Method and system for dynamic usage of multiple tables for internet protocol hosts |
US20150180782A1 (en) * | 2013-12-24 | 2015-06-25 | Todd Rimmer | Method, apparatus and system for qos within high performance fabrics |
US20150180790A1 (en) * | 2013-12-20 | 2015-06-25 | Todd Rimmer | Method and system for flexible credit exchange within high performance fabrics |
US20150222533A1 (en) * | 2014-02-05 | 2015-08-06 | Intel Corporation | Transport of ethernet packet data with wire-speed and packet data rate match |
US20150244617A1 (en) * | 2012-06-06 | 2015-08-27 | Juniper Networks, Inc. | Physical path determination for virtual network packet flows |
US20150326476A1 (en) * | 2014-05-12 | 2015-11-12 | Google Inc. | Prefix-aware weighted cost multi-path group reduction |
US20160050272A1 (en) * | 2014-08-12 | 2016-02-18 | Eingot Llc | Zero-knowledge environment based social networking engine |
Family Cites Families (26)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7246148B1 (en) * | 1995-09-29 | 2007-07-17 | Cisco Technology, Inc. | Enhanced network services using a subnetwork of communicating processors |
US6888797B1 (en) * | 1999-05-05 | 2005-05-03 | Lucent Technologies Inc. | Hashing-based network load balancing |
US6757742B1 (en) * | 2000-05-25 | 2004-06-29 | Advanced Micro Devices, Inc. | Computer-based system for validating hash-based table lookup schemes in a network switch |
FR2857194B1 (en) * | 2003-07-04 | 2005-09-09 | Thales Sa | METHOD FOR TRANSMITTING ADDITIONAL INFORMATION BY HEADER COMPRESSION |
JP4341413B2 (en) * | 2003-07-11 | 2009-10-07 | 株式会社日立製作所 | PACKET TRANSFER APPARATUS HAVING STATISTICS COLLECTION APPARATUS AND STATISTICS COLLECTION METHOD |
US7957372B2 (en) * | 2004-07-22 | 2011-06-07 | International Business Machines Corporation | Automatically detecting distributed port scans in computer networks |
WO2007052477A1 (en) * | 2005-11-04 | 2007-05-10 | Nec Corporation | Message authentication device, message authentication method, message authentication program, and recording medium therefor |
US8344853B1 (en) * | 2006-05-16 | 2013-01-01 | Eigent Technologies, Llc | Secure RFID system and method |
US7930560B2 (en) * | 2007-07-17 | 2011-04-19 | Kabushiki Kaisha Oricom | Personal information management system, personal information management program, and personal information protecting method |
US7818303B2 (en) * | 2008-01-29 | 2010-10-19 | Microsoft Corporation | Web graph compression through scalable pattern mining |
EP2244243B1 (en) * | 2008-02-20 | 2017-12-13 | Mitsubishi Electric Corporation | Verifying device |
US8385960B2 (en) * | 2008-03-26 | 2013-02-26 | Koninklijke Philips Electronics N.V. | Method for communicating in mobile system |
US8111649B1 (en) * | 2008-03-31 | 2012-02-07 | Google Inc. | Method and apparatus for enabling a host to influence how a packet is routed through a network |
US8521751B2 (en) * | 2008-08-22 | 2013-08-27 | Nec Corporation | Search device, a search method and a program |
US8259585B1 (en) * | 2009-04-17 | 2012-09-04 | Juniper Networks, Inc. | Dynamic link load balancing |
JP4883160B2 (en) * | 2009-09-30 | 2012-02-22 | 富士通株式会社 | Communication apparatus and frame transmission method |
RU2556457C2 (en) * | 2010-05-28 | 2015-07-10 | Нек Корпорейшн | Communication system, node, control device, communication method and program |
US8446910B2 (en) * | 2011-04-14 | 2013-05-21 | Cisco Technology, Inc. | Methods for even hash distribution for port channel with a large number of ports |
US9143449B2 (en) * | 2012-07-31 | 2015-09-22 | Cisco Technology, Inc. | Methods and apparatuses for improving database search performance |
JP5930056B2 (en) * | 2012-09-27 | 2016-06-08 | 日本電気株式会社 | Binary data conversion method, apparatus and program |
CN104838620B (en) * | 2012-10-17 | 2018-05-11 | 瑞典爱立信有限公司 | The apparatus and method of incident management in telecommunications network |
US9602331B2 (en) * | 2012-10-31 | 2017-03-21 | Cisco Technology, Inc. | Shared interface among multiple compute units |
WO2014117843A1 (en) * | 2013-01-31 | 2014-08-07 | Telefonaktiebolaget L M Ericsson (Publ) | Method and firewall for soliciting incoming packets |
US9154934B2 (en) * | 2013-03-28 | 2015-10-06 | Futurewei Technologies, Inc. | System and method for pre-association discovery |
JPWO2015025845A1 (en) * | 2013-08-20 | 2017-03-02 | 日本電気株式会社 | Communication system, switch, controller, ancillary data management apparatus, data transfer method and program |
US9565114B1 (en) * | 2014-03-08 | 2017-02-07 | Google Inc. | Weighted load balancing using scaled parallel hashing |
-
2014
- 2014-03-18 US US14/217,921 patent/US9565114B1/en active Active
-
2016
- 2016-12-31 US US15/396,512 patent/US11075986B2/en active Active
Patent Citations (40)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6580721B1 (en) * | 1998-08-11 | 2003-06-17 | Nortel Networks Limited | Routing and rate control in a universal transfer mode network |
US6356546B1 (en) * | 1998-08-11 | 2002-03-12 | Nortel Networks Limited | Universal transfer method and network with distributed switch |
US6765866B1 (en) * | 2000-02-29 | 2004-07-20 | Mosaid Technologies, Inc. | Link aggregation |
US6721800B1 (en) * | 2000-04-10 | 2004-04-13 | International Business Machines Corporation | System using weighted next hop option in routing table to include probability of routing a packet for providing equal cost multipath forwarding packets |
US20030105976A1 (en) * | 2000-11-30 | 2003-06-05 | Copeland John A. | Flow-based detection of network intrusions |
US20030016666A1 (en) * | 2001-07-18 | 2003-01-23 | Nec Corporation | Packet distributing apparatus and packet distributing method |
US7523218B1 (en) * | 2002-04-30 | 2009-04-21 | University Of Florida Research Foundation, Inc. | O(log n) dynamic router tables for prefixes and ranges |
US20090116488A1 (en) * | 2003-06-03 | 2009-05-07 | Nokia Siemens Networks Gmbh & Co Kg | Method for distributing traffic by means of hash codes according to a nominal traffic distribution scheme in a packet-oriented network employing multi-path routing |
US7697432B2 (en) * | 2003-06-27 | 2010-04-13 | Broadcom Corporation | Equal and weighted cost multipath load balancing in a network device |
US20070288760A1 (en) * | 2003-12-15 | 2007-12-13 | Pitney Bowes Inc. | Method For Mail Address Block Image Information Encoding, Protection And Recovery In Postal Payment Applications |
US20060178153A1 (en) * | 2005-02-08 | 2006-08-10 | Tenny Nathan E | Method and apparatus for allocating resources in a multicast/broadcast communications system |
US7843926B1 (en) * | 2005-04-05 | 2010-11-30 | Oracle America, Inc. | System for providing virtualization of network interfaces at various layers |
US7898959B1 (en) * | 2007-06-28 | 2011-03-01 | Marvell Israel (Misl) Ltd. | Method for weighted load-balancing among network interfaces |
US20090268674A1 (en) * | 2008-04-23 | 2009-10-29 | Honeywell International Inc. | Apparatus and method for medium access control in wireless communication networks |
US20110188503A1 (en) * | 2008-08-13 | 2011-08-04 | Gnodal Limited | Ethernet Forwarding Database Method |
US20100302935A1 (en) * | 2009-05-27 | 2010-12-02 | Yin Zhang | Method and system for resilient routing reconfiguration |
US20110013638A1 (en) * | 2009-07-14 | 2011-01-20 | Broadcom Corporation | Node based path selection randomization |
US20110013639A1 (en) * | 2009-07-14 | 2011-01-20 | Broadcom Corporation | Flow based path selection randomization using parallel hash functions |
US20110161657A1 (en) * | 2009-12-31 | 2011-06-30 | Verizon Patent And Licensing Inc. | Method and system for providing traffic hashing and network level security |
US20110202612A1 (en) * | 2010-02-12 | 2011-08-18 | Jeffrey Alan Craig | Methods, systems, and computer readable media for providing origin routing at a diameter node |
US20110310739A1 (en) * | 2010-06-22 | 2011-12-22 | Gunes Aybay | Methods and apparatus for virtual channel flow control associated with a switch fabric |
US20120170575A1 (en) * | 2010-12-29 | 2012-07-05 | Juniper Networks, Inc. | Methods and apparatus for validation of equal cost multi path (ecmp) paths in a switch fabric system |
US20130232104A1 (en) * | 2011-08-02 | 2013-09-05 | Cavium, Inc. | Duplication in decision trees |
US8982700B1 (en) * | 2012-01-27 | 2015-03-17 | Google Inc. | System and method for minimizing hardware resources for given performance using weighted cost multi-path flow distribution |
US20130201989A1 (en) * | 2012-02-08 | 2013-08-08 | Radisys Corporation | Stateless load balancer in a multi-node system for transparent processing with packet preservation |
US20130308455A1 (en) * | 2012-05-21 | 2013-11-21 | Cisco Technology, Inc. | Methods and apparatus for load balancing across member ports for traffic egressing out of a port channel |
US20130329584A1 (en) * | 2012-06-06 | 2013-12-12 | Tirthankar Ghose | Finding latency through a physical network in a virtualized network |
US20150244617A1 (en) * | 2012-06-06 | 2015-08-27 | Juniper Networks, Inc. | Physical path determination for virtual network packet flows |
US20140032156A1 (en) * | 2012-07-30 | 2014-01-30 | Synopsys, Inc. | Layout-aware test pattern generation and fault detection |
US20140040433A1 (en) * | 2012-08-02 | 2014-02-06 | Talksum, Inc. | Mechanism for facilitating real-time streaming, filtering and routing of data |
US20140181298A1 (en) * | 2012-12-20 | 2014-06-26 | International Business Machines Corporation | Method and apparatus for managing a plurality of sessions in a multi-path routing based network |
US20140198656A1 (en) * | 2013-01-15 | 2014-07-17 | Brocade Communications Systems, Inc. | Adaptive link aggregation and virtual link aggregation |
US20140301394A1 (en) * | 2013-04-04 | 2014-10-09 | Marvell Israel (M.I.S.L) Ltd. | Exact match hash lookup databases in network switch devices |
US20150078375A1 (en) * | 2013-09-13 | 2015-03-19 | Broadcom Corporation | Mutable Hash for Network Hash Polarization |
US20150163140A1 (en) * | 2013-12-09 | 2015-06-11 | Edward J. Rovner | Method and system for dynamic usage of multiple tables for internet protocol hosts |
US20150180790A1 (en) * | 2013-12-20 | 2015-06-25 | Todd Rimmer | Method and system for flexible credit exchange within high performance fabrics |
US20150180782A1 (en) * | 2013-12-24 | 2015-06-25 | Todd Rimmer | Method, apparatus and system for qos within high performance fabrics |
US20150222533A1 (en) * | 2014-02-05 | 2015-08-06 | Intel Corporation | Transport of ethernet packet data with wire-speed and packet data rate match |
US20150326476A1 (en) * | 2014-05-12 | 2015-11-12 | Google Inc. | Prefix-aware weighted cost multi-path group reduction |
US20160050272A1 (en) * | 2014-08-12 | 2016-02-18 | Eingot Llc | Zero-knowledge environment based social networking engine |
Non-Patent Citations (3)
Title |
---|
Al-Fares, Mohammad, et al. A Scalable, Commodity Data Center Network Architecture, ACM SIGCOMM Computer Communication Review, vol. 38. No. 4, pp. 63-74, ACM, 2008. |
Liu, et al, "zUpdate: Updating Data Center Networks with Zero Loss", SIGCOMM'13, Aug. 12-16, 2013, 12 pages. |
Zhou, Junlan, et al. WCMP: Weighted Cost Multipathing for Improved Fairness in Data Centers, Proceedings of the Ninth European Conference on Computer Systems. ACM, Apr. 13-16, 2014. |
Cited By (63)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11445051B2 (en) | 2013-09-16 | 2022-09-13 | Amazon Technologies, Inc. | Configurable parser and a method for parsing information units |
US10863009B2 (en) * | 2013-09-16 | 2020-12-08 | Amazon Technologies, Inc. | Generic data integrity check |
US11677866B2 (en) | 2013-09-16 | 2023-06-13 | Amazon Technologies. Inc. | Configurable parser and a method for parsing information units |
US20170149877A1 (en) * | 2014-03-08 | 2017-05-25 | Google Inc. | Weighted load balancing using scaled parallel hashing |
US11075986B2 (en) * | 2014-03-08 | 2021-07-27 | Google Llc | Weighted load balancing using scaled parallel hashing |
US20230370109A1 (en) * | 2014-08-12 | 2023-11-16 | Qorvo Us, Inc. | Switchable rf transmit/receive multiplexer |
US20190222251A1 (en) * | 2014-08-12 | 2019-07-18 | Qorvo Us, Inc. | Switchable rf transmit/receive multiplexer |
US10063479B2 (en) | 2014-10-06 | 2018-08-28 | Barefoot Networks, Inc. | Fast adjusting load balancer |
US10268634B1 (en) | 2014-10-06 | 2019-04-23 | Barefoot Networks, Inc. | Proxy hash table |
US11080252B1 (en) | 2014-10-06 | 2021-08-03 | Barefoot Networks, Inc. | Proxy hash table |
US10374956B1 (en) * | 2015-09-25 | 2019-08-06 | Amazon Technologies, Inc. | Managing a hierarchical network |
US11863458B1 (en) | 2016-01-30 | 2024-01-02 | Innovium, Inc. | Reflected packets |
US11044204B1 (en) | 2016-01-30 | 2021-06-22 | Innovium, Inc. | Visibility packets with inflated latency |
US10313231B1 (en) * | 2016-02-08 | 2019-06-04 | Barefoot Networks, Inc. | Resilient hashing for forwarding packets |
US20210194800A1 (en) * | 2016-02-08 | 2021-06-24 | Barefoot Networks, Inc. | Resilient hashing for forwarding packets |
US10063407B1 (en) | 2016-02-08 | 2018-08-28 | Barefoot Networks, Inc. | Identifying and marking failed egress links in data plane |
US11811902B2 (en) * | 2016-02-08 | 2023-11-07 | Barefoot Networks, Inc. | Resilient hashing for forwarding packets |
US11310099B2 (en) | 2016-02-08 | 2022-04-19 | Barefoot Networks, Inc. | Identifying and marking failed egress links in data plane |
US10666552B2 (en) * | 2016-02-12 | 2020-05-26 | Univeristy-Industry Cooperation Group Of Kyung Hee University | Apparatus for forwarding interest in parallel using multipath in content-centric networking and method thereof |
US11736388B1 (en) | 2016-03-02 | 2023-08-22 | Innovium, Inc. | Load balancing path assignments techniques |
US11057307B1 (en) * | 2016-03-02 | 2021-07-06 | Innovium, Inc. | Load balancing path assignments techniques |
US11134014B2 (en) * | 2016-07-19 | 2021-09-28 | Huawei Technologies Co., Ltd. | Load balancing method, apparatus, and device |
US20190140956A1 (en) * | 2016-07-19 | 2019-05-09 | Huawei Technologies Co., Ltd. | Load balancing method, apparatus, and device |
US10084687B1 (en) * | 2016-11-17 | 2018-09-25 | Barefoot Networks, Inc. | Weighted-cost multi-pathing using range lookups |
US10791046B2 (en) | 2016-11-17 | 2020-09-29 | Barefoot Networks, Inc. | Weighted-cost multi-pathing using range lookups |
US20190190816A1 (en) * | 2016-11-17 | 2019-06-20 | Barefoot Networks, Inc. | Weighted-cost multi-pathing using range lookups |
US11665104B1 (en) | 2017-01-16 | 2023-05-30 | Innovium, Inc. | Delay-based tagging in a network switch |
US11855901B1 (en) | 2017-01-16 | 2023-12-26 | Innovium, Inc. | Visibility sampling |
US11075847B1 (en) | 2017-01-16 | 2021-07-27 | Innovium, Inc. | Visibility sampling |
US10728173B1 (en) | 2017-03-05 | 2020-07-28 | Barefoot Networks, Inc. | Equal cost multiple path group failover for multicast |
US10404619B1 (en) | 2017-03-05 | 2019-09-03 | Barefoot Networks, Inc. | Link aggregation group failover for multicast |
US10237206B1 (en) | 2017-03-05 | 2019-03-19 | Barefoot Networks, Inc. | Equal cost multiple path group failover for multicast |
US11271869B1 (en) | 2017-03-05 | 2022-03-08 | Barefoot Networks, Inc. | Link aggregation group failover for multicast |
US11716291B1 (en) | 2017-03-05 | 2023-08-01 | Barefoot Networks, Inc. | Link aggregation group failover for multicast |
US11632606B2 (en) | 2017-03-29 | 2023-04-18 | Fungible, Inc. | Data center network having optical permutors |
US11469922B2 (en) | 2017-03-29 | 2022-10-11 | Fungible, Inc. | Data center network with multiplexed communication of data packets across servers |
US11777839B2 (en) | 2017-03-29 | 2023-10-03 | Microsoft Technology Licensing, Llc | Data center network with packet spraying |
CN110731070A (en) * | 2017-03-29 | 2020-01-24 | 芬基波尔有限责任公司 | Non-blocking arbitrary to arbitrary data center networks with grouped injection via multiple alternate data paths |
US11360895B2 (en) | 2017-04-10 | 2022-06-14 | Fungible, Inc. | Relay consistent memory management in a multiple processor system |
US11809321B2 (en) | 2017-04-10 | 2023-11-07 | Microsoft Technology Licensing, Llc | Memory management in a multiple processor system |
US10530694B1 (en) | 2017-05-01 | 2020-01-07 | Barefoot Networks, Inc. | Forwarding element with a data plane load balancer |
US10158573B1 (en) | 2017-05-01 | 2018-12-18 | Barefoot Networks, Inc. | Forwarding element with a data plane load balancer |
US11303472B2 (en) | 2017-07-10 | 2022-04-12 | Fungible, Inc. | Data processing unit for compute nodes and storage nodes |
US11546189B2 (en) | 2017-07-10 | 2023-01-03 | Fungible, Inc. | Access node for data centers |
US11842216B2 (en) | 2017-07-10 | 2023-12-12 | Microsoft Technology Licensing, Llc | Data processing unit for stream processing |
US11824683B2 (en) | 2017-07-10 | 2023-11-21 | Microsoft Technology Licensing, Llc | Data processing unit for compute nodes and storage nodes |
US11601359B2 (en) | 2017-09-29 | 2023-03-07 | Fungible, Inc. | Resilient network communication using selective multipath packet flow spraying |
US11178262B2 (en) | 2017-09-29 | 2021-11-16 | Fungible, Inc. | Fabric control protocol for data center networks with packet spraying over multiple alternate data paths |
US11412076B2 (en) | 2017-09-29 | 2022-08-09 | Fungible, Inc. | Network access node virtual fabrics configured dynamically over an underlay network |
US11734179B2 (en) | 2018-02-02 | 2023-08-22 | Fungible, Inc. | Efficient work unit processing in a multicore system |
CN110518921B (en) * | 2018-05-21 | 2021-05-04 | 北京小米松果电子有限公司 | Method, apparatus and storage medium for multipath combining and electronic device |
CN110518921A (en) * | 2018-05-21 | 2019-11-29 | 北京松果电子有限公司 | The method, apparatus and storage medium and electronic equipment of multi-path combing |
CN111726299B (en) * | 2019-03-18 | 2023-05-09 | 华为技术有限公司 | Flow balancing method and device |
CN111726299A (en) * | 2019-03-18 | 2020-09-29 | 华为技术有限公司 | Flow balancing method and device |
US11625378B2 (en) | 2019-05-31 | 2023-04-11 | Cisco Technology, Inc. | Selecting interfaces for device-group identifiers |
US11334546B2 (en) * | 2019-05-31 | 2022-05-17 | Cisco Technology, Inc. | Selecting interfaces for device-group identifiers |
CN113206783A (en) * | 2020-01-31 | 2021-08-03 | 安华高科技股份有限公司 | Method and apparatus for weighted cost multi-path packet processing |
CN113206783B (en) * | 2020-01-31 | 2024-04-23 | 安华高科技股份有限公司 | Method and apparatus for weighted cost multipath packet processing |
US11784932B2 (en) | 2020-11-06 | 2023-10-10 | Innovium, Inc. | Delay-based automatic queue management and tail drop |
US11621904B1 (en) | 2020-11-06 | 2023-04-04 | Innovium, Inc. | Path telemetry data collection |
US11943128B1 (en) | 2020-11-06 | 2024-03-26 | Innovium, Inc. | Path telemetry data collection |
CN113111070A (en) * | 2021-05-08 | 2021-07-13 | 福建天晴数码有限公司 | Method and system for performing database and table division based on grouping |
US11968129B1 (en) | 2023-04-28 | 2024-04-23 | Innovium, Inc. | Delay-based tagging in a network switch |
Also Published As
Publication number | Publication date |
---|---|
US11075986B2 (en) | 2021-07-27 |
US20170149877A1 (en) | 2017-05-25 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11075986B2 (en) | Weighted load balancing using scaled parallel hashing | |
US9571400B1 (en) | Weighted load balancing in a multistage network using hierarchical ECMP | |
US9608913B1 (en) | Weighted load balancing in a multistage network | |
US10129140B2 (en) | Server-centric high performance network architecture for modular data centers | |
CN107580769B (en) | Method and apparatus for load balancing in a network switch | |
US8503456B2 (en) | Flow based path selection randomization | |
US10348646B2 (en) | Two-stage port-channel resolution in a multistage fabric switch | |
US9716592B1 (en) | Traffic distribution over multiple paths in a network while maintaining flow affinity | |
US10084687B1 (en) | Weighted-cost multi-pathing using range lookups | |
US8036126B2 (en) | System and method for compressing internet protocol routing tables | |
EP2276207A1 (en) | Node based path selection randomization | |
JP2017516387A (en) | Reduction of prefix-aware weighted cost multiple path groups | |
US7180894B2 (en) | Load balancing engine | |
US20150078375A1 (en) | Mutable Hash for Network Hash Polarization | |
EP3562097B1 (en) | Establishment for table entry of equal-cost path | |
US10560367B2 (en) | Bidirectional constrained path search | |
Sheu et al. | Efficient unicast routing algorithms in Software-Defined Networking | |
US20230131022A1 (en) | Telemetry and Buffer-Capacity Based Circuits for Load-Balanced Fine-Grained Adaptive Routing in High-Performance System Interconnect | |
US11765072B2 (en) | Weighted bandwidth allocation for adaptive routing | |
Zhu et al. | AMLR: an adaptive multi-level routing algorithm for dragonfly network | |
US20180212881A1 (en) | Load-based compression of forwarding tables in network devices | |
US11757780B2 (en) | Filter, port-capacity and bandwidth-capacity based circuits for load-balanced fine-grained adaptive routing in high-performance system interconnect | |
EP3902212B1 (en) | A method to mitigate hash correlation in multi-path networks | |
US20230014645A1 (en) | Load-Balanced Fine-Grained Adaptive Routing in High-Performance System Interconnect | |
Ziedins | A paradox in a queueing network with state-dependent routing and loss |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:KABBANI, ABDUL;VAHDAT, AMIN;SIGNING DATES FROM 20140312 TO 20140314;REEL/FRAME:033030/0681 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044097/0658Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |