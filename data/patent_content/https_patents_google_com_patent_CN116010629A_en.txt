CN116010629A - Operational suggestion of media content - Google Patents
Operational suggestion of media content Download PDFInfo
- Publication number
- CN116010629A CN116010629A CN202310081793.3A CN202310081793A CN116010629A CN 116010629 A CN116010629 A CN 116010629A CN 202310081793 A CN202310081793 A CN 202310081793A CN 116010629 A CN116010629 A CN 116010629A
- Authority
- CN
- China
- Prior art keywords
- media content
- content
- application
- video
- action
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 230000009471 action Effects 0.000 claims abstract description 236
- 238000012545 processing Methods 0.000 claims abstract description 46
- 238000009877 rendering Methods 0.000 claims abstract description 19
- 230000004044 response Effects 0.000 claims abstract description 12
- 238000000034 method Methods 0.000 claims description 89
- 238000000605 extraction Methods 0.000 claims description 62
- 238000013518 transcription Methods 0.000 claims description 26
- 230000035897 transcription Effects 0.000 claims description 26
- 230000006870 function Effects 0.000 claims description 13
- 230000002730 additional effect Effects 0.000 claims description 6
- 238000010801 machine learning Methods 0.000 claims description 6
- 238000003058 natural language processing Methods 0.000 claims description 3
- 239000004615 ingredient Substances 0.000 description 89
- 235000013601 eggs Nutrition 0.000 description 18
- 239000008267 milk Substances 0.000 description 16
- 235000013336 milk Nutrition 0.000 description 16
- 210000004080 milk Anatomy 0.000 description 16
- 238000010411 cooking Methods 0.000 description 15
- 230000008569 process Effects 0.000 description 15
- 238000003860 storage Methods 0.000 description 12
- 235000015041 whisky Nutrition 0.000 description 12
- 235000008429 bread Nutrition 0.000 description 11
- 238000012790 confirmation Methods 0.000 description 9
- 238000004891 communication Methods 0.000 description 8
- 239000000284 extract Substances 0.000 description 8
- 235000013305 food Nutrition 0.000 description 8
- 230000003993 interaction Effects 0.000 description 8
- 235000013527 bean curd Nutrition 0.000 description 7
- 230000015654 memory Effects 0.000 description 7
- 240000007124 Brassica oleracea Species 0.000 description 6
- 235000003899 Brassica oleracea var acephala Nutrition 0.000 description 6
- 235000011301 Brassica oleracea var capitata Nutrition 0.000 description 6
- 235000001169 Brassica oleracea var oleracea Nutrition 0.000 description 6
- 239000000463 material Substances 0.000 description 6
- 235000015277 pork Nutrition 0.000 description 6
- 238000010586 diagram Methods 0.000 description 3
- 230000010006 flight Effects 0.000 description 3
- 230000006872 improvement Effects 0.000 description 3
- 238000012360 testing method Methods 0.000 description 3
- 238000012549 training Methods 0.000 description 3
- 241000238557 Decapoda Species 0.000 description 2
- 240000008042 Zea mays Species 0.000 description 2
- 235000005824 Zea mays ssp. parviglumis Nutrition 0.000 description 2
- 235000002017 Zea mays subsp mays Nutrition 0.000 description 2
- 235000005822 corn Nutrition 0.000 description 2
- 238000009826 distribution Methods 0.000 description 2
- 230000000694 effects Effects 0.000 description 2
- 238000005516 engineering process Methods 0.000 description 2
- 235000013312 flour Nutrition 0.000 description 2
- 235000021109 kimchi Nutrition 0.000 description 2
- 230000007246 mechanism Effects 0.000 description 2
- 239000000203 mixture Substances 0.000 description 2
- 238000012986 modification Methods 0.000 description 2
- 230000004048 modification Effects 0.000 description 2
- 230000002093 peripheral effect Effects 0.000 description 2
- 238000002360 preparation method Methods 0.000 description 2
- 230000000007 visual effect Effects 0.000 description 2
- 244000291564 Allium cepa Species 0.000 description 1
- 235000002732 Allium cepa var. cepa Nutrition 0.000 description 1
- 235000000536 Brassica rapa subsp pekinensis Nutrition 0.000 description 1
- 241000499436 Brassica rapa subsp. pekinensis Species 0.000 description 1
- 240000004922 Vigna radiata Species 0.000 description 1
- 235000010721 Vigna radiata var radiata Nutrition 0.000 description 1
- 235000011469 Vigna radiata var sublobata Nutrition 0.000 description 1
- 230000004308 accommodation Effects 0.000 description 1
- 230000003213 activating effect Effects 0.000 description 1
- 238000010420 art technique Methods 0.000 description 1
- 230000003190 augmentative effect Effects 0.000 description 1
- 235000015241 bacon Nutrition 0.000 description 1
- 238000004590 computer program Methods 0.000 description 1
- 238000010276 construction Methods 0.000 description 1
- 235000021186 dishes Nutrition 0.000 description 1
- 238000001914 filtration Methods 0.000 description 1
- 239000011521 glass Substances 0.000 description 1
- 230000002452 interceptive effect Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 235000013372 meat Nutrition 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 238000005457 optimization Methods 0.000 description 1
- 230000002085 persistent effect Effects 0.000 description 1
- 235000021395 porridge Nutrition 0.000 description 1
- 235000020995 raw meat Nutrition 0.000 description 1
- 238000012552 review Methods 0.000 description 1
- 230000011664 signaling Effects 0.000 description 1
- 235000011888 snacks Nutrition 0.000 description 1
- 235000013555 soy sauce Nutrition 0.000 description 1
- 235000013599 spices Nutrition 0.000 description 1
- 239000000725 suspension Substances 0.000 description 1
- 230000002123 temporal effect Effects 0.000 description 1
- 235000013311 vegetables Nutrition 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/958—Organisation or management of web site content, e.g. publishing, maintaining pages or automatic linking
- G06F16/972—Access to data in other repository systems, e.g. legacy data or dynamic Web page generation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/40—Information retrieval; Database structures therefor; File system structures therefor of multimedia data, e.g. slideshows comprising image and additional audio data
- G06F16/45—Clustering; Classification
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/40—Information retrieval; Database structures therefor; File system structures therefor of multimedia data, e.g. slideshows comprising image and additional audio data
- G06F16/43—Querying
- G06F16/435—Filtering based on additional data, e.g. user or group profiles
Abstract
The present disclosure relates to operational suggestions of media content. Embodiments relate to processing media content and/or associated metadata to classify the media content into a first category of a plurality of predefined categories. Versions of those embodiments further relate to extracting target content from the media content; generating an action corresponding to the application based on the extracted target content; and generating, based on the generated action, a selectable suggestion that includes a text portion describing the action. Some of those versions further involve causing the selectable suggestion to be displayed at a display of a client device along with rendering of the media content. The selectable suggestion, when selected, causes the application to perform the action. The target content may be extracted based on the first category and may be extracted based on the first category in response to the media content being classified into the first category.
Description
Technical Field
The present disclosure relates to operational suggestions of media content.
Background
Today, people often access media content (e.g., video, music, audio-bearing slides, video blogs) published or shared on websites, applications, or content sharing platforms to learn, entertain, or learn about information. In browsing such media content, a user may wish to access a third party application (e.g., a note taking application) to perform one or more actions related to the media content using information extracted from (or associated with) the media content. For example, when viewing a cooking video via a social media platform, a user may wish to save a recipe provided by the cooking video in a note-taking application for future use. Using prior art techniques, the user would have to first open the note taking application and manually write down the recipe (if the recipe is displayed in the cooking video or as an image with the cooking video), save a screenshot of the recipe (if the recipe is displayed in text format) or copy and paste. This means that the user will need to keep the cooking video (e.g., manually pause/close the video or leave a portion of the video unattended) to save the recipe information in the note taking application.
Continuing with this scenario, the user may wish to try a recipe by ordering items listed as recipe ingredients. In this case, the user would have to open the grocery application (or access the website) and manually search for and add ingredients to the shopping cart of the grocery application. Searching for ingredients from each ingredient mentioned in the confirmation cooking video to the use of the grocery application one by one can be a time consuming process that not only requires significant time and effort from the user, but also occupies the intensive computing resources of the client device that the user uses to view the cooking video and perform grocery shopping. To assist the user in the above and other similarly applicable situations, it is desirable to assist the user in performing one or more desired actions without leaving the media content that the user is currently browsing, to avoid activating additional widgets or applications that may require extensive follow-up operations like manual input or searching.
Disclosure of Invention
Some embodiments disclosed herein relate to generating and displaying actionable suggestions of media content, such as video or audio, where the actionable suggestions may include text portions describing actions (e.g., adding to a shopping cart, knowing more information about the brand) to be performed via a third party application (e.g., a note taking application) or a first party application (e.g., an automated assistant). The media content may be displayed via a content access application (sometimes referred to as a "content sharing application") at a client computing device (sometimes referred to as a "client device"). The media content may be received directly by the server computing device and/or the client computing device (or alternatively, the address of the media content may be received) before or while the media content is displayed to generate the actionable suggestion. For example, the server computing device may parse the address of the media content to retrieve the media content and/or metadata associated with the media content for further processing (e.g., to generate operational suggestions).
In some implementations, the media content can be received using a category tag. In other implementations, the media content may be received without a category tag. When media content is received without a classification tag, the media content and/or metadata associated with the media content may be processed to generate a classification tag (e.g., a music or recipe tag), and the generated classification tag may be assigned to the media content such that the media content is classified into a corresponding category (e.g., recipe or recipe recommendation category) of one or more predefined categories (e.g., recipe category, music mix category, movie category, travel category, test ready category, shopping shipping category, experience sharing category, story category, biography category, indoor play category, dog training category, concert category, etc.). Optionally, each of the one or more predefined categories may correspond to a predefined classification tag, or alternatively, one or more predefined classification tags. For example, a predefined "music" category may correspond to a predefined "music" category label, or a predefined "music" category may correspond to a "music singer" category label and an additional predefined "music song" category label. In the latter case, the content extraction parameter of the predefined "music singer" category label may be a singer extraction parameter, and thus the content extraction parameter of the predefined "music song" category label may be a song extraction parameter, unlike the content extraction parameter of the predefined "music song" category label. More description about content extraction parameters can be found elsewhere in this disclosure.
As non-limiting examples, the one or more predefined categories may be three predefined categories including: travel recommendation category, music recommendation category, and recipe recommendation category. In this example, given a received video as media content, the received video and/or metadata of the video (e.g., a title or short description of the received video) may be processed to determine whether the received video should be categorized as belonging to a travel recommendation category, to a music recommendation category, to a recipe recommendation category, or not belonging to any of the three categories. In the event that the process indicates that the video does not belong to any of these predefined categories, no classification tags are correspondingly generated and assigned to the video, or the video may optionally be classified into a "null" category and/or assigned a "null" classification tag, as described below.
Continuing with the above example, assume that processing of the received video and/or metadata of the received video indicates that the received video should be classified exclusively into the travel recommendation category. For example, processing may include generating a corresponding probability for each of the predefined categories based on the received video and/or metadata. The processing may indicate an exclusive classification into the travel recommendation category based on the corresponding probabilities for the travel recommendation category meeting the threshold and all other corresponding probabilities not meeting the threshold. In this case, a travel recommendation tag corresponding to the travel recommendation category may be assigned to the received video. As another example, assume that processing of the received video and/or metadata of the received video indicates that the received video should not be classified into any predefined categories. For example, processing may include generating a corresponding probability for each of the predefined categories based on the received video and/or metadata. The process may indicate that the video should not be classified into any category based on the corresponding probabilities all failing to meet the threshold. In this case, no category label is assigned to the video, or a "null" category label may be assigned to the video. As described herein, certain further processing of the video may be bypassed when the video has a "null" category label or lacks any assignment of predefined category labels. For example, even when the video includes content that meets the content extraction parameters of one of the predefined category labels, target content extraction from the video may be bypassed. For example, the content extraction parameters of the recipe classification tag may be such that food and quantity pairs are extracted from the video transcript (and/or video frame). Although a video including "6 eggs and 1 gallon of milk" is transcribed (e.g., the video may be a math class video that uses exactly "6 eggs and 1 gallon of milk" as part of an example math problem), the extraction of "6 eggs and 1 gallon of milk" will be skipped because the video includes a "null" classification tag or lacks any predefined classification tag. In these and other ways, when it is determined that the video has a predefined category, various computational efficiencies can be achieved by performing only target content extraction for the video that is specific to the predefined category (or predefined category label). For example, the computing resources involved in the extraction of the target content may be saved, as well as the computer resources involved in rendering the actionable advice for the extracted content and/or in performing actions corresponding to the selected actionable advice.
In some implementations, media content can be categorized into more than one of the one or more predefined categories. Continuing with the above example, where the one or more predefined categories are predefined to include three predefined categories (e.g., travel category, music category, and recipe category), videos including recommended recipes and recommended songs to be enjoyed when preparing recipes may be categorized into both music recommendation category and recipe recommendation category. In this case, a first category label (e.g., a music recommendation label) and a second category label (e.g., a recipe recommendation label) may be assigned to the received video. Subsequently, a first type of targeted content (e.g., a name of a recommended song, lyrics or audio clips of the recommended song, and/or a singer of the recommended song) may be extracted from the received video based on the first category label, and a second type of targeted content (e.g., ingredients of a recipe and/or cooking instructions) may be extracted from the received video based on the second category label
In some implementations, the media content includes a plurality of video frames or image frames. In this case, processing the media content to generate a category label for the media content may include: a plurality of video frames (or image frames) are processed. For example, processing a plurality of video frames (or image frames) may include: one or more graphical (e.g., image) objects are detected from a plurality of video frames or image frames, one or more target objects are determined from the one or more detected graphical objects, and a classification tag for the media content is generated based on the one or more determined target objects. Optionally, determining the one or more target objects may include: determining a frequency and/or duration of the one or more graphical objects occurring in a plurality of video frames (or image frames), and determining a graphical object having a frequency (or duration) that meets a first threshold as a target object. Alternatively or additionally, processing the plurality of video frames (or image frames) may include: one or more keywords displayed on a plurality of video frames (or image frames) are detected and/or a determination is made as to whether a frequency and/or duration of the one or more keywords displayed on the plurality of video frames (or image frames) meets a first threshold (or a different threshold).
In some other implementations, the media content includes an audio portion, and processing the media content to generate the corresponding category label may include: generating a transcript of the audio portion, detecting one or more keywords (e.g., 1 teaspoon parawhiskey) from the transcript of the audio portion, and classifying the media content based on the one or more detected keywords. Alternatively, the one or more keywords may be one or more terms from transcriptions of the audio portions having detected frequencies that meet a second threshold, where the second threshold may be different from (or the same as) the first threshold. Alternatively or additionally, the one or more keywords may be determined based at least on certain sentence structures (such as terms of "dumpling" mentioned immediately after "how to cook", "prepare", etc.). Alternatively or additionally, the one or more keywords may be determined based at least on consideration of metadata associated with the media content (e.g., a title of the video, "how to cook the dumpling").
In some implementations, media content may include both video (or image) frames and audio portions, and classification tags may be generated for such media content by processing the video (or image) frames and/or transcripts obtained from the audio portions. Alternatively, the transcript may be obtained and processed prior to processing the video (or image) frames to categorize the media content. For example, a transcription obtained by identifying the audio portion of the media content may be processed using a Machine Learning (ML) model and a classification tag and confidence measure output. If the confidence measure exceeds a predefined threshold, processing of the video (or image) frame may be omitted and the classification tag output by the ML model may be applied as a classification tag for the media content.
Alternatively or additionally, the category labels may be generated using metadata associated with the media content (e.g., title of the media, brief description/introduction of the media content, wiki page of artist mentioned in the media content, or links to wiki page). Given video shared via a social media application as an example of the aforementioned media content, metadata associated with the media content (i.e., shared video) may include, but is not limited to: a title of the media content, a manual tag of the media content, a manual description of the media content, one or more manual descriptive words of the media content, and/or comments to the media content retrieved from a content sharing application that is displaying the media content at the display.
When media content having a classification tag is received (e.g., metadata associated with the media content includes a classification tag), the media content may or may not need to be processed to classify the media content into a corresponding predefined category (or be assigned a predefined classification tag). In some embodiments, the classification tags received from metadata associated with the media content may be compared to the one or more predefined categories (or one or more predefined classification tags), and if the received classification tags match one of the predefined categories, the received classification tags may be assigned to the media content. In this case, the step of processing the media content to classify the media content into a corresponding category (or generating a classification tag) may be skipped or bypassed. However, if the received classification tags do not match any predefined categories, the media content may be processed to classify the media content.
Alternatively, when the received classification tag matches one of the predefined categories, but there is a need to improve the accuracy of the appropriate classification tag assigned to the media content (or the accuracy with which the media content is classified into the appropriate category), the media content may still be processed to determine whether the media content belongs to any of the one or more predefined categories. For example, media content may be processed and determined to belong to a corresponding predefined category, where the corresponding predefined category matches the received category label. In this case, the received class label may be considered accurate. If the received category label does not match the corresponding predefined category, the received category label may be deemed inaccurate (and/or may be discarded or removed from metadata associated with the media content) and a new category label may be generated and/or assigned based on the corresponding predefined category to replace the received category label.
In some implementations, the target content may be extracted from the media content. In some implementations, the target content may be extracted based on the classification tags assigned to the media content. As a non-limiting example, for videos classified into recipe categories (which may be one of the predefined categories), ingredients (in their text or graphic/image representation) of the recipes introduced in the video may be extracted as target content. Ingredients may be extracted as target content based on the ingredient extraction parameters assigned to recipe categories and videos categorized into recipe categories.
As another non-limiting example, assume a first video and a second video, wherein the first video introduces a hotel (e.g., in a lewis wili city center) featuring an exquisite dining selection (e.g., a senior steak restaurant of the hotel) and the second video is a celebrity chef of the senior steak restaurant of the hotel demonstrating how to prepare dishes for steak restaurant service. In this example, both the first and second videos may include some of the same information. For example, both the first and second videos may include the name of the hotel, the address of the hotel, and/or an image showing the appearance of the hotel's building, the name/image of the chef, etc. However, the first and second videos also include different content and/or different metadata such that the first video is exclusively categorized into a first predefined category (e.g., hotel recommendations) and the second video may be exclusively categorized into a second, different predefined category (e.g., recipes).
In the non-limiting example described above, for the first video, based on the first video being classified into the hotel recommendation category, the name of the hotel may be extracted from the first video as the target content (of the first video). For example, the hotel recommendation category may be associated with content extraction parameters that cause the hotel name to be extracted. Conversely, based on the second video being classified into the recipe category, ingredients of a dish (supplied at the steak restaurant) may be extracted from the second video as target content. For example, recipe categories may be associated with content extraction parameters that cause extraction of ingredients. Notably, even if the second video also includes the name of the hotel, the name of the hotel is not extracted from the second video, because the content extraction parameters assigned to the recipe category do not cause the name of the hotel to be extracted. In other words, the content extraction parameters assigned to recipe categories are different from those assigned to hotel recommendation categories, and the content extraction parameters of the hotel recommendation category that cause the hotel name to be extracted are absent. Continuing with the example, after classifying the first and/or second videos and performing content extraction, an action of booking a hotel accommodation may be recommended for the first video and displayed as an operational suggestion to a user browsing the first video, and/or an action of adding ingredients to the note taking application may be recommended for the second video and displayed as an operational suggestion to a user browsing the second video. In these and other ways, the content extraction performed on a given media content may depend on the classification of the media content. Thus, extracting a given content from the media content (and generating related actions and rendering corresponding operational suggestions) can only occur when it is first determined that the classification of the media content is associated with the content extraction parameters that cause the given content to be extracted. Thus, although the given content occurs in the particular media content, the given content may not be extracted from the given media content based on the determined classification of the given media content. Furthermore, for media content having only a particular classification and/or lacking any other particular classification, extraction of a given content (and its corresponding utilization) may be completely bypassed. For example, when a video is not determined to have any of a number of predefined classifications, the extraction of given content from the video may be bypassed entirely.
In some implementations, an action corresponding to one or more applications may be determined based on the extracted target content, and a first application may be selected from the one or more applications to perform the action. For example, given recipe videos, a list of ingredients may be extracted as target content based on recipe videos classified into a predefined category (here, a "recipe" category). In this example, based on the extracted target content being an ingredient list, the action may be copying and pasting the ingredient list to an electronic note created using a first third party application (e.g., an online note-taking application) or a second third party application (e.g., a local note-taking application). Based on using the local note-taking application more frequently than the online note-taking application at the corresponding client device, the local note-taking application may be selected to perform the action (i.e., save the list of ingredients in the electronic note of the local note-taking application).
In some implementations, more than one action may be determined based on the extracted target content. The more than one action may be determined based on a single category label (or a single predefined category), and optionally, an action of the more than one action may be selected and performed. For example, in addition to the act of copying and pasting the ingredient list to the electronic note via the local note-taking application (which the user may have access to), an additional act of adding the ingredient list to the electronic shopping cart of the grocery application may be determined. Optionally, in this example, the grocery store application may be selected to perform the act of adding the list of ingredients to its electronic shopping cart based on historical user data (which indicates that the user currently browsing media content is more frequent to add ingredients to the shopping cart than to save ingredients in electronic notes while viewing similar media content, i.e., video categorized with recipe labels), without saving the list of ingredients in the electronic notes of the local note taking application.
Alternatively, the more than one action may be determined based on a single category label (or a single predefined category), and may be performed without selection. For example, when the historical user data indicates that the user is currently browsing media content to use the local note-taking application to save the ingredient name and the frequency of adding ingredients to the shopping cart both meets a certain frequency threshold, a first action of saving the list of ingredients in the electronic notes of the local note-taking application and a second action of adding the list of ingredients to the electronic shopping cart of the grocery store application may be recommended and performed.
Alternatively, the more than one action may be determined based on a plurality of classification tags (or a plurality of predefined categories into which the media content falls). For example, in an example where a received video is assigned a first category label (i.e., a music recommendation label) and a second category label (i.e., a recipe recommendation label), a first type of target content (e.g., a name of a recommended song) is extracted from the received video based on the first category label and a second type of target content (e.g., ingredients of a recipe) is extracted from the received video based on the second category label. In this example, based on the extracted target content (i.e., the first and second types of target content), a first action may be determined for the first type of target content (e.g., adding a recommended song to a playlist of a music application) and a second action may be determined for the second type of target content (e.g., adding an ingredient to a shopping cart of a grocery application).
In some implementations, based on the determined action and the selected first third party application, a suggestion can be generated, where the suggestion includes a text portion describing the action. The text portion describing the action may be displayed with the media content at a display of the client computing device. As a non-limiting example, given that the determined action is "add milk and eggs to shopping cart", and that the selected first third party application is "grocery store application a", the suggestion may include a text portion displayed with the media content that the suggestion was generated, such as "add milk and eggs to shopping cart of grocery store application a".
In some implementations, the suggestion is displayed at the display when a predefined period of time (e.g., 15 seconds) has elapsed since the media content was displayed. Here, as a non-limiting example, the predefined period of time may be determined from statistical user data indicating successful attraction of the user's attention.
Alternatively, in some other implementations, the suggestion is displayed at the display when the target content of the media content is audibly or visually rendered. As non-limiting examples, a video (i.e., the aforementioned media content) may introduce a set of building blocks specifically chosen for preschool children and include video segments showing how the set of building blocks is used in different ways, and the video may have been categorized into one or more predefined categories (e.g., toy category, snack category, book category, clothing) or the video may have been assigned a toy classification tag ("toy" tag).
In the above example, the name of the set of building blocks may be extracted from the video frames of the video as target content (based on the video being assigned a "toy" tag), and the action of adding the set of building blocks to the shopping cart of the third party application M may be determined based on the extracted content. Here, the suggestion may be generated based on the determined action (i.e., purchasing the set of building blocks) and the corresponding third party application (i.e., third party application M with the set of building blocks for sale), namely "add set of blocks" to shopping cart of application M, where "set of blocks" represents the name of the set of building blocks. In this example, when video frames containing the names of the set of building blocks are rendered to a user on a display, suggestions (i.e., "shopping cart to add set of blocks to application M") may be rendered to the user of the same display.
In some implementations, the advice is displayed for a predefined period of time (e.g., ten seconds or other predefined period of time) and will automatically disappear after being displayed for the predefined period of time.
In some implementations, the suggestions are displayed in selectable elements and embedded with links to perform actions. Alternatively, the embedded link may be a URL identifying the action and the name of the selected first third party application to which the action corresponds. In various implementations, when a selectable element (or a suggested text portion) that displays a suggestion is selected, a link is performed to cause an action to be performed. The actions herein (e.g., adding a plurality of food items to a shopping cart of a "shopping app") may be performed by a selected first third party application (i.e., a "shopping app") or may be performed by an automated assistant installed on a client computing device and in communication with the selected first third party application (i.e., a "shopping app").
When the selected first third party application performs an action, the link (e.g., URL) may contain the name of the selected first third party application and an action description containing a plurality of parameters of the action, wherein the plurality of parameters may be determined based at least on the extracted target content. When an action is performed by an automated assistant in communication with the selected first third party application, the link (e.g., URL) may contain the name of the automated assistant, the name of the selected first third party application, and an action description comprising a plurality of parameters of the action. Optionally, after the linking is performed such that the action is being performed, the media content (e.g., video) is still displayed at the display of the client computing device without any suspension of the media content.
In some embodiments, selecting the first third party application from the one or more third party applications comprises: ranking scores are generated for the one or more third-party applications, respectively, the one or more third-party applications are ranked based on the generated ranking scores, and the first third-party application is selected based on the first third-party application having a ranking score that meets a threshold. In some implementations, the ranking score is generated based on user history data, based on whether a user of the client device browsing media content via a display of the client device has a registered account for each of the one or more third-party applications, and/or based on whether an action matches a function of each of the one or more third-party applications.
In some implementations, after the user selects the suggestion to render at the display (i.e., performs the link), an action is performed and at the display, additional suggestions can be rendered along with the media content to replace the suggestion. Additional suggestions may be generated by the server computing device (and/or the client computing device) to include: the text portion of the additional action to be performed via the selected first third party application is suggested. For example, after a user clicks on a suggestion to perform an action to add a food item extracted from the video to a shopping cart of "ShoppingApp," an additional suggestion to perform an additional action to check out the shopping cart via "ShoppingApp" may be rendered to the user while the video is being displayed. The user may select an additional suggestion, which may be embedded with an additional link that, when executed, causes a window of "ShoppingApp" (application version or Web version) to be displayed as a new user interface of the client computing device, or to pop up as an overlay in relation to the video for the user to complete an additional checkout action.
In some implementations, more than one action may be determined, and accordingly, more than one suggestion may be generated and displayed on a display. For example, the method herein may include: extracting target content from the media content; and determining a plurality of actions respectively corresponding to the one or more candidate applications based at least on the extracted target content. The method may further comprise: filtering one or more actions from a plurality of actions; one or more suggestions are generated based on the filtered one or more actions and the corresponding one or more candidate applications, wherein the filtered one or more actions include a first action related to a function of the first candidate application, and wherein the first action is performed via an automated assistant in communication with the first candidate application. In this example, the one or more generated suggestions may be displayed at a display of the client computing device along with the media content.
The foregoing is provided merely as an overview of some embodiments. Those and/or other embodiments are disclosed in greater detail herein.
Various embodiments may include a non-transitory computer-readable storage medium storing instructions executable by a processor to perform a method, such as one or more methods described herein. Other various embodiments may include a system comprising a memory and one or more hardware processors operable to execute instructions stored in the memory to perform methods, such as one or more of the methods described herein.
Drawings
The foregoing and other aspects, features, and advantages of certain embodiments of the present disclosure will be more apparent from the following description taken in conjunction with the accompanying drawings. In the drawings:
FIG. 1 depicts a block diagram of an example environment that demonstrates various aspects of the present disclosure, and in which embodiments disclosed herein may be implemented.
FIG. 2A depicts an example user interface showing suggestions, according to various embodiments.
FIG. 2B depicts another example user interface showing suggestions, according to various embodiments.
FIG. 3 is a flowchart illustrating an example method of displaying operational suggestions of media content according to various embodiments.
Fig. 4A, 4B, 4C, and 4D together illustrate examples in which actionable suggestions are provided for media content categorized into a first category for user interaction according to various embodiments.
5A, 5B, 5C, and 5D together illustrate another example in which actionable suggestions are provided for media content categorized into a first category for user interaction according to various embodiments.
FIG. 6 illustrates an example architecture of a computing device according to various embodiments.
Fig. 7A and 7B illustrate examples in which actionable suggestions are provided for media content categorized into a second category for user interaction according to various embodiments.
FIG. 8 illustrates an example in which actionable suggestions are provided for media content categorized into a third category for user interaction according to various embodiments.
FIG. 9 is a flowchart illustrating an example method of displaying operational suggestions of video according to various embodiments.
FIG. 10 is another flow chart illustrating an example method of displaying operational suggestions of video according to various embodiments.
Detailed Description
The following description with reference to the drawings is provided for the understanding of various embodiments of the present disclosure. It is to be understood that different features from different embodiments may be combined with and/or interchanged with one another. Further, those of ordinary skill in the art will recognize that various changes and modifications of the various embodiments described herein can be made without departing from the scope and spirit of the present disclosure. Descriptions of well-known or repeated functions and constructions may be omitted for clarity and conciseness.
The terms and words used in the following description and claims are not limited to a bookend, but are used by the inventors only to achieve a clear and consistent understanding of the present disclosure. Accordingly, it should be understood by those skilled in the art that the following descriptions of the various embodiments of the present disclosure are provided for illustration only and not for the purpose of limiting the disclosure as defined by the appended claims and their equivalents.
FIG. 1 is a block diagram of an example environment that demonstrates various aspects of the present disclosure and in which embodiments disclosed herein may be implemented. As shown in fig. 1, the environment may include a client computing device 11, and a server computing device 13 (or other device) in communication with the client computing device 11 via one or more networks 15. The one or more networks 15 may include, for example, a Local Area Network (LAN), a Wide Area Network (WAN) such as the internet, and/or any other suitable network.
The client computing device 11 may be, for example, a desktop computing device, a laptop computing device, a tablet computing device, a cell phone computing device, a computing device of a vehicle (e.g., an in-vehicle entertainment system), an interactive speaker, a smart appliance such as a smart television, and/or a wearable apparatus including a computing device (e.g., glasses with computing device, virtual or augmented reality computing device), and the disclosure is not limited thereto. In various implementations, the client computing device 11 may include a content access application 110, one or more third party applications 112A-N, a data store 116, and optionally an automated assistant 114 (which may be, by default, a "first party application" installed at the client computing device). In some implementations, a user of the client computing device 11 can interact with the content access application 110, the one or more third party applications 112A-N, and/or one or more smart devices (not shown in FIG. 1), all of which communicate with the client computing device 11 via the automated assistant 114.
In various implementations, the content access application 110 may be a stand-alone media player, a Web browser, or a social media application, and the disclosure is not limited thereto. The content access application 110 may include a rendering engine 1101 that identifies and retrieves media content 17 (e.g., video content, audio content, slides) for rendering in the visible region of the content access application 110 of the client computing device 11. As a non-limiting example, the rendering engine 1101 may render a user interface 1103 of the content access application 110, the user interface 1103 showing an initial video frame of video content at the client computing device 11. Third party applications 112A-N may include, for example, note taking applications, shopping applications (for grocery stores, clothing, furniture, department stores, tickets, traffic, hotels, travel plans, food orders and distribution, courses, coaching or test preparation), messaging applications, and/or any other suitable applications (or services) installed or accessible at client computing device 11. In some implementations, the user of the client computing device 11 can have a registered account associated with one or more of the third party applications 112A-N.
The server computing device 13 may be, for example, a Web server, a proxy server, a VPN server, or any other type of server as desired. In various implementations, the server computing device 13 may include a content parsing engine 130, a content classification engine 132, and a suggestion engine 134. In various implementations, the content parsing engine 130 can receive media content accessible via the content access application 110 (e.g., video blogs uploaded by registered users of the content access application 110 for sharing with other users of the content access application 110), and/or metadata associated with the media content (e.g., title, description (if any), descriptive text recognized via speech recognition, comments to the media content). Alternatively, instead of the media content itself, the content parsing engine 130 may access the media content (and/or metadata associated with the media content) by retrieving and parsing an address (e.g., URL) of the media content.
In some implementations, the content parsing engine 130 can determine whether the retrieved media content (e.g., video) and/or associated metadata includes one or more keywords based on which the content classification engine 132 can determine whether the retrieved media content falls into any of one or more predefined content categories (sometimes referred to as "predefined categories," including recipe categories, travel plan categories, etc.) instead of "content categories. As a non-limiting example, the content parsing engine 130 may receive only video without any descriptive text and without defining a title (e.g., locally saved video with undefined title), and in this case, the content parsing engine 130 will have to process the video to determine whether the transcription (and/or embedded text) of the video includes one or more keywords or key terms, and/or whether one or more target objects are detected from the video frames (or image frames in the case of a slide show) of the video. For example, the content parsing engine 130 may determine that the transcription of the video includes key terms such as "how to cook", "recipe", and/or "ingredients", and the content classification engine 132 may use these key terms to generate a classification tag (e.g., here a "recipe" tag) for the video, indicating that the video is classified into a first content category (e.g., recipe category) of a plurality of predefined content categories. Alternatively or additionally, the content parsing engine 130 may detect one or more objects from video frames included in the video, and the content classification engine 132 may generate "recipe" tags for the video based on identifying one or more key objects (e.g., kitchen, chopping board, raw meat, chopped vegetables, spice bottles or oil bottles, cooking tools, ingredients lists, etc.) from the one or more detected objects and/or key terms determined from the video transcription.
Alternatively, a classification tag (e.g., a "recipe" tag) may be displayed to a user of the content access application 110 that encounters media content, thereby signaling to the user that the media content provides a recipe. Alternatively, the category labels may not be displayed at all to the user of the content access application 110. Optionally, the content classification engine 132 may additionally generate a confidence metric that indicates how accurately the classification tags (e.g., recipe tags for the video) were generated.
Continuing the non-limiting example above, instead of receiving only video, content resolution engine 130 may receive the address of the video and resolve the address to retrieve not only the video but also metadata of the video. Metadata for a video may include (1) text data including, but not limited to: title of the video, description of the video (by the video creator, editor, or user sharing the video, etc.), descriptive text saved for the video, comments made by the reviewer of the video; and optionally (2) non-text data, such as temporal data associated with the video. In this case, the content parsing engine 130 may first process metadata associated with the video to extract text data from the metadata, and the content classification engine 132 uses the extracted text data to determine whether the video falls into any of the one or more predefined content categories.
For example, the extracted text data may include one or more keywords (e.g., a keyword "recipe" detected from a video title, i.e., "kimchi recipe") based on which the content classification engine 132 may determine that the video belongs to a first content category (i.e., recipe category). Note that if no key terms are detected based on processing metadata, the content parsing engine 130 may be invoked by the content classification engine 132 to process the video (i.e., process the audio data of the video to recognize a transcription of the video, and/or process the video frames of the video to determine one or more objects shown in the video frames). The content classification engine 132 may then use the transcription and/or the one or more objects to again determine whether the video falls within any of the one or more predefined content categories. If, in addition to the aforementioned metadata, the content classification engine 132 determines that the video does not fall into any of the one or more predefined content categories based on the processed video, the content classification engine 132 may assign a "skip" tag or a "null" classification tag to the video such that the video does not need to be further processed in order to generate a suggestion (e.g., an operational suggestion) for the video.
Alternatively, the server computing device 13 may include a content extraction engine 136, wherein the content extraction engine 136 extracts target content from media content, such as video, using content extraction parameters assigned to classification tags of the media content. For example, for a video providing multiple songs all performed by the same singer, the video may be assigned a predefined "song singer" category label, and the content extraction parameters of the predefined "song singer" category label may be singer extraction parameters, including, for example, singer name parameters. In this example, the content extraction engine 136 may extract singer information (singer's name, her piece of sound, etc.) from the video using the singer extraction parameters as target content for the video. As another example, for videos providing multiple songs performed by different singers, the videos may be assigned a predefined "song" category label, and the content extraction parameters of the predefined "singer" category label may be song extraction parameters including, for example, title parameters and/or lyric parameters. In this example, the content extraction engine 136 may extract song information (title of song, lyrics of song, etc.) from the video using the song extraction parameters as target content.
In various implementations, the suggestion engine 134 can include a suggestion generation engine 1341 to generate suggestions that include text portions (i.e., in natural language) and optionally non-text portions (e.g., emoji), symbols, etc.). The suggestion generation engine 1341 can generate suggestions tailored to the classification tags (or content categories to which the video is classified), wherein the suggested natural language content can be based on one or more key terms being identified by the content classification engine 132 from metadata (or from the video), and/or based on one or more target objects being identified by the content classification engine 132 from video frames of the video. Alternatively, in some implementations, the suggestion generation engine 1341 can generate suggestions based on target content extracted from media content (e.g., video) by the content extraction engine 136. For example, when the content extraction engine 136 extracts the name of the singer from the video as the target content, the suggestion generation engine 1341 may determine an action of "adding the new song of the singer to your playlist" based on identifying/finding the new song using the name of the singer, and generate a suggestion including a text portion describing the action (e.g., "add the new song of the singer to your playlist") and/or a suggestion including an embedded link that causes the action to be performed. As another example, when the content extraction engine 136 extracts lyrics of a song from a video as target content, the suggestion generation engine 1341 may determine an action to "download the song" and generate a suggestion that includes a text portion describing such an action to "download the song" and/or an embedded link that causes the song to be downloaded.
In various implementations, the suggestion may be an operable suggestion that suggests an action that may be performed by one or more third party applications 112A-N (or by the automated assistant 114). In this case, the suggestion engine 134 may include an application selection engine 1345, wherein the application selection engine 1345 may select an application of the one or more third-party applications 112A-N and/or the automated assistant 114 to perform the action suggested by the operational suggestion. The application selection engine 1345 may inform the suggestion generation engine 1341 of the selected application, and the suggestion generation engine 1341 may accordingly generate an actionable suggestion by including information associated with the selected application (e.g., a name or icon representing the selected application) in the actionable suggestion. For example, for the action of "download Song A", the application selection engine 1345 may determine a music application (for which the user has a registered account providing access to the download service of the music application) that provides Song A for download by a third party application to perform the action (i.e., "download Song A").
In some implementations, the application selection engine 1345 can select an appropriate application by first generating a score for each of the third party applications 112A-N and/or the automated assistant 114, and then ranking the third party applications 112A-N and/or the automated assistant 114 using the generated scores. The application with the highest ranking score may be selected as the appropriate application. In some implementations, given the first third party application 112A, the application selection engine 1345 can determine a score for the first third party application 112A based on, for example, target content determined from the video, metadata associated with the video, and/or user data (e.g., user settings, user history data, etc.).
In various implementations, the suggestion engine 134 can further include a suggestion rendering engine 1343 that causes generated suggestions to be rendered with video in a selectable manner (e.g., selectable elements). For example, suggestion rendering engine 1343 may cause the generated suggestion (e.g., operational suggestion 171) to be rendered as an overlay over a portion of video 17.
In various embodiments, the generated suggestions may be rendered at predefined times, in a predefined format, and/or within a predefined period of time. For example, the generated suggestions may be rendered by the suggestion rendering engine 1343 when the video has been played within a predefined period of time (e.g., about 30 seconds from the beginning of the video), wherein the predefined period of time may be selected based on statistical user data (e.g., historical user data) and indicates that the video has attracted the attention of a user who is viewing the video. As another example, when displaying a video frame containing a recipe, the generated suggestion may be rendered by suggestion rendering engine 1343. The generated suggestions may be displayed, for example, for a predefined period of time (e.g., about 5 seconds), where the predefined period of time may also be determined based on statistical user data. The generated suggestions may be rendered using a predefined format (e.g., size, appearance, and location). For example, the generated suggestions may be rendered as a bottom region that minimizes possible negative impact on the user's video viewing experience, or may be rendered as an overlay in the center region of the video while pausing the video to obtain the greatest possible attention from the user. However, the examples herein are for illustrative purposes only, and further examples may be provided throughout this disclosure. The examples herein are not intended to be limiting.
FIG. 2A depicts an example user interface showing suggestions generated by the suggestion rendering engine 1343 in FIG. 1, according to various embodiments. FIG. 2B depicts another example user interface showing suggestions generated by the suggestion rendering engine 1343 in FIG. 1, according to various embodiments. Referring to fig. 2A, a user of a client computing device may open a Web browser 201a and access a content sharing platform 203 via the Web browser 201a (for the content sharing platform 203, the user may have a registered account 205). The user may search for "Taylor's best songs" via text input (or audio input) at search field 203b and select to view video with title 210 (i.e., "Taylor's best songs") from a list of media content (not shown in fig. 2A) returned as the search result of the user search (i.e., "Taylor's best songs").
As shown in fig. 2A, the interface 200 of the Web browser 201a may include a first section 201, the first section 201 showing the name of the Web browser 201a and a tab 201b corresponding to media content 207 (e.g., video content of video titled "Best of Taylor") currently displayed on the content sharing platform 203. The interface 200 of the Web browser 201a may further include a second section 202, the second section 202 showing the address 202a of the media content 207, and optionally a selectable button 202b, wherein the selectable button 202b may be selected to open a page showing account information for a user account associated with the Web browser 201 a. The interface 200 of the Web browser 201a may further include a third section 200A (also referred to as "user interface of the content-sharing platform user interface"), the third section 200A including a first selectable element 203a (e.g., representing the name or symbol of the content sharing platform, which may contain an embedded link to the content sharing platform home page), a search bar 203b, a second selectable element 203c representing a registered account of the content sharing platform (e.g., an account with a user name of "K"), a scroll bar 206, and a media display area 204 displaying media content 207 (e.g., currently playing song a as part of video content entitled "Best of Taylor").
The third section 200A may further include a third selectable element 209 that contains an embedded link to a Channel (e.g., "Channel X") that collects all media content associated with (e.g., uploaded, liked, saved, shared, etc.) the registered account "K". The third section 200A may further include a content introduction section 205 located near (e.g., below) the media display section 207. The content introduction section 205 may include/display a title 210 of the media content 207, a function row 211 that displays time information (number of views up to the current time, content release date, etc.) related to the media content 207, and selectable buttons (e.g., like, save to a folder, share with friends or family).
The content introduction section 205 may further include channel information associated with users of the content sharing platform that upload media content 207 for sharing with others. For example, the content presentation area 205 may include a selectable button 212a that includes an embedded link to the channel X home page, the name 212b of channel X (i.e., "BEST MUSIC OFFICIAL CHANNEL (best music official channel)"), the presentation 212c of channel X (i.e., "More about us): HTTP:// WECOLLECTECTMUSIC.COM …"), and/or a selectable button 212d that allows a user of the content sharing platform to subscribe to channel X. Optionally, the content introduction section 205 may further include: an introduction to media content 207 below introduction 212c of channel X (not shown in fig. 2A, which shows a comment area that may require the user to scroll down scroll bar 206), and/or other users comment on media content 207 and may receive a reply from a user who owns the content sharing platform of channel X. Here, the name 212b of channel X, the introduction 212c of channel X, the introduction of the media content 207, comments and replies to the comment area (if any) and the aforementioned title 210 of the media content 207 and time information related to the media content 207 may be referred to as "metadata" associated with the media content 207 or collected as part of the "metadata".
In various implementations, the user interface 200A of the content sharing platform may display the actionable advice 208 in natural language (e.g., "Ticket on hole Taylor to sing Song A at Fantasy Concert this May (Ticket sales: taylor will sing Song A at this Authority's fantasy concert)"). All ("Ticket on hole: taylor to sing Song A at Fantasy Concert this May") or a portion thereof ("Ticket sell") of the actionable advice 208 may be selectable to provide further information displayed, for example, using a pop-up window (not shown in fig. 2A). In some implementations, the name of a particular Song (i.e., "Song a," which may be the target content of the media content 207 extracted to generate the suggestion 208) may not be displayed at the user interface 200A of the content sharing platform displaying the video content entitled "Best of Taylor," nor identified from metadata associated with the video content entitled "Best of Taylor," but may still be detected by processing the video frames or audio data of the video content entitled "Best of Taylor. In some implementations, referring to fig. 2B, the name of a particular Song (i.e., "Song a") may be displayed at the user interface 200A as a tile 210 of the media content 207, meaning that the name of the particular Song (i.e., song a) may be detected from metadata associated with the media content 207.
FIG. 3 is a flowchart illustrating an example method of displaying operational suggestions of media content according to various embodiments. For convenience, the operations of method 300 are described with reference to a system performing the operations. The system of method 300 includes one or more processors and/or other components of a client device and/or a server device. Furthermore, although the operations of method 300 are illustrated in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted, or added.
As shown in fig. 3, in various embodiments, a method 300 of displaying operational suggestions for media content may be performed by a system, wherein the system receives media content (e.g., video) at block 301. Alternatively, instead of receiving the media content, an address (local file address or URL) of the media content may be received. In this case, the address of the media content may be resolved, and the media content may be retrieved using the resolved address. The media content herein may be video (travel video, cooking video, movie trailers, programs, video blogs, recorded dog training courses, yoga courses, etc.), audio (music, audio books, classroom recordings, recorded interviews with celebrities, etc.), slides with or without side notes, or any other suitable form.
As a non-limiting example, referring to fig. 4A, media content (i.e., video content 407) may be displayed at a user interface 400A of a content sharing application 403a that is part of an interface 400 of a Web browser 401 a. In this example, the user may have a registered account 403c (e.g., account "K") of the content sharing application 403a, and the interface 400 of the Web browser 401a may have a tab 401b, the tab 401b indicating that the user is currently using the content sharing application 403a to access the video content 407. The interface 400 of the Web browser 401a may further include a URL 402a of the video content 407 and one or more buttons 402b that the user may click to return to a previous Web page, advance or refresh the current page.
Further, in this example, the user interface 400A of the content sharing application 403a may have a search bar 403b, a media display area 404 displaying the video content 407, a channel-X409, a title 410 of the video content 407 (e.g., "TEN BEST JAZZ SONGS (ten best jazz songs))"), additional information or functionality associated with the video content 407Information field 411, channel button 412a selectable to open the home page of the channel (e.g., channel "X") collecting video content 407, channel name 412b (e.g., "JAZZ MUSIC OFFICIAL CHANNEL) subscription button 412d selectable to subscribe to channel 412 a.
In various implementations, at block 303, the system may process the media content and/or metadata associated with the media content to classify the media content into a first one of the one or more predefined categories. Here, the one or more predefined categories may include, but are not limited to: recipe categories, music recommendation categories (or simply "music categories"), movie recommendation categories (or simply "movie categories"), travel plan categories (or simply "travel categories"), and tutorial categories, which may be defined separately prior to processing.
As a non-limiting example, referring to fig. 4A, video content 407 (media content) and/or titles 410 (i.e., "TEN BEST JAZZ SONGS") such as video content 407 and channel names 412b (i.e., "JAZZ MUSIC OFFICIAL CHANNEL") may be processedvideo content 407 into a music category that is one of the predefined categories.
Optionally, in some implementations, the system may process the media content and/or metadata associated with the media content to classify the media content into a first category (block 303) by: at block 3031, the media content is processed to obtain a transcription of the audio portion of the media content. Alternatively or additionally, at block 3031, the media content may be processed to obtain a plurality of video frames of a video portion of the media content. Processing media content can be useful when receiving media content without desired or representative content indication metadata. For example, a video may be received along with an unspecified title (e.g., "video 1"), a creation date/time of the video, and/or a size of the video, but no metadata of this type indicates what the video is about. In another case, the video may be received with the heading "recipe" indicating that the video content is about a recipe, while the video is actually teaching how to pronounce the word "recipe". In an additional case, when the address of the video is provided, the address of the video may point to the video itself (or a web page embedded with the video but not displaying a specific description about the video). In these cases, classifying the video using only metadata associated with the video can be inaccurate. Processing video (e.g., processing video frames and/or audio data associated with the video) to understand the video content of the video may thus improve the accuracy of the classification, which facilitates subsequent optimization of the type (or content) of operational suggestions delivered for consideration by an associated user (e.g., video viewer).
In some implementations, the system can process the media content and/or metadata associated with the media content to classify the media content into a first category (block 303) by: at block 3033, one or more keywords are detected from transcripts obtained from the audio portion of the media content. Alternatively or additionally, at block 3033, detecting one or more target objects is performed on a plurality of video frames obtained from the video portion of the media content (e.g., by identifying a graphical or textual representation of the target object). In these implementations, the system may further process the media content and/or metadata associated with the media content to classify the media content into the first category by: at block 3035, media content is classified into a first category based on the one or more detected keywords (and/or based on the one or more detected target objects).
As a non-limiting example, a URL may be retrieved and directed to a web page displaying a video titled "Taylor". In this example, processing metadata associated with the video (i.e., the video title "Taylor") can make it difficult to categorize the video into an appropriate one of one or more predefined categories (e.g., music, recipes, travel, etc.). A user who sees the video title "Taylor" might consider this to be a biographical video, and the video might be a musical mix with multiple pieces individually selected from the songs most popular by Taylor. In such cases, instead of or in addition to processing the video tiles (i.e., "Taylor"), the video itself may also be processed to obtain the audio portion of the video and the plurality of video frames forming the video. The audio portion of the video may be processed using speech recognition to generate a transcription of the video. Based on the transcription of the video, one or more keywords may be detected from the video. For example, lyrics of one or more songs of Taylor may be detected from a transcription of a video as keywords. Alternatively or additionally, one or more target objects (e.g., a graphical or textual representation of a scene, item) may be detected from the video based on the plurality of video frames. For example, an image of a Taylor wearing her famous performance costume or a widely known scene from a music video of Taylor may be detected as a target object from a plurality of video frames. Further, the video may be classified into a first category or another specific category (e.g., a music category) of a plurality of predefined categories based on one or more detected keywords (e.g., lyrics) and/or based on one or more detected target objects (e.g., images or scenes).
In various implementations, at block 305, the system may extract target content from the media content based on the media content classified into the first category. For example, media content (e.g., cooking videos) may be categorized into a first category (i.e., recipe category). In this example, for example, a list of ingredients of a recipe may be extracted from media content as target content using content extraction parameters (e.g., a food name extraction parameter and a food quantity extraction parameter) assigned to (or predefined for) a recipe category.
As a non-limiting example, referring to fig. 4A, based on the video content 407 being classified into a music category, a target content (e.g., names of ten individual jazz songs including song B) may be determined/extracted from a transcription of the video content 407 and/or a video frame of the video content 407 using music extraction parameters corresponding to the music category. The music extraction parameters may include, for example, lyrics extraction parameters and/or song name extraction parameters. For example, lyrics of ten jazz songs may be extracted from a transcription of the video content 407 based on the lyrics extraction parameters, and a name of each of the ten jazz songs may be determined using the lyrics of the ten jazz songs, wherein the name (and/or lyrics) of the ten jazz songs are determined as the target content. Alternatively or additionally, names of ten jazz songs may be extracted from one or more video frames of the video content 407 based on the song name extraction parameters, and the names of the ten jazz songs may be determined as target content.
In various embodiments, at block 307, the system may determine actions executable by one or more third party applications based at least on the extracted target content. For example, when the extracted target content is a recipe for cooking fresh shrimp and corn porridge, the action may be to add ingredients listed in the recipe (shrimp, corn flour, bacon, green onion, etc.) to a shopping cart of a third party application (e.g., a grocery shopping application), or to create an electronic note that records those ingredients. In some implementations, the actions may be additionally determined based on user history data (e.g., online purchase receipts, frequency of notes recorded using a widget such as a note-taking application), statistics (e.g., most popular actions performed by many users while viewing media content), application data (e.g., whether the application performing the actions is installed at a client computing device with which the user browses media content), and other suitable data.
As a non-limiting example, referring to fig. 4A, an action (e.g., adding information to a note such as "Concert alert: local Band to play Song B at Jazz Hall this Sunday (Concert alert: local band will play song B in jazz Concert hall on this day) may be determined based on target content (e.g., lyrics of song B, regardless of whether song B collected in video content 407 was performed by its original singer or by local band) and/or based on user history data (e.g., registered user K of content sharing platform 403a was historically frequently added to annotation information while viewing video content belonging to the music category). Continuing with this example, instead of or in addition to adding information to the note taking application, namely, "Concert alert: local Band to play Song B at Jazz Hall this Sunday", more detailed information of the Concert (see graphical element 4082 in FIG. 4D as an example) may also be added to the note taking application.
In various embodiments, at block 309, the system may select a first third party application from the one or more third party applications to perform the action based on the determined action. For example, when the action is to reserve two movie tickets, a particular movie ticket application may be selected from one or more movie ticket applications to reserve the two movie tickets. In some implementations, one or more movie ticket applications may be ranked based on their ranking scores, and the movie ticket application corresponding to the highest ranking score may be selected to subscribe to both movie tickets. Here, ranking scores may be generated for each of one or more movie ticket applications based on factors including, but not limited to: device data (e.g., whether a movie ticket application is installed at a client computing device that the user is currently accessing media content), historical user data (e.g., whether the user has subscribed to a movie ticket using the movie ticket application; based on historically subscribing to movie tickets in Saturday afternoon, the user appears to prefer to watch movies in Saturday afternoon), and availability data (e.g., when movie tickets are available via the movie ticket application, the movie ticket application may be a mobile application developed by a particular theater). In some implementations, the third party application performing the action may be predetermined so that the ranking process may be omitted.
As a non-limiting example, referring to FIG. 4A, given a determined action (e.g., adding the following information to a note: "Concert alert: local Band to play Song B at Jazz Hall this Sunday"), a note-taking application may be selected from note-taking applications provided by different developers to perform the action, where the selected note-taking application is the local widget that is most frequently (most recently or by default) used by a user to add notes, reminders, shopping lists, and the like.
In various embodiments, at block 311, the system may generate a suggestion that includes a text portion of the suggested action based on the action and the selected first third party application. Alternatively or additionally, the text portion may specify a selected first third party application to perform the action. In various implementations, the suggestion (or a portion thereof, such as an icon) may be embedded with a selectable link that, when selected, causes an action to be performed or causes an interface of the selected third party application to be opened as an overlay of media content for further user interaction. For example, referring to FIG. 4A, suggestion 408 may include a text portion, i.e., "Concert alert: local Band to play Song B at Jazz Hall this Sunday. Add to note? (concert alerts: local band will play song b. Add to note at jazz concert hall on this sunday. In this example, advice 408 suggests an action to add a Concert alert to the note, namely, "Local Band to play Song B at Jazz Hall this sunday" (the local band will perform song B at jazz Concert hall on this day of the week), or "conveert alert: local Band to play Song B at Jazz Hall this sunday" (the Concert alert: the local band will perform song B at jazz Concert hall on this day of the week).
In various implementations, at block 313, the system can cause the suggestion to be displayed at a user interface that displays the media content. As a non-limiting example, referring to fig. 4A, the suggestion 408 may be displayed in the selectable button 4081 at the user interface 400A of the content sharing application 403a along with the video content 407. Alternatively, the suggestion 408 may be displayed as a selectable pop-up widget that has minimized the negative impact of the user K's experience of viewing the video content 407. Alternatively, the suggestion 408 may be displayed when song B begins to play, and the display of the suggestion 408 may continue for a predefined period of time (e.g., 10 s) during which song B is still playing. Alternatively, the suggestion 408 may be displayed in a larger size in a center area of the media display area 404, and song B may pause when the suggestion 408 is displayed, and play may resume later when the suggestion 408 disappears. The manner in which advice 408 (or other advice) is displayed (e.g., size, location, duration, shape, color, etc.) is not limited to the examples provided herein.
In some implementations, referring to fig. 4B and 4C, after user K selects selectable suggestion 408 via, for example, mouse cursor 4082, the action suggested by suggestion 408 is performed, and accordingly selectable element 4081 may be modified to display confirmation message 408 '(e.g., "Concert alert saved in your note-turning app. To review, click here. (concert alert saved in your note recording application. To see, please click here.))", in which case the size, location, and other parameters of selectable element 4081 may be adjusted based on the length, font, and other parameters of confirmation message 408', and the disclosure is not limited thereto.
In some implementations, referring to FIG. 4D, user K may click on selectable element 4081 (or a portion thereof, e.g., an area displaying bold and underlined words "here") to view a note added to the note taking application, where a screenshot of the note (or a portion 408 "of the note, i.e.," 06-06-2022note1:Local Band to play Song B at Jazz Hall this Sunday (source: xxx; link: xxx), "Added from Content-sharing app. Reminder:1day before effect (default) (06-06-2022 note1: local band will perform on Song B (source: xxx; link: xxx). Added from the content sharing application: 1day before (default))") may be pop-up added as graphical element 4083 to the note taking application at user interface 400A of content sharing application 403 a. Alternatively, instead of a screen capture, the content of the note may be displayed as an overlay with respect to the video content 407, where the format of such content may be different from the content correspondingly displayed in the note-taking application.
5A, 5B, 5C, and 5D together illustrate examples in which operational suggestions are provided for video categorized into one of a plurality of predefined categories, according to various embodiments. As shown in fig. 5A, video content 507 of a video may be displayed at a video display area 504 of a user interface 500A of a content sharing application 503a, where the user interface 500A may be part of an interface 500 of a Web browser 501 a. In this example, the user may have a registered account 503c (e.g., account "K") of the content sharing application 503a, and the interface 500 of the Web browser 501a may have a tab 501b, the tab 501b indicating that the user is currently using the content sharing application 503a to access video content 507 of the video. The interface 500 of the Web browser 501a may further include a URL 502a for the video, and one or more buttons 502b that the user may click to return to a previous Web page, advance, or refresh the current page. The video may have a title 510 (e.g., TEN BEST JAZZ SONGS). Metadata associated with the video may include a title 510, information 511 such as the number of views and time and date of video creation, a name 512a of a channel that uploaded the video and may be subscribed to via a button 512d, a name 512b of the channel 512a, and additional channel information 512c.
In some embodiments, video (or video content 507) may be categorized into music categories based on metadata of the video (e.g., title 510 of the video). Alternatively or additionally, the video content 507 may be processed to classify videos into music categories. Based on videos classified into the music categories, the video content 507 may be processed to extract target content (e.g., names of songs A-J). Based on the extracted target content, one or more actions may be determined that are customized to the extracted target content. The one or more actions may include, but are not limited to: purchasing one or more of the downloaded songs A-J via the first third party application and searching the Internet (and/or the third party application) for live concerts that will play one or more of the songs A-J. Alternatively, the one or more actions may be ranked, and the highest ranked action may be selected as the action to be performed. The highest ranked actions may be performed by the corresponding third party application or automated assistant.
Based on the selected action (e.g., the highest ranked action) and/or the corresponding third party application, a suggestion such as suggestion 508 may be generated and displayed in selectable element 5081. Advice 508 may be natural language (Ticket on hole: local Band to play Song B at Jazz Hall this sunday. Se Ticket options.
Referring to fig. 5B and 5C, user K may use mouse cursor 5081 to select selectable element 5081, and selectable element 5081 may be updated with ticket information 508' (e.g., "New | Local Band to play Song B at Jazz Hall this sunday. Click for a ticket at TICKETAPP: general without seat- $25,general with seat- $35, vip front row- $65= > seecarb. As shown in fig. 5D, after user K selects to add two VIP FRONT ROW tickets, selectable element 5081 may be updated to display confirmation information 5081 "(e.g.," TICKETAPP shopping cart: VIP FRONT ROW (2) $130= > Check out. In some implementations, user K can select (e.g., click on) the confirmation information 5081 and can pop-up as an overlay of the video content 507 for user K to complete the checkout process without leaving the content sharing application 503a (e.g., closing the tab 501b, or opening a new user interface or new tab associated with the third party application TICKETAPP).
Alternatively, after user K selects to add two VIP FRONT tickets, selectable element 5081 may be updated to display confirmation information 5081 "(e.g.," TICKETAPP shopping cart: VIP FRONT ROW (2) $130 (TICKETAPP shopping cart: VIP FRONT ROW (2) $130) "). In this case, once user K selects to add two VIP front tickets to the shopping cart of TICKETAPP, user K may select to end viewing video content 407 and then may resume the checkout process by opening TICKETAPP, where the opened tickapp may display a shopping bag with two added VIP front tickets.
Fig. 7A and 7B illustrate examples in which operational suggestions are provided for media content categorized into a second category (e.g., recipe category) for user interaction, according to various embodiments. As shown in fig. 7A, a user of the client apparatus 70 may use a content access application to view a video having a title 703a, i.e., mandu (korean dumpling) made at home. Video content 701a of the video may be displayed in a video display area 701 of a user interface 700 of the client device 70. Metadata associated with the video may include a title 703a, video summary information 703b, a channel section 703c of channel F for uploading or sharing the video. Alternatively, a comment on the video (e.g., comment 705a from user M saying "Yummy | (happy |)") may be displayed at comment area 703 of user interface 700 of client device 70, and similar videos (e.g., 707a and 707 b) recommended to the user may be displayed at recommendation area 707 of user interface 700 of client device 70. Based on the metadata and/or video content 701a, videos may be categorized into recipe categories.
Further, referring to fig. 7B, after the user has watched the video for a minimum required period of time (e.g., 10 seconds), the recipes extracted from the video content 701a, as well as one or more operational suggestions (e.g., suggestion 707B and suggestion 707 c) may be displayed at the recommendation area 707. Suggestion 707b may suggest a first action to add a list of ingredients (minced meat, nappa cabbage, tofu, mung bean flour, kimchi, dumpling wrappers, soy sauce) to a shopping cart of a first third party application (e.g., grocery store distribution application). The suggestion 707b may suggest a second action to add this recipe to a second third party application that the user uses to create a set of recipes (and/or to share food prepared using those recipes). As non-limiting examples, the first and second actions may be determined based on videos classified into recipe categories and/or based on whether the user has a registered account with a third party application that will perform the first and second actions, respectively.
FIG. 8 illustrates an example in which actionable suggestions are provided for media content categorized into a third category (e.g., travel category) for user interaction according to various embodiments. As shown in fig. 8, a user of the client apparatus 80 may use the content access application to view a video blog having a title 803a, i.e., a plan for this summer. Video content 801a of the video may be displayed in a video display area 801 of a user interface 800 of the client device 80. Metadata associated with the video may include a title 803a, video summary information 803b, a channel section 803c of channel F for uploading or sharing the video. Alternatively, a comment on the video (e.g., comment 805a from user M saying "I was just way, my vlog if you are interested-.
In this example, based on processing of the video content 801a, the video may be categorized into travel categories, and target content may be extracted from the video content 801a, where the target content is customized for the travel categories (i.e., showing places to visit each day and itineraries 807a of hotels to stay). Alternatively, the target content, i.e., the travel 807a, may be displayed within the suggestion region 807. The target content may be applied to determine one or more actions, where the one or more actions may include, for example, querying flights to Paris, and sharing itineraries with friends or family. One or more suggestions may be generated and displayed based on the one or more actions and the corresponding application for performing the one or more actions. The one or more suggestions may include a first suggestion 807b that suggests a first action, namely, "checking flight to Paris (query for flights to Paris)". Alternatively or additionally, the one or more suggestions may include a second suggestion 807c that suggests a second action, namely "share this itinerary with my parents via WETRAVEL app (sharing this itinerary with my parents via a WETRAVEL application)". Once the user selects the first suggestion 807b (or the second suggestion 807 c), the first action (or the second action) may be performed accordingly without the user having to leave or close the user interface 800 of the client device 80.
Fig. 6 is a block diagram of an example computing device 610, the example computing device 610 may optionally be used to perform one or more aspects of the techniques described herein. In some implementations, one or more of the client computing devices, cloud-based automation assistant components, and/or other components may include one or more components of the example computing device 610.
The computing device 610 typically includes at least one processor 614 in communication with a plurality of peripheral devices via a bus subsystem 612. These peripheral devices may include a storage subsystem 624, which storage subsystem 624 includes, for example, a memory subsystem 625 and a file storage subsystem 626, a user interface output device 620, a user interface input device 622, and a network interface subsystem 616. The input devices and output devices allow a user to interact with the computing device 610. The network interface subsystem 616 provides an interface to external networks and is coupled to corresponding interface devices among other computing devices.
User interface input devices 622 may include a keyboard, a pointing device such as a mouse, trackball, touch pad, or tablet, a scanner, a touch screen incorporated into a display, an audio input device such as a voice recognition system, a microphone, and/or other types of input devices. In general, use of the term "input device" is intended to include all possible types of devices and ways of inputting information onto the computing device 610 or a communication network.
The user interface output device 620 may include a display subsystem, a printer, a facsimile machine, or a non-visual display such as an audio output device. The display subsystem may include a Cathode Ray Tube (CRT), a flat panel device such as a Liquid Crystal Display (LCD), a projection device, or some other mechanism for creating a viewable image. The display subsystem may also provide for non-visual display, such as via an audio output device. In general, use of the term "output device" is intended to include all possible types of devices and ways to output information from computing device 610 to a user or another machine or computing device.
These software modules are typically executed by processor 614 alone or in combination with other processors. The memory 625 used in the storage subsystem 624 may include a number of memories, including a main Random Access Memory (RAM) 630 for storing instructions and data during program execution and a Read Only Memory (ROM) 632 in which fixed instructions are stored. File storage subsystem 626 may provide persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive, and associated removable media, CD-ROM drive, optical drive, or removable media cartridge. Modules implementing the functions of certain embodiments may be stored in the storage subsystem 624 by the file storage subsystem 626, or in other machines accessible by the processor 614.
Although several embodiments have been described and illustrated herein, various other means and/or structures for performing the functions described herein and/or obtaining the results and/or one or more advantages described herein may be used and each of such variations and/or modifications are considered to be within the scope of the embodiments described herein. More generally, all parameters, dimensions, materials, and configurations described herein are exemplary and the actual parameters, dimensions, materials, and/or configurations will depend upon the specific application or applications in which the teachings are used. Those skilled in the art will recognize, or be able to ascertain using no more than routine experimentation, many equivalents to the specific embodiments described herein. It is, therefore, to be understood that the foregoing embodiments are presented by way of example only and that, within the scope of the appended claims and equivalents thereto, the embodiments may be practiced otherwise than as specifically described and claimed. Embodiments of the present disclosure are directed to each individual feature, system, article, material, kit, and/or method described herein. In addition, if two or more such features, systems, articles, materials, kits, and/or methods are not mutually inconsistent, any combination of such two or more such features, systems, articles, materials, kits, and/or methods is included within the scope of the present disclosure.
In various implementations, referring to fig. 9, a method 900 of displaying operational suggestions for media content is provided, wherein the operations of the method 900 are described with reference to a system performing the operations. The system of method 900 includes one or more processors and/or other components of a client device and/or a server device. Furthermore, although the operations of method 900 are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted, or added. At block 901, the system may assign a category label to the video. These and other embodiments of the technology disclosed herein may optionally include one or more of the following features. In some implementations, the tags can be assigned by the creator of the video. In some implementations, the category labels may be assigned by one or more viewers of the video. In some implementations, category labels may be assigned based on processing metadata associated with the video. Here, the metadata may include a title of the video created by the video creator, a video description of the video created by the video creator, video declarative text manually created for the video (which may be different from transcription of the video obtained via speech recognition, and may or may not be displayed to a viewer of the video), and other applicable data of the video.
In some implementations, the category labels may be assigned based on processing the video itself. For example, category labels may be assigned based on processing a transcription of a video and/or video frames of the video, where the transcription of the video may be obtained via speech recognition of the video. In some implementations, category labels may be assigned based on processing video and metadata. The category labels may be, for example, recipe labels, music labels, movie labels, travel labels, test preparation labels, shopping haul labels, experience sharing labels, story labels, biographical labels, indoor video game, dog training labels, concert labels, and the like, and the disclosure is not limited thereto.
In various embodiments, at block 903, the system may extract the target content from the video based on the assigned category label. In some implementations, for different category labels, different content extraction parameters may be identified, and different categories of target content may be extracted from the video based on the content extraction parameters identified correspondingly for the respective categories. As a non-limiting example, given a recipe tag, an ingredient list and/or information associated with the ingredient list (name, quantity, ingredient description, etc.) may be extracted from the video as target content using the ingredient extraction parameters of the recipe tag. As another non-limiting example, given a concert tag, a list of songs (and/or information associated with the list of songs, such as the title of the song, the performer of the song, whether it is a live version or a recorded version, etc.) may be extracted from the video as targeted content based on the concert extraction parameters predefined for the concert tag. As a further non-limiting example, given a travel tag, a list of destinations (and/or information associated with the list of destinations) may be extracted from video as target content based on travel extraction parameters assigned to the travel tag, where the information associated with the list of destinations may include, but is not limited to: hotel information, traffic information (flights, trains, buses, etc.), restaurant information, ticket information, travel information, comment information, celebrity information, etc. As an additional non-limiting example, given a home improvement tag, a list of tools/materials and associated information (e.g., color, manufacturer, quantity, etc.) may be extracted from the video as targeted content based on the home improvement extraction parameters of the home improvement tag.
In various implementations, at block 905, the system may determine actions that may be performed by one or more candidate applications based at least on the extracted target content. The one or more candidate applications may include one or more third party applications and automated assistants (which may also be referred to as "first party applications").
As a non-limiting example, when extracting an ingredient list (and/or information associated with the ingredient list) from a video as target content, adding the ingredient list to a shopping cart may be determined as an action, wherein such action may be performed (or eventually fulfilled/completed) by one or more candidate applications. Alternatively or additionally, given the list of ingredients (and/or information associated with the list of ingredients) extracted as the target content, adding the list of ingredients and the information associated with the list of ingredients to the electronic note may be determined as an action, and the action may be performed by a local note-taking application or a Web-based note-taking application query. Alternatively or additionally, given a list of ingredients (and/or information associated with the list of ingredients) extracted as target content, a reminder to set up to purchase the list of ingredients may be determined as an action. However, the examples herein are not intended to be limiting.
As another non-limiting example, given a list of songs (with or without singer information) extracted as target content, an order may be determined for an action to be performed by a third party application (e.g., a ticket application named "TICKETAPP") that will play a concert for one or more songs. For such actions (ordering concert tickets), one or more intermediate actions may need to be performed. For example, an intermediate action may be performed via an automated assistant, namely searching the internet for a concert that will play one of the songs in the list extracted from the video ("target content"). In this example, the automated assistant may search for and identify that the local band will perform song B, which is one of the songs extracted from the video during the performance of the jazz concert hall on the current day, with the ticket currently being sold via a third party application (i.e., ticket application "TICKETAPP"). Correspondingly, the addition of one or more performance tickets to the shopping cart of ticket application "TICKETAPP" may be determined in an action recommended to the user, which may be fulfilled via ticket application "TICKETAPP".
In various embodiments, optionally, at block 907, the system may determine (or select) a candidate application from the one or more candidate applications to perform (or fulfill) an action. Alternatively, the candidate application may be determined based on user history data of the candidate application (e.g., the frequency of candidate applications being used by the current user or other users during the past week or month). Alternatively, the candidate application may be determined based on a match between the actions and functions of the candidate application. Alternatively, a ranking score may be generated for the one or more candidate applications, and the candidate application with the highest ranking score may be selected as the candidate application to perform the action. The ranking scores herein may be generated based on user history data, matches between actions and functions of candidate applications, and the like.
In various embodiments, at block 909, the system may generate a suggestion of the suggested action based on the selected candidate application. As a non-limiting example, the suggestion may include a text portion that describes the action in natural language. Optionally, the suggestion may further include one or more icons or emoticons, where the icons or emoticons may or may not indicate the type of action (or a candidate application representing a final fulfillment action).
Optionally, in some implementations, the system may generate a suggestion (909) of a suggested action by: at block 9091, a suggestion is generated to include a description of the action in natural language (the "text portion" described above) for reading by a user of a content sharing application (which may also be referred to as a "content access application") delivering the video. For example, referring to fig. 2A, the suggestion 208 may include a text portion (i.e., "add concert ticket (add concert ticket)") describing an action in natural language (i.e., "Taylor to sing Song A at Fantasy Concert this May, add ticket.
Alternatively, the system may generate a suggestion (909) of a suggested action by: at block 9093, a link is generated identifying a command to perform an action corresponding to the selected candidate application. As a non-limiting example, the link may be a URL that causes one or more Taylor fantasy concert tickets to be added to a shopping cart of a ticket application such as "TICKETAPP" (or to a shopping cart of a ticket website such as "www.the-best-accept-ticket.com"). Alternatively, the system may generate a suggestion (909) of a suggested action by: at block 9095, the suggestion (or a portion thereof) is embedded with a link.
In various implementations, at block 911, the system may cause the suggestion to be displayed along with the video. For example, the suggestion may be displayed as a selectable element at a user interface of a content sharing application that displays the video. After reading the suggestion that suggests the action associated with the selected candidate application, the user of the content sharing application may select a selectable element in which the suggestion is displayed to execute a link embedded in the suggestion, wherein execution of the link fulfills the action (e.g., adding a concert ticket to a shopping cart of ticket application "TICKETAPP"). In some implementations, execution of the link is implemented in the background without opening the ticket application so that the user does not need to close or stop watching the video.
In some implementations, continuing with the above example, after performing the action, a confirmation message (e.g., "ticket successfully added to shopping cart" or "ticket successfully added to shopping cart, checkout. Alternatively, the confirmation message may disappear after a certain period of time (e.g., 10 s) is displayed, and after the confirmation message disappears, a selectable button embedded with a link for checkout of a shopping cart of the ticket application "TICKETAP" may be displayed at the user interface of the content sharing application for further user interaction in case the user subsequently decides to checkout a ticket added to the shopping cart without interrupting the user's experience of viewing the video. Alternatively, selectable buttons embedded with links to shopping cart checkout may be minimized, but configured with an attractive appearance to alert the user to their presence.
In various implementations, referring to fig. 10, a method 1000 of displaying operational suggestions for media content is provided, wherein the operations of the method 1000 are described with reference to a system performing the operations. The system of method 1000 includes one or more processors and/or other components of a client device and/or a server device. Furthermore, although the operations of method 1000 are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted, or added. In various embodiments, at block 1001, the system receives video. Alternatively, instead of video, an animated slide show, audio (e.g., music), or a combination thereof may be received. Optionally, after receiving the video, it may be determined whether the video includes a category label. If no category tags are detected or retrieved, the video and/or metadata associated with the video may be processed to determine category tags for the video.
In various embodiments, at block 1003, the system extracts the target content from the video. Alternatively, the type (or category) of target content to be extracted from the video may be determined or predefined based on the classification tag of the video. For example, for a video having a category tag (i.e., recipe tag), the category of target content to be extracted from the video may be predefined as "ingredients", and an ingredient list may be extracted from the video as target content.
In various embodiments, at block 1005, the system determines one or more actions corresponding to the one or more candidate applications based at least on the extracted target content. For example, for an ingredient list extracted from a video, a first action to add the ingredient list to a today's note in a first candidate application (e.g., note taking application) may be determined, and/or a second action to add the ingredient list to a shopping cart of a second third party application (e.g., shopping application) may be determined. Alternatively, a third action or more other than the first, second, and third actions may be determined, and the present disclosure is not limited thereto.
In various embodiments, optionally, at block 1007, the system filters the one or more actions. For example, the aforementioned first action or first and second actions may be filtered out from a plurality of actions including first, second, third and fourth actions.
In various embodiments, at block 1009, the system generates one or more corresponding suggestions based on the filtered one or more actions, each suggestion suggesting a respective one of the filtered one or more actions. As a non-limiting example, the one or more corresponding suggestions may include a first suggestion and a second suggestion. For example, the first suggestion may be "add the following ingredients to notes to make Mandu (Korean dumplings): pork, cabbage, tofu, dumpling wrappers "based on a first action of adding a list of ingredients (pork, cabbage, tofu, dumpling wrappers) to the today's notes in a first candidate application (e.g., note taking application). The second suggestion may be, for example, "add to shopping cart: pork, cabbage, tofu, dumpling wrappers "based on a second action of adding a list of ingredients (pork, cabbage, tofu, and dumpling wrappers) to a shopping cart of a second third party application (e.g., shopping application).
In various embodiments, at block 1011, the system causes the one or more corresponding suggestions to be rendered at a display in which the video is displayed. As described above, the one or more corresponding suggestions may be displayed in one or more selectable elements, respectively, and may be embedded with corresponding links, respectively, that when executed may cause corresponding ones of the filtered one or more actions to be performed. For example, when the user selects the first suggestion, a link embedded in the first suggestion may be performed, so that the first action may be performed. For example, when the user selects the first suggestion, notes containing the extracted target content and/or other information, such as "ingredients for making Mandu (korean dumplings)": pork, cabbage, tofu, dumpling wrappers "or" 06-06-2021 note: ingredients for preparing Mandus (korean dumplings): pork, cabbage, bean curd, and dumpling wrappers. The source is as follows: xxx "may be created in a note taking application without requiring the user to leave the video they are currently watching to open the note taking application, a series of manual inputs or switching operations between the content sharing application providing the video and the note taking application. In some implementations, the suggestions are pre-generated and indexed in the index based on the document information, and identifying the suggestions includes using the document information to identify the pre-generated suggestions in the index. In these embodiments, the document information may include a resource identifier of the document. In these embodiments, the resource identifier may be a uniform resource locator of the document.
In some implementations, a method implemented by one or more processors is provided, and the method includes: media content is received. The address of the media content may be used to receive or retrieve the media content, or the media content may be received directly. The media content may be video, audio, an automatic play slide show, or any other suitable content. In some embodiments, the method further comprises: media content and/or metadata associated with the media content is processed to classify the media content into a first category of a plurality of predefined categories. For example, media content may be processed to classify the media content into a first category (sometimes also referred to as a "first predefined category") of a plurality of predefined categories. Alternatively, metadata associated with the media content may be processed to classify the media content into a first category. Alternatively, both the media content and metadata associated with the media content are processed to classify the media content into the first category.
In some embodiments, the method further comprises: target content is extracted from the media content, wherein the target content is extracted based on the first category and is extracted based on the first category in response to the media content being classified into the first category. In some embodiments, the method further comprises: based on the extracted target content, an action corresponding to an application separate from the alternative application via which the media content is rendered is generated. An alternative application may be the aforementioned content access application, such as a social media platform, and the disclosure is not limited thereto. In some embodiments, the method further comprises: based on the generated action and the application to which the generated action corresponds, a selectable suggestion is generated, wherein the selectable suggestion includes a text portion describing the generated action.
In some embodiments, the method further comprises: such that the selectable suggestion is displayed at a display of the client device along with rendering of the media content at the alternative application. As a non-limiting example, when media content is rendered at an alternative application (e.g., a social media application), selectable suggestions may be displayed, wherein the selectable suggestions are displayed as overlays of the media content. In some implementations, the selectable suggestion, when selected, causes the application to perform an action. For example, the application may cause one or more items to be added to a shopping cart of the shopping application. In some other implementations, the selectable suggestion, when selected, causes an automated assistant (in communication with the application) to perform an action, wherein the action is related to a service or function of the application. For example, the automated assistant may cause one or more items to be added to a shopping cart of the shopping application.
These and other embodiments of the technology disclosed herein may optionally include one or more of the following features.
In some implementations, the media content includes a plurality of video frames or image frames, and processing the media content to classify the media content into the first category includes: detecting one or more target objects from a plurality of video frames or image frames; and classifying the media content based on the one or more detected target objects.
In some implementations, the media content includes an audio portion, and processing the media content to classify the media content into a first category includes: generating a transcription of the audio portion; detecting one or more keywords from the transcription of the audio portion; and categorizing the media content based on the one or more detected keywords.
In some implementations, processing the media content and/or metadata associated with the media content to classify the media content into a first category includes: processing the media content and/or metadata associated with the media content using a machine learning model to generate an output; and classifying the media content into a first category based on the output. As a non-limiting example, the output generated by the ML model may include a plurality of probabilities, wherein each of the probabilities may indicate how likely the media content belongs to a respective one of the one or more predefined categories. In this example, the media content may be classified into the first category based on a first probability indicating how likely the media content belongs to the first category that satisfies the threshold, while all other probabilities in the plurality of probabilities fail to satisfy the threshold.
In some implementations, processing the media content and/or metadata associated with the media content to classify the media content into a first category includes: it is determined whether metadata associated with the media content includes any category labels from a plurality of predefined category labels. In response to determining that the metadata associated with the media content includes a category label, the method may further include: bypassing processing of the media content and classifying the media content into a first category based on a classification tag included in metadata associated with the media content.
In some implementations, among other things, the metadata associated with the media content includes a title of the media content, a manual description of the media content, one or more manual descriptive words in the media content, and/or comments to the media content retrieved from a content sharing application that is displaying the media content on the display.
In some implementations, extracting the target content from the media content based on the first category includes: identifying content extraction parameters based on the content extraction parameters being assigned to the first category, wherein the content extraction parameters lack any assignment to one or more additional categories in the predefined categories; and in response to the content extraction parameter being assigned to the first category and the media content being categorized into the first category, extracting the target content from the media content using the content extraction parameter. In some implementations, the content extraction parameters are assigned only to the first category, and lack any assignment to any other of the predefined categories.
In some implementations, the selectable suggestion is embedded with a link, and when the selectable suggestion is selected, the link is executed to cause the action to be performed.
In some implementations, the media content can continue to be rendered uninterrupted at the display during execution of the action by the application. Alternatively, the media content may be paused at the display during the execution of the action by the application.
In some embodiments, the method further comprises: selecting an application from a plurality of candidate applications, wherein selecting the application comprises: generating a corresponding ranking score for each of the applications; ranking the applications based on the generated corresponding ranking scores; and selecting an application based on the ranking.
In some implementations, generating the corresponding ranking score includes: based on (1) historical data for an active user of the client device, historical data for a corresponding one of the applications; and/or (2) whether the active user has a registered account with a corresponding one of the applications, each of the corresponding ranking scores being generated.
In some embodiments, the method further comprises: receiving a selection of a suggestion; and in response to receiving the selection of the suggestion: such that the action is performed by the application. In some embodiments, the method further comprises: such that at the display, the additional suggestion is rendered along with the media content to replace the suggestion, wherein the additional suggestion includes a text portion that suggests an additional action to be performed via the application.
In some embodiments, the method further comprises: receiving a selection of additional suggestions; and in response to receiving the selection of the suggestion, causing the user interface of the application to be rendered as an overlay with respect to the media content, wherein the user interacts with the user interface of the application to perform the additional action.
In some implementations, receiving the media content includes: receiving an address of the media content; and resolving the address of the media content to retrieve the media content and/or metadata associated with the media content.
In some implementations, the suggestions are displayed at the display when the target content of the media content is rendered.
In some implementations, the suggestion is displayed at the display when a predefined period of time has elapsed since the media content was displayed.
In some embodiments, the advice is displayed for a predefined period of time and automatically disappears after being displayed for the predefined period of time.
In some implementations, a method implemented by one or more processors is provided, and the method includes: extracting target content from the media content depending on the determined media content classification; determining a plurality of candidate actions based on the extracted target content, each candidate action utilizing the extracted target content; selecting a subset of the candidate actions, including a first action that causes the first application to perform a first function based on the extracted target content and a second action that causes the second application to perform a second function based on the extracted target content; generating a plurality of selectable suggestions based on the selected subset of candidate actions, including a first selectable suggestion that when selected causes a first action to be performed by a first application and a second selectable suggestion that when selected causes a second action to be performed by a second application; and in response to determining that the client device is displaying the media content, causing a plurality of selectable suggestions to be displayed at the client device along with the media content.
In some embodiments, a method implemented by one or more processors is provided and includes: displaying media content via a content sharing application at a client device; and displaying the suggestions and the media content via the same interface of the content sharing application. In these implementations, suggestions can be generated based on target content extracted from media content. The suggestion may include a text portion describing an action corresponding to the application. The suggestion may be embedded with a link that, when executed, causes an action to be performed via the application. The suggestions may be displayed in selectable elements that, when selected, perform linking.
In some implementations, a method implemented by one or more processors is provided, and the method includes: media content is identified. Here, as a non-limiting example, the media content may be a video or a web page including a brief description of the video and the video. As another non-limiting example, the media content may be a voice book or music. The media content may be rendered visually or audibly via a content access application such as a Web browser or media player. The method may further comprise: natural language processing is performed on the transcription of the media content to annotate each of the transcribed plurality of phrases with a corresponding entity tag (or "entity category" or "category"). As a non-limiting example, when the media content is a web page that includes video and a short description of the video, the transcription of the media content may include: transcription of video obtained from speech recognition of video and a short description of video. In this example, the transcript of the media content may be subjected to natural language processing to annotate a plurality of phrases or words (e.g., bouillon, two eggs, 3 cups of milk, 8 pieces of french bread, and 1 teaspoon of bouillon whiskey) with a plurality of corresponding entity tags (e.g., "prepared food," "ingredient quantity," "ingredient name," etc.) throughout the transcript. In other words, transcription of media content may be handled as annotating a first phrase or word (i.e., "bouillon") with a first entity tag (i.e., "cuisine"), annotating a second phrase or word (i.e., "two") with a second entity tag (i.e., "ingredient quantity"), annotating a third phrase or word (i.e., "egg") with a third entity tag (i.e., "ingredient quantity"), annotating a fourth phrase or word (i.e., "3 cups") with a third entity tag (i.e., "ingredient quantity"), annotating a fifth phrase or word (i.e., "milk") with a third entity tag (i.e., "ingredient quantity"), annotating a sixth phrase or word (i.e., "8 pieces") with a second entity tag (i.e., "ingredient quantity"), annotating a seventh phrase or word (i.e., "french bread") with a third entity tag (i.e., "ingredient quantity"), annotating an eighth phrase or word (i.e., "1 key") with a third entity tag (i.e., "ingredient quantity"), and annotating a ninth entity tag (i.e., "ingredient quantity") or "bouillon") with a third entity tag (i.e., "ingredient quantity").
The foregoing method may further comprise: determining that the transcribed phrase sequence corresponds to an action, wherein determining that the phrase sequence corresponds to an action comprises: a corresponding entity class of the phrase sequence is determined to correspond to the action. For example, the actions may be determined based on corresponding entity categories (or entity tags) of the phrase sequence and/or based on associations between corresponding entity categories. Given a phrase or word sequence (i.e., a bouillon, two eggs, 3-cup milk, 8 pieces of french bread, and 1-teaspoon bouillon whiskey, or alternatively a bouillon is prepared using two eggs, 3-cup milk, 8 pieces of french bread, and 1-teaspoon bouillon whiskey ") and their corresponding annotated entity categories (" cuisine "," ingredient number "," ingredient name ", and/or" annotated "cook" prepared for the phrase "and an association between entity categories of notes at times (e.g., an association between" ingredient number "and" ingredient name ", an association between" cuisine "and" ingredient name ", and/or an association between" cook "and" cuisine "), the action corresponding to the phrase sequence may be determined as an action to add ingredients (e.g., eggs, milk, french bread, and bouillon whiskey) mentioned in the media content to the electronic shopping cart.
Alternatively, the action corresponding to the phrase sequence may be determined as an action to save ingredients mentioned in the media content (e.g., eggs, milk, french bread, and bouillon whiskey) in the electronic note. Depending on user activity/history data (e.g., a user often uses a digital photo album to save images/screenshots of movie scenes with movie names after viewing an official movie trailer using a social media application), application data (e.g., a digital photo album application is installed or accessible via a client device, while an electronic note recording application is not accessible via a client device), and/or other applicable data, the action corresponding to the phrase sequence may be determined to be other actions (e.g., save a video screenshot showing a movie name). The examples of actions corresponding to the phrase sequences herein are for illustration purposes only and are not intended to be limiting.
Alternatively, the action (e.g., adding ingredients to an electronic shopping cart) may be performed by an application, or may be performed by more than one application (in which case a particular application may be selected from among the more than one application to perform the action). Such actions may be performed by opening an application to run in the foreground to receive additional user input (e.g., removing an added ingredient from a shopping cart, changing the amount of added ingredient, or searching for and adding an additional ingredient to a shopping cart, etc.). Alternatively, the actions may be performed by an application running in the background without opening the application and running in the foreground, such that the user's experience of browsing the media content will not be interrupted and the computing resources may be conserved.
The foregoing method may further comprise: responsive to determining that the phrase sequence corresponds to an action: an application action is generated based on the phrase sequence, the application action comprising content (e.g., an embedded URL having one or more parameters such as ingredient name and/or ingredient number) determined based on the phrase sequence, wherein generation of the application action may cause the action to be performed by the application (separate from the foregoing content access application). In other words, while an action does not necessarily identify an application that is capable of performing the action, an application action specifies an application to perform the action. For example, based on phrases or words (i.e., bouillon, two eggs, 3 cups of milk, 8 pieces of french bread, and 1 teaspoon of bouillon whiskey) corresponding to the action of "saving a recipe electronically", an application action of "saving a recipe in an electronic note of note-taking application a" may be determined.
Alternatively, the content of the application action may be or may include one or more parameters of the application action, wherein the one or more parameters may be used to cause the action to be performed by the application. For example, the content of the application action may include a link (e.g., URL) or command with the one or more parameters. In this case, generating the application action including the content determined based on the phrase sequence may be generating an executable link or command that, when executed, causes the action to be performed by the application using the one or more parameters.
The one or more parameters may, for example, include: (a) A name or identifier of an application to perform the action, and (b) one or more objects (e.g., half-dozens of eggs) determined based on (i.e., not necessarily identical to) a phrase or word (e.g., two eggs) extracted from a transcription of media content (e.g., a cooking video). As non-limiting examples, the one or more parameters may include the name of the ingredients and/or their associated quantity (e.g., "two eggs," "3 cups of milk," "8 slices of french bread," and "1 teaspoon wave side whiskey") that are accurately referenced in the cooking recipe, and the name of the application performing the action (e.g., "note record application a") (e.g., electronically saving the ingredient names and/or their associated quantity).
As another non-limiting example, when the action is adding an ingredient to a shopping cart and the grocery application a is determined or selected to perform the action, the application action may be "adding an ingredient to a shopping cart of grocery application a". In this case, the one or more parameters of the application action may include the ingredients and the number of grocery item names (where the number may be a modified number, such as "8 ounce wave side whiskey", instead of the original number required by the recipe, such as "1 teaspoon wave side whiskey", where the modified number may be determined based on the package size of the grocery item, which should be greater than or equal to the original number mentioned in the recipe) in order to perform the cooking action.
In the above example, the application action may be adding "half-play egg, half-gallon milk, 1 pound french bread, and 4 ounce teaspoon side whiskey" to the shopping cart of Web-based grocery store application a (or alternatively, adding "half-play egg, half-gallon milk, 1 pound french bread, and 4 ounce teaspoon side whiskey" to the shopping cart of grocery store application a), while the media content mentions "two eggs", "3 cups of milk", "8 pieces of french bread", and "1 teaspoon side whiskey" as ingredients of the recipe. It should be noted that the application actions may alternatively be performed by an automated assistant in communication with the application. For example, instead of sending the one or more parameters to the grocery application a to add ingredients to the grocery application a's shopping cart, the one or more parameters may be sent to an automated assistant according to an embedded link so that the automated assistant adds ingredients to the grocery application a's shopping cart.
The foregoing method may further comprise: based on the generated application action, generating a selectable suggestion including a text portion describing the application action; and responsive to rendering the media content at the alternative application at the client device, causing the selectable suggestion to be displayed at a display of the client device along with the rendering of the media content at the alternative application, wherein the selectable suggestion, when selected, causes the application to perform an application action. Here, the alternative application may be a content access application, such as a Web browser, social media application, or any other suitable application. Descriptions of the selectable suggestions and descriptions of the rendering of the selectable suggestions may be found in other parts of the present disclosure and are not repeated here.
Additionally, some embodiments include one or more processors (e.g., a Central Processing Unit (CPU), a Graphics Processing Unit (GPU), and/or a Tensor Processing Unit (TPU)) of the one or more computing devices, wherein the one or more processors are operable to execute instructions stored in an associated memory, and wherein the instructions are configured to cause performance of any of the aforementioned methods. Some embodiments also include one or more non-transitory computer-readable storage media storing computer instructions executable by one or more processors to perform any of the foregoing methods. Some embodiments also include a computer program product comprising instructions executable by one or more processors to perform any of the foregoing methods.
Claims (20)
1. A method implemented using one or more processors, the method comprising:
receiving media content;
processing the media content and/or metadata associated with the media content to classify the media content into a first one of a plurality of predefined categories;
extracting target content from the media content, wherein the target content is extracted based on the first category in response to the media content being classified into the first category;
Generating, based on the extracted target content, an action corresponding to an application separate from an alternative application via which the media content is rendered;
based on the generated action and the application, generating a selectable suggestion comprising a text portion describing the action; and
causing the selectable suggestion to be displayed at a display of a client device along with rendering of the media content at the alternative application, wherein the selectable suggestion, when selected, causes the application to perform the action.
2. The method of claim 1, wherein the media content comprises a plurality of video frames or image frames, and processing the media content to classify the media content into the first category comprises:
detecting one or more target objects from the plurality of video frames or image frames; and
the media content is classified based on the one or more target objects.
3. The method of claim 1, wherein the media content comprises an audio portion, and processing the media content to classify the media content into the first category comprises:
generating a transcription of the audio portion;
Detecting one or more keywords from the transcription of the audio portion; and
the media content is classified based on the one or more keywords.
4. The method of claim 1, wherein processing the media content and/or metadata associated with the media content to classify the media content into the first category comprises:
processing the media content and/or the metadata associated with the media content using a machine learning model to generate an output; and
the media content is classified into the first category based on the output.
5. The method of claim 1, wherein processing the media content and/or metadata associated with the media content to classify the media content into the first category comprises:
determining whether the metadata associated with the media content includes a category tag of a plurality of predefined category tags, and
responsive to determining that the metadata associated with the media content includes a category tag of the plurality of predefined category tags:
bypassing processing of the media content
The media content is classified into the first category based on the classification tag included in the metadata associated with the media content.
6. The method of claim 1, wherein the metadata associated with the media content includes a title of the media content, a manual description of the media content, one or more manual descriptive words in the media content, and/or comments to the media content retrieved from a content sharing application that is displaying the media content on the display.
7. The method of claim 1, wherein extracting the target content from the media content based on the first category comprises:
identifying content extraction parameters assigned to the first category; and
the target content is extracted from the media content using the identified content extraction parameters.
8. The method of claim 7, wherein the content extraction parameters are assigned only to the first category and lack any assignment to any other predefined category of the plurality of predefined categories.
9. The method of claim 1, wherein the selectable suggestion is embedded with a link, and when the selectable suggestion is selected, the link is executed to cause the action to be performed.
10. The method of claim 1, wherein the media content continues to be rendered uninterrupted at the display during execution of the action by the application.
11. The method of claim 1, further comprising selecting the application from a plurality of candidate applications, wherein selecting the application comprises:
generating a corresponding ranking score for each of the plurality of candidate applications;
ranking the plurality of candidate applications based on the generated corresponding ranking scores; and
the application is selected from the plurality of candidate applications based on the ranking.
12. The method of claim 11, wherein the corresponding ranking score for each of the plurality of candidate applications is generated based on: historical data for a corresponding one of the plurality of candidate applications for an active user of the client device; and/or whether the active user has a registered account with the corresponding one of the plurality of candidate applications.
13. The method of claim 1, further comprising:
receiving a selection of the selectable suggestion; and
in response to receiving the selection of the selectable suggestion:
Causing the action to be performed by the application; and
such that at the display, additional suggestions are rendered in conjunction with the media content to replace the selectable suggestions, wherein the additional suggestions include text portions that suggest additional actions to be performed via the application.
14. The method of claim 13, further comprising:
receiving a selection of the additional suggestion; and
in response to receiving the selection of the additional suggestion, causing a user interface of the application to be rendered as an overlay with respect to the media content, wherein the user interacts with the user interface of the application to perform the additional action.
15. The method of claim 1, wherein receiving the media content comprises:
receiving an address of the media content; and
the address of the media content is parsed to retrieve the media content and/or the metadata associated with the media content.
16. The method of claim 1, wherein the selectable suggestion is displayed at the display when the target content of the media content is rendered.
17. The method of claim 1, wherein the selectable suggestion is displayed at the display when a predefined period of time has elapsed since the media content was displayed.
18. The method of claim 1, wherein the selectable suggestion is displayed for a predefined period of time and automatically disappears after display for the predefined period of time.
19. A method implemented using one or more processors, the method comprising:
extracting target content from the media content depending on the determined media content classification;
determining a plurality of candidate actions based on the extracted target content, each candidate action utilizing the extracted target content;
selecting a subset of the plurality of candidate actions, the subset comprising a first action that causes the first application to perform a first function based on the extracted target content and a second action that causes the second application to perform a second function based on the extracted target content;
generating a plurality of selectable suggestions based on the subset of the plurality of candidate actions, the plurality of selectable suggestions including a first selectable suggestion that when selected causes the first action to be performed by the first application and a second selectable suggestion that when selected causes the second action to be performed by the second application; and
In response to determining that the client device is displaying the media content:
the plurality of selectable suggestions are caused to be displayed at the client device along with the media content.
20. A method implemented using one or more processors, the method comprising:
identifying media content;
performing natural language processing on the transcription of the media content to annotate each of the transcribed plurality of phrases with a corresponding category;
determining that the transcribed phrase sequence corresponds to an action, wherein determining that the phrase sequence corresponds to the action comprises determining that a corresponding category of the phrase sequence corresponds to the action;
responsive to determining that the phrase sequence corresponds to the action:
generating an application action based on the phrase sequence, the application action comprising content based on the phrase sequence and causing the action to be performed by the application; and
generating, based on the application action, a selectable suggestion comprising a text portion describing the application action; and
in response to rendering the media content at an alternative application at the client device:
causing the selectable suggestion to be displayed at a display of the client device along with rendering of the media content at the alternative application, wherein the selectable suggestion, when selected, causes the application to perform the application action.
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/946,806 | 2022-09-16 | ||
US17/946,806 US20240095273A1 (en) | 2022-09-16 | 2022-09-16 | Actionable suggestions for media content |
Publications (1)
Publication Number | Publication Date |
---|---|
CN116010629A true CN116010629A (en) | 2023-04-25 |
Family
ID=86023159
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202310081793.3A Pending CN116010629A (en) | 2022-09-16 | 2023-01-17 | Operational suggestion of media content |
Country Status (2)
Country | Link |
---|---|
US (1) | US20240095273A1 (en) |
CN (1) | CN116010629A (en) |
Family Cites Families (14)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8255948B1 (en) * | 2008-04-23 | 2012-08-28 | Google Inc. | Demographic classifiers from media content |
EP2366162A1 (en) * | 2008-11-18 | 2011-09-21 | Collabrx, Inc. | Individualized cancer treatment |
US9479838B2 (en) * | 2009-11-24 | 2016-10-25 | Sam Makhlouf | System and method for distributing media content from multiple sources |
US9431002B2 (en) * | 2014-03-04 | 2016-08-30 | Tribune Digital Ventures, Llc | Real time popularity based audible content aquisition |
US10599706B2 (en) * | 2014-03-20 | 2020-03-24 | Gracenote Digital Ventures, Llc | Retrieving and playing out media content for a personalized playlist |
KR102533342B1 (en) * | 2014-03-25 | 2023-05-17 | 터치튠즈 뮤직 컴퍼니, 엘엘씨 | Digital jukebox device with improved user interfaces, and associated methods |
US9286892B2 (en) * | 2014-04-01 | 2016-03-15 | Google Inc. | Language modeling in speech recognition |
US9858346B2 (en) * | 2015-02-24 | 2018-01-02 | Echostar Technologies Llc | Apparatus, systems and methods for content playlist based on user location |
US20170150231A1 (en) * | 2015-11-19 | 2017-05-25 | Echostar Technologies Llc | Media content delivery selection |
US10524011B2 (en) * | 2015-12-30 | 2019-12-31 | Facebook, Inc. | Systems and methods for utilizing social metrics to provide videos in video categories |
US20170214980A1 (en) * | 2016-01-21 | 2017-07-27 | Hashplay Inc. | Method and system for presenting media content in environment |
US11347816B2 (en) * | 2017-12-01 | 2022-05-31 | At&T Intellectual Property I, L.P. | Adaptive clustering of media content from multiple different domains |
CN112004454B (en) * | 2018-04-11 | 2023-10-03 | 富士胶片株式会社 | endoscope system |
EP4018353A4 (en) * | 2019-08-22 | 2023-09-06 | The Governing Council of the University of Toronto | Systems and methods for extracting information from a dialogue |
-
2022
- 2022-09-16 US US17/946,806 patent/US20240095273A1/en active Pending
-
2023
- 2023-01-17 CN CN202310081793.3A patent/CN116010629A/en active Pending
Also Published As
Publication number | Publication date |
---|---|
US20240095273A1 (en) | 2024-03-21 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11023545B2 (en) | Method and device for displaying recommended contents | |
US20230153346A1 (en) | Predicting topics of potential relevance based on retrieved/created digital media files | |
US8352549B2 (en) | System and method for creating topic neighborhoods in a networked system | |
US9792303B2 (en) | Identifying data from keyword searches of color palettes and keyword trends | |
US9177407B2 (en) | Method and system for assembling animated media based on keyword and string input | |
KR101132509B1 (en) | Mobile system, search system and search result providing method for mobile search | |
US20100332497A1 (en) | Presenting an assembled sequence of preview videos | |
JP5395729B2 (en) | Information presentation device | |
JP2011519080A (en) | Method and apparatus for selecting related content for display in relation to media | |
EP2743874A1 (en) | Targeting objects to users based on search results in an online system | |
JP2014517364A (en) | Relevant extraction system and method for surf shopping | |
TW201243632A (en) | Search assistant system and method | |
US20220122147A1 (en) | Emotion calculation device, emotion calculation method, and program | |
US20160034957A1 (en) | Generating Advertisements for Search Results Associated With Entities Based on Aggregated Entity Bids | |
WO2023020167A1 (en) | Information display method and apparatus, computer device, and storage medium | |
US20230280966A1 (en) | Audio segment recommendation | |
US8725795B1 (en) | Content segment optimization techniques | |
US9418141B2 (en) | Systems and methods for providing a multi-function search box for creating word pages | |
CN111444405A (en) | User interaction method and device for searching, mobile terminal and storage medium | |
US20190332353A1 (en) | Gamifying voice search experience for children | |
US20240095273A1 (en) | Actionable suggestions for media content | |
US20150055936A1 (en) | Method and apparatus for dynamic presentation of composite media | |
CN112052656A (en) | Recommending topic patterns for documents | |
US20160034958A1 (en) | Generating Advertisements For Search Results That Are Associated With Entities | |
US20140362297A1 (en) | Method and apparatus for dynamic presentation of composite media |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |