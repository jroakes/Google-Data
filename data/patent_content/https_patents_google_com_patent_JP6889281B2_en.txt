JP6889281B2 - Analyzing electronic conversations for presentations in alternative interfaces - Google Patents
Analyzing electronic conversations for presentations in alternative interfaces Download PDFInfo
- Publication number
- JP6889281B2 JP6889281B2 JP2019559778A JP2019559778A JP6889281B2 JP 6889281 B2 JP6889281 B2 JP 6889281B2 JP 2019559778 A JP2019559778 A JP 2019559778A JP 2019559778 A JP2019559778 A JP 2019559778A JP 6889281 B2 JP6889281 B2 JP 6889281B2
- Authority
- JP
- Japan
- Prior art keywords
- conversation
- objects
- framing
- electronic
- linguistic
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
- G06F3/167—Audio in a user interface, e.g. using voice commands for navigating, audio feedback
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0481—Interaction techniques based on graphical user interfaces [GUI] based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance
- G06F3/04817—Interaction techniques based on graphical user interfaces [GUI] based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance using icons
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/205—Parsing
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/30—Semantic analysis
- G06F40/35—Discourse or dialogue representation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/40—Processing or translation of natural language
- G06F40/58—Use of machine translation, e.g. for multi-lingual retrieval, for server-side translation for client devices or for real-time translation
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/226—Procedures used during a speech recognition process, e.g. man-machine dialogue using non-speech characteristics
- G10L2015/228—Procedures used during a speech recognition process, e.g. man-machine dialogue using non-speech characteristics of application context
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L12/00—Data switching networks
- H04L12/02—Details
- H04L12/16—Arrangements for providing special services to substations
- H04L12/18—Arrangements for providing special services to substations for broadcast or conference, e.g. multicast
- H04L12/1813—Arrangements for providing special services to substations for broadcast or conference, e.g. multicast for computer conferences, e.g. chat rooms
- H04L12/1831—Tracking arrangements for later retrieval, e.g. recording contents, participants activities or behavior, network status
Description
モバイルデバイスのユーザは、電子会話に参加できる。電子会話は、混合メディア（例えば、１つまたは複数のテキストメッセージ、絵文字などの記号、省略されたテキスト短縮形、画像、ビデオ、マルチメディアオブジェクト、ユニフォームリソースロケーターなどの他のリソースへのリンクなど）を含む。ユーザは、ディスプレイ上で電子会話を閲覧することが適切でない設定にある場合がある（例えば、ユーザが車両を操作している場合）。従って、音声などの代替インタフェースを介した電子会話のプレゼンテーションは、あるモバイルデバイスユーザにとって有用であり得る。 Users of mobile devices can participate in electronic conversations. Electronic conversations are mixed media (eg, one or more text messages, symbols such as pictograms, abbreviated text abbreviations, images, videos, multimedia objects, links to other resources such as uniform resource locators, etc.) including. The user may be in a setting where it is not appropriate to view the electronic conversation on the display (eg, when the user is operating the vehicle). Therefore, presentation of electronic conversations via alternative interfaces such as voice can be useful for some mobile device users.
本明細書で提供される背景の説明は、本開示の状況を概略的に提示することを目的としている。この背景技術の欄において記載される限りにおいて、現時点で明記されている本発明者らの取り組み、ならびにその他、出願時に先行技術としてみなされない記載の態様は、明示的にも黙示的にも、本開示に対する先行技術として認めたものではない。 The background description provided herein is intended to illustrate the context of this disclosure. As far as this background art section is concerned, the efforts of the inventors as specified at this time, as well as other aspects of the description that are not considered prior art at the time of filing, are expressed and implied by the present invention. It is not recognized as prior art for disclosure.
いくつかの実装形態は、一般に電子メッセージングに関連し、特に、音声インタフェースまたは他の非表示インタフェースのような代替インタフェース（例えば、非表示インタフェース）における少なくとも部分的なプレゼンテーションのために電子会話を解析する方法およびシステムに関連する。 Some implementations are generally related to electronic messaging, especially analyzing electronic conversations for at least partial presentations in alternative interfaces such as voice interfaces or other hidden interfaces (eg, hidden interfaces). Related to methods and systems.
いくつかの実装形態は、コンピュータ実装方法を含み得る。方法は、複数のオブジェクトを含む電子会話内の１つまたは複数のオブジェクトを識別することであって、複数のオブジェクトは、異なるメディアタイプのものである、識別すること、１つまたは複数のオブジェクトを１つまたは複数のオブジェクトグループにグループ化することであって、各オブジェクトグループは、少なくとも１つのオブジェクトを含む、グループ化することを含み得る。方法は、１つまたは複数のオブジェクトグループに基づいて電子会話を分析して電子会話の会話構造を決定すること、電子会話の会話構造に基づいて１つまたは複数のオブジェクトグループに会話フレーミング（conversational framing）を適用して音声インタフェース会話プレゼンテーションを生成することも含み得る。電子会話のオブジェクトは、各メッセージまたは会話の一部内の情報に基づいて決定され得る。例えば、マルチメディアメッセージング会話には、各メッセージまたは会話の一部のコンテンツのタイプを示すヘッダー情報を含む会話内のオブジェクトのエンコードが存在し得る。例えば、テキスト部分は、ヘッダーまたはメッセージの他の部分に示される第１のタイプを有し、画像は、ヘッダーまたは他の部分で示される第２のタイプを有するなどの場合がある。従って、電子会話を分析し、電子会話の会話構造を決定することは、例えば、各オブジェクトが含むヘッダー情報を使用して、電子会話が含むオブジェクトのエンコードを分析することを含んでよく、ヘッダー情報は、１つまたは複数のオブジェクトが１つまたは複数のオブジェクトグループにグループ化される基となるオブジェクトタイプを示す。方法は、さらに、音声出力デバイスによる出力のために構成された前記音声インタフェース会話プレゼンテーションを提供することを含み得る。 Some implementations may include computer implementation methods. The method is to identify one or more objects in an electronic conversation that contains multiple objects, where the objects are of different media types, to identify one or more objects. Grouping into one or more object groups, each object group may include grouping, including at least one object. The method is to analyze the electronic conversation based on one or more object groups to determine the conversation structure of the electronic conversation, and to conversational framing to one or more object groups based on the conversation structure of the electronic conversation. ) Can also be applied to generate a voice interface conversational presentation. The objects of an electronic conversation can be determined based on the information in each message or part of the conversation. For example, a multimedia messaging conversation may have an encoding of an object in the conversation that contains header information that indicates the type of content for each message or part of the conversation. For example, the text portion may have a first type indicated in the header or other part of the message, the image may have a second type indicated in the header or other portion, and so on. Therefore, analyzing an electronic conversation and determining the conversation structure of the electronic conversation may include, for example, using the header information contained in each object to analyze the encoding of the objects contained in the electronic conversation. Indicates the underlying object type in which one or more objects are grouped into one or more object groups. The method may further include providing said voice interface conversational presentation configured for output by a voice output device.
いくつかの実装形態では、１つまたは複数のオブジェクトを識別することは、１つまたは複数の言語オブジェクトおよび１つまたは複数の非言語オブジェクトを識別することを含み得る。１つまたは複数のオブジェクトをグループ化することは、連続する言語オブジェクトをグループ化すること、連続する非言語オブジェクトをグループ化すること、および電子会話のシーケンス情報を保持することを含み得る。いくつかの実装形態では、各グループが、同じ（オブジェクト）タイプの１つまたは複数のオブジェクトを含む。いくつかの実装形態では、会話フレーミングを適用することは、非言語オブジェクトのコンテンツを自動的に識別すること、非言語オブジェクトのテキスト説明を含めることを含み得る。 In some implementations, identifying one or more objects may include identifying one or more linguistic objects and one or more non-linguistic objects. Grouping one or more objects can include grouping contiguous linguistic objects, grouping contiguous non-linguistic objects, and retaining sequence information for electronic conversations. In some implementations, each group contains one or more objects of the same (object) type. In some implementations, applying conversational framing may include automatically identifying the content of a non-linguistic object, including a text description of the non-linguistic object.
いくつかの実装形態では、会話フレーミングを適用することは、オブジェクトグループの前、または２つのオブジェクトグループの間に、事前定義された会話フレーム部分を挿入するための会話参照ポイントを使用することを含むことができ、参照ポイントは、電子会話を分析する際に識別され、新しいオブジェクトグループの開始を示す。 In some implementations, applying conversation framing involves using a conversation reference point to insert a predefined conversation frame portion before or between two object groups. Reference points can be identified when analyzing electronic conversations and indicate the start of a new object group.
会話フレーミングを適用することは、音声インタフェース会話プレゼンテーションの冒頭に、導入会話フレーミング部分を挿入することを含み得る。いくつかの実装形態では、導入会話フレーミング部分は、電子会話の１人または複数の参加者の識別を含み得る。 Applying conversational framing may include inserting an introductory conversational framing portion at the beginning of a voice interface conversational presentation. In some implementations, the introductory conversation framing portion may include the identification of one or more participants in an electronic conversation.
会話フレーミングを適用することは、１つまたは複数の中間会話フレーミング部分をオブジェクトグループのペア間に挿入することを含み得る。１つまたは複数のオブジェクトグループは、少なくとも２つのオブジェクトグループを含むことができ、１つまたは複数の中間会話フレーミング部分は、少なくとも２つのオブジェクトグループの、１つまたは複数のそれぞれのペア間に挿入され得る。 Applying conversational framing can include inserting one or more intermediate conversational framing pieces between pairs of object groups. One or more object groups can include at least two object groups, and one or more intermediate conversation framing portions are inserted between each pair of at least two object groups. obtain.
会話フレーミングを適用することは、最後のオブジェクトグループと前のオブジェクトグループとの間に結論会話フレーミング部分を挿入することを含むことができ、前のオブジェクトグループは、最後のオブジェクトグループの直前にある。電子会話内の１つまたは複数のオブジェクトを識別することは、電子会話内のオブジェクトのエンコードを分析することを含み得る。 Applying conversation framing can include inserting a conclusion conversation framing part between the last object group and the previous object group, with the previous object group just before the last object group. Identifying one or more objects in an electronic conversation can include analyzing the encoding of the objects in the electronic conversation.
電子会話を分析することは、少なくとも１つのルックアップテーブルに基づいて、識別されたオブジェクトのテキスト表現を提供することを含み得る。いくつかの実装形態では、ルックアップテーブルの各インデックスが少なくとも１つのテキスト表現に関連付けられるように、ルックアップテーブルは、オブジェクトに対応するインデックスと、インデックスに対応するテキスト表現とを含み得る。従って、分析は、例えば、グラフィカル記号の表現を言語要素として（例えば、テキスト説明として）提供することを含むことができ、記号要素を言語要素として表すことは、ルックアップテーブルを使用して、記号（例えば、絵文字）に対応する数値コードを検索し、テーブルからその記号の言語説明またはテキストを取り出し、会話の代替インタフェースプレゼンテーションのための要素としてその言語説明を提供することを含み得る。例えば、笑っている顔の絵文字が分析されて、「笑顔」などのテキストとして表され得る。従って、分析は、例えば、省略形のテキストを展開することも含むことができ（例えば、ＣＵＬ８ｔｒは、「後で会いましょう（see you later）」と展開され得る）、オブジェクト「ＣＵＬ８ｔｒ」または対応するインデックスは、「また後で会いましょう」というテキスト表現に関連付けられている。従って、会話フレーミングを適用することは、省略形のテキストを展開すること、省略形のテキストを所定の位置に残すこと、およびテキストを別の言語のテキストで置換することのうちの１つまたは複数を含み得る。会話フレーミングを適用することは、グラフィカル記号をグラフィカル記号のテキスト説明に変換することを含み得る。 Analyzing an electronic conversation can include providing a textual representation of the identified object based on at least one look-up table. In some implementations, a lookup table may include an index corresponding to an object and a text representation corresponding to the index so that each index of the lookup table is associated with at least one text representation. Thus, the analysis can include, for example, providing a representation of the graphical symbol as a linguistic element (eg, as a text description), and representing the symbol element as a linguistic element uses a lookup table to symbolize. It may include searching for the numeric code corresponding to (eg, pictogram), extracting the linguistic description or text of the symbol from the table, and providing the linguistic description as an element for an alternative interface presentation of the conversation. For example, a smiling face emoji can be analyzed and represented as text such as "smile". Thus, the analysis can also include, for example, expanding abbreviated text (eg, CUL8tr can be expanded as "see you later"), object "CUL8tr" or corresponding. The index to do is associated with the text expression "See you later". Therefore, applying conversational framing is one or more of expanding the abbreviated text, leaving the abbreviated text in place, and replacing the text with text in another language. May include. Applying conversational framing can include converting a graphical symbol into a textual description of the graphical symbol.
方法は、音声インタフェースプレゼンテーションがプレゼンテーションの適切な形式であることを、動作コンテキストが示すという決定がなされた場合、ユーザに音声クエリを提示すること、および音声クエリに対する音声応答を受信することを含み得る。方法は、音声応答に基づいて、音声インタフェース会話プレゼンテーションが音声出力デバイスから出力されるようにすることをさらに含み得る。電子会話は、テキスト部分と、画像、グラフィカル記号、およびユニフォームリソースロケーターのうちの少なくとも１つとを含み得る。 The method may include presenting a voice query to the user and receiving a voice response to the voice query if it is determined that the operating context indicates that the voice interface presentation is in the proper format of the presentation. .. The method may further include ensuring that the voice interface conversational presentation is output from the voice output device based on the voice response. The electronic conversation may include a text portion and at least one of an image, a graphical symbol, and a uniform resource locator.
方法は、デバイスのコンテキストを判定することを含み得る。方法は、デバイスのコンテキストが音声出力に適したものである場合（すなわち、プレゼンテーションの適切な形式）、音声インタフェース会話プレゼンテーションが音声出力デバイスを介して出力されるようにすること、デバイスのコンテキストが視覚表示に適したものである場合、電子会話をディスプレイデバイスに表示させることをさらに含み得る。デバイスのコンテキストを判定することは、例えば、デバイスの少なくとも１つのセンサ、特に音声出力デバイスまたはディスプレイデバイスの少なくとも１つのセンサを使用することを含み、それを介してデバイスの周囲の測定データが受信される。測定データは、デバイスのコンテキストを示し、例えば、ユーザが車両を運転していることを示すカメラ画像を含む場合があり、その結果、音声または他の非表示インタフェースによる電子会話のプレゼンテーションは、視覚的なインタフェースよりも適したプレゼンテーション形式である。 The method may include determining the context of the device. The method is to ensure that the audio interface conversational presentation is output through the audio output device, if the device context is suitable for audio output (ie, the proper format of the presentation), the device context is visual. If suitable for display, it may further include displaying the electronic conversation on a display device. Determining the context of a device involves, for example, using at least one sensor of the device, particularly at least one sensor of an audio output device or display device, through which measurement data around the device is received. To. The measurement data may indicate the context of the device and may include, for example, a camera image indicating that the user is driving a vehicle, so that the presentation of the electronic conversation by voice or other hidden interface is visual. It is a presentation format that is more suitable than a simple interface.
いくつかの実装形態は、１つまたは複数のプロセッサによって実行されると、１つまたは複数のプロセッサに動作を実行させるソフトウェア命令を格納した非一時的コンピュータ可読媒体に結合された１つまたは複数のプロセッサを含むシステムを含むことができる。動作は、複数のオブジェクトを含む電子会話内の１つまたは複数のオブジェクトを識別することであって、複数のオブジェクトは異なるメディアタイプのものである、識別すること、電子会話を分析して電子会話の会話構造を決定することを含み得る。動作は、電子会話の会話構造に基づいて、１つまたは複数のオブジェクトに会話フレーミングを適用し、代替インタフェース会話プレゼンテーションを生成することも含み得る。動作はさらに、非表示出力デバイスによる出力のために構成された少なくとも一部を有する代替インタフェース会話プレゼンテーションを提供することを含み得る。 Some embodiments, when executed by one or more processors, are combined with one or more non-temporary computer-readable media containing software instructions that cause one or more processors to perform operations. It can include a system that includes a processor. The action is to identify one or more objects in an electronic conversation that contains multiple objects, the multiple objects are of different media types, identifying, analyzing the electronic conversation and electronic conversation. May include determining the conversational structure of. The action may also include applying conversation framing to one or more objects based on the conversation structure of the electronic conversation to generate an alternative interface conversation presentation. The operation may further include providing an alternative interface conversation presentation with at least a portion configured for output by a hidden output device.
１つまたは複数のオブジェクトを識別することは、１つまたは複数の言語オブジェクトおよび１つまたは複数の非言語オブジェクトを識別することを含み得る。１つまたは複数のオブジェクトを識別することは、１つまたは複数のオブジェクトのエンコードを分析することを含み得る。 Identifying one or more objects can include identifying one or more linguistic objects and one or more non-linguistic objects. Identifying one or more objects can include analyzing the encoding of one or more objects.
いくつかの実装形態は、１つまたは複数のプロセッサによって実行されると、１つまたは複数のプロセッサに動作を実行させるソフトウェア命令を格納した非一時的コンピュータ可読媒体を含み得る。動作は、複数のオブジェクトを含む電子会話内の１つまたは複数のオブジェクトを識別することであって、複数のオブジェクトは異なるメディアタイプのものである、識別すること、電子会話を分析して電子会話の会話構造を決定することを含み得る。動作は、電子会話の会話構造に基づいて、１つまたは複数のオブジェクトに会話フレーミングを適用し、代替インタフェース会話プレゼンテーションを生成することも含み得る。動作はさらに、非表示出力デバイスによる出力のために構成された少なくとも一部を有する代替インタフェース会話プレゼンテーションを提供することを含み得る。１つまたは複数のオブジェクトを識別することは、１つまたは複数の言語オブジェクトおよび１つまたは複数の非言語オブジェクトを識別することを含み得る。 Some embodiments may include a non-transitory computer-readable medium containing software instructions that cause one or more processors to perform an operation when executed by one or more processors. The action is to identify one or more objects in an electronic conversation that contains multiple objects, the multiple objects are of different media types, identifying, analyzing the electronic conversation and electronic conversation. May include determining the conversational structure of. The action may also include applying conversation framing to one or more objects based on the conversation structure of the electronic conversation to generate an alternative interface conversation presentation. The operation may further include providing an alternative interface conversation presentation with at least a portion configured for output by a hidden output device. Identifying one or more objects can include identifying one or more linguistic objects and one or more non-linguistic objects.
本明細書で提供されるシステムおよび方法は、いくつかの従来のメッセージングシステムおよび方法の１つまたは複数の欠陥を克服し得る。例えば、電子メッセージングシステムは、ユーザが他のユーザと電子会話（例えば、電話、タブレット、ウェアラブルデバイス、コンピュータなどの電子デバイスを使用して行われ、チャットまたはメッセージングプラットフォーム、ソーシャルネットワークなどの電子プラットフォームによって仲介される会話）を行うことを可能にする。電子会話は、ユーザが受信されたメッセージを閲覧し、メッセージを送信し、電子会話への参加者を追加または削除し、会話を保存するなどのためのユーザインタフェースを提供するチャットまたはメッセージングアプリケーションを介して実行され得る。メッセージは、言語メッセージ、例えばテキスト、および非言語メッセージ、例えば、画像、ビデオ、ＵＲＬ、インタラクティブオブジェクト（例えば、招待、支払いの通知）、コンピュータファイルなどを含み得る。 The systems and methods provided herein can overcome one or more deficiencies in some conventional messaging systems and methods. For example, an electronic messaging system is one in which a user makes an electronic conversation with another user using an electronic device such as a telephone, tablet, wearable device, computer, etc., and is mediated by an electronic platform such as a chat or messaging platform, a social network, etc. It is possible to have a conversation). Electronic conversations are via chat or messaging applications that provide a user interface for users to view received messages, send messages, add or remove participants in electronic conversations, save conversations, and more. Can be executed. The message may include linguistic messages such as text and non-linguistic messages such as images, videos, URLs, interactive objects (eg invitations, payment notifications), computer files and the like.
電話、ウェアラブルデバイス、ヘッドマウントデバイス、タブレット、パーソナルコンピュータなどのモバイルデバイスを簡単に利用できるため、ユーザは、さまざまな設定やコンテキストにおいて互いに電子会話に参加することが可能である。ユーザが電子会話に参加している可能性のある設定およびコンテキストのいくつかは、会話の視覚的な表示に適さない場合がある。いくつかの従来のメッセージングシステムは、会話のための視覚表示ユーザインタフェース以外の代替インタフェースを提供しないか、効率的ではない代替インタフェースを提供する場合があり、電子会話を効率的に提示することがユーザにとって有用であり得る。 The ease of use of mobile devices such as telephones, wearable devices, head-mounted devices, tablets and personal computers allows users to participate in electronic conversations with each other in a variety of settings and contexts. Some of the settings and contexts in which the user may be participating in the electronic conversation may not be suitable for the visual display of the conversation. Some traditional messaging systems may not provide alternative interfaces other than the visual display user interface for conversations, or may provide inefficient alternative interfaces, and it is the user's ability to present electronic conversations efficiently. Can be useful for.
例えば、いくつかの従来のメッセージングシステムは、会話を提示するための代替インタフェース、例えば、会話の音声インタフェースプレゼンテーションなどを提供し得る。しかしながら、このようなプレゼンテーションは、１つまたは複数の制限のために有用でない場合がある。例えば、会話の従来の音声プレゼンテーションのいくつかは、非言語的な会話要素の逐語的な音声出力（例えば、他のリソースへのリンク、画像、ビデオ、絵文字、省略形テキストなど）を含み得る。非言語アイテムのそのような逐語的な音声出力は、ユーザの時間の非効率的な使用であり、プロセッサ利用、バッテリーまたは電力使用、メモリ利用などに関しても非効率的である可能性がある。例えば、対応するウェブページのタイトル（例えば、「Google Pixel、素晴らしいレビューでリリース」）ではなく、ＵＲＬ全体（例えば、http://www.technologynews.com/consumer/smartphone/2017/oct/google-pixel-released-to-great-reviews/）の音声プレゼンテーション（例えば、スピーカーデバイスを使用）には、より大きなバッテリー電力とプロセッサ利用が必要とされる可能性があり、前者のプレゼンテーションが、ユーザにとってより効果的である。 For example, some traditional messaging systems may provide an alternative interface for presenting a conversation, such as a voice interface presentation of the conversation. However, such presentations may not be useful due to one or more limitations. For example, some traditional audio presentations of conversation may include verbatim audio output of nonverbal conversational elements (eg, links to other resources, images, videos, pictograms, abbreviation text, etc.). Such verbatim audio output of non-verbal items is an inefficient use of the user's time and can also be inefficient in terms of processor utilization, battery or power usage, memory utilization, and so on. For example, the entire URL (eg http://www.technologynews.com/consumer/smartphone/2017/oct/google-pixel) rather than the title of the corresponding web page (eg "Google Pixel, released with great review") -released-to-great-reviews /) audio presentations (eg using speaker devices) may require more battery power and processor utilization, the former presentation being more effective for the user Is the target.
従来のメッセージングシステムは、代替インタフェースプレゼンテーションを提供するときに非言語的会話要素を認識しないか、代替インタフェースプレゼンテーションのための非言語的要素を効率的に解釈またはフォーマットしない場合がある。さらに、従来のメッセージングシステムは、ユーザがコンテキストを収集し、代替インタフェースプレゼンテーション内の会話の流れに従うことができるように、代替インタフェースでのプレゼンテーションのための会話フレーミングを提供しない場合がある。 Traditional messaging systems may not recognize nonverbal conversational elements when providing alternative interface presentations, or may not efficiently interpret or format nonverbal elements for alternative interface presentations. In addition, traditional messaging systems may not provide conversation framing for presentations on alternative interfaces so that users can collect context and follow the flow of conversation within the alternative interface presentation.
本明細書で説明する例示的なシステムおよび方法は、従来のメッセージングシステムの１つまたは複数の欠陥を克服して、非言語会話要素を処理し、会話フレーミングも提供する電子会話の代替インタフェースプレゼンテーションをユーザに提供することができる。いくつかの従来のメッセージングシステムの技術的な問題は、そのようなシステムが非言語的な会話要素を解釈せず、会話の代替インタフェースプレゼンテーションのための会話フレーミングを提供しないことであり得る。さらに、代替インタフェースを提供する従来のシステムは、デバイスの計算サイクル、メモリ使用量および／または電力使用量に関して非効率的である可能性のある逐語的な非言語アイテムを提示する代替インタフェースプレゼンテーションを生成する可能性がある。 The exemplary systems and methods described herein provide an alternative interface presentation for electronic conversation that overcomes one or more deficiencies in traditional messaging systems, handles nonverbal conversational elements, and also provides conversational framing. It can be provided to the user. The technical problem with some traditional messaging systems may be that such systems do not interpret nonverbal conversational elements and provide conversational framing for alternative interface presentations of conversations. In addition, traditional systems that provide alternative interfaces generate alternative interface presentations that present verbatim non-verbal items that may be inefficient with respect to device compute cycles, memory usage and / or power usage. there's a possibility that.
開示される主題は、電子会話の代替インタフェースプレゼンテーション（例えば、マルチメディアチャット会話の音声表示）を生成する特定の技術に関する。代替インタフェースプレゼンテーションは、コンピュータ上のプロセスをインスタンス化して会話を解析し、会話内のオブジェクトとそれらのオブジェクトのタイプ（例えば、言語または非言語）を決定することにより、会話を解析することに基づく。コンピュータ上のプロセスは、会話オブジェクトの１つまたは複数のグループを決定し、１つまたは複数のグループに対し会話フレーミングを提供し得る。 The subject matter disclosed relates to a particular technique for producing an alternative interface presentation of an electronic conversation (eg, a voice display of a multimedia chat conversation). Alternative interface presentations are based on analyzing conversations by instantiating processes on the computer, analyzing the conversation, and determining the objects in the conversation and the types of those objects (eg, linguistic or non-linguistic). A process on a computer may determine one or more groups of conversation objects and provide conversation framing to one or more groups.
特定の実装形態では、次の利点のうちの１つまたは複数を実現し得る。本明細書で説明する方法およびシステムに基づいて会話の代替インタフェースプレゼンテーションを生成する利点は、代替インタフェースプレゼンテーションが、（例えば、表示インタフェースが適切でない場合にユーザがメッセージを受信できるようにすることにより）ユーザにとってより効率的であり得ること、およびプレゼンテーションを提供するデバイスにとってより効率的であり得ること（例えば、計算リソース、バッテリーまたは電力リソース、および／またはメモリリソースの節約）である。別の利点は、デバイスが、非言語的な会話要素を解釈し、それらの要素をより効率的に提示することに基づいて、（例えば、非言語オブジェクトのテキストによる言語要約を提示することにより）より短いプレゼンテーション期間で会話を提示できることであり、その結果、処理操作が少なくなり、システム全体の待ち時間が短縮されることである。会話言語の規則で提示する別の利点は、インタフェースプレゼンテーションの新しい形式を学習する必要がなくなることを含む（例えば、会話言語に精通していることにより、ユーザは会話音声インタフェースを理解するためのトレーニングを必要としない場合がある）。デバイスの使用のためのトレーニング期間を排除することに加えて、会話音声インタフェースで提示することは、ユーザの認知負荷を軽減し、潜在的にデバイスの使用効率を改善するのに役立ち得る。 In a particular implementation, one or more of the following advantages may be realized: The advantage of generating an alternative interface presentation of a conversation based on the methods and systems described herein is that the alternative interface presentation (eg, by allowing the user to receive a message if the display interface is not appropriate). It can be more efficient for the user and more efficient for the device providing the presentation (eg, saving computing resources, battery or power resources, and / or memory resources). Another advantage is that the device interprets non-verbal conversational elements and presents those elements more efficiently (eg, by presenting a textual linguistic summary of non-verbal objects). The ability to present conversations in a shorter presentation period results in fewer processing operations and lower system-wide latency. Another advantage presented in conversational language rules includes eliminating the need to learn new forms of interface presentations (eg, by being familiar with conversational languages, users are trained to understand conversational voice interfaces. May not be needed). In addition to eliminating the training period for device use, presenting with a conversational voice interface can help reduce the cognitive load on the user and potentially improve device usage efficiency.
いくつかの実装形態のさらなる利点は、代替インタフェースで会話を提示する決定が、ユーザの許可により取得されるユーザのコンテキスト（例えば、デバイスの位置、デバイスの移動、カレンダー上のスケジュールされたアクティビティなどのうちの１つまたは複数によって示される）に基づき得ることであり、これにより、使用のコンテキストにふさわしい、または適したインタフェースを使用して会話が自動的に提示されるようになり、これにより、デバイスのより安全な使用（例えば、ユーザが車両を操作しているとき）、従来のユーザインタフェースが不適切なコンテキストでのデバイスの使用（例えば、ユーザがアクティビティ、例えば料理、トレーニング、掃除などに従事している場合）、よりタイムリーなデバイスの使用（例えば、ユーザはよりタイムリーな方法で会話に参加できるようになり得る）、および対話の改善（例えば、ユーザが使用コンテキストまたは設定を変更したときに会話に参加できる）などの利点がもたらされ得る。 A further advantage of some implementations is that the decision to present a conversation in an alternative interface is obtained with the user's permission, such as device location, device movement, scheduled activity on the calendar, etc. Based on (indicated by one or more of them), this allows the conversation to be automatically presented using an interface that is appropriate or suitable for the context of use, thereby allowing the device. Safer use of (eg, when the user is operating the vehicle), use of the device in a context where the traditional user interface is inappropriate (eg, the user engages in activities such as cooking, training, cleaning, etc.) (When), more timely use of the device (eg, the user may be able to participate in the conversation in a more timely manner), and improved interaction (eg, when the user changes the usage context or settings). Can bring benefits such as (you can participate in conversations).
図１は、本明細書で説明されるいくつかの実装形態で使用され得る例示的なネットワーク環境１００のブロック図を示している。いくつかの実装形態では、ネットワーク環境１００は、１つまたは複数のサーバシステム、例えば図１の例のサーバシステム１０２を含む。サーバシステム１０２は、例えばネットワーク１３０と通信することができる。サーバシステム１０２は、サーバデバイス１０４およびデータベース１０６または他の記憶デバイスを含み得る。ネットワーク環境１００は、ネットワーク１３０を介して、相互に、および／またはサーバシステム１０２と通信することができる１つまたは複数のクライアントデバイス、例えば、クライアントデバイス１２０、１２２、１２４、および１２６を含み得る。ネットワーク１３０は、インターネット、ローカルエリアネットワーク（ＬＡＮ）、無線ネットワーク、スイッチまたはハブ接続などのうちの１つまたは複数を含む、任意のタイプの通信ネットワークであり得る。いくつかの実装形態では、ネットワーク１３０は、例えばピアツーピア無線プロトコルを使用した、デバイス間のピアツーピア通信１３２を含み得る。
FIG. 1 shows a block diagram of an
説明を簡単にするために、図１は、サーバシステム１０２、サーバデバイス１０４、およびデータベース１０６の１つのブロックを示し、クライアントデバイス１２０、１２２、１２４、および１２６の４つのブロックを示している。サーバシステム１０２、１０４、および１０６を表すブロックは、複数のシステム、サーバデバイス、およびネットワークデータベースを表すことができ、ブロックは、示されているものとは異なる構成で提供され得る。例えば、サーバシステム１０２は、ネットワーク１３０を介して他のサーバシステムと通信できる複数のサーバシステムを表し得る。いくつかの例では、データベース１０６および／または他の記憶デバイスは、サーバデバイス１０４とは別個のサーバシステムブロックで提供され、ネットワーク１３０を介してサーバデバイス１０４および他のサーバシステムと通信することができる。また、任意の数のクライアントデバイスが存在し得る。
For simplicity, FIG. 1 shows one block of server system 102,
各クライアントデバイスは、デスクトップコンピュータ、ラップトップコンピュータ、ポータブルまたはモバイルデバイス、カメラ、携帯電話、スマートフォン、タブレットコンピュータ、テレビ、ＴＶセットトップボックスまたはエンターテインメントデバイス、ウェアラブルデバイス（ディスプレイメガネまたはゴーグル、ヘッドマウントディスプレイ（ＨＭＤ）、腕時計、ヘッドセット、アームバンド、ジュエリーなど）、仮想現実（ＶＲ）および／または拡張現実（ＡＲ）対応デバイス、携帯情報端末（ＰＤＡ）、メディアプレーヤー、ゲームデバイスなど、任意のタイプの電子デバイスであり得る。一部のクライアントデバイスは、データベース１０６または他のストレージに類似したローカルデータベースも有し得る。他の実装形態では、ネットワーク環境１００は、示された構成要素の全てを有しない場合があり、かつ／または本明細書に記載されたものの代わりにまたはそれに加えて他のタイプの要素を含む他の要素を有し得る。
Each client device is a desktop computer, laptop computer, portable or mobile device, camera, mobile phone, smartphone, tablet computer, TV, TV set top box or entertainment device, wearable device (display glasses or goggles, head mount display (HMD)). ), Watches, headsets, armbands, jewelry, etc.), virtual reality (VR) and / or augmented reality (AR) capable devices, personal digital assistants (PDAs), media players, gaming devices, and any type of electronic device. Can be. Some client devices may also have a local database similar to
様々な実装形態において、エンドユーザＵ１、Ｕ２、Ｕ３、およびＵ４は、会話の１人または複数の参加者を含むことができ、それぞれのクライアントデバイス１２０、１２２、１２４、および１２６を使用してサーバシステム１０２および／または相互に通信することができる。いくつかの例では、ユーザＵ１、Ｕ２、Ｕ３、およびＵ４は、それぞれのクライアントデバイスおよび／またはサーバシステム１０２上で実行されるアプリケーションを介して、および／またはサーバシステム１０２上で実装されるネットワークサービス、例えば画像共有サービス、メッセージングサービス、ソーシャルネットワークサービス、または他のタイプのネットワークサービスを介して相互に対話することができる。例えば、それぞれのクライアントデバイス１２０、１２２、１２４、および１２６は、１つまたは複数のサーバシステム（例えば、サーバシステム１０２）との間でデータを通信することができる。
In various implementations, end users U1, U2, U3, and U4 can include one or more participants in the conversation and use the
いくつかの実装形態では、サーバシステム１０２は、各クライアントデバイスがサーバシステム１０２および／またはネットワークサービスにアップロードされた通信コンテンツまたは共有コンテンツを受信できるように、適切なデータをクライアントデバイスに提供し得る。いくつかの例では、ユーザは、音声またはビデオ会議、音声、ビデオ、またはテキストチャット、または他の通信モードまたはアプリケーションを介して対話できる。いくつかの例では、ネットワークサービスは、ユーザがさまざまな通信を実行し、リンクや関連付けを形成し、画像、画像構成（例えば、１つまたは複数の画像を含むアルバム、画像コラージュ、動画など）、音声データ、その他のタイプのコンテンツなどの共有コンテンツをアップロードおよび投稿し、さまざまな形式のデータを受信し、および／または社会関連機能を実行することを可能にする任意のシステムを含み得る。例えば、ネットワークサービスは、ユーザが特定または複数の他のユーザにメッセージを送信し、ネットワークサービス内の他のユーザとの関連付けの形でソーシャルリンクを形成し、ユーザリスト、フレンドリスト、または他のユーザグループで他のユーザをグループ化し、テキスト、画像、画像構成、音声シーケンスまたは録音、またはネットワークサービスの指定されたセットのユーザによるアクセスのための他の種類のコンテンツを含むコンテンツを投稿または送信し、ライブビデオ、音声、および／またはテキストビデオ会議、またはサービスの他のユーザとのチャットなどに参加することを可能にする。いくつかの実装形態では、「ユーザ」は、１つまたは複数のプログラムまたは仮想エンティティ、およびシステムまたはネットワークとのインタフェースを持つ人物を含み得る。 In some implementations, the server system 102 may provide the client device with appropriate data so that each client device can receive communication or shared content uploaded to the server system 102 and / or network services. In some examples, users can interact via voice or video conferencing, voice, video, or text chat, or other communication modes or applications. In some examples, network services allow users to perform various communications, form links and associations, and image, image composition (eg, albums containing one or more images, image collages, videos, etc.), It may include any system that allows you to upload and post shared content, such as voice data and other types of content, receive data in various formats, and / or perform social-related functions. For example, a network service allows a user to send a message to a specific or multiple other users to form a social link in the form of an association with another user within the network service, a user list, a friend list, or another user. Group other users in a group and post or send content that contains text, images, image composition, audio sequences or recordings, or other types of content for access by a specified set of users of network services. Allows you to participate in live video, audio, and / or text video conferencing, or chat with other users of the service. In some implementations, a "user" can include one or more programs or virtual entities, and a person who has an interface with a system or network.
ユーザインタフェースは、クライアントデバイス１２０、１２２、１２４、１２６（代替的にはサーバシステム１０２）上での、画像、画像構成、データ、および他のコンテンツ、ならびに通信、プライバシー設定、通知、および他のデータの表示を可能にできる。そのようなインタフェースは、クライアントデバイス上のソフトウェア、サーバデバイス上のソフトウェア、および／またはサーバデバイス１０４上で実行されるクライアントソフトウェアとサーバソフトウェアとの組み合わせ、例えばサーバシステム１０２と通信するアプリケーションソフトウェアまたはクライアントソフトウェアを使用して表示され得る。ユーザインタフェースは、クライアントデバイスまたはサーバデバイスのディスプレイデバイス、例えばディスプレイスクリーン、プロジェクタなどによって表示され得る。いくつかの実装形態では、サーバシステム上で実行されるアプリケーションプログラムは、クライアントデバイスと通信して、クライアントデバイスでユーザ入力を受信し、クライアントデバイスで視覚データ、音声データなどのデータを出力できる。
User interfaces include images, image configurations, data, and other content on
本明細書で説明される機能の様々な実装形態は、任意のタイプのシステムおよび／またはサービスを使用し得る。例えば、ソーシャルネットワーキングサービス、画像収集および共有サービス、アシストメッセージングサービス、または他のネットワークサービス（例えば、インターネットに接続された）は、クライアントおよびサーバデバイスによってアクセスされる１つまたは複数の説明された機能を含み得る。任意のタイプの電子デバイスが、本明細書で説明されている機能を利用できる。いくつかの実装形態は、コンピュータネットワークから切断された、または断続的に接続されたクライアントまたはサーバデバイス上に、本明細書に記載の１つまたは複数の機能を提供することができる。いくつかの例では、表示デバイスを含むまたは表示デバイスに接続されたクライアントデバイスは、（例えば、通信ネットワークを介して接続されていない）クライアントデバイスのローカル記憶デバイスに格納された画像を調べて表示でき、ユーザにより閲覧可能な、本明細書に記載されるような機能および結果を提供し得る。 Various implementations of the functionality described herein may use any type of system and / or service. For example, social networking services, image collection and sharing services, assisted messaging services, or other network services (eg, connected to the Internet) have one or more described features accessed by client and server devices. Can include. Any type of electronic device can take advantage of the features described herein. Some implementations may provide one or more of the features described herein on a client or server device that is disconnected or intermittently connected from the computer network. In some examples, a client device that contains or is connected to a display device can inspect and display images stored on the client device's local storage device (eg, not connected via a communication network). , Can provide features and results as described herein that are viewable by the user.
図２は、いくつかの実装形態による、音声インタフェースなどの代替インタフェースでのプレゼンテーションのために電子会話を解析し、会話的にフレーム化するための例示的な方法２００（例えば、コンピュータ実装方法）を示すフロー図である。 FIG. 2 illustrates an exemplary method 200 (eg, a computer implementation method) for analyzing and conversationally framing an electronic conversation for presentation on an alternative interface, such as a voice interface, in several implementations. It is a flow chart which shows.
いくつかの実装形態では、方法２００は、例えば図１に示されるようなサーバシステム１０２上で実装され得る。他の実装形態では、方法２００の一部またはすべては、図１に示される１つまたは複数のクライアントデバイス１２０、１２２、１２４、または１２６、１つまたは複数のサーバデバイス、および／またはサーバデバイスおよびクライアントデバイスの両方上で実装され得る。記載の例では、実施するシステムは、１つまたは複数のデジタルハードウェアプロセッサまたは処理回路（「プロセッサ」）と、１つまたは複数の記憶デバイス（例えば、データベース１０６または他の記憶装置）とを含む。いくつかの実装形態では、１つまたは複数のサーバおよび／またはクライアントの異なる構成要素は、方法２００の異なるブロックまたは他の部分を実行することができる。
In some implementations,
いくつかの実装形態は、ユーザ入力および／またはデバイスコンテキスト（ユーザの許可を得て）に基づいて方法２００を開始することができる。例えば、ユーザは、表示されたユーザインタフェースから方法２００の開始を選択した可能性がある。いくつかの実装形態では、方法２００またはその一部は、ユーザ入力を介したユーザによるガイダンスで実行され得る。例えば、いくつかの実装形態は、音声インタフェースプレゼンテーションがプレゼンテーションの適切な形式であることを、動作コンテキストが示すという決定がなされた場合、ユーザに音声クエリを提示すること、および音声クエリに対する音声応答を受信することを含み得る。実装形態は、音声応答に基づいて、音声インタフェース会話プレゼンテーションが音声出力デバイスから出力されるようにすることを含み得る。システムは、ユーザからの明示的な許可がない限り、デバイスコンテキスト、場所などのようなユーザ情報を使用、処理、または格納しない。
Some implementations can initiate
いくつかの実装形態では、デバイスのコンテキストが音声（または他の非表示）インタフェースによる電子会話のプレゼンテーションが、プレゼンテーションの適切な形式であると判断された場合（例えば、ユーザが車を運転しているというコンテキスト、またはユーザが物理的な制限またはディスプレイを閲覧するのに適さない周囲などの他の制限のために非視覚インタフェースを要求したというコンテキストをデバイスが検出した場合）、方法２００は自動的に呼び出され得る（またはユーザの許可で自動的に呼び出され得る）。もう１つのコンテキストは、ユーザのデバイスのバッテリー残量が少ない場合、またはユーザが電源から離れている場合であり、この場合、ディスプレイ画面をオフにして音声ユーザインタフェースを提示することは、バッテリー容量を節約するために有利であり得る。コンテキストは、アプリケーションまたはシステムがコンテキスト情報を取得するための明示的な許可がユーザから与えられた場合に判定され得る。
In some implementations, if the device context determines that a presentation of an electronic conversation through a voice (or other hidden) interface is the appropriate format for the presentation (eg, the user is driving a car). If the device detects a context that the user has requested a non-visual interface due to physical restrictions or other restrictions such as surroundings that are not suitable for viewing the display),
いくつかの実装形態では、方法２００または方法の一部は、デバイスによって自動的に開始され得る。例えば、方法（またはその一部）は、定期的に実行されるか、または、１つまたは複数の特定のイベントまたは条件の発生に基づいて実行され得る。例えば、そのようなイベントまたは条件は、デバイス（例えば、ユーザデバイス）によって受信された、またはデバイスにアップロードされた、またはデバイスによりアクセス可能なメッセージ、方法２００の最後の実行以降に満了した所定の期間、および／または方法２００を実装するデバイスの設定で指定できる、発生する１つまたは複数の他のイベントまたは条件を含み得る。いくつかの実装形態では、そのような条件は、（ユーザの同意を得てデバイスまたは方法からアクセス可能な）ユーザの保存されたカスタム設定でユーザにより事前に指定され得る。いくつかの例では、デバイス（サーバまたはクライアント）は、電子会話メッセージを受信する１つまたは複数のアプリケーションにアクセスして方法２００を実行し得る（ユーザの同意が受信された場合）。別の例では、カメラ、携帯電話、タブレットコンピュータ、ウェアラブルデバイス、または他のクライアントデバイスは、電子会話メッセージを受信し、方法２００を実行することができる。加えて、または代替的に、クライアントデバイスは、ネットワークを介して１つまたは複数の電子会話メッセージをサーバに送信でき、サーバは、方法２００を使用してメッセージを処理できる。
In some implementations,
ブロック２０２では、電子会話内の１つまたは複数のオブジェクトが識別される。システムがユーザの電子会話にアクセスする前に、ユーザの許可が取得される。電子会話は、様々な異なるタイプのメッセージを有する会話などの混合メディア電子会話を含み得る。メッセージタイプは、テキストメッセージ、音声メッセージ、画像、ビデオ、記号（例えば、絵文字）、省略形のテキスト、他の言語のテキスト、インタラクティブオブジェクト、マルチメディアオブジェクト、通貨、仮想ギフト、インタラクティブ仮想オブジェクト、ゲームオブジェクトなどを含み得る。会話のオブジェクトは、各メッセージまたは会話の一部内の情報に基づいて決定され得る。例えば、マルチメディアメッセージング会話には、各メッセージまたは会話の一部のコンテンツのタイプを示すヘッダー情報を含む会話内のオブジェクトのエンコードが存在し得る。例えば、テキスト部分は、ヘッダーまたはメッセージの他の部分に示される第１のタイプを有し、画像は、ヘッダーまたは他の部分で示される第２のタイプを有するなどの場合がある。処理は２０４に続く。
２０４では、２０２で識別されたオブジェクトが、任意選択で１つまたは複数のオブジェクトグループにグループ化されることができ、各グループは、同じオブジェクトタイプ（例えば、グループ化された言語オブジェクト、画像、ＵＲＬなどのようなタイプによりグループ化された非言語オブジェクト）の１つまたは複数のオブジェクトを含む。例えば、会話が２つのテキストメッセージとそれに続く３つの画像、次にテキストメッセージおよび絵文字を含む場合、グループ化は、２つのテキストメッセージを含む第１のグループ、３つの画像の第２のグループ、テキストメッセージの第３のグループ、および絵文字を有する第４のグループを含み得る。別の例は、テキストの後にビデオとＵＲＬが続き、さらにテキストが続き、三目並べボードゲームオブジェクトが続くものを含む。オブジェクトのグループ化は、言語的および非言語的オブジェクトに基づいて、または独立したグループ、例えば、言語、ビデオ、画像、インタラクティブオブジェクトなどに基づいて行うことができ、適切な会話フレーミングが使用される。グループは、会話内のメッセージのシーケンスに対応する情報を保存およびキャプチャするように形成および編成され得る。シーケンス情報は、会話フレーミングを提供する際に、および会話を提示する際に使用され得る。 In 204, the objects identified in 202 can optionally be grouped into one or more object groups, each group having the same object type (eg, grouped language objects, images, URLs). Includes one or more objects (non-linguistic objects) grouped by type such as. For example, if the conversation contains two text messages followed by three images, then a text message and an emoji, the grouping is a first group containing two text messages, a second group of three images, text. It may include a third group of messages and a fourth group of pictograms. Another example includes text followed by video and URL, followed by text, followed by a tic-tac-toe board game object. Object grouping can be based on linguistic and non-verbal objects, or on independent groups such as language, video, images, interactive objects, etc., and appropriate conversation framing is used. Groups can be formed and organized to store and capture information that corresponds to a sequence of messages in a conversation. Sequence information can be used in providing conversation framing and in presenting conversations.
別の例では、図３のＡの電子会話３００が解析され（例えば２０２で）、言語オブジェクト（３０２）、２つの非言語オブジェクト（３０４、３０６）、言語オブジェクト２（３０８）、および非言語オブジェクト（３１０）を含むと判定され得る。電子会話３００は、ブロック２０４に従ってグループ化されて、図３のＢのグループ化された会話３０１を生成することができ、グループ化された会話３０１は、第１の言語オブジェクトグループ３１２、第１の非言語グループ３１４、第２の言語グループ３１６、および第２の非言語グループ３１８を含む。処理は２０６に続く。
In another example, the
２０６では、会話の会話構造を決定するために会話が分析される。ユーザの許可は、電子会話を分析する前に取得される（例えば、分析の許可は、電子会話へのアクセスの許可と併せて提供されるか、または個別に提供され得る）。会話を分析することは、２０４からの１つまたは複数のグループを分析すること（グループ化が使用された場合）および／または２０２で識別されたオブジェクトを分析することを含み得る。分析は、会話の構造を決定し、会話フレーミングを提供するために使用され得る会話参照ポイントを識別する。会話参照ポイントは、オブジェクトグループのペア間の会話におけるポイントを含み得る。 At 206, the conversation is analyzed to determine the conversation structure of the conversation. The user's permission is obtained before analyzing the electronic conversation (eg, the analysis permission may be provided in conjunction with or individually provided with permission to access the electronic conversation). Analyzing the conversation can include analyzing one or more groups from 204 (if grouping was used) and / or the objects identified by 202. The analysis determines the structure of the conversation and identifies conversation reference points that can be used to provide conversation framing. Conversation reference points can include points in conversations between pairs of object groups.
会話の分析は、非言語オブジェクトまたはオブジェクトのグループの分析も任意選択で含み得る。分析は、非言語オブジェクトを識別すること、音声などの代替インタフェースでのプレゼンテーションに適している可能性があるオブジェクトの言語表現を提供することを含み得る。例えば、３つの画像オブジェクトのグループが分析され、（例えば、ヘッダー情報の分析を介して）画像タイプの３つのオブジェクトであると決定され、「３つの写真」または類似の言語会話要素として表され得る。別の例では、システムは、音声インタフェースで招待状、例えば、「送信者Ａは明日の午後１０時の彼の家でのパーティーにあなたを招待しました」を提示し得る。別の例では、システムは、音声インタフェースで支払いまたはギフトの受領、例えば、「送信者Ａが１０ドルをあなたに送信し、「これは昨日の映画のチケット代です」と述べました」を提示し得る。このような分析は、アニメーション画像や動画にも適用され得る。 Conversation analysis may optionally include analysis of non-linguistic objects or groups of objects. Analysis can include identifying non-verbal objects and providing linguistic representations of objects that may be suitable for presentation on alternative interfaces such as audio. For example, a group of three image objects is analyzed and determined to be three objects of image type (eg, through analysis of header information) and can be represented as "three pictures" or similar linguistic conversation elements. .. In another example, the system may present an invitation with a voice interface, for example, "Sender A invited you to a party at his home at 10 pm tomorrow." In another example, the system presents a payment or gift receipt via a voice interface, for example, "Sender A sent you $ 10 and said," This is the ticket price for yesterday's movie. " Can be done. Such analysis may also be applied to animated images and moving images.
分析は、システムまたはサービスを使用して、画像、アニメーション画像、またはビデオのコンテンツを識別し、非言語オブジェクトのコンテンツの標示を提供することも含み得る。画像、ビデオなどの非言語オブジェクトのコンテンツへのアクセス、および画像コンテンツの分析などを実行する処理は、ユーザの明示的な許可を受けて実行されることが可能である。そして、コンテンツ標示は、非言語オブジェクトの言語表現に含まれ得る。例えば、メッセージが、テキストオブジェクト（「これらの場所をチェックしてください」）、３つの画像オブジェクト（例えば、ビーチリゾートの３つの写真）、および最後のテキストオブジェクト（「どれが好きか教えてください」）を含む場合、３つの画像オブジェクトの分析は、画像コンテンツ分析のためにシステムに画像を送信すること、コンテンツ分析の結果を音声表現で利用することを含み得る。例えば、画像コンテンツ分析が、３つの画像がそれぞれビーチリゾートであるという結果を返すと、分析は、非言語画像オブジェクトの言語表現として「ビーチリゾートの３つの写真」などの言語表現を生成できる。分析のために非言語オブジェクトを外部システムに送信することに加えて、またはその代替として、非言語オブジェクトメタデータを使用して、非言語オブジェクトのコンテンツまたは特徴を決定し、コンテンツのローカル分析が実行され得る。 The analysis may also include using a system or service to identify the content of an image, animated image, or video and provide a indication of the content of a non-verbal object. The process of accessing the content of non-linguistic objects such as images and videos, analyzing the image content, and the like can be performed with the explicit permission of the user. Content markings can then be included in the linguistic representation of non-linguistic objects. For example, the message is a text object ("Check these places"), three image objects (eg, three pictures of a beach resort), and the last text object ("Tell me which one you like"). ), The analysis of the three image objects may include sending an image to the system for image content analysis and using the results of the content analysis in a voice representation. For example, if the image content analysis returns the result that each of the three images is a beach resort, the analysis can generate a linguistic representation such as "three pictures of the beach resort" as the linguistic representation of the non-linguistic image object. In addition to or as an alternative to sending non-linguistic objects to external systems for analysis, non-linguistic object metadata is used to determine the content or characteristics of non-linguistic objects and perform local analysis of the content. Can be done.
分析は、グラフィカル記号の表現を言語要素として（例えば、テキスト説明として）提供することを含み得る。例えば、笑っている顔の絵文字は、分析されて、「笑顔」などのテキストとして表され得る。記号要素を言語要素として表すプログラム分析は、ルックアップテーブルを使用して、記号（例えば、絵文字）に対応する数値コードを検索し、テーブルからその記号の言語説明またはテキストを取り出し、会話の代替インタフェースプレゼンテーションのための要素としてその言語説明を提供することを含み得る。 The analysis may include providing a representation of the graphical symbol as a linguistic element (eg, as a text description). For example, a smiling face emoji can be analyzed and represented as text such as "smile". Program analysis, which represents a symbol element as a language element, uses a look-up table to look up the numeric code that corresponds to the symbol (for example, a pictogram), retrieve the language description or text of that symbol from the table, and use an alternative interface for conversation. It may include providing the linguistic description as an element for the presentation.
分析は、省略形のテキストを展開することも含み得る（例えば、ＣＵＬ８ｔｒは「また後で会いましょう（see you later）」と展開され得る）。省略形のテキストの展開は、テーブル検索またはその他の適切な方法で実現され得る。分析は、省略形のテキスト（または他の言語または非言語オブジェクト）を別の言語に翻訳することも含み得る。例えば、英語の省略形コードが使用されているが、ユーザがスペイン語話者である場合、システムは、英語の省略形コードを言語プレゼンテーションのためにスペイン語の単語に展開し得る。 The analysis can also include expanding the abbreviated text (for example, CUL8tr can be expanded as "see you later"). Expansion of the abbreviation text can be achieved by table search or other suitable method. The analysis may also include translating abbreviated text (or other language or non-linguistic objects) into another language. For example, if an English abbreviation code is used, but the user is a Spanish speaker, the system may expand the English abbreviation code into Spanish words for language presentations.
別の例では、図３のＢの非言語グループ３１４が分析され、図４の言語表現４０４を生成することができる。また、図３のＢの第２の非言語グループ３１８が分析され、図４の言語表現４０８を生成することができる。処理は２０８に続く。
In another example, the
２０８において、会話フレーミングが分析された会話に適用される。例えば、会話フレーミングは、２０２のオブジェクト、２０４のオブジェクトのグループ、および／または２０６で生成された言語表現のうちの１つまたは複数に適用され得る。会話フレーミングは、メッセージの送信者に関する情報を任意選択で含むことができる導入会話フレーミング部分を含み得る。例えば、導入会話フレーミングは、「それは言います（It says）」を含んでもよく、または導入会話フレーミングは、送信者を参照して「メアリーは言います（Mary says）」などを含んでもよい。導入会話フレーミング部分は、音声インタフェース会話プレゼンテーション（または点字などの他の代替インタフェース会話プレゼンテーション、または音声および制限付きディスプレイの組み合わせなど）の冒頭に挿入され得る。 At 208, conversation framing applies to the analyzed conversation. For example, conversation framing can be applied to one or more of 202 objects, a group of 204 objects, and / or a linguistic expression generated by 206. Conversation framing may include an introductory conversation framing portion that can optionally include information about the sender of the message. For example, the introductory conversation framing may include "It says", or the introductory conversation framing may include "Mary says" with reference to the sender. The introductory conversation framing portion may be inserted at the beginning of a voice interface conversation presentation (or other alternative interface conversation presentation such as Braille, or a combination of voice and restricted display).
会話フレーミングは、「次にそれは言います（Then it say）」などの１つまたは複数の中間フレーミング部分を任意選択で含み得る。中間フレーミング部分の存在と数は、会話内のオブジェクトまたはオブジェクトのグループの数、または代替インタフェースにおけるプレゼンテーションのために解析およびフレーム化される会話の一部に依存し得る。中間会話フレーミング部分は、オブジェクトグループの１つまたは複数のそれぞれのペアの間に挿入され得る。 Conversational framing can optionally include one or more intermediate framing parts such as "Then it say". The presence and number of intermediate framing parts can depend on the number of objects or groups of objects in the conversation, or part of the conversation that is parsed and framed for presentation in an alternative interface. The intermediate conversation framing portion can be inserted between each pair of objects.
会話フレーミングは、最後のオブジェクトグループの前に（または最後のオブジェクトグループと、最後のオブジェクトグループの隣の前のオブジェクトグループとの間に）挿入される「そして次にそれは言います（And then it says）」または「そして最後にそれは言います（And finally it says）」などのオプションの結論会話フレーミング部分を含み得る。導入、中間、および／または結論会話のフレーミング部分は、言語オブジェクトまたは非言語オブジェクトの言語表現と組み合わせられ得る。例えば、第２の非言語グループ３１８（絵文字）は、結論会話フレーミング要素と組み合わせられて、絵文字の言語表現と結論会話フレーミング部分（例えば、言語表現４０８）とを含む会話要素を生成し得る。一部の実装形態は、「５０件の未読メッセージがあります。ジェシカとショーンはタイでの休暇について話し、いくつかの写真を交換し、１２月の第１週のチケットを見つけました。」のような、より長い概要（複数のメッセージの概要、メッセージ数の概要、複数の会話の概要など）を提供し得る。 Conversation framing is inserted before the last object group (or between the last object group and the previous object group next to the last object group) "and then it says (And then it says). Can include optional conclusion conversation framing parts such as ")" or "And finally it says". Introductory, intermediate, and / or conclusions The framing part of a conversation can be combined with the linguistic representation of a linguistic or non-linguistic object. For example, a second non-linguistic group 318 (pictogram) may be combined with a conclusion conversation framing element to generate a conversational element that includes a linguistic representation of the pictogram and a conclusion conversation framing portion (eg, linguistic expression 408). Some implementations say, "There are 50 unread messages. Jessica and Sean talked about their vacation in Thailand, exchanged some photos, and found a ticket for the first week of December." It can provide a longer summary, such as a summary of multiple messages, a summary of the number of messages, a summary of multiple conversations, and so on.
例えば、図３のＢのグループ化された会話３０１は、図４に示されるような音声インタフェース会話プレゼンテーション４００を生成するために適用される会話フレーミングを有することができる。会話フレーミングは、導入会話フレーミング部分４０２、中間会話フレーミング部分４０６、および言語表現４０８（結論会話フレーミング部分を含む）を含むことができる。処理は２１０に続く。
For example, the grouped
２１０では、代替インタフェース会話プレゼンテーションが、出力として提供される。例えば、音声インタフェース会話プレゼンテーション４００は、スピーカー、ヘッドフォンなどの音声出力デバイスを介して再生するための出力として提供され得る。 At 210, an alternative interface conversation presentation is provided as output. For example, the voice interface conversation presentation 400 may be provided as an output for playback via a voice output device such as a speaker, headphones, or the like.
図２では、様々なブロック（例えば、ブロック２０２〜２１０）が、順次実行されるものとして示されている。しかしながら、これらのブロックは、特定の実施形態に適するように便宜的に再配置されてもよく、これらのブロックまたはその一部は、いくつかの実施形態で同時に実行されてもよいことが理解されるであろう。また、いくつかの例では、さまざまなブロックが、削除され、追加のブロックに分割され、および／または他のブロックと組み合わせられ得ることも理解されよう。テーブルを使用して、テーブルの値に基づいて閾値を決定できる。 In FIG. 2, various blocks (eg, blocks 202-210) are shown as being executed sequentially. However, it is understood that these blocks may be conveniently rearranged to suit a particular embodiment, and that these blocks or parts thereof may be performed simultaneously in some embodiments. Will be. It will also be appreciated that in some examples various blocks can be deleted, split into additional blocks, and / or combined with other blocks. You can use a table to determine the threshold based on the values in the table.
図３のＡおよびＢは、例示的な電子会話３００および例示的なグループ化された電子会話３０１の図を示す。電子会話３００は、第１の言語オブジェクト３０２（例えば、「これらは私が見つけることのできる最良の選択肢です」、または別の例では「これらは私が見つけることのできる最良の選択肢であるとユーザＡは言っています」）を含む。電子会話３００は、２つの非言語オブジェクト３０４および３０６（例えば、ＵＲＬ）を含む。電子会話３００は、第２の言語オブジェクト３０８（例えば、「どう思いますか？（What do you think?）」）に続き、非言語オブジェクト３１０（例えば、笑顔の絵文字）が続く。
A and B of FIG. 3 show diagrams of an exemplary
グループ化された電子会話３０１は、第１の言語オブジェクトグループ３１２、第１の非言語オブジェクトグループ３１４、第２の言語オブジェクトグループ３１６、および第２の非言語オブジェクトグループ３１８を含む。
The grouped
図４は、音声インタフェース会話プレゼンテーション４００の図を示し、これは、導入会話フレーミング４０２、第１の言語オブジェクトグループ３１２、第１の非言語オブジェクトグループ４０４の会話表現、中間会話フレーミング４０６、第２の言語オブジェクトグループ３１６、および第２の非言語オブジェクトグループ４０８の会話フレーミング（結論会話フレーミングを含む）を含む。
FIG. 4 shows a diagram of a voice interface conversation presentation 400, which is a conversational representation of an introductory conversation framing 402, a first
図５は、本明細書に記載の１つまたは複数の機能を実装するために使用することができる例示的なデバイス５００のブロック図である。一例では、デバイス５００を使用して、コンピュータデバイス、例えば、サーバデバイス（例えば、図１のサーバデバイス１０４）を実装し、本明細書で説明する適切な方法実装形態を実行することができる。デバイス５００は、任意の適切なコンピュータシステム、サーバ、または他の電子もしくはハードウェアデバイスであり得る。例えば、デバイス５００は、メインフレームコンピュータ、デスクトップコンピュータ、ワークステーション、ポータブルコンピュータ、または電子デバイス（ポータブルデバイス、モバイルデバイス、携帯電話、スマートフォン、タブレットコンピュータ、テレビ、ＴＶセットトップボックス、携帯情報端末（ＰＤＡ）、メディアプレーヤー、ゲームデバイス、ウェアラブルデバイスなど）であってよい。いくつかの実装形態では、デバイス５００は、プロセッサ５０２、メモリ５０４、およびＩ／Ｏインタフェース５０６を含む。
FIG. 5 is a block diagram of an
プロセッサ５０２は、プログラムコードを実行し、デバイス５００の基本動作を制御するための１つまたは複数のプロセッサおよび／または処理回路であり得る。「プロセッサ」は、データ、信号または他の情報を処理する任意の適切なハードウェアおよび／またはソフトウェアシステム、機構、またはコンポーネントを含む。プロセッサは、汎用中央処理ユニット（ＣＰＵ）、複数の処理ユニット、機能を達成するための専用回路、または他のシステムを有するシステムを含み得る。処理は特定の地理的位置に限定される必要はなく、または時間的な制限がある必要もない。例えば、プロセッサは、「リアルタイム」、「オフライン」、「バッチモード」などでその機能を実行し得る。処理の一部は、異なる時間および異なる場所で、異なる（または同じ）処理システムによって実行され得る。コンピュータは、メモリと通信する任意のプロセッサであり得る。
メモリ５０４は、通常、プロセッサ５０２によるアクセスのためにデバイス５００に設けられ、プロセッサにより実行される命令を格納するのに適した、例えば、ランダムアクセスメモリ（ＲＡＭ）、読み取り専用メモリ（ＲＯＭ）、電気的消去可能読み取り専用メモリ（ＥＥＰＲＯＭ）、フラッシュメモリなどの任意の適切なプロセッサ読み取り可能記憶媒体とすることができ、プロセッサ５０２とは別個に配置され、および／またはそれと一体化され得る。メモリ５０４は、オペレーティングシステム５０８、１つまたは複数のアプリケーション５１０、例えば、代替インタフェースプレゼンテーションアプリケーション５１２、他のアプリケーション５１４、およびアプリケーションデータ５２０を含む、プロセッサ５０２によってサーバデバイス５００上で動作するソフトウェアを格納することができる。いくつかの実装形態では、アプリケーション５１０は、プロセッサ５０２が本明細書に記載の機能、例えば、図２の方法のうちのいくつかまたは全てを実施することを可能とする命令を含み得る。
Memory 504 is typically provided in
例えば、アプリケーション５１０は、本明細書で説明するように、電子会話解析および会話フレーミング、および他の機能、例えば、音声インタフェースまたは他の非表示インタフェースまたは非表示出力デバイスによる出力のために構成された会話を提供することができる音声インタフェースプレゼンテーションアプリケーション４１２を含み得る。いくつかの実装形態では、音声インタフェースプレゼンテーションアプリケーションは、デバイス５００の音声デバイスでの出力のための音声インタフェースプレゼンテーションを提供するために、１つまたは複数の電子会話メッセージを受信し、メッセージを解析し、解析されたメッセージに会話フレーミングを追加することを含み得る。他のアプリケーション５１４（またはエンジン）は、同様に、または代替的に、アプリケーション５１０、例えば、電子メールアプリケーション、ＳＭＳおよび他の電話通信アプリケーション、ウェブブラウザアプリケーション、メディアディスプレイアプリケーション、通信アプリケーション、ウェブホスティングエンジンまたはアプリケーション、ソーシャルネットワーキングエンジンまたはアプリケーションなどに含まれてもよい。あるいは、メモリ５０４内のソフトウェアのうちの任意のものは、他の任意の適切なストレージロケーションまたはコンピュータ可読媒体に格納され得る。加えて、メモリ５０４（および／または他の接続された記憶装置）は、電子会話メッセージ、電子会話メッセージのグループ、会話フレーミング要素、ユーザデータおよびプリファレンス、および本明細書に記載の機能で用いられる他の命令およびデータなどのアプリケーションデータを格納し得る。メモリ５０４および他の任意の種類の記憶装置（磁気ディスク、光ディスク、磁気テープ、または他の有形の媒体）は、「記憶装置」または「記憶デバイス」と見なすことができる。
For example, application 510 is configured for electronic conversation analysis and conversation framing, and output by other functions, such as voice interfaces or other hidden interfaces or hidden output devices, as described herein. It may include a voice interface presentation application 412 capable of providing a conversation. In some implementations, the voice interface presentation application receives one or more electronic conversation messages, parses the message, and provides a voice interface presentation for output on the voice device of
例えば、アプリケーションデータ５２０は、グループ５２２およびフレーミング５２４を含み得る。例えば、グループ５２２は、グループ化された電子会話メッセージまたは部分（例えば、テキストグループ、画像グループ、ＵＲＬグループなど）を含み得る。会話フレーミング５２４は、音声または他のインタフェースによるプレゼンテーションのために電子会話グループをフレーム化するために使用され得る複数の会話フレーミング要素を含むことができる。例えば、会話フレーミング要素は、フレーミング部分のリストまたはテーブルに格納され得る。例えば、単一の会話フレーミング要素は、「写真（a picture）」、「＜Ｘ＞の写真（a picture of <X>）」、「動画（a video）」、「＜Ｘ＞の動画（a video of <X>）」、「ウェブリンク（a weblink）」、「＜Ｘ＞へのリンク（a link to <X>）」などを含み得る。類似タイプの複数のグループ化の会話フレーミング部分は、「いくつかの写真（some pictures）」、「＜Ｘ＞のいくつかの写真（some pictures of <X>）」などを含み得る。一部の実装形態では、機械学習を使用して、上記のＸで示されるような画像説明を決定できる（例えば、ユーザは画像をＸとして説明し、自動化されたアシスタントは、類似の画像をＸとして説明する）。 For example, application data 520 may include group 522 and framing 524. For example, group 522 may include grouped electronic conversation messages or parts (eg, text groups, image groups, URL groups, etc.). Conversation framing 524 can include multiple conversation framing elements that can be used to frame electronic conversation groups for presentations via voice or other interfaces. For example, conversation framing elements can be stored in a list or table of framing parts. For example, a single conversation framing element is "a picture", "a picture of <X>", "a video", "<X> video (a)". It may include "video of <X>)", "a weblink", "link to <X>", and the like. A conversation framing portion of a plurality of similar types of groups may include "some pictures", "some pictures of <X>", and the like. In some implementations, machine learning can be used to determine an image description as indicated by X above (eg, the user describes the image as X, and an automated assistant Xs a similar image. Explain as).
Ｉ／Ｏインタフェース５０６は、デバイス５００を他のシステムおよびデバイスとインタフェースすることを可能にするための機能を提供することができる。例えば、ネットワーク通信デバイス、記憶デバイス（例えば、メモリおよび／またはデータベース１０６）、および入力／出力デバイスは、Ｉ／Ｏインタフェース５０６を介して通信することができる。いくつかの実装形態では、Ｉ／Ｏインタフェースは、入力デバイス（キーボード、ポインティングデバイス、タッチスクリーン、マイクロフォン、カメラ、スキャナなど）および／または出力デバイス（表示デバイス、スピーカーデバイス、プリンター、モーター、触覚出力デバイスなど）を含むインタフェースデバイスに接続することができる。音声入力／出力デバイス５３０は、本明細書で説明されるように、音声入力を受信し、音声出力（例えば、音声インタフェース出力）を提供するために使用され得る入力および出力デバイスの一例である。音声入力／出力デバイス５３０は、ローカル接続（例えば、有線バス、無線インタフェース）および／またはネットワーク接続を介してデバイス５００に接続されることが可能であり、任意の適切なデバイスとすることができ、そのいくつかの例が以下に説明される。
The I /
説明を簡単にするために、図５は、プロセッサ５０２、メモリ５０４、Ｉ／Ｏインタフェース５０６、およびソフトウェアブロック５０８および５１０の各々について１つのブロックを示している。これらのブロックは、１つまたは複数のプロセッサまたは処理回路、オペレーティングシステム、メモリ、Ｉ／Ｏインタフェース、アプリケーション、および／またはソフトウェアモジュールを表すことができる。他の実装形態では、デバイス５００は、示された構成要素の全てを有しない場合があり、かつ／または本明細書に示されたものの代わりにまたはそれに加えて他のタイプの要素を含む他の要素を有し得る。サーバシステム１０２は、本明細書のいくつかの実装形態で説明されるように、動作を実行するものとして説明されているが、サーバシステム１０２または同様のシステム、またはそのようなシステムに関連する任意の適切な１つまたは複数のプロセッサの任意の適切なコンポーネントまたはコンポーネントの組合せが、説明されている動作を実行し得る。
For simplicity, FIG. 5 shows one block for each of
クライアントデバイスは、本明細書で説明される機能、例えば図１に示されるクライアントデバイス１２０〜１２６と共に実装および／または使用され得る。クライアントデバイスの例は、デバイス５００と同様のいくつかのコンポーネント、例えばプロセッサ５０２、メモリ５０４、およびＩ／Ｏインタフェース５０６を含むコンピュータデバイスであり得る。クライアントデバイスに適したオペレーティングシステム、ソフトウェア、およびアプリケーション、例えば、画像管理ソフトウェア、クライアントグループ通信アプリケーションソフトウェアなどは、メモリで提供され、プロセッサにより使用され得る。クライアントデバイスのためのＩ／Ｏインタフェースは、ネットワーク通信デバイス、および入出力デバイス、例えば、音をキャプチャするためのマイク、画像またはビデオをキャプチャするためのカメラ、音を出力するためのオーディオスピーカーデバイス、画像やビデオを出力するための表示デバイス、またはその他の出力デバイスに接続され得る。音声入力／出力デバイス５３０は、例えば、音声入力（例えば、音声コマンド）を受信し、音声出力（例えば、音声インタフェース）を提供するためにデバイス５００に接続される（または含まれる）ことが可能であり、マイク、スピーカー、ヘッドフォンなどのような適切なデバイスを含み得る。一部の実装形態は、音声出力デバイス、例えば、テキストを話す音声出力または合成を提供できる。
Client devices can be implemented and / or used with the features described herein, eg, client devices 120-126 shown in FIG. An example of a client device can be a computer device that includes several components similar to
本明細書に記載の１つまたは複数の方法（例えば、方法２００）は、コンピュータプログラム命令またはコードによって実装することができ、それらはコンピュータ上で実行することができる。例えば、コードは、１つまたは複数のデジタルプロセッサ（例えば、マイクロプロセッサまたは他の処理回路）によって実装することができ、半導体または固体メモリ、磁気テープ、リムーバブルコンピュータディスケット、ランダムアクセスメモリ（ＲＡＭ）、リードオンリーメモリ（ＲＯＭ）、フラッシュメモリ、リジッドメモリを含む、例えば、磁気、光学、電磁気、または半導体の記憶媒体などの、非一時的コンピュータ可読媒体（例えば、記憶媒体）を含むコンピュータプログラム製品に格納することができる。プログラム命令は、例えば、サーバ（例えば、分散システムおよび／またはクラウドコンピューティングシステム）から配信されるサービスとしてのソフトウェア（ＳａａＳ）の形で、電子信号に含まれ、電子信号として提供されることもできる。代替的に、１つまたは複数の方法は、ハードウェア（論理ゲートなど）、またはハードウェアとソフトウェアの組み合わせで、実施することができる。ハードウェアの例は、プログラマブルプロセッサ（例えば、フィールドプログラマブルゲートアレイ（ＦＰＧＡ）、コンプレックスプログラマブルロジックデバイス）、汎用プロセッサ、グラフィックプロセッサ、特定用途向け集積回路（ＡＳＩＣ）などであり得る。１つまたは複数の方法は、システム上で実行されているアプリケーションの一部またはコンポーネントとして、あるいは他のアプリケーションおよびオペレーティングシステムと共に実行されているアプリケーションまたはソフトウェアとして実行することができる。 One or more of the methods described herein (eg, Method 200) can be implemented by computer program instructions or code, which can be performed on a computer. For example, the code can be implemented by one or more digital processors (eg, microprocessors or other processing circuits), semiconductor or solid-state memory, magnetic tape, removable computer diskettes, random access memory (RAM), reads. Store in computer program products that include non-temporary computer-readable media (eg, storage media), including only memory (ROM), flash memory, rigid memory, such as magnetic, optical, electromagnetic, or semiconductor storage media. be able to. Program instructions may be included in an electronic signal and provided as an electronic signal, for example, in the form of software as a service (Software as a Service) delivered from a server (eg, a distributed system and / or a cloud computing system). .. Alternatively, one or more methods can be implemented in hardware (such as logic gates) or in combination of hardware and software. Examples of hardware can be programmable processors (eg, field programmable gate arrays (FPGAs), complex programmable logic devices), general purpose processors, graphics processors, application specific integrated circuits (ASICs), and the like. One or more methods can be run as part or component of an application running on the system, or as an application or software running with other applications and operating systems.
本明細書に記載の１つまたは複数の方法は、任意のタイプのコンピューティングデバイス上で実行することができるスタンドアロンプログラム、ウェブブラウザ上で実行されるプログラム、モバイルコンピューティングデバイス（携帯電話、スマートフォン、タブレットコンピュータ、ウェアラブルデバイス（腕時計、アームバンド、宝飾品、帽子、ゴーグルまたはメガネなど）、ラップトップコンピュータなど）上で実行されるモバイルアプリケーション（「ａｐｐ」）において実行することができる。一例では、クライアント／サーバアーキテクチャを使用することができ、例えば、（クライアントデバイスとしての）モバイルコンピューティングデバイスは、ユーザ入力データをサーバデバイスに送信し、出力のための（例えば、表示のための）最終出力データをサーバから受信する。別の例では、すべての計算は、モバイルコンピューティングデバイス上のモバイルアプリ（および／または他のアプリ）内で実行することができる。別の例では、計算は、モバイルコンピューティングデバイスと１つまたは複数のサーバデバイスとの間で分割することができる。 One or more of the methods described herein are stand-alone programs that can be run on any type of computing device, programs that run on a web browser, mobile computing devices (mobile phones, smartphones, etc.). It can be run in mobile applications (“app”) running on tablet computers, wearable devices (watches, armbands, jewelery, hats, goggles or glasses, etc.), laptop computers, etc. In one example, a client / server architecture can be used, for example, a mobile computing device (as a client device) sends user input data to a server device for output (eg, for display). Receive the final output data from the server. In another example, all calculations can be performed within a mobile app (and / or other app) on a mobile computing device. In another example, the computation can be split between the mobile computing device and one or more server devices.
いくつかの実装形態では、代替のインタフェースは、音声以外の他の非視覚的態様、例えば、視覚障害のあるユーザ向けの点字、および触覚（例えば、「love」のステッカーは、触覚出力デバイスからの特定のビートまたはパターンに対応し得る）を含み得る。いくつかの実装形態では、会話は、音声および視覚ディスプレイなどの出力デバイスの組み合わせを使用して出力され得る。例えば、いくつかのデバイス（例えば、時計、他のウェアラブル）は、あるメディア（例えば、テキスト、写真）の表示に適しているが、他のメディア（例えば、ビデオ、パノラマ写真、３Ｄメディアなど）には適さない場合がある。いくつかのデバイスは、特定のメディアを再生できない場合がある（例えば、音声出力機能のないデバイス、３Ｄコンテンツを出力する２Ｄディスプレイを有するデバイス、カラーを表示する白黒スクリーンなど）。上記のコンテキストでは、電子会話のプレゼンテーションは、テキストまたは音声形式の会話フレーミング（例えば、ビデオ、３Ｄコンテンツなどの説明）とともに、言語および非言語オブジェクト（例えば、テキストおよび画像）を含み得る。例えば、家庭用またはその他の設定用のアシスタント製品は、音声出力のみを有するか、または音声および制限されたディスプレイを有する場合がある。そのような例では、実装形態は、利用可能な出力デバイスに適した形式で会話オブジェクトを出力すること、利用可能な適切な出力デバイスを持たないオブジェクトに会話フレーミングを提供することを含むように、電子会話のプレゼンテーションを調整し得る。 In some implementations, alternative interfaces are non-visual aspects other than speech, such as Braille for visually impaired users, and tactile sensations (eg, the "love" sticker is from a haptic output device. Can correspond to a particular beat or pattern). In some implementations, conversations can be output using a combination of output devices such as audio and visual displays. For example, some devices (eg watches, other wearables) are suitable for displaying some media (eg text, photos), but for other media (eg videos, panoramic photos, 3D media, etc.). May not be suitable. Some devices may not be able to play certain media (eg, devices without audio output capabilities, devices with 2D displays that output 3D content, black and white screens that display color, etc.). In the above context, an electronic conversation presentation may include linguistic and non-linguistic objects (eg, text and images), as well as textual or audio form of conversation framing (eg, description of video, 3D content, etc.). For example, home or other configuration assistant products may have audio output only, or may have audio and a restricted display. In such an example, the implementation includes outputting the conversation object in a format suitable for the available output device, and providing conversation framing to the object that does not have the appropriate output device available. You can coordinate the presentation of electronic conversations.
非言語オブジェクトは、静止画像（例えば、動きのない単一のフレーム）、アニメーション画像、ビデオ（例えば、複数のフレームを有する）などを含み得る。例えば、静止画像は、固定された表情の１つまたは複数の顔を描写し、アニメーション画像は、画像内で変化する表情で１つまたは複数の顔を描写し得る（例えば、目が閉じた状態と開いた状態との間を遷移する顔、笑顔でないポジションから笑顔のポジションへと動く口を有する顔などをキャプチャするライブ写真）。ビデオは、１人または複数人の人物を表す複数のフレームを含み得る。 Non-verbal objects can include still images (eg, a single frame with no motion), animated images, video (eg, having multiple frames), and the like. For example, a still image may depict one or more faces with fixed facial expressions, and an animated image may depict one or more faces with varying facial expressions within the image (eg, with eyes closed). A live photo that captures a face that transitions between the open state and a face that has a mouth that moves from a non-smiling position to a smiling position). The video may include multiple frames representing one or more people.
非言語オブジェクトの分析は、アクションを検出すること、およびビデオの顔認識を行って（例えば、ビデオ全体またはビデオフレームの一部を使用してビデオを分析し得る）、言語説明（例えば、「馬に乗っているアレン」）を生成すること、顔認識なしでアクションを検出すること（例えば、「歩いている２人の人」）、有名人および／または映画クリップを検出すること（例えば、「ミッション・インポッシブルからの抜粋」）、他の種類のビデオまたはアニメーションのオブジェクトおよび動きを検出すること（例えば、「猫とネズミを示すアニメーション」）を含み得る。 Analysis of non-verbal objects involves detecting actions and performing face recognition in a video (eg, the video can be analyzed using the entire video or part of a video frame) and a linguistic description (eg, "horse". To generate "Allen riding on"), to detect actions without face recognition (eg "two people walking"), to detect celebrities and / or movie clips (eg "mission" • Excerpts from Impossible ”), detecting objects and movements in other types of video or animation (eg,“ animation showing cats and mice ”).
他の非言語オブジェクトおよび関連するテキスト表現の例は、３６０度ビュー（例えば、タージマハルの没入型ビュー）、シネマグラフ／アニメーションＧＩＦ（「あなたの友人のボブが目を回すジェスチャーをしています」）、音声（例えば、音声トランスクリプトの概要、音楽や歌などの音声の認識）、ゲームおよびインタラクティブマルチメディアオブジェクト（例えば、ジェスチャーで投げることができるフットボール）、およびミーム（例えば、「バットマンキャラクタージョーカーの笑い声」）を含む。 Examples of other non-verbal objects and related text representations are 360 degree views (eg Taj Mahal's immersive view), cinemagraph / animated GIFs ("Your friend Bob is making a look-ahead gesture"). , Voice (eg voice transcript overview, voice recognition such as music and songs), games and interactive multimedia objects (eg football that can be thrown with gestures), and memes (eg "Batman character joker laughter" ")including.
非言語オブジェクトは、非言語オブジェクトの言語表現または概要を生成するために分析されることが可能なメタデータを含み得る。非言語オブジェクトの言語説明は、電子会話を仲介しているサーバデバイス（例えば、１０４）によって生成され得る。いくつかの実装形態では、電子会話に使用されるクライアントデバイスは、非言語オブジェクトの言語またはテキスト説明を生成し得る。本明細書で説明される技術は、暗号化されていない会話および暗号化された会話で使用され得る。エンドツーエンドの暗号化された会話では、方法２００は、クライアントデバイス上でのみ実行され得る。いくつかの実装形態では、非言語オブジェクトは、分析される前に処理され得る（例えば、圧縮、低解像度への変換など）。ユーザの同意なしに、会話に関する情報がサーバに提供されることはない。
A non-linguistic object may contain metadata that can be analyzed to generate a linguistic representation or summary of the non-linguistic object. A linguistic description of a non-linguistic object can be generated by a server device (eg, 104) that mediates electronic conversation. In some implementations, the client device used for electronic conversation may generate a linguistic or textual description of a non-linguistic object. The techniques described herein can be used in unencrypted and encrypted conversations. In end-to-end encrypted conversations,
一部の実装形態では、代替インタフェースにおける解析、会話フレーミング、およびプレゼンテーションは、支援ソフトウェアアプリケーションまたはボットによって全体的または部分的に実行され得る。ボットは、例えば、クライアントデバイス１２０のようなクライアントデバイスおよび／またはサーバデバイス１０４のようなサーバデバイス上で動作するメッセージングアプリケーションを介して、ユーザが通常テキストまたは音声を通じて対話する、１つまたは複数のコンピュータ上で実装される自動化されたサービスを含み得る。ボットは、ボットが様々なメッセージングアプリケーションまたは他のアプリケーションのユーザと対話できるように、ボットプロバイダによって実装され得る。いくつかの実装形態では、メッセージングアプリケーションのプロバイダは、１つまたは複数のボットを提供することもある。いくつかの実装形態では、メッセージングアプリケーションのプロバイダによって提供されるボットは、ボットが他のメッセージングアプリケーションに含まれることができるように、例えば他のプロバイダによって提供されるように構成され得る。ボットは他のモードに比べていくつかの利点を提供することが可能である。ユーザは、テキストおよび／または音声を介してボットと対話することができ、これは、ウェブサイト、ソフトウェアアプリケーション、電話通話、例えば、対話式音声応答（interactive voice response：ＩＶＲ）サービス、またはサービスとやり取りする他の方法を使用するのに必要である可能性がある学習と比較して、最小限の学習しか必要とせず、または全く学習を必要としない可能性がある。メッセージングサービスまたはアプリケーションにボットを組み込むことで、ユーザは、メッセージングサービス内で旅行の計画、買い物、イベントのスケジュール、情報の取得などのさまざまなタスクを他のユーザと協力することも可能となり、タスクを実行するためのさまざまなアプリケーション（例えば、タクシー予約アプリケーション、レストラン予約アプリケーション、カレンダーアプリケーションなど）間またはＷｅｂサイト間での切り替えのような面倒な操作を排除することができる。
In some implementations, analysis, conversation framing, and presentation in alternative interfaces can be performed in whole or in part by assistive software applications or bots. A bot is one or more computers with which users typically interact through text or voice, for example, through a messaging application running on a client device such as
本明細書で説明される代替インタフェースでのプレゼンテーションのために電子会話を解析するためのボットは、１つまたは複数のメッセージングアプリケーションに関連して、１または複数のユーザ（例えば、ユーザＵ１〜Ｕ４のいずれか）と対話するように構成されたコンピュータプログラムまたはアプリケーション（例えば、ソフトウェアアプリケーション）として実装され得る。 A bot for analyzing an electronic conversation for presentation on an alternative interface as described herein refers to one or more users (eg, users U1 to U4) in connection with one or more messaging applications. It can be implemented as a computer program or application (eg, a software application) that is configured to interact with either).
メッセージングアプリケーションのユーザと通信することができるボットを実装することは、多くの利点を提供し得る。前述のように、ボットは、電子会話を解析し、クライアントデバイスが使用されているコンテキストに適した代替インタフェースでそれらの会話を提示できる（例えば、ユーザが車両を操作している場合は、音声インタフェースで会話を提示する）。 Implementing a bot that can communicate with users of messaging applications can offer many benefits. As mentioned earlier, the bot can parse electronic conversations and present those conversations with an alternative interface suitable for the context in which the client device is used (eg, a voice interface if the user is manipulating the vehicle). Present the conversation at).
特定の実施形態では、ボットは会話型インタフェースを使用して自然言語（例えば、会話フレーミング）を使用してユーザと会話的に対話することができる。いくつかの実施形態では、ボットは、例えばレストランの住所の要求に応じて、「レストランＲの位置はＬである」などのテンプレートを使用するなど、テンプレートベースのフォーマットを使用して、ユーザと対話するための文章を作成することができる。場合によっては、例えばボットがユーザと対話するために自然言語を使用するかどうか、ボットがテンプレートベースの対話を使用するかどうかなど、ユーザが、ボット対話フォーマットを選択できるようにすることが可能であり得る。 In certain embodiments, the bot can use a conversational interface to interact conversationally with the user using natural language (eg, conversational framing). In some embodiments, the bot interacts with the user using a template-based format, such as using a template such as "Restaurant R is located at L" in response to a request for a restaurant address. You can create sentences to do. In some cases, it is possible to allow the user to choose a bot dialogue format, such as whether the bot uses natural language to interact with the user, whether the bot uses template-based dialogue, and so on. possible.
ボットが自然言語を使用して会話的に対話する場合、ボットの対話の内容および／またはスタイルは、自然言語処理を使用して判定される会話のコンテンツ、会話中のユーザの識別情報、１つまたは複数の会話コンテキスト（例えば、ユーザの対話に関する履歴情報、ソーシャルグラフに基づく会話内のユーザ間の関係）、外部条件（例えば、天気、交通）、ユーザのスケジュール、ユーザに関連する関連コンテキストなどのうちの一つまたは複数に基づいて動的に変化し得る。このような場合、ボットの対話の内容とスタイルは、会話に参加しているユーザが同意した要素のみに基づいて異なる。 When a bot interacts conversationally using natural language, the content and / or style of the bot's dialogue is the content of the conversation determined using natural language processing, the identity of the user in conversation, and one. Or multiple conversation contexts (eg, historical information about a user's dialogue, relationships between users in a conversation based on social graphs), external conditions (eg, weather, traffic), user's schedule, related contexts related to the user, etc. It can change dynamically based on one or more of them. In such cases, the content and style of the bot's dialogue will differ based only on the factors agreed by the users participating in the conversation.
一例として、会話のユーザがフォーマルな言葉を使用していると判断された場合（例えば、俗語や絵文字がない、または最小限の俗語または絵文字）、ボットもフォーマルな言葉を使用してその会話内で対話することができ、逆もまた同様である。 As an example, if it is determined that the user of the conversation is using formal language (eg, no slang or emoji, or minimal slang or emoji), the bot will also use the formal language in the conversation. You can interact with, and vice versa.
特定の実施形態では、会話に参加しているユーザは、例えば、ボット名またはボットハンドル（例えば、タクシー（taxi）、＠タクシーボット（@taxibot）、＠ムービー（@movies）など）を入力すること、音声コマンド（例えば、「銀行ボットを呼び出して」など）を使用すること、ユーザインタフェース要素（例えば、ボット名またはハンドルでラベル付けされたボタンまたはその他の要素）をアクティブにすることなどによって、特定のボットまたは特定のタスクを実行するボットを呼び出すことが可能とされ得る。ボットが呼び出されると、ユーザは、ボットが音声インタフェースでユーザに受信されたメッセージを解析して読むことを要求し得る。 In certain embodiments, the user participating in the conversation enters, for example, a bot name or bot handle (eg, taxi, @taxibot, @movies, etc.). , By using voice commands (eg, "call a bank bot"), by activating user interface elements (eg, buttons labeled with bot names or handles or other elements), etc. It may be possible to call a bot or a bot that performs a particular task. When the bot is called, the user may require the bot to parse and read the message received by the user on the voice interface.
特定の実施形態では、ボットは、具体的に呼び出されることなく、メッセージング会話内の情報またはアクションを自動的に提案することができる。即ち、ユーザはボットを特に起動する必要がないことがあり得る。これらの実施形態では、ボットは、継続的にまたは別々の時点でのユーザのコンテキスト（ユーザの許可により取得される）の分析および理解に依存することができる。コンテキストの分析は、特定のユーザのニーズを理解し、いつボットが支援を提案すべきかを識別するために使用することができる。一例として、ボットは、ユーザが車両にいることを示唆する方法でユーザデバイスが動いていると判断し、ボットは、着信メッセージが解析され、音声インタフェースまたは他の代替インタフェースで提示されることを示唆し得る。 In certain embodiments, the bot can automatically suggest information or actions within a messaging conversation without being specifically called. That is, the user may not need to launch the bot in particular. In these embodiments, the bot can rely on the analysis and understanding of the user's context (obtained with the user's permission) continuously or at different times. Contextual analysis can be used to understand the needs of a particular user and identify when the bot should offer assistance. As an example, the bot determines that the user device is moving in a way that suggests that the user is in the vehicle, and the bot suggests that the incoming message is parsed and presented on a voice interface or other alternative interface. Can be done.
ボットが具体的に呼び出されることなく、メッセージング会話内において情報またはアクションを自動的に提案することができる実施形態では、例えば、メッセージング会話に参加している一または複数のユーザが、ユーザの会話の分析をボットが実施することについて同意しない場合、そのような機能は無効にされる。さらに、そのような機能は、ユーザ入力に基づいて一時的に無効にされてもよい。例えば、会話がプライベートまたはセンシティブであることをユーザが示したとき、会話コンテキストの分析は、ユーザがボットをアクティブにするための入力を提供するまで一時停止される。さらに、分析機能が無効化されているという標示を、例えばユーザインタフェース要素を用いて、会話の参加者に提供することができる。 In an embodiment in which information or actions can be automatically proposed within a messaging conversation without the bot being specifically called, for example, one or more users participating in the messaging conversation may participate in the user's conversation. If you do not agree that the bot will perform the analysis, such features will be disabled. In addition, such features may be temporarily disabled based on user input. For example, when the user indicates that the conversation is private or sensitive, the analysis of the conversation context is suspended until the user provides input to activate the bot. In addition, a sign that the analysis function has been disabled can be provided to the participants in the conversation, for example using a user interface element.
様々な実装形態では、ボットは様々な構成で実装され得る。例えば、ボットはクライアントデバイス（例えば、１２０-１２６）上に実装され得る。この例では、ボットは、クライアントデバイスに対してローカルなソフトウェアアプリケーション内のモジュールであり得る。ユーザが、クライアントデバイス上のメッセージングアプリケーションに関連してボットと会話することができるように、ボットはクライアントデバイス上にローカルに実装され得る。 In different implementations, the bot can be implemented in different configurations. For example, the bot can be implemented on a client device (eg 120-126). In this example, the bot can be a module in a software application that is local to the client device. The bot can be implemented locally on the client device so that the user can talk to the bot in connection with the messaging application on the client device.
別の例では、ボットは、クライアントデバイスおよびサーバデバイスの両方に実装され得る。この例では、ボットは、例えば、ボット機能の一部がクライアントボットおよびサーバボットの各々によって提供される、クライアント−サーバコンピュータプログラムとして実装され得る。いくつかの実装形態において、ボットは、例えば、複数のクライアントデバイスおよびサーバ（例えば、クライアントデバイス、サーバデバイスなど）にわたって分散されたモジュールを有する分散アプリケーションとして実施されてもよい。いくつかの実装形態では、ボットは、サーバデバイス上に実装されるサーバアプリケーションとして実装され得る。 In another example, the bot can be implemented on both client and server devices. In this example, the bot can be implemented, for example, as a client-server computer program in which some of the bot functionality is provided by each of the client and server bots. In some implementations, the bot may be implemented, for example, as a distributed application with modules distributed across multiple client devices and servers (eg, client devices, server devices, etc.). In some implementations, the bot can be implemented as a server application implemented on a server device.
クライアントのみ、サーバのみ、クライアント−サーバ、分散などのような異なる実装形態は、異なる利点を提供し得る。例えば、クライアントのみの実装形態では、例えば、ネットワークアクセスなしで、ボット機能をローカルに提供することができ、このことは、例えば、ユーザがネットワークのカバレッジエリア外にある場合、またはネットワーク帯域幅が狭いか、または制限されている任意のエリア内にある場合など、特定の状況で有利となり得る。サーバのみ、クライアント−サーバ、または分散構成などの１つまたは複数のサーバを含む実装では、例えば、チケットの予約などのクライアントデバイスでローカルに提供できない特定の機能が許可され得る。 Different implementations such as client-only, server-only, client-server, distributed, etc. can offer different benefits. For example, a client-only implementation can provide bot functionality locally, for example, without network access, which can be done, for example, if the user is outside the coverage area of the network or the network bandwidth is narrow. It can be advantageous in certain situations, such as when it is within any restricted area. Implementations involving one or more servers, such as server only, client-server, or distributed configurations, may allow certain features that cannot be provided locally on the client device, such as ticket reservation.
ボットは、メッセージングアプリケーションとは異なるものであってよく、いくつかの実装形態では、１つまたは複数のボットは、メッセージングアプリケーションの一部として実装され得る。ボットがメッセージングアプリケーションの一部として実装される実装形態では、ボットを実行する前にユーザの許可が取得され得る。いくつかの実装形態において、メッセージングアプリケーションのプロバイダおよびユーザとは異なるサードパーティが、ユーザと通信することができるボットを提供し得る。 The bot may be different from the messaging application, and in some implementations, one or more bots may be implemented as part of the messaging application. In implementations where the bot is implemented as part of a messaging application, the user's permission may be obtained before the bot is executed. In some implementations, the provider of the messaging application and a third party other than the user may provide a bot that can communicate with the user.
非言語オブジェクトのオブジェクト認識および解析、並びに会話フレーミングの識別およびレンダリングは、機械学習技術を使用して実行され得る。例えば、言語オブジェクトは、ＬＳＴＭモデルを使用して解析および要約され、画像／ビデオコンテンツは、オブジェクト認識用のためにトレーニングされた機械学習モデルを使用して解析され得る。インタラクティブなオブジェクトは、これらのタイプのオブジェクトなどのために特別にトレーニングされたモデルを使用して認識され得る。例えば、電子的コミュニケーション分析アプリケーションは、メッセージングアプリケーションとのユーザの対話を向上させることが可能な、例えばディープラーニングモデルなどの機械学習を実装することができる。機械学習モデルは、合成データ、例えば、ユーザ情報を使用せずにコンピュータによって自動的に生成されたデータを使用してトレーニングすることができる。いくつかの実装形態では、機械学習モデルは、例えば、トレーニングのためにユーザデータを利用する許可がユーザから明示的に得られているサンプルデータに基づいてトレーニングされ得る。例えば、サンプルデータは、受信されたメッセージを含み得る。サンプルデータに基づいて、機械学習モデルは、受信されたメッセージを解析して会話フレーミングを提供する方法を予測し、これは次に、代替インタフェースを介してプレゼンテーションとして提供され得る。 Object recognition and analysis of non-verbal objects, as well as identification and rendering of conversation framing, can be performed using machine learning techniques. For example, language objects can be analyzed and summarized using LSTM models, and image / video content can be analyzed using machine learning models trained for object recognition. Interactive objects can be recognized using models specially trained for these types of objects and the like. For example, an electronic communication analysis application can implement machine learning, such as a deep learning model, which can improve user interaction with a messaging application. Machine learning models can be trained using synthetic data, such as data automatically generated by a computer without the use of user information. In some implementations, the machine learning model can be trained, for example, based on sample data for which the user has explicitly obtained permission to use the user data for training. For example, the sample data may include received messages. Based on the sample data, the machine learning model predicts how to parse the received message to provide conversational framing, which can then be provided as a presentation via an alternative interface.
いくつかの実装形態では、機械学習は、サーバデバイス、クライアントデバイス、またはその両方で実装され得る。いくつかの実装形態では、単純な機械学習モデルがクライアントデバイス上で実施され（例えば、クライアントデバイスのメモリ、ストレージ、および処理制約内でモデルの動作を可能にするため）、複雑な機械学習モデルがサーバデバイス上で実施され得る。ユーザが機械学習技術の使用について同意しない場合、そのような技術は実装されない。いくつかの実装形態において、ユーザは、クライアントデバイス上でのみ実施される機械学習のために同意を選択的に提供し得る。これらの実装形態では、機械学習モデルに対する更新または機械学習モデルによって使用されるユーザ情報は、ローカルに格納または使用され、かつサーバデバイスまたは他のクライアントデバイスなどの他のデバイスとは共有されないように、機械学習モデルがクライアントデバイス上で実施され得る。 In some implementations, machine learning can be implemented on server devices, client devices, or both. In some implementations, a simple machine learning model is implemented on the client device (for example, to allow the model to work within the client device's memory, storage, and processing constraints), and a complex machine learning model It can be done on the server device. If the user disagrees with the use of machine learning techniques, such techniques will not be implemented. In some implementations, the user may selectively provide consent for machine learning performed only on the client device. In these implementations, updates to the machine learning model or user information used by the machine learning model is stored or used locally and is not shared with other devices such as server devices or other client devices. A machine learning model can be implemented on the client device.
いくつかの実装形態では、機械学習アプリケーションは、１つまたは複数のプロセッサが本明細書に記載の機能、例えば、図２の方法のうちのいくつかまたは全てを実施することを可能とする命令を含み得る。 In some implementations, machine learning applications provide instructions that allow one or more processors to perform some or all of the functions described herein, eg, the method of FIG. Can include.
様々な実装形態では、本明細書に記載の機能を実行する機械学習アプリケーションは、ベイズ分類器、サポートベクターマシン、ニューラルネットワーク、または他の学習技術を利用することができる。いくつかの実装形態では、機械学習アプリケーションは、トレーニングされたモデル、推論エンジン、およびデータを含み得る。いくつかの実装形態では、データは、トレーニングデータ、例えばトレーニングされたモデルを生成するために使用されるデータを含み得る。例えば、トレーニングデータは、テキスト、画像、音声、ビデオなどのような任意の種類のデータを含み得る。トレーニングデータは、任意のソース、例えば、トレーニングのために特別にマークされたデータリポジトリ、機械学習のためのトレーニングデータとして使用するために許可が与えられているデータなどから取得することができる。１または複数のユーザが、機械学習モデル、例えば、トレーニングされたモデルをトレーニングするためにそれぞれのユーザデータの使用を許可する実装形態では、トレーニングデータは、そのようなユーザデータを含むことができる。ユーザがそれぞれのユーザデータの使用を許可する実装形態では、データは、画像（例えば、写真または他のユーザ生成画像）、通信（例えば、電子メール、テキストメッセージなどのチャットデータ、音声、ビデオなど）、およびドキュメント（例えば、スプレッドシート、テキストドキュメント、プレゼンテーションなど）などの許可されたデータを含み得る。 In various implementations, machine learning applications that perform the functions described herein can utilize Bayes classifiers, support vector machines, neural networks, or other learning techniques. In some implementations, the machine learning application may include trained models, inference engines, and data. In some implementations, the data may include training data, such as data used to generate a trained model. For example, training data can include any type of data such as text, images, audio, video, and so on. Training data can be obtained from any source, such as a data repository specially marked for training, data that has been granted permission to be used as training data for machine learning, and so on. In an implementation that allows one or more users to use their respective user data to train a machine learning model, eg, a trained model, the training data can include such user data. In an implementation that allows users to use their respective user data, the data may be images (eg, photos or other user-generated images), communications (eg, chat data such as emails, text messages, audio, video, etc.). , And may include authorized data such as documents (eg spreadsheets, text documents, presentations, etc.).
いくつかの実装形態では、トレーニングデータは、例えばシミュレートされた会話から生成されたデータ、コンピュータ生成画像など、訓練されているコンテキストにおけるユーザ入力または活動に基づいていないデータのような、トレーニングの目的で生成された合成データを含み得る。いくつかの実装形態では、機械学習アプリケーションは、データを除外する。例えば、これらの実装形態では、トレーニングされたモデルは、例えば異なるデバイス上で生成され、機械学習アプリケーションの一部として提供され得る。様々な実装形態では、トレーニングされたモデルは、モデル構造または形態、ならびに関連する重みを含むデータファイルとして提供され得る。推論エンジンは、トレーニングされたモデルのデータファイルを読み取り、トレーニングされたモデルで指定されたモデル構造または形態に基づく、ノードの接続性、層、および重みを用いてニューラルネットワークを実装することができる。 In some embodiments, the training data is for training purposes, such as data generated from simulated conversations, computer-generated images, and other data that is not based on user input or activity in the context being trained. May include synthetic data generated in. In some implementations, machine learning applications exclude data. For example, in these implementations, the trained model can be generated, for example, on different devices and provided as part of a machine learning application. In various implementations, the trained model can be provided as a data file containing the model structure or form, as well as the associated weights. The inference engine can read the data file of the trained model and implement a neural network with node connectivity, layers, and weights based on the model structure or morphology specified in the trained model.
機械学習アプリケーションは、トレーニングされたモデルも含み得る。いくつかの実装形態では、トレーニングされたモデルは、１つまたは複数のモデル形態または構造を含み得る。例えば、モデル形態または構造は、線形ネットワーク、複数の層（例えば、入力層と出力層との間の「隠れ層」、各層は線形ネットワークである）を実装するディープニューラルネットワーク、畳み込みニューラルネットワーク（例えば、入力データを複数の部分またはタイルに分割または区分けし、１つまたは複数のニューラルネットワーク層を用いて各タイルを別個に処理し、各タイルの処理からの結果を集約するネットワーク）、ｓｅｑｕｅｎｃｅ−ｔｏ−ｓｅｑｕｅｎｃｅニューラルネットワーク（例えば、文中の単語、ビデオのフレームなどの入力シーケンシャルデータを取り、結果シーケンスを出力として生成するネットワーク）のような、任意のタイプのニューラルネットワークを含み得る。モデル形態または構造は、様々なノード間の接続性およびノードの層への編成を指定することができる。例えば、第１層（例えば入力層）のノードは、入力データまたはアプリケーションデータとしてデータを受け取ることができる。そのようなデータは、例えば、トレーニングされたモデルが画像分析のために使用される場合、例えば、ノードごとに１つまたは複数のピクセルを含むことができる。後続の中間層は、モデル形態または構造で指定された接続性に従って、前の層のノードの入力出力を受け取ることができる。これらの層は、隠れ層とも呼ばれる。最終層（例えば、出力層）は、機械学習アプリケーションの出力を生成する。例えば、出力は、特定のトレーニングされたモデルに応じて、画像に対するラベルのセット、画像を他の画像と比較することを可能にする画像の表現（例えば、画像に対する特徴ベクトル）、入力文に応答した出力文、入力データのための１つまたは複数のカテゴリなどであり得る。いくつかの実装形態では、モデル形態または構造は、各層のノードの数および／またはタイプも指定する。 Machine learning applications can also include trained models. In some implementations, the trained model may include one or more model forms or structures. For example, the model form or structure is a linear network, a deep neural network that implements multiple layers (eg, a "hidden layer" between the input and output layers, each layer is a linear network), a convolutional neural network (eg,). , A network that divides or divides the input data into multiple parts or tiles, processes each tile separately using one or more neural network layers, and aggregates the results from the processing of each tile), sequence-to It can include any type of neural network, such as a −sequence neural network (eg, a network that takes input sequential data such as words in a sentence, frames of a video, and produces a resulting sequence as output). The model form or structure can specify connectivity between various nodes and organization of nodes into layers. For example, a node in the first layer (eg, an input layer) can receive data as input data or application data. Such data can include, for example, one or more pixels per node, for example, when the trained model is used for image analysis. Subsequent middle layers can receive the inputs and outputs of the nodes of the previous layer according to the connectivity specified in the model form or structure. These layers are also called hidden layers. The final layer (eg, the output layer) produces the output of the machine learning application. For example, the output responds to a set of labels for an image, a representation of the image that allows the image to be compared to other images (eg, feature vectors for the image), input statements, depending on the particular trained model. It can be an output statement, one or more categories for input data, and so on. In some implementations, the model form or structure also specifies the number and / or type of nodes in each layer.
異なる実装形態では、トレーニングされたモデルは、モデル構造または形態ごとに層に配置された複数のノードを含むことができる。いくつかの実装形態では、ノードは、メモリなしの計算ノード、例えば、１単位の入力を処理して１単位の出力を生成するように構成されたものであり得る。ノードによって実行される計算は、例えば、複数のノード入力の各々に重みを乗算すること、重み付き和を取得すること、および重み付き和をバイアスまたは切片値で調整してノード出力を生成することを含み得る。いくつかの実装形態では、計算は、調整された重み付き和にステップ／活性化関数を適用することを含み得る。いくつかの実装形態では、ステップ／活性化関数は、非線形関数であり得る。様々な実装形態では、計算は、行列乗算などの演算を含み得る。いくつかの実装形態では、複数のノードによる計算は、例えばマルチコアプロセッサの複数のプロセッサコアを使用して、ＧＰＵの個々の処理ユニットを使用して、または専用のニューラル回路を使用して、並列して実行され得る。いくつかの実装形態では、ノードは、メモリを含むことができ、例えば、後続の入力を処理する際に１つまたは複数の以前の入力を格納して使用することが可能であり得る。例えば、メモリを有するノードは、長期短期記憶（ＬＳＴＭ）ノードを含み得る。ＬＳＴＭノードは、ノードが有限状態機械（ＦＳＭ）のように動作することを可能にする「状態」を維持するためにメモリを使用することができる。そのようなノードを有するモデルは、連続データ、例えば文章または段落内の単語、ビデオ内のフレーム、スピーチまたは他の音声などを処理するのに有用であり得る。 In different implementations, the trained model can include multiple nodes arranged in layers for each model structure or form. In some implementations, the node can be a memoryless compute node, eg, one that is configured to process one unit of input to produce one unit of output. Calculations performed by a node are, for example, multiplying each of multiple node inputs by a weight, obtaining a weighted sum, and adjusting the weighted sum with a bias or intercept value to produce a node output. May include. In some implementations, the calculation may include applying a step / activation function to the adjusted weighted sum. In some implementations, the step / activation function can be a non-linear function. In various implementations, calculations can include operations such as matrix multiplication. In some implementations, multi-node computations are performed in parallel, for example using multiple processor cores in a multi-core processor, using individual processing units on the GPU, or using dedicated neural circuits. Can be executed. In some implementations, the node can include memory, for example, it may be possible to store and use one or more previous inputs when processing subsequent inputs. For example, a node with memory may include a long short term memory (LSTM) node. The LSTM node can use memory to maintain a "state" that allows the node to behave like a finite state machine (FSM). Models with such nodes can be useful for processing continuous data, such as words in sentences or paragraphs, frames in videos, speeches or other sounds.
いくつかの実装形態では、トレーニングされたモデルは、個々のノードに対する埋め込みまたは重みを含み得る。例えば、モデルは、モデルの形態または構造によって指定されるような層に編成された複数のノードとして開始されてもよい。初期化時に、モデル形態ごとに接続されているノード、例えばニューラルネットワークの連続する層におけるノード、の各ペアの間の接続に、それぞれの重みが適用され得る。例えば、それぞれの重みは、ランダムに割り当てられてもよく、またはデフォルト値に初期化されてもよい。次いで、モデルが、例えばデータを使用してトレーニングされ、結果を生成することができる。 In some implementations, the trained model may include embeddings or weights for individual nodes. For example, the model may start as multiple nodes organized in layers as specified by the form or structure of the model. At initialization, the respective weights can be applied to the connections between each pair of nodes connected for each model form, eg, nodes in successive layers of a neural network. For example, each weight may be randomly assigned or initialized to a default value. The model can then be trained, for example using the data, to produce results.
例えば、トレーニングは、教師あり学習技術を適用することを含み得る。教師あり学習では、トレーニングデータは、複数の入力（例えば、一組の画像）および各入力に対する対応する予想される出力（例えば、各画像に対する１つまたは複数のラベル）を含み得る。モデルの出力と、期待される出力との比較に基づいて、重みの値が、例えば同様の入力が与えられたときにモデルが期待される出力を生成する確率を高めるように、自動的に調整される。 For example, training may include applying supervised learning techniques. In supervised learning, training data can include multiple inputs (eg, a set of images) and corresponding expected outputs for each input (eg, one or more labels for each image). Based on a comparison of the model's output with the expected output, the weight values are automatically adjusted to increase the probability that the model will produce the expected output, for example given similar inputs. Will be done.
いくつかの実装形態では、トレーニングは、教師なし学習技術を適用することを含み得る。教師なし学習では、入力データのみが提供され、モデルは、データを区別するように、例えば、入力データを複数のグループにクラスタ化するようにトレーニングされることが可能であり、各グループは、何らかの点で類似した入力データを含む。例えば、モデルが、抽象画像（例えば、合成画像、人間が描いた画像など）を自然画像（例えば、写真）から区別するように、画像を区別するようにトレーニングされ得る。 In some implementations, training may include applying unsupervised learning techniques. In unsupervised learning, only the input data is provided and the model can be trained to distinguish the data, for example, to cluster the input data into multiple groups, where each group has something to do with it. Contains input data that is similar in terms of points. For example, a model can be trained to distinguish between images, such as abstract images (eg, composite images, images drawn by humans, etc.) from natural images (eg, photographs).
別の例では、教師なし学習を使用してトレーニングされたモデルは、入力文における単語の使用に基づいて単語をクラスタ化することができる。いくつかの実装形態では、教師なし学習を使用して、例えば機械学習アプリケーションによって使用され得る知識表現を生成することができる。様々な実装形態では、トレーニングされたモデルは、モデル構造に対応する重みまたは埋め込みの集合を含む。データが省略される実装形態では、機械学習アプリケーションは、例えば、機械学習アプリケーションの開発者、またはサードパーティなどによる事前のトレーニングに基づくトレーニングされたモデルを含み得る。いくつかの実装形態では、トレーニングされたモデルは、例えば重みを提供するサーバからダウンロードされた、固定された重みのセットを含み得る。 In another example, a model trained using unsupervised learning can cluster words based on the use of words in input sentences. In some implementations, unsupervised learning can be used to generate knowledge representations that can be used, for example, by machine learning applications. In various implementations, the trained model contains a set of weights or embeddings that correspond to the model structure. In implementations where data is omitted, the machine learning application may include, for example, a trained model based on prior training by the developer of the machine learning application, or a third party. In some implementations, the trained model may include, for example, a fixed set of weights downloaded from a server that provides weights.
機械学習アプリケーションは、推論エンジンも含み得る。推論エンジンは、トレーニングされたモデルをアプリケーションデータなどのデータに適用して推論を提供するように構成されている。いくつかの実装形態では、推論エンジンは、プロセッサによって実行されるべきソフトウェアコードを含み得る。いくつかの実装形態では、推論エンジンは、プロセッサがトレーニングされたモデルを適用することを可能にする回路構成（例えば、プログラマブルプロセッサ、フィールドプログラマブルゲートアレイ（ＦＰＧＡ）など）を指定することができる。いくつかの実装形態では、推論エンジンは、ソフトウェア命令、ハードウェア命令、またはそれらの組み合わせを含み得る。いくつかの実装形態では、推論エンジンは、例えばトレーニングされたモデルをアプリケーションデータに適用して推論を生成するために、オペレーティングシステムおよび／または他のアプリケーションによって使用されて、推論エンジンを呼び出すことができるアプリケーションプログラミングインタフェース（ＡＰＩ）を提供し得る。 Machine learning applications can also include inference engines. The inference engine is configured to apply the trained model to data such as application data to provide inference. In some implementations, the inference engine may contain software code that should be executed by the processor. In some embodiments, the inference engine can specify a circuit configuration (eg, programmable processor, field programmable gate array (FPGA), etc.) that allows the processor to apply the trained model. In some implementations, the inference engine may include software instructions, hardware instructions, or a combination thereof. In some implementations, an inference engine can be used by the operating system and / or other applications to call an inference engine, for example to apply a trained model to application data to generate inference. It may provide an application programming interface (API).
機械学習アプリケーションは、複数の技術的利点を提供することができる。例えば、トレーニングされたモデルが、教師なし学習に基づいて生成される場合、トレーニングされたモデルが、推論エンジンによって適用されて、入力データ、例えばアプリケーションデータから知識表現（例えば、数値表現）を生成することができる。例えば、画像分析のためにトレーニングされたモデルは、入力画像（例えば１０ＭＢ）よりも実質的に小さいサイズ（例えば１ＫＢ）を有する画像の表現を生成し得る。いくつかの実装形態では、そのような表現は、出力（例えば、ラベル、分類、画像を説明する文章など）を生成するための処理コスト（例えば、計算コスト、メモリ使用量など）を削減するのに役立ち得る。いくつかの実装形態では、そのような表現は、推論エンジンの出力から出力を生成する異なる機械学習アプリケーションへの入力として提供され得る。いくつかの実装形態では、機械学習アプリケーションによって生成された知識表現は、例えばネットワークを介して、さらなる処理を実行する異なるデバイスに提供され得る。そのような実装形態では、画像ではなく知識表現を提供することにより、実質的な技術的利益が提供され、例えば、より低いコストでより速いデータ伝送を可能にすることができる。別の例では、ドキュメントをクラスタ化するためにトレーニングされたモデルは、入力ドキュメントからドキュメントクラスタを生成することができる。ドキュメントクラスタは、元のドキュメントにアクセスする必要なしに、さらなる処理（例えば、ドキュメントがトピックに関連しているかどうかの判定、そのドキュメントの分類カテゴリの判定など）に適し、従って計算コストを節約することができる。 Machine learning applications can offer multiple technical benefits. For example, if a trained model is generated based on unsupervised learning, the trained model is applied by an inference engine to generate knowledge representations (eg, numerical representations) from input data, such as application data. be able to. For example, a model trained for image analysis can produce a representation of an image having a size substantially smaller (eg 1KB) than the input image (eg 10MB). In some implementations, such representations reduce the processing costs (eg, computational costs, memory usage, etc.) for producing output (eg, labels, classifications, text describing images, etc.). Can help. In some implementations, such a representation can be provided as an input to a different machine learning application that produces an output from the output of an inference engine. In some implementations, the knowledge representation generated by the machine learning application may be provided to different devices performing further processing, eg, via a network. In such an implementation, providing knowledge representation rather than images provides substantial technical benefits, such as enabling faster data transmission at lower cost. In another example, a model trained to cluster documents can generate document clusters from input documents. Document clusters are suitable for further processing (eg, determining if a document is related to a topic, determining the classification category of that document, etc.) without having to access the original document, thus saving computational costs. Can be done.
いくつかの実装形態では、機械学習アプリケーションは、オフライン方式で実装され得る。これらの実装形態では、トレーニングされたモデルは、第一段階で生成され、機械学習アプリケーションの一部として提供され得る。いくつかの実装形態では、機械学習アプリケーションは、オンライン方式で実装され得る。例えば、そのような実装形態では、機械学習アプリケーションを呼び出すアプリケーション（例えば、オペレーティングシステム、および／または１つまたは複数の他のアプリケーション）は、機械学習アプリケーションによって生成された推論を利用することができ、例えば、その推論をユーザに提供し、システムログ（例えば、ユーザによって許可された場合、推論に基づいてユーザによって取られたアクション、またはさらなる処理のための入力として利用された場合、さらなる処理の結果）を生成することができる。システムログは、定期的に、例えば、毎時、毎月、毎四半期などに生成されてもよく、ユーザの許可を得て、トレーニングされたモデルを更新するために、例えばトレーニングされたモデルの埋め込みを更新するために使用され得る。 In some implementations, the machine learning application can be implemented offline. In these implementations, the trained model can be generated in the first stage and provided as part of a machine learning application. In some implementations, the machine learning application can be implemented online. For example, in such an implementation, an application that calls a machine learning application (eg, an operating system and / or one or more other applications) can take advantage of the inference generated by the machine learning application. For example, if the inference is provided to the user and used as a system log (eg, if permitted by the user, an action taken by the user based on the inference, or as input for further processing, the result of further processing. ) Can be generated. The system log may be generated on a regular basis, for example hourly, monthly, quarterly, etc., and with the user's permission, for example updating the training model embedding to update the trained model. Can be used to
いくつかの実装形態では、機械学習アプリケーションは、機械学習アプリケーションが実行されるデバイスの特定の構成に適応することができる方法で実装され得る。例えば、機械学習アプリケーションは、利用可能な計算資源、例えばプロセッサを利用する計算グラフを決定することができる。例えば、機械学習アプリケーションが複数のデバイス上の分散型アプリケーションとして実施される場合、機械学習アプリケーションは、計算を最適化する方法で個々のデバイス上で実行されるべき計算を決定することができる。別の例では、機械学習アプリケーションは、プロセッサが特定の数（例えば、１０００）のＧＰＵコアを有するＧＰＵを含み、それに応じて推論エンジンを（例えば、１０００個の個々のプロセスまたはスレッドとして）実装することを決定し得る。 In some implementations, the machine learning application can be implemented in a way that can be adapted to the particular configuration of the device on which the machine learning application runs. For example, a machine learning application can determine computational graphs that utilize available computational resources, such as processors. For example, when a machine learning application is implemented as a distributed application on multiple devices, the machine learning application can determine the computations to be performed on the individual devices in a way that optimizes the computations. In another example, the machine learning application includes GPUs in which the processor has a certain number of GPU cores (eg, 1000) and implements an inference engine accordingly (eg, as 1000 individual processes or threads). You can decide that.
いくつかの実装形態では、機械学習アプリケーションは、トレーニングされたモデルのアンサンブルを実施し得る。例えば、トレーニングされたモデルは、それぞれ同じ入力データに適用可能な複数のトレーニングされたモデルを含み得る。これらの実装形態では、機械学習アプリケーションは、例えば、利用可能な計算リソース、以前の推論での成功率などに基づいて、特定のトレーニングされたモデルを選択することができる。いくつかの実装形態では、機械学習アプリケーションは、複数のトレーニングされたモデルが適用されるように推論エンジンを実行することができる。これらの実装形態では、機械学習アプリケーションは、個々のモデルの適用からの出力を、例えば、各トレーニング済みモデルの適用からの個々の出力を採点する投票技術を使用して、または１つまたは複数の特定の出力を選択することによって、組み合わせることができる。さらに、これらの実装形態では、機械学習アプリケーションは、個々のトレーニングされたモデルを適用するための時間閾値（例えば、０．５ｍｓ）を適用し、その時間閾値内で利用可能な個々の出力のみを利用することができる。時間閾値内に受信されない出力は利用されず、例えば破棄され得る。例えば、そのようなアプローチは、例えばオペレーティングシステム、または１つまたは複数のアプリケーションによって、機械学習アプリケーションを呼び出す間に指定された時間制限がある場合に適切であり得る。 In some implementations, the machine learning application may implement an ensemble of trained models. For example, a trained model may include multiple trained models, each applicable to the same input data. In these implementations, the machine learning application can select a particular trained model based on, for example, available computational resources, success rates in previous inferences, and so on. In some implementations, machine learning applications can run inference engines so that multiple trained models are applied. In these implementations, the machine learning application uses voting techniques to score the output from the application of individual models, eg, the individual output from the application of each trained model, or one or more. It can be combined by selecting specific outputs. Further, in these implementations, the machine learning application applies a time threshold (eg, 0.5 ms) for applying individual trained models and only the individual outputs available within that time threshold. It can be used. Output that is not received within the time threshold is not used and can be discarded, for example. For example, such an approach may be appropriate when there is a specified time limit between invoking a machine learning application, for example by an operating system, or one or more applications.
異なる実装形態では、機械学習アプリケーションは、異なるタイプの出力を生成することができる。例えば、機械学習アプリケーションは、表現またはクラスタ（例えば、入力データの数値表現）、ラベル（例えば、画像、ドキュメントなどを含む入力データに対する）、フレーズまたは文章（例えば、入力文に対する応答としての使用に適している画像またはビデオの説明など）、画像（例えば、入力に応答して機械学習アプリケーションによって生成される）、音声またはビデオ（例えば、入力ビデオに応答して、機械学習アプリケーションは、特定の効果が適用された出力ビデオ、例えば、トレーニングされたモデルがコミックブックまたは特定のアーティストなどからのトレーニングデータを使用してトレーニングされる場合、コミックブックまたは特定のアーティストのスタイルでレンダリングされた出力ビデオを生成し得る）を提供することができる。いくつかの実装形態では、機械学習アプリケーションは、呼び出し側アプリケーション、例えば、オペレーティングシステムまたは１つまたは複数のアプリケーションによって指定されたフォーマットに基づいて出力を生成することができる。いくつかの実装形態では、呼び出し側アプリケーションは、別の機械学習アプリケーションであり得る。例えば、そのような構成は、敵対的生成ネットワークで使用され得、呼び出し側機械学習アプリケーションが、機械学習アプリケーションからの出力を使用してトレーニングされ、その逆も同様である。 In different implementations, machine learning applications can produce different types of output. For example, machine learning applications are suitable for use as representations or clusters (eg, numerical representations of input data), labels (eg for input data including images, documents, etc.), phrases or sentences (eg, for response to input statements). An image or video description (such as a description of an image or video), an image (eg, produced by a machine learning application in response to input), audio or video (eg, in response to an input video, a machine learning application has a particular effect. If the applied output video, for example, the trained model is trained using training data from a comic book or a specific artist, etc., generate an output video rendered in the style of the comic book or a specific artist. Get) can be provided. In some implementations, the machine learning application can generate output based on the format specified by the calling application, eg, the operating system or one or more applications. In some implementations, the calling application can be another machine learning application. For example, such a configuration can be used in a hostile generation network, where the calling machine learning application is trained using the output from the machine learning application and vice versa.
あるいは、メモリ内のソフトウェアのいずれも、他の任意の適切な記憶場所またはコンピュータ可読媒体に記憶することができる。さらに、メモリ（および／または他の接続された単数または複数の記憶デバイス）は、１つまたは複数のメッセージ、１つまたは複数の分類法、電子百科事典、辞書、シソーラス、ナレッジベース、メッセージデータ、文法、ユーザプリファレンス、および／または本明細書で説明された機能で使用される他の命令およびデータを格納することができる。メモリおよび他の任意の種類の記憶装置（磁気ディスク、光ディスク、磁気テープ、または他の有形の媒体）は、「記憶装置」または「記憶デバイス」と見なすことができる。 Alternatively, any of the software in memory can be stored on any other suitable storage location or computer-readable medium. In addition, memory (and / or other connected singular or multiple storage devices) includes one or more messages, one or more classifications, electronic encyclopedias, dictionaries, thesauruses, knowledge bases, message data, etc. It can store grammars, user preferences, and / or other instructions and data used in the functions described herein. Memory and any other type of storage device (magnetic disk, optical disk, magnetic tape, or other tangible medium) can be considered a "storage device" or "storage device".
Ｉ／Ｏインタフェースは、サーバデバイスを他のシステムおよびデバイスとインタフェースすることを可能にするための機能を提供することができる。インタフェースされたデバイスは、デバイスの一部として含めることができ、または分離してデバイスと通信することができる。例えば、ネットワーク通信デバイス、記憶デバイス（例えば、メモリおよび／またはデータベース１０６）、および入力／出力デバイスは、Ｉ／Ｏインタフェースを介して通信することができる。いくつかの実装形態では、Ｉ／Ｏインタフェースは、入力デバイス（キーボード、ポインティングデバイス、タッチスクリーン、マイクロフォン、カメラ、スキャナ、センサなど）および／または出力デバイス（表示デバイス、スピーカーデバイス、プリンター、モーターなど）のようなインタフェースデバイスに接続することができる。 The I / O interface can provide functionality that allows a server device to interface with other systems and devices. The interfaced device can be included as part of the device or can be separated and communicate with the device. For example, network communication devices, storage devices (eg, memory and / or database 106), and input / output devices can communicate via I / O interfaces. In some embodiments, the I / O interface is an input device (keyboard, pointing device, touch screen, microphone, camera, scanner, sensor, etc.) and / or output device (display device, speaker device, printer, motor, etc.). Can be connected to interface devices such as.
Ｉ／Ｏインタフェースに接続することができるインタフェースデバイスのいくつかの例は、コンテンツ、例えば、本明細書で説明されるような出力アプリケーションの画像、ビデオ、および／またはユーザインタフェースを表示するために使用できる１つまたは複数の表示デバイスを含み得る。表示デバイスは、ローカル接続（例えば、表示バス）および／またはネットワーク接続を介してデバイスに接続されることができ、任意の適切な表示デバイスとすることができる。表示デバイスは、ＬＣＤ、ＬＥＤ、またはプラズマディスプレイスクリーン、ＣＲＴ、テレビ、モニタ、タッチスクリーン、３Ｄディスプレイスクリーン、または他の視覚的表示デバイスのような任意の適切な表示デバイスを含むことができる。例えば、表示デバイスは、モバイルデバイスに提供されるフラットディスプレイスクリーン、ゴーグルまたはヘッドセットデバイスに提供される複数の表示スクリーン、またはコンピュータデバイス用のモニタスクリーンであり得る。 Some examples of interface devices that can be connected to an I / O interface are used to display content, such as images, videos, and / or user interfaces for output applications as described herein. It may include one or more display devices that can. The display device can be connected to the device via a local connection (eg, display bus) and / or a network connection and can be any suitable display device. Display devices can include LCDs, LEDs, or any suitable display device such as plasma display screens, CRTs, televisions, monitors, touch screens, 3D display screens, or other visual display devices. For example, the display device can be a flat display screen provided for a mobile device, multiple display screens provided for goggles or a headset device, or a monitor screen for a computer device.
Ｉ／Ｏインタフェースは、他の入力および出力デバイスとインタフェースすることができる。いくつかの例は、画像をキャプチャすることができる１つまたは複数のカメラを含む。いくつかの実装形態は、（例えば、キャプチャされた画像、音声コマンドなどの一部として）音声をキャプチャするためのマイクロフォン、音声を出力するためのオーディオスピーカーデバイス、または他の入出力デバイスを提供することができる。 The I / O interface can interface with other input and output devices. Some examples include one or more cameras capable of capturing images. Some implementations provide a microphone for capturing audio (eg, as part of a captured image, audio command, etc.), an audio speaker device for outputting audio, or other input / output device. be able to.
説明は、その特定の実装形態に関して記載されているが、これらの特定の実装形態は単なる例示であり、限定的なものではない。例に示されている概念は、他の例および実装形態にも適用することができる。 Although the description describes the particular implementation, these particular implementations are merely exemplary and not limiting. The concepts presented in the examples can be applied to other examples and implementations.
本明細書で説明される特定の実装形態が、ユーザに関する個人情報（例えば、ユーザデータ、ユーザのソーシャルネットワークに関する情報、ユーザの所在地および所在地における時間、ユーザのバイオメトリック情報、ユーザの活動およびデモグラフィック情報）を収集または使用し得る状況において、ユーザには、情報が収集されるかどうか、個人情報が格納されるかどうか、個人情報が使用されるかどうか、およびユーザに関する情報がどのように収集、格納、および使用されるかを制御するための１つまたは複数の機会が提供される。すなわち、本明細書で説明されたシステムおよび方法は、特に、関連するユーザからの明確な許可を受けたときに、ユーザの個人情報を収集、格納および／または使用する。例えば、ユーザは、プログラムまたは機能がその特定のユーザまたはプログラムまたは機能に関連する他のユーザについてのユーザ情報を収集するかどうかについての制御を提供される。個人情報が収集される各ユーザには、そのユーザに関連した情報収集の制御を可能にする１つまたは複数のオプションが提供され、それにより、情報が収集されてよいかどうか、およびその情報のどの部分が収集されるかに関する許可または認可を与える。例えば、ユーザには、１つまたは複数のそのような制御オプションが、通信ネットワークを通じて提供されてよい。さらに、特定のデータは、個人を特定可能な情報が削除されるように、格納または使用される前に１つまたは複数の方法で処理され得る。一例として、個人を識別可能な情報が判定され得ないように、ユーザの識別情報が取り扱われ得る。別の例として、ユーザの地理的位置は、ユーザの特定の位置が判定され得ないように、より大きな領域に一般化することができる。 The particular embodiments described herein include personal information about the user (eg, user data, information about the user's social network, the location and time at the location of the user, biometric information about the user, activity and demographics of the user. In situations where information) can be collected or used, the user will be asked if information is collected, if personal information is stored, if personal information is used, and how information about the user is collected. It provides one or more opportunities to control, store, and use. That is, the systems and methods described herein collect, store and / or use a user's personal information, especially with the explicit permission of the relevant user. For example, a user is provided with control over whether a program or feature collects user information about that particular user or other users associated with the program or feature. Each user for whom personal information is collected is provided with one or more options that allow control of the information collection associated with that user, thereby whether the information may be collected and of that information. Give permission or authorization as to what parts will be collected. For example, the user may be provided with one or more such control options through a communication network. In addition, certain data may be processed in one or more ways before being stored or used so that personally identifiable information is removed. As an example, user identification information may be treated so that personally identifiable information cannot be determined. As another example, the geographic location of a user can be generalized to a larger area so that a particular location of the user cannot be determined.
本開示に記載されている機能ブロック、動作、特徴、方法、デバイス、およびシステムは、当業者に知られているように、システム、デバイス、および機能ブロックの異なる組み合わせに統合または分割することができることに留意されたい。特定の実装形態のルーチンを実装するために、任意の適切なプログラミング言語およびプログラミング技術が使用され得る。例えば、手続き型またはオブジェクト指向などの異なるプログラミング技術が使用され得る。ルーチンは、単一の処理デバイスまたは複数のプロセッサ上で実行することができる。ステップ、動作、または計算は特定の順序で提示され得るが、異なる特定の実装形態においてはその順序は変更されてもよい。いくつかの実装形態では、本明細書において連続的なものとして示されている複数のステップまたは動作が同時に実行され得る。 The functional blocks, behaviors, features, methods, devices, and systems described in this disclosure can be integrated or subdivided into different combinations of systems, devices, and functional blocks, as known to those of skill in the art. Please note. Any suitable programming language and programming technique may be used to implement a routine of a particular implementation. For example, different programming techniques such as procedural or object-oriented may be used. Routines can be run on a single processing device or on multiple processors. Steps, actions, or calculations may be presented in a particular order, but in different particular implementations the order may change. In some implementations, multiple steps or actions, which are shown herein as continuous, may be performed simultaneously.
Claims (21)
複数のオブジェクトを含む電子会話内の１つまたは複数のオブジェクトを識別することであって、前記複数のオブジェクトは、異なるメディアタイプのものである、前記識別すること、
前記１つまたは複数のオブジェクトを１つまたは複数のオブジェクトグループにグループ化することであって、各オブジェクトグループは、少なくとも１つのオブジェクトを含む、前記グループ化すること、
前記１つまたは複数のオブジェクトグループに基づいて前記電子会話を分析し、前記電子会話の会話構造を判定すること、
前記電子会話の会話構造に基づいて、前記１つまたは複数のオブジェクトグループに会話フレーミングを適用し、音声インタフェース会話プレゼンテーションを生成すること、
音声出力デバイスによる出力のために構成された前記音声インタフェース会話プレゼンテーションを提供すること
を含み、
前記会話フレーミングを適用することは、オブジェクトグループのペア間に１つまたは複数の中間会話フレーミング部分を挿入することを含み、
前記１つまたは複数のオブジェクトグループは、少なくとも２つのオブジェクトグループを含み、
前記１つまたは複数の中間会話フレーミング部分は、前記少なくとも２つのオブジェクトグループの、１つまたは複数のそれぞれのペア間に挿入される、コンピュータ実装方法。 It ’s a computer implementation method.
Identifying one or more objects in an electronic conversation that includes a plurality of objects, wherein the plurality of objects are of different media types.
Grouping the one or more objects into one or more object groups, wherein each object group comprises at least one object.
Analyzing the electronic conversation based on the one or more object groups to determine the conversation structure of the electronic conversation.
To generate a voice interface conversation presentation by applying conversation framing to the one or more object groups based on the conversation structure of the electronic conversation.
Look including providing said voice interface dialogue presentation that is configured for output by the audio output device,
Applying the conversation framing involves inserting one or more intermediate conversation framing portions between pairs of object groups.
The one or more object groups include at least two object groups.
A computer implementation method in which the one or more intermediate conversation framing portions are inserted between each one or more pairs of the at least two object groups.
連続する言語オブジェクトをグループ化すること、
連続する非言語オブジェクトをグループ化すること、
前記電子会話のシーケンス情報を保持すること
を含む、請求項２に記載のコンピュータ実装方法。 Grouping the one or more objects mentioned above
Grouping consecutive language objects,
Grouping consecutive non-linguistic objects,
The computer implementation method according to claim 2, wherein the sequence information of the electronic conversation is retained.
前記音声クエリに対する音声応答を受信すること、
前記音声応答に基づいて、前記音声インタフェース会話プレゼンテーションが前記音声出力デバイスから出力されるようにすること
をさらに含む、請求項１乃至１３のいずれか一項に記載のコンピュータ実装方法。 Presenting a voice query to the user when it is determined that the operating context indicates that the voice interface presentation is a better presentation format than the display interface.
Receiving a voice response to the voice query,
The computer implementation method according to any one of claims 1 to 13 , further comprising causing the voice interface conversation presentation to be output from the voice output device based on the voice response.
前記デバイスのコンテキストが、視覚表示よりも音声出力の方が適しているコンテキストである場合、前記音声インタフェース会話プレゼンテーションが前記音声出力デバイスを介して出力されるようにすること、
前記デバイスのコンテキストが、音声出力よりも視覚表示の方が適しているコンテキストである場合、前記電子会話がディスプレイデバイスに表示されるようにすること
をさらに含む、請求項１乃至１５のいずれか一項に記載のコンピュータ実装方法。 Determining device context,
When the context of the device is a context in which audio output is more suitable than visual display, the audio interface conversation presentation is to be output via the audio output device.
One of claims 1 to 15 , further comprising allowing the electronic conversation to be displayed on a display device when the context of the device is a context in which visual display is more suitable than audio output. The computer implementation method described in the section.
１つまたは複数のプロセッサによって実行されると、１つまたは複数のプロセッサに動作を実行させるソフトウェア命令を格納した非一時的コンピュータ可読媒体に結合された１つまたは複数のプロセッサ
を備え、前記動作は、
複数のオブジェクトを含む電子会話における１つまたは複数のオブジェクトを識別することであって、前記複数のオブジェクトは、異なるメディアタイプのものである、前記識別すること、
前記電子会話を分析し、前記電子会話の会話構造を判定すること、
前記電子会話の会話構造に基づいて、前記１つまたは複数のオブジェクトに会話フレーミングを適用し、代替インタフェース会話プレゼンテーションを生成すること、
非表示出力デバイスによる出力のために構成された少なくとも一部を有する代替インタフェース会話プレゼンテーションを提供すること
を含み、
前記会話フレーミングを適用することは、オブジェクトグループのペア間に１つまたは複数の中間会話フレーミング部分を挿入することを含み、
前記１つまたは複数のオブジェクトグループは、少なくとも２つのオブジェクトグループを含み、
前記１つまたは複数の中間会話フレーミング部分は、前記少なくとも２つのオブジェクトグループの、１つまたは複数のそれぞれのペア間に挿入される、システム。 It ’s a system,
When executed by one or more processors, the operation comprises one or more processors coupled to a non-temporary computer-readable medium containing software instructions that cause one or more processors to perform the operation. ,
Identifying one or more objects in an electronic conversation involving a plurality of objects, wherein the plurality of objects are of different media types.
Analyzing the electronic conversation to determine the conversation structure of the electronic conversation,
Applying conversation framing to the one or more objects based on the conversation structure of the electronic conversation to generate an alternative interface conversation presentation.
Look including to provide an alternative interface conversation presentations having at least a portion configured for output by the non-display output device,
Applying the conversation framing involves inserting one or more intermediate conversation framing portions between pairs of object groups.
The one or more object groups include at least two object groups.
A system in which the one or more intermediate conversation framing portions are inserted between each one or more pairs of the at least two object groups .
複数のオブジェクトを含む電子会話における１つまたは複数のオブジェクトを識別することであって、前記複数のオブジェクトは、異なるメディアタイプのものである、前記識別すること、
前記電子会話を分析し、前記電子会話の会話構造を判定すること、
前記電子会話の会話構造に基づいて、前記１つまたは複数のオブジェクトに会話フレーミングを適用し、代替インタフェース会話プレゼンテーションを生成すること、
非表示出力デバイスによる出力のために構成された少なくとも一部を有する代替インタフェース会話プレゼンテーションを提供すること
を含み、
前記会話フレーミングを適用することは、オブジェクトグループのペア間に１つまたは複数の中間会話フレーミング部分を挿入することを含み、
前記１つまたは複数のオブジェクトグループは、少なくとも２つのオブジェクトグループを含み、
前記１つまたは複数の中間会話フレーミング部分は、前記少なくとも２つのオブジェクトグループの、１つまたは複数のそれぞれのペア間に挿入される、コンピュータプログラム。 A computer program comprising software instructions that cause one or more processors to perform an operation when executed by one or more processors.
Identifying one or more objects in an electronic conversation involving a plurality of objects, wherein the plurality of objects are of different media types.
Analyzing the electronic conversation to determine the conversation structure of the electronic conversation,
Applying conversation framing to the one or more objects based on the conversation structure of the electronic conversation to generate an alternative interface conversation presentation.
Look including to provide an alternative interface conversation presentations having at least a portion configured for output by the non-display output device,
Applying the conversation framing involves inserting one or more intermediate conversation framing portions between pairs of object groups.
The one or more object groups include at least two object groups.
A computer program in which the one or more intermediate conversation framing portions are inserted between each one or more pairs of the at least two object groups.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
JP2021085306A JP7391913B2 (en) | 2017-11-06 | 2021-05-20 | Parsing electronic conversations for presentation in alternative interfaces |
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/805,049 | 2017-11-06 | ||
US15/805,049 US10599391B2 (en) | 2017-11-06 | 2017-11-06 | Parsing electronic conversations for presentation in an alternative interface |
PCT/US2018/058030 WO2019089469A1 (en) | 2017-11-06 | 2018-10-29 | Parsing electronic conversations for presentation in an alternative interface |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2021085306A Division JP7391913B2 (en) | 2017-11-06 | 2021-05-20 | Parsing electronic conversations for presentation in alternative interfaces |
Publications (2)
Publication Number | Publication Date |
---|---|
JP2020521995A JP2020521995A (en) | 2020-07-27 |
JP6889281B2 true JP6889281B2 (en) | 2021-06-18 |
Family
ID=64317003
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2019559778A Active JP6889281B2 (en) | 2017-11-06 | 2018-10-29 | Analyzing electronic conversations for presentations in alternative interfaces |
JP2021085306A Active JP7391913B2 (en) | 2017-11-06 | 2021-05-20 | Parsing electronic conversations for presentation in alternative interfaces |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2021085306A Active JP7391913B2 (en) | 2017-11-06 | 2021-05-20 | Parsing electronic conversations for presentation in alternative interfaces |
Country Status (5)
Country | Link |
---|---|
US (2) | US10599391B2 (en) |
EP (1) | EP3612926B1 (en) |
JP (2) | JP6889281B2 (en) |
CN (2) | CN111279349B (en) |
WO (1) | WO2019089469A1 (en) |
Families Citing this family (12)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2018192500A1 (en) * | 2017-04-19 | 2018-10-25 | 上海寒武纪信息科技有限公司 | Processing apparatus and processing method |
US10599391B2 (en) * | 2017-11-06 | 2020-03-24 | Google Llc | Parsing electronic conversations for presentation in an alternative interface |
KR102424520B1 (en) * | 2017-11-29 | 2022-07-25 | 삼성전자주식회사 | Electronic device and method for operating the same |
KR102515023B1 (en) * | 2018-02-23 | 2023-03-29 | 삼성전자주식회사 | Electronic apparatus and control method thereof |
US11063887B2 (en) * | 2018-09-13 | 2021-07-13 | Sharp Kabushiki Kaisha | Information processing apparatus, user terminal apparatus, and control method |
US10747894B1 (en) * | 2018-09-24 | 2020-08-18 | Amazon Technologies, Inc. | Sensitive data management |
US11303588B1 (en) * | 2019-09-05 | 2022-04-12 | Meta Platforms, Inc. | Automating a response to a message communicated to a business entity via an online messaging application |
US11151331B1 (en) * | 2020-05-13 | 2021-10-19 | International Business Machines Corporation | Layered neural networks to evaluate communication distribution |
CN114513770B (en) * | 2020-10-29 | 2024-01-30 | 伊姆西Ip控股有限责任公司 | Method, system and medium for deploying application |
JP7132576B2 (en) * | 2020-10-30 | 2022-09-07 | ソプラ株式会社 | Security ID Conversation Search System |
US11935007B1 (en) * | 2022-12-27 | 2024-03-19 | Dropbox, Inc. | Generating collaborative content items to provide customizable graphical representations in online graphical user interfaces |
US11947902B1 (en) * | 2023-03-03 | 2024-04-02 | Microsoft Technology Licensing, Llc | Efficient multi-turn generative AI model suggested message generation |
Family Cites Families (45)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JPH10254470A (en) * | 1997-03-13 | 1998-09-25 | Fujitsu Ten Ltd | Text voice synthesizer |
US7533149B2 (en) * | 2004-04-30 | 2009-05-12 | Microsoft Corporation | Maintaining multiple versions of message bodies in a common database |
JP4476725B2 (en) * | 2004-07-21 | 2010-06-09 | 株式会社東芝 | Digital broadcast receiver |
JP2006039307A (en) * | 2004-07-28 | 2006-02-09 | Kojima Press Co Ltd | Voice information transmitting device for automobile |
JP2006318086A (en) * | 2005-05-11 | 2006-11-24 | Sharp Corp | Device for selecting template, mobile phone having this device, method of selecting template, program for making computer function as this device for selecting template, and recording medium |
JP4781735B2 (en) * | 2005-06-29 | 2011-09-28 | 京セラ株式会社 | Mobile phone |
US7783592B2 (en) * | 2006-01-10 | 2010-08-24 | Aol Inc. | Indicating recent content publication activity by a user |
JP4901383B2 (en) * | 2006-09-13 | 2012-03-21 | ソフトバンクモバイル株式会社 | Media conversion message system |
JP2008081031A (en) * | 2006-09-28 | 2008-04-10 | Clarion Co Ltd | On-vehicle device with notifying function |
US8996376B2 (en) | 2008-04-05 | 2015-03-31 | Apple Inc. | Intelligent text-to-speech conversion |
JP2010039289A (en) * | 2008-08-06 | 2010-02-18 | Toshiba Corp | Mobile electronic device |
JP2010072704A (en) * | 2008-09-16 | 2010-04-02 | Toshiba Corp | Interface device and input method |
US9043474B2 (en) * | 2010-01-20 | 2015-05-26 | Microsoft Technology Licensing, Llc | Communication sessions among devices and interfaces with mixed capabilities |
JP5643545B2 (en) * | 2010-05-25 | 2014-12-17 | 京セラ株式会社 | Communication terminal and e-mail reading method |
US20110320373A1 (en) * | 2010-06-25 | 2011-12-29 | Microsoft Corporation | Product conversations among social groups |
US8352908B2 (en) | 2010-06-28 | 2013-01-08 | International Business Machines Corporation | Multi-modal conversion tool for form-type applications |
WO2012150602A1 (en) * | 2011-05-03 | 2012-11-08 | Yogesh Chunilal Rathod | A system and method for dynamically monitoring, recording, processing, attaching dynamic, contextual & accessible active links & presenting of physical or digital activities, actions, locations, logs, life stream, behavior & status |
US20130110866A1 (en) * | 2011-10-28 | 2013-05-02 | Microsoft Corporation | Information system incorporating real-time data sources to enrich connections among users |
US9736089B2 (en) * | 2011-11-02 | 2017-08-15 | Blackberry Limited | System and method for enabling voice and video communications using a messaging application |
US8693643B2 (en) * | 2011-11-16 | 2014-04-08 | At&T Mobility Ii Llc | Integrating visual voicemail within a threaded communication environment |
US10313279B2 (en) * | 2011-12-12 | 2019-06-04 | Rcs Ip, Llc | Live video-chat function within text messaging environment |
US20140046976A1 (en) * | 2012-08-11 | 2014-02-13 | Guangsheng Zhang | Systems, methods, and user interface for effectively presenting information |
AU2013234381A1 (en) * | 2012-09-26 | 2014-04-10 | PicSafe IP Holdings Pty Ltd | Data handling system and method |
PL401346A1 (en) | 2012-10-25 | 2014-04-28 | Ivona Software Spółka Z Ograniczoną Odpowiedzialnością | Generation of customized audio programs from textual content |
EP2772908B1 (en) * | 2013-02-27 | 2016-06-01 | BlackBerry Limited | Method And Apparatus For Voice Control Of A Mobile Device |
WO2014197737A1 (en) * | 2013-06-08 | 2014-12-11 | Apple Inc. | Automatically adapting user interfaces for hands-free interaction |
CN104753985B (en) * | 2013-12-30 | 2018-12-28 | 腾讯科技（深圳）有限公司 | Session list display methods and device |
JP2016014969A (en) * | 2014-07-01 | 2016-01-28 | クラリオン株式会社 | On-vehicle device and information processing system |
US10101716B2 (en) * | 2014-12-04 | 2018-10-16 | Belkin International, Inc. | Autonomous, distributed, rule-based intelligence |
US10565993B2 (en) * | 2015-01-30 | 2020-02-18 | Disney Enterprises, Inc. | Enhancing group decisions within social messaging applications |
US9876868B2 (en) * | 2015-03-12 | 2018-01-23 | International Business Machines Corporation | GPS suggestions based on social points of interest (POI) metadata |
US20170353410A1 (en) * | 2015-05-06 | 2017-12-07 | Matt Gonzales | Messaging Sharing System and Method of Use |
CN106383648A (en) * | 2015-07-27 | 2017-02-08 | 青岛海信电器股份有限公司 | Intelligent terminal voice display method and apparatus |
KR102490438B1 (en) * | 2015-09-02 | 2023-01-19 | 삼성전자주식회사 | Display apparatus and control method thereof |
WO2017075498A1 (en) * | 2015-10-30 | 2017-05-04 | Forq, Inc. | Digital recipe library and network with food image recognition services |
US9946862B2 (en) * | 2015-12-01 | 2018-04-17 | Qualcomm Incorporated | Electronic device generating notification based on context data in response to speech phrase from user |
US10116603B1 (en) * | 2015-12-10 | 2018-10-30 | Google Llc | Methods, systems, and media for identifying and presenting video objects linked to a source video |
KR20170100175A (en) * | 2016-02-25 | 2017-09-04 | 삼성전자주식회사 | Electronic device and method for operating thereof |
US10015124B2 (en) * | 2016-09-20 | 2018-07-03 | Google Llc | Automatic response suggestions based on images received in messaging applications |
US20180173377A1 (en) * | 2016-12-15 | 2018-06-21 | Microsoft Technology Licensing, Llc | Condensed communication chain control surfacing |
US20180174587A1 (en) * | 2016-12-16 | 2018-06-21 | Kyocera Document Solution Inc. | Audio transcription system |
US10146768B2 (en) * | 2017-01-25 | 2018-12-04 | Google Llc | Automatic suggested responses to images received in messages using language model |
US11018884B2 (en) * | 2017-04-24 | 2021-05-25 | Microsoft Technology Licensing, Llc | Interactive timeline that displays representations of notable events based on a filter or a search |
US10348658B2 (en) * | 2017-06-15 | 2019-07-09 | Google Llc | Suggested items for use with embedded applications in chat conversations |
US10599391B2 (en) * | 2017-11-06 | 2020-03-24 | Google Llc | Parsing electronic conversations for presentation in an alternative interface |
-
2017
- 2017-11-06 US US15/805,049 patent/US10599391B2/en active Active
-
2018
- 2018-10-29 CN CN201880035073.0A patent/CN111279349B/en active Active
- 2018-10-29 WO PCT/US2018/058030 patent/WO2019089469A1/en unknown
- 2018-10-29 JP JP2019559778A patent/JP6889281B2/en active Active
- 2018-10-29 CN CN202311678743.XA patent/CN117669605A/en active Pending
- 2018-10-29 EP EP18803835.0A patent/EP3612926B1/en active Active
-
2020
- 2020-03-20 US US16/826,019 patent/US11036469B2/en active Active
-
2021
- 2021-05-20 JP JP2021085306A patent/JP7391913B2/en active Active
Also Published As
Publication number | Publication date |
---|---|
US20200218503A1 (en) | 2020-07-09 |
JP7391913B2 (en) | 2023-12-05 |
EP3612926A1 (en) | 2020-02-26 |
CN117669605A (en) | 2024-03-08 |
EP3612926B1 (en) | 2023-07-26 |
WO2019089469A1 (en) | 2019-05-09 |
US10599391B2 (en) | 2020-03-24 |
JP2021185478A (en) | 2021-12-09 |
US20190138267A1 (en) | 2019-05-09 |
CN111279349A (en) | 2020-06-12 |
JP2020521995A (en) | 2020-07-27 |
CN111279349B (en) | 2024-01-05 |
US11036469B2 (en) | 2021-06-15 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
JP6889281B2 (en) | Analyzing electronic conversations for presentations in alternative interfaces | |
US11509616B2 (en) | Assistance during audio and video calls | |
US11829404B2 (en) | Functional image archiving | |
CN110298906B (en) | Method and device for generating information | |
CN107632706B (en) | Application data processing method and system of multi-modal virtual human | |
CN111476871B (en) | Method and device for generating video | |
CN107294837A (en) | Engaged in the dialogue interactive method and system using virtual robot | |
JP2022525272A (en) | Image display with selective motion drawing | |
JP7158478B2 (en) | image selection suggestions | |
KR102461919B1 (en) | Technology to capture and edit dynamic depth images | |
WO2019085625A1 (en) | Emotion picture recommendation method and apparatus | |
CN110674706B (en) | Social contact method and device, electronic equipment and storage medium | |
CN113792196A (en) | Method and device for man-machine interaction based on multi-modal dialog state representation | |
US11973806B2 (en) | Alteration of event user interfaces of an online conferencing service | |
CN113767379A (en) | Rendering content using content proxies and/or stored content parameters | |
CN113785540B (en) | Method, medium and system for generating content promotions using machine learning nominators | |
CN110709869B (en) | Suggested items for use with embedded applications in chat conversations |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20191031 |
|
A621 | Written request for application examination |
Free format text: JAPANESE INTERMEDIATE CODE: A621Effective date: 20191031 |
|
A977 | Report on retrieval |
Free format text: JAPANESE INTERMEDIATE CODE: A971007Effective date: 20201029 |
|
A131 | Notification of reasons for refusal |
Free format text: JAPANESE INTERMEDIATE CODE: A131Effective date: 20201110 |
|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20210210 |
|
TRDD | Decision of grant or rejection written | ||
A01 | Written decision to grant a patent or to grant a registration (utility model) |
Free format text: JAPANESE INTERMEDIATE CODE: A01Effective date: 20210420 |
|
A61 | First payment of annual fees (during grant procedure) |
Free format text: JAPANESE INTERMEDIATE CODE: A61Effective date: 20210520 |
|
R150 | Certificate of patent or registration of utility model |
Ref document number: 6889281Country of ref document: JPFree format text: JAPANESE INTERMEDIATE CODE: R150 |