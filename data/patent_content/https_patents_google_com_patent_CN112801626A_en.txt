CN112801626A - Automated assistant-implemented method and related storage medium - Google Patents
Automated assistant-implemented method and related storage medium Download PDFInfo
- Publication number
- CN112801626A CN112801626A CN202110154214.4A CN202110154214A CN112801626A CN 112801626 A CN112801626 A CN 112801626A CN 202110154214 A CN202110154214 A CN 202110154214A CN 112801626 A CN112801626 A CN 112801626A
- Authority
- CN
- China
- Prior art keywords
- user
- task
- automated assistant
- custom
- values
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06Q—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES; SYSTEMS OR METHODS SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES, NOT OTHERWISE PROVIDED FOR
- G06Q10/00—Administration; Management
- G06Q10/10—Office automation; Time management
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/33—Querying
- G06F16/332—Query formulation
- G06F16/3329—Natural language query formulation or dialogue systems
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/33—Querying
- G06F16/3331—Query processing
- G06F16/334—Query execution
- G06F16/3343—Query execution using phonetics
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/33—Querying
- G06F16/3331—Query processing
- G06F16/334—Query execution
- G06F16/3344—Query execution using natural language analysis
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/33—Querying
- G06F16/338—Presentation of query results
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/279—Recognition of textual entities
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/30—Semantic analysis
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/30—Semantic analysis
- G06F40/35—Discourse or dialogue representation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/46—Multiprogramming arrangements
- G06F9/48—Program initiating; Program switching, e.g. by interrupt
- G06F9/4806—Task transfer initiation or dispatching
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/04—Segmentation; Word boundary detection
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/1815—Semantic context, e.g. disambiguation of the recognition hypotheses based on word meaning
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/28—Constructional details of speech recognition systems
- G10L15/30—Distributed recognition, e.g. in client-server systems, for mobile phones or network applications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/48—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use
- G10L25/51—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use for comparison or discrimination
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/223—Execution procedure of a spoken command
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/225—Feedback of the input speech
Abstract
The present disclosure relates to automated assistant-implemented methods and related storage media. The technology involves allowing a user to employ voice-based human-machine dialog to program an automated assistant with a custom routine or "dialog routine" that can be later called to complete a task. In various implementations, a first free-form natural language input may be received from a user that identifies a command to map to a task and a slot required to be filled with a value to fulfill the task. A dialog routine may be stored that includes a mapping between the command and the task, and that accepts as input a value for filling the slot. Subsequent free-form natural language input may be received from the user to (i) invoke the conversation routine based on the mapping, and/or (ii) identify a value for filling the slot. Data indicative of at least the value may be transmitted to a remote computing device for fulfilling the task.
Description
Description of the cases
The application belongs to divisional application of Chinese patent application 201880039314.9 with application date of 2018, 10 and 2.
Technical Field
The present disclosure relates to automated assistant-implemented methods and related storage media.
Background
Humans may engage in human-to-human conversations using an interactive software application, also referred to herein as an "automated assistant" (also referred to as a "chat robot," "interactive personal assistant," "intelligent personal assistant," "personal voice assistant," "conversation agent," etc.). For example, a human being (which may be referred to as a "user" when they interact with an automated assistant) may provide commands, queries, and/or requests (collectively referred to herein as "queries") using free-form natural language input that may include spoken utterances that are converted to text and then processed, and/or typed free-form natural language input.
Typically, automated assistants are configured to perform various tasks, for example, in response to various predetermined specification commands to which the tasks are mapped. These tasks may include such things as ordering items (e.g., food, products, services, etc.), playing media (e.g., music, video), modifying shopping lists, performing home controls (e.g., controlling a thermostat, controlling one or more lights, etc.), answering questions, ordering tickets, and so forth. While natural language analysis and semantic processing enable users to issue subtle variants of specification commands, these variants may only shift within limits where natural language analysis and semantic processing can determine which task to perform. In short, despite many advances in natural language and semantic analysis, task-oriented dialog management is still relatively rigid. Additionally, users often do not know or forget to specify commands, and thus may not be able to invoke automated assistants to perform many of the tasks they can do. Moreover, adding new tasks requires third party developers to add new specification commands, and automated assistants typically take time and resources to learn acceptable changes to those specification commands.
Disclosure of Invention
Techniques are described herein for allowing a user to use a voice-based human-machine conversation to program an automated assistant with a custom routine or "conversation routine" that can be later called to complete a task. In some implementations, the user can cause the automated assistant to learn a new conversation routine by providing free-form natural language input that includes commands for performing tasks. If the automated assistant is unable to interpret the command, the automated assistant may ask for clarification from the user regarding the command. For example, in some implementations, the automated assistant can prompt the user to identify one or more slots that are required to be filled with values to fulfill a task. In other implementations, the user may proactively identify the slot without prompting from the automated assistant. In some implementations, the user may, for example, upon request by the automated assistant or proactively provide an enumerated list of possible values for populating one or more of the slots. The automated assistant may then store a conversation routine that includes a mapping between commands and tasks, and the conversation routine accepts as input one or more values for populating one or more slots. The user may later invoke the dialog routine using free-form natural language input including commands or some grammatical/semantic change thereof.
Once the conversation routine is called and the slot of the conversation routine is populated with values by the user, the automated assistant can take various actions. In some implementations, the automated assistant can send data indicative of at least the user-provided slot, the slot itself, and/or data indicative of the command/task to the remote computing system. In some cases, such transmission may cause the remote computing system to output, for example, a natural language output or other data indicative of values/slots/commands/tasks to another person. Such natural language output may be provided to others in various ways (which may not require the request to be processed by others installing or configuring their own third-party software agents) (e.g., via email, text message, automated phone call, etc.). The other person may then perform the task.
Additionally or alternatively, in some implementations, various aspects of the conversation routine (such as slots, potential slot values, commands, etc.) may be compared to similar components of a plurality of known candidate tasks (e.g., for which the user is not aware of canonical commands). A mapping may be generated between the best matching candidate task and the user's command such that future use of the automated assistant's command (or grammatical and/or semantic changes thereof) by the user will invoke the dialog routine and, ultimately, the best matching candidate task. If multiple candidate tasks match the dialog routine equally, the user may be prompted to select one task, or other signals such as the user's context, prior application usage, etc. may be used to break ties.
Assume that the user is involved (engage) with the automated assistant in the following dialog:
the user: "I want a pizza (I want pizza)"
AA: "I don't know how to order pizza"
The user: "to order a pizza, you need to know the type of crust and the list of Toppings for ordering pizza"
AA: "what are the permissive pizza crusts types? (what are the possible pizza crust types?
The user: "thin crust or thick crust"
AA: "what are the permissible tops? (what is a possible topping)?
The user: "her are the permissible values"
AA: "okay, ready to order a pizza?
The user: "yes, get me a thin crust pizza for tomato topping me with a tomato topping"
The command in this scenario is "I want pizza" and the task is to order pizza. The user-defined slot that is required to be filled in order to fulfill the task includes a list of types of skins and toppings.
In some implementations, the task of ordering a pizza can be accomplished by providing a natural language output to the pizza shop (which the user can specify or can be automatically selected, e.g., based on distance, rating, price, known user preferences, etc.), e.g., via email, text message, automated phone call, etc. An employee of a pizza shop may receive natural language output via the output of one or more computing devices (e.g., a computer terminal in the shop, the employee's phone, a speaker in the shop, etc.), which may say a language like "< User > who like to order a < crust _ style > pizza with < topping 1, topping 2, > (< User > wants to order a < crust _ style > pizza with < topping 1, topping 2, >).
In some embodiments, the pizza shop employee may be asked to confirm the user's request, for example by pressing "1" or by saying "OK", "I accept", etc. Once confirmation is received, in some embodiments, the requesting user's automated assistant may or may not provide a confirmatory output, such as "your pizza is on the way". In some implementations, the natural language output provided at the pizza shop may also convey other information, such as payment information, the user's address, and the like. Such other information may be obtained from the requesting user at the same time as the dialog routine is created or determined automatically, e.g., based on the user's profile.
In other embodiments where the commands are mapped to predetermined third party software agents (e.g., a third party software agent for a particular pizza parlor), the task of ordering a pizza may be accomplished automatically via the third party software agent. For example, information indicating the slot/value may be provided to the third-party software agent in various forms. Assuming all necessary slots are filled with the appropriate values, a third party software agent may perform the task of placing a pizza order for the user. If it is not known why the third-party software agent requires additional information (e.g., additional slot values), it may interface with the automated assistant to cause the automated assistant to prompt the user for the requested additional information.
The techniques described herein may yield various technical advantages. As noted above, task-based dialog management is currently handled primarily as a canonical command that is manually created and mapped to a predefined task. This is limited in its scalability, as it requires third party developers to create and inform users of these mappings. As such, it requires the user to learn the canonical commands and remember them for later use. For these reasons, users with limited ability to provide input to complete a task, such as physically disabled users and/or users engaged in other tasks (e.g., driving), may have the trouble of having the automated assistant perform the task. Furthermore, when a user attempts to invoke a task on an unexplained command, additional computing resources are required to disambiguate the user's request or otherwise seek clarification. By allowing users to create their own dialog routines that are invoked using custom commands, users are more likely to remember the commands and/or are able to successfully and/or more quickly complete tasks via automated assistants. This may conserve computing resources that may or may not be necessary for the aforementioned disambiguation/clarification. Further, in some implementations, user-created dialog routines may be shared with other users, enabling the automated assistant to be more responsive to "long tail" commands from individual users that may be used by others.
In some implementations, there is provided a method performed by one or more processors, the method comprising: receiving, at one or more input components of a computing device, a first free-form natural language from a user, wherein the first free-form natural language input comprises a command to perform a task; performing semantic processing on the free-form natural language input; determining, based on the semantic processing, that the automated assistant is unable to interpret the command; providing, at one or more output components of the computing device, an output that solicits clarification from a user regarding the command; receiving a second free-form natural language input from the user at one or more of the input components, wherein the second free-form natural language input identifies one or more slots that are required to be filled with values in order to fulfill the task; storing a conversation routine that includes a mapping between the command and the task, and that accepts as input one or more values for populating the one or more slots; receiving a third free-form natural language input from the user at one or more of the input components, wherein the third free-form natural language input invokes the dialog routine based on the mapping; identifying, based on the third free-form natural language input or additional free-form natural language inputs, one or more values to be used to fill the one or more slots required to be filled with values in order to fulfill the task; and transmitting data to a remote computing device indicating at least one or more values to be used to fill the one or more slots, wherein the transmitting causes the remote computing device to fulfill the task.
These and other implementations of the technology disclosed herein may optionally include one or more of the following features.
In various embodiments, the method may further comprise: comparing the conversation routine to a plurality of candidate tasks that can be performed by the automated assistant; and selecting the task to which the command is mapped from the plurality of candidate tasks based on the comparison. In various implementations, the task to which the command is mapped includes a third party proxy task, wherein the sending causes the remote computing device to execute the third party proxy task using the one or more values to populate the one or more slots. In various implementations, the comparing may include comparing the one or more slots required to be filled in order to fulfill the task with one or more slots associated with each of the plurality of candidate tasks.
In various embodiments, the method may further comprise: receiving a fourth free-form natural language input from the user at one or more of the input components prior to the storing. In various implementations, the fourth free-form natural language input may include a user-provided enumeration list of possible values for filling one or more of the slots. In various implementations, the comparing may include, for each of the plurality of candidate tasks, comparing the user-provided enumerated list of possible values to an enumerated list of possible values for populating one or more slots of the candidate task.
In various implementations, the data indicating at least the one or more values may further include one or both of an indication of the command or an indication of the task to which the command is mapped. In various implementations, the data indicative of at least the one or more values may be in the form of natural language output requesting that the task be performed based on the one or more values, and the sending causes the remote computing device to provide the natural language as output.
In another closely related aspect, a method may comprise: receiving a first free-form natural language input from a user at one or more input components, wherein the first free-form natural language input identifies a command that the user intends to be mapped to a task and one or more slots that are required to be populated with values in order to fulfill the task; storing a conversation routine that includes a mapping between the command and the task, and that accepts as input one or more values for populating the one or more slots; receiving a second free-form natural language input from the user at one or more of the input components, wherein the second free-form natural language input invokes the dialog routine based on the mapping; identifying, based on the second free-form natural language input or additional free-form natural language inputs, one or more values to be used to fill the one or more slots required to be filled with values in order to fulfill the task; and transmitting data to a remote computing device indicating at least one or more values to be used to fill the one or more slots, wherein the transmitting causes the remote computing device to fulfill the task.
Further, some embodiments include one or more processors of one or more computing devices, wherein the one or more processors are operable to execute instructions stored in an associated memory, and wherein the instructions are configured to cause performance of any of the aforementioned methods. Some embodiments also include one or more non-transitory computer-readable storage media storing computer instructions executable by one or more processors to perform any of the foregoing methods.
It should be appreciated that all combinations of the above concepts and additional concepts described in greater detail herein are contemplated as being part of the subject matter disclosed herein. For example, all combinations of claimed subject matter appearing at the end of this disclosure are contemplated as being part of the subject matter disclosed herein.
Drawings
FIG. 1 is a block diagram of an example environment in which embodiments disclosed herein may be implemented.
FIG. 2 schematically depicts one example of how data generated during invocation of a dialog routine may flow between various components, in accordance with various embodiments.
FIG. 3 schematically illustrates one example of how data may be exchanged between various components when a dialog routine is called, in accordance with various embodiments.
Fig. 4 depicts a flowchart illustrating an example method according to embodiments disclosed herein.
Fig. 5 illustrates an example architecture of a computing device.
Detailed Description
Turning now to fig. 1, an example environment is illustrated in which the techniques disclosed herein may be implemented. The example environment includes a plurality of client computing devices 1061-N. Each client device 106 may execute a respective instance of the automated assistant client 118. One or more cloud-based automated assistant components 119, such as natural language processor 122, may be implemented on one or more computing systems (collectively "cloud" computing systems) communicatively coupled to client device 106 via one or more local and/or wide area networks (e.g., the internet), indicated generally at 1101-N。
In some implementations, the instance of the automated assistant client 118 through its interaction with the one or more cloud-based automated assistant components 119 can form what appears from the user's perspective to be a logical instance of the automated assistant 120 with which the user can interface in a man-machine conversation. Two examples of such automated assistants 120 are depicted in fig. 1. The first automated assistant 120A, which is enclosed by the dashed line, is for operating the first client device 1061And includes an automated assistant client 1181And one or more cloud-based automated assistant components 119. The second automated assistant 120B, which is included by the chain double-dashed line, is for operating another client device 106NAnd includes an automated assistant client 118NAnd one or more cloud-based automated assistant components 119. It should therefore be appreciated that in some implementations, each user interfacing with the automated assistant client 118 executing on the client device 106 may actually interface with his or her own logical instance of the automated assistant 120. For the sake of brevity and simplicity, the term "automated assistant" as used herein like "serving" a particular user "will refer to a combination of an automated assistant client 118 executing on a client device 106 operated by the user and one or more cloud-based automated assistant components 119 (which may be shared among multiple automated assistant clients 118). It should also be understood that in some embodimentsIn this regard, the automated assistant 120 can respond to requests from any user regardless of whether the user is actually "served" by that particular instance of the automated assistant 120.
Client device 1061-NMay include, for example, one or more of the following: a desktop computing device, a laptop computing device, a tablet computing device, a mobile phone computing device, a computing device of a user's vehicle (e.g., an in-vehicle communication system, an in-vehicle entertainment system, an in-vehicle navigation system), a stand-alone interactive speaker, a smart home appliance such as a smart television, and/or a wearable apparatus of a user that includes a computing device (e.g., a watch of a user with a computing device, glasses of a user with a computing device, a virtual or augmented reality computing device). Additional and/or alternative client computing devices may be provided.
In various implementations, the client computing device 1061-NCan operate a variety of different applications, such as multiple message exchange clients 1071-NTo a corresponding one of them. Messaging client 1071-NMay occur in various forms and these forms may span the client computing device 1061-NAnd/or may be varied and/or may be at the client computing device 1061-NOperate multiple modalities on a single client computing device. In some implementations, the message exchange client 1071-NMay occur in the form of: a short messaging service ("SMS") and/or multimedia messaging service ("MMS") client, an online chat client (e.g., instant messenger, internet relay chat or "IRC," etc.), a messaging application associated with a social network, a personal assistant messaging service dedicated to conversations with the automated assistant 120, and so forth. In some implementations, the message exchange client 107 can be implemented via a web page or other resource presented by a web browser (not depicted) or other application of the client computing device 1061-NOne or more of the above.
As described in more detail herein, the automated assistant 120 is via one or more client devices 1061-NTo a userInterface input and output devices to participate in a human-machine conversation session with one or more users. In some implementations, the automated assistant 120 can respond to the request by the user via the client device 1061-NIs engaged in a human-machine conversation session with the user by user interface input provided by one or more user interface input devices of one of the plurality of user interface devices. In some of those implementations, the user interface input is explicitly directed to the automated assistant 120. For example, message exchange client 1071-NCan be a personal assistant messaging service dedicated to talking with the automated assistant 120 and user interface input provided via the personal assistant messaging service can be automatically provided to the automated assistant 120. Additionally, for example, user interface inputs can be explicitly directed to the messaging client 107 based on the particular user interface input indicating that the automated assistant 120 is to be invoked1-N Automated assistant 120 in one or more of. For example, the particular user interface input may be one or more typed characters (e.g., @ Automated Assistant), user interaction with hardware buttons and/or virtual buttons (e.g., tapping, long tapping), spoken commands (e.g., "Hey Automated Assistant"), and/or other particular user interface inputs.
In some implementations, the automated assistant 120 can participate in the conversation session in response to the user interface input even when the user interface input is not explicitly directed to the automated assistant 120. For example, the automated assistant 120 may examine the content of the user interface input and participate in the conversation session in response to the presence of certain terms in the user interface input and/or based on other cues. In many implementations, the automated assistant 120 can conduct an interactive voice response ("IVR") such that the user can speak commands, search, etc., and the automated assistant can utilize natural language processing and/or one or more grammars to convert the utterances to text and respond to the text accordingly. In some implementations, the automated assistant 120 can additionally or alternatively respond to the utterance without converting the utterance to text. For example, the automated assistant 120 can convert the voice input into an embedding, into an entity representation (that indicates one or more entities present in the voice input), and/or other "non-text" representations and operate on such non-text representations. Accordingly, embodiments described herein as operating based on text converted from voice input may additionally and/or alternatively operate directly on voice input and/or other non-text representations of voice input.
Client computing device 1061-NAnd computing devices operating cloud-based automated assistant components 119 may each include one or more memories for storing data and software applications, one or more processors for accessing data and executing applications, and other components that facilitate communication over a network. By the client computing device 1061-NAnd/or the operations performed by the automated assistant 120 may be distributed across multiple computer systems. The automated assistant 120 can be implemented as a computer program running on one or more computers coupled to each other over a network, for example, in one or more locations.
As noted above, in various embodiments, the client computing device 1061-NEach of which may operate the automated assistant client 118. In various implementations, each automated assistant client 118 may include a corresponding speech capture/text-to-speech ("TTS")/STT module 114. In other implementations, one or more aspects of the speech capture/TTS/STT module 114 may be implemented separately from the automated assistant client 118.
Each speech capture/TTS/STT module 114 may be configured to perform one or more functions: capturing the user's voice, for example, via a microphone (which may include presence sensor 105 in some cases); converting the captured audio to text (and/or to other representations or embeddings); and/or converting text to speech. For example, in some implementations, because the client devices 106 may be relatively constrained in terms of computing resources (e.g., processor cycles, memory, battery, etc.), the speech capture/TTS/STT module 114 local to each client device 106 may be configured to convert a limited number of different spoken phrases, particularly phrases invoking the automated assistant 120, into text (or into other forms, such as reduced dimensionality embedding). Other speech inputs may be sent to the cloud-based automated assistant component 119, which may include the cloud-based TTS module 116 and/or the cloud-based STT module 117.
The cloud-based STT module 117 may be configured to utilize the virtually unlimited resources of the cloud to convert audio data captured by the speech capture/TTS/STT module 114 into text (which may then be provided to the natural language processor 122). Cloud-based TTS module 116 may be configured to utilize the virtually unlimited resources of the cloud to convert text data (e.g., natural language responses formulated by automated assistant 120) into computer-generated speech output. In some implementations, the TTS module 116 can provide the computer-generated speech output to the client device 106 for direct output, e.g., using one or more speakers. In other implementations, the text data (e.g., natural language response) generated by the automated assistant 120 can be provided to the speech capture/TTS/STT module 114, which can then convert the text data into computer-generated speech that is output locally.
The automated assistant 120 (and in particular the cloud-based automated assistant component 119) may include a natural language processor 122, the aforementioned TTS module 116, the aforementioned STT module 117, a dialog state tracker 124, a dialog manager 126, and a natural language generator 128 (which may be combined with the TTS module 116 in some implementations). In some implementations, one or more of the engines and/or modules of the automated assistant 120 can be omitted, combined, and/or implemented in a component separate from the automated assistant 120.
In some implementations, the automated assistant 120 responds by the client device 1061-NGenerates responsive content from various inputs generated by a user of one of the automated assistants 120 during the human-machine conversation session with the automated assistant 120. The automated assistant 120 can provide responsive content (e.g., over one or more networks when separate from the user's client device) for presentation to the user as part of a conversation session.For example, the automated assistant 120 may respond via the client device 1061-NThe response content is generated from the free-form natural language input provided by one of the first and second processors. As used herein, free-form natural language input is input that is formulated by a user and is not limited to a set of options presented for selection by the user.
As used herein, a "conversation session" can include a logically self-contained exchange of one or more messages between a user and the automated assistant 120 (and in some cases, other human participants) and/or the performance of one or more responsive actions by the automated assistant 120. The automated assistant 120 can distinguish between multiple conversation sessions with the user based on various signals, such as the passage of time between sessions, a change in user context between sessions (e.g., location, before/during/after scheduling a meeting, etc.), detection of one or more intermediate interactions between the user and the client device rather than a conversation between the user and the automated assistant (e.g., the user temporarily switches applications, the user walks away and then later returns to a standalone voice activated product), locking/hibernation of the client device between sessions, a change in the client device for interfacing with one or more instances of the automated assistant 120, and so forth.
Processing by the natural language processor 122 of the automated assistant 120 via the client device 106 by the user1-NThe generated free-form natural language input, and in some implementations, the annotated output, may be generated for use by one or more other components of the automated assistant 120. For example, the natural language processor 122 may process data received by a user via the client device 1061The one or more user interface input devices of (a) generating a natural language free form input. The generated annotated output includes one or more annotations of the natural language input and optionally one or more (e.g., all) of the items of the natural language input.
In some implementations, the natural language processor 122 is configured to recognize and annotate various types of grammar information in the natural language input. For example, the natural language processor 122 may include a part-of-speech tagger (not depicted) configured to annotate an item with its grammatical role. For example, a part-of-speech tagger may tag each item with a part-of-speech such as "noun", "verb", "adjective", "pronoun", and the like. Additionally, for example, in some implementations the natural language processor 122 may additionally and/or alternatively include a dependency parser (not depicted) configured to determine syntactic relationships between terms in the natural language input. For example, the dependency parser may determine which terms modify other terms, subjects and verbs of the sentence, etc. (e.g., parse trees) — and may annotate such dependencies.
In some implementations, the natural language processor 122 can additionally and/or alternatively include an entity tagger (not depicted) configured to annotate entity references in one or more segments, such as references to people (including, for example, literary characters, celebrities, public characters, etc.), organizations, locations (both real and fictional), and so forth. In some implementations, data about the entities may be stored in one or more databases, such as in a knowledge graph (not depicted). In some implementations, the knowledge graph can include nodes representing known entities (and in some cases, entity attributes) and edges connecting the nodes and representing relationships between the entities. For example, a "banana" node may be connected (e.g., as a child node) to a "fruit" node, which in turn may be connected (e.g., as a child node) to a "product" and/or "food" node. As another example, a restaurant called a "virtual cafe" may be represented by a node that also includes attributes such as its address, the type of food served, business hours, contact information, and the like. A "fictitious cafe" node may in some embodiments be connected by edges (e.g., representing a child-parent relationship) to one or more other nodes, such as a "restaurant" node, a "business" node, a node representing a city and/or state in which the restaurant is located, and so forth.
The entity tagger of the natural language processor 122 may annotate references to entities at a high level of granularity (e.g., to enable identification of all references to a class of entities such as people) and/or at a lower level of granularity (e.g., to enable identification of all references to a particular entity such as a particular person). The entity tagger may rely on the content of the natural language input to resolve a particular entity and/or may optionally communicate with a knowledge graph or other entity database to resolve a particular entity.
In some implementations, the natural language processor 122 can additionally and/or alternatively include a coreference parser (not depicted) configured to group or "cluster" references to the same entity based on one or more contextual cues. For example, a coreference parser can be utilized to parse the term "there" in the natural language input "Iliked Hypersynthetic Caf time we ate there (i like a Hypothetical cafe where we last used a meal)" into "Hypersynthetic cafe".
In some implementations, one or more components of the natural language processor 122 may rely on annotations from one or more other components of the natural language processor 122. For example, in some implementations, a named entity tagger may rely on annotations from co-referent resolvers and/or dependency resolvers in annotating all references to a particular entity. Additionally, for example, in some embodiments a co-referent parser may rely on annotations from dependency parsers when clustering references to the same entity. In some implementations, in processing a particular natural language input, one or more components of the natural language processor 122 may determine one or more annotations using related prior inputs and/or other related data in addition to the particular natural language input.
In the context of task-oriented dialog, the natural language processor 122 may be configured to map free-form natural language input provided by a user at each turn of a dialog session to a semantic representation, which may be referred to herein as "dialog behavior". Semantic representations whether dialog behaviors generated from user input or other semantic representations of automated assistant utterances can take various forms. In some implementations, the semantic representation can be modeled as a discrete semantic framework. In other embodiments, the semantic representation may be formed as a vector embedding, for example in a continuous semantic space.
In some implementations, the conversation behavior (or more generally, the semantic representation) may indicate, among other things, one or more slot/value pairs that correspond to parameters of certain actions or tasks that the user may be attempting to perform via the automated assistant 120. For example, assume that a user provides free-form natural language input in the form: "Suggest an Indian restaurant for diner tonight" suggests eating at an Indian restaurant tonight ". In some implementations, the natural language processor 122 can map the user input to dialog behavior that includes parameters such as, for example: intent (find residual); inform (meal diner, time tonight) (intention (find _ restaurant); notice (food india, dinner, time tonight)). Dialog behavior may occur in various forms, such as "greeting" (e.g., invoking the automated assistant 120), "inform" (e.g., providing parameters for slot population), "intent" (e.g., finding an entity, ordering something), request (e.g., requesting specific information about an entity), "confirm", "affirm", and "thank you" (optionally, the dialog session may be closed and/or used as positive feedback and/or to indicate that a positive bonus value should be provided). These are merely examples and are not intended to be limiting.
The dialog state tracker 124 may be configured to track "dialog states" including, for example, the belief states of the user's goals (or "intentions") during a human-machine dialog session (and/or across multiple dialog sessions). In determining the dialog state, some dialog state trackers may seek to determine the most likely value for a slot instantiated in a dialog based on user and system utterances in the dialog session. Some techniques utilize a fixed ontology that defines a set of slots and the sets of values associated with those slots. Additionally or alternatively, some techniques may be customized for individual slots and/or domains. For example, some techniques may require training the model for each slot bit type in each domain.
The dialog manager 126 may be configured to map the current dialog state, e.g., as provided by the dialog state tracker 124, to one or more "response actions" of a plurality of candidate response actions that are then performed by the automated assistant 120. The response action may occur in various forms depending on the current dialog state. For example, initial and intermediate dialog states corresponding to turns of a dialog session that occurred before the last turn (e.g., when the end user desired task was performed) may be mapped to various response actions including the automated assistant 120 outputting additional natural language dialogs. This response dialog may include, for example, a request by the user to provide parameters for some action (i.e., fill a slot) that the dialog state tracker 124 believes the user intends to perform.
In some implementations, the dialog manager 126 can include a machine learning model such as a neural network. In some such embodiments, the neural network may take the form of, for example, a feed-forward neural network having two hidden layers followed by a softmax layer. However, other configurations of neural networks and other types of machine learning models may be employed. In some embodiments where the dialog manager 126 employs a neural network, inputs to the neural network may include, but are not limited to, user actions, previous response actions (i.e., actions performed by the dialog manager in a previous round), current dialog state (e.g., a binary vector provided by the dialog state tracker 124 indicating which slots have been filled), and/or other values.
In various embodiments, dialog manager 126 may operate at a semantic representation level. For example, the dialog manager 126 may receive new observations in the form of a semantic dialog framework (which may include, for example, dialog behavior provided by the natural language processor 122 and/or dialog state provided by the dialog state tracker 124) and randomly select a response action from a plurality of candidate response actions. The natural language generator 128 may be configured to map the response action selected by the dialog manager 126 to one or more utterances provided as output to the user, for example, at the end of each round of the dialog session.
As noted above, in various embodiments, the user may be able to create a customized "conversation routine" that the automated assistant 120 may later be able to effectively reformulate to accomplish various user-defined or user-selected tasks. In various implementations, the dialog routine may include a mapping between a command (e.g., a free form natural language utterance or reduced dimensionality embedding converted to text, a typed free form natural language input, etc.) and a task to be performed, in whole or in part, by the automated assistant 120 in response to the command. Further, in some cases, a session routine may include one or more user-defined "slots" (also referred to as "parameters" or "attributes") that are required to be filled with values (also referred to herein as "slot value") in order to fulfill a task. In various implementations, the dialog routine, once created, may accept one or more values as input to fill one or more slots. In some implementations, the conversation routine may also include, for one or more slots associated with the conversation routine, one or more user enumerated values that may be used to fill the slots, although this is not required.
In various implementations, tasks associated with the conversation routine may be performed by the automated assistant 120 when one or more necessary slots are populated with values. For example, assume that a user invokes a dialog routine that requires two slots to be filled with values. If during the call, the user provides values for two slots, the automated assistant 120 can use those provided slot bit values to perform tasks associated with the conversation routine without asking the user for additional information. Thus, it is possible that the dialog routine, when called, only involves a single "turn" of the dialog (assuming the user has previously provided all necessary parameters). On the other hand, if the user fails to provide a value for at least one required slot, the automated assistant 120 can automatically provide a natural language output that evaluates to the required, yet unfilled, slot index.
In some implementations, each client device 106 can include a local conversation routine index 113 configured to store one or more conversation routines created by one or more users at the device. In some implementations, each local conversation routine index 113 can store conversation routines created by any user at the corresponding client device 106. Additionally or alternatively, in some implementations, each local conversation routine index 113 can store conversation routines created by a particular user operating the coordinated "ecosystem" of the client device 106. In some cases, each client device 106 of the coordinating ecosystem may store a conversation routine created by the controlling user. For example, assume that a user is at a first client device (e.g., 106) in the form of a stand-alone interactive speaker1) A dialog routine is created. In some implementations, the conversation routine can be propagated to other client devices 106 (e.g., a smartphone, a tablet computer, another speaker, a smart television, a vehicle computing system, etc.) that form part of the same coordinated ecosystem of client devices 106, and stored in local conversation routine indexes 113 of the other client devices 106.
In some implementations, dialog routines created by individual users may be shared among multiple users. To this end, in some implementations, the global conversation routine engine 130 may be configured to store conversation routines created by multiple users in the global conversation routine index 132. In some implementations, the conversation routines stored in the global conversation routine index 132 may be utilized by the selected user based on permissions granted by the creator (e.g., via one or more access control lists). In other implementations, the conversation routines stored in the global conversation routine index 132 may be made available to all users for free. In some implementations, conversation routines created by a particular user at one client device 106 of the coordinated ecosystem of client devices can be stored in the global conversation routine index 132 and thereafter can be utilized (e.g., for optional download or online use) by the particular user at other client devices of the coordinated ecosystem. In some implementations, the global conversation routine engine 130 may be able to access both globally available conversation routines in the global conversation routine index 132 and locally available conversation routines stored in the local conversation routine index 113.
In some implementations, the dialog routine may be limited to being called by its creator. For example, in some implementations, voice recognition techniques may be used to assign newly created conversation routines to the voice profile of their creator. When the conversation routine is later invoked, the automated assistant 120 can compare the speaker's voice to the voice profile associated with the conversation routine. If there is a match, the speaker may be authorized to invoke the dialog routine. If the speaker's voice does not match the voice profile associated with the conversation routine, the speaker may not be allowed to invoke the conversation routine in some cases.
In some implementations, a user can create a custom dialog routine that effectively reloads existing canonical commands and associated tasks. Assume that a user creates a new dialog routine for performing a user-defined task and calls the new dialog routine using canonical commands that were previously mapped to a different task. In the future, when the particular user invokes the dialog routine, the user-defined tasks associated with the dialog routine may be fulfilled, rather than specifying a different task to which the command was previously mapped. In some implementations, the user-defined task may be performed in response to the canonical command only if it is the creator-user that called the conversation routine (e.g., which may be determined by matching the speaker's voice to the voice profile of the creator of the conversation routine). If another user issues or otherwise provides a canonical command, a different task that is traditionally mapped to the canonical command may be performed instead.
Referring again to FIG. 1, in some implementations, the task switcher 134 may be configured to route data generated when a conversation routine is invoked by a user to one or more appropriate remote computing systems/devices, e.g., so that tasks associated with the conversation routine may be fulfilled. Although the task switcher 134 is described separately from the cloud-based automation assistant component 119, this is not intended to be limiting. In various implementations, the task switcher 134 may form an integral part of the automated assistant 120. In some implementations, the data routed by the task switcher 134 to the appropriate remote computing device may include one or more values to be used to populate one or more slots associated with the called conversation routine. Additionally or alternatively, depending on the nature of the remote computing system/device, the data routed by the task switcher 134 may include other information, such as slots to be populated, data indicating invocation commands, data indicating tasks to be performed (e.g., the user's perceived intent), and so forth. In some implementations, once the remote computing systems/devices perform their roles in fulfilling the task, they can return response data to the automated assistant 120 directly and/or via the task switcher 134. In various implementations, the automated assistant 120 may then generate (e.g., by the natural language generator 128) natural language output to provide to the user, for example, via invoking one or more audio and/or visual output devices of the client device 106 operated by the user.
In some implementations, the task switcher 134 can be operatively coupled with a task index 136. The task index 136 may store a plurality of candidate tasks that may be performed in whole or in part by the automated assistant 120. In some implementations, the candidate tasks can include third-party software agents configured to automatically respond to orders, participate in human-machine conversations (e.g., as chat robots), and so forth. In various embodiments, these third party software agents may interact with the user via the automated assistant 120, with the automated assistant 120 acting as an intermediary. In other embodiments, particularly where the third party agent itself is a chat bot, the third party agent may be directly connected to the user, for example, through the automated assistant 120 and/or task switcher 134. Additionally or alternatively, in some implementations, the candidate tasks may include aggregating information provided by the user into a particular form, e.g., by filling a particular slot, and presenting the information (e.g., in a predetermined format) to a third party, such as a human. In some implementations, the candidate tasks may additionally or alternatively include tasks that do not necessarily need to be submitted to a third party, in which case the task switcher 134 may not route information to the remote computing device.
Assume that the user creates a new dialog routine to map the custom command to a task that has not yet been determined. In various implementations, the task switcher 134 (or one or more components of the automated assistant 120) may compare the new conversation routine to a plurality of candidate tasks in the task index 136. For example, one or more user-defined slots associated with a new conversation routine may be compared to slots associated with candidate tasks in the task index 136. Additionally or alternatively, one or more user enumerated values that may be used to fill slots of a new conversation routine may be compared to enumerated values that may be used to fill slots associated with one or more of the plurality of candidate tasks. Additionally or alternatively, other aspects of the new dialog routine (such as the command to be mapped, one or more other trigger words included in the user's call, etc.) may be compared to various attributes of the plurality of candidate tasks. Based on the comparison, a task to which the command is to be mapped may be selected from the plurality of candidate tasks.
Suppose that the user creates a new dialog routine called by the command "I wait to order tacos (I want to order tortillas)". Assume further that this new dialog routine is intended to place a food order to the mexican restaurant to be determined (perhaps the user is relying on the automated assistant 120 to guide the user in making the best choice). The user may define various slots associated with this task, such as shell type (e.g., crisp, soft, flour, corn, etc.), meat selection, type of cheese, type of sauce, topping, etc., for example, by participating in a natural language conversation with the automated assistant 120. In some embodiments, these slots may be compared to slots to be filled of existing third party food ordering applications (i.e., third party agents) to determine which third party agent is most appropriate. There may be a plurality of third party agents configured to receive orders for mexican food. For example, a first software agent may accept an order for a predetermined menu item (e.g., no options for customizing an ingredient). The second software agent may accept a custom tortilla order and may therefore be associated with a slot such as a topping, shell type, or the like. The new tortilla order dialog routine (including its associated slot) may be compared to the first software agent and the second software agent. Because the second software agent has slots that are more closely aligned with those slots defined by the user in the new conversation routine, the second software agent may be selected, for example, by the task switcher 134 for mapping by command "I wait to order tacos" (or a sufficiently grammatical/semantically similar utterance).
When the dialog routine defines one or more slots that require filling in order to complete a task, the user is not required to proactively fill the slots when the dialog routine is initially called. Conversely, in various embodiments, when the user invokes the dialog routine, to the extent that the user does not provide values for the necessary slots during the invocation, the automated assistant 120 may cause (e.g., audible, visual) output to be provided, for example, as natural language output asking the user for such values. For example, in the case of the tortilla order dialog routine above, assume that the user later provides the utterance "I wait to order tags". Because this conversation routine has a slot that is required to be filled, the automated assistant 120 can respond by prompting the user for a value to be filled in any missing slot (e.g., shell type, topping, meat, etc.). On the other hand, in some implementations, the user may proactively fill the slot when the conversation routine is called. Suppose the user utters the phrase "I want to order the rope fish tacos with hard shells (I want to order some fish tortillas with hard shells)". In this example, the slots for the shell type and meat have been filled with the respective values "hard shells" and "fish". Thus, the automated assistant 120 may prompt the user only for any missing slot bit value (such as a topping). Once all necessary slots are filled with values, in some embodiments, the task switcher 134 may take action to cause the task to be performed.
Fig. 2 depicts one example of how the free-form natural language input ("FFNLI" in fig. 2 and elsewhere) provided by a user may be used to invoke a dialog routine and how data aggregated by the automated assistant 120 as part of implementing the dialog routine may be propagated to various components for fulfillment tasks. The user provides (during one or more rounds of a man-machine conversation session) the FFNLI to the automated assistant 120 in typed form or as a spoken utterance. The automated assistant 120 interprets and parses the FFNLI into various semantic information, such as user intent, slot(s) to be filled, value(s) to be used to fill the slot, etc., for example, by a natural language processor 122 (not depicted in fig. 2) and/or a dialog state tracker 124 (also not depicted in fig. 2).
The automated assistant 120, for example through the dialog manager 126 (not depicted in fig. 2), can consult the dialog routine engine 130 to identify dialog routines that include mappings between commands and tasks included in the FFNLI provided by the user. In some implementations, the conversation routine engine 130 can consult one or both of the local conversation routine index 113 or the global conversation routine index 132 of the computing device operated by the user. Once the automated assistant 120 selects a matching conversation routine (e.g., including the most semantically/grammatically similar conversation routine to the commands contained in the user's FFNLI), the automated assistant 120 may prompt the user for values for filling all unfilled and necessary slots for the conversation routine, if necessary.
Once all necessary slots are filled, the automation assistant 120 may provide data to the task switcher 134 indicating values for at least filling the slots. In some cases, the data may also identify the slot itself and/or one or more tasks that are mapped to the user's commands. The task switcher 134 may then select what will be referred to herein as a "service" for facilitating performance of the task. For example, in FIG. 2, the services include a public switched telephone network ("PSTN") service 240, a service 242 for handling SMS and MMS messages, an email service 244, and one or more third party software agents 246. As indicated by the ellipses, any other number of additional services may or may not be utilized by the task switcher 134. These services may be used to route data indicative of a called conversation routine or simply a "task request" to one or more remote computing devices.
For example, the PSTN service 240 may be configured to receive data indicative of the called conversation routine (including a value to fill any necessary slots) and provide the data to the third party client device 248. In this scenario, the third party client device 248 may take the form of a computing device configured to receive telephone calls, such as a cellular telephone, a conventional telephone, a voice over IP ("VOIP") telephone, a computing device configured to make/receive telephone calls, and so forth. In some implementations, the information provided to such third party client devices 248 may include natural language output, for example, generated by the automated assistant 120 (e.g., by the natural language generator 128) and/or by the PSTN service 240. This natural language output may include, for example, a computer-generated utterance conveying a task to be performed and parameters associated with the task (i.e., values of the necessary slots) and/or enable the recipient to engage in limited conversations designed to enable performance of the user's task (e.g., much like a nuisance call). This natural language output may be presented, for example, by the third party computing device 248 as a human-perceptible output 250, e.g., audibly, visually, as haptic feedback, and so forth.
Assume that a conversation routine is created to place a pizza order. Assume further that the task identified for the conversation routine (e.g., by the user or by the task switcher 134) is to provide the user's pizza order to a particular pizza shop that lacks its own third party software agent. In some such implementations, in response to the invocation of the dialog routine, the PSTN service 240 may place a telephone call to a telephone at a particular pizza shop. When an employee at a particular pizza store answers the call, the PSTN service 240 may initiate an automated (e.g., IVR) conversation informing the pizza store that the employee user wishes to order a pizza with a crust type and toppings specified by the user when the user invokes the conversation routine. In some implementations, the pizza shop employee may be asked to confirm that the pizza shop will fulfill the user's order, e.g., by pressing "1", providing verbal confirmation, etc. Upon receiving this confirmation, it may be provided, for example, to the PSTN service 240, which PSTN service 240 may in turn forward the confirmation information (e.g., via the task switcher 134) to the automated assistant 120, which automated assistant 120 may then notify the user that the pizza is on the way (e.g., using an audible and/or visual natural language output such as "your pizza is on the way"). In some implementations, a pizza shop employee may be able to request additional information that the user may not have specified at the time the conversation routine was invoked (e.g., a slot that was not specified during creation of the conversation routine).
SMS/MMS service 242 may be used in a similar manner. In various implementations, the SMS/MMS service 242 may be provided with data indicative of the called conversation routine, such as one or more slots/values, for example, by the task switcher 134. Based on this data, SMS/MMS service 242 may generate and send text messages in various formats (e.g., SMS, MMS, etc.) to third party client device 248, which again may be a smartphone or another similar device 248. A person (e.g., a pizza store employee) operating the third party client device 248 may then use the text message (e.g., speak it, have it read aloud, etc.) as a human perceptible output 250. In some embodiments, the text message may ask the person to provide a response, such as "REPLY '1' IF YOU CAN fill the order," REPLY "1" IF YOU CAN fill the order, "REPLY" 2 "IF YOU CAN not fill the order," REPLY "2". In this manner, similar to the example described above in the context of the PTSN service 240, a first user invoking a conversation routine may exchange data asynchronously with a second user operating a third party device 248, so that the second user may help fulfill tasks associated with the invoked conversation routine. Email service 244 may operate similarly to SMS/MMS service 242, except that email service 244 utilizes an email-related communication protocol (such as IMAP, POP, SMTP, etc.) to generate and/or exchange email with third party computing device 248.
The services 240 and 244 and task switcher 134 enable a user to create a dialog routine to interface with a third party while reducing the requirements of the third party to implement complex software services with which to interact. However, at least some third parties may prefer to build third party software agents 246 and/or have the ability to build third party software agents 246, which third party software agents 246 are configured to interact with remote users automatically (e.g., through automated assistants 120 involved by those remote users). Thus, in various embodiments, one or more third party software agents 246 may be configured to interact with the automation assistant 120 and/or task switcher 134, enabling a user to create conversation routines that may be matched against these third party agents 246.
Assume that a user creates a conversation routine that matches a particular third party agent 246 (as described above) based on slots, enumerated potential slot values, other information, and the like. When called, the conversation routine may cause the automated assistant 120 to send data indicative of the conversation routine (including the slot bit value provided by the user) to the task switch 134. The task switcher 134 may in turn provide this data to the matching third party software agent 246. In some embodiments, the third party software agent 246 may perform tasks associated with the conversation routine and return results (e.g., success/failure messages, natural language output, etc.) to the task switcher 134, for example.
As indicated by the arrow from the third party agent 246 directly to the automated assistant 120, in some embodiments, the third party software agent 246 may interface directly with the automated assistant 120. For example, in some implementations, the third-party software agent 246 can provide data (e.g., state data) to the automated assistant 120 that enables the automated assistant 120 to generate natural language output, e.g., by the natural language generator 128, which is then presented, e.g., as audible and/or visual output, to a user that invoked the conversation routine. Additionally or alternatively, the third-party software agent 246 can generate its own natural language output, which is then provided to the automated assistant 120, which automated assistant 120 in turn outputs the natural language output to the user.
The above examples are not intended to be limiting, as indicated by the other of the various arrows in fig. 2. For example, in some implementations, the task switcher 134 may provide data indicative of the invoked conversation routine to one or more services 240 and 244, and these services may in turn provide this data (or modified data) to one or more third party software agents 246. Some of these third-party software agents 246 may be configured to receive, for example, text messages or emails, and automatically generate responses that may be returned to the task switcher 134 and on to the automated assistant 120.
The conversation routine configured in accordance with selected aspects of the present disclosure is not limited to tasks performed/fulfilled remotely from the client device 106. Conversely, in some implementations, the user may engage the automated assistant 120 to create a dialog routine that performs various tasks locally. As a non-limiting example, a user can create a dialog routine that configures multiple settings of a mobile device, such as a smartphone, at once using a single command. For example, a user can create a dialog routine that receives all Wi-Fi settings, bluetooth settings, and hotspot settings as inputs at once and changes these settings accordingly. As another example, a user can create a dialog routine that is called if the user says "I'm gonna be late (I am going late)". The user may indicate that the automated assistant 120 this command should cause the automated assistant 120 to notify another person (such as the user's spouse) that the user will arrive at a certain destination late, for example, using a text message, email, etc. In some cases, the slot for such a conversation routine may include a predicted time that the user will reach the user's predetermined destination, which may be populated by the user or automatically predicted, e.g., by the automated assistant 120, based on location coordinate data, calendar data, or the like.
In some implementations, the user may be able to configure the dialog routine to use preselected slot location values in particular slots, so that the user need not provide these slot location values, and for those values will not be prompted when the user does not provide them. Assume that the user creates a pizza order dialog routine. Further assume that the user always prefers a thin skin. In various embodiments, unless otherwise specified by the user, the user may indicate that the automated assistant 120 should automatically fill the slot "crust type" with the default "thin crust" when this particular dialog routine is called. In this way, if the user occasionally wants to order different skin types (e.g., the user has a visitor who prefers a thick skin), the user may invoke a dialog routine as usual, except that the user may specifically request a different type of skin (e.g., "Hey assistant, order me a hand-ordered pizza"). If the user simply says "Hey assistant, order me a pizza," the automated assistant 120 may have assumed a thin skin and prompted the user for other necessary slot values. In some implementations, the automated assistant 120 can "learn" over time which slot-level values are preferred by the user. Later, when the user invokes the conversation routine without explicitly providing those learned slot values, for example, if the user has provided those slot values more than a predetermined number of times or more than a certain threshold frequency of invoking the conversation routine, the automated assistant 120 may assume those values (or ask the user to confirm those slot values).
Fig. 3 depicts an example process flow that may occur when a user invokes a pizza order dialog routine in accordance with various implementations. At 301, the user invokes a pizza Order dialog routine by, for example, issuing a call phrase "Order a thin crust pizza" to the automated assistant client 118. At 302, the automated assistant client 118 provides the invocation phrase to the cloud-based automated assistant component ("CBAAC") 119, e.g., as a record, transcribed text segmentation, reduced dimensionality embedding, etc. At 303, various components of CBAAC 119 (such as natural language processor 122, dialog state tracker 124, dialog manager 126, etc.) may use various cues (such as dialog context, verb/noun dictionary, canonical utterances, synonym dictionary (e.g., thesaurus), etc.) to process the request as described above to extract information such as objects of "pizza" and attributes of "thin crust" (or "slot values").
At 304, this extracted data may be provided to the task switcher 134. In some implementations, at 305, the task switcher 134 can consult the conversation routine engine 130 to identify a conversation routine that matches the user's request, e.g., based on the data extracted at 303 and received at 304. As shown in fig. 3, the dialog routine identified in this example includes an action of "order" (which may itself be a slot), an object of "pizza" (which may also be a slot in some cases), an attribute of "crust" (or "slot") (which is required), another attribute of "topping" (or slot) (which is also required), and a so-called "implementer" of "order _ service". The "implementer" may be, for example, any of the services 240 and/or one or more third party software agents 246 of FIG. 2, depending on how the user created the conversation routine and/or whether the conversation routine matches a particular task (e.g., a particular third party software agent 246).
At 306, it may be determined, for example, by the task switcher 134 that one or more necessary slots for the conversation routine have not been filled with a value. Thus, the task switcher 134 can notify a component such as the automation assistant 120 (e.g., the automation assistant client 118 in fig. 3, but it may be another component such as one or more CBAAC 119) that one or more slots remain to be populated with slot bit values. In some implementations, the task switcher 134 can generate the necessary natural language output (e.g., "what topping") to prompt the user for these unfilled slots, and the automated assistant client 118 can simply provide this natural language output to the user, e.g., at 307. In other implementations, the data provided to the automated assistant client 118 can provide notification of missing information, and the automated assistant client 118 can interface with one or more components of the CBAAC 119 to generate a natural language output that is presented to the user to prompt the user for a missing slot value.
Although not shown in FIG. 3 for brevity and completeness, the user-provided slot value may be returned to the task switcher 134. The task switcher 134 may then be able to formulate a complete task, at 308, with all necessary slots filled with the user-provided slot values. This complete task may be provided, for example, by the task switcher 134 to the appropriate implementers 350, which, as noted above, may be one or more services 240, 244, one or more third party software agents 246, and so forth.
Fig. 4 is a flow chart illustrating an example method 400 in accordance with embodiments disclosed herein. For convenience, the operations of the flow diagrams are described with reference to a system that performs the operations. This system may include various components of various computer systems, such as one or more components of a computing system implementing the automated assistant 120. Further, while the operations of method 400 are shown in a particular order, this is not intended to be limiting. One or more operations may be reordered, omitted, or added.
At block 402, the system may receive a first free-form natural language input from a user, for example, at one or more input components of the client device 106. In various implementations, the first free-form natural language input may include commands for performing a task. As a working example, assume that the user provides the spoken utterance "I want a pizza".
At block 404, the system may perform semantic processing on the free-form natural language input. For example, one or more CBAAC 119 can compare the user's utterance (or its reduced dimensionality embedding) to one or more canonical commands, various dictionaries, and the like. The natural language processor 122 may perform various aspects of the analysis described above to identify entities, perform reference resolution, tag part of speech, and so forth. At step 406, the system may determine that the automated assistant 120 is unable to interpret the command based on the semantic processing of block 404. In some implementations, at block 408, the system can provide an output at one or more output components of the client device 106 that solicits clarification from the user regarding the command, such as outputting a natural language output: "I don't know how to order pizza".
At block 410, the system may receive a second free-form natural language input from the user at one or more of the input components. In various implementations, the second free-form natural language input may identify one or more slots that are required to be filled with values to fulfill the task. For example, a user may provide natural language input such as "to order a pizza, you need to know the type of crust and a list of toppings" for ordering a pizza. This particular free-form natural language input identifies two slots: a list of skin types and toppings (which may be technically any number of slots depending on how many toppings the user desires).
As alluded to above, in some implementations, a user may be able to enumerate a list of potential or candidate slot bit values for a given slot of a conversation routine. In some implementations, this may actually limit the slot to one or more values from the enumerated list. In some cases, enumerating possible values for a slot may enable the automated assistant 120 to determine which slot is to be filled with a particular value and/or to determine that the provided slot value is invalid. For example, assume that a user calls a dialog routine by the phrase "order me a pizza with thick crust, tomato, and tires (order me a pizza with thick crust, tomato, and tires)". The automated assistant 120 can match "thick crust" with the slot "crust type" based on "thick crust" being one of an enumerated list of potential values. The same applies to "tomato" and slot "tops". However, because "tires" are not likely to be in the enumerated list of potential topping, the automated assistant 120 may require the user to make corrections to the specified topping tires. In other implementations, the user-provided enumeration list may simply include non-limiting slot values that may be used by the automated assistant 120 (e.g., as suggestions to be provided to the user during future invocations of the conversation routine). This may be beneficial in contexts such as pizza ordering where the list of possible pizza toppings may be large, and may vary widely across pizza companies and/or over time (e.g., a pizza shop may provide different toppings at different times of the year depending on the season product).
Continuing with the working example, the automated assistant 120 may ask a query such as "what is the possible pizza crust type? "or" what is the possible topping? "such a problem. The user may respond to each such question by providing an enumeration list of possibilities and indicating whether the enumeration list is intended to be constraining (i.e., slot bit values outside of those enumeration lists are not allowed) or simply exemplary. In some cases, the user may not be limited to a particular value in response to a given slot, such that the automated assistant 120 is unconstrained and the slot may be populated with any slot bit value provided by the user.
Returning to FIG. 4, once the user has completed defining any necessary/optional slots and/or enumerating a list of potential slot values, at block 412, the system (e.g., the conversation routine engine 130) may store a conversation routine that includes a mapping between commands and tasks provided by the user. The created conversation routine may be configured to accept as input one or more values for populating the one or more slots and cause tasks associated with the conversation routine to be performed, for example, at a remote computing device as previously described. The dialog routines may be stored in various formats, and it is not critical in the context of this disclosure which format is used.
In some implementations, the various operations of fig. 4, such as operations 402 and 408, can be omitted, particularly where the user explicitly requests that the automated assistant 120 generate a dialog routine rather than where the automated assistant 120 first fails to interpret what the user said. For example, the user can simply speak a phrase to the automated assistant 120 to trigger the creation of a dialog routine, such as: "Hey Assistant, I want to reach you a new try (hi assistant, I want to teach you a new game)" or something that is to do. This may trigger portions of method 400 that begin, for example, at block 410. Of course, many users may not know that the automated assistant 120 is able to learn the conversation routine. Thus, it may be beneficial for the automated assistant 120 to guide the user through the process as described above with respect to block 402-.
At some later time, at block 414, the system may receive subsequent free-form natural language input from the user at one or more input components of the same client device 106 or a different client device 106 (e.g., another client device of the same coordinated ecosystem of client devices). Subsequent free-form natural language input may include a command or some grammatical and/or semantic change thereof, which may invoke a dialog routine based on the mapping stored at block 412.
At block 416, the system may identify one or more values to be used to fill one or more slots that are required to be filled with values in order to fulfill a task associated with the conversation routine based on subsequent free form natural language input or additional free form natural language input (e.g., solicited from a user that fails to provide one or more required slot bit values at the time of invocation of the dialog box). For example, if the user simply calls a dialog routine without providing a value for any necessary slots, the automated assistant 120 may ask the user for slot values, e.g., one at a time, in batches, etc.
In some implementations, at block 418, the system, e.g., via one or more of the task switcher 134 and/or the service 240 and 244, can send data indicating at least one or more values to be used to fill the one or more slots, e.g., to a remote computing device such as a third party client device 248 and/or to a third party software agent 246. In various implementations, the sending may cause the remote computing device to perform the task. For example, if the remote computing device operates the third-party software agent 246, receiving data, for example, from the task switcher 134 may trigger the third-party software agent 246 to fulfill a task using a user-provided slot value.
The techniques described herein may be used to effectively "glue together" tasks that may be performed by a variety of different third-party software applications (e.g., third-party software agents). Indeed, it is entirely possible to create a single conversation routine that has multiple tasks performed by multiple parties. For example, the user can create a dialogue routine called by a phrase such as "Hey Assistant, I wait to take my wife to diner and a movie (hi Assistant, I want to take my wife to eat dinner and watch a movie)". A user may define slots associated with multiple tasks (such as making dinner reservations and purchasing movie tickets) in a single conversation routine. Slots for dinner reservations may include, for example, restaurants (assuming the user has selected a particular restaurant), food types (if the user has not selected a restaurant), price ranges, time ranges, review ranges (e.g., over three stars), and so forth. The slots for purchasing movie tickets may include, for example, movies, theaters, time ranges, price ranges, and the like. Later, when the user invokes this "diner and a movie" reservation, the automated assistant 120 may ask the user for such values to the extent that the user does not proactively provide slot values to fill individual slots. Once the automated assistant has slot values for all necessary slots for each task of the conversation routine, the automated assistant 120 can send data to the various remote computing devices as previously described to cause each task to be fulfilled. In some implementations, the automated assistant 120 can post to the user which tasks are fulfilled and which are still pending. In some implementations, the automated assistant 120 can notify the user when all tasks are fulfilled (or in the event that one or more of the tasks cannot be fulfilled).
In some cases (whether or not multiple tasks are glued together in a single talk reservation), the automated assistant 120 may prompt the user for a particular slot value by first searching for potential slot values (e.g., movies in a movie theater, show times, available dinner reservations, etc.) and then presenting these potential slot values to the user (e.g., as suggestions or as enumerated listings of possibilities). In some implementations, the automated assistant 120 can leverage various aspects of the user, such as the user's preferences, past user activities, and so forth, to narrow down such lists. For example, if a user (and/or the user's spouse) prefers a particular type of movie (e.g., commentary high, comedy, horror, action, drama, etc.), the automated assistant 120 may narrow the list of slot values before presenting them to the user.
The automated assistant 120 may take various approaches regarding payments that may be necessary to fulfill a particular task (e.g., order a product, make a reservation, etc.). In some implementations, the automated assistant 120 can access payment information (e.g., one or more credit cards) that the automated assistant 120 can provide, for example, to the user that the third-party software agent 246 provides, as necessary. In some implementations, when the user creates a dialog routine to fulfill a task requiring payment, the automated assistant 120 can prompt the user for payment information and/or for permission to use the payment information already associated with the user's profile. In some embodiments, data indicative of the called conversation routine (including one or more slot values) is provided to a third party computing device (e.g., 248) to be output as natural language output, the user's payment information may or may not be provided. In case payment information of the user is not provided, for example, when ordering food, the food provider may simply request payment from the user when shipping the food to the user's home.
In some implementations, the automated assistant 120 can "learn" a new conversation routine by analyzing user engagement with one or more applications running on one or more client computing devices to detect patterns. In various implementations, the automated assistant 120 can provide natural language output to the user, for example, proactively during an existing human-machine conversation or as another type of notification (e.g., pop-up cards, text messages, etc.) asking the user whether they would like to assign a sequence of commonly performed actions/tasks to spoken commands, in effect building and recommending a conversation routine without the user explicitly asking for the conversation routine.
As an example, assume that a user repeatedly visits a single food ordering website (e.g., associated with a restaurant), views a web page associated with a menu, and then opens a separate phone application where the user operates to place a call to or from a phone number associated with the same food ordering website. The automated assistant 120 can detect this pattern and generate a dialog routine for recommendation to the user. In some implementations, the automated assistant 120 can crawl a menu web page for potential slots and/or potential slot values that can be incorporated into the conversation routine and map one or more commands (which the automated assistant 120 can suggest or can be provided by the user) to the food ordering task. In this case, the food ordering task may include calling a telephone number and outputting a natural language message (e.g., a nuisance call) to an employee of the food ordering website as described above with respect to the PSTN 240.
Other sequences of actions for ordering food (or generally performing other tasks) can also be detected. For example, assume that a user typically opens a third party client application to order food, and that the third party client application is a GUI-based application. The automated assistant 120 can detect this and determine, for example, that the third-party client application is interfacing with a third-party software agent (e.g., 246). In addition to interacting with the third party client application, this third party software agent 246 may also have been configured to be interactable with an automated assistant. In such a scenario, the automated assistant 120 can generate a dialog routine to interact with the third-party software agent 246. Alternatively, assume that the third party software agent 246 is not currently able to interact with the automated assistant. In some implementations, the automated assistant can determine what information the third party client application provides for each order and can use that information to generate a slot for the conversation routine. When the user later invokes the dialog routine, the automated assistant 120 may fill the necessary slots and then generate data that is compatible with the third-party software agent 246 based on the slot/slot value.
Fig. 5 is a block diagram of an example computing device 510 that may optionally be utilized to perform one or more aspects of the techniques described herein. In some implementations, one or more of the client computing devices and/or other components may include one or more components of the example computing device 510.
The user interface input devices 522 may include a keyboard, a pointing device such as a mouse, trackball, touchpad, or graphics tablet, a scanner, a touch screen incorporated into a display, an audio input device such as a voice recognition system, a microphone, and/or other types of input devices. In general, use of the term "input device" is intended to include all possible types of devices and ways to input information into computing device 510 or onto a communication network.
User interface output devices 520 may include a display subsystem, a printer, a facsimile machine, or a non-visual display such as an audio output device. The display subsystem may include a Cathode Ray Tube (CRT), a flat panel device such as a Liquid Crystal Display (LCD), a projection device, or some other mechanism for creating a visible image. The display subsystem may also provide non-visual displays, for example, via an audio output device. In general, use of the term "output device" is intended to include all possible types of devices and ways to output information from computing device 510 to a user or to another machine or computing device.
These software modules are typically executed by processor 514 alone or in combination with other processors. Memory 525 used in storage subsystem 524 may include a number of memories including a main Random Access Memory (RAM)530 for storing instructions and data during program execution and a Read Only Memory (ROM)532 in which fixed instructions are stored. File storage subsystem 526 may provide persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive along with associated removable media, a CD-ROM drive, an optical drive, or removable media cartridges. Modules implementing the functionality of certain embodiments may be stored by file storage subsystem 526 in storage subsystem 524, or in other machines accessible by processor 514.
In situations where certain embodiments discussed herein may collect or use personal information about a user (e.g., user data extracted from other electronic communications, information about the user's social network, the user's location, the user's time, the user's biometric information, as well as the user's activities and demographic information, relationships between users, etc.), the user is provided with one or more opportunities to control whether to collect information, whether to store personal information, whether to use personal information, and how to collect, store, and use information about the user. That is, the systems and methods discussed herein collect, store, and/or use user personal information only upon receiving explicit authorization from the relevant user to do so.
For example, a user is provided with control over whether a program or feature collects user information about a particular user or other users related to the program or feature. Each user who is to collect personal information is presented with one or more options to allow control of information collection associated with that user to provide permission or authorization as to whether information is collected and as to which portions of the information are to be collected. For example, one or more of such control options may be provided to the user over a communications network. In addition, certain data may be processed in one or more ways before it is stored or used, so that personally identifiable information is removed. As one example, the identification of the user may be processed such that personally identifiable information cannot be determined. As another example, the geographic location of the user may be generalized to a larger area such that a particular location of the user cannot be determined.
While several embodiments have been described and illustrated herein, various other means and/or structures for performing the function and/or obtaining the result and/or one or more of the advantages described herein may be utilized and each of such variations and/or modifications is deemed to be within the scope of the embodiments described herein. More generally, all parameters, dimensions, materials, and configurations described herein are intended to be exemplary, and the actual parameters, dimensions, materials, and/or configurations will depend upon the specific application or applications for which the teachings are used. Those skilled in the art will recognize, or be able to ascertain using no more than routine experimentation, many equivalents to the specific embodiments described herein. It is, therefore, to be understood that the foregoing embodiments are presented by way of example only and that, within the scope of the appended claims and equivalents thereto, the embodiments may be practiced otherwise than as specifically described and claimed. Embodiments of the present disclosure are directed to each individual feature, system, article, material, kit, and/or method described herein. In addition, any combination of two or more such features, systems, articles, materials, kits, and/or methods, if such features, systems, articles, materials, kits, and/or methods are not mutually inconsistent, is included within the scope of the present disclosure.
Claims (20)
1. A method implemented by one or more processors, comprising:
receiving, at one or more input components of a computing device, one or more speech inputs from a user, the one or more speech inputs directed to an automated assistant executed by one or more of the processors, wherein the one or more speech inputs define a custom voice command and identify a task to be performed in response to the automated assistant receiving the custom voice command and one or more slots required to be populated with values in order to fulfill the task;
identifying the custom voice command, the task, and the one or more slots based on a speech recognition output generated from the one or more speech inputs; and
creating and storing a custom dialog routine that includes a mapping between the custom voice command and the task, and that accepts as input one or more values for populating the one or more slots, wherein a subsequent utterance of the custom command causes the automated assistant to participate in the custom dialog routine.
2. The method of claim 1, further comprising:
receiving, by the automated assistant, a subsequent speech input, wherein the subsequent speech input comprises the custom command;
identifying the custom command based on a subsequent speech recognition output generated from the subsequent speech input; and
engaging the automated assistant in the custom dialog routine based on recognizing the custom command in the subsequent speech input and based on the mapping.
3. The method of claim 2, further comprising: identifying the one or more values to be used to fill the one or more slots required to be filled with values to fulfill the task based on additional subsequent speech recognition output generated from the subsequent speech input or additional subsequent speech inputs.
4. The method of claim 3, wherein the task to which the command is mapped comprises a third party proxy task, and further comprising: transmitting, to a remote computing device, data indicating at least the one or more values to be used to populate the one or more slots, wherein the transmitting causes a third party agent executing on the remote computing device to fulfill the third party agent task.
5. The method of claim 2, further comprising: generating a natural language output as part of the automated assistant participating in the custom conversation routine, the natural language output requesting that the task be performed based on one or more values provided to fill one or more of the slots.
6. The method of claim 5, further comprising: the natural language output is sent to a remote computing device as part of a simple message service ("SMS") text message.
7. The method of claim 5, further comprising: transmitting the natural language output to a remote device to cause the remote device to audibly output the natural language output.
8. The method of claim 1, wherein the one or more voice inputs comprise a slot fill voice input from the user, wherein the slot fill voice input comprises an enumerated list of user-provided possible values to fill one or more of the slots.
9. A system comprising one or more processors and memory storing instructions that, in response to execution of the instructions by the one or more processors, cause the one or more processors to:
receiving, at one or more input components of a computing device, one or more speech inputs from a user, the one or more speech inputs directed to an automated assistant executed by one or more of the processors, wherein the one or more speech inputs define a custom voice command and identify a task to be performed in response to the automated assistant receiving the custom voice command and one or more slots required to be populated with values in order to fulfill the task;
identifying the custom voice command, the task, and the one or more slots based on a speech recognition output generated from the one or more speech inputs; and
creating and storing a custom dialog routine that includes a mapping between the custom voice command and the task, and that accepts as input one or more values for populating the one or more slots, wherein a subsequent utterance of the custom command causes the automated assistant to participate in the custom dialog routine.
10. The system of claim 9, further comprising instructions to:
receiving, by the automated assistant, a subsequent speech input, wherein the subsequent speech input comprises the custom command;
identifying the custom command based on a subsequent speech recognition output generated from the subsequent speech input; and
engaging the automated assistant in the custom dialog routine based on recognizing the custom command in the subsequent speech input and based on the mapping.
11. The system of claim 10, further comprising instructions to: identifying the one or more values to be used to fill the one or more slots required to be filled with values to fulfill the task based on additional subsequent speech recognition output generated from the subsequent speech input or additional subsequent speech inputs.
12. The system of claim 11, wherein the task to which the command is mapped comprises a third party proxy task, and further comprising instructions to: transmitting, to a remote computing device, data indicating at least the one or more values to be used to populate the one or more slots, wherein the transmitting causes a third party agent executing on the remote computing device to fulfill the third party agent task.
13. The system of claim 10, further comprising instructions to: generating a natural language output as part of the automated assistant participating in the custom conversation routine, the natural language output requesting that the task be performed based on one or more values provided to fill one or more of the slots.
14. The system of claim 13, further comprising instructions to: the natural language output is sent to a remote computing device as part of a simple message service ("SMS") text message.
15. The system of claim 13, further comprising instructions to: transmitting the natural language output to a remote device to cause the remote device to audibly output the natural language output.
16. The system of claim 9, wherein the one or more voice inputs comprise a slot fill voice input from the user, wherein the slot fill voice input comprises an enumerated list of user-provided possible values to fill one or more of the slots.
17. At least one non-transitory computer-readable medium comprising instructions that, in response to execution of the instructions by one or more processors, cause the one or more processors to:
receiving, at one or more input components of a computing device, one or more speech inputs from a user, the one or more speech inputs directed to an automated assistant executed by one or more of the processors, wherein the one or more speech inputs define a custom voice command and identify a task to be performed in response to the automated assistant receiving the custom voice command and one or more slots required to be populated with values in order to fulfill the task;
identifying the custom voice command, the task, and the one or more slots based on a speech recognition output generated from the one or more speech inputs; and
creating and storing a custom dialog routine that includes a mapping between the custom voice command and the task, and that accepts as input one or more values for populating the one or more slots, wherein a subsequent utterance of the custom command causes the automated assistant to participate in the custom dialog routine.
18. The at least one non-transitory computer-readable medium of claim 17, further comprising instructions to:
receiving, by the automated assistant, a subsequent speech input, wherein the subsequent speech input comprises the custom command;
identifying the custom command based on a subsequent speech recognition output generated from the subsequent speech input; and
engaging the automated assistant in the custom dialog routine based on recognizing the custom command in the subsequent speech input and based on the mapping.
19. The at least one non-transitory computer-readable medium of claim 18, further comprising instructions to: identifying the one or more values to be used to fill the one or more slots required to be filled with values to fulfill the task based on additional subsequent speech recognition output generated from the subsequent speech input or additional subsequent speech inputs.
20. The at least one non-transitory computer-readable medium of claim 19, wherein the task to which the command is mapped comprises a third-party proxy task, and further comprising instructions to: transmitting, to a remote computing device, data indicating at least the one or more values to be used to populate the one or more slots, wherein the transmitting causes a third party agent executing on the remote computing device to fulfill the third party agent task.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/724,217 US10431219B2 (en) | 2017-10-03 | 2017-10-03 | User-programmable automated assistant |
US15/724,217 | 2017-10-03 | ||
CN201880039314.9A CN110785763B (en) | 2017-10-03 | 2018-10-02 | Automated assistant-implemented method and related storage medium |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201880039314.9A Division CN110785763B (en) | 2017-10-03 | 2018-10-02 | Automated assistant-implemented method and related storage medium |
Publications (1)
Publication Number | Publication Date |
---|---|
CN112801626A true CN112801626A (en) | 2021-05-14 |
Family
ID=63998775
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202110154214.4A Pending CN112801626A (en) | 2017-10-03 | 2018-10-02 | Automated assistant-implemented method and related storage medium |
CN201880039314.9A Active CN110785763B (en) | 2017-10-03 | 2018-10-02 | Automated assistant-implemented method and related storage medium |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201880039314.9A Active CN110785763B (en) | 2017-10-03 | 2018-10-02 | Automated assistant-implemented method and related storage medium |
Country Status (6)
Country | Link |
---|---|
US (3) | US10431219B2 (en) |
EP (2) | EP3692455A1 (en) |
JP (3) | JP6888125B2 (en) |
KR (3) | KR102625761B1 (en) |
CN (2) | CN112801626A (en) |
WO (1) | WO2019070684A1 (en) |
Families Citing this family (43)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10431219B2 (en) * | 2017-10-03 | 2019-10-01 | Google Llc | User-programmable automated assistant |
US10991369B1 (en) * | 2018-01-31 | 2021-04-27 | Progress Software Corporation | Cognitive flow |
US10713441B2 (en) * | 2018-03-23 | 2020-07-14 | Servicenow, Inc. | Hybrid learning system for natural language intent extraction from a dialog utterance |
US10777203B1 (en) * | 2018-03-23 | 2020-09-15 | Amazon Technologies, Inc. | Speech interface device with caching component |
US11100146B1 (en) * | 2018-03-23 | 2021-08-24 | Amazon Technologies, Inc. | System management using natural language statements |
US20190348033A1 (en) * | 2018-05-10 | 2019-11-14 | Fujitsu Limited | Generating a command for a voice assistant using vocal input |
US10832010B2 (en) * | 2018-06-05 | 2020-11-10 | International Business Machines Corporation | Training of conversational agent using natural language |
US10567334B1 (en) * | 2018-06-28 | 2020-02-18 | Amazon Technologies, Inc. | Domain mapping for privacy preservation |
CN110811115A (en) * | 2018-08-13 | 2020-02-21 | 丽宝大数据股份有限公司 | Electronic cosmetic mirror device and script operation method thereof |
US11308281B1 (en) * | 2018-11-08 | 2022-04-19 | Amazon Technologies, Inc. | Slot type resolution process |
US11281857B1 (en) * | 2018-11-08 | 2022-03-22 | Amazon Technologies, Inc. | Composite slot type resolution |
US11138374B1 (en) * | 2018-11-08 | 2021-10-05 | Amazon Technologies, Inc. | Slot type authoring |
US11232784B1 (en) | 2019-05-29 | 2022-01-25 | Amazon Technologies, Inc. | Natural language dialog scoring |
US11238241B1 (en) * | 2019-05-29 | 2022-02-01 | Amazon Technologies, Inc. | Natural language dialog scoring |
US11475883B1 (en) | 2019-05-29 | 2022-10-18 | Amazon Technologies, Inc. | Natural language dialog scoring |
US11227592B1 (en) * | 2019-06-27 | 2022-01-18 | Amazon Technologies, Inc. | Contextual content for voice user interfaces |
CN110223695B (en) * | 2019-06-27 | 2021-08-27 | 维沃移动通信有限公司 | Task creation method and mobile terminal |
US20210004246A1 (en) * | 2019-07-01 | 2021-01-07 | International Business Machines Corporation | Automated cognitive analysis and specialized skills capture |
US11487945B2 (en) | 2019-07-02 | 2022-11-01 | Servicenow, Inc. | Predictive similarity scoring subsystem in a natural language understanding (NLU) framework |
CN110413756B (en) * | 2019-07-29 | 2022-02-15 | 北京小米智能科技有限公司 | Method, device and equipment for processing natural language |
KR20210026962A (en) * | 2019-09-02 | 2021-03-10 | 삼성전자주식회사 | Apparatus and method for providing voice assistant service |
US11289086B2 (en) * | 2019-11-01 | 2022-03-29 | Microsoft Technology Licensing, Llc | Selective response rendering for virtual assistants |
US11481417B2 (en) | 2019-11-06 | 2022-10-25 | Servicenow, Inc. | Generation and utilization of vector indexes for data processing systems and methods |
US11455357B2 (en) | 2019-11-06 | 2022-09-27 | Servicenow, Inc. | Data processing systems and methods |
US11468238B2 (en) | 2019-11-06 | 2022-10-11 | ServiceNow Inc. | Data processing systems and methods |
JP2021081896A (en) * | 2019-11-18 | 2021-05-27 | ウイングアーク１ｓｔ株式会社 | Chat system, chat control device and program for chat |
US11762937B2 (en) * | 2019-11-29 | 2023-09-19 | Ricoh Company, Ltd. | Information processing apparatus, information processing system, and method of processing information |
US11676586B2 (en) * | 2019-12-10 | 2023-06-13 | Rovi Guides, Inc. | Systems and methods for providing voice command recommendations |
KR20210113488A (en) | 2020-03-05 | 2021-09-16 | 삼성전자주식회사 | A method and an apparatus for automatic extraction of voice agent new functions by means of usage logs analytics |
CN111353035B (en) * | 2020-03-11 | 2021-02-19 | 镁佳(北京)科技有限公司 | Man-machine conversation method and device, readable storage medium and electronic equipment |
US11748713B1 (en) * | 2020-03-31 | 2023-09-05 | Amazon Technologies, Inc. | Data ingestion and understanding for natural language processing systems |
CN111612482A (en) * | 2020-05-22 | 2020-09-01 | 云知声智能科技股份有限公司 | Conversation management method, device and equipment |
US20220075960A1 (en) * | 2020-09-09 | 2022-03-10 | Achieve Intelligent Technologies, Inc. | Interactive Communication System with Natural Language Adaptive Components |
US11748660B2 (en) * | 2020-09-17 | 2023-09-05 | Google Llc | Automated assistant training and/or execution of inter-user procedures |
CN112255962A (en) * | 2020-10-30 | 2021-01-22 | 浙江佳乐科仪股份有限公司 | PLC programming system based on artificial intelligence |
AU2021204758A1 (en) * | 2020-11-20 | 2022-06-16 | Soul Machines | Autonomous animation in embodied agents |
US11409829B2 (en) * | 2020-12-21 | 2022-08-09 | Capital One Services, Llc | Methods and systems for redirecting a user from a third party website to a provider website |
CN112740323B (en) * | 2020-12-26 | 2022-10-11 | 华为技术有限公司 | Voice understanding method and device |
US11790611B2 (en) * | 2020-12-30 | 2023-10-17 | Meta Platforms, Inc. | Visual editor for designing augmented-reality effects that utilize voice recognition |
KR20220102879A (en) * | 2021-01-14 | 2022-07-21 | 삼성전자주식회사 | Electronic apparatus and method for controlling thereof |
US11900933B2 (en) * | 2021-04-30 | 2024-02-13 | Edst, Llc | User-customizable and domain-specific responses for a virtual assistant for multi-dwelling units |
WO2022270603A1 (en) * | 2021-06-23 | 2022-12-29 | Hishab Japan Company Limited | A system and method for delivering domain or use-case switch suggestion for an ongoing conversation |
US20230050045A1 (en) * | 2021-08-12 | 2023-02-16 | Yohana Llc | Systems and methods for proposal acceptance in a task determination system |
Family Cites Families (30)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6272672B1 (en) * | 1995-09-06 | 2001-08-07 | Melvin E. Conway | Dataflow processing with events |
US5936623A (en) * | 1996-11-18 | 1999-08-10 | International Business Machines Corporation | Method and apparatus for selecting a particular object from among a large number of objects |
US6118939A (en) * | 1998-01-22 | 2000-09-12 | International Business Machines Corporation | Method and system for a replaceable application interface at the user task level |
US20040085162A1 (en) * | 2000-11-29 | 2004-05-06 | Rajeev Agarwal | Method and apparatus for providing a mixed-initiative dialog between a user and a machine |
US20020133347A1 (en) | 2000-12-29 | 2002-09-19 | Eberhard Schoneburg | Method and apparatus for natural language dialog interface |
US7496357B2 (en) * | 2002-03-04 | 2009-02-24 | At&T Intellectual Property I, L.P. | Automated telephone assistant device and associated methods |
US8015143B2 (en) | 2002-05-22 | 2011-09-06 | Estes Timothy W | Knowledge discovery agent system and method |
JP4107093B2 (en) * | 2003-01-30 | 2008-06-25 | 株式会社日立製作所 | Interactive terminal device and interactive application providing method |
US7228278B2 (en) * | 2004-07-06 | 2007-06-05 | Voxify, Inc. | Multi-slot dialog systems and methods |
US7627466B2 (en) | 2005-11-09 | 2009-12-01 | Microsoft Corporation | Natural language interface for driving adaptive scenarios |
US9318108B2 (en) * | 2010-01-18 | 2016-04-19 | Apple Inc. | Intelligent automated assistant |
JP4451435B2 (en) * | 2006-12-06 | 2010-04-14 | 本田技研工業株式会社 | Language understanding device, language understanding method, and computer program |
US8140335B2 (en) * | 2007-12-11 | 2012-03-20 | Voicebox Technologies, Inc. | System and method for providing a natural language voice user interface in an integrated voice navigation services environment |
US20110016421A1 (en) | 2009-07-20 | 2011-01-20 | Microsoft Corporation | Task oriented user interface platform |
US8818926B2 (en) | 2009-09-29 | 2014-08-26 | Richard Scot Wallace | Method for personalizing chat bots |
US9569500B2 (en) | 2012-04-27 | 2017-02-14 | Quixey, Inc. | Providing a customizable application search |
JP6001944B2 (en) * | 2012-07-24 | 2016-10-05 | 日本電信電話株式会社 | Voice command control device, voice command control method, and voice command control program |
WO2014209157A1 (en) * | 2013-06-27 | 2014-12-31 | Obschestvo S Ogranichennoy Otvetstvennostiyu "Speaktoit" | Generating dialog recommendations for chat information systems |
KR101759009B1 (en) * | 2013-03-15 | 2017-07-17 | 애플 인크. | Training an at least partial voice command system |
US9875494B2 (en) | 2013-04-16 | 2018-01-23 | Sri International | Using intents to analyze and personalize a user's dialog experience with a virtual personal assistant |
US9519461B2 (en) | 2013-06-20 | 2016-12-13 | Viv Labs, Inc. | Dynamically evolving cognitive architecture system based on third-party developers |
KR101749009B1 (en) * | 2013-08-06 | 2017-06-19 | 애플 인크. | Auto-activating smart responses based on activities from remote devices |
US9189742B2 (en) | 2013-11-20 | 2015-11-17 | Justin London | Adaptive virtual intelligent agent |
CN107112013B (en) * | 2014-09-14 | 2020-10-23 | 谷歌有限责任公司 | Platform for creating customizable dialog system engines |
US10083688B2 (en) * | 2015-05-27 | 2018-09-25 | Apple Inc. | Device voice control for selecting a displayed affordance |
KR20170033722A (en) * | 2015-09-17 | 2017-03-27 | 삼성전자주식회사 | Apparatus and method for processing user's locution, and dialog management apparatus |
US10170106B2 (en) * | 2015-10-21 | 2019-01-01 | Google Llc | Parameter collection and automatic dialog generation in dialog systems |
US11516153B2 (en) * | 2016-01-25 | 2022-11-29 | Expensify, Inc. | Computer system providing a chat engine |
US20180314532A1 (en) * | 2017-04-26 | 2018-11-01 | Google Inc. | Organizing messages exchanged in human-to-computer dialogs with automated assistants |
US10431219B2 (en) * | 2017-10-03 | 2019-10-01 | Google Llc | User-programmable automated assistant |
-
2017
- 2017-10-03 US US15/724,217 patent/US10431219B2/en active Active
-
2018
- 2018-10-02 CN CN202110154214.4A patent/CN112801626A/en active Pending
- 2018-10-02 KR KR1020227023229A patent/KR102625761B1/en active IP Right Grant
- 2018-10-02 KR KR1020217039859A patent/KR102424261B1/en active IP Right Grant
- 2018-10-02 KR KR1020197036462A patent/KR102337820B1/en active IP Right Grant
- 2018-10-02 CN CN201880039314.9A patent/CN110785763B/en active Active
- 2018-10-02 EP EP18793329.6A patent/EP3692455A1/en not_active Withdrawn
- 2018-10-02 EP EP23195499.1A patent/EP4350569A1/en active Pending
- 2018-10-02 JP JP2019568319A patent/JP6888125B2/en active Active
- 2018-10-02 WO PCT/US2018/053937 patent/WO2019070684A1/en unknown
-
2019
- 2019-08-23 US US16/549,457 patent/US11276400B2/en active Active
-
2021
- 2021-05-18 JP JP2021083849A patent/JP2021144228A/en active Pending
-
2022
- 2022-01-10 US US17/572,310 patent/US11887595B2/en active Active
-
2023
- 2023-09-22 JP JP2023158376A patent/JP2023178292A/en active Pending
Also Published As
Publication number | Publication date |
---|---|
US11276400B2 (en) | 2022-03-15 |
KR102424261B1 (en) | 2022-07-25 |
JP6888125B2 (en) | 2021-06-16 |
US20190103101A1 (en) | 2019-04-04 |
CN110785763B (en) | 2021-02-19 |
US20220130387A1 (en) | 2022-04-28 |
KR20210150622A (en) | 2021-12-10 |
EP4350569A1 (en) | 2024-04-10 |
JP2023178292A (en) | 2023-12-14 |
WO2019070684A1 (en) | 2019-04-11 |
KR102625761B1 (en) | 2024-01-16 |
EP3692455A1 (en) | 2020-08-12 |
KR20220103187A (en) | 2022-07-21 |
JP2021144228A (en) | 2021-09-24 |
US20190378510A1 (en) | 2019-12-12 |
KR20200006566A (en) | 2020-01-20 |
US11887595B2 (en) | 2024-01-30 |
CN110785763A (en) | 2020-02-11 |
US10431219B2 (en) | 2019-10-01 |
KR102337820B1 (en) | 2021-12-09 |
JP2020535452A (en) | 2020-12-03 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN110785763B (en) | Automated assistant-implemented method and related storage medium | |
JP7032504B2 (en) | Automatic assistant with meeting ability | |
US11727220B2 (en) | Transitioning between prior dialog contexts with automated assistants | |
KR102178738B1 (en) | Automated assistant calls from appropriate agents | |
CN112262381A (en) | Assembling and evaluating automated assistant responses to privacy issues | |
CN110050303B (en) | Voice-to-text conversion based on third party proxy content | |
CN112867985A (en) | Determining whether to automatically resume a first automated assistant session after interrupting suspension of a second session |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |