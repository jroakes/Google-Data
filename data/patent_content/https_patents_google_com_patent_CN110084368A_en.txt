CN110084368A - System and method for regularization neural network - Google Patents
System and method for regularization neural network Download PDFInfo
- Publication number
- CN110084368A CN110084368A CN201910324199.6A CN201910324199A CN110084368A CN 110084368 A CN110084368 A CN 110084368A CN 201910324199 A CN201910324199 A CN 201910324199A CN 110084368 A CN110084368 A CN 110084368A
- Authority
- CN
- China
- Prior art keywords
- neuron
- layer
- neural network
- neurons
- loss
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000013528 artificial neural network Methods 0.000 title claims abstract description 111
- 238000000034 method Methods 0.000 title claims abstract description 76
- 210000002569 neuron Anatomy 0.000 claims abstract description 351
- 238000012549 training Methods 0.000 claims abstract description 67
- 239000013598 vector Substances 0.000 claims description 53
- 230000009286 beneficial effect Effects 0.000 claims description 34
- 230000004913 activation Effects 0.000 claims description 31
- 238000005457 optimization Methods 0.000 claims description 5
- 230000007423 decrease Effects 0.000 claims description 4
- 230000002596 correlated effect Effects 0.000 claims description 2
- 230000000875 corresponding effect Effects 0.000 claims description 2
- 238000010801 machine learning Methods 0.000 abstract description 22
- 230000006870 function Effects 0.000 description 32
- 230000015654 memory Effects 0.000 description 17
- 230000008901 benefit Effects 0.000 description 15
- 239000011159 matrix material Substances 0.000 description 15
- 239000000243 solution Substances 0.000 description 15
- 230000008859 change Effects 0.000 description 12
- 230000000694 effects Effects 0.000 description 11
- 238000005516 engineering process Methods 0.000 description 8
- 238000003860 storage Methods 0.000 description 8
- 239000008186 active pharmaceutical agent Substances 0.000 description 7
- 230000008569 process Effects 0.000 description 7
- 238000012545 processing Methods 0.000 description 7
- 238000004891 communication Methods 0.000 description 6
- 238000010586 diagram Methods 0.000 description 6
- 238000000429 assembly Methods 0.000 description 5
- 230000000712 assembly Effects 0.000 description 5
- 239000000203 mixture Substances 0.000 description 5
- 238000006116 polymerization reaction Methods 0.000 description 5
- 238000013016 damping Methods 0.000 description 4
- 230000009977 dual effect Effects 0.000 description 4
- 210000005036 nerve Anatomy 0.000 description 4
- 230000000644 propagated effect Effects 0.000 description 4
- 238000009825 accumulation Methods 0.000 description 3
- 230000006399 behavior Effects 0.000 description 3
- 238000003780 insertion Methods 0.000 description 3
- 230000037431 insertion Effects 0.000 description 3
- 230000004048 modification Effects 0.000 description 3
- 238000012986 modification Methods 0.000 description 3
- 230000000306 recurrent effect Effects 0.000 description 3
- 230000009467 reduction Effects 0.000 description 3
- 238000006467 substitution reaction Methods 0.000 description 3
- 230000001419 dependent effect Effects 0.000 description 2
- 238000013461 design Methods 0.000 description 2
- 238000004821 distillation Methods 0.000 description 2
- 230000006872 improvement Effects 0.000 description 2
- 210000004218 nerve net Anatomy 0.000 description 2
- 230000001537 neural effect Effects 0.000 description 2
- 230000004770 neurodegeneration Effects 0.000 description 2
- 210000001176 projection neuron Anatomy 0.000 description 2
- 230000017105 transposition Effects 0.000 description 2
- 241001269238 Data Species 0.000 description 1
- YTAHJIFKAKIKAV-XNMGPUDCSA-N [(1R)-3-morpholin-4-yl-1-phenylpropyl] N-[(3S)-2-oxo-5-phenyl-1,3-dihydro-1,4-benzodiazepin-3-yl]carbamate Chemical compound O=C1[C@H](N=C(C2=C(N1)C=CC=C2)C1=CC=CC=C1)NC(O[C@H](CCN1CCOCC1)C1=CC=CC=C1)=O YTAHJIFKAKIKAV-XNMGPUDCSA-N 0.000 description 1
- 230000006978 adaptation Effects 0.000 description 1
- 230000015572 biosynthetic process Effects 0.000 description 1
- 238000004422 calculation algorithm Methods 0.000 description 1
- 238000004364 calculation method Methods 0.000 description 1
- 239000002131 composite material Substances 0.000 description 1
- 230000006835 compression Effects 0.000 description 1
- 238000007906 compression Methods 0.000 description 1
- 239000012141 concentrate Substances 0.000 description 1
- 238000013527 convolutional neural network Methods 0.000 description 1
- 230000003247 decreasing effect Effects 0.000 description 1
- 230000002950 deficient Effects 0.000 description 1
- 238000000151 deposition Methods 0.000 description 1
- 238000009795 derivation Methods 0.000 description 1
- 238000009826 distribution Methods 0.000 description 1
- 230000005611 electricity Effects 0.000 description 1
- 230000008030 elimination Effects 0.000 description 1
- 238000003379 elimination reaction Methods 0.000 description 1
- 238000002513 implantation Methods 0.000 description 1
- 230000005764 inhibitory process Effects 0.000 description 1
- 238000002347 injection Methods 0.000 description 1
- 239000007924 injection Substances 0.000 description 1
- 230000003993 interaction Effects 0.000 description 1
- 210000003127 knee Anatomy 0.000 description 1
- 238000012417 linear regression Methods 0.000 description 1
- 230000007774 longterm Effects 0.000 description 1
- 238000005259 measurement Methods 0.000 description 1
- 230000035772 mutation Effects 0.000 description 1
- 230000006855 networking Effects 0.000 description 1
- 238000010606 normalization Methods 0.000 description 1
- 238000011022 operating instruction Methods 0.000 description 1
- 210000004205 output neuron Anatomy 0.000 description 1
- 238000012856 packing Methods 0.000 description 1
- 230000001902 propagating effect Effects 0.000 description 1
- 230000011218 segmentation Effects 0.000 description 1
- 230000006403 short-term memory Effects 0.000 description 1
- 230000006641 stabilisation Effects 0.000 description 1
- 238000011105 stabilization Methods 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/21—Design or setup of recognition systems or techniques; Extraction of features in feature space; Blind source separation
- G06F18/213—Feature extraction, e.g. by transforming the feature space; Summarisation; Mappings, e.g. subspace methods
- G06F18/2132—Feature extraction, e.g. by transforming the feature space; Summarisation; Mappings, e.g. subspace methods based on discrimination criteria, e.g. discriminant analysis
- G06F18/21322—Rendering the within-class scatter matrix non-singular
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/21—Design or setup of recognition systems or techniques; Extraction of features in feature space; Blind source separation
- G06F18/213—Feature extraction, e.g. by transforming the feature space; Summarisation; Mappings, e.g. subspace methods
- G06F18/2135—Feature extraction, e.g. by transforming the feature space; Summarisation; Mappings, e.g. subspace methods based on approximation criteria, e.g. principal component analysis
- G06F18/21355—Feature extraction, e.g. by transforming the feature space; Summarisation; Mappings, e.g. subspace methods based on approximation criteria, e.g. principal component analysis nonlinear criteria, e.g. embedding a manifold in a Euclidean space
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/21—Design or setup of recognition systems or techniques; Extraction of features in feature space; Blind source separation
- G06F18/214—Generating training patterns; Bootstrap methods, e.g. bagging or boosting
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/24—Classification techniques
- G06F18/241—Classification techniques relating to the classification model, e.g. parametric or non-parametric approaches
- G06F18/2413—Classification techniques relating to the classification model, e.g. parametric or non-parametric approaches based on distances to training or reference patterns
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/082—Learning methods modifying the architecture, e.g. adding, deleting or silencing nodes or connections
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/21—Design or setup of recognition systems or techniques; Extraction of features in feature space; Blind source separation
- G06F18/213—Feature extraction, e.g. by transforming the feature space; Summarisation; Mappings, e.g. subspace methods
- G06F18/2132—Feature extraction, e.g. by transforming the feature space; Summarisation; Mappings, e.g. subspace methods based on discrimination criteria, e.g. discriminant analysis
- G06F18/21322—Rendering the within-class scatter matrix non-singular
- G06F18/21326—Rendering the within-class scatter matrix non-singular involving optimisations, e.g. using regularisation techniques
Abstract
The disclosure is usually directed to machine learning.More specifically, this disclosure relates to by the neuron of decorrelation neural network during the training of neural network or other parameters come the system and method for regularization neural network, so that these parameters be promoted mutually to innovate.
Description
Related application
This application claims the priority of U.S. Provisional Patent Application No. 62/660,617, which passes through reference
It is integrally incorporated herein.
Technical field
The disclosure is usually directed to machine learning.More specifically, this disclosure relates to by being solved during the training of neural network
The neuron and/or other assemblies or parameter of related (decorrelate) neural network come regularization neural network system and
Method.
Background technique
Quantum jump has been provided in neural network in terms of big data prediction, and improves the prediction standard of machine learning
Exactness and ability.However, the larger numbers of parameter learnt is attempted to realize such performance due to neural network,
Their possible significant ground overfitting training datas, potentially cause to the bad extensive of data unobservable during training.
Neural network is also subjected in training potential unstability and not reproducible.For not reproducible, for example, having
May in identical data still available two very different nerve nets of the identical network structure of stand-alone training twice
Network, and it is possible to which different predictions is generated to data unobservable during training.This can occur do not guaranteeing with identical
Sequential access training example highly-parallel and distributed training system in.Then, if then which network controls
Subsequent training example is seen, then the network can shift the evolution of very different ground with covariant.These deviations can be from
Different random initialization, parallelization and/or the sequence of training example generate.
Have been illustrated in neural network that there are bulk redundancies.Redundancy may cause above-mentioned overfitting and other problems.
Specifically, network can be predicted by overparameterization well, some of parameters by other parameters.Because study (is lost
Regret) punishment is paid than required more parameters, and convergence rate and generalization ability slow down, so this is specifically resulted in
The overfitting of the training example of limit.
Therefore, when the re -training in identical data but potential different initialization, parallelization with training example
When with sequence, neural network by overfitting, to the potential bad extensive, unstability of invisible data and can not be again
Existing property.These problems make using neural network dispose large scale system become difficult, sometimes even not possible with, due to
It can not lead to massive losses using the significant advantage and advantage of neural network when more accurately prediction is provided.Current solution party
Case requires to repeat in training and deployment, causes the excessive use of CPU and memory, and hinder deployment more due to being deficient in resources
The ability of multisystem.
Summary of the invention
The aspect and advantage of embodiment of the disclosure will illustrate partly in the following description, or can be from description middle school
It practises, or can be learnt by the practice of embodiment.
One exemplary aspect of the disclosure is for a kind of for training the method implemented by computer of neural network.This method
Including by one or more data for calculating equipment and obtaining description neural network.Neural network includes multilayer neuron.This method
Neural network is trained by neural network backpropagation loss function including calculating equipment by one or more.Loss function is retouched
State performance of the neural network relative to one group of training example.Equipment is calculated by one or more to damage by neural network backpropagation
Losing function includes, for each layer in one or more layers in multiple layers of neural network: being set by one or more calculating
The standby gradient for determining loss function relative to the neuron for including in layer.For at least layer, loss function includes innovation loss
, innovation loss item is that each of the one or more neurons for including or unit are provided based on one in layer in layer
Or the penalty values of the ability of other multiple neurons, to predict the value of such neuron.Equipment is calculated by one or more
Include by neural network backpropagation loss function, for each of one or more layers in multiple layers of neural network
Layer: it is modified in the layer by the gradient that one or more calculating equipment are based at least partially on the loss function including innovation loss item
Including neuron, with the neuron for including in this layer of decorrelation.
Another exemplary aspect of the disclosure is for a kind of for training the method implemented by computer of neural network.This method
Including by one or more data for calculating equipment and obtaining description neural network.Neural network includes multilayer neuron.This method
It is pre- by other the one or more neurons for including in identical layer including being based at least partially on by one or more calculating equipment
Survey the error of the value of such neuron come for include in one or more layers in multiple layers one or more neurons in
Each neuron determine beneficial to score (benefit score).This method includes calculating equipment at least by one or more
Be based in part at least one neuron determine beneficial score come modify with one or more neurons in as extremely
A few associated weight.
In some embodiments, it is based at least partially on by one or more calculating equipment beneficial to score and is modified and one
The associated weight of at least one neuron in a or multiple neurons includes that it is small in error to calculate equipment by one or more
By weight canonical cancellation when threshold value.
In some embodiments, this method further include by one or more calculate equipment by weight canonical cancellation it
Afterwards, it randomly reinitializes one or more associated with neuron and links (link).
In some embodiments, for each of one or more neurons neuron, by including in identical layer
The errors of value of other one or more neuron predictive neurons include, according to include in identical layer it is one or more its
The activation weight of one or more regularizations of his neuron estimates the error of the complete activation weight of such neuron.
In some embodiments, by one or more at least one calculated in apparatus modifications and one or more neurons
A associated weight of neuron includes, and calculating equipment by one or more will be at least one of with one or more neuron
The associated weight modification of neuron is the value of regularization, and the value of the regularization is equal to beneficial point applied to such neuron
The dot product of the value of several Sigmoid functions or hyperbolic tangent function and such neuron.
It in some embodiments, include: institute beneficial to score for each of one or more neurons neuron
There is the accumulation square error of batch and present lot；The average every example error of every batch of；Or one group of nearest batch sliding or
Damped expoential window error.
Another exemplary aspect of the disclosure is for a kind of for training the method implemented by computer of neural network.This method
Including by one or more data for calculating equipment and obtaining description neural network.Neural network includes passing through multiple chains in succession respectively
The multiple neurons connect.This method includes calculating equipment by one or more to instruct by neural network backpropagation loss function
Practice neural network.Loss function describes performance of the neural network relative to one group of training example.It is set by one or more calculating
It is standby to include by neural network backpropagation loss function, for one or more neurons, link or the deviation of neural network
Each of: by it is one or more calculate equipment determine loss function relative to neural network one or more neurons,
The gradient of link or deviation.For at least one or more neuron, link or the deviation of neural network, loss function includes wound
New loss item, innovation loss item are that each of one or more neurons, link or deviation are provided based on one or more
The penalty values of the ability of other a neurons, link or deviation, to predict the value of such neuron, link or deviation.By one
A or multiple calculating equipment include calculating equipment at least partly by one or more by neural network backpropagation loss function
One or more neurons of neural network, link or inclined are modified based on the gradient for the loss function for including innovation loss item in ground
Difference, with one or more neurons, link or the deviation of decorrelation neural network.
In some embodiments, other one or more neurons, link or the deviation of neural network can be included
One or more neurons of prediction are attempted with them in neural network, link or deviation identical layer in.
In some embodiments, at least one of one or more neurons, link or deviation of neural network quilt
Including in neural network and such at least one neuron, link or deviation attempt prediction one or more neurons,
In link or the different layer of deviation.
In some embodiments, for from the trained batch of each of this layer, randomly choose one of neural network or
At least one of multiple neurons, link or deviation.
In some embodiments, example is small batch training batch to calculate creative solution associated loss.
In some embodiments, input layer insertion randomly or is definitely initialized as facilitating connection to their layer
With the nonzero value of the decorrelation of tower.
Other aspects of the disclosure are directed to various systems, device, non-transitory computer-readable medium, user interface and electricity
Sub- equipment.
With reference to the following description and the appended claims, it is better understood with the various embodiments of the disclosure these and other
Features, aspects and advantages.Comprising in the present specification and constituting part thereof of attached drawing and showing the example embodiment of the disclosure,
And it is used to explain relative theory together with specification.
Detailed description of the invention
Being discussed in detail for the embodiment for being directed to those of ordinary skill in the art is elaborated in the description, with reference to attached
Figure, in which:
Figure 1A depicts the block diagram of the exemplary computing system of training neural network according to an example embodiment of the present disclosure.
Figure 1B depicts the block diagram of Example Computing Device according to an example embodiment of the present disclosure.
Fig. 1 C depicts the block diagram of Example Computing Device according to an example embodiment of the present disclosure.
Fig. 2A-Fig. 2 C depicts exemplary neural network according to an example embodiment of the present disclosure.
It is intended to identify the identical feature in various embodiments across the duplicate Ref. No. of multiple attached drawings.
Specific embodiment
It summarizes and introduces
The exemplary aspect of the disclosure be directed to through the neuron of decorrelation neural network during the training of neural network or
The system and method that person's other parameters or component carry out regularization neural network.Specifically, present disclose provides one kind for solving phase
Close new departure of the relevant neuron in the layer (for example, output layer or hidden layer) in neural network.In some embodiments
In, the system and method for the disclosure can be identified provides the layer neuron of less increased predicted value in layer.It can be by the layer
The neuron predicted well of other neurons increase network less predicted value.This neuron can be forced to increase
It is moved on the direction of its increased value.This can be carried out by the supplementary loss of the layer of injection neural network, so that these are refreshing
Neuron is pushed to predict away from each other through the loss gradient in member.
Therefore, in some embodiments, scheme described herein is not merely based on the neuron pair in decorrelation layer, and
And the wound of the information more than other neurons in this layer is also pushed to based on each neuron in the layer for ensuring neural network
Newly.As an example, this can be by ensuring by the value of the given neuron of other some or all of neurons prediction in layer
Least mean-square error be maximized to carry out.The variation of these exemplary scenarios can enforce such constraint by applying
Additional every layer of regularizer lose to implement.As an example, constraint can be bright in the form of L2 regularization and with glug
The form that day dual variable (dual variable) optimizes applies.Therefore, present disclose provides the innovations for measuring neuron
And the relevant loss of application is so that its maximized multiple strategy.Neuron can with but not necessarily include nonlinear activation.For example,
Neuron may include and implement linearly or nonlinearly activation primitive, with based on the input value for being fed as input to neuron come
Neuron output is provided.
In addition, the disclosure also demonstrates, other than example regularization type and Lagrange duality variable technology, may be used also
To implement the supplementary technology similar to minimum description length (Minimum Description Length, MDL) regularization to fill
Defeated (instill) neuron innovation.Specifically, the innovation of neuron is (for example, its bring is more than pre- to its by other neurons
The additional dimension of the value of survey) it can be used for zooming in or out the weight of neuron.In one example, if neuron is wound
New, then its weight will not reduce (or can amplify), and if it is not innovation, its weight can reduce, and permit
Perhaps other neurons are taken over, and in some cases, force neuron finally by regularization.
Therefore, present disclose provides the system and method that can be used for regularization neural network, so as to improve network reproduction
Property, extensive and stability.Technical effect and benefit as an example, can be by eliminating to entirety (ensemble) training
It needs to realize that resource is saved, so that the RAM and CPU that save in deployment are used, it is ensured that training faster, and also potentially change
Kind forecasting accuracy.Technical effect and benefit as another example, the neural network trained according to the disclosed systems and methods
The prediction deviation and overfitting for showing reduction, obtain to the more preferable extensive of invisible data.
More specifically, considering the layer (for example, hidden layer of neural network) of neural network.It is assumed that saving to given layer by network
All information of study, because it serves as information from the bottleneck for being input to output.Due to overparameterization, the nerve of some activation
Member can be related to other neurons.In other words, the subset of the neuron of the activation in layer can be used to predict other sons
Collection.If it is the case, study loss is then spent to may cause slower convergence on predictable neuron.In turn,
This may cause overfitting and undesirable extensive and not reproducible and unstable network.Specifically, such redundancy
The diversity that solution can be increased reduces the reproducibility of network.The problem can be solved from both direction: Xie Xiang
It closes；Or it forces related.Present disclose provides the system and method using decorrelation scheme.
Certain technologies increase the layer loss applied to the activation of all neurons in the layer in neural network, middle layer
Loss punishment has the layer of the neuron pair of high covariance.Therefore, layer loss operates on the basis of by (pair-wise).
The backpropagation of the loss reduces such covariance between.Overall layer target is arranged to omit the member on diagonal line
Element all covariances square (or square of the Frobenius norm of covariance matrix) and.It is obtained in a collection of training example
Obtain the estimation of covariance.Gradient is applied in backpropagation on this supplementary loss.Although this scheme has advantage, due to following
Two reasons, it is not fully solved problem:
Firstly, covariance may not be the right metric that given neuron explains the degree of another neuron, because it
The mean value of two neurons is omitted.Due to, typically without any other knowledge for predicting mean value, only leading in layer
Cross the certain relationships for checking that covariance may lose between the neuron in layer.For example, two neurons can have (very
It is big) equal expected value --- one with big variance and another is with small variance --- but in both cases,
Variance is much smaller than mean value.In addition, the covariance between neuron is very small.One neuron still can predict another mind
Most of energy through member, but the correlation based on covariance will imply that situation is really not so.Covariance constraints are to mean value
Any constraint is not applied, therefore mean value can be widely different.For example, if the sum of mean value of two neurons is 1, they
Mean value can be any two value that summation is 1.When applying only covariance constraints, such different a variety of solutions are not eliminated
A possibility that scheme.Further, since the activation in neural network is nonlinear, therefore the neuron of expectability activation is not being criticized
There to be mean value 0 in secondary.In fact, for ReLU, this be it is impossible, except non-neuron is not activated in the batch
Any example.Only covariance constraints are it is implicitly assumed that 0 mean value.
Secondly, covariance or correlation only provide the relationship between two elements.The quadratic sum of covariance is punished
The relationship between other neurons is omitted.For example, the neuron linearly relied on if there is three, in order to predict third mind
Through member, both first and second neurons by penalized without considering the relationship between them, rather than between them
Segmentation punishment.This can potentially exaggerate the punishment to some neurons, while underestimate the punishment to other neurons.
All aspects of this disclosure solve both of these problems.Specifically, in some embodiments, instead of using association side
The system and method for difference, the disclosure use correlation, because not to the estimator of mean value in layer.So, a target is
The innovation (for example, activation of each neuron) of each neuron is relative to other minds in this layer substantially in forcing layer
Information through member maximizes.For example, it is contemplated that having being tieed up in Euclidean space to the information in given layer by n for n neuron
Point provides, and square error is valid metric.
In order to maximize wound of the neuron (for example, wherein activation be also possible to linear) of activation relative to its companion
Newly, the system and method for the disclosure can attempt the neuron that activation is predicted according to the neuron of other activation.It is minimizing
The optimum prediction device in terms of mean square error in example training batch (batch) is conditional mean estimator.However, due to being difficult to
It is calculated, therefore in some embodiments, the disclosure can alternatively consider Optimal Linear Predictor.Because not having in layer
For the other information of the loss except optimization layer, so in some embodiments, linear homogeneous estimator can be used.It changes
Sentence is talked about, and since final goal only " sees " neuron of the activation of layer, anything should not be determined using other
Whether neuron has novelty relative to the every other information unit for traveling to final goal.
The first example policy that the disclosure provides is using linear homogeneous least mean-square error (Minimum Mean
Square Error, MMSE) estimator carrys out each neuron in prediction interval.In view of the estimation to statistic correlation and mean value
It is accurately that the estimator gives (easily to be located according to the good estimation for each activation of other activation in given calculating
The constraint of reason).According to the aspect of the disclosure, target in some embodiments is to keep the error of the estimator as big as possible；
Error is bigger, and the innovation of neuron is better.The solution requires multiple invert (inverse) of big matrix.However, if this
Be it is reluctant, then the potential suboptimum that may serve as the proposition of the second example policy is replaced based on the solution of recurrence
It minimizes the error.
K exemplary n neurons in training batch are considered as k by the third example policy provided by the disclosure
The nk dimensional vector of exemplary neuron value.J-th of vector indicates j-th of neuron in k example of training batch
State.Its projection on other n-1 vector can be subtracted, from each neuron vector to generate by other n-1 vector
Predict the prediction error of the vector.In other words, it can produce the error vector orthogonal with other vectors.The norm of the vector is
The prediction error of batch.It is also possible to attempt for example, by backpropagation norm square reversion (invert) value ladder
It spends to maximize square of the norm.Alternatively, it can be minimized the norm of the projection of the vector to other vectors.
In following part, the present disclosure describes the details of strategy presented above.Firstly, the disclosure demonstrates how to answer
With the example of the decorrelation of neuron pair.Next, the disclosure demonstrates the base based on maximum innovation for calculating network layer
In three example policies of the target of mean square error (Mean Square Error, MSE).Then, the disclosure is demonstrated the mesh
Mark is applied to two exemplary scenarios of the layer of network: for enforcing the class regularization of maximum MMSE (innovation) constraint to batch
Scheme and the variant for using the optimization based on Lagrange constraint.Next, how description utilizes square error innovation estimation
Device on layer to apply the scheme based on online benefit (class MDL regularization).Finally, solve a little actual complexity issues,
And several ideas presented on how to apply the technology in systems in practice.
Therefore, there is provided herein the sides based on innovation or mean square error by other neuron prediction interval neurons in layer
Case.One basic idea is, if can be with error by a small margin from other neuron predictive neurons in layer, the mind
It may be redundancy through member.Increase its innovation on other neurons by pushing the gradient of neuron, proposes to understand phase
The method for closing neuron.Alternatively, it if proposing neuron not is innovation, uses based on the scheme beneficial to score just
Then change neuron, wherein the innovation of the score and neuron is closely related.
Although the part being discussed herein concentrates on the neuron in hidden layer, innovation Regularization Technique described herein
It can be executed in any layer.Technology is executed on top layer and/or bottom to be sufficient, but is calculated as in addition to increased
Except this, there is no limit do not include more layers.For example, the calculating cost for executing the technology may make expectation only implement in top layer
It is to reduce complexity.But by it be applied to other layers be also possible to it is beneficial.Therefore, unless otherwise specified, otherwise at this
Any reference of layer is included in open input layer within the scope of its, hidden layer and output layer (e.g., including by single mind
Layer through member composition, wherein component (for example, tower) of each layer towards complete network).
In addition, concentrating on neural network, scheme herein (for example, utilizing the scheme of MSE decorrelation) although discussing
It is more general than being only used for neural network.They can also be used to search the correlation in linear model between feature.Specifically, one
In a little embodiments, they can be only applied to character subset present in the batch rekeying of linear model, rather than entire special
Collection is closed.
Although preactivate can be applied in addition, usually describing the program on the neuron of the activation of layer
Neuron, link, deviation or neural network other parameters or aspect.Due to it is expected to survey at the point closest to final output
The innovation of layer is measured, and because the influence to final goal is finally paid close attention to, application constraint upon activation is reasonable.
However, may not be very useful if many neurons do not activate, and application preactivate scheme is also possible.Together
Sample, other than nonlinear activation or substitution nonlinear activation, scheme described herein can be applied to linearly activate.This
Outside, the program can also be applied to any parameter in other assemblies and link or network.Specifically, in some embodiments
In, it can be applied to different neurons with cross-layer.However, the scheme that the disclosure mainly considers is to apply it in unbroken layer,
Because layer should capture all information from layer and neuron below, and will be somebody's turn to do (compression) information and be relayed to target.
In some embodiments, the method for proposition can be applied particularly to closest to the output in overall model structure
Layer.One example of such constellation (constellation) includes several independent networks, and each of network is exporting
There is a neuron at end.Additional innovation loss can be applied to the layer of the output neuron of the component of composite entity, but compel
Keep each independent black box different from each other and innovates on other networks.
It in some instances, can be by being forced to input insertion wherein inputting the character representation by being encoded into insertion
It executes different random initializtions and carrys out the black box of duplicate network to encourage the innovation of neuron or networking component.
The system and method for the disclosure provide multiple technical effects and benefit.Technical effect as an example, this public affairs
The system and method opened, which make it possible to generate, shows the overfitting less to invisible data and better extensive, stabilization
The neural network or other machines learning model of property and reproducibility.Therefore, the system and method for the disclosure make it possible to generate phase
The machine learning model of superior function is shown for the prior art.In addition, the system and method for the disclosure can enable to
More rapid convergence neural network, because in some embodiments, pushing neuron to innovate on its companion in the training of every wheel.
Technical effect as another example, the system and method for the disclosure make it possible to improve processing and memory resource
It utilizes.More specifically, attempting to solve overfitting, bad extensive, unstability and/or not reproducible certain prior arts
Performance dependent on overall plan.These overall plans require duplication (can be large-scale) multiple nets in training and deployment
Network.Repetition or this training of redundant network and deployment substantially increase the consumption of both processing and memory resource.Therefore,
It solves overfitting, bad extensive, unstability by providing and/or not reproducible does not depend on or be less dependent on entirety
The solution of the performance of scheme, the system and method for the disclosure release a large amount of processing and memory resource, and otherwise this is big
The processing of amount and memory resource will be specific to such overall plan.Alternatively, if maintaining similar overall structure, wound
New encouragement method can rely on entirety to promote better reproducibility.
Decorrelation
Consider the layer with the neural network of n neuron.The network trains k exemplary batches every time.For some
Example, the n for enabling Z represent the activation of the neuron of the end of expression layer tie up random column vector.The correlation matrix of layer is given by following formula
Out
C2=E [ZZT]
Wherein subscript T represents transposition operator.The element of the correlation matrix Cz in current training batch can be used in we
The experience of i, j are estimated
In order to make these estimations effectively, we should have more enough than the neuronal quantity in layer show in batch
Example, i.e. κ > > n.Through the disclosure, following notation convention will be used.Subscript indicates the neuron (or layer neuron) of activation
J is indexed, wherein 1≤j≤n.Multiple (two) subscript i, j represent the relationship between two such neurons.Subscript l, wherein 1
≤ l≤k represents the exemplary index in training batch.
According to the aspect of the disclosure, by increasing layer loss, such as following example layer is lost, can be to the correlation in layer
Quadratic sum apply constraint:
Wherein | | Cz||FRepresent CzFrobenius norm and diag representing matrix diagonal element.Note that this
Loss can be standardized (normalize) by the quantity n of neuron.Increase regularizer weight λ and will be superimposed with total
The activation of neuron of the gradient of the loss of body target to given layer is propagated downwards, and regularization can be enforced, the canonical
The amplitude of correlation of the activation in layer between will be reduced by changing.This solves such a fact, that is, utilizes the class of covariance
The correlation of the mean value of activation is not can solve like scheme.
Above scheme can be extended, so that correlation is continuously updated through during training, rather than again by every batch of
It calculates.To a kind of this embodiment can to batch use exponential damping window form, by the value being previously calculated multiplied by
Small normal number (positive constant) (for example, less than 1), and will for present lot calculating correlation with it is small
Another constant in 1 is added.
Alternative scheme
The decorrelating method being presented above using the decorrelation between layer or component output or connects covariance to training batch
(de-covariance).Effect similar with decorrelation can be generated in unknown losses.Anti- distillation loss (anti-can be referred to as
Distillation loss) loss can be used for export and push away each other, such as L2Loss,
It can force and predict that different unknown losses is also possible.
Correlation and covariance can be modified to lose to eliminate because component output repeats caused by related to except
Loss.It is related to b's and c that correlation between reduction output a and b between output a and c may repeat (double) calculating a
Component between correlation.It, can be to component Z in order to eliminate this repeat countiCarry out Gram-Schmidt normal orthogonal
Change to generate the basis of batch vector (batch vector).It is not in vector ZiBetween increase decorrelation loss, but for j <
I can be in ZiAnd BjBetween increase decorrelation loss, wherein BjIt is to be generated at Gram-Schmidt orthonormalization step j
Basis vector.Only these correlations are applied to lose and will ensure that each additional step attempts only to eliminate the throwing that previous steps are not eliminated
Shadow (or correlation).
Innovation is inculcated in layer
MMSE estimator
In order to solve the maximization of the innovation of the neuron of each activation (under feasible linear restriction), we are from by Z-j
N-1 dimensional vectors represent, other neurons derive jth layer neuron ZjLinear MMSE estimator.Equally, in order to make phase
Close property estimation it is effective, batch size should relative to neuron quantity it is sufficiently large, i.e. k > > n.Enable Cz；-jIt represents from CzIn
(n-1) × (n-1) the dimension correlation matrix that discarding jth row and jth arrange.It enables
bj=E [ZjZ-j]
It is the n-1 dimension cross correlation vector between j-th of neuron of layer and every other neuron.Note that bjEqual to tool
There is the C of j-th of element of omissionzJth column,.Then, according to Z-jZj(homogeneous) linear MMSE estimator be given by
Wherein
aj=(Cz；-j)-1·bj。
For example, estimating Z to the example l in batchjWhen error be given by
Wherein lowercase is used to represent the instantiation value of stochastic variable.It can be estimated with every example, wherein subscript l generation
Exemplary index in table batch.Average batch error is given by
Average batch square error is given by
Square average batch square error
Expected MSE can also be calculated directly from correlation.It is given by
Using this expression formula, we do not need to calculate the exemplary error of each of batch.On the contrary, we estimate from batch
Correlation, Inverted matrix Cz；-j, and with the equation for batch calculating MSE.
In order to avoid possible singular point, if our neuron is not activated during batch or in the batch
Very small part be activated, then we can by they exclude predict and additional propagation loss except.
Innovation of inverting is lost:
Layer loss in training batch is the superposition of the loss in the example of neuron and batch
It can be with every example and every layer of neuron standardization loss.It is, however, possible to this can not be it is necessary.No matter such as
What, standardization or shortage standardization can be absorbed in regularization scaling (scale) factor lambda, when the target loss phase with network
In conjunction with when, zoom factor λ can be added in the loss.Loss can be decomposed with the every example of every neuron, and can be with
Only each neuron is decomposed.
The innovation that innovation loss is intended to that network is pushed to maximize each neuron in layer.This means that (minimum) mean square forecast
Error should be maximized.In order to the constraint is superimposed with the network losses that are minimized, the concept of error can be converted
For the loss that can be minimized.The requirement will be met by applying negative batch mean square error loss.However, such loss is not
Convex (it is recessed).Therefore, in the case where MSE is 0, the gradient of loss is 0.About 0, the amplitude with very little.MSE
Region close to 0 is the region that we need big gradient neuron is pushed quickly to be innovated on other neurons.
In some embodiments, we can have the loss of reversion MSE, without negating MSE.This loss exists really
Big gradient is provided at 0 region.We can inhibit the loss by increasing a positve term β on denominator, will be in gradient
Limit is set as 0MSE, i.e., is unlimited without this gradient.For j-th of neuron in first of example, this causes following damage
It loses
In order to which number is convenient, increase coefficient 1/2 to offset 2 in gradient.(the canonical that it will be finally increased by one
Change factor lambda to offset.) layer loss be given by
Every neuron is given relative to the gradient of i-th of neuron in (relative to) example l by following formula per exemplary loss
Out
Note that the gradient have example l in every neuron j relative in layer any neuron (including j-th nerve
Member) loss.We use aj；vTo represent n-1 dimensional vector ajV component.Difference between most latter two region is only
It is because by removal z-jIn j-th of component and move up component.
It was noted that gradient pushes the weight of the neuron of non-innovation far from the relevant estimation based on other neurons
Device.However, the neuron in estimator is also pushed to the opposite direction of the neuron far from estimation by it.This may potentially weaken
Other neurons.In some embodiments, it may be more desirable to only allow the neuron (predicted of moving projection
Neuron) without moving projection neuron (predicting neuron).This can be by stopping the gradient on the i-th neuron
Propagation apply, wherein i ≠ j.But, it may be unnecessary to.Substitution presented below based on beneficial to score (MDL canonical
Change) method can solve this problem.
If we have very relevant neuron (in extreme circumstances, equal neuron) i and j, and the two
On a neuron i innovation loss gradient by negate the two on another neuron j innovation loss gradient,
And they may never remove this correlated condition.In order to solve the problems, in some embodiments, we
Priority can be distinguished in neuron.A kind of scheme to this is different just by the loss distribution for each neuron
Change coefficient then to avoid such problems.But we can be randomly assigned the inverse of the ratio of the correlation between neuron,
This may lead to the problem of same.This can be solved by the regularization coefficient between random permutation batch.When we retouch
It states and how to utilize regularization when applied layer on network loses, the theme will be discussed in more detail.Equally, finally describe based on
It can be more steady for such a problem beneficial to the scheme of score (MDL regularization).
Alternative innovation loss:
As the alternative scheme for MSE loss of inverting, the decreasing function of the MSE with positive goal value also can be used,
As long as they meet convexity requirement, and realize greatest gradient at the region of 0MSE.Unfortunately, relative to square error
Exponential damping is by the problem identical as negative MSE (0 gradient relative to the non-square error at 0 square error).However, phase
These requirements are met for the function of the square root decaying of MSE, and can be used.Loss is relative under as two
The L for the error that formula provides1The inverse loss of norm
Wherein, equally, β is for inhibition function to avoid the unlimited gradient at 0.Alternatively Laplce loses
Parameter alpha wherein can be used.
Batch square error:
For the loss L from other neuron predictive neurons jjIt is decomposed in example in batch.But the program
Statistic correlation by batch all examples estimate.Consider that the loss of the polymerization in example is also reasonable.It is returned with linear
Return difference, gradient is no longer linear in error term.The expection of the error or square error of batch is calculated, and in the updating
It will cause different batch gradients using expected value.
Using have damping inverse Squared Error Loss (dampened inverse square loss) expected error (and
It is not expection of the application as previously described to the loss of batch), the loss of the polymerization of all neurons is given by:
Every neurone loss is
Relative to exampleNeuron j differential
All examples in batch of summing
All exemplary obtained gradients are equal to the gradient of expected error in batch in batch.When we divide in example
Xie Shi, the sum of gradient of batch are the expections of identical function
For just and sufficiently large x and for sufficiently small β, the amplitude of the function is convex.Therefore, pass through Jensen
Inequality, the gradient of every example calculations can cause bigger gradient step (if they are all in a same direction).
Similarly, for sufficiently large negative value, can equally occur on opposite direction.
For some i ≠ j, by the gradient relative to j respectively multiplied by the factor-a in the case where i < j and i < jj；iOr-
aj；i-1, LjRelative toGradient be expressed be similar to every example gradient.
Using the expected square error of the inverse Squared Error Loss with damping, the loss of the polymerization of all neurons is by following formula
It provides
Every neurone loss is
Relative to exampleThe differential of neuron j be,
All examples in batch of summing
Due to EX2≥(EX)2, therefore the amplitude of the gradient of batch MSE is less than the amplitude of the mean error of batch, (is mentioning
Under conditions of) its that polymerization for being less than the exemplary gradient in batch.
Equally, for some i ≠ j, by the gradient relative to j respectively multiplied by the factor-in the case where i < j and i > j
aj；iOr-aj；i-1, LjRelative toGradient be expressed be similar to each exemplary gradient.
If we directly calculate MSE or the projection by presenting below is innovated, expected mean square error is used
Gradient can be nature fitting.
Gradient decline returns estimator
The set of n MMSE estimator in the layer of n neuron needs to invert n (n-1) × (n-1) dimension correlation matrixes
Cz；-j.In some cases, this may be computation-intensive.As alternative scheme, least-squares linear regression problem
It can use the Squared Error Loss during trained batch relative to vector aj
To solve.This can be carried out by executing the successive ignition of batch gradient decline in batch.It can for institute
There is n neuron to carry out.Although we can be with the enough numbers of iteration to guarantee the convergence to solution, we can also be with
The iteration of a small amount of fixed quantity is executed for the solution of some possible suboptimums.Then, which can be used for calculating
The loss and gradient propagated downwards from this layer, rather than MMSE.After minimizing relative to the error of aj, remaining step is present
The error loss such as the reversion presented for MMSE will be minimized.Endless total regression may cause the minimum value of some suboptimums, existing
The maximization of error is executed in the minimum value in these suboptimums.Incomplete minimum in the first step may exaggerate neuron
Innovation.Despite suboptimum, it is horizontal that it still can execute enough regularizations, but have it is more feasible, less calculate it is close
The scheme of collection.
If we can utilize O (n with every layer2) additional storage, then we can execute terraced several times in given batch
Degree decline iteration, to improve estimator αjCoefficient.Then, our packing coefficient ajState, and using depositing in next batch
The value thermal starting next iteration of storage.This can improve result and make vector αjThe value of MMSE is realized close to it, but cost is attached
The memory space added.
It is innovated by the batch of projection
For the scheme based on projection, we should make k >=n, that is, we should in batch at least with it is neural in layer
The example of member as many.Otherwise, the dimension in the space that the neuron of layer is crossed over is limited by the quantity k for the neuron being less than in layer
System.
In order to which the scheme based on projection is presented, it would be desirable to the n neuron and k of the dimension of expression layer it is exemplary it
Between the symbol that defines before transposition.It enables
It is column vector, indicates the k value of the neuron j of k training example in batch.It defines k × n and ties up matrix
X=[x1, x2..., xn]
The grade of column vector as the layer neuron value on all layers of neuron connects.It enables
B-j=[b-j；1, b-j；2..., b-j；n-1]
It is k × (n-1) dimension matrix, column are across the basis vector that crossed over space is arranged by the X that omission jth arranges.It can
To obtain B by executing Gram-Schmidt orthonormalization to the X column for excluding jth column-jColumn.B-jActual column will
Depending on the sequence that Gram-Schmidt process executes on X column, but this will not influence our interested practical projection mistakes
Difference.The complexity of this process will discuss at the end of this part.
Now, for all j, 1≤j≤n, it would be desirable to calculate vector xjIt is projected in by B-jThe mistake in the space crossed over
Difference.This error is given by
Wherein < xj, b-j；i> it is inner product between two vectors.ejL componentIt is the error component of example l.The mistake
Poor ejGiving cannot be by other n-1 vector xi, the x of i ≠ j explanation (or prediction)jPart.In other words, error is
The value of j layer neuron and by this layer other n-1 neuron between their prediction, k training example in batch
On difference vector.
The MSE of neuron j in batch is by ejThe squared norm of standardization (for example, per exemplary) provide
Due to the projection process on exemplary complete batch, express about batch mean square error loss target seemingly this
In natural scheme.Therefore, layer loss is defined as by we
Can by follow the derivation of loss linear step (including for each j generate basis matrix B-j) derive phase
ForGradient.
We can also be defined using the equation for similar loss in MSE strategy per exemplary loss.With ejIn
The corresponding error of each elementIt can be inserted into equation.Equally, gradient can be from executing to obtainLinear step
In derive.
Matrix B-jGeneration require each of the n-1 vector from X incrementally projecting to B-jFormation
Column on, projection is subtracted from vector, and residual error is normalized into B-jNew column in.It is required on the basis of xj is projected to another
One n-1 projection (multiplication).Projection is all the multiplication of O (k) every time.For complete procedure, there is O (1+2+3+...+n-1)=O
(n2) a projection/normalization step.It therefore, is O by the vector j complexity projected in the basis created by other vectors
(kn2) a multiplication.If we execute this operation for each j, we will need O (kn3) a multiplication.
This complexity can be reduced by some basis vectors of shared creation.It is a that we can store O (nklogn)
Element, and only carry out O (kn2Logn) a operation, rather than O (nk) a element (a base of O (n) is stored at any given time
Plinth, each of k element).We are by vector xjIt is divided into the set of two equal (or almost equal size).For each
Set generates basis.It is stored as the cascade for the n basis vector that two set are formed.Then, each set is again broken down into two
Half.Enabling A is original top half, and B is original lower half portion.The original vector of A is divided in half into two set Λ now1
And A2, and those of B is divided into set B1And B2.For set ΛvEach of, we are on the basis formed for set B
(between set individually) continue Gram-Schmidt process on vector.Similarly, this is also using the basis formed for A pair
BvIt carries out.The process is recursively repeated, until each remaining set has single vector.For every in these single vectors
One, its (sibling) born of the same parents from previous step is projected in the set on the path by the single vector now
To generate error on the basis of vector composition.This process has a step of O (log n), and each step has O (n2) a projection,
Each takes a operation of O (k).
Alternative projection loss
We can directly minimize projection energy of any vector on the space that other vectors are crossed over, rather than make
With projection error and minimize the loss being inversely proportional with error.We can be to component xjExecute Gram-Schmidt normal orthogonal
Change, to generate the basis of batch vector.In addition in vector xjBetween increase decorrelation loss except, we will also be in xjAnd B-jColumn
Between increase decorrelation loss, wherein B-jIt is basis matrix as defined above.xjLoss will be applied to that from xjProject to B-j
All column energy on.Only applying loss to these correlations will ensure that each additional step attempts only to eliminate previous steps not
The projection (or correlation) of elimination.If to xjAnd B-jBoth column apply loss, and allow gradient propagate backward to this two
A vector, then for each j, we can reduce the complexity of process and only execute it on column i, so that i < j.This allows
Single matrix B is constructed to consider all correlations, but identical correlation can suffer from repeatedly compensating.
Applied layer is innovated on network
It will be shown how that layer creative solution related constraint, which is applied to neural network, during backpropagation updates now.These
Method is suitable for any innovation costing bio disturbance strategy being presented above.
Innovation implantation is used as regularization
In training network, a kind of scheme of applied layer innovation Loss constraint is L2The form of class regularization.In backpropagation
Period, when entering layer from above layer, the loss that this layer is seen is that (full target is added from other layers from loss above
Potential regularization loss) and innovation lose scaling version summation
Wherein,Applied to above-mentioned any (polymerizeing in both example and layer neuron) total loss.System
Number λ is regularization coefficient.
As described above, if two layer neuron i and j are equal (or highly relevant) for all examples,
Loss on neuron i will push them in one direction away from each other.Loss on j can each push them to opposite
Direction.This may cause the polymerization gradient that not will push any neuron.In order to break such symmetry, difference can be used
Zoom factor λjRegularization is carried out to the constraint of each neuron.Therefore the regularization layer applied loses
If neuron is the version for the scaling being relative to each other, the λ of the ratio between gradient is offset in selection by weiWith
λjIt is still possible (although being very unlikely to), still causes similar problems as described above.In order to solve this problem, I
Can be to each λjValue applies small random perturbation.The regularization zoom factor λ of batchjIt may include that there is additional low side
The fixation mean value of difference (relative to mean value) 0- mean value normal random variable is drawn primary for each batch and each neuron j.
It is inculcated using the innovation of constrained optimization
As the substitution of regularization scheme, creative solution associated loss can be applied by Lagrangian constrained optimization.It is right
In each element (for example, for each neuron of n neuron of layer) of constraint, we add dual variable λj, and
The loss being superimposed with the loss propagated downwards in a network
Relative toIt minimizes, but relative to dual variable λjIt maximizes.We can also the increase pair in training batch
The L of mutation amount2Regularization.
It is inculcated using the innovation of class minimum description length (MDL) regularization scheme
The beneficial score of improvement of the MDL regularization of linear model using measurement feature in loss.This feature is not having
It is trained in the case where regularization, but sees the every other feature of regularization.It is used for beneficial to score by reducing its power
Carry out regularization this feature again.With it is big beneficial when, hardly scale, have it is lower beneficial to when, this feature can be with
It reduces.It can be used for refusing completely its feature beneficial to score lower than the threshold value beneficial to the threshold value μ of score.
Beneficial point of the class MDL regularization scheme for layer can be served as the mean square error that the neuron in layer calculates
Number.Using this scheme, the training neuron in back-propagation algorithm and without regularization.However, in order to execute innovation just
Then change, the neuron of estimation uses the version of the regularization of every other neuron.Then, propagated forward is by the canonical of neuron
The version of change travels on network.Backpropagation receives the gradient of the version of regularization, they are scaled back full release, so as to
Propagate backward to following layer.
βjRepresent the beneficial score of j-th of neuron of this layer.To represent the version of regularization.The first two is innovated
Calculative strategy estimates j-th of neuron according to the activation weight of the regularization of every other neuron by following formula
Complete activation weight
And (for example, example l) error is given by
The weight of the regularization of neuron is given by
Wherein ρ and ξ is parameter, and σ represents Sigmoid function
Threshold value μ can be set in we, if the β for beingj≤ μ, then we willCanonical turns to 0.It note that and have linearly
The MDL of model is different, and the lower limit of mean square error is 0, that is, we cannot have negative beneficial score.Therefore, we can pass through
Selection (height) negative value to reduce zoom factor using parameter ξ.Alternatively, Sigmoid function can be replaced by another function.
Possible alternative solution is tanh, i.e.,
Wherein ξ >=0.
For the innovation by projection, we are predicted using the weight of the regularization of other neurons without regularization
Weight.This means that version without regularization of the matrix X using neuron weight, and basis matrix B-jUse the version of regularization
This building.Lower complexity version application causes the B on the vector of regularization-jAll steps, but last projection step
The rapid column for being applied to X.As long as note that we do not set 0 for threshold vector, this is not different, because of xjIt is all
ExampleIdentical scaling will occur.If we will have βjThe threshold value of the neuron j of < μ is set as 0, then this can be by xj's
Institute is important to be set as 0.This will exclude x during Gram-Schmidtj, with the B for finding all i≤j-i.Therefore, it
The error for other neurons that potential increase can be predicted by j-th of neuron.This generates desired effects, and wherein we lose
The neuron being seldom worth to other neuron increases is abandoned, to increase the benefit of other neurons.
It enablesRepresentative travels toGradient.Then, fromIt travels toBe then attached to the gradient of following layer by
Following chain rule provides
Wherein σ ' (ρ βj+ ξ, μ) edited versions of Sigmoid are represented, (assuming that we use Sigmoid) is for βj≤ μ is cut
It collects to 0.
Beneficial to score can be all batches and present lot accumulation square error or we can be standardized
It is every batch of per exemplary average.We can also use sliding (or damped expoential) window in one group of nearest batch.Cause
For the square error that we have summed in example under any circumstance, and we to obtained mean square error using non-thread
Property operation, so polymerizeing every example square error or being not different using between average batch square error.
Initial beneficial score before training starts can be set to be some sufficiently large values for all neurons, with true
Them are protected to give the opportunity to train.However, it is possible to which some of neurons can be with prior to the random initializtion of other neurons
Break symmetry, and it is beneficial to allow some neurons to be evolved to first, keeps redundant neurons regularization.It will be for all j
Initial beneficial score be set as 0 for deconditioning because it can be any reversed to prevent by setting 0 for all neurons
It propagates.
It can be by the design beneficial to score come adjusting parameter ρ.Neuron with harmonic(-)mean or accumulation MSE (is depended on
In the design alternative of benefit score) its weight that the other assemblies of network above tegillum are seen is turned to very by canonical
Small value turns to 0 by canonical if lower than threshold value μ.This will stop updating propagating down into the mind for being connected to the neuron
Through member.However, they may will affect other nerves in current layer if these neurons begin to deviate from their current value
Member changes, and the MSE of the neuron of estimation regularization is regrowed, the neuron is caused to have again in the updating
Effect.
If neuron turns to 0 by canonical in many batches, the parameter which is presented may not become again
It obtains useful.It is beneficial that random initializtion causes the link of neuron and its deviation can be, therefore network can be able to be the mind
Parameter that is different, coming in handy is found through member.It can make other on neuron and present lot in the following manner now
Neuron is orthogonal to be initialized: the random batch value for generating neuron；It is generated using the first two strategy from other neurons
The linear MMSE estimator (or deriving estimator from projection strategy) of neuron；Batch is subtracted from the value generated at random
Each of exemplary MMSE estimated value, or using projection strategy, be applied to neuron only to find residual error；And it will
Obtained gradient travels to the link and deviation of connection.This can be carried out in several batches, thus following link adaptation in
New value.
In some embodiments, most latter two steps can by seen based on them they below activation ask
Solution meets the value of the neuron of the variable of deviation and is directly connected to the equation of the link of following neuron to replace, and distributes
These solutions are with these links of thermal starting again.
For certain layers, the relevant neuron that can be predicted from other neurons in layer indicates not increase to network pre-
The nuisance parameter of measured value.This can also include the parameter for indicating not having helpful feature to prediction.This conceptually means mind
Innovation through member can have the meaning of beneficial score.Although in some cases, such beneficial score is similar to MDL regularization
Beneficial to the effect of score, but correlation dimension is more than the capture of conventional MDL score.It also captures the correlation between feature.
For being connected to the top hidden layer of output loss, the loss using driving beneficial to score (such as makes in the linear model using MDL
Have loss) it is possible to that the noise reduction improvement of another dimension can be increased to neural network.
MDL scheme can quickly eliminate bad neuron than the regularization scheme based on gradient, should the canonical based on gradient
Change scheme require iterative gradient propagation, until neuron in otiose situation by regularization.In case of such case,
It can be preferably extensive and preferably eliminates overfitting.
As already mentioned, for by neuron weight be scaled innovate beneficial to score function Sigmoid function only
It is a kind of selection.Although saying it is reasonable for using the MDL regularization of linear model, because it will be beneficial to score conduct
Logarithm advantage indicates that this feature is beneficial to model, but herein, the concept of probability is somewhat detached from the degree of calculating
Amount.Other than already mentioned tanh, other dull scaling functions in [0,1] with target may be more preferable.
It is directly related that the beneficial score of class MDL scheme with us attempts maximized target.Target is bigger, and (neuron more has
It is innovative), benefit is bigger.By nonreversible innovation, our also not different behaviors, no matter square of our average batch
Error or target.
The additional advantage of MDL regularization driving strategy is that we will not apply loss directly on network.In addition to right
Except the regularization effect of the neuron of regularization itself, network can be trained in the case where no supplementary loss.Wound
It newly is only used for the neuron that regularization is discussed, without influencing other neurons.This can be prevented can be by non-natural regularization
The deviation of formation is (as we are in MDL regularization ratio L1As being seen in scheme).It can also be avoided as two
The innovative goals of the problem of a identical neuron describes, two identical neurons can mutually negate, because to one
Other neurons of the effect of constraint value of neuron.
Physical constraint and mini batch processed (Mini-Batching)
It all may excessively be calculated using any strategy proposed for given batch come the innovation of the neuron of computation layer close
Collection.Computation complexity is O (kn2) or it is bigger.If a layer is made of thousands of neurons, and exemplary batch is by thousands of
A composition examples, then such complexity is infeasible.In order to solve this problem, we can be with dimension in office or two
Mini batch is used in dimension.Example can by it is mini in batches for smaller batch (although in order to meet dimension requirement, these batches
The secondary quantity that should be greater than the neuron in batch).Neuron in layer can also be divided into multiple batches by us, and be calculated
The associated loss of the lesser mini batch of layer neuron.Both methods can also combine.
The layer is divided by mini batch processed neuron is forced incoherent set, and the neuron from different sets
Between correlation may possibly still be present.This is not ideal, but can be (at least partly) by existing periodically or randomly
(shuffle) mini batch subregion is shuffled between different batches to avoid.Shuffling at random, which will ensure that, fifty-fifty constrains innovation
Evenly applied in the subset of layer neuron.Mini batch processed is carried out to the relatively small subset of opposite layer neuron and is completely being criticized
Complexity will be substantially reduced by carrying out shuffling subregion between secondary, even and if make this scheme in the case where biggish layer when be also can
Capable.
Example in the neural network of set (ensemble) for including tower uses
One sample application of technique described herein is neural network, which includes the set of independent tower
(for example, from the separate network for being input to output diminution).Specifically, as an example, Fig. 2A shows the set including tower
Neural network 200, the set of the tower includes tower 202,204 and 206.Show three towers for illustrative purposes, but net
Network may include any number of independent tower.
In some embodiments, each tower 202,204,206 can receive the specific input of tower, and the specific input of the tower can be with
Carry out the more early tower specific part of automatic network 200.For example, tower 202 receives the specific input of tower from the part of network 200 252；Tower 204
The specific input of tower is received from the part of network 200 254；Tower 206 receives the specific input of tower from the part of network 200 256.It is alternative
Ground or additionally, each tower 202,204,206 can receive the shared input that can come from the more early part of network 200.For example,
Each tower 202,204 and 206 receives shared input from the part of network 200 258.
Neural network 200 includes the neuron for being organized into multiple layers.As an example, these layers include output layer 218 and all
Such as multiple hidden layers of hidden layer 214 and 216.Hidden layer 216 is shown at one group of softmax or Sigmoid and operates it
Before, and therefore can be referred to as logits layers in some cases.As shown in layer 214 and 216, in some cases, nerve
Member (for example, neuron 241 and 244 is all in layer 214, but can exist respectively in identical layer but in different towers
In independent tower 202 and 204).
The neuron or other parameters for including in some sample applications of technique described herein, in a tower can phases
For the neuron decorrelation for including in other towers.It application can be solved at hidden layer (for example, 214 or 216) or output layer 218
It is related.
In some embodiments, decorrelation technique can be executed by including innovation loss item in loss function,
The each neuron for including in a tower in innovation loss Xiang Weita is provided based on the neuron in other towers, rather than from
The penalty values of the ability of other neurons in own tower, to predict the value of such neuron.Decorrelation is made in subsequent description
For any loss for forcing neuron to be innovated on other neurons, such as, but not limited to decorrelation or previously described
What innovation loss.
As an example, neuron 241 can be with neuron 244-248 decorrelation.In addition, in some embodiments
In, decorrelation technique is not applied to from neuron 242 or 243 decorrelation neurons 241.Therefore, innovation loss can be applied to
Neuron 241, innovation loss provide for neuron 241 based on the neuron 244-248 in other towers 204,206, rather than
The penalty values of the ability of other neurons 242,243 in its own tower 202, with the value of predictive neuron 241.Innovation loss can
It is some in all neuron 241-248 to be respectively applied to.
Similarly, the neuron in layer 216 and 218 can be with other neuron solution phases in the tower of other in identical layer
It closes.As an example, neuron 262 can be with the decorrelation of neuron 261 and 263.As another example, output 283 can be with
Export 281 and 282 decorrelations.Therefore, it can be answered before (for example, at layer 214 and 214) softmax or Sigmoid operation
With decorrelation and/or decorrelation can be applied after (for example, at layer 218) softmax or Sigmoid operation.If mark
Label be it is binary, then softmax can with Sigmoid function replace.Otherwise, in some embodiments, on softmax
Each label value that decorrelation can be applied to softmax (then can be 4 for example, if there is 4 different label values
In each carry out neuron between decorrelation.It may be 3 progress in 4, wherein the 4th label softmax
Value comes from other 3).
Therefore, decorrelation can be applied on one or more different layers of network 200, which includes such as 214
Hidden layer, such as 216 logits layer, and/or such as 218 output layer.
Fig. 2 B shows another exemplary neural network 300.The network 200 of neural network 300 and Fig. 2A is closely similar, in addition to
In the network 300 of Fig. 2 B, the neuron 261,262 and 263 of tower 202,204 and 206 in the softmax for generating output 304 or
Individually shared neuron 302 is combined into before Sigmoid layer 306.According to decorrelation technique described herein, layer 214 and/or
216 neuron can be with decorrelation.Therefore, in some instances, one or more shared neurons can be combined into Jiang Ta
Decorrelation is executed before (for example, shared neuron 302).As shown in Figure 2 B, tower 202,204,206 arrives shared neuron 302
Combination can occur before softmax layer 306.In some instances, the combination of neuron can be applied to summation, put down
, Mixture of expert (gated mixture of experts) or other forms are gated.
Fig. 2 C shows another exemplary neural network 400.The network 300 of neural network 400 and Fig. 2 B is closely similar, in addition to
In the network 400 of Fig. 2 C, the neuron 261,262 and 263 of tower 202,204 and 206 is combined after softmax layer 404
At single shared output 402.Therefore, as shown in Figure 2 C, the combination that tower 202,204,206 arrives shared neuron 402 can be
Occur after softmax layer 404.
It, according to another aspect, during the training period, can be initial by the input value for being embedded into network 200 referring again to Fig. 2A
Turn to value than 0.For example, nonzero value can will be initialized as to the input value of part 252,254,256 and/or 258.
Specifically, in some embodiments, input value can be initialized as random value.Input value, which is initialized as non-zero, to be helped
In decorrelation technique described herein because they can provide can increased decorrelation during the training period primary quantity.?
In some embodiments, value embedded can be initialized as nonzero value nonrandomly or transmit as from previously trained model
Value.
Example apparatus and system
Figure 1A depicts the block diagram of exemplary computing system 100 according to an example embodiment of the present disclosure.System 100 includes logical
Cross the communicatively coupled user calculating equipment 102 of network 180, server computing systems 130 and training computing system 150.
User calculating equipment 102 can be any kind of calculating equipment, and such as, personal computing devices are (for example, knee
Laptop or desktop computer), mobile computing device (for example, smart phone or tablet computer), game console or control
Device, wearable computing devices, the calculating equipment of embedding assembly equipment or any other type.
User calculating equipment 102 includes one or more processors 112 and memory 114.One or more processors 112
It can be any suitable processing equipment (for example, processor core, microprocessor, ASIC, FPGA, controller, microcontroller etc.),
And it can be a processor or the multiple processors being operably connected.Memory 114 may include one or more non-
Temporary computer readable storage medium, RAM, ROM, EEPROM, EPROM, flash memory device, disk etc. and
Their combination.Memory 114 can store data 116 and be run by processor 112 so that user calculating equipment 102 executes behaviour
The instruction 118 of work.
In some embodiments, user calculating equipment 102 can store or including one or more machine learning models
120.For example, machine learning model 120 can be or can otherwise include various machine learning models, such as nerve net
Network (for example, deep neural network) or other kinds of machine learning model, including nonlinear model and/or linear model.Mind
It may include feedforward neural network, recurrent neural network (for example, long-term short-term memory recurrent neural network), convolution mind through network
Neural network through network or other forms.
In some embodiments, one or more machine learning models 120 can be calculated by network 180 from server
System 130 receives, and is stored in user calculating equipment memory 114, then by the use of one or more processors 112 or with it
He implements mode.In some embodiments, the multiple of individual machine learning model 120 can be implemented in user calculating equipment 102
Parallel instances.
Alternatively or additionally, one or more machine learning models 140 can be included in server computing systems
It otherwise stores and implements in 130 or by server computing systems 130, server computing systems 130 are according to client-clothes
Business device relationship is communicated with user calculating equipment 102.For example, machine learning model 140 can be implemented by server computing systems 140
For a part of web services.Therefore, can store and implement at user calculating equipment 102 one or more models 120 and/
Or one or more models 140 can be stored and implemented at server computing systems 130.
User calculating equipment 102 can also include the one or more user's input modules 122 for receiving user's input.Example
Such as, user's input module 122 can be the sensitive sensitive component of the touch to user's input object (for example, finger or stylus)
(for example, touch-sensitive display panel or touch tablet).Sensitive component can be used for implementing dummy keyboard.Other example user input module packets
Include other devices that microphone, conventional keyboard or user can input provided by user.
Server computing systems 130 include one or more processors 132 and memory 134.One or more processors
132 can be any suitable processing equipment (for example, processor core, microprocessor, ASIC, FPGA, controller, microcontroller
Deng), and the multiple processors that can be a processor or be operably connected.Memory 134 may include one or more
A non-transitory computer-readable storage media, RAM, ROM, EEPROM, EPROM, flash memory device, disk etc.,
And their combination.Memory 134 can store data 136 and be run by processor 132 so that server computing systems 130
Execute the instruction 138 of operation.
In some embodiments, server computing systems 130 are including one or more server computing devices or by one
A or multiple server computing devices are otherwise implemented.It include that the calculating of multiple servers is set in server computing systems 130
In the case where standby, such server computing device can be according to sequence computing architecture, parallel computation framework or they certain
Combination is to operate.
As described above, server computing systems 130 can store or otherwise include one or more machine learning
Model 140.For example, model 140 can be or can otherwise include various machine learning models.Example machine learns mould
Type includes neural network or other multilayered nonlinear models.Exemplary neural network include feedforward neural network, deep neural network,
Recurrent neural network and convolutional neural networks.
User calculating equipment 102 and/or server computing systems 130 can be communicatively coupled via with by network 180
The interaction of training computing system 150 carry out training pattern 120 and/or 140.Training computing system 150 can be calculated with server
System 130 separates, or can be a part of server computing systems 130.
Training computing system 150 includes one or more processors 152 and memory 154.One or more processors 152
It can be any suitable processing equipment (for example, processor core, microprocessor, ASIC, FPGA, controller, microcontroller etc.),
And it can be a processor or the multiple processors being operably connected.Memory 154 may include one or more non-
Temporary computer readable storage medium, RAM, ROM, EEPROM, EPROM, flash memory device, disk etc. and
Their combination.Memory 154 can store data 156 and be run by processor 152 so that training computing system 150 executes behaviour
The instruction 158 of work.In some embodiments, training computing system 150 include one or more server computing devices or by
One or more server computing devices are otherwise implemented.
Training computing system 150 may include model trainer 160, and model trainer 160 uses various training or study
Technology (such as backpropagation of error) training is stored in user calculating equipment 102 and/or server computing systems 130
Machine learning model 120 and/or 140.In some embodiments, the backpropagation for executing error may include executing at any time
Between the backpropagation that is truncated.Model trainer 160 can execute a variety of extensive technologies (for example, weight decaying, loss etc.) to change
The generalization ability of the kind model trained.
Specifically, model trainer 160 can based on one group of 162 training machine learning model 120 of training data and/or
140.Model trainer 160 can execute any decorrelation technique described herein.
In some embodiments, if agreement has been provided in user, training example can be by user calculating equipment
102 provide.Therefore, in such an embodiment, the model 120 for being supplied to user calculating equipment 102 can be calculated by training
System 150 is in the training from the received user's specific data of user calculating equipment 102.In some cases, which can be by
Referred to as personalized model.
Model trainer 160 includes for providing the computer logic of desired function.Model trainer 160 can be
It controls and implements in the hardware, firmware and/or software of general processor.For example, in some embodiments, model trainer 160
Including store on a storage device, be loaded into memory and by one or more processors run program file.At other
In embodiment, model trainer 160 include one or more groups of computers can operating instruction, it is readable to be stored in tangible computer
In storage medium, such as RAM hard disk or optically or magnetically medium.
Network 180 can be any kind of communication network, such as local area network (for example, Intranet), wide area network (for example,
Internet) or some combinations, and may include any amount of wired or wireless link.In general, the communication on network 180
Various communication protocol (for example, TCP/IP, HTTP, SMTP, FTP), coding or format (for example, HTML, XML) can be used
And/or protection scheme (for example, VPN, secure HTTP, SSL) is realized via any kind of wiredly and/or wirelessly connection.
Figure 1A shows the exemplary computing system that can be used for implementing the disclosure.Also other can be used and calculates system
System.For example, in some embodiments, user calculating equipment 102 may include model trainer 160 and training dataset 162.
In such an embodiment, model 120 can be trained to and be used locally the two at user calculating equipment 102.?
In some such embodiments, user calculating equipment 102 can be implemented model trainer 160 with based on user's specific data come
Personalized model 120.
Figure 1B depicts the block diagram of the Example Computing Device 10 executed according to an example embodiment of the present disclosure.Calculate equipment 10
It can be user calculating equipment or server computing device.
Calculating equipment 10 includes multiple applications (for example, arriving N using 1).Each application includes the machine learning library of oneself
(multiple) machine learning model.For example, each application may include machine learning model.Sample application includes that text message passes
Pass application, e-mail applications, dictation application, dummy keyboard application, browser application etc..
As shown in fig. 1b, each application can be (such as, one or more with multiple other assemblies of calculating equipment
Sensor, context manager, equipment state component and/or add-on assemble) communication.In some embodiments, each application
API (for example, public API) can be used to communicate with each apparatus assembly.In some embodiments, each using
API is specific to the application.
Fig. 1 C depicts the block diagram of the Example Computing Device 50 executed according to an example embodiment of the present disclosure.Calculate equipment 50
It can be user calculating equipment or server computing device.
Calculating equipment 50 includes multiple applications (for example, arriving N using 1).Each application is communicated with central intelligent layer.Example
Using including text message transmitting application, e-mail applications, dictation application, dummy keyboard application, browser application etc..One
In a little embodiments, each application can be used API (for example, public API across all applications) and center intelligent layer and (and deposit
Storage is in (multiple) model wherein) communication.
Central intelligent layer includes multiple machine learning models.For example, as shown in Figure 1 C, can be provided accordingly for each application
Machine learning model (for example, model) and by central intelligent layer-management.In other embodiments, two or more applications can
To share individual machine learning model.For example, in some embodiments, central intelligent layer can provide individually for all applications
Model (for example, single model).In some embodiments, central intelligent layer is included in the operating system for calculating equipment 50
Or otherwise implemented by the operating system of calculating equipment 50.
Central intelligent layer can be communicated with central equipment data Layer.Central equipment data Layer can be the collection for calculating equipment 50
Middle data repository.As shown in Figure 1 C, central equipment data Layer can be with multiple other assemblies (such as one of calculating equipment
A or multiple sensors, context manager, equipment state component and/or add-on assemble) communication.In some embodiments,
Central equipment data Layer can be used API (for example, privately owned API) and communicate with each apparatus assembly.
Additional disclosure
Technical Reference server, database, software application and other computer based systems for being discussed herein and this
The movement and the information for being sent to these systems and being sent from these systems that a little systems are taken.Computer based system is intrinsic
Flexibility allow component between it is various it is possible configuration, combination and task and function division.For example, the mistake being discussed herein
Multiple equipment or the component of individual equipment or component or work in combination can be used to implement in journey.Database and application can be in lists
Implement or can be distributed across multiple systems in a system.Distributed component can sequence or parallel work-flow.
Although this theme is described in detail about the various specific example embodiments of this theme, each example is
By explaining rather than limit the disclosure to provide.After obtaining to the understanding of foregoing teachings, those skilled in the art can be with
Easily generate change, variation and the equivalent to these embodiments.Therefore, this theme disclosure is not precluded to this theme
These modification, variation and/or it is increased include that this will be apparent to practitioners skilled in this.For example,
The feature that a part as one embodiment shows or describes can be used together with another embodiment, to generate another
Embodiment.Therefore, the disclosure is intended to cover these changes, variation and equivalent.
Claims (24)
1. a kind of for training the method implemented by computer of neural network, which comprises
By one or more data for calculating equipment and obtaining description neural network, the neural network includes multilayer neuron；With
And
Equipment is calculated by neural network backpropagation loss function to train neural network, wherein the damage by one or more
It loses function and describes performance of the neural network relative to one group of training example, and equipment is wherein calculated by one or more and is led to
Crossing neural network backpropagation loss function includes, for every in one or more layers in multiple layers of the neural network
A layer:
Gradient of the loss function relative to the neuron for including in layer is determined by one or more of calculating equipment,
In, at least for the layer, the loss function includes innovation loss item, and the innovation loss item is include one in the layer
Each of a or multiple neurons neuron provides the ability based on one or more of the layer other neurons
Penalty values, to predict the value of such neuron；And
The gradient of the loss function including the innovation loss item is based at least partially on by one or more of calculating equipment
Come the neuron for modifying the neuron for including in the layer to include in layer described in decorrelation.
2. the method implemented by computer according to claim 1, wherein at least one of one or more of layers layer
The corresponding portion of two or more independent towers including the neural network, and wherein the innovation loss item is the tower
In a tower in include each neuron, other based on the neuron in other towers rather than in its own tower are provided
The penalty values of the ability of neuron, to predict the value of such neuron.
3. the method implemented by computer according to claim 1, wherein one or more layers of the neural network include
One or more hidden layers of the neural network.
4. the method implemented by computer according to claim 1, wherein for the one or more mind for including in the layer
Through each of member neuron, the innovation loss item pushes the gradient of such neuron to wrap in said layer to increase it
The innovation on other one or more neurons included.
5. the method implemented by computer according to claim 1, wherein for the one or more mind for including in the layer
Through each of member neuron, the penalty values that are provided by the innovation loss item with one by including in the layer or
Other multiple neurons predict that the error of the value of such neuron is negatively correlated.
6. the method implemented by computer according to claim 5, wherein for the one or more mind for including in the layer
Through each of member neuron, the value of such neuron is predicted by other the one or more neurons for including in the layer
Error include error associated with Linear Minimum Mean-Square Error Estimation device.
7. the method implemented by computer according to claim 6, wherein for the one or more mind for including in the layer
Through each of member neuron, the Linear Minimum Mean-Square Error Estimation device includes being averaged of calculating in a collection of training example
Batch error.
8. the method implemented by computer according to claim 6, wherein for the one or more mind for including in the layer
Through each of member neuron, the Linear Minimum Mean-Square Error Estimation device includes the phase directly according to the estimation in the layer
The expected mean square error that closing property calculates.
9. the method implemented by computer according to claim 6, wherein for each of one or more of layers
Layer, the innovation loss item are based at least partially on the linear minimum mean square estimation of all neurons for including in the layer
Device is averaged to provide the penalty values.
10. the method implemented by computer according to claim 6, wherein for the one or more for including in the layer
Each of neuron neuron, the innovation loss item is relative to the linear minimum mean-squared error for such neuron
The square root of estimator is decayed.
11. the method implemented by computer according to claim 5, wherein for the one or more for including in the layer
Each of neuron neuron, the error that the innovation loss item is based at least partially on the prediction are inverted to provide
The penalty values.
12. the method implemented by computer according to claim 5, wherein for the one or more for including in the layer
Each of neuron neuron predicts such neuron by other the one or more neurons for including in the layer
The error of value is asked by solving relative to the linear least-squares recurrence for other the one or more neurons for including in the layer
Topic is to determine.
13. the method implemented by computer according to claim 5, wherein for the one or more for including in the layer
Each of neuron neuron predicts such neuron by other the one or more neurons for including in the layer
The error of value includes that gradient decline returns estimator.
14. the method implemented by computer according to claim 5, wherein for the one or more for including in the layer
Each of neuron neuron predicts such neuron by other the one or more neurons for including in the layer
The error of value on neuron example vector by executing parallel Gram-Schmidt orthogonalization and calculating the residual of last vector
Difference determines.
15. the method implemented by computer according to claim 5, wherein for the one or more for including in the layer
Each of neuron neuron predicts such neuron by other the one or more neurons for including in the layer
The error of value includes projecting to the column vector for indicating the value of the neuron by one including one or more basis vectors
Or the error spatially that multiple basis matrixes are crossed over.
16. the method implemented by computer according to claim 5, wherein for the one or more for including in the layer
Each of neuron neuron, the basis vector crossed over by other the one or more neurons for including from the layer
To predict that the squared norm of projection of the value of such neuron includes for inculcate the supplementary loss innovated.
17. the method implemented by computer according to claim 1, wherein the one or more neurons for including in the layer
Including all neurons for including in the layer.
18. the method implemented by computer according to claim 1, wherein innovation loss item include according to scaling because
Son is added to the regularization term of main loss.
19. the method implemented by computer according to claim 18, wherein the regularization term includes L2 regularization term.
20. the method implemented by computer according to claim 1, wherein for each of one or more of layers
Layer, the loss function include the every neuron regularization zoom factor for preventing from carrying out neuron symmetric constraints.
21. the method implemented by computer according to claim 1, wherein at least by one or more of calculating equipment
The gradient of the loss function is based in part on to modify the neuron for including in the layer and include, by one or more of meters
It calculates equipment and applies the innovation loss item by Lagrangian constrained optimization.
22. the method implemented by computer according to claim 1, wherein for each of one or more of layers
Layer, item is lost in the application innovation after the activation for the neuron for including in said layer.
23. a kind of for training the method implemented by computer of neural network, which comprises
By one or more data for calculating equipment and obtaining description neural network, the neural network includes multilayer neuron；With
And
For each of one or more neurons for including in one or more layers in multiple layers neuron, by one
Or multiple calculating equipment are based at least partially on one or more such minds of other neurons prediction by including in identical layer
The error of value through member is determined beneficial to score；And
By one or more calculate equipment be based at least partially at least one such neuron is determined it is described beneficial
Score modifies weight associated at least one neuron in one or more of neurons.
24. a kind of for training the method implemented by computer of neural network, which comprises
By one or more data for calculating equipment and obtaining description neural network, the neural network includes passing through multiple chains respectively
The multiple neurons connect in succession；And
Equipment is calculated by neural network backpropagation loss function to train neural network, wherein losing letter by one or more
Performance of the number description neural network relative to one group of training example, and wherein pass through mind by one or more of calculating equipment
Include through network backpropagation loss function, for each in one or more neurons of neural network, link or deviation
It is a:
Determine loss function relative to one or more neurons of neural network, link by one or more of calculating equipment
Or the gradient of deviation, wherein at least for one or more neurons, link or the deviation of neural network, the loss function
Item is lost including innovation, the innovation loss item provides for each of one or more neurons, link or deviation to be based on
The penalty values of the ability of other one or more neurons, link or deviation, to predict such neuron, link or deviation
Value；And
The gradient of the loss function including the innovation loss item is based at least partially on by one or more of calculating equipment
One or more neurons, link or the deviation of neural network are modified, with the one or more of neural network described in decorrelation
Neuron, link or deviation.
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201862660617P | 2018-04-20 | 2018-04-20 | |
US62/660,617 | 2018-04-20 |
Publications (1)
Publication Number | Publication Date |
---|---|
CN110084368A true CN110084368A (en) | 2019-08-02 |
Family
ID=67415975
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201910324199.6A Pending CN110084368A (en) | 2018-04-20 | 2019-04-22 | System and method for regularization neural network |
Country Status (2)
Country | Link |
---|---|
US (1) | US11436496B2 (en) |
CN (1) | CN110084368A (en) |
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN111640425A (en) * | 2020-05-22 | 2020-09-08 | 北京百度网讯科技有限公司 | Model training and intention recognition method, device, equipment and storage medium |
WO2021147365A1 (en) * | 2020-01-23 | 2021-07-29 | 华为技术有限公司 | Image processing model training method and device |
Families Citing this family (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11681923B2 (en) * | 2019-04-19 | 2023-06-20 | Samsung Electronics Co., Ltd. | Multi-model structures for classification and intent determination |
US20210133571A1 (en) * | 2019-11-05 | 2021-05-06 | California Institute Of Technology | Systems and Methods for Training Neural Networks |
KR102235588B1 (en) * | 2019-12-09 | 2021-04-02 | 한국로봇융합연구원 | Apparatus and method of the inference classification performance evaluation for each layer of artificial intelligence model composed of multiple layers |
US20230244949A1 (en) * | 2020-02-24 | 2023-08-03 | The Board Of Regents Of The University Of Texas System | Methods and systems to train neural networks |
CN111429005B (en) * | 2020-03-24 | 2023-06-02 | 淮南师范学院 | Teaching evaluation method based on small amount of student feedback |
US11836600B2 (en) * | 2020-08-20 | 2023-12-05 | D5Ai Llc | Targeted incremental growth with continual learning in deep neural networks |
US20220180175A1 (en) * | 2020-12-08 | 2022-06-09 | Oxford University Innovation Limited | Optical neural network |
CN112862095B (en) * | 2021-02-02 | 2023-09-29 | 浙江大华技术股份有限公司 | Self-distillation learning method and device based on feature analysis and readable storage medium |
CN112951349A (en) * | 2021-03-24 | 2021-06-11 | 湖州槐坎南方水泥有限公司 | Method for predicting strength of portland cement clinker based on DNN neural network |
US20230267285A1 (en) * | 2022-02-07 | 2023-08-24 | Nvidia Corporation | Using one or more neural networks to perform text translation |
CN116738120B (en) * | 2023-08-11 | 2023-11-03 | 齐鲁工业大学(山东省科学院) | Copper grade SCN modeling algorithm for X fluorescence grade analyzer |
Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20160321522A1 (en) * | 2015-04-30 | 2016-11-03 | Canon Kabushiki Kaisha | Devices, systems, and methods for pairwise multi-task feature learning |
US20170024642A1 (en) * | 2015-03-13 | 2017-01-26 | Deep Genomics Incorporated | System and method for training neural networks |
CN106485324A (en) * | 2016-10-09 | 2017-03-08 | 成都快眼科技有限公司 | A kind of convolutional neural networks optimization method |
US20170161640A1 (en) * | 2015-12-04 | 2017-06-08 | Google Inc. | Regularization of machine learning models |
Family Cites Families (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9489373B2 (en) * | 2013-07-12 | 2016-11-08 | Microsoft Technology Licensing, Llc | Interactive segment extraction in computer-human interactive learning |
US11042796B2 (en) * | 2016-11-03 | 2021-06-22 | Salesforce.Com, Inc. | Training a joint many-task neural network model using successive regularization |
US11315019B2 (en) * | 2017-11-15 | 2022-04-26 | Google Llc | Learning neural network structure |
US20190278600A1 (en) * | 2018-03-09 | 2019-09-12 | Nvidia Corporation | Tiled compressed sparse matrix format |
US11429862B2 (en) * | 2018-03-20 | 2022-08-30 | Sri International | Dynamic adaptation of deep neural networks |
-
2019
- 2019-04-18 US US16/388,430 patent/US11436496B2/en active Active
- 2019-04-22 CN CN201910324199.6A patent/CN110084368A/en active Pending
Patent Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20170024642A1 (en) * | 2015-03-13 | 2017-01-26 | Deep Genomics Incorporated | System and method for training neural networks |
US20160321522A1 (en) * | 2015-04-30 | 2016-11-03 | Canon Kabushiki Kaisha | Devices, systems, and methods for pairwise multi-task feature learning |
US20170161640A1 (en) * | 2015-12-04 | 2017-06-08 | Google Inc. | Regularization of machine learning models |
CN106485324A (en) * | 2016-10-09 | 2017-03-08 | 成都快眼科技有限公司 | A kind of convolutional neural networks optimization method |
Non-Patent Citations (1)
Title |
---|
董军著: ""心迹"的计算 隐性知识的人工智能途径", 31 December 2016, 上海：上海科学技术出版社, pages: 168 - 184 * |
Cited By (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2021147365A1 (en) * | 2020-01-23 | 2021-07-29 | 华为技术有限公司 | Image processing model training method and device |
CN111640425A (en) * | 2020-05-22 | 2020-09-08 | 北京百度网讯科技有限公司 | Model training and intention recognition method, device, equipment and storage medium |
CN111640425B (en) * | 2020-05-22 | 2023-08-15 | 北京百度网讯科技有限公司 | Model training and intention recognition method, device, equipment and storage medium |
Also Published As
Publication number | Publication date |
---|---|
US20190325313A1 (en) | 2019-10-24 |
US11436496B2 (en) | 2022-09-06 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN110084368A (en) | System and method for regularization neural network | |
Nakagawa et al. | Graph-based knowledge tracing: modeling student proficiency using graph neural network | |
Hinton | Where do features come from? | |
Das et al. | Artificial neural network trained by particle swarm optimization for non-linear channel equalization | |
Oentaryo et al. | Online probabilistic learning for fuzzy inference system | |
Kumar et al. | Performance evaluation of metaheuristics algorithms for workload prediction in cloud environment | |
Crone | Stepwise selection of artificial neural network models for time series prediction | |
Adhikari et al. | Combining multiple time series models through a robust weighted mechanism | |
Al-hnaity et al. | Predicting financial time series data using hybrid model | |
CN113050931A (en) | Symbolic network link prediction method based on graph attention machine mechanism | |
Bagarello et al. | A model of adaptive decision-making from representation of information environment by quantum fields | |
Heylighen et al. | Foundations for a Mathematical Model of the Global Brain: architecture, components, and specifications | |
Clempner | A game theory model for manipulation based on Machiavellianism: Moral and ethical behavior | |
Erdogan et al. | Forecasting Euro and Turkish Lira Exchange Rates with Artificial Neural Networks (ANN) | |
Osoba et al. | Causal modeling with feedback fuzzy cognitive maps | |
Ororbia II et al. | Learning to adapt by minimizing discrepancy | |
Zhou et al. | Towards real time team optimization | |
Jin et al. | Federated Learning: Fundamentals and Advances | |
Gashler et al. | Missing value imputation with unsupervised backpropagation | |
de la Fuente et al. | A framework for hybrid dynamic evolutionary algorithms: multiple offspring sampling (MOS) | |
Hattori et al. | Associative memory for intelligent control | |
Han et al. | A new approach for function approximation incorporating adaptive particle swarm optimization and a priori information | |
Elsayed et al. | Utility-based perturbed gradient descent: An optimizer for continual learning | |
Ororbia et al. | Large-scale gradient-free deep learning with recursive local representation alignment | |
Huang et al. | Optimizer amalgamation |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |