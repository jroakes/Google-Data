US20170102915A1 - Automatic batch voice commands - Google Patents
Automatic batch voice commands Download PDFInfo
- Publication number
- US20170102915A1 US20170102915A1 US14/882,353 US201514882353A US2017102915A1 US 20170102915 A1 US20170102915 A1 US 20170102915A1 US 201514882353 A US201514882353 A US 201514882353A US 2017102915 A1 US2017102915 A1 US 2017102915A1
- Authority
- US
- United States
- Prior art keywords
- user
- services
- computing device
- input data
- intended task
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
- G06F3/167—Audio in a user interface, e.g. using voice commands for navigating, audio feedback
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/957—Browsing optimisation, e.g. caching or content distillation
- G06F16/9574—Browsing optimisation, e.g. caching or content distillation of access to content, e.g. by caching
-
- G06F17/30902—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06Q—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES; SYSTEMS OR METHODS SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES, NOT OTHERWISE PROVIDED FOR
- G06Q10/00—Administration; Management
- G06Q10/02—Reservations, e.g. for tickets, services or events
- G06Q10/025—Coordination of plural reservations, e.g. plural trip segments, transportation combined with accommodation
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/226—Procedures used during a speech recognition process, e.g. man-machine dialogue using non-speech characteristics
- G10L2015/228—Procedures used during a speech recognition process, e.g. man-machine dialogue using non-speech characteristics of application context
Definitions
- Voice input may be used to access a feature of a computing device.
- aspects of the subject technology relate to a computer-implemented method for utilizing voice input for initiating batch processes in a computing device.
- the method includes receiving voice input data from a computing device.
- the method further includes determining an intended task based on the received voice input data.
- the method further includes Obtaining contextual information related to the intended task.
- the method further includes determining a plurality of services to be accessed at the computing device based on the intended task and the obtained contextual information.
- the method further includes providing instructions associated with the plurality of services for transmission to the computing device and for execution at the computing device.
- the system includes one or more processors and a non-transitory computer-readable medium including instructions stored therein, which, when processed by the one or more processors, cause the one or more processors to perform operations.
- the operations include receiving voice input data from a computing device.
- the operations also include determining an intended task based on the received voice input data.
- the operations also include obtaining contextual information related to the intended task, where the contextual information comprises social graph information.
- the operations also include determining a plurality of services to be accessed at the computing device based on the intended task and the obtained contextual information.
- the operations also include providing instructions associated with the plurality of services for transmission to the computing device and for execution at the computing device.
- aspects of the subject technology also relates to a non-transitory machine-readable medium including instructions stored therein, which when executed by a machine, cause the machine to perform operations.
- the operations include receiving voice input data from a computing device.
- the operations also include determining an intended task based on the received voice input data.
- the operations also include obtaining contextual information related to the intended task.
- the operations also include determining a plurality of tabs to open based on the intended task and the obtained contextual information.
- the operations also include providing for opening the determined plurality of tabs.
- FIG. 1 illustrates an example network environment in accordance with various aspects of the subject technology.
- FIG. 2 shows a flowchart illustrating an example process 200 for processing voice commands, in accordance with various aspects of the subject technology.
- FIG. 3 shows an example interface in accordance with various aspects of the subject technology.
- FIG. 4 conceptually illustrates an example electronic system with which some implementations of the subject technology can be implemented.
- the subject technology enables a user to utilize voice input to initiate batch processes on a computing device.
- Batch processes can involve accessing one or more services on the computing device. Accessing one or more services may be associated with using features of web services or applications on the computing device.
- the computing device may be associated with a user account.
- the user account may be a cloud-based user account that is used to access various web services. Examples of web services include email, social network, operating system, web based applications (e.g., text editor, spreadsheet application, presentation application), among others. Access to the web services can be granted through authentication of user account credentials. User authentication may be initiated by signing into the user account through, for example, a web portal, a web application, application log-in page, among others.
- Voice input data may be received from a computing device.
- voice input data may be processed through speech recognition to determine one or more words corresponding to the voice input data and stored as text.
- An intended task may be determined based on the received voice input data.
- Contextual information related to the intended task may be obtained.
- Contextual information may include social graph information and user historical activity.
- a plurality of services to be accessed on the computing device may be determined based on the intended task and the contextual information. The plurality of services may be associated with applications or websites. Instructions associated with the plurality of services are provided for transmission to the computing device and for execution at the computing device.
- FIG. 1 illustrates an example network environment 100 in which voice commands may be utilized for accessing a plurality of services at a computing device.
- the network environment 100 may include one or more computing devices 102 , 104 and 106 , network 108 , and server 110 .
- Each of the computing devices 102 , 104 and 106 , and server 110 can communicate with each other through a network 108 .
- Server 110 can include one or more computing devices 112 and one or more data stores 114 .
- Computing devices 102 , 104 and 106 can represent various forms of processing devices.
- processing devices can include a desktop computer, a laptop computer, a handheld computer, a personal digital assistant (PDA), a cellular telephone, a network appliance, a camera, a smart phone, an enhanced general packet radio service (EGPRS) mobile phone, a media player, a navigation device, an email device, a game console, smart appliances or a combination of any of these data processing devices or other data processing devices.
- PDA personal digital assistant
- GPRS enhanced general packet radio service
- Some computing devices, such as computing devices 102 , 104 and 106 may have the capabilities to process user voice input.
- computing devices 102 , 104 and 106 may include microphones, and may have instructions stored in memory, which when executed by their respective processors, allow computing devices 102 , 104 and 106 to record the user voice commands.
- computing devices 102 , 104 and 106 may include processing circuitry for speech recognition and voice recognition.
- Computing device 102 , 104 and 106 may also include a speaker or an audio output connection.
- computing devices 102 , 104 and 106 may be associated with an online or cloud-based user account.
- the user account may be a cloud-based user account that is used to access various web services. Examples of web services include email, social network, operating system, web based applications (e.g., text editor, spreadsheet application, presentation application), among others. Access to the web services can be granted through authentication of user account credentials. User authentication may be initiated by signing into the user account through, for example, a web portal, a web application, application log-in page, among others.
- Information stored in connection with the user account may be located in the data store 114 associated with the server 110 . In some aspects, information stored in connection with the user account may be located on a separate server (not pictured).
- the server 110 may be any system or device having a processor, a memory, and communications capability for exchanging data with other computing devices, including for example, computing devices 102 , 104 and 106 .
- the server 110 may utilize credential information associated with the cloud-based user account to access various web services associated with the cloud-based user account.
- the server 110 can be a single computing device (e.g., computing device 112 ).
- the server 110 can represent more than one computing device working together to perform the actions of a computer server (e.g., server farm).
- the server 110 can represent various forms of servers including, but not limited to, a web server, an application server, a proxy server, a network server, or a server farm.
- the server 110 may process a voice input data to generate instructions to be sent to a client device (e.g., computing device 102 , 104 or 106 ).
- the server 110 may receive a voice input data.
- the voice input data may be a raw audio recording of a voice input from a user captured at the client device.
- the voice input data may comprise additional data associated with the voice input captured at the client device such as the location of the client device and the time when the voice input was captured.
- the server 110 may determine an intended task.
- the server 110 can obtain contextual information based on the intended task.
- the contextual information may be retrieved from data store 114 and may also be associated with the user account.
- the server 110 may determine a plurality of services to be accessed at the client device.
- the server 110 may provide instructions associated with the plurality of services for transmission to the computing device and for execution at the client device.
- the computing devices may communicate wirelessly through a communication interface (not shown), which may include digital signal processing circuitry where necessary.
- the communication interface may provide for communications under various modes or protocols, for example, Global System for Mobile communication (GSM) voice calls, Short Message Service (SMS), Enhanced Messaging Service (EMS) or Multimedia Messaging Service (MMS) messaging, Code Division Multiple Access (CDMA), Time Division Multiple Access (TDMA), Personal Digital Cellular (PDC), Wideband Code Division Multiple Access (WCDMA), CDMA2000, or General Packet Radio System (GPRS), among others.
- GSM Global System for Mobile communication
- SMS Short Message Service
- EMS Enhanced Messaging Service
- MMS Multimedia Messaging Service
- CDMA Code Division Multiple Access
- TDMA Time Division Multiple Access
- PDC Personal Digital Cellular
- WCDMA Wideband Code Division Multiple Access
- CDMA2000 Code Division Multiple Access 2000
- GPRS General Packet Radio System
- network environment 100 can be a distributed client/server system that spans one or more networks such as, for example, network 108 .
- Network 108 can be a large computer network such as, for example, a local area network (LAN), wide area network (WAN), the Internet, a cellular network, or a combination thereof connecting any number of mobile clients, fixed clients, and servers.
- the network 108 can include, but is not limited to, any one or more of the following network topologies, including a bus network, a star network, a ring network, a mesh network, a star-bus network, tree or hierarchical network, and the like.
- communication between each client (e.g., computing devices 102 , 104 and 106 ) and server (e.g., server 110 ) can occur via a virtual private network (VPN), Secure Shell (SSH) tunnel, or other secure network connection.
- VPN virtual private network
- SSH Secure Shell
- network 108 may further include a corporate network (e.g., intranet) and one or more wireless access points.
- FIG. 2 shows a flowchart illustrating an example process 200 for processing voice commands, in accordance with various aspects of the subject technology.
- the steps of the process 200 do not need to be performed in the order shown. It is understood that the depicted order is an illustration of one or more example approaches, and the subject technology is not meant to be limited to the specific order or hierarchy presented. The steps can be rearranged, and/or two or more of the steps can be performed simultaneously. FIG. 2 will be discussed with reference to FIG. 3 .
- voice input data is received from a computing device (e.g., computing device 102 ).
- the voice input data may include, for example, a raw audio file recorded at the computing device 102 , processed words based on the raw audio file, a location of the computing device 102 , timestamp, among others.
- a user may speak a voice input for performing a task near the computing device 102 .
- the computing device 102 may be always listening for a voice input and may detect the voice input through a microphone of the computing device 102 . For example, the user may wish to plan a trip, and may state “I want to plan a trip” near the computing device 102 .
- the computing device 102 may record the voice input (e.g., “I want to plan a trip”) to create a raw audio file and send the voice input data comprising the raw audio file to the server 110 .
- the server 110 may process the raw audio file to determine the words corresponding to the audio through, for example, the use of speech recognition.
- the computing device 102 may create the raw audio file of the voice input and may process the raw audio file to determine corresponding words.
- the determined words may be stored as text and the computing device 102 may send the voice input data to the server 110 , where the voice input data comprises text corresponding to the determined words.
- the computing device 102 may be associated with a cloud-based user account.
- the user account may be associated with web services such as email, social network, web based applications, among others.
- the computing device 102 may have access to these services when a user is signed into the user account.
- the computing device 102 may only process the voice input when user intent to use a voice command is detected. In some cases, computing device 102 may not detect voice input, until user intent to use a voice command is detected. User intent to use a voice command may be detected through user input through a button on the computing device 102 or a user element displayed on the computing device 102 . In other cases, the computing device 102 may be always listening for voice input. The computing device 102 may detect the voice input through a microphone of the computing device 102 and may compare the voice input with a trigger phrase. The comparison may be done on computing device 102 . Alternatively, the detected voice input may be sent to server 110 and the server 110 may compare the detected voice input with the trigger phrase.
- the trigger phrase may indicate that the user is intending to use a voice command.
- a trigger phrase may be predetermined by the user and may include a name of the computing device 102 , such as, “Okay Device.” For example, a user may wish to search vacation spots and may state “Okay device, search vacations spots near me” near computing device 102 .
- the computing device 102 may detect, through its microphone, the phrase “Okay Device.”
- the computing device 102 may determine that the trigger phrase has been detected and begin recording the rest of the voice input, for example, “search vacation spots near me.”
- the voice input data comprising a recording of the voice input may be sent to the server 110 from the computing device 102 .
- an intended task is determined based on the voice input data.
- the server 110 may determine that the voice input data is associated with the user through voice recognition before proceeding to the rest of the process 200 . For example, the server 110 may access a sample voice recording of the user associated with the user account, and compare the sample voice recording with the voice input data to determine that the voice input data is associated with the user. The server 110 may ignore voice input data that is not associated with the user.
- the server 110 may determine the intended task by comparing the received voice input data to a plurality of tasks available to the user.
- the voice input data may include a raw audio file of the voice input recorded at the computing device 102 .
- the server 110 may process the raw audio file to determine corresponding words.
- the plurality of tasks available to the user may be stored in an indexed table.
- a comparison of the received voice data to a plurality of tasks available to the user may be done through searching the indexed table for each of the determined words.
- the search may be a linear search, a binary search, or any other search methods known in the art.
- a confidence value may be calculated based on how close an entry in the indexed table is to the determined words.
- the confidence value may indicate how close an entry of the indexed table is to the determined words and may be based on, for example, the number of the determined words that matches portions of an entry in the index table.
- the server 110 may determine that the intended task corresponds to a task that matches the determined words. If no task in the plurality of tasks available to the user matches all of the determined words, then a task in the indexed table with the highest confidence value may be determined to be the intended task.
- the server 110 may have access to a predetermined list of supported tasks from, for example, the data store 114 .
- the list of supported tasks may comprise tasks that the server 110 may recognize and have corresponding instructions to be sent to the computing device 102 .
- Each of the supported tasks may be associated with one or more services. In some cases, one or more tasks in the list of supported tasks may not be available to the computing device 102 , because, for example, the computing device 102 may not have access to certain features or applications.
- the plurality of tasks available to the user may be determined by comparing the list of supported tasks to the plurality of services available to the user on the computing device 102 .
- the plurality of services available to the user on the computing device 102 may comprise services that the computing device 102 is capable of accessing through an application installed on the computing device 102 or through web services associated with the user account.
- Server 110 may access tasks available to the user through web services associated with the user account.
- a task available to the user may relate to opening a cloud-based text editor on the computing device 102 that may allow the user to view and edit text documents.
- the server 110 may access a list of services available to the user in association with the cloud-based text editor, which may include, for example, opening a document, sharing a document, or formatting a document with a previously saved template, among others.
- the user may have one or more applications installed on the computing device 102 that are not associated with the user account.
- the one or more applications may communicate information associated with their capabilities to the server 110 via an application programming interface and the server 110 may store the information in data store 114 in association with the user account.
- a task available to a user may relate to opening a browser and navigating to one or more websites in different tabs or windows.
- the user may wish to plan a trip, and may state “I want to plan a trip” near the computing device 102 .”
- the server 110 may receive the voice input data and process the raw audio file to determine the words through, for example, the use of speech recognition. Through a comparison of the plurality of tasks available to the user and one or more determined words, the server 110 may determine that the intended task is planning a trip.
- the server 110 may search an indexed table comprising the plurality of tasks available to the user for each of the one or more determined words and determine a confidence value of each of the plurality of tasks.
- the server 110 may determine that the intended task may be an entry in the indexed table with the highest confidence value.
- the intended task may be associated with a plurality of services to be accessed.
- the task related to planning a trip may be associated with a flight booking website, hotel reservation website and a general search website.
- Computing device 102 may access these websites through opening tabs for each of the websites or starting applications that are associated with each of the websites.
- contextual information related to the intended task may be obtained.
- the contextual information may be any information associated with the intended tasks.
- contextual information may be obtained from the user.
- the server 110 may determine that additional information may be needed from the user to complete the intended tasks.
- the server 110 may provide for transmission to the computing device 102 a request for information associated with the intended task.
- the computing device 102 may display the request for information.
- the computing device 102 may provide an audio feedback (e.g., through a speaker or audio output connection) corresponding to the request for information.
- the audio feedback may be provided using text-to-speech algorithms on the request for information.
- the user may respond to the request for information by a second voice input or other user inputs on the computing device 102 .
- Other user inputs may include pressing or touching a user interface element corresponding to an answer to the request for information.
- the computing device 102 may send the user input data to the server 110 .
- the user may wish to plan a trip, and may state “I want to plan a trip” near the computing device 102 .”
- the server 110 may determine that the intended task is planning a trip.
- the server 110 may further determine that additional information may be required to perform the task of planning a trip. This may be determined based on user historical activity.
- the server 110 may have access to previous actions taken by the user after receiving the voice input.
- the server 110 may provide for transmission to the computing device 102 a request for information associated with the intended task.
- the computing device 102 may provide audio feedback corresponding to a follow-up question regarding the desired destination of the trip by using text-to-speech algorithms.
- the user of computing device 102 may respond to the follow-up question by stating “I want to go to Paris.”
- a user input data comprising information related to the user response may be captured by the computing device 102 .
- Computing device 102 may send the user input data to the server 110 .
- the contextual information may be social graph information, a user historical activity, among others.
- Social graph information may be information associated with a graph that depicts personal relationship among users in a social networking service.
- the server 110 may access the social graph information from the social networking service associated with the user account. Examples of social graph information include contact information, degrees of contact, or information related to behavior of other users in the user's social graph.
- Contact information associated with a user may be information necessary to contact the user. For example, contact information may include cellphone numbers, email addresses, user identification for other communication services, among others. Information associated with degrees of contact may indicate how close one user is to another user in a social graph.
- the social graph information may be related to the intended task when at least one service associated with the intended task requires the social graph information.
- the plurality of services associated with the intended task may comprise communicating with one or more users of the social network using contact information.
- the server 110 may search for the second user's contact information. The user may wish to work on a project with a second user, and may state “I want to work on an essay with Bob,” near the computing device 102 .
- the server 110 may receive the voice input data associated with a first voice input, “I want to work on an essay with Bob,” and determine that the intended task is to “work on an essay with [Contact].”
- the server 110 may access the social graph information associated with the user and search for Bob's contact information, Bob's contact information may be used to start a communication between the user and Bob via, for example, a messaging service.
- Bob's contact information may be used to start a new document on a cloud-based text editor that may be editable by Bob.
- User historical activity may be past user activity associated with the intended task and may be associated with a user account.
- the server 110 may store the types of services previously accessed and the order in which the services were previously accessed in association with an intended task.
- the server 110 may determine that the services currently being accessed are associated with an intended task based on the name of the service or content provided by service and store the user activity in association with the intended task. For example, a user may be browsing the internet to plan a trip. The user may first access a flight booking website. The user may book a flight and then visit a hotel reservation website to reserve a room. The specific websites that the user accessed and the order in which the websites were accessed may be stored in association with the intended task.
- a plurality of services to be accessed at the computing device 102 may be determined based on the intended task and the obtained contextual information.
- the server 110 may have access to a list of plurality of tasks available to the user on the computing device 102 .
- the list of plurality of tasks available to the user may be created from accessing information regarding features of the web services associated with the user account or may be obtained from applications installed on the computing device 102 through, for example, an application programming interface (API).
- API application programming interface
- the server 110 may select an intended task among the list of plurality of tasks based on methods described above.
- the intended task may be associated with a predefined plurality of services to be accessed.
- the server 110 may use the contextual information to customize the plurality of services to be accessed. For example, the server 110 may use the contextual information to initiate a feature of a service, to select a subset of the plurality of services associated with the intended tasks, to determine the order in which the plurality of services are accessed and displayed, among others.
- the contextual information may be used to initiate a feature of a service. Some services may require some information to be initiated and contextual information may be utilized to supply the required information. As mentioned above, some tasks may require social graph information. For instance, when the intended task involves interacting with a second user, one service associated with the intended task may be a video call with the second user.
- the server 110 may determine from contextual information the contact information of the second user. The server 110 may send instructions associated with a video call to the second user to the computing device 102 for execution at the computing device 102 .
- the server 110 may rank the plurality of services associated with the intended task based on the user historical activity and the social graph information.
- the server 110 may rank the plurality of services based on a calculated score for each of the plurality of services associated with the intended task.
- the score may be determined through a weighted value of the user historical activity and social graph information. On example calculation may be in the format provided below:
- each of the f n is a factor associated with either the user historical activity or the social graph information and each of the w n is a weight assigned to the factors.
- the factors associated with historical activity may be previous activity by the user associated with the intended task. Examples of previous activity by the user may be, for example, the number of times a service was accessed, the number of times a service provided by server 110 was not used by the user, or the sequence of services accessed for a given task.
- the server 110 may have a counter that is incremented each time a service is accessed. The number associated with the counter may be used as a value of one of the factors.
- the server 110 may keep track of the number of times the user opts to use a different service instead of the one provided by the server 110 .
- the factors associated with social graph information may be the number of times the user's contacts in the social graph posts about a certain service. In some cases a greater weight would be given to posts shared by contacts more closely related to the user.
- the degree of connection may be determined by a type of connection between the user and a contact, or a number of shared connections between the user and the contact.
- the server 110 may select a subset the plurality of services associated with the intended task.
- the intended task may be to view the latest news.
- the server 110 may obtain contextual information related to the intended task, such as most popular news sites.
- the server 110 may access user historical activity comprising the most frequently visited user websites related to the news.
- the server 110 may receive social graph information which may include news articles shared by people in the user's social graph. Social graph information may further comprise information regarding degrees of contact.
- the intended task may be associated with a plurality of services to be performed at the computing device 102 .
- the intended task may be associated with a plurality of news websites to open.
- the server 110 may determine a subset of the plurality of news websites based on various factors, such as the frequency of user visits to the news websites and whether the news websites have been shared by contacts in the user's social graph. News websites that are shared by contacts closer to the user (contacts who have a greater number of shared connections with the user in the social network) or shared more frequently may be given greater weight.
- the server 110 may select a predetermined number of websites to open.
- the server 110 may determine the order in which to display or access the plurality of services on the computing device by ranking the plurality of services based on the contextual information.
- the ranking may be determined based on the user historical activity, where the user historical activity may include information on a sequence of services being accessed in association with an intended task.
- the services may be displayed in the order of their respective ranks.
- the services may be categorized into a plurality of types of services, and the server 110 may determine that the services with the highest rank in each of the plurality of types of services are displayed.
- the intended task may be to plan a trip.
- the server 110 may Obtain contextual information related to planning a trip. This may involve accessing user historical activity. In particular, the server 110 may determine the most frequently accessed trip planning websites.
- the server 110 may determine that the plurality of services to be accessed at the computing device 102 is associated with three different types of websites.
- Server 102 may rank all of the websites associated with the intended task and select the highest ranked website among the first type of websites, the highest ranked website among the second type of websites, and the highest ranked websites among the third type of websites, Computing device 102 may access these three websites by opening three different tabs, where each of the three tabs is accessing a website. Based on user historical activity, the server 110 may determine the order and the number of tabs to be opened. In some cases, the server 110 may determine the placement of tabs to be displayed.
- the first service may be associated with a flight booking website
- the second service may be associated with a hotel reservation website
- third service may be associated with a search result page based on a search query related to the destination.
- the order of the tabs displayed on the user interface may correspond to a sequence of past user activity related to the intended task.
- FIG. 3 shows an example interface that may be displayed as a result of instructions provided by the server 110 in block 225 . More specifically, FIG. 3 shows an interface 305 as displayed on a screen of the computing device 102 .
- the interface 305 includes tabs 310 , 315 , and 320 .
- the computing device 102 may display interface 305 with tabs 310 , 315 and 320 .
- Instructions associated with the plurality of services to be accessed may comprise instructions for opening a plurality of tabs, where each of the plurality of tabs accesses a website associated with a respective one of the plurality of services.
- tab 310 may display a flight booking website
- tab 315 may display a hotel booking website
- tab 320 may be general information about the destination generated from a search query.
- the specific websites and the order in which the tabs are presented may be based on user historical activity and social graph information.
- accessing the plurality of services is not limited to opening tabs.
- accessing the plurality of services may be associated with opening one or more new browser windows or opening one or more applications installed on the computing device 102
- the instructions associated with plurality of services to be accessed may comprise instructions for opening a plurality of applications, where each of the plurality of applications corresponds to a respective one of the plurality of services.
- the process 200 may be associated with a user changing the user's credit card information.
- the user may need to update the user's credit card information on one or more websites due to, for example, a change in the expiration dates.
- the user may state “I want to change my credit card information” near the computing device 102 .
- the computing device 102 may capture a voice input as described above and send a voice input data to the server 110 .
- the server 110 may receive the voice input data and in block 210 , the server 110 may determine the intended task.
- the intended task may be to update the user's credit card information and may be determined by comparing the voice input data to the plurality of tasks available to the user as described above.
- the server 110 may obtain contextual information related to the intended task.
- the server 110 may obtain user historical activity.
- the user historical activity may include the user's past browsing history as well as user accounts for shopping websites.
- the server 110 may determine the plurality of services to be accessed on computing device 105 based on the intended task and the obtained contextual information.
- the server 110 may determine a plurality of websites to be accessed based on the intended task and the contextual information, where each of the tabs may correspond to a website that may include the user's credit card information.
- the server 110 may provide the instructions associated with the plurality of services for transmission to the computing device and for execution at the computing device 102 .
- the computing device 102 may display the interface 305 with tabs 310 , 315 and 320 , where each of the tabs corresponding to a page associated with changing the credit card information. Through the process, the user may quickly access multiple websites to update the user's credit card.
- the server 110 may receive user feedback data from the computing device 102 .
- User feedback data may comprise any user interaction with the computing device 102 that is detected within a predetermined time threshold after the server 110 provided the instructions to the computing device 102 .
- the server 110 may determine that the user did not intend to access the plurality of services when the user accesses a different service. If the computing device 102 does not carry out a task intended by the user in response to a voice input, then the user may interact with the computing device 102 shortly after the instructions associated with the plurality of services are provided for transmission to the computing device 102 .
- the voice input data, the intended task, the user feedback data and the determination that the plurality of services was not intended by the user may be stored in association with the user historical activity. This information may be accessed next time a voice input data is received from the computing device 102 .
- the intended task may be to plan a trip.
- the server 110 may provide instructions associated with a first plurality of services for transmission to the computing device 102 and for execution at the computing device 102 .
- the first service may be associated with a first flight booking website
- the second service may be associated with a hotel reservation website
- third service may be associated with a search result page based on a search query related to the destination.
- the server 110 may detect that computing device 102 is accessing a different service, for example a second flight booking website. Based on this information, the server may determine that the user did not intend to access the first flight booking website.
- the server 110 may store information associated with the voice input data, the intended task, the user feedback data and the determination that the plurality of services was not intended by the user. Next time the server determines that the intended task is planning a trip, the server 110 may access the stored information to determine a second plurality of services to be accessed at the computing device, where the second plurality of services includes the second flight booking website.
- FIG. 4 conceptually illustrates an example electronic system with which some implementations of the subject technology can be implemented.
- Electronic system 400 can be a computer, phone, FDA, or any other sort of electronic device. Such an electronic system includes various types of computer readable media and interfaces for various other types of computer readable media.
- Electronic system 400 includes a bus 408 , processing unit(s) 412 , a system memory 404 , a read-only memory (ROM) 410 , a permanent storage device 402 , an input device interface 414 , an output device interface 406 , and a network interface 416 .
- processing unit(s) 412 includes a system memory 404 , a read-only memory (ROM) 410 , a permanent storage device 402 , an input device interface 414 , an output device interface 406 , and a network interface 416 .
- ROM read-only memory
- Bus 408 collectively represents all system, peripheral, and chipset buses that communicatively connect the numerous internal devices of electronic system 400 .
- bus 408 communicatively connects processing unit(s) 412 with ROM 410 , system memory 404 , and permanent storage device 402 .
- processing unit(s) 412 retrieves instructions to execute and data to process in order to execute the processes of the subject disclosure.
- the processing unit(s) can be a single processor or a multi-core processor in different implementations.
- ROM 410 stores static data and instructions that are needed by processing unit(s) 412 and other modules of the electronic system.
- Permanent storage device 402 is a read-and-write memory device. This device is a non-volatile memory unit that stores instructions and data even when electronic system 400 is off.
- a mass-storage device for example, a magnetic or optical disk and its corresponding disk drive
- system memory 404 is a read-and-write memory device. However, unlike storage device 402 , system memory 404 is a volatile read-and-write memory, such as a random access memory. System memory 404 stores some of the instructions and data that the processor needs at runtime. In some implementations, the processes of the subject disclosure are stored in system memory 404 , permanent storage device 402 , or ROM 410 .
- the various memory units include instructions fir displaying web pages, processing user entries to the web pages, and generating URLs, in accordance with some implementations. From these various memory units, processing unit(s) 412 retrieves instructions to execute and data to process in order to execute the processes of some implementations.
- Bus 408 also connects to input and output device interfaces 414 and 406 .
- Input device interface 414 enables the user to communicate information and select commands to the electronic system.
- Input devices used with input device interface 414 include, for example, alphanumeric keyboards and pointing devices (also called “cursor control devices”).
- Output device interfaces 406 enables, for example, the display of images generated by the electronic system 400 .
- Output devices used with output device interface 406 include, for example, printers and display devices, for example, cathode ray tubes (CRT) or liquid crystal displays (LCD). Some implementations include devices, for example, a touchscreen that functions as both input and output devices.
- CTR cathode ray tubes
- LCD liquid crystal displays
- bus 408 also couples electronic system 400 to a network. (not shown) through a network interface 416 .
- the computer can be a part of a network of computers (for example, a local area network (LAN), a wide area network (WAN), or an Intranet, or a network of networks, for example, the Internet. Any or all components of electronic system 400 can be used in conjunction with the subject disclosure.
- Computer readable storage medium also referred to as computer readable medium.
- processing unit(s) e.g., one or more processors, cores of processors, or other processing units
- processing unit(s) e.g., one or more processors, cores of processors, or other processing units
- Examples of computer readable media include, but are not limited to, CD-ROMs, flash drives, RAM chips, hard drives, EPROMs, etc.
- the computer readable media does not include carrier waves and electronic signals passing wirelessly or over wired connections.
- the term “software” is meant to include firmware residing in read-only memory or applications stored in magnetic storage, which can be read into memory for processing by a processor.
- multiple software aspects of the subject disclosure can be implemented as sub-parts of a larger program while remaining distinct software aspects of the subject disclosure.
- multiple software aspects can also be implemented as separate programs.
- any combination of separate programs that together implement a software aspect described here is within the scope of the subject disclosure.
- the software programs when installed to operate on one or more electronic systems, define one or more specific machine implementations that execute and perform the operations of the software programs.
- a computer program (also known as a program, software, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be deployed in any form, including as a standalone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment.
- a computer program may, but need not, correspond to a file in a file system.
- a program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code).
- a computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- Some implementations include electronic components, for example, microprocessors, storage, and memory that store computer program instructions in a machine-readable or computer-readable medium (alternatively referred to as computer-readable storage media, machine-readable media, or machine-readable storage media).
- computer-readable media include RAM, ROM, read-only compact discs (CD-ROM), recordable compact discs (CD-R), rewritable compact discs (CD-RW), read-only digital versatile discs (e.g., DVD-ROM, dual-layer DVD-ROM), a variety of recordable/rewritable DVDs (e.g., DVD-RAM, DVD-RW, DVD+RW, etc.), flash memory (e.g., SD cards, mini-SD cards, micro-SD cards, etc.), magnetic or solid state hard drives, read-only and recordable Blu-Ray® discs, ultra-density optical discs, any other optical or magnetic media, and floppy disks.
- CD-ROM compact discs
- CD-R recordable compact
- the computer-readable media can store a computer program that is executable by at least one processing unit and includes sets of instructions for performing various operations.
- Examples of computer programs or computer code include machine code, for example, is produced by a compiler, and files including higher-level code that are executed by a computer, an electronic component, or a microprocessor using an interpreter.
- ASICs application specific integrated circuits
- FPGAs field programmable gate arrays
- integrated circuits execute instructions that are stored on the circuit itself.
- the terms “computer”, “server”, “processor”, and “memory” all refer to electronic or other technological devices. These terms exclude people or groups of people.
- display or displaying means displaying on an electronic device.
- computer readable medium and “computer readable media” are entirely restricted to tangible, physical objects that store information in a form that is readable by a computer. These terms exclude any wireless signals, wired download signals, and any other ephemeral signals.
- implementations of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a cathode ray tube (CRT) or liquid crystal display (LCD) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a cathode ray tube (CRT) or liquid crystal display (LCD) monitor
- keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.
- a computer can interact with a user by sending documents to and receiving documents from a device that
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface or a web browser through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back end, middleware, or front end components.
- the components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network.
- Examples of communication networks include a local area network (LAN) and a wide area network (WAN), an inter-network (e.g., the Internet), and peer-to-peer networks (e.g., ad hoc peer-t)-peer networks).
- LAN local area network
- WAN wide area network
- inter-network e.g., the Internet
- peer-to-peer networks e.g., ad hoc peer-t
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- a server transmits data (e.g., an HTML page) to a client device (e.g., for purposes of displaying data to and receiving user input from a user interacting with the client device).
- client device e.g., for purposes of displaying data to and receiving user input from a user interacting with the client device.
- Data generated at the client device e.g., a result of the user interaction
- any specific order or hierarchy of steps in the processes disclosed is an illustration of example approaches. Based upon design preferences, it is understood that the specific order or hierarchy of steps in the processes may be rearranged, or that all illustrated steps be performed. Some of the steps may be performed simultaneously. For example, in certain circumstances, multitasking and parallel processing may be advantageous. Moreover, the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
- a phrase such as an “aspect” does not imply that such aspect is essential to the subject technology or that such aspect applies to all configurations of the subject technology.
- a disclosure relating to an aspect may apply to all configurations, or one or more configurations.
- a phrase such as an aspect may refer to one or more aspects and vice versa.
- a phrase such as a “configuration” does not imply that such configuration is essential to the subject technology or that such configuration applies to all configurations of the subject technology.
- a disclosure relating to a configuration may apply to all configurations, or one or more configurations.
- a phrase such as a configuration may refer to one or more configurations and vice versa.
Abstract
Description
- As computing devices become more advanced, users may be able to carry out more complex tasks. In some cases, it is more convenient to interact with computing devices without the use of traditional input/output peripherals like a keyboard. One such method is using voice input. Voice input may be used to access a feature of a computing device.
- Aspects of the subject technology relate to a computer-implemented method for utilizing voice input for initiating batch processes in a computing device. The method includes receiving voice input data from a computing device. The method further includes determining an intended task based on the received voice input data. The method further includes Obtaining contextual information related to the intended task. The method further includes determining a plurality of services to be accessed at the computing device based on the intended task and the obtained contextual information. The method further includes providing instructions associated with the plurality of services for transmission to the computing device and for execution at the computing device.
- Aspects of the subject technology also relates to a system. The system includes one or more processors and a non-transitory computer-readable medium including instructions stored therein, which, when processed by the one or more processors, cause the one or more processors to perform operations. The operations include receiving voice input data from a computing device. The operations also include determining an intended task based on the received voice input data. The operations also include obtaining contextual information related to the intended task, where the contextual information comprises social graph information. The operations also include determining a plurality of services to be accessed at the computing device based on the intended task and the obtained contextual information. The operations also include providing instructions associated with the plurality of services for transmission to the computing device and for execution at the computing device.
- Aspects of the subject technology also relates to a non-transitory machine-readable medium including instructions stored therein, which when executed by a machine, cause the machine to perform operations. The operations include receiving voice input data from a computing device. The operations also include determining an intended task based on the received voice input data. The operations also include obtaining contextual information related to the intended task. The operations also include determining a plurality of tabs to open based on the intended task and the obtained contextual information. The operations also include providing for opening the determined plurality of tabs.
- It is understood that other configurations of the subject technology will become readily apparent to those skilled in the art from the following detailed description, where various configurations of the subject technology are shown and described by way of illustration. As will be realized, the subject technology is capable of other and different configurations and its several details are capable of modification in various other respects, all without departing from the scope of the subject technology. Accordingly, the drawings and detailed description are to be regarded as illustrative in nature and not as restrictive.
- The accompanying drawings, which are included to provide further understanding and are incorporated in and constitute a part of this specification, illustrate disclosed aspects and together with the description serve to explain the principles of the disclosed aspects.
-
FIG. 1 illustrates an example network environment in accordance with various aspects of the subject technology. -
FIG. 2 shows a flowchart illustrating anexample process 200 for processing voice commands, in accordance with various aspects of the subject technology. -
FIG. 3 shows an example interface in accordance with various aspects of the subject technology. -
FIG. 4 conceptually illustrates an example electronic system with which some implementations of the subject technology can be implemented. - The detailed description set forth below is intended as a description of various configurations of the subject technology and is not intended to represent the only configurations in which the subject technology may be practiced. The appended drawings are incorporated herein and constitute a part of the detailed description. The detailed description includes specific details for the purpose of providing a thorough understanding of the subject technology. However, the subject technology is not limited to the specific details set forth herein and may be practiced without these specific details. In some instances, structures and components are shown in block diagram form in order to avoid obscuring the concepts of the subject technology.
- The subject technology enables a user to utilize voice input to initiate batch processes on a computing device. Batch processes can involve accessing one or more services on the computing device. Accessing one or more services may be associated with using features of web services or applications on the computing device. According to various aspects, the computing device may be associated with a user account. The user account may be a cloud-based user account that is used to access various web services. Examples of web services include email, social network, operating system, web based applications (e.g., text editor, spreadsheet application, presentation application), among others. Access to the web services can be granted through authentication of user account credentials. User authentication may be initiated by signing into the user account through, for example, a web portal, a web application, application log-in page, among others.
- Voice input data may be received from a computing device. In some embodiments, voice input data may be processed through speech recognition to determine one or more words corresponding to the voice input data and stored as text. An intended task may be determined based on the received voice input data. Contextual information related to the intended task may be obtained. Contextual information may include social graph information and user historical activity. A plurality of services to be accessed on the computing device may be determined based on the intended task and the contextual information. The plurality of services may be associated with applications or websites. Instructions associated with the plurality of services are provided for transmission to the computing device and for execution at the computing device.
-
FIG. 1 illustrates anexample network environment 100 in which voice commands may be utilized for accessing a plurality of services at a computing device. Thenetwork environment 100 may include one ormore computing devices network 108, andserver 110. Each of thecomputing devices server 110 can communicate with each other through anetwork 108.Server 110 can include one ormore computing devices 112 and one ormore data stores 114. -
Computing devices computing devices computing devices computing devices computing devices Computing device - According to various implementations,
computing devices data store 114 associated with theserver 110. In some aspects, information stored in connection with the user account may be located on a separate server (not pictured). - The
server 110 may be any system or device having a processor, a memory, and communications capability for exchanging data with other computing devices, including for example,computing devices server 110 may utilize credential information associated with the cloud-based user account to access various web services associated with the cloud-based user account. In one more implementations, theserver 110 can be a single computing device (e.g., computing device 112). In other implementations, theserver 110 can represent more than one computing device working together to perform the actions of a computer server (e.g., server farm). Further, theserver 110 can represent various forms of servers including, but not limited to, a web server, an application server, a proxy server, a network server, or a server farm. - In example aspects, the
server 110 may process a voice input data to generate instructions to be sent to a client device (e.g.,computing device server 110 may receive a voice input data. The voice input data may be a raw audio recording of a voice input from a user captured at the client device. In some implementations, the voice input data may comprise additional data associated with the voice input captured at the client device such as the location of the client device and the time when the voice input was captured. Based on the voice input data, theserver 110 may determine an intended task. Theserver 110 can obtain contextual information based on the intended task. The contextual information may be retrieved fromdata store 114 and may also be associated with the user account. Based on the intended task and the obtained contextual information, theserver 110 may determine a plurality of services to be accessed at the client device. Theserver 110 may provide instructions associated with the plurality of services for transmission to the computing device and for execution at the client device. - The computing devices, including
computing devices server 110, may communicate wirelessly through a communication interface (not shown), which may include digital signal processing circuitry where necessary. The communication interface may provide for communications under various modes or protocols, for example, Global System for Mobile communication (GSM) voice calls, Short Message Service (SMS), Enhanced Messaging Service (EMS) or Multimedia Messaging Service (MMS) messaging, Code Division Multiple Access (CDMA), Time Division Multiple Access (TDMA), Personal Digital Cellular (PDC), Wideband Code Division Multiple Access (WCDMA), CDMA2000, or General Packet Radio System (GPRS), among others. For example, the communication may occur through a radio-frequency transceiver (not shown), In addition, short-range communication may occur, for example, using a Bluetooth, WiFi, or other such transceiver. - In some aspects,
network environment 100 can be a distributed client/server system that spans one or more networks such as, for example,network 108.Network 108 can be a large computer network such as, for example, a local area network (LAN), wide area network (WAN), the Internet, a cellular network, or a combination thereof connecting any number of mobile clients, fixed clients, and servers. Further, thenetwork 108 can include, but is not limited to, any one or more of the following network topologies, including a bus network, a star network, a ring network, a mesh network, a star-bus network, tree or hierarchical network, and the like. In some aspects, communication between each client (e.g.,computing devices network 108 may further include a corporate network (e.g., intranet) and one or more wireless access points. -
FIG. 2 shows a flowchart illustrating anexample process 200 for processing voice commands, in accordance with various aspects of the subject technology. The steps of theprocess 200 do not need to be performed in the order shown. It is understood that the depicted order is an illustration of one or more example approaches, and the subject technology is not meant to be limited to the specific order or hierarchy presented. The steps can be rearranged, and/or two or more of the steps can be performed simultaneously.FIG. 2 will be discussed with reference toFIG. 3 . - In
block 205 ofFIG. 2 , voice input data is received from a computing device (e.g., computing device 102). The voice input data may include, for example, a raw audio file recorded at thecomputing device 102, processed words based on the raw audio file, a location of thecomputing device 102, timestamp, among others. A user may speak a voice input for performing a task near thecomputing device 102. Thecomputing device 102 may be always listening for a voice input and may detect the voice input through a microphone of thecomputing device 102. For example, the user may wish to plan a trip, and may state “I want to plan a trip” near thecomputing device 102. Thecomputing device 102 may record the voice input (e.g., “I want to plan a trip”) to create a raw audio file and send the voice input data comprising the raw audio file to theserver 110. Theserver 110 may process the raw audio file to determine the words corresponding to the audio through, for example, the use of speech recognition. In some aspects, thecomputing device 102 may create the raw audio file of the voice input and may process the raw audio file to determine corresponding words. The determined words may be stored as text and thecomputing device 102 may send the voice input data to theserver 110, where the voice input data comprises text corresponding to the determined words. - In some cases, the
computing device 102 may be associated with a cloud-based user account. As mentioned previously, the user account may be associated with web services such as email, social network, web based applications, among others. Thecomputing device 102 may have access to these services when a user is signed into the user account. - In some implementations, the
computing device 102 may only process the voice input when user intent to use a voice command is detected. In some cases,computing device 102 may not detect voice input, until user intent to use a voice command is detected. User intent to use a voice command may be detected through user input through a button on thecomputing device 102 or a user element displayed on thecomputing device 102. In other cases, thecomputing device 102 may be always listening for voice input. Thecomputing device 102 may detect the voice input through a microphone of thecomputing device 102 and may compare the voice input with a trigger phrase. The comparison may be done oncomputing device 102. Alternatively, the detected voice input may be sent toserver 110 and theserver 110 may compare the detected voice input with the trigger phrase. The trigger phrase may indicate that the user is intending to use a voice command. A trigger phrase may be predetermined by the user and may include a name of thecomputing device 102, such as, “Okay Device.” For example, a user may wish to search vacation spots and may state “Okay device, search vacations spots near me” nearcomputing device 102. Thecomputing device 102 may detect, through its microphone, the phrase “Okay Device.” Thecomputing device 102 may determine that the trigger phrase has been detected and begin recording the rest of the voice input, for example, “search vacation spots near me.” The voice input data comprising a recording of the voice input may be sent to theserver 110 from thecomputing device 102. - In
block 210 ofFIG. 2 , an intended task is determined based on the voice input data. In some implementations, theserver 110 may determine that the voice input data is associated with the user through voice recognition before proceeding to the rest of theprocess 200. For example, theserver 110 may access a sample voice recording of the user associated with the user account, and compare the sample voice recording with the voice input data to determine that the voice input data is associated with the user. Theserver 110 may ignore voice input data that is not associated with the user. - The
server 110 may determine the intended task by comparing the received voice input data to a plurality of tasks available to the user. As mentioned previously, the voice input data may include a raw audio file of the voice input recorded at thecomputing device 102. Theserver 110 may process the raw audio file to determine corresponding words. The plurality of tasks available to the user may be stored in an indexed table. A comparison of the received voice data to a plurality of tasks available to the user may be done through searching the indexed table for each of the determined words. The search may be a linear search, a binary search, or any other search methods known in the art. In some cases, a confidence value may be calculated based on how close an entry in the indexed table is to the determined words. The confidence value may indicate how close an entry of the indexed table is to the determined words and may be based on, for example, the number of the determined words that matches portions of an entry in the index table. Theserver 110 may determine that the intended task corresponds to a task that matches the determined words. If no task in the plurality of tasks available to the user matches all of the determined words, then a task in the indexed table with the highest confidence value may be determined to be the intended task. - The
server 110 may have access to a predetermined list of supported tasks from, for example, thedata store 114. The list of supported tasks may comprise tasks that theserver 110 may recognize and have corresponding instructions to be sent to thecomputing device 102. Each of the supported tasks may be associated with one or more services. In some cases, one or more tasks in the list of supported tasks may not be available to thecomputing device 102, because, for example, thecomputing device 102 may not have access to certain features or applications. The plurality of tasks available to the user may be determined by comparing the list of supported tasks to the plurality of services available to the user on thecomputing device 102. The plurality of services available to the user on thecomputing device 102 may comprise services that thecomputing device 102 is capable of accessing through an application installed on thecomputing device 102 or through web services associated with the user account. -
Server 110 may access tasks available to the user through web services associated with the user account. For example, a task available to the user may relate to opening a cloud-based text editor on thecomputing device 102 that may allow the user to view and edit text documents. Theserver 110 may access a list of services available to the user in association with the cloud-based text editor, which may include, for example, opening a document, sharing a document, or formatting a document with a previously saved template, among others. In some implementations, the user may have one or more applications installed on thecomputing device 102 that are not associated with the user account. The one or more applications may communicate information associated with their capabilities to theserver 110 via an application programming interface and theserver 110 may store the information indata store 114 in association with the user account. - In another example, a task available to a user may relate to opening a browser and navigating to one or more websites in different tabs or windows. The user may wish to plan a trip, and may state “I want to plan a trip” near the
computing device 102.” As discussed above, theserver 110 may receive the voice input data and process the raw audio file to determine the words through, for example, the use of speech recognition. Through a comparison of the plurality of tasks available to the user and one or more determined words, theserver 110 may determine that the intended task is planning a trip. As mentioned previously, theserver 110 may search an indexed table comprising the plurality of tasks available to the user for each of the one or more determined words and determine a confidence value of each of the plurality of tasks. Theserver 110 may determine that the intended task may be an entry in the indexed table with the highest confidence value. The intended task may be associated with a plurality of services to be accessed. For example, the task related to planning a trip may be associated with a flight booking website, hotel reservation website and a general search website.Computing device 102 may access these websites through opening tabs for each of the websites or starting applications that are associated with each of the websites. - In
block 215 ofFIG. 2 , contextual information related to the intended task may be obtained. The contextual information may be any information associated with the intended tasks. In some cases, contextual information may be obtained from the user. Theserver 110 may determine that additional information may be needed from the user to complete the intended tasks. Theserver 110 may provide for transmission to the computing device 102 a request for information associated with the intended task. Thecomputing device 102 may display the request for information. Alternatively, thecomputing device 102 may provide an audio feedback (e.g., through a speaker or audio output connection) corresponding to the request for information. The audio feedback may be provided using text-to-speech algorithms on the request for information. The user may respond to the request for information by a second voice input or other user inputs on thecomputing device 102. Other user inputs may include pressing or touching a user interface element corresponding to an answer to the request for information. Thecomputing device 102 may send the user input data to theserver 110. - For example, the user may wish to plan a trip, and may state “I want to plan a trip” near the
computing device 102.” As described above, theserver 110 may determine that the intended task is planning a trip. Theserver 110 may further determine that additional information may be required to perform the task of planning a trip. This may be determined based on user historical activity. In some aspects, theserver 110 may have access to previous actions taken by the user after receiving the voice input. Theserver 110 may provide for transmission to the computing device 102 a request for information associated with the intended task. Thecomputing device 102 may provide audio feedback corresponding to a follow-up question regarding the desired destination of the trip by using text-to-speech algorithms. The user ofcomputing device 102 may respond to the follow-up question by stating “I want to go to Paris.” A user input data comprising information related to the user response may be captured by thecomputing device 102.Computing device 102 may send the user input data to theserver 110. - In some aspects, the contextual information may be social graph information, a user historical activity, among others. Social graph information may be information associated with a graph that depicts personal relationship among users in a social networking service. The
server 110 may access the social graph information from the social networking service associated with the user account. Examples of social graph information include contact information, degrees of contact, or information related to behavior of other users in the user's social graph. Contact information associated with a user may be information necessary to contact the user. For example, contact information may include cellphone numbers, email addresses, user identification for other communication services, among others. Information associated with degrees of contact may indicate how close one user is to another user in a social graph. - The social graph information may be related to the intended task when at least one service associated with the intended task requires the social graph information. The plurality of services associated with the intended task may comprise communicating with one or more users of the social network using contact information. As an example, when the intended task involves interacting with a second user, the
server 110 may search for the second user's contact information. The user may wish to work on a project with a second user, and may state “I want to work on an essay with Bob,” near thecomputing device 102. Theserver 110 may receive the voice input data associated with a first voice input, “I want to work on an essay with Bob,” and determine that the intended task is to “work on an essay with [Contact].” Theserver 110 may access the social graph information associated with the user and search for Bob's contact information, Bob's contact information may be used to start a communication between the user and Bob via, for example, a messaging service. In addition, Bob's contact information may be used to start a new document on a cloud-based text editor that may be editable by Bob. - User historical activity may be past user activity associated with the intended task and may be associated with a user account. The
server 110 may store the types of services previously accessed and the order in which the services were previously accessed in association with an intended task. Theserver 110 may determine that the services currently being accessed are associated with an intended task based on the name of the service or content provided by service and store the user activity in association with the intended task. For example, a user may be browsing the internet to plan a trip. The user may first access a flight booking website. The user may book a flight and then visit a hotel reservation website to reserve a room. The specific websites that the user accessed and the order in which the websites were accessed may be stored in association with the intended task. - In
block 220, a plurality of services to be accessed at thecomputing device 102 may be determined based on the intended task and the obtained contextual information. As mentioned previously, theserver 110 may have access to a list of plurality of tasks available to the user on thecomputing device 102. The list of plurality of tasks available to the user may be created from accessing information regarding features of the web services associated with the user account or may be obtained from applications installed on thecomputing device 102 through, for example, an application programming interface (API). Theserver 110 may select an intended task among the list of plurality of tasks based on methods described above. The intended task may be associated with a predefined plurality of services to be accessed. Theserver 110 may use the contextual information to customize the plurality of services to be accessed. For example, theserver 110 may use the contextual information to initiate a feature of a service, to select a subset of the plurality of services associated with the intended tasks, to determine the order in which the plurality of services are accessed and displayed, among others. - The contextual information may be used to initiate a feature of a service. Some services may require some information to be initiated and contextual information may be utilized to supply the required information. As mentioned above, some tasks may require social graph information. For instance, when the intended task involves interacting with a second user, one service associated with the intended task may be a video call with the second user. The
server 110 may determine from contextual information the contact information of the second user. Theserver 110 may send instructions associated with a video call to the second user to thecomputing device 102 for execution at thecomputing device 102. - The
server 110 may rank the plurality of services associated with the intended task based on the user historical activity and the social graph information. Theserver 110 may rank the plurality of services based on a calculated score for each of the plurality of services associated with the intended task. The score may be determined through a weighted value of the user historical activity and social graph information. On example calculation may be in the format provided below: -
Calculated Score=Σi=1 n w n f n Equation(1) - where each of the fn is a factor associated with either the user historical activity or the social graph information and each of the wn is a weight assigned to the factors. The factors associated with historical activity may be previous activity by the user associated with the intended task. Examples of previous activity by the user may be, for example, the number of times a service was accessed, the number of times a service provided by
server 110 was not used by the user, or the sequence of services accessed for a given task. Theserver 110 may have a counter that is incremented each time a service is accessed. The number associated with the counter may be used as a value of one of the factors. In another example, as further discussed below, theserver 110 may keep track of the number of times the user opts to use a different service instead of the one provided by theserver 110. The factors associated with social graph information may be the number of times the user's contacts in the social graph posts about a certain service. In some cases a greater weight would be given to posts shared by contacts more closely related to the user. The degree of connection may be determined by a type of connection between the user and a contact, or a number of shared connections between the user and the contact. Although a method of calculating a relevance score based on a weighted sum is disclosed, other calculations may also be used, such as weighted product model. - Based on the calculated relevance score for each of the plurality of services associated with the intended task, the
server 110 may select a subset the plurality of services associated with the intended task. For example, the intended task may be to view the latest news. Theserver 110 may obtain contextual information related to the intended task, such as most popular news sites. Theserver 110 may access user historical activity comprising the most frequently visited user websites related to the news. In some cases, theserver 110 may receive social graph information which may include news articles shared by people in the user's social graph. Social graph information may further comprise information regarding degrees of contact. The intended task may be associated with a plurality of services to be performed at thecomputing device 102. In particular, the intended task may be associated with a plurality of news websites to open. Theserver 110 may determine a subset of the plurality of news websites based on various factors, such as the frequency of user visits to the news websites and whether the news websites have been shared by contacts in the user's social graph. News websites that are shared by contacts closer to the user (contacts who have a greater number of shared connections with the user in the social network) or shared more frequently may be given greater weight. Theserver 110 may select a predetermined number of websites to open. - In some aspects, the
server 110 may determine the order in which to display or access the plurality of services on the computing device by ranking the plurality of services based on the contextual information. The ranking may be determined based on the user historical activity, where the user historical activity may include information on a sequence of services being accessed in association with an intended task. The services may be displayed in the order of their respective ranks. In some cases, the services may be categorized into a plurality of types of services, and theserver 110 may determine that the services with the highest rank in each of the plurality of types of services are displayed. For example, the intended task may be to plan a trip. Theserver 110 may Obtain contextual information related to planning a trip. This may involve accessing user historical activity. In particular, theserver 110 may determine the most frequently accessed trip planning websites. Theserver 110 may determine that the plurality of services to be accessed at thecomputing device 102 is associated with three different types of websites.Server 102 may rank all of the websites associated with the intended task and select the highest ranked website among the first type of websites, the highest ranked website among the second type of websites, and the highest ranked websites among the third type of websites,Computing device 102 may access these three websites by opening three different tabs, where each of the three tabs is accessing a website. Based on user historical activity, theserver 110 may determine the order and the number of tabs to be opened. In some cases, theserver 110 may determine the placement of tabs to be displayed. The first service may be associated with a flight booking website, the second service may be associated with a hotel reservation website, and third service may be associated with a search result page based on a search query related to the destination. The order of the tabs displayed on the user interface may correspond to a sequence of past user activity related to the intended task. - In
block 225, instructions associated with the plurality of services are provided for transmission to thecomputing device 102 for execution at thecomputing device 102.FIG. 3 shows an example interface that may be displayed as a result of instructions provided by theserver 110 inblock 225. More specifically,FIG. 3 shows aninterface 305 as displayed on a screen of thecomputing device 102. Theinterface 305 includestabs - Based on the instructions from the
server 110, thecomputing device 102 may displayinterface 305 withtabs tab 310 may display a flight booking website,tab 315 may display a hotel booking website, andtab 320 may be general information about the destination generated from a search query. The specific websites and the order in which the tabs are presented may be based on user historical activity and social graph information. Although the plurality of services to be accessed atcomputing device 102 is described in association with opening a plurality of tabs, accessing the plurality of services is not limited to opening tabs. In some instances, accessing the plurality of services may be associated with opening one or more new browser windows or opening one or more applications installed on thecomputing device 102, The instructions associated with plurality of services to be accessed may comprise instructions for opening a plurality of applications, where each of the plurality of applications corresponds to a respective one of the plurality of services. - In one or more implementations, the
process 200 may be associated with a user changing the user's credit card information. For example, the user may need to update the user's credit card information on one or more websites due to, for example, a change in the expiration dates. The user may state “I want to change my credit card information” near thecomputing device 102. Thecomputing device 102 may capture a voice input as described above and send a voice input data to theserver 110. Inblock 205, theserver 110 may receive the voice input data and inblock 210, theserver 110 may determine the intended task. The intended task may be to update the user's credit card information and may be determined by comparing the voice input data to the plurality of tasks available to the user as described above. Inblock 215, theserver 110 may obtain contextual information related to the intended task. In this example, theserver 110 may obtain user historical activity. The user historical activity may include the user's past browsing history as well as user accounts for shopping websites. Inblock 220, theserver 110 may determine the plurality of services to be accessed on computing device 105 based on the intended task and the obtained contextual information. For example, theserver 110 may determine a plurality of websites to be accessed based on the intended task and the contextual information, where each of the tabs may correspond to a website that may include the user's credit card information. Inblock 225, theserver 110 may provide the instructions associated with the plurality of services for transmission to the computing device and for execution at thecomputing device 102. Thecomputing device 102 may display theinterface 305 withtabs - In some aspects, the
server 110 may receive user feedback data from thecomputing device 102. User feedback data may comprise any user interaction with thecomputing device 102 that is detected within a predetermined time threshold after theserver 110 provided the instructions to thecomputing device 102. Theserver 110 may determine that the user did not intend to access the plurality of services when the user accesses a different service. If thecomputing device 102 does not carry out a task intended by the user in response to a voice input, then the user may interact with thecomputing device 102 shortly after the instructions associated with the plurality of services are provided for transmission to thecomputing device 102. The voice input data, the intended task, the user feedback data and the determination that the plurality of services was not intended by the user may be stored in association with the user historical activity. This information may be accessed next time a voice input data is received from thecomputing device 102. - For example, the intended task may be to plan a trip. Using the methods described above, the
server 110 may provide instructions associated with a first plurality of services for transmission to thecomputing device 102 and for execution at thecomputing device 102. The first service may be associated with a first flight booking website, the second service may be associated with a hotel reservation website, and third service may be associated with a search result page based on a search query related to the destination. Theserver 110 may detect thatcomputing device 102 is accessing a different service, for example a second flight booking website. Based on this information, the server may determine that the user did not intend to access the first flight booking website. Theserver 110 may store information associated with the voice input data, the intended task, the user feedback data and the determination that the plurality of services was not intended by the user. Next time the server determines that the intended task is planning a trip, theserver 110 may access the stored information to determine a second plurality of services to be accessed at the computing device, where the second plurality of services includes the second flight booking website. -
FIG. 4 conceptually illustrates an example electronic system with which some implementations of the subject technology can be implemented.Electronic system 400 can be a computer, phone, FDA, or any other sort of electronic device. Such an electronic system includes various types of computer readable media and interfaces for various other types of computer readable media.Electronic system 400 includes abus 408, processing unit(s) 412, asystem memory 404, a read-only memory (ROM) 410, apermanent storage device 402, aninput device interface 414, anoutput device interface 406, and anetwork interface 416. -
Bus 408 collectively represents all system, peripheral, and chipset buses that communicatively connect the numerous internal devices ofelectronic system 400. For instance,bus 408 communicatively connects processing unit(s) 412 withROM 410,system memory 404, andpermanent storage device 402. - From these various memory units, processing unit(s) 412 retrieves instructions to execute and data to process in order to execute the processes of the subject disclosure. The processing unit(s) can be a single processor or a multi-core processor in different implementations.
-
ROM 410 stores static data and instructions that are needed by processing unit(s) 412 and other modules of the electronic system.Permanent storage device 402, on the other hand, is a read-and-write memory device. This device is a non-volatile memory unit that stores instructions and data even whenelectronic system 400 is off Some implementations of the subject disclosure use a mass-storage device (for example, a magnetic or optical disk and its corresponding disk drive) aspermanent storage device 402. - Other implementations use a removable storage device (for example, a floppy disk, flash drive, and its corresponding disk drive) as
permanent storage device 402. Likepermanent storage device 402,system memory 404 is a read-and-write memory device. However, unlikestorage device 402,system memory 404 is a volatile read-and-write memory, such as a random access memory.System memory 404 stores some of the instructions and data that the processor needs at runtime. In some implementations, the processes of the subject disclosure are stored insystem memory 404,permanent storage device 402, orROM 410. For example, the various memory units include instructions fir displaying web pages, processing user entries to the web pages, and generating URLs, in accordance with some implementations. From these various memory units, processing unit(s) 412 retrieves instructions to execute and data to process in order to execute the processes of some implementations. -
Bus 408 also connects to input and output device interfaces 414 and 406.Input device interface 414 enables the user to communicate information and select commands to the electronic system. Input devices used withinput device interface 414 include, for example, alphanumeric keyboards and pointing devices (also called “cursor control devices”). Output device interfaces 406 enables, for example, the display of images generated by theelectronic system 400. Output devices used withoutput device interface 406 include, for example, printers and display devices, for example, cathode ray tubes (CRT) or liquid crystal displays (LCD). Some implementations include devices, for example, a touchscreen that functions as both input and output devices. - Finally, as shown in
FIG. 4 ,bus 408 also coupleselectronic system 400 to a network. (not shown) through anetwork interface 416. In this manner, the computer can be a part of a network of computers (for example, a local area network (LAN), a wide area network (WAN), or an Intranet, or a network of networks, for example, the Internet. Any or all components ofelectronic system 400 can be used in conjunction with the subject disclosure. - Many of the above-described features and applications are implemented as software processes that are specified as a set of instructions recorded on a computer readable storage medium (also referred to as computer readable medium). When these instructions are executed by one or more processing unit(s) (e.g., one or more processors, cores of processors, or other processing units), they cause the processing unit(s) to perform the actions indicated in the instructions. Examples of computer readable media include, but are not limited to, CD-ROMs, flash drives, RAM chips, hard drives, EPROMs, etc. The computer readable media does not include carrier waves and electronic signals passing wirelessly or over wired connections.
- In this specification, the term “software” is meant to include firmware residing in read-only memory or applications stored in magnetic storage, which can be read into memory for processing by a processor. Also, in some implementations, multiple software aspects of the subject disclosure can be implemented as sub-parts of a larger program while remaining distinct software aspects of the subject disclosure. In some implementations, multiple software aspects can also be implemented as separate programs. Finally, any combination of separate programs that together implement a software aspect described here is within the scope of the subject disclosure. In some implementations, the software programs, when installed to operate on one or more electronic systems, define one or more specific machine implementations that execute and perform the operations of the software programs.
- A computer program (also known as a program, software, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be deployed in any form, including as a standalone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment. A computer program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- These functions described above can be implemented in digital electronic circuitry, in computer software, firmware, or hardware. The techniques can be implemented using one or more computer program products. Programmable processors and computers can be included in or packaged as mobile devices. The processes and logic flows can be performed by one or more programmable processors and by one or more programmable logic circuitry. General and special purpose computing devices and storage devices can be interconnected through communication networks.
- Some implementations include electronic components, for example, microprocessors, storage, and memory that store computer program instructions in a machine-readable or computer-readable medium (alternatively referred to as computer-readable storage media, machine-readable media, or machine-readable storage media). Some examples of such computer-readable media include RAM, ROM, read-only compact discs (CD-ROM), recordable compact discs (CD-R), rewritable compact discs (CD-RW), read-only digital versatile discs (e.g., DVD-ROM, dual-layer DVD-ROM), a variety of recordable/rewritable DVDs (e.g., DVD-RAM, DVD-RW, DVD+RW, etc.), flash memory (e.g., SD cards, mini-SD cards, micro-SD cards, etc.), magnetic or solid state hard drives, read-only and recordable Blu-Ray® discs, ultra-density optical discs, any other optical or magnetic media, and floppy disks. The computer-readable media can store a computer program that is executable by at least one processing unit and includes sets of instructions for performing various operations. Examples of computer programs or computer code include machine code, for example, is produced by a compiler, and files including higher-level code that are executed by a computer, an electronic component, or a microprocessor using an interpreter.
- While the above discussion primarily refers to microprocessor or multi-core processors that execute software, some implementations are performed by one or more integrated circuits, for example, application specific integrated circuits (ASICs) or field programmable gate arrays (FPGAs). In some implementations, such integrated circuits execute instructions that are stored on the circuit itself.
- As used in this specification and any claims of this application, the terms “computer”, “server”, “processor”, and “memory” all refer to electronic or other technological devices. These terms exclude people or groups of people. For the purposes of the specification, the terms display or displaying means displaying on an electronic device. As used in this specification and any claims of this application, the terms “computer readable medium” and “computer readable media” are entirely restricted to tangible, physical objects that store information in a form that is readable by a computer. These terms exclude any wireless signals, wired download signals, and any other ephemeral signals.
- To provide for interaction with a user, implementations of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a cathode ray tube (CRT) or liquid crystal display (LCD) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input. In addition, a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web browser on a user's client device in response to requests received from the web browser.
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface or a web browser through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back end, middleware, or front end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (LAN) and a wide area network (WAN), an inter-network (e.g., the Internet), and peer-to-peer networks (e.g., ad hoc peer-t)-peer networks).
- The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, a server transmits data (e.g., an HTML page) to a client device (e.g., for purposes of displaying data to and receiving user input from a user interacting with the client device). Data generated at the client device (e.g., a result of the user interaction) can be received from the client device at the server.
- It is understood that any specific order or hierarchy of steps in the processes disclosed is an illustration of example approaches. Based upon design preferences, it is understood that the specific order or hierarchy of steps in the processes may be rearranged, or that all illustrated steps be performed. Some of the steps may be performed simultaneously. For example, in certain circumstances, multitasking and parallel processing may be advantageous. Moreover, the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
- The previous description is provided to enable any person skilled in the art to practice the various aspects described herein. Various modifications to these aspects will be readily apparent to those skilled in the art, and the generic principles defined herein may be applied to other aspects. Thus, the claims are not intended to be limited to the aspects shown herein, but are to be accorded the full scope consistent with the language claims, wherein reference to an element in the singular is not intended to mean “one and only one” unless specifically so stated, but rather “one or more”. Unless specifically stated otherwise, the term “some” refers to one or more. Pronouns in the masculine (e.g., his) include the feminine and neuter gender (e.g., her and its) and vice versa. Headings and subheadings, if any, are used for convenience only and do not limit the subject disclosure.
- A phrase such as an “aspect” does not imply that such aspect is essential to the subject technology or that such aspect applies to all configurations of the subject technology. A disclosure relating to an aspect may apply to all configurations, or one or more configurations. A phrase such as an aspect may refer to one or more aspects and vice versa. A phrase such as a “configuration” does not imply that such configuration is essential to the subject technology or that such configuration applies to all configurations of the subject technology. A disclosure relating to a configuration may apply to all configurations, or one or more configurations. A phrase such as a configuration may refer to one or more configurations and vice versa.
Claims (20)
Priority Applications (4)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/882,353 US10891106B2 (en) | 2015-10-13 | 2015-10-13 | Automatic batch voice commands |
EP16781623.0A EP3362885A1 (en) | 2015-10-13 | 2016-09-29 | Automatic batch voice commands |
PCT/US2016/054524 WO2017065985A1 (en) | 2015-10-13 | 2016-09-29 | Automatic batch voice commands |
CN201680041581.0A CN107850992A (en) | 2015-10-13 | 2016-09-29 | Automatic batch voice command |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/882,353 US10891106B2 (en) | 2015-10-13 | 2015-10-13 | Automatic batch voice commands |
Publications (2)
Publication Number | Publication Date |
---|---|
US20170102915A1 true US20170102915A1 (en) | 2017-04-13 |
US10891106B2 US10891106B2 (en) | 2021-01-12 |
Family
ID=57133436
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US14/882,353 Active US10891106B2 (en) | 2015-10-13 | 2015-10-13 | Automatic batch voice commands |
Country Status (4)
Country | Link |
---|---|
US (1) | US10891106B2 (en) |
EP (1) | EP3362885A1 (en) |
CN (1) | CN107850992A (en) |
WO (1) | WO2017065985A1 (en) |
Cited By (101)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10311144B2 (en) | 2017-05-16 | 2019-06-04 | Apple Inc. | Emoji word sense disambiguation |
CN109857791A (en) * | 2018-11-20 | 2019-06-07 | 成都材智科技有限公司 | A kind of batch data processing method and device |
US10390213B2 (en) | 2014-09-30 | 2019-08-20 | Apple Inc. | Social reminders |
US10395654B2 (en) | 2017-05-11 | 2019-08-27 | Apple Inc. | Text normalization based on a data-driven learning network |
US10403283B1 (en) | 2018-06-01 | 2019-09-03 | Apple Inc. | Voice interaction at a primary device to access call functionality of a companion device |
US10403278B2 (en) | 2017-05-16 | 2019-09-03 | Apple Inc. | Methods and systems for phonetic matching in digital assistant services |
US10417405B2 (en) | 2011-03-21 | 2019-09-17 | Apple Inc. | Device access using voice authentication |
US10417266B2 (en) | 2017-05-09 | 2019-09-17 | Apple Inc. | Context-aware ranking of intelligent response suggestions |
US10417344B2 (en) | 2014-05-30 | 2019-09-17 | Apple Inc. | Exemplar-based natural language processing |
US10438595B2 (en) | 2014-09-30 | 2019-10-08 | Apple Inc. | Speaker identification and unsupervised speaker adaptation techniques |
US10453443B2 (en) | 2014-09-30 | 2019-10-22 | Apple Inc. | Providing an indication of the suitability of speech recognition |
US10474753B2 (en) | 2016-09-07 | 2019-11-12 | Apple Inc. | Language identification using recurrent neural networks |
US10496705B1 (en) | 2018-06-03 | 2019-12-03 | Apple Inc. | Accelerated task performance |
US10529332B2 (en) | 2015-03-08 | 2020-01-07 | Apple Inc. | Virtual assistant activation |
EP3599545A1 (en) * | 2018-07-25 | 2020-01-29 | Avaya Inc. | System and method for creating a cntextualized after call workflow |
US10580409B2 (en) | 2016-06-11 | 2020-03-03 | Apple Inc. | Application integration with a digital assistant |
US10592604B2 (en) | 2018-03-12 | 2020-03-17 | Apple Inc. | Inverse text normalization for automatic speech recognition |
US10657966B2 (en) | 2014-05-30 | 2020-05-19 | Apple Inc. | Better resolution when referencing to concepts |
US10681212B2 (en) | 2015-06-05 | 2020-06-09 | Apple Inc. | Virtual assistant aided communication with 3rd party service in a communication session |
US10692504B2 (en) | 2010-02-25 | 2020-06-23 | Apple Inc. | User profiling for voice input processing |
US10699717B2 (en) | 2014-05-30 | 2020-06-30 | Apple Inc. | Intelligent assistant for home automation |
US10714117B2 (en) | 2013-02-07 | 2020-07-14 | Apple Inc. | Voice trigger for a digital assistant |
US10726832B2 (en) | 2017-05-11 | 2020-07-28 | Apple Inc. | Maintaining privacy of personal information |
US10733375B2 (en) | 2018-01-31 | 2020-08-04 | Apple Inc. | Knowledge-based framework for improving natural language understanding |
US10741185B2 (en) | 2010-01-18 | 2020-08-11 | Apple Inc. | Intelligent automated assistant |
US10741181B2 (en) | 2017-05-09 | 2020-08-11 | Apple Inc. | User interface for correcting recognition errors |
US10748546B2 (en) | 2017-05-16 | 2020-08-18 | Apple Inc. | Digital assistant services based on device capabilities |
US10769385B2 (en) | 2013-06-09 | 2020-09-08 | Apple Inc. | System and method for inferring user intent from speech inputs |
US10839159B2 (en) | 2018-09-28 | 2020-11-17 | Apple Inc. | Named entity normalization in a spoken dialog system |
US10878809B2 (en) | 2014-05-30 | 2020-12-29 | Apple Inc. | Multi-command single utterance input method |
US10892996B2 (en) | 2018-06-01 | 2021-01-12 | Apple Inc. | Variable latency device coordination |
US10909171B2 (en) | 2017-05-16 | 2021-02-02 | Apple Inc. | Intelligent automated assistant for media exploration |
US10930282B2 (en) | 2015-03-08 | 2021-02-23 | Apple Inc. | Competing devices responding to voice triggers |
US10928918B2 (en) | 2018-05-07 | 2021-02-23 | Apple Inc. | Raise to speak |
US10942703B2 (en) | 2015-12-23 | 2021-03-09 | Apple Inc. | Proactive assistance based on dialog communication between devices |
US10942702B2 (en) | 2016-06-11 | 2021-03-09 | Apple Inc. | Intelligent device arbitration and control |
US10956666B2 (en) | 2015-11-09 | 2021-03-23 | Apple Inc. | Unconventional virtual assistant interactions |
US20210104232A1 (en) * | 2019-10-07 | 2021-04-08 | Samsung Electronics Co., Ltd. | Electronic device for processing user utterance and method of operating same |
US10984780B2 (en) | 2018-05-21 | 2021-04-20 | Apple Inc. | Global semantic word embeddings using bi-directional recurrent neural networks |
US11010127B2 (en) | 2015-06-29 | 2021-05-18 | Apple Inc. | Virtual assistant for media playback |
US11009970B2 (en) | 2018-06-01 | 2021-05-18 | Apple Inc. | Attention aware virtual assistant dismissal |
US11010561B2 (en) | 2018-09-27 | 2021-05-18 | Apple Inc. | Sentiment prediction from textual data |
US11037565B2 (en) | 2016-06-10 | 2021-06-15 | Apple Inc. | Intelligent digital assistant in a multi-tasking environment |
US11048473B2 (en) | 2013-06-09 | 2021-06-29 | Apple Inc. | Device, method, and graphical user interface for enabling conversation persistence across two or more instances of a digital assistant |
US11070949B2 (en) | 2015-05-27 | 2021-07-20 | Apple Inc. | Systems and methods for proactively identifying and surfacing relevant content on an electronic device with a touch-sensitive display |
US11120372B2 (en) | 2011-06-03 | 2021-09-14 | Apple Inc. | Performing actions associated with task items that represent tasks to perform |
US11126400B2 (en) | 2015-09-08 | 2021-09-21 | Apple Inc. | Zero latency digital assistant |
US11127397B2 (en) | 2015-05-27 | 2021-09-21 | Apple Inc. | Device voice control |
US11133008B2 (en) | 2014-05-30 | 2021-09-28 | Apple Inc. | Reducing the need for manual start/end-pointing and trigger phrases |
US11140099B2 (en) | 2019-05-21 | 2021-10-05 | Apple Inc. | Providing message response suggestions |
US11145294B2 (en) | 2018-05-07 | 2021-10-12 | Apple Inc. | Intelligent automated assistant for delivering content from user experiences |
US11170166B2 (en) | 2018-09-28 | 2021-11-09 | Apple Inc. | Neural typographical error modeling via generative adversarial networks |
US11217251B2 (en) | 2019-05-06 | 2022-01-04 | Apple Inc. | Spoken notifications |
US11227589B2 (en) | 2016-06-06 | 2022-01-18 | Apple Inc. | Intelligent list reading |
US11231904B2 (en) | 2015-03-06 | 2022-01-25 | Apple Inc. | Reducing response latency of intelligent automated assistants |
US11237797B2 (en) | 2019-05-31 | 2022-02-01 | Apple Inc. | User activity shortcut suggestions |
US11269678B2 (en) | 2012-05-15 | 2022-03-08 | Apple Inc. | Systems and methods for integrating third party services with a digital assistant |
US11289073B2 (en) | 2019-05-31 | 2022-03-29 | Apple Inc. | Device text to speech |
US11301477B2 (en) | 2017-05-12 | 2022-04-12 | Apple Inc. | Feedback analysis of a digital assistant |
US11307752B2 (en) | 2019-05-06 | 2022-04-19 | Apple Inc. | User configurable task triggers |
US11314370B2 (en) | 2013-12-06 | 2022-04-26 | Apple Inc. | Method for extracting salient dialog usage from live data |
US11348573B2 (en) | 2019-03-18 | 2022-05-31 | Apple Inc. | Multimodality in digital assistant systems |
US11348582B2 (en) | 2008-10-02 | 2022-05-31 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
US11360641B2 (en) | 2019-06-01 | 2022-06-14 | Apple Inc. | Increasing the relevance of new available information |
US11380310B2 (en) | 2017-05-12 | 2022-07-05 | Apple Inc. | Low-latency intelligent automated assistant |
US11388291B2 (en) | 2013-03-14 | 2022-07-12 | Apple Inc. | System and method for processing voicemail |
US11386266B2 (en) | 2018-06-01 | 2022-07-12 | Apple Inc. | Text correction |
US11405466B2 (en) | 2017-05-12 | 2022-08-02 | Apple Inc. | Synchronization and task delegation of a digital assistant |
US11423886B2 (en) | 2010-01-18 | 2022-08-23 | Apple Inc. | Task flow identification based on user intent |
US11423908B2 (en) | 2019-05-06 | 2022-08-23 | Apple Inc. | Interpreting spoken requests |
US11462215B2 (en) | 2018-09-28 | 2022-10-04 | Apple Inc. | Multi-modal inputs for voice commands |
US11468282B2 (en) | 2015-05-15 | 2022-10-11 | Apple Inc. | Virtual assistant in a communication session |
US11467802B2 (en) | 2017-05-11 | 2022-10-11 | Apple Inc. | Maintaining privacy of personal information |
US11475898B2 (en) | 2018-10-26 | 2022-10-18 | Apple Inc. | Low-latency multi-speaker speech recognition |
US11475884B2 (en) | 2019-05-06 | 2022-10-18 | Apple Inc. | Reducing digital assistant latency when a language is incorrectly determined |
US11488406B2 (en) | 2019-09-25 | 2022-11-01 | Apple Inc. | Text detection using global geometry estimators |
US11495218B2 (en) | 2018-06-01 | 2022-11-08 | Apple Inc. | Virtual assistant operation in multi-device environments |
US11496600B2 (en) | 2019-05-31 | 2022-11-08 | Apple Inc. | Remote execution of machine-learned models |
US11500672B2 (en) | 2015-09-08 | 2022-11-15 | Apple Inc. | Distributed personal assistant |
US11516537B2 (en) | 2014-06-30 | 2022-11-29 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US11526368B2 (en) | 2015-11-06 | 2022-12-13 | Apple Inc. | Intelligent automated assistant in a messaging environment |
US11532306B2 (en) | 2017-05-16 | 2022-12-20 | Apple Inc. | Detecting a trigger of a digital assistant |
US11580990B2 (en) | 2017-05-12 | 2023-02-14 | Apple Inc. | User-specific acoustic models |
US11605385B2 (en) * | 2019-10-31 | 2023-03-14 | International Business Machines Corporation | Project issue tracking via automated voice recognition |
US11638059B2 (en) | 2019-01-04 | 2023-04-25 | Apple Inc. | Content playback on multiple devices |
US11657813B2 (en) | 2019-05-31 | 2023-05-23 | Apple Inc. | Voice identification in digital assistant systems |
US11656884B2 (en) | 2017-01-09 | 2023-05-23 | Apple Inc. | Application integration with a digital assistant |
US11671920B2 (en) | 2007-04-03 | 2023-06-06 | Apple Inc. | Method and system for operating a multifunction portable electronic device using voice-activation |
US11696060B2 (en) | 2020-07-21 | 2023-07-04 | Apple Inc. | User identification using headphones |
US11710482B2 (en) | 2018-03-26 | 2023-07-25 | Apple Inc. | Natural assistant interaction |
US11765209B2 (en) | 2020-05-11 | 2023-09-19 | Apple Inc. | Digital assistant hardware abstraction |
US11776542B1 (en) | 2021-03-30 | 2023-10-03 | Amazon Technologies, Inc. | Selecting dialog acts using controlled randomness and offline optimization |
US11790914B2 (en) | 2019-06-01 | 2023-10-17 | Apple Inc. | Methods and user interfaces for voice-based control of electronic devices |
US11798547B2 (en) | 2013-03-15 | 2023-10-24 | Apple Inc. | Voice activated device for use with a voice-based digital assistant |
US11810578B2 (en) | 2020-05-11 | 2023-11-07 | Apple Inc. | Device arbitration for digital assistant-based intercom systems |
US11809483B2 (en) | 2015-09-08 | 2023-11-07 | Apple Inc. | Intelligent automated assistant for media search and playback |
US11838734B2 (en) | 2020-07-20 | 2023-12-05 | Apple Inc. | Multi-device audio adjustment coordination |
US11853536B2 (en) | 2015-09-08 | 2023-12-26 | Apple Inc. | Intelligent automated assistant in a media environment |
US11914848B2 (en) | 2020-05-11 | 2024-02-27 | Apple Inc. | Providing relevant data items based on context |
US11928604B2 (en) | 2005-09-08 | 2024-03-12 | Apple Inc. | Method and apparatus for building an intelligent automated assistant |
US11978445B1 (en) * | 2021-03-30 | 2024-05-07 | Amazon Technologies, Inc. | Confidence scoring for selecting tones and text of voice browsing conversations |
Families Citing this family (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN111048078A (en) * | 2018-10-15 | 2020-04-21 | 阿里巴巴集团控股有限公司 | Voice composite instruction processing method and system, voice processing device and medium |
CN117275472A (en) * | 2022-06-13 | 2023-12-22 | 华为技术有限公司 | Voice control method, device and equipment |
Citations (38)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20060074658A1 (en) * | 2004-10-01 | 2006-04-06 | Siemens Information And Communication Mobile, Llc | Systems and methods for hands-free voice-activated devices |
US20060265217A1 (en) * | 2005-05-20 | 2006-11-23 | General Motors Corporation | Method and system for eliminating redundant voice recognition feedback |
US20080255851A1 (en) * | 2007-04-12 | 2008-10-16 | Soonthorn Ativanichayaphong | Speech-Enabled Content Navigation And Control Of A Distributed Multimodal Browser |
US20100031166A1 (en) * | 2008-07-29 | 2010-02-04 | International Business Machines Corporation | System and method for web browsing using placemarks and contextual relationships in a data processing system |
US20100058200A1 (en) * | 2007-08-22 | 2010-03-04 | Yap, Inc. | Facilitating presentation by mobile device of additional content for a word or phrase upon utterance thereof |
US20110301955A1 (en) * | 2010-06-07 | 2011-12-08 | Google Inc. | Predicting and Learning Carrier Phrases for Speech Input |
US20120060083A1 (en) * | 2009-02-26 | 2012-03-08 | Song Yuan | Method for Use in Association With A Multi-Tab Interpretation and Rendering Function |
US20120303445A1 (en) * | 2007-08-22 | 2012-11-29 | Victor Roditis Jablokov | Facilitating presentation of ads relating to words of a message |
US20130073568A1 (en) * | 2011-09-21 | 2013-03-21 | Vladimir Federov | Ranking structured objects and actions on a social networking system |
US20130110615A1 (en) * | 2011-04-22 | 2013-05-02 | Nhn Business Platform Corporation | System and method for controlling advertisement based on user benefit |
US20130110992A1 (en) * | 2011-10-28 | 2013-05-02 | Research In Motion Limited | Electronic device management using interdomain profile-based inferences |
US20130185072A1 (en) * | 2010-06-24 | 2013-07-18 | Honda Motor Co., Ltd. | Communication System and Method Between an On-Vehicle Voice Recognition System and an Off-Vehicle Voice Recognition System |
US20130226590A1 (en) * | 2012-02-29 | 2013-08-29 | Pantech Co., Ltd. | Voice input apparatus and method |
US20130246050A1 (en) * | 2012-03-16 | 2013-09-19 | France Telecom | Voice control of applications by associating user input with action-context identifier pairs |
US20140013353A1 (en) * | 2009-05-08 | 2014-01-09 | Comcast Interactive Media, Llc | Social Network Based Recommendation Method and System |
US20140207446A1 (en) * | 2013-01-24 | 2014-07-24 | Microsoft Corporation | Indefinite speech inputs |
US20140278435A1 (en) * | 2013-03-12 | 2014-09-18 | Nuance Communications, Inc. | Methods and apparatus for detecting a voice command |
US20140365956A1 (en) * | 2013-06-09 | 2014-12-11 | Apple Inc. | Device, method, and graphical user interface for navigating between user interfaces |
US9020825B1 (en) * | 2012-09-25 | 2015-04-28 | Rawles Llc | Voice gestures |
US20150170172A1 (en) * | 2011-12-19 | 2015-06-18 | Eventsq Llc | Content recommendation based on user feedback of content in a networked environment captured using a single action |
US20150199965A1 (en) * | 2014-01-16 | 2015-07-16 | CloudCar Inc. | System and method for recognition and automatic correction of voice commands |
US20150205462A1 (en) * | 2009-10-13 | 2015-07-23 | Google Inc. | Browser tab management |
US20150215665A1 (en) * | 2014-01-30 | 2015-07-30 | Echostar Technologies L.L.C. | Methods and apparatus to synchronize second screen content with audio/video programming using closed captioning data |
US20150228275A1 (en) * | 2014-02-10 | 2015-08-13 | Mitsubishi Electric Research Laboratories, Inc. | Statistical Voice Dialog System and Method |
US20150254216A1 (en) * | 2014-03-06 | 2015-09-10 | International Business Machines Corporation | Contextual hyperlink insertion |
US20150254058A1 (en) * | 2014-03-04 | 2015-09-10 | Microsoft Technology Licensing, Llc | Voice control shortcuts |
US20150255086A1 (en) * | 2014-03-07 | 2015-09-10 | Ebay Inc. | Interactive voice response interface for webpage navigation |
US20150279366A1 (en) * | 2014-03-28 | 2015-10-01 | Cubic Robotics, Inc. | Voice driven operating system for interfacing with electronic devices: system, method, and architecture |
US20150332673A1 (en) * | 2014-05-13 | 2015-11-19 | Nuance Communications, Inc. | Revising language model scores based on semantic class hypotheses |
US20150339754A1 (en) * | 2014-05-22 | 2015-11-26 | Craig J. Bloem | Systems and methods for customizing search results and recommendations |
US20150340042A1 (en) * | 2013-03-12 | 2015-11-26 | Nuance Communications, Inc. | Methods and apparatus for detecting a voice command |
US20160004501A1 (en) * | 2014-07-01 | 2016-01-07 | Honeywell International Inc. | Audio command intent determination system and method |
US20160139750A1 (en) * | 2014-11-17 | 2016-05-19 | Microsoft Technology Licensing | Tab Sweeping and Grouping |
US20160147400A1 (en) * | 2014-11-26 | 2016-05-26 | Microsoft Technology Licensing, Llc | Tab based browser content sharing |
US20160170592A1 (en) * | 2014-12-10 | 2016-06-16 | International Business Machines Corporation | Transitioning browser tabs from one environment context to another |
US9436951B1 (en) * | 2007-08-22 | 2016-09-06 | Amazon Technologies, Inc. | Facilitating presentation by mobile device of additional content for a word or phrase upon utterance thereof |
US20170199638A1 (en) * | 2016-01-07 | 2017-07-13 | International Business Machines Corporation | Automatic browser tab groupings |
US20180260081A1 (en) * | 2014-07-30 | 2018-09-13 | Google Inc. | Task switching or task launching based on a ranked list of tasks |
Family Cites Families (35)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
ES2288467T3 (en) | 1999-10-26 | 2008-01-16 | Iontas Limited | SUPERVISION OF THE USE OF COMPUTERS. |
US20060248471A1 (en) | 2005-04-29 | 2006-11-02 | Microsoft Corporation | System and method for providing a window management mode |
US7757234B2 (en) | 2005-10-24 | 2010-07-13 | Sap Aktiengesellschaft | Methods and software for a batch processing framework for wizard-based processes |
WO2008081946A1 (en) | 2006-12-28 | 2008-07-10 | Kyocera Corporation | Mobile information terminal |
US8219406B2 (en) | 2007-03-15 | 2012-07-10 | Microsoft Corporation | Speech-centric multimodal user interface design in mobile technology |
US8510743B2 (en) | 2007-10-31 | 2013-08-13 | Google Inc. | Terminating computer applications |
US20090241058A1 (en) | 2008-03-18 | 2009-09-24 | Cuill, Inc. | Apparatus and method for displaying search results with an associated anchor area |
US8479105B2 (en) | 2008-03-24 | 2013-07-02 | International Business Machines Corporation | Managing graphical user interface objects in a computing environment |
EP2177977B1 (en) | 2008-10-17 | 2011-03-30 | Research In Motion Limited | Multi-directional navigation between focus points on a display |
US8291261B2 (en) | 2008-11-05 | 2012-10-16 | Vulcan Technologies Llc | Lightweight application-level runtime state save-and-restore utility |
CN101782909A (en) * | 2009-01-19 | 2010-07-21 | 杨云国 | Search engine based on operation intention of user |
US10540976B2 (en) | 2009-06-05 | 2020-01-21 | Apple Inc. | Contextual voice commands |
US10241644B2 (en) | 2011-06-03 | 2019-03-26 | Apple Inc. | Actionable reminder entries |
US20110082685A1 (en) * | 2009-10-05 | 2011-04-07 | Sony Ericsson Mobile Communications Ab | Provisioning text services based on assignment of language attributes to contact entry |
WO2012027605A2 (en) | 2010-08-27 | 2012-03-01 | Intel Corporation | Intelligent remote control system |
US9223611B2 (en) | 2010-12-28 | 2015-12-29 | Microsoft Technology Licensing, Llc | Storing and resuming application runtime state |
US8892635B2 (en) | 2011-01-06 | 2014-11-18 | Oracle International Corporation | Techniques for detecting inactive browser windows |
JP5643430B2 (en) * | 2011-06-28 | 2014-12-17 | インターナショナル・ビジネス・マシーンズ・コーポレーションＩｎｔｅｒｎａｔｉｏｎａｌ Ｂｕｓｉｎｅｓｓ Ｍａｃｈｉｎｅｓ Ｃｏｒｐｏｒａｔｉｏｎ | Information processing apparatus, method, and program for obtaining weight for each feature amount in subjective hierarchical clustering |
US20130238326A1 (en) | 2012-03-08 | 2013-09-12 | Lg Electronics Inc. | Apparatus and method for multiple device voice control |
CN102739869A (en) * | 2012-06-26 | 2012-10-17 | 华为终端有限公司 | Method and terminal for searching for information of target contact person by voice |
KR20140070040A (en) | 2012-11-30 | 2014-06-10 | 삼성전자주식회사 | Apparatus and method for managing a plurality of objects displayed on touch screen |
CN103870000B (en) * | 2012-12-11 | 2018-12-14 | 百度国际科技（深圳）有限公司 | The method and device that candidate item caused by a kind of pair of input method is ranked up |
CN103064945B (en) * | 2012-12-26 | 2016-01-06 | 吉林大学 | Based on the Situational searching method of body |
US20140189572A1 (en) | 2012-12-31 | 2014-07-03 | Motorola Mobility Llc | Ranking and Display of Results from Applications and Services with Integrated Feedback |
CA2896985A1 (en) | 2013-01-03 | 2014-07-10 | Meta Company | Extramissive spatial imaging digital eye glass for virtual or augmediated vision |
US9123345B2 (en) | 2013-03-14 | 2015-09-01 | Honda Motor Co., Ltd. | Voice interface systems and methods |
KR102115397B1 (en) | 2013-04-01 | 2020-05-26 | 삼성전자주식회사 | Portable apparatus and method for displaying a playlist |
US20140302470A1 (en) | 2013-04-08 | 2014-10-09 | Healthy Connections, Inc | Managing lifestyle resources system and method |
KR102163684B1 (en) | 2013-07-19 | 2020-10-12 | 삼성전자주식회사 | Method and apparatus for constructing a home screen of the device |
CN104427109B (en) * | 2013-08-30 | 2017-04-19 | 联想(北京)有限公司 | Method for establishing contact item by voices and electronic equipment |
CN104702758B (en) * | 2013-12-05 | 2019-05-10 | 中兴通讯股份有限公司 | A kind of terminal and its method for managing multimedia notepad |
CN103888803B (en) * | 2014-02-25 | 2017-05-03 | 四川长虹电器股份有限公司 | Method and system for controlling insertion of advertisement by television program voice |
CN104063454A (en) * | 2014-06-24 | 2014-09-24 | 北京奇虎科技有限公司 | Search push method and device for mining user demands |
WO2016094807A1 (en) | 2014-12-11 | 2016-06-16 | Vishal Sharma | Virtual assistant system to enable actionable messaging |
CN107483698A (en) * | 2017-09-05 | 2017-12-15 | 北京珠穆朗玛移动通信有限公司 | A kind of contact management method, mobile terminal and storage medium |
-
2015
- 2015-10-13 US US14/882,353 patent/US10891106B2/en active Active
-
2016
- 2016-09-29 CN CN201680041581.0A patent/CN107850992A/en active Pending
- 2016-09-29 EP EP16781623.0A patent/EP3362885A1/en not_active Ceased
- 2016-09-29 WO PCT/US2016/054524 patent/WO2017065985A1/en unknown
Patent Citations (39)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20060074658A1 (en) * | 2004-10-01 | 2006-04-06 | Siemens Information And Communication Mobile, Llc | Systems and methods for hands-free voice-activated devices |
US20060265217A1 (en) * | 2005-05-20 | 2006-11-23 | General Motors Corporation | Method and system for eliminating redundant voice recognition feedback |
US20080255851A1 (en) * | 2007-04-12 | 2008-10-16 | Soonthorn Ativanichayaphong | Speech-Enabled Content Navigation And Control Of A Distributed Multimodal Browser |
US20100058200A1 (en) * | 2007-08-22 | 2010-03-04 | Yap, Inc. | Facilitating presentation by mobile device of additional content for a word or phrase upon utterance thereof |
US8296377B1 (en) * | 2007-08-22 | 2012-10-23 | Canyon IP Holdings, LLC. | Facilitating presentation by mobile device of additional content for a word or phrase upon utterance thereof |
US20120303445A1 (en) * | 2007-08-22 | 2012-11-29 | Victor Roditis Jablokov | Facilitating presentation of ads relating to words of a message |
US9436951B1 (en) * | 2007-08-22 | 2016-09-06 | Amazon Technologies, Inc. | Facilitating presentation by mobile device of additional content for a word or phrase upon utterance thereof |
US20100031166A1 (en) * | 2008-07-29 | 2010-02-04 | International Business Machines Corporation | System and method for web browsing using placemarks and contextual relationships in a data processing system |
US20120060083A1 (en) * | 2009-02-26 | 2012-03-08 | Song Yuan | Method for Use in Association With A Multi-Tab Interpretation and Rendering Function |
US20140013353A1 (en) * | 2009-05-08 | 2014-01-09 | Comcast Interactive Media, Llc | Social Network Based Recommendation Method and System |
US20150205462A1 (en) * | 2009-10-13 | 2015-07-23 | Google Inc. | Browser tab management |
US20110301955A1 (en) * | 2010-06-07 | 2011-12-08 | Google Inc. | Predicting and Learning Carrier Phrases for Speech Input |
US20130185072A1 (en) * | 2010-06-24 | 2013-07-18 | Honda Motor Co., Ltd. | Communication System and Method Between an On-Vehicle Voice Recognition System and an Off-Vehicle Voice Recognition System |
US20130110615A1 (en) * | 2011-04-22 | 2013-05-02 | Nhn Business Platform Corporation | System and method for controlling advertisement based on user benefit |
US20130073568A1 (en) * | 2011-09-21 | 2013-03-21 | Vladimir Federov | Ranking structured objects and actions on a social networking system |
US20130110992A1 (en) * | 2011-10-28 | 2013-05-02 | Research In Motion Limited | Electronic device management using interdomain profile-based inferences |
US20150170172A1 (en) * | 2011-12-19 | 2015-06-18 | Eventsq Llc | Content recommendation based on user feedback of content in a networked environment captured using a single action |
US20130226590A1 (en) * | 2012-02-29 | 2013-08-29 | Pantech Co., Ltd. | Voice input apparatus and method |
US20130246050A1 (en) * | 2012-03-16 | 2013-09-19 | France Telecom | Voice control of applications by associating user input with action-context identifier pairs |
US9020825B1 (en) * | 2012-09-25 | 2015-04-28 | Rawles Llc | Voice gestures |
US20140207446A1 (en) * | 2013-01-24 | 2014-07-24 | Microsoft Corporation | Indefinite speech inputs |
US20140278435A1 (en) * | 2013-03-12 | 2014-09-18 | Nuance Communications, Inc. | Methods and apparatus for detecting a voice command |
US20150340042A1 (en) * | 2013-03-12 | 2015-11-26 | Nuance Communications, Inc. | Methods and apparatus for detecting a voice command |
US20140365956A1 (en) * | 2013-06-09 | 2014-12-11 | Apple Inc. | Device, method, and graphical user interface for navigating between user interfaces |
US20150199965A1 (en) * | 2014-01-16 | 2015-07-16 | CloudCar Inc. | System and method for recognition and automatic correction of voice commands |
US20150215665A1 (en) * | 2014-01-30 | 2015-07-30 | Echostar Technologies L.L.C. | Methods and apparatus to synchronize second screen content with audio/video programming using closed captioning data |
US20150228275A1 (en) * | 2014-02-10 | 2015-08-13 | Mitsubishi Electric Research Laboratories, Inc. | Statistical Voice Dialog System and Method |
US20150254058A1 (en) * | 2014-03-04 | 2015-09-10 | Microsoft Technology Licensing, Llc | Voice control shortcuts |
US20150254216A1 (en) * | 2014-03-06 | 2015-09-10 | International Business Machines Corporation | Contextual hyperlink insertion |
US20150255086A1 (en) * | 2014-03-07 | 2015-09-10 | Ebay Inc. | Interactive voice response interface for webpage navigation |
US20150279366A1 (en) * | 2014-03-28 | 2015-10-01 | Cubic Robotics, Inc. | Voice driven operating system for interfacing with electronic devices: system, method, and architecture |
US20150332673A1 (en) * | 2014-05-13 | 2015-11-19 | Nuance Communications, Inc. | Revising language model scores based on semantic class hypotheses |
US20150339754A1 (en) * | 2014-05-22 | 2015-11-26 | Craig J. Bloem | Systems and methods for customizing search results and recommendations |
US20160004501A1 (en) * | 2014-07-01 | 2016-01-07 | Honeywell International Inc. | Audio command intent determination system and method |
US20180260081A1 (en) * | 2014-07-30 | 2018-09-13 | Google Inc. | Task switching or task launching based on a ranked list of tasks |
US20160139750A1 (en) * | 2014-11-17 | 2016-05-19 | Microsoft Technology Licensing | Tab Sweeping and Grouping |
US20160147400A1 (en) * | 2014-11-26 | 2016-05-26 | Microsoft Technology Licensing, Llc | Tab based browser content sharing |
US20160170592A1 (en) * | 2014-12-10 | 2016-06-16 | International Business Machines Corporation | Transitioning browser tabs from one environment context to another |
US20170199638A1 (en) * | 2016-01-07 | 2017-07-13 | International Business Machines Corporation | Automatic browser tab groupings |
Cited By (156)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11928604B2 (en) | 2005-09-08 | 2024-03-12 | Apple Inc. | Method and apparatus for building an intelligent automated assistant |
US11671920B2 (en) | 2007-04-03 | 2023-06-06 | Apple Inc. | Method and system for operating a multifunction portable electronic device using voice-activation |
US11979836B2 (en) | 2007-04-03 | 2024-05-07 | Apple Inc. | Method and system for operating a multi-function portable electronic device using voice-activation |
US11348582B2 (en) | 2008-10-02 | 2022-05-31 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
US11900936B2 (en) | 2008-10-02 | 2024-02-13 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
US11423886B2 (en) | 2010-01-18 | 2022-08-23 | Apple Inc. | Task flow identification based on user intent |
US10741185B2 (en) | 2010-01-18 | 2020-08-11 | Apple Inc. | Intelligent automated assistant |
US10692504B2 (en) | 2010-02-25 | 2020-06-23 | Apple Inc. | User profiling for voice input processing |
US10417405B2 (en) | 2011-03-21 | 2019-09-17 | Apple Inc. | Device access using voice authentication |
US11120372B2 (en) | 2011-06-03 | 2021-09-14 | Apple Inc. | Performing actions associated with task items that represent tasks to perform |
US11321116B2 (en) | 2012-05-15 | 2022-05-03 | Apple Inc. | Systems and methods for integrating third party services with a digital assistant |
US11269678B2 (en) | 2012-05-15 | 2022-03-08 | Apple Inc. | Systems and methods for integrating third party services with a digital assistant |
US10978090B2 (en) | 2013-02-07 | 2021-04-13 | Apple Inc. | Voice trigger for a digital assistant |
US11636869B2 (en) | 2013-02-07 | 2023-04-25 | Apple Inc. | Voice trigger for a digital assistant |
US11557310B2 (en) | 2013-02-07 | 2023-01-17 | Apple Inc. | Voice trigger for a digital assistant |
US10714117B2 (en) | 2013-02-07 | 2020-07-14 | Apple Inc. | Voice trigger for a digital assistant |
US11862186B2 (en) | 2013-02-07 | 2024-01-02 | Apple Inc. | Voice trigger for a digital assistant |
US11388291B2 (en) | 2013-03-14 | 2022-07-12 | Apple Inc. | System and method for processing voicemail |
US11798547B2 (en) | 2013-03-15 | 2023-10-24 | Apple Inc. | Voice activated device for use with a voice-based digital assistant |
US11048473B2 (en) | 2013-06-09 | 2021-06-29 | Apple Inc. | Device, method, and graphical user interface for enabling conversation persistence across two or more instances of a digital assistant |
US10769385B2 (en) | 2013-06-09 | 2020-09-08 | Apple Inc. | System and method for inferring user intent from speech inputs |
US11727219B2 (en) | 2013-06-09 | 2023-08-15 | Apple Inc. | System and method for inferring user intent from speech inputs |
US11314370B2 (en) | 2013-12-06 | 2022-04-26 | Apple Inc. | Method for extracting salient dialog usage from live data |
US11699448B2 (en) | 2014-05-30 | 2023-07-11 | Apple Inc. | Intelligent assistant for home automation |
US11257504B2 (en) | 2014-05-30 | 2022-02-22 | Apple Inc. | Intelligent assistant for home automation |
US10714095B2 (en) | 2014-05-30 | 2020-07-14 | Apple Inc. | Intelligent assistant for home automation |
US11133008B2 (en) | 2014-05-30 | 2021-09-28 | Apple Inc. | Reducing the need for manual start/end-pointing and trigger phrases |
US10699717B2 (en) | 2014-05-30 | 2020-06-30 | Apple Inc. | Intelligent assistant for home automation |
US11810562B2 (en) | 2014-05-30 | 2023-11-07 | Apple Inc. | Reducing the need for manual start/end-pointing and trigger phrases |
US10417344B2 (en) | 2014-05-30 | 2019-09-17 | Apple Inc. | Exemplar-based natural language processing |
US11670289B2 (en) | 2014-05-30 | 2023-06-06 | Apple Inc. | Multi-command single utterance input method |
US10878809B2 (en) | 2014-05-30 | 2020-12-29 | Apple Inc. | Multi-command single utterance input method |
US10657966B2 (en) | 2014-05-30 | 2020-05-19 | Apple Inc. | Better resolution when referencing to concepts |
US11516537B2 (en) | 2014-06-30 | 2022-11-29 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US11838579B2 (en) | 2014-06-30 | 2023-12-05 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US10390213B2 (en) | 2014-09-30 | 2019-08-20 | Apple Inc. | Social reminders |
US10453443B2 (en) | 2014-09-30 | 2019-10-22 | Apple Inc. | Providing an indication of the suitability of speech recognition |
US10438595B2 (en) | 2014-09-30 | 2019-10-08 | Apple Inc. | Speaker identification and unsupervised speaker adaptation techniques |
US11231904B2 (en) | 2015-03-06 | 2022-01-25 | Apple Inc. | Reducing response latency of intelligent automated assistants |
US10529332B2 (en) | 2015-03-08 | 2020-01-07 | Apple Inc. | Virtual assistant activation |
US11842734B2 (en) | 2015-03-08 | 2023-12-12 | Apple Inc. | Virtual assistant activation |
US10930282B2 (en) | 2015-03-08 | 2021-02-23 | Apple Inc. | Competing devices responding to voice triggers |
US11087759B2 (en) | 2015-03-08 | 2021-08-10 | Apple Inc. | Virtual assistant activation |
US11468282B2 (en) | 2015-05-15 | 2022-10-11 | Apple Inc. | Virtual assistant in a communication session |
US11127397B2 (en) | 2015-05-27 | 2021-09-21 | Apple Inc. | Device voice control |
US11070949B2 (en) | 2015-05-27 | 2021-07-20 | Apple Inc. | Systems and methods for proactively identifying and surfacing relevant content on an electronic device with a touch-sensitive display |
US10681212B2 (en) | 2015-06-05 | 2020-06-09 | Apple Inc. | Virtual assistant aided communication with 3rd party service in a communication session |
US11947873B2 (en) | 2015-06-29 | 2024-04-02 | Apple Inc. | Virtual assistant for media playback |
US11010127B2 (en) | 2015-06-29 | 2021-05-18 | Apple Inc. | Virtual assistant for media playback |
US11853536B2 (en) | 2015-09-08 | 2023-12-26 | Apple Inc. | Intelligent automated assistant in a media environment |
US11954405B2 (en) | 2015-09-08 | 2024-04-09 | Apple Inc. | Zero latency digital assistant |
US11500672B2 (en) | 2015-09-08 | 2022-11-15 | Apple Inc. | Distributed personal assistant |
US11550542B2 (en) | 2015-09-08 | 2023-01-10 | Apple Inc. | Zero latency digital assistant |
US11809483B2 (en) | 2015-09-08 | 2023-11-07 | Apple Inc. | Intelligent automated assistant for media search and playback |
US11126400B2 (en) | 2015-09-08 | 2021-09-21 | Apple Inc. | Zero latency digital assistant |
US11526368B2 (en) | 2015-11-06 | 2022-12-13 | Apple Inc. | Intelligent automated assistant in a messaging environment |
US11809886B2 (en) | 2015-11-06 | 2023-11-07 | Apple Inc. | Intelligent automated assistant in a messaging environment |
US10956666B2 (en) | 2015-11-09 | 2021-03-23 | Apple Inc. | Unconventional virtual assistant interactions |
US11886805B2 (en) | 2015-11-09 | 2024-01-30 | Apple Inc. | Unconventional virtual assistant interactions |
US11853647B2 (en) | 2015-12-23 | 2023-12-26 | Apple Inc. | Proactive assistance based on dialog communication between devices |
US10942703B2 (en) | 2015-12-23 | 2021-03-09 | Apple Inc. | Proactive assistance based on dialog communication between devices |
US11227589B2 (en) | 2016-06-06 | 2022-01-18 | Apple Inc. | Intelligent list reading |
US11657820B2 (en) | 2016-06-10 | 2023-05-23 | Apple Inc. | Intelligent digital assistant in a multi-tasking environment |
US11037565B2 (en) | 2016-06-10 | 2021-06-15 | Apple Inc. | Intelligent digital assistant in a multi-tasking environment |
US10942702B2 (en) | 2016-06-11 | 2021-03-09 | Apple Inc. | Intelligent device arbitration and control |
US11152002B2 (en) | 2016-06-11 | 2021-10-19 | Apple Inc. | Application integration with a digital assistant |
US10580409B2 (en) | 2016-06-11 | 2020-03-03 | Apple Inc. | Application integration with a digital assistant |
US11749275B2 (en) | 2016-06-11 | 2023-09-05 | Apple Inc. | Application integration with a digital assistant |
US11809783B2 (en) | 2016-06-11 | 2023-11-07 | Apple Inc. | Intelligent device arbitration and control |
US10474753B2 (en) | 2016-09-07 | 2019-11-12 | Apple Inc. | Language identification using recurrent neural networks |
US11656884B2 (en) | 2017-01-09 | 2023-05-23 | Apple Inc. | Application integration with a digital assistant |
US10741181B2 (en) | 2017-05-09 | 2020-08-11 | Apple Inc. | User interface for correcting recognition errors |
US10417266B2 (en) | 2017-05-09 | 2019-09-17 | Apple Inc. | Context-aware ranking of intelligent response suggestions |
US10847142B2 (en) | 2017-05-11 | 2020-11-24 | Apple Inc. | Maintaining privacy of personal information |
US11467802B2 (en) | 2017-05-11 | 2022-10-11 | Apple Inc. | Maintaining privacy of personal information |
US10395654B2 (en) | 2017-05-11 | 2019-08-27 | Apple Inc. | Text normalization based on a data-driven learning network |
US10726832B2 (en) | 2017-05-11 | 2020-07-28 | Apple Inc. | Maintaining privacy of personal information |
US11599331B2 (en) | 2017-05-11 | 2023-03-07 | Apple Inc. | Maintaining privacy of personal information |
US11538469B2 (en) | 2017-05-12 | 2022-12-27 | Apple Inc. | Low-latency intelligent automated assistant |
US11862151B2 (en) | 2017-05-12 | 2024-01-02 | Apple Inc. | Low-latency intelligent automated assistant |
US11837237B2 (en) | 2017-05-12 | 2023-12-05 | Apple Inc. | User-specific acoustic models |
US11380310B2 (en) | 2017-05-12 | 2022-07-05 | Apple Inc. | Low-latency intelligent automated assistant |
US11301477B2 (en) | 2017-05-12 | 2022-04-12 | Apple Inc. | Feedback analysis of a digital assistant |
US11580990B2 (en) | 2017-05-12 | 2023-02-14 | Apple Inc. | User-specific acoustic models |
US11405466B2 (en) | 2017-05-12 | 2022-08-02 | Apple Inc. | Synchronization and task delegation of a digital assistant |
US11675829B2 (en) | 2017-05-16 | 2023-06-13 | Apple Inc. | Intelligent automated assistant for media exploration |
US10909171B2 (en) | 2017-05-16 | 2021-02-02 | Apple Inc. | Intelligent automated assistant for media exploration |
US10403278B2 (en) | 2017-05-16 | 2019-09-03 | Apple Inc. | Methods and systems for phonetic matching in digital assistant services |
US11532306B2 (en) | 2017-05-16 | 2022-12-20 | Apple Inc. | Detecting a trigger of a digital assistant |
US10311144B2 (en) | 2017-05-16 | 2019-06-04 | Apple Inc. | Emoji word sense disambiguation |
US10748546B2 (en) | 2017-05-16 | 2020-08-18 | Apple Inc. | Digital assistant services based on device capabilities |
US10733375B2 (en) | 2018-01-31 | 2020-08-04 | Apple Inc. | Knowledge-based framework for improving natural language understanding |
US10592604B2 (en) | 2018-03-12 | 2020-03-17 | Apple Inc. | Inverse text normalization for automatic speech recognition |
US11710482B2 (en) | 2018-03-26 | 2023-07-25 | Apple Inc. | Natural assistant interaction |
US11854539B2 (en) | 2018-05-07 | 2023-12-26 | Apple Inc. | Intelligent automated assistant for delivering content from user experiences |
US11487364B2 (en) | 2018-05-07 | 2022-11-01 | Apple Inc. | Raise to speak |
US10928918B2 (en) | 2018-05-07 | 2021-02-23 | Apple Inc. | Raise to speak |
US11169616B2 (en) | 2018-05-07 | 2021-11-09 | Apple Inc. | Raise to speak |
US11900923B2 (en) | 2018-05-07 | 2024-02-13 | Apple Inc. | Intelligent automated assistant for delivering content from user experiences |
US11907436B2 (en) | 2018-05-07 | 2024-02-20 | Apple Inc. | Raise to speak |
US11145294B2 (en) | 2018-05-07 | 2021-10-12 | Apple Inc. | Intelligent automated assistant for delivering content from user experiences |
US10984780B2 (en) | 2018-05-21 | 2021-04-20 | Apple Inc. | Global semantic word embeddings using bi-directional recurrent neural networks |
US10720160B2 (en) | 2018-06-01 | 2020-07-21 | Apple Inc. | Voice interaction at a primary device to access call functionality of a companion device |
US11495218B2 (en) | 2018-06-01 | 2022-11-08 | Apple Inc. | Virtual assistant operation in multi-device environments |
US10403283B1 (en) | 2018-06-01 | 2019-09-03 | Apple Inc. | Voice interaction at a primary device to access call functionality of a companion device |
US11431642B2 (en) | 2018-06-01 | 2022-08-30 | Apple Inc. | Variable latency device coordination |
US11630525B2 (en) | 2018-06-01 | 2023-04-18 | Apple Inc. | Attention aware virtual assistant dismissal |
US10892996B2 (en) | 2018-06-01 | 2021-01-12 | Apple Inc. | Variable latency device coordination |
US10984798B2 (en) | 2018-06-01 | 2021-04-20 | Apple Inc. | Voice interaction at a primary device to access call functionality of a companion device |
US11360577B2 (en) | 2018-06-01 | 2022-06-14 | Apple Inc. | Attention aware virtual assistant dismissal |
US11386266B2 (en) | 2018-06-01 | 2022-07-12 | Apple Inc. | Text correction |
US11009970B2 (en) | 2018-06-01 | 2021-05-18 | Apple Inc. | Attention aware virtual assistant dismissal |
US10944859B2 (en) | 2018-06-03 | 2021-03-09 | Apple Inc. | Accelerated task performance |
US10496705B1 (en) | 2018-06-03 | 2019-12-03 | Apple Inc. | Accelerated task performance |
US10504518B1 (en) | 2018-06-03 | 2019-12-10 | Apple Inc. | Accelerated task performance |
US11281439B2 (en) | 2018-07-25 | 2022-03-22 | Avaya Inc. | System and method for creating a contextualized after call workflow |
CN110784594A (en) * | 2018-07-25 | 2020-02-11 | 阿瓦亚公司 | System and method for creating a post-call contextualized workflow |
JP2020017279A (en) * | 2018-07-25 | 2020-01-30 | アバイア インコーポレーテッド | System and method for creating contextualized-after-call workflow |
EP3599545A1 (en) * | 2018-07-25 | 2020-01-29 | Avaya Inc. | System and method for creating a cntextualized after call workflow |
US11010561B2 (en) | 2018-09-27 | 2021-05-18 | Apple Inc. | Sentiment prediction from textual data |
US11893992B2 (en) | 2018-09-28 | 2024-02-06 | Apple Inc. | Multi-modal inputs for voice commands |
US11462215B2 (en) | 2018-09-28 | 2022-10-04 | Apple Inc. | Multi-modal inputs for voice commands |
US10839159B2 (en) | 2018-09-28 | 2020-11-17 | Apple Inc. | Named entity normalization in a spoken dialog system |
US11170166B2 (en) | 2018-09-28 | 2021-11-09 | Apple Inc. | Neural typographical error modeling via generative adversarial networks |
US11475898B2 (en) | 2018-10-26 | 2022-10-18 | Apple Inc. | Low-latency multi-speaker speech recognition |
CN109857791A (en) * | 2018-11-20 | 2019-06-07 | 成都材智科技有限公司 | A kind of batch data processing method and device |
US11638059B2 (en) | 2019-01-04 | 2023-04-25 | Apple Inc. | Content playback on multiple devices |
US11348573B2 (en) | 2019-03-18 | 2022-05-31 | Apple Inc. | Multimodality in digital assistant systems |
US11783815B2 (en) | 2019-03-18 | 2023-10-10 | Apple Inc. | Multimodality in digital assistant systems |
US11675491B2 (en) | 2019-05-06 | 2023-06-13 | Apple Inc. | User configurable task triggers |
US11217251B2 (en) | 2019-05-06 | 2022-01-04 | Apple Inc. | Spoken notifications |
US11307752B2 (en) | 2019-05-06 | 2022-04-19 | Apple Inc. | User configurable task triggers |
US11475884B2 (en) | 2019-05-06 | 2022-10-18 | Apple Inc. | Reducing digital assistant latency when a language is incorrectly determined |
US11423908B2 (en) | 2019-05-06 | 2022-08-23 | Apple Inc. | Interpreting spoken requests |
US11705130B2 (en) | 2019-05-06 | 2023-07-18 | Apple Inc. | Spoken notifications |
US11140099B2 (en) | 2019-05-21 | 2021-10-05 | Apple Inc. | Providing message response suggestions |
US11888791B2 (en) | 2019-05-21 | 2024-01-30 | Apple Inc. | Providing message response suggestions |
US11496600B2 (en) | 2019-05-31 | 2022-11-08 | Apple Inc. | Remote execution of machine-learned models |
US11237797B2 (en) | 2019-05-31 | 2022-02-01 | Apple Inc. | User activity shortcut suggestions |
US11657813B2 (en) | 2019-05-31 | 2023-05-23 | Apple Inc. | Voice identification in digital assistant systems |
US11289073B2 (en) | 2019-05-31 | 2022-03-29 | Apple Inc. | Device text to speech |
US11360739B2 (en) | 2019-05-31 | 2022-06-14 | Apple Inc. | User activity shortcut suggestions |
US11360641B2 (en) | 2019-06-01 | 2022-06-14 | Apple Inc. | Increasing the relevance of new available information |
US11790914B2 (en) | 2019-06-01 | 2023-10-17 | Apple Inc. | Methods and user interfaces for voice-based control of electronic devices |
US11488406B2 (en) | 2019-09-25 | 2022-11-01 | Apple Inc. | Text detection using global geometry estimators |
US20210104232A1 (en) * | 2019-10-07 | 2021-04-08 | Samsung Electronics Co., Ltd. | Electronic device for processing user utterance and method of operating same |
US11605385B2 (en) * | 2019-10-31 | 2023-03-14 | International Business Machines Corporation | Project issue tracking via automated voice recognition |
US11914848B2 (en) | 2020-05-11 | 2024-02-27 | Apple Inc. | Providing relevant data items based on context |
US11924254B2 (en) | 2020-05-11 | 2024-03-05 | Apple Inc. | Digital assistant hardware abstraction |
US11765209B2 (en) | 2020-05-11 | 2023-09-19 | Apple Inc. | Digital assistant hardware abstraction |
US11810578B2 (en) | 2020-05-11 | 2023-11-07 | Apple Inc. | Device arbitration for digital assistant-based intercom systems |
US11838734B2 (en) | 2020-07-20 | 2023-12-05 | Apple Inc. | Multi-device audio adjustment coordination |
US11750962B2 (en) | 2020-07-21 | 2023-09-05 | Apple Inc. | User identification using headphones |
US11696060B2 (en) | 2020-07-21 | 2023-07-04 | Apple Inc. | User identification using headphones |
US11776542B1 (en) | 2021-03-30 | 2023-10-03 | Amazon Technologies, Inc. | Selecting dialog acts using controlled randomness and offline optimization |
US11978445B1 (en) * | 2021-03-30 | 2024-05-07 | Amazon Technologies, Inc. | Confidence scoring for selecting tones and text of voice browsing conversations |
Also Published As
Publication number | Publication date |
---|---|
CN107850992A (en) | 2018-03-27 |
EP3362885A1 (en) | 2018-08-22 |
US10891106B2 (en) | 2021-01-12 |
WO2017065985A1 (en) | 2017-04-20 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US10891106B2 (en) | Automatic batch voice commands | |
US11749266B2 (en) | Voice commands across devices | |
US9930167B2 (en) | Messaging application with in-application search functionality | |
US9100337B1 (en) | Enabling cookies for a website | |
US9600258B2 (en) | Suggestions to install and/or open a native application | |
EP3123386B1 (en) | Granting permission in association with an application | |
EP3030989B1 (en) | Providing information in association with a search field | |
US9270760B2 (en) | Cross-platform child mode for applications | |
US20170024765A1 (en) | Adaptive user suggestions in a social networking service | |
US20180129396A1 (en) | Providing shortcut assistance for launching applications | |
US10445413B2 (en) | Sharing links which include user input | |
US20160070770A1 (en) | Suggesting social groups from user social graphs | |
US20140337404A1 (en) | System and method for providing access points | |
US9721032B2 (en) | Contextual URL suggestions | |
CA3005631C (en) | Secondary computing device assistant | |
US10095773B1 (en) | Processing a social endorsement for an item | |
US9160613B1 (en) | Ranking plural cookies | |
KR20160054687A (en) | Bookmark Management Method and System Using Messenger |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:KUSCHER, ALEXANDER FRIEDRICH;BALASUBRAMANIAN, SANTHOSH;ZHA, TIANTIAN;SIGNING DATES FROM 20150921 TO 20151006;REEL/FRAME:036838/0244 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044129/0001Effective date: 20170929 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: FINAL REJECTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE AFTER FINAL ACTION FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: PUBLICATIONS -- ISSUE FEE PAYMENT VERIFIED |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |