US10719509B2 - Hierarchical quantization for fast inner product search - Google Patents
Hierarchical quantization for fast inner product search Download PDFInfo
- Publication number
- US10719509B2 US10719509B2 US15/290,198 US201615290198A US10719509B2 US 10719509 B2 US10719509 B2 US 10719509B2 US 201615290198 A US201615290198 A US 201615290198A US 10719509 B2 US10719509 B2 US 10719509B2
- Authority
- US
- United States
- Prior art keywords
- vector
- database
- residual
- query
- quantization
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
- 238000013139 quantization Methods 0.000 title claims description 190
- 239000013598 vector Substances 0.000 claims abstract description 283
- 238000000034 method Methods 0.000 claims abstract description 82
- 230000015654 memory Effects 0.000 claims description 50
- 230000009466 transformation Effects 0.000 claims description 29
- 238000005192 partition Methods 0.000 claims description 13
- 230000001131 transforming effect Effects 0.000 claims description 8
- 230000004044 response Effects 0.000 claims description 5
- 238000000638 solvent extraction Methods 0.000 claims description 5
- 239000000047 product Substances 0.000 description 117
- 230000008569 process Effects 0.000 description 31
- 238000004891 communication Methods 0.000 description 13
- 239000011159 matrix material Substances 0.000 description 12
- 238000010586 diagram Methods 0.000 description 8
- 238000004590 computer program Methods 0.000 description 7
- 230000003287 optical effect Effects 0.000 description 6
- 238000012545 processing Methods 0.000 description 6
- 230000008901 benefit Effects 0.000 description 4
- 230000006870 function Effects 0.000 description 4
- 238000004422 calculation algorithm Methods 0.000 description 3
- 238000004364 calculation method Methods 0.000 description 3
- 230000009467 reduction Effects 0.000 description 3
- 238000005070 sampling Methods 0.000 description 3
- ORILYTVJVMAKLC-UHFFFAOYSA-N Adamantane Natural products C1C(C2)CC3CC1CC2C3 ORILYTVJVMAKLC-UHFFFAOYSA-N 0.000 description 2
- 238000013528 artificial neural network Methods 0.000 description 2
- 238000012512 characterization method Methods 0.000 description 2
- 239000002131 composite material Substances 0.000 description 2
- 230000007423 decrease Effects 0.000 description 2
- 230000006872 improvement Effects 0.000 description 2
- 238000005457 optimization Methods 0.000 description 2
- 239000004065 semiconductor Substances 0.000 description 2
- 239000007787 solid Substances 0.000 description 2
- 239000000758 substrate Substances 0.000 description 2
- 238000012549 training Methods 0.000 description 2
- 230000009471 action Effects 0.000 description 1
- 239000006227 byproduct Substances 0.000 description 1
- 230000001413 cellular effect Effects 0.000 description 1
- 238000013145 classification model Methods 0.000 description 1
- 238000007796 conventional method Methods 0.000 description 1
- 230000001419 dependent effect Effects 0.000 description 1
- 230000000694 effects Effects 0.000 description 1
- 238000011156 evaluation Methods 0.000 description 1
- 238000012804 iterative process Methods 0.000 description 1
- 238000010801 machine learning Methods 0.000 description 1
- 238000013507 mapping Methods 0.000 description 1
- 239000000463 material Substances 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000006855 networking Effects 0.000 description 1
- 230000000135 prohibitive effect Effects 0.000 description 1
- 230000004043 responsiveness Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/24—Querying
- G06F16/245—Query processing
- G06F16/2453—Query optimisation
- G06F16/24534—Query rewriting; Transformation
- G06F16/24537—Query rewriting; Transformation of operators
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F17/00—Digital computing or data processing equipment or methods, specially adapted for specific functions
- G06F17/10—Complex mathematical operations
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/22—Indexing; Data structures therefor; Storage structures
- G06F16/2228—Indexing structures
- G06F16/2237—Vectors, bitmaps or matrices
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/24—Querying
- G06F16/245—Query processing
- G06F16/2457—Query processing with adaptation to user needs
- G06F16/24578—Query processing with adaptation to user needs using ranking
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/28—Databases characterised by their database models, e.g. relational or object models
- G06F16/284—Relational databases
- G06F16/285—Clustering or classification
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/33—Querying
- G06F16/3331—Query processing
- G06F16/334—Query execution
- G06F16/3347—Query execution using vector based model
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/35—Clustering; Classification
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/23—Clustering techniques
- G06F18/231—Hierarchical techniques, i.e. dividing or merging pattern sets so as to obtain a dendrogram
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/23—Clustering techniques
- G06F18/232—Non-hierarchical techniques
- G06F18/2321—Non-hierarchical techniques using statistics or function optimisation, e.g. modelling of probability density functions
- G06F18/23213—Non-hierarchical techniques using statistics or function optimisation, e.g. modelling of probability density functions with fixed number of clusters, e.g. K-means clustering
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/24—Classification techniques
- G06F18/241—Classification techniques relating to the classification model, e.g. parametric or non-parametric approaches
- G06F18/2413—Classification techniques relating to the classification model, e.g. parametric or non-parametric approaches based on distances to training or reference patterns
- G06F18/24133—Distances to prototypes
- G06F18/24137—Distances to cluster centroïds
-
- G06K9/6219—
-
- G06K9/6223—
-
- G06K9/6272—
Definitions
- Implementations provide a fast approximation of the inner product that is orders of magnitude faster than a brute-force method while retaining high accuracy and a small memory footprint.
- the method includes a hierarchical quantization of database items, the hierarchy including at least two layers.
- the first layer is vector quantization (VQ) and the second layer is product quantization (PQ).
- the system may perform an orthogonal transformation on residuals between the quantization layers.
- the system may quantize database vectors (e.g., database items represented in dense, high dimensional points in a vector space) via vector quantization.
- the database items may be clustered and a cluster center determined for each cluster and entered into a codebook (the VQ codebook or first-layer codebook).
- a codebook the VQ codebook or first-layer codebook.
- Each of the database items is then mapped to its respective cluster via a VQ code word which represents the corresponding entry in the VQ codebook.
- the system may then determine a residual for the database item, the residual being the difference between the cluster center that the database item is mapped to, i.e., the entry in the VQ codebook the item maps to, and the database vector.
- the residual has a much smaller diameter than the database items, which yields significant reductions in quantization error.
- the system may then transform the residuals via a learned transformation.
- the learned transformation lowers the variance within each subspace of product quantization, which substantially lowers quantization error and results in a higher recall.
- the transformed residual may be submitted to product quantization, where each transformed residual is projected into subspaces and each subspace (or chunk) of the residual is assigned an entry in a PQ codebook (using a PQ code word) for the subspace.
- the PQ codebook may also be referred to as the second-layer codebook.
- clusters are generated and each transformed residual mapped to one of the clusters for the subspace.
- Each subspace has its own PQ code words (i.e., a different cluster assignment).
- the final quantized representation of the database item is a concatenation of the code words for each layer, e.g., an entry into a VQ codebook, and a PQ code word for each subspace.
- the additional VQ code word would be concatenated after the first VQ code word.
- the VQ codebook and the PQ codebooks may be jointly learned with the transformation by minimizing quantization error over the database.
- the system computes the inner product between the query vector and each of the VQ code words and the system selects one or more of the VQ code words that are most similar to the query based on the results of the inner product computation.
- the system may then compute the residual for the query, e.g., the difference between the query vector and the VQ code word most similar to the query. If more than one VQ code word is selected, the system may generate a residual query vector for each selected VQ code word.
- the system may then transform the residual query vector (or vectors) with the learned transformation. In some implementations, the system may submit the transformed residual to another layer (e.g., additional vector quantization).
- the system may project the transformed residual query vector into subspaces and compare the query vector projection to the quantized database entries that are mapped to the same VQ code word, one PQ subspace at a time. For example, the system may select any quantized database items with the same VQ code word and, for a first chunk (a PQ subspace) of the query, determine the cluster identifier, i.e., a PQ code word, for the first subspace of each of the selected quantized database items, and use that identifier to identify the cluster center in the PQ codebook in the subspace. The system may then perform the dot product between the chunk of the query and the PQ codebook entry for the quantized data item.
- the cluster identifier i.e., a PQ code word
- the result of the dot product is a similarity score for the subspace and the similarity between the query and the database item is the sum of the dot product for all subspaces.
- the system may repeat this for any database items mapped to a VQ code word selected for the query.
- the system only performs an inner product with a portion of the full database vectors, improving query response time.
- the VQ and PQ codebooks and the transformation may be jointly learned using stochastic gradient descent. At each iteration, the gradient of quantization error is computed on a mini-batch of data for a fixed assignment of data points to code words. After performing a descent step, code word assignments for the data points are recomputed.
- the transformation may be initialized by sampling from (0.0.1) and parameterized via the Cayley characterization of orthogonal matrices.
- a computer system includes at least one processor and memory storing a database of quantized items.
- Each of the quantized items includes a first entry into a first code book and a plurality of second entries into a second code book, where each of the plurality of second entries represents a respective subspace of k subspaces.
- the memory also includes instructions that, when executed by the at least one processor, cause the system to perform operations.
- the operations can include determining a cluster center from the first code book that is most similar to a query vector, calculating a residual vector from the cluster center and the query vector, transforming the residual vector using a learned transformation, and projecting the transformed residual vector into the k subspaces.
- the operations can also include, for each of the quantized items having a first index that corresponds to the cluster center determined for the query vector, calculating, for each subspace, an inner product between the quantized item and the transformed residual vector, and calculating a similarity score between the quantized item and the query by summing the inner products.
- the operations may also include providing items with highest similarity scores in response to the query.
- a method includes clustering a data store of database items represented as high-dimensionality vectors and selecting a cluster center for each cluster and storing the cluster center as an entry in a first layer codebook.
- the method may also include, for each of the database items, calculating a residual based on the cluster center for the cluster the database item is assigned to, projecting the residual into subspaces, determining, for each of the subspaces, an entry in a second layer codebook for the subspace, and storing the entry in the first layer codebook and the respective entry in the second layer codebook for each of the subspaces as a quantized vector for the database item.
- a method can include partitioning vectors in a database into m partitions using vector quantization, so that each vector has an assigned vector quantization code word and calculating, for each of the vectors, a respective residual, the residual being the difference between the vector and a cluster center corresponding to the vector quantization code word.
- the method may further include applying product quantization to each of the residuals, producing, for each residual, a product quantization code word for each of k subspaces, storing, for each vector, the assigned vector quantization code word and k product quantization code words for the residual of the vector, and using the vector quantization code words to select a portion of the database vectors most similar to a query vector.
- the method may also include, for each of the database vectors in the portion, using the product quantization code words to determine database vectors from the portion most similar to the query vector.
- a computer program product embodied on a computer-readable storage device includes instructions that, when executed by at least one processor formed in a substrate, cause a computing device to perform any of the disclosed methods, operations, or processes disclosed herein.
- implementations provide fast maximum inner product searches on large, dense, high dimensional datasets.
- datasets are often associated with recommendation or classification systems, such finding an image, video, or product similar to a query image, video, or product.
- Another example of such a problem is a classification model that computes the probability of nearby words given a target word using inner product. The search avoids a full scan of the dataset with minimal loss of recall while providing high performance on modern CPU architectures.
- Implementations include a hierarchical combination of vector quantization and product quantization that greatly reduces error in approximating inner products for large, dense, high-dimensional datasets with low latency (e.g., faster processing time).
- the codebooks and transformation can be jointly trained end-to-end, which results in lower approximation error in representing the dataset, improving recall.
- Some implementations provide an in-register lookup table to compute the inner product between subspaces of the query and the quantized database items, which exploits the instruction level parallelism capabilities of modern processors and provides significant improvement over in-memory lookups.
- the final complexity of the search is
- n O ⁇ ( nkt m )
- k is the number of subspaces
- m is the number of vector quantizers (e.g., the number of entries in the VQ code book)
- t is the number of VQ code words selected for the query vector
- n is the number of database items.
- FIG. 1 illustrates an example system in accordance with the disclosed subject matter.
- FIG. 2 illustrates a block diagram of hierarchical quantization of database vectors representing search items, in accordance with disclosed subject matter.
- FIG. 3 illustrates a flow diagram of an example process for performing hierarchical quantization on a database item, according to an implementation.
- FIG. 4 illustrates a flow diagram of an example query process using hierarchical quantization and maximum inner-product search, according to an implementation.
- FIG. 5 illustrates a flow diagram of an example process for jointly learning an orthogonal transformation and codebooks for the hierarchical quantization, in accordance with disclosed subject matter.
- FIGS. 6A to 6D are graphs illustrating the benefits of various implementations.
- FIG. 7 shows an example of a computer device that can be used to implement the described techniques.
- FIG. 8 shows an example of a distributed computer device that can be used to implement the described techniques.
- FIG. 1 is a block diagram of a scalable inference system in accordance with an example implementation.
- the system 100 may be used to hierarchically quantize a database of items and compute an inner-product with a query vector to find relevant database items for use in applications like recommendation systems, categorization in machine learning algorithms, and other systems that use a nearest neighbor computation.
- the system 100 jointly learns the codebooks for the hierarchical levels and reduces the processing time needed to perform the inner product search while still retaining high quality results.
- the depiction of system 100 in FIG. 1 is described as a server-based search system. However, other configurations and applications may be used. For example, some operations may be performed on a client device.
- system 100 is described as a search system
- the methods and techniques of disclosed implementations can be used in any task that uses a Maximum Inner Product, such as classifications performed in the last layer of neural networks with a large number (e.g., millions) of output nodes, for example.
- implementations are not limited to search systems but can be used in any system addressing a MIPS problem.
- the scalable inference system 100 may be a computing device or devices that take the form of a number of different devices, for example a standard server, a group of such servers, or a rack server system, such as server 110 .
- system 100 may be implemented in a personal computer, for example a laptop computer.
- the server 110 may be an example of computer device 700 , as depicted in FIG. 7 or computer device 800 , as depicted in FIG. 8 .
- the server 110 can include one or more processors formed in a substrate configured to execute one or more machine executable instructions or pieces of software, firmware, or a combination thereof.
- the processors can be semiconductor-based—that is, the processors can include semiconductor material that can perform digital logic.
- the processors may also include registers capable of performing data-level parallelism, e.g., single-instruction multiple-data (SIMD) registers.
- SIMD single-instruction multiple-data
- the server 110 can also include an operating system and one or more computer memories, for example a main memory, configured to store one or more pieces of data, either temporarily, permanently, semi-permanently, or a combination thereof.
- the memory may include any type of storage device that stores information in a format that can be read and/or executed by the one or more processors.
- the memory may include volatile memory, non-volatile memory, or a combination thereof, and store modules that, when executed by the one or more processors, perform certain operations.
- the modules may be stored in an external storage device and loaded into the memory of server 110 .
- the modules may include a quantization engine 126 and a query engine 120 .
- the quantization engine 126 may hierarchically quantize a database of database items 132 and, in the process, generate a codebook for each level in the hierarchy, e.g., VQ codebook 134 , PQ codebooks 136 .
- the result of the hierarchical quantization is quantized database items 130 .
- the VQ codebook 134 represents a first layer in the hierarchy and the PQ codebooks 136 represent a second layer.
- the quantization engine 126 may generate additional codebooks, e.g., one per layer.
- the system 100 may generate a second VQ codebook for the additional vector quantization layer.
- the resulting quantized database items 130 and the codebooks e.g., VQ codebook 134 and PQ codebook 136 , has a smaller memory footprint than the database of database items 132 .
- the database items 132 may be a database of vectors.
- a vector may be thought of as an array of floating point numbers with a dimensionality of d, or in other words an array with d positions.
- a query, such as query 182 may also be expressed as vectors of dimension d.
- the quantization engine 126 may quantize the database items 132 .
- the quantization engine 126 may first perform vector quantization on the database items 132 to assign each database item an entry in the VQ codebook 134 .
- each database item 132 has a respective VQ code word, the code word identifying an entry in the VQ codebook 134 .
- the quantization engine 126 may then determine a residual vector for each database item, the residual vector being the difference between the database item vector and the cluster center, e.g., the vector corresponding to the entry in the VQ codebook 134 that the database item is mapped to.
- the quantization engine 126 may then transform the residual using a learned rotation.
- the learned rotation reduces the variance in each subspace of the subsequent product quantization, which yields substantially lower quantization error and higher recall.
- the quantization engine 126 may then further quantize the rotated residual vector via product quantization, or in other words project the transformed residual into subspaces and map each subspace to an entry in the PQ codebook generated for the subspace.
- each subspace of the transformed residual has a respective PQ code word, the PQ code word for a subspace identifying an entry in the PQ codebook for that subspace.
- the system may store the VQ code word and the respective PQ code word for each subspace as a quantized database item in quantized database items 130 .
- FIG. 2 illustrates example database items 132 and quantized database items 130 .
- the database items 132 include n entries, or n distinct database items.
- Each database item in database items 132 e.g., database vector (DBV) 1 , DBV 2 , etc.
- the quantization engine 126 may first quantize the database items 132 via vector quantization using VQ codebook 134 .
- VQ codebook 134 In vector quantization each database vector is assigned a VQ code word from the VQ codebook 134 .
- a VQ code word represents a specific entry in the VQ codebook 134 , and each entry in the VQ codebook 134 is a cluster center, or in other words a data point that best represents the cluster.
- the vector quantization associates each database item with a cluster.
- the VQ codebook 134 is learned and generated by the quantization engine 126 and the assignments of the database items may occur concurrently with the generation of the VQ codebook 134 .
- the VQ codebook 134 may have M entries, which may be determined when the VQ codebook 134 is learned or may be provided as a parameter.
- the database items 132 may be partitioned into M partitions.
- the quantization engine 126 stores the VQ code word for each database item as the first portion of a quantized database item in quantized database items 130 . This represents the first layer in the hierarchical quantization.
- the quantization engine may use the VQ code word and the VQ codebook 134 to generate a residual for each database item.
- a residual is the difference between the database vector and the cluster center associated with the database vector. The difference may be calculated by subtracting the database vector from the cluster center vector (or vice versa).
- the quantization engine 126 may use vector quantization to obtain a residual dataset, e.g., database item residuals 232 , that has a much smaller diameter than the original vectors, e.g., database items 132 .
- the database item residual 232 still have d dimensions, but the variance within the values of the floating point numbers in the vector is reduced.
- the quantization engine 126 may only store these residuals 232 temporarily because they are further quantized in another layer of the hierarchy. The smaller diameter yields significant reductions in quantization error when further quantized using product quantization (the second layer).
- the quantization engine 126 may rotate the database item residuals 232 using a learned rotation.
- the learned rotation may be used to achieve an optimal distribution of information to the various subspaces generated by product quantization.
- the learned rotation may be optimized to reduce error using stochastic gradient descent.
- the rotation may be learned jointly with the VQ codebook 134 and the PQ codebooks 136 to minimize the quantization error.
- the learned rotation thus provides smaller quantization error over random rotation.
- no rotation or random rotation may be performed. If a database item residual vector 232 is subjected to learned rotation, it is referred to as transformed, or a transformed residual.
- the transformed residual may undergo another round of vector quantization, adding layers to the hierarchy. After each vector quantization, the system may again compute a residual, which may undergo the product quantization.
- the quantization engine 126 projects the database item residuals 232 , which may be transformed, into K subspaces.
- a subspace is a block of elements from each residual database item vector occurring at the same vector position.
- d is divisible by K so that each block includes the same number of elements.
- FIG. 2 Such an implementation is shown in FIG. 2 , where each subspace is a block of six elements.
- direct division may result in subspaces where the number of elements in each subspace is not equal.
- division may be based on random or learned projection of the vectors.
- variably sized subspaces may be generated by assigning the first mod (d, K) subspaces an extra dimension each. In such implementations, the number of elements in each block may not be equal.
- the projection or division results in K subspaces or chunks, each subspace having n rows of six elements.
- the quantization engine 126 may generate PQ codebooks 136 .
- the PQ codebooks 136 may include one codebook for each subspace.
- the PQ codebooks 136 include K PQ codebooks.
- the PQ codebook for each subspace may be referred to collectively as a PQ codebook for the database items 132 .
- Each PQ codebook may include an entry for each of J clusters.
- the quantity of clusters, J may be determined as the PQ codebooks 136 are generated or the quantity J may be passed in as a parameter to the quantization engine 126 .
- the parameter may indicate that the quantization engine 126 should generate 16 clusters for each codebook or 256 clusters for each codebook.
- the number of clusters (i.e., the value of J) may depend on the size of a register, e.g., a SIMD register. In other words, to improve computing time, the number of clusters may be limited to the number of parallel lookups a register in the system can perform.
- each cluster will have a cluster center. The cluster center is the entry for that cluster in the codebook.
- subspace K i.e., Chunk (K)
- K has six elements from each of n vectors (the transformed residuals).
- the quantization engine 126 may cluster the n vectors of six elements each into one of J clusters.
- the cluster center need not match a database vector subspace, but may represent six elements that serve as the cluster center.
- the codebook for the k th subspace may be represented by S (k) . Because each codebook has J entries, the j th entry in the codebook may be represented by S j (k) .
- the system may learn the cluster centers for the VQ codebook and the PQ codebooks with the rotation, jointly.
- the quantization engine 126 may use conventional clustering algorithms based on a Euclidean distance or k-means and use a stochastic gradient descent where, at each iteration the gradient of quantization error is computed on a mini-batch of data for a fixed assignment of data points to code words. After performing a descent step, code word assignments for the database items are recomputed. In this manner the assignment of code words to database items can take place concurrently with learning of the codebooks.
- the parameters of the skew-symmetric matrix A are d(d ⁇ 1)/2 so computing the transformation matrix R can involve d ⁇ d matrix inversion at each iteration. If d is high-dimensional (e.g., more than thousands) the system may restrict the number of parameters of A. This trades off capacity and computational cost.
- the quantization engine 126 may initialize the VQ codebook 134 using random samples from the database of database items 132 and may initialize the PQ codebooks using the residuals (e.g., after vector quantization) of a set of independent samples. To allow the vector quantization layer a chance to partition the space, the quantization engine 126 may optimize only the vector quantization error for several epochs before initializing the PQ codebooks 136 and doing full joint training. The quantization engine 126 may initialize the parameters of the skew-symmetric matrix A by sampling from N (0.0.1).
- the system may assign database vectors to an entry in the codebook via a M-dimensional (for the VQ codebook) or J-dimensional (for the PQ codebooks) one-hot assignment vector.
- a one-hot assignment vector for the vector x e.g., ⁇ x
- the assignment vectors for x may be the quantized database item.
- the assignment vector may be the code word.
- the dot product of the assignment vector and the codebook may thus provide the cluster center (e.g., the quantization) of the vector x (or of the k th subspace of vector x).
- the information in the quantized database item may be a pointer to the codebook entry.
- the quantization engine 126 may generate the quantized database item by concatenating the code words from the different levels. In the example of FIG. 2 , the VQ code word is first, followed by the PQ code word for each of the K subspaces. If the system 100 uses additional layers, the code word (or words) from each additional layer may be concatenated in the order of quantization.
- the quantization engine 126 may store the quantized database items 130 , the VQ codebook 134 , the PQ codebooks 136 and the learned transformation matrix R for use by the query engine 120 .
- the modules may thus include query engine 120 .
- the query engine 120 may be configured to use the codebooks and quantized database items 130 to identify database items 132 that are responsive to a query 182 and to provide a result 184 in response to the query 182 .
- the query engine 120 may include modules or engines that create a query vector from the query 182 using conventional techniques. The query engine 120 may determine which of the clusters from the VQ code book 134 that the query is closest to.
- this may include calculating an inner product between the query vector and each cluster center and selecting the cluster center with the maximum inner product.
- the query engine 120 may select more than one cluster center as “closest,” e.g., selecting the top t clusters with the highest inner product.
- the query engine 120 may determine a VQ code word (or words) for the query 182 .
- the query engine 120 may use the VQ code word (or words) to reduce the computational time of the inner product search by limiting the comparison of the query vector to only those quantized database items that share the VQ code word. Thus, rather than comparing the query vector to each database item, only those database items that share the VQ code word are considered.
- the query engine 120 selects multiple VQ code words, the quantized database items that correspond to the additional VQ code words would also be included in the comparison with the query 182 .
- the system calculates the residual for the query 182 , e.g., by subtracting the query vector from the cluster center that corresponds with the VQ code word. If the system 100 has transformed the quantized database items, the query engine 120 may also transform the residual query vector. The system may project the residual query vector into subspaces. The subspaces that the query residual is projected into match the subspaces that the database items 132 are projected onto. Thus, the residual query vector may have K subspaces.
- the query engine 120 may generate lookup table 138 .
- Lookup table 138 may store the result of the inner product of each cluster center in each subspace with the corresponding subspace of the query vector.
- the system may pre-compute the inner product between each data point in each PQ codebook and the corresponding residual query vector subspace and store the result in the lookup table 138 .
- This may result in a table or database where the result of the inner product can be accessed by knowing the PQ code word for any particular subspace (e.g., which cluster in which subspace).
- the lookup table 138 may be stored in-register, e.g., in SIMD registers.
- the query engine 120 may then determine the inner product of each quantized database item associated with a selected VQ code word and the query. To accomplish this, the query engine 120 may, for each examined quantized database vector, determine the PQ codebook assignment, i.e., the PQ code word, in each subspace and determine the inner product of the data point represented by the PQ codebook assignment in that subspace and the corresponding subspace of the residual query vector. In implementations that use the lookup table 138 , the system may do a lookup in the table for the PQ codebook entry and subspace. Thus—rather than perform the inner product operation between the query and the PQ codebook entry for a database item, the query engine 120 can use a lookup into the lookup table 138 .
- the PQ codebook assignment i.e., the PQ code word
- the lookup tables 138 may be stored in-register and the system may store the corresponding PQ code words for a database item in-register. In this manner the system may perform 16 parallel (or 32 parallel) lookups in one CPU cycle.
- the system may perform the inner product.
- the query engine 120 may approximate the inner product between the database item and the query as the sum of the results of the inner product in each subspace between the PQ portion (second portion, or second layer portions) of the quantized database item and the query.
- the approximated inner product between a query and DBV 1 is the sum of the inner product between quantized chunk ( 1 ) through quantized chunk (K) of the quantized search item 1 and the residual query vector. This may be represented by
- ⁇ k 1 K ⁇ S index ⁇ ( ⁇ PQ ⁇ ( r x ) ( k ) ) ( k ) where S (k) is the lookup table for the k th subspace, and ⁇ PQ(r x ) (k) is the PQ code word for the k th subspace of database item x.
- the search has determined the database items responsive to the query.
- the query engine 120 may include a ranking engine that orders the results 184 by the similarity score, i.e., the highest inner products.
- the query engine 120 may provide the results 184 for display at a client device, such as client 170 .
- the responsive database item may be used for other purposes, such as classification.
- Scalable inference system 100 may be in communication with client(s) 170 over network 160 .
- Clients 170 may allow a user to provide query 182 to the query engine 120 and to receive result 184 , which includes database items found responsive to the query based on the approximate inner product with the search query using the quantized database items.
- Network 160 may be for example, the Internet or the network 160 can be a wired or wireless local area network (LAN), wide area network (WAN), etc., implemented using, for example, gateway devices, bridges, switches, and/or so forth.
- the scalable inference system 100 may communicate with and transmit data to/from clients 170 .
- the client 170 may include applications, such as search app 175 that performs some or all of the functions of the query engine 120 .
- the quantized database items 130 do not take up much memory compared to database items 132 and may be of a size suitable for storage on the client, such as in data store 180 .
- Data store 180 may include any type of non-volatile memory, such as flash, SD, RAM, disk, etc.
- the server 110 may transmit the quantized database items 130 , the VQ code book 134 , and the PQ codebooks 136 to the client 170 and the search application 175 may perform the actions described above with regard to query engine 120 .
- the client 170 may be another server or system.
- Client 170 may be another example of computing device 800 or computing device 700 .
- scalable inference system 100 may be in communication with or include other computing devices that provide updates to the database items 132 .
- Scalable inference system 100 represents one example configuration and other configurations are possible.
- components of system 100 may be combined or distributed in a manner differently than illustrated.
- one or more of the query engine 120 and the quantization engine 126 may be combined into a single module or engine.
- components or features of the query engine 120 , the quantization engine 126 may be distributed between two or more modules or engines, or even distributed across multiple computing devices.
- database items 132 and/or quantized database items 130 may be distributed across multiple computing devices.
- FIG. 3 illustrates a flow diagram of an example process 300 for using hierarchical quantization to prepare a database of items for a fast maximum inner product search (MIPS), according to an implementation.
- Process 300 may be performed by a scalable inference system, such as system 100 of FIG. 1 .
- Process 300 is an example of hierarchical quantization of a single database item, performed by quantization engine 126 of FIG. 1 . It is understood that the system may also perform process 300 concurrently on all database vectors and that process 300 may be performed concurrently with the generating or learning of the codebooks for quantization, as described in more detail with regard to FIG. 5 .
- Process 300 may be performed periodically by the system so that the quantized database items and codebooks generated stay current. For example, the system may perform process 300 once a day, once a week, once an hour, etc. depending on how often the database of items is updated with new items.
- Process 300 may begin with the scalable inference system assigning each database item, i.e., each database vector an entry in a vector quantization (VQ) codebook ( 305 ).
- VQ codebook assigns each database item a VQ code word.
- the VQ code word points to an entry in the VQ codebook, which contains (or points to) a cluster center.
- the VQ code word can also be referred to as a cluster identifier.
- the cluster center is a vector of the same dimensionality as the database item vectors and is most representative of the database items in the cluster.
- the VQ codebook may be generated via a learning process, which may also perform the mapping of database items to VQ codebook entries.
- the system may calculate a residual vector for each of the database items ( 310 ).
- the residual vector is the difference between the database item vector and the cluster center corresponding to the database item's VQ code word.
- Real world data is often clusterable, with the diameter of clusters substantially lower than the diameter of the dataset as a whole.
- the system may use vector quantization to obtain the residual dataset, which has a much smaller diameter, yielding significant reductions in quantization error when quantized with product quantization.
- Hierarchical quantization thus takes advantage of vector quantization, which is well-suited to approximating the low dimensional component, and product quantization, which is well-suited to capture high dimensional data from the residuals.
- the system may perform a learned rotation or transformation on the residual vector ( 315 ).
- the rotation is learned jointly with the codebooks.
- the learned rotation provides better recall, as demonstrated by FIG. 6C .
- the learned rotation may be a matrix R ⁇ d ⁇ d applied to the residuals of the vector quantization.
- the transformation may be a random, but fixed, permutation. In other words, the permutation is randomly generated but once the permutation is generated it is fixed and can be applied to all database vectors and all query vectors. However, random transformation does not produce the same recall as a learned rotation.
- step 315 is optional and the residual vectors are left unchanged.
- the system may project each of the residual vectors into subspaces ( 320 ).
- each subspace may have an equal number of elements from the vector.
- the subspaces may not have equal numbers of elements.
- the subspaces may also be referred to as chunks.
- the system may assign each subspace an entry in a product quantization (PQ) codebook for the subspace ( 325 ).
- PQ codebook for a particular subspace thus includes an entry for each cluster, with the cluster center as the entry.
- the cluster center has the same number of elements as the portion of the residual vectors in the subspace.
- each subspace has a PQ codebook
- each codebook has J entries.
- the value of J may depend on parameters provided to the procedure that generates the PQ codebooks or the procedure may determine the value based on the data.
- the value of J may depend on the capacity of a register, e.g., a SIMD register or other register.
- the value of J may be 16 so that a single register can hold the entire PQ codebook for the subspace k (e.g., S (k) .
- Each database vector subspace may be mapped or assigned to one of the J entries in the codebook for the subspace.
- a specific entry j in the PQ codebook for subspace k be represented as S j (k) .
- the assignment may occur as part of generating the codebook. For example, when clustering is used, each residual vector subspace may be assigned to one of the clusters, as the clusters are generated from the residual vectors for a subspace.
- the system may generate a quantized vector for each database vector by concatenating the VQ code word with each the PQ code word for each subspace ( 330 ).
- the VQ code word may be a code word for the first level of the hierarchy, and the PQ code words (one for each subspace) may be for a second level of the hierarchy.
- ⁇ PQ ⁇ ( r x ) ( ⁇ PQ ( 1 ) ⁇ ( r x ( 1 ) ) ⁇ PQ ( 2 ) ⁇ ( r x ( 2 ) ) ⁇ ⁇ PQ ( K ) ⁇ ( r x ( K ) ) )
- r x ( r x ( 1 ) r x ( 2 ) ⁇ r x ( K ) )
- ⁇ PQ (k) ( ⁇ ) the concatenation of code words obtained by dividing the rotated residuals r x into K subspaces 1 to K, and quantizing the subspaces independently by vector quantizers ⁇ PQ (k) ( ⁇ ) to minimize quantization error:
- ⁇ PQ (k) (r x (k) ) argmin s ⁇ s j (k) ⁇ ⁇ s ⁇ r x (k) ⁇ 2
- S (k) ⁇ d (k) xj is the PQ codebook for
- This representation i.e., the quantized database item, has an overall bitrate of log 2 m+K log 2 j, where m is the number of entries in the VQ codebook, j is the number of entries in each PQ codebook, and K is the number of subspaces.
- the system may store each quantized database item, along with the VQ codebook, the PQ codebooks, and the learned rotation R in a data store, database, or other memory.
- Process 300 then ends, having generated structures that can be used to approximate the maximum inner product between query items and the database items in a highly efficient manner and accurate manner.
- FIG. 3 illustrates a hierarchy with two layers
- the system may include more than two layers in the hierarchy.
- the system may perform one or more additional vector quantization layers or one or more additional product quantization layers, as needed and/or supported by the data.
- Each additional layer would add an additional code word to the quantized database item.
- each layer receives the residual calculated in the previous layer.
- FIG. 4 illustrates a flow diagram of an example process 400 for identifying responsive database items using hierarchical quantization, in accordance with disclosed subject matter.
- Process 400 may be performed by a scalable inference system, such as system 100 of FIG. 1 .
- Process 400 may be performed each time a query is received in order to determine the database items that have a maximum inner product with the query vector. Those items with the highest inner product are most responsive to the query, or in other words most like the query.
- the query vector may also represent an item to be categorized, e.g., as part of a last layer of a neural network with a large number of output nodes.
- Process 400 may begin with the system determining the inner product of the query vector and each entry in the VQ code book ( 405 ). This provides a VQ code word is that is most similar to the query.
- the system may select the VQ code book entry that is most similar (based on the result of the inner product calculation) and calculate a residual for the query ( 410 ), e.g., as the difference between the query vector and the selected VQ code book entry.
- the system may generate a residual for each selected VQ code book entry, so that each selected vector quantization entry has a respective residual query vector.
- the system may transform the residual query vector(s) ( 415 ). In implementations that use a transformation, the transformation or rotation is the same one used in step 315 of FIG. 3 .
- the scalable inference system may also project the residual query vector (or vectors) into subspaces ( 420 ). The projection of the query vector is done in the same manner as projection of the database item vectors as part of step 320 in FIG. 3 .
- the system may then optionally generate a lookup table ( 425 ).
- the lookup table may include one entry for each entry of each PQ codebook.
- the system may perform, for each subspace (i.e., each PQ codebook), an inner product between the elements of the residual query vector in the subspace and the elements of each PQ codebook entry in the subspace.
- each subspace i.e., each PQ codebook
- the lookup table will have J entries for that subspace.
- the system may use the lookup table to speed calculation of the inner products with quantized database items as part of step 435 below, but use of a lookup table is optional.
- the lookup table is stored in registers and value of J is constrained by characteristics of the registers, e.g., is 16 or 32.
- the system may then compute similarity score for each quantized database item that shares the VQ code word (the VQ codebook entry) selected in step 410 . Accordingly, the system may select a quantized database item that shares the VQ code word ( 430 ) and calculate, for each subspace, the inner product between the residual query elements in that subspace and the quantized database item elements for the subspace ( 435 ), which is represented by a PQ codebook entry assignment in the subspace, e.g. Quantized chunk 1 or Quantized Chunk (K) of FIG. 2 .
- the system may determine the PQ codebook entry from the subspace of the quantized database item, determine the data point (e.g., cluster center) for the PQ codebook entry, and compute the inner product between the residual query subspace and the data point.
- the system may determine the PQ codebook entry for the quantized database item subspace and lookup the inner product result for that PQ codebook entry in the lookup table.
- the system may calculate a similarity score for the database item ( 440 ) by summing the inner product of each subspace, as calculated in step 435 .
- the similarity score is an approximate inner product between the quantized database item and the query.
- the system may sum K values, each representing an inner product calculation for a subspace.
- the system may repeat steps 430 to 440 ( 445 , Yes) until a similarity score has been computed for each database item ( 445 , No) mapped to the same VQ code word as the residual query vector.
- steps 430 to 440 may also be expressed as
- the system can exploit the instruction level parallelism capabilities of the central processors (CPUs). For example, the system may use one register (e.g., a SIMD register) to hold a lookup table v (k) and another to register to hold the indices of PQ code words for a given quantized database item.
- the system may use register instructions to perform several parallel lookups, e.g., 16 parallel lookups (PSHUFB, SSSE3) or 32 parallel lookups (VPSHUFB, AVX2). This represents a significant improvement over in-memory codebook lookup tables, which only have a throughput of one lookup per CPU cycle.
- the system may repeat steps 425 to 445 for the other VQ code words.
- the system may then return the database items, e.g., identifiers that identify the database items or the database vectors themselves, that have the highest similarity scores ( 450 ).
- the VQ code word is used to decrease the quantity of database items for which the query vector is compared, e.g., via the inner product operation. This decreases the processing time improving the responsiveness of the system while the product quantization of the residual provides a high degree of accuracy.
- the complexity of a search performed by the system may be expressed as
- n O ⁇ ( nkt m )
- k is the number of subspaces
- m is the number of vector quantizers (e.g., the number of entries in the VQ code book)
- t is the number of VQ code words selected for the query vector
- n is the number of database items.
- the system may rescore the top scoring database items using an exact dot product computation.
- the system may compute an exact dot product for the items that have highest similarity scores and use the exact dot products to determine the database items that will be presented to the query requestor.
- the system may use N items as the search result for the query requestor and compute the exact dot product between the query vector and the database vectors for the top 10*N database items, e.g., those 10*N with the highest similarity scores as determined using the quantized vectors.
- the system may then use the top N database items with the highest actual dot product. This increases the accuracy of the search result, but requires much less time to determine than computing dot products for all database items.
- the system may provide a search result that includes information about those items for display to the user who provided the query. Process 400 then ends, having identified the most responsive items.
- FIG. 5 illustrates a flow diagram of an example process 500 for jointly learning an orthogonal transformation and codebooks for hierarchical quantization, in accordance with disclosed subject matter.
- Process 500 may be performed by a scalable inference system, such as system 100 of FIG. 1 .
- Process 500 trains and optimizes task-dependent objective functions to predict clusters in each hierarchical layer and an optional learned rotation starting from random samples from the database.
- the system uses stochastic gradient descent where, at each iteration, the gradient of quantization error is computed on a mini-batch of data for a fixed assignment of data points (database items) to code words. After performing a descent step, code word assignments for the data points are recomputed.
- process 500 uses an iterative process to alternate between solving the codebooks for each layer and the assignments of the database items to a codebook entry.
- Process 500 may be performed as part of or concurrently with process 300 of FIG. 3 .
- Process 500 may begin with the scalable inference system assigning each VQ codebook entry a random database vector ( 505 ).
- the system may optimize the vector quantization error for several epochs using stochastic gradient descent on a mini-batch of data ( 510 ). This allows the vector quantization a chance to partition the space prior to initializing the PQ codebook entries and doing full joint training.
- the system may initialize the PQ codebook entries by, generating a residual for a set of independent samples from vector quantization, projecting the residuals into subspaces and assigning entries in the PQ codebooks values from the respective subspaces of the residuals ( 515 ).
- the system may also initialize the rotation matrix by populating a skew-symmetric matrix using a sampling from N (0.0.1).
- the system may then optimize the vector quantization error, transformation error, and product quantization error using stochastic gradient descent on a mini-batch (e.g., 2000 items) of data ( 525 ). This may include finding a set of violated constraints, (but not necessarily all violated constraints), adjusting the codebook assignments for detected violations using gradient descent so that the violations no longer appear as having an approximation that is larger than the database item with the largest dot product.
- a mini-batch e.g. 2000 items
- This may include finding a set of violated constraints, (but not necessarily all violated constraints), adjusting the codebook assignments for detected violations using gradient descent so that the violations no longer appear as having an approximation that is larger than the database item with the largest dot product.
- a violated constraint occurs when the approximate dot product generated using the hierarchical layer (i.e., using the codebooks and transformation) indicates a value between a first quantized database item and the query is greater than that of a value between a second quantized database item and the query, but the second database item (i.e., the original database item vector) actually has a highest dot product with the query.
- the approximation indicates the first database item has a higher similarity than the second database item, but the actual inner product of the query and the second item is the most similar (has the maximum inner product).
- the system may use the Adam optimization algorithm, described in Kingma et al, “Adam: A method for stochastic optimization,” CoRR, abs/1412.6980, 2014, to optimize the parameters.
- the system may determine whether additional iterations of the above steps are needed ( 530 ). If no violations were found in step 525 , the iterations may be complete. If the iterations reach a set number (e.g., 30), the iterations may be complete. If the iterations are not complete ( 530 , No), the system may continue adjusting the parameters by looking for violations, adjusting the assignments, and adjusting the codebook. If the iterations are complete ( 530 , Yes), process 500 ends, having generated the VQ codebook, the PQ codebooks, and the learned transformation. When the system includes additional layers in the hierarchy, the additional codebooks are learned jointly in a similar manner.
- FIGS. 6A to 6D illustrate the benefits of implementations that use hierarchical quantization.
- evaluations of hierarchical quantization and other quantization processes are compared across four benchmark databases.
- Table 1 illustrates the characteristics of the four databases:
- FIG. 6A illustrates a comparison of the efficiency of different distance computations. Specifically, FIG. 6A illustrates time spent (in ⁇ s) per query in a linear search of a database.
- hierarchical quantization using an in-register codebook lookup table LUT16 is compared with 1) Hamming Distance of binary codes (using XOR and POPCNT instructions; and 2) Asymmetric Distance to a product quantized code (PQ, using an in-memory lookup table). All three use the same number of bits (64): Hamming uses 64-bit binary codes, PQ uses 8 subspaces with 256 quantizers in each subspace, and LUT16 uses 16 subspaces with 16 quantizers in each subspace.
- the timing includes both distance computation and Top-N selection.
- LUT16 out-performs both PQ and Hamming and hierarchical quantization (LUT16) is significantly faster than PQ with in-memory lookup tables (by a factor of 5 in larger databases) and is slightly faster than Hamming distance computations.
- FIG. 6B illustrates Precision/Recall curves when retrieving Top-10 neighbors on all four databases.
- FIG. 6B compares hierarchical quantization using in-register codebook lookup tables (Hierarchical) with four baseline methods: Signed ALSH, Simple LSH, Composite Quantization, and QUIPS.
- FIG. 6B illustrates that hierarchical quantization tends to significantly outperform all four baseline methods, with better performance on larger databases. Thus hierarchical quantization is highly scalable.
- FIG. 6C illustrates recall@N for retrieving Top-10 neighbors, comparing hierarchical quantization with and without the learned rotation of the residuals on the largest database (e.g., word2vec_wiki).
- FIG. 6C illustrates that the learned rotation gives a significant boost to recall.
- FIG. 6D illustrates recall@N for retrieving Top-10 neighbors when searching different fractions of the VQ partitions on the largest database.
- a system using hierarchical quantization partitions the database using vector quantization into m partitions and only searches those database items that share the same VQ code words selected for the query item (e.g., at step 410 of FIG. 4 ). Thus, the system searches a faction of the database. This speeds processing time, but can affect recall.
- FIG. 6D illustrates the recall curve of the largest dataset under different search fractions t/m (where t is the number of VQ code words selected for the query item, and m is the total number of partitions in the VQ codebook). As illustrated by FIG. 6D , there is virtually no loss of recall when the search fraction is 1 ⁇ 4 (25%) and less than a 2% loss when the search fraction is 1/16(6.25%). The number of partitions selected (t) can thus be adjusted to favor speed (lower t) or recall (higher t), but FIG. 6D illustrates that t can be much lower than m and still achieve accurate results.
- FIG. 7 shows an example of a generic computer device 700 , which may be server 110 , and/or client 170 of FIG. 1 , which may be used with the techniques described here.
- Computing device 700 is intended to represent various example forms of computing devices, such as laptops, desktops, workstations, personal digital assistants, cellular telephones, smart phones, tablets, servers, and other computing devices, including wearable devices.
- the components shown here, their connections and relationships, and their functions, are meant to be examples only, and are not meant to limit implementations of the inventions described and/or claimed in this document.
- Computing device 700 includes a processor 702 , memory 704 , a storage device 706 , and expansion ports 710 connected via an interface 708 .
- computing device 700 may include transceiver 746 , communication interface 744 , and a GPS (Global Positioning System) receiver module 748 , among other components, connected via interface 708 .
- Device 700 may communicate wirelessly through communication interface 744 , which may include digital signal processing circuitry where necessary.
- Each of the components 702 , 704 , 706 , 708 , 710 , 740 , 744 , 746 , and 748 may be mounted on a common motherboard or in other manners as appropriate.
- the processor 702 can process instructions for execution within the computing device 700 , including instructions stored in the memory 704 or on the storage device 706 to display graphical information for a GUI on an external input/output device, such as display 716 .
- Display 716 may be a monitor or a flat touchscreen display.
- multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory.
- multiple computing devices 700 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system).
- the memory 704 stores information within the computing device 700 .
- the memory 704 is a volatile memory unit or units.
- the memory 704 is a non-volatile memory unit or units.
- the memory 704 may also be another form of computer-readable medium, such as a magnetic or optical disk.
- the memory 704 may include expansion memory provided through an expansion interface.
- the storage device 706 is capable of providing mass storage for the computing device 700 .
- the storage device 706 may be or include a computer-readable medium, such as a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations.
- a computer program product can be tangibly embodied in such a computer-readable medium.
- the computer program product may also include instructions that, when executed, perform one or more methods, such as those described above.
- the computer- or machine-readable medium is a storage device such as the memory 704 , the storage device 706 , or memory on processor 702 .
- the interface 708 may be a high speed controller that manages bandwidth-intensive operations for the computing device 700 or a low speed controller that manages lower bandwidth-intensive operations, or a combination of such controllers.
- An external interface 740 may be provided so as to enable near area communication of device 700 with other devices.
- controller 708 may be coupled to storage device 706 and expansion port 714 .
- the expansion port which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet) may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- the computing device 700 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a standard server 730 , or multiple times in a group of such servers. It may also be implemented as part of a rack server system. In addition, it may be implemented in a personal computer such as a laptop computer 722 , or smart phone 736 . An entire system may be made up of multiple computing devices 700 communicating with each other. Other configurations are possible.
- FIG. 8 shows an example of a generic computer device 800 , which may be server 110 of FIG. 1 , which may be used with the techniques described here.
- Computing device 800 is intended to represent various example forms of large-scale data processing devices, such as servers, blade servers, datacenters, mainframes, and other large-scale computing devices.
- Computing device 800 may be a distributed system having multiple processors, possibly including network attached storage nodes, that are interconnected by one or more communication networks.
- the components shown here, their connections and relationships, and their functions, are meant to be examples only, and are not meant to limit implementations of the inventions described and/or claimed in this document.
- Distributed computing system 800 may include any number of computing devices 880 .
- Computing devices 880 may include a server or rack servers, mainframes, etc. communicating over a local or wide-area network, dedicated optical links, modems, bridges, routers, switches, wired or wireless networks, etc.
- each computing device may include multiple racks.
- computing device 880 a includes multiple racks 858 a - 858 n .
- Each rack may include one or more processors, such as processors 852 a - 852 n and 862 a - 862 n .
- the processors may include data processors, network attached storage devices, and other computer controlled devices.
- one processor may operate as a master processor and control the scheduling and data distribution tasks.
- Processors may be interconnected through one or more rack switches 858 , and one or more racks may be connected through switch 878 .
- Switch 878 may handle communications between multiple connected computing devices 800 .
- Each rack may include memory, such as memory 854 and memory 864 , and storage, such as 856 and 866 .
- Storage 856 and 866 may provide mass storage and may include volatile or non-volatile storage, such as network-attached disks, floppy disks, hard disks, optical disks, tapes, flash memory or other similar solid state memory devices, or an array of devices, including devices in a storage area network or other configurations.
- Storage 856 or 866 may be shared between multiple processors, multiple racks, or multiple computing devices and may include a computer-readable medium storing instructions executable by one or more of the processors.
- Memory 854 and 864 may include, e.g., volatile memory unit or units, a non-volatile memory unit or units, and/or other forms of computer-readable media, such as a magnetic or optical disks, flash memory, cache, Random Access Memory (RAM), Read Only Memory (ROM), and combinations thereof. Memory, such as memory 854 may also be shared between processors 852 a - 852 n . Data structures, such as an index, may be stored, for example, across storage 856 and memory 854 . Computing device 800 may include other components not shown, such as controllers, buses, input/output devices, communications modules, etc.
- An entire system such as system 100 , may be made up of multiple computing devices 800 communicating with each other.
- device 880 a may communicate with devices 880 b , 880 c , and 880 d , and these may collectively be known as system 100 .
- system 100 of FIG. 1 may include one or more computing devices 800 . Some of the computing devices may be located geographically close to each other, and others may be located geographically distant.
- the layout of system 800 is an example only and the system may take on other layouts or configurations.
- a computer system includes at least one processor and memory storing a database of quantized items.
- Each of the quantized items include a first entry into a first code book and a plurality of second entries into a second code book, where each of the plurality of second entries represents a respective subspace of k subspaces.
- the memory also includes instructions that, when executed by the at least one processor, cause the system to perform operations.
- the operations can include determining an entry in the first code book that is most similar to a query vector, calculating a residual vector from the entry in the first code book and the query vector, transforming the residual vector using a learned transformation, and projecting the transformed residual vector into the k subspaces.
- the operations can also include, for each of the quantized items having a first entry that matches the entry in the first code book that is most similar to the query vector, calculating, for each subspace, an inner product between the quantized item and the transformed residual vector, and calculating a similarity score between the quantized item and the query by summing the inner products.
- the operations may also include providing items with highest similarity scores in response to the query.
- k may be 16 and the operations may also include calculating, for each subspace, an inner product between the transformed residual vector and each entry in the second code book and storing the calculated inner products in a codebook lookup table in-register storage.
- Each subspace may have a corresponding register.
- the transformation may be jointly learned with the first code book and the second code book.
- the first code book may undergo initializing and x epochs of learning before initializing the second code book and performing the joint learning.
- the residual may be the difference between the entry in the first code book and the query vector.
- the database may be large, e.g., with millions of quantized items.
- a method includes clustering a data store of database items represented as high-dimensionality vectors and selecting a cluster center for each cluster and storing the cluster center as an entry in a first layer codebook.
- the method may also include, for each of the database items, calculating a residual based on the cluster center for the cluster the database item is assigned to, projecting the residual into subspaces, determining, for each of the subspaces, an entry in a second layer codebook for the subspace, and storing the entry in the first layer codebook and the respective entry in the second layer codebook for each of the subspaces as a quantized vector for the database item.
- the quantized vectors may be used to determine responsive database items using a maximum inner-product search.
- the method may also include transforming the residual using a learned rotation prior to projecting the residual into subspaces.
- the method may include determining t clusters from the first codebook most similar to a query vector based on an inner product operation and calculating, for each of the t clusters, a residual of the query vector based on the cluster center for the cluster.
- the method may also include projecting each residual of the query into the subspaces and determining, for each database item assigned to one of the t clusters, a maximum inner product score with the query vector.
- the maximum inner product score is based on a sum over the subspaces of an inner product calculated between the residual for the database item and the residual of the query for the cluster assigned to the database item.
- the method may also include identifying from the database items assigned to one of the t clusters, database items most similar to the query vector based on the maximum inner product score.
- the database items most similar to the query vector are used to categorize an item represented by the query vector or to provide database items responsive to the query vector.
- the method may include transforming the residual using a learned rotation prior to projecting the residual into subspaces, wherein the learned rotation is jointly trained with parameters of the first layer codebook and the second layer codebook.
- a method can include partitioning vectors in a database into m partitions using vector quantization, so that each vector has an assigned vector quantization code word and calculating, for each of the vectors, a respective residual, the residual being the difference between the vector and a cluster center corresponding to the vector quantization code word.
- the method may further include applying product quantization to each of the residuals, producing, for each residual, a product quantization code word for each of k subspaces, storing, for each vector, the assigned vector quantization code word and k product quantization code words for the residual of the vector, and using the vector quantization code words to select a portion of the database vectors most similar to a query vector.
- the method may also include, for each of the database vectors in the portion, using the product quantization code words to determine database vectors from the portion most similar to the query vector.
- the method may also include transforming the residual using a learned rotation prior to applying product quantization.
- the learned rotation can be learned jointly with a codebook for the vector quantization and codebooks for the product quantization.
- using the vector quantization code words to select a portion of the database vectors may include performing an inner product between the query vector and each cluster center to produce a similarity value for each cluster center; and selecting t cluster centers with highest similarity values. The portion is the vectors in the database having a vector quantization code word corresponding to one of the t cluster centers.
- the respective residual is a first respective residual
- the vector quantization code word is a first vector quantization code word
- the method may also include partitioning the first respective residuals into a plurality of second partitions using a second vector quantization, so that each vector has an assigned second vector quantization code word and calculating, for each of the first respective residuals, a second respective residual, the second residual being the difference between the first respective residual and the cluster center corresponding to the second vector quantization code word.
- the product quantization may be applied to the second respective residuals and the second vector quantization code word is stored with the first vector quantization code word and the k product quantization code words.
- Various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- a programmable processor which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- the systems and techniques described here can be implemented in a computing system that includes a back end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front end component (e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the systems and techniques described here), or any combination of such back end, middleware, or front end components.
- the components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include a local area network (“LAN”), a wide area network (“WAN”), and the Internet.
- LAN local area network
- WAN wide area network
- the Internet the global information network
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network.
- the relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
Abstract
Description
where k is the number of subspaces, m is the number of vector quantizers (e.g., the number of entries in the VQ code book), t is the number of VQ code words selected for the query vector, and n is the number of database items. Thus, when k much smaller than data dimensionality d, and t is much smaller than m, the complexity of the search is much faster than a brute-force search
where S(k) is the lookup table for the kth subspace, and ϕPQ(r
x≈{circumflex over (x)}=ϕ VQ(x)+R TϕPQ(r x),r x =R(x−ϕ VQ(x)),
where ϕVQ(x)=argminu∈U
the concatenation of code words obtained by dividing the rotated residuals rx into
where k is the kth subspace of K subspaces, j is the jth entry of J entries in a PQ codebook,
where k is the number of subspaces, m is the number of vector quantizers (e.g., the number of entries in the VQ code book), t is the number of VQ code words selected for the query vector, and n is the number of database items.
TABLE 1 | ||||
Dataset | Dimension (d) | Size (n) | ||
movielens | 150 | 10,681 | ||
|
300 | 17,770 | ||
|
200 | 71,291 | ||
|
500 | 3,519,681 | ||
Claims (20)
Priority Applications (4)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/290,198 US10719509B2 (en) | 2016-10-11 | 2016-10-11 | Hierarchical quantization for fast inner product search |
CN201780063107.2A CN110168525B (en) | 2016-10-11 | 2017-09-20 | Fast database search system and method |
EP17777757.0A EP3526689A1 (en) | 2016-10-11 | 2017-09-20 | Fast database search systems and methods |
PCT/US2017/052517 WO2018071148A1 (en) | 2016-10-11 | 2017-09-20 | Fast database search systems and methods |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/290,198 US10719509B2 (en) | 2016-10-11 | 2016-10-11 | Hierarchical quantization for fast inner product search |
Publications (2)
Publication Number | Publication Date |
---|---|
US20180101570A1 US20180101570A1 (en) | 2018-04-12 |
US10719509B2 true US10719509B2 (en) | 2020-07-21 |
Family
ID=59997498
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/290,198 Active 2037-07-14 US10719509B2 (en) | 2016-10-11 | 2016-10-11 | Hierarchical quantization for fast inner product search |
Country Status (4)
Country | Link |
---|---|
US (1) | US10719509B2 (en) |
EP (1) | EP3526689A1 (en) |
CN (1) | CN110168525B (en) |
WO (1) | WO2018071148A1 (en) |
Families Citing this family (21)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10181168B2 (en) * | 2014-03-31 | 2019-01-15 | Hitachi Kokusa1 Electric, Inc. | Personal safety verification system and similarity search method for data encrypted for confidentiality |
CN108241745B (en) * | 2018-01-08 | 2020-04-28 | 阿里巴巴集团控股有限公司 | Sample set processing method and device and sample query method and device |
US20190244080A1 (en) * | 2018-02-02 | 2019-08-08 | Wisconsin Alumni Research Foundation | Neural Network Processor with On-Chip Convolution Kernel Storage |
US11120070B2 (en) * | 2018-05-21 | 2021-09-14 | Microsoft Technology Licensing, Llc | System and method for attribute-based visual search over a computer communication network |
CN110727769B (en) * | 2018-06-29 | 2024-04-19 | 阿里巴巴（中国）有限公司 | Corpus generation method and device and man-machine interaction processing method and device |
CN109543069B (en) * | 2018-10-31 | 2021-07-13 | 北京达佳互联信息技术有限公司 | Video recommendation method and device and computer-readable storage medium |
US11354287B2 (en) * | 2019-02-07 | 2022-06-07 | Google Llc | Local orthogonal decomposition for maximum inner product search |
CN111221915B (en) * | 2019-04-18 | 2024-01-09 | 西安睿德培欣教育科技有限公司 | Online learning resource quality analysis method based on CWK-means |
US11775589B2 (en) * | 2019-08-26 | 2023-10-03 | Google Llc | Systems and methods for weighted quantization |
US11494734B2 (en) * | 2019-09-11 | 2022-11-08 | Ila Design Group Llc | Automatically determining inventory items that meet selection criteria in a high-dimensionality inventory dataset |
CN114245896A (en) * | 2019-10-31 | 2022-03-25 | 北京欧珀通信有限公司 | Vector query method and device, electronic equipment and storage medium |
JP2021149613A (en) * | 2020-03-19 | 2021-09-27 | 株式会社野村総合研究所 | Natural language processing apparatus and program |
CN113495965A (en) * | 2020-04-08 | 2021-10-12 | 百度在线网络技术（北京）有限公司 | Multimedia content retrieval method, device, equipment and storage medium |
US11914670B2 (en) * | 2020-09-08 | 2024-02-27 | Huawei Technologies Co., Ltd. | Methods and systems for product quantization-based compression of a matrix |
CN112116436B (en) * | 2020-10-14 | 2023-07-25 | 中国平安人寿保险股份有限公司 | Intelligent recommendation method and device, computer equipment and readable storage medium |
CN112418298B (en) * | 2020-11-19 | 2021-12-03 | 北京云从科技有限公司 | Data retrieval method, device and computer readable storage medium |
CN113159211B (en) * | 2021-04-30 | 2022-11-08 | 杭州好安供应链管理有限公司 | Method, computing device and computer storage medium for similar image retrieval |
US11886445B2 (en) * | 2021-06-29 | 2024-01-30 | United States Of America As Represented By The Secretary Of The Army | Classification engineering using regional locality-sensitive hashing (LSH) searches |
EP4160434A4 (en) * | 2021-08-16 | 2023-12-13 | Baidu Online Network Technology (Beijing) Co., Ltd | Method and apparatus for constructing search database, and device and storage medium |
CN113656373A (en) * | 2021-08-16 | 2021-11-16 | 百度在线网络技术（北京）有限公司 | Method, device, equipment and storage medium for constructing retrieval database |
CN117609488B (en) * | 2024-01-22 | 2024-03-26 | 清华大学 | Method and device for searching small-weight code words, computer storage medium and terminal |
Citations (16)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5194950A (en) * | 1988-02-29 | 1993-03-16 | Mitsubishi Denki Kabushiki Kaisha | Vector quantizer |
US5677986A (en) * | 1994-05-27 | 1997-10-14 | Kabushiki Kaisha Toshiba | Vector quantizing apparatus |
US6081274A (en) * | 1996-09-02 | 2000-06-27 | Ricoh Company, Ltd. | Shading processing device |
US6404925B1 (en) * | 1999-03-11 | 2002-06-11 | Fuji Xerox Co., Ltd. | Methods and apparatuses for segmenting an audio-visual recording using image similarity searching and audio speaker recognition |
US20020178158A1 (en) * | 1999-12-21 | 2002-11-28 | Yuji Kanno | Vector index preparing method, similar vector searching method, and apparatuses for the methods |
US20040220944A1 (en) | 2003-05-01 | 2004-11-04 | Behrens Clifford A | Information retrieval and text mining using distributed latent semantic indexing |
US20070263746A1 (en) * | 2006-05-12 | 2007-11-15 | Nokia Corporation | Feedback frame structure for subspace tracking precoding |
US20090304296A1 (en) * | 2008-06-06 | 2009-12-10 | Microsoft Corporation | Compression of MQDF Classifier Using Flexible Sub-Vector Grouping |
US20110080965A1 (en) * | 2009-10-05 | 2011-04-07 | Samsung Electronics Co., Ltd. | Method and system for feedback of channel information |
US20140016698A1 (en) * | 2012-07-11 | 2014-01-16 | Qualcomm Incorporated | Rotation of prediction residual blocks in video coding with transform skipping |
US20140258295A1 (en) | 2013-03-08 | 2014-09-11 | Microsoft Corporation | Approximate K-Means via Cluster Closures |
US20160148120A1 (en) | 2014-11-20 | 2016-05-26 | International Business Machines Corporation | Calculation apparatus, calculation method, learning apparatus, learning method, and program |
US20160259816A1 (en) * | 2015-03-05 | 2016-09-08 | Nant Holdings Ip, Llc | Global signatures for large-scale image recognition |
US20170026665A1 (en) * | 2014-03-13 | 2017-01-26 | Zte Corporation | Method and device for compressing local feature descriptor, and storage medium |
US20180341805A1 (en) * | 2015-11-06 | 2018-11-29 | Thomson Licensing | Method and Apparatus for Generating Codebooks for Efficient Search |
US10255323B1 (en) | 2015-08-31 | 2019-04-09 | Google Llc | Quantization-based fast inner product search |
-
2016
- 2016-10-11 US US15/290,198 patent/US10719509B2/en active Active
-
2017
- 2017-09-20 CN CN201780063107.2A patent/CN110168525B/en active Active
- 2017-09-20 EP EP17777757.0A patent/EP3526689A1/en active Pending
- 2017-09-20 WO PCT/US2017/052517 patent/WO2018071148A1/en unknown
Patent Citations (19)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5194950A (en) * | 1988-02-29 | 1993-03-16 | Mitsubishi Denki Kabushiki Kaisha | Vector quantizer |
US5677986A (en) * | 1994-05-27 | 1997-10-14 | Kabushiki Kaisha Toshiba | Vector quantizing apparatus |
US6081274A (en) * | 1996-09-02 | 2000-06-27 | Ricoh Company, Ltd. | Shading processing device |
US6404925B1 (en) * | 1999-03-11 | 2002-06-11 | Fuji Xerox Co., Ltd. | Methods and apparatuses for segmenting an audio-visual recording using image similarity searching and audio speaker recognition |
US20020178158A1 (en) * | 1999-12-21 | 2002-11-28 | Yuji Kanno | Vector index preparing method, similar vector searching method, and apparatuses for the methods |
US7007019B2 (en) * | 1999-12-21 | 2006-02-28 | Matsushita Electric Industrial Co., Ltd. | Vector index preparing method, similar vector searching method, and apparatuses for the methods |
US20040220944A1 (en) | 2003-05-01 | 2004-11-04 | Behrens Clifford A | Information retrieval and text mining using distributed latent semantic indexing |
US7152065B2 (en) | 2003-05-01 | 2006-12-19 | Telcordia Technologies, Inc. | Information retrieval and text mining using distributed latent semantic indexing |
US20070263746A1 (en) * | 2006-05-12 | 2007-11-15 | Nokia Corporation | Feedback frame structure for subspace tracking precoding |
US20090304296A1 (en) * | 2008-06-06 | 2009-12-10 | Microsoft Corporation | Compression of MQDF Classifier Using Flexible Sub-Vector Grouping |
US8077994B2 (en) | 2008-06-06 | 2011-12-13 | Microsoft Corporation | Compression of MQDF classifier using flexible sub-vector grouping |
US20110080965A1 (en) * | 2009-10-05 | 2011-04-07 | Samsung Electronics Co., Ltd. | Method and system for feedback of channel information |
US20140016698A1 (en) * | 2012-07-11 | 2014-01-16 | Qualcomm Incorporated | Rotation of prediction residual blocks in video coding with transform skipping |
US20140258295A1 (en) | 2013-03-08 | 2014-09-11 | Microsoft Corporation | Approximate K-Means via Cluster Closures |
US20170026665A1 (en) * | 2014-03-13 | 2017-01-26 | Zte Corporation | Method and device for compressing local feature descriptor, and storage medium |
US20160148120A1 (en) | 2014-11-20 | 2016-05-26 | International Business Machines Corporation | Calculation apparatus, calculation method, learning apparatus, learning method, and program |
US20160259816A1 (en) * | 2015-03-05 | 2016-09-08 | Nant Holdings Ip, Llc | Global signatures for large-scale image recognition |
US10255323B1 (en) | 2015-08-31 | 2019-04-09 | Google Llc | Quantization-based fast inner product search |
US20180341805A1 (en) * | 2015-11-06 | 2018-11-29 | Thomson Licensing | Method and Apparatus for Generating Codebooks for Efficient Search |
Non-Patent Citations (52)
Title |
---|
Andoni et al., "Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions", 47th Annual IEEE Symposium on Foundations of Computer Science, 2006, pp. 459-468. |
André, et al., "Cache locality is not enough: high-performance nearest neighbor search with product quantization fast scan", Proceedings of the VLDB Endowment 9.4, 2015, pp. 288-299. |
Auvolat et al., "Clustering is Efficient for Approximate Maximum Inner Product Search", arXiv preprint arXiv:1507.05910, 2015, 10 pages. |
Babenko , et al., "Efficient Indexing of Billion-Scale Datasets of Deep Descriptors", 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2055-2063. |
Bachrach et al., "Speeding Up the Xbox Recommender System using a Euclidean Transformation for Inner-Product Spaces", In Proceedings of the 8th ACM Conference on Recommender Systems, 2014, pp. 257-265. |
Bayardo , et al., "Scaling up all pairs similarity search", Proceedings of the 16th international conference on World Wide Web, May 2007, pp. 131-140. |
Bennet et al., "The Netflix Prize", In KDD Cup and Workshop in conjunction with KDD, Aug. 12, 2007, 4 Pages. |
Billsus , et al., "Learning Collaborative Information Filters", Proceedings of the Fifteenth International Conference on Machine Learning (ICML '98), 1998, pp. 46-54. |
Bottou et al., "Convergence Properties of the K-means Algorithms", In Advances in Neural Information Processing Systems 7, 1994, pp. 585-592. |
Cheng , et al., "Wide & Deep Learning for Recommender Systems", Proceedings of the 1st Workshop on Deep Learning for Recommender Systems (DLRS 2016), 2016, pp. 7-10. |
Cohen et al., "Approximating Matrix Multiplication for Pattern Recognition Tasks", Journal of Algorithms, vol. 30, Issue 2, 1999, pp. 211-252. |
Cover , et al., "Elements of Information Theory", 2nd Edition, Wiley-Interscience, Inc., 1991, 563 pages. |
Cremonesi et al., "Performance of Recommender Algorithms on Top-N Recommendation Tasks", Proceedings of the Fourth ACM Conference on Recommender Systems, Sep. 2010, pp. 39-46. |
Davidson et al., "The YoutTbe Video Recommendation System", Proceedings of the Fourth ACM Conference on Recommender Systems, Sep. 2010, pp. 293-296. |
Dean et al., "Fast, Accurate Detection of 100,000 Object Classes on a Single Machine", Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2013, 8 pages. |
Du et al., "Inner Product Similarity Search Using Compositional Codes", CoRR,abs/1406.4966, Jun. 2014, pp. 1-20. |
Elsayed, et al., "Pairwise Document Similarity in Large Collections With MapReduce"; Proceedings of ACL-08: HLT, Short Papers (Companion Volume), Jun. 2008; pp. 265-268. |
Fukunaga et al., "A Branch and Bound Algorithm for Computing K-Nearest Neighbors", IEEE Transactions on Computers vol. 100, No. 7, Jul. 1975, pp. 750-753. |
Ge , et al., "Optimized Product Quantization", IEEE Transactions on Pattern Analysis and Machine Intelligence 36.4, Apr. 2014, pp. 744-755. |
Gersho, et al., "Chapter 12-Constrained Vector Quantization", The Springer International Series in Engineering and Computer Science (Communications and Information Theory), vol. 159, Springer, Boston, MA, 1992, pp. 407-485. |
Gersho, et al., "Chapter 12—Constrained Vector Quantization", The Springer International Series in Engineering and Computer Science (Communications and Information Theory), vol. 159, Springer, Boston, MA, 1992, pp. 407-485. |
Gong et al., "Iterative Quantization: A Procrustean Approach to Learning Binary Codes", Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2011, pp. pp. 817-824. |
Guo et al., "Quantization Based Fast Inner Product Search", arXiv preprint arXiv:1509.01469, 2015, 17 pages. |
Halton, "A Combinatorial Proof of Cayley's Theorem on Pfaffians", Journal of Combinatorial Theory, vol. 1, 1966, pp. 224-232. |
Harper , et al., "The MovieLens Datasets: History and Context", ACM Transactions on Interactive Intelligent Systems 5.4, 2015, 20 pages. |
International Search Report and Written Opinion for PCT Application No. PCT/US2017/052517, dated Nov. 6, 2017, 14 pages. |
Jegou et al., "Product Quantization for Nearest Neighbor Search", IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, Issue 1, 2011, 14 pages. |
Kalantidis, et al., "Locally Optimized Product Quantization for Approximate Nearest Neighbor Search", Computer Vision Foundation, Jun. 2014, 8 pages. |
Kingma et al., "Adam: A method for stochastic optimization", CoRR, abs/1412.6980, Jul. 2015, pp. 1-15. |
Koenigstein et al., "Efficient Retrieval of Recommendations in a Matrix Factorization Framework", Proceedings of the 21st ACM International Conference on Information and Knowledge Management, Nov. 2012, pp. 535-544. |
Le , et al., "Distributed Representations of Sentences and Documents", Proceedings of the 31st International Conference on Machine Learning, vol. 32, 2014, pp. 1188-1196. |
Ley , "The DBLP Computer Science Bibliography: Evolution, Research Issues, Perspectives", String Processing and Information Retrieval, 2002, pp. 1-10. |
Lynch , "Big Data: How do your data grow?", Nature, vol. 455, Sep. 2008, pp. 28-29. |
Martinez , et al., "Stacked Quantizers for Compositional Vector Compression", available on-line at <https://arxiv.org/pdf/1411.2173.pdf>, Nov. 8, 2014, 8 pages. |
Mikolov et al., "Distributed Representations of Words and Phrases and Their Compositionality", In Advances in neural information processing systems, 2013, pp. 3111-3119. |
Moffat , et al., "Self-indexing inverted files for fast text retrieval", ACM Transactions on Information Systems (TOIS) 14(4), 1996, pp. 349-379. |
Neyshabur et al., "A simpler and better LSH for Maximum Inner Product Search (MIPS)", arXiv:1410.5518, Oct. 2014, 9 Pages. |
Nister et al., "Scalable Recognition with a Vocabulary Tree", Computer vision and pattern recognition, IEEE computer society conference, 2006, pp. 2161-2168. |
Norouzi et al., "Cartesian k-Means", Computer Vision and Pattern Recognition (CVPR), IEEE Conference, 2013, pp. 3017-3024. |
Palangi , et al., "Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis and Application to Information Retrieval", IEEE/ACM Transactions on Audio, Speech and Language Processing. 24.4, Jan. 16, 2016, pp. 694-707. |
Ram et al., "Maximum Inner-Product Search Using Cone Trees", In SIGKDD International Conference on Knowledge Discovery and Data Mining, 2012, 9 pages. |
Sabin et al., "Product Code Vector Quantizers for Waveform and Voice Coding", IEEE Transactions on Acoustics, Speech, and Signal Processing, 1984, pp. 474-488. |
Shan , et al., "Deep Crossing: Web-Scale Modeling Without Manually Crafted Combinatorial Features", Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Aug. 13-17, 2016, pp. 255-262. |
Shen et al., "Learning Binary Codes for Maximum Inner Product Search" In ICCV, 2015, pp. 4148-4156. |
Shicong, et al., "Learning Better Encoding for Approximate Nearest Neighbor Search with Dictionary Annealing", retrieved from https://ai2-s2-pdfs.s3.amazonaws.com/9e8b/2623b432250733479265fdeaa819b7a55b35.pdf, Jul. 6, 2015, 10 pages. |
Shrivastava et al., "An Improved Scheme for Asymmetric LSH", arXiv:1410.5410, Oct. 2014, 10 pages. |
Shrivastava et al., "Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS)", In Advances in Neural Information Processing Systems, 2014, pp. 2321-2329. |
Spyromitros-Xioufis, et al., "A Comprehensive Study Over VLAD and Product Quantization in Large-Scale Image Retrieval", IEEE Transactions on Multimedia, IEEE Service Center, vol. 16, No. 6, Oct. 2014, pp. 1713-1728. |
Szegedy et al., "Going Deeper with Convolutions", arXiv:1409.4842, 2014, pp. 1-9. |
Wong , et al., "Implementations of partial document ranking using inverted files", Information Processing & Management 29.5, 1993, pp. 647-669. |
Written Opinion for International Application PCT/US2017/052517, dated Jan. 14, 2019, 7 pages. |
Zobel , et al., "Inverted files for text search engines", ACM computing surveys (CSUR), vol. 38, No. 2, Article 6, Jul. 2006, 56 pages. |
Also Published As
Publication number | Publication date |
---|---|
WO2018071148A1 (en) | 2018-04-19 |
CN110168525A (en) | 2019-08-23 |
EP3526689A1 (en) | 2019-08-21 |
CN110168525B (en) | 2022-03-01 |
US20180101570A1 (en) | 2018-04-12 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US10719509B2 (en) | Hierarchical quantization for fast inner product search | |
US11651286B2 (en) | Method and system for distributed machine learning | |
US10255323B1 (en) | Quantization-based fast inner product search | |
He et al. | K-means hashing: An affinity-preserving quantization method for learning binary compact codes | |
US11392596B2 (en) | Efficient inner product operations | |
Guo et al. | Quantization based fast inner product search | |
Ge et al. | Optimized product quantization for approximate nearest neighbor search | |
Hsieh et al. | A divide-and-conquer solver for kernel support vector machines | |
Wu et al. | Multiscale quantization for fast similarity search | |
Gong et al. | Compressing deep convolutional networks using vector quantization | |
Leng et al. | Hashing for distributed data | |
EP3115908A1 (en) | Method and apparatus for multimedia content indexing and retrieval based on product quantization | |
Ding et al. | Towards accurate post-training quantization for vision transformer | |
Abdelhadi et al. | Accelerated approximate nearest neighbors search through hierarchical product quantization | |
Jain et al. | Approximate search with quantized sparse representations | |
Rakotomamonjy et al. | Personalised federated learning on heterogeneous feature spaces | |
Xu et al. | Soda: Similar 3d object detection accelerator at network edge for autonomous driving | |
EP3115909A1 (en) | Method and apparatus for multimedia content indexing and retrieval based on product quantization | |
Guo et al. | Parametric and nonparametric residual vector quantization optimizations for ANN search | |
Mourão et al. | Balancing search space partitions by sparse coding for distributed redundant media indexing and retrieval | |
Wu et al. | Local orthogonal decomposition for maximum inner product search | |
Guo et al. | New loss functions for fast maximum inner product search | |
Chiu et al. | Effective product quantization-based indexing for nearest neighbor search | |
US20240086408A1 (en) | Data query apparatus, method, and storage medium | |
Klicpera et al. | Warpspeed Computation of Optimal Transport, Graph Distances, and Embedding Alignment |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:KUMAR, SANJIV;SIMCHA, DAVID MORRIS;SURESH, ANANDA THEERTHA;AND OTHERS;SIGNING DATES FROM 20161011 TO 20161014;REEL/FRAME:040406/0843 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044567/0001Effective date: 20170929 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: FINAL REJECTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE AFTER FINAL ACTION FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |