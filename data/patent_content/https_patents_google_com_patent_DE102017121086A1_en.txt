DE102017121086A1 - CONSISTENT LANGUAGE-CONTROLLED DEVICES - Google Patents
CONSISTENT LANGUAGE-CONTROLLED DEVICES Download PDFInfo
- Publication number
- DE102017121086A1 DE102017121086A1 DE102017121086.5A DE102017121086A DE102017121086A1 DE 102017121086 A1 DE102017121086 A1 DE 102017121086A1 DE 102017121086 A DE102017121086 A DE 102017121086A DE 102017121086 A1 DE102017121086 A1 DE 102017121086A1
- Authority
- DE
- Germany
- Prior art keywords
- computing device
- audio data
- transcription
- utterance
- response
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/28—Constructional details of speech recognition systems
- G10L15/30—Distributed recognition, e.g. in client-server systems, for mobile phones or network applications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
- G06F3/167—Audio in a user interface, e.g. using voice commands for navigating, audio feedback
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L13/00—Speech synthesis; Text to speech systems
- G10L13/02—Methods for producing synthetic speech; Speech synthesisers
- G10L13/033—Voice editing, e.g. manipulating the voice of the synthesiser
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L13/00—Speech synthesis; Text to speech systems
- G10L13/08—Text analysis or generation of parameters for speech synthesis out of text, e.g. grapheme to phoneme translation, prosody generation or stress or intonation determination
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L2015/088—Word spotting
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/223—Execution procedure of a spoken command
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/226—Procedures used during a speech recognition process, e.g. man-machine dialogue using non-speech characteristics
- G10L2015/228—Procedures used during a speech recognition process, e.g. man-machine dialogue using non-speech characteristics of application context
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04W—WIRELESS COMMUNICATION NETWORKS
- H04W4/00—Services specially adapted for wireless communication networks; Facilities therefor
- H04W4/80—Services using short range communication, e.g. near-field communication [NFC], radio-frequency identification [RFID] or low energy communication
Abstract
Verfahren, Systeme und Vorrichtungen, einschließlich ComputerProgrammen, die auf einem computerlesbaren Speichermedium zum Zusammenwirken zwischen mehreren sprachgesteuerten Geräten codiert sind, werden offenbart. In einem Aspekt beinhaltet ein Verfahren die Aktionen des Identifizierens eines zweiten Computergeräts, das zum Antworten auf ein bestimmtes, zuvor festgelegtes Triggerwort konfiguriert ist, durch ein erstes Computergerät; Empfangen von Audiodaten, die einer Äußerung entsprechen; Empfangen einer Transkription von zusätzlichen Audiodaten, die durch das zweite Computergerät als Antwort auf die Äußerung ausgegeben wurden; basierend auf der Transkription der zusätzlichen Audiodaten und basierend auf der Äußerung Generieren einer Transkription, die einer Antwort auf die zusätzlichen Audiodaten entspricht; und Bereitstellen der Transkription, die der Antwort entspricht, zum Ausgeben.Methods, systems and devices, including computer programs, encoded on a computer-readable storage medium for interworking between multiple voice-operated devices are disclosed. In one aspect, a method includes the actions of identifying a second computing device configured to respond to a particular predetermined trigger word by a first computing device; Receiving audio data corresponding to an utterance; Receiving a transcription of additional audio data output by the second computing device in response to the utterance; based on the transcription of the additional audio data and based on the statement generating a transcription corresponding to a response to the additional audio data; and providing the transcription corresponding to the response for outputting.
Description
TECHNISCHES GEBIETTECHNICAL AREA
Diese Beschreibung betrifft allgemein die automatisierte Spracherkennung.This description relates generally to automated speech recognition.
HINTERGRUNDBACKGROUND
Die Realität eines sprachgesteuerten Haushaltes oder einer anderen Umgebung - d. h. eine, in der ein Benutzer nur eine Abfrage oder einen Befehl laut aussprechen muss und ein computerbasiertes System die Abfrage beantwortet und/oder verursacht, dass der Befehl ausgeführt wird - ist da. Eine sprachgesteuerte Umgebung (z. B. Haushalt, Arbeitsplatz, Schule usw.) kann unter Verwendung eines Netzwerks von verbundenen Mikrofongeräten implementiert werden, die in verschiedenen Räumen oder Bereichen der Umgebung verteilt sind. Durch solch ein Netzwerk von Mikrofonen kann ein Benutzer das System prinzipiell überall in der Umgebung mündlich abfragen, ohne sich vor oder auch nur in der Nähe eines Computers oder eines anderen Geräts befinden zu müssen. Während ein Benutzer in der Küche kocht, kann er beispielsweise das System fragen „Wie viele Milliliter sind in drei Tassen?“. und als Reaktion, z. B. in Form einer künstlichen Sprachausgabe, eine Antwort vom System erhalten. Alternativ könnte ein Benutzer dem System Fragen stellen, wie „Wann schließt die Tankstelle in meiner Nähe“ oder vor dem Verlassen des Hauses „Soll ich heute einen Mantel anziehen?“. Des Weiteren kann ein Benutzer eine Abfrage des Systems anfordern und/oder einen Befehl ausgeben, der die persönlichen Informationen des Benutzers betrifft. Beispielsweise könnte ein Benutzer das System fragen „Wann ist meine Besprechung mit John?“. oder das System anweisen „Erinnere mich daran, John anzurufen, wenn ich nach Hause komme“.The reality of a voice-controlled household or other environment - d. H. one in which a user needs to pronounce only one query or command aloud and a computer-based system responds to the query and / or causes the command to be executed - is there. A voice-controlled environment (eg, home, work, school, etc.) may be implemented using a network of connected microphone devices distributed in different rooms or areas of the environment. Through such a network of microphones, a user can, in principle, interrogate the system orally throughout the environment without having to be in front of, or even near, a computer or other device. For example, while a user is cooking in the kitchen, he may ask the system, "How many milliliters are in three cups?". and as a reaction, e.g. In the form of an artificial speech output, receive a response from the system. Alternatively, a user could ask questions to the system, such as "When does the gas station close to me" or before leaving the house, "Should I wear a coat today?". Furthermore, a user may request a query of the system and / or issue a command concerning the personal information of the user. For example, a user might ask the system "When is my meeting with John?". or instruct the system "Remind me to call John when I get home."
KURZDARSTELLUNGSUMMARY
Triggerwörter können verwendet werden, um das Aufgreifen von Äußerungen zu vermeiden, die in der Umgebung gemacht wurden, die nicht an das System gerichtet sind. Ein Triggerwort (auch als ein „Aufmerksamkeitswort“ oder „Sprachaktionsinitiierungsbefehl“ bezeichnet) ist ein zuvor festgelegtes Wort oder ein Begriff, der gesprochen wird, um die Aufmerksamkeit des Systems zu erregen. In einer exemplarischen Umgebung besteht das Triggerwort zum Erregen der Aufmerksamkeit des Systems aus den Worten „OK Computer“. Wenn das System erkennt, dass der Benutzer das Triggerwort gesprochen hat, gibt das System einen Bereit-Status zum Empfangen weiterer Benutzerbefehle ein.Trigger words can be used to avoid picking up utterances made in the environment that are not addressed to the system. A trigger word (also referred to as a "attention word" or "voice action initiation command") is a predetermined word or phrase that is spoken to arouse the attention of the system. In an exemplary environment, the trigger word for attracting the attention of the system is the words "OK Computer". When the system detects that the user has spoken the trigger word, the system enters a ready status to receive further user commands.
In sprachgesteuerten Umgebungen können Geräte ununterbrochen auf Triggerwörter hören. Wenn mehrere Geräte in derselben Umgebung vorhanden sind, die konfiguriert sind, um auf ein bestimmtes Triggerwort zu reagieren, kann jede Äußerung, die das Triggerwort beinhaltet, alle Geräte auslösen und redundante Reaktionen von den mehreren Geräten bereitstellen. Beispielsweise können Alice, Bob und Carl nach einem Restaurant Ausschau halten, wohin sie zum Mittagessen gehen. Alice kann sagen „OK Computer, suche mir Restaurants in der Nähe“. Alle drei ihrer Telefone können gleichzeitig eine Antwort bereitstellen, z. B. eine Liste von Restaurants. Zum Verbessern der Benutzererfahrung kann es wünschenswert sein, wenn die Telefone miteinander kommunizieren, um unterschiedliche Antworten auf koordinierte Art ähnlich einer tatsächlichen Unterhaltung bereitzustellen. Alices Telefon kann eine Liste von Restaurants in der Nähe bereitstellen und Bobs Telefon kann eine Antwort bereitstellen, bei der eines der Restaurants aus der Liste der von Alices Telefon bereitgestellten Restaurants in der Nähe ausgewählt wird. Carls Telefon kann eine Antwort bereitstellen, bei der dem ausgewählten Restaurant basierend auf Carls Bewertung des Restaurants in den sozialen Medien zugestimmt wird.In speech-driven environments, devices can continuously hear trigger words. If there are multiple devices in the same environment configured to respond to a particular trigger word, any utterance that includes the trigger word can trigger all devices and provide redundant responses from the multiple devices. For example, Alice, Bob and Carl can look for a restaurant to go to for lunch. Alice can say "OK computer, look for restaurants nearby". All three of their phones can simultaneously provide a response, e.g. For example, a list of restaurants. To enhance the user experience, it may be desirable for the telephones to communicate with one another to provide different responses in a coordinated manner similar to actual entertainment. Alice's phone can provide a list of nearby restaurants, and Bob's phone can provide an answer by selecting one of the restaurants from the list of nearby restaurants provided by Alice's phone. Carl's phone can provide an answer to the selected restaurant based on Carl's rating of the restaurant on social media.
Gemäß einem innovativen Aspekt des in dieser Spezifikation beschriebenen Gegenstandes identifiziert ein erstes Computergerät ein oder mehrere andere Computergeräte, die konfiguriert sind, um auf ein bestimmtes, zuvor festgelegtes Triggerwort zu antworten. Das erste Computergerät empfängt eine durch einen Benutzer ausgesprochene Äußerung. Vor dem Ausführen von Spracherkennung an den Audiodaten, die der Äußerung entsprechen, bestimmt das erste Computergerät, dass die Audiodaten das Triggerwort beinhalten und dass ein zweites Computergerät bereits die Audiodaten verarbeitet. Das erste Computergerät empfängt die durch das zweite Computergerät in Reaktion auf die Äußerung ausgegebene Antwort. Basierend auf dieser Antwort und der Äußerung generiert das erste Computergerät eine Antwort und stellt die Antwort zum Ausgeben bereit. Daher erweitert das erste Computergerät die Antwort von dem zweiten Computergerät und stellt eine neue Antwort bereit, die zusätzliche Informationen für die Benutzer bereitstellen kann.According to an innovative aspect of the subject matter described in this specification, a first computing device identifies one or more other computing devices that are configured to respond to a particular, predetermined trigger word. The first computing device receives an utterance made by a user. Before performing speech recognition on the audio data corresponding to the utterance, the first computing device determines that the audio data includes the trigger word and that a second computing device is already processing the audio data. The first computing device receives the response output by the second computing device in response to the utterance. Based on this answer and the utterance, the first computing device generates a response and provides the answer for issuing. Therefore, the first computing device expands the response from the second computing device and provides a new response that can provide additional information to the users.
Im Allgemeinen kann ein weiterer innovativer Aspekt des in dieser Spezifikation beschriebenen Gegenstandes in Verfahren enthalten sein, die die Aktionen des Identifizierens eines zweiten Computergeräts, das zum Antworten auf ein bestimmtes, zuvor festgelegtes Triggerwort konfiguriert ist, durch ein erstes Computergerät; Empfangen von Audiodaten, die einer Äußerung entsprechen; Empfangen einer Transkription von zusätzlichen Audiodaten, die durch das zweite Computergerät als Antwort auf die Äußerung ausgegeben wurden; basierend auf der Transkription der zusätzlichen Audiodaten und basierend auf der Äußerung Generieren einer Transkription, die einer Antwort auf die zusätzlichen Audiodaten entspricht; und Bereitstellen der Transkription, die der Antwort entspricht, zum Ausgeben beinhalten.In general, another innovative aspect of the subject matter described in this specification may be included in methods that include the actions of identifying a second computing device configured to respond to a particular, predetermined trigger word by a first computing device; Receiving audio data corresponding to an utterance; Receiving a transcription of additional audio data output by the second computing device in response to the utterance; based on the transcription of the additional audio data and based on the expression generating a transcription, the corresponds to a response to the additional audio data; and providing the transcription corresponding to the response to output.
Diese und andere Ausführungsformen können gegebenenfalls jeweils ein oder mehrere der folgenden Merkmale beinhalten. In einigen Implementierungen umfasst das Bereitstellen der Transkription zum Ausgeben, die der Antwort entspricht, Bereitstellen eines Anfangsabschnitts der Transkription für einen Sprachsynthesizer des ersten Computergeräts; und Bereitstellen (i) eines verbleibenden Abschnitts der Transkription für das zweite Computergerät und (ii) Anweisungen zum Ausgeben des verbleibenden Abschnitts der Transkription unter Verwendung eines Sprachsythesizers des zweiten Computergeräts.These and other embodiments may optionally include one or more of the following features, respectively. In some implementations, providing the transcription for output corresponding to the response comprises providing a beginning portion of the transcription for a speech synthesizer of the first computing device; and providing (i) a remaining portion of the transcription for the second computing device, and (ii) instructions for outputting the remainder of the transcription using a speech synthesizer of the second computing device.
In einigen Implementierungen beinhalten die Aktionen des Weiteren das Ausführen von Spracherkennung an den Audiodaten, die der Äußerung entsprechen: Bestimmen, dass die Audiodaten, die der Äußerung entsprechen, das bestimmte, zuvor festgelegte Triggerwort beinhalten; und Empfangen von Daten, die angeben, dass das zweite Computergerät auf die Audiodaten antwortet.In some implementations, the actions further include performing speech recognition on the audio data corresponding to the utterance: determining that the audio data corresponding to the utterance includes the particular predefined trigger word; and receiving data indicating that the second computing device is responding to the audio data.
In einigen Implementierungen umfasst das Bereitstellen der Transkription zum Ausgeben, die der Antwort entspricht, Bereitstellen der Transkription, die der Antwort entspricht, für einen Sprachsynthesizer.In some implementations, providing the transcription for output corresponding to the response comprises providing the transcription corresponding to the answer to a speech synthesizer.
In einigen Implementierungen werden die der synthetisierten Transkription entsprechenden Audiodaten durch ein drittes Computergerät empfangen, das konfiguriert ist, um eine Antwort basierend auf den der synthetisierten Transkription entsprechenden Audiodaten, der Transkription der zusätzlichen Audiodaten und der Äußerung zu generieren.In some implementations, the audio data corresponding to the synthesized transcription is received by a third computing device configured to generate a response based on the audio data corresponding to the synthesized transcription, the transcription of the additional audio data, and the utterance.
In einigen Implementierungen beinhalten die Aktionen des Weiteren nach dem Bereitstellen der Transkription, die der Antwort entspricht, zum Ausgeben, Empfangen von Audiodaten durch das erste Computergerät, die einer zweiten Äußerung entsprechen; basierend auf der Transkription der zusätzlichen Audiodaten, basierend auf der Äußerung und basierend auf der zweiten Äußerung, Generieren einer zusätzlichen Transkription, die einer Antwort auf die Audiodaten entspricht, die der zweiten Äußerung entsprechen; und Bereitstellen der zusätzlichen Transkription zum Ausgeben.Further, in some implementations, after providing the transcription corresponding to the response, the actions include outputting audio data received by the first computing device that corresponds to a second utterance; based on the transcription of the additional audio data, based on the utterance and based on the second utterance, generating an additional transcription corresponding to a response to the audio data corresponding to the second utterance; and providing the additional transcription for output.
In einigen Implementierungen beinhalten die Aktionen des Weiteren Empfangen von Daten, die eine Art von Sprachsynthesizer angeben, der durch das zweite Computergerät zum Ausgeben der zusätzlichen Audiodaten verwendet wird, wobei eine Art von Sprachsynthesizer, der die der Antwort entsprechende Transkription empfing, anders als die Art von Sprachsynthesizer ist, der durch das zweite Computergerät zum Ausgeben der zusätzlichen Audiodaten verwendet wird.In some implementations, the actions further include receiving data indicative of a type of speech synthesizer used by the second computing device to output the additional audio data, wherein a type of speech synthesizer that received the transcription corresponding to the response differs from the type of speech synthesizer used by the second computing device to output the additional audio data.
In einigen Implementierungen umfasst das Bereitstellen der Transkription zum Ausgeben, die der Antwort entspricht, Bereitstellen der Transkription zum Anzeigen auf einer Anzeige des ersten Computergeräts.In some implementations, providing the transcription for output corresponding to the response comprises providing the transcription for display on a display of the first computing device.
In einigen Implementierungen beinhaltet Empfangen von Daten, die angeben, dass das zweite Computergerät auf die Audiodaten antwortet, Empfangen eines Kurzstreckenfunksignals von dem zweiten Computergerät, das angibt, dass das zweite Computergerät auf die Audiodaten antwortet. In einigen Implementierungen beinhaltet Empfangen von Daten, die angeben, dass das zweite Computergerät auf die Audiodaten antwortet, Empfangen von Daten von dem zweiten Computergerät und durch ein lokales Netzwerk, die angeben, dass das zweite Computergerät auf die Audiodaten antwortet. In einigen Implementierungen beinhaltet Empfangen von Daten, die angeben, dass das zweite Computergerät auf die Audiodaten antwortet, Empfangen von Daten von einem Server, die angeben, dass das zweite Computergerät auf die Audiodaten antwortet.In some implementations, receiving data indicating that the second computing device is responding to the audio data includes receiving a short distance radio signal from the second computing device indicating that the second computing device is responding to the audio data. In some implementations, receiving data indicating that the second computing device is responding to the audio data includes receiving data from the second computing device and through a local area network indicating that the second computing device is responding to the audio data. In some implementations, receiving data indicating that the second computing device is responding to the audio data includes receiving data from a server indicating that the second computing device is responding to the audio data.
In bestimmten Aspekten beinhaltet Bestimmen, dass die der Äußerung entsprechenden Audiodaten das bestimmte, zuvor festgelegte Triggerwort beinhalten, die Schritte des Extrahierens von Audiomerkmalen aus den Audiodaten, die der Äußerung entsprechen; Erzeugung einer Triggerwort-Konfidenzpunktzahl durch Verarbeiten der Audiomerkmale; Bestimmen, dass die Triggerwort-Konfidenzpunktzahl einen Triggerwort-Konfidenzschwellenwert erreicht; und basierend auf Bestimmen, dass die Triggerwort-Vertrauenspunktzahl einen Triggerwort-Vertrauensschwellenwert erreicht, Bestimmen, dass die Audiodaten, die der Äußerung entsprechen, das bestimmte, zuvor festgelegte Triggerwort beinhalten.In certain aspects, determining that the audio data corresponding to the utterance includes the determined, predetermined trigger word includes the steps of extracting audio features from the audio data corresponding to the utterance; Generating a trigger word confidence score by processing the audio features; Determining that the trigger word confidence score reaches a trigger word confidence threshold; and determining that the trigger word confidence score reaches a trigger word confidence threshold based on determining that the audio data corresponding to the utterance includes the determined, predetermined trigger word.
In einigen Implementierungen umfasst Generieren einer Transkription, die einer Antwort auf die zusätzlichen Audiodaten entspricht, Bestimmen von Benutzerinformationen, die der Transkription der zusätzlichen Audiodaten zugeordnet sind, die einem ersten Benutzer des ersten Computergeräts oder einem zweiten Benutzer des zweiten Computergeräts zugeordnet sind; und wobei die Transkription basierend auf den Benutzerinformationen generiert wird.In some implementations, generating a transcription corresponding to a response to the additional audio data comprises determining user information associated with the transcription of the additional audio data associated with a first user of the first computing device or a second user of the second computing device; and wherein the transcription is generated based on the user information.
In bestimmten Aspekten umfasst Generieren einer Transkription, die einer Antwort auf die zusätzlichen Audiodaten entspricht, die Aktionen des Aufrufens von Daten, die der Transkription der zusätzlichen Audiodaten zugeordnet sind; und Generieren der Transkription basierend auf den aufgerufenen Daten.In certain aspects, generating a transcription corresponding to a response to the additional audio data comprises the actions of invoking data associated with the transcription of the additional audio data; and generate the transcription based on the retrieved data.
Die Aktionen beinhalten des Weiteren Bestimmen eines Standortes des ersten Computergeräts und Generieren der Transkription basierend auf dem Standort des ersten Computergeräts. Die Aktionen beinhalten des Weiteren Bereitstellen der Audiodaten, die der Äußerung entsprechen, als Antwort auf Empfangen von Daten, die angeben, dass das zweite Computergerät auf die Audiodaten antwortet, für das zweite Computergerät oder für einen Server.The actions further include determining a location of the first computing device and generating the transcription based on the location of the first computing device. The actions further include providing the audio data corresponding to the utterance in response to receiving data indicating that the second computing device is responding to the audio data for the second computing device or for a server.
In einigen Implementierungen beinhalten die Aktionen des Weiteren Generieren eines ersten Audiofingerabdrucks der Audiodaten, die der Äußerung entsprechen, Empfangen eines zweiten Audiofingerabdrucks der Audiodaten von dem zweiten Computergerät, die der Äußerung entsprechen; Vergleichen des ersten Audiofingerabdrucks mit dem zweiten Audiofingerabdruck; und basierend auf Vergleichen des ersten Audiofingerabdrucks mit dem zweiten Audiofingerabdruck Bestimmen, dass die von dem ersten Computergerät empfangenen Audiodaten den von dem zweiten Computergerät empfangenen Audiodaten entsprechen.In some implementations, the actions further include generating a first audio fingerprint of the audio data corresponding to the utterance, receiving a second audio fingerprint of the audio data from the second computing device corresponding to the utterance; Comparing the first audio fingerprint with the second audio fingerprint; and determining, based on comparing the first audio fingerprint with the second audio fingerprint, that the audio data received from the first computing device corresponds to the audio data received from the second computing device.
In einigen Implementierungen beinhalten weitere Aktionen Empfangen einer Transkription der Audiodaten von dem zweiten Computergerät, die der Äußerung entsprechen, wobei Generieren der Transkription, die der Antwort auf die zusätzlichen Audiodaten entspricht, des Weiteren auf der Transkription der Audiodaten basiert, die der Äußerung entsprechen.In some implementations, further actions include receiving a transcription of the audio data from the second computing device that corresponds to the utterance, wherein generating the transcription corresponding to the answer to the additional audio data is further based on the transcription of the audio data corresponding to the utterance.
In einigen Implementierungen sind das erste Computergerät und das zweite Computergerät in der Lage, Kurzstreckenfunkkommunikationen voneinander zu erkennen.In some implementations, the first computing device and the second computing device are capable of detecting short-range radio communications from each other.
In einigen Implementierungen sind das erste Computergerät und das zweite Computergerät am gleichen Standort positioniert.In some implementations, the first computing device and the second computing device are located at the same location.
Andere Ausführungsformen dieses Aspekts beinhalten entsprechende Systeme, Vorrichtungen und Computerprogramme, die auf Computerspeichergeräte aufgezeichnet sind, von denen jedes konfiguriert ist, die Vorgänge der Verfahren auszuführen.Other embodiments of this aspect include corresponding systems, devices, and computer programs recorded on computer storage devices, each of which is configured to perform the operations of the methods.
Bestimmte Ausführungsformen des in dieser Spezifikation beschriebenen Gegenstands können so implementiert werden, dass sie einen oder mehrere der folgenden Vorteile verwirklichen. Zunächst kann ein zusammenwirkendes sprachgesteuertes Gerätesystem Gesprächssuche intelligenter und natürlicher gestalten, indem es nahtlos mehrere und sequenzielle Antworten auf koordinierte Art ähnlich wie bei einem tatsächlichen Gespräch bereitstellt. Vor dem Bereitstellen einer Antwort auf eine Abfrage kann ein Gerät die verfügbaren Informationen zum Generieren einer Antwort verarbeiten. Jedes Gerät kann eine einzigartige Antwort bereitstellen, die zum Gespräch hinzugefügt werden kann, und benutzerspezifisch oder standortspezifisch sein kann. Das Gerät kann entweder die einzigartige Antwort bereitstellen oder die Informationen an ein anderes Gerät weiterleiten. Bereitstellen von Informationen für andere Geräte ermöglicht dem Gerät, Gespräche menschlicher zu gestalten, indem absichtlich Teile der einzigartigen Antwort weggelassen werden, sodass andere Geräte die Antwort vervollständigen können. Zweitens kann ein zusammenwirkendes sprachgesteuertes Gerätesystem das Problem lösen, dass mehrere Geräte auf dieselbe Abfrage auf nahezu identische Art antworten. Beispielsweise ermöglicht Kommunikation zwischen den Geräten, dass die Geräte bestimmen, wie und wann die Geräte auf die Abfrage antworten. Dadurch wird die Verschwendung von Computerressourcen und Batterieleistung reduziert, indem redundante Ausgaben vermieden werden. Drittens kann das zusammenwirkende sprachgesteuerte Gerätesystem zum Verbessern der Sprachverarbeitung durch Verwenden mehrerer Geräte mit unterschiedlichen Mikrofonen an unterschiedlichen Standorten zum Verarbeiten von Audiodaten verwendet werden, um eine genauere Transkription der Audiodaten zu erhalten.Certain embodiments of the subject matter described in this specification may be implemented to achieve one or more of the following advantages. First, a collaborative voice-driven device system can make call discovery smarter and more natural by seamlessly providing multiple and sequential responses in a coordinated manner similar to an actual conversation. Before providing a response to a query, a device can process the available information to generate a response. Each device can provide a unique answer that can be added to the conversation and can be user-specific or site-specific. The device can either provide the unique response or forward the information to another device. Providing information for other devices allows the device to make conversations more human by deliberately omitting parts of the unique response so that other devices can complete the response. Second, a cooperative voice-driven device system can solve the problem that multiple devices respond to the same query in a nearly identical manner. For example, communication between the devices allows the devices to determine how and when the devices respond to the query. This reduces the waste of computer resources and battery performance by avoiding redundant expense. Third, the cooperative voice-controlled device system can be used to enhance speech processing by using multiple devices with different microphones at different locations to process audio data to obtain more accurate transcription of the audio data.
Die Details von einer oder mehreren Ausführungsformen des in dieser Patentschrift beschriebenen Gegenstands sind in den begleitenden Zeichnungen und der nachfolgenden Beschreibung dargelegt. Weitere Merkmale, Aspekte und Vorteile des Gegenstands werden anhand der Beschreibung, der Zeichnungen und der Patentansprüche ersichtlich.The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other features, aspects and advantages of the subject matter will become apparent from the description, the drawings and the claims.
Figurenlistelist of figures
-
1 zeigt ein Diagramm eines exemplarischen Systems zum Zusammenwirken zwischen mehreren sprachgesteuerten Geräten.1 Fig. 12 shows a diagram of an exemplary system for interaction between multiple voice-operated devices. -
2 zeigt ein Diagramm eines exemplarischen Verfahrens zum Zusammenwirken zwischen mehreren sprachgesteuerten Geräten.2 FIG. 12 is a diagram of an exemplary method of interaction between multiple voice-activated devices. FIG. -
3 zeigt ein Beispiel eines Computergeräts und eines mobilen Computergeräts.3 shows an example of a computing device and a mobile computing device.
In den unterschiedlichen Zeichnungen werden gleiche Bezugszeichen und Bezeichnungen für gleiche Elemente verwendet.In the various drawings, like reference numerals and designations are used for like elements.
AUSFÜHRLICHE BESCHREIBUNGDETAILED DESCRIPTION
Ausführlicher ausgedrückt, beginnt die Sequenz von Ereignissen in
In einigen Implementierungen identifizieren die Computergeräte andere Computergeräte, die zum Antworten auf das Triggerwort konfiguriert sind, durch Identifizieren des an jedem Gerät angemeldeten Benutzers. In einigen Implementierungen, und in diesem Beispiel, kann Computergerät
In einigen Implementierungen können beide Computergeräte
In einigen Implementierungen können die Computergeräte
In einigen Implementierungen können mehr als zwei Computergeräte vorhanden sein, die zum Antworten auf das bestimmte Triggerwort konfiguriert sind. Jedes Computergerät kann die anderen Computergeräte identifizieren, die zum Antworten auf das bestimmte Triggerwort konfiguriert sind, und kann die Geräte-IDs für die anderen Computergeräte in der Gerätegruppe speichern.In some implementations, there may be more than two computing devices configured to respond to the particular trigger word. Each computing device may identify the other computing devices that are configured to respond to the particular trigger word and may store the device IDs for the other computing devices in the device group.
In einigen Implementierungen können die Computergeräte an demselben Standort positioniert sein, sodass sie einen selben Standort oder Platz teilen. Die Computergeräte können innerhalb eines zuvor festgelegten Abstands voneinander, oder in demselben Raum sein. Die Computergeräte können in derselben akustischen Umgebung sein. In einigen Beispielen können die Computergeräte virtuell am gleichen Standort positioniert sein, z. B. wenn die Computergeräte an einer Telefon- oder Videokonferenz teilnehmen.In some implementations, the computing devices may be located at the same location so that they share a same location or space. The computing devices may be within a predetermined distance from each other, or in the same room. The computing devices may be in the same acoustic environment. In some examples, the computing devices may be virtually located at the same location, e.g. For example, when the computing devices participate in a telephone or video conference.
In einigen Implementierungen können die Computergeräte andere Computergeräte identifizieren, die zum Antworten auf das Triggerwort durch eine Kurzstreckenkommunikation, wie z. B. das Bluetooth-Protokoll oder das Bluetooth-Low-Energy (BLE)-Protokoll, konfiguriert sind. Beispielsweise kann das Computergerät
In diesem Beispiel und in Stufe B spricht Alice die Äußerung
Die jeweiligen Mikrofone
In Stufe C stellen das jeweilige Audiosubsystem
Die Triggerwortprozessoren
In einigen Implementierungen senden ein oder mehrere der Computergeräte die verarbeiteten Audiodaten zu einem Server und der Server berechnet eine Triggerwort-Konfidenzpunktzahl. In diesem Fall beinhaltet der Server einen Triggerwortprozessor ähnlich den Triggerwortprozessoren
In einigen Implementierungen stellt das System das Vorhandensein des Triggerwortes in den Audiodaten fest, ohne eine Spracherkennung der Audiodaten durchzuführen.In some implementations, the system detects the presence of the trigger word in the audio data without performing speech recognition of the audio data.
In einigen Implementierungen kann jedes Computergerät Audiofingerabdrücke der empfangenen Audiodaten generieren. Computergerät
In einigen Implementierungen können die Audiodaten von den Computergeräten zu einem Server gesendet werden. Der Server kann ein Audiofingerabdruckmodul zum Generieren der Audiofingerabdrücke und Ausführen des Vergleichs beinhalten. In einigen Implementierungen können die Computergeräte auch eine Geräte-ID mit den Audiodaten senden. In einigen Implementierungen können die Computergeräte auch Standortinformationen, die den Computergeräten zugeordnet sind, mit den Audiodaten senden.In some implementations, the audio data may be sent from the computing devices to a server. The server may include an audio fingerprint module for generating the audio fingerprints and performing the comparison. In some implementations, the computing devices may also send a device ID with the audio data. In some implementations, the computing devices may also send location information associated with the computing devices with the audio data.
In Stufe D identifiziert das sprachgesteuerte System
In einigen Implementierungen kann der Antwortindikator
In einigen Implementierungen kann der Antwortindikator
In einigen Implementierungen kann Bestimmen, welches Gerät anfänglich auf eine Benutzeräußerung antwortet, Bestimmen beinhalten, welches Gerät sich am Nächsten zu einem Benutzer befindet. Die Lautstärke der durch das Computergerät empfangenen Audiodaten kann einen Abstand zwischen dem Computergerät und der Quelle des Audios widerspiegeln. Für die Direktweg-Signalausbreitung ist die Lautstärke etwa umgekehrt proportional zu dem Quadrat des Abstandes zwischen der Quelle und dem Empfänger.In some implementations, determining which device initially responds to a user utterance may include determining which device is closest to a user. The volume of the audio data received by the computing device may reflect a distance between the computing device and the source of the audio. For direct path signal propagation, the volume is approximately inversely proportional to the square of the distance between the source and the receiver.
In einigen Implementierungen können die Computergeräte eine Lautstärkepunktzahl für die Audiodaten unter Verwendung von einer oder einer Kombination der folgenden Techniken berechnen. Eine Technik kann darin bestehen, den Schalldruck oder Schalldruckpegel zu berechnen, wie er durch das Mikrofon empfangen wird, wenn der Benutzer die Äußerung spricht. Je höher der Schalldruck oder der Schalldruckpegel, umso höher die Lautstärke. Eine zweite Technik besteht im Berechnen des Effektivwerts der Audiodaten. Je höher der Effektivwert der Audiodaten, umso höher die Lautstärke. Eine dritte Technik besteht im Berechnen der Schallintensität der Audiodaten. Je höher die Schallintensität der Audiodaten, umso höher die Lautstärke. Eine vierte Technik besteht im Berechnen der Schallleistung der Audiodaten. Je höher die Schallleistung, umso höher die Lautstärke. Das Computergerät mit der höchsten Lautstärkepunktzahl kann als das Computergerät identifiziert werden, das anfänglich auf die Äußerung
In einigen Implementierungen kann Bestimmen, welches Gerät anfänglich auf eine Benutzeräußerung antwortet, Bestimmen beinhalten, welches Gerät die klarsten Audiodaten empfängt. Jedes Computergerät kann eine Klarheitspunktzahl berechnen, die basierend auf dem Signal-Rausch-Verhältnis der Audiodaten bestimmt wird, die der Äußerung
In einigen Implementierungen kann Bestimmen, welches Gerät anfänglich auf eine Benutzeräußerung antwortet, Ausführen der Sprecheridentifikation an den Audiodaten entsprechend der Äußerung
In einigen Implementierungen können die Computergeräte eine von diesen Punktzahlen oder eine Kombination derselben verwenden, um eine Gesamtkonfidenzpunktzahl zu bestimmen. Die Gesamtkonfidenzpunktzahl kann zum Identifizieren des Computergeräts verwendet werden, das anfänglich auf die Benutzeräußerung
In einigen Implementierungen kann jedes Computergerät einen Punktzahlvergleicher zum Vergleichen der Gesamtkonfidenzpunktzahlen von allen Computergeräten verwenden, die zum Antworten auf das Triggerwort konfiguriert sind. Das Computergerät mit der höchsten Gesamtkonfidenzpunktzahl kann die Spracherkennung an den Audiodaten nach dem Triggerwort initiieren. Ein Computergerät, das nicht die höchste Gesamtkonfidenzpunktzahl aufweist, kann auf Daten warten, die angeben, dass ein anderes Computergerät begonnen hat, die Audiodaten zu verarbeiten.In some implementations, each computing device may use a score comparator to compare the total confidence scores of all computing devices that are configured to respond to the trigger word. The computing device with the highest total confidence score may initiate speech recognition on the audio data after the trigger word. A computing device that does not have the highest overall confidence score may wait for data indicating that another computing device has started to process the audio data.
In einigen Implementierungen kann Bestimmen, welches Gerät anfänglich auf eine Benutzeräußerung antwortet, Analysieren der Einstellungen des Computergeräts umfassen. Wenn beispielsweise die Äußerung auf Französisch, und ein Telefon mit französischen Spracheinstellungen in der Nähe ist, dann ist dies wahrscheinlich das Computergerät, wofür die Äußerung vorgesehen war.In some implementations, determining which device initially responds to a user utterance may include analyzing the settings of the computing device. For example, if the statement is in French, and a phone with French language settings is nearby, then this is probably the computing device for which the utterance was intended.
In einigen Implementierungen kann Bestimmen, welches Gerät anfänglich auf eine Benutzeräußerung antwortet, durch Analysieren der Semantik des Befehls oder der Abfrage erfolgen, die in der Äußerung beinhaltet ist, und durch Korrelieren derselben mit dem Status und den Informationen der Computergeräte. Wenn beispielsweise die Abfrage lautet „Mit wem treffe ich mich um zwei Uhr?“, dann kann das sprachgesteuerte System bestimmen, dass die Abfrage für das Computergerät vorgesehen ist, das mit einem Kalender synchronisiert ist, und einen Termin um zwei Uhr hat.In some implementations, determining which device initially responds to a user utterance may be accomplished by analyzing the semantics of the command or query contained in the utterance and correlating it with the status and information of the user Computer equipment. For example, if the query is "Who am I meeting at two o'clock?", Then the voice-controlled system can determine that the query is for the computing device that is synchronized with a calendar and has an appointment at two o'clock.
In Stufe E verarbeitet das Computergerät
In einigen Implementierungen kann das Computergerät
In einigen Implementierungen kann das Computergerät
In Stufe F stellt das Computergerät
In einigen Implementierungen kann die der Antwort entsprechende Transkription
In Stufe G sendet Computergerät
In einigen Implementierungen können ein oder mehrere der Computergeräte ein Privatsphärenmodul beinhalten, das die übertragenen oder empfangenen Daten basierend auf Benutzereinstellungen oder -voreinstellungen verwaltet oder filtert.In some implementations, one or more of the computing devices may include a privacy module that manages or filters the transmitted or received data based on user preferences or preferences.
In Stufe H verarbeitet das Computergerät
In einigen Implementierungen kann das Computergerät
In einigen Implementierungen kann Computergerät
In Stufe I stellt das Computergerät
In einigen Implementierungen umfasst das Bereitstellen der Transkription zum Ausgeben, die der Antwort entspricht, Bereitstellen der Transkription zum Anzeigen auf einer Anzeige
In einigen Implementierungen können die Parameter, die die durch den Sprachsynthesizer produzierte Stimme umfassen (z. B. Tonhöhe, Tonfall, Akzent, Geschwindigkeit, Beugung usw.), angepasst werden. Dies ermöglicht die Erzeugung von individuellen Stimmen für jedes Computergerät, wodurch die Benutzererfahrung verbessert wird. Der Sprachsynthesizer kann eine Form von Maschinenlernen zum Generieren der parametrischen Darstellung von Sprache zum Synthetisieren der Sprache verwenden. Beispielsweise kann ein neuronales Netzwerk zum Generieren der Sprachparameter verwendet werden.In some implementations, the parameters that comprise the voice produced by the speech synthesizer (eg, pitch, pitch, accent, velocity, inflection, etc.) may be adjusted. This allows the generation of individual voices for each computing device, thereby improving the user experience. The speech synthesizer may use some form of machine learning to generate the parametric representation of speech synthesizing speech. For example, a neural network can be used to generate the speech parameters.
In einigen Implementierungen können ein oder mehrere Computergeräte einer anfänglichen Antwort mit einer Frage oder einer Aufforderung für den Benutzer folgen, um zu antworten, bevor weiter fortzufahren. Wenn beispielsweise Alice die Äußerung spricht „OK Computer, finde für mich ein gutes Steakhaus in der Nähe zum Mittagessen“, kann Alices Gerät eine Liste von Restaurants in der Nähe bereitstellen. Bobs Gerät kann auf Bobs persönliche Informationen zugreifen und die Antwort bereitstellen „Bob mag argentinische Steaks, es gibt eine Menge guter Steakhäuser in der Nähe. Würden Sie gern mal eins besuchen?“ Eines oder mehrere der Computergeräte kann auf eine Antwort auf die Frage warten und basierend auf der Antwort fortfahren. Nach Empfangen der Antwort „Ja“, könnte Bobs Gerät eine Folgeantwort „Rural Society in 123, Madison Avenue hat gute Bewertungen“ bereitstellen.In some implementations, one or more computing devices may follow an initial response with a question or a prompt for the user to respond before proceeding. For example, when Alice pronounces "OK Computer, I find a good steakhouse nearby for lunch," Alice's device can provide a list of nearby restaurants. Bob's device can access Bob's personal information and provide the answer. "Bob likes Argentine steaks, there are a lot of good steakhouses nearby. Would you like to visit one? "One or more of the computing devices can wait for an answer to the question and continue based on the answer. After receiving the answer "yes", Bob's device could provide a follow-up response "Rural Society at 123, Madison Avenue has good reviews".
In einigen Implementierungen kann das zusammenwirkende sprachgesteuerte Gerätesystem mehr als zwei Geräte aufweisen. Beispielsweise kann eine Gruppe von drei Benutzern Alice, Bob und Carl versuchen, Wegbeschreibungen zu einem Park in der Nähe zu erhalten, wobei jeder sein eigenes Gerät haben kann. Alice spricht die Äußerung „OK Computer, gib mir eine Wegbeschreibung zu Prospect Park“. Alices Gerät kann auf Alices persönliche Informationen zugreifen, die angeben, dass sie gewöhnlich den öffentlichen Personennahverkehr (ÖPNV) nutzt. Alices Gerät kann die Informationen zusammen mit den von einem ÖPNV-Webtool erhaltenen Informationen zum Bereitstellen der Antwort benutzen „In 12 Minuten fährt eine Bahn von Carol Square Station ab, die um 16:59 Uhr an der Prospect Park Station ankommt. Der Fußweg zur Carol Square Station beträgt 4 Minuten, und der Fußweg von der Prospect Park Station zum Eingang des Prospect Park beträgt 5 Minuten“. Bobs Gerät kann diese Antwort durch ein Netzwerk empfangen. Durch Generieren einer Antwort kann Bobs Gerät auf Bobs persönliche Informationen zugreifen und sieht, dass Bob gewöhnlich vorzieht, mit dem Auto zu fahren, und dass er sein Auto in einer Garage in der Nähe geparkt hat. Die Geräte können den Standortwechsel zur Garage, und die Diskussion zwischen Alice, Bob und Carl erkennen, mit dem Auto zu fahren, anstatt den ÖPNV zu nehmen. Bobs Gerät kann ein Web-Landkarten-Suchtool zum Bereitstellen von Wegbeschreibungen zum Park verwenden. Während der Fahrt kann Carls Gerät die Wegbeschreibungen durch das Netzwerk empfangen und kann auf ein Echtzeit-Verkehrstool zugreifen um zu bestimmen, dass es eine schnellere Strecke gibt, und eine Antwort bereitstellen „Wenn Sie Main St anstatt Rock Rd nehmen, dann sind Sie
In einigen Implementierungen können die Geräte vor dem Bereitstellen einer Antwort zusammenwirken, um vollständigere und relevante Antworten zu generieren. Dies kann durch Verwenden der verfügbaren Informationen und Teilen der persönlichen Informationen der Benutzer erfolgen, sofern ein Benutzer vorher ausgewählt hat, die persönlichen Informationen zu teilen. Wenn beispielsweise Alice sagt „OK Computer, finde für mich ein gutes Steakhaus in der Nähe zum Mittagessen“, können alle Geräte beginnen, die Abfrage zu verarbeiten, bevor eine Antwort bereitgestellt wird. Bobs Telefon kann Daten zu den anderen Geräten senden, die angeben, dass Bob Vegetarier ist. Dies ermöglicht den Geräten, ihre Antworten so zu individualisieren, dass sie nur vegetarierfreundliche Restaurant-Optionen bereitstellen.In some implementations, before providing a response, the devices may work together to generate more complete and relevant answers. This can be done by using the available information and sharing the personal information of the users, if a user has previously selected to share the personal information. For example, if Alice says, "Ok Computer, I find a good steakhouse nearby for lunch," all devices can begin processing the query before providing a response. Bob's phone can send data to the other devices indicating that Bob is a vegetarian. This allows the devices to personalize their responses to provide only vegetarian-friendly dining options.
In einigen Implementierungen können die Geräte entweder eine Antwort bereitstellen oder die Informationen an ein anderes Gerät weiterleiten. Gespräche können menschlicher oder humorvoller gestaltet werden, indem die Antworten absichtlich koordiniert werden, oder durch Weglassen von Teilen aus einer Antwort durch ein Gerät, die von einem anderen Gerät zu vervollständigen ist. Beispielsweise in dem obigen Beispiel, bei dem Bobs Telefon Daten geteilt hat, die angeben, dass Bob Vegetarier ist, stellt Alices Telefon als Antwort auf „Sollten wir Al's Steakhouse in 123 Main Street versuchen?“, keine Antwort bereit, sondern wartet ab, dass Bobs Telefon die Antwort „Sind Steaks vegetarisch?“ bereitstellt.In some implementations, the devices may either provide a response or forward the information to another device. Conversations can be made more humane or humorous by deliberately coordinating responses, or by omitting parts from a response from a device to be completed by another device. For example, in the above example where Bob's phone has shared data indicating that Bob is a vegetarian, Alice's phone does not provide an answer in response to "Should we try Al's Steakhouse at 123 Main Street?" But waits for that Bob's phone provides the answer "Are steaks vegetarian?".
Das erste Computergerät identifiziert ein zweites Computergerät, das zum Antworten auf ein bestimmtes, zuvor festgelegtes Triggerwort (
In einigen Implementierungen kann das erste Computergerät andere Computergeräte identifizieren, die demselben Benutzer gehören. Beim Einrichten eines neuen Computergeräts kann ein Teil des Einrichtungsverfahrens darin bestehen, andere Computergeräte zu identifizieren, die demselben Benutzer gehören. Dies kann durch Erkennen anderer Geräte erfolgen, an denen der Benutzer angemeldet ist. Sobald das erste Computergerät ein anderes Computergerät identifiziert, können die Computergeräte Daten unter Verwendung eines Ultraschallfrequenzsignals austauschen, das einen Bitstrom so codiert, dass die Computergeräte Daten austauschen, wenn sie ein Triggerwort identifizieren.In some implementations, the first computing device may identify other computing devices owned by the same user. When setting up a new computing device, part of the setup process may be to identify other computing devices owned by the same user. This can be done by detecting other devices the user is logged on to. Once the first computing device identifies another computing device, the computing devices may exchange data using an ultrasonic frequency signal that encodes a bitstream such that the computing devices exchange data when identifying a triggering word.
Das Ultraschallfrequenzsignal kann durch einen Sprecher übertragen werden. Beispielsweise kann ein Benutzer einen Haushaltsassistenten einrichten, wobei ein Teil des Einrichtungsprozesses im Suchen nach anderen Computergeräten besteht, die sich in der Nähe befinden, und die auf Triggerwörter antworten. Der Haushaltsassistent kann ein Telefon und ein Tablet identifizieren, an denen der Benutzer angemeldet ist. Beispielsweise können der Thermostat, Telefon und Tablet Daten unter Verwendung eines Ultraschallsignals austauschen nach Identifizieren, dass alle in einem selben Raum positioniert sind. Das Ultraschallsignal kann einen Bitstrom beinhalten, der jedes der Geräte identifiziert. Die Geräte können den Ultraschallkanal nach Erkennen eines Triggerwortes verwenden.The ultrasonic frequency signal may be transmitted by a speaker. For example, a user may set up a home assistant, where part of the setup process is to search for other nearby computing devices that respond to trigger words. The Home Assistant can identify a phone and a tablet the user is logged in to. For example, the thermostat, telephone and tablet can exchange data using an ultrasound signal after identifying that all are in the same room. The ultrasound signal may include a bitstream identifying each of the devices. The devices can use the ultrasound channel after detecting a trigger word.
In einigen Implementierungen können die Computergeräte andere Computergeräte identifizieren, die zum Antworten auf das Triggerwort durch Identifizieren von Computergeräten konfiguriert sind, die konfiguriert sind, um zu antworten, wenn das Triggerwort durch denselben Benutzer durch Sprecheridentifizierung gesprochen wird. Beispielsweise können die Computergeräte
Ein Benutzer spricht die Äußerung und ein Mikrofon des ersten Computergeräts empfängt die Audiodaten der Äußerung (
In einigen Implementierungen bestimmt das erste Computergerät, dass die Audiodaten, die der Äußerung entsprechen, das bestimmte, zuvor festgelegte Triggerwort beinhalten (
Die Triggerwort-Konfidenzpunktzahl kann auf eine Skala von eins zu eins normalisiert werden, wobei eins die höchste Wahrscheinlichkeit angibt, dass die Äußerung ein Triggerwort beinhaltet. Das erste Computergerät kann die Triggerwort-Konfidenzpunktzahl mit einem Triggerwort-Konfidenzschwellenwert vergleichen. Wenn die Triggerwort-Konfidenzpunktzahl einen Triggerwort-Konfidenzschwellenwert erreicht, kann das erste Computergerät bestimmen, dass die Audiodaten, die der Äußerung entsprechen, das zuvor festgelegte Triggerwort beinhalten.The trigger word confidence score may be normalized to a one-to-one scale, where one indicates the highest probability that the utterance includes a trigger word. The first computing device may compare the trigger word confidence score with a trigger word confidence threshold. When the trigger word confidence score reaches a trigger word confidence threshold, the first computing device may determine that the audio data corresponding to the utterance includes the predetermined trigger word.
Erreicht die Triggerwort-Konfidenzpunktzahl den Triggerwort-Konfidenzschwellenwert nicht, dann bestimmt das System, dass die Audiodaten das Triggerwort nicht beinhalten. Beträgt der Schwellenwert beispielsweise 0,75, so werden Audiodaten mit einer Triggerwort-Konfidenzpunktzahl von über 0,75 als Triggerwort enthaltend eingestuft, während Audiodaten mit einer Triggerwort-Konfidenzpunktzahl von unter 0,75 als nicht Triggerwort enthaltend eingestuft werden.If the trigger word confidence score does not reach the trigger word confidence threshold, then the system determines that the audio data does not include the trigger word. For example, if the threshold is 0.75, then audio data having a trigger word confidence score greater than 0.75 is considered to contain the trigger word, while audio data having a trigger word confidence score of below 0.75 as not containing a trigger word.
In einigen Implementierungen stellt das System das Vorhandensein des Triggerwortes in den Audiodaten fest, ohne eine Spracherkennung der Audiodaten durchzuführen.In some implementations, the system detects the presence of the trigger word in the audio data without performing speech recognition of the audio data.
In einigen Implementierungen bestimmt das erste Computergerät, dass die Wahrscheinlichkeit, dass die Äußerung ein bestimmtes, zuvor festgelegtes Triggerwort beinhaltet, durch Vergleichen der Audiodaten der Äußerung mit einer Gruppe von Audioproben, die das Triggerwort beinhalten.In some implementations, the first computing device determines that the likelihood that the utterance includes a particular, predetermined trigger word by comparing the audio data of the utterance with a group of audio samples containing the trigger word.
In einigen Implementierungen empfängt das erste Computergerät Daten, die angeben, dass das zweite Computergerät auf die Audiodaten antwortet (
Das erste Computergerät empfängt eine Transkription von zusätzlichen Audiodaten, die durch das zweite Computergerät ausgegeben werden, als Antwort auf die Äußerung (
In einigen Implementierungen kann das zweite Computergerät eine Transkription von zusätzlichen Audiodaten als Antwort auf die Äußerungsantwort auf die empfangenen Daten basierend auf Benutzerinformationen generieren, die den Audiodaten zugeordnet sind. Die Benutzerinformationen können einem Benutzer des ersten Computergeräts zugeordnet sein (z. B. Kalender, Kontakte, Mediendateien, Informationen in den sozialen Medien, persönliche Voreinstellungen usw.).In some implementations, the second computing device may generate a transcription of additional audio data in response to the utterance response to the received data based on user information associated with the audio data. The user information may be associated with a user of the first computing device (eg, calendars, contacts, media files, social media information, personal preferences, etc.).
Wenn beispielsweise Bob die Äußerung „OK Computer, plane eine Besprechung mit Bob für diesen Nachmittag“ spricht, dann kann Computergerät 104b Informationen von dem Bob zugeordneten Kalender verwenden, der auf dem Computergerät
Das erste Computergerät generiert eine Transkription, die einer Antwort auf die zusätzlichen Audiodaten entspricht, basierend auf der Transkription der zusätzlichen Audiodaten und der Äußerung (
In einigen Implementierungen können ein oder mehrere der Computergeräte
In einigen Implementierungen kann das Computergerät Daten aufrufen, die der Transkription der zusätzlichen Audiodaten zugeordnet sind, und die Transkription basierend auf den aufgerufenen Daten generieren. Wenn beispielsweise die Transkription der zusätzlichen Audiodaten eine Abfrage beinhaltet, die Vorschläge für Restaurants in der Nähe abfragt, kann das Computergerät eine Websuche ausführen, Daten aufzurufen, die Restaurants in der Nähe entsprechen. Das Computergerät kann diese aufgerufenen Daten zum Generieren der Transkription entsprechend der Antwort verwenden.In some implementations, the computing device may call data associated with the transcription of the additional audio data and generate the transcription based on the retrieved data. For example, if the transcription of the additional audio includes a query that queries restaurants in the vicinity, the computing device may perform a web search to retrieve data corresponding to nearby restaurants. The computing device may use this retrieved data to generate the transcription corresponding to the response.
Das Computergerät stellt die der Antwort (
In einigen Implementierungen kann die der Antwort entsprechende Transkription zu einer Anzeige des Computergeräts gesendet werden.In some implementations, the transcription corresponding to the answer may be sent to a display of the computing device.
In einigen Implementierungen kann Zusammenwirken zwischen mehreren Geräten zum Verbessern der Qualität von Spracherkennung verwendet werden. Da unterschiedliche Geräte unterschiedliche Sensoren (Mikrofone) aufweisen und in den Umgebungen des Benutzers unterschiedlich positioniert sind, könnte es sein, dass keines von ihnen die gesamte Benutzerabfrage korrekt transkribiert hat, wobei jedoch Kombinieren der Transkriptionen aller Computergeräte perfekt zu der Abfrage passen könnte. In einem anderen Beispiel, wenn sich der Benutzer um einen Raum bewegt, und andere Computergeräte zu unterschiedlichen Zeiten während des Sprechens der Abfrage nahe bei dem Benutzer sind, kann durch Verwenden der Transkriptionen von jedem der Computergeräte und deren Kombination eine genauere Transkription erhalten werden.In some implementations, interaction between multiple devices may be used to improve the quality of speech recognition. Because different devices have different sensors (microphones) and are positioned differently in the user's environment, none of them might have correctly transcribed the entire user query, but combining the transcriptions of all computing devices could perfectly match the query. In another example, if the user is moving around a room and other computing devices are near the user at different times while speaking the query, a more accurate transcription can be obtained by using the transcriptions from each of the computing devices and their combination.
Das Computergerät
Der Speicher
Das Speichergerät
Die Hochgeschwindigkeitsschnittstelle
Das Computergerät
Alternativ können Komponenten des Computergeräts
Das mobile Computergerät
Der Prozessor
Der Prozessor
Der Speicher
Der Speicher kann, wie nachfolgend beschrieben, beispielsweise Flash-Speicher und/oder NVRAM-Speicher (nicht flüchtigen Direktzugriffsspeicher) umfassen. In einigen Implementierungen werden Anweisungen in einem Informationsträger gespeichert, sodass die Anweisungen, wenn sie von einem oder mehreren Verarbeitungsgeräten (zum Beispiel Prozessor
Das mobile Computergerät
Das mobile Computergerät
Das mobile Computergerät
Verschiedene Implementierungen der hier beschriebenen Systeme und Techniken können in digitalen elektronischen Schaltungen, integrierten Schaltungen, speziell konzipierten ASICs (anwendungsorientierten integrierten Schaltungen), Computerhardware, Firmware, Software und/oder Kombinationen derselben realisiert sein. Diese verschiedenen Implementierungen können eine Implementierung in einem oder mehreren Computerprogrammen beinhalten, die auf einem programmierbaren System ausführbar und/oder interpretierbar sind, das mindestens einen programmierbaren Prozessor beinhaltet, bei dem es sich um einen Spezial- oder Universalprozessor handeln kann und der zum Empfangen von Daten und Anweisungen von und zum Übertragen von Daten und Befehlen an ein Speichersystem, mindestens ein Eingabegerät und mindestens ein Ausgabegerät gekoppelt ist.Various implementations of the systems and techniques described herein may be implemented in digital electronic circuits, integrated circuits, specially designed ASICs (application-oriented integrated circuits), computer hardware, firmware, software, and / or combinations thereof. These various implementations may include implementation in one or more computer programs executable and / or interpretable on a programmable system including at least one programmable processor, which may be a special purpose or general purpose processor, and for receiving data and instructions from and for transmitting data and commands to a storage system, at least one input device, and at least one output device.
Diese Computerprogramme (auch bekannt als Programme, Software, Softwareanwendungen oder Code) beinhalten Maschinenbefehle für einen programmierbaren Prozessor und können in einer höheren prozeduralen und/oder objektorientierten Programmiersprache und/oder in Assembler-/Maschinensprache implementiert sein. Wie hierin verwendet, bezeichnen die Begriffe „maschinenlesbares Medium“ und „computerlesbares Medium“ ein beliebiges Computerprogrammprodukt, eine beliebige Vorrichtung und/oder ein beliebiges Gerät (z. B. magnetische Platten, optische Datenträger, Speicher, programmierbare Logikbausteine (PLDs)), die verwendet werden, um einem programmierbaren Prozessor, einschließlich eines maschinenlesbaren Mediums, das Maschinenbefehle als ein maschinenlesbares Signal empfängt, Maschinenbefehle und/oder Daten bereitzustellen. Der Begriff „maschinenlesbares Signal“ bezeichnet ein beliebiges Signal, das verwendet wird, um einem programmierbaren Prozessor Maschinenbefehle und/oder Daten bereitzustellen.These computer programs (also known as programs, software, software applications or code) include machine instructions for a programmable processor and may be implemented in a higher level of procedural and / or object-oriented programming language and / or assembler / machine language. As used herein, the terms "machine-readable medium" and "computer-readable medium" refer to any computer program product, device, and / or device (eg, magnetic disks, optical disks, memory, programmable logic devices (PLDs)) that may be used to provide machine instructions and / or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term "machine-readable signal" refers to any signal used to provide machine instructions and / or data to a programmable processor.
Um eine Interaktion mit einem Benutzer bereitzustellen, können die hier beschriebenen Systeme und Techniken auf einem Computer implementiert werden, der ein Anzeigegerät (wie z. B. einen CRT- (Kathodenstrahlröhren) oder LCD-(Flüssigkristallanzeige) -Monitor) aufweist, um dem Benutzer Informationen anzuzeigen, sowie eine Tastatur und ein Zeigegerät (z. B. eine Maus oder ein Trackball) aufweist, mittels denen der Benutzer eine Eingabe an dem Computer vornehmen kann. Es können auch andere Arten von Geräten verwendet werden, um eine Interaktion mit einem Benutzer zu ermöglichen; beispielsweise kann eine an den Benutzer bereitgestellte Rückmeldung in einer beliebigen Form von sensorischer Rückmeldung (wie z. B. einer visuellen Rückmeldung, akustischen Rückmeldung oder taktilen Rückmeldung, erfolgen); zudem kann die Eingabe vom Benutzer ebenfalls in beliebiger Form, darunter auch als akustische, taktile oder Spracheingabe, empfangen werden.To provide interaction with a user, the systems and techniques described herein may be implemented on a computer having a display device (such as a CRT (Cathode Ray Tube) or LCD (Liquid Crystal Display) monitor) to the user Display information, and a keyboard and a pointing device (eg, a mouse or a trackball), by means of which the user can make an input to the computer. Other types of devices may be used to facilitate interaction with a user; for example, feedback provided to the user may be in any form of sensory feedback (such as visual feedback, audible feedback, or tactile feedback); In addition, the input can also be received by the user in any form, including as acoustic, tactile or voice input.
Die hier beschriebenen Systeme und Techniken können in einem Computersystem implementiert werden, das eine Backendkomponente (z. B. einen Datenserver) beinhaltet, oder das eine Middlewarekomponente (z. B. einen Anwendungsserver) beinhaltet, oder das eine Frontendkomponente (z. B. einen Clientcomputer mit einer grafischen Benutzeroberfläche oder einem Webbrowser beinhaltet, durch die bzw. den ein Benutzer mit den hier beschriebenen Implementierungen des Systems und der Techniken interagieren kann), oder eine Kombination dieser Backend-, Middleware- oder Frontendkomponenten beinhaltet. Die Komponenten des Systems können durch eine beliebige Form oder ein beliebiges Medium digitaler Datenkommunikation (wie z. B. ein Kommunikationsnetzwerk) miteinander verbunden sein. Beispiele von Kommunikationsnetzwerken beinhalten ein lokales Netzwerk („LAN“), ein Großraumnetzwerk („WAN“) und das Internet.The systems and techniques described herein may be implemented in a computer system that includes a back end component (eg, a data server), or that includes a middleware component (eg, an application server), or that has a front end component (eg, a Client computer with a graphical user interface or web browser through which a user may interact with the implementations of the system and techniques described herein), or a combination of these backend, middleware or frontend components. The components of the system may be interconnected by any form or medium of digital data communication (such as a communications network). Examples of communication networks include a local area network ("LAN"), a wide area network ("WAN"), and the Internet.
Das Computersystem kann Clients und Server beinhalten. Ein Client und Server befinden sich im Allgemeinen entfernt voneinander und interagieren typischerweise über ein Kommunikationsnetzwerk. Die Beziehung von Client und Server ergibt sich durch Computerprogramme, die auf den jeweiligen Computern ausgeführt werden und in einer Client-Server-Beziehung zueinander stehen.The computer system may include clients and servers. A client and server are generally remote and typically interact over a communications network. The relationship between client and server results from computer programs running on the respective computers and having a client-server relationship.
Obwohl einige Implementierungen vorstehend im Detail beschrieben wurden, sind andere Modifikationen möglich. Während eine Kundenanwendung als Zugreifen auf den bzw. die Delegierte(n) beschrieben ist, können in anderen Implementierungen der bzw. die Delegierte(n) durch andere Anwendungen verwendet werden, die durch einen oder mehrere Prozessoren implementiert sind, wie beispielsweise eine Anwendung, die auf einem oder mehreren Servern ausgeführt wird. Außerdem erfordern die in den Figuren dargestellten logischen Abläufe nicht die bestimmte dargestellte Reihenfolge oder sequenzielle Reihenfolge, um wünschenswerte Ergebnisse zu erzielen. Darüber hinaus können andere Aktionen vorgesehen oder Aktionen aus den beschriebenen Abläufen eliminiert werden und andere Komponenten zu den beschriebenen Systemen hinzugefügt oder aus denselben entfernt werden. Dementsprechend liegen andere Implementierungen im Geltungsbereich der folgenden Ansprüche.Although some implementations have been described in detail above, other modifications are possible. While a customer application is described as accessing the delegate (s), in other implementations the delegate (s) may be used by other applications implemented by one or more processors, such as an application running on one or more servers. In addition, the logical flows shown in the figures do not require the particular order or sequential order shown to achieve desirable results. In addition, other actions may be provided or actions may be eliminated from the described procedures and other components added to or removed from the described systems. Accordingly, other implementations are within the scope of the following claims.
Claims (20)
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/387,884 | 2016-12-22 | ||
US15/387,884 US10559309B2 (en) | 2016-12-22 | 2016-12-22 | Collaborative voice controlled devices |
Publications (2)
Publication Number | Publication Date |
---|---|
DE102017121086A1 true DE102017121086A1 (en) | 2018-06-28 |
DE102017121086B4 DE102017121086B4 (en) | 2021-12-30 |
Family
ID=60117370
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
DE202017105526.4U Active DE202017105526U1 (en) | 2016-12-22 | 2017-09-12 | Interactive voice operated devices |
DE102017121086.5A Active DE102017121086B4 (en) | 2016-12-22 | 2017-09-12 | INTERACTIVE VOICE ACTIVATED DEVICES |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
DE202017105526.4U Active DE202017105526U1 (en) | 2016-12-22 | 2017-09-12 | Interactive voice operated devices |
Country Status (8)
Country | Link |
---|---|
US (3) | US10559309B2 (en) |
EP (1) | EP3559945B1 (en) |
CN (2) | CN108228699B (en) |
DE (2) | DE202017105526U1 (en) |
GB (1) | GB2558342B (en) |
IE (1) | IE20170206A1 (en) |
SG (1) | SG10201707702YA (en) |
WO (1) | WO2018118136A1 (en) |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11521618B2 (en) | 2016-12-22 | 2022-12-06 | Google Llc | Collaborative voice controlled devices |
Families Citing this family (78)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9947316B2 (en) | 2016-02-22 | 2018-04-17 | Sonos, Inc. | Voice control of a media playback system |
US9965247B2 (en) | 2016-02-22 | 2018-05-08 | Sonos, Inc. | Voice controlled media playback system based on user profile |
US10095470B2 (en) | 2016-02-22 | 2018-10-09 | Sonos, Inc. | Audio response playback |
US10509626B2 (en) | 2016-02-22 | 2019-12-17 | Sonos, Inc | Handling of loss of pairing between networked devices |
US10743101B2 (en) | 2016-02-22 | 2020-08-11 | Sonos, Inc. | Content mixing |
US10264030B2 (en) | 2016-02-22 | 2019-04-16 | Sonos, Inc. | Networked microphone device control |
US9978390B2 (en) | 2016-06-09 | 2018-05-22 | Sonos, Inc. | Dynamic player selection for audio signal processing |
US10134399B2 (en) | 2016-07-15 | 2018-11-20 | Sonos, Inc. | Contextualization of voice inputs |
US10115400B2 (en) | 2016-08-05 | 2018-10-30 | Sonos, Inc. | Multiple voice services |
US9942678B1 (en) | 2016-09-27 | 2018-04-10 | Sonos, Inc. | Audio playback settings for voice interaction |
JP2018054850A (en) * | 2016-09-28 | 2018-04-05 | 株式会社東芝 | Information processing system, information processor, information processing method, and program |
US10181323B2 (en) | 2016-10-19 | 2019-01-15 | Sonos, Inc. | Arbitration-based voice recognition |
US10748531B2 (en) * | 2017-04-13 | 2020-08-18 | Harman International Industries, Incorporated | Management layer for multiple intelligent personal assistant services |
US10861476B2 (en) | 2017-05-24 | 2020-12-08 | Modulate, Inc. | System and method for building a voice database |
US10395650B2 (en) | 2017-06-05 | 2019-08-27 | Google Llc | Recorded media hotword trigger suppression |
US10176808B1 (en) * | 2017-06-20 | 2019-01-08 | Microsoft Technology Licensing, Llc | Utilizing spoken cues to influence response rendering for virtual assistants |
SG11201901441QA (en) * | 2017-08-02 | 2019-03-28 | Panasonic Ip Man Co Ltd | Information processing apparatus, speech recognition system, and information processing method |
US10475449B2 (en) | 2017-08-07 | 2019-11-12 | Sonos, Inc. | Wake-word detection suppression |
US10048930B1 (en) | 2017-09-08 | 2018-08-14 | Sonos, Inc. | Dynamic computation of system response volume |
KR102489914B1 (en) * | 2017-09-15 | 2023-01-20 | 삼성전자주식회사 | Electronic Device and method for controlling the electronic device |
US10531157B1 (en) * | 2017-09-21 | 2020-01-07 | Amazon Technologies, Inc. | Presentation and management of audio and visual content across devices |
US10446165B2 (en) | 2017-09-27 | 2019-10-15 | Sonos, Inc. | Robust short-time fourier transform acoustic echo cancellation during audio playback |
US10482868B2 (en) | 2017-09-28 | 2019-11-19 | Sonos, Inc. | Multi-channel acoustic echo cancellation |
US10621981B2 (en) | 2017-09-28 | 2020-04-14 | Sonos, Inc. | Tone interference cancellation |
US10466962B2 (en) | 2017-09-29 | 2019-11-05 | Sonos, Inc. | Media playback system with voice assistance |
WO2019152722A1 (en) | 2018-01-31 | 2019-08-08 | Sonos, Inc. | Device designation of playback and network microphone device arrangements |
US10600408B1 (en) * | 2018-03-23 | 2020-03-24 | Amazon Technologies, Inc. | Content output management based on speech quality |
US11175880B2 (en) | 2018-05-10 | 2021-11-16 | Sonos, Inc. | Systems and methods for voice-assisted media content selection |
US10692496B2 (en) | 2018-05-22 | 2020-06-23 | Google Llc | Hotword suppression |
JP2019204025A (en) * | 2018-05-24 | 2019-11-28 | レノボ・シンガポール・プライベート・リミテッド | Electronic apparatus, control method, and program |
US10959029B2 (en) | 2018-05-25 | 2021-03-23 | Sonos, Inc. | Determining and adapting to changes in microphone performance of playback devices |
US10681460B2 (en) | 2018-06-28 | 2020-06-09 | Sonos, Inc. | Systems and methods for associating playback devices with voice assistant services |
WO2020013946A1 (en) | 2018-07-13 | 2020-01-16 | Google Llc | End-to-end streaming keyword spotting |
KR20200015267A (en) * | 2018-08-03 | 2020-02-12 | 삼성전자주식회사 | Electronic device for determining an electronic device to perform speech recognition and method for the same |
US11076035B2 (en) | 2018-08-28 | 2021-07-27 | Sonos, Inc. | Do not disturb feature for audio notifications |
US10461710B1 (en) | 2018-08-28 | 2019-10-29 | Sonos, Inc. | Media playback system with maximum volume setting |
US10587430B1 (en) | 2018-09-14 | 2020-03-10 | Sonos, Inc. | Networked devices, systems, and methods for associating playback devices based on sound codes |
US11315553B2 (en) * | 2018-09-20 | 2022-04-26 | Samsung Electronics Co., Ltd. | Electronic device and method for providing or obtaining data for training thereof |
US11024331B2 (en) | 2018-09-21 | 2021-06-01 | Sonos, Inc. | Voice detection optimization using sound metadata |
US11100923B2 (en) | 2018-09-28 | 2021-08-24 | Sonos, Inc. | Systems and methods for selective wake word detection using neural network models |
US11899519B2 (en) | 2018-10-23 | 2024-02-13 | Sonos, Inc. | Multiple stage network microphone device with reduced power consumption and processing load |
KR20200047311A (en) * | 2018-10-24 | 2020-05-07 | 삼성전자주식회사 | Method And Apparatus For Speech Recognition In Multi-device Environment |
EP3654249A1 (en) | 2018-11-15 | 2020-05-20 | Snips | Dilated convolutions and gating for efficient keyword spotting |
US11183183B2 (en) | 2018-12-07 | 2021-11-23 | Sonos, Inc. | Systems and methods of operating media playback systems having multiple voice assistant services |
US11830485B2 (en) * | 2018-12-11 | 2023-11-28 | Amazon Technologies, Inc. | Multiple speech processing system with synthesized speech styles |
US11132989B2 (en) | 2018-12-13 | 2021-09-28 | Sonos, Inc. | Networked microphone devices, systems, and methods of localized arbitration |
US10602268B1 (en) | 2018-12-20 | 2020-03-24 | Sonos, Inc. | Optimization of network microphone devices using noise classification |
KR102584588B1 (en) | 2019-01-21 | 2023-10-05 | 삼성전자주식회사 | Electronic device and controlling method of electronic device |
US11315556B2 (en) | 2019-02-08 | 2022-04-26 | Sonos, Inc. | Devices, systems, and methods for distributed voice processing by transmitting sound data associated with a wake word to an appropriate device for identification |
US10867604B2 (en) | 2019-02-08 | 2020-12-15 | Sonos, Inc. | Devices, systems, and methods for distributed voice processing |
EP3709194A1 (en) | 2019-03-15 | 2020-09-16 | Spotify AB | Ensemble-based data comparison |
US11120794B2 (en) | 2019-05-03 | 2021-09-14 | Sonos, Inc. | Voice assistant persistence across multiple network microphone devices |
US11200894B2 (en) | 2019-06-12 | 2021-12-14 | Sonos, Inc. | Network microphone device with command keyword eventing |
US11361756B2 (en) | 2019-06-12 | 2022-06-14 | Sonos, Inc. | Conditional wake word eventing based on environment |
US10586540B1 (en) | 2019-06-12 | 2020-03-10 | Sonos, Inc. | Network microphone device with command keyword conditioning |
US11138975B2 (en) | 2019-07-31 | 2021-10-05 | Sonos, Inc. | Locally distributed keyword detection |
US11138969B2 (en) | 2019-07-31 | 2021-10-05 | Sonos, Inc. | Locally distributed keyword detection |
US10871943B1 (en) | 2019-07-31 | 2020-12-22 | Sonos, Inc. | Noise classification for event detection |
WO2021030759A1 (en) | 2019-08-14 | 2021-02-18 | Modulate, Inc. | Generation and detection of watermark for real-time voice conversion |
US11094319B2 (en) | 2019-08-30 | 2021-08-17 | Spotify Ab | Systems and methods for generating a cleaned version of ambient sound |
US11189286B2 (en) | 2019-10-22 | 2021-11-30 | Sonos, Inc. | VAS toggle based on device orientation |
US11200900B2 (en) | 2019-12-20 | 2021-12-14 | Sonos, Inc. | Offline voice control |
US11562740B2 (en) | 2020-01-07 | 2023-01-24 | Sonos, Inc. | Voice verification for media playback |
US11308958B2 (en) | 2020-02-07 | 2022-04-19 | Sonos, Inc. | Localized wakeword verification |
US11328722B2 (en) * | 2020-02-11 | 2022-05-10 | Spotify Ab | Systems and methods for generating a singular voice audio stream |
US11308959B2 (en) | 2020-02-11 | 2022-04-19 | Spotify Ab | Dynamic adjustment of wake word acceptance tolerance thresholds in voice-controlled devices |
US11758360B2 (en) * | 2020-02-28 | 2023-09-12 | Comcast Cable Communications, Llc | Methods, systems, and apparatuses for presence detection |
US11727919B2 (en) | 2020-05-20 | 2023-08-15 | Sonos, Inc. | Memory allocation for keyword spotting engines |
US11308962B2 (en) * | 2020-05-20 | 2022-04-19 | Sonos, Inc. | Input detection windowing |
US11482224B2 (en) | 2020-05-20 | 2022-10-25 | Sonos, Inc. | Command keywords with input detection windowing |
WO2021251953A1 (en) * | 2020-06-09 | 2021-12-16 | Google Llc | Generation of interactive audio tracks from visual content |
US11698771B2 (en) | 2020-08-25 | 2023-07-11 | Sonos, Inc. | Vocal guidance engines for playback devices |
US11749284B2 (en) * | 2020-11-13 | 2023-09-05 | Google Llc | Dynamically adapting on-device models, of grouped assistant devices, for cooperative processing of assistant requests |
US20210225374A1 (en) * | 2020-12-23 | 2021-07-22 | Intel Corporation | Method and system of environment-sensitive wake-on-voice initiation using ultrasound |
CN114822525A (en) * | 2021-01-29 | 2022-07-29 | 华为技术有限公司 | Voice control method and electronic equipment |
US20220293109A1 (en) * | 2021-03-11 | 2022-09-15 | Google Llc | Device arbitration for local execution of automatic speech recognition |
CN113362802A (en) * | 2021-05-28 | 2021-09-07 | 维沃移动通信有限公司 | Voice generation method and device and electronic equipment |
US11922938B1 (en) | 2021-11-22 | 2024-03-05 | Amazon Technologies, Inc. | Access to multiple virtual assistants |
Family Cites Families (99)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US4363102A (en) | 1981-03-27 | 1982-12-07 | Bell Telephone Laboratories, Incorporated | Speaker identification system using word recognition templates |
US5659665A (en) | 1994-12-08 | 1997-08-19 | Lucent Technologies Inc. | Method and apparatus for including speech recognition capabilities in a computer system |
JP3674990B2 (en) | 1995-08-21 | 2005-07-27 | セイコーエプソン株式会社 | Speech recognition dialogue apparatus and speech recognition dialogue processing method |
SE511418C2 (en) | 1997-03-13 | 1999-09-27 | Telia Ab | Method of speech verification / identification via modeling of typical non-typical characteristics. |
US6076055A (en) | 1997-05-27 | 2000-06-13 | Ameritech | Speaker verification method |
US5897616A (en) | 1997-06-11 | 1999-04-27 | International Business Machines Corporation | Apparatus and methods for speaker verification/identification/classification employing non-acoustic and/or acoustic models and databases |
US6141644A (en) | 1998-09-04 | 2000-10-31 | Matsushita Electric Industrial Co., Ltd. | Speaker verification and speaker identification based on eigenvoices |
US6744860B1 (en) | 1998-12-31 | 2004-06-01 | Bell Atlantic Network Services | Methods and apparatus for initiating a voice-dialing operation |
US6671672B1 (en) | 1999-03-30 | 2003-12-30 | Nuance Communications | Voice authentication system having cognitive recall mechanism for password verification |
US6408272B1 (en) | 1999-04-12 | 2002-06-18 | General Magic, Inc. | Distributed voice user interface |
US6789060B1 (en) * | 1999-11-01 | 2004-09-07 | Gene J. Wolfe | Network based speech transcription that maintains dynamic templates |
DE10015960C2 (en) | 2000-03-30 | 2003-01-16 | Micronas Munich Gmbh | Speech recognition method and speech recognition device |
US6567775B1 (en) | 2000-04-26 | 2003-05-20 | International Business Machines Corporation | Fusion of audio and video based speaker identification for multimedia information access |
US6826159B1 (en) | 2000-05-24 | 2004-11-30 | Cisco Technology, Inc. | System and method for providing speaker identification in a conference call |
EP1168736A1 (en) | 2000-06-30 | 2002-01-02 | Alcatel | Telecommunication system and method with a speech recognizer |
US7016833B2 (en) | 2000-11-21 | 2006-03-21 | The Regents Of The University Of California | Speaker verification system using acoustic data and non-acoustic data |
US6973426B1 (en) | 2000-12-29 | 2005-12-06 | Cisco Technology, Inc. | Method and apparatus for performing speaker verification based on speaker independent recognition of commands |
US6701293B2 (en) | 2001-06-13 | 2004-03-02 | Intel Corporation | Combining N-best lists from multiple speech recognizers |
JP4224250B2 (en) | 2002-04-17 | 2009-02-12 | パイオニア株式会社 | Speech recognition apparatus, speech recognition method, and speech recognition program |
US20030231746A1 (en) | 2002-06-14 | 2003-12-18 | Hunter Karla Rae | Teleconference speaker identification |
TW200409525A (en) | 2002-11-26 | 2004-06-01 | Lite On Technology Corp | Voice identification method for cellular phone and cellular phone with voiceprint password |
EP1429314A1 (en) | 2002-12-13 | 2004-06-16 | Sony International (Europe) GmbH | Correction of energy as input feature for speech processing |
US7222072B2 (en) | 2003-02-13 | 2007-05-22 | Sbc Properties, L.P. | Bio-phonetic multi-phrase speaker identity verification |
US20070198262A1 (en) | 2003-08-20 | 2007-08-23 | Mindlin Bernardo G | Topological voiceprints for speaker identification |
EP1511277A1 (en) | 2003-08-29 | 2005-03-02 | Swisscom AG | Method for answering an incoming event with a phone device, and adapted phone device |
US20050165607A1 (en) | 2004-01-22 | 2005-07-28 | At&T Corp. | System and method to disambiguate and clarify user intention in a spoken dialog system |
US7720012B1 (en) | 2004-07-09 | 2010-05-18 | Arrowhead Center, Inc. | Speaker identification in the presence of packet losses |
US8412521B2 (en) | 2004-08-20 | 2013-04-02 | Multimodal Technologies, Llc | Discriminative training of document transcription system |
KR100679043B1 (en) | 2005-02-15 | 2007-02-05 | 삼성전자주식회사 | Apparatus and method for spoken dialogue interface with task-structured frames |
US8041570B2 (en) | 2005-05-31 | 2011-10-18 | Robert Bosch Corporation | Dialogue management using scripts |
US7640160B2 (en) | 2005-08-05 | 2009-12-29 | Voicebox Technologies, Inc. | Systems and methods for responding to natural language speech utterance |
US7555172B2 (en) | 2005-10-31 | 2009-06-30 | Xerox Corporation | Dynamic range detection and adjustment |
US7603275B2 (en) | 2005-10-31 | 2009-10-13 | Hitachi, Ltd. | System, method and computer program product for verifying an identity using voiced to unvoiced classifiers |
US8099288B2 (en) | 2007-02-12 | 2012-01-17 | Microsoft Corp. | Text-dependent speaker verification |
US20110060587A1 (en) | 2007-03-07 | 2011-03-10 | Phillips Michael S | Command and control utilizing ancillary information in a mobile voice-to-speech application |
US20080252595A1 (en) | 2007-04-11 | 2008-10-16 | Marc Boillot | Method and Device for Virtual Navigation and Voice Processing |
US8503686B2 (en) | 2007-05-25 | 2013-08-06 | Aliphcom | Vibration sensor and acoustic voice activity detection system (VADS) for use with electronic systems |
US8385233B2 (en) | 2007-06-12 | 2013-02-26 | Microsoft Corporation | Active speaker identification |
GB2450886B (en) | 2007-07-10 | 2009-12-16 | Motorola Inc | Voice activity detector and a method of operation |
CN105045777A (en) * | 2007-08-01 | 2015-11-11 | 金格软件有限公司 | Automatic context sensitive language correction and enhancement using an internet corpus |
US8099289B2 (en) * | 2008-02-13 | 2012-01-17 | Sensory, Inc. | Voice interface and search for electronic devices including bluetooth headsets and remote systems |
GB2458461A (en) | 2008-03-17 | 2009-09-23 | Kai Yu | Spoken language learning system |
US8504365B2 (en) | 2008-04-11 | 2013-08-06 | At&T Intellectual Property I, L.P. | System and method for detecting synthetic speaker verification |
US8145482B2 (en) | 2008-05-25 | 2012-03-27 | Ezra Daya | Enhancing analysis of test key phrases from acoustic sources with key phrase training models |
US8589161B2 (en) * | 2008-05-27 | 2013-11-19 | Voicebox Technologies, Inc. | System and method for an integrated, multi-modal, multi-device natural language voice services environment |
US9305548B2 (en) * | 2008-05-27 | 2016-04-05 | Voicebox Technologies Corporation | System and method for an integrated, multi-modal, multi-device natural language voice services environment |
KR101056511B1 (en) | 2008-05-28 | 2011-08-11 | (주)파워보이스 | Speech Segment Detection and Continuous Speech Recognition System in Noisy Environment Using Real-Time Call Command Recognition |
US8676586B2 (en) | 2008-09-16 | 2014-03-18 | Nice Systems Ltd | Method and apparatus for interaction or discourse analytics |
US9922640B2 (en) | 2008-10-17 | 2018-03-20 | Ashwin P Rao | System and method for multimodal utterance detection |
KR101519104B1 (en) | 2008-10-30 | 2015-05-11 | 삼성전자 주식회사 | Apparatus and method for detecting target sound |
US8209174B2 (en) | 2009-04-17 | 2012-06-26 | Saudi Arabian Oil Company | Speaker verification system |
US9858925B2 (en) | 2009-06-05 | 2018-01-02 | Apple Inc. | Using context information to facilitate processing of commands in a virtual assistant |
CN101923853B (en) | 2009-06-12 | 2013-01-23 | 华为技术有限公司 | Speaker recognition method, equipment and system |
US9171541B2 (en) * | 2009-11-10 | 2015-10-27 | Voicebox Technologies Corporation | System and method for hybrid processing in a natural language voice services environment |
US8917632B2 (en) * | 2010-04-07 | 2014-12-23 | Apple Inc. | Different rate controller configurations for different cameras of a mobile device |
KR101672212B1 (en) | 2010-06-15 | 2016-11-04 | 엘지전자 주식회사 | Mobile terminal and operation method thereof |
US8565818B1 (en) * | 2010-09-16 | 2013-10-22 | Sprint Communications Company L.P. | Broadband wireless router |
US20120079095A1 (en) * | 2010-09-24 | 2012-03-29 | Amazon Technologies, Inc. | Cloud-based device synchronization |
US8719018B2 (en) | 2010-10-25 | 2014-05-06 | Lockheed Martin Corporation | Biometric speaker identification |
US8874773B2 (en) * | 2010-11-30 | 2014-10-28 | Gary W. Grube | Obtaining group and individual emergency preparedness communication information |
CN102741918B (en) | 2010-12-24 | 2014-11-19 | 华为技术有限公司 | Method and apparatus for voice activity detection |
US8660847B2 (en) | 2011-09-02 | 2014-02-25 | Microsoft Corporation | Integrated local and cloud based speech recognition |
US8340975B1 (en) * | 2011-10-04 | 2012-12-25 | Theodore Alfred Rosenberger | Interactive speech recognition device and system for hands-free building control |
WO2013078388A1 (en) | 2011-11-21 | 2013-05-30 | Robert Bosch Gmbh | Methods and systems for adapting grammars in hybrid speech recognition engines for enhancing local sr performance |
US8825020B2 (en) | 2012-01-12 | 2014-09-02 | Sensory, Incorporated | Information access and device control using mobile phones and audio in the home environment |
KR102056461B1 (en) | 2012-06-15 | 2019-12-16 | 삼성전자주식회사 | Display apparatus and method for controlling the display apparatus |
US9195383B2 (en) * | 2012-06-29 | 2015-11-24 | Spotify Ab | Systems and methods for multi-path control signals for media presentation devices |
JP6131537B2 (en) | 2012-07-04 | 2017-05-24 | セイコーエプソン株式会社 | Speech recognition system, speech recognition program, recording medium, and speech recognition method |
TWI474317B (en) | 2012-07-06 | 2015-02-21 | Realtek Semiconductor Corp | Signal processing apparatus and signal processing method |
US9053710B1 (en) * | 2012-09-10 | 2015-06-09 | Amazon Technologies, Inc. | Audio content presentation using a presentation profile in a content header |
US8983836B2 (en) | 2012-09-26 | 2015-03-17 | International Business Machines Corporation | Captioning using socially derived acoustic profiles |
US8996372B1 (en) | 2012-10-30 | 2015-03-31 | Amazon Technologies, Inc. | Using adaptation data with cloud-based speech recognition |
US9275642B2 (en) | 2012-11-13 | 2016-03-01 | Unified Computer Intelligence Corporation | Voice-operated internet-ready ubiquitous computing device and method thereof |
US9704486B2 (en) * | 2012-12-11 | 2017-07-11 | Amazon Technologies, Inc. | Speech recognition power management |
US9349386B2 (en) | 2013-03-07 | 2016-05-24 | Analog Device Global | System and method for processor wake-up based on sensor data |
US9361885B2 (en) | 2013-03-12 | 2016-06-07 | Nuance Communications, Inc. | Methods and apparatus for detecting a voice command |
US9312826B2 (en) | 2013-03-13 | 2016-04-12 | Kopin Corporation | Apparatuses and methods for acoustic channel auto-balancing during multi-channel signal extraction |
US8768687B1 (en) | 2013-04-29 | 2014-07-01 | Google Inc. | Machine translation of indirect speech |
US9305554B2 (en) * | 2013-07-17 | 2016-04-05 | Samsung Electronics Co., Ltd. | Multi-level speech recognition |
US9865255B2 (en) | 2013-08-29 | 2018-01-09 | Panasonic Intellectual Property Corporation Of America | Speech recognition method and speech recognition apparatus |
US8775191B1 (en) | 2013-11-13 | 2014-07-08 | Google Inc. | Efficient utterance-specific endpointer triggering for always-on hotwording |
US9373321B2 (en) | 2013-12-02 | 2016-06-21 | Cypress Semiconductor Corporation | Generation of wake-up words |
US9589564B2 (en) * | 2014-02-05 | 2017-03-07 | Google Inc. | Multiple speech locale-specific hotword classifiers for selection of a speech locale |
US9431021B1 (en) | 2014-03-27 | 2016-08-30 | Amazon Technologies, Inc. | Device grouping for audio based interactivity |
US9286892B2 (en) * | 2014-04-01 | 2016-03-15 | Google Inc. | Language modeling in speech recognition |
US9607613B2 (en) * | 2014-04-23 | 2017-03-28 | Google Inc. | Speech endpointing based on word comparisons |
US9257120B1 (en) * | 2014-07-18 | 2016-02-09 | Google Inc. | Speaker verification using co-location information |
US9263042B1 (en) | 2014-07-25 | 2016-02-16 | Google Inc. | Providing pre-computed hotword models |
US9418663B2 (en) | 2014-07-31 | 2016-08-16 | Google Inc. | Conversational agent with a particular spoken style of speech |
US9424841B2 (en) | 2014-10-09 | 2016-08-23 | Google Inc. | Hotword detection on multiple devices |
US9318107B1 (en) * | 2014-10-09 | 2016-04-19 | Google Inc. | Hotword detection on multiple devices |
CN105575395A (en) * | 2014-10-14 | 2016-05-11 | 中兴通讯股份有限公司 | Voice wake-up method and apparatus, terminal, and processing method thereof |
US9812126B2 (en) * | 2014-11-28 | 2017-11-07 | Microsoft Technology Licensing, Llc | Device arbitration for listening devices |
US9721566B2 (en) | 2015-03-08 | 2017-08-01 | Apple Inc. | Competing devices responding to voice triggers |
US9996316B2 (en) | 2015-09-28 | 2018-06-12 | Amazon Technologies, Inc. | Mediation of wakeword response for multiple devices |
CN105554283B (en) | 2015-12-21 | 2019-01-15 | 联想(北京)有限公司 | A kind of information processing method and electronic equipment |
US9779735B2 (en) * | 2016-02-24 | 2017-10-03 | Google Inc. | Methods and systems for detecting and processing speech signals |
US9972320B2 (en) * | 2016-08-24 | 2018-05-15 | Google Llc | Hotword detection on multiple devices |
US10559309B2 (en) * | 2016-12-22 | 2020-02-11 | Google Llc | Collaborative voice controlled devices |
-
2016
- 2016-12-22 US US15/387,884 patent/US10559309B2/en active Active
-
2017
- 2017-08-02 WO PCT/US2017/045107 patent/WO2018118136A1/en unknown
- 2017-08-02 EP EP17883852.0A patent/EP3559945B1/en active Active
- 2017-09-12 DE DE202017105526.4U patent/DE202017105526U1/en active Active
- 2017-09-12 DE DE102017121086.5A patent/DE102017121086B4/en active Active
- 2017-09-13 GB GB1714754.7A patent/GB2558342B/en active Active
- 2017-09-18 SG SG10201707702YA patent/SG10201707702YA/en unknown
- 2017-09-28 IE IE20170206A patent/IE20170206A1/en not_active IP Right Cessation
- 2017-09-30 CN CN201710918710.6A patent/CN108228699B/en active Active
- 2017-09-30 CN CN202210102465.2A patent/CN114566161A/en active Pending
-
2019
- 2019-12-17 US US16/716,654 patent/US11521618B2/en active Active
-
2022
- 2022-12-05 US US18/074,758 patent/US11893995B2/en active Active
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11521618B2 (en) | 2016-12-22 | 2022-12-06 | Google Llc | Collaborative voice controlled devices |
US11893995B2 (en) | 2016-12-22 | 2024-02-06 | Google Llc | Generating additional synthesized voice output based on prior utterance and synthesized voice output provided in response to the prior utterance |
Also Published As
Publication number | Publication date |
---|---|
US20200126563A1 (en) | 2020-04-23 |
GB201714754D0 (en) | 2017-10-25 |
EP3559945A4 (en) | 2020-01-22 |
US20180182397A1 (en) | 2018-06-28 |
SG10201707702YA (en) | 2018-07-30 |
US11893995B2 (en) | 2024-02-06 |
IE20170206A1 (en) | 2018-06-27 |
GB2558342A (en) | 2018-07-11 |
WO2018118136A1 (en) | 2018-06-28 |
DE102017121086B4 (en) | 2021-12-30 |
US11521618B2 (en) | 2022-12-06 |
US20230206923A1 (en) | 2023-06-29 |
EP3559945A1 (en) | 2019-10-30 |
US10559309B2 (en) | 2020-02-11 |
EP3559945B1 (en) | 2023-12-27 |
DE202017105526U1 (en) | 2018-04-16 |
CN114566161A (en) | 2022-05-31 |
CN108228699A (en) | 2018-06-29 |
GB2558342B (en) | 2020-06-03 |
CN108228699B (en) | 2022-02-11 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
DE102017121086B4 (en) | INTERACTIVE VOICE ACTIVATED DEVICES | |
DE202017104895U1 (en) | Hotword detection on multiple devices | |
US11721326B2 (en) | Multi-user authentication on a device | |
DE102017122182A1 (en) | Identification of a virtual assistant from nearby computing devices | |
DE202017106466U1 (en) | Real-time streaming dialog management | |
DE202015010012U1 (en) | Wake word detection on multiple devices | |
DE102016125812A1 (en) | Learn pronunciations of a personalized entity | |
DE212015000207U1 (en) | Enhanced automatic speech recognition based on user feedback | |
DE112016004863T5 (en) | Parameter collection and automatic dialog generation in dialog systems | |
DE102016125494A1 (en) | The secure execution of speech features using context-sensitive signals | |
DE102017125196A1 (en) | Proactive recording of unsolicited content in human-computer dialogues | |
DE212014000045U1 (en) | Voice trigger for a digital assistant | |
DE102017115383A1 (en) | AUDIO SLICER | |
DE102016125141A1 (en) | Search result with previous retrieval of language requests | |
DE112022000504T5 (en) | Interactive content delivery | |
CN111095397A (en) | Natural language data generation system and method | |
DE112021003164T5 (en) | Systems and methods for recognizing voice commands to create a peer-to-peer communication link | |
DE202017105719U1 (en) | Context Hotwords | |
US20230239407A1 (en) | Communication system and evaluation method | |
Spiegl et al. | FAU IISAH Corpus--A German Speech Database Consisting of Human-Machine and Human-Human Interaction Acquired by Close-Talking and Far-Distance Microphones. | |
DE102022116004A1 (en) | Techniques for generating conversation topics in a virtual environment | |
CN116566758A (en) | Audio and video conference quick response, teaching questioning response method and electronic equipment | |
CN116823174A (en) | Office demand information processing method and device |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
R012 | Request for examination validly filed | ||
R079 | Amendment of ipc main class |
Free format text: PREVIOUS MAIN CLASS: G10L0015220000Ipc: G10L0015320000 |
|
R016 | Response to examination communication | ||
R018 | Grant decision by examination section/examining division | ||
R020 | Patent grant now final |