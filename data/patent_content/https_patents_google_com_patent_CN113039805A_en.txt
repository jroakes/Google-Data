CN113039805A - Accurately automatically cropping media content by frame using multiple markers - Google Patents
Accurately automatically cropping media content by frame using multiple markers Download PDFInfo
- Publication number
- CN113039805A CN113039805A CN201980004991.1A CN201980004991A CN113039805A CN 113039805 A CN113039805 A CN 113039805A CN 201980004991 A CN201980004991 A CN 201980004991A CN 113039805 A CN113039805 A CN 113039805A
- Authority
- CN
- China
- Prior art keywords
- fingerprint
- frame
- fingerprints
- data processing
- processing system
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/70—Information retrieval; Database structures therefor; File system structures therefor of video data
- G06F16/78—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/783—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
- G06F16/7837—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content using objects detected or recognised in the video content
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/10—File systems; File servers
- G06F16/14—Details of searching files based on file metadata
- G06F16/148—File search processing
- G06F16/152—File search processing using file content signatures, e.g. hash values
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/40—Information retrieval; Database structures therefor; File system structures therefor of multimedia data, e.g. slideshows comprising image and additional audio data
- G06F16/48—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/483—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/70—Information retrieval; Database structures therefor; File system structures therefor of video data
- G06F16/78—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/783—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
- G06F16/7847—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content using low-level visual features of the video content
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/40—Scenes; Scene-specific elements in video content
- G06V20/46—Extracting features or characteristics from the video content, e.g. video fingerprints, representative shots or key frames
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/40—Scenes; Scene-specific elements in video content
- G06V20/48—Matching video sequences
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/20—Servers specifically adapted for the distribution of content, e.g. VOD servers; Operations thereof
- H04N21/23—Processing of content or additional data; Elementary server operations; Server middleware
- H04N21/231—Content storage operation, e.g. caching movies for short term storage, replicating data over plural servers, prioritizing data for deletion
- H04N21/23109—Content storage operation, e.g. caching movies for short term storage, replicating data over plural servers, prioritizing data for deletion by placing content in organized collections, e.g. EPG data repository
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/20—Servers specifically adapted for the distribution of content, e.g. VOD servers; Operations thereof
- H04N21/23—Processing of content or additional data; Elementary server operations; Server middleware
- H04N21/234—Processing of video elementary streams, e.g. splicing of video streams, manipulating MPEG-4 scene graphs
- H04N21/23418—Processing of video elementary streams, e.g. splicing of video streams, manipulating MPEG-4 scene graphs involving operations for analysing video streams, e.g. detecting features or characteristics
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/43—Processing of content or additional data, e.g. demultiplexing additional data from a digital video stream; Elementary client operations, e.g. monitoring of home network or synchronising decoder's clock; Client middleware
- H04N21/434—Disassembling of a multiplex stream, e.g. demultiplexing audio and video streams, extraction of additional data from a video stream; Remultiplexing of multiplex streams; Extraction or processing of SI; Disassembling of packetised elementary stream
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/43—Processing of content or additional data, e.g. demultiplexing additional data from a digital video stream; Elementary client operations, e.g. monitoring of home network or synchronising decoder's clock; Client middleware
- H04N21/44—Processing of video elementary streams, e.g. splicing a video clip retrieved from local storage with an incoming video stream, rendering scenes according to MPEG-4 scene graphs
- H04N21/44008—Processing of video elementary streams, e.g. splicing a video clip retrieved from local storage with an incoming video stream, rendering scenes according to MPEG-4 scene graphs involving operations for analysing video streams, e.g. detecting features or characteristics in the video stream
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/80—Generation or processing of content or additional data by content creator independently of the distribution process; Content per se
- H04N21/81—Monomedia components thereof
- H04N21/812—Monomedia components thereof involving advertisement data
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/80—Generation or processing of content or additional data by content creator independently of the distribution process; Content per se
- H04N21/83—Generation or processing of protective or descriptive data associated with content; Content structuring
- H04N21/835—Generation of protective data, e.g. certificates
- H04N21/8352—Generation of protective data, e.g. certificates involving content or source identification data, e.g. Unique Material Identifier [UMID]
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/80—Generation or processing of content or additional data by content creator independently of the distribution process; Content per se
- H04N21/83—Generation or processing of protective or descriptive data associated with content; Content structuring
- H04N21/845—Structuring of content, e.g. decomposing content into time segments
- H04N21/8456—Structuring of content, e.g. decomposing content into time segments by decomposing the content in the time domain, e.g. in time segments
Abstract
At least one aspect of the present disclosure is directed to systems and methods for extracting media segments based on fingerprint matching. The method can comprise the following steps: receiving a media stream comprising a plurality of frames; and generating a plurality of fingerprints corresponding to each frame. The method may receive a target timestamp and determine a target fingerprint from a plurality of target fingerprints that corresponds to the target timestamp. The method may retrieve candidate fingerprints, each candidate fingerprint corresponding to a frame in a candidate media stream. The method may compare the target fingerprint with the candidate fingerprints to determine matching candidate fingerprints. The method may match fingerprints corresponding to media frames before and after the target fingerprint to determine the upper and lower boundaries of the segment of interest. The method may extract a segment of interest based on the boundary and provide it to a corresponding party.
Description
Background
In a computer networking environment, such as the internet, third party content providers provide third party content items for display on end user computing devices. These third-party content items (e.g., advertisements) may be displayed on web pages associated with respective publishers. These third-party content items may include content that identifies the third-party content provider that provided the content item.
Disclosure of Invention
The present disclosure is generally directed to using a method to analyze variable length media streams to identify and extract segments of interest. Segments of interest may be identified because they are repeated more than once in the media stream, or are present in other media streams. The present disclosure describes a method of identifying and extracting segments of interest in a frame-accurate manner, while reducing processing and storage requirements, as compared to other methods. Furthermore, the methods described herein may identify long media streams or even infinitely long media streams and extract segments from long media streams or even infinitely long media streams.
The present disclosure is generally directed to a technical solution for identifying repeated segments in a media stream using a hash fingerprinting technique. The method may receive the media stream, for example, from a broadcast or from a type of computer storage. The received media stream may include a plurality of video frames. As in other solutions, instead of analyzing the original media content of each video frame individually, the present solution may generate a fingerprint for each frame that represents both the corresponding frame and frames within a period of time before or after the corresponding frame. For example, for each frame of the media stream, the method may generate a fingerprint that represents all frames within a window of three seconds after the respective frame.
To generate a fingerprint of a frame, the present solution may select a number of frames within a time window before or after the frame in question. The number of frames may be predetermined or determined based on other factors. The frames may be selected based on a predetermined or calculated interval within the time window. Each selected frame may be hashed using a hashing algorithm suitable for media hashing and comparison (e.g., a locality sensitive hashing algorithm). After generation, each hash may be truncated to a certain bit width to reduce the size footprint of the final fingerprint. The fingerprint may be generated by concatenating each truncated hash to a single value or data structure. The fingerprint generated for each frame in the present solution is unique for the frame in the time window relative to the respective frame of the media stream. For example, if the window time period is determined to be three seconds after the selected frame, the method may perform the steps outlined above to generate a fingerprint for each frame representing the frame in question and three seconds of the media in which the frame is made. By generating the media fingerprint in this way, the present solution provides a significant computational improvement for the comparison of two media streams, while reducing the required storage of media comparison data.
The present solution may aggregate frames into multiple fingerprints representing the entire media stream. The plurality of fingerprints may be stored in a database or other computer storage medium for analysis. The present solution may associate each of a plurality of fingerprints with a respective frame of the media stream, e.g. by sequence or by time stamping. The method may receive a target timestamp corresponding to a frame in the media stream, which may correspond to a frame within a segment of interest of the media stream. The present solution may determine a target fingerprint among a plurality of fingerprints representing media streams corresponding to the target timestamps. For example, the present solution may determine a fingerprint corresponding to a time period that includes the target timestamp. The present solution may select the target fingerprint such that the target timestamp represents the exact midpoint of the time period represented by the selected target fingerprint.
To determine whether a segment of interest is repeated in other media streams, the present solution may retrieve a candidate plurality of fingerprints, each fingerprint of the candidate plurality of fingerprints corresponding to a frame in the candidate media stream. In some implementations, the candidate media stream may be the same as the first media stream. The present solution may compare the target fingerprint with each of the second plurality of fingerprints to determine a matching location in the second media stream. If a matching fingerprint is found, the method may then find other matches before and after the matching location of the target fingerprint. For example, the method may continue to match each fingerprint of the two media streams corresponding to a timestamp that is less than the matching location of the target fingerprint. The present solution may continue to match fingerprints until the lower boundary of the timestamp of the segment of interest may be determined. Likewise, to determine the upper boundary of the segment of interest, the present solution may continue to match each fingerprint of the two media streams corresponding to a timestamp greater than the matching location of the target footnote (footprint) until a non-match occurs.
The present solution may use the upper and lower boundaries to identify segments of interest in either of the two media streams. The present solution may extract segments of interest from the media stream. Because a fingerprint is generated for each frame in both media streams, the extracted segment of interest is frame accurate in both media streams. The extracted segments of interest may then be stored in a database or provided to a party of interest.
At least one aspect of the present disclosure is directed to a method of extracting matching media segments based on frame fingerprints. The method may include receiving, by a data processing system having one or more processors, a media stream comprising a plurality of frames. The method can comprise the following steps: for each of the plurality of frames, selecting, by the data processing system, a second plurality of frames from the plurality of frames based on the fingerprint window value. The method may include, for each frame of the plurality of frames, hashing, by the data processing system, each frame of the second plurality of frames to create a plurality of hashes. The method may include, for each frame of the plurality of frames, truncating, by the data processing system, each hash of the plurality of hashes to generate a plurality of truncated hashes. The method may include, for each of a plurality of frames, concatenating, by the data processing system, the plurality of truncated hashes to calculate a frame fingerprint value. A frame fingerprint value may be associated with each frame of the plurality of frames and a timestamp corresponding to each frame. The method may include aggregating, by the data processing system, each calculated frame fingerprint value to assemble a plurality of fingerprints. The method may include receiving, by a data processing system, a target timestamp corresponding to a target frame of a plurality of frames. The method may include determining, by the data processing system, a target fingerprint of the plurality of fingerprints based on the target timestamp and the fingerprint window value. The method may include retrieving, by the data processing system, a second plurality of fingerprints, each of the second plurality of fingerprints associated with a frame in the second media stream. The method may include comparing, by the data processing system, the target fingerprint to each of the second plurality of fingerprints to determine a matching location corresponding to a location in the second media stream. The method can comprise the following steps: matching, by the data processing system, a first fingerprint of the plurality of fingerprints associated with a timestamp less than the target timestamp with a second fingerprint of a second plurality of fingerprints to determine a lower boundary timestamp. The method can comprise the following steps: matching, by the data processing system, a second fingerprint of the plurality of fingerprints associated with a timestamp greater than the target timestamp with a third fingerprint of the second plurality of fingerprints to determine an upper bound timestamp. The method may include extracting, by the data processing system, the media segment from the second media stream based on the lower bound timestamp and the upper bound timestamp. The method may include providing, by a data processing system, a media segment for storage in a database.
In some implementations, the method may include storing, by the data processing system, the plurality of fingerprints in a database. In some implementations, the method may include retrieving a second plurality of fingerprints from a database.
In some implementations, the method may include receiving, by the data processing system, a third media stream including a third plurality of frames. In some implementations, the method may include: for each of the third plurality of frames, a fourth plurality of frames is selected by the data processing system from the third plurality of frames based on the fingerprint window value. In some implementations, the method may include: for each of the third plurality of frames, hashing, by the data processing system, each of the fourth plurality of frames to create a second plurality of hashes. In some implementations, the method may include: for each of the third plurality of frames, truncating, by the data processing system, each of the second plurality of hashes to generate a second plurality of truncated hashes. In some implementations, the method may include: for each of the third plurality of frames, concatenating, by the data processing system, the second plurality of truncated hashes to calculate a second frame fingerprint value. In some implementations, the method may include: aggregating, by the data processing system, each second frame fingerprint value associated with a timestamp corresponding to a respective frame of the third plurality of frames to assemble the second plurality of fingerprints.
In some implementations, the method may include hashing each of the second plurality of frames using a locality-sensitive hashing algorithm. In some implementations, the second media stream is the same as the first media stream. In some implementations, the method may include calculating, by the data processing system, a similarity value between the target fingerprint and each fingerprint of the second plurality of fingerprints.
In some implementations, the method can include determining, by the data processing system, whether the similarity value is equal to or greater than a similarity threshold. In some implementations, the method may include: responsive to the similarity value being equal to or greater than the similarity threshold, a matching location corresponding to a location in the second media stream is determined by the data processing system.
In some implementations, the method can include extracting, by the data processing system, a fifth plurality of fingerprints from the second plurality of fingerprints based on the lower bound timestamps and the upper bound timestamps. In some implementations, the method may include storing, by the data processing system, the fifth plurality of fingerprints in a database. In some implementations, the method may include providing, by the data processing system, a fifth plurality of fingerprints.
At least one other aspect of the present disclosure is directed to a system for extracting matching media segments based on frame fingerprints, the system comprising a data processing system having one or more processors. A data processing system may receive a media stream comprising a plurality of frames. The data processing system may select, for each of the plurality of frames, a second plurality of frames from the plurality of frames based on the fingerprint window value. For each frame of the plurality of frames, the data processing system may hash each frame of the second plurality of frames to create a plurality of hashes. For each frame of the plurality of frames, the data processing system may truncate each hash of the plurality of hashes to generate a plurality of truncated hashes. For each of the plurality of frames, the data processing system may concatenate the plurality of truncated hashes to calculate a frame fingerprint value. The data processing system may aggregate each calculated frame fingerprint value associated with a timestamp corresponding to a respective frame of the plurality of frames to assemble a plurality of fingerprints. The data processing system may associate each calculated frame fingerprint value with a respective frame of the plurality of frames. The data processing system may receive a target timestamp corresponding to a target frame of the plurality of frames. The data processing system may determine a target fingerprint of the plurality of fingerprints based on the target timestamp and the fingerprint window value. The data processing system may retrieve a second plurality of fingerprints, each of the second plurality of fingerprints associated with a frame in the second media stream. The data processing system may compare the target fingerprint to each of the second plurality of fingerprints to determine a matching location corresponding to a location in the second media stream. The data processing system may match a first fingerprint of the plurality of fingerprints associated with a timestamp less than the target timestamp with a second fingerprint of a second plurality of fingerprints to determine a lower boundary timestamp. The data processing system may match a third fingerprint of the plurality of fingerprints associated with a timestamp greater than the target timestamp with a fourth fingerprint of the second plurality of fingerprints to determine an upper bound timestamp. The data processing system may provide the media segments for storage in a database.
In some implementations, the data processing system is further configured to store the plurality of fingerprints in a database. In some implementations, the data processing system is further configured to retrieve a second plurality of fingerprints from the database.
In some implementations, the data processing system is further configured to receive a third media stream including a third plurality of frames. In some implementations, the data processing system is further configured to select, for each of the third plurality of frames, a fourth plurality of frames from the third plurality of frames based on the fingerprint window value. In some implementations, the data processing system is further configured to hash, for each of the third plurality of frames, each of the fourth plurality of frames to create a second plurality of hashes. In some implementations, the data processing system is further configured to truncate, for each of the third plurality of frames, each hash of the second plurality of hashes to generate a second plurality of truncated hashes. In some implementations, the data processing system is further configured to concatenate, for each frame in the third plurality of frames, the second plurality of truncated hashes to calculate a second frame fingerprint value. In some implementations, the data processing system is further configured to aggregate each second frame fingerprint value associated with a timestamp corresponding to a respective frame of the third plurality of frames to assemble the second plurality of fingerprints.
In some implementations, the data processing system is further configured to hash each of the second plurality of frames using a locality sensitive hashing algorithm. In some implementations, the first media stream is the same as the second media stream. In some implementations, the data processing system is further configured to calculate a similarity value between the target fingerprint and each fingerprint of the second plurality of fingerprints.
In some implementations, the data processing system is further configured to determine whether the similarity value is equal to or greater than a similarity threshold. In some implementations, the data processing system is further configured to determine a matching location corresponding to a location in the second media stream in response to the similarity value being equal to or greater than the similarity threshold.
In some implementations, the data processing system is further configured to extract a fifth plurality of fingerprints from the second plurality of fingerprints based on the lower bound timestamps and the upper bound timestamps. In some implementations, the data processing system is further configured to store the fifth plurality of fingerprints in a database. In some implementations, the data processing system is further configured to provide a fifth plurality of fingerprints.
These and other aspects and implementations are discussed in detail below. The foregoing information and the following detailed description include illustrative examples of various aspects and implementations, and provide an overview or framework for understanding the nature and character of the claimed aspects and implementations. The accompanying drawings are included to provide an illustration and a further understanding of the various aspects and implementations, and are incorporated in and constitute a part of this specification. Aspects may be combined, and it will be readily understood that features described in the context of one aspect of the invention may be combined with other aspects. Aspects may be implemented in any convenient form. For example, by means of a suitable computer program, which may be carried on a suitable carrier medium (computer readable medium), which may be a tangible carrier medium (e.g. a diskette) or an intangible carrier medium (e.g. a communications signal). Aspects may also be implemented using suitable apparatus, which may take the form of a programmable computer running a computer program arranged to implement the aspects.
Drawings
The accompanying drawings are not intended to be drawn to scale. Like reference numbers and designations in the various drawings indicate like elements. For purposes of clarity, not every component may be labeled in every drawing. In the drawings:
FIG. 1 shows a block diagram depicting an example environment for frame accurate extraction of media segments based on fingerprint matching.
FIG. 2 illustrates a flow diagram of an example method for frame accurate extraction of media segments based on fingerprint matching.
FIG. 3 illustrates a flow chart of an example method of generating fingerprint values for frames in a media stream.
FIG. 4 illustrates a flow chart of an example method for determining a lower boundary of a matching media segment using two sets of media fingerprints.
FIG. 5 illustrates a flow chart of an example method for determining an upper boundary of a matching media segment using two sets of media fingerprints.
Fig. 6 shows a diagram illustrating an example generation based on fingerprint values for frames in a media stream.
FIG. 7 shows a diagram illustrating an example of using two sets of media fingerprints to determine an upper boundary and a lower boundary of a matching media segment.
FIG. 8 shows the general architecture of an illustrative computer system that may be used to implement any of the computers discussed herein.
Detailed description of the invention
Following are various concepts related to methods, apparatus, and systems for privacy preserving determination of intersections of sets of user identifiers and various detailed descriptions of implementations thereof. The various concepts introduced above and discussed in greater detail below may be implemented in any of numerous ways, as the described concepts are not limited to any particular implementation.
FIG. 1 depicts a system or environment 100 for extracting media segments based on frame-accurate fingerprint comparisons. The environment 100 may include at least one computer network 110. The network 110 may be a computer network, which may include one or more local area networks, wide area networks, private networks, public networks, and the Internet. The environment 100 may include at least one media source 150. The environment 100 may include at least one data processing system 105. The environment 100 may include at least one database 115. The data processing system 105 may include at least one frame selector 120. The data processing system 105 may include at least one frame hasher 125. The data processing system 105 may include at least one fingerprint calculator 130. The data processing system 105 may include at least one target frame matcher 135. The data processing system 105 may include at least one boundary calculator 140. The data processing system 105 may include at least one segment extractor 145. In some implementations, the data processing system 105 may include a media source 150. In some implementations, the data processing system 105 may include a database 115.
Each of the components of system 100 (e.g., network 110, database 115, frame selector 120, frame hasher 125, fingerprint calculator 130, target frame matcher 135, boundary calculator 140, and segment extractor 145 and media source 150 of data processing system 105) may be implemented using the components of computing system 800 described in detail herein in connection with fig. 8. For example, the data processing system 105 may include a server or other computing device. Media source 150 may also include a server or other computing device. Each component of the data processing system 105 may perform the functions detailed herein.
The media source 150 may acquire, receive, or identify a media stream (sometimes referred to herein generally as a stream). Each media stream may include at least one media frame (sometimes referred to herein generally as a frame), which may be a frame of a video stream, such as MPEG-4h.264 video. The frame may include video information, audio information, and closed caption information. Video information may be encoded using a video codec (e.g., H.263, H.264, HEVC, MPEG4, Theora, 3GP, Windows Media 8, Quicktime, MPEG-4, VP8, VP6, MPEG1, MPEG2, MPEG-TS, or the like). The audio information may be encoded using an audio codec (e.g., MP3, AAC, HE-AAC, AC3, EAC3, Vorbis, WMA, or PCM, etc.). The closed caption information may be encoded using a closed caption format (e.g., WEBVTT, CEA-608, CEA-708, DFXP, SAMI, SCC, SRT, TTML, or 3GPP, etc.). Each frame may include a timestamp corresponding to its position in the media stream. The media stream may have a fixed number of frames per unit time, for example 24 frames per second. The media stream may be an adaptive bitrate stream. The media stream may be a live video stream, such as an internet video stream, or a live television stream. The live television stream may be received from a cable endpoint serviced by a cable provider, a satellite antenna serviced by a satellite service provider, or a television antenna. In some implementations, the media source 150 may receive media streams from one or more media sources (e.g., the internet, a cable connection, a satellite connection, a radio broadcast, and/or a television broadcast, etc.). In some implementations, media source 160 may retrieve one or more media streams from database 115.
Each frame of the media stream acquired by the media source 150 may also include an index value (e.g., a time or sequence number). The index value may correspond to an order of frames included in the media stream. In some implementations, the index value may correspond to a time at which the frame was received by the media source 150. The media source 150 may generate and associate a time index with each frame in each media stream. The frames may be stored, for example, in a structured template, in a memory area of the computer system filled with certain values, or in a generic data structure (e.g., a class or class C structure). In some implementations, in addition to receiving data in the media stream, the media source 150 may decode information in the media stream. In some implementations, the media source 150 may generate at least one media stream comprising one or more frames. For example, the media source 150 may be a computing system coupled with a camera sensor configured to record video and audio. In this example, the media source 150 may generate a media stream that includes frames with video and audio information, each frame associated with a timestamp.
The media source 150 may transmit frames of the media stream to the data processing system 105 via the network 110 for processing. In some implementations, the media source 150 may transmit frames of the media stream in a continuous stream. In some implementations, the media source 150 may provide frames of the media stream at regular intervals (e.g., one frame per millisecond). In some implementations, the media source 150 may transmit frames at variable time interval schedules. In some implementations, the media source 150 may provide the media stream to the data processing system in bursts. A burst may include a number of frames, each frame corresponding to a timestamp and/or index value. In some implementations, the media source 150 may send data directly to the data processing system 105 via a communication interface. In some implementations, the media source 150 may provide the frames based on a time-independent schedule.
Frame selector 120 may receive frames from media source 150 via network 110. As the frames are received, the frame selector may store and maintain the frames on a computer-readable storage device (e.g., database 115). Frame selector 120 may store and maintain the frames in a temporary buffer. The temporary buffer may be created on a computer storage device, such as database 115 or in conjunction with memory 825 of FIG. 8. Frame selector 120 may also access database 115 via a network to retrieve one or more frames for processing. Frame selector 120 may store and select frames based on the fingerprint window values. In some implementations, frame selector 120 may store enough frames to meet the selection requirements of the fingerprint window values. The fingerprint window value may be predetermined, may be received from another computing device over network 110, or may be received from a component of data processing system 105. The fingerprint window value may be used to determine which frames in the media stream to select to generate a fingerprint for a corresponding frame in the media stream. For example, the fingerprint window value may be a time value that indicates that frames should be selected within a particular time period relative to the respective frame. The fingerprint window value may be a counter value indicating that a certain number of frames should be selected from the media stream before or after the corresponding frame. The fingerprint window value may be an interval value indicating that a frame should be selected if it corresponds to a timestamp that satisfies an interval condition (e.g., once every 25 milliseconds). Frame selector 120 may select one or more frames from a media stream received from media source 150 via network 110.
For each frame received by media source 150, frame selector 120 may select a set of frames from the media stream that represent the time period indicated by the fingerprint window value. For example, if the fingerprint window value is three seconds, frame selector 120 may select a predetermined number of frames within three for each frame received from media source 150. Frame selector 150 may store the selected frames in a data structure in volatile or non-volatile computer memory, such as database 115 or in conjunction with memory 825 of fig. 8. In some implementations, frame selector 120 may select frames from media source 150. The selected frames of each frame of the media stream may be stored in a separate data structure in computer memory by frame selector 120. In some implementations, the selected frames of each frame of the media stream may be stored in a single data structure in computer memory by frame selector 120. The stored selected frames are associated with corresponding frames of a media stream received from media source 150, e.g., encoded using a timestamp, a frame index value, or somewhere in a data structure or computer memory.
Frame hasher 125 may hash one or more frames received by media source 150 or retrieved from a computer memory (e.g., database 115). The hash frame 125 may hash one or more frames selected by the frame selector 120. Frame selector 120 may provide one or more selected frames to frame hasher 125. The frame hasher 125 may hash the frame using one or more hashing algorithms, such as a locality sensitive hashing algorithm. The frame hasher 125 can select a locality sensitive hashing algorithm to allow the data processing system 105 to compare two hashed frames. For example, a locality sensitive hashing algorithm may be selected to allow the data processing system 105 to compute a similarity value between two frames. The frame hasher 125 may use bit sampling as a locality sensitive hashing algorithm. The frame hasher 125 may use a least-direction independent permutation locality sensitive hashing algorithm. The frame hasher 125 may use other types of locality sensitive hashing algorithms, such as a random projection method. Frame hasher 125 may hash frames using a conventional hashing algorithm (e.g., SHA-1, SHA-2, MD5, etc.).
For each frame selected by frame selector 120, the frame hasher may use a hashing algorithm to create a hashed frame. The hash frame created may have a fixed bit width (e.g., 8 bits, 16 bits, 32 bits, 64 bits, etc.). In some implementations, the hash frame created may have a variable bit width. In some implementations, the frame hasher 125 may decode or pre-process the frame before applying the hashing algorithm to create the corresponding hashed frame. For example, frame hasher 125 may decode a frame using a corresponding codec to access the original frame information before applying a hashing algorithm to the original frame information to create a hashed frame. The frame hasher 125 may decode and hash the audio information, video information, and/or closed caption information included in the frame, and/or hash the audio information, video information, and/or closed caption information included in the frame as different hashes. The frame hasher 125 may create the hashes and store them in a computer memory, such as in the database 115 or memory 825 in conjunction with fig. 8. Frame hasher 125 may store the hashed frames in a data structure, such as a C-like data structure or an area of computer memory. Frame hasher 125 may store the hashed frames in a buffer, which may be a temporary area of computer memory used to store data. Frame hasher 125 may associate each hashed frame with a respective timestamp, index value, or positioning data to indicate the location of the hashed frame in the respective media stream. The frame hasher 125 may provide the hashed frames to the fingerprint calculator 130.
The fingerprint calculator 130 may calculate fingerprints of one or more frames included in the media stream. For example, the fingerprint calculator 130 may process one or more hashes received from the frame hasher 125 to calculate fingerprints corresponding to time periods in the media stream to which the one or more frames belong. The fingerprint calculator 130 may calculate a fingerprint for each frame in the media stream received from the media source 150. In some implementations, the fingerprint calculator 130 may calculate the fingerprint based on one or more framed hashes retrieved from a computer memory, e.g., in conjunction with the database 115 or memory 825 in fig. 8. The fingerprint calculator 130 may store each calculated fingerprint in a computer memory, for example, in a data structure. The fingerprint calculator 130 may store the calculated fingerprint or a data structure including one or more calculated fingerprints in a computer memory, such as the database 115 or memory 825 in conjunction with fig. 8.
The fingerprint calculator 130 may truncate each hash frame created by the frame hasher 125 corresponding to the frame selected by the frame selector 120 to generate a truncated hash frame. Truncating the hash frame may include selecting a fixed range of bits, such as the 8 least significant bits, in the hash frame. Truncating the hash frame may include storing the truncated bits in a data structure in a computer memory, such as database 115 or memory 825 in conjunction with fig. 8. For example, if a hash frame has the binary value 01100100, the fingerprint calculator may truncate the hash frame by selecting the four most significant bits (0110 in this example). Truncating the hash frame may reduce the overall size of the hash frame while still retaining sufficient information for further processing, which is an improvement over other implementations. In some implementations, the fingerprint calculator 130 may skip performing the truncation step and instead use the entire hashed frame for further processing. In some implementations, truncating the hash frame may include selecting any combination of bits in the hash frame to generate the truncated hash. For example, if the hash frame has a binary value of 00110001, the fingerprint calculator 130 may select each of the bits in positions 6, 4, 2, and 0 to generate a truncated hash frame having a value of 0101. The fingerprint calculator 130 may further process the truncated hash frame to calculate a frame fingerprint value.
The fingerprint calculator 130 may concatenate the truncated hash frames corresponding to each frame selected by the frame selector 120 to calculate a frame fingerprint value corresponding to a time period in the media stream. In some implementations, the fingerprint calculator 130 may access computer memory to retrieve one or more truncated hash frames to calculate the fingerprint value. Each frame selected by frame selector 120 may correspond to a time period in the media stream. Hashing, truncating, and concatenating each selected frame may reduce the size of the data in each frame without sacrificing the information needed to compute the similarity value between each frame. In this way, frames may be compared in a manner that is much more computationally efficient than other implementations. Moreover, the transmission of concatenated, truncated, and hashed frames may use significantly less network bandwidth than sending the entire media stream for comparison, which is an improvement over other implementations. The fingerprint calculator 130 may calculate a fingerprint for each frame in the media stream received from the media source 150. The fingerprint may correspond to a time window, such as three seconds. In this example, each fingerprint may correspond to a time window (e.g., a sliding window) after each frame in the media stream that is equal to approximately three seconds.
Although three seconds of media information may occupy a significant portion of the computer memory, selecting only a few frames from a window, applying a hashing algorithm to each selected frame, and truncating each hash corresponding to the window may greatly reduce the storage requirements for the media information. If a locality sensitive hashing algorithm is used, the media information may be compared to other fingerprints to calculate a similarity value that indicates the similarity of the media information included in the two sets of frames. The fingerprint calculator 130 may calculate a final fingerprint corresponding to the window of media information by concatenating each truncated hash frame together into a single value. For example, if three frames are selected from a time window in the media stream and their truncated hash values are equal to 0110, 0101, and 1000, the fingerprint calculator 130 may concatenate each of these values to generate the fingerprint value 011001011000. Although in this example, the frames are shown as being concatenated in a particular order, it should be understood that each truncated hash frame may be concatenated in any combination or permutation. The fingerprint calculator 130 may associate each fingerprint value with a timestamp or index value to indicate a location in the media stream to which the fingerprint corresponds.
The fingerprint calculator 130 may store each calculated fingerprint value in a computer memory, such as the database 115 or memory 825 in conjunction with fig. 8. The fingerprint value may be stored in a data structure in the computer memory. The data structure may include a timestamp corresponding to the earliest frame in the fingerprint and a value specifying a time window to which the fingerprint corresponds. The data structure may also include time information indicating the location of the window in the corresponding media stream. The fingerprint calculator 130 may aggregate each calculated fingerprint value together in a single data structure. For example, the fingerprint calculator 139 may continuously calculate fingerprints from a live media stream received from the media source 150. In some implementations, the fingerprint calculator 130 may continuously update the data structure including fingerprints corresponding to the media stream as each fingerprint is calculated. The data structures may be continuously accessed and updated in computer memory, such as in database 115 or in storage 825 in conjunction with FIG. 8.
The target frame matcher 135 may determine a target fingerprint corresponding to the target timestamp and the indicated target media stream received by the target frame matcher 135. The target frame matcher 135 may access computer memory to retrieve one or more fingerprints corresponding to the media stream to determine a target fingerprint. For example, target frame matcher 135 may access database 115, which database 115 may contain a data structure that includes fingerprints indexed by timestamps. In this example, the target frame matcher may determine the target fingerprint by accessing the database entry at an index value equal to the target timestamp received by the target frame matcher 135. In some implementations, fingerprints may be indexed by different values. The target frame matcher may search a data structure containing fingerprints corresponding to the target media stream to determine which fingerprint has a corresponding timestamp or index value that is closest to the target timestamp or index value. In some implementations, the target timestamp or index value can correspond to a fingerprint covering a time period that includes the target timestamp. In some implementations, target frame matcher 135 may determine the target fingerprint as the fingerprint whose representative time period (e.g., window) is most closely bisected by the target timestamp. For example, if the target timestamp indicates a frame that is exactly ten seconds from the first frame of the media stream, target frame matcher 135 may determine the target fingerprint as a fingerprint that represents a time period of 8.5 seconds to 11.5 seconds of the media stream (e.g., the window value of the fingerprint is 3 seconds). In this example, target frame matcher 135 determines the target fingerprint as one whose window is bisected by the frame corresponding to the target timestamp, since 10 seconds is exactly between 8.5 seconds and 11.5 seconds.
The target frame matcher 135 may access computer memory to retrieve a candidate set of fingerprints. In some implementations, the candidate set of fingerprints correspond to candidate media streams. In some other implementations, the candidate set of fingerprints may correspond to a target media stream indicated by a target timestamp. In this way, the data processing system 105 can calculate matching fingerprint values from different media streams and the same media stream and take advantage of the observation that the segment of interest can be repeated on one or more media streams. In some implementations, the set of fingerprint candidates may be generated by the fingerprint calculator 130 based on candidate media streams received by the media source 150. Target frame matcher 135 may retrieve the set of fingerprint candidates from a database (e.g., database 115). In some embodiments, target frame matcher 135 may retrieve the candidate set of fingerprints from computer memory (e.g., in conjunction with memory 825 of fig. 8). Each fingerprint in the candidate set of fingerprints may correspond to a frame and a time window in a media stream (e.g., a media stream received by the media source 150). The fingerprints in the set of fingerprint candidates may be indexed by a timestamp value, an index value corresponding to a position of the associated frame in the respective media stream, or other values. The target frame matcher 135 may store the retrieved fingerprint candidate set in computer memory for further processing, for example in memory 825 or database 115 in connection with fig. 8.
The target frame matcher 135 may compare the target fingerprint determined based on the target timestamp with each of the candidate set of fingerprints to determine a matching location in the candidate media stream. The matching location may indicate that a frame in the time window indicated by the target fingerprint has been repeated in the candidate media stream. In some implementations, the candidate media stream may be the same as the target media stream indicated by the target fingerprint, but at a different time period. In this way, the data processing system 105 can identify matching fingerprints that have been repeated in the same media stream or in one or more different media streams. When compared to a target fingerprint, the target frame matcher 135 may determine the location by calculating a similarity value (e.g., a hamming distance, other locally-based hash distance values, etc.) for each of the fingerprint candidate sets. If the target frame matcher 135 determines that the similarity value between the target fingerprint and one of the frames in the fingerprint candidate set is greater than or equal to a predetermined similarity threshold, the target frame matcher 135 may access the values of the timestamps corresponding to the respective fingerprints in the fingerprint candidate set. If the target frame matcher 135 determines that the similarity value is less than the predetermined similarity threshold, the target frame matcher 135 may proceed to calculate similarity values for other fingerprints associated with the candidate media streams. In some implementations, if target frame matcher 135 determines that there are no matching fingerprints in the fingerprint candidate set, target frame matcher 135 may retrieve another fingerprint candidate set altogether from database 115 or other computer memory (e.g., in conjunction with memory 825 of fig. 8).
The value of the matching timestamp may be an index value corresponding to an index of a frame in the candidate media stream corresponding to the candidate set of fingerprints. The index value may be a value indicating the location of the fingerprint in the candidate media stream. For example, an index value of zero may indicate that the fingerprint corresponds to the first frame in the candidate media stream, an index value of 1 may indicate that the fingerprint corresponds to the second frame in the media stream, and so on. The value of the matching timestamp may also indicate the exact time that the frame occurred in the candidate media stream. The value of the matching timestamp may be a relative time value (e.g., the relative timestamp of the first frame is zero), or the value of the timestamp may be an absolute time (e.g., the time of day maintained on the data processing system 105, the time of day value maintained on the media source 150, or the absolute time of day that the corresponding frame appears in the broadcast, etc.), and the matching fingerprint window appears in the candidate media stream.
The boundary calculator 140 may enumerate candidate fingerprints corresponding to frames in the candidate media streams before and after the candidate timestamp for further comparison with the target media stream. The boundary calculator 140 may also enumerate fingerprints corresponding to frames before and/or after the target timestamp in the target media stream. The target timestamp received by the target frame matcher 135 may correspond to a frame in the segment of interest. The boundary calculator 140 may calculate the upper and lower boundaries of the segment of interest by comparing the fingerprints in the target media stream and the candidate media streams.
The boundary calculator 140 may match at least one of the fingerprints of the target media stream with at least one of the fingerprints of the candidate media streams to determine a lower boundary timestamp for the segment of interest. Boundary calculator 140 may access candidate fingerprints for frames in the candidate media stream immediately preceding the matching frame determined by target frame matcher 135. The boundary calculator 140 may access the candidate fingerprints from a computer memory (e.g., the database 115 or memory 824 in conjunction with fig. 8). The boundary calculator 140 may access fingerprints in frames in the target media stream immediately preceding the target frame determined by the target frame matcher 135. The boundary calculator 140 may access the fingerprint by accessing a computer memory (e.g., the database 115 or memory 825 in conjunction with fig. 8). Upon accessing the candidate fingerprint and the fingerprint of the target media stream, the boundary calculator 140 may compare the two fingerprints to determine a lower boundary timestamp. In some implementations, comparing two fingerprints includes computing a fingerprint similarity value (e.g., a hamming distance, other locality-based hash distance values, etc.). If the fingerprint similarity value is greater than or equal to a predetermined threshold, the boundary calculator 140 may determine that the two fingerprints are sufficiently similar. If the similarity value is less than the predetermined threshold, the boundary calculator 140 may determine that the two fingerprints are not sufficiently similar. In some implementations, the boundary calculator 140 may determine that two fingerprints are not similar enough if they do not completely match. If the boundary calculator 140 determines that the two fingerprints are sufficiently similar, the boundary calculator may access the fingerprint in each of the candidate media stream and the target media stream immediately preceding the compared fingerprint and perform the comparison, as detailed herein. In this way, the boundary calculator 140 may compare each frame of the two media streams starting from the matching location and ending at the beginning of the streams until a mismatch occurs or the beginning of at least one stream has been reached. A mismatch occurs when the two fingerprints of the two media streams are not sufficiently similar, or when the start of a candidate media stream or a target media stream is reached.
If the boundary calculator 140 determines that a mismatch has occurred, the boundary calculator 140 may retrieve timestamps corresponding to candidate fingerprints used in the comparison of previous matches. The boundary calculator 140 may retrieve the timestamp from a data structure that includes a candidate fingerprint determined to be sufficiently similar to the target media stream from a computer memory (e.g., the database 115 or memory 825 in conjunction with fig. 8). Because the boundary calculator 140 has determined that a mismatch has occurred between the two streams, the segment of interest (e.g., a segment that repeats across two or more media streams) may begin at the frame indicated by the timestamp contained in the last matched fingerprint. By accessing the timestamp of the last matching fingerprint, the boundary calculator can determine the lower boundary of the segment of interest in the candidate media stream. In some implementations, the boundary calculator 140 can calculate the lower boundary by accessing a timestamp associated with the last matching fingerprint in the target media stream.
The boundary calculator 140 may match at least one fingerprint of the target media stream with at least one fingerprint of the candidate media stream to determine an upper boundary timestamp of the segment of interest in the at least one media stream. Boundary calculator 140 may access a candidate fingerprint for a frame in the candidate media stream that immediately follows the matching frame determined by target frame matcher 135. The boundary calculator 140 may access the candidate fingerprints from a computer memory (e.g., the database 115 or memory 824 in conjunction with fig. 8). The boundary calculator 140 may access fingerprints of frames in the target media stream immediately following the target frame determined by the target frame matcher 135. The boundary calculator 140 may access the fingerprint by accessing a computer memory (e.g., the database 115 or memory 825 in conjunction with fig. 8). Upon accessing the candidate fingerprint and the fingerprint of the target media stream, the boundary calculator 140 may compare the two fingerprints to determine an upper boundary timestamp. In some implementations, comparing two fingerprints includes computing a fingerprint similarity value (e.g., a hamming distance, other locality-based hash distance values, etc.). If the fingerprint similarity value is greater than or equal to a predetermined threshold, the boundary calculator 140 may determine that the two fingerprints are sufficiently similar. If the similarity value is less than the predetermined threshold, the boundary calculator 140 may determine that the two fingerprints are not sufficiently similar. In some implementations, the boundary calculator 140 may determine that two fingerprints are not similar enough if they do not completely match. If the boundary calculator determines that the two fingerprints are sufficiently similar, the boundary calculator may access the fingerprints immediately following the compared fingerprint in each of the candidate media stream and the target media stream and perform the comparison as detailed herein. In this manner, the boundary calculator 140 may compare each of the two media streams starting from the matching location and ending at the last frame of the streams until a mismatch occurs or the last frame of at least one stream has been reached. A mismatch occurs when the two fingerprints of the two media streams are not sufficiently similar, or when the last frame of the candidate media stream or the target media stream is reached.
If the boundary calculator 140 determines that a mismatch has occurred, the boundary calculator 140 may retrieve timestamps corresponding to candidate fingerprints used in the comparison of previous matches. The boundary calculator 140 may retrieve the timestamp from a data structure that includes candidate fingerprints that are determined to be sufficiently similar to the target media stream from a computer memory (e.g., the database 115 or memory 825 in conjunction with fig. 8). Because the boundary calculator 140 has determined that a mismatch has occurred between the two streams, the segment of interest (e.g., a segment that repeats across two or more media streams) may end at the frame indicated by the timestamp included in the last matched fingerprint. By accessing the timestamp of the last matching fingerprint, the boundary calculator can determine the upper boundary of the segment of interest in the candidate media stream. In some implementations, the boundary calculator 140 can calculate the upper boundary by accessing the timestamp associated with the last matching fingerprint in the target media stream.
The segment extractor 145 may extract the segment of interest based on the lower and upper boundary timestamps. The upper and lower boundary timestamps may correspond to the candidate media stream or the target media stream. In some implementations, frames of the candidate media stream and/or the target media stream are stored in a computer memory, such as database 115 or memory 825 in conjunction with fig. 8. In some implementations, the segment extractor 145 may access all frames having timestamp values that include and are between the lower and upper boundary timestamp values determined by the boundary calculator 140. The segment extractor 145 may retrieve the accessed frames to assemble frame-accurate segments of interest that have been extracted based on the repeated frames in the one or more media streams. In some implementations, the segment extractor 145 may access all fingerprint values using timestamp values that include and are between the lower and upper boundary timestamp values determined by the boundary calculator 140. Segment extractor 145 may retrieve the accessed fingerprints in order to assemble frame-accurate segments of interest that have been extracted based on the repeated fingerprints in one or more media streams. The segment extractor 145 may assemble segments of interest based on the retrieved frames or the retrieved fingerprint values.
The segment extractor 145 may provide the extracted media segments for storage in a database, such as database 115. The extracted media segment may be one or more frames or one or more fingerprint values representing frames in the segment of interest. In some implementations, the segment extractor 145 can send the segments of interest to a third party, e.g., to a third party computing device, via the network 110. In some implementations, the segment extractor 145 may provide the extracted segments of interest to the media source 150 via the network 110. In some implementations, providing the segment of interest can include providing one or more media frames of the segment of interest. In some implementations, providing the segment of interest can include providing one or more fingerprints corresponding to one or more frames of the segment of interest. In some implementations, providing the segment of interest can include providing a timestamp for each frame and/or fingerprint included in the segment of interest. In some implementations, providing the segment of interest can include metadata about the segment of interest (e.g., duration, content, media stream containing the segment, absolute timestamp corresponding to the segment, relative timestamp corresponding to the timestamp, etc.). In some implementations, providing the segment of interest can include storing metadata, fingerprints, and/or frames associated with the segment of interest in a data structure in a computer memory, such as database 115 or storage 825 in conjunction with fig. 8.
Referring now to fig. 2, a flow diagram of a method for frame accurate extraction of media segments based on fingerprint matching is depicted. The method 200 may be implemented or performed using the data processing system 105 described above in connection with FIG. 1 or the computer system 800 described below in connection with FIG. 8. In brief overview, a data processing system may receive a frame from a media stream (202). The data processing system may generate a fingerprint for each frame (204). The data processing system may aggregate the fingerprints (206). The data processing system may receive a target timestamp (208). The data processing system may determine a target fingerprint (210). The data processing system may retrieve a candidate plurality of fingerprints (212). The data processing system may determine whether a matching fingerprint is found (214). The data processing system may match the fingerprints to determine an upper boundary and a lower boundary (216). The data processing system may extract the media segment (218). The data processing system may provide a media segment (220).
A data processing system (e.g., data processing system 105) may receive a frame from a media stream (202). The data processing system may receive one or more media frames included in one or more media streams from at least one media source (e.g., media source 150). Each media stream may include at least one media frame (sometimes referred to herein generally as a frame), which may be a frame of a video stream, such as MPEG-4h.264 video. The frame may include video information, audio information, and closed caption information. Video information may be encoded using a video codec (e.g., H.263, H.264, HEVC, MPEG4, Theora, 3GP, Windows Media 8, QuickTime, MPEG-4, VP8, VP6, MPEG1, MPEG2, MPEG-TS, etc.). Audio information may be encoded using an audio codec (e.g., MP3, AAC, HE-AAC, AC3, EAC3, Vorbis, WMA, PCM, etc.). Closed caption information may be encoded using a closed caption format (e.g., WEBVTT, CEA-608, CEA-708, DFXP, SAMI, SCC, SRT, TTML, 3GPP, etc.). Each frame may include a timestamp corresponding to its position in the media stream. The media stream may have a fixed number of frames per unit time, for example 24 frames per second. The media stream may be an adaptive bitrate stream. The media stream may be a live video stream, such as an internet video stream, or a live television stream. The live television stream may be received from a cable endpoint serviced by a cable provider, a satellite antenna serviced by a satellite service provider, or a television antenna. The data processing system may receive one or more frames via a communication interface, such as a network or a communication bus.
A data processing system (e.g., data processing system 105) may generate a fingerprint for each frame (204). The data processing system may perform the steps necessary to generate a fingerprint for each media frame based on the window values, for example, using method 204 in conjunction with fig. 3. Fig. 6 shows an example diagram of this process. In some implementations, the window value may be a value corresponding to a time period that occurs before or after a frame in the respective media stream. For example, the window value may be a period of time (e.g., 3 seconds after the selected frame), a number of frames (e.g., 300 frames of the media stream before the selected frame), or a segment of the media stream that includes the selected frame. Each window of the media stream may correspond to a single frame in the media stream, and each frame in the media stream may be associated with a corresponding frame window. The fingerprint generated for each frame may be based on the frame window associated therewith. The window (e.g., sliding window) of frames may be based on a predetermined value or provided through the media stream.
A data processing system (e.g., data processing system 105) may aggregate the fingerprints (206). In some implementations, the fingerprints may be computed before being aggregated into a final data structure representing the media stream. In some implementations, fingerprints corresponding to media streams may be assembled into data structures in computer memory (e.g., database 115) when computed by a data processing system. The data processing system may periodically update a data structure that may include one or more fingerprint values. In some implementations, aggregating one or more fingerprint values may include associating each fingerprint value with a corresponding timestamp. The timestamp may correspond to a frame associated with the fingerprint, such as a frame timestamp or a frame index. The frame timestamp may be an absolute timestamp (e.g., the time that the frame appears in the media stream), a relative timestamp (e.g., the amount of time into the media stream when the frame appears), or a frame index value (e.g., the frame number in a list of frames in their order of occurrence). The data structure may also include a timestamp value associated with each fingerprint.
A data processing system (e.g., data processing system 105) may receive a target timestamp (208). The target timestamp may correspond to a particular time in the target media stream. The target media stream may include one or more frames, and each frame may include a corresponding timestamp. The target timestamp may be an index value corresponding to a list of frames in the target media stream indexed by the index value. The target timestamp may be a relative timestamp corresponding to a time period after the first frame in the media stream. For example, the target timestamp may be 5 seconds, which means the frame corresponding to the fifth second of the target media stream. The target timestamp may be an absolute timestamp corresponding to an absolute time of when the target media stream (e.g., the live media stream) is being played. The data processing system may receive the target timestamp from the external computing device via the network. In some implementations, the data processing system can receive the target timestamp from a user input.
A data processing system (e.g., data processing system 105) may determine a target fingerprint (210). The data processing system may search the one or more fingerprints generated in step (204) to determine which fingerprint corresponds to the target timestamp received in step (208). Each fingerprint may correspond to a frame in the target media stream indicated by the target timestamp. In some implementations, each fingerprint can be associated with a timestamp corresponding to a respective frame in the target media stream. In some implementations, each fingerprint can be associated with an index value that corresponds to an order in which frames in the media stream occur. The data processing system may search for fingerprints associated with the target media stream and compare each associated timestamp with the target timestamp received in step (208). In some implementations, the data processing system can select a target fingerprint from the fingerprints if the data processing system is associated with a timestamp that is closest to the target timestamp. In some implementations, the data processing system can determine an index value for the target fingerprint. In some implementations, the time stamp of the fingerprint must exactly match the target time stamp to be selected as the target fingerprint.
A data processing system (e.g., data processing system 105) may retrieve a candidate plurality of fingerprints (212). In some implementations, the data processing system may retrieve the candidate fingerprint from a database, such as database 115. The candidate fingerprint may be associated with a candidate media stream. In some implementations, the candidate media stream may be the same as the target media stream. In some implementations, candidate media streams may be received from media sources (e.g., media source 150) and candidate fingerprints may be generated using, for example, in conjunction with method 204 of fig. 3. The candidate plurality of fingerprints may each be associated with a corresponding candidate timestamp. The candidate timestamps may correspond to frames in the candidate media streams. In some implementations, the candidate media stream may include a series of frames that may constitute a segment of interest. Segments of interest may also be included in the target media stream.
A data processing system (e.g., data processing system 105) may determine whether a matching fingerprint is found (214). The data processing system may compare each candidate fingerprint retrieved in step (212) with the target fingerprint determined in step (210). The data processing system may determine a similarity value for each candidate fingerprint using a distance equation associated with the hash algorithm used to calculate the target fingerprint and each candidate fingerprint. The calculated similarity value of the candidate fingerprint may be determined by comparing the candidate fingerprint with the target fingerprint. If any candidate fingerprint is sufficiently similar to the target fingerprint, the data processing system may determine that a matching fingerprint has been found. For example, if the similarity value calculated for a candidate fingerprint is greater than or equal to a predetermined similarity threshold, the data processing system may determine that the respective candidate fingerprint is sufficiently similar to the target fingerprint and may find a match. When a match is found, the data processing system may determine the index value of the matching candidate fingerprint, and the data processing system may perform step (216) of method 200. If no candidate fingerprint is sufficiently similar to the target fingerprint, the data processing system may determine that no match is found in the candidate fingerprint. If no match is found in the candidate fingerprints, the data processing system may return to step (212) to retrieve another set of candidate fingerprints associated with another candidate media stream.
A data processing system (e.g., data processing system 105) may determine an upper boundary and a lower boundary (216). The data processing system may use method 216A in conjunction with fig. 4 to calculate the lower bound. The data processing system may calculate the computational boundary using the method 216B in conjunction with fig. 5. The lower boundary may correspond to a timestamp in the candidate media stream that corresponds to a first frame in a segment of interest in the candidate media stream. The upper boundary may correspond to a timestamp in the candidate media stream that corresponds to a last frame in a segment of interest in the candidate media stream. The data processing system may compute the upper and lower bounds by comparing the fingerprints in the target media stream with the fingerprints in the candidate media stream, starting with the target fingerprint and ending with the first and last fingerprints of the segment of interest. The data processing system may determine the upper or lower boundary by comparing the candidate media stream and the target media stream until a mismatch is found. A mismatch between the two streams (e.g., fingerprints that are not sufficiently similar) may mean that the mismatched fingerprints represent frames that may not be part of the segment of interest.
A data processing system (e.g., data processing system 105) may extract the media segments (218). The media segment may be a segment of interest, which may be defined by the upper and lower boundaries determined in step (216). The data processing system may extract the media segment by accessing a frame of the media segment (e.g., a candidate media segment) that includes a timestamp that is less than the upper boundary timestamp and less than the lower boundary timestamp. In some implementations, a data processing system can access a fingerprint corresponding to a frame that includes a timestamp that is less than an upper bound timestamp and greater than a lower bound timestamp. In some implementations, the data processing system can include frames and/or fingerprints corresponding to upper and lower boundary timestamps in the extracted segment of interest. The data processing system may access the frames and/or fingerprints from a computer memory, such as a database (e.g., database 115) or a computer memory (e.g., in conjunction with memory 825 of fig. 8).
A data processing system (e.g., data processing system 105) may provide the media segments (220). The data processing system may provide the media segments extracted in step (218). In some implementations, the data processing system may provide the media segments via a computer network (e.g., network 110). In some implementations, the data processing system may provide the media segments via a communication bus. When providing the media segments, the data processing system may provide frames that include the media segments and/or fingerprints that may correspond to the frames that include the media segments. In some implementations, the frames that make up a media segment may include respective timestamps. In some implementations, providing the media segment can include metadata about the media segment (e.g., duration, content, media stream containing the segment, absolute timestamp corresponding to the segment, relative timestamp corresponding to the timestamp, etc.). In some implementations, providing the media segments may include storing metadata, fingerprints, and/or frames associated with the media segments in a data structure in a computer memory (e.g., memory 825 in conjunction with fig. 8) or a database (e.g., database 115).
Referring now to FIG. 3, a flow diagram of a method 204 for generating fingerprint values for frames in a media stream is depicted. The method 204 may be implemented or performed using the data processing system 105 described above in connection with FIG. 1 or the computer system 800 described below in connection with FIG. 8. Method 204 may be implemented as part of step 204 of method 200 described above in connection with fig. 2. Briefly, a data processing system may select an ith frame from a plurality of frames (302). The data processing system may select m frames based on the plurality of frames (304). The data processing system may select a jth frame from the m frames (306). The data processing system may hash the jth frame (308). The data processing system may truncate the jth frame (310). The data processing system may determine whether all m frames have been processed (312). The data processing system may increment a counter register j (314). The data processing system may concatenate the truncated hash frames (316). The data processing system may determine whether all n frames have been processed (318). The data processing system may increment a counter register i (320).
The data processing system may select an ith frame from a plurality of frames (302). In some implementations, i can specify a counter register that indexes an iteration of the loop each time method 204 is executed. In some implementations, the value of i is used to index multiple frames for processing. In some implementations, all iterations of the loop indexed by i may be performed in parallel. In some implementations, in a first iteration of the loop indexed by i in method 204, the data processing system may select a first frame from a plurality of frames (ith, i ═ 1). It should be understood that the frames may be selected in any order. The data processing system may select each frame from the plurality of frames to generate a respective fingerprint value corresponding to the predetermined window value.
The data processing system may select m frames based on the plurality of frames (304). The data processing system may generate a fingerprint for each frame in the media stream. The media stream may comprise one or more frames. The data processing system may generate a fingerprint based on each frame and one or more other frames within a specified window value (e.g., a time period, a number of frames in a media stream, a sliding window, etc.). The data processing system may select m frames from the media stream before or after the frame associated with the fingerprint to be generated. For example, the data processing system may select m frames after the ith frame selected in step (302). The m frames may be selected based on a time interval, which may be included in the window value. For example, the window value may be three seconds, specifying that m frames may be selected after the selected ith frame. Further extending the example, the window value may specify that the selected m frames may be selected based on a time interval (e.g., one frame every 0.5 seconds). In this example, m may be equal to six selected frames, which may be selected because there are six frames that satisfy the window condition and the time interval condition. In some implementations, the ith frame may be included in the selected m frames. In some implementations, the data processing system can select frames that are not within the specified window value.
The data processing system may select a jth frame from the m frames (306). In some implementations, j may specify a counter register that indexes an iteration of the loop each time method 204 is executed. In some implementations, the value of j is used to index the m frames selected in step (304) for processing. In some implementations, all iterations of the loop indexed by j may be performed in parallel. In some implementations, in the first iteration of the loop indexed by j in method 204, the data processing system may select the first frame from m selected frames (jth, j ═ 1). It should be understood that the frames may be selected in any order. The data processing system may select each frame from the selected m frames to generate a respective fingerprint value corresponding to the predetermined window value.
The data processing system may hash the jth frame (308). The data processing system may hash the jth frame selected in step (306) using one or more hashing algorithms or functions. In some implementations, the hash function can be a locality sensitive hash function. In some implementations, the hash function is another type of hash function (e.g., SHA-1, SHA-2, MD5, etc.). For example, a locally sensitive hashing algorithm may be selected to allow the data processing system to compute a similarity value between two frames. The data processing system may use bit sampling as a locally sensitive hashing algorithm. The data processing system may use a min-wise independent permutation locality sensitive hashing algorithm (min-wise independent locality sensitive hashing). The data processing system may use other types of locality sensitive hashing algorithms, such as a random projection method. The hash frame may have a fixed bit width (e.g., 8 bits, 16 bits, 32 bits, 64 bits, etc.). In some implementations, the hash frame may have a variable bit width. In some implementations, the data processing system may decode or pre-process the frame before applying the hashing algorithm to create the corresponding hashed frame. For example, the data processing may decode a frame using a corresponding codec to access the original frame information before applying a hashing algorithm to the original frame information to create a hashed frame. The data processing system may decode and hash audio information, video information, and/or closed caption information included in the frame and/or audio information, video information, and/or closed caption information included in the frame that is hashed as a different hash. The data processing system may create and store the hash in a computer memory, such as a database (e.g., database 115) or a computer memory (e.g., in conjunction with storage 825 of fig. 8). The data processing system may store the hashed frame in a data structure, such as a C-like data structure or a region of computer memory. The data processing system may store the hashed frames in a buffer, which may be a temporary area of computer memory used to store data. The data processing system may associate each hash frame with a respective timestamp, index value, or positioning data to indicate the location of the hash frame in the respective media stream.
The data processing system may truncate the jth frame (310). The data processing system may truncate the jth frame of the hash created in step (308) to generate a jth truncated hashed frame. Truncating the jth hash frame may include selecting a fixed range of bits, e.g., the 8 least significant bits, in the jth hash frame. Truncating the jth hash frame may include storing the truncated bits in a data structure in a computer memory (e.g., in conjunction with memory 825 of fig. 8) or in a database (e.g., database 115). For example, if the binary value of the jth hash frame is 01100100, the data processing system may truncate the hash frame by selecting the four most significant bits (0110 in this example). Truncating the jth hash frame may reduce the overall size of the jth hash frame while still retaining sufficient information for further processing, which is an improvement over other implementations. In some implementations, the data processing system may skip performing the truncation step and instead use the entire hash frame for further processing. In some implementations, truncating the jth hash frame may include selecting any combination of bits in the jth hash frame to generate the jth truncated hash frame. For example, if the binary value of the jth hash frame is 00110001, the data processing system may select each of the bits 6, 4, 2, and 0 to generate a truncated hash frame with a value of 0101.
The data processing system may determine whether all m frames have been processed (312). Generating the fingerprint for the ith frame selected in step (302) may include generating a truncated hash frame for each of the m frames selected in step (304). In some implementations, the data processing system can compare the counter register j to the number m to determine whether each of the m frames has been processed. In some implementations, the progress of the loop indexed by j is tracked using a data structure. In some implementations, the data processing system may record an entry in the data structure indicating that a corresponding frame of the m frames has been hashed and truncated as in steps (308) and (310), respectively. The data processing system may access the data structure to determine whether all m frames have been hashed and truncated. In some implementations, the data processing system can determine that m frames have been hashed and truncated by comparing the counter register j to the number m. If j is equal to the value m, the data processing system may determine that all m frames have been hashed and truncated and the data processing system may perform step (316) of method 204. In such an implementation, if the counter register j is not equal to the number m, the data processing system may proceed to step (314).
The data processing system may increment a counter register j (314). The data processing system may increment counter register j by adding a value of one to the register. In some implementations, the data processing system may record in the data structure that the jth frame has been hashed and truncated as in steps (308) and (310), respectively. After incrementing counter register j, the data processing system may perform step (306) of method 204.
The data processing system may concatenate the truncated hash frames (316). The data processing system may generate a fingerprint representing a window value based on each frame in the media stream. The selected ith frame may be associated with the generated fingerprint. The data processing system may generate a fingerprint associated with the selected ith frame by concatenating each of the m truncated hash frames generated in steps (306) - (314). The data processing system may compute the ith fingerprint by concatenating each of the m truncated hash frames together into a single value. For example, if three frames (e.g., m-3) are selected from a time window of the media stream and their truncated hash values are equal to 0110, 0101, and 1000, the data processing system may concatenate each of these values to generate a fingerprint value 011001011000. Although in this example, the m truncated hash frames are shown concatenated in a particular order, it should be understood that each truncated hash frame may be concatenated in any combination or permutation. The data processing system may associate each fingerprint value with a timestamp or index value to indicate a location in the media stream to which the fingerprint corresponds.
The data processing system may determine whether all n frames have been processed (318). Generating a fingerprint for each frame in the media stream may include: selecting m frames from the media stream; generating a truncated hash frame for each of the m frames; and concatenating the m truncated hash frames for each of the n frames in the media stream. In some implementations, the data processing system can compare the counter register i to the number of frames in the media stream n to determine whether a fingerprint has been generated for each of the n frames. In some implementations, a data structure is used to track the progress of the loop indexed by i. In some implementations, the data processing system can record an entry in the data structure indicating that a respective fingerprint for a respective frame of the m frames has been generated by method 204. The data processing system may access the data structure to determine whether a fingerprint has been generated for each of the n frames. In some implementations, the data processing system can determine that n frames have generated corresponding fingerprints by comparing the counter register i to the number n. If i is equal to the value n, the data processing system may determine that all n frames have generated corresponding fingerprints and the data processing system may complete execution of method 204. In such an implementation, if the counter register i is not equal to the number n, the data processing system may move to step (320).
The data processing system may increment a counter register i (320). The data processing system may increment counter register i by adding a value of one to the register. In some implementations, the data processing system can record in the data structure that a corresponding fingerprint has been generated for the ith frame. After incrementing counter register j, the data processing system may perform step (302) of method 204.
Referring now to FIG. 4, a flow diagram of a method 216A for determining a lower boundary of a matching media segment is depicted. The method 216A may be implemented or performed using the data processing system 105 described above in connection with FIG. 1 or the computer system 800 described below in connection with FIG. 8. Method 216A may be implemented as part of step 216 of method 200 described in conjunction with fig. 2. Briefly, a data processing system may select an ith fingerprint from a first plurality of fingerprints (402). The data processing system may select a jth fingerprint from the second plurality of fingerprints (404). The data processing system may compare the ith fingerprint to the jth fingerprint (406). The data processing system may determine whether a match exists (408). The data processing system may decrement counter registers i and j (410). The data processing system may calculate a lower bound (412).
The data processing system may select an ith fingerprint from the first plurality of fingerprints (402). In some implementations, the counter value i can be a counter register for indexing loops. The first plurality of fingerprints may be a plurality of fingerprints (sometimes referred to herein generally as target media streams) generated in step (204) of method 200. The data processing system may select the ith fingerprint to compare with the fingerprints in the candidate plurality of fingerprints retrieved in step (212) to determine a lower boundary of the matching segment of interest. In some implementations, each fingerprint may correspond to a single frame in each media stream, meaning that if a matching fingerprint is found in both media streams, there may be a matching frame in each of the two media streams. In some implementations, in the first iteration of the loop, the value of i may be equal to the index value of the target fingerprint determined in step (210) of method 200.
The data processing system may select a jth fingerprint from the second plurality of fingerprints (404). In some implementations, the counter value j may be a counter register for indexing cycles. The second plurality of fingerprints may be a candidate plurality of fingerprints (sometimes referred to herein generally as candidate media streams) retrieved by the data processing system in step (212) of method 200. The data processing system may select the jth fingerprint to compare to a fingerprint of a plurality of fingerprints generated in step (204) of method 200, which may correspond to the target media stream. The data processing system may perform an iterative comparison between the fingerprint associated with the target media stream and the fingerprint associated with the candidate media stream to determine a lower boundary of the segment of interest. In some implementations, each fingerprint may correspond to a single frame in the respective media stream, meaning that if a matching fingerprint is found in both media streams, there may be a matching frame in each of the two media streams. In some implementations, in the first iteration of the loop, the value of j may be equal to the index value of the candidate fingerprint determined in step (214) of method 200.
The data processing system may compare the ith and jth fingerprints (406). The data processing system may determine whether a match exists (408). The data processing system may determine a similarity value for each candidate fingerprint using a distance equation associated with a hashing algorithm (e.g., the hashing algorithm used to generate each fingerprint). The similarity value of the candidate fingerprint may be calculated by comparing the candidate fingerprint to the target fingerprint using a distance equation (e.g., hamming distance, other locality-based hashing algorithm distance functions, etc.). If the ith target fingerprint and the jth candidate fingerprint are sufficiently similar, the data processing system may determine that a match has been found. For example, if the similarity values calculated for the ith and jth fingerprints are greater than or equal to a predetermined similarity threshold, the data processing system may determine that the corresponding jth fingerprint is sufficiently similar to the ith fingerprint and a match may be found. When a match is found, the data processing system may perform step (410) of method 216A. If no match is found, the data processing system may determine the boundaries of the matching segments in the target media stream and the candidate media streams and perform step (412) of method 216A.
The data processing system may decrement counter registers i and j (410). The data processing system may compare each candidate fingerprint to the target fingerprint to determine a lower boundary timestamp. In some implementations, the data processing system may start with the fingerprint index value determined for the target media stream in step (210) and the fingerprint index value determined for the candidate media stream in step (214) and end when there is no match or when the beginning of one stream is reached. To determine the next fingerprint to compare, the data processing system may select the fingerprint having the index value next closest to the beginning of each stream (e.g., one less index value than the current index of i and j for each respective media stream). In some implementations, the data processing system may decrement each of the counter registers i and j by 1 to determine the next fingerprint nearest the beginning of each media stream. If the beginning of the target media stream or the beginning of the candidate media stream has been reached (e.g., no more fingerprints are available for comparison), the data processing system may determine that the lower boundary timestamp is equal to the timestamp associated with the previously compared fingerprint in the candidate media stream. In some implementations, if the beginning of the target media stream or the beginning of the candidate media stream has been reached, the data processing system may determine that the lower boundary timestamp is equal to a timestamp associated with a previously compared fingerprint in the target media stream.
The data processing system may calculate a lower bound (412). If a mismatch occurs between two fingerprints in the first plurality of fingerprints and the second plurality of fingerprints (e.g., the target media stream and the candidate media stream, respectively), the data processing system may determine a lower boundary of the segment of interest. The segment of interest may be a media segment in the target media stream that is also present in the candidate media stream. The data processing system may compare fingerprints associated with the target and candidate media streams to determine the boundaries of matching segments of interest. In some implementations, the lower boundary of the segment of interest can be a timestamp corresponding to the first frame (e.g., the starting frame) of the media segment of interest. In some implementations, the lower boundary may correspond to a frame index of the first frame in the segment of interest. When a mismatch between the two fingerprints is determined in step (408), the data processing system may access the last matching fingerprint (e.g., corresponding to index j +1) of the candidate media stream to determine a timestamp of the frame corresponding to the fingerprint. In some implementations, the data processing system can access the last matching fingerprint (e.g., corresponding to index i +1) of the target media stream to determine a timestamp of the frame corresponding to the fingerprint. The lower bound may be equal to the timestamp of the last matching fingerprint associated with the candidate media stream. In some implementations, the lower boundary may be equal to the timestamp of the last matching fingerprint associated with the target media stream. In some implementations, the lower bound may be equal to the last matching index (e.g., j +1) of the candidate media stream. In some implementations, the lower bound may be equal to the last matching index (e.g., i +1) of the target media stream. The data processing system may use the lower boundary to extract the segment of interest, for example, by performing step (218) of method 200.
Referring now to FIG. 5, a flow diagram of a method 216B for determining an upper boundary of a matching media segment is depicted. The method 216B may be implemented or performed using the data processing system 105 described above in connection with FIG. 1 or the computer system 800 described below in connection with FIG. 8. Method 216B may be implemented as part of step 216 of method 200 described in conjunction with fig. 2. Briefly, a data processing system may select an ith fingerprint from a first plurality of fingerprints (502). The data processing system may select a jth fingerprint from the second plurality of fingerprints (504). The data processing system may compare the ith and jth fingerprints (506). The data processing system may determine whether a match exists (508). The data processing system may increment counter registers i and j (510). The data processing system may compute an upper boundary (512).
The data processing system may select an ith fingerprint from the first plurality of fingerprints (502). In some implementations, the counter value i can be a counter register for indexing loops. The first plurality of fingerprints may be a plurality of fingerprints (sometimes referred to herein generally as target media streams) generated in step (204) of method 200. The data processing system may select the ith fingerprint to compare with one of the candidate plurality of fingerprints retrieved in step (212) to determine an upper boundary of the matching segment of interest. In some implementations, each fingerprint may correspond to a single frame in each media stream, meaning that if a matching fingerprint is found in both media streams, there may be a matching frame in each of the two media streams. In some implementations, in the first iteration of the loop, the value of i may be equal to the index value of the target fingerprint determined in step (210) of method 200.
The data processing system may select a jth fingerprint from the second plurality of fingerprints (504). In some implementations, the counter value j may be a counter register for indexing cycles. The second plurality of fingerprints may be a candidate plurality of fingerprints (sometimes referred to herein generally as candidate media streams) retrieved by the data processing system in step (212) of method 200. The data processing system may select the jth fingerprint to compare to a fingerprint of a plurality of fingerprints generated in step (204) of method 200, which may correspond to the target media stream. The data processing system may perform an iterative comparison between the fingerprint associated with the target media stream and the fingerprint associated with the candidate media stream to determine an upper boundary of the segment of interest. In some implementations, each fingerprint may correspond to a single frame in the respective media stream, meaning that if a matching fingerprint is found in both media streams, there may be a matching frame in each of the two media streams. In some implementations, in the first iteration of the loop, the value of j may be equal to the index value of the candidate fingerprint determined in step (214) of method 200.
The data processing system may compare the ith and jth fingerprints (506). The data processing system may determine whether a match exists (508). The data processing system may determine a similarity value for each candidate fingerprint using a distance equation associated with a hashing algorithm (e.g., the hashing algorithm used to generate each fingerprint). The similarity value of the candidate fingerprint may be calculated by comparing the candidate fingerprint to the target fingerprint using a distance equation (e.g., hamming distance, other locality-based hashing algorithm distance functions, etc.). If the ith target fingerprint and the jth candidate fingerprint are sufficiently similar, the data processing system may determine that a match has been found. For example, if the similarity values calculated for the ith and jth fingerprints are greater than or equal to a predetermined similarity threshold, the data processing system may determine that the corresponding jth fingerprint is sufficiently similar to the ith fingerprint and a match may be found. When a match is found, the data processing system may perform step (510) of method 216B. If no match is found, the data processing system may determine the boundaries of the matching segments in the target media stream and the candidate media streams and perform step (512) of method 216B.
The data processing system may increment counter registers i and j (510). The data processing system may compare each of the candidate fingerprint and the target fingerprint to determine an upper bound timestamp. In some implementations, the data processing system may start with the fingerprint index value determined for the target media stream in step (210), start with the fingerprint index value determined for the candidate media streams in step (214), and end when there is no match or when the end of one of the media streams is reached. To determine the next fingerprint to compare, the data processing system may select a fingerprint having the next index value closest to the end of each stream (e.g., one greater than the current index of i and j for each respective media stream). In some implementations, the data processing system may increment each of the counter registers i and j by one to determine the next closest fingerprint to the end of each media stream. If the end of the target media stream or the end of the candidate media stream has been reached (e.g., no more fingerprints are available for comparison), the data processing system may determine that the upper bound timestamp is equal to a timestamp associated with a previously compared fingerprint in the candidate media stream. In some implementations, if the beginning of the target media stream or the beginning of the candidate media stream has been reached, the data processing system may determine that the lower boundary timestamp is equal to a timestamp associated with a previously compared fingerprint in the target media stream.
The data processing system may compute an upper boundary (512). If a mismatch occurs between two fingerprints in the first plurality of fingerprints and the second plurality of fingerprints (e.g., the target media stream and the candidate media stream, respectively), the data processing system may determine an upper boundary of the segment of interest. In some implementations, the segment of interest may be a media segment in the target media stream that is also present in the candidate media stream. The data processing system may compare fingerprints associated with the target and candidate media streams to determine the boundaries of matching segments of interest. In some implementations, the upper boundary of the segment of interest can be a timestamp corresponding to the last frame (e.g., the final frame) of the media segment of interest. In some implementations, the upper boundary may correspond to a frame index of a final frame in the segment of interest. When a mismatch between the two fingerprints is determined in step (508), the data processing system may access the last matching fingerprint (e.g., corresponding to index j-1) of the candidate media stream to determine a timestamp of the frame corresponding to the fingerprint. In some implementations, the data processing system can access the last matching fingerprint (e.g., corresponding to index i-1) of the target media stream to determine a timestamp of the frame corresponding to the fingerprint. The upper bound may be equal to the timestamp of the last matching fingerprint associated with the candidate media stream. In some implementations, the lower boundary may be equal to the timestamp of the last matching fingerprint associated with the target media stream. In some implementations, the upper bound may be equal to the last matching index (e.g., j-1) of the candidate media stream. In some implementations, the upper boundary may be equal to the last matching index (e.g., i-1) of the target media stream. The data processing system may use the upper boundary to extract the segment of interest, for example, by performing step (218) of method 200.
Referring now to fig. 6, a diagram illustrating an example generation based on fingerprint values for frames in a media stream is depicted. In this illustration, frames 605A-D (sometimes referred to herein generally as frames 605) represent selected frames used to generate fingerprint 615. The generated fingerprint 615 may be associated with one of the selected frames, such as the first selected frame 605A. Frames 605A-D may correspond to time periods in a media stream that is comprised of more frames (not shown). Each frame 605 may be hashed using a hash function to generate corresponding hashed frames 610A-D (sometimes referred to herein generally as hashed frames 610). The hash frame 610 may be a bit stream having a fixed width. Certain bits (e.g., the most significant bits as shown) may be selected by a data processing system (e.g., data processing system 105) and concatenated into a corresponding fingerprint 615. Although the figure shows the most significant bits of the hash frame 610 as being when selected for concatenation, it should be understood that any bit in the hash frame 610 may be selected for the final fingerprint 615. Likewise, the order of each hash frame 610 included in the fingerprint 615 may be any arrangement, order, combination, or permutation. In some implementations, more than four frames 605 may be selected to create the fingerprint 615. In some implementations, additional metadata may be included in the fingerprint, e.g., a timestamp refers to the location in the corresponding media stream where the fingerprint is represented.
Referring now to fig. 7, a diagram illustrating an example of using two sets of media fingerprints to determine an upper boundary and a lower boundary of a matching media segment is depicted. The graph includes a first plurality of fingerprints 710A-I (sometimes referred to herein generally as the first plurality of fingerprints 710). The graph includes a second plurality of fingerprints 725A-I (sometimes referred to herein generally as the second plurality of fingerprints 725). The diagram includes a target segment of interest 715 in a first plurality of fingerprints 710A-I. The figure includes candidate segments of interest 730 in the second plurality of frames 725. The figure includes a target fingerprint index 735. The graph includes a matching candidate fingerprint index 740. The diagram includes a lower boundary index 745. The diagram includes an upper boundary index 745.
The data processing system may receive an index of target fingerprints, such as target fingerprint index 735. The data processing system may compare the target fingerprint 710E with each of the second plurality of fingerprints 725 to determine a candidate fingerprint index 740 corresponding to the matching fingerprint 725F. To determine the boundary of the segment of interest 730 in the candidate media stream, the data processing system may compare the first plurality of fingerprints 710 to the second plurality of fingerprints.
To determine the lower boundary 745, the data processing system may begin with the target fingerprint and the matching candidate fingerprint and compare the next two fingerprints closest to the beginning of each of the plurality of fingerprints to determine if the fingerprints match. For example, the data processing system may next compare the first plurality of fingerprints 710D and the second plurality of fingerprints 725E. Because these two matches, the data processing system may continue to compare the next two fingerprints closest to the beginning of each stream, in this example fingerprints 710C and 725D. Because fingerprint 710D and fingerprint 725D match, the data processing system may continue to compare the next two fingerprints closest to the beginning of each stream, 710B and 725C in this example. In this example, the fingerprints do not match. In this way, the data processing system may determine the lower boundary 745 of the segment of interest 730 to correspond to the candidate fingerprint 725D because it is the last matching fingerprint in the second plurality of fingerprints. In some implementations, the data processing system can determine the lower boundary 745 of the segment of interest 715 to correspond to the target fingerprint 710C.
To determine the lower bound 750, the data processing system may begin with the target fingerprint and the matching candidate fingerprint and compare the next two fingerprints closest to the end of each of the plurality of fingerprints to determine whether the fingerprints match. For example, the data processing system may next compare the fingerprint 710F of the first plurality of fingerprints to the fingerprint 725G of the second plurality of fingerprints. Because of these two matches, the data processing system may continue to compare the next two fingerprints closest to the beginning of each stream, in this example fingerprint 710G and fingerprint 725H. Because fingerprint 710G and fingerprint 725H match, the data processing system may continue to compare the next two fingerprints closest to the beginning of each stream, 710H and 725I in this example. In this example, the fingerprints do not match. In this way, the data processing system may determine the upper boundary 750 of the segment of interest 730 to correspond to the candidate fingerprint 725H because it is the last matching fingerprint in the second plurality of fingerprints. In some implementations, the data processing system can determine the upper boundary 750 of the segment of interest 730 to correspond to the target fingerprint 710G.
Fig. 8 illustrates the general architecture of an exemplary computer system 1200, according to some implementations, which exemplary computer system 1200 can be used to implement any of the computer systems discussed herein. Computer system 800 may be used to provide information for display via network 810. The computer system 800 of fig. 8 includes: one or more processors 820 communicatively coupled to a memory 825, one or more communication interfaces 805, one or more output devices 810 (e.g., one or more display units), and one or more input devices 815. The processor 820 may be included in the data processing system 105 or other component of the system 800 (e.g., the data processing system 805).
In the computer system 800 of fig. 8, the memory 825 may comprise any computer-readable storage medium and may store computer instructions such as processor-executable instructions for implementing the various functions described herein for the various systems, as well as any data associated therewith, generated thereby or received via a communication interface or input device (if any). Referring again to the system 800 of FIG. 8, the data processing system may include a memory 825 to store information relating to a set of user identifiers, generated vectors, and the like. One or more processors 820 shown in fig. 8 may be used to execute instructions stored in memory 825, and in so doing, may also read from or write to memory various information that is processed and/or generated in accordance with the execution of the instructions.
The processor 820 of the computer system 800 shown in fig. 8 may also be communicatively coupled to the communication interface 805 or control the communication interface 805 to send or receive various information in accordance with the execution of instructions. For example, one or more communication interfaces 805 may be coupled to a wired or wireless network, bus, or other communication means, and thus may allow computer system 800 to send information to and receive information from other devices (e.g., other computer systems). Although not explicitly shown in the system of fig. 8, one or more communication interfaces facilitate the flow of information between components of the system 800. In some implementations, the communication interface(s) may be configured (e.g., via various hardware or software components) to provide a website that is an access portal to at least some aspects of the communication interface 805. Examples of communication interface 805 include a user interface (e.g., a web page) through which a user may communicate with data processing system 800.
An output device 810 of the computer system 800 shown in fig. 8, for example, may be provided to allow various information to be viewed or otherwise perceived in connection with execution of the instructions. An input device 815 may be provided, for example, to allow a user to manually adjust, select, input data, or interact with the processor in various ways during instruction execution. Additional information regarding general computer system architectures that may be used for the various systems discussed herein is further provided herein.
Implementations of the subject matter and the operations described in this specification can be implemented in digital electronic circuitry, or in computer software embodied in tangible media, firmware, or hardware, including the structures disclosed in this specification and their equivalents, or in combinations of one or more of them. Implementations of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more components of computer program instructions, encoded on computer storage media for execution by, or to control the operation of, data processing apparatus. The program instructions may be encoded on an artificially generated propagated signal (e.g., a machine-generated electrical, optical, or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus). The computer storage media may be or be included in a computer-readable storage device, a computer-readable storage substrate, a random or serial access memory array or device, or a combination of one or more of them. Further, although the computer storage medium is not a propagated signal, the computer storage medium can comprise a source or destination of computer program instructions encoded in an artificially generated propagated signal. The computer storage media may also be, or be embodied in, one or more separate physical components or media (e.g., multiple CDs, diskettes, or other storage devices).
The features disclosed herein may be implemented on a smart television module (or connected television module, hybrid television module, etc.) that may include a processing module configured to integrate an internet connection with more traditional television programming sources (e.g., television programming sources received via cable, satellite, wireless, or other signals). The smart television module may be physically incorporated into a television set or may comprise a separate device such as a set-top box, a blu-ray or other digital media player, a game console, a hotel television system, and other companion devices. The smart television module may be configured to allow viewers to search for and locate videos, movies, photos, and other content on a network, on a local cable television channel, on a satellite television channel, or stored on a local hard disk. A set-top box (STB) or set-top unit (STU) may include an information device that may contain a tuner and connect to a television and an external source of signals, convert the signals to content, and then display on a television screen or other display device. The smart television module may be configured to provide a home screen or top level screen including icons for a number of different applications (e.g., a Web browser and a number of streaming media services), a connected cable or satellite media source, other "Web" channels, and the like. The smart television module may be further configured to provide an electronic program guide to the user. The companion application of the smart television module may operate on the mobile computing device to provide additional information to the user about available programming, to allow the user to control the smart television module, and so on. In alternative implementations, these features may be implemented on a laptop or other personal computer, smartphone, other mobile phone, palmtop computer, tablet, or other computing device.
The operations described in this specification may be implemented as operations performed by a data processing apparatus on data stored on one or more computer-readable storage devices or received from other sources.
The terms "data processing apparatus," "data processing system," "user equipment" or "computing device" encompass all types of devices, apparatuses and machines for processing data, including for example programmable processors, computers, systems on a chip or a plurality or combination of the foregoing. The apparatus can comprise special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). The apparatus can include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, a cross-platform runtime environment, a virtual machine, or a combination of one or more of them. The apparatus and execution environment may implement a variety of different computing model infrastructures, such as web services, distributed computing and grid computing infrastructures.
A computer program (also known as a program, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment. A computer program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub-programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform actions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read-only memory or a random access memory or both. Components of a computer include a processor for performing actions in accordance with instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, a computer need not have such devices. Further, the computer may be embedded in another device, e.g., a mobile telephone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game player, a Global Positioning System (GPS) receiver, or a portable storage device (e.g., a Universal Serial Bus (USB) flash drive). Devices suitable for storing computer program instructions and data include all forms of non-volatile memory, media and storage devices, including: for example, semiconductor memory devices such as EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
To provide for interaction with a user, an implementation of the subject matter described in this specification can be implemented on a computer having a display device (e.g., a CRT (cathode ray tube), plasma or LCD (liquid crystal display) monitor) for displaying information to the user and a keyboard and a pointing device (e.g., a mouse or a trackball) by which the user can provide input to the computer. Other kinds of devices may also be used to provide for interaction with the user. For example, feedback provided to the user can include any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input. In addition, the computer may interact with the user by sending and receiving documents to and from the device used by the user. For example, a web page is sent to a web browser on a user's client device in response to a request received by the web browser.
Implementations of the subject matter described in this specification can be implemented in a computing system that includes a back end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front end component (e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification), or any combination of one or more such back end, middleware, or front end components. The components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include local area networks ("LANs") and wide area networks ("WANs"), the internet (e.g., the internet), and peer-to-peer networks (e.g., ad hoc peer-to-peer networks).
A computing system, such as data processing system 805, may include clients and servers. For example, the data processing system 805 may include one or more data centers or one or more servers in a server farm. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some implementations, the server sends data (e.g., HTML pages) to the client device (e.g., for the purpose of displaying data to and receiving user input from a user interacting with the client device). Data generated at the client device (e.g., a result of the user interaction) may be received from the client device at the server.
While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any inventions or of what may be claimed, but rather as descriptions of features specific to particular implementations of the systems and methods described herein. Certain features that are described in this specification in the context of separate implementations can also be implemented in combination in a single implementation. Conversely, various features that are described in the context of a single implementation can also be implemented in multiple implementations separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, the actions recited in the claims can be performed in a different order and still achieve desirable results. In addition, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results.
In some cases, multitasking and parallel processing may be advantageous. Moreover, the separation of various system components in the implementations described above should not be understood as requiring such separation in all implementations, and it should be understood that the described program components and systems can generally be integrated in a single software product or packaged into multiple software products. For example, the data processing system 805 may be a single module, a logical device with one or more processing modules, one or more servers, or a portion of a search engine.
Having now described some illustrative implementations and embodiments, it is apparent that the foregoing is illustrative and not limiting, having been presented by way of example. In particular, although many of the examples presented herein involve specific combinations of method acts or system components, those acts and those components may be combined in other ways to accomplish the same objectives. Acts, components and features discussed only in connection with one implementation are not intended to be excluded from other implementations or a similar role in other implementations.
The phraseology and terminology used herein is for the purpose of description and should not be regarded as limiting. The use of "including," "comprising," "having," "containing," "involving," "characterized by … …," and variations thereof herein, is meant to encompass the items listed thereafter and equivalents thereof as well as additional items as alternative implementations consisting only of the items listed thereafter. In one implementation, the systems and methods described herein are comprised of each combination of one or more of the elements, acts, or components described.
Any reference herein to an implementation or element or act of the systems and methods in the singular may also include an implementation that includes a plurality of these elements, and any reference herein to any implementation or element or act in the plural may also encompass an implementation that includes only one element. References in the singular or plural form are not intended to limit the presently disclosed systems or methods, their components, acts or elements to a single or multiple configurations. Reference to any action or element based on any information, action, or element may include an implementation in which the action or element is based, at least in part, on any information, action, or element.
Any implementations disclosed herein may be combined with any other implementations, and references to "an implementation," "some implementations," "an alternative implementation," "various implementations," "one implementation," etc. are not necessarily mutually exclusive and are intended to indicate that a particular feature, structure, or characteristic described in connection with the implementation may be included in at least one implementation. Such terms as used herein do not necessarily all refer to the same implementation. Any implementation may be combined, included or excluded, with any other implementation in any manner consistent with aspects and implementations disclosed herein.
References to "or" may be construed as inclusive such that any term described using "or" may refer to any single, more than one, and all of the described terms.
Where technical features in the drawings, specification or any claim are followed by reference signs, the reference signs have been included for the sole purpose of increasing the intelligibility of the drawings, specification and claims. Accordingly, the reference signs or absence thereof has no limiting effect on the scope of any claim elements.
The systems and methods described herein may be embodied in other specific forms without departing from the characteristics thereof. Although the examples provided herein relate to controlling the display of information resource content, the systems and methods described herein may include application to other environments. The foregoing implementations are illustrative, and not limiting of the described systems and methods. The scope of the systems and methods described herein is, therefore, indicated by the appended claims rather than by the foregoing description, and all changes that come within the meaning and range of equivalency of the claims are intended to be embraced therein.
Claims (20)
1. A media segment extraction method based on fingerprint matching comprises the following steps:
receiving, by a data processing system having one or more processors, a media stream comprising a plurality of frames;
for each frame of the plurality of frames:
the data processing system selects a second plurality of frames from the plurality of frames based on the fingerprint window value;
the data processing system hashes each of the second plurality of frames to create a plurality of hashes;
the data processing system truncates each of the plurality of hashes to generate a plurality of truncated hashes;
the data processing system concatenates the plurality of truncated hashes to calculate a frame fingerprint value, a frame fingerprint value associated with each frame of the plurality of frames, and a timestamp corresponding to each frame;
the data processing system aggregates each calculated frame fingerprint value to assemble a plurality of fingerprints;
the data processing system receives a target timestamp corresponding to a target frame in the plurality of frames;
the data processing system determines a target fingerprint of the plurality of fingerprints based on the target timestamp and the fingerprint window value;
the data processing system retrieves a second plurality of fingerprints, each of the second plurality of fingerprints associated with a frame in a second media stream;
the data processing system comparing the target fingerprint to each of the second plurality of fingerprints to determine a matching location corresponding to a location in the second media stream;
the data processing system matching a first fingerprint of the plurality of fingerprints associated with a timestamp less than the target timestamp with a second fingerprint of a second plurality of fingerprints, determining a lower boundary timestamp;
the data processing system matching a third fingerprint of the plurality of fingerprints associated with a timestamp greater than the target timestamp with a fourth fingerprint of the second plurality of fingerprints, determining an upper bound timestamp;
the data processing system extracts media segments from the second media stream according to the lower boundary timestamp and the upper boundary timestamp; and
the media segments are provided by the data processing system for storage in a database.
2. The method of claim 1, wherein aggregating each calculated frame fingerprint value further comprises:
the data processing system stores a plurality of fingerprints in a database.
3. The method of claim 1, wherein retrieving the second plurality of fingerprints further comprises retrieving the second plurality of fingerprints from a database.
4. The method of claim 1, wherein retrieving the second plurality of fingerprints comprises:
the data processing system receives a third media stream comprising a third plurality of frames;
for each of the third plurality of frames:
the data processing system selects a fourth plurality of frames from the third plurality of frames based on the fingerprint window value;
the data processing system hashes each of the fourth plurality of frames to create a second plurality of hashes;
the data processing system truncates each of the second plurality of hashes to generate a second plurality of truncated hashes;
the data processing system concatenates the second plurality of truncated hashes to calculate a second frame fingerprint value; and
aggregating, by the data processing system, each second frame fingerprint value to assemble a second plurality of fingerprints, each second frame fingerprint value associated with a timestamp corresponding to a respective frame of the third plurality of frames.
5. The method of claim 1, wherein hashing each of the second plurality of frames comprises: hashing each of the second plurality of frames using a locality sensitive hashing algorithm.
6. The method of claim 1, wherein the second media stream is the same as the first media stream.
7. The method of claim 1, wherein comparing the target fingerprint to each fingerprint of the second plurality of fingerprints to determine a matching location further comprises:
the data processing system calculates a similarity value between the target fingerprint and each of the second plurality of fingerprints.
8. The method of claim 7, further comprising:
the data processing system determines whether the similarity value is equal to or greater than a similarity threshold; and
the data processing system determines a matching location corresponding to a location in the second media stream in response to the similarity value being equal to or greater than the similarity threshold.
9. The method of claim 1, wherein extracting media segments from the second media stream based on the lower boundary timestamp and the upper boundary timestamp comprises:
the data processing system extracting a third plurality of fingerprints from the second plurality of fingerprints based on the lower bound timestamps and the upper bound timestamps; and
the data processing system stores the third plurality of fingerprints in a database.
10. The method of claim 9, further comprising:
the third plurality of fingerprints is provided by the data processing system.
11. A system comprising one or more processors configured to:
receiving a media stream comprising a plurality of frames;
for each frame of the plurality of frames:
selecting a second plurality of frames from the plurality of frames based on the fingerprint window value;
hashing each of the second plurality of frames to create a plurality of hashes;
truncating each of the plurality of hashes to generate a plurality of truncated hashes;
concatenating the plurality of truncated hashes to compute a frame fingerprint value;
aggregating each calculated frame fingerprint value to assemble a plurality of fingerprints, each calculated frame fingerprint value associated with a timestamp corresponding to a respective frame of the plurality of frames;
associating each calculated frame fingerprint value with a respective frame of the plurality of frames;
receiving a target timestamp in the plurality of frames corresponding to a target frame;
determining a target fingerprint of the plurality of fingerprints from the target timestamp and the fingerprint window value;
retrieving a second plurality of fingerprints, each of the second plurality of fingerprints associated with a frame in a second media stream;
comparing the target fingerprint to each of the second plurality of fingerprints to determine a matching location corresponding to a location in the second media stream;
matching a first fingerprint of the plurality of fingerprints associated with a timestamp less than the target timestamp with a second fingerprint of a second plurality of fingerprints to determine a lower boundary timestamp;
matching a second fingerprint of the plurality of fingerprints associated with timestamps greater than the target timestamp with a third fingerprint of the second plurality of fingerprints to determine an upper bound timestamp; and
media segments are provided for storage in a database.
12. The system of claim 11, further configured to:
a plurality of fingerprints is stored in a database.
13. The system of claim 11, further configured to:
a second plurality of fingerprints is retrieved from the database.
14. The system of claim 11, further configured to:
receiving a third media stream comprising a third plurality of frames;
for each of the third plurality of frames:
selecting a fourth plurality of frames from the third plurality of frames based on the fingerprint window value;
hashing each of the fourth plurality of frames to create a second plurality of hashes;
truncating each hash of the second plurality of hashes to generate a second plurality of truncated hashes;
concatenating the second plurality of truncated hashes to compute a second frame fingerprint value; and
aggregating each second frame fingerprint value to assemble a second plurality of fingerprints, each second frame fingerprint value associated with a timestamp corresponding to a respective frame of the third plurality of frames.
15. The system of claim 11, further configured to:
each of the second plurality of frames is hashed using a locality sensitive hashing algorithm.
16. The system of claim 11, wherein the first media stream is the same as the second media stream.
17. The system of claim 11, further configured to:
a similarity value between the target fingerprint and each of the second plurality of fingerprints is calculated.
18. The system of claim 11, further configured to:
determining whether the similarity value is equal to or greater than a similarity threshold; and
determining a matching location corresponding to a location in the second media stream in response to the similarity value being equal to or greater than the similarity threshold.
19. The system of claim 11, further configured to:
extracting a third plurality of fingerprints from the second plurality of fingerprints based on the lower bound timestamps and the upper bound timestamps; and
the third plurality of fingerprints is stored in a database.
20. The system of claim 19, further configured to:
a third plurality of fingerprints is provided.
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2019/058175 WO2021080617A1 (en) | 2019-10-25 | 2019-10-25 | Frame-accurate automated cutting of media content by using multiple airings |
Publications (2)
Publication Number | Publication Date |
---|---|
CN113039805A true CN113039805A (en) | 2021-06-25 |
CN113039805B CN113039805B (en) | 2023-07-14 |
Family
ID=68582445
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201980004991.1A Active CN113039805B (en) | 2019-10-25 | 2019-10-25 | Media fragment extraction method and system based on fingerprint matching |
Country Status (4)
Country | Link |
---|---|
US (1) | US11328014B2 (en) |
EP (1) | EP3844967B1 (en) |
CN (1) | CN113039805B (en) |
WO (1) | WO2021080617A1 (en) |
Families Citing this family (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
GB2620431A (en) * | 2022-07-08 | 2024-01-10 | Advanced Risc Mach Ltd | Monitoring sensor data using expiring hashes |
GB2622270A (en) * | 2022-09-12 | 2024-03-13 | Sony Interactive Entertainment Europe Ltd | A system and method for identifying a cutscene |
Citations (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20020083440A1 (en) * | 2000-12-26 | 2002-06-27 | Jean-Charles Dupuis | Advertising extracting system |
US20090290764A1 (en) * | 2008-05-23 | 2009-11-26 | Fiebrink Rebecca A | System and Method for Media Fingerprint Indexing |
CN104769600A (en) * | 2012-04-03 | 2015-07-08 | 谷歌公司 | Detection of potentially copyrighted content in user-initiated live streams |
CN106022813A (en) * | 2015-03-30 | 2016-10-12 | 尼尔森（美国）有限公司 | Methods and apparatus to report reference media data to multiple data collection facilities |
CN107534796A (en) * | 2015-03-17 | 2018-01-02 | 奈飞公司 | Detect the fragment of video frequency program |
EP3264325A1 (en) * | 2016-06-27 | 2018-01-03 | Facebook, Inc. | Systems and methods for identifying matching content |
CN107852252A (en) * | 2015-06-12 | 2018-03-27 | 索伦森媒体有限公司 | Fingerprint matching is recognized by automated content to detect channel to change |
CN107851104A (en) * | 2015-04-23 | 2018-03-27 | 索伦森媒体有限公司 | Automated content identification fingerprint sequence matching |
Family Cites Families (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8196164B1 (en) | 2011-10-17 | 2012-06-05 | Google Inc. | Detecting advertisements using subtitle repetition |
US9924222B2 (en) * | 2016-02-29 | 2018-03-20 | Gracenote, Inc. | Media channel identification with multi-match detection and disambiguation based on location |
-
2019
- 2019-10-25 WO PCT/US2019/058175 patent/WO2021080617A1/en unknown
- 2019-10-25 EP EP19805027.0A patent/EP3844967B1/en active Active
- 2019-10-25 US US16/756,319 patent/US11328014B2/en active Active
- 2019-10-25 CN CN201980004991.1A patent/CN113039805B/en active Active
Patent Citations (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20020083440A1 (en) * | 2000-12-26 | 2002-06-27 | Jean-Charles Dupuis | Advertising extracting system |
US20090290764A1 (en) * | 2008-05-23 | 2009-11-26 | Fiebrink Rebecca A | System and Method for Media Fingerprint Indexing |
CN104769600A (en) * | 2012-04-03 | 2015-07-08 | 谷歌公司 | Detection of potentially copyrighted content in user-initiated live streams |
CN107609357A (en) * | 2012-04-03 | 2018-01-19 | 谷歌公司 | May copyrighted content in the live stream that detection user is initiated |
CN107534796A (en) * | 2015-03-17 | 2018-01-02 | 奈飞公司 | Detect the fragment of video frequency program |
CN106022813A (en) * | 2015-03-30 | 2016-10-12 | 尼尔森（美国）有限公司 | Methods and apparatus to report reference media data to multiple data collection facilities |
CN107851104A (en) * | 2015-04-23 | 2018-03-27 | 索伦森媒体有限公司 | Automated content identification fingerprint sequence matching |
CN107852252A (en) * | 2015-06-12 | 2018-03-27 | 索伦森媒体有限公司 | Fingerprint matching is recognized by automated content to detect channel to change |
EP3264325A1 (en) * | 2016-06-27 | 2018-01-03 | Facebook, Inc. | Systems and methods for identifying matching content |
Also Published As
Publication number | Publication date |
---|---|
CN113039805B (en) | 2023-07-14 |
US20210124777A1 (en) | 2021-04-29 |
EP3844967B1 (en) | 2023-01-25 |
EP3844967A1 (en) | 2021-07-07 |
WO2021080617A1 (en) | 2021-04-29 |
US11328014B2 (en) | 2022-05-10 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US9913001B2 (en) | System and method for generating segmented content based on related data ranking | |
US20210312186A1 (en) | Summarizing video content | |
US9740775B2 (en) | Video retrieval based on optimized selected fingerprints | |
US11350184B2 (en) | Providing advanced playback and control functionality to video client | |
CN108509611B (en) | Method and device for pushing information | |
JP6920475B2 (en) | Modify digital video content | |
US11818428B2 (en) | Identifying viewing characteristics of an audience of a content channel | |
WO2018011682A1 (en) | Method and system for real time, dynamic, adaptive and non-sequential stitching of clips of videos | |
CN113039805B (en) | Media fragment extraction method and system based on fingerprint matching | |
US10057606B2 (en) | Systems and methods for automated application of business rules using temporal metadata and content fingerprinting | |
US20170060862A1 (en) | Method and system for content retrieval based on rate-coverage optimization | |
US11669784B2 (en) | Content provider recommendations to improve targetting and other settings | |
CN113297417B (en) | Video pushing method, device, electronic equipment and storage medium | |
US20160105731A1 (en) | Systems and methods for identifying and acquiring information regarding remotely displayed video content | |
US20230259837A1 (en) | Content provider recommendations to improve targetting and other settings | |
Kim | A new framework for automatic extraction of key frames using DC Image activity | |
CN113836351A (en) | Method and device for determining homologous video clips and electronic equipment |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |