US20210374345A1 - Processing large-scale textual inputs using neural networks - Google Patents
Processing large-scale textual inputs using neural networks Download PDFInfo
- Publication number
- US20210374345A1 US20210374345A1 US17/336,093 US202117336093A US2021374345A1 US 20210374345 A1 US20210374345 A1 US 20210374345A1 US 202117336093 A US202117336093 A US 202117336093A US 2021374345 A1 US2021374345 A1 US 2021374345A1
- Authority
- US
- United States
- Prior art keywords
- sequence
- neural network
- encoder
- input
- tokens
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000013528 artificial neural network Methods 0.000 title claims abstract description 211
- 238000012545 processing Methods 0.000 title claims description 40
- 238000000034 method Methods 0.000 claims abstract description 59
- 230000008569 process Effects 0.000 claims abstract description 35
- 238000010801 machine learning Methods 0.000 claims abstract description 30
- 238000012549 training Methods 0.000 claims description 45
- 230000007246 mechanism Effects 0.000 claims description 7
- 238000004590 computer program Methods 0.000 abstract description 15
- 230000006870 function Effects 0.000 description 10
- 238000004891 communication Methods 0.000 description 5
- 230000003287 optical effect Effects 0.000 description 4
- 230000026676 system process Effects 0.000 description 4
- 230000008901 benefit Effects 0.000 description 3
- 230000003993 interaction Effects 0.000 description 3
- 230000009466 transformation Effects 0.000 description 3
- ORILYTVJVMAKLC-UHFFFAOYSA-N Adamantane Natural products C1C(C2)CC3CC1CC2C3 ORILYTVJVMAKLC-UHFFFAOYSA-N 0.000 description 2
- 238000013500 data storage Methods 0.000 description 2
- 238000010586 diagram Methods 0.000 description 2
- 238000000605 extraction Methods 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 241000009334 Singa Species 0.000 description 1
- 230000004913 activation Effects 0.000 description 1
- 230000003044 adaptive effect Effects 0.000 description 1
- 230000002457 bidirectional effect Effects 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 230000001149 cognitive effect Effects 0.000 description 1
- 238000009499 grossing Methods 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000007726 management method Methods 0.000 description 1
- 238000004519 manufacturing process Methods 0.000 description 1
- 239000011159 matrix material Substances 0.000 description 1
- 238000003058 natural language processing Methods 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 230000004044 response Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/279—Recognition of textual entities
- G06F40/284—Lexical analysis, e.g. tokenisation or collocates
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
Definitions
- This specification relates to performing a machine learning task on a tuple of input sequences using neural networks.
- Neural networks are machine learning models that employ one or more layers of nonlinear units to predict an output for a received input.
- Some neural networks include one or more hidden layers in addition to an output layer. The output of each hidden layer is used as input to the next layer in the network, i.e., the next hidden layer or the output layer.
- Each layer of the network generates an output from a received input in accordance with current values of a respective set of parameters.
- This specification describes a system implemented as computer programs on one or more computers in one or more locations that implements and trains a neural network to perform a machine learning task on a received tuple of input sequences.
- Each input sequence in turn has a respective network input at each of a plurality of input positions in an input order.
- Different input sequences can have different numbers of network inputs.
- the neural network can be configured to generate any kind of score, classification, or regression output based on the input.
- the neural network can be configured to perform a text processing task, e.g., to receive an input that includes multiple text sequences that are from one or more text documents and to process the input to generate an output for the text processing task.
- the text processing task can be a semantic text matching task, a machine reading comprehension task, a question answering task, a passage ranking task, or a key phrase extraction task.
- each input to the neural network can be a tuple of two input sequences, where a first input sequence specifies Internet resources (e.g., web pages), documents, or portions of documents and a second input sequence specifies a set of one or more words or phrases (e.g., key words, key terms, or concepts), and the output generated by the neural network for a given input tuple may be a score for the set of the one or more words or phrases, with the score representing an estimated relevance of the set of word or phrase with respect to the Internet resource, document, or document portion.
- Internet resources e.g., web pages
- documents e.g., web pages
- a second input sequence specifies a set of one or more words or phrases (e.g., key words, key terms, or concepts)
- the output generated by the neural network for a given input tuple may be a score for the set of the one or more words or phrases, with the score representing an estimated relevance of the set of word or phrase with respect to the Internet resource, document,
- each input to the neural network can be a tuple of two input sequences, where a first input sequence specifies a question (e.g., a question query issued to a search engine) and a second input sequence specifies a set of one or more text segments (e.g., Internet resources (e.g., web pages), documents, or portions of documents), and the output generated by the neural network for a given input may be a score for the set of the one or more text segments, with the score representing an estimated likelihood that the set of one or more text segments includes content that provides an answer to the question.
- a question e.g., a question query issued to a search engine
- a second input sequence specifies a set of one or more text segments (e.g., Internet resources (e.g., web pages), documents, or portions of documents)
- the output generated by the neural network for a given input may be a score for the set of the one or more text segments, with the score representing an estimated likelihood that the set of one or more text segments includes
- a computer-implemented method comprising receiving, at each of a plurality of encoder neural networks, a respective input sequence from a tuple of respective input sequences; processing, using one or more encoder network layers of each of the plurality of encoder neural networks, the respective input sequence to generate an encoded representation of the respective input sequence, the encoded representation comprising a sequence of tokens; processing, using a projection layer of each of the plurality of encoder neural networks, each of some or all of the tokens in the sequence of tokens to generate a lower-dimensional representation of the token; receiving, at a head neural network and from each of the plurality of encoder neural networks, lower-dimensional representations of a respective proper subset of the sequence of tokens generated by the encoder neural network; and processing, using the head neural network, the lower-dimensional representations to generate an output.
- the head neural network may be further configured to access the lower-dimensional representations of the respective proper subsets of the sequences of tokens generated by the encoder neural networks from a memory.
- the lower-dimensional representations of the tokens generated by different projection layers may have different dimensions from each other.
- Each input sequence may have a respective network input at each of a plurality of input positions in an input order.
- the sequence of tokens generated by the encoder neural network may comprise a corresponding token for each network input in the input sequence.
- the method may further comprise, for each sequence of tokens generated by the one or more encoder network layers of the encoder neural network from the input sequence: determining the respective proper subset of the sequence of tokens based on respective positions of the tokens in the sequence and on a length of the input sequence.
- the respective proper subset of the sequence of tokens may comprise first N tokens in the sequence of tokens, and wherein N is a predetermined positive integer.
- the one or more encoder network layers may comprise an attention layer that is configured to: receive an input sequence for the layer comprising a respective layer input at each of one or more positions; and generate an attended input sequence at least in part by applying an attention mechanism to the input sequence for the layer, the attended input sequence comprising a respective attended layer input at each of the one or more positions.
- an attention layer that is configured to: receive an input sequence for the layer comprising a respective layer input at each of one or more positions; and generate an attended input sequence at least in part by applying an attention mechanism to the input sequence for the layer, the attended input sequence comprising a respective attended layer input at each of the one or more positions.
- the machine learning task may be a semantic text matching task.
- the training may further comprise: receiving another training tuple; processing the training tuple using the trained neural network to generate another teacher network output; and training the neural network using the other teacher network output generated by the trained neural network, including adjusting parameter values of the one or more encoder network layers of the encoder neural networks.
- a computer storage medium encoded with instructions that, when executed by one or more computers, cause the one or more computers to perform the operations of the method aspect.
- the described techniques also allow for the system to process the inputs in a data efficient, and, therefore, computing resource efficient manner. Specifically, by identifying proper subsets of respective sequences of output tokens generated by the encoder neural networks and by making use of encoder-specific projection layers, the system can generate compact representations of the inputs to provide to a head neural network for generating high-quality network outputs with minimum loss of representational capacity of the information contained within the original inputs.
- FIG. 1 shows an example neural network system.
- FIG. 2 is a flow diagram of an example process for processing a tuple of input sequences to generate an output.
- FIG. 3 is an illustration of selecting tokens from encoded representations of input sequences.
- This specification describes a system implemented as computer programs on one or more computers in one or more locations that implements and trains a neural network to perform a machine learning task on a tuple of input sequences.
- Each input sequence in turn has a respective network input at each of a plurality of input positions in an input order.
- Different input sequences can have different numbers of network inputs.
- the neural network can be configured to generate any kind of score, classification, or regression output based on the tuple of input sequences.
- the neural network can be configured to perform a text processing task, e.g., to receive an input that includes multiple text sequences that are from one or more text documents and to process the input to generate an output for the text processing task.
- the text processing task can be a semantic text matching task, a machine reading comprehension task, a question answering task, a passage ranking task, or a key phrase extraction task.
- each input to the neural network can be a tuple of two input sequences, where a first input sequence specifies Internet resources (e.g., web pages), documents, or portions of documents and a second input sequence specifies a set of one or more words or phrases (e.g., key words, key terms, or concepts), and the output generated by the neural network for a given input tuple may be a score for the set of the one or more words or phrases, with the score representing an estimated relevance of the set of word or phrase with respect to the Internet resource, document, or document portion.
- Internet resources e.g., web pages
- documents e.g., web pages
- a second input sequence specifies a set of one or more words or phrases (e.g., key words, key terms, or concepts)
- the output generated by the neural network for a given input tuple may be a score for the set of the one or more words or phrases, with the score representing an estimated relevance of the set of word or phrase with respect to the Internet resource, document,
- each input to the neural network be a tuple of two input sequences, where a first input sequence specifies a question (e.g., a question query issued to a search engine) and a second input sequence specifies a set of one or more text segments (e.g., Internet resources (e.g., web pages), documents, or portions of documents), and the output generated by the neural network for a given input may be a score for the set of the one or more text segments, with the score representing an estimated likelihood that the set of one or more text segments includes content that provides an answer to the question.
- a question e.g., a question query issued to a search engine
- a second input sequence specifies a set of one or more text segments (e.g., Internet resources (e.g., web pages), documents, or portions of documents)
- the output generated by the neural network for a given input may be a score for the set of the one or more text segments, with the score representing an estimated likelihood that the set of one or more text segments includes content
- FIG. 1 shows an example neural network system 100 .
- the neural network system 100 is an example of a system implemented as computer programs on one or more computers in one or more locations, in which the systems, components, and techniques described below can be implemented.
- the neural network system 100 can receive a tuple of input sequences 102 and perform a machine learning task on the tuple of input sequences 102 to generate an output 152 for the machine learning task.
- a tuple refers to a data structure having an ordered set of two or more data elements, e.g., two or more input sequences.
- An n-tuple refers to a tuple having n ordered elements.
- a 3-tuple would include 3 elements (e.g., input sequence A, input sequence B, input sequence C) in an order ⁇ input sequence A, input sequence B, input sequence C>that is different than a 3-tuple consisting of ⁇ input sequence C, input sequence A, input sequence B>.
- the neural network system 100 includes a plurality of encoder neural networks 120 A-N that are each configured to process an input sequence from the tuple 102 , e.g., input sequence A 104 A, to generate a lower-dimensional representation, e.g., lower-dimensional representation 122 A, of the input sequence and a head neural network 130 that is configured to generate the output 152 from the lower-dimensional representations 122 A-N.
- encoder neural networks 120 A-N that are each configured to process an input sequence from the tuple 102 , e.g., input sequence A 104 A, to generate a lower-dimensional representation, e.g., lower-dimensional representation 122 A, of the input sequence and a head neural network 130 that is configured to generate the output 152 from the lower-dimensional representations 122 A-N.
- a lower-dimensional representation can be an encoded representation of an input sequence, i.e., in the form of an ordered collection of data values such as numerical values, that has a lower dimensionality than that of the data structure used to represent the input sequence.
- the lower-dimensional representation can be a vector or a matrix of fixed size.
- each encoder neural network 120 A-N can include multiple encoder layers followed by a projection layer.
- the encoder neural network A 120 A can include a stack of multiple encoder layers 110 A arranged in a predetermined order, followed by a projection layer 114 A arranged atop the stack of the multiple encoder layers 110 A.
- each of some or all of the encoder layers included in the encoder neural network can operate on a respective input sequence that includes a respective network input (e.g., in the form of a vector) at each of one or more positions in an input order.
- a respective network input e.g., in the form of a vector
- the neural network system 100 uses the encoder layers included in the encoder neural network, e.g., encoder layers 110 A, to process an input sequence, e.g., input sequence A 104 A, data derived from the input sequence, or both to generate an encoded representation of the input sequence.
- the encoded representation has a sequence of multiple tokens, e.g., tokens 112 A.
- the neural network system 100 can use the encoder network layers 110 A to generate a corresponding token for each network input in the input sequence 104 A.
- the encoded representation is the output of the last encoder layer prior to the projection layer or a combination of the outputs of multiple encoder layers.
- a token refers to a portion of the encoded representation which, as described above, can be in the form of an ordered collection of numerical values.
- each token can include one or more numerical values.
- Each token can be of substantially similar length to one another.
- the neural network system 100 uses the projection layer, e.g., projection layer 114 A, to project the sequence of tokens into a lower-dimensional space, i.e., to generate the lower-dimensional representation, e.g., lower-dimensional representation 122 A, of the sequence of tokens, e.g., tokens 112 A, e.g., by applying a predetermined linear transformation.
- the projection layer e.g., projection layer 114 A
- the neural network system 100 uses a truncation technique to generate the lower-dimensional representations. That is, instead of projecting the entire sequences of tokens into the lower-dimensional space, the system 100 first determines a selected portion of each encoded representation generated by corresponding stacks of encoder layers 110 A-N, and then provides only the selected tokens from the encoded representations to the projection layers 114 A-N. Correspondingly, the neural network system 100 projects, i.e., by using the projection layers 114 A-N, the selected smaller subsets of tokens into the lower-dimensional space.
- the selected portion of each encoded representation can include the N first (or last) tokens of the sequence of tokens generated by the encoder layers, where N is a configurable parameter of the neural network system 100 .
- N can be a positive integer the exact value of which may vary between different encoder neural networks 120 A-N.
- the parameter can be a tunable parameter that can be specified, e.g., from a user of the system, e.g., using an application programming interface (API) made available by the system 100 .
- API application programming interface
- the parameter can be a dynamic parameter the value of which is determined by the system from the lengths of the input sequences while performing the given machine learning task.
- each encoder neural network 120 A-N includes one or more attention layers. That is, the multiple encoder network layers, e.g., encoder layers 110 A, include at least one attention layer that is configured to receive an input sequence for the layer comprising a respective layer input at each of one or more positions, and thereafter generate an attended input sequence at least in part by applying an attention mechanism to the input sequence for the layer.
- the attended input sequence includes a respective attended layer input at each of the one or more positions.
- each encoder neural network 120 A-N also includes other layers, e.g., fully-connected layers, embedding layers, and activation layers, either in place of or in addition to the attention layers.
- the encoder network layers are the layers of a self-attention neural network.
- Examples of configurations of self-attention neural networks and the specifics of the other components of self-attention neural networks, e.g., embedding layers that embed inputs to the encoder and the decoder, the feed-forward layers within the layers of the attention network, and the output layers of the attention neural network that generate the network outputs, are described in more detail in Vaswani, et al, Attention Is All You Need, arXiv:1706.03762, Raffel, et al, and Devlin et al, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, arXiv:1810.04805, the entire contents of which are hereby incorporated by reference herein in their entirety.
- the neural network system 100 includes a plurality of encoder neural networks, e.g., encoder neural networks 120 A-N, each configured to process a respective input sequence, e.g., input sequence 104 A-N.
- the encoder neural networks need not have a one-to-one correspondence with the input sequences and there may be a different number of encoder neural networks.
- the system 100 may use the same encoder neural network to process different input sequences from the same received tuple 102 to generate different lower-dimensional representations, and thus there may be a smaller number of encoder neural networks than that of input sequences included in the tuple.
- each encoder neural network may have a different network architecture than one another.
- the encoder neural networks may include different numbers of encoder layers, encoder layers with different configurations, or both.
- the neural network system 100 allows the head neural network 130 to generate the output 152 for the given machine learning task by processing a much more compact (and therefore, more data-efficient) representation of the tuple of input sequences 102 with minimum loss of representational capacity of the information contained within the original tuple 102 .
- the neural network system 100 can thus operate in a scalable manner to determine an output 152 from a tuple 102 of a substantially large number of input sequences 104 A-N.
- the tuple can include multiple input sequences representing billions and, possibly, trillions of documents, web pages, or other structured text content, and the output can be an answer string to a user-specified question that is determined by the system from the context of the documents or web pages.
- the neural network system 100 uses the head neural network 130 to generate the output 152 from the lower-dimensional representations 122 A-N generated by the encoder neural networks 120 A-N.
- the head neural network 130 can be configured to receive as input a combination, e.g., a vector concatenation, of the lower-dimensional representations 122 A-N and to process the combined input using the head neural network 130 to generate the output 152 .
- the head neural network 130 can include any of a variety of types of neural network layers that are suitable for the given machine learning task, including, for example, one or more fully-connected layers, one or more attention layers, and/or one or more embedding layers. In the case of multiple layers, they may be stacked, so as to pass data successively between them in a certain layer order.
- the head neural network 130 also includes an output layer that is configured to receive the data generated by one or more preceding layers and to generate the output 152 , e.g., by applying a transformation to the received data to generate a regression or classification output that includes a respective score for each of some or all of the input sequences in the tuple, e.g., with each score for an input sequence representing a relevance measure or a likelihood of being relevant with respect to another input sequence in the tuple.
- FIG. 2 is a flow diagram of an example process 200 for processing a tuple of input sequences to generate an output.
- the process 200 will be described as being performed by a system of one or more computers located in one or more locations.
- a neural network system e.g., neural network system 100 of FIG. 1 , appropriately programmed in accordance with this specification, can perform the process 200 .
- the system receives, at each of a plurality of encoder neural networks, a respective input sequence from a tuple of respective input sequences ( 202 ).
- Each input sequence includes a respective network input at each of multiple positions in an input order.
- the lengths, i.e., numbers of network inputs, of different input sequences within a same tuple may vary from one another.
- the system processes, using one or more encoder layers of each of the plurality of encoder neural networks, the respective input sequence to generate an encoded representation of the respective input sequence ( 204 ).
- the encoded representation can be a sequence of the multiple tokens that is represented, for example, as a vector or other ordered collection of multiple numeric values, where each token can include one or more numerical values.
- the encoder neural network including multiple encoder layers, they may be stacked, so as to perform successive operations on the respective input sequence to generate the encoded representation, i.e., in accordance with the configurations and associated parameter values of the encoder layers.
- the plurality of encoder neural networks can have the same architecture. That is, the configurations of and connections between the encoder layers within each encoder neural network are the same across all encoder neural networks.
- different encoder neural networks can have encoder layers that are of different configurations, different connections, or both.
- different encoder neural networks can have different numbers of encoder layers. In either implementation, the values of the parameters associated with the encoder layers, which may be learned by the system during training, are typically different across different encoder neural networks.
- the encoder layers of encoder neural networks include at least one attention layer and, optionally, one feed-forward layer.
- the attention layer is configured to receive an input sequence for the layer comprising a respective layer input at each of one or more positions, and thereafter generate an attended input sequence at least in part by applying an attention mechanism, e.g., a self-attention mechanism, e.g., a multi-head self-attention mechanism, to the input sequence for the layer.
- the attended input sequence includes a respective attended layer input at each of the one or more positions.
- the feed-forward layer when included, then operates on the attended input sequence to generate an output sequence for the layer, from which the encoded representation may be determined or otherwise derived.
- the system processes, using a projection layer of each of the plurality of encoder neural networks, each of some or all of the tokens in the sequence of tokens to generate a lower-dimensional representation of the token ( 206 ).
- the projection layer can apply a predetermined linear transformation to a token in order to project the token into a lower-dimensional space.
- the system can generate the lower-dimensional representations of the input sequences by using the projection layer of each encoder neural network to project all of the tokens included in the encoded representations of the input sequences into the lower-dimensional space.
- the system can instead select a proper subset of tokens included in each encoded representation and thereafter use the projection layers to project the selected proper subsets of the tokens to generate the lower-dimensional representations of the input sequences.
- the system makes a respective determination of which proper subset to select for each of the tokens in the encoded representation and sometimes selects proper subsets of the tokens that are of different sizes from different encoded representations.
- the lower-dimensional representations of the subset of tokens generated by different projection layers across different encoder neural networks can have different dimensions from one another.
- the proper subsets of tokens may be selected in any of a variety of ways.
- FIG. 3 is an illustration of selecting tokens from encoded representations of input sequences.
- the system can determine a respective proper subset of the sequence of tokens based on respective positions of the tokens in the sequence, for example selecting the first few tokens or the last few tokens from a sequence of tokens.
- the system can also determine a respective proper subset of the sequence of tokens based on a length of the input sequence, for example selecting more tokens from encoded representations generated from longer input sequences.
- the system can select the first N or last N tokens, where N is a fixed fraction of the total number of tokens in the sequence.
- the system receives, at a head neural network and from each of the plurality of encoder neural networks, lower-dimensional representations of a respective proper subset of the sequence of tokens generated by the encoder neural network ( 208 ).
- the encoder neural networks and the head neural network share access to the same memory or a data storage that is accessible to the system.
- the system can store the lower-dimensional representations of the respective proper subsets of the sequences of tokens generated by the encoder neural networks in memory or data storage accessible to the head neural network, e.g., in addition to or instead of directly providing these representations to the head neural network, e.g., through a wired or wireless network. The system can then retrieve these representations whenever an output needs to be generated by using the head neural network based on processing some or all of these stored, i.e., pre-computed, representations.
- this can allow for the system to perform a given machine learning task with reduced inference time.
- this can be further advantageous in cases where there are more possible combinations than the number of distinct input sequences, because a lower-dimensional representation for the same input sequence need not be regenerated as the system processes different tuples of input sequences.
- the given task is to predict the relevance between a query and a document.
- the system can generate and store, e.g., prior to receiving a user input to begin performing the task, respective lower-dimensional representations for all available input sequences to the system which may include millions of queries and millions of documents and, when at inference time, use the lightweight head neural network to efficiently process different pairs of pre-stored lower-dimensional representations to generate as output a relevance score for each different query-document pair.
- the system processes, using the head neural network, the lower-dimensional representations to generate an output ( 210 ).
- the system can generate a combined, e.g., concatenated, input for the head neural network from the lower-dimensional representations of the respective small proper subsets of tokens that have been generated as a result of processing the input sequences using the encoder neural networks.
- the system then uses the head neural network to process the combined input to generate the network output.
- the output can be any kind of score, classification, or regression output based on the tuple of input sequences.
- the process 200 can be performed as part of predicting an output for a tuple of multiple input sequences for which the desired output, i.e., the output that should be generated by the system for the tuple of multiple input sequences, is not known.
- the process 200 can also be performed as part of processing tuples of input sequences derived from a set of training data, i.e., tuples of input sequences derived from a set of inputs for which the output that should be generated by the system is known, in order to train the encoder neural networks and the head neural network to determine trained values for the parameters of the neural networks, so that the system can summarize the information of the entire input sequence to selected tokens of the encoded representations of the input sequence and generate accurate output scores. Specifically, the system can do this by optimizing an objective function that is specific to the given machine learning task.
- the objective function may vary across different tasks, but typically, the objective function measures a difference between the predicted output and the known, desired output or another target output that is derived from the known, desired output.
- a cross-entropy loss function e.g., in the case of classification tasks
- MSE mean squared error
- the system can repeatedly perform the process 200 on inputs selected from a set of training data as part of a conventional machine learning training technique to train the initial neural network layers, e.g., a gradient descent with backpropagation training technique that uses a conventional optimizer, e.g., stochastic gradient descent, RMSprop, or Adam optimizer, including Adam with weight decay (“AdamW”) optimizer.
- a conventional optimizer e.g., stochastic gradient descent, RMSprop, or Adam optimizer, including Adam with weight decay (“AdamW”) optimizer.
- the system can incorporate any number of techniques to improve the speed, the effectiveness, or both of the training process.
- the system can use dropout, label smoothing, or both to reduce overfitting.
- the system can perform the training using a distributed architecture that trains multiple instances of the encoder neural networks in parallel.
- the system can initialize a portion of the parameters of the encoder neural networks in accordance with a predetermined set of parameter values, rather than randomly initialized values. This can improve the overall training effectiveness in terms of required computational resources.
- the system can initialize parameter values of the one or more encoder network layers of each encoder neural network with trained values of parameters of another, pre-trained neural network.
- the other neural network can be a self-attention neural network that has already been trained to attain at least a threshold level of performance (e.g., accuracy) on a relevant machine learning task, e.g., a natural language processing or understanding task that involves operating on textual data, information derived from textual data, or both.
- the system can obtain an instance of the neural network by first instantiating the encoder neural networks according to the architecture and trained parameter values of (a portion of) the other self-attention neural network, and then attaching the projection layers and the head neural network to the encoder network layers included in the encoder neural networks that have been instantiated in this way.
- the system can then proceed to train the obtained neural network on the given machine learning task as described above.
- the system makes use of a teacher neural network during the training.
- the teacher neural network can be a specialist neural network with a cumbersome architecture (e.g., with more layers, more parameters, or both) that has already been trained to attain at least a threshold level of performance on the same given machine learning task as the system is configured to perform.
- the system first processes the training input using a trained neural network to generate a teacher network output, and then trains the encoder and head neural networks using the teacher network output generated by the teacher neural network, i.e., trains the head neural network to generate a training output for the training input that match the teacher network output.
- the system can do this by optimizing a cross-entropy loss function:
- y i is the training output generated by the neural network and p i may be computed by applying a sigmoid function on the teacher network output which is in the form of logits.
- the system trains the neural networks using a two-stage process.
- the system adjusts only parameter values of the projection layers of the encoder neural networks and parameter values of the head neural network, while keeping the parameter values of the one or more encoder network layers fixed to their values that have been randomly initialized or otherwise predetermined.
- the system adjusts values of all of the network parameters, including parameter values of the one or more encoder network layers of the encoder neural networks.
- Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly-embodied computer software or firmware, in computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible non transitory storage medium for execution by, or to control the operation of, data processing apparatus.
- the computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them.
- the program instructions can be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.
- data processing apparatus refers to data processing hardware and encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers.
- the apparatus can also be, or further include, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- the apparatus can optionally include, in addition to hardware, code that creates an execution environment for computer programs, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- a computer program which may also be referred to or described as a program, software, a software application, an app, a module, a software module, a script, or code, can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages; and it can be deployed in any form, including as a stand alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.
- a program may, but need not, correspond to a file in a file system.
- a program can be stored in a portion of a file that holds other programs or data, e.g., one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, e.g., files that store one or more modules, sub programs, or portions of code.
- a computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a data communication network.
- the term “database” is used broadly to refer to any collection of data: the data does not need to be structured in any particular way, or structured at all, and it can be stored on storage devices in one or more locations.
- the index database can include multiple collections of data, each of which may be organized and accessed differently.
- engine is used broadly to refer to a software-based system, subsystem, or process that is programmed to perform one or more specific functions.
- an engine will be implemented as one or more software modules or components, installed on one or more computers in one or more locations. In some cases, one or more computers will be dedicated to a particular engine; in other cases, multiple engines can be installed and running on the same computer or computers.
- the processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows can also be performed by special purpose logic circuitry, e.g., an FPGA or an ASIC, or by a combination of special purpose logic circuitry and one or more programmed computers.
- Computers suitable for the execution of a computer program can be based on general or special purpose microprocessors or both, or any other kind of central processing unit.
- a central processing unit will receive instructions and data from a read only memory or a random access memory or both.
- the essential elements of a computer are a central processing unit for performing or executing instructions and one or more memory devices for storing instructions and data.
- the central processing unit and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks. However, a computer need not have such devices.
- Computer readable media suitable for storing computer program instructions and data include all forms of non volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks.
- semiconductor memory devices e.g., EPROM, EEPROM, and flash memory devices
- magnetic disks e.g., internal hard disks or removable disks
- magneto optical disks e.g., CD ROM and DVD-ROM disks.
- embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.
- Data processing apparatus for implementing machine learning models can also include, for example, special-purpose hardware accelerator units for processing common and compute-intensive parts of machine learning training or production, i.e., inference, workloads.
- Machine learning models can be implemented and deployed using a machine learning framework, e.g., a TensorFlow framework, a Microsoft Cognitive Toolkit framework, an Apache Singa framework, or an Apache MXNet framework.
- a machine learning framework e.g., a TensorFlow framework, a Microsoft Cognitive Toolkit framework, an Apache Singa framework, or an Apache MXNet framework.
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface, a web browser, or an app through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back end, middleware, or front end components.
- the components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (LAN) and a wide area network (WAN), e.g., the Internet.
- LAN local area network
- WAN wide area network
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- a server transmits data, e.g., an HTML page, to a user device, e.g., for purposes of displaying data to and receiving user input from a user interacting with the device, which acts as a client.
- Data generated at the user device e.g., a result of the user interaction, can be received at the server from the device.
Abstract
Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for performing a machine learning task on a tuple of respective input sequences to generate an output. In one aspect, one of the systems includes a neural network comprising a plurality of encoder neural networks and a head neural network, each encoder neural network configured to: receive a respective input sequence from the tuple; process the respective input sequence using one or more encoder network layers to generate an encoded representation comprising a sequence of tokens; and process each of some or all of the tokens in the sequence of tokens using a projection layer to generate a lower-dimensional representation, and the head neural network configured to: receive lower-dimensional representations of a respective proper subset of the sequence of tokens generated by the encoder neural network; and process the lower-dimensional representations to generate the output.
Description
- This application claims the benefit of the filing date of U.S. Application No. 63/032,996, filed on Jun. 1, 2020. The disclosure of the prior application is considered part of and is incorporated by reference in the disclosure of this application.
- This specification relates to performing a machine learning task on a tuple of input sequences using neural networks.
- Neural networks are machine learning models that employ one or more layers of nonlinear units to predict an output for a received input. Some neural networks include one or more hidden layers in addition to an output layer. The output of each hidden layer is used as input to the next layer in the network, i.e., the next hidden layer or the output layer. Each layer of the network generates an output from a received input in accordance with current values of a respective set of parameters.
- This specification describes a system implemented as computer programs on one or more computers in one or more locations that implements and trains a neural network to perform a machine learning task on a received tuple of input sequences. Each input sequence in turn has a respective network input at each of a plurality of input positions in an input order. Different input sequences can have different numbers of network inputs. Depending on the specifics of different machine learning tasks, the neural network can be configured to generate any kind of score, classification, or regression output based on the input.
- For example, the neural network can be configured to perform a text processing task, e.g., to receive an input that includes multiple text sequences that are from one or more text documents and to process the input to generate an output for the text processing task. For example, the text processing task can be a semantic text matching task, a machine reading comprehension task, a question answering task, a passage ranking task, or a key phrase extraction task.
- For example, each input to the neural network can be a tuple of two input sequences, where a first input sequence specifies Internet resources (e.g., web pages), documents, or portions of documents and a second input sequence specifies a set of one or more words or phrases (e.g., key words, key terms, or concepts), and the output generated by the neural network for a given input tuple may be a score for the set of the one or more words or phrases, with the score representing an estimated relevance of the set of word or phrase with respect to the Internet resource, document, or document portion.
- As another example, each input to the neural network can be a tuple of two input sequences, where a first input sequence specifies a question (e.g., a question query issued to a search engine) and a second input sequence specifies a set of one or more text segments (e.g., Internet resources (e.g., web pages), documents, or portions of documents), and the output generated by the neural network for a given input may be a score for the set of the one or more text segments, with the score representing an estimated likelihood that the set of one or more text segments includes content that provides an answer to the question.
- According to an aspect, there is provided a computer-implemented method comprising receiving, at each of a plurality of encoder neural networks, a respective input sequence from a tuple of respective input sequences; processing, using one or more encoder network layers of each of the plurality of encoder neural networks, the respective input sequence to generate an encoded representation of the respective input sequence, the encoded representation comprising a sequence of tokens; processing, using a projection layer of each of the plurality of encoder neural networks, each of some or all of the tokens in the sequence of tokens to generate a lower-dimensional representation of the token; receiving, at a head neural network and from each of the plurality of encoder neural networks, lower-dimensional representations of a respective proper subset of the sequence of tokens generated by the encoder neural network; and processing, using the head neural network, the lower-dimensional representations to generate an output.
- The head neural network may be further configured to access the lower-dimensional representations of the respective proper subsets of the sequences of tokens generated by the encoder neural networks from a memory.
- The lower-dimensional representations of the tokens generated by different projection layers may have different dimensions from each other.
- Each input sequence may have a respective network input at each of a plurality of input positions in an input order.
- The sequence of tokens generated by the encoder neural network may comprise a corresponding token for each network input in the input sequence.
- The method may further comprise, for each sequence of tokens generated by the one or more encoder network layers of the encoder neural network from the input sequence: determining the respective proper subset of the sequence of tokens based on respective positions of the tokens in the sequence and on a length of the input sequence.
- The respective proper subset of the sequence of tokens may comprise first N tokens in the sequence of tokens, and wherein N is a predetermined positive integer.
- The one or more encoder network layers may comprise an attention layer that is configured to: receive an input sequence for the layer comprising a respective layer input at each of one or more positions; and generate an attended input sequence at least in part by applying an attention mechanism to the input sequence for the layer, the attended input sequence comprising a respective attended layer input at each of the one or more positions.
- The machine learning task may be a semantic text matching task.
- The method may further comprise training the plurality of encoder neural networks and the head neural network including initializing parameter values of the one or more encoder network layers of each encoder neural network with a predetermined set of parameter values.
- The training may further comprise: receiving a training tuple; processing the training tuple using a trained neural network to generate a teacher network output; and training the neural network using the teacher network output generated by the trained neural network, wherein the training comprises adjusting only parameter values of the projection layers of the encoder neural networks and parameter values of the head neural network.
- The training may further comprise: receiving another training tuple; processing the training tuple using the trained neural network to generate another teacher network output; and training the neural network using the other teacher network output generated by the trained neural network, including adjusting parameter values of the one or more encoder network layers of the encoder neural networks.
- According to another aspect, there is provided a system comprising one or more computers and one or more storage devices storing instructions that are operable, when executed by the one or more computers, to cause the one or more computers to perform the operations of the above method aspect.
- According to a further aspect, there is provided a computer storage medium encoded with instructions that, when executed by one or more computers, cause the one or more computers to perform the operations of the method aspect.
- It will be appreciated that features described in the context of one aspect may be combined with features described in the context of another aspect.
- Particular embodiments of the subject matter described in this specification can be implemented so as to realize one or more of the following advantages.
- The described techniques allow for a system to implement a neural network with a flexible and adaptive architecture that is scalable for processing long-range input sequences. In particular, by implementing different numbers of encoder neural networks that are each configured to process a respective input sequence in parallel with each other, the system can use the neural network to process an arbitrary number of input sequences each of an arbitrary length and thereby endow the neural network with the capability of effectively performing any of a variety of appropriate machine learning tasks that involve operating on large-scale textual inputs, data derived from large-scale textual inputs, or both.
- The described techniques also allow for the system to process the inputs in a data efficient, and, therefore, computing resource efficient manner. Specifically, by identifying proper subsets of respective sequences of output tokens generated by the encoder neural networks and by making use of encoder-specific projection layers, the system can generate compact representations of the inputs to provide to a head neural network for generating high-quality network outputs with minimum loss of representational capacity of the information contained within the original inputs.
- The details of one or more embodiments of the subject matter of this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
-
FIG. 1 shows an example neural network system. -
FIG. 2 is a flow diagram of an example process for processing a tuple of input sequences to generate an output. -
FIG. 3 is an illustration of selecting tokens from encoded representations of input sequences. - Like reference numbers and designations in the various drawings indicate like elements.
- This specification describes a system implemented as computer programs on one or more computers in one or more locations that implements and trains a neural network to perform a machine learning task on a tuple of input sequences. Each input sequence in turn has a respective network input at each of a plurality of input positions in an input order. Different input sequences can have different numbers of network inputs. Depending on the specifics of different machine learning tasks, the neural network can be configured to generate any kind of score, classification, or regression output based on the tuple of input sequences.
- For example, the neural network can be configured to perform a text processing task, e.g., to receive an input that includes multiple text sequences that are from one or more text documents and to process the input to generate an output for the text processing task. For example, the text processing task can be a semantic text matching task, a machine reading comprehension task, a question answering task, a passage ranking task, or a key phrase extraction task.
- For example, each input to the neural network can be a tuple of two input sequences, where a first input sequence specifies Internet resources (e.g., web pages), documents, or portions of documents and a second input sequence specifies a set of one or more words or phrases (e.g., key words, key terms, or concepts), and the output generated by the neural network for a given input tuple may be a score for the set of the one or more words or phrases, with the score representing an estimated relevance of the set of word or phrase with respect to the Internet resource, document, or document portion.
- As another example, each input to the neural network be a tuple of two input sequences, where a first input sequence specifies a question (e.g., a question query issued to a search engine) and a second input sequence specifies a set of one or more text segments (e.g., Internet resources (e.g., web pages), documents, or portions of documents), and the output generated by the neural network for a given input may be a score for the set of the one or more text segments, with the score representing an estimated likelihood that the set of one or more text segments includes content that provides an answer to the question.
-
FIG. 1 shows an exampleneural network system 100. Theneural network system 100 is an example of a system implemented as computer programs on one or more computers in one or more locations, in which the systems, components, and techniques described below can be implemented. - The
neural network system 100 can receive a tuple ofinput sequences 102 and perform a machine learning task on the tuple ofinput sequences 102 to generate anoutput 152 for the machine learning task. - As used herein, a tuple refers to a data structure having an ordered set of two or more data elements, e.g., two or more input sequences. An n-tuple refers to a tuple having n ordered elements. For example, a 3-tuple would include 3 elements (e.g., input sequence A, input sequence B, input sequence C) in an order<input sequence A, input sequence B, input sequence C>that is different than a 3-tuple consisting of <input sequence C, input sequence A, input sequence B>.
- The
neural network system 100 includes a plurality of encoderneural networks 120A-N that are each configured to process an input sequence from thetuple 102, e.g.,input sequence A 104A, to generate a lower-dimensional representation, e.g., lower-dimensional representation 122A, of the input sequence and a headneural network 130 that is configured to generate theoutput 152 from the lower-dimensional representations 122A-N. - As used herein, a lower-dimensional representation can be an encoded representation of an input sequence, i.e., in the form of an ordered collection of data values such as numerical values, that has a lower dimensionality than that of the data structure used to represent the input sequence. For example, the lower-dimensional representation can be a vector or a matrix of fixed size.
- To generate the lower-dimensional representation of the input sequence, each encoder
neural network 120A-N can include multiple encoder layers followed by a projection layer. For example, the encoderneural network A 120A can include a stack ofmultiple encoder layers 110A arranged in a predetermined order, followed by aprojection layer 114A arranged atop the stack of themultiple encoder layers 110A. - Example configurations of the encoder layers will be described in more detail below, but typically, each of some or all of the encoder layers included in the encoder neural network can operate on a respective input sequence that includes a respective network input (e.g., in the form of a vector) at each of one or more positions in an input order.
- At a high level, at each encoder neural network, e.g., encoder neural network A 120A, the
neural network system 100 uses the encoder layers included in the encoder neural network, e.g., encoder layers 110A, to process an input sequence, e.g.,input sequence A 104A, data derived from the input sequence, or both to generate an encoded representation of the input sequence. The encoded representation has a sequence of multiple tokens, e.g.,tokens 112A. For example, theneural network system 100 can use the encoder network layers 110A to generate a corresponding token for each network input in theinput sequence 104A. Typically, the encoded representation is the output of the last encoder layer prior to the projection layer or a combination of the outputs of multiple encoder layers. - As used herein, a token refers to a portion of the encoded representation which, as described above, can be in the form of an ordered collection of numerical values. For example, each token can include one or more numerical values. Each token can be of substantially similar length to one another.
- The
neural network system 100 then uses the projection layer, e.g.,projection layer 114A, to project the sequence of tokens into a lower-dimensional space, i.e., to generate the lower-dimensional representation, e.g., lower-dimensional representation 122A, of the sequence of tokens, e.g.,tokens 112A, e.g., by applying a predetermined linear transformation. - In some implementations, the
neural network system 100 uses a truncation technique to generate the lower-dimensional representations. That is, instead of projecting the entire sequences of tokens into the lower-dimensional space, thesystem 100 first determines a selected portion of each encoded representation generated by corresponding stacks ofencoder layers 110A-N, and then provides only the selected tokens from the encoded representations to the projection layers 114A-N. Correspondingly, theneural network system 100 projects, i.e., by using the projection layers 114A-N, the selected smaller subsets of tokens into the lower-dimensional space. This can decrease runtime latency of theneural network system 100 for performing the given machine learning task, because the amount of information (i.e., in terms of input sequence length) to be consumed and processed by the headneural network 130 is reduced and thus the time complexity of the headneural network 130 is reduced. - When represented in the form of a data structure of fixed size, e.g., a vector, the selected portion of each encoded representation can include the N first (or last) tokens of the sequence of tokens generated by the encoder layers, where N is a configurable parameter of the
neural network system 100. N can be a positive integer the exact value of which may vary between different encoderneural networks 120A-N. For example, the parameter can be a tunable parameter that can be specified, e.g., from a user of the system, e.g., using an application programming interface (API) made available by thesystem 100. As another example, the parameter can be a dynamic parameter the value of which is determined by the system from the lengths of the input sequences while performing the given machine learning task. - In some implementations, each encoder
neural network 120A-N includes one or more attention layers. That is, the multiple encoder network layers, e.g., encoder layers 110A, include at least one attention layer that is configured to receive an input sequence for the layer comprising a respective layer input at each of one or more positions, and thereafter generate an attended input sequence at least in part by applying an attention mechanism to the input sequence for the layer. The attended input sequence includes a respective attended layer input at each of the one or more positions. - In some such implementations, each encoder
neural network 120A-N also includes other layers, e.g., fully-connected layers, embedding layers, and activation layers, either in place of or in addition to the attention layers. - In some such implementations, the encoder network layers are the layers of a self-attention neural network. Examples of configurations of self-attention neural networks and the specifics of the other components of self-attention neural networks, e.g., embedding layers that embed inputs to the encoder and the decoder, the feed-forward layers within the layers of the attention network, and the output layers of the attention neural network that generate the network outputs, are described in more detail in Vaswani, et al, Attention Is All You Need, arXiv:1706.03762, Raffel, et al, and Devlin et al, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, arXiv:1810.04805, the entire contents of which are hereby incorporated by reference herein in their entirety.
- In the example of
FIG. 1 , theneural network system 100 includes a plurality of encoder neural networks, e.g., encoderneural networks 120A-N, each configured to process a respective input sequence, e.g.,input sequence 104A-N. However, the encoder neural networks need not have a one-to-one correspondence with the input sequences and there may be a different number of encoder neural networks. For example, thesystem 100 may use the same encoder neural network to process different input sequences from the same receivedtuple 102 to generate different lower-dimensional representations, and thus there may be a smaller number of encoder neural networks than that of input sequences included in the tuple. In addition, each encoder neural network may have a different network architecture than one another. For example, the encoder neural networks may include different numbers of encoder layers, encoder layers with different configurations, or both. - By generating lower-
dimensional representations 122A-N as described above, theneural network system 100 allows the headneural network 130 to generate theoutput 152 for the given machine learning task by processing a much more compact (and therefore, more data-efficient) representation of the tuple ofinput sequences 102 with minimum loss of representational capacity of the information contained within theoriginal tuple 102. Theneural network system 100 can thus operate in a scalable manner to determine anoutput 152 from atuple 102 of a substantially large number ofinput sequences 104A-N. For example, the tuple can include multiple input sequences representing billions and, possibly, trillions of documents, web pages, or other structured text content, and the output can be an answer string to a user-specified question that is determined by the system from the context of the documents or web pages. - The
neural network system 100 then uses the headneural network 130 to generate theoutput 152 from the lower-dimensional representations 122A-N generated by the encoderneural networks 120A-N. For example, the headneural network 130 can be configured to receive as input a combination, e.g., a vector concatenation, of the lower-dimensional representations 122A-N and to process the combined input using the headneural network 130 to generate theoutput 152. - As similarly described above, the head
neural network 130 can include any of a variety of types of neural network layers that are suitable for the given machine learning task, including, for example, one or more fully-connected layers, one or more attention layers, and/or one or more embedding layers. In the case of multiple layers, they may be stacked, so as to pass data successively between them in a certain layer order. The headneural network 130 also includes an output layer that is configured to receive the data generated by one or more preceding layers and to generate theoutput 152, e.g., by applying a transformation to the received data to generate a regression or classification output that includes a respective score for each of some or all of the input sequences in the tuple, e.g., with each score for an input sequence representing a relevance measure or a likelihood of being relevant with respect to another input sequence in the tuple. -
FIG. 2 is a flow diagram of anexample process 200 for processing a tuple of input sequences to generate an output. For convenience, theprocess 200 will be described as being performed by a system of one or more computers located in one or more locations. For example, a neural network system, e.g.,neural network system 100 ofFIG. 1 , appropriately programmed in accordance with this specification, can perform theprocess 200. - The system receives, at each of a plurality of encoder neural networks, a respective input sequence from a tuple of respective input sequences (202). Each input sequence includes a respective network input at each of multiple positions in an input order. The lengths, i.e., numbers of network inputs, of different input sequences within a same tuple may vary from one another.
- The system processes, using one or more encoder layers of each of the plurality of encoder neural networks, the respective input sequence to generate an encoded representation of the respective input sequence (204). Generally, the encoded representation can be a sequence of the multiple tokens that is represented, for example, as a vector or other ordered collection of multiple numeric values, where each token can include one or more numerical values.
- In the case of the encoder neural network including multiple encoder layers, they may be stacked, so as to perform successive operations on the respective input sequence to generate the encoded representation, i.e., in accordance with the configurations and associated parameter values of the encoder layers.
- In some implementations, the plurality of encoder neural networks can have the same architecture. That is, the configurations of and connections between the encoder layers within each encoder neural network are the same across all encoder neural networks. In other implementations, different encoder neural networks can have encoder layers that are of different configurations, different connections, or both. In addition, different encoder neural networks can have different numbers of encoder layers. In either implementation, the values of the parameters associated with the encoder layers, which may be learned by the system during training, are typically different across different encoder neural networks.
- In some implementations, the encoder layers of encoder neural networks include at least one attention layer and, optionally, one feed-forward layer. The attention layer is configured to receive an input sequence for the layer comprising a respective layer input at each of one or more positions, and thereafter generate an attended input sequence at least in part by applying an attention mechanism, e.g., a self-attention mechanism, e.g., a multi-head self-attention mechanism, to the input sequence for the layer. The attended input sequence includes a respective attended layer input at each of the one or more positions. The feed-forward layer, when included, then operates on the attended input sequence to generate an output sequence for the layer, from which the encoded representation may be determined or otherwise derived.
- The system processes, using a projection layer of each of the plurality of encoder neural networks, each of some or all of the tokens in the sequence of tokens to generate a lower-dimensional representation of the token (206). For example, the projection layer can apply a predetermined linear transformation to a token in order to project the token into a lower-dimensional space.
- In some implementations, the system can generate the lower-dimensional representations of the input sequences by using the projection layer of each encoder neural network to project all of the tokens included in the encoded representations of the input sequences into the lower-dimensional space.
- In other implementations, especially those that involve operating on long-length input sequences, the system can instead select a proper subset of tokens included in each encoded representation and thereafter use the projection layers to project the selected proper subsets of the tokens to generate the lower-dimensional representations of the input sequences. In other words, the system makes a respective determination of which proper subset to select for each of the tokens in the encoded representation and sometimes selects proper subsets of the tokens that are of different sizes from different encoded representations. Correspondingly, the lower-dimensional representations of the subset of tokens generated by different projection layers across different encoder neural networks can have different dimensions from one another.
- In these implementations, the proper subsets of tokens may be selected in any of a variety of ways.
-
FIG. 3 is an illustration of selecting tokens from encoded representations of input sequences. In the example ofFIG. 3 , for each sequence of tokens generated by the one or more encoder network layers of the encoder neural network from the input sequence, the system can determine a respective proper subset of the sequence of tokens based on respective positions of the tokens in the sequence, for example selecting the first few tokens or the last few tokens from a sequence of tokens. The system can also determine a respective proper subset of the sequence of tokens based on a length of the input sequence, for example selecting more tokens from encoded representations generated from longer input sequences. As a particular example, the system can select the first N or last N tokens, where N is a fixed fraction of the total number of tokens in the sequence. - As depicted in
FIG. 3 , for encoder neural network A, the system selects N=2 leftmost tokens in the sequence of tokens generated by the encoder neural network A from processing input sequence A. For encoder neural network B, the system selects M=3 leftmost tokens in the sequence of tokens generated by the encoder neural network B from processing input sequence B. - The system receives, at a head neural network and from each of the plurality of encoder neural networks, lower-dimensional representations of a respective proper subset of the sequence of tokens generated by the encoder neural network (208).
- In some implementations, the encoder neural networks and the head neural network share access to the same memory or a data storage that is accessible to the system. In these implementations, the system can store the lower-dimensional representations of the respective proper subsets of the sequences of tokens generated by the encoder neural networks in memory or data storage accessible to the head neural network, e.g., in addition to or instead of directly providing these representations to the head neural network, e.g., through a wired or wireless network. The system can then retrieve these representations whenever an output needs to be generated by using the head neural network based on processing some or all of these stored, i.e., pre-computed, representations.
- In various cases, this can allow for the system to perform a given machine learning task with reduced inference time. In addition, this can be further advantageous in cases where there are more possible combinations than the number of distinct input sequences, because a lower-dimensional representation for the same input sequence need not be regenerated as the system processes different tuples of input sequences. As a concrete example, the given task is to predict the relevance between a query and a document. The system can generate and store, e.g., prior to receiving a user input to begin performing the task, respective lower-dimensional representations for all available input sequences to the system which may include millions of queries and millions of documents and, when at inference time, use the lightweight head neural network to efficiently process different pairs of pre-stored lower-dimensional representations to generate as output a relevance score for each different query-document pair.
- The system processes, using the head neural network, the lower-dimensional representations to generate an output (210). Specifically, the system can generate a combined, e.g., concatenated, input for the head neural network from the lower-dimensional representations of the respective small proper subsets of tokens that have been generated as a result of processing the input sequences using the encoder neural networks. The system then uses the head neural network to process the combined input to generate the network output. Depending on the specifics of the given machine learning task, the output can be any kind of score, classification, or regression output based on the tuple of input sequences.
- In general, the
process 200 can be performed as part of predicting an output for a tuple of multiple input sequences for which the desired output, i.e., the output that should be generated by the system for the tuple of multiple input sequences, is not known. - The
process 200 can also be performed as part of processing tuples of input sequences derived from a set of training data, i.e., tuples of input sequences derived from a set of inputs for which the output that should be generated by the system is known, in order to train the encoder neural networks and the head neural network to determine trained values for the parameters of the neural networks, so that the system can summarize the information of the entire input sequence to selected tokens of the encoded representations of the input sequence and generate accurate output scores. Specifically, the system can do this by optimizing an objective function that is specific to the given machine learning task. The exact forms of the objective function may vary across different tasks, but typically, the objective function measures a difference between the predicted output and the known, desired output or another target output that is derived from the known, desired output. A cross-entropy loss function, e.g., in the case of classification tasks, and a mean squared error (MSE) loss function, e.g., in the case of regression tasks, are examples of suitable objective functions that can be used by the system during the training. - The system can repeatedly perform the
process 200 on inputs selected from a set of training data as part of a conventional machine learning training technique to train the initial neural network layers, e.g., a gradient descent with backpropagation training technique that uses a conventional optimizer, e.g., stochastic gradient descent, RMSprop, or Adam optimizer, including Adam with weight decay (“AdamW”) optimizer. During training, the system can incorporate any number of techniques to improve the speed, the effectiveness, or both of the training process. For example, the system can use dropout, label smoothing, or both to reduce overfitting. As another example, the system can perform the training using a distributed architecture that trains multiple instances of the encoder neural networks in parallel. - In some implementations, prior to the commencement of the training, the system can initialize a portion of the parameters of the encoder neural networks in accordance with a predetermined set of parameter values, rather than randomly initialized values. This can improve the overall training effectiveness in terms of required computational resources. For example, the system can initialize parameter values of the one or more encoder network layers of each encoder neural network with trained values of parameters of another, pre-trained neural network. For example, the other neural network can be a self-attention neural network that has already been trained to attain at least a threshold level of performance (e.g., accuracy) on a relevant machine learning task, e.g., a natural language processing or understanding task that involves operating on textual data, information derived from textual data, or both.
- That is, the system can obtain an instance of the neural network by first instantiating the encoder neural networks according to the architecture and trained parameter values of (a portion of) the other self-attention neural network, and then attaching the projection layers and the head neural network to the encoder network layers included in the encoder neural networks that have been instantiated in this way. The system can then proceed to train the obtained neural network on the given machine learning task as described above.
- In some implementations, the system makes use of a teacher neural network during the training. For example, the teacher neural network can be a specialist neural network with a cumbersome architecture (e.g., with more layers, more parameters, or both) that has already been trained to attain at least a threshold level of performance on the same given machine learning task as the system is configured to perform. Specifically, for each training input (i.e., a training tuple of input sequences), the system first processes the training input using a trained neural network to generate a teacher network output, and then trains the encoder and head neural networks using the teacher network output generated by the teacher neural network, i.e., trains the head neural network to generate a training output for the training input that match the teacher network output.
- For example, the system can do this by optimizing a cross-entropy loss function:
-
- where yi is the training output generated by the neural network and pi may be computed by applying a sigmoid function on the teacher network output which is in the form of logits.
- This can further improve the effectiveness of training by allowing for the system to make use of unlabeled training data, which is typically much more readily available in large amounts, compared with labeled (e.g., human-annotated) training data. In addition, once trained using the cumbersome teacher neural network, the neural networks can generate outputs that are not significantly less accurate than outputs generated by the cumbersome neural network despite being easier to deploy or using fewer computational resources than the cumbersome neural network. In some implementations, the system trains the neural networks using a two-stage process. During the first (“pre-training”) stage, the system adjusts only parameter values of the projection layers of the encoder neural networks and parameter values of the head neural network, while keeping the parameter values of the one or more encoder network layers fixed to their values that have been randomly initialized or otherwise predetermined. After the pre-training, that is, during the second (“fine-tuning”) stage, the system adjusts values of all of the network parameters, including parameter values of the one or more encoder network layers of the encoder neural networks.
- This specification uses the term “configured” in connection with systems and computer program components. For a system of one or more computers to be configured to perform particular operations or actions means that the system has installed on it software, firmware, hardware, or a combination of them that in operation cause the system to perform the operations or actions. For one or more computer programs to be configured to perform particular operations or actions means that the one or more programs include instructions that, when executed by data processing apparatus, cause the apparatus to perform the operations or actions.
- Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly-embodied computer software or firmware, in computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible non transitory storage medium for execution by, or to control the operation of, data processing apparatus. The computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them. Alternatively or in addition, the program instructions can be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.
- The term “data processing apparatus” refers to data processing hardware and encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. The apparatus can also be, or further include, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). The apparatus can optionally include, in addition to hardware, code that creates an execution environment for computer programs, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- A computer program, which may also be referred to or described as a program, software, a software application, an app, a module, a software module, a script, or code, can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages; and it can be deployed in any form, including as a stand alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data, e.g., one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, e.g., files that store one or more modules, sub programs, or portions of code. A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a data communication network.
- In this specification, the term “database” is used broadly to refer to any collection of data: the data does not need to be structured in any particular way, or structured at all, and it can be stored on storage devices in one or more locations. Thus, for example, the index database can include multiple collections of data, each of which may be organized and accessed differently.
- Similarly, in this specification the term “engine” is used broadly to refer to a software-based system, subsystem, or process that is programmed to perform one or more specific functions. Generally, an engine will be implemented as one or more software modules or components, installed on one or more computers in one or more locations. In some cases, one or more computers will be dedicated to a particular engine; in other cases, multiple engines can be installed and running on the same computer or computers.
- The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by special purpose logic circuitry, e.g., an FPGA or an ASIC, or by a combination of special purpose logic circuitry and one or more programmed computers.
- Computers suitable for the execution of a computer program can be based on general or special purpose microprocessors or both, or any other kind of central processing unit. Generally, a central processing unit will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a central processing unit for performing or executing instructions and one or more memory devices for storing instructions and data. The central processing unit and the memory can be supplemented by, or incorporated in, special purpose logic circuitry. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks. However, a computer need not have such devices. Moreover, a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, e.g., a universal serial bus (USB) flash drive, to name just a few.
- Computer readable media suitable for storing computer program instructions and data include all forms of non volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks.
- To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input. In addition, a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web browser on a user's device in response to requests received from the web browser. Also, a computer can interact with a user by sending text messages or other forms of message to a personal device, e.g., a smartphone that is running a messaging application, and receiving responsive messages from the user in return.
- Data processing apparatus for implementing machine learning models can also include, for example, special-purpose hardware accelerator units for processing common and compute-intensive parts of machine learning training or production, i.e., inference, workloads.
- Machine learning models can be implemented and deployed using a machine learning framework, e.g., a TensorFlow framework, a Microsoft Cognitive Toolkit framework, an Apache Singa framework, or an Apache MXNet framework.
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface, a web browser, or an app through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back end, middleware, or front end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (LAN) and a wide area network (WAN), e.g., the Internet.
- The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, a server transmits data, e.g., an HTML page, to a user device, e.g., for purposes of displaying data to and receiving user input from a user interacting with the device, which acts as a client. Data generated at the user device, e.g., a result of the user interaction, can be received at the server from the device.
- While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any invention or on the scope of what may be claimed, but rather as descriptions of features that may be specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations and even initially be claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
- Similarly, while operations are depicted in the drawings and recited in the claims in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In certain circumstances, multitasking and parallel processing may be advantageous. Moreover, the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
- Particular embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous.
Claims (21)
1. A system for performing a machine learning task on a tuple of respective input sequences to generate an output, the system comprising one or more computers and one or more storage devices storing instructions that, when executed by the one or more computers, cause the one or more computers to perform one or more operations to implement:
a neural network configured to perform the machine learning task, the neural network comprising (i) a plurality of encoder neural networks each comprising one or more encoder network layers and a projection layer and (ii) a head neural network, each encoder neural network configured to:
receive a respective input sequence from the tuple;
process the respective input sequence using the one or more encoder network layers to generate an encoded representation of the respective input sequence, the encoded representation comprising a sequence of tokens; and
process each of some or all of the tokens in the sequence of tokens using the projection layer to generate a lower-dimensional representation of the token, and the head neural network configured to:
receive, from each of the plurality of encoder neural networks, lower-dimensional representations of a respective proper subset of the sequence of tokens generated by the encoder neural network; and
process the lower-dimensional representations to generate the output.
2. The system of claim 1 , wherein the head neural network is further configured to access the lower-dimensional representations of the respective proper subsets of the sequences of tokens generated by the encoder neural networks from a memory.
3. The system of claim 1 , wherein the lower-dimensional representations of the tokens generated by different projection layers have different dimensions from each other.
4. The system of claim 1 , wherein each input sequence has a respective network input at each of a plurality of input positions in an input order.
5. The system of claim 4 , wherein the sequence of tokens generated by the encoder neural network comprises a corresponding token for each network input in the input sequence.
6. The system of claim 4 , wherein the operations further comprise:
for each sequence of tokens generated by the one or more encoder network layers of the encoder neural network from the input sequence:
determining the respective proper subset of the sequence of tokens based on respective positions of the tokens in the sequence and on a length of the input sequence.
7. The system of claim 1 , wherein the respective proper subset of the sequence of tokens comprises first N tokens in the sequence of tokens, and wherein N is a predetermined positive integer.
8. The system of claim 1 , wherein the machine learning task is a semantic text matching task.
9. The system of claim 1 , wherein the one or more encoder network layers comprise an attention layer that is configured to:
receive an input sequence for the layer comprising a respective layer input at each of one or more positions; and
generate an attended input sequence at least in part by applying an attention mechanism to the input sequence for the layer, the attended input sequence comprising a respective attended layer input at each of the one or more positions.
10. A computer-implemented method comprising:
receiving, at each of a plurality of encoder neural networks, a respective input sequence from a tuple of respective input sequences;
processing, using one or more encoder network layers of each of the plurality of encoder neural networks, the respective input sequence to generate an encoded representation of the respective input sequence, the encoded representation comprising a sequence of tokens;
processing, using a projection layer of each of the plurality of encoder neural networks, each of some or all of the tokens in the sequence of tokens to generate a lower-dimensional representation of the token;
receiving, at a head neural network and from each of the plurality of encoder neural networks, lower-dimensional representations of a respective proper subset of the sequence of tokens generated by the encoder neural network; and
processing, using the head neural network, the lower-dimensional representations to generate an output.
11. The method of claim 10 , wherein the head neural network is further configured to access the lower-dimensional representations of the respective proper subsets of the sequences of tokens generated by the encoder neural networks from a memory.
12. The method of claim 10 , wherein the lower-dimensional representations of the tokens generated by different projection layers have different dimensions from each other.
13. The method of claim 10 , wherein each input sequence has a respective network input at each of a plurality of input positions in an input order.
14. The method of claim 13 , wherein the sequence of tokens generated by the encoder neural network comprises a corresponding token for each network input in the input sequence.
15. The method of claim 13 , wherein the operations further comprise:
for each sequence of tokens generated by the one or more encoder network layers of the encoder neural network from the input sequence:
determining the respective proper subset of the sequence of tokens based on respective positions of the tokens in the sequence and on a length of the input sequence.
16. The method of claim 10 , wherein the respective proper subset of the sequence of tokens comprises first N tokens in the sequence of tokens, and wherein N is a predetermined positive integer.
17. The method of claim 10 , wherein the one or more encoder network layers comprise an attention layer that is configured to:
receive an input sequence for the layer comprising a respective layer input at each of one or more positions; and
generate an attended input sequence at least in part by applying an attention mechanism to the input sequence for the layer, the attended input sequence comprising a respective attended layer input at each of the one or more positions.
18. The method of claim 10 , further comprising training the plurality of encoder neural networks and the head neural network including initializing parameter values of the one or more encoder network layers of each encoder neural network with a predetermined set of parameter values.
19. The method of claim 18 , wherein the training further comprises:
receiving a training tuple;
processing the training tuple using a trained neural network to generate a teacher network output; and
training the neural network using the teacher network output generated by the trained neural network, wherein the training comprises adjusting only parameter values of the projection layers of the encoder neural networks and parameter values of the head neural network.
20. The method of claim 19 , wherein the training further comprises:
receiving another training tuple;
processing the training tuple using the trained neural network to generate another teacher network output; and
training the neural network using the other teacher network output generated by the trained neural network, including adjusting parameter values of the one or more encoder network layers of the encoder neural networks.
21. One or more computer storage media storing instructions that when executed by one or more computers cause the one or more computers to implement:
a neural network configured to perform a machine learning task on a tuple of respective input sequences to generate a network output, the neural network comprising (i) a plurality of encoder neural networks each comprising one or more encoder network layers and a projection layer and (ii) a head neural network, each encoder neural network configured to:
receive a respective input sequence from the tuple;
process the respective input sequence using the one or more encoder network layers to generate an encoded representation of the respective input sequence, the encoded representation comprising a sequence of tokens; and
process each of some or all of the tokens in the sequence of tokens using the projection layer to generate a lower-dimensional representation of the token, and the head neural network configured to:
receive, from each of the plurality of encoder neural networks, lower-dimensional representations of a respective proper subset of the sequence of tokens generated by the encoder neural network; and
process the lower-dimensional representations to generate the network output.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/336,093 US20210374345A1 (en) | 2020-06-01 | 2021-06-01 | Processing large-scale textual inputs using neural networks |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202063032996P | 2020-06-01 | 2020-06-01 | |
US17/336,093 US20210374345A1 (en) | 2020-06-01 | 2021-06-01 | Processing large-scale textual inputs using neural networks |
Publications (1)
Publication Number | Publication Date |
---|---|
US20210374345A1 true US20210374345A1 (en) | 2021-12-02 |
Family
ID=78705002
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US17/336,093 Pending US20210374345A1 (en) | 2020-06-01 | 2021-06-01 | Processing large-scale textual inputs using neural networks |
Country Status (1)
Country | Link |
---|---|
US (1) | US20210374345A1 (en) |
Cited By (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20210358180A1 (en) * | 2018-09-27 | 2021-11-18 | Google Llc | Data compression using integer neural networks |
CN114549608A (en) * | 2022-04-22 | 2022-05-27 | 季华实验室 | Point cloud fusion method and device, electronic equipment and storage medium |
CN114842454A (en) * | 2022-06-27 | 2022-08-02 | 小米汽车科技有限公司 | Obstacle detection method, device, equipment, storage medium, chip and vehicle |
US20230107640A1 (en) * | 2021-10-05 | 2023-04-06 | Salesforce.Com, Inc. | Systems and methods for long document summarization |
Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20180336472A1 (en) * | 2017-05-20 | 2018-11-22 | Google Llc | Projection neural networks |
US20190251165A1 (en) * | 2018-02-09 | 2019-08-15 | Digital Genius Limited | Conversational agent |
US20200034436A1 (en) * | 2018-07-26 | 2020-01-30 | Google Llc | Machine translation using neural network models |
-
2021
- 2021-06-01 US US17/336,093 patent/US20210374345A1/en active Pending
Patent Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20180336472A1 (en) * | 2017-05-20 | 2018-11-22 | Google Llc | Projection neural networks |
US20190251165A1 (en) * | 2018-02-09 | 2019-08-15 | Digital Genius Limited | Conversational agent |
US20200034436A1 (en) * | 2018-07-26 | 2020-01-30 | Google Llc | Machine translation using neural network models |
Cited By (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20210358180A1 (en) * | 2018-09-27 | 2021-11-18 | Google Llc | Data compression using integer neural networks |
US11869221B2 (en) * | 2018-09-27 | 2024-01-09 | Google Llc | Data compression using integer neural networks |
US20230107640A1 (en) * | 2021-10-05 | 2023-04-06 | Salesforce.Com, Inc. | Systems and methods for long document summarization |
US11941346B2 (en) * | 2021-10-05 | 2024-03-26 | Salesforce, Inc. | Systems and methods for long document summarization |
CN114549608A (en) * | 2022-04-22 | 2022-05-27 | 季华实验室 | Point cloud fusion method and device, electronic equipment and storage medium |
CN114842454A (en) * | 2022-06-27 | 2022-08-02 | 小米汽车科技有限公司 | Obstacle detection method, device, equipment, storage medium, chip and vehicle |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11669744B2 (en) | Regularized neural network architecture search | |
US11003856B2 (en) | Processing text using neural networks | |
US20210374345A1 (en) | Processing large-scale textual inputs using neural networks | |
US20230153613A1 (en) | Attention-based decoder-only sequence transduction neural networks | |
US11803751B2 (en) | Training text summarization neural networks with an extracted segments prediction objective | |
US20220121906A1 (en) | Task-aware neural network architecture search | |
US10387531B1 (en) | Processing structured documents using convolutional neural networks | |
US20230049747A1 (en) | Training machine learning models using teacher annealing | |
US11455514B2 (en) | Hierarchical device placement with reinforcement learning | |
US11481646B2 (en) | Selecting answer spans from electronic documents using neural networks | |
US20210248473A1 (en) | Attention neural networks with linear units | |
US20210350244A1 (en) | Attention neural networks with locality-sensitive hashing | |
US20220383119A1 (en) | Granular neural network architecture search over low-level primitives | |
US10671909B2 (en) | Decreasing neural network inference times using softmax approximation | |
US20220253680A1 (en) | Sparse and differentiable mixture of experts neural networks | |
US11481609B2 (en) | Computationally efficient expressive output layers for neural networks | |
US20230154161A1 (en) | Memory-optimized contrastive learning | |
US20230029590A1 (en) | Evaluating output sequences using an auto-regressive language model neural network | |
WO2023192674A1 (en) | Attention neural networks with parallel attention and feed-forward layers | |
US20240013769A1 (en) | Vocabulary selection for text processing tasks using power indices | |
US20240078379A1 (en) | Attention neural networks with n-grammer layers | |
US11886976B1 (en) | Efficient decoding of output sequences using adaptive early exiting | |
US20230401451A1 (en) | Determining hyperparameters using sequence generation neural networks | |
US20220367052A1 (en) | Neural networks with feedforward spatial transformation units | |
WO2023175089A1 (en) | Generating output sequences with inline evidence using language model neural networks |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:RAMAN, KARTHIK;YANG, LIU;BENDERSKY, MIKE;AND OTHERS;REEL/FRAME:056448/0424Effective date: 20210603 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: FINAL REJECTION MAILED |