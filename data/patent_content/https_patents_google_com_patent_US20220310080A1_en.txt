US20220310080A1 - Multi-Task Learning for End-To-End Automated Speech Recognition Confidence and Deletion Estimation - Google Patents
Multi-Task Learning for End-To-End Automated Speech Recognition Confidence and Deletion Estimation Download PDFInfo
- Publication number
- US20220310080A1 US20220310080A1 US17/643,826 US202117643826A US2022310080A1 US 20220310080 A1 US20220310080 A1 US 20220310080A1 US 202117643826 A US202117643826 A US 202117643826A US 2022310080 A1 US2022310080 A1 US 2022310080A1
- Authority
- US
- United States
- Prior art keywords
- utterance
- confidence
- word
- sub
- transcription
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000012217 deletion Methods 0.000 title claims description 33
- 230000037430 deletion Effects 0.000 title claims description 33
- 238000013518 transcription Methods 0.000 claims abstract description 81
- 230000035897 transcription Effects 0.000 claims abstract description 80
- 238000000034 method Methods 0.000 claims abstract description 45
- 230000015654 memory Effects 0.000 claims description 43
- 238000012549 training Methods 0.000 claims description 29
- 238000012545 processing Methods 0.000 claims description 24
- 230000007246 mechanism Effects 0.000 claims description 19
- 238000004891 communication Methods 0.000 claims description 15
- 230000008569 process Effects 0.000 claims description 12
- 230000000306 recurrent effect Effects 0.000 claims description 8
- 230000001537 neural effect Effects 0.000 claims description 7
- 230000004931 aggregating effect Effects 0.000 claims description 4
- 238000003860 storage Methods 0.000 description 12
- 238000013528 artificial neural network Methods 0.000 description 11
- 238000004590 computer program Methods 0.000 description 8
- 230000003287 optical effect Effects 0.000 description 6
- 238000006467 substitution reaction Methods 0.000 description 6
- 230000006870 function Effects 0.000 description 5
- 238000003780 insertion Methods 0.000 description 5
- 230000037431 insertion Effects 0.000 description 5
- 238000003058 natural language processing Methods 0.000 description 5
- 230000004044 response Effects 0.000 description 5
- 230000009471 action Effects 0.000 description 3
- 238000009826 distribution Methods 0.000 description 3
- 230000008901 benefit Effects 0.000 description 2
- 230000008859 change Effects 0.000 description 2
- 238000012937 correction Methods 0.000 description 2
- 230000003993 interaction Effects 0.000 description 2
- 230000006855 networking Effects 0.000 description 2
- 230000009467 reduction Effects 0.000 description 2
- 239000004065 semiconductor Substances 0.000 description 2
- 230000003068 static effect Effects 0.000 description 2
- 206010068829 Overconfidence Diseases 0.000 description 1
- 230000002776 aggregation Effects 0.000 description 1
- 238000004220 aggregation Methods 0.000 description 1
- 238000013459 approach Methods 0.000 description 1
- 238000003066 decision tree Methods 0.000 description 1
- 230000007812 deficiency Effects 0.000 description 1
- 238000013461 design Methods 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 238000010348 incorporation Methods 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000012423 maintenance Methods 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 238000010606 normalization Methods 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 238000005549 size reduction Methods 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 230000005236 sound signal Effects 0.000 description 1
- 230000003595 spectral effect Effects 0.000 description 1
- 238000010025 steaming Methods 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/183—Speech classification or search using natural language modelling using context dependencies, e.g. language models
- G10L15/19—Grammatical context, e.g. disambiguation of the recognition hypotheses based on word sequence rules
- G10L15/197—Probabilistic grammars, e.g. word n-grams
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/02—Feature extraction for speech recognition; Selection of recognition unit
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/04—Segmentation; Word boundary detection
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/06—Creation of reference templates; Training of speech recognition systems, e.g. adaptation to the characteristics of the speaker's voice
- G10L15/063—Training
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/16—Speech classification or search using artificial neural networks
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/28—Constructional details of speech recognition systems
- G10L15/32—Multiple recognisers used in sequence or in parallel; Score combination systems therefor, e.g. voting systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/28—Constructional details of speech recognition systems
- G10L15/30—Distributed recognition, e.g. in client-server systems, for mobile phones or network applications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/02—Feature extraction for speech recognition; Selection of recognition unit
- G10L2015/025—Phonemes, fenemes or fenones being the recognition units
Definitions
- This disclosure relates to multi-task learning for end-to-end automated speech recognition confidence and deletion estimation.
- Modern automatic speech recognition (ASR) systems focus on providing not only quality/accuracy (e.g., low word error rates (WERs)), but also low latency (e.g., a short delay between the user speaking and a transcription appearing).
- WERs word error rates
- the ASR system decode utterances in a streaming fashion that corresponds to displaying a transcription of an utterance in real-time, or even faster than real-time, as a user speaks.
- an application on the mobile phone using the ASR system may require the speech recognition to be streaming such that words, word pieces, and/or individual characters appear on the screen as soon as they are spoken.
- the speech recognition strives to run on the mobile device in a manner that minimizes an impact from latency and inaccuracy that may detrimentally affect the user's experience.
- One aspect of the disclosure provides a computer-implemented method for multi-task learning for end-to-end automated speech recognition confidence and deletion estimation.
- the computer-implemented method when executed on data processing hardware causes the data processing hardware to perform operations including receiving, from a first speech recognizer, a speech recognition result corresponding to a transcription of an utterance spoken by a user.
- the speech recognition result includes a sequence of hypothesized sub-word units that form one or more words of the transcription of the utterance, each sub-word unit output from the first speech recognizer at a corresponding output step.
- the operations also include obtaining a respective confidence embedding associated with the corresponding output step when the corresponding sub-word unit was output from the first speech recognizer.
- the operations further use the confidence estimation module for generating, using a first attention mechanism that self-attends to the respective confidence embedding for the corresponding sub-word unit and the confidence embeddings obtained for any other sub-word units in the sequence of hypothesized sub-word units that proceed the corresponding sub-word unit, a confidence feature vector.
- the operations continue to use the confidence estimation module for generating, using a second attention mechanism that cross-attends to a sequence of encodings each associated with a corresponding acoustic frame segmented from audio data that corresponds to the utterance, an acoustic context vector. Further, the operations then use the confidence estimation module for generating, as output from an output layer of the confidence estimation module, a respective confidence output score for the corresponding sub-word unit based on the confidence feature vector and the acoustic feature vector received as input by the output layer of the confidence estimation module. The operations also include determining, based on the respective confidence output score generated for each sub-word unit in the sequence of hypothesized sub-word units, an utterance-level confidence score for the transcription of the utterance.
- determining the utterance-level confidence score for the transcription of the utterance includes, for each word of the one or more words of the transcription of the utterance, determining a respective word-level confidence score that is equal to the respective confidence output score generated for the final sub-word unit in the corresponding word.
- These implementations further include aggregating the respective word-level confidence scores determined for each of the one or more words of the transcription to determine the utterance-level confidence score for the transcription of the utterance.
- the operations further include estimating, using a hierarchical attention mechanism, a probability that the speech recognition result for the utterance is recognized correctly, determining an utterance-level loss for the speech recognition result based on an estimated probability that the that the speech recognition result for the utterance is recognized correctly and a ground-truth label indicating whether or not the utterance is recognized correctly, and training the confidence estimation model on the utterance-level loss.
- the confidence estimation model and the first speech recognizer are trained jointly.
- the operations may include estimating, at each position in a sequence of words of the utterance, a number of deletions, determining a deletion loss for the speech recognition result and the estimated number of deletions and a ground-truth number of deletions at each position in the sequence of words, and training the confidence estimation model on the deletion loss.
- the operations further include, after training the confidence estimation model, rescoring candidate speech recognition results recognized by the first speech recognizer.
- the sub-word units may include wordpieces or graphemes.
- the first speech recognizer includes a transducer decoder model configured to generate multiple candidate hypotheses, each candidate hypothesis corresponding to a candidate transcription for the utterance and represented by a respective sequence of hypothesized sub-word units.
- the transducer decoder model may include a Recurrent Neural Network-Transducer (RNN-T) model architecture.
- the operations further include determining whether the utterance-level confidence score for the transcription of the utterance satisfies a confidence threshold. In these implementations, when the utterance-level confidence score for the transcription of the utterance fails to satisfy the confidence threshold: the operations further include rejecting the transcription output from the first speech recognizer and instructing a second speech recognizer to process audio data corresponding to the utterance spoken by the user to re-transcribe the utterance spoken by the user.
- the first speech recognizer may reside on a user device associated with the user
- the second speech recognizer may execute on a remote server in communication with the user device
- the user device may transmit the audio data to the remote server when the utterance-level confidence score for the transcription of the utterance fails to satisfy the confidence threshold.
- the second speech recognizer may be more computationally-intensive than the first speech recognizer.
- the system includes data processing hardware and memory hardware in communication with the data processing hardware.
- the memory hardware stores instructions that when executed on the data processing hardware causes the data processing hardware to perform operations including receiving, from a first speech recognizer, a speech recognition result corresponding to a transcription of an utterance spoken by a user.
- the speech recognition result includes a sequence of hypothesized sub-word units that form one or more words of the transcription of the utterance, each sub-word unit output from the first speech recognizer at a corresponding output step.
- the operations also include obtaining a respective confidence embedding associated with the corresponding output step when the corresponding sub-word unit was output from the first speech recognizer.
- the operations also use the confidence estimation module for generating, using a first attention mechanism that self-attends to the respective confidence embedding for the corresponding sub-word unit and the confidence embeddings obtained for any other sub-word units in the sequence of hypothesized sub-word units that proceed the corresponding sub-word unit, a confidence feature vector.
- the operations continue to use the confidence estimation module for generating, using a second attention mechanism that cross-attends to a sequence of encodings each associated with a corresponding acoustic frame segmented from audio data that corresponds to the utterance, an acoustic context vector. Further, the operations use the confidence estimation module for generating, as output from an output layer of the confidence estimation module, a respective confidence output score for the corresponding sub-word unit based on the confidence feature vector and the acoustic feature vector received as input by the output layer of the confidence estimation module. The operations also include determining, based on the respective confidence output score generated for each sub-word unit in the sequence of hypothesized sub-word units, an utterance-level confidence score for the transcription of the utterance.
- determining the utterance-level confidence score for the transcription of the utterance includes, for each word of the one or more words of the transcription of the utterance, determining a respective word-level confidence score that is equal to the respective confidence output score generated for the final sub-word unit in the corresponding word.
- These implementations further include aggregating the respective word-level confidence scores determined for each of the one or more words of the transcription to determine the utterance-level confidence score for the transcription of the utterance.
- the operations further include estimating, using a hierarchical attention mechanism, a probability that the speech recognition result for the utterance is recognized correctly, determining an utterance-level loss for the speech recognition result based on an estimated probability that the that the speech recognition result for the utterance is recognized correctly and a ground-truth label indicating whether or not the utterance is recognized correctly, and training the confidence estimation model on the utterance-level loss.
- the confidence estimation model and the first speech recognizer are trained jointly.
- the operations may include estimating, at each position in a sequence of words of the utterance, a number of deletions, determining a deletion loss for the speech recognition result and the estimated number of deletions and a ground-truth number of deletions at each position in the sequence of words, and training the confidence estimation model on the deletion loss.
- the operations further include, after training the confidence estimation model, rescoring candidate speech recognition results recognized by the first speech recognizer.
- the sub-word units may include wordpieces or graphemes.
- the first speech recognizer includes a transducer decoder model configured to generate multiple candidate hypotheses, each candidate hypothesis corresponding to a candidate transcription for the utterance and represented by a respective sequence of hypothesized sub-word units.
- the transducer decoder model may include a Recurrent Neural Network-Transducer (RNN-T) model architecture.
- the operations further include determining whether the utterance-level confidence score for the transcription of the utterance satisfies a confidence threshold. In these examples, when the utterance-level confidence score for the transcription of the utterance fails to satisfy the confidence threshold, the operations further include rejecting the transcription output from the first speech recognizer and instructing a second speech recognizer to process audio data corresponding to the utterance spoken by the user to re-transcribe the utterance spoken by the user.
- the first speech recognizer may reside on a user device associated with the user
- the second speech recognizer may execute on a remote server in communication with the user device
- the user device may transmit the audio data to the remote server when the utterance-level confidence score for the transcription of the utterance fails to satisfy the confidence threshold.
- the second speech recognizer may be more computationally-intensive than the first speech recognizer.
- FIG. 1 is a schematic view of an example speech environment.
- FIG. 2 is a schematic view of an example speech recognizer and a confidence estimation model overlain on the speech recognizer.
- FIG. 3 is a schematic view of training the confidence estimation module of FIG. 2 using multi-task learning.
- FIG. 4 is a schematic view of an example confidence-based routine for selecting an appropriate speech recognizer based on utterance-level confidence.
- FIG. 5 is a flowchart of an example arrangement of operations for a method of determining an utterance-level confidence score of a transcription output by a speech recognizer
- FIG. 6 is a schematic view of an example computing device that may be used to implement the systems and methods described herein.
- ASR Automated speech recognition
- WERs word error rates
- E2E end-to-end ASR models, such as the Recurrent Neural Network-Transducer (RNN-T), the transformer or conformer transducer, and attention-based encoder-decoder models, have gained popularity in achieving state-of-the-art performance in accuracy and latency.
- RNN-T Recurrent Neural Network-Transducer
- RNN-T Recurrent Neural Network-Transducer
- the transformer or conformer transducer the transformer or conformer transducer
- attention-based encoder-decoder models have gained popularity in achieving state-of-the-art performance in accuracy and latency.
- E2E models apply a sequence-to-sequence approach to jointly learn acoustic and language modeling in a single neural network that is trained end to end from training data, e.g., utterance-transcription pairs.
- sequence-to-sequence models include “attention-based” models and “listen-attend-spell” (LAS) models.
- a LAS model transcribes speech utterances into characters using a listener component, an attender component, and a speller component.
- the listener is a recurrent neural network (RNN) encoder that receives an audio input (e.g., a time-frequency representation of speech input) and maps the audio input to a higher-level feature representation.
- the attender attends to the higher-level feature to learn an alignment between input features and predicted subword units (e.g., a grapheme or a wordpiece).
- the speller is an attention-based RNN decoder that generates character sequences from the input by producing a probability distribution over a set of hypothesized words.
- all components of a model may be trained jointly as a single end-to-end (E2E) neural network.
- E2E model refers to a model whose architecture is constructed entirely of a neural network.
- a fully neural network functions without external and/or manually designed components (e.g., finite state transducers, a lexicon, or text normalization modules). Additionally, when training E2E models, these models generally do not require bootstrapping from decision trees or time alignments from a separate system.
- an ASR system decodes in a streaming fashion that corresponds to displaying a transcription of an utterance in real-time, or even faster than real-time, as a user speaks.
- an application on the mobile phone using the ASR system may require the speech recognition to be streaming such that words, word pieces, and/or individual characters appear on the screen as soon as they are spoken.
- it is also likely that the user of the mobile phone has a low tolerance for latency.
- sequence-to-sequence models such as the LAS model that function by reviewing an entire input sequence of audio before generating output text, do not allow for streaming outputs as inputs are received. Due to this deficiency, deploying the LAS model for speech applications that are latency sensitive and/or require real-time voice transcription may pose issues. This makes an LAS model alone not an ideal model for mobile technology (e.g., mobile phones) that often relies on real-time applications (e.g., real-time communication applications).
- RNN-T recurrent neural network transducer
- a recurrent neural network transducer does not employ an attention mechanism and, unlike other sequence-to-sequence models that generally need to process an entire sequence (e.g., audio waveform) to produce an output (e.g., a sentence), the RNN-T continuously processes input samples and streams output symbols, a feature that is particularly attractive for real-time communication. For instance, speech recognition with an RNN-T may output characters one-by-one as spoken.
- an RNN-T uses a feedback loop that feeds symbols predicted by the model back into itself to predict the next symbols.
- an RNN-T may scale to a fraction of the size of a server-based speech recognition model. With the size reduction, the RNN-T may be deployed entirely on-device and be able to run offline (i.e., without a network connection); therefore, avoiding unreliability issues with communication networks.
- the RNN-T model alone, however, still lags behind a large state-of-the-art conventional model (e.g., a server-based model with separate AM, PM, and LMs) in terms of quality (e.g., speech recognition accuracy).
- a non-streaming E2E, LAS model has speech recognition quality that is comparable to large state-of-the-art conventional models.
- a two-pass speech recognition system e.g., shown in FIG. 2A ) was developed that includes a first-pass component of an RNN-T network followed by a second-pass component of a LAS network.
- the two-pass model benefits from the streaming nature of an RNN-T model with low latency while improving the accuracy of the RNN-T model through the second-pass incorporating the LAS network.
- the LAS network increases the latency when compared to only a RNN-T model, the increase in latency is reasonably slight and complies with latency constraints for on-device operation.
- a two-pass model achieves a 17-22% WER reduction when compared to a RNN-T alone and has a similar WER when compared to a large conventional model.
- Confidence scores are an important feature of ASR systems that support many downstream applications to mitigate speech recognition errors. For example, unlabeled utterances with recognition results output from an ASR model that achieve high confidence may be used for semi-supervised training of the ASR model which may reduce the expense of using only transcribed utterances for training.
- utterances with recognition results that achieve low word-level confidence may prompt the user to correct any mis-transcribed words.
- recognition results with low confidence may result in passing audio for the corresponding utterance to a different, more computationally extensive ASR model (e.g., server-side) for improving recognition on the utterance.
- E2E ASR models While conventional hybrid ASR systems can easily estimate word-level confidence scores from word posterior probabilities computed from lattices or confusion networks and then aggregated to provide an utterance-level confidence, the deep neural networks employed by E2E ASR models tend to exhibit overconfidence when predicting words. As many E2E ASR models are configured to output recognition results at the sub-word level, simply learning confidence scores for each sub-word recognized by the ASR model using a corresponding fixed sub-word tokenization for the word as a reference sequence can lead to incorrect ground truth labels used for training confidence estimation models since recognition results may contain multiple valid tokenizations.
- a reference fixed sub-word sequence for the utterance “Good morning” may be “go, od, morn, ing” while a hypothesized sub-word sequence recognized by the ASR model may be “go, od, mor, ning, mom”.
- the sub-word labels for the corresponding hypothesized sub-words “mor” and “ning” recognized by the ASR model would be labeled incorrect because they do not match the corresponding reference fixed sub-words “morn” and “ing” for the word “morning”.
- implementations herein are directed toward a confidence estimation module that applies self-attention in order to estimate word-level confidence for each recognized word using only the confidence of the final hypothesized sub-word unit recognized by the ASR model that makes up the corresponding word.
- the contents of U.S. application Ser. No. 17/182,592, filed on Feb. 23, 2021 are incorporated by reference in their entirety. Learning only the word-level confidence of each hypothesized word recognized by the ASR model provides an estimate of a ratio of correct words to a total number of words in the hypothesis.
- training labels do not provide any signals about whether the entire utterance is recognized correctly, or whether there are any deletions in the hypothesis recognized by the ASR model.
- estimating word-level confidence does not model deletions, and an overall utterance confidence based on an aggregation of the word-level confidences for the words in the utterance, discards many useful word-level training signals. Implementations herein are further directed toward applying multi-task learning for joint training the confidence model using objectives related to word training, utterance training, and deletion training.
- FIG. 1 is an example speech environment 100 in which a user 10 interacts with a user device 110 through voice input.
- the user device 110 (also referred to generally as device 110 ) includes a computing device that is configured to capture sounds (e.g., streaming audio data) from one or more users 10 within the speech-enabled environment 100 .
- the streaming audio data 202 may refer to a spoken utterance by the user 10 that functions as an audible query, a command for the device 110 , or an audible communication captured by the device 110 .
- Speech-enabled systems of the device 110 may field the query or the command by answering the query and/or causing the command to be performed.
- the user device 110 may correspond to any computing device capable of receiving audio data 202 .
- Some examples of user devices 110 include, but are not limited to, mobile devices (e.g., mobile phones, tablets, laptops, etc.), computers, wearable devices (e.g., smart watches), smart appliances, internet of things (IoT) devices, smart speakers/displays, vehicle infotainment systems, etc.
- the user device 110 includes data processing hardware 112 and memory hardware 114 in communication with the data processing hardware 112 and storing instructions, that when executed by the data processing hardware 112 , cause the data processing hardware 112 to perform one or more operations.
- the user device 110 further includes an audio subsystem 116 with an audio capture device (e.g., microphone) 116 , 116 a for capturing and converting spoken utterances 12 within the speech-enabled system 100 into electrical signals and a speech output device (e.g., a speaker) 116 , 116 b for communicating an audible audio signal (e.g., as output audio data from the device 110 ).
- an audio capture device e.g., microphone
- a speech output device e.g., a speaker
- the user device 110 may implement an array of audio capture devices 116 a without departing from the scope of the present disclosure, whereby one or more capture devices 116 a in the array may not physically reside on the user device 110 , but be in communication with the audio subsystem 116 .
- the user device 110 implements a speech recognizer 200 that is configured to perform speech recognition on audio data 202 corresponding to an utterance 12 spoken by the user 10 .
- the audio capture device 116 a is configured to capture acoustic sounds representing the utterance 12 and convert the acoustic sounds into the audio data 202 associated with a digital format compatible with the speech recognizer 200 .
- the digital format associated with the audio data 202 may correspond to acoustic frames (e.g., parameterized acoustic frames), such as mel frames.
- the parameterized acoustic frames correspond to log-mel filterbank energies.
- FIG. 1 shows the user device 100 implementing the speech recognizer 200 for performing speech recognition on-device
- other implementations include a remote server 410 ( FIG. 4 ) implementing the speech recognizer 200 by processing the audio data 202 transmitted by the user device 110 via a network and providing a transcription 204 of the audio data 202 back to the user device 110 .
- the user device 110 utilizes both a local speech recognizer 200 residing on the user device 110 and a server-side speech recognizer 402 ( FIG. 4 ) that executes on the remote server 410 .
- user device 110 may use the local speech recognizer 200 when a network connection is not available or for speech applications that are latency sensitive and/or require streaming transcription, while the server-side speech recognizer 402 may be leveraged when additional resources are required to improve speech recognition accuracy as described in greater detail below with reference to FIG. 4 .
- the user 10 interacts with a program or application 118 executing on the user device 110 that uses the speech recognizer 200 .
- FIG. 1 depicts the user 10 communicating with an automated assistant application 118 .
- the user e.g., Bill
- the user greets the automated assistant application 118 by speaking an utterance 12 , “Good morning”, that is captured by the audio capture device 116 a and converted into corresponding audio data 202 (e.g. as acoustic frames) for processing by the speech recognizer 200 .
- the speech recognizer 200 transcribes the audio data 202 representing the utterance 12 into a transcription 204 (e.g., a text representation of “Good morning”).
- the automated assistant application 118 may apply natural language processing on the transcription 204 to generate a response 119 for output to the user 10 that conveys the message, “Good Morning Bill, the first meeting today on your calendar is at 9:00 AM.”
- Natural language processing generally refers to a process of interpreting written language (e.g., the transcription 204 ) and determining whether the written language prompts any action.
- the assistant application 118 uses natural language processing to recognize that the utterance 12 spoken by the user 10 is intended to invoke the assistant application 118 to accesses a calendar application of the user 10 and provide the response 119 indicating what time the user's 10 first meeting is today.
- the assistant application 118 returns the response 119 to the user 12 as a synthesized speech representation for audible output through the audio output device 116 a and/or as text for display on a screen in communication with the user device 110 .
- the user device 110 displays transcriptions 204 of utterances 12 spoken by the user 10 and corresponding responses 119 from the assistant application 118 as a conversation on the screen.
- natural language processing may occur on a remote system in communication with the data processing hardware 112 of the user device 110 .
- the speech recognizer 200 processes incoming audio data 202 in real-time to provide a streaming transcriptions 204 .
- the speech recognizer 200 is configured to produce a sequence of hypothesized sub-word units that make up the words of the utterance 12 spoken by the user 10 .
- the hypothesized sub-word units may include word pieces or individual characters (e.g., graphemes).
- the sequence of hypothesized sub-word units recognized by the speech recognizer include “SOS_go od_mor ning” in which the ‘SOS’ indicates a start of speech tag and each word boundary indicator (‘_’) indicates a beginning/starting sub-word unit for each word.
- a speech recognizer 200 includes a recurrent neural network-transducer (RNN-T) decoder 220 for predicting a speech recognition result/hypothesis 222 and implements a confidence estimation module (CEM) 300 for estimating a confidence 302 of the speech recognition result/hypothesis 222 .
- the CEM 300 may use the utterance-level confidence 302 to rescore the speech recognition result/hypothesis 222 predicted by the speech recognizer 200 .
- the speech recognition result/hypothesis 222 corresponds to a sequence of sub-word units, such as word pieces or graphemes, that when aggregated together form a transcription 204 for an utterance.
- the speech recognizer 200 includes an encoder 210 and the RNN-T decoder 220 , whereby the RNN-T decoder 220 includes a prediction network and a joint network.
- the RNN-T decoder 220 may produce multiple candidate hypotheses H as output 222 and the CEM 300 may rescores/re-ranks a top-K candidate hypotheses H to identify a highest scoring candidate hypothesis as a final recognition result corresponding to the transcription 204 ( FIG. 1 ).
- the decoder 220 may similarly include other types of transducer model architectures without departing from the scope of the present disclosure.
- the decoder 220 may include one of Transformer-Transducer, Convolutional Neural Network-Transducer (ConvNet-Transducer), or Conformer-Transducer model architectures in lieu of the RNN-T model architecture.
- the at least one shared encoder 210 is configured to receive, as input, the audio data 202 corresponding to the utterance 12 as a sequence of acoustic frames.
- the acoustic frames may be previously processed by the audio subsystem 116 into parameterized acoustic frames (e.g., mel frames and/or spectral frames).
- the parameterized acoustic frames correspond to log-mel filterbank energies with log-mel features.
- each parameterized acoustic frame includes 128-dimensional log-mel features computed within a short shifting window (e.g., 32 milliseconds and shifted every 10 milliseconds).
- Each feature may be stacked with previous frames (e.g., three previous frames) to form a higher-dimensional vector (e.g., a 512-dimensional vector using the three previous frames).
- the features forming the vector may then be downsampled (e.g., to a 30 millisecond frame rate).
- the encoder 210 For each acoustic frame x 1:T of the audio data 202 input to the encoder 210 , the encoder 210 is configured to generate, as output 212 , a corresponding encoding e 1:T . Each of the number of frames in x denoted by T corresponds to a respective time step.
- the encoder 210 includes a long-short term memory (LSTM) neural network.
- the LSTM neural network may include eight (8) LSTM layers.
- each layer may have 2,048 hidden units followed by a 640-dimensional projection layer.
- the RNN-T decoder 220 includes a prediction network and a joint network.
- the prediction network may have two LSTM layers of 2,048 hidden units and a 640-dimensional projection per layer as well as an embedding layer of 128 units.
- the outputs 212 of the shared encoder 210 and the prediction network may be fed into the joint network that includes a softmax predicting layer.
- the joint network of the RNN-T decoder 220 includes 640 hidden units followed by a softmax layer that predicts 4,096 mixed-case word pieces.
- the RNN-T decoder 220 receives, as input, the encoding e generated as output 212 from the encoder 210 for each acoustic frame x and generates, as output 222 , one or more candidate hypotheses H each represented by a respective sequence of hypothesized sub-word units y 1 , y 2 , y 3 , . . . , y M .
- one candidate hypothesis H may include a first sequence of hypothesized sub-word units [_go, od, _mor, ning] and another candidate hypothesis H may include a second sequence of hypothesized sub-word units [_go, od, _morn, ing].
- the respective sequence of hypothesized sub-word units y 1 , y 2 , y 3 , . . . , y M representing each candidate hypothesis H corresponds to a candidate transcription for the utterance 12 .
- Each sub-word unit y i in each respective sequence of hypothesized sub-word units y 1 , y 2 , y 3 , . . . , y M denotes a probability distribution over possible sub-units.
- the sub-word unit with the highest probability in the probability distribution may be selected as the hypothesized sub-word in the respective sequence of hypothesized sub-word units.
- the CEM 300 may receive, as input, the sequence of encodings e, e 1:T 212 output from the encoder 210 and the top-K candidate hypotheses H generated as output 222 from the RNN-T decoder 220 .
- K is equal to four (4) so that the top four candidate hypotheses H are provided as input to the CEM 300 for rescoring by determining an utterance-level confidence 350 for each hypotheses H.
- the candidate hypothesis H associated with a highest confidence 350 may be output as the transcription 204 .
- the sub-word unit vocabulary of possible sub-word units is typically smaller compared to a word vocabulary.
- the sub-word unit vocabulary may include graphemes or wordpieces (WP).
- An example WP vocabulary may include 4,096 WPs. While examples of the present disclosure use WPs as the sub-word units generated as output from the speech recognizer, graphemes can be similarly utilized as the sub-word units output from the speech recognizer without departing from the scope of the present disclosure. Accordingly, to compute a word error rate (WER) for a candidate hypothesis H, the respective sequence of hypothesized sub-word units (e.g., WPs) needs to be converted into its corresponding word sequence w 1 , w 2 , . . .
- WER word error rate
- agg can be arithmetic mean, minimum, product, or a neural network.
- a WP edit includes a correct (cor) label when a hypothesized WP matches a reference WP, a substitution (sub) label when a valid hypothesized WP does not match a reference WP, and an insertion (ins) when a hypothesized WP is misrecognized.
- Table 1 shows an example where the word “morning” is correctly transcribed, but results in two substitutions in the WP edit distance output.
- implementations herein are directed toward a transformer-based CEM 300 that leverages a confidence output at the final WP of every word as a word-level confidence while ignoring the confidence of all other preceding WPs of every word.
- the speech recognizer 200 is frozen and ground-truth WP labels of correct, insertion, and substitution are used.
- FIG. 3 shows an example of the transformer-based CEM 300 overlain on top of the speech recognizer 200 .
- FIG. 3 depicts only the actions of the CEM 300 predicting a confidence output c(y i ) 302 for the i-th WP in a respective sequence of hypothesized WPs y 1 , y 2 , y 3 , . . . , y M output by the RNN-T decoder 220 .
- FIG. 3 depicts the CEM 300 predicting the confidence output c(“ning”) for the “ning” WP in the respective sequence of hypothesized WPs [_go, od, _mor, ning] that converts into the corresponding word sequence “good morning”.
- the RNN-T decoder 220 generates, as output 222 , one or more candidate hypotheses H each represented by a respective sequence of hypothesized sub-word units y 1 , y 2 , y 3 , . . . , y M .
- the RNN-T decoder 220 may generate four candidate hypotheses as output 222 .
- one candidate hypothesis H generated as output 222 from the RNN-T decoder 220 may include a first sequence of hypothesized sub-word units [_go, od, _mor, ning] and another candidate hypothesis H may include a second sequence of hypothesized sub-word units [_go, od, _morn, ing].
- the encoder 210 generates the sequence of encodings e 1:T 212 conveying acoustic context where T corresponds to a number of acoustic frames x segmented from the utterance 12 .
- a confidence embedding b(y i ) 301 representing a set of confidence features obtained from the speech recognizer 200 is provided as input to the CEM 300 for determining a respective confidence output c(y i ) 302 .
- the i-th sub-word unit corresponds to the WP “ning”.
- the confidence embedding b(y i ) 301 conveys one or more of an input subword+positional embedding Emb(y i ) feature (e.g., Emb(ning)), a log posterior log (p(y i )) feature, and a top-K(i) feature.
- the log posterior log (p(y 1 )) feature indicates a probability value associated with the probability/likelihood that sub-word unity, includes the WP “ning”, and the top-K(i) feature indicates the K largest log probabilities at decoder index (e.g., time step) i.
- the top-K(i) feature provides probability values for each candidate hypothesis H in the top-K at decoder index (e.g., time step) i.
- decoder index e.g., time step
- implementing the CEM 300 as a transformer permits: (1) the use of word edit distance output as ground truth training labels (i.e., correction, substitution, and insertion labels) by leveraging the confidence output c(y j , Q j ) at the final WP of every word c word (w j ) as a dedicated word-level confidence 302 ; and (2) the incorporation of information/features from every WP that makes up the word.
- a self-attention mechanism 310 of the transformer-based CEM 300 applies self-attention to a confidence feature vector b based on the confidence embedding b(y i ) 301 for the i-th sub-word unit corresponding to the WP “ning” as well as confidence embeddings for earlier sub-word units in the same word.
- the confidence feature vector b may be expressed as follows.
- an acoustic cross-attention mechanism 320 of the transformer-based CEM 300 applies acoustic cross-attention (CA(e)) to the sequence of encodings e, e 1:T 212 output from the encoder 210 to generate an acoustic context vector 322 for improving the accuracy in estimating the respective confidence output c(y i ) 302 for the i-th sub-word unit corresponding the WP “ning”.
- a Softmax output layer 340 uses the self-attention confidence feature vector SA(b) and the cross-attention CA(e) acoustic context vector 322 to permit the transformer-based CEM 300 to produce the dedicated confidence output c(y i ) 302 for the i-th sub-word unit as follows.
- the CEM 300 may determine a confidence 302 for each word c word (w j ) using a confidence output c(y j , Q j ) at the final WP and estimate a word correct ratio ( ) as follows.
- the confidence 302 for the word c word (morning) in the example shown corresponds to the confidence output c(ning) of the final WP that makes up the word.
- The includes one type of utterance-level confidence score.
- the CEM 300 may be trained jointly with the speech recognizer 200 , or the CEM 300 and the speech recognizer may be trained separately from one another.
- the CEM 300 is trained using a binary cross-entropy word-level loss as follows.
- Table 1 shows that d(w j ) is equal to one when a Levenshtein word-level edit distance for the word w j outputs the “correct” (cor) label when the hypothesized word matches the reference word, and d(w j ) is equal to zero when the Levenshtein word-level edit distance for the word w j outputs the “insertion” (ins) or “substitution” (sub) labels when the hypothesized word does not match the reference word.
- Table 1 also shows the CEM 300 applying an end-of word mask loss m to focus only on the final WP making up the word and ignore WP losses associated with earlier WPs that make up the same word.
- the CEM may be further trained on an utterance-level loss using the following ground truth.
- a hierarchical attention mechanism 380 performs the following:
- the utterance feature s utt is a summary of the sequence of token features y 1 -y M .
- the MLP for the hierarchical attention mechanism 380 includes two layers and may include hidden and output dimensions of 320 and 1, respectively.
- the CEM may further estimate a raw word error rate (WER) for the hypothesis H based on an estimate of a number of deletions.
- WER raw word error rate
- a tertiary task may be defined with a ground truth e j as a number of deletions between the j ⁇ 1 and j-th word.
- a number of deletions at any position may include any non-negative integer.
- a MLP may be trained with Poisson regression loss as follows.
- the f(wj) is generated using self-attention, and has linguistic information up to the j-th word. Accordingly, it is well suited for predicting ej, the number of deletions right before the jth word.
- the WER estimate may be computed as follows.
- FIG. 4 shows a schematic view 400 of an example confidence-based routine for selecting an appropriate speech recognizer to transcribe an utterance 12 .
- a first speech recognizer 200 operates as a default speech recognizer for generating a transcription 204 by processing incoming audio data 202 corresponding to an utterance 12 spoken by a user 10 .
- the first speech recognizer 200 may correspond to a local speech recognizer that executes on a user device 110 associated with the user 10 .
- the first speech recognizer 200 also implements the CEM 300 for determining an utterance-level confidence score 350 for a speech recognition result 222 output by the first speech recognizer 200 that corresponds to the transcription 204 .
- the confidence-based routine determines whether the utterance-level confidence score 350 for the utterance 12 transcribed by the first speech recognizer 200 satisfies a confidence threshold.
- utterance-level confidence scores 350 greater than the confidence threshold satisfy the confidence threshold while utterance-level confidence scores 350 less than or equal to the confidence threshold fail to satisfy the confidence threshold.
- the confidence threshold e.g., decision block 450 is “Yes”
- the transcription 204 generated by the first speech recognizer 200 is accepted to achieve on-device gains in quality, latency, and reliability.
- the accepted transcription 204 may display, or continue to display, on the user device 110 and/or be passed to a downstream natural language understanding (NLU) module for interpreting the transcription 204 and performing a related action/operation if necessary.
- NLU natural language understanding
- the confidence-based routine rejects the transcription 204 generated by the first speech recognizer 200 and passes the audio data 202 to the second speech recognizer 402 for processing to re-transcribe the utterance 12 .
- the transcription 204 generated by the second speech recognizer 402 may be passed back to the user device 110 and/or to the downstream NLU module for interpretation.
- the confidence-based routine causes the user device 110 to transmit the audio data 202 to the remote server 410 via a network (not shown) so that the second speech recognizer 402 executing thereon can transcribe the utterance 12 .
- the second speech recognizer 402 may leverage a large language model trained on large-scale language model training data making the second speech recognizer 402 more suitable for recognizing proper nouns or less-common words not present in the training data used to train the first speech recognizer 200 .
- the first speech recognizer 200 is generally more accurate (e.g., achieves lower word error rates) for recognizing short-form utterances than the second speech recognizer 402 implementing the larger language model and lexicon, the first speech recognizer 200 may ultimately be less accurate at recognizing long-tail utterances than the second speech recognizer 402 .
- the confidence-based routine may send all utterances with confidence scores 350 less than the confidence threshold to the second speech recognizer 402 for generating the transcription 204 , and transcribe a majority of utterances on-device 110 using the first speech recognizer 200 to gain quality, latency, and reliability.
- the second speech recognizer 402 could also execute on-device.
- the second speech recognizer 402 may be associated with a more computationally-intensive speech recognizer that may generate more accurate speech recognition results on certain utterances than the first speech recognizer 200 , but at the cost of reduced latency and increased power consumption.
- the confidence-based routine may leverage the second speech recognizer 402 to transcribe utterances 12 when utterance-level confidence scores associated with recognition results generated by the first speech recognizer 200 are less than the confidence threshold.
- a software application may refer to computer software that causes a computing device to perform a task.
- a software application may be referred to as an “application,” an “app,” or a “program.”
- Example applications include, but are not limited to, system diagnostic applications, system management applications, system maintenance applications, word processing applications, spreadsheet applications, messaging applications, media streaming applications, social networking applications, and gaming applications.
- the non-transitory memory may be physical devices used to store programs (e.g., sequences of instructions) or data (e.g., program state information) on a temporary or permanent basis for use by a computing device.
- the non-transitory memory may be volatile and/or non-volatile addressable semiconductor memory. Examples of non-volatile memory include, but are not limited to, flash memory and read-only memory (ROM)/programmable read-only memory (PROM)/erasable programmable read-only memory (EPROM)/electronically erasable programmable read-only memory (EEPROM) (e.g., typically used for firmware, such as boot programs).
- Examples of volatile memory include, but are not limited to, random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), phase change memory (PCM) as well as disks or tapes.
- FIG. 5 is a flowchart of an example arrangement of operations for a method 500 of estimating word-level confidence for a word recognized by a speech recognizer using only a confidence of the final hypothesized sub-word unit for that word.
- the data processing hardware 112 FIG. 1
- the data processing hardware 112 may execute instructions stored on the memory hardware 114 ( FIG. 1 ) to perform the example arrangement of operations for the method 500 .
- the method 500 includes receiving, from a first speech recognizer 200 , a speech recognition result 232 corresponding to a transcription 204 of an utterance 12 spoken by a user 10 .
- the first speech recognizer 200 may be configured in a two-pass decoding architecture as discussed above with reference to FIGS. 2A and 2B .
- the speech recognition result 232 is a highest-scoring candidate hypothesis re-scored by a rescoring decoder of the speech recognizer 200 and includes a sequence of hypothesized sub-word units that form one or more words of the utterance 12 , each sub-word unit output from the speech recognizer 200 at a corresponding output step.
- the method 500 uses a confidence estimation module (CEM) 300 , for each sub-word unit in the sequence of hypothesized sub-word units, the method 500 performs operations 504 , 506 , 508 , 510 .
- the method 500 includes obtaining a respective confidence embedding 242 that represents a set of confidence features associated with the corresponding output step when the corresponding sub-word unit was output from the first speech recognizer 200 .
- the method 500 includes generating, using a first attention mechanism 310 that self-attends to the respective confidence embedding b(y i ) 242 for the corresponding sub-word unit and the confidence embeddings b(y 1 )-b(y 1-i ) obtained for any other sub-word units in the sequence of hypothesized sub-word units that proceed the corresponding sub-word unit, a confidence feature vector SA(b).
- the method 500 includes generating, using a second attention mechanism 320 that cross-attends to a sequence of acoustic encodings e, err 252 each associated with a corresponding acoustic frame x T segmented from audio data 202 that corresponds to the utterance 12 , an acoustic context vector CA(e) 322 .
- the method 500 includes generating, as output from an output layer 340 of the CEM 300 , a respective confidence output score 302 for the corresponding sub-word unit based on the confidence feature vector SA(b) and the acoustic feature vector CA(e) 322 received as input by the output layer of the CEM 300 .
- the method 500 includes determining an utterance-level confidence score 350 for the transcription of the utterance.
- the operations may further include rejecting the transcription 204 output by the first speech recognizer 200 and instructing a second speech recognizer 402 to process audio data 14 corresponding to the utterance to re-transcribe the utterance.
- the second speech recognizer 402 may be more computationally extensive than the first speech recognizer 200 , and thus may be more accurate for performing speech recognition on the audio data 14 than the first speech recognizer 200 .
- the first speech recognizer 200 may execute entirely on-device to provide streaming transcription capabilities with little latency, while the second speech recognizer 402 may execute on a remote server leveraging potentially infinite computing/memory resources for accurately performing speech recognition at reduced latency.
- a low utterance-level confidence score 350 for a transcription 204 output by the first speech recognizer 200 executing on-device may serve as an indicator to invoke the more computationally-intensive second speech recognizer 200 .
- the second speech recognizer 402 could also execute on-device, but may be more computationally-intensive for improving speech recognition accuracy at the cost of reduced-latency and increased power consumption and computing.
- FIG. 6 is schematic view of an example computing device 600 that may be used to implement the systems and methods described in this document.
- the computing device 600 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers.
- the components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document.
- the computing device 600 includes a processor 610 , memory 620 , a storage device 630 , a high-speed interface/controller 640 connecting to the memory 620 and high-speed expansion ports 650 , and a low speed interface/controller 660 connecting to a low speed bus 670 and a storage device 630 .
- Each of the components 610 , 620 , 630 , 640 , 650 , and 660 are interconnected using various busses, and may be mounted on a common motherboard or in other manners as appropriate.
- the processor 610 can process instructions for execution within the computing device 600 , including instructions stored in the memory 620 or on the storage device 630 to display graphical information for a graphical user interface (GUI) on an external input/output device, such as display 680 coupled to high speed interface 640 .
- GUI graphical user interface
- multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory.
- multiple computing devices 600 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system).
- the memory 620 stores information non-transitorily within the computing device 600 .
- the memory 620 may be a computer-readable medium, a volatile memory unit(s), or non-volatile memory unit(s).
- the non-transitory memory 620 may be physical devices used to store programs (e.g., sequences of instructions) or data (e.g., program state information) on a temporary or permanent basis for use by the computing device 600 .
- non-volatile memory examples include, but are not limited to, flash memory and read-only memory (ROM)/programmable read-only memory (PROM)/erasable programmable read-only memory (EPROM)/electronically erasable programmable read-only memory (EEPROM) (e.g., typically used for firmware, such as boot programs).
- volatile memory examples include, but are not limited to, random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), phase change memory (PCM) as well as disks or tapes.
- the storage device 630 is capable of providing mass storage for the computing device 600 .
- the storage device 630 is a computer-readable medium.
- the storage device 630 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations.
- a computer program product is tangibly embodied in an information carrier.
- the computer program product contains instructions that, when executed, perform one or more methods, such as those described above.
- the information carrier is a computer- or machine-readable medium, such as the memory 620 , the storage device 630 , or memory on processor 610 .
- the high speed controller 640 manages bandwidth-intensive operations for the computing device 600 , while the low speed controller 660 manages lower bandwidth-intensive operations. Such allocation of duties is exemplary only.
- the high-speed controller 640 is coupled to the memory 620 , the display 680 (e.g., through a graphics processor or accelerator), and to the high-speed expansion ports 650 , which may accept various expansion cards (not shown).
- the low-speed controller 660 is coupled to the storage device 630 and a low-speed expansion port 690 .
- the low-speed expansion port 690 which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet), may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- input/output devices such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- the computing device 600 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a standard server 600 a or multiple times in a group of such servers 600 a , as a laptop computer 600 b , or as part of a rack server system 600 c.
- implementations of the systems and techniques described herein can be realized in digital electronic and/or optical circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof.
- ASICs application specific integrated circuits
- These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- the processes and logic flows described in this specification can be performed by one or more programmable processors, also referred to as data processing hardware, executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows can also be performed by special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer.
- a processor will receive instructions and data from a read only memory or a random access memory or both.
- the essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- mass storage devices for storing data
- a computer need not have such devices.
- Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks.
- the processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- one or more aspects of the disclosure can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor, or touch screen for displaying information to the user and optionally a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor, or touch screen for displaying information to the user and optionally a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- Other kinds of devices can be used to provide interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input
Abstract
A method including receiving a speech recognition result corresponding to a transcription of an utterance spoken by a user. For each sub-word unit in a sequence of hypothesized sub-word units of the speech recognition result, using a confidence estimation module to: obtain a respective confidence embedding associated with the corresponding output step when the corresponding sub-word unit was output from the first speech recognizer; generate a confidence feature vector; generate an acoustic context vector; and generate a respective confidence output score for the corresponding sub-word unit based on the confidence feature vector and the acoustic feature vector received as input by the output layer of the confidence estimation module. The method also includes determining, based on the respective confidence output score generated for each sub-word unit in the sequence of hypothesized sub-word units, an utterance-level confidence score for the transcription of the utterance.
Description
- This U.S. patent application claims priority under 35 U.S.C. § 119(e) to U.S. Provisional Application 63/166,399, filed on Mar. 26, 2021. The disclosure of this prior application is considered part of the disclosure of this application and is hereby incorporated by reference in its entirety.
- This disclosure relates to multi-task learning for end-to-end automated speech recognition confidence and deletion estimation.
- Modern automatic speech recognition (ASR) systems focus on providing not only quality/accuracy (e.g., low word error rates (WERs)), but also low latency (e.g., a short delay between the user speaking and a transcription appearing). Moreover, when using an ASR system today there is a demand that the ASR system decode utterances in a streaming fashion that corresponds to displaying a transcription of an utterance in real-time, or even faster than real-time, as a user speaks. To illustrate, when an ASR system is deployed on a mobile phone that experiences direct user interactivity, an application on the mobile phone using the ASR system may require the speech recognition to be streaming such that words, word pieces, and/or individual characters appear on the screen as soon as they are spoken. Here, it is also likely that the user of the mobile phone has a low tolerance for latency. Due to this low tolerance, the speech recognition strives to run on the mobile device in a manner that minimizes an impact from latency and inaccuracy that may detrimentally affect the user's experience.
- One aspect of the disclosure provides a computer-implemented method for multi-task learning for end-to-end automated speech recognition confidence and deletion estimation. The computer-implemented method when executed on data processing hardware causes the data processing hardware to perform operations including receiving, from a first speech recognizer, a speech recognition result corresponding to a transcription of an utterance spoken by a user. The speech recognition result includes a sequence of hypothesized sub-word units that form one or more words of the transcription of the utterance, each sub-word unit output from the first speech recognizer at a corresponding output step. Using a confidence estimation module, for each sub-word unit in the sequence of hypothesized sub-word units, the operations also include obtaining a respective confidence embedding associated with the corresponding output step when the corresponding sub-word unit was output from the first speech recognizer. The operations further use the confidence estimation module for generating, using a first attention mechanism that self-attends to the respective confidence embedding for the corresponding sub-word unit and the confidence embeddings obtained for any other sub-word units in the sequence of hypothesized sub-word units that proceed the corresponding sub-word unit, a confidence feature vector. The operations continue to use the confidence estimation module for generating, using a second attention mechanism that cross-attends to a sequence of encodings each associated with a corresponding acoustic frame segmented from audio data that corresponds to the utterance, an acoustic context vector. Further, the operations then use the confidence estimation module for generating, as output from an output layer of the confidence estimation module, a respective confidence output score for the corresponding sub-word unit based on the confidence feature vector and the acoustic feature vector received as input by the output layer of the confidence estimation module. The operations also include determining, based on the respective confidence output score generated for each sub-word unit in the sequence of hypothesized sub-word units, an utterance-level confidence score for the transcription of the utterance.
- Implementations of the disclosure may include one or more of the following optional features. In some implementations, determining the utterance-level confidence score for the transcription of the utterance includes, for each word of the one or more words of the transcription of the utterance, determining a respective word-level confidence score that is equal to the respective confidence output score generated for the final sub-word unit in the corresponding word. These implementations further include aggregating the respective word-level confidence scores determined for each of the one or more words of the transcription to determine the utterance-level confidence score for the transcription of the utterance.
- In some additional implementations, the operations further include estimating, using a hierarchical attention mechanism, a probability that the speech recognition result for the utterance is recognized correctly, determining an utterance-level loss for the speech recognition result based on an estimated probability that the that the speech recognition result for the utterance is recognized correctly and a ground-truth label indicating whether or not the utterance is recognized correctly, and training the confidence estimation model on the utterance-level loss.
- In some examples, the confidence estimation model and the first speech recognizer are trained jointly. The operations may include estimating, at each position in a sequence of words of the utterance, a number of deletions, determining a deletion loss for the speech recognition result and the estimated number of deletions and a ground-truth number of deletions at each position in the sequence of words, and training the confidence estimation model on the deletion loss.
- In some implementations, the operations further include, after training the confidence estimation model, rescoring candidate speech recognition results recognized by the first speech recognizer. The sub-word units may include wordpieces or graphemes.
- In some examples, the first speech recognizer includes a transducer decoder model configured to generate multiple candidate hypotheses, each candidate hypothesis corresponding to a candidate transcription for the utterance and represented by a respective sequence of hypothesized sub-word units. In these implementations, the transducer decoder model may include a Recurrent Neural Network-Transducer (RNN-T) model architecture.
- In some implementations, the operations further include determining whether the utterance-level confidence score for the transcription of the utterance satisfies a confidence threshold. In these implementations, when the utterance-level confidence score for the transcription of the utterance fails to satisfy the confidence threshold: the operations further include rejecting the transcription output from the first speech recognizer and instructing a second speech recognizer to process audio data corresponding to the utterance spoken by the user to re-transcribe the utterance spoken by the user. In these implementations, the first speech recognizer may reside on a user device associated with the user, the second speech recognizer may execute on a remote server in communication with the user device, and the user device may transmit the audio data to the remote server when the utterance-level confidence score for the transcription of the utterance fails to satisfy the confidence threshold. Also in these implementations, the second speech recognizer may be more computationally-intensive than the first speech recognizer.
- Another aspect of the disclosure provides a system for multi-task learning for end-to-end automated speech recognition confidence and deletion estimation. The system includes data processing hardware and memory hardware in communication with the data processing hardware. The memory hardware stores instructions that when executed on the data processing hardware causes the data processing hardware to perform operations including receiving, from a first speech recognizer, a speech recognition result corresponding to a transcription of an utterance spoken by a user. The speech recognition result includes a sequence of hypothesized sub-word units that form one or more words of the transcription of the utterance, each sub-word unit output from the first speech recognizer at a corresponding output step. Using a confidence estimation module, for each sub-word unit in the sequence of hypothesized sub-word units, the operations also include obtaining a respective confidence embedding associated with the corresponding output step when the corresponding sub-word unit was output from the first speech recognizer. The operations also use the confidence estimation module for generating, using a first attention mechanism that self-attends to the respective confidence embedding for the corresponding sub-word unit and the confidence embeddings obtained for any other sub-word units in the sequence of hypothesized sub-word units that proceed the corresponding sub-word unit, a confidence feature vector. The operations continue to use the confidence estimation module for generating, using a second attention mechanism that cross-attends to a sequence of encodings each associated with a corresponding acoustic frame segmented from audio data that corresponds to the utterance, an acoustic context vector. Further, the operations use the confidence estimation module for generating, as output from an output layer of the confidence estimation module, a respective confidence output score for the corresponding sub-word unit based on the confidence feature vector and the acoustic feature vector received as input by the output layer of the confidence estimation module. The operations also include determining, based on the respective confidence output score generated for each sub-word unit in the sequence of hypothesized sub-word units, an utterance-level confidence score for the transcription of the utterance.
- This aspect may include one or more of the following optional features. In some implementations, determining the utterance-level confidence score for the transcription of the utterance includes, for each word of the one or more words of the transcription of the utterance, determining a respective word-level confidence score that is equal to the respective confidence output score generated for the final sub-word unit in the corresponding word. These implementations further include aggregating the respective word-level confidence scores determined for each of the one or more words of the transcription to determine the utterance-level confidence score for the transcription of the utterance.
- In some additional implementations, the operations further include estimating, using a hierarchical attention mechanism, a probability that the speech recognition result for the utterance is recognized correctly, determining an utterance-level loss for the speech recognition result based on an estimated probability that the that the speech recognition result for the utterance is recognized correctly and a ground-truth label indicating whether or not the utterance is recognized correctly, and training the confidence estimation model on the utterance-level loss.
- In some examples, the confidence estimation model and the first speech recognizer are trained jointly. The operations may include estimating, at each position in a sequence of words of the utterance, a number of deletions, determining a deletion loss for the speech recognition result and the estimated number of deletions and a ground-truth number of deletions at each position in the sequence of words, and training the confidence estimation model on the deletion loss.
- In some implementations, the operations further include, after training the confidence estimation model, rescoring candidate speech recognition results recognized by the first speech recognizer. The sub-word units may include wordpieces or graphemes.
- In some implementations, the first speech recognizer includes a transducer decoder model configured to generate multiple candidate hypotheses, each candidate hypothesis corresponding to a candidate transcription for the utterance and represented by a respective sequence of hypothesized sub-word units. In these implementations, the transducer decoder model may include a Recurrent Neural Network-Transducer (RNN-T) model architecture.
- In some examples, the operations further include determining whether the utterance-level confidence score for the transcription of the utterance satisfies a confidence threshold. In these examples, when the utterance-level confidence score for the transcription of the utterance fails to satisfy the confidence threshold, the operations further include rejecting the transcription output from the first speech recognizer and instructing a second speech recognizer to process audio data corresponding to the utterance spoken by the user to re-transcribe the utterance spoken by the user. In these examples, the first speech recognizer may reside on a user device associated with the user, the second speech recognizer may execute on a remote server in communication with the user device, and the user device may transmit the audio data to the remote server when the utterance-level confidence score for the transcription of the utterance fails to satisfy the confidence threshold. Also in these examples, the second speech recognizer may be more computationally-intensive than the first speech recognizer.
- The details of one or more implementations of the disclosure are set forth in the accompanying drawings and the description below. Other aspects, features, and advantages will be apparent from the description and drawings, and from the claims.
-
FIG. 1 is a schematic view of an example speech environment. -
FIG. 2 is a schematic view of an example speech recognizer and a confidence estimation model overlain on the speech recognizer. -
FIG. 3 is a schematic view of training the confidence estimation module ofFIG. 2 using multi-task learning. -
FIG. 4 is a schematic view of an example confidence-based routine for selecting an appropriate speech recognizer based on utterance-level confidence. -
FIG. 5 is a flowchart of an example arrangement of operations for a method of determining an utterance-level confidence score of a transcription output by a speech recognizer -
FIG. 6 is a schematic view of an example computing device that may be used to implement the systems and methods described herein. - Like reference symbols in the various drawings indicate like elements.
- Automated speech recognition (ASR) systems focus on providing not only quality/accuracy (e.g., low word error rates (WERs)), but also low latency (e.g., a short delay between the user speaking and a transcription appearing). Recently, end-to-end (E2E) ASR models, such as the Recurrent Neural Network-Transducer (RNN-T), the transformer or conformer transducer, and attention-based encoder-decoder models, have gained popularity in achieving state-of-the-art performance in accuracy and latency. In contrast to conventional hybrid ASR systems that include separate acoustic, pronunciation, and language models, E2E models apply a sequence-to-sequence approach to jointly learn acoustic and language modeling in a single neural network that is trained end to end from training data, e.g., utterance-transcription pairs.
- Examples of sequence-to-sequence models include “attention-based” models and “listen-attend-spell” (LAS) models. A LAS model transcribes speech utterances into characters using a listener component, an attender component, and a speller component. Here, the listener is a recurrent neural network (RNN) encoder that receives an audio input (e.g., a time-frequency representation of speech input) and maps the audio input to a higher-level feature representation. The attender attends to the higher-level feature to learn an alignment between input features and predicted subword units (e.g., a grapheme or a wordpiece). The speller is an attention-based RNN decoder that generates character sequences from the input by producing a probability distribution over a set of hypothesized words. With an integrated structure, all components of a model may be trained jointly as a single end-to-end (E2E) neural network. Here, an E2E model refers to a model whose architecture is constructed entirely of a neural network. A fully neural network functions without external and/or manually designed components (e.g., finite state transducers, a lexicon, or text normalization modules). Additionally, when training E2E models, these models generally do not require bootstrapping from decision trees or time alignments from a separate system.
- Moreover, when using an ASR system today there is a demand that the ASR system decode utterances in a streaming fashion that corresponds to displaying a transcription of an utterance in real-time, or even faster than real-time, as a user speaks. To illustrate, when an ASR system is deployed on a mobile phone that experiences direct user interactivity, an application on the mobile phone using the ASR system may require the speech recognition to be streaming such that words, word pieces, and/or individual characters appear on the screen as soon as they are spoken. Here, it is also likely that the user of the mobile phone has a low tolerance for latency. Due to this low tolerance, the speech recognition strives to run on the mobile device in a manner that minimizes an impact from latency and inaccuracy that may detrimentally affect the user's experience. However, sequence-to-sequence models such as the LAS model that function by reviewing an entire input sequence of audio before generating output text, do not allow for streaming outputs as inputs are received. Due to this deficiency, deploying the LAS model for speech applications that are latency sensitive and/or require real-time voice transcription may pose issues. This makes an LAS model alone not an ideal model for mobile technology (e.g., mobile phones) that often relies on real-time applications (e.g., real-time communication applications).
- Another form of a sequence-to-sequence model known as a recurrent neural network transducer (RNN-T) does not employ an attention mechanism and, unlike other sequence-to-sequence models that generally need to process an entire sequence (e.g., audio waveform) to produce an output (e.g., a sentence), the RNN-T continuously processes input samples and streams output symbols, a feature that is particularly attractive for real-time communication. For instance, speech recognition with an RNN-T may output characters one-by-one as spoken. Here, an RNN-T uses a feedback loop that feeds symbols predicted by the model back into itself to predict the next symbols. Because decoding the RNN-T includes a beam search through a single neural network instead of a large decoder graph, an RNN-T may scale to a fraction of the size of a server-based speech recognition model. With the size reduction, the RNN-T may be deployed entirely on-device and be able to run offline (i.e., without a network connection); therefore, avoiding unreliability issues with communication networks.
- The RNN-T model alone, however, still lags behind a large state-of-the-art conventional model (e.g., a server-based model with separate AM, PM, and LMs) in terms of quality (e.g., speech recognition accuracy). Yet a non-streaming E2E, LAS model has speech recognition quality that is comparable to large state-of-the-art conventional models. To capitalize on the quality of a non-steaming E2E LAS model, a two-pass speech recognition system (e.g., shown in
FIG. 2A ) was developed that includes a first-pass component of an RNN-T network followed by a second-pass component of a LAS network. With this design, the two-pass model benefits from the streaming nature of an RNN-T model with low latency while improving the accuracy of the RNN-T model through the second-pass incorporating the LAS network. Although the LAS network increases the latency when compared to only a RNN-T model, the increase in latency is reasonably slight and complies with latency constraints for on-device operation. With respect to accuracy, a two-pass model achieves a 17-22% WER reduction when compared to a RNN-T alone and has a similar WER when compared to a large conventional model. - Confidence scores are an important feature of ASR systems that support many downstream applications to mitigate speech recognition errors. For example, unlabeled utterances with recognition results output from an ASR model that achieve high confidence may be used for semi-supervised training of the ASR model which may reduce the expense of using only transcribed utterances for training. On the other hand, in applications such as spoken dialog systems in which a user interacts with a digital assistant executing on a computing device, utterances with recognition results that achieve low word-level confidence may prompt the user to correct any mis-transcribed words. Additionally, recognition results with low confidence may result in passing audio for the corresponding utterance to a different, more computationally extensive ASR model (e.g., server-side) for improving recognition on the utterance.
- While conventional hybrid ASR systems can easily estimate word-level confidence scores from word posterior probabilities computed from lattices or confusion networks and then aggregated to provide an utterance-level confidence, the deep neural networks employed by E2E ASR models tend to exhibit overconfidence when predicting words. As many E2E ASR models are configured to output recognition results at the sub-word level, simply learning confidence scores for each sub-word recognized by the ASR model using a corresponding fixed sub-word tokenization for the word as a reference sequence can lead to incorrect ground truth labels used for training confidence estimation models since recognition results may contain multiple valid tokenizations. For instance, a reference fixed sub-word sequence for the utterance “Good morning” may be “go, od, morn, ing” while a hypothesized sub-word sequence recognized by the ASR model may be “go, od, mor, ning, mom”. Here, even though the word “morning” is correctly recognized by the ASR model, the sub-word labels for the corresponding hypothesized sub-words “mor” and “ning” recognized by the ASR model would be labeled incorrect because they do not match the corresponding reference fixed sub-words “morn” and “ing” for the word “morning”.
- To alleviate the drawbacks associated with estimating sub-word confidence scores for hypothesized sub-word sequences recognized by ASR models due to mismatches between reference fixed sub-word sequences, implementations herein are directed toward a confidence estimation module that applies self-attention in order to estimate word-level confidence for each recognized word using only the confidence of the final hypothesized sub-word unit recognized by the ASR model that makes up the corresponding word. The contents of U.S. application Ser. No. 17/182,592, filed on Feb. 23, 2021 are incorporated by reference in their entirety. Learning only the word-level confidence of each hypothesized word recognized by the ASR model provides an estimate of a ratio of correct words to a total number of words in the hypothesis. However, training labels do not provide any signals about whether the entire utterance is recognized correctly, or whether there are any deletions in the hypothesis recognized by the ASR model. Thus, estimating word-level confidence does not model deletions, and an overall utterance confidence based on an aggregation of the word-level confidences for the words in the utterance, discards many useful word-level training signals. Implementations herein are further directed toward applying multi-task learning for joint training the confidence model using objectives related to word training, utterance training, and deletion training.
-
FIG. 1 is anexample speech environment 100 in which auser 10 interacts with auser device 110 through voice input. The user device 110 (also referred to generally as device 110) includes a computing device that is configured to capture sounds (e.g., streaming audio data) from one ormore users 10 within the speech-enabledenvironment 100. Here, the streamingaudio data 202 may refer to a spoken utterance by theuser 10 that functions as an audible query, a command for thedevice 110, or an audible communication captured by thedevice 110. Speech-enabled systems of thedevice 110 may field the query or the command by answering the query and/or causing the command to be performed. - The
user device 110 may correspond to any computing device capable of receivingaudio data 202. Some examples ofuser devices 110 include, but are not limited to, mobile devices (e.g., mobile phones, tablets, laptops, etc.), computers, wearable devices (e.g., smart watches), smart appliances, internet of things (IoT) devices, smart speakers/displays, vehicle infotainment systems, etc. Theuser device 110 includesdata processing hardware 112 andmemory hardware 114 in communication with thedata processing hardware 112 and storing instructions, that when executed by thedata processing hardware 112, cause thedata processing hardware 112 to perform one or more operations. Theuser device 110 further includes anaudio subsystem 116 with an audio capture device (e.g., microphone) 116, 116 a for capturing and converting spokenutterances 12 within the speech-enabledsystem 100 into electrical signals and a speech output device (e.g., a speaker) 116, 116 b for communicating an audible audio signal (e.g., as output audio data from the device 110). While theuser device 110 implements a single audio capture device 116 a in the example shown, theuser device 110 may implement an array of audio capture devices 116 a without departing from the scope of the present disclosure, whereby one or more capture devices 116 a in the array may not physically reside on theuser device 110, but be in communication with theaudio subsystem 116. In the example shown, the user device 110 (e.g., using thehardware 112, 114) implements aspeech recognizer 200 that is configured to perform speech recognition onaudio data 202 corresponding to anutterance 12 spoken by theuser 10. Here, the audio capture device 116 a is configured to capture acoustic sounds representing theutterance 12 and convert the acoustic sounds into theaudio data 202 associated with a digital format compatible with thespeech recognizer 200. The digital format associated with theaudio data 202 may correspond to acoustic frames (e.g., parameterized acoustic frames), such as mel frames. For instance, the parameterized acoustic frames correspond to log-mel filterbank energies. - While
FIG. 1 shows theuser device 100 implementing thespeech recognizer 200 for performing speech recognition on-device, other implementations include a remote server 410 (FIG. 4 ) implementing thespeech recognizer 200 by processing theaudio data 202 transmitted by theuser device 110 via a network and providing atranscription 204 of theaudio data 202 back to theuser device 110. In some additional implementations, theuser device 110 utilizes both alocal speech recognizer 200 residing on theuser device 110 and a server-side speech recognizer 402 (FIG. 4 ) that executes on theremote server 410. Here,user device 110 may use thelocal speech recognizer 200 when a network connection is not available or for speech applications that are latency sensitive and/or require streaming transcription, while the server-side speech recognizer 402 may be leveraged when additional resources are required to improve speech recognition accuracy as described in greater detail below with reference toFIG. 4 . - In some examples, the
user 10 interacts with a program orapplication 118 executing on theuser device 110 that uses thespeech recognizer 200. For instance,FIG. 1 depicts theuser 10 communicating with anautomated assistant application 118. In this example, the user (e.g., Bill) 10 greets the automatedassistant application 118 by speaking anutterance 12, “Good morning”, that is captured by the audio capture device 116 a and converted into corresponding audio data 202 (e.g. as acoustic frames) for processing by thespeech recognizer 200. In this example, thespeech recognizer 200 transcribes theaudio data 202 representing theutterance 12 into a transcription 204 (e.g., a text representation of “Good morning”). Here, theautomated assistant application 118 may apply natural language processing on thetranscription 204 to generate aresponse 119 for output to theuser 10 that conveys the message, “Good Morning Bill, the first meeting today on your calendar is at 9:00 AM.” Natural language processing generally refers to a process of interpreting written language (e.g., the transcription 204) and determining whether the written language prompts any action. In this example, theassistant application 118 uses natural language processing to recognize that theutterance 12 spoken by theuser 10 is intended to invoke theassistant application 118 to accesses a calendar application of theuser 10 and provide theresponse 119 indicating what time the user's 10 first meeting is today. That is, by recognizing these details with natural language processing, theassistant application 118 returns theresponse 119 to theuser 12 as a synthesized speech representation for audible output through the audio output device 116 a and/or as text for display on a screen in communication with theuser device 110. In some examples, theuser device 110displays transcriptions 204 ofutterances 12 spoken by theuser 10 andcorresponding responses 119 from theassistant application 118 as a conversation on the screen. In some configurations, natural language processing may occur on a remote system in communication with thedata processing hardware 112 of theuser device 110. - In some examples, the
speech recognizer 200 processesincoming audio data 202 in real-time to provide astreaming transcriptions 204. Here, thespeech recognizer 200 is configured to produce a sequence of hypothesized sub-word units that make up the words of theutterance 12 spoken by theuser 10. The hypothesized sub-word units may include word pieces or individual characters (e.g., graphemes). In the example shown, the sequence of hypothesized sub-word units recognized by the speech recognizer include “SOS_go od_mor ning” in which the ‘SOS’ indicates a start of speech tag and each word boundary indicator (‘_’) indicates a beginning/starting sub-word unit for each word. - Referring to
FIG. 2 , in some implementations, aspeech recognizer 200 includes a recurrent neural network-transducer (RNN-T)decoder 220 for predicting a speech recognition result/hypothesis 222 and implements a confidence estimation module (CEM) 300 for estimating aconfidence 302 of the speech recognition result/hypothesis 222. TheCEM 300 may use the utterance-level confidence 302 to rescore the speech recognition result/hypothesis 222 predicted by thespeech recognizer 200. Here, the speech recognition result/hypothesis 222 corresponds to a sequence of sub-word units, such as word pieces or graphemes, that when aggregated together form atranscription 204 for an utterance. Generally speaking, thespeech recognizer 200 includes anencoder 210 and the RNN-T decoder 220, whereby the RNN-T decoder 220 includes a prediction network and a joint network. The RNN-T decoder 220 may produce multiple candidate hypotheses H asoutput 222 and theCEM 300 may rescores/re-ranks a top-K candidate hypotheses H to identify a highest scoring candidate hypothesis as a final recognition result corresponding to the transcription 204 (FIG. 1 ). Moreover, while examples herein depict thedecoder 220 having an RNN-T model architecture, thedecoder 220 may similarly include other types of transducer model architectures without departing from the scope of the present disclosure. For instance, thedecoder 220 may include one of Transformer-Transducer, Convolutional Neural Network-Transducer (ConvNet-Transducer), or Conformer-Transducer model architectures in lieu of the RNN-T model architecture. - The at least one shared
encoder 210 is configured to receive, as input, theaudio data 202 corresponding to theutterance 12 as a sequence of acoustic frames. The acoustic frames may be previously processed by theaudio subsystem 116 into parameterized acoustic frames (e.g., mel frames and/or spectral frames). In some implementations, the parameterized acoustic frames correspond to log-mel filterbank energies with log-mel features. For instance, the parameterized input acoustic frames representing theaudio data 202 input into theencoder 210 may be represented as x=(x1, . . . , xT), where xtϵaudio data 202 input to theencoder 210, theencoder 210 is configured to generate, asoutput 212, a corresponding encoding e1:T. Each of the number of frames in x denoted by T corresponds to a respective time step. - Although the structure of the
encoder 210 may be implemented in different ways, in some implementations, theencoder 210 includes a long-short term memory (LSTM) neural network. For instance, the LSTM neural network may include eight (8) LSTM layers. Here, each layer may have 2,048 hidden units followed by a 640-dimensional projection layer. In some examples, a time-reduction layer is inserted with the reduction factor N=2 after the second LSTM layer of theencoder 210. - In some implementations, the RNN-
T decoder 220 includes a prediction network and a joint network. Here, the prediction network may have two LSTM layers of 2,048 hidden units and a 640-dimensional projection per layer as well as an embedding layer of 128 units. Theoutputs 212 of the sharedencoder 210 and the prediction network may be fed into the joint network that includes a softmax predicting layer. In some examples, the joint network of the RNN-T decoder 220 includes 640 hidden units followed by a softmax layer that predicts 4,096 mixed-case word pieces. - The RNN-
T decoder 220 receives, as input, the encoding e generated asoutput 212 from theencoder 210 for each acoustic frame x and generates, asoutput 222, one or more candidate hypotheses H each represented by a respective sequence of hypothesized sub-word units y1, y2, y3, . . . , yM. For instance, in the example where theuser 10 utters “Good morning”, one candidate hypothesis H may include a first sequence of hypothesized sub-word units [_go, od, _mor, ning] and another candidate hypothesis H may include a second sequence of hypothesized sub-word units [_go, od, _morn, ing]. There may be a multitude of other candidate hypothesis H as well. Here, the respective sequence of hypothesized sub-word units y1, y2, y3, . . . , yM representing each candidate hypothesis H corresponds to a candidate transcription for theutterance 12. Each sub-word unit yi in each respective sequence of hypothesized sub-word units y1, y2, y3, . . . , yM denotes a probability distribution over possible sub-units. The sub-word unit with the highest probability in the probability distribution may be selected as the hypothesized sub-word in the respective sequence of hypothesized sub-word units. - With continued reference to
FIG. 2 , theCEM 300 may receive, as input, the sequence of encodings e,e 1:T 212 output from theencoder 210 and the top-K candidate hypotheses H generated asoutput 222 from the RNN-T decoder 220. In one example, K is equal to four (4) so that the top four candidate hypotheses H are provided as input to theCEM 300 for rescoring by determining an utterance-level confidence 350 for each hypotheses H. The candidate hypothesis H associated with ahighest confidence 350 may be output as thetranscription 204. - To decrease the size of the softmax layer, the sub-word unit vocabulary of possible sub-word units is typically smaller compared to a word vocabulary. The sub-word unit vocabulary may include graphemes or wordpieces (WP). An example WP vocabulary may include 4,096 WPs. While examples of the present disclosure use WPs as the sub-word units generated as output from the speech recognizer, graphemes can be similarly utilized as the sub-word units output from the speech recognizer without departing from the scope of the present disclosure. Accordingly, to compute a word error rate (WER) for a candidate hypothesis H, the respective sequence of hypothesized sub-word units (e.g., WPs) needs to be converted into its corresponding word sequence w1, w2, . . . , wL. This procedure for converting a sub-word sequence into a word sequence is uniquely determined since the first sub-word unit (e.g., WP) of each word begins with the word boundary indicator (‘_’). Similarly, for a word wj including Qj WPs, where yj, q denotes the q-th WP of the j-th word, a simple technique for computing word confidence can be expressed as follows.
-
C word(w j)=agg(c(y j,1), . . . ,c(y j ,Q j) (1) - Wherein agg can be arithmetic mean, minimum, product, or a neural network. However, since each word wj can be divided into multiple different valid WP combinations due to a mismatch between WP correctness and word correctness, using Equation 1 to estimate word-level confidence creates an undesirable computational burden during training since a search over all possible reference tokenizations for the one having a fewest WP edits is required. As used herein, a WP edit includes a correct (cor) label when a hypothesized WP matches a reference WP, a substitution (sub) label when a valid hypothesized WP does not match a reference WP, and an insertion (ins) when a hypothesized WP is misrecognized. Table 1 below shows an example where the word “morning” is correctly transcribed, but results in two substitutions in the WP edit distance output.
- To cure the inherent mismatch between WP correctness and word correctness resulting from
speech recognizers 200 that output at the WP level as depicted in Table 1, implementations herein are directed toward a transformer-basedCEM 300 that leverages a confidence output at the final WP of every word as a word-level confidence while ignoring the confidence of all other preceding WPs of every word. During training, thespeech recognizer 200 is frozen and ground-truth WP labels of correct, insertion, and substitution are used. -
FIG. 3 shows an example of the transformer-basedCEM 300 overlain on top of thespeech recognizer 200. For clarity,FIG. 3 depicts only the actions of theCEM 300 predicting a confidence output c(yi) 302 for the i-th WP in a respective sequence of hypothesized WPs y1, y2, y3, . . . , yM output by the RNN-T decoder 220. Specifically,FIG. 3 depicts theCEM 300 predicting the confidence output c(“ning”) for the “ning” WP in the respective sequence of hypothesized WPs [_go, od, _mor, ning] that converts into the corresponding word sequence “good morning”. - The RNN-
T decoder 220 generates, asoutput 222, one or more candidate hypotheses H each represented by a respective sequence of hypothesized sub-word units y1, y2, y3, . . . , yM. Here, the RNN-T decoder 220 may generate four candidate hypotheses asoutput 222. In the example shown, for theutterance 12 “Good morning”, one candidate hypothesis H generated asoutput 222 from the RNN-T decoder 220 may include a first sequence of hypothesized sub-word units [_go, od, _mor, ning] and another candidate hypothesis H may include a second sequence of hypothesized sub-word units [_go, od, _morn, ing]. At the same time, theencoder 210 generates the sequence of encodings e1:T 212 conveying acoustic context where T corresponds to a number of acoustic frames x segmented from theutterance 12. - For each sub-word unit (yi) in the sequence of hypothesized sub-word (e.g., WP) units [sos_go, od, _mor, ning eos] representing the candidate hypothesis H generated as
output 222 from the RNN-T decoder 220, a confidence embedding b(yi) 301 representing a set of confidence features obtained from thespeech recognizer 200 is provided as input to theCEM 300 for determining a respective confidence output c(yi) 302. In the example shown, the i-th sub-word unit corresponds to the WP “ning”. Here, the confidence embedding b(yi) 301 conveys one or more of an input subword+positional embedding Emb(yi) feature (e.g., Emb(ning)), a log posterior log (p(yi)) feature, and a top-K(i) feature. The log posterior log (p(y1)) feature indicates a probability value associated with the probability/likelihood that sub-word unity, includes the WP “ning”, and the top-K(i) feature indicates the K largest log probabilities at decoder index (e.g., time step) i. Stated differently, the top-K(i) feature provides probability values for each candidate hypothesis H in the top-K at decoder index (e.g., time step) i. Since both thespeech recognizer 200 and theCEM 300 are configured to generate an output for each time step at the sub-word (e.g., WP) level, implementing theCEM 300 as a transformer permits: (1) the use of word edit distance output as ground truth training labels (i.e., correction, substitution, and insertion labels) by leveraging the confidence output c(yj, Qj) at the final WP of every word cword(wj) as a dedicated word-level confidence 302; and (2) the incorporation of information/features from every WP that makes up the word. In the example shown, a self-attention mechanism 310 of the transformer-basedCEM 300 applies self-attention to a confidence feature vector b based on the confidence embedding b(yi) 301 for the i-th sub-word unit corresponding to the WP “ning” as well as confidence embeddings for earlier sub-word units in the same word. The confidence feature vector b may be expressed as follows. -
b={b(y 1), . . . ,b(y i)} (2) - Additionally, an acoustic cross-attention mechanism 320 of the transformer-based
CEM 300 applies acoustic cross-attention (CA(e)) to the sequence of encodings e,e 1:T 212 output from theencoder 210 to generate anacoustic context vector 322 for improving the accuracy in estimating the respective confidence output c(yi) 302 for the i-th sub-word unit corresponding the WP “ning”. Finally, aSoftmax output layer 340 uses the self-attention confidence feature vector SA(b) and the cross-attention CA(e)acoustic context vector 322 to permit the transformer-basedCEM 300 to produce the dedicated confidence output c(yi) 302 for the i-th sub-word unit as follows. -
[f(y 1), . . . ,f(y M)]=Transformer(CA(e),SA(b)) (3) -
[c(y i),i(y i),s(y i)]=Softmax(f(y i)), (4) - where the
Softmax output layer 340 is trained with the correction, substitution, and insertion ground-truth WP labels associated with WP edit distance. TheCEM 300 may determine aconfidence 302 for each word cword(wj) using a confidence output c(yj, Qj) at the final WP and estimate a word correct ratio ( -
- The
CEM 300 may be trained jointly with thespeech recognizer 200, or theCEM 300 and the speech recognizer may be trained separately from one another. In some examples, theCEM 300 is trained using a binary cross-entropy word-level loss as follows. - where Table 1 shows that d(wj) is equal to one when a Levenshtein word-level edit distance for the word wj outputs the “correct” (cor) label when the hypothesized word matches the reference word, and d(wj) is equal to zero when the Levenshtein word-level edit distance for the word wj outputs the “insertion” (ins) or “substitution” (sub) labels when the hypothesized word does not match the reference word. Notably, since the
speech recognizer 200 and theCEM 300 output at the sub-word level (e.g., output every WP), Table 1 also shows theCEM 300 applying an end-of word mask loss m to focus only on the final WP making up the word and ignore WP losses associated with earlier WPs that make up the same word. - The CEM may be further trained on an utterance-level loss using the following ground truth.
-
- Here, the presence of any deletions causes a value of e to be zero, and the signal is back propagated to internal features of the
CEM 300. To extract utterance features and make the prediction, ahierarchical attention mechanism 380 performs the following: -
- wherein parameters W1, b, and w2 that generate sutt can be trained with binary cross-entropy loss: L=−[e log sutt+(1−e) log(1−sutt)] where sutt provides an estimate of the probability that an entire utterance is recognized with zero WER, which is useful for ranking utterances. The sutt does not, however, yield an estimate of the raw WER. Intuitively, the utterance feature sutt is a summary of the sequence of token features y1-yM. The MLP for the
hierarchical attention mechanism 380 includes two layers and may include hidden and output dimensions of 320 and 1, respectively. - The CEM may further estimate a raw word error rate (WER) for the hypothesis H based on an estimate of a number of deletions. Here, a tertiary task may be defined with a ground truth ej as a number of deletions between the j−1 and j-th word. Generally, a number of deletions at any position may include any non-negative integer. To extract deletion features and make predictions, a MLP may be trained with Poisson regression loss as follows.
-
r w(w j)=MLP(f(w j)) (12) - the f(wj) is generated using self-attention, and has linguistic information up to the j-th word. Accordingly, it is well suited for predicting ej, the number of deletions right before the jth word.
- Accordingly, the WER estimate may be computed as follows.
-
-
FIG. 4 shows aschematic view 400 of an example confidence-based routine for selecting an appropriate speech recognizer to transcribe anutterance 12. In the example shown, afirst speech recognizer 200 operates as a default speech recognizer for generating atranscription 204 by processingincoming audio data 202 corresponding to anutterance 12 spoken by auser 10. Thefirst speech recognizer 200 may correspond to a local speech recognizer that executes on auser device 110 associated with theuser 10. Thefirst speech recognizer 200 also implements theCEM 300 for determining an utterance-level confidence score 350 for aspeech recognition result 222 output by thefirst speech recognizer 200 that corresponds to thetranscription 204. - In some implementations, the confidence-based routine determines whether the utterance-
level confidence score 350 for theutterance 12 transcribed by thefirst speech recognizer 200 satisfies a confidence threshold. In the example shown, utterance-level confidence scores 350 greater than the confidence threshold satisfy the confidence threshold while utterance-level confidence scores 350 less than or equal to the confidence threshold fail to satisfy the confidence threshold. When the utterance-level confidence score 350 satisfies (e.g., is greater than) the confidence threshold (e.g.,decision block 450 is “Yes”), then thetranscription 204 generated by thefirst speech recognizer 200 is accepted to achieve on-device gains in quality, latency, and reliability. Here, the acceptedtranscription 204 may display, or continue to display, on theuser device 110 and/or be passed to a downstream natural language understanding (NLU) module for interpreting thetranscription 204 and performing a related action/operation if necessary. - When the utterance-
level confidence score 350 fails to satisfy (e.g., is less than) the confidence threshold (e.g.,decision block 450 is “No”), then the confidence-based routine rejects thetranscription 204 generated by thefirst speech recognizer 200 and passes theaudio data 202 to thesecond speech recognizer 402 for processing to re-transcribe theutterance 12. Thetranscription 204 generated by thesecond speech recognizer 402 may be passed back to theuser device 110 and/or to the downstream NLU module for interpretation. In examples where thefirst speech recognizer 200 is local and executing on-device 110 and thesecond speech recognizer 402 is server-side and executing on aremote server 410, the confidence-based routine causes theuser device 110 to transmit theaudio data 202 to theremote server 410 via a network (not shown) so that thesecond speech recognizer 402 executing thereon can transcribe theutterance 12. Thesecond speech recognizer 402 may leverage a large language model trained on large-scale language model training data making thesecond speech recognizer 402 more suitable for recognizing proper nouns or less-common words not present in the training data used to train thefirst speech recognizer 200. - In some examples, the
first speech recognizer 200 is generally more accurate (e.g., achieves lower word error rates) for recognizing short-form utterances than thesecond speech recognizer 402 implementing the larger language model and lexicon, thefirst speech recognizer 200 may ultimately be less accurate at recognizing long-tail utterances than thesecond speech recognizer 402. As thus, the confidence-based routine may send all utterances withconfidence scores 350 less than the confidence threshold to thesecond speech recognizer 402 for generating thetranscription 204, and transcribe a majority of utterances on-device 110 using thefirst speech recognizer 200 to gain quality, latency, and reliability. - While the
second speech recognizer 402 is shown as being server-side, thesecond speech recognizer 402 could also execute on-device. For instance, thesecond speech recognizer 402 may be associated with a more computationally-intensive speech recognizer that may generate more accurate speech recognition results on certain utterances than thefirst speech recognizer 200, but at the cost of reduced latency and increased power consumption. As such, the confidence-based routine may leverage thesecond speech recognizer 402 to transcribeutterances 12 when utterance-level confidence scores associated with recognition results generated by thefirst speech recognizer 200 are less than the confidence threshold. - A software application (i.e., a software resource) may refer to computer software that causes a computing device to perform a task. In some examples, a software application may be referred to as an “application,” an “app,” or a “program.” Example applications include, but are not limited to, system diagnostic applications, system management applications, system maintenance applications, word processing applications, spreadsheet applications, messaging applications, media streaming applications, social networking applications, and gaming applications.
- The non-transitory memory may be physical devices used to store programs (e.g., sequences of instructions) or data (e.g., program state information) on a temporary or permanent basis for use by a computing device. The non-transitory memory may be volatile and/or non-volatile addressable semiconductor memory. Examples of non-volatile memory include, but are not limited to, flash memory and read-only memory (ROM)/programmable read-only memory (PROM)/erasable programmable read-only memory (EPROM)/electronically erasable programmable read-only memory (EEPROM) (e.g., typically used for firmware, such as boot programs). Examples of volatile memory include, but are not limited to, random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), phase change memory (PCM) as well as disks or tapes.
-
FIG. 5 is a flowchart of an example arrangement of operations for amethod 500 of estimating word-level confidence for a word recognized by a speech recognizer using only a confidence of the final hypothesized sub-word unit for that word. The data processing hardware 112 (FIG. 1 ) may execute instructions stored on the memory hardware 114 (FIG. 1 ) to perform the example arrangement of operations for themethod 500. Atoperation 502, themethod 500 includes receiving, from afirst speech recognizer 200, a speech recognition result 232 corresponding to atranscription 204 of anutterance 12 spoken by auser 10. Thefirst speech recognizer 200 may be configured in a two-pass decoding architecture as discussed above with reference toFIGS. 2A and 2B . Here, the speech recognition result 232 is a highest-scoring candidate hypothesis re-scored by a rescoring decoder of thespeech recognizer 200 and includes a sequence of hypothesized sub-word units that form one or more words of theutterance 12, each sub-word unit output from thespeech recognizer 200 at a corresponding output step. - Using a confidence estimation module (CEM) 300, for each sub-word unit in the sequence of hypothesized sub-word units, the
method 500 performsoperations operation 504, themethod 500 includes obtaining a respective confidence embedding 242 that represents a set of confidence features associated with the corresponding output step when the corresponding sub-word unit was output from thefirst speech recognizer 200. Atoperation 506, themethod 500 includes generating, using afirst attention mechanism 310 that self-attends to the respective confidence embedding b(yi) 242 for the corresponding sub-word unit and the confidence embeddings b(y1)-b(y1-i) obtained for any other sub-word units in the sequence of hypothesized sub-word units that proceed the corresponding sub-word unit, a confidence feature vector SA(b). Atoperation 508, themethod 500 includes generating, using a second attention mechanism 320 that cross-attends to a sequence of acoustic encodings e, err 252 each associated with a corresponding acoustic frame xT segmented fromaudio data 202 that corresponds to theutterance 12, an acoustic context vector CA(e) 322. Atoperation 510, themethod 500 includes generating, as output from anoutput layer 340 of theCEM 300, a respectiveconfidence output score 302 for the corresponding sub-word unit based on the confidence feature vector SA(b) and the acoustic feature vector CA(e) 322 received as input by the output layer of theCEM 300. - At
operation 512, based on the respective confidence output score generated for each sub-word unit in the sequence of hypothesized sub-word units, themethod 500 includes determining an utterance-level confidence score 350 for the transcription of the utterance. When the utterance-level confidence score 350 is less that a confidence threshold, the operations may further include rejecting thetranscription 204 output by thefirst speech recognizer 200 and instructing asecond speech recognizer 402 to process audio data 14 corresponding to the utterance to re-transcribe the utterance. Thesecond speech recognizer 402 may be more computationally extensive than thefirst speech recognizer 200, and thus may be more accurate for performing speech recognition on the audio data 14 than thefirst speech recognizer 200. For instance, thefirst speech recognizer 200 may execute entirely on-device to provide streaming transcription capabilities with little latency, while thesecond speech recognizer 402 may execute on a remote server leveraging potentially infinite computing/memory resources for accurately performing speech recognition at reduced latency. As thus, a low utterance-level confidence score 350 for atranscription 204 output by thefirst speech recognizer 200 executing on-device may serve as an indicator to invoke the more computationally-intensivesecond speech recognizer 200. Without departing from the scope of the present disclosure, thesecond speech recognizer 402 could also execute on-device, but may be more computationally-intensive for improving speech recognition accuracy at the cost of reduced-latency and increased power consumption and computing. -
FIG. 6 is schematic view of anexample computing device 600 that may be used to implement the systems and methods described in this document. Thecomputing device 600 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers. The components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document. - The
computing device 600 includes aprocessor 610,memory 620, astorage device 630, a high-speed interface/controller 640 connecting to thememory 620 and high-speed expansion ports 650, and a low speed interface/controller 660 connecting to a low speed bus 670 and astorage device 630. Each of thecomponents processor 610 can process instructions for execution within thecomputing device 600, including instructions stored in thememory 620 or on thestorage device 630 to display graphical information for a graphical user interface (GUI) on an external input/output device, such asdisplay 680 coupled tohigh speed interface 640. In other implementations, multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory. Also,multiple computing devices 600 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system). - The
memory 620 stores information non-transitorily within thecomputing device 600. Thememory 620 may be a computer-readable medium, a volatile memory unit(s), or non-volatile memory unit(s). Thenon-transitory memory 620 may be physical devices used to store programs (e.g., sequences of instructions) or data (e.g., program state information) on a temporary or permanent basis for use by thecomputing device 600. Examples of non-volatile memory include, but are not limited to, flash memory and read-only memory (ROM)/programmable read-only memory (PROM)/erasable programmable read-only memory (EPROM)/electronically erasable programmable read-only memory (EEPROM) (e.g., typically used for firmware, such as boot programs). Examples of volatile memory include, but are not limited to, random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), phase change memory (PCM) as well as disks or tapes. - The
storage device 630 is capable of providing mass storage for thecomputing device 600. In some implementations, thestorage device 630 is a computer-readable medium. In various different implementations, thestorage device 630 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations. In additional implementations, a computer program product is tangibly embodied in an information carrier. The computer program product contains instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer- or machine-readable medium, such as thememory 620, thestorage device 630, or memory onprocessor 610. - The
high speed controller 640 manages bandwidth-intensive operations for thecomputing device 600, while thelow speed controller 660 manages lower bandwidth-intensive operations. Such allocation of duties is exemplary only. In some implementations, the high-speed controller 640 is coupled to thememory 620, the display 680 (e.g., through a graphics processor or accelerator), and to the high-speed expansion ports 650, which may accept various expansion cards (not shown). In some implementations, the low-speed controller 660 is coupled to thestorage device 630 and a low-speed expansion port 690. The low-speed expansion port 690, which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet), may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter. - The
computing device 600 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as astandard server 600 a or multiple times in a group ofsuch servers 600 a, as alaptop computer 600 b, or as part of arack server system 600 c. - Various implementations of the systems and techniques described herein can be realized in digital electronic and/or optical circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof. These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- These computer programs (also known as programs, software, software applications or code) include machine instructions for a programmable processor, and can be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. As used herein, the terms “machine-readable medium” and “computer-readable medium” refer to any computer program product, non-transitory computer readable medium, apparatus and/or device (e.g., magnetic discs, optical disks, memory, Programmable Logic Devices (PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term “machine-readable signal” refers to any signal used to provide machine instructions and/or data to a programmable processor.
- The processes and logic flows described in this specification can be performed by one or more programmable processors, also referred to as data processing hardware, executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks. However, a computer need not have such devices. Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- To provide for interaction with a user, one or more aspects of the disclosure can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor, or touch screen for displaying information to the user and optionally a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices can be used to provide interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input. In addition, a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web browser on a user's client device in response to requests received from the web browser.
- A number of implementations have been described. Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the disclosure. Accordingly, other implementations are within the scope of the following claims.
Claims (26)
1. A computer-implemented method when executed on data processing hardware causes the data processing hardware to perform operations comprising:
receiving, from a first speech recognizer, a speech recognition result corresponding to a transcription of an utterance spoken by a user, the speech recognition result comprising a sequence of hypothesized sub-word units that form one or more words of the transcription of the utterance, each sub-word unit output from the first speech recognizer at a corresponding output step;
using a confidence estimation module, for each sub-word unit in the sequence of hypothesized sub-word units:
obtaining a respective confidence embedding associated with the corresponding output step when the corresponding sub-word unit was output from the first speech recognizer;
generating, using a first attention mechanism that self-attends to the respective confidence embedding for the corresponding sub-word unit and the confidence embeddings obtained for any other sub-word units in the sequence of hypothesized sub-word units that proceed the corresponding sub-word unit, a confidence feature vector;
generating, using a second attention mechanism that cross-attends to a sequence of encodings each associated with a corresponding acoustic frame segmented from audio data that corresponds to the utterance, an acoustic context vector; and
generating, as output from an output layer of the confidence estimation module, a respective confidence output score for the corresponding sub-word unit based on the confidence feature vector and the acoustic feature vector received as input by the output layer of the confidence estimation module; and
determining, based on the respective confidence output score generated for each sub-word unit in the sequence of hypothesized sub-word units, an utterance-level confidence score for the transcription of the utterance.
2. The computer-implemented method of claim 1 , wherein determining the utterance-level confidence score for the transcription of the utterance comprises:
for each word of the one or more words of the transcription of the utterance, determining a respective word-level confidence score that is equal to the respective confidence output score generated for the final sub-word unit in the corresponding word; and
aggregating the respective word-level confidence scores determined for each of the one or more words of the transcription to determine the utterance-level confidence score for the transcription of the utterance.
3. The computer-implemented method of claim 1 , where the operations further comprise:
estimating, using a hierarchical attention mechanism, a probability that the speech recognition result for the utterance is recognized correctly;
determining an utterance-level loss for the speech recognition result based on an estimated probability that the that the speech recognition result for the utterance is recognized correctly and a ground-truth label indicating whether or not the utterance is recognized correctly; and
training the confidence estimation model on the utterance-level loss.
4. The computer-implemented method of claim 1 , wherein the confidence estimation model and the first speech recognizer are trained jointly.
5. The computer-implemented method of claim 1 , wherein the operations further comprise:
estimating, at each position in a sequence of words of the utterance, a number of deletions;
determining a deletion loss for the speech recognition result and the estimated number of deletions and a ground-truth number of deletions at each position in the sequence of words; and
training the confidence estimation model on the deletion loss.
6. The computer-implemented method of claim 1 , wherein the operations further comprise, after training the confidence estimation model, rescoring candidate speech recognition results recognized by the first speech recognizer.
7. The computer-implemented method of claim 1 , wherein the sub-word units comprise wordpieces.
8. The computer-implemented method of claim 1 , wherein the sub-word units comprise graphemes.
9. The computer-implemented method of claim 1 , wherein the first speech recognizer comprises a transducer decoder model configured to generate multiple candidate hypotheses, each candidate hypothesis corresponding to a candidate transcription for the utterance and represented by a respective sequence of hypothesized sub-word units.
10. The computer-implemented method of claim 9 , wherein the transducer decoder model comprises a Recurrent Neural Network-Transducer (RNN-T) model architecture.
11. The computer-implemented method of claim 1 , wherein the operations further comprise:
determining whether the utterance-level confidence score for the transcription of the utterance satisfies a confidence threshold; and
when the utterance-level confidence score for the transcription of the utterance fails to satisfy the confidence threshold:
rejecting the transcription output from the first speech recognizer; and
instructing a second speech recognizer to process audio data corresponding to the utterance spoken by the user to re-transcribe the utterance spoken by the user.
12. The computer-implemented method of claim 11 , wherein:
the first speech recognizer resides on a user device associated with the user;
the second speech recognizer executes on a remote server in communication with the user device; and
the user device transmits the audio data to the remote server when the utterance-level confidence score for the transcription of the utterance fails to satisfy the confidence threshold.
13. The computer-implemented method of claim 11 , wherein the second speech recognizer is more computationally-intensive than the first speech recognizer.
14. A system comprising:
data processing hardware; and
memory hardware in communication with the data processing hardware and storing instructions that when executed on the data processing hardware cause the data processing hardware to perform operations comprising:
receiving, from a first speech recognizer, a speech recognition result corresponding to a transcription of an utterance spoken by a user, the speech recognition result comprising a sequence of hypothesized sub-word units that form one or more words of the transcription of the utterance, each sub-word unit output from the first speech recognizer at a corresponding output step;
using a confidence estimation module, for each sub-word unit in the sequence of hypothesized sub-word units:
obtaining a respective confidence embedding associated with the corresponding output step when the corresponding sub-word unit was output from the first speech recognizer;
generating, using a first attention mechanism that self-attends to the respective confidence embedding for the corresponding sub-word unit and the confidence embeddings obtained for any other sub-word units in the sequence of hypothesized sub-word units that proceed the corresponding sub-word unit, a confidence feature vector;
generating, using a second attention mechanism that cross-attends to a sequence of encodings each associated with a corresponding acoustic frame segmented from audio data that corresponds to the utterance, an acoustic context vector; and
generating, as output from an output layer of the confidence estimation module, a respective confidence output score for the corresponding sub-word unit based on the confidence feature vector and the acoustic feature vector received as input by the output layer of the confidence estimation module; and
determining, based on the respective confidence output score generated for each sub-word unit in the sequence of hypothesized sub-word units, an utterance-level confidence score for the transcription of the utterance.
15. The system of claim 14 , wherein determining the utterance-level confidence score for the transcription of the utterance comprises:
for each word of the one or more words of the transcription of the utterance, determining a respective word-level confidence score that is equal to the respective confidence output score generated for the final sub-word unit in the corresponding word; and
aggregating the respective word-level confidence scores determined for each of the one or more words of the transcription to determine the utterance-level confidence score for the transcription of the utterance.
16. The system of claim 14 , where the operations further comprise:
estimating, using a hierarchical attention mechanism, a probability that the speech recognition result for the utterance is recognized correctly;
determining an utterance-level loss for the speech recognition result based on an estimated probability that the that the speech recognition result for the utterance is recognized correctly and a ground-truth label indicating whether or not the utterance is recognized correctly; and
training the confidence estimation model on the utterance-level loss.
17. The system of claim 14 , wherein the confidence estimation model and the first speech recognizer are trained jointly.
18. The system of claim 14 , wherein the operations further comprise:
estimating, at each position in a sequence of words of the utterance, a number of deletions;
determining a deletion loss for the speech recognition result and the estimated number of deletions and a ground-truth number of deletions at each position in the sequence of words; and
training the confidence estimation model on the deletion loss.
19. The system of claim 14 , wherein the operations further comprise, after training the confidence estimation model, rescoring candidate speech recognition results recognized by the first speech recognizer.
20. The system of claim 14 , wherein the sub-word units comprise wordpieces.
21. The system of claim 14 , wherein the sub-word units comprise graphemes.
22. The system of claim 14 , wherein the first speech recognizer comprises a transducer decoder model configured to generate multiple candidate hypotheses, each candidate hypothesis corresponding to a candidate transcription for the utterance and represented by a respective sequence of hypothesized sub-word units.
23. The system of claim 22 , wherein the transducer decoder model comprises a Recurrent Neural Network-Transducer (RNN-T) model architecture.
24. The system of claim 14 , wherein the operations further comprise:
determining whether the utterance-level confidence score for the transcription of the utterance satisfies a confidence threshold; and
when the utterance-level confidence score for the transcription of the utterance fails to satisfy the confidence threshold:
rejecting the transcription output from the first speech recognizer; and
instructing a second speech recognizer to process audio data corresponding to the utterance spoken by the user to re-transcribe the utterance spoken by the user.
25. The system of claim 24 , wherein:
the first speech recognizer resides on a user device associated with the user;
the second speech recognizer executes on a remote server in communication with the user device; and
the user device transmits the audio data to the remote server when the utterance-level confidence score for the transcription of the utterance fails to satisfy the confidence threshold.
26. The system of claim 24 , wherein the second speech recognizer is more computationally-intensive than the first speech recognizer.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/643,826 US20220310080A1 (en) | 2021-03-26 | 2021-12-11 | Multi-Task Learning for End-To-End Automated Speech Recognition Confidence and Deletion Estimation |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202163166399P | 2021-03-26 | 2021-03-26 | |
US17/643,826 US20220310080A1 (en) | 2021-03-26 | 2021-12-11 | Multi-Task Learning for End-To-End Automated Speech Recognition Confidence and Deletion Estimation |
Publications (1)
Publication Number | Publication Date |
---|---|
US20220310080A1 true US20220310080A1 (en) | 2022-09-29 |
Family
ID=79287854
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US17/643,826 Pending US20220310080A1 (en) | 2021-03-26 | 2021-12-11 | Multi-Task Learning for End-To-End Automated Speech Recognition Confidence and Deletion Estimation |
Country Status (6)
Country | Link |
---|---|
US (1) | US20220310080A1 (en) |
EP (1) | EP4315321A1 (en) |
JP (1) | JP2024511176A (en) |
KR (1) | KR20230158608A (en) |
CN (1) | CN117099157A (en) |
WO (1) | WO2022203731A1 (en) |
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20230186898A1 (en) * | 2021-12-14 | 2023-06-15 | Google Llc | Lattice Speech Corrections |
CN117708568A (en) * | 2024-02-02 | 2024-03-15 | 智慧眼科技股份有限公司 | Feature extraction method and device for large language model, computer equipment and medium |
-
2021
- 2021-12-11 WO PCT/US2021/062972 patent/WO2022203731A1/en active Application Filing
- 2021-12-11 EP EP21840306.1A patent/EP4315321A1/en active Pending
- 2021-12-11 KR KR1020237036276A patent/KR20230158608A/en unknown
- 2021-12-11 CN CN202180096393.9A patent/CN117099157A/en active Pending
- 2021-12-11 US US17/643,826 patent/US20220310080A1/en active Pending
- 2021-12-11 JP JP2023558841A patent/JP2024511176A/en active Pending
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20230186898A1 (en) * | 2021-12-14 | 2023-06-15 | Google Llc | Lattice Speech Corrections |
CN117708568A (en) * | 2024-02-02 | 2024-03-15 | 智慧眼科技股份有限公司 | Feature extraction method and device for large language model, computer equipment and medium |
Also Published As
Publication number | Publication date |
---|---|
EP4315321A1 (en) | 2024-02-07 |
WO2022203731A1 (en) | 2022-09-29 |
KR20230158608A (en) | 2023-11-20 |
CN117099157A (en) | 2023-11-21 |
JP2024511176A (en) | 2024-03-12 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11610586B2 (en) | Learning word-level confidence for subword end-to-end automatic speech recognition | |
US11741947B2 (en) | Transformer transducer: one model unifying streaming and non-streaming speech recognition | |
US11908461B2 (en) | Deliberation model-based two-pass end-to-end speech recognition | |
US20230377564A1 (en) | Proper noun recognition in end-to-end speech recognition | |
US20220310080A1 (en) | Multi-Task Learning for End-To-End Automated Speech Recognition Confidence and Deletion Estimation | |
US20230186901A1 (en) | Attention-Based Joint Acoustic and Text On-Device End-to-End Model | |
US20230104228A1 (en) | Joint Unsupervised and Supervised Training for Multilingual ASR | |
US20220310097A1 (en) | Reducing Streaming ASR Model Delay With Self Alignment | |
US20240029720A1 (en) | Context-aware Neural Confidence Estimation for Rare Word Speech Recognition | |
US11580956B2 (en) | Emitting word timings with end-to-end models | |
US20230298563A1 (en) | Deliberation by Text-Only and Semi-Supervised Training | |
US20240153495A1 (en) | Multi-Output Decoders for Multi-Task Learning of ASR and Auxiliary Tasks | |
US20240029716A1 (en) | Streaming Automatic Speech Recognition With Non-Streaming Model Distillation | |
US20220310081A1 (en) | Multilingual Re-Scoring Models for Automatic Speech Recognition | |
US20230298570A1 (en) | Rare Word Recognition with LM-aware MWER Training |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:QIU, DAVID;HE, YANZHANG;ZHANG, YU;AND OTHERS;REEL/FRAME:058703/0826Effective date: 20210326 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |