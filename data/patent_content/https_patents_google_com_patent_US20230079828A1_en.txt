US20230079828A1 - STFT-Based Echo Muter - Google Patents
STFT-Based Echo Muter Download PDFInfo
- Publication number
- US20230079828A1 US20230079828A1 US17/643,825 US202117643825A US2023079828A1 US 20230079828 A1 US20230079828 A1 US 20230079828A1 US 202117643825 A US202117643825 A US 202117643825A US 2023079828 A1 US2023079828 A1 US 2023079828A1
- Authority
- US
- United States
- Prior art keywords
- frame
- talk
- double
- microphone signal
- echo
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000012545 processing Methods 0.000 claims abstract description 66
- 238000000034 method Methods 0.000 claims abstract description 39
- 230000015654 memory Effects 0.000 claims description 46
- 230000005236 sound signal Effects 0.000 claims description 29
- 238000004891 communication Methods 0.000 claims description 13
- 230000001629 suppression Effects 0.000 claims description 4
- 230000008569 process Effects 0.000 description 10
- 230000004044 response Effects 0.000 description 10
- 238000004590 computer program Methods 0.000 description 8
- 230000003287 optical effect Effects 0.000 description 6
- 230000006870 function Effects 0.000 description 4
- 238000013518 transcription Methods 0.000 description 3
- 230000035897 transcription Effects 0.000 description 3
- 238000013459 approach Methods 0.000 description 2
- 230000008859 change Effects 0.000 description 2
- 230000003993 interaction Effects 0.000 description 2
- 230000006855 networking Effects 0.000 description 2
- 239000004065 semiconductor Substances 0.000 description 2
- 230000003068 static effect Effects 0.000 description 2
- 238000012935 Averaging Methods 0.000 description 1
- 238000006243 chemical reaction Methods 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000012423 maintenance Methods 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000002093 peripheral effect Effects 0.000 description 1
- 238000009877 rendering Methods 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 238000012549 training Methods 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L21/00—Processing of the speech or voice signal to produce another audible or non-audible signal, e.g. visual or tactile, in order to modify its quality or its intelligibility
- G10L21/02—Speech enhancement, e.g. noise reduction or echo cancellation
- G10L21/0208—Noise filtering
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L21/00—Processing of the speech or voice signal to produce another audible or non-audible signal, e.g. visual or tactile, in order to modify its quality or its intelligibility
- G10L21/02—Speech enhancement, e.g. noise reduction or echo cancellation
- G10L21/0208—Noise filtering
- G10L21/0216—Noise filtering characterised by the method used for estimating noise
- G10L21/0224—Processing in the time domain
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L21/00—Processing of the speech or voice signal to produce another audible or non-audible signal, e.g. visual or tactile, in order to modify its quality or its intelligibility
- G10L21/02—Speech enhancement, e.g. noise reduction or echo cancellation
- G10L21/0208—Noise filtering
- G10L21/0216—Noise filtering characterised by the method used for estimating noise
- G10L21/0232—Processing in the frequency domain
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/26—Speech to text systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L21/00—Processing of the speech or voice signal to produce another audible or non-audible signal, e.g. visual or tactile, in order to modify its quality or its intelligibility
- G10L21/02—Speech enhancement, e.g. noise reduction or echo cancellation
- G10L21/0208—Noise filtering
- G10L2021/02082—Noise filtering the noise being echo, reverberation of the speech
Definitions
- This disclosure relates to a Short-Time Fourier Transform (STFT)-based echo muter.
- STFT Short-Time Fourier Transform
- Speech-enabled devices are capable of generating synthesized playback audio and communicating the synthesized playback audio to one or more users within a speech environment. While the speech-enabled device outputs the synthesized playback audio, a microphone of the speech-enabled device may capture the synthesized playback audio as acoustic echo while actively capturing speech spoken by a user directed toward the speech-enabled device. Unfortunately, with acoustic echo originating from the synthesized playback audio, it may be difficult for a speech recognizer to recognize the speech spoken by the user that occurs during the echo from the synthesized playback audio.
- One aspect of the disclosure provides a computer-implemented method of performing speech recognition using an STFT-based echo muter.
- the computer-implemented method when executed on data processing hardware causes the data processing hardware to perform operations that include receiving a microphone signal including acoustic echo captured by a microphone.
- the acoustic echo corresponds to audio content played back from an acoustic speaker.
- the operations also include receiving a reference signal including a sequence of frames representing the audio content transmitted in a reference channel before the acoustic speaker plays back the audio content.
- the operations For each frame in a sequence of frames of the microphone signal, the operations also include processing, using an acoustic echo canceler configured to receive a respective frame in the sequence of frames of the reference signal as input, the respective frame of the microphone signal to generate a respective output signal frame that cancels the acoustic echo from the respective frame of the microphone signal.
- the operations also include determining, using a Double-talk Detector (DTD), based on the respective frame of the reference signal and the respective output signal frame, whether the respective frame of the microphone signal includes a double-talk frame or an echo-only frame.
- DTD Double-talk Detector
- the operations also include muting the respective output signal frame.
- the operations After muting the respective output signal frame for each respective frame in the sequence of frames of the microphone signal that includes the echo-only frame, the operations also include performing speech processing on the respective output signal frame for each respective frame in the sequence of frames of the microphone signal that includes the double-talk frame.
- Implementations of the disclosure may include one or more of the following optional features.
- a portion of the microphone signal further includes audio signal representing target speech captured by the microphone, and determining whether the respective frame of the microphone signal includes the double-talk frame or the echo-only frame when the respective frame of the microphone signal includes the audio signal representing the target speech.
- the target speech is spoken while the audio content is played back from the acoustic speaker.
- performing speech processing includes performing speech recognition using an automatic speech recognition (ASR) model.
- ASR automatic speech recognition
- the operations prior to using the DTD to determine whether the respective frame of the microphone signal includes the double-talk frame or the echo-only frame, the operations further include converting each respective frame of the microphone, reference, and output signals into a Short-Time Fourier Transform (STFT) domain.
- STFT Short-Time Fourier Transform
- determining whether the respective frame of the microphone signal includes the double-talk frame or the echo-only frame includes computing, using the DTD, a respective first frame-level double-talk indicator based on a cross-correlation between the respective frame of the microphone signal and the respective frame of the reference signal, and computing, using the DTD, a respective second frame-level double-talk indicator based on a cross-correlation between the respective frame of the microphone signal and the respective frame of the output signal.
- These examples also include determining whether or not at least one of the respective first or second frame-level double-talk indicators satisfy a double-talk threshold and when at least one of the respective first or second frame-level double-talk indicators satisfies the double talk threshold, determining the respective frame of the microphone signal includes the double-talk frame.
- determining whether the respective frame of the microphone signal includes the double-talk frame or the echo-only frame may further include determining the respective frame of the microphone signal includes the echo-only frame when both of the respective first and second frame-level double-talk indicators fail to satisfy the double talk threshold.
- the respective first frame-level double-talk indicator and the respective second frame-level double-talk indicator may both be computed over a predetermined range of frequency subbands.
- determining whether or not at least one of the respective first or second frame-level double-talk indicators satisfy the double-talk threshold may include determining that at least one of the respective first or second frame-level double-talk indicators satisfy the double-talk threshold when a minimum of the respective first and second frame-level double-talk indicators is less than the double-talk threshold.
- the operations further include computing, using the DTD, a respective first frame-level double-talk indicator based on a cross-correlation between the respective frame of the microphone signal and one of the respective frame of the reference signal or the respective frame of the output signal.
- determining whether the respective frame of the microphone signal includes the double-talk frame or the echo-only frame is based on the respective first frame-level double-talk indicator.
- the operations may further include computing, using the DTD, a respective second frame-level double-talk indicator based on a cross-correlation between the respective frame of the microphone signal and the other one of the respective frame of the reference signal or the respective frame of the output signal, where determining whether the respective frame of the microphone signal includes the double-talk frame or the echo-only frame is further based on the respective second frame-level double-talk indicator.
- the acoustic echo canceler includes a linear acoustic echo canceler.
- the data processing hardware, the microphone, and the acoustic speaker reside on a user computing device.
- performing speech processing on the respective output signal frame for each respective frame in the sequence of the microphone signal that includes the double-talk frame includes performing speech processing on the respective output signal frame without performing acoustic echo suppression on the respective output signal frame.
- the system includes data processing hardware and memory hardware in communication with the data processing hardware.
- the memory hardware stores instructions that when executed on the data processing hardware causes the date processing hardware to perform operations including receiving a microphone signal including acoustic echo captured by a microphone.
- the acoustic echo corresponds to audio content played back from an acoustic speaker.
- the operations also include receiving a reference signal including a sequence of frames representing the audio content transmitted in a reference channel before the acoustic speaker plays back the audio content.
- the operation For each frame in a sequence of frames of the microphone signal, the operation includes processing, using an acoustic echo canceler configured to receive a respective frame in the sequence of frames of the reference signal as input, the respective frame of the microphone signal to generate a respective output signal frame that cancels the acoustic echo from the respective frame of the microphone signal.
- the operations also include determining, using a Double-talk Detector (DTD), based on the respective frame of the reference signal and the respective output signal frame, whether the respective frame of the microphone signal includes a double-talk frame or an echo-only frame.
- DTD Double-talk Detector
- the operations For each respective frame in the sequence of frames of the microphone signal that includes the echo-only frame, the operations include muting the respective output signal frame. After muting the respective output signal frame for each respective frame in the sequence of frames of the microphone signal that includes the echo-only frame, the operations include performing speech processing on the respective output signal frame for each respective frame in the sequence of frames of the microphone signal that includes the double-talk frame
- a portion of the microphone signal further includes audio signal representing target speech captured by the microphone, and determining whether the respective frame of the microphone signal includes the double-talk frame or the echo-only frame when the respective frame of the microphone signal includes the audio signal representing the target speech.
- the target speech is spoken while the audio content is played back from the acoustic speaker.
- performing speech processing includes performing speech recognition using an automatic speech recognition (ASR) model.
- ASR automatic speech recognition
- the operations prior to using the DTD to determine whether the respective frame of the microphone signal includes the double-talk frame or the echo-only frame, the operations further include converting each respective frame of the microphone, reference, and output signals into a Short-Time Fourier Transform (STFT) domain.
- STFT Short-Time Fourier Transform
- determining whether the respective frame of the microphone signal includes the double-talk frame or the echo-only frame includes computing, using the DTD, a respective first frame-level double-talk indicator based on a cross-correlation between the respective frame of the microphone signal and the respective frame of the reference signal, and computing, using the DTD, a respective second frame-level double-talk indicator based on a cross-correlation between the respective frame of the microphone signal and the respective frame of the output signal.
- These examples also include determining whether or not at least one of the respective first or second frame-level double-talk indicators satisfy a double-talk threshold and when at least one of the respective first or second frame-level double-talk indicators satisfies the double talk threshold, determining the respective frame of the microphone signal includes the double-talk frame.
- determining whether the respective frame of the microphone signal includes the double-talk frame or the echo-only frame may further include determining the respective frame of the microphone signal includes the echo-only frame when both of the respective first and second frame-level double-talk indicators fail to satisfy the double talk threshold.
- the respective first frame-level double-talk indicator and the respective second frame-level double-talk indicator may both be computed over a predetermined range of frequency subbands.
- determining whether or not at least one of the respective first or second frame-level double-talk indicators satisfy the double-talk threshold may include determining that at least one of the respective first or second frame-level double-talk indicators satisfy the double-talk threshold when a minimum of the respective first and second frame-level double-talk indicators is less than the double-talk threshold.
- the operations further include computing, using the DTD, a respective first frame-level double-talk indicator based on a cross-correlation between the respective frame of the microphone signal and one of the respective frame of the reference signal or the respective frame of the output signal.
- determining whether the respective frame of the microphone signal includes the double-talk frame or the echo-only frame is based on the respective first frame-level double-talk indicator.
- the operations may further include computing, using the DTD, a respective second frame-level double-talk indicator based on a cross-correlation between the respective frame of the microphone signal and the other one of the respective frame of the reference signal or the respective frame of the output signal, where determining whether the respective frame of the microphone signal includes the double-talk frame or the echo-only frame is further based on the respective second frame-level double-talk indicator.
- the acoustic echo canceler includes a linear acoustic echo canceler.
- the data processing hardware, the microphone, and the acoustic speaker reside on a user computing device.
- performing speech processing on the respective output signal frame for each respective frame in the sequence of the microphone signal that includes the double-talk frame includes performing speech processing on the respective output signal frame without performing acoustic echo suppression on the respective output signal frame.
- FIG. 1 is a schematic view of an example speech environment using an Acoustic Echo Cancellation (AEC) system.
- AEC Acoustic Echo Cancellation
- FIG. 2 is a schematic view of the AEC system.
- FIG. 3 is a flowchart of an example arrangement of operations for a method of implementing the Acoustic Echo Cancellation system.
- FIG. 4 is a schematic view of an example computing device that may be used to implement the systems and methods described herein.
- Speech-enabled devices are capable of generating synthesized playback audio and communicating the synthesized playback audio to one or more users within a speech environment.
- synthesized playback audio refers to audio generated by the speech-enabled device that originates from the speech-enabled device itself or machine processing systems associated with the speech-enabled device rather a person or other source of audible sound external to the speech-enabled device.
- the speech-enabled device generates synthesized playback audio using a text-to-speech (TTS) system.
- TTS text-to-speech
- a TTS system converts text to an audio representation of the text where the audio representation of the text is modeled to be like that of a spoken utterance using human language.
- an audio output component e.g., acoustic speaker
- an audio capturing component e.g., a microphone
- the speech-enabled device may also playback other types of audio content, such as media content (e.g., music), that may similarly be captured by the audio capturing component as acoustic echo while actively capturing audio signals within the speech environment.
- a speech recognizer implemented at the speech-enabled device, or implemented at a remote system in communication with the speech-enabled device, to understand spoken utterances occurring during the echo from the synthesized playback audio.
- a speech-enabled device often generates synthesized playback audio as a response to a query or command from a user of the speech-enabled device.
- the user may ask the speech-enabled device “what will the weather be like today?”
- the speech-enabled device or remote system in communication with the speech-enabled device, initially has to determine or process the spoken utterance from the user.
- the speech-enabled device is able to recognize that the spoken utterance corresponds to a query from the user (e.g., regarding the weather) and that, as a query, the user anticipates a response from the speech-enabled device.
- the speech-enabled device uses a speech recognition system (e.g., an automatic speech recognition (ASR) system) to determine the content of the spoken utterance.
- ASR automatic speech recognition
- the speech recognition system receives an audio signal or audio data and generates a transcript of text representing the characters, words, and/or sentences spoken in the audio signal.
- Speech recognition may become more complicated when the speech capturing component of the speech-enabled device receives echo and/or distortion at the same time as all or part of one or more utterances spoken by the user(s) to the speech-enabled device. For instance, one or more microphones of the speech-enabled device are fed some portion of the synthesized playback audio signal as echo or acoustic feedback.
- the echo from the synthesized playback audio combined with one or more spoken utterances results in the speech-enabled device receiving an audio signal with overlapping speech.
- the overlapping speech refers to a double-talk event where an instance in the audio signal where acoustic echo from the synthesized playback audio occurs at the same time (i.e., simultaneous or concurrently) as the one or more spoken utterances.
- the speech recognition system may have a difficult time processing the audio signal received at the speech-enabled device. That is, the overlapping speech may compromise the speech recognition system's ability to generate an accurate transcript for the one or more spoken utterances.
- the speech-enabled device may fail to accurately respond or respond at all to a query or a command from a spoken utterance by the user.
- the speech-enabled device may want to avoid using its processing resources attempting to interpret audible sound that is actually echo from the synthesized playback audio signal and/or from the surroundings.
- AEC acoustic echo cancelation
- the AEC system uses an audio signal to cancel the echo related to the audio content played back from the acoustic speaker.
- the audio signal employed by the AEC system to cancel the echo unavoidably creates residual echo, which may further deteriorate the performance of the speech recognition system.
- the residual echo created by the AEC system improves the speech recognition system's ability to spot the wake word.
- this residual echo negatively affects the performance of the speech recognition system.
- One way to reduce this residual echo is by processing the audio signal employed by the AEC system with a post-filter (e.g., an echo suppressor).
- a post-filter e.g., an echo suppressor
- speech recognition systems are generally sensitive to this post-filter processing of the audio signal, rendering the use of post-filters a suboptimal solution.
- a speech environment 100 includes a user 10 communicating a spoken utterance 12 to a speech-enabled device 110 (also referred to as a device 110 or a user device 110 ).
- the user 10 i.e., speaker of the utterance 12
- the device 110 is configured to capture sounds from one or more users 10 within the speech environment 100 .
- the audio sounds may refer to a spoken utterance 12 by the user 10 that functions as an audible query, a command for the device 110 , or an audible communication captured by the device 110 .
- Speech-enabled systems of the device 110 or associated with the device 110 may field the query for the command by answering the query and/or causing the command to be performed.
- the device 110 receives a microphone signal 202 that includes acoustic echo 156 captured by an audio capturing device 116 (also referred to as a microphone) and/or the spoken utterance 12 by the user 10 .
- the acoustic echo 156 corresponds to audio content 154 played back from an audio output device 118 (also referred to as an acoustic speaker).
- the device 110 may correspond to any computing device associated with the user 10 and capable of receiving microphone signals 202 .
- Some examples of user devices 110 include, but are not limited to, mobile devices (e.g., mobile phones, tablets, laptops, etc.), computers, wearable devices (e.g., smart watches), smart appliances, and internet of things (IoT) devices, smart speakers, etc.
- the device 110 includes data processing hardware 112 and memory hardware 114 in communication with the data processing hardware 112 and storing instructions, that when executed by the data processing hardware 112 , cause the data processing hardware 112 to perform one or more operations.
- the device 110 includes one or more applications (i.e., software applications) where each application may utilize one or more speech processing systems 140 , 150 , 200 associated with device 110 to perform various functions within the application.
- the device 110 includes an assistant application configured to communicate synthesized playback audio content 154 to the user 10 to assist the user 10 with various tasks.
- the assistant application or a media application is configured to playback audio content 154 that includes media content (e.g., music, talk radio, podcast content, television/movie content).
- the application corresponds to an assistant application configured to communicate audio content 154 as synthesized speech for playback from the acoustic speaker 118 to communicate/converse with, or assist, the user 10 with the performance of various tasks.
- the assistant application may audibly output synthesized speech that is responsive to queries/commands submitted by the user 10 to the assistant application.
- the audio content 154 played back from the acoustic speaker 118 corresponds to notifications/alerts such as, without limitation, a timer ending, an incoming phone call alert, a doorbell chime, an audio message, etc.
- the device 110 includes the microphone 116 for capturing and converting audio data 14 within the speech environment 100 into electrical microphone signals 202 , and the acoustic speaker 118 for communicating/outputting the playback audio content 154 (e.g., synthesized playback audio content). While the device 110 implements a microphone 116 in the example shown, the device 110 may implement an array of microphones 116 without departing from the scope of the present disclosure, whereby one or more microphones 116 in the array may not physically reside on the device 110 , but be in communication with interfaces/peripherals of the device 110 . For example, the device 110 may correspond to a vehicle infotainment system that leverages an array of microphones positioned throughout the vehicle.
- the acoustic speaker 118 may include one or more speakers either residing on the device 110 , in communication therewith, or a combination where one or more speakers reside on the device 110 and one or more other speakers are physically removed from the device 110 but in communication with the device 110 .
- the device 110 may be configured to communicate via a network 120 with a remote system 130 .
- the remote system 130 may include remote resources 132 , such as remote data processing hardware 134 (e.g., remote servers or CPUs) and/or remote memory hardware 136 (e.g., remote databases or other storage hardware).
- the device 110 may utilize the remote resources 132 to perform various functionality related to speech processing and/or synthesized playback communication.
- the device 110 is configured to perform speech recognition using a speech recognition system 140 (e.g., using a speech recognition model 145 ).
- the device may be configured to perform conversion of text to speech using a TTS system 150 and acoustic echo cancelation using an AEC system 200 .
- These systems 140 , 150 , 200 may reside on the device 110 (referred to as on-device systems) or reside remotely (e.g., reside on the remote system 130 ), but in communication with the device 110 . In some examples, some of these systems 140 , 150 , 200 reside locally or on-device while others reside remotely. In other words, any of these systems 140 , 150 , 200 may be local or remote in any combination. For instance, when a system 140 , 150 , 200 is rather large in size or processing requirements, the system 140 , 150 , 200 may reside in the remote system 130 .
- the one or more systems 140 , 150 , 200 may reside on the device 110 using the data processing hardware 112 and/or the memory hardware 114 .
- the one or more of the systems 140 , 150 , 200 may reside on both locally/on-device and remotely.
- one or more of the systems 140 , 150 , 200 may default to execute on the remote system 130 when a connection to the network 120 between the device 110 and remote system 130 is available, but when the connection is lost or the network 120 is unavailable, the systems 140 , 150 , 200 instead execute locally on the device 110 .
- a speech recognition system 140 receives an audio signal 202 as an input and transcribes that audio signal into a transcription 142 as an output. Generally speaking, by converting the audio signal 202 into the transcription 142 , the speech recognition system 140 allows the device 110 to recognize when a spoken utterance 12 from the user 10 corresponds to a query, a command, or some other form of audio communication.
- the transcription 142 refers to a sequence of text that the device 110 may then use to generate a response to the query or the command. For instance, if the user 10 asks the device 110 the question of “what will the weather be like today,” the device 110 passes the audio signal corresponding to the question “what will the weather be like today” to the speech recognition system 140 .
- the speech recognized system 140 converts the audio signal into a transcript that includes the text of “what will the weather be like today?”
- the device 110 may then determine a response to the query using the text or portions of the text. For instance, in order to determine the weather for the current day (i.e., today), the device 110 passes the text (e.g., “what will the weather be like today?”) or identifying portions of the text (e.g., “weather” and “today”) to a search engine. The search engine may then return one or more search results that the device 110 interprets to generate a response for the user 10 .
- the device 110 or a system associated with the device 110 identifies text 152 that the device 110 will communicate to the user 10 as a response to a query of the spoken utterance 12 .
- the device 110 may then use the TTS system 150 to convert the text 152 into corresponding synthesized playback audio 154 for the device 110 to communicate to the user 10 (e.g., audibly communicate to the user 10 ) as the response to the query of the spoken utterance 12 .
- the TTS system 150 receives, as input, text 152 and converts the text 152 to an output of synthesized playback audio 154 where the synthesized playback audio 154 is an audio signal defining an audible rendition of the text 152 .
- the TTS system 150 includes a text encoder that processes the text 152 into an encoded format (e.g., a text embedding).
- the TTS system 150 may use a trained text-to-speech model to generate the synthesized playback audio 154 from the encoded format of the text 152 .
- the TTS system 150 communicates the synthesized playback audio 154 to the device 110 to allow the device 110 to output the synthesized playback audio 154 .
- the device 110 outputs the synthesized playback audio 154 of “today is sunny” at the speaker 118 of the device 110 .
- the synthesized playback audio 154 e.g., synthesized speech
- the synthesized playback audio 154 generates the echo 156 which is captured by the microphone 116 .
- the microphone 116 may also be simultaneously capturing another spoken utterance 12 from the user 10 that corresponds to target speech directed toward the device 110 . For example, FIG.
- the user 10 inquires more about the weather, in a spoken utterance 12 to the device 110 , by stating “what about tomorrow?”
- the user 10 speaks the utterance 12 as part of a continued conversation scenario where the device 110 maintains the microphone 116 open and the speech recognition system 140 active to permit the user 10 to provide follow-up queries for recognition by the speech recognition system 140 without requiring the user 10 to speak a hotword (e.g., a predetermined word or phrase that when detected triggers the device 110 to invoke speech recognition).
- a hotword e.g., a predetermined word or phrase that when detected triggers the device 110 to invoke speech recognition.
- the echo 156 aids the speech recognition system 140 in converting the audio signal 14 into a transcript.
- the user device 110 may process the microphone signal 202 containing the echo 156 , causing the user device 110 to process its own playback audio 154 output from the speaker 118 .
- the spoken utterance 12 and the echo 156 are both captured by the microphone 116 simultaneously to form the microphone signal 202 .
- the microphone signal 202 includes a portion that only includes the echo 156 corresponding to the playback audio content 154 output from the speaker 118 before the user 10 speaks the utterance 12 , an overlapped portion (e.g., overlapped region 204 ) where the utterance 12 spoken by the user 10 overlaps with some portion of the playback audio content 154 output from the speaker 118 , and a portion that includes only the utterance 12 spoken by the user 12 after the acoustic speaker 118 stops output of the playback audio content 154 .
- an overlapped portion e.g., overlapped region 204
- an overlapping region 204 in the captured microphone signal 202 corresponds to a double-talk event that indicates the instance where the portion of the utterance 12 and the portion of the synthesized playback audio 154 overlap with one another in the captured microphone signal 202 .
- the speech recognition system 140 may have issues recognizing the follow-on query 12 corresponding to the weather inquiry “what about tomorrow” in the audio signal 202 captured by the microphone 116 since the utterance 12 is mixed with the echo 156 of the synthesized playback audio content 154 .
- one way to reduce the echo 156 in the microphone signal 202 is by processing the microphone signal 202 with an echo suppressor filter.
- the speech recognition system 140 has difficulty processing this type of filtered microphone signal 202 .
- the device 110 includes an AEC system 200 to process the microphone signal 202 and provide the output to the speech recognition system 140 .
- the AEC system 200 ( FIG. 2 ) includes an acoustic echo canceller 210 , a double-talk detector 220 , and an echo muter 230 , and is configured to mute audio output signal frames that are identified as echo only, while permitting audio frames that include double-talk to pass through for processing by the speech recognition system 140 .
- the AEC system 200 receives the microphone signal 202 including the follow-on query 12 mixed with the echo 156 . For each audio frame in the microphone signal, the AEC system 200 processes the audio frame and determines whether the audio frame is echo only, or includes double-talk.
- the acoustic echo canceller 210 of the AEC system 200 is configured to receive a microphone signal 202 that may simultaneously include respective frames of the of target speech 12 directed toward the device and a reference signal 158 corresponding to respective frames representing the playback audio 154 captured by the microphone 12 .
- the acoustic echo canceller 210 processes the microphone signal 202 using the reference signal 158 to cancel the echo 156 in the microphone signal 202 , thereby producing an output signal frame 206 .
- the AEC system 200 further converts each respective frame of the microphone signal 202 , the reference signal 158 , and the output signal 206 into a Short-Time Fourier Transform (STFT) domain using the following equation:
- STFT Short-Time Fourier Transform
- L denotes the frame-hop
- w[n] denotes an analysis window with a possible N point
- l denotes the frame index
- k denotes the subband index
- the double-talk detector 220 then receives the respective frames in the STFT domain of the microphone signal m[n] 202 (denoted as M(l,k)), the reference signal x[n] 158 (denoted as X(l,k)), and the residual echo output signal r[n] 206 (denoted as (R(l,k)), and determines whether the respective frames include a double-talk frame or an echo-only frame.
- the double-talk detector 220 uses two double-talk indicators 208 per subband in each respective frame.
- the two double-talk indicators 208 a , 208 b may be expressed as follows:
- c PQ (k) denotes the complex cross-correlation for each subband k, at a lag of zero, of the STFT domain of the playback audio frames 154 P(l,k) and Q(l,k).
- P(l,k) and Q(l,k) are associateD with the time-domain signalS p[n] and q[n] respectively.
- the complex cross-correlation c PQ (k) may be expressed as follows:
- a cross-correlation for each subband determines the energy of the subband by using an exponentially weighted averaging over the STFT domain using the following equations:
- c PQ ( l,k ) ⁇ c PQ ( l ⁇ 1, k )+(1 ⁇ ) P ( l,k ) Q *( l,k )
- Equation (2) may be reformulated and expressed as follows:
- a first double-talk indicator 208 a is computed by a cross-correlation between the microphone signal 202 and the reference signal 158
- the second double-talk indicator 208 b is computed by a cross-correlation between the microphone signal 202 and the output signal 206 .
- the double-talk detector 220 uses the indicators and subband energies to compute double-talk indicators 208 a , 208 b for each respective frame by computing a weighted average of a subband indicator over a limited number of subbands. For example, subbands between 700 and 2400 Hz may be used for determining the double-talk indicators 208 a , 208 b .
- Each double-talk indicator 208 a , 208 b is weighed by the energy of the residual in the given subband by cross-correlating each frame in the sequence of frames to determine an energy in the band.
- the first double-talk indicator 208 a may be calculated as follows:
- a second double-talk indicator 208 b may be calculated as follows:
- the double-talk detector 220 determines whether the respective frame is echo-only which should be muted, or includes double-talk which should be passed through. This is determined as follows:
- the respective frame is passed through for processing by the speech recognition system 140 ).
- both of the double-talk indicators 208 a , 208 b fail to satisfy the double-talk threshold, the respective frame is identified as echo-only and is muted (i.e., not passed for processing by the speech recognition system). Requiring the minimum of the double-talk indicators 208 a , 208 b to satisfy the double-talk threshold favors passing through frames over muting frames.
- the double-talk threshold ⁇ may be computed using a data-driven approach. Meta-data is collected from utterances to segment the utterances into echo-only and double-talk intervals. Next, the frame-level double-talk indicators 208 a , 208 b are calculated for all of the frames in the training dataset. For a given threshold ⁇ , the percentage of misclassification of echo-only and double-talk frames is calculated. This may be repeated for a range of thresholds ⁇ with the target of identifying a threshold that results in a misclassification of double-talk frames that is 10% or less.
- FIG. 3 includes a flowchart of an example arrangement of operations for a method 300 of performing speech recognition using an STFT-based echo muter 230 .
- the method 300 includes receiving a microphone signal 202 including acoustic echo 156 captured by a microphone 116 .
- the acoustic echo 156 corresponds to audio content 154 played back from an acoustic speaker 118 .
- the method 300 includes receiving a reference signal 158 including a sequence of frames representing the audio content 154 transmitted in a reference channel before the acoustic speaker 118 plays back the audio content 154 .
- the method 300 For each frame in a sequence of frames of the microphone signal 202 , the method 300 , at operation 306 , includes processing, using an acoustic echo canceler 210 configured to receive a respective frame in the sequence of frames of the reference signal 158 as input, the respective frame of the microphone signal 202 to generate a respective output signal frame 206 that cancels the acoustic echo 156 from the respective frame of the microphone signal 202 .
- the method 300 includes determining, using a double-talk-detector 220 , based on the respective frame of the reference signal 158 and the respective output signal frame 206 , whether the respective frame of the microphone signal 202 includes a double-talk frame or an echo-only frame.
- the method 300 at operation 310 includes muting the respective output signal frame 206 .
- the method 300 includes performing speech processing on the respective output signal frame 206 for each respective frame in the sequence frames of the microphone signal 202 that includes the double-talk frame.
- FIG. 4 is schematic view of an example computing device 400 that may be used to implement the systems and methods described in this document.
- the computing device 400 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers.
- the components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document.
- the computing device 400 includes a processor 410 , memory 420 , a storage device 430 , a high-speed interface/controller 440 connecting to the memory 420 and high-speed expansion ports 450 , and a low speed interface/controller 460 connecting to a low speed bus 470 and a storage device 430 .
- Each of the components 410 , 420 , 430 , 440 , 450 , and 460 are interconnected using various busses, and may be mounted on a common motherboard or in other manners as appropriate.
- the processor 410 (also referred to as “data processing hardware 410 ” that may include the data processing hardware 112 of the user computing device 110 or the data processing hardware 134 of the remote system 130 ) can process instructions for execution within the computing device 400 , including instructions stored in the memory 420 or on the storage device 430 to display graphical information for a graphical user interface (GUI) on an external input/output device, such as display 480 coupled to high speed interface 440 .
- GUI graphical user interface
- multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory.
- multiple computing devices 400 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system).
- the memory 420 (also referred to as “memory hardware 420 ” that may include the memory hardware 114 of the user computing device 110 or the memory hardware 136 of the remote system 130 ) stores information non-transitorily within the computing device 400 .
- the memory 420 may be a computer-readable medium, a volatile memory unit(s), or non-volatile memory unit(s).
- the non-transitory memory 420 may be physical devices used to store programs (e.g., sequences of instructions) or data (e.g., program state information) on a temporary or permanent basis for use by the computing device 400 .
- non-volatile memory examples include, but are not limited to, flash memory and read-only memory (ROM)/programmable read-only memory (PROM)/erasable programmable read-only memory (EPROM)/electronically erasable programmable read-only memory (EEPROM) (e.g., typically used for firmware, such as boot programs).
- volatile memory examples include, but are not limited to, random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), phase change memory (PCM) as well as disks or tapes.
- the storage device 430 is capable of providing mass storage for the computing device 400 .
- the storage device 430 is a computer-readable medium.
- the storage device 430 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations.
- a computer program product is tangibly embodied in an information carrier.
- the computer program product contains instructions that, when executed, perform one or more methods, such as those described above.
- the information carrier is a computer- or machine-readable medium, such as the memory 420 , the storage device 430 , or memory on processor 410 .
- the high speed controller 440 manages bandwidth-intensive operations for the computing device 400 , while the low speed controller 460 manages lower bandwidth-intensive operations. Such allocation of duties is exemplary only.
- the high-speed controller 440 is coupled to the memory 420 , the display 480 (e.g., through a graphics processor or accelerator), and to the high-speed expansion ports 450 , which may accept various expansion cards (not shown).
- the low-speed controller 460 is coupled to the storage device 430 and a low-speed expansion port 490 .
- the low-speed expansion port 490 which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet), may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- input/output devices such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- the computing device 400 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a standard server 400 a or multiple times in a group of such servers 400 a , as a laptop computer 400 b , or as part of a rack server system 400 c.
- implementations of the systems and techniques described herein can be realized in digital electronic and/or optical circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof.
- ASICs application specific integrated circuits
- These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- a software application may refer to computer software that causes a computing device to perform a task.
- a software application may be referred to as an “application,” an “app,” or a “program.”
- Example applications include, but are not limited to, system diagnostic applications, system management applications, system maintenance applications, word processing applications, spreadsheet applications, messaging applications, media streaming applications, social networking applications, and gaming applications.
- the non-transitory memory may be physical devices used to store programs (e.g., sequences of instructions) or data (e.g., program state information) on a temporary or permanent basis for use by a computing device.
- the non-transitory memory may be volatile and/or non-volatile addressable semiconductor memory. Examples of non-volatile memory include, but are not limited to, flash memory and read-only memory (ROM)/programmable read-only memory (PROM)/erasable programmable read-only memory (EPROM)/electronically erasable programmable read-only memory (EEPROM) (e.g., typically used for firmware, such as boot programs).
- Examples of volatile memory include, but are not limited to, random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), phase change memory (PCM) as well as disks or tapes.
- the processes and logic flows described in this specification can be performed by one or more programmable processors, also referred to as data processing hardware, executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows can also be performed by special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer.
- a processor will receive instructions and data from a read only memory or a random access memory or both.
- the essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- mass storage devices for storing data
- a computer need not have such devices.
- Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks.
- the processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- one or more aspects of the disclosure can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor, or touch screen for displaying information to the user and optionally a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor, or touch screen for displaying information to the user and optionally a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- Other kinds of devices can be used to provide interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input
Abstract
Description
- This U.S. patent application claims priority under 35 U.S.C. § 119(e) to U.S. Provisional Application 63/261,281, filed on Sep. 16, 2021. The disclosure of this prior application is considered part of the disclosure of this application and is hereby incorporated by reference in its entirety.
- This disclosure relates to a Short-Time Fourier Transform (STFT)-based echo muter.
- Speech-enabled devices are capable of generating synthesized playback audio and communicating the synthesized playback audio to one or more users within a speech environment. While the speech-enabled device outputs the synthesized playback audio, a microphone of the speech-enabled device may capture the synthesized playback audio as acoustic echo while actively capturing speech spoken by a user directed toward the speech-enabled device. Unfortunately, with acoustic echo originating from the synthesized playback audio, it may be difficult for a speech recognizer to recognize the speech spoken by the user that occurs during the echo from the synthesized playback audio.
- One aspect of the disclosure provides a computer-implemented method of performing speech recognition using an STFT-based echo muter. The computer-implemented method when executed on data processing hardware causes the data processing hardware to perform operations that include receiving a microphone signal including acoustic echo captured by a microphone. The acoustic echo corresponds to audio content played back from an acoustic speaker. The operations also include receiving a reference signal including a sequence of frames representing the audio content transmitted in a reference channel before the acoustic speaker plays back the audio content. For each frame in a sequence of frames of the microphone signal, the operations also include processing, using an acoustic echo canceler configured to receive a respective frame in the sequence of frames of the reference signal as input, the respective frame of the microphone signal to generate a respective output signal frame that cancels the acoustic echo from the respective frame of the microphone signal. The operations also include determining, using a Double-talk Detector (DTD), based on the respective frame of the reference signal and the respective output signal frame, whether the respective frame of the microphone signal includes a double-talk frame or an echo-only frame. For each respective frame in the sequence of frames of the microphone signal that includes the echo-only frame, the operations also include muting the respective output signal frame. After muting the respective output signal frame for each respective frame in the sequence of frames of the microphone signal that includes the echo-only frame, the operations also include performing speech processing on the respective output signal frame for each respective frame in the sequence of frames of the microphone signal that includes the double-talk frame.
- Implementations of the disclosure may include one or more of the following optional features. In some implementations, a portion of the microphone signal further includes audio signal representing target speech captured by the microphone, and determining whether the respective frame of the microphone signal includes the double-talk frame or the echo-only frame when the respective frame of the microphone signal includes the audio signal representing the target speech. Here, the target speech is spoken while the audio content is played back from the acoustic speaker. In some examples, performing speech processing includes performing speech recognition using an automatic speech recognition (ASR) model. In some implementations, prior to using the DTD to determine whether the respective frame of the microphone signal includes the double-talk frame or the echo-only frame, the operations further include converting each respective frame of the microphone, reference, and output signals into a Short-Time Fourier Transform (STFT) domain.
- In some examples, determining whether the respective frame of the microphone signal includes the double-talk frame or the echo-only frame includes computing, using the DTD, a respective first frame-level double-talk indicator based on a cross-correlation between the respective frame of the microphone signal and the respective frame of the reference signal, and computing, using the DTD, a respective second frame-level double-talk indicator based on a cross-correlation between the respective frame of the microphone signal and the respective frame of the output signal. These examples also include determining whether or not at least one of the respective first or second frame-level double-talk indicators satisfy a double-talk threshold and when at least one of the respective first or second frame-level double-talk indicators satisfies the double talk threshold, determining the respective frame of the microphone signal includes the double-talk frame. In these examples, determining whether the respective frame of the microphone signal includes the double-talk frame or the echo-only frame may further include determining the respective frame of the microphone signal includes the echo-only frame when both of the respective first and second frame-level double-talk indicators fail to satisfy the double talk threshold. The respective first frame-level double-talk indicator and the respective second frame-level double-talk indicator may both be computed over a predetermined range of frequency subbands. Additionally or alternatively, determining whether or not at least one of the respective first or second frame-level double-talk indicators satisfy the double-talk threshold may include determining that at least one of the respective first or second frame-level double-talk indicators satisfy the double-talk threshold when a minimum of the respective first and second frame-level double-talk indicators is less than the double-talk threshold.
- In some implementations, for each frame in the sequence of frames of the microphone signal, the operations further include computing, using the DTD, a respective first frame-level double-talk indicator based on a cross-correlation between the respective frame of the microphone signal and one of the respective frame of the reference signal or the respective frame of the output signal. Here, determining whether the respective frame of the microphone signal includes the double-talk frame or the echo-only frame is based on the respective first frame-level double-talk indicator. In these implementations, the operations may further include computing, using the DTD, a respective second frame-level double-talk indicator based on a cross-correlation between the respective frame of the microphone signal and the other one of the respective frame of the reference signal or the respective frame of the output signal, where determining whether the respective frame of the microphone signal includes the double-talk frame or the echo-only frame is further based on the respective second frame-level double-talk indicator.
- In some examples, the acoustic echo canceler includes a linear acoustic echo canceler. In some implementations, the data processing hardware, the microphone, and the acoustic speaker reside on a user computing device. In some examples, performing speech processing on the respective output signal frame for each respective frame in the sequence of the microphone signal that includes the double-talk frame includes performing speech processing on the respective output signal frame without performing acoustic echo suppression on the respective output signal frame.
- Another aspect of the disclosure provides a system for performing speech recognition using an STFT-based echo muter. The system includes data processing hardware and memory hardware in communication with the data processing hardware. The memory hardware stores instructions that when executed on the data processing hardware causes the date processing hardware to perform operations including receiving a microphone signal including acoustic echo captured by a microphone. The acoustic echo corresponds to audio content played back from an acoustic speaker. The operations also include receiving a reference signal including a sequence of frames representing the audio content transmitted in a reference channel before the acoustic speaker plays back the audio content. For each frame in a sequence of frames of the microphone signal, the operation includes processing, using an acoustic echo canceler configured to receive a respective frame in the sequence of frames of the reference signal as input, the respective frame of the microphone signal to generate a respective output signal frame that cancels the acoustic echo from the respective frame of the microphone signal. The operations also include determining, using a Double-talk Detector (DTD), based on the respective frame of the reference signal and the respective output signal frame, whether the respective frame of the microphone signal includes a double-talk frame or an echo-only frame. For each respective frame in the sequence of frames of the microphone signal that includes the echo-only frame, the operations include muting the respective output signal frame. After muting the respective output signal frame for each respective frame in the sequence of frames of the microphone signal that includes the echo-only frame, the operations include performing speech processing on the respective output signal frame for each respective frame in the sequence of frames of the microphone signal that includes the double-talk frame.
- This aspect may include one or more of the following optional features. In some implementations, a portion of the microphone signal further includes audio signal representing target speech captured by the microphone, and determining whether the respective frame of the microphone signal includes the double-talk frame or the echo-only frame when the respective frame of the microphone signal includes the audio signal representing the target speech. Here, the target speech is spoken while the audio content is played back from the acoustic speaker. In some examples, performing speech processing includes performing speech recognition using an automatic speech recognition (ASR) model. In some implementations, prior to using the DTD to determine whether the respective frame of the microphone signal includes the double-talk frame or the echo-only frame, the operations further include converting each respective frame of the microphone, reference, and output signals into a Short-Time Fourier Transform (STFT) domain.
- In some examples, determining whether the respective frame of the microphone signal includes the double-talk frame or the echo-only frame includes computing, using the DTD, a respective first frame-level double-talk indicator based on a cross-correlation between the respective frame of the microphone signal and the respective frame of the reference signal, and computing, using the DTD, a respective second frame-level double-talk indicator based on a cross-correlation between the respective frame of the microphone signal and the respective frame of the output signal. These examples also include determining whether or not at least one of the respective first or second frame-level double-talk indicators satisfy a double-talk threshold and when at least one of the respective first or second frame-level double-talk indicators satisfies the double talk threshold, determining the respective frame of the microphone signal includes the double-talk frame. In these examples, determining whether the respective frame of the microphone signal includes the double-talk frame or the echo-only frame may further include determining the respective frame of the microphone signal includes the echo-only frame when both of the respective first and second frame-level double-talk indicators fail to satisfy the double talk threshold. The respective first frame-level double-talk indicator and the respective second frame-level double-talk indicator may both be computed over a predetermined range of frequency subbands. Additionally or alternatively, determining whether or not at least one of the respective first or second frame-level double-talk indicators satisfy the double-talk threshold may include determining that at least one of the respective first or second frame-level double-talk indicators satisfy the double-talk threshold when a minimum of the respective first and second frame-level double-talk indicators is less than the double-talk threshold.
- In some implementations, for each frame in the sequence of frames of the microphone signal, the operations further include computing, using the DTD, a respective first frame-level double-talk indicator based on a cross-correlation between the respective frame of the microphone signal and one of the respective frame of the reference signal or the respective frame of the output signal. Here, determining whether the respective frame of the microphone signal includes the double-talk frame or the echo-only frame is based on the respective first frame-level double-talk indicator. In these implementations, the operations may further include computing, using the DTD, a respective second frame-level double-talk indicator based on a cross-correlation between the respective frame of the microphone signal and the other one of the respective frame of the reference signal or the respective frame of the output signal, where determining whether the respective frame of the microphone signal includes the double-talk frame or the echo-only frame is further based on the respective second frame-level double-talk indicator.
- In some examples, the acoustic echo canceler includes a linear acoustic echo canceler. In some implementations, the data processing hardware, the microphone, and the acoustic speaker reside on a user computing device. In some examples, performing speech processing on the respective output signal frame for each respective frame in the sequence of the microphone signal that includes the double-talk frame includes performing speech processing on the respective output signal frame without performing acoustic echo suppression on the respective output signal frame.
- The details of one or more implementations of the disclosure are set forth in the accompanying drawings and the description below. Other aspects, features, and advantages will be apparent from the description and drawings, and from the claims.
-
FIG. 1 is a schematic view of an example speech environment using an Acoustic Echo Cancellation (AEC) system. -
FIG. 2 is a schematic view of the AEC system. -
FIG. 3 is a flowchart of an example arrangement of operations for a method of implementing the Acoustic Echo Cancellation system. -
FIG. 4 is a schematic view of an example computing device that may be used to implement the systems and methods described herein. - Like reference symbols in the various drawings indicate like elements.
- Speech-enabled devices are capable of generating synthesized playback audio and communicating the synthesized playback audio to one or more users within a speech environment. Here, synthesized playback audio refers to audio generated by the speech-enabled device that originates from the speech-enabled device itself or machine processing systems associated with the speech-enabled device rather a person or other source of audible sound external to the speech-enabled device. Generally speaking, the speech-enabled device generates synthesized playback audio using a text-to-speech (TTS) system. A TTS system converts text to an audio representation of the text where the audio representation of the text is modeled to be like that of a spoken utterance using human language.
- While an audio output component (e.g., acoustic speaker) of the speech-enabled device outputs the synthesized playback audio, an audio capturing component (e.g., a microphone) of the speech-enabled device may still be actively capturing (i.e., listening to) audio signals within the speech environment. This means that some portion of synthesized playback audio output from the speaker will be received at the audio capturing component as acoustic echo. The speech-enabled device may also playback other types of audio content, such as media content (e.g., music), that may similarly be captured by the audio capturing component as acoustic echo while actively capturing audio signals within the speech environment. Unfortunately, with acoustic echo originating from the playback audio content (e.g., synthesized audio content), it may be difficult for a speech recognizer implemented at the speech-enabled device, or implemented at a remote system in communication with the speech-enabled device, to understand spoken utterances occurring during the echo from the synthesized playback audio. In other words, a speech-enabled device often generates synthesized playback audio as a response to a query or command from a user of the speech-enabled device. For instance, the user may ask the speech-enabled device “what will the weather be like today?” When the speech-enabled device receives this query or question from the user, the speech-enabled device, or remote system in communication with the speech-enabled device, initially has to determine or process the spoken utterance from the user. By processing the spoken utterance, the speech-enabled device is able to recognize that the spoken utterance corresponds to a query from the user (e.g., regarding the weather) and that, as a query, the user anticipates a response from the speech-enabled device.
- Typically, the speech-enabled device uses a speech recognition system (e.g., an automatic speech recognition (ASR) system) to determine the content of the spoken utterance. The speech recognition system receives an audio signal or audio data and generates a transcript of text representing the characters, words, and/or sentences spoken in the audio signal. Speech recognition, however, may become more complicated when the speech capturing component of the speech-enabled device receives echo and/or distortion at the same time as all or part of one or more utterances spoken by the user(s) to the speech-enabled device. For instance, one or more microphones of the speech-enabled device are fed some portion of the synthesized playback audio signal as echo or acoustic feedback. The echo from the synthesized playback audio combined with one or more spoken utterances results in the speech-enabled device receiving an audio signal with overlapping speech. Here, the overlapping speech refers to a double-talk event where an instance in the audio signal where acoustic echo from the synthesized playback audio occurs at the same time (i.e., simultaneous or concurrently) as the one or more spoken utterances. During the double-talk event, the speech recognition system may have a difficult time processing the audio signal received at the speech-enabled device. That is, the overlapping speech may compromise the speech recognition system's ability to generate an accurate transcript for the one or more spoken utterances. Without an accurate transcript from the speech recognition system, the speech-enabled device may fail to accurately respond or respond at all to a query or a command from a spoken utterance by the user. Alternatively, the speech-enabled device may want to avoid using its processing resources attempting to interpret audible sound that is actually echo from the synthesized playback audio signal and/or from the surroundings.
- One approach to combat distortion or echo captured by audio capturing components of the speech-enabled device is to use an acoustic echo cancelation (AEC) system. In an AEC system, the AEC system uses an audio signal to cancel the echo related to the audio content played back from the acoustic speaker. However, the audio signal employed by the AEC system to cancel the echo unavoidably creates residual echo, which may further deteriorate the performance of the speech recognition system. In wake word applications, where a predetermined word or phrase is spoken to invoke speech recognition by the speech-enabled device, the residual echo created by the AEC system improves the speech recognition system's ability to spot the wake word. However, in wake word-less applications, this residual echo negatively affects the performance of the speech recognition system. One way to reduce this residual echo is by processing the audio signal employed by the AEC system with a post-filter (e.g., an echo suppressor). However, speech recognition systems are generally sensitive to this post-filter processing of the audio signal, rendering the use of post-filters a suboptimal solution.
- Referring to
FIG. 1 , in some implementations, aspeech environment 100 includes auser 10 communicating a spokenutterance 12 to a speech-enabled device 110 (also referred to as adevice 110 or a user device 110). The user 10 (i.e., speaker of the utterance 12) may speak theutterance 12 as a query or a command to solicit a response from thedevice 110. Thedevice 110 is configured to capture sounds from one ormore users 10 within thespeech environment 100. Here, the audio sounds may refer to a spokenutterance 12 by theuser 10 that functions as an audible query, a command for thedevice 110, or an audible communication captured by thedevice 110. Speech-enabled systems of thedevice 110 or associated with thedevice 110 may field the query for the command by answering the query and/or causing the command to be performed. - Here, the
device 110 receives amicrophone signal 202 that includesacoustic echo 156 captured by an audio capturing device 116 (also referred to as a microphone) and/or the spokenutterance 12 by theuser 10. Theacoustic echo 156 corresponds toaudio content 154 played back from an audio output device 118 (also referred to as an acoustic speaker). Thedevice 110 may correspond to any computing device associated with theuser 10 and capable of receiving microphone signals 202. Some examples ofuser devices 110 include, but are not limited to, mobile devices (e.g., mobile phones, tablets, laptops, etc.), computers, wearable devices (e.g., smart watches), smart appliances, and internet of things (IoT) devices, smart speakers, etc. Thedevice 110 includesdata processing hardware 112 andmemory hardware 114 in communication with thedata processing hardware 112 and storing instructions, that when executed by thedata processing hardware 112, cause thedata processing hardware 112 to perform one or more operations. In some examples, thedevice 110 includes one or more applications (i.e., software applications) where each application may utilize one or morespeech processing systems device 110 to perform various functions within the application. For instance, thedevice 110 includes an assistant application configured to communicate synthesizedplayback audio content 154 to theuser 10 to assist theuser 10 with various tasks. In other examples, the assistant application or a media application is configured to playbackaudio content 154 that includes media content (e.g., music, talk radio, podcast content, television/movie content). In additional examples, the application corresponds to an assistant application configured to communicateaudio content 154 as synthesized speech for playback from theacoustic speaker 118 to communicate/converse with, or assist, theuser 10 with the performance of various tasks. For example, the assistant application may audibly output synthesized speech that is responsive to queries/commands submitted by theuser 10 to the assistant application. In additional examples, theaudio content 154 played back from theacoustic speaker 118 corresponds to notifications/alerts such as, without limitation, a timer ending, an incoming phone call alert, a doorbell chime, an audio message, etc. - The
device 110 includes themicrophone 116 for capturing and convertingaudio data 14 within thespeech environment 100 into electrical microphone signals 202, and theacoustic speaker 118 for communicating/outputting the playback audio content 154 (e.g., synthesized playback audio content). While thedevice 110 implements amicrophone 116 in the example shown, thedevice 110 may implement an array ofmicrophones 116 without departing from the scope of the present disclosure, whereby one ormore microphones 116 in the array may not physically reside on thedevice 110, but be in communication with interfaces/peripherals of thedevice 110. For example, thedevice 110 may correspond to a vehicle infotainment system that leverages an array of microphones positioned throughout the vehicle. Similarly, theacoustic speaker 118 may include one or more speakers either residing on thedevice 110, in communication therewith, or a combination where one or more speakers reside on thedevice 110 and one or more other speakers are physically removed from thedevice 110 but in communication with thedevice 110. - Furthermore, the
device 110 may be configured to communicate via anetwork 120 with aremote system 130. Theremote system 130 may includeremote resources 132, such as remote data processing hardware 134 (e.g., remote servers or CPUs) and/or remote memory hardware 136 (e.g., remote databases or other storage hardware). Thedevice 110 may utilize theremote resources 132 to perform various functionality related to speech processing and/or synthesized playback communication. For instance, thedevice 110 is configured to perform speech recognition using a speech recognition system 140 (e.g., using a speech recognition model 145). Additionally, the device may be configured to perform conversion of text to speech using aTTS system 150 and acoustic echo cancelation using anAEC system 200. Thesesystems device 110. In some examples, some of thesesystems systems system system remote system 130. Yet when thedevice 110 may support the size or the processing requirements of one ormore systems more systems device 110 using thedata processing hardware 112 and/or thememory hardware 114. Optionally, the one or more of thesystems systems remote system 130 when a connection to thenetwork 120 between thedevice 110 andremote system 130 is available, but when the connection is lost or thenetwork 120 is unavailable, thesystems device 110. - A
speech recognition system 140 receives anaudio signal 202 as an input and transcribes that audio signal into atranscription 142 as an output. Generally speaking, by converting theaudio signal 202 into thetranscription 142, thespeech recognition system 140 allows thedevice 110 to recognize when a spokenutterance 12 from theuser 10 corresponds to a query, a command, or some other form of audio communication. Thetranscription 142 refers to a sequence of text that thedevice 110 may then use to generate a response to the query or the command. For instance, if theuser 10 asks thedevice 110 the question of “what will the weather be like today,” thedevice 110 passes the audio signal corresponding to the question “what will the weather be like today” to thespeech recognition system 140. The speech recognizedsystem 140 converts the audio signal into a transcript that includes the text of “what will the weather be like today?” Thedevice 110 may then determine a response to the query using the text or portions of the text. For instance, in order to determine the weather for the current day (i.e., today), thedevice 110 passes the text (e.g., “what will the weather be like today?”) or identifying portions of the text (e.g., “weather” and “today”) to a search engine. The search engine may then return one or more search results that thedevice 110 interprets to generate a response for theuser 10. - In some implementations, the
device 110 or a system associated with thedevice 110 identifies text 152 that thedevice 110 will communicate to theuser 10 as a response to a query of the spokenutterance 12. Thedevice 110 may then use theTTS system 150 to convert the text 152 into correspondingsynthesized playback audio 154 for thedevice 110 to communicate to the user 10 (e.g., audibly communicate to the user 10) as the response to the query of the spokenutterance 12. In other words, theTTS system 150 receives, as input, text 152 and converts the text 152 to an output of synthesizedplayback audio 154 where the synthesizedplayback audio 154 is an audio signal defining an audible rendition of the text 152. In some examples, theTTS system 150 includes a text encoder that processes the text 152 into an encoded format (e.g., a text embedding). Here, theTTS system 150 may use a trained text-to-speech model to generate the synthesizedplayback audio 154 from the encoded format of the text 152. Once generated, theTTS system 150 communicates the synthesizedplayback audio 154 to thedevice 110 to allow thedevice 110 to output the synthesizedplayback audio 154. For instance, thedevice 110 outputs the synthesizedplayback audio 154 of “today is sunny” at thespeaker 118 of thedevice 110. - With continued reference to
FIG. 1 , when thedevice 110 outputs the synthesized playback audio 154 (e.g., synthesized speech), the synthesizedplayback audio 154 generates theecho 156 which is captured by themicrophone 116. Unfortunately, in addition to theecho 156, themicrophone 116 may also be simultaneously capturing another spokenutterance 12 from theuser 10 that corresponds to target speech directed toward thedevice 110. For example,FIG. 1 depicts that, as thedevice 110 outputs the synthesizedplayback audio content 154, theuser 10 inquires more about the weather, in a spokenutterance 12 to thedevice 110, by stating “what about tomorrow?” Notably, theuser 10 speaks theutterance 12 as part of a continued conversation scenario where thedevice 110 maintains themicrophone 116 open and thespeech recognition system 140 active to permit theuser 10 to provide follow-up queries for recognition by thespeech recognition system 140 without requiring theuser 10 to speak a hotword (e.g., a predetermined word or phrase that when detected triggers thedevice 110 to invoke speech recognition). When theutterance 12 spoken by theuser 10 includes a hotword, theecho 156 aids thespeech recognition system 140 in converting theaudio signal 14 into a transcript. However, when thedevice 110 does not require theuser 10 to speak a hotword to perform speech recognition, theuser device 110 may process themicrophone signal 202 containing theecho 156, causing theuser device 110 to process itsown playback audio 154 output from thespeaker 118. - Here, the spoken
utterance 12 and theecho 156 are both captured by themicrophone 116 simultaneously to form themicrophone signal 202. In other words, themicrophone signal 202 includes a portion that only includes theecho 156 corresponding to theplayback audio content 154 output from thespeaker 118 before theuser 10 speaks theutterance 12, an overlapped portion (e.g., overlapped region 204) where theutterance 12 spoken by theuser 10 overlaps with some portion of theplayback audio content 154 output from thespeaker 118, and a portion that includes only theutterance 12 spoken by theuser 12 after theacoustic speaker 118 stops output of theplayback audio content 154. - In
FIG. 1 , an overlappingregion 204 in the capturedmicrophone signal 202 corresponds to a double-talk event that indicates the instance where the portion of theutterance 12 and the portion of the synthesizedplayback audio 154 overlap with one another in the capturedmicrophone signal 202. During the double-talk event, thespeech recognition system 140 may have issues recognizing the follow-onquery 12 corresponding to the weather inquiry “what about tomorrow” in theaudio signal 202 captured by themicrophone 116 since theutterance 12 is mixed with theecho 156 of the synthesizedplayback audio content 154. As discussed above, one way to reduce theecho 156 in themicrophone signal 202 is by processing themicrophone signal 202 with an echo suppressor filter. However, thespeech recognition system 140 has difficulty processing this type of filteredmicrophone signal 202. - To resolve this, the
device 110 includes anAEC system 200 to process themicrophone signal 202 and provide the output to thespeech recognition system 140. The AEC system 200 (FIG. 2 ) includes anacoustic echo canceller 210, a double-talk detector 220, and anecho muter 230, and is configured to mute audio output signal frames that are identified as echo only, while permitting audio frames that include double-talk to pass through for processing by thespeech recognition system 140. In other words, theAEC system 200 receives themicrophone signal 202 including the follow-onquery 12 mixed with theecho 156. For each audio frame in the microphone signal, theAEC system 200 processes the audio frame and determines whether the audio frame is echo only, or includes double-talk. - With reference to
FIG. 2 , theacoustic echo canceller 210 of theAEC system 200 is configured to receive amicrophone signal 202 that may simultaneously include respective frames of the oftarget speech 12 directed toward the device and areference signal 158 corresponding to respective frames representing theplayback audio 154 captured by themicrophone 12. For each frame in the sequence of frames, theacoustic echo canceller 210 processes themicrophone signal 202 using thereference signal 158 to cancel theecho 156 in themicrophone signal 202, thereby producing anoutput signal frame 206. In some examples, theAEC system 200 further converts each respective frame of themicrophone signal 202, thereference signal 158, and theoutput signal 206 into a Short-Time Fourier Transform (STFT) domain using the following equation: -
Y(l,k)=Σn y[n]w[n−lL]exp(—j2πkn/N),k=0, . . . ,N−1 (1) - where L denotes the frame-hop, w[n] denotes an analysis window with a possible N point, l denotes the frame index, and k denotes the subband index.
- The double-
talk detector 220 then receives the respective frames in the STFT domain of the microphone signal m[n] 202 (denoted as M(l,k)), the reference signal x[n] 158 (denoted as X(l,k)), and the residual echo output signal r[n] 206 (denoted as (R(l,k)), and determines whether the respective frames include a double-talk frame or an echo-only frame. To accomplish this, the double-talk detector 220 uses two double-talk indicators 208 per subband in each respective frame. The two double-talk indicators 208 a, 208 b may be expressed as follows: -
- where cPQ(k) denotes the complex cross-correlation for each subband k, at a lag of zero, of the STFT domain of the playback audio frames 154 P(l,k) and Q(l,k). Here, P(l,k) and Q(l,k) are associateD with the time-domain signalS p[n] and q[n] respectively. The complex cross-correlation cPQ(k) may be expressed as follows:
-
c PQ(k)=E{P(l,k)Q*(l,k)} (3) - Where E{⋅} denotes the mathematical expectation and ⋅* denotes the complex conjugate. Further, the energy in the subband k may be expressed as follows:
-
σp 2(k)=E{P(l,k)P*(l,k)} (4) - A cross-correlation for each subband determines the energy of the subband by using an exponentially weighted averaging over the STFT domain using the following equations:
-
c PQ(l,k)=ρc PQ(l−1,k)+(1−ρ)P(l,k)Q*(l,k) -
σ2(l,k)=ρσ2(l−1,k)+(1−ρ)P(l,k)P*(l,k) (5) - where ρ denotes the exponential weighting factor (i.e., a forgetting factor). For example, an exponential weighting factor ρ of 0.9 may balance estimation accuracy and response time. Using this, Equation (2) may be reformulated and expressed as follows:
-
- In some examples, a first double-
talk indicator 208 a is computed by a cross-correlation between themicrophone signal 202 and thereference signal 158, while the second double-talk indicator 208 b is computed by a cross-correlation between themicrophone signal 202 and theoutput signal 206. - The double-
talk detector 220 uses the indicators and subband energies to compute double-talk indicators 208 a, 208 b for each respective frame by computing a weighted average of a subband indicator over a limited number of subbands. For example, subbands between 700 and 2400 Hz may be used for determining the double-talk indicators 208 a, 208 b. Each double-talk indicator 208 a, 208 b is weighed by the energy of the residual in the given subband by cross-correlating each frame in the sequence of frames to determine an energy in the band. The higher the residual energy in a subband indicates a higher likelihood of the presence of double-talk, leading the double-talk indicator 208 a, 208 b to indicate that the frame contains double-talk. The first double-talk indicator 208 a may be calculated as follows: -
- Likewise, a second double-talk indicator 208 b may be calculated as follows:
-
- Once the double-
talk detector 220 has computed the double-talk indicators 208 a, 208 b as output, it is provided to theecho muter 230 as input for determining whether or not at least one of the double-talk indicators 208 a, 208 b satisfy a double-talk threshold. In some examples, the double-talk detector 220 only computes one of the first or second double-talk indicators 208 a, 208 b as input to theecho muter 230 for determining whether the double-talk threshold is satisfied. Given a threshold τ, the double-talk detector 220 determines whether the respective frame is echo-only which should be muted, or includes double-talk which should be passed through. This is determined as follows: -
- When at least one of the double-
talk indicators 208 a, 208 b satisfy the double-talk threshold, the respective frame is passed through for processing by the speech recognition system 140). When both of the double-talk indicators 208 a, 208 b fail to satisfy the double-talk threshold, the respective frame is identified as echo-only and is muted (i.e., not passed for processing by the speech recognition system). Requiring the minimum of the double-talk indicators 208 a, 208 b to satisfy the double-talk threshold favors passing through frames over muting frames. - The double-talk threshold τ may be computed using a data-driven approach. Meta-data is collected from utterances to segment the utterances into echo-only and double-talk intervals. Next, the frame-level double-
talk indicators 208 a, 208 b are calculated for all of the frames in the training dataset. For a given threshold τ, the percentage of misclassification of echo-only and double-talk frames is calculated. This may be repeated for a range of thresholds τ with the target of identifying a threshold that results in a misclassification of double-talk frames that is 10% or less. -
FIG. 3 includes a flowchart of an example arrangement of operations for amethod 300 of performing speech recognition using an STFT-basedecho muter 230. Atoperation 302, themethod 300 includes receiving amicrophone signal 202 includingacoustic echo 156 captured by amicrophone 116. Theacoustic echo 156 corresponds toaudio content 154 played back from anacoustic speaker 118. Atoperation 304, themethod 300 includes receiving areference signal 158 including a sequence of frames representing theaudio content 154 transmitted in a reference channel before theacoustic speaker 118 plays back theaudio content 154. - For each frame in a sequence of frames of the
microphone signal 202, themethod 300, atoperation 306, includes processing, using anacoustic echo canceler 210 configured to receive a respective frame in the sequence of frames of thereference signal 158 as input, the respective frame of themicrophone signal 202 to generate a respectiveoutput signal frame 206 that cancels theacoustic echo 156 from the respective frame of themicrophone signal 202. Atoperation 308, themethod 300 includes determining, using a double-talk-detector 220, based on the respective frame of thereference signal 158 and the respectiveoutput signal frame 206, whether the respective frame of themicrophone signal 202 includes a double-talk frame or an echo-only frame. For each respective frame in the sequence of frames of themicrophone signal 202 that includes the echo-only frame, themethod 300 atoperation 310 includes muting the respectiveoutput signal frame 206. Atoperation 312, after muting the respectiveoutput signal frame 206 for each respective frame in the sequence of frames of themicrophone signal 202 that includes the echo-only frame, themethod 300 includes performing speech processing on the respectiveoutput signal frame 206 for each respective frame in the sequence frames of themicrophone signal 202 that includes the double-talk frame. -
FIG. 4 is schematic view of anexample computing device 400 that may be used to implement the systems and methods described in this document. Thecomputing device 400 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers. The components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document. - The
computing device 400 includes aprocessor 410,memory 420, astorage device 430, a high-speed interface/controller 440 connecting to thememory 420 and high-speed expansion ports 450, and a low speed interface/controller 460 connecting to a low speed bus 470 and astorage device 430. Each of thecomponents data processing hardware 410” that may include thedata processing hardware 112 of theuser computing device 110 or thedata processing hardware 134 of the remote system 130) can process instructions for execution within thecomputing device 400, including instructions stored in thememory 420 or on thestorage device 430 to display graphical information for a graphical user interface (GUI) on an external input/output device, such asdisplay 480 coupled tohigh speed interface 440. In other implementations, multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory. Also,multiple computing devices 400 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system). - The memory 420 (also referred to as “
memory hardware 420” that may include thememory hardware 114 of theuser computing device 110 or thememory hardware 136 of the remote system 130) stores information non-transitorily within thecomputing device 400. Thememory 420 may be a computer-readable medium, a volatile memory unit(s), or non-volatile memory unit(s). Thenon-transitory memory 420 may be physical devices used to store programs (e.g., sequences of instructions) or data (e.g., program state information) on a temporary or permanent basis for use by thecomputing device 400. Examples of non-volatile memory include, but are not limited to, flash memory and read-only memory (ROM)/programmable read-only memory (PROM)/erasable programmable read-only memory (EPROM)/electronically erasable programmable read-only memory (EEPROM) (e.g., typically used for firmware, such as boot programs). Examples of volatile memory include, but are not limited to, random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), phase change memory (PCM) as well as disks or tapes. - The
storage device 430 is capable of providing mass storage for thecomputing device 400. In some implementations, thestorage device 430 is a computer-readable medium. In various different implementations, thestorage device 430 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations. In additional implementations, a computer program product is tangibly embodied in an information carrier. The computer program product contains instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer- or machine-readable medium, such as thememory 420, thestorage device 430, or memory onprocessor 410. - The
high speed controller 440 manages bandwidth-intensive operations for thecomputing device 400, while thelow speed controller 460 manages lower bandwidth-intensive operations. Such allocation of duties is exemplary only. In some implementations, the high-speed controller 440 is coupled to thememory 420, the display 480 (e.g., through a graphics processor or accelerator), and to the high-speed expansion ports 450, which may accept various expansion cards (not shown). In some implementations, the low-speed controller 460 is coupled to thestorage device 430 and a low-speed expansion port 490. The low-speed expansion port 490, which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet), may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter. - The
computing device 400 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as astandard server 400 a or multiple times in a group ofsuch servers 400 a, as alaptop computer 400 b, or as part of arack server system 400 c. - Various implementations of the systems and techniques described herein can be realized in digital electronic and/or optical circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof. These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- A software application (i.e., a software resource) may refer to computer software that causes a computing device to perform a task. In some examples, a software application may be referred to as an “application,” an “app,” or a “program.” Example applications include, but are not limited to, system diagnostic applications, system management applications, system maintenance applications, word processing applications, spreadsheet applications, messaging applications, media streaming applications, social networking applications, and gaming applications.
- The non-transitory memory may be physical devices used to store programs (e.g., sequences of instructions) or data (e.g., program state information) on a temporary or permanent basis for use by a computing device. The non-transitory memory may be volatile and/or non-volatile addressable semiconductor memory. Examples of non-volatile memory include, but are not limited to, flash memory and read-only memory (ROM)/programmable read-only memory (PROM)/erasable programmable read-only memory (EPROM)/electronically erasable programmable read-only memory (EEPROM) (e.g., typically used for firmware, such as boot programs). Examples of volatile memory include, but are not limited to, random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), phase change memory (PCM) as well as disks or tapes.
- These computer programs (also known as programs, software, software applications or code) include machine instructions for a programmable processor, and can be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. As used herein, the terms “machine-readable medium” and “computer-readable medium” refer to any computer program product, non-transitory computer readable medium, apparatus and/or device (e.g., magnetic discs, optical disks, memory, Programmable Logic Devices (PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term “machine-readable signal” refers to any signal used to provide machine instructions and/or data to a programmable processor.
- The processes and logic flows described in this specification can be performed by one or more programmable processors, also referred to as data processing hardware, executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks. However, a computer need not have such devices. Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- To provide for interaction with a user, one or more aspects of the disclosure can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor, or touch screen for displaying information to the user and optionally a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices can be used to provide interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input. In addition, a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web browser on a user's client device in response to requests received from the web browser.
- A number of implementations have been described. Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the disclosure. Accordingly, other implementations are within the scope of the following claims.
Claims (26)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/643,825 US20230079828A1 (en) | 2021-09-16 | 2021-12-11 | STFT-Based Echo Muter |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202163261281P | 2021-09-16 | 2021-09-16 | |
US17/643,825 US20230079828A1 (en) | 2021-09-16 | 2021-12-11 | STFT-Based Echo Muter |
Publications (1)
Publication Number | Publication Date |
---|---|
US20230079828A1 true US20230079828A1 (en) | 2023-03-16 |
Family
ID=80050852
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US17/643,825 Pending US20230079828A1 (en) | 2021-09-16 | 2021-12-11 | STFT-Based Echo Muter |
Country Status (4)
Country | Link |
---|---|
US (1) | US20230079828A1 (en) |
KR (1) | KR20240049592A (en) |
CN (1) | CN117940995A (en) |
WO (1) | WO2023043470A1 (en) |
Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10586534B1 (en) * | 2017-09-27 | 2020-03-10 | Amazon Technologies, Inc. | Voice-controlled device control using acoustic echo cancellation statistics |
US10622009B1 (en) * | 2018-09-10 | 2020-04-14 | Amazon Technologies, Inc. | Methods for detecting double-talk |
Family Cites Families (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6606382B2 (en) * | 2000-01-27 | 2003-08-12 | Qualcomm Incorporated | System and method for implementation of an echo canceller |
-
2021
- 2021-12-11 US US17/643,825 patent/US20230079828A1/en active Pending
- 2021-12-11 KR KR1020247009521A patent/KR20240049592A/en unknown
- 2021-12-11 CN CN202180102359.8A patent/CN117940995A/en active Pending
- 2021-12-11 WO PCT/US2021/062970 patent/WO2023043470A1/en active Application Filing
Patent Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10586534B1 (en) * | 2017-09-27 | 2020-03-10 | Amazon Technologies, Inc. | Voice-controlled device control using acoustic echo cancellation statistics |
US10622009B1 (en) * | 2018-09-10 | 2020-04-14 | Amazon Technologies, Inc. | Methods for detecting double-talk |
Non-Patent Citations (1)
Title |
---|
Seon Joon Park et al., Integrated Echo and Noise Canceler for Hands-Free Applications, 49 IEEE Trans. on Circuits and Sys.—II: Analog and Digital Signal Process. 188 (March 2002) (Year: 2002) * |
Also Published As
Publication number | Publication date |
---|---|
CN117940995A (en) | 2024-04-26 |
WO2023043470A1 (en) | 2023-03-23 |
KR20240049592A (en) | 2024-04-16 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US9293133B2 (en) | Improving voice communication over a network | |
US9536540B2 (en) | Speech signal separation and synthesis based on auditory scene analysis and speech modeling | |
US10839820B2 (en) | Voice processing method, apparatus, device and storage medium | |
US10706842B2 (en) | Selective adaptation and utilization of noise reduction technique in invocation phrase detection | |
US11699453B2 (en) | Adaptive multichannel dereverberation for automatic speech recognition | |
US20230317096A1 (en) | Audio signal processing method and apparatus, electronic device, and storage medium | |
US20170213556A1 (en) | Methods And Apparatus For Speech Segmentation Using Multiple Metadata | |
US20220366927A1 (en) | End-To-End Time-Domain Multitask Learning for ML-Based Speech Enhancement | |
US11776563B2 (en) | Textual echo cancellation | |
US20230079828A1 (en) | STFT-Based Echo Muter | |
Lee et al. | Dialogue enabling speech-to-text user assistive agent system for hearing-impaired person | |
US11176957B2 (en) | Low complexity detection of voiced speech and pitch estimation | |
US20230298612A1 (en) | Microphone Array Configuration Invariant, Streaming, Multichannel Neural Enhancement Frontend for Automatic Speech Recognition | |
JP2022544065A (en) | Method and Apparatus for Normalizing Features Extracted from Audio Data for Signal Recognition or Correction | |
US20230038982A1 (en) | Joint Acoustic Echo Cancelation, Speech Enhancement, and Voice Separation for Automatic Speech Recognition | |
US20230298609A1 (en) | Generalized Automatic Speech Recognition for Joint Acoustic Echo Cancellation, Speech Enhancement, and Voice Separation | |
US20230267949A1 (en) | Streaming Vocoder | |
Vijayendra et al. | Word boundary detection for Gujarati speech recognition using in-ear microphone | |
Ives et al. | A 350 MHz, 200 kW CW, multiple beam IOT |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:SHABESTARY, TURAJ ZAKIZADEH;REEL/FRAME:058703/0513Effective date: 20210916 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:SHABESTARY, TURAJ ZAKIZADEH;NARAYANAN, ARUN;REEL/FRAME:059673/0808Effective date: 20210916 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: FINAL REJECTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS |