JP2014503882A - Navigation with guidance through geographically located panoramas - Google Patents
Navigation with guidance through geographically located panoramas Download PDFInfo
- Publication number
- JP2014503882A JP2014503882A JP2013541029A JP2013541029A JP2014503882A JP 2014503882 A JP2014503882 A JP 2014503882A JP 2013541029 A JP2013541029 A JP 2013541029A JP 2013541029 A JP2013541029 A JP 2013541029A JP 2014503882 A JP2014503882 A JP 2014503882A
- Authority
- JP
- Japan
- Prior art keywords
- navigation
- virtual camera
- panoramic image
- path
- dimensional
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
- 238000000034 method Methods 0.000 claims abstract description 36
- 230000000007 visual effect Effects 0.000 claims abstract description 32
- 238000009877 rendering Methods 0.000 claims abstract description 21
- 238000012937 correction Methods 0.000 claims description 25
- 238000002156 mixing Methods 0.000 claims 4
- 230000002457 bidirectional effect Effects 0.000 claims 2
- 230000002452 interceptive effect Effects 0.000 abstract description 5
- 238000010586 diagram Methods 0.000 description 30
- 230000015654 memory Effects 0.000 description 17
- 238000004891 communication Methods 0.000 description 16
- 238000004590 computer program Methods 0.000 description 10
- 230000003993 interaction Effects 0.000 description 9
- 238000012545 processing Methods 0.000 description 7
- 230000006870 function Effects 0.000 description 5
- 230000003287 optical effect Effects 0.000 description 4
- 230000008859 change Effects 0.000 description 3
- 238000012986 modification Methods 0.000 description 3
- 230000004048 modification Effects 0.000 description 3
- 230000008569 process Effects 0.000 description 3
- 230000009471 action Effects 0.000 description 2
- 230000006399 behavior Effects 0.000 description 2
- 210000001747 pupil Anatomy 0.000 description 2
- 230000004044 response Effects 0.000 description 2
- 230000004913 activation Effects 0.000 description 1
- 230000006978 adaptation Effects 0.000 description 1
- 230000008901 benefit Effects 0.000 description 1
- 230000001413 cellular effect Effects 0.000 description 1
- 239000000835 fiber Substances 0.000 description 1
- 230000010354 integration Effects 0.000 description 1
- 230000009467 reduction Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 238000012360 testing method Methods 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000007704 transition Effects 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T19/00—Manipulating 3D models or images for computer graphics
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T15/00—3D [Three Dimensional] image rendering
- G06T15/10—Geometric effects
- G06T15/20—Perspective computation
Abstract
双方向バーチャル３次元環境における誘導付きナビゲーションのための能力が、提供される。そのような能力は、自由形式のナビゲーションの感覚をユーザに提供することによって、ユーザ経験を向上させ得る。ユーザを良好な視覚的品質のあるエリアに制約し、自由形式のナビゲーションのメタファーを分断することなく、より良好なレンダリング結果を伴うビューポイントに向かって、ユーザを緻密に誘導することが必要であり得る。加えて、そのような能力は、双方向バーチャル３次元環境内において、ユーザが、街路を「運転」し、湾曲道路をたどり、交差点の周囲で方向転換することを可能にし得る。さらに、この能力は、道路網および／または進路に基づいて、ナビゲーションを組み込む任意の３次元グラフィックシステムに加え、画像ベースのレンダリング技法にも適用可能であり得る。Capabilities for guided navigation in an interactive virtual three-dimensional environment are provided. Such capabilities can improve the user experience by providing the user with a sense of free-form navigation. It is necessary to constrain the user to an area with good visual quality and guide the user closely towards a viewpoint with better rendering results without disrupting the free-form navigation metaphor obtain. In addition, such capabilities may allow a user to “drive” a street, follow a curved road, and turn around an intersection within an interactive virtual three-dimensional environment. Furthermore, this capability may be applicable to any 3D graphics system that incorporates navigation based on road networks and / or courses, as well as image-based rendering techniques.
Description
実施形態は、概して、３次元グラフィックに関する。 Embodiments generally relate to three-dimensional graphics.
３次元データを表示するように、３次元環境を通してナビゲートするためのシステムが存在する。３次元環境は、どの３次元データを表示するかを定義する、バーチャルカメラを含む。バーチャルカメラは、その位置および配向に従った眺め（ｐｅｒｓｐｅｃｔｉｖｅ）を有する。バーチャルカメラの眺めを変更することによって、ユーザは、３次元環境を通してナビゲートすることができる。 There are systems for navigating through a 3D environment to display 3D data. The 3D environment includes a virtual camera that defines which 3D data to display. A virtual camera has a perspective according to its position and orientation. By changing the view of the virtual camera, the user can navigate through the three-dimensional environment.
地理情報システムは、３次元環境を通してナビゲートするためにバーチャルカメラを使用するシステムのタイプの１つである。地理情報システムは、地球の実質的に球形の３次元モデルを記憶し、読み出し、操作し、表示するためのシステムである。３次元モデルは、衛星画像、地図、建造物および地形のモデル、ならびに他の地理的特徴を含み得る。さらに、３次元環境は、種々の地理的場所に対応する、実世界の景色の地理的位置指定写真を表示するために使用され得る。例えば、地理的位置指定写真は、主要大都市の街路網に対応し得る。そのような写真はまた、３６０度の街路レベルのビューを提供する、パノラマ画像を含み得る。 Geographic information systems are one type of system that uses virtual cameras to navigate through a three-dimensional environment. A geographic information system is a system for storing, reading, manipulating, and displaying a substantially spherical three-dimensional model of the earth. The three-dimensional model may include satellite images, maps, building and terrain models, and other geographic features. In addition, the three-dimensional environment can be used to display geographically-positioned photographs of real-world scenes that correspond to various geographic locations. For example, a geographic location photo may correspond to a street network of a major metropolis. Such photos may also include panoramic images that provide a 360 degree street level view.
地理情報システムにおけるバーチャルカメラは、異なる眺めから地球の球形３次元モデルを見得る。地球のモデルの空中ビューは、衛星画像を示し得るが、地形および建造物は、表示されない場合がある。一方で、モデルの地上レベルのビュー（ｖｉｅｗ）は、地形および建造物を詳細に示し得る。しかしながら、モデルのいくつかのビューは、ＧＩＳによって表示される画像コンテンツの正確なまたは視覚的に容認可能な表現ではない場合がある。例えば、ＧＩＳにおいて表示される写真画像は、例えば、画像を捕捉するために使用されるカメラの位置に基づいて、単一のビューポイント（ｖｉｅｗｐｏｉｎｔ）またはある範囲のビューポイントからのみ正確であり得る。従来のシステムでは、３次元環境における画像の異なるビュー間のナビゲーションは、困難かつユーザにとって視覚的に魅力のないものとなり得る。 Virtual cameras in geographic information systems can see a spherical three-dimensional model of the Earth from different views. An aerial view of a model of the earth may show satellite images, but terrain and buildings may not be displayed. On the other hand, the ground level view of the model may show terrain and buildings in detail. However, some views of the model may not be an accurate or visually acceptable representation of the image content displayed by the GIS. For example, a photographic image displayed in a GIS may only be accurate from a single viewpoint or a range of viewpoints, for example based on the position of the camera used to capture the image. In conventional systems, navigation between different views of an image in a three-dimensional environment can be difficult and visually unattractive to the user.
双方向バーチャル３次元環境における誘導付きナビゲーションのための能力が、提供される。そのような能力は、自由形式のナビゲーションの感覚をユーザに提供することによって、ユーザ経験を向上させ得る。ユーザを良好な視覚的品質のあるエリアに制約し、自由形式のナビゲーションのメタファーを分断することなく、より良好なレンダリング結果を伴うビューポイントに向かって、ユーザを緻密に誘導することが必要であり得る。加えて、そのような能力は、双方向バーチャル３次元環境内において、ユーザが、街路を「運転」し、湾曲道路をたどり、交差点の周囲で方向転換することを可能にし得る。さらに、この能力は、道路網および／または進路に基づいて、ナビゲーションを組み込む、任意の３次元グラフィックシステムに加え、画像ベースのレンダリング技法にも適用可能であり得る。 Capabilities for guided navigation in an interactive virtual three-dimensional environment are provided. Such capabilities can improve the user experience by providing the user with a sense of free-form navigation. It is necessary to constrain the user to an area with good visual quality and guide the user closely towards a viewpoint with better rendering results without disrupting the free-form navigation metaphor obtain. In addition, such capabilities may allow a user to “drive” a street, follow a curved road, and turn around an intersection within an interactive virtual three-dimensional environment. In addition, this capability may be applicable to any 3D graphics system that incorporates navigation based on road networks and / or courses, as well as image-based rendering techniques.
ある実施形態では、第１のパノラマ画像および第２のパノラマ画像に対するポリゴンの３次元メッシュが、それぞれの第１および第２のパノラマ画像内に表される景色内の種々の点に関連付けられた深度値に基づいて生成される。第１のパノラマ画像は、第２のパノラマ画像へのリンクを含む。リンクは、３次元空間における、パノラマ画像を接続する無障害進路（例えば、街路）を表す。３次元メッシュにおいて、第１および第２のパノラマ画像の各々に対する十分な視覚的品質の領域が決定される。十分な視覚的品質の領域は、深度値に基づくことができ、第１および第２のパノラマ画像内の画像オブジェクトの視覚的に正確な表現となるように３次元メッシュがレンダリングされ得るようなビューポイントに対応し得る。ナビゲーション半径が、第１および第２のパノラマ画像の各々の視覚的品質の決定された領域に基づいて、第１および第２のパノラマ画像の各々に対して計算される。３次元メッシュにおける、第１と第２のパノラマ画像との間の進路に対するナビゲーション経路は、画像の各々の計算されたナビゲーション半径に基づいて作成される。ナビゲーション経路は、バーチャルカメラが移動され得る３次元環境内の空間の境界された体積を規定するために使用される。ナビゲーション経路は、バーチャルカメラが環境内の異なるビューポイント間を移動する場合に、視覚的品質が維持されることを確実にする。 In some embodiments, the depth associated with the three-dimensional mesh of polygons for the first panorama image and the second panorama image is associated with various points in the scene represented in the respective first and second panorama images. Generated based on the value. The first panoramic image includes a link to the second panoramic image. The link represents a non-failing path (for example, a street) that connects panoramic images in a three-dimensional space. In the three-dimensional mesh, an area of sufficient visual quality for each of the first and second panoramic images is determined. A region with sufficient visual quality can be based on depth values, such that a 3D mesh can be rendered to be a visually accurate representation of the image objects in the first and second panoramic images. Can correspond to points. A navigation radius is calculated for each of the first and second panoramic images based on the determined region of visual quality of each of the first and second panoramic images. A navigation path for the path between the first and second panoramic images in the three-dimensional mesh is created based on the calculated navigation radius of each of the images. The navigation path is used to define the bounded volume of space in the 3D environment where the virtual camera can be moved. The navigation path ensures that visual quality is maintained when the virtual camera moves between different viewpoints in the environment.
実施形態は、ハードウェア、ファームウェア、ソフトウェア、またはそれらの組み合わせを使用して、実装され得、１つ以上のコンピュータシステムまたは他の処理システム内に実装され得る。 Embodiments may be implemented using hardware, firmware, software, or combinations thereof, and may be implemented within one or more computer systems or other processing systems.
本発明のさらなる実施形態、特徴、および利点、ならびに種々の実施形態の構造および動作が、添付図面を参照して以下で詳細に説明される。本発明は、本明細書に説明される特定の実施形態に限定されないことに留意されたい。そのような実施形態は、例証的目的のためだけに本明細書に提示される。追加の実施形態は、本明細書に含まれる情報に基づいて、当業者に明白となるであろう。 Further embodiments, features, and advantages of the present invention, as well as the structure and operation of the various embodiments, are described in detail below with reference to the accompanying drawings. It should be noted that the present invention is not limited to the specific embodiments described herein. Such embodiments are presented herein for illustrative purposes only. Additional embodiments will be apparent to those skilled in the art based on the information contained herein.
実施形態は、付随の図面を参照して、単なる一例として説明される。図面中、類似番号は、同一または機能的に同様の要素を示し得る。ある要素が最初に出現する図面は、典型的には、対応する参照番号における１つまたは複数の最左の数字によって示される。 Embodiments are described by way of example only with reference to the accompanying drawings. In the drawings, like numbers can indicate identical or functionally similar elements. The drawing in which an element first appears is typically indicated by one or more leftmost digits in the corresponding reference number.
本明細書に組み込まれ、本明細書の一部を形成する添付図面は、本発明の実施形態を図示し、説明とともに、さらに本発明の原理を説明し、かつ当業者が本発明を作製し、使用することを可能にする役割を果たす。 The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate embodiments of the invention, together with the description, further explain the principles of the invention and allow those skilled in the art to make the invention. Play a role that allows you to use.
実施形態は、双方向バーチャル３次元環境における誘導付きナビゲーションに関する。本発明は、特定のアプリケーションのための例証的実施形態を参照して本明細書に説明されるが、実施形態は、それらに限定されないことを理解されたい。他の実施形態も可能であって、本明細書の教示の精神および範囲における実施形態、ならびに実施形態が有意に有用となるであろう追加の分野に対して修正を行うことができる。さらに、特定の特徴、構造、または特性が、実施形態と関連して説明される場合、明示的に説明されるかどうかにかかわらず、他の実施形態と関連して、そのような特徴、構造、または特性を達成することが当業者の知識の範囲内であることが想定される。 Embodiments relate to navigation with guidance in an interactive virtual three-dimensional environment. Although the invention is described herein with reference to illustrative embodiments for particular applications, it should be understood that the embodiments are not limited thereto. Other embodiments are possible and modifications may be made to the embodiments within the spirit and scope of the teachings herein, and to additional areas where the embodiments will be significantly useful. Further, when a particular feature, structure, or characteristic is described in connection with an embodiment, such feature, structure, or structure in connection with other embodiments, whether or not explicitly described. It is envisaged that achieving the characteristics is within the knowledge of one of ordinary skill in the art.
また、実施形態が、本明細書に説明されるように、図に図示されるソフトウェア、ハードウェア、ファームウェア、および／またはエンティティの多くの異なる実施形態に実装されることができることは、当業者に明白であろう。実施形態を実装するためのハードウェアの特殊制御を伴う、任意の実際のソフトウェアコードは、発明を実施するための形態の限定ではない。したがって、実施形態の動作挙動は、本明細書に提示される詳細のレベルを前提として、実施形態の修正および変形例が可能であるという了解の下で説明されるであろう。 It will also be appreciated by those skilled in the art that embodiments may be implemented in many different embodiments of the software, hardware, firmware, and / or entities illustrated in the figures, as described herein. It will be obvious. Any actual software code with special control of the hardware to implement the embodiments is not a limitation of the forms for carrying out the invention. Accordingly, the operational behavior of the embodiments will be described with the understanding that modifications and variations of the embodiments are possible given the level of detail presented herein.
以下の発明を実施するための形態では、「一実施形態」、「実施形態」、「例示的実施形態」等への言及は、説明される実施形態が、特定の特徴、構造、または特性を含み得るが、全実施形態が、必ずしも、特定の特徴、構造、または特性を含まなくてもよいことを示す。また、そのような語句は、必ずしも、同じ実施形態を指しているとは限らない。さらに、特定の特徴、構造、または特性が、実施形態と関連して説明される場合、明示的に説明されるかどうかにかかわらず、他の実施形態と関連して、そのような特徴、構造、または特性を達成することが当業者の知識の範囲内であることが想定される。 In the following detailed description, references to “one embodiment,” “embodiment,” “exemplary embodiment,” and the like indicate that the described embodiment has certain features, structures, or characteristics. Although shown, all embodiments may not necessarily include a particular feature, structure, or characteristic. Moreover, such phrases are not necessarily referring to the same embodiment. Further, when a particular feature, structure, or characteristic is described in connection with an embodiment, such feature, structure, or structure in connection with other embodiments, whether or not explicitly described. It is envisaged that achieving the characteristics is within the knowledge of one of ordinary skill in the art.
用語「パノラマ画像」および「パノラマ」は、実世界の景色の３６０度ビューを提供する、任意のパノラマ画像を広義かつ包括的に指すために本明細書で使用される。パノラマまたはパノラマ画像は、複数のパノラマ画像またはパノラマ画像タイルの形態で記憶され得る。そのようなパノラマはさらに、ディスプレイ上に提示されることができるパノラマ３次元画像を含む、３次元画像を含み得る。そのようなディスプレイは、画像を見るための任意のタイプの電子ディスプレイであることができるか、または３次元画像を見るように適合される、任意のタイプのレンダリングデバイスであることができる。さらに、各パノラマ画像は、画像によって表される景色内の種々の点に対応する、深度情報に関連付けられ得る。以下にさらに詳細に説明されるように、この深度情報は、パノラマのための３次元幾何学形状を生成するために使用されることができる。 The terms “panoramic image” and “panorama” are used herein to refer broadly and comprehensively to any panoramic image that provides a 360 degree view of the real world landscape. The panorama or panoramic image may be stored in the form of a plurality of panoramic images or panoramic image tiles. Such panoramas may further include 3D images, including panoramic 3D images that can be presented on a display. Such a display can be any type of electronic display for viewing images, or it can be any type of rendering device adapted to view 3D images. In addition, each panoramic image can be associated with depth information corresponding to various points in the scene represented by the image. As described in more detail below, this depth information can be used to generate a three-dimensional geometry for the panorama.
（街路網に沿った、誘導付きナビゲーションのためのシステム）
図１は、ある実施形態による、街路網に沿った、誘導付きナビゲーションのためのシステム１００を図示する略図である。システム１００は、ネットワーク１３０を横断して、サーバ１４０に通信可能に連結される、クライアント１０２を含む。ネットワーク１３０は、データ通信を搬送し得る任意のネットワークまたはネットワークの組み合わせであることができる。そのようなネットワークは、ローカルエリアネットワーク、ミディアムエリアネットワーク、および／またはインターネット等の広域ネットワークを含むことができるが、それらに限定されない。
(System for guided navigation along the street network)
FIG. 1 is a schematic diagram illustrating a system 100 for guided navigation along a street network, according to an embodiment. System 100 includes a client 102 that is communicatively coupled to a server 140 across a network 130. The network 130 can be any network or combination of networks that can carry data communications. Such networks can include, but are not limited to, wide area networks such as local area networks, medium area networks, and / or the Internet.
クライアント１０２は、プロセッサ、ローカルメモリ、ディスプレイ、および１つ以上の入力デバイス（マウス、ＱＷＥＲＴＹキーボード、タッチスクリーン、マイクロホン、ジョイスティック、またはＴ９キーボード）を伴う、コンピュータであることができる。そのようなコンピューティングデバイスは、携帯電話、携帯情報端末（ＰＤＡ）、コンピュータ、コンピュータ群、セットトップボックス、または命令を処理可能な他の類似タイプのデバイスを含むことができるが、それらに限定されない。サーバ１４０は、同様に、データをクライアント１０２にサービス提供可能な任意の汎用コンピュータを使用して、実装されることができる。サーバ１４０のみ示されるが、任意の数のサーバが、必要に応じて、使用され得る。 The client 102 can be a computer with a processor, local memory, display, and one or more input devices (mouse, QWERTY keyboard, touch screen, microphone, joystick, or T9 keyboard). Such computing devices can include, but are not limited to, mobile phones, personal digital assistants (PDAs), computers, computer groups, set top boxes, or other similar types of devices capable of processing instructions. . Server 140 can also be implemented using any general purpose computer capable of serving data to client 102. Although only server 140 is shown, any number of servers may be used as needed.
ある実施形態では、クライアント１０２は、ユーザ相互作用モジュール１１０およびレンダラモジュール１２２を含む。ユーザ相互作用モジュール１１０は、標的モジュール１１２、移動モジュール１１４、進路プランナモジュール１１６、および進路移動モジュール１１８を含む。ユーザ相互作用モジュール１１０、レンダラモジュール１２２、移動モデル１１４、標的モジュール１１２、進路プランナモジュール１１６、および進路移動モジュール１１８の実施形態は、ハードウェア、ソフトウェア、ファームウェア、または任意のそれらの組み合わせにおいて実装され得る。 In some embodiments, the client 102 includes a user interaction module 110 and a renderer module 122. The user interaction module 110 includes a target module 112, a movement module 114, a course planner module 116, and a course movement module 118. Embodiments of user interaction module 110, renderer module 122, movement model 114, target module 112, course planner module 116, and course movement module 118 may be implemented in hardware, software, firmware, or any combination thereof. .
説明の容易性および例証目的のために、システム１００およびその構成要素の実施形態は、クライアント−サーバコンピュータアーキテクチャ内で動作する、地理的情報システム（ＧＩＳ）に照らして説明されるが、実施形態は、それらに限定されることを意図するものではない。そのようなＧＩＳは、以下にさらに詳細に説明されるように、バーチャルカメラの眺めから、バーチャル３次元環境内の写真パノラマをレンダリングおよびナビゲートするために使用されることができる。そのようなＧＩＳの実施例は、Ｇｏｏｇｌｅ Ｉｎｃ．（Ｍｏｕｎｔａｉｎ Ｖｉｅｗ，ＣＡ）製Ｇｏｏｇｌｅ Ｅａｒｔｈを含むが、それに限定されない。 For ease of explanation and illustrative purposes, the embodiment of the system 100 and its components will be described in the context of a geographic information system (GIS) operating within a client-server computer architecture. It is not intended to be limited thereto. Such a GIS can be used to render and navigate a photo panorama in a virtual 3D environment from a virtual camera view, as will be described in more detail below. An example of such a GIS is Google Inc. Including, but not limited to, Google Earth (Mountain View, CA).
故に、クライアント１０２は、インターフェースをＧＩＳに提供するように構成される、地理空間ブラウザ（図示せず）を含み得る。そのような地理空間ブラウザは、クライアント１０２によって実行可能であるか、またはウェブまたはインターネットブラウザのプラグインとして動作可能である独立型クライアントアプリケーションであることができる。ある実施例では、サーバ１４０は、地球の実質的球状表現上の着目の地理的領域に対応するＧＩＳデータを伝送するように構成されるＧＩＳサーバである。例えば、ある要求は、地理的場所に関連付けられた緯度／経度データ点を有する境界ボックスの形態において、境界された地理的検索エリアを含み得る。要求の受信に応答して、サーバ１４０は、サーバ１４０が、ネットワーク（例えば、ネットワーク１３０）を経由して通信可能に連結される、データベースまたは１つ以上の異なるサーバと通信し得る。そのようなＧＩＳデータは、画像および非画像データの両方を含み得る。画像データは衛星画像、デジタル地図、航空写真、および街路レベルの写真を含み得るが、それらに限定されない。非画像データは、例えば、画像および写真に関連付けられたメタデータを含み得る。 Thus, the client 102 may include a geospatial browser (not shown) that is configured to provide an interface to the GIS. Such a geospatial browser can be a stand-alone client application that can be executed by the client 102 or that can operate as a plug-in for a web or Internet browser. In one embodiment, server 140 is a GIS server configured to transmit GIS data corresponding to a geographic region of interest on a substantially spherical representation of the earth. For example, a request may include a bounded geographic search area in the form of a bounding box having latitude / longitude data points associated with the geographic location. In response to receiving the request, server 140 may communicate with a database or one or more different servers to which server 140 is communicatively coupled via a network (eg, network 130). Such GIS data may include both image and non-image data. Image data may include, but is not limited to, satellite images, digital maps, aerial photographs, and street level photographs. Non-image data may include, for example, metadata associated with images and photos.
ある実施形態では、街路レベルの写真は、実世界景色の３６０度ビューを提示するパノラマを含み得る。さらに、各パノラマは、パノラマに関連付けられた情報を指定するために使用される、メタデータを含み得るか、またはそれに関連付けられ得る。そのようなメタデータは、バーチャルカメラの位置（例えば、ヨー、ピッチ、およびロール）、ＧＰＳ座標、およびパノラマ識別子（例えば、ＧＩＳ内のパノラマを識別する、文字列値）を含み得るが、それらに限定されない。 In some embodiments, street level photographs may include a panorama that presents a 360 degree view of real world scenery. Further, each panorama may include or be associated with metadata that is used to specify information associated with the panorama. Such metadata may include virtual camera positions (eg, yaw, pitch, and roll), GPS coordinates, and panorama identifiers (eg, string values that identify panoramas in GIS), It is not limited.
ある実施形態による、標的モジュール１１２は、３次元環境内のバーチャルカメラの現在のビュー方向および／またはビュー位置に基づいて、３次元環境内の標的場所を決定する。一実施形態では、標的モジュール１１２は、住所または座標等、標的場所を指定する、ユーザからの入力を受け取る。別の実施形態では、標的モジュール１１２は、ユーザによって選択されたビューポート上の位置を表す位置データを受信し、ユーザによって選択された位置に基づいて、半直線を延長し、３次元モデル内のオブジェクトと半直線の交差点である標的場所を決定し得る。ある実施形態では、標的位置は、３次元環境内の写真パノラマの中心（例えば、以下にさらに説明される、図２のパノラマ中心２０４）に関連付けられ得る。 According to an embodiment, the target module 112 determines a target location in the three-dimensional environment based on the current view direction and / or view position of the virtual camera in the three-dimensional environment. In one embodiment, the target module 112 receives input from a user specifying a target location, such as an address or coordinates. In another embodiment, the target module 112 receives position data representing a position on the viewport selected by the user, and extends a half line based on the position selected by the user in the 3D model. A target location that is the intersection of an object and a half line may be determined. In some embodiments, the target location may be associated with the center of the photographic panorama within the three-dimensional environment (eg, the panorama center 204 of FIG. 2, described further below).
進路移動モジュール１１８は、進路に沿って、標的場所に向かって、３次元環境内でバーチャルカメラを移動させ、バーチャルカメラが、進路に沿って、移動するにつれて、バーチャルカメラが、標的場所に向くように配向する。進路移動モジュール１１８は、サーバ１４０から地理的情報を先読みするために、進路を使用し得る。 The path moving module 118 moves the virtual camera in the three-dimensional environment along the path toward the target location so that the virtual camera is directed toward the target location as the virtual camera moves along the path. Oriented to The path travel module 118 may use the path to prefetch geographic information from the server 140.
移動モデル１１４は、ビュー仕様を構築する。ビュー仕様は、３次元空間内のバーチャルカメラのビュー可能体積（ｖｉｅｗａｂｌｅ ｖｏｌｕｍｅ）と、例えば、３次元地図に対する錐台の位置および配向とを規定する。ある実施形態では、ビュー可能体積は、例えば、角錐台の形状であり得る、視錐台（または、単に「錐台」）によって規定される。錐台は、現在の視野に応じて変化することができる、最小および最大ビュー距離（ｖｉｅｗ ｄｉｓｔａｎｃｅ）を有し得る。３次元地図のユーザのビューが、操作されるにつれて、錐台の配向および位置が、３次元バーチャル環境に対して変化する。したがって、ユーザ入力が受信されるにつれて、ビュー仕様もまた変化する。ビュー仕様は、例えば、クライアント１０２のローカルメモリ内に記憶され得、そこで、ＧＩＳデータを描くために、レンダラモジュール１２２によって使用される。 The movement model 114 builds a view specification. The view specification defines the viewable volume of the virtual camera in 3D space and the position and orientation of the frustum, for example with respect to the 3D map. In some embodiments, the viewable volume is defined by a viewing frustum (or simply “frustum”), which may be, for example, in the shape of a truncated pyramid. The frustum may have a minimum and maximum view distance that can vary depending on the current field of view. As the user's view of the 3D map is manipulated, the orientation and position of the frustum changes relative to the 3D virtual environment. Thus, as user input is received, the view specification also changes. The view specification can be stored, for example, in the local memory of the client 102 where it is used by the renderer module 122 to draw GIS data.
一実施形態によると、記憶されたビュー仕様は、バーチャルカメラのナビゲーションパラメータを指定する。ナビゲーションパラメータは、３次元空間内のバーチャルカメラの位置および配向を指定するために使用されることができる。そのようなパラメータは、方向座標、（例えば、緯度、経度、北、南、その中間）、高度、およびピッチ／傾斜（例えば、レベル、下方、上方、その中間）、ヨー／ロール（例えば、レベル、時計回り傾斜、反時計回り傾斜、その中間）、水平視野、および垂直視野を含むことができるが、それらに限定されない。 According to one embodiment, the stored view specification specifies the navigation parameters of the virtual camera. Navigation parameters can be used to specify the position and orientation of the virtual camera in three-dimensional space. Such parameters include directional coordinates (eg, latitude, longitude, north, south, midway), altitude, and pitch / tilt (eg, level, down, up, midway), yaw / roll (eg, level , Clockwise tilt, counterclockwise tilt, intermediate), horizontal field of view, and vertical field of view.
（深度統合）
ある実施形態では、写真パノラマは、ある実施形態による、パノラマ画像によって表される景色内の種々の点に対応する、深度情報を含む。深度情報は、例えば、第１の位置に対する景色内の種々の点の接近度を記述する深度値を含むことができる。第１の位置は、例えば、画像を捕捉するために使用される、画像捕捉デバイス（例えば、特殊デジタルカメラ）の位置であることができる。ある実施形態では、景色内に表されるオブジェクトの表面は、点集合として表され得る。各点は、順に、ベクトルとして表され得、それによって、各点は、画像捕捉デバイスまでのその距離と、そのような画像捕捉デバイスが指向される方向に対するその角度とに関して記憶される。
(Depth integration)
In certain embodiments, a photographic panorama includes depth information corresponding to various points in the scene represented by the panoramic image, according to certain embodiments. The depth information can include, for example, a depth value that describes the proximity of various points in the scene to the first location. The first position can be, for example, the position of an image capture device (eg, a special digital camera) used to capture an image. In some embodiments, the surface of an object represented in the scene may be represented as a point set. Each point may in turn be represented as a vector, whereby each point is stored with respect to its distance to the image capture device and its angle relative to the direction in which such image capture device is directed.
深度情報は、限定されないが、レーザ測距器および画像マッチングの使用を含む、種々の方法で収集され得る。ある実施形態では、若干離間されるが、同一景色を捉える、２つ以上のカメラを採用するカメラ配列が使用され得る。ある実施形態によると、画像マッチングは、画像内の各点における距離を決定するために、各カメラによって捕捉される画像間の若干の差異を分析するために使用される。別の実施形態では、距離情報は、車両上に搭載され、特定の速度で進行している、単一ビデオカメラを使用して、車両が前方に進行するにつれて、景色の画像を捕捉することによって、コンパイルされ得る。レーザ測距器はまた、画像を撮影するカメラと組み合わせて使用され得る。画像マッチングを使用することによって、捕捉された画像の後続フレームが、オブジェクトとカメラとの間の異なる距離を抽出するために比較され得る。例えば、カメラ位置から遠い距離に位置する画像オブジェクトは、カメラ位置のより近くに位置する画像オブジェクトより長くフレーム内に留まるであろう。 Depth information can be collected in a variety of ways, including but not limited to the use of laser rangefinders and image matching. In some embodiments, a camera arrangement that employs two or more cameras that are slightly spaced but capture the same scene may be used. According to one embodiment, image matching is used to analyze slight differences between images captured by each camera to determine the distance at each point in the image. In another embodiment, the distance information is obtained by capturing a landscape image as the vehicle travels forward using a single video camera mounted on the vehicle and traveling at a specific speed. Can be compiled. Laser rangefinders can also be used in combination with cameras that take images. By using image matching, subsequent frames of the captured image can be compared to extract different distances between the object and the camera. For example, an image object located at a distance far from the camera position will remain in the frame longer than an image object located closer to the camera position.
いくつかの形式が、他より有利であり得るが、実施形態は、深度情報を記憶する任意の特定の形式に限定されない。ある実施形態では、深度情報は、離散値のグリッドを備える深度地図としてサーバから送信され、グリッドの各要素は、２次元画像の画素に対応する。各画素における深度地図の値は、第１の位置から画像オブジェクトまでの距離を表し得る。例えば、各画素における深度地図の値は、画像を捕捉するために使用されるカメラのカメラ位置からの距離と、画像内に表される画像オブジェクトとを表し得る。種々のファイル形式のうちの任意の１つが、そのような深度地図のために使用され得ることは、本説明を前提として、当業者に明白となるであろう。例えば、深度地図は、拡張マークアップ言語（ＸＭＬ）ファイルとして記憶され得る。ある実施形態では、パノラマ画像に関連付けられた深度地図および他の情報は、画像自体から独立して記憶されることができる。 Some forms may be more advantageous than others, but embodiments are not limited to any particular form of storing depth information. In one embodiment, the depth information is transmitted from the server as a depth map comprising a discrete value grid, where each element of the grid corresponds to a pixel of the two-dimensional image. The value of the depth map at each pixel may represent the distance from the first position to the image object. For example, the depth map value at each pixel may represent the distance from the camera position of the camera used to capture the image and the image object represented in the image. It will be apparent to those skilled in the art given this description that any one of a variety of file formats can be used for such depth maps. For example, the depth map can be stored as an Extensible Markup Language (XML) file. In some embodiments, the depth map and other information associated with the panoramic image can be stored independently of the image itself.
ある実施形態によると、一式のパノラマ画像が、バーチャル３次元環境に関連付けられた現在の視野に基づいて、サーバ１４０から読み出されることができる。さらに、各パノラマ画像に関連付けられた深度値を使用して、各パノラマのための３次元幾何学形状を生成することができる。生成された３次元幾何学形状は、例えば、限定ではないが、前述のように、クライアント１０２内に実装される、地理空間ブラウザの表示エリア内の３次元環境を伴う写真テクスチャによって、レンダラモジュール１２２によって生成およびレンダリングされることができる、隙間の無いポリゴンの３次元メッシュ（例えば、三角形）であり得る。 According to an embodiment, a set of panoramic images can be retrieved from the server 140 based on the current field of view associated with the virtual 3D environment. In addition, the depth value associated with each panoramic image can be used to generate a three-dimensional geometric shape for each panorama. The generated 3D geometry is, for example, but not limited to, the renderer module 122, as described above, with a photo texture with a 3D environment in the display area of the geospatial browser implemented in the client 102. Can be a three-dimensional mesh of polygons (eg, triangles) with no gaps that can be generated and rendered by.
（パノラマグラフ／接続情報）
ある実施形態では、各パノラマ画像のメタデータは、１つ以上の近隣パノラマへの空間リンク、接続、または進路を指定する、接続情報を含む。ある実施形態では、ユーザ相互作用モジュール１１０（または、その構成要素の任意の組み合わせまたは部分的組み合わせ）は、各パノラマ画像のメタデータ内に含まれる接続情報に基いて、空間的にリンクされたパノラマの指向性グラフ（または、単に「パノラマグラフ」または「接続グラフ」）を構築する。そのようなパノラマグラフは、例えば、クライアント１０２における、ローカルメモリ（例えば、キャッシュメモリ）内に記憶され得る。ある実施形態では、パノラマグラフを使用して、３次元環境内でレンダリングされた異なるパノラマ間のリンクに沿って、ＧＩＳのユーザを誘導することができる。ある実施例では、そのようなリンクは、ＧＩＳのバーチャル３次元環境内の街路網に対応し、パノラマ画像は、網目の街路に沿って、一連の異なる街路レベルのビューを表す。この点において、パノラマグラフは、近隣パノラマ間の街路または道路網（または、「道路網」）を表すことができる。
(Panorama graph / connection information)
In some embodiments, the metadata for each panoramic image includes connection information that specifies a spatial link, connection, or course to one or more neighboring panoramas. In some embodiments, the user interaction module 110 (or any combination or sub-combination of its components) can be used to spatially link panoramas based on connection information contained within the metadata of each panorama image. A directivity graph (or simply a “panoramic graph” or “connection graph”). Such a panoramic graph can be stored, for example, in a local memory (eg, a cache memory) at the client 102. In some embodiments, panoramic graphs can be used to guide GIS users along links between different panoramas rendered in a three-dimensional environment. In one embodiment, such a link corresponds to a street network in a GIS virtual three-dimensional environment, and the panoramic image represents a series of different street level views along the street of the mesh. In this regard, the panoramic graph can represent a street or road network (or “road network”) between neighboring panoramas.
図１に図示されないが、クライアント１０２は、ある実施形態による、ナビゲーション制御を提供し、ユーザが、３次元環境内のそのような道路網またはパノラマグラフに沿って、ナビゲートすることを可能にするために使用され得る、グラフィカルユーザインターフェース（ＧＵＩ）を含むことができる。ある実施形態では、ＧＩＳは、ユーザが、パノラマ画像に関連付けられた種々の場所とビューポイントとの間をナビゲートすることを可能にする、制御を提供することができる。例えば、ナビゲーション制御は、パノラマ画像のテクスチャ化された３次元モデルによって、３次元オーバーレイとしてレンダリングされ得る。そのようなナビゲーション制御は、ユーザが、３次元環境内でバーチャルカメラのビュー位置およびビュー方向を変更し、異なるパノラマ画像に関連付けられた場所間をナビゲートすることを可能することができる。 Although not illustrated in FIG. 1, the client 102 provides navigation control and allows a user to navigate along such a road network or panoramic graph in a three-dimensional environment, according to an embodiment. A graphical user interface (GUI) can be included that can be used. In some embodiments, the GIS can provide controls that allow a user to navigate between various locations associated with a panoramic image and a viewpoint. For example, the navigation control can be rendered as a three-dimensional overlay with a textured three-dimensional model of the panoramic image. Such navigation control can allow a user to change the view position and view direction of a virtual camera within a three-dimensional environment and navigate between locations associated with different panoramic images.
ある実施形態では、各パノラマに関連付けられた接続情報（すなわち、メタデータ）、深度情報、および写真テクスチャは、ネットワーク１３０を経由して、サーバ１４０からクライアント１０２にストリームされることができる。例えば、写真テクスチャは、３次元環境内のバーチャルカメラのビューポイントに基づいて、異なる分解能で、画像タイルの形態においてストリームされ得る。さらに、パノラマおよび任意の関連付けられた情報は、バーチャルカメラのビューポイントに基づいて、ストリームされ得る。 In some embodiments, connection information (ie, metadata), depth information, and photo textures associated with each panorama can be streamed from server 140 to client 102 via network 130. For example, photographic textures can be streamed in the form of image tiles at different resolutions based on the virtual camera viewpoint in a three-dimensional environment. Further, the panorama and any associated information may be streamed based on the virtual camera viewpoint.
（ナビゲーション経路制約）
街路レベルの景色のパノラマは、単一点のビュー（ｓｉｎｇｌｅ ｐｏｉｎｔ−ｏｆ−ｖｉｅｗ）のみからの景色の正確な表現を提示し得る。例えば、最良の視覚的品質、すなわち、パノラマ内で表される景色の最も正確な視覚的描写を提示する、ビューポイントは、パノラマが撮影された正確な位置に対応し、すなわち、パノラマ画像を捕捉するために使用される画像捕捉デバイスの原位置に対応する、３次元環境内のバーチャルカメラの場所に関連付けられ得る。例えば、この位置は、パノラマの中心に対応し得る。さらに、３次元環境内のレンダリングされたパノラマの視覚的品質は、バーチャルカメラが、パノラマの中心から離れて移動するにつれて、劣化し得る。
(Navigation route restriction)
A street level landscape panorama may present an accurate representation of the landscape from a single point-of-view only. For example, the viewpoint that presents the best visual quality, ie the most accurate visual depiction of the scene represented in the panorama, corresponds to the exact location where the panorama was taken, ie captures the panoramic image Can be associated with the location of the virtual camera in the three-dimensional environment corresponding to the original location of the image capture device used to do. For example, this position may correspond to the center of the panorama. Furthermore, the visual quality of the rendered panorama in the three-dimensional environment can degrade as the virtual camera moves away from the center of the panorama.
しかしながら、（例えば、クライアント１０２に連結されたディスプレイデバイス上にレンダリングされた）バーチャルカメラの眺めからパノラマを見ているユーザが、依然として、レンダリングの３次元品質を経験することができるように、ある程度の偏差が、所望される。ユーザはまた、ナビゲーション制御を使用して、３次元空間の異なる部分を探索可能であるべきである。したがって、実施形態は、バーチャルカメラが、３次元環境内でレンダリングされる場合、不良画像品質を有するパノラマのビューポイントへと移動しないよう防止するように構成され得る。さらに、以下にさらに詳細に説明されるように、実施形態は、バーチャルカメラ（および、ユーザ）が、良好な画像品質のエリアをたどることを可能にする。 However, to some extent so that a user viewing a panorama from a virtual camera view (eg, rendered on a display device coupled to client 102) can still experience the three-dimensional quality of the rendering. Deviation is desired. The user should also be able to explore different parts of the three-dimensional space using navigation controls. Thus, embodiments may be configured to prevent a virtual camera from moving to a panoramic viewpoint with poor image quality when rendered in a 3D environment. Furthermore, as described in more detail below, embodiments allow a virtual camera (and user) to follow areas of good image quality.
ある実施形態では、パノラマのための「既知の良好な」、十分な、または容認可能な視覚的品質のエリアまたは領域は、３次元環境内のパノラマの視覚的表現に関連付けられた１つ以上の基準に基づく。ある実施例では、十分な視覚的品質の領域は、表示分解能が、所定の閾値を超える、パノラマの領域に対応する。別の実施例では、そのような領域は、パノラマ画像によって表される景色内のオブジェクトの歪みを回避するように、表示分解能が、十分な品質である、パノラマ内の点に基づき得る。さらに別の実施例では、パノラマのそのような領域は、３次元環境内のパノラマのためにレンダリングされた正面平面の最も近傍の決定された場所に対応し得る。 In some embodiments, a “known good”, sufficient, or acceptable visual quality area or region for a panorama is one or more associated with a visual representation of the panorama in a three-dimensional environment. Based on criteria. In one embodiment, a region of sufficient visual quality corresponds to a panoramic region whose display resolution exceeds a predetermined threshold. In another example, such regions may be based on points in the panorama where the display resolution is of sufficient quality to avoid distortion of objects in the scene represented by the panoramic image. In yet another example, such a region of the panorama may correspond to a determined location nearest the front plane rendered for the panorama in the three-dimensional environment.
前述のように、接続またはパノラマグラフは、３次元環境内の場所に対応するパノラマに関連付けられた接続情報を使用して、ユーザ相互作用モジュール１１０によって生成されることができる。以下にさらに詳細に説明されるように、進路プランナモジュール１１６は、１つ以上のパノラマに関連付けられた接続情報を使用して、パノラマ間のナビゲーション経路を構築する。そのようなナビゲーション経路は、生成されたパノラマグラフの最も近傍のパノラマの周囲のある一定の体積の空間にバーチャルカメラを制約するために使用される。 As described above, a connection or panorama graph can be generated by the user interaction module 110 using connection information associated with a panorama corresponding to a location in a three-dimensional environment. As will be described in more detail below, the course planner module 116 uses the connection information associated with one or more panoramas to build a navigation path between the panoramas. Such a navigation path is used to constrain the virtual camera to a certain volume of space around the nearest panorama of the generated panoramic graph.
ある実施形態による、パノラマグラフ内のパノラマ毎に、進路プランナモジュール１１６は、パノラマの表現が３次元環境内で見られ得る最も視覚的に正確なビューポイントに対応するバーチャルカメラの位置に対して、ナビゲーション半径またはローミング距離を計算することができる。例えば、そのような位置は、前述のように、パノラマ（例えば、パノラマの中心）を捕捉するために使用される、画像捕捉デバイスの位置に対応し得る。 For each panorama in the panoramic graph, according to an embodiment, the course planner module 116 is for the position of the virtual camera corresponding to the most visually accurate viewpoint that the representation of the panorama can be seen in the 3D environment. Navigation radius or roaming distance can be calculated. For example, such a position may correspond to the position of the image capture device used to capture the panorama (eg, the center of the panorama), as described above.
ナビゲーション半径またはローミング距離は、定数または関数であり得、限定されないが、パノラマ間の距離および深度メッシュの品質を含むいくつかの要因に依存し得る。例えば、進路プランナモジュール１１６は、３次元環境内のパノラマ中心から、レンダリングされる最も近傍平面までの距離を評価することによって、パノラマのためのナビゲーション半径を計算する。他の考慮点も、ナビゲーション半径を限定するために使用されることができることは、本説明を前提として、当業者に明白となるであろう。 The navigation radius or roaming distance may be a constant or function and may depend on several factors including, but not limited to, the distance between panoramas and the quality of the depth mesh. For example, the course planner module 116 calculates the navigation radius for the panorama by evaluating the distance from the panorama center in the 3D environment to the nearest plane to be rendered. It will be apparent to those skilled in the art given the present description that other considerations can also be used to limit the navigation radius.
ある実施例では、レンダラモジュール１２２が、３次元環境内のパノラマのテクスチャ化された３次元モデルをレンダリングするにつれて、広範囲の深度情報が、利用可能でないエリアを充填するために、テクスチャをストレッチさせ得る。この実施例では、ナビゲーション半径を限定し得る考慮点は、遠隔場所から捉えられるであろう、テクスチャストレッチングの容認可能な量に対応する、所定の閾値であり得る。進路移動モジュール１１８は、次いで、図２に関して以下にさらに詳細に説明されるように、バーチャルカメラの初期場所に関連付けられたパノラマのナビゲーション半径および標的場所に関連付けられたパノラマのナビゲーション半径を使用して、ナビゲーションまたは衝突経路を生成することができる。 In one embodiment, as renderer module 122 renders a panoramic textured 3D model in a 3D environment, extensive depth information may stretch the texture to fill the unavailable area. . In this example, a consideration that may limit the navigation radius may be a predetermined threshold that corresponds to an acceptable amount of texture stretching that would be captured from a remote location. The path travel module 118 then uses the panorama navigation radius associated with the initial location of the virtual camera and the panorama navigation radius associated with the target location, as described in more detail below with respect to FIG. Navigation or collision paths can be generated.
図２は、ある実施形態による、３次元環境における、誘導付きナビゲーションのための深度メッシュ２０２内の例示的ナビゲーション経路２００の略図である。図２に示される例示的ナビゲーション経路では、単一接続は、進路２１０に沿って、２つのパノラマをリンクする。進路２１０は、例えば、３次元環境内の街路を表し得る。図２に示されるように、進路２１０は、パノラマ中心２０４（例えば、第１のパノラマの中心）に対応する初期位置から、パノラマ中心２０６（例えば、第２のパノラマの中心）に対応する標的位置まで延在する。 FIG. 2 is a schematic diagram of an exemplary navigation path 200 within a depth mesh 202 for guided navigation in a three-dimensional environment, according to an embodiment. In the exemplary navigation path shown in FIG. 2, a single connection links two panoramas along a course 210. The course 210 may represent a street in a three-dimensional environment, for example. As shown in FIG. 2, the path 210 starts from an initial position corresponding to the panorama center 204 (eg, the center of the first panorama) to a target position corresponding to the panorama center 206 (eg, the center of the second panorama). Extend to.
図１に戻って参照すると、進路移動モジュール１１８を使用して、進路２１０に対して、深度メッシュ２０２内にシェル２２０を作成し得る。ある実施形態では、進路プランナモジュール１１６は、３次元環境内の街路の表現に沿って、進路２１０を決定する。図２の例示的ナビゲーション経路２００に戻って参照すると、図１の進路プランナモジュール１１６を使用して、３次元環境内のパノラマ中心２０４とパノラマ中心２０６との間の街路の表現に沿って、進路２１０を決定し得る。 Referring back to FIG. 1, the path movement module 118 may be used to create a shell 220 in the depth mesh 202 for the path 210. In some embodiments, the course planner module 116 determines a course 210 along with a street representation in a three-dimensional environment. Referring back to the exemplary navigation path 200 of FIG. 2, the course planner module 116 of FIG. 1 is used to follow the path along the street representation between the panorama center 204 and the panorama center 206 in the three-dimensional environment. 210 may be determined.
図２に図示されるナビゲーション経路のシェル２２０は、３次元環境内のパノラマ画像の一部をレンダリングするために使用される制約表面を提供する。さらに、ナビゲーション経路の制約表面は、それを越えて、３次元環境内のナビゲーション（例えば、バーチャルカメラの移動）が、許容されない境界を規定する。故に、この表面は、経験の視覚的品質を維持するために、ナビゲーションのためのはっきりとした境界として使用され得る。ユーザが、３次元環境内をナビゲートし、能動的にレンダリングされたパノラマが変化するにつれて、制約表面は、バーチャルカメラのすぐ近傍を考慮するために、進路移動モジュール１１８によって更新されることができる。 The navigation path shell 220 illustrated in FIG. 2 provides a constraining surface that is used to render a portion of a panoramic image within a three-dimensional environment. Furthermore, the constraining surface of the navigation path goes beyond that to define boundaries where navigation within a three-dimensional environment (eg, movement of a virtual camera) is not allowed. Therefore, this surface can be used as a clear boundary for navigation to maintain the visual quality of the experience. As the user navigates through the three-dimensional environment and the actively rendered panorama changes, the constrained surface can be updated by the path movement module 118 to take into account the immediate vicinity of the virtual camera. .
ある実施形態では、深度メッシュ２０２は、前述のように、２つのパノラマに関連付けられた深度地図に基づいて、レンダラモジュール１２２によって生成されるポリゴンの３次元の隙間の無いメッシュであり得る。ある実施形態では、進路移動モジュール１１８は、前述のように、各パノラマの計算されたナビゲーション半径に基づいて、進路２１０に沿う各パノラマにおけるナビゲーション球体を補間することによって、シェルを構築する。視覚的実施例のために、シェル２２０が、３次元空間内で描写され、上方から見られる場合、２つの球体によって境界された円錐形断面として現れるであろう。また、前述のように、各ナビゲーション球体は、３次元環境内の各パノラマのための「既知の良好な」または容認可能なビューポイントの領域を表す。 In some embodiments, the depth mesh 202 may be a three-dimensional gapless mesh of polygons generated by the renderer module 122 based on depth maps associated with the two panoramas, as described above. In some embodiments, the path movement module 118 constructs a shell by interpolating the navigation spheres in each panorama along the path 210 based on the calculated navigation radius of each panorama, as described above. For the visual example, when the shell 220 is depicted in three-dimensional space and viewed from above, it will appear as a conical section bounded by two spheres. Also, as described above, each navigation sphere represents a region of “known good” or acceptable viewpoint for each panorama in the three-dimensional environment.
ある実施形態では、各パノラマは、地理的に位置指定される。したがって、バーチャル３次元環境内のパノラマの場所は、地理的場所に対応することができる。加えて、３次元環境内の異なる場所間のバーチャルカメラの移動（例えば、ユーザ入力に基づく）は、ＧＩＳ内の地理的場所間の動きに対応することができる。 In some embodiments, each panorama is geographically located. Accordingly, the panoramic location within the virtual three-dimensional environment can correspond to a geographical location. In addition, movement of the virtual camera between different locations within the three-dimensional environment (eg, based on user input) can correspond to movement between geographic locations within the GIS.
図２は、例証目的のためだけに提示され、実施形態は、それに限定されないことに留意されたい。当業者は、本説明を前提として、実施形態が、パノラマグラフ内で一緒にリンクされた複数のパノラマのために使用され得ることを理解するであろう。ある実施例では、複数のパノラマは、３次元環境内の交差点の表現に関連付けられ得る。そのような複数のパノラマのナビゲーション経路は、予測不可能な方法で交差し得る。したがって、進路移動モジュール１１８は、例えば、現在レンダリングされているパノラマ（または、「アクティブパノラマ」）からのパノラマまたは接続グラフの奥深くにある所定の数のリンクである、全可能なナビゲーション経路を使用して、一連の衝突試験を行い得る。例えば、進路移動モジュール１１８は、アクティブパノラマから、最大２つのパノラマのためのナビゲーション経路に関連付けられた境界幾何学形状を構築し得る。アクティブパノラマに接続された任意の数のパノラマのためのナビゲーション経路が、必要に応じて、構築され得ることは、本説明を前提として、当業者に明白となるであろう。アクティブパノラマはまた、３次元環境内のバーチャルカメラの現在の場所に対応し得ることに留意されたい。 Note that FIG. 2 is presented for illustrative purposes only, and embodiments are not limited thereto. Those skilled in the art will appreciate that, given this description, embodiments may be used for multiple panoramas linked together in a panoramic graph. In some embodiments, multiple panoramas can be associated with a representation of an intersection in a three-dimensional environment. Such panoramic navigation paths may intersect in an unpredictable way. Thus, the path navigation module 118 uses all possible navigation paths, eg, a predetermined number of links deep in the panorama or connection graph from the currently rendered panorama (or “active panorama”). A series of crash tests can be performed. For example, the path travel module 118 may construct a boundary geometry associated with the navigation path for up to two panoramas from the active panorama. It will be apparent to those skilled in the art, given the present description, that navigation paths for any number of panoramas connected to an active panorama can be constructed as needed. Note that the active panorama may also correspond to the current location of the virtual camera in the 3D environment.
図６Ａ−Ｂは、それぞれ、ある実施形態による、複数のナビゲーション経路６０１、６０２、および６０３の交差点を図示する略図６００Ａおよび６００Ｂである。図６Ａに示される実施例では、複数の分岐を有するシェルが、ナビゲーション経路６０１−６０３に基づいて、生成され得る。略図６００Ａに示されるように、各ナビゲーション経路は、パノラマ中心６２０に対応するパノラマに接続される、３つの街路の各々を表す。また、示されるように、ナビゲーション経路６０１−６０３（シェルの分岐として）は、バーチャルカメラが、あるナビゲーションまたは衝突経路から別のものに移動する場合、「無衝突区域」として作用する、ナビゲーションフィレット６１４、６１５、および６１６を含むことができる。さらに、ナビゲーションフィレット６１４−６１６は、フィレット付きエッジ（例えば、丸い角）を有する。そのようなフィレット付きエッジは、交差点に関連付けられた鋭利な角を回避するのに役立ち、それによって、ナビゲーションの間、スムーズなユーザ経験を提供することができる。実施形態は、これらの丸い角を迅速に生成するための種々の新規な式／アルゴリズムを利用し得ることに留意されたい。 6A-B are diagrams 600A and 600B illustrating the intersection of multiple navigation paths 601, 602, and 603, respectively, according to an embodiment. In the example shown in FIG. 6A, a shell having multiple branches may be generated based on navigation paths 601-603. As shown in diagram 600A, each navigation path represents each of three streets connected to the panorama corresponding to panorama center 620. Also, as shown, the navigation path 601-603 (as a branch of the shell) is a navigation fillet 614 that acts as a “no collision area” when the virtual camera moves from one navigation or collision path to another. , 615, and 616. Further, the navigation fillets 614-616 have filleted edges (eg, rounded corners). Such filleted edges can help avoid sharp corners associated with intersections, thereby providing a smooth user experience during navigation. Note that embodiments may utilize a variety of novel equations / algorithms to quickly generate these rounded corners.
ある実施形態では、ナビゲーションフィレット６１４−６１６は、２つの異なるナビゲーション経路（例えば、ナビゲーション経路６１１および６１２）間において、衝突球体６３０を接線方向に適合することによって構築される。例えば、ナビゲーションフィレット６１４は、パノラマ中心６２０に対する接触点の距離に従って、パラメータ化され得る。略図６００Ａに示されるように、ナビゲーションフィレット６１４のフィレット幅（「ｆｗ」）は、例えば、ナビゲーション経路６０１および６０２に対応するナビゲーション幅（「ｎｗ」）を考慮することによって、算出され得る。 In certain embodiments, the navigation fillets 614-616 are constructed by tangentially matching the collision sphere 630 between two different navigation paths (eg, navigation paths 611 and 612). For example, the navigation fillet 614 can be parameterized according to the distance of the touch point relative to the panorama center 620. As shown in diagram 600A, the fillet width (“fw”) of navigation fillet 614 may be calculated, for example, by considering the navigation width (“nw”) corresponding to navigation paths 601 and 602.
いったんフィレットが、略図６００Ａに示されるように、鋭利な交差点の周囲に構築されると、進路移動モジュール１１８は、ある実施形態による、図６Ｂの略図６００Ｂに示されるように、ナビゲーション経路およびフィレットによって作成された制約表面の結合に対する衝突を試験することができる。例えば、略図６００Ｂにおけるナビゲーション経路によって提供される制約表面のそのような結合は、前述のように、バーチャルカメラの移動が制約され得る、十分なまたは容認可能な視覚的品質の領域を表し得る。 Once the fillet is built around a sharp intersection, as shown in diagram 600A, the path movement module 118 may be used by the navigation path and fillet, as shown in diagram 600B of FIG. 6B, according to an embodiment. Collisions against the constrained surface created can be tested. For example, such a combination of constraining surfaces provided by the navigation path in diagram 600B may represent a region of sufficient or acceptable visual quality where movement of the virtual camera may be constrained as described above.
ある実施形態では、進路プランナモジュール１１６は、３次元環境内の街路網の表現における交差点において、方向転換するための進路を決定することができる。例えば、曲がり角において、進路プランナモジュール１１６は、進路内の曲がり角にスプラインを補間し、バーチャルカメラの移動を平滑化し得る。 In some embodiments, the route planner module 116 can determine a route to turn at an intersection in a representation of a street network in a three-dimensional environment. For example, at a turn, the course planner module 116 may interpolate a spline at a turn in the course and smooth the movement of the virtual camera.
（ナビゲーション経路制約を使用した誘導付きナビゲーション）
前述のように、実施形態は、バーチャルカメラ（および、ユーザ）が、パノラマの３次元表現が、３次元環境内でレンダリングされる場合、良好な画像品質のエリアをたどることを可能にする。また、前述のように、パノラマは、１つ以上の街路に沿って、種々の景色の地上レベルのビューを提供する。したがって、バーチャルカメラの移動は、３次元環境内の街路の表現をたどる。ある実施形態では、進路移動モジュール１１８はさらに、ユーザ入力、例えば、キーボードおよび／またはマウスイベント（例えば、上方矢印、右矢印、ダブルクリック等）に基づいて、そのようなナビゲーションを促進するように構成される。その結果、進路移動モジュール１１８は、街路の表現に沿って、進路上でバーチャルカメラを誘導する一方、ユーザが、ナビゲーション制御および／または追加のユーザ入力を使用することによって、進路を保とうとするいかなる過度の負担も軽減させる。
(Navigation with guidance using navigation route constraints)
As described above, embodiments allow a virtual camera (and user) to follow areas of good image quality when a 3D representation of a panorama is rendered in a 3D environment. Also, as mentioned above, panoramas provide ground level views of various landscapes along one or more streets. Thus, the movement of the virtual camera follows the street representation in the three-dimensional environment. In certain embodiments, the path travel module 118 is further configured to facilitate such navigation based on user input, eg, keyboard and / or mouse events (eg, up arrow, right arrow, double click, etc.). Is done. As a result, the path movement module 118 guides the virtual camera on the path along the street representation, while the user uses any navigation control and / or additional user input to keep track of the path. Reduce excessive burden.
例えば、３次元環境内のナビゲーションに関与する従来のアプリケーションでは、キーボードの上方矢印キーの押下は、概して、その進行方向に沿って、バーチャルカメラを前方に移動させる。しかしながら、本明細書に説明されるように、３次元環境内におけるパノラマのナビゲーションに適用されるようなこの挙動は、バーチャルカメラが針路から外れる結果をもたらし得る。その結果、ユーザは、街路を進むように、繰り返し、異なるキーを使用する必要があり得る。これらの懸念を解決するために、実施形態は、以下にさらに詳細に説明されるように、いくつかの異なる技法を適用する。 For example, in conventional applications involving navigation within a three-dimensional environment, pressing the up arrow key on the keyboard generally moves the virtual camera forward along its direction of travel. However, as described herein, this behavior, as applied to panoramic navigation within a three-dimensional environment, can result in the virtual camera moving off the course. As a result, the user may need to repeatedly use different keys to navigate the street. In order to resolve these concerns, the embodiments apply a number of different techniques, as described in more detail below.
自由形式のナビゲーションのユーザ経験を維持する一方、より良好な視覚的品質のエリアに向かって、ユーザを誘導するのに役立てるために、ユーザ相互作用モジュール１１０は、あるユーザ入力（例えば、キーボードの矢印キー）に関連付けられた意味を変更させるように動作可能である。これは、以下により詳細に説明されるように、例えば、「道路をたどる」ジェスチャが検出されると、道路に沿って、ユーザを緻密に誘導することを可能にする。そのような「道路をたどる」ジェスチャが起動されると、移動モジュール１１４および／または進路移動モジュール１１８は、バーチャルカメラの移動に補正を適用するように構成することができる。 To help guide the user towards an area of better visual quality while maintaining the user experience of free-form navigation, the user interaction module 110 may provide certain user input (eg, keyboard arrows) It is operable to change the meaning associated with the key. This allows for a precise guidance of the user along the road, for example when a “follow the road” gesture is detected, as will be explained in more detail below. When such a “follow the road” gesture is activated, the movement module 114 and / or the path movement module 118 may be configured to apply a correction to the movement of the virtual camera.
ある実施形態では、進路に沿って、移動されるにつれて、ビュー方向補正およびビュー位置補正が、バーチャルカメラの移動に適用される。ビュー位置補正は、パノラマ間のリンクの中心に向かって、バーチャルカメラを移動させる。ビュー方向補正は、若干、より進路の中心に向かって、バーチャルカメラのビューを整列させる。さらに、いったんジェスチャが、所定の時間の間、起動されると、バーチャルカメラのビューは、進路の中心に戻されることができる。そのようなビュー補正の適用は、バーチャルカメラが、湾曲道路網をたどることを可能にする。 In some embodiments, view direction correction and view position correction are applied to the movement of the virtual camera as it is moved along the path. The view position correction moves the virtual camera toward the center of the link between the panoramas. The view direction correction slightly aligns the virtual camera view towards the center of the path. Furthermore, once the gesture is activated for a predetermined time, the virtual camera view can be returned to the center of the path. Such view correction application allows the virtual camera to follow a curved road network.
そのような「道路をたどる」ジェスチャが、いくつかの方法において、ユーザによって起動され得ることは、本説明を前提として、当業者に明白となるであろう。一実施例では、ユーザは、バーチャルカメラのビュー方向が、道路方向に沿った進路にほぼ対応する間、左または右矢印キーに触れることなく、一定時間の間、例えば、上方矢印キーを押下し続け得る。言い換えると、バーチャルカメラのビュー方向が、ある角度公差（例えば、４５度）内において、３次元環境内に表される街路に沿っており、ユーザが、上方矢印キーを押下する場合、ビューは、最初は、ユーザを前方に移動させるであろう。しかしながら、一定時間（例えば、１．５秒）後、「道路をたどる」ジェスチャが、起動され得、進路移動モジュール１１８は、街路に沿って移動しようとし得る、ユーザの意図を決定することができる。 It will be apparent to those skilled in the art, given this description, that such “follow the road” gesture can be activated by the user in several ways. In one embodiment, the user presses the up arrow key for a period of time, for example, without touching the left or right arrow key while the virtual camera view direction corresponds approximately to a path along the road direction. You can continue. In other words, if the viewing direction of the virtual camera is along a street represented in a three-dimensional environment within an angular tolerance (eg 45 degrees) and the user presses the up arrow key, the view is Initially, the user will be moved forward. However, after a certain amount of time (eg, 1.5 seconds), a “follow the road” gesture may be activated and the path movement module 118 may determine a user's intention that may attempt to move along the street. .
ある実施形態では、進路移動モジュール１１８によって適用されるビュー方向および／またはビュー位置補正の大きさは、ユーザ入力に基づいて、スムーズにパラメータ化されることができる。例えば、時間に基づく所定の閾値が、そのようなパラメータ化のために使用され得る。この実施例では、「道路をたどる」ジェスチャが検出されると、任意の補正の大きさが、数秒にわたって、いくつかの所定の最大補正量までスムーズに漸増され得る。さらに、相反するユーザ入力またはカメラ位置付けは、任意の補正量のスムーズな漸減を生じさせ得る。 In certain embodiments, the magnitude of the view direction and / or view position correction applied by the path movement module 118 can be smoothly parameterized based on user input. For example, a predetermined threshold based on time may be used for such parameterization. In this example, when a “follow the road” gesture is detected, the magnitude of any correction can be smoothly ramped up to some predetermined maximum correction amount over several seconds. Furthermore, conflicting user input or camera positioning can cause a smooth gradual reduction of any correction amount.
ジェスチャの起動に応じて、ある実施形態によると、進路移動モジュール１１８は、道路に向かって、バーチャルカメラのビューを徐々に整列させ、道路の中心に向かって、ユーザを移動させる。ユーザの眺めから、この経験は、車の運転との類似性を感じ得る。ある実施形態では、進路移動モジュール１１８は、自動的に、３次元環境内に表される道路網内のカーブをたどるであろう。ユーザは、例えば、左−右矢印キーを叩打することによって、バーチャルカメラの任意の自動方向転換を阻止し得る。この場合、進路移動モジュール１１８は、自動ナビゲーションを中断し、ビューの補正を開始する前に、ある所定の時間の間、待機することができる。実施形態は、任意の数の方法を使用して、道路をたどろうとするユーザの意図を推定または予測し得ること、従って、そのような「道路をたどる」ジェスチャが起動されるべきか否かを推定または予測し得ることは、本説明を前提として、当業者に明白となるであろう。 In response to activation of the gesture, according to one embodiment, the path movement module 118 gradually aligns the virtual camera views toward the road and moves the user toward the center of the road. From the user's view, this experience can feel similar to driving a car. In some embodiments, the path travel module 118 will automatically follow a curve in the road network represented in the three-dimensional environment. The user may prevent any automatic turning of the virtual camera, for example by hitting the left-right arrow keys. In this case, the course moving module 118 can interrupt the automatic navigation and wait for a certain predetermined time before starting to correct the view. Embodiments can use any number of methods to estimate or predict a user's intention to follow a road, and thus whether such a "follow a road" gesture should be invoked. It will be apparent to those skilled in the art given this description.
前述のように、進路移動モジュール１１８は、バーチャルカメラのビュー位置および角度に基づいて、角を方向転換しようとするユーザの意図の決定を試みる。ある実施形態では、進路プランナモジュール１１６は、道路の中心において、新しい街路方向に向かう、新しい街路への方向転換を実行するであろう特定の進路を決定する。ある実施形態では、進路プランナモジュール１１６は、前述のように、適切な道路を選択し、新しい対応する進路を動的に構築する一方、また、ユーザが、道路をたどることを中断することを所望するかどうかを検出しようとする。 As described above, the path movement module 118 attempts to determine a user's intention to turn a corner based on the view position and angle of the virtual camera. In some embodiments, the route planner module 116 determines a particular route that will perform a turn to a new street, toward the new street direction, at the center of the road. In some embodiments, the course planner module 116 selects an appropriate road and dynamically builds a new corresponding course, as described above, while also desiring that the user cease to follow the road. Try to detect if you want.
さらに、いったん検出されたユーザ入力（例えば、キーの長押し）が、「道路をたどる」ジェスチャであると決定されると、進路プランナモジュール１１６はまた、バーチャルカメラの以前の移動方向に沿って、パノラマ中心（または、前述のように、視覚的品質向上領域）があるかどうかを決定し得る。そのような以前の移動方向は、例えば、パノラマ中心（または、領域）の位置および／またはその位置への到着時間に基づいて、ある公差内にあり得る。中心が見つけられた場合、バーチャルカメラは、短時間の間、もともとの方向に近づくように移動し続けるように、短時間オートパイロットが、開始されることができる。これによって、ビューが、より良好な視覚的品質の領域に到着することが可能となる。ユーザにとって、これは、本説明を前提として、当業者に明白となるであろうように、いくつかの他の公知のナビゲーションシステムにおいて見出され得る、自然運動量減衰または「投てき」ジェスチャのようなものであり得る。しかしながら、これらの他のシステムと異なり、本明細書に説明されるような実施形態に対する目標の１つは、前述のように、視覚的忠実性または品質が向上した領域に向かって、ユーザを緻密に誘導することである。 Further, once the detected user input (eg, a long key press) is determined to be a “follow the road” gesture, the course planner module 116 also follows the previous movement direction of the virtual camera, It may be determined whether there is a panorama center (or a visual quality enhancement region as described above). Such previous direction of movement may be within a certain tolerance, for example based on the location of the panorama center (or region) and / or the arrival time at that location. If the center is found, a short time autopilot can be initiated so that the virtual camera continues to move closer to the original direction for a short time. This allows the view to arrive in a region with better visual quality. For the user, given this description, this is like a natural momentum attenuation or “throw” gesture, as may be found in some other known navigation systems, as would be apparent to one skilled in the art. Can be a thing. However, unlike these other systems, one of the goals for embodiments as described herein is to elaborate the user towards areas of increased visual fidelity or quality, as described above. Is to induce.
（誘導付きナビゲーションの使用例）
図３Ａ−Ｅは、街路中心に向かって、および／またはそれに沿って、バーチャルカメラ（および、ユーザ）を緻密に誘導することに関連付けられる、種々の可能な使用例を図示する略図３００Ａ−Ｅである。図３Ａ−Ｅに示される略図の各々は、交差点に関連付けられたパノラマの道路網を含む。さらに、各略図は、パノラマの各々に対するナビゲーション経路半径３０２を含む。説明の容易性のために、ナビゲーション経路幅は、示されない。略図の各々において、バーチャルカメラのビューおよび進路方向は、バーチャルカメラに関連付けられた矢印の方向にある。ユーザは、例えば、「前方移動」アクションを実行し得る（例えば、前述のように、矢印キーを選択することによって）。
(Example of navigation with guidance)
FIGS. 3A-E are schematic diagrams 300A-E illustrating various possible uses associated with closely navigating a virtual camera (and user) toward and / or along a street center. is there. Each of the diagrams shown in FIGS. 3A-E includes a panoramic road network associated with the intersection. In addition, each schematic includes a navigation path radius 302 for each of the panoramas. For ease of explanation, the navigation path width is not shown. In each of the diagrams, the view and course direction of the virtual camera is in the direction of the arrow associated with the virtual camera. The user may, for example, perform a “move forward” action (eg, by selecting an arrow key as described above).
さらに、当業者は、本説明を前提として、実施形態が、任意の数の新規な技法を採用し、略図３００Ａ−Ｅによって図示されるように、３次元環境内のバーチャルカメラの所望の動きを実装し得ることを理解するであろう。例えば、そのような技法は、各交差点に到達するための種々のカーブを構築し、これらのカーブに関連付けられた相対的緊張（ｔｅｎｓｉｏｎ）を比較することを伴い得る。 Further, given the present description, those skilled in the art will assume that the embodiments employ any number of novel techniques to achieve the desired movement of the virtual camera in the three-dimensional environment, as illustrated by diagrams 300A-E. You will understand that it can be implemented. For example, such techniques may involve constructing various curves to reach each intersection and comparing the relative tensions associated with these curves.
図３Ａは、ある実施形態による、異なる角度において、２つのパノラマ間の街路３１２（図３Ａ−Ｅに示される実施例において交差点の中心線として表される）に接近するバーチャルカメラ３１０およびバーチャルカメラ３２０の例示的進路を図示する略図３００Ａである。バーチャルカメラ３１０の進路は、グレージング角において、パノラマ中心３０１およびパノラマ中心３０３を接続する街路３１２に接近している。例えば、このビュー方向は、ユーザが、街路３１２を横断する位置に移動するのではなく、標的位置へと街路３１２を移動することに興味があることを示し得る。バーチャルカメラ３１０の動きに関して、何らアクションがとられない場合、街路３１２を越えて移動し、望ましくない場所にナビゲートするであろう。したがって、「途中合流コース」において、バーチャルカメラ３１０の進路を街路３１２と整列させるように、補正が行われることができる。ユーザが、実際に、街路３１２を越えて、特定の標的へとナビゲートすることを好む場合、ユーザは、例えば、ユーザのキーボード上の矢印キーを使用して、または以下に説明される、バーチャルカメラ３２０のビュー方向と同様に、街路３１２により垂直となるように、バーチャルカメラ３１０のビューを方向転換することによって、補正を無効にすることができる。 FIG. 3A illustrates a virtual camera 310 and a virtual camera 320 approaching a street 312 between two panoramas (represented as an intersection centerline in the example shown in FIGS. 3A-E) at different angles, according to an embodiment. 3B is a diagram 300A illustrating an exemplary path of The course of the virtual camera 310 is close to the street 312 connecting the panorama center 301 and the panorama center 303 at the glazing angle. For example, this view direction may indicate that the user is interested in moving the street 312 to the target location rather than moving to a location that crosses the street 312. If no action is taken regarding the movement of the virtual camera 310, it will move across the street 312 and navigate to an undesirable location. Accordingly, correction can be performed so that the course of the virtual camera 310 is aligned with the street 312 in the “halfway merge course”. If the user actually prefers to navigate beyond the street 312 to a specific target, the user can use a virtual key, eg, using the arrow keys on the user's keyboard or as described below. Similar to the view direction of the camera 320, the correction can be disabled by turning the view of the virtual camera 310 so that it is perpendicular to the street 312.
バーチャルカメラ３１０と対照的に、略垂直角度において、バーチャルカメラ３２０は、パノラマ中心３０４および３０５によって表される２つのパノラマ間の街路３１２に接近している。この場合、ユーザは、ビューポイントから街路を横断して位置する、標的３２２（例えば、建造物３０６の正面）を見ることに関心があり得る。バーチャルカメラ３２０の進路が、バーチャルカメラ３１０のように、途中合流コース上に置くように補正される場合、結果は、ユーザにとって、望ましいものではなく、食い違うものとなり得る。したがって、バーチャルカメラ３２０のビュー角度は、街路３１２に沿って整列されるように補正されるべきではない。ユーザが、実際には、街路３１２を移動することを好む場合、ユーザは、ユーザ入力を通して、誘導付きナビゲーションを手動で無効にすることによって、バーチャルカメラ３１０と同様に、バーチャルカメラ３２０のビュー方向を整列させることができる。 In contrast to the virtual camera 310, at a substantially vertical angle, the virtual camera 320 is approaching the street 312 between the two panoramas represented by the panorama centers 304 and 305. In this case, the user may be interested in seeing the target 322 (eg, the front of the building 306) located across the street from the viewpoint. If the path of the virtual camera 320 is corrected to be placed on a midway confluence course, like the virtual camera 310, the results are not desirable for the user and can be inconsistent. Accordingly, the viewing angle of the virtual camera 320 should not be corrected to be aligned along the street 312. If the user actually prefers to move on the street 312, the user can change the view direction of the virtual camera 320, similar to the virtual camera 310, by manually disabling guided navigation through user input. Can be aligned.
図３Ｂは、ある実施形態による、異なる角度において、街路３１２から離れるように移動する、バーチャルカメラ３３０およびバーチャルカメラ３４０の例示的進路を図示する略図３００Ｂである。この実施例では、バーチャルカメラ３４０に対する進路は、図３Ａのバーチャルカメラ３２０に類似する（すなわち、標的場所３４２に向かう）。しかしながら、バーチャルカメラ３３０に対する進路は、最初は、比較的に小さい角度（例えば、初期進路３３１によって示されるように）において、街路３１２（中心線）からはずれる。図３Ａのバーチャルカメラ３１０と同様に、バーチャルカメラ３３０の進路は、街路３１２に沿って整列するように補正される。ユーザが、ナビゲーション経路外の建造物に関心がある場合、例えば、適切なキーストロークを入力することによって、または、例えば、ナビゲーション制御を介して、道路とより垂直な角度にビューポイントを方向転換させることによって、補正を無効にすることができる。 FIG. 3B is a diagram 300B illustrating exemplary courses of the virtual camera 330 and the virtual camera 340 moving away from the street 312 at different angles, according to an embodiment. In this example, the path to virtual camera 340 is similar to virtual camera 320 of FIG. 3A (ie, toward target location 342). However, the path to the virtual camera 330 initially deviates from the street 312 (center line) at a relatively small angle (eg, as indicated by the initial path 331). Similar to the virtual camera 310 of FIG. 3A, the path of the virtual camera 330 is corrected to align along the street 312. If the user is interested in buildings outside the navigation path, turn the viewpoint to an angle more perpendicular to the road, for example, by entering the appropriate keystrokes or, for example, via navigation controls Thus, the correction can be invalidated.
図３Ｃは、ある実施形態による、交差点を横断して、街路３５２と整列させる、バーチャルカメラのための例示的進路３５０を図示する略図３００Ｃである。バーチャルカメラ３５０のビュー方向は、街路３１２および３５２両方に対して、等角度の範囲を定めると仮定され得る。バーチャルカメラ３５０の進路が、街路３５２と整列されるべきであることが、略図３００Ｃから明白と考えられる得るが、略図に図示される進路の入射角は、街路３５２との整列のために十分ではない場合がある。むしろ、固定前進速度を前提とすると、それに沿って、バーチャルカメラ３５０が整列されるべき、街路（３５２）は、より大きな総角度偏向を要求する場合さえ、最小の方向転換率を要求する。したがって、旋回率指標を利用して、他の街路と比較して、最小の方向転換率をもたらす街路に基づいて、進路整列のために適切な街路を選択し得る。 FIG. 3C is a diagram 300C illustrating an exemplary course 350 for a virtual camera that aligns with a street 352 across an intersection, according to an embodiment. The viewing direction of the virtual camera 350 can be assumed to define equiangular ranges for both streets 312 and 352. Although it may be apparent from the schematic 300C that the path of the virtual camera 350 should be aligned with the street 352, the angle of incidence of the path illustrated in the schematic is not sufficient for alignment with the street 352. There may not be. Rather, given a fixed forward speed, the street (352) along which the virtual camera 350 is to be aligned requires a minimum turnaround rate, even if it requires a larger total angular deflection. Thus, the turn rate index may be used to select an appropriate street for course alignment based on the street that yields the least turn rate compared to other streets.
図３Ｄおよび３Ｅは、誘導付きナビゲーション補正とナビゲーション経路半径３０２の相互作用を解明する。図３Ｄは、ある実施形態による、交差点内の異なる街路と整列する、バーチャルカメラ３６０のための例示的進路を図示する略図３００Ｄである。図３Ｅは、ある実施形態による、交差点内の異なる街路と整列する、バーチャルカメラ３７０のための例示的進路を図示する略図３００Ｅである。 3D and 3E elucidate the interaction between guided navigation correction and navigation path radius 302. FIG. 3D is a diagram 300D illustrating an exemplary course for a virtual camera 360 that aligns with different streets within an intersection, according to an embodiment. FIG. 3E is a diagram 300E illustrating an exemplary course for a virtual camera 370 aligned with different streets within an intersection, according to an embodiment.
バーチャルカメラ３６０および３７０の両方のビュー方向に基づいて、両方に対する標的進路は、街路３１２に対応し、街路３１２は、略図３００Ｄおよび３００Ｅにそれぞれ示されるように、各バーチャルカメラの初期位置から、より長い距離（例えば、街路３５２に対する初期位置の距離と比較して）に位置する。各略図に示されるように、バーチャルカメラ３６０のための進路は、その現在のビュー方向に基づいて、最も遠い街路（３１２）に補正される。一方、バーチャルカメラ３７０のための進路は、この最も遠い街路を標的とするように補正されず、代わりに、街路３５２に沿って誘導される。バーチャルカメラ３６０の進路とバーチャルカメラ３７０の進路との間の差異は、ナビゲーション経路半径３０２からの許容される距離である。例えば、バーチャルカメラ３７０が、街路３１２に補正された場合、バーチャルカメラの眺めから３次元環境を表示するために使用されるビューポートは、許容されるナビゲーション経路外にはずれるであろう（例えば、前述の図２のナビゲーション経路２００によって図示されるように）。ナビゲーション経路外のナビゲーションは、許容され得ない、不良または望ましくないビューをもたらし得る。その結果、バーチャルカメラ３７０は、このバーチャルカメラの初期位置により近い距離にある、街路３５２に補正される。 Based on the view direction of both virtual cameras 360 and 370, the target path for both corresponds to street 312, which is more from the initial position of each virtual camera, as shown in diagrams 300D and 300E, respectively. Located at a long distance (eg, compared to the initial position distance to the street 352). As shown in each schematic, the path for the virtual camera 360 is corrected to the farthest street (312) based on its current view direction. On the other hand, the path for the virtual camera 370 is not corrected to target this farthest street, but is instead guided along the street 352. The difference between the path of the virtual camera 360 and the path of the virtual camera 370 is the allowable distance from the navigation path radius 302. For example, if the virtual camera 370 is corrected to the street 312, the viewport used to display the 3D environment from the virtual camera view will deviate from the allowed navigation path (eg, as described above). As illustrated by the navigation path 200 of FIG. Navigation outside the navigation path can lead to unacceptable, bad or undesirable views. As a result, the virtual camera 370 is corrected to a street 352 that is closer to the initial position of the virtual camera.
（道路およびパノラマとの自動整列）
ある実施例では、バーチャルカメラは、地球の空中の眺めに対応する初期位置を有し得る。ある実施形態では、ユーザは、３次元環境内の適切な位置を選択することによって、地球上の地理的場所に対応する標的を選択し得る。バーチャルカメラは、次いで、選択された標的に基づいて、より低い高度まで、徐々にズームダウンし得る。地上レベル近傍の閾値高度に到達し得、これは、ユーザが、地上レベルのオブジェクトを間近で見ようとしていることを推測するために使用され得る。例えば、衛星画像が、ぼやける、またはあまり有用ではなくなり得る、地上レベル近傍では、いくつかの要因が、そのようなユーザの意図を推測するために、考慮され得る。そのような要因は、地上への接近度、バーチャルカメラの移動、およびユーザ入力（例えば、マウスの動き）を含むことができるが、それに限定されない。
(Automatic alignment with roads and panoramas)
In some embodiments, the virtual camera may have an initial position that corresponds to an aerial view of the earth. In certain embodiments, the user may select a target corresponding to a geographic location on the globe by selecting an appropriate location within the three-dimensional environment. The virtual camera may then gradually zoom down to a lower altitude based on the selected target. A threshold altitude near the ground level may be reached, which may be used to infer that the user is looking at a ground level object up close. For example, near the ground level where satellite images may be blurred or less useful, several factors may be considered to infer such user intent. Such factors can include, but are not limited to, proximity to the ground, virtual camera movement, and user input (eg, mouse movement).
ある実施形態では、進路移動モジュール１１８は、空中の眺めから地上レベルの眺めへの遷移を通して、空間認識を維持し、バーチャルカメラを視覚的コンテキストの最大量を維持する、有用位置に設置する。ある実施形態では、進路プランナモジュール１１６は、最大視覚的品質が維持されることを確実にするために、自動的に、パノラマの中心に対応する初期位置に、バーチャルカメラ（例えば、オートパイロット進路を介して）を設置し得る。決定された初期位置に位置し得る、画像オブジェクト（例えば、建造物）が、初期ビューを曖昧にし、ユーザを混乱させないように防止するために、その位置までの単純オートパイロット進路では、十分ではない場合がある。故に、接続またはパノラマグラフが、前述のように、使用され、バーチャルカメラのビューを再配向し、道路方向に沿って、そのビューが、画像オブジェクトによって遮断されない、近傍パノラマを捉えることができる。道路に沿って捉えることは、空中からの「地球」のビューと街路または地上レベルのビューレンダリング経験との間の視覚的連続性を最大限にする一方、ユーザに、より多くの視覚的コンテキストを提供する。さらに、進路移動モジュール１１８によって行われるバーチャルカメラの再配向は、ユーザを混乱させないように、徐々に生じることができる。 In some embodiments, the path movement module 118 maintains spatial awareness through transition from an aerial view to a ground level view and places the virtual camera in a useful position that maintains the maximum amount of visual context. In some embodiments, the course planner module 116 automatically sets a virtual camera (eg, an autopilot course) to an initial position corresponding to the center of the panorama to ensure that maximum visual quality is maintained. Through). A simple autopilot path to that position is not sufficient to prevent image objects (eg, buildings) that may be located at the determined initial position from obscure the initial view and not disturb the user. There is a case. Hence, a connection or panorama graph can be used, as described above, to reorient the virtual camera view and to capture a nearby panorama along the road direction that view is not obstructed by the image object. Capturing along the road maximizes the visual continuity between the “Earth” view from the air and the street or ground level view rendering experience, while giving the user more visual context. provide. Further, the virtual camera reorientation performed by the path movement module 118 can occur gradually so as not to confuse the user.
バーチャルカメラの眺めが、街路レベルのビューに変更される必要がある時、道路網は、進路プランナモジュール１１６に未知であり得るため、オートパイロットナビゲーションが、直ちに、例えば、標的モジュール１１２によって決定された３次元環境内の標的場所へと開始されることができる。加えて、移動モデル１１４は、標的場所近傍の場所に対するパノラマデータを含む、標的場所に関連付けられたパノラマデータに対して、ネットワーク１３０を経由して、サーバ１４０に、一式のフェッチまたは要求を開始することができる。それらのフェッチが返されると、標的モジュール１１２は、標的場所への最近傍パノラマを決定し、さらに、オートパイロット目的地として、決定されたパノラマを選択し得る。ある実施例では、パノラマの選択は、決定された標的場所のある距離閾値内に位置するかどうかに基づき得る。別の実施例では、標的場所に対して表示されるべき最良パノラマの選択は、限定されないが、３次元環境内のパノラマに関連付けられた場所に対する標的位置の接近度、および現在の位置から標的に向かせるためのバーチャルカメラの角度偏差を含む、いくつかの要因を重み付けすることによって決定され得る。 When the virtual camera view needs to be changed to a street level view, the road network may be unknown to the route planner module 116, so autopilot navigation is immediately determined, for example, by the target module 112. It can be initiated to a target location within the 3D environment. In addition, the mobility model 114 initiates a set of fetches or requests to the server 140 via the network 130 for panorama data associated with the target location, including panorama data for locations near the target location. be able to. Once those fetches are returned, the target module 112 may determine the nearest panorama to the target location and may further select the determined panorama as the autopilot destination. In some embodiments, the selection of the panorama may be based on whether the determined target location is within a certain distance threshold. In another example, the selection of the best panorama to be displayed for a target location includes, but is not limited to, the proximity of the target location to the location associated with the panorama in the 3D environment, and the current location to the target. It can be determined by weighting several factors, including the angular deviation of the virtual camera to point.
進路プランナモジュール１１６は、次いで、パノラマに関連付けられ、サーバ１４０から受信されたパノラマ接続情報から、街路方向を計算することができる。現在のビュー方向に基づいて、進路プランナモジュール１１６は、最も近い街路、およびビューに適用される角回転を最小限にするように、３次元環境内に表される街路に沿う適切なビュー方向を選択することができる。この情報が、既知となるとすぐに、進路移動モジュール１１８は、バーチャルカメラのビューの回転を開始し、街路と整列させることができる。 The course planner module 116 can then calculate the street direction from the panorama connection information associated with the panorama and received from the server 140. Based on the current view direction, the course planner module 116 determines the appropriate view direction along the street represented in the 3D environment to minimize the closest street and the angular rotation applied to the view. You can choose. As soon as this information is known, the path movement module 118 can begin rotating the view of the virtual camera and align it with the street.
実施形態はまた、このプロセスの間に生じ得る、任意の追加の複雑性に対処するように構成されることができることは、本説明を前提として、当業者に明白となるであろう。例えば、パノラマに対する後続の未処理のフェッチは、最初に決定されたものより近いパノラマを表し得る。別の実施例では、ユーザ相互作用モジュール１１０に対して、以前は未知であった、別の街路が存在し得る。したがって、その構成要素を含む、ユーザ相互作用モジュール１１０の実施形態は、ユーザを混乱させることなく、そのような複雑性を考慮するように、オートパイロット目的地および配向を動的に更新するように動作可能である。 It will be apparent to those skilled in the art given the present description that embodiments may also be configured to address any additional complexity that may occur during this process. For example, a subsequent raw fetch for a panorama may represent a panorama that is closer than originally determined. In another example, there may be other streets that were previously unknown to the user interaction module 110. Thus, embodiments of the user interaction module 110, including its components, can dynamically update autopilot destinations and orientations to take into account such complexity without disrupting the user. It is possible to operate.
（方法）
図４は、ある実施形態による、標的に向かう、街路網に沿った、バーチャルカメラの誘導付きナビゲーションのための方法４００を図示する流れ図である。説明の容易性のために、方法４００は、前述の図１のシステム１００に関して説明される。しかしながら、方法４００は、それらに限定されるように意図されない。
(Method)
FIG. 4 is a flow diagram illustrating a method 400 for guided navigation of a virtual camera along a street network toward a target, according to an embodiment. For ease of explanation, the method 400 will be described with respect to the system 100 of FIG. 1 described above. However, the method 400 is not intended to be limited thereto.
方法４００は、ステップ４０２において、標的を決定することから開始する。標的は、３次元環境内の任意の点または領域であり得る。ある実施例では、３次元環境は、複数のモデルを含んでもよく、標的は、モデルまたはモデル上の位置であり得る。ユーザは、標的場所を指定し得る。例えば、ユーザは、３次元環境内の場所を指定するための住所または座標を入力し得る。ある実施形態では、場所は、地上または街路レベルであり得る。 The method 400 begins at step 402 by determining a target. A target can be any point or region within a three-dimensional environment. In certain examples, the three-dimensional environment may include multiple models and the target may be a model or a location on the model. The user can specify a target location. For example, the user may enter an address or coordinates to specify a location within the 3D environment. In certain embodiments, the location may be on the ground or street level.
ある実施例では、ユーザは、バーチャルカメラの眺めから、３次元環境を表示する、ビューポート内の位置を選択し得る。ＧＩＳは、ユーザによって選択されたビューポート上の位置を表す、位置データを受信し得る。ＧＩＳは、ユーザによって選択された位置およびバーチャルカメラの焦点距離に基づいて、３次元空間内の点を決定し得る。ある実施例では、決定された点と焦点との間の距離は、バーチャルカメラの焦点距離に対応し得る。 In some embodiments, the user may select a position in the viewport that displays the three-dimensional environment from the virtual camera view. The GIS may receive location data representing the location on the viewport selected by the user. The GIS may determine a point in three-dimensional space based on the position selected by the user and the focal length of the virtual camera. In some embodiments, the distance between the determined point and the focal point may correspond to the focal length of the virtual camera.
いったん３次元環境内の点が、ユーザによって選択された位置に基づいて決定されると、ＧＩＳは、ユーザによって選択された位置に基づいて、半直線を延長し得る。ある実施例では、ＧＩＳは、バーチャルカメラの焦点または入射瞳から、半直線を延長し得る。半直線は、決定された点を通して、焦点または入射瞳から、延長し得る。 Once a point in the three-dimensional environment is determined based on the position selected by the user, the GIS can extend the half line based on the position selected by the user. In some embodiments, the GIS may extend a half line from the focus or entrance pupil of the virtual camera. The half line may extend from the focal point or entrance pupil through the determined point.
延長された半直線によって、標的場所が、半直線に基づいて決定され得る。ある実施形態では、３次元モデルは、建造物モデル等の複数のモデルを含み得る。その実施形態では、標的場所は、３次元環境内のモデルとの半直線の交差点であると決定され得る。このように、標的場所は、ビューポート内のユーザ選択に基づいて、決定され得る。 With the extended half line, the target location can be determined based on the half line. In some embodiments, the three-dimensional model may include multiple models, such as a building model. In that embodiment, the target location may be determined to be a half-line intersection with the model in the three-dimensional environment. In this way, the target location can be determined based on a user selection in the viewport.
いったん標的場所が、ステップ４０２において決定されると、標的場所までの進路に対するナビゲーション経路が、ステップ４０４において生成される。進路は、例えば、３次元環境内に表される街路網に沿ってもよい。さらに、進路は、前述のように、３次元環境内の３次元モデルとしてレンダリングされたパノラマの中心に対応する場所間にあり得る。ナビゲーション経路は、また、前述のように、３次元環境内のバーチャルカメラの移動に対する制約表面およびナビゲーション境界を規定する。ある実施例では、パノラマ写真は、一連の点内の各点に位置し得、パノラマ写真は、各パノラマに関連付けられた接続情報に基づいて、パノラマまたは接続グラフ内で相互にリンクされ得る。例えば、バーチャルカメラが、特定の点に位置する時、ＧＩＳは、バーチャルカメラの眺めから、写真画像データを表示し得る。 Once the target location is determined in step 402, a navigation path for the route to the target location is generated in step 404. The course may be along a street network represented in a three-dimensional environment, for example. Further, the course may be between locations corresponding to the center of the panorama rendered as a 3D model in the 3D environment, as described above. The navigation path also defines constraining surfaces and navigation boundaries for movement of the virtual camera within the three-dimensional environment, as described above. In certain embodiments, panoramic photos may be located at each point in a series of points, and the panoramic photos may be linked to each other in the panorama or connection graph based on connection information associated with each panorama. For example, when the virtual camera is located at a specific point, the GIS may display photographic image data from the virtual camera view.
進路は、標的場所に近接するように、バーチャルカメラを誘導し得る。例えば、進路は、標的場所に最も近い街路網内の位置へとバーチャルカメラを誘導し得る。代替として、および加えて、進路は、最大距離（例えば、パノラマの中心に対する最大ローミング距離）を超えないように決定され得る。 The course may guide the virtual camera closer to the target location. For example, the course may guide the virtual camera to a location in the street network that is closest to the target location. Alternatively and additionally, the course may be determined not to exceed a maximum distance (eg, a maximum roaming distance to the center of the panorama).
バーチャルカメラが、ナビゲーション経路内の進路に沿って、移動される場合に、ステップ４０６において、標的場所に向くように配向され得る。ステップ４０８では、バーチャルカメラは、限定されないが、前述のように、３次元環境内のバーチャルカメラのビュー方向および位置を含む、いくつかの要因に基づいて、標的場所に向かって、自動的に、ナビゲーション経路内を誘導され得る。さらに、ユーザは、また、前述のように、ユーザ入力を通して、任意の自動誘導を無効にし得る。 If the virtual camera is moved along a path in the navigation path, it may be oriented at step 406 to face the target location. In step 408, the virtual camera automatically moves toward the target location based on a number of factors, including but not limited to, the viewing direction and position of the virtual camera in the three-dimensional environment, as described above. It can be guided in the navigation path. In addition, the user may also override any automatic guidance through user input as described above.
バーチャルカメラが、標的場所へとより近くに移動されるにつれて、標的場所に対応する景色が、レンダリングされ、ユーザに表示されることができる。ある実施形態では、ステップ４０８はさらに、標的場所に基づいて表示されるべきパノラマを選択することを含む。しかしながら、適切なパノラマの選択は、単に、標的場所へのパノラマの接近度に基づくのみではない場合がある。別の重要な要因は、例えば、バーチャルカメラが、ビュー内に標的を維持するために、移動されなければならないビュー角度であり得る。例えば、最も近いパノラマは、標的より道路に沿って遠くにあり得る。しかしながら、ビュー内に標的を維持するために、バーチャルカメラは、方向転換し、ほぼ後方に向く必要があるであろう。故に、選択されたパノラマは、バーチャルカメラの初期位置、すなわち、起点に最も近いものであるが、また、カメラ回転の量が最小限にされるように、３次元環境内の標的場所から十分遠くに位置する。 As the virtual camera is moved closer to the target location, a view corresponding to the target location can be rendered and displayed to the user. In some embodiments, step 408 further includes selecting a panorama to be displayed based on the target location. However, the selection of an appropriate panorama may not be based solely on the proximity of the panorama to the target location. Another important factor may be, for example, the viewing angle at which the virtual camera must be moved to maintain the target in the view. For example, the closest panorama may be farther along the road than the target. However, in order to keep the target in view, the virtual camera will need to turn around and point almost backwards. Thus, the selected panorama is closest to the initial position of the virtual camera, i.e., the origin, but is also far enough from the target location in the 3D environment so that the amount of camera rotation is minimized. Located in.
（例示的コンピュータシステム実装）
図１−４に示される実施形態、あるいはその任意の部分または機能は、ハードウェア、ソフトウェアモジュール、ファームウェア、その上に記憶される命令を有する、有形コンピュータ可読媒体、またはそれらの組み合わせを使用して、実装され得、１つ以上のコンピュータシステムまたは他の処理システム内に実装され得る。
(Example computer system implementation)
The embodiment shown in FIGS. 1-4, or any portion or function thereof, uses hardware, software modules, firmware, tangible computer readable media having instructions stored thereon, or combinations thereof. Can be implemented in one or more computer systems or other processing systems.
図５は、実施形態またはその一部が、コンピュータ可読コードとして実装され得る、例示的コンピュータシステム５００を図示する。例えば、図１におけるクライアント１０２は、ハードウェア、ソフトウェア、ファームウェア、その上に記憶される命令を有する、有形コンピュータ可読媒体、またはそれらの組み合わせを使用して、コンピュータシステム５００内に実装されることができ、１つ以上のコンピュータシステムまたは他の処理システム内に実装され得る。ハードウェア、ソフトウェア、またはそのようなものの任意の組み合わせは、図１−４におけるモジュールおよび構成要素のいずれかを具現化し得る。 FIG. 5 illustrates an example computer system 500 in which embodiments or portions thereof may be implemented as computer readable code. For example, client 102 in FIG. 1 may be implemented in computer system 500 using hardware, software, firmware, a tangible computer readable medium having instructions stored thereon, or a combination thereof. Can be implemented in one or more computer systems or other processing systems. Hardware, software, or any combination of such may embody any of the modules and components in FIGS. 1-4.
プログラマブル論理が使用される場合、そのような論理は、市販のプロセッシングプラットフォームまたは特殊目的デバイス上で実行し得る。当業者は、開示される主題の実施形態が、マルチコアマルチプロセッサシステム、ミニコンピュータ、メインフレームコンピュータ、分散機能によってリンクまたはクラスタ化されるコンピュータ、ならびに事実上、任意のデバイスに埋め込まれ得る、パーペイシブまたは小型コンピュータを含む、種々のコンピュータシステム構成とともに実践されることができることを理解し得る。 If programmable logic is used, such logic may execute on commercially available processing platforms or special purpose devices. Those skilled in the art will recognize that embodiments of the disclosed subject matter can be embedded in multicore multiprocessor systems, minicomputers, mainframe computers, computers linked or clustered by distributed functions, and virtually any device, It can be appreciated that the invention can be practiced with a variety of computer system configurations, including small computers.
例えば、少なくとも１つのプロセッサデバイスおよびメモリを使用して、前述の説明される実施形態を実装し得る。プロセッサデバイスは、単一プロセッサ、複数のプロセッサ、またはそれらの組み合わせであり得る。プロセッサデバイスは、１つ以上のプロセッサ「コア」を有し得る。 For example, the previously described embodiments may be implemented using at least one processor device and memory. The processor device may be a single processor, multiple processors, or a combination thereof. A processor device may have one or more processor “cores”.
本発明の種々の実施形態は、この例示的コンピュータシステム５００の観点から説明される。本説明を熟読後、他のコンピュータシステムおよび／またはコンピュータアーキテクチャを使用して、本発明の実施形態をどのように実装するべきかは、当業者に明白となるであろう。動作は、シーケンシャルプロセスとして説明され得るが、動作のうちのいくつかは、実際は、並行して、同時に、および／または分散環境において、かつ単一または多重プロセッサ機械によってアクセスするために、ローカルあるいは遠隔に記憶されるプログラムコードによって、行われ得る。加えて、いくつかの実施形態では、動作の順序は、開示される主題の精神から逸脱することなく、再配列され得る。 Various embodiments of the invention are described in terms of this exemplary computer system 500. After reading this description, it will become apparent to one skilled in the art how to implement embodiments of the invention using other computer systems and / or computer architectures. Although operations may be described as sequential processes, some of the operations are actually local or remote for access in parallel, simultaneously, and / or in a distributed environment and by a single or multiprocessor machine. Can be performed by the program code stored in In addition, in some embodiments, the order of operations can be rearranged without departing from the spirit of the disclosed subject matter.
プロセッサデバイス５０４は、特殊目的または汎用プロセッサデバイスであり得る。当業者によって理解されるように、プロセッサデバイス５０４はまた、マルチコア／マルチプロセッサシステム内の単一プロセッサであり得、そのようなシステムは、単独で、あるいはクラスタまたはサーバファーム内で動作するコンピューティングデバイス内のクラスタ内で動作する。プロセッサデバイス５０４は、通信インフラストラクチャ５０６、例えば、バス、メッセージキュー、ネットワーク、またはマルチコアメッセージ受け渡し方式に接続される。 The processor device 504 may be a special purpose or general purpose processor device. As will be appreciated by those skilled in the art, the processor device 504 may also be a single processor in a multi-core / multi-processor system, such a system operating alone or in a cluster or server farm. Works within a cluster. The processor device 504 is connected to a communication infrastructure 506, eg, a bus, message queue, network, or multi-core message passing scheme.
コンピュータシステム５００はまた、メインメモリ５０８、例えば、ランダムアクセスメモリ（ＲＡＭ）を含み、また、二次メモリ５１０を含み得る。二次メモリ５１０は、例えば、ハードディスクドライブ５１２およびリムーバブル記憶ドライブ５１４を含み得る。リムーバブル記憶ドライブ５１４は、フロッピー（登録商標）ディスクドライブ、磁気テープドライブ、光ディスクドライブ、フラッシュメモリ、または同等物を備え得る。リムーバブル記憶ドライブ５１４は、公知の様式において、リムーバブル記憶ユニット５１８からの読取および／またはそこへの書込を行う。リムーバブル記憶ユニット５１８は、リムーバブル記憶ドライブ５１４によって読み取られ、そこに書き込まれる、フロッピー（登録商標）ディスク、磁気テープ、光ディスク等を備え得る。当業者によって理解されるように、リムーバブル記憶ユニット５１８は、その中に記憶されたコンピュータソフトウェアおよび／またはデータを有する、コンピュータ使用可能記憶媒体を含む。 Computer system 500 also includes main memory 508, for example, random access memory (RAM), and may include secondary memory 510. Secondary memory 510 may include, for example, a hard disk drive 512 and a removable storage drive 514. The removable storage drive 514 may comprise a floppy disk drive, magnetic tape drive, optical disk drive, flash memory, or the like. The removable storage drive 514 reads from and / or writes to the removable storage unit 518 in a known manner. The removable storage unit 518 may comprise a floppy disk, magnetic tape, optical disk, etc. that is read and written by the removable storage drive 514. As will be appreciated by those skilled in the art, the removable storage unit 518 includes a computer usable storage medium having computer software and / or data stored therein.
代替実装では、二次メモリ５１０は、コンピュータプログラムまたは他の命令をコンピュータシステム５００内にロード可能にするための他の類似手段を含み得る。そのような手段は、例えば、リムーバブル記憶ユニット５２２およびインターフェース５２０を含み得る。そのような手段の実施例は、プログラムカートリッジおよびカートリッジインターフェース（ビデオゲームデバイス内に見られるもの等）、リムーバブルメモリチップ（ＥＰＲＯＭまたはＰＲＯＭ等）および関連付けられたソケット、ならびにソフトウェアおよびデータをリムーバブル記憶ユニット５２２からコンピュータシステム５００に転送可能にする、他のリムーバブル記憶ユニット５２２およびインターフェース５２０を含み得る。 In alternative implementations, secondary memory 510 may include other similar means for allowing computer programs or other instructions to be loaded into computer system 500. Such means may include, for example, a removable storage unit 522 and an interface 520. Examples of such means include program cartridges and cartridge interfaces (such as those found in video game devices), removable memory chips (such as EPROM or PROM) and associated sockets, and software and data removable storage unit 522. Other removable storage units 522 and interfaces 520 may be included that enable transfer from the computer system 500 to the computer system 500.
コンピュータシステム５００はまた、通信インターフェース５２４を含み得る。通信インターフェース５２４は、ソフトウェアおよびデータをコンピュータシステム５００と外部デバイスとの間で転送可能にする。通信インターフェース５２４は、モデム、ネットワークインターフェース（イーサネット（登録商標）カード等）、通信ポート、ＰＣＭＣＩＡスロットおよびカード、または同等物を含み得る。通信インターフェース５２４を介して転送されるソフトウェアおよびデータは、電子、電磁、光学、または通信インターフェース５２４によって受信可能な他の信号であり得る、信号の形態であり得る。これらの信号は、通信進路５２６を介して、通信インターフェース５２４に提供され得る。通信進路５２６は、信号を搬送し、有線またはケーブル、光ファイバ、電話回線、携帯電話リンク、ＲＦリンク、または他の通信経路を使用して、実装され得る。 Computer system 500 may also include a communication interface 524. Communication interface 524 enables software and data to be transferred between computer system 500 and external devices. Communication interface 524 may include a modem, a network interface (such as an Ethernet card), a communication port, a PCMCIA slot and card, or the like. Software and data transferred via communication interface 524 may be in the form of signals, which may be electronic, electromagnetic, optical, or other signals receivable by communication interface 524. These signals may be provided to communication interface 524 via communication path 526. Communication path 526 carries signals and may be implemented using wired or cable, fiber optic, telephone lines, cellular telephone links, RF links, or other communication paths.
本書では、用語「コンピュータプログラム媒体」および「コンピュータ使用可能媒体」は、概して、リムーバブル記憶ユニット５１８、リムーバブル記憶ユニット５２２、およびドライブ５１２内にインストールされるハードディスク等の媒体を指すために使用される。コンピュータプログラム媒体およびコンピュータ使用可能媒体はまた、メモリ半導体（例えば、ＤＲＡＭ等）であり得る、メインメモリ５０８および二次メモリ５１０等のメモリを指し得る。 In this document, the terms “computer program medium” and “computer usable medium” are generally used to refer to media such as a removable storage unit 518, a removable storage unit 522, and a hard disk installed in a drive 512. Computer program media and computer usable media may also refer to memories, such as main memory 508 and secondary memory 510, which may be memory semiconductors (eg, DRAM, etc.).
コンピュータプログラム（コンピュータ制御論理とも呼ばれる）は、メインメモリ５０８および／または二次メモリ５１０内に記憶される。コンピュータプログラムはまた、通信インターフェース５２４を介して、受信され得る。そのようなコンピュータプログラムは、実行されると、コンピュータシステム５００が、本明細書に論じられるような実施形態を実装することを可能にする。特に、コンピュータプログラムは、実行されると、プロセッサデバイス５０４が、前述の図４の流れ図４００によって図示される方法における段階等、本発明の実施形態のプロセスを実装することを可能にする。故に、そのようなコンピュータプログラムは、コンピュータシステム５００のコントローラを表す。実施形態が、ソフトウェアを使用して実装される場合、ソフトウェアは、コンピュータプログラム製品内に記憶され、リムーバブル記憶ドライブ５１４、インターフェース５２０、ハードディスクドライブ５１２、または通信インターフェース５２４を使用して、コンピュータシステム５００内にロードされ得る。 Computer programs (also called computer control logic) are stored in main memory 508 and / or secondary memory 510. Computer programs may also be received via communication interface 524. Such computer programs, when executed, enable the computer system 500 to implement embodiments as discussed herein. In particular, the computer program, when executed, enables the processor device 504 to implement the processes of embodiments of the present invention, such as the steps in the method illustrated by the flowchart 400 of FIG. 4 described above. Thus, such a computer program represents a controller of computer system 500. If the embodiment is implemented using software, the software is stored within the computer program product and within the computer system 500 using the removable storage drive 514, interface 520, hard disk drive 512, or communication interface 524. Can be loaded.
本発明の実施形態はまた、任意のコンピュータ使用可能媒体上に記憶されるソフトウェアを備える、コンピュータプログラム製品を対象とし得る。そのようなソフトウェアは、１つ以上のデータ処理デバイス内で実装されると、データ処理デバイスを本明細書に説明されるように動作させる。本発明の実施形態は、任意のコンピュータ使用可能または可読媒体を採用する。コンピュータ使用可能媒体の実施例として、一次記憶デバイス（例えば、任意のタイプのランダムアクセスメモリ）、二次記憶装置デバイス（例えば、ハードドライブ、フロッピー（登録商標）ディスク、ＣＤ ＲＯＭＳ、ＺＩＰディスク、テープ、磁気記憶デバイス、光学記憶デバイス、ＭＥＭＳ、ナノテクノロジー記憶デバイス等）、および通信媒体（例えば、有線および無線通信ネットワーク、ローカルエリアネットワーク、広域ネットワーク、イントラネット等）が挙げられるが、それらに限定されない。 Embodiments of the present invention may also be directed to a computer program product comprising software stored on any computer usable medium. Such software, when implemented in one or more data processing devices, causes the data processing devices to operate as described herein. Embodiments of the invention employ any computer-usable or readable medium. Examples of computer usable media include primary storage devices (eg, any type of random access memory), secondary storage devices (eg, hard drives, floppy disks, CD ROMS, ZIP disks, tapes, Magnetic storage devices, optical storage devices, MEMS, nanotechnology storage devices, etc.) and communication media (eg, wired and wireless communication networks, local area networks, wide area networks, intranets, etc.), but are not limited thereto.
（結論）
概要および要約の項は、本発明者らによって検討されるような本発明の全てではないが１つ以上の例示的実施形態を記載し得、したがって、決して本発明および添付の請求項を限定することを目的としない。
(Conclusion)
The summary and summary sections may describe one but more than one exemplary embodiment of the invention as discussed by the inventors, and thus never limit the invention and the appended claims. Not intended for that.
本発明の実施形態は、その指定機能および関係の実装を図示する機能的構成要素を用いて、上記で説明された。これらの機能的構成要素の境界は、説明の便宜上、本明細書では任意に規定した。その指定機能および関係が適切に実施される限り、代替的境界を規定することができる。 Embodiments of the present invention have been described above using functional components that illustrate the implementation of the designated functions and relationships. The boundaries of these functional components are arbitrarily defined in this specification for convenience of explanation. Alternative boundaries can be defined as long as the designated functions and relationships are properly implemented.
特定の実施形態の先述の説明は、本発明の一般概念から逸脱することなく、必要以上の実験を伴わずに、当技術分野内の知識を適用することによって、他者がそのような特定の実施形態を容易に修正し、および／または種々のアプリケーションに適合させることができる、本発明の一般的性質を完全に明らかにする。したがって、そのような適合および修正は、本明細書で提示される教示および指導に基づいて、開示された実施形態の同等物の意味および範囲内となることを目的とする。本明細書の用語または表現が、教示および指導に照らして当業者によって解釈されるものであるように、本明細書の表現または用語は、限定ではなく説明の目的によるものであることを理解されたい。 The foregoing description of specific embodiments has been described by others by applying knowledge within the art without undue experimentation without departing from the general concept of the invention. It fully clarifies the general nature of the invention in which embodiments can be easily modified and / or adapted to various applications. Accordingly, such adaptations and modifications are intended to be within the meaning and scope of the equivalents of the disclosed embodiments based on the teachings and guidance presented herein. It will be understood that the terminology or terminology herein is for purposes of illustration and not limitation, as the terminology or terminology herein is to be interpreted by one of ordinary skill in the art in light of the teachings and guidance. I want.
本発明の幅および範囲は、上記の例示的実施形態のうちのいずれによっても限定されるべきではないが、以下の請求項およびそれらの同等物のみに従って定義されるべきである。 The breadth and scope of the present invention should not be limited by any of the above-described exemplary embodiments, but should be defined only in accordance with the following claims and their equivalents.
Claims (30)
前記３次元環境内に前記バーチャルカメラのビューポイントからのパノラマ画像の第１の３次元表現を表示することと、
前記第１のパノラマ画像に関連付けられたメタデータに基づいて、前記第１のパノラマ画像にリンクされた１つ以上の追加のパノラマ画像を識別することと、
前記３次元環境内における、前記第１のパノラマ画像および前記１つ以上の追加のパノラマ画像の各々に対して良好な視覚的品質の領域を決定することと、
各良好な視覚的品質の領域に基づいて、前記第１のパノラマ画像と前記１つ以上の追加のパノラマ画像との間の進路に対する１つ以上のナビゲーション経路を生成することであって、各ナビゲーション経路は、前記バーチャルカメラの移動を制約するレンダリング表面を有する、ことと、
ユーザ入力に基づいて、前記第１のパノラマ画像に関連付けられた第１の位置から、前記１つ以上の追加のパノラマ画像のうちの第２のパノラマ画像に関連付けられた第２の位置に向かう前記進路に沿って、前記３次元環境内において前記バーチャルカメラを移動させることであって、前記３次元空間内の前記バーチャルカメラの移動は、各ナビゲーション経路によって規定される空間の体積に制約される、ことと
を含む、方法。 A computer-implemented method for guided navigation of a virtual camera in a bidirectional 3D environment, comprising:
Displaying a first three-dimensional representation of a panoramic image from a viewpoint of the virtual camera in the three-dimensional environment;
Identifying one or more additional panoramic images linked to the first panoramic image based on metadata associated with the first panoramic image;
Determining a region of good visual quality for each of the first panoramic image and the one or more additional panoramic images within the three-dimensional environment;
Generating one or more navigation paths for a path between the first panoramic image and the one or more additional panoramic images based on each good visual quality region, each navigation The path has a rendering surface that constrains movement of the virtual camera;
Based on a user input, from the first position associated with the first panoramic image to the second position associated with a second panoramic image of the one or more additional panoramic images. Moving the virtual camera in the three-dimensional environment along a path, the movement of the virtual camera in the three-dimensional space being constrained by the volume of the space defined by each navigation path; And a method.
前記パノラマ画像の各々に関連付けられた深度地図に基づいて、前記第１のパノラマ画像および前記１つ以上の追加のパノラマ画像の各々に対してナビゲーション半径を計算することを含み、前記ナビゲーション半径は、前記３次元環境内における、前記パノラマ画像の各々に対する前記良好な視覚的品質の領域を表す、請求項１に記載の方法。 Determining the area of good visual quality is
Calculating a navigation radius for each of the first panoramic image and the one or more additional panoramic images based on a depth map associated with each of the panoramic images, the navigation radius comprising: The method of claim 1, wherein the method represents the region of good visual quality for each of the panoramic images within the three-dimensional environment.
前記１つ以上の追加のナビゲーション経路内の２つの異なるナビゲーション経路の間に衝突球体を接するように適合させることと
をさらに含む、請求項１に記載の方法。 Constructing a navigation fillet around an intersection of the one or more navigation paths, wherein the navigation fillet represents a collision-free area;
The method of claim 1, further comprising: adapting a collision sphere between two different navigation paths in the one or more additional navigation paths.
前記第１および第２のパノラマ画像の各々の前記決定された視覚的品質の領域に基づいて、前記３次元メッシュにおいて、前記第１および第２のパノラマ画像の各々に対する前記ナビゲーション半径を計算することと
をさらに含む、請求項４に記載の方法。 Determining first and second positions in the three-dimensional mesh corresponding to the respective centers of the first and second panoramic images, wherein the first and second positions are the first and second positions; And corresponding to the exact viewpoint of the image object in the second panoramic image;
Calculating the navigation radius for each of the first and second panoramic images in the three-dimensional mesh based on the determined visual quality region of each of the first and second panoramic images. The method of claim 4, further comprising:
バーチャルカメラのビュー方向に基づいて、前記３次元環境における、前記第２のパノラマ画像に関連付けられた標的場所を決定することと、
前記第１のパノラマ画像に関連付けられた初期場所に対応する前記第１の位置から、前記決定された標的場所に対応する前記第２の位置に向かって、前記ナビゲーション経路内で前記バーチャルカメラを移動させることと、
前記バーチャルカメラが前記進路に沿って移動される場合に、前記バーチャルカメラの前記ビュー方向を前記標的場所に向けて整列させることであって、前記整列させることは、最小の補正角度に基づく、ことと
を含む、請求項１に記載の方法。 The moving is
Determining a target location associated with the second panoramic image in the three-dimensional environment based on a view direction of a virtual camera;
Moving the virtual camera in the navigation path from the first position corresponding to the initial location associated with the first panoramic image toward the second position corresponding to the determined target location And letting
Aligning the view direction of the virtual camera toward the target location when the virtual camera is moved along the path, the aligning being based on a minimum correction angle; The method of claim 1, comprising:
前記進路に対する前記バーチャルカメラの前記ビュー方向の整列と、ビュー方向補正およびビュー位置補正の大きさのパラメータ化とに基づいて、前記バーチャルカメラが前記３次元環境内で移動される場合に、前記ビュー方向補正および前記ビュー位置補正を適用することをさらに含む、請求項１２に記載の方法。 The aligning includes
When the virtual camera is moved within the three-dimensional environment based on alignment of the view direction of the virtual camera with respect to the path and parameterization of magnitude of view direction correction and view position correction, the view The method of claim 12, further comprising applying direction correction and the view position correction.
前記初期位置から前記標的位置への前記進路に沿って前記バーチャルカメラが移動される場合に、前記３次元環境内における、前記第２のパノラマ画像に対応する前記３次元メッシュの第２の部分を自動的にレンダリングすることと、
前記第１の３次元メッシュの前記第２の部分がレンダリングされる場合、前記第１の３次元メッシュの前記第２の部分を、前記３次元環境における前記レンダリングされた第１の部分とブレンディングすることと
をさらに含む、請求項１に記載の方法。 Rendering a first portion of the three-dimensional mesh corresponding to the first panoramic image in the three-dimensional environment from a view of the virtual camera;
When the virtual camera is moved along the path from the initial position to the target position, the second portion of the three-dimensional mesh corresponding to the second panoramic image in the three-dimensional environment is Automatically rendering,
If the second portion of the first three-dimensional mesh is rendered, the second portion of the first three-dimensional mesh is blended with the rendered first portion in the three-dimensional environment. The method of claim 1, further comprising:
前記第２の部分を自動的にレンダリングすることは、前記第２のパノラマ画像に基づいて、第２のテクスチャに従って、前記３次元環境内の前記３次元メッシュの前記第２の部分を自動的にレンダリングすることを含み、
前記ブレンディングすることは、第２の３Ｄメッシュがレンダリングされる場合に、ブレンディングパラメータに基づいて、前記第１のテクスチャと前記第２のテクスチャとをブレンディングすることを含む、請求項９に記載の方法。 Rendering the first portion comprises rendering the first portion of the three-dimensional mesh in the three-dimensional environment according to a first texture based on the first panoramic image;
The rendering of the second part automatically is based on the second panoramic image and automatically renders the second part of the 3D mesh in the 3D environment according to a second texture. Including rendering,
The method of claim 9, wherein the blending comprises blending the first texture and the second texture based on blending parameters when a second 3D mesh is rendered. .
レンダラモジュールであって、前記レンダラモジュールは、
前記３次元環境内に前記バーチャルカメラのビューポイントからのパノラマ画像の第１の３次元表現を表示することと、
前記第１のパノラマ画像に関連付けられたメタデータに基づいて、前記第１のパノラマ画像にリンクされた１つ以上の追加のパノラマ画像を識別することと
を行うように構成されている、レンダラモジュールと、
前記３次元環境内における、前記第１のパノラマ画像および前記１つ以上の追加のパノラマ画像の各々に対して良好な視覚的品質の領域を決定するように構成されている進路プランナモジュールと、
進路移動モジュールであって、前記進路移動モジュールは、
各良好な視覚的品質の領域に基づいて、前記第１のパノラマ画像と前記１つ以上の追加のパノラマ画像との間の進路に対する１つ以上のナビゲーション経路を生成することであって、各ナビゲーション経路は、前記バーチャルカメラの移動を制約するレンダリング表面を有する、ことと、
ユーザ入力に基づいて、前記第１のパノラマ画像に関連付けられた第１の位置から、前記１つ以上の追加のパノラマ画像のうちの第２のパノラマ画像に関連付けられた第２の位置に向かう前記進路に沿って、前記３次元環境内において前記バーチャルカメラを移動させることであって、前記３次元空間内の前記バーチャルカメラの移動は、各ナビゲーション経路によって規定される空間の体積に制約される、ことと
を行うように構成されている、進路移動モジュールと
を備えている、システム。 A system for guided navigation of a virtual camera in a bidirectional 3D environment,
A renderer module, wherein the renderer module is
Displaying a first three-dimensional representation of a panoramic image from a viewpoint of the virtual camera in the three-dimensional environment;
A renderer module configured to identify one or more additional panoramic images linked to the first panoramic image based on metadata associated with the first panoramic image. When,
A course planner module configured to determine a region of good visual quality for each of the first panoramic image and the one or more additional panoramic images within the three-dimensional environment;
A route moving module, wherein the route moving module is
Generating one or more navigation paths for a path between the first panoramic image and the one or more additional panoramic images based on each good visual quality region, each navigation The path has a rendering surface that constrains movement of the virtual camera;
Based on a user input, from the first position associated with the first panoramic image to the second position associated with a second panoramic image of the one or more additional panoramic images. Moving the virtual camera in the three-dimensional environment along a path, the movement of the virtual camera in the three-dimensional space being constrained by the volume of the space defined by each navigation path; A system comprising: a path movement module configured to perform
前記進路プランナモジュールは、前記１つ以上の追加のナビゲーション経路内の２つの異なるナビゲーション経路の間に衝突球体を接するように適合させるようにさらに構成されている、
請求項１８に記載のシステム。 The course planner module is further configured to build a navigation fillet around an intersection of the one or more navigation paths, the navigation fillet representing a collision free area;
The course planner module is further configured to adapt a collision sphere between two different navigation paths in the one or more additional navigation paths.
The system of claim 18.
前記第１および第２のパノラマ画像の各々の中心に対応する、前記３次元メッシュにおける第１および第２の位置を決定することであって、前記第１および第２の位置は、前記第１および第２のパノラマ画像内の前記画像オブジェクトの正確なビューポイントに対応している、ことと、
前記第１および第２のパノラマ画像の各々の前記決定された視覚的品質の領域に基づいて、前記３次元メッシュにおいて、前記第１および第２のパノラマ画像の各々に対する前記ナビゲーション半径を計算することと
を行うようにさらに構成されている、
請求項２１に記載のシステム。 The course planner module is
Determining first and second positions in the three-dimensional mesh corresponding to the respective centers of the first and second panoramic images, wherein the first and second positions are the first and second positions; And corresponding to the exact viewpoint of the image object in the second panoramic image;
Calculating the navigation radius for each of the first and second panoramic images in the three-dimensional mesh based on the determined visual quality region of each of the first and second panoramic images. And is further configured to do
The system of claim 21.
バーチャルカメラのビュー方向に基づいて、前記３次元環境における、前記第２のパノラマ画像に関連付けられた標的場所を決定することと、
前記第１のパノラマ画像に関連付けられた初期場所に対応する前記第１の位置から、前記決定された標的場所に対応する前記第２の位置に向かって、前記ナビゲーション経路内で前記バーチャルカメラを移動させることと、
前記バーチャルカメラが前記進路に沿って移動される場合に、前記バーチャルカメラの前記ビュー方向を前記標的場所に向けて整列させることと
を行うようにさらに構成されている、
請求項１８に記載のシステム。 The path moving module is
Determining a target location associated with the second panoramic image in the three-dimensional environment based on a view direction of a virtual camera;
Moving the virtual camera in the navigation path from the first position corresponding to the initial location associated with the first panoramic image toward the second position corresponding to the determined target location And letting
When the virtual camera is moved along the path, the virtual camera is further configured to align the view direction of the virtual camera toward the target location;
The system of claim 18.
前記バーチャルカメラの眺めから、前記３次元環境における、前記第１のパノラマ画像に対応する前記３次元メッシュの第１の部分をレンダリングすることと、
前記初期位置から前記標的位置への前記進路に沿って前記バーチャルカメラが移動される場合に、前記３次元環境内における、前記第２のパノラマ画像に対応する前記３次元メッシュの第２の部分を自動的にレンダリングすることと、
前記第１の３次元メッシュの前記第２の部分がレンダリングされる場合、前記第１の３次元メッシュの前記第２の部分を、前記３次元環境における前記レンダリングされた第１の部分とブレンディングすることと
を行うようにさらに構成されている、
請求項１８に記載のシステム。 The renderer module is
Rendering a first portion of the three-dimensional mesh corresponding to the first panoramic image in the three-dimensional environment from a view of the virtual camera;
When the virtual camera is moved along the path from the initial position to the target position, the second portion of the three-dimensional mesh corresponding to the second panoramic image in the three-dimensional environment is Automatically rendering,
If the second portion of the first three-dimensional mesh is rendered, the second portion of the first three-dimensional mesh is blended with the rendered first portion in the three-dimensional environment. Are further configured to do things,
The system of claim 18.
前記第１のパノラマ画像に基づいて、第１のテクスチャに従って、前記３次元環境内の前記３次元メッシュの前記第１の部分をレンダリングすることと、
前記第２のパノラマ画像に基づいて、第２のテクスチャに従って、前記３次元環境内の前記３次元メッシュの前記第２の部分を自動的にレンダリングすることと、
第２の３Ｄメッシュがレンダリングされる場合に、ブレンディングパラメータに基づいて、前記第１のテクスチャと前記第２のテクスチャとをブレンディングすることと
を行うようにさらに構成されている、請求項１８に記載のシステム。 The renderer module is
Rendering the first portion of the three-dimensional mesh in the three-dimensional environment according to a first texture based on the first panoramic image;
Automatically rendering the second portion of the three-dimensional mesh in the three-dimensional environment according to a second texture based on the second panoramic image;
The method of claim 18, further configured to blend the first texture and the second texture based on blending parameters when a second 3D mesh is rendered. System.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US41712010P | 2010-11-24 | 2010-11-24 | |
US61/417,120 | 2010-11-24 | ||
PCT/US2011/061906 WO2012071445A2 (en) | 2010-11-24 | 2011-11-22 | Guided navigation through geo-located panoramas |
Publications (2)
Publication Number | Publication Date |
---|---|
JP2014503882A true JP2014503882A (en) | 2014-02-13 |
JP5899232B2 JP5899232B2 (en) | 2016-04-06 |
Family
ID=45401159
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2013541029A Active JP5899232B2 (en) | 2010-11-24 | 2011-11-22 | Navigation with guidance through geographically located panoramas |
Country Status (7)
Country | Link |
---|---|
US (1) | US8823707B2 (en) |
EP (1) | EP2643822B1 (en) |
JP (1) | JP5899232B2 (en) |
AU (1) | AU2011332885B2 (en) |
CA (1) | CA2818695C (en) |
DE (1) | DE202011110924U1 (en) |
WO (1) | WO2012071445A2 (en) |
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2018528509A (en) * | 2015-07-08 | 2018-09-27 | コリア ユニバーシティ リサーチ アンド ビジネス ファウンデーションＫｏｒｅａ Ｕｎｉｖｅｒｓｉｔｙ Ｒｅｓｅａｒｃｈ Ａｎｄ Ｂｕｓｉｎｅｓｓ Ｆｏｕｎｄａｔｉｏｎ | Projected image generation method and apparatus, and mapping method between image pixel and depth value |
JPWO2018025660A1 (en) * | 2016-08-05 | 2019-05-30 | ソニー株式会社 | Image processing apparatus and image processing method |
Families Citing this family (69)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP5652097B2 (en) | 2010-10-01 | 2015-01-14 | ソニー株式会社 | Image processing apparatus, program, and image processing method |
US9848290B2 (en) | 2011-03-10 | 2017-12-19 | Aruba Networks, Inc. | Location based computerized system and method thereof |
US9557885B2 (en) | 2011-08-09 | 2017-01-31 | Gopro, Inc. | Digital media editing |
US9121724B2 (en) * | 2011-09-30 | 2015-09-01 | Apple Inc. | 3D position tracking for panoramic imagery navigation |
US20130141433A1 (en) * | 2011-12-02 | 2013-06-06 | Per Astrand | Methods, Systems and Computer Program Products for Creating Three Dimensional Meshes from Two Dimensional Images |
US8773503B2 (en) | 2012-01-20 | 2014-07-08 | Thermal Imaging Radar, LLC | Automated panoramic camera and sensor platform with computer and optional power supply |
US9396577B2 (en) * | 2012-02-16 | 2016-07-19 | Google Inc. | Using embedded camera parameters to determine a position for a three-dimensional model |
US20130321407A1 (en) * | 2012-06-02 | 2013-12-05 | Schlumberger Technology Corporation | Spatial data services |
US9218685B2 (en) * | 2012-06-05 | 2015-12-22 | Apple Inc. | System and method for highlighting a feature in a 3D map while preserving depth |
US9786097B2 (en) | 2012-06-22 | 2017-10-10 | Matterport, Inc. | Multi-modal method for interacting with 3D models |
US10163261B2 (en) * | 2014-03-19 | 2018-12-25 | Matterport, Inc. | Selecting two-dimensional imagery data for display within a three-dimensional model |
US10139985B2 (en) | 2012-06-22 | 2018-11-27 | Matterport, Inc. | Defining, displaying and interacting with tags in a three-dimensional model |
US8928666B2 (en) * | 2012-10-11 | 2015-01-06 | Google Inc. | Navigating visual data associated with a point of interest |
US9305371B2 (en) | 2013-03-14 | 2016-04-05 | Uber Technologies, Inc. | Translated view navigation for visualizations |
US9712746B2 (en) | 2013-03-14 | 2017-07-18 | Microsoft Technology Licensing, Llc | Image capture and ordering |
US20140267587A1 (en) * | 2013-03-14 | 2014-09-18 | Microsoft Corporation | Panorama packet |
US9685896B2 (en) | 2013-04-09 | 2017-06-20 | Thermal Imaging Radar, LLC | Stepper motor control and fire detection system |
CN105074789B (en) | 2013-04-09 | 2019-03-12 | 热成像雷达有限责任公司 | Fire detection system |
US9165397B2 (en) * | 2013-06-19 | 2015-10-20 | Google Inc. | Texture blending between view-dependent texture and base texture in a geographic information system |
US10330931B2 (en) * | 2013-06-28 | 2019-06-25 | Microsoft Technology Licensing, Llc | Space carving based on human physical data |
KR102248161B1 (en) | 2013-08-09 | 2021-05-04 | 써멀 이미징 레이다 엘엘씨 | Methods for analyzing thermal image data using a plurality of virtual devices and methods for correlating depth values to image pixels |
US9652667B2 (en) | 2014-03-04 | 2017-05-16 | Gopro, Inc. | Automatic generation of video from spherical content using audio/visual analysis |
US9392212B1 (en) | 2014-04-17 | 2016-07-12 | Visionary Vr, Inc. | System and method for presenting virtual reality content to a user |
US9189839B1 (en) | 2014-04-24 | 2015-11-17 | Google Inc. | Automatically generating panorama tours |
GB2526263B (en) * | 2014-05-08 | 2019-02-06 | Sony Interactive Entertainment Europe Ltd | Image capture method and apparatus |
US9002647B1 (en) | 2014-06-27 | 2015-04-07 | Google Inc. | Generating turn-by-turn direction previews |
US9418472B2 (en) * | 2014-07-17 | 2016-08-16 | Google Inc. | Blending between street view and earth view |
US9792502B2 (en) | 2014-07-23 | 2017-10-17 | Gopro, Inc. | Generating video summaries for a video using video summary templates |
US9685194B2 (en) | 2014-07-23 | 2017-06-20 | Gopro, Inc. | Voice-based video tagging |
US20160191796A1 (en) * | 2014-12-30 | 2016-06-30 | Nokia Corporation | Methods And Apparatuses For Directional View In Panoramic Content |
US9734870B2 (en) | 2015-01-05 | 2017-08-15 | Gopro, Inc. | Media identifier generation for camera-captured media |
US9679605B2 (en) | 2015-01-29 | 2017-06-13 | Gopro, Inc. | Variable playback speed template for video editing application |
EP3065108A1 (en) * | 2015-03-04 | 2016-09-07 | Samsung Electronics Co., Ltd. | Displaying a virtual tour |
MX368852B (en) | 2015-03-31 | 2019-10-18 | Thermal Imaging Radar Llc | Setting different background model sensitivities by user defined regions and background filters. |
USD776181S1 (en) | 2015-04-06 | 2017-01-10 | Thermal Imaging Radar, LLC | Camera |
US10186012B2 (en) | 2015-05-20 | 2019-01-22 | Gopro, Inc. | Virtual lens simulation for video and photo cropping |
CN105761302A (en) * | 2015-05-29 | 2016-07-13 | 华北电力大学 | Three-dimensional digital navigation channel system and application thereof |
US9631932B2 (en) | 2015-06-05 | 2017-04-25 | Nokia Technologies Oy | Crowd sourced interaction of browsing behavior in a 3D map |
US9665170B1 (en) | 2015-06-10 | 2017-05-30 | Visionary Vr, Inc. | System and method for presenting virtual reality content to a user based on body posture |
US9721611B2 (en) | 2015-10-20 | 2017-08-01 | Gopro, Inc. | System and method of generating video from video clips based on moments of interest within the video clips |
US10204273B2 (en) | 2015-10-20 | 2019-02-12 | Gopro, Inc. | System and method of providing recommendations of moments of interest within video clips post capture |
US11627199B2 (en) * | 2016-01-05 | 2023-04-11 | Schneider Electric USA, Inc. | System and methods for creating a geospatial network model in a client environment |
US10109319B2 (en) | 2016-01-08 | 2018-10-23 | Gopro, Inc. | Digital media editing |
US9812175B2 (en) | 2016-02-04 | 2017-11-07 | Gopro, Inc. | Systems and methods for annotating a video |
US9838731B1 (en) | 2016-04-07 | 2017-12-05 | Gopro, Inc. | Systems and methods for audio track selection in video editing with audio mixing option |
US9794632B1 (en) | 2016-04-07 | 2017-10-17 | Gopro, Inc. | Systems and methods for synchronization based on audio track changes in video editing |
US9838730B1 (en) | 2016-04-07 | 2017-12-05 | Gopro, Inc. | Systems and methods for audio track selection in video editing |
US10185891B1 (en) | 2016-07-08 | 2019-01-22 | Gopro, Inc. | Systems and methods for compact convolutional neural networks |
US9836853B1 (en) | 2016-09-06 | 2017-12-05 | Gopro, Inc. | Three-dimensional convolutional neural networks for video highlight detection |
US10284809B1 (en) | 2016-11-07 | 2019-05-07 | Gopro, Inc. | Systems and methods for intelligently synchronizing events in visual content with musical features in audio content |
US10262639B1 (en) | 2016-11-08 | 2019-04-16 | Gopro, Inc. | Systems and methods for detecting musical features in audio content |
JP6849430B2 (en) * | 2016-12-27 | 2021-03-24 | キヤノン株式会社 | Image processing equipment, image processing methods, and programs |
US10762653B2 (en) * | 2016-12-27 | 2020-09-01 | Canon Kabushiki Kaisha | Generation apparatus of virtual viewpoint image, generation method, and storage medium |
US10534966B1 (en) | 2017-02-02 | 2020-01-14 | Gopro, Inc. | Systems and methods for identifying activities and/or events represented in a video |
US10127943B1 (en) | 2017-03-02 | 2018-11-13 | Gopro, Inc. | Systems and methods for modifying videos based on music |
US10185895B1 (en) | 2017-03-23 | 2019-01-22 | Gopro, Inc. | Systems and methods for classifying activities captured within images |
US10235798B2 (en) * | 2017-03-24 | 2019-03-19 | Mz Ip Holdings, Llc | System and method for rendering shadows for a virtual environment |
US10083718B1 (en) | 2017-03-24 | 2018-09-25 | Gopro, Inc. | Systems and methods for editing videos based on motion |
US10187690B1 (en) | 2017-04-24 | 2019-01-22 | Gopro, Inc. | Systems and methods to detect and correlate user responses to media content |
US11112857B2 (en) * | 2017-04-28 | 2021-09-07 | Sony Interactive Entertainment Inc. | Information processing apparatus, information processing method, and program |
US10540054B2 (en) * | 2017-09-20 | 2020-01-21 | Matterport, Inc. | Navigation point selection for navigating through virtual environments |
US10574886B2 (en) | 2017-11-02 | 2020-02-25 | Thermal Imaging Radar, LLC | Generating panoramic video for video management systems |
US10859389B2 (en) * | 2018-01-03 | 2020-12-08 | Wipro Limited | Method for generation of a safe navigation path for a vehicle and system thereof |
CN109002165B (en) * | 2018-07-16 | 2021-03-16 | 中国计量大学 | Virtual space real walking guide system with space positioning device |
US10818077B2 (en) | 2018-12-14 | 2020-10-27 | Canon Kabushiki Kaisha | Method, system and apparatus for controlling a virtual camera |
WO2020246261A1 (en) * | 2019-06-05 | 2020-12-10 | ソニー株式会社 | Mobile body, position estimation method, and program |
CN110458871A (en) * | 2019-08-14 | 2019-11-15 | 上海霁目信息科技有限公司 | The method for registering of model and panorama sketch, system, equipment and medium and map |
US11601605B2 (en) | 2019-11-22 | 2023-03-07 | Thermal Imaging Radar, LLC | Thermal imaging camera device |
CN112802206B (en) * | 2021-02-07 | 2022-10-14 | 北京字节跳动网络技术有限公司 | Roaming view generation method, device, equipment and storage medium |
Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20100215250A1 (en) * | 2009-02-24 | 2010-08-26 | Google Inc. | System and method of indicating transition between street level images |
JP2010531007A (en) * | 2007-05-25 | 2010-09-16 | グーグル インコーポレイテッド | Draw, view, and annotate panoramic images and their applications |
Family Cites Families (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6522325B1 (en) * | 1998-04-02 | 2003-02-18 | Kewazinga Corp. | Navigable telepresence method and system utilizing an array of cameras |
US7990394B2 (en) * | 2007-05-25 | 2011-08-02 | Google Inc. | Viewing and navigating within panoramic images, and applications thereof |
WO2008150153A1 (en) * | 2007-06-08 | 2008-12-11 | Tele Atlas B.V. | Method of and apparatus for producing a multi-viewpoint panorama |
US8525825B2 (en) * | 2008-02-27 | 2013-09-03 | Google Inc. | Using image content to facilitate navigation in panoramic image data |
-
2011
- 2011-11-22 JP JP2013541029A patent/JP5899232B2/en active Active
- 2011-11-22 EP EP11799904.5A patent/EP2643822B1/en active Active
- 2011-11-22 CA CA2818695A patent/CA2818695C/en active Active
- 2011-11-22 WO PCT/US2011/061906 patent/WO2012071445A2/en active Application Filing
- 2011-11-22 AU AU2011332885A patent/AU2011332885B2/en active Active
- 2011-11-22 US US13/302,470 patent/US8823707B2/en active Active
- 2011-11-22 DE DE202011110924.4U patent/DE202011110924U1/en not_active Expired - Lifetime
Patent Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2010531007A (en) * | 2007-05-25 | 2010-09-16 | グーグル インコーポレイテッド | Draw, view, and annotate panoramic images and their applications |
US20100215250A1 (en) * | 2009-02-24 | 2010-08-26 | Google Inc. | System and method of indicating transition between street level images |
Cited By (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2018528509A (en) * | 2015-07-08 | 2018-09-27 | コリア ユニバーシティ リサーチ アンド ビジネス ファウンデーションＫｏｒｅａ Ｕｎｉｖｅｒｓｉｔｙ Ｒｅｓｅａｒｃｈ Ａｎｄ Ｂｕｓｉｎｅｓｓ Ｆｏｕｎｄａｔｉｏｎ | Projected image generation method and apparatus, and mapping method between image pixel and depth value |
JPWO2018025660A1 (en) * | 2016-08-05 | 2019-05-30 | ソニー株式会社 | Image processing apparatus and image processing method |
JP7127539B2 (en) | 2016-08-05 | 2022-08-30 | ソニーグループ株式会社 | Image processing device and image processing method |
Also Published As
Publication number | Publication date |
---|---|
CA2818695C (en) | 2018-06-26 |
AU2011332885B2 (en) | 2016-07-07 |
DE202011110924U1 (en) | 2017-04-25 |
WO2012071445A3 (en) | 2012-07-19 |
JP5899232B2 (en) | 2016-04-06 |
EP2643822B1 (en) | 2017-03-22 |
US20120127169A1 (en) | 2012-05-24 |
AU2011332885A1 (en) | 2013-06-13 |
EP2643822A2 (en) | 2013-10-02 |
WO2012071445A2 (en) | 2012-05-31 |
US8823707B2 (en) | 2014-09-02 |
CA2818695A1 (en) | 2012-05-31 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
JP5899232B2 (en) | Navigation with guidance through geographically located panoramas | |
US11663785B2 (en) | Augmented and virtual reality | |
US9024947B2 (en) | Rendering and navigating photographic panoramas with depth information in a geographic information system | |
US8686995B2 (en) | Path planning for street level navigation in a three-dimensional environment, and applications thereof | |
JP5861150B2 (en) | Image information output method | |
US8390617B1 (en) | Visualizing oblique images | |
EP2732436B1 (en) | Simulating three-dimensional features | |
US20090289937A1 (en) | Multi-scale navigational visualtization | |
US20140267273A1 (en) | System and method for overlaying two-dimensional map elements over terrain geometry | |
JP2014504384A (en) | Generation of 3D virtual tour from 2D images | |
US20150154798A1 (en) | Visual Transitions for Photo Tours Between Imagery in a 3D Space | |
US20110242271A1 (en) | Synthesizing Panoramic Three-Dimensional Images | |
JP2011138258A (en) | View reproduction system | |
US8884950B1 (en) | Pose data via user interaction | |
JP2011243076A (en) | Object management image generation device and object management image generation program | |
JP2008219390A (en) | Image reader | |
CN114359498A (en) | Map display method, device, equipment and computer program product | |
Huang et al. | Innovative Application and Improvement of Panoramic Digital Technology in Indoor Display Scenes |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
A621 | Written request for application examination |
Free format text: JAPANESE INTERMEDIATE CODE: A621Effective date: 20141105 |
|
A977 | Report on retrieval |
Free format text: JAPANESE INTERMEDIATE CODE: A971007Effective date: 20150918 |
|
RD03 | Notification of appointment of power of attorney |
Free format text: JAPANESE INTERMEDIATE CODE: A7423Effective date: 20151007 |
|
A131 | Notification of reasons for refusal |
Free format text: JAPANESE INTERMEDIATE CODE: A131Effective date: 20151009 |
|
RD04 | Notification of resignation of power of attorney |
Free format text: JAPANESE INTERMEDIATE CODE: A7424Effective date: 20151019 |
|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20160104 |
|
TRDD | Decision of grant or rejection written | ||
A01 | Written decision to grant a patent or to grant a registration (utility model) |
Free format text: JAPANESE INTERMEDIATE CODE: A01Effective date: 20160208 |
|
A61 | First payment of annual fees (during grant procedure) |
Free format text: JAPANESE INTERMEDIATE CODE: A61Effective date: 20160307 |
|
R150 | Certificate of patent or registration of utility model |
Ref document number: 5899232Country of ref document: JPFree format text: JAPANESE INTERMEDIATE CODE: R150 |
|
S533 | Written request for registration of change of name |
Free format text: JAPANESE INTERMEDIATE CODE: R313533 |
|
R350 | Written notification of registration of transfer |
Free format text: JAPANESE INTERMEDIATE CODE: R350 |
|
R250 | Receipt of annual fees |
Free format text: JAPANESE INTERMEDIATE CODE: R250 |
|
R250 | Receipt of annual fees |
Free format text: JAPANESE INTERMEDIATE CODE: R250 |
|
R250 | Receipt of annual fees |
Free format text: JAPANESE INTERMEDIATE CODE: R250 |
|
R250 | Receipt of annual fees |
Free format text: JAPANESE INTERMEDIATE CODE: R250 |
|
R250 | Receipt of annual fees |
Free format text: JAPANESE INTERMEDIATE CODE: R250 |
|
R250 | Receipt of annual fees |
Free format text: JAPANESE INTERMEDIATE CODE: R250 |