CN110692241A - Diversified motion using multiple global motion models - Google Patents
Diversified motion using multiple global motion models Download PDFInfo
- Publication number
- CN110692241A CN110692241A CN201880035987.7A CN201880035987A CN110692241A CN 110692241 A CN110692241 A CN 110692241A CN 201880035987 A CN201880035987 A CN 201880035987A CN 110692241 A CN110692241 A CN 110692241A
- Authority
- CN
- China
- Prior art keywords
- motion
- current frame
- frame
- motion model
- block
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/50—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding
- H04N19/503—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding involving temporal prediction
- H04N19/51—Motion estimation or motion compensation
- H04N19/527—Global motion vector estimation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/20—Analysis of motion
- G06T7/246—Analysis of motion using feature-based methods, e.g. the tracking of corners or segments
- G06T7/251—Analysis of motion using feature-based methods, e.g. the tracking of corners or segments involving models
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/102—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or selection affected or controlled by the adaptive coding
- H04N19/103—Selection of coding mode or of prediction mode
- H04N19/109—Selection of coding mode or of prediction mode among a plurality of temporal predictive coding modes
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/102—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or selection affected or controlled by the adaptive coding
- H04N19/119—Adaptive subdivision aspects, e.g. subdivision of a picture into rectangular or non-rectangular coding blocks
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/102—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or selection affected or controlled by the adaptive coding
- H04N19/124—Quantisation
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/134—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or criterion affecting or controlling the adaptive coding
- H04N19/136—Incoming video signal characteristics or properties
- H04N19/137—Motion inside a coding unit, e.g. average field, frame or block difference
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/134—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or criterion affecting or controlling the adaptive coding
- H04N19/146—Data rate or code amount at the encoder output
- H04N19/147—Data rate or code amount at the encoder output according to rate distortion criteria
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/134—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or criterion affecting or controlling the adaptive coding
- H04N19/157—Assigned coding mode, i.e. the coding mode being predefined or preselected to be further used for selection of another element or parameter
- H04N19/159—Prediction type, e.g. intra-frame, inter-frame or bidirectional frame prediction
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/134—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or criterion affecting or controlling the adaptive coding
- H04N19/167—Position within a video image, e.g. region of interest [ROI]
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/169—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding
- H04N19/17—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being an image region, e.g. an object
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/169—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding
- H04N19/17—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being an image region, e.g. an object
- H04N19/172—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being an image region, e.g. an object the region being a picture, frame or field
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/169—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding
- H04N19/17—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being an image region, e.g. an object
- H04N19/176—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being an image region, e.g. an object the region being a block, e.g. a macroblock
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/189—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the adaptation method, adaptation tool or adaptation type used for the adaptive coding
- H04N19/192—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the adaptation method, adaptation tool or adaptation type used for the adaptive coding the adaptation method, adaptation tool or adaptation type being iterative or recursive
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/44—Decoders specially adapted therefor, e.g. video decoders which are asymmetric with respect to the encoder
- H04N19/45—Decoders specially adapted therefor, e.g. video decoders which are asymmetric with respect to the encoder performing compensation of the inverse transform mismatch, e.g. Inverse Discrete Cosine Transform [IDCT] mismatch
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/50—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding
- H04N19/503—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding involving temporal prediction
- H04N19/51—Motion estimation or motion compensation
- H04N19/537—Motion estimation other than block-based
- H04N19/543—Motion estimation other than block-based using regions
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/50—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding
- H04N19/503—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding involving temporal prediction
- H04N19/51—Motion estimation or motion compensation
- H04N19/573—Motion compensation with multiple frame prediction using two or more reference frames in a given prediction direction
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/134—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or criterion affecting or controlling the adaptive coding
- H04N19/136—Incoming video signal characteristics or properties
- H04N19/14—Coding unit complexity, e.g. amount of activity or edge presence estimation
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/50—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding
- H04N19/503—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding involving temporal prediction
- H04N19/51—Motion estimation or motion compensation
- H04N19/557—Motion estimation characterised by stopping computation or iteration based on certain criteria, e.g. error magnitude being too large or early exit
Abstract
A method of encoding a current frame of video includes jointly determining respective motion models for reference frames, and encoding the current frame using the respective motion models. The reference frame is used to encode the current frame. Jointly determining the respective motion models for the reference frames comprises determining respective aggregate residuals for combinations of candidate motion models, and selecting the combination of candidate motion models corresponding to the smallest aggregate residual. The respective motion models correspond to the candidate motion models of the selected combination.
Description
Background
A digital video stream may represent video using a sequence of frames or still images. Digital video may be used for a variety of applications including, for example, video conferencing, high definition video entertainment, video advertising, or sharing of user-generated video. Digital video streams may contain large amounts of data and consume a large amount of computing or communication resources of a computing device for processing, transmission, or storage of video data. Various methods have been proposed for reducing the amount of data in a video stream, including compression and other encoding techniques.
Motion estimation and compensation based coding may be performed by decomposing a frame or picture into blocks predicted based on one or more prediction blocks of a reference frame. The difference (i.e., the residual) between the block and the predicted block is compressed and encoded in the bitstream. The decoder uses the difference and the reference frame to reconstruct the frame or picture.
Disclosure of Invention
A method for encoding a current frame of a video includes jointly determining respective motion models for reference frames, and encoding the current frame using the respective motion models. The reference frame is used to encode the current frame. Jointly determining the respective motion models for the reference frames comprises determining respective aggregate residuals for combinations of candidate motion models, and selecting the combination of candidate motion models corresponding to the smallest aggregate residual. The respective motion models correspond to the candidate motion models of the selected combination.
In this method and other embodiments taught herein, an aggregate residual for a combination of candidate motion models may refer to a combination (e.g., sum or weighted sum) of one or more (e.g., all) residuals, each generated by predicting a respective block of the current frame using a respective candidate motion model from the combination. The aggregate residuals may be generated by predicting the frame using a combination of the candidate motion models, such that the respective candidate motion models may be used to predict the block of the current frame (and generate the respective residuals).
According to another aspect of the disclosure, an apparatus for encoding a current frame of video includes a processor and a non-transitory storage medium. The processor is configured to execute instructions stored in the non-transitory storage medium to jointly determine motion models for the reference frames, such that a respective motion model is determined for the respective reference frames, and the current frame is encoded using the respective motion model.
A method for decoding a current frame includes decoding respective motion models for reference frames of the current frame from an encoded bitstream; and in response to determining to encode the current block of the current frame using at least one of the respective motion models, decode the current block using the at least one of the respective motion models. The respective motion models are jointly determined by the encoder.
An apparatus for decoding a current frame of video according to another aspect of the present disclosure includes a processor and a non-transitory storage medium. The processor is configured to execute instructions stored in the non-transitory storage medium to decode respective motion models for reference frames of a current frame from the encoded bitstream, wherein the encoder jointly determines the respective motion models; and in response to determining to encode the current block of the current frame using at least one of the respective motion models, decode the current block using the at least one of the respective motion models.
These and other aspects of the disclosure are disclosed in the following detailed description of the embodiments, the appended claims and the accompanying drawings.
Drawings
The description herein makes reference to the accompanying drawings described below wherein like reference numerals refer to like parts throughout the several views.
Fig. 1 is a schematic diagram of a video encoding and decoding system.
Fig. 2 is a block diagram of an example of a computing device that may implement a transmitting station or a receiving station.
Fig. 3 is a diagram of a video stream to be encoded and then decoded.
Fig. 4 is a block diagram of an encoder according to an embodiment of the present disclosure.
Fig. 5 is a block diagram of a decoder according to an embodiment of the present disclosure.
Fig. 6 is a flowchart of a process for encoding a current block using a segmentation-based parameterized motion model according to an embodiment of the present disclosure.
Fig. 7 is a diagram of frame segmentation according to an embodiment of the present disclosure.
Fig. 8 is an illustration of an example of motion within a video frame according to an embodiment of the disclosure.
Fig. 9A-D are illustrations of an example of warping pixels of a block of a video frame according to a parameterized motion model, according to an embodiment of the present disclosure.
Fig. 10 is a flowchart of a process for decoding a current block using a segmentation-based parameterized motion model, according to an embodiment of the present disclosure.
Fig. 11 is an example of global motion according to an embodiment of the present disclosure.
Fig. 12 is an example of encoding a current frame using global motion according to an embodiment of the present disclosure.
Fig. 13 is an example of a diversified global local motion according to an embodiment of the present disclosure.
Fig. 14 is a flowchart of a process for encoding a current frame of video using diverse motion, according to an embodiment of the present disclosure.
FIG. 15 is a flow chart of a process for jointly determining a motion model according to an embodiment of the present disclosure.
Fig. 16 is an example of a combination of motion models according to an embodiment of the present disclosure.
FIG. 17 is a flow chart of a process for jointly determining a motion model according to another embodiment of the present disclosure.
FIG. 18 is a flow chart of a process for jointly determining a motion model according to yet another embodiment of the present disclosure.
Fig. 19 is a flowchart of a process for decoding a current frame according to one embodiment of the present disclosure.
Detailed Description
As described above, compression schemes related to encoding video streams may include decomposing an image into blocks; and generating a digital video output bitstream (i.e., an encoded bitstream) using one or more techniques to limit the information included in the output bitstream. The received bitstream may be decoded to recreate the block and source images from limited information. Encoding a video stream or a portion thereof (e.g., a frame or a block) may include using temporal or spatial similarities in the video stream to improve coding efficiency. For example, a current block of a video stream may be encoded based on identifying differences (residuals) between previously encoded pixel values, or combinations of previously encoded pixel values, and those in the current block.
Encoding using spatial similarity may be referred to as intra prediction. Intra-prediction attempts to predict the pixel values of a block in a frame of a video stream using pixels surrounding the block, that is, pixels in the same frame as the block but outside the block.
Encoding using temporal similarity may be referred to as inter prediction. Inter-prediction attempts to predict pixel values of a block of a current frame using one or more blocks that may be displaced from one or more reference frames. A reference frame is a frame (i.e., a picture) that occurs earlier or later in time in the video stream than the current frame. A reference frame occurring later in time than the current frame may be received by the decoder before the current frame. For example, the compressed bitstream 420 of fig. 5 may be organized such that temporally later reference frames are included before the current frame. Inter prediction may be performed using motion vectors representing translational motion, i.e., pixel offsets in the x-axis and y-axis of a predicted block in a reference frame compared to the block being predicted. Some codecs use up to eight reference frames that can be stored in a frame buffer. The motion vector may reference (i.e., use) one of the reference frames of the frame buffer.
Two predictor blocks may be combined to form a composite predictor for a block or region of a video picture. A composite predictor may be created by combining two or more predictors determined using, for example, the prediction methods described above (i.e., inter and/or intra prediction). For example, the composite predictor may be a combination of the first predictor and the second predictor, which may be two intra predictors (i.e., intra + intra), an intra predictor and an inter predictor (i.e., intra + inter), or two inter predictors (i.e., inter + inter).
The above-described motion compensated (referred to herein as translational motion compensation or translational motion) video compression and decompression methods assume pure translational motion between blocks. The translational motion compensation model is performed as a rectangular transformation. The translational motion determined at the block level is referred to herein as "conventional motion compensation".
However, not all motion within a block may be described and/or efficiently described using a translational motion model of a reference block relative to a reference frame. For example, some motions may include zoom, shear, or rotational motions, which may be separate motions or motions in conjunction with translational motions. This motion may be due to, for example, camera motion and may be appropriate for all or at least many blocks of a frame. Thus, the motion is "global" to the frame. As mentioned and further described below, the global motion itself may be a translational motion. As such, predicting a block of a current frame using a translational global motion model may result in better performance (e.g., improved compression) than using local translational motion at the block level (i.e., conventional motion compensation). When encoding a block using inter prediction, global motion may be used to generate a reference block. Alternatively, a translation motion vector found by a motion search may be used.
The global motion may be represented by a "parameterized motion model" or a "motion model". A single motion model for each reference frame may not accurately predict all potential motion for the frame. For example, a single motion model for a reference frame performs well in terms of rate-distortion optimization for video with consistent motion. However, a video frame may include two or more moving segments made up of a set of blocks of the video frame. These segments may include, for example, one or more foreground objects moving in different directions and a background moving in another direction. In particular, for example, a video with strong disparity may not be able to obtain a consistent gain by using a single motion model.
Embodiments of the present disclosure describe using multiple motion models per reference frame. For several reference frames, the current video frame may be segmented with respect to the reference frames, and a parametric motion model may be identified for these segments. Each of the parameterized motion models associated with a segment corresponds to a motion model type. The "segment" used in connection with global motion, as detailed in the description of fig. 6-19, is distinguished from the "segment" used with reference to fig. 3 below. A "slice" relating to global motion is a collection of blocks in the current frame that may or may not be contiguous.
In some cases, whether or not one or more motion models are identified for one or more reference frames, the reference frames alone may not correctly describe global motion in the current frame. As such, it is advantageous to identify global motion jointly rather than independently (i.e., on a per-reference-frame basis). For example, if the global motion associated with a first reference frame accurately describes a first portion of a current frame (e.g., the background of the current frame), then the global motion associated with a second reference frame and/or a third reference frame may be used to describe (i.e., estimate) the global motion of other portions of the current frame. That is, in determining the motion model of the second and/or third reference frames, the first portion of the current frame may be ignored and a motion model appropriate for the other portion (e.g., the foreground of the current frame) may be derived. By jointly inferring global motion across at least some reference frames available for encoding the current frame, errors associated with the residuals may be reduced. Jointly identified (e.g., inferred, calculated, computed, etc.) global motion models are referred to herein as diversified motion models.
Further details of techniques for encoding and decoding a current block of a video frame using a segmentation-based parameterized motion model, and jointly inferring more details of global motion (i.e., diverse motion) available for encoding a current frame on at least some reference frames, are described herein first with reference to a system in which the techniques may be implemented.
Fig. 1 is a schematic diagram of a video encoding and decoding system 100. Transmitting station 102 may be, for example, a computer having an internal configuration such as the hardware depicted in fig. 2. However, other suitable implementations of transmitting station 102 are possible. For example, the processing of transmitting station 102 may be distributed among multiple devices.
In one example, the receiving station 106 may be a computer having an internal configuration such as the hardware described in fig. 2. However, other suitable implementations of the receiving station 106 are possible. For example, the processing of the receiving station 106 may be distributed among multiple devices.
Other implementations of the video encoding and decoding system 100 are possible. For example, embodiments may omit network 104. In another embodiment, the video stream may be encoded and then stored for later transmission to the receiving station 106 or any other device having memory. In one implementation, the receiving station 106 receives an encoded video stream (e.g., via the network 104, a computer bus, and/or some communication path) and stores the video stream for later decoding. In an exemplary embodiment, the real-time transport protocol (RTP) is used to transmit the encoded video over the network 104. In another embodiment, transport protocols other than RTP may be used, such as the hypertext transfer protocol (HTTP) -based video streaming protocol.
When used in a videoconferencing system, for example, transmitter station 102 and/or receiving station 106 may include both the ability to encode and decode video streams as described below. For example, receiving station 106 may be a video conference participant that receives an encoded video bitstream from a video conference server (e.g., transmitting station 102) for decoding and viewing, and further encodes and sends his or her own video bitstream to the video conference server for decoding and viewing by other participants.
Fig. 2 is a block diagram of an example of a computing device 200 that may implement a transmitting station or a receiving station. For example, computing device 200 may implement one or both of transmitting station 102 and receiving station 106 of fig. 1. Computing device 200 may be in the form of a computing system including multiple computing devices, or in the form of one computing device, such as a mobile phone, a tablet computer, a laptop computer, a notebook computer, a desktop computer, and so forth.
The CPU202 in the computing device 200 may be a conventional central processing unit. Alternatively, the CPU202 may be any other type of device or devices now existing or later developed that is capable of manipulating or processing information. Although the disclosed embodiments may be implemented with one processor (e.g., CPU202) as shown, more than one processor may be used to achieve speed and efficiency advantages.
The memory 204 in the computing device 200 may be a Read Only Memory (ROM) device or a Random Access Memory (RAM) device in one implementation. Any other suitable type of storage device may be used as memory 204. The memory 204 may include code and data 206 that are accessed by the CPU202 using the bus 212. The memory 204 may further include an operating system 208 and application programs 210, the application programs 210 including at least one program that allows the CPU202 to perform the methods described herein. For example, the application 210 may include applications 1 through N, which further include a video coding application that performs the methods described herein. Computing device 200 may also include secondary storage 214, which may be, for example, a memory card for use with a mobile computing device. Because video communication sessions may contain a large amount of information, they may be stored in whole or in part in secondary storage 214 and loaded into memory 204 for processing as needed.
Although fig. 2 illustrates the CPU202 and memory 204 of the computing device 200 as being integrated into one unit, other configurations may be used. The operations of CPU202 may be distributed across multiple machines (where a single machine may have one or more processors), which may be directly coupled or coupled across a local area network or other network. Memory 204 may be distributed across multiple machines, such as a network-based memory or memory in multiple machines that perform operations for computing device 200. Although illustrated as a single bus, the bus 212 of the computing device 200 may be comprised of multiple buses. Further, secondary storage 214 may be directly coupled to other components of computing device 200, or may be accessed via a network, and may comprise an integrated unit such as a memory card or multiple units such as multiple memory cards. Thus, computing device 200 may be implemented in a wide variety of configurations.
Fig. 3 is a diagram of an example of a video stream 300 to be encoded and then decoded. The video stream 300 includes a video sequence 302. At the next level, the video sequence 302 includes a plurality of adjacent frames 304. Although three frames are illustrated as adjacent frames 304, the video sequence 302 may include any number of adjacent frames 304. The adjacent frames 304 may then be further subdivided into individual frames, such as frame 306. At the next level, the frame 306 may be divided into a series of planes or slices 308. For example, the segments 308 may be a subset of frames that allow parallel processing. The segment 308 may also be a subset of a frame that separates the video data into individual colors. For example, a frame 306 of color video data may include a luminance plane and two chrominance planes. The segments 308 may be sampled at different resolutions.
Regardless of whether frame 306 is divided into segments 308, frame 306 may be further subdivided into blocks 310, and blocks 310 may contain data corresponding to, for example, 16 x 16 pixels in frame 306. The block 310 may also be arranged to include data from one or more segments 308 of pixel data. The block 310 may also be any other suitable size, such as 4 × 4 pixels, 8 × 8 pixels, 16 × 8 pixels, 8 × 16 pixels, 64 × 64 pixels, 128 × 128 pixels, or larger. The terms "block" and "macroblock" may be used interchangeably herein, unless otherwise specified.
Fig. 4 is a block diagram of an encoder 400 according to an embodiment of the present disclosure. As described above, encoder 400 may be implemented in transmitting station 102, such as by providing a computer software program stored in a memory, such as memory 204. The computer software program may include machine readable instructions that when executed by a processor, such as CPU202, cause transmitting station 102 to encode video data in the manner described in fig. 4. Encoder 400 may also be implemented as dedicated hardware included, for example, in transmitting station 102. In a particularly desirable embodiment, encoder 400 is a hardware encoder.
The encoder 400 has the following stages for performing various functions in the forward path (shown by the solid connecting lines) to generate an encoded or compressed bitstream 420 using the video stream 300 as input: an intra/inter prediction stage 402, a transform stage 404, a quantization stage 406, and an entropy coding stage 408. The encoder 400 may also include a reconstruction path (shown by dotted connecting lines) to reconstruct the frame used to encode the future blocks. In fig. 4, the encoder 400 has the following stages for performing various functions in the reconstruction path: an inverse quantization stage 410, an inverse transform stage 412, a reconstruction stage 414, and a loop filtering stage 416. Other structural changes of the encoder 400 may be used to encode the video stream 300.
When the video stream 300 is presented for encoding, respective adjacent frames 304, such as frame 306, may be processed in units of blocks. In the intra/inter prediction stage 402, the respective blocks may be encoded using intra prediction (also referred to as intra-prediction) or inter prediction (also referred to as inter-prediction). In any case, a prediction block may be formed. In the case of intra prediction, a prediction block may be formed from samples in the current frame that have been previously encoded and reconstructed. In the case of inter prediction, a prediction block may be formed from samples in one or more previously constructed reference frames. Embodiments for forming a prediction block are discussed below with reference to fig. 6, 7 and 8, for example, using a parameterized motion model identified for encoding a current block of a video frame.
Next, still referring to fig. 4, at the intra/inter prediction stage 402, the prediction block may be subtracted from the current block to generate a residual block (also referred to as a residual). The transform stage 404 transforms the residual into, for example, transform coefficients in the frequency domain using a block-based transform. The quantization stage 406 converts the transform coefficients into discrete quantum values, referred to as quantized transform coefficients, using quantizer values or quantization levels. For example, the transform coefficients may be divided by the quantizer value and truncated. The quantized transform coefficients are then entropy encoded by entropy encoding stage 408. The entropy coded coefficients are then output to the compressed bitstream 420 along with other information used to decode the block (which may include, for example, the type of prediction used, the transform type, the motion vector, and the quantizer value). The compressed bitstream 420 may be formatted using various techniques, such as Variable Length Coding (VLC) or arithmetic coding. The compressed bitstream 420 may also be referred to as an encoded video stream or an encoded video bitstream, and these terms may be used interchangeably herein.
The reconstruction path in fig. 4 (shown by the dashed connecting lines) may be used to ensure that the encoder 400 and decoder 500 (described below) use the same reference frame to decode the compressed bitstream 420. The reconstruction path performs functions similar to those occurring during the decoding process (described below), including inverse quantization of the quantized transform coefficients in an inverse quantization stage 410, and inverse transformation of the inverse quantized transform coefficients in an inverse transformation stage 412 to generate a block of derived residuals (also referred to as derived residuals). In the reconstruction stage 414, the prediction block predicted in the intra/inter prediction stage 402 may be added to the derived residuals to create a reconstructed block. Loop filtering stage 416 may be applied to the reconstructed block to reduce distortion such as blocking artifacts.
Other variations of the encoder 400 may be used to encode the compressed bitstream 420. For example, the non-transform based encoder 400 may quantize the residual signal directly without the transform stage 404 for certain blocks or frames. In another embodiment, the encoder may have the quantization stage 406 and the inverse quantization stage 410 combined into a common stage.
Fig. 5 is a block diagram of a decoder 500 according to an embodiment of the present disclosure. The decoder 500 may be implemented in the receiving station 106, for example, by providing a computer software program stored in the memory 204. The computer software program may include machine readable instructions that, when executed by a processor such as CPU202, cause receiving station 106 to decode video data in the manner described in fig. 5. Decoder 500 may also be implemented in hardware included in, for example, transmitting station 102 or receiving station 106.
Similar to the reconstruction path of the encoder 400 discussed above, the decoder 500 in one example includes the following stages that perform various functions to generate an output video stream 516 from the compressed bitstream 420: entropy decoding stage 502, inverse quantization stage 504, inverse transform stage 506, intra/inter prediction stage 508, reconstruction stage 510, loop filtering stage 512, and post filtering stage 514. Other structural variations of the decoder 500 may be used to decode the compressed bitstream 420.
When the compressed bitstream 420 is presented for decoding, data elements within the compressed bitstream 420 may be decoded by the entropy decoding stage 502 to generate a set of quantized transform coefficients. Inverse quantization stage 504 inverse quantizes the quantized transform coefficients (e.g., by multiplying the quantized transform coefficients by quantizer values), and inverse transform stage 506 inverse transforms the inverse quantized transform coefficients to generate derivative residuals that may be the same as the derivative residuals created by inverse transform stage 412 in encoder 400. Using header information decoded from the compressed bitstream 420, the decoder 500 may use the intra/inter prediction stage 508 to create the same prediction block as was created in the encoder 400, e.g., at the intra/inter prediction stage 402. In the reconstruction stage 510, the prediction block may be added to the derived residual to create a reconstructed block. Loop filtering stage 512 may be applied to the reconstructed block to reduce blocking artifacts.
Other filtering may be applied to the reconstructed block. In an example, the post-filtering stage 514 can include a deblocking filter applied to the reconstructed block to reduce block distortion, and the result is output as the output video stream 516. The output video stream 516 may also be referred to as a decoded video stream, and these terms may be used interchangeably herein. Other variations of the decoder 500 may be used to decode the compressed bitstream 420. For example, the decoder 500 may generate the output video stream 516 without the post-filtering stage 514.
FIG. 6 is a flow diagram of a process 600 for encoding a current block using a segment-based parameterized motion model, according to an embodiment of the present disclosure. Process 600 may be implemented in an encoder, such as encoder 400 of fig. 4.
Process 600 may be implemented, for example, as a software program executable by a computing device, such as transmitting station 102. The software programs may include machine-readable instructions (e.g., executable instructions) that are stored in a memory, such as memory 204 or secondary storage 214, and that are executable by a processor, such as CPU202, to cause a computing device to perform process 600. In at least some embodiments, the process 600 may be performed in whole or in part by the intra/inter prediction stage 402 of the encoder 400 of fig. 4.
Process 600 may be implemented using dedicated hardware or firmware. Some computing devices may have multiple memories, multiple processors, or both. Different processors, memories, or both may be used to distribute the steps or operations of process 600. The use of the terms "processor" or "memory" in the singular encompasses computing devices having one processor or one memory and devices having multiple processors or multiple memories used in performing some or all of the described steps or operations.
Process 600 is described with reference to fig. 7. Fig. 7 is a diagram 700 of frame segmentation according to an embodiment of the present disclosure. Fig. 7 includes a current frame 701. Reference frames, such as reference frame 704 and reference frame 706, in frame buffer 702 may be used to encode blocks of current frame 701. The current frame 701 includes the head and shoulders of the person 720 and other background objects.
At 602, the process 600 segments a video frame relative to a reference frame, resulting in a segmentation. The process 600 may segment a video frame with respect to more than one reference frame. A segment may include one or more segments. The segment includes a segment containing the current block and a parameterized motion model for the segment. Fig. 7 depicts three fragments: segment 722, depicted by one set of shaded blocks, segment 718, depicted by another set of differently shaded blocks, and segment 714, which includes the remaining blocks of the frame that make up the background of the frame. Segment 722 includes current block 716.
For each of at least some of the reference frames of the frame buffer 702, the process 600 may segment the current frame 701. Process 600 may use an image segmentation technique that exploits the motion of objects between a reference frame and a current frame. A parametric motion model is then associated with each segment, as further described with reference to fig. 8.
Image segmentation may be performed using the points of interest to generate a parameterized motion model. For example, the process 600 may determine a first point of interest in a reference frame (e.g., reference frame 704), and a second point of interest in the current frame 701. The first point of interest and the second point of interest may be determined using a speeded-up segmentation test Features (FAST) algorithm. The first point of interest and the second point of interest are then matched. The process 600 may use the matched points of interest to determine a parameterized motion model for the matched points of interest.
The process 600 may use a random sample consensus (RANSAC) method to fit a model (i.e., a parameterized motion model) to the match points. RANSAC is an iterative algorithm that can be used to estimate model parameters (i.e., parameters of a parametric motion model) from data containing inliers and outliers. Inliers are data points (i.e., pixels) of the current frame that fit into the parameterized motion model. The process 600 may determine segments based on inliers. That is, the process 600 may include inliers in a segment. The inlier-based segment (referred to as a foreground segment) may correspond to motion in the current frame corresponding to a foreground object. However, this need not be the case. That is, the foreground segment may include background objects or blocks. The foreground segment may not include all foreground objects or blocks.
Outliers are data points (i.e., pixels) that are not suitable for the current frame of the parameterized motion model. Process 600 may determine the second segment based on the outlier. The outlier-based segment (referred to as a background segment) may correspond to a relatively static background object of the current frame. However, this need not be the case. Alternatively, instead of determining the second segment based on the outlier, the process 600 may use the outlier to determine additional segments. For example, the process 600 may recursively apply the same process as described above to determine additional segments. For example, the process 600 determines the three segments 714, 718, and 722 by applying the process described above to the current frame 701 and using the reference frame 704. In the case where, for example, the shoulders of person 720 move in one direction relative to the reference frame and the head moves in another direction, two segments 718 and 722 may be identified for person 720.
The process 600 may determine a parameterized motion model based on the motion model type (e.g., using RANSAC). For example, the RANSAC algorithm may determine a parameterized motion model based on the type of motion model provided by the process 600. Different motion model types may be used. The available motion model types include translational motion model types, similarity motion model types, affine motion model types, and homographic motion model types of increasing complexity. Additional or fewer motion model types may also be used. With further reference to fig. 9A-9D, some motion model types are illustrated.
In some cases, the parameterized motion model determined by the RANSAC method may contain more parameters than are necessary to provide a good approximation of global motion (e.g., relative to a measure of error) for the segment. For example, requesting an affine model from RANSAC may return a six-parameter model (as described with reference to fig. 9A-9D), even though a four-parameter model is sufficient to provide a good approximation of the segment. As such, the process 600 may iteratively evaluate the available model types starting from the simplest model type (e.g., translational motion model type) to the most complex model (e.g., homographic motion model type). If it is determined that the lower complexity model yields a measure of error within a predetermined threshold, a parameterized motion model corresponding to the lower complexity model is determined as the parameterized motion model for the segment.
In an embodiment, an error advantage (error advantage) associated with a model type may be used as the error metric. The error advantage E can be defined as:
E＝∑α|cxy-wxy|0.6
in the above equation, α is a weight value, cxyIs the pixel at (x, y) in the current frame, wxyIs the pixel at (x, y) in the warped frame. If the model type yields an error advantage E below a predetermined threshold, a parameterized motion model corresponding to the model type is associated with the segment. If no model type yields an error advantage E below a predetermined threshold, then a translational motion model type may be assumed for the segment.
In an embodiment, the process 600 does not evaluate a homographic motion model type; instead, the process 600 stops at the similarity motion model type. This is done to reduce decoder complexity.
The process 600 may segment the current frame relative to (or based on) each reference frame of the frame buffer 702. In FIG. 7, frame buffer 702 includes eight (8) reference frames. Assuming that the process 600 determines two (2) segments (i.e., foreground and background segments) for each reference frame, the segmentation results in a total of 16 segments. Each of the 16 segments corresponds to a respective parameterized motion model, resulting in 16 parameterized motion models.
As further explained with reference to fig. 8, a prediction block for the current block is determined based on available segments of the reference frame that contain the current block. Likewise, if an encoder, such as encoder 400 of fig. 4, determines a prediction block using 16 parameterized motion models, then a decoder (such as decoder 500 of fig. 5) also reconstructs the current block using 16 parameterized motion models. As such, 16 parametric motion models are encoded in an encoded bitstream (such as bitstream 420 generated by encoder 400 and received by decoder 500).
The parameters encoding e.g. 16 parametric motion models in the encoded bitstream may be stronger than the prediction gain of the segment-based parametric motion model. As such, the process 600 may determine a subset of the frame buffer's reference frames, resulting in a best fit for a particular fragment. For a segment, multiple reference frames (e.g., three frames) are selected, and a parametric motion model is determined relative to the frames and encoded in the encoded bitstream. For example, process 600 may determine a parameterized motion model for a segment based on a golden reference frame, an alternate reference frame, and a last reference frame in a frame buffer. The golden reference frame may be a reference frame that may be used as a forward prediction frame for encoding the current frame. The last reference frame may be used as a forward predicted frame for encoding the current frame. The alternative reference frame may be used as a backward reference frame for encoding the current frame.
Encoding the parametric motion model may mean encoding the parameters of the parametric motion model in the header of the current frame being encoded. Encoding the parameterized motion model may include encoding a motion model type corresponding to the parameterized motion model.
In the case of encoding a motion model type, a decoder (such as decoder 500 of fig. 5) decodes the motion model type and determines parameters of a parameterized motion model of the motion model type in a manner similar to the encoder. To limit the complexity of the decoder, the encoder may encode motion model types that are simpler than the most complex motion model types. That is, for example, the encoder may determine a parameterized motion model for a segment using a motion model type that is not more complex than a similarity motion model type.
Referring again to fig. 7, a foreground segment 722 is obtained from the reference frame 704 (as indicated by line 708). A background segment 714 is obtained from the reference frame 706 (as shown by line 710). That is, each segment may be obtained from a different reference frame. However, this is not essential. Some slices may be obtained from the same reference frame. For example, and as shown in fig. 7, the foreground segment 722 and the background segment 714 may be obtained from the same reference frame 706 (shown by line 712 and line 710, respectively). Fragment 718, although not specifically indicated in fig. 7, may also be obtained from any reference frame of frame buffer 702.
Fig. 8 is an illustration of an example of motion within a video frame 800 according to an embodiment of the disclosure. Although not specifically indicated, it should be understood that the endpoints of the motion direction (e.g., motion 808) of fig. 8 refer to pixel locations within the reference frame. This is so because motion is described with respect to another frame, such as a reference frame. The endpoints shown may not be endpoints in the same reference frame. One or more of the blocks 802A, B, C within the video frame 800 may include a warping motion. Warping motion is motion that may not be accurately predicted using motion vectors determined via translational motion compensation (e.g., conventional motion compensation as described above). For example, the motion within one of the blocks 802A, B, C may be scaled, rotated, or otherwise moved in any number of different directions in a manner that is not entirely linear. Alternatively, the motion within one of the blocks 802A, B, C may be a translational motion that is more efficiently described using a global translational motion. As such, a parametric motion model may be used to form or generate a prediction block for encoding or decoding one of blocks 802A, B, C.
The motion within the video frame 800 may include global motion. A video frame may contain more than one global motion. Blocks exhibiting the same global motion may be grouped into segments. A segment may or may comprise contiguous and/or non-contiguous blocks.
In addition to global motion, the video frame 800 may have local motion within a portion of the video frame 800. For example, local motion is shown at 812. Local motion within the video frame 800 may be contained within one block or within multiple adjacent or non-adjacent blocks. The video frame 800 may include a plurality of different local motions.
The header 814 of the video frame 800 includes references to reference frames that may be used to encode or decode the blocks 802A-802D. References to reference frames in the header 814 may be used for the parameterized motion models associated with those reference frames. The parametric motion model corresponds to a motion model type (described later with reference to fig. 9A-D) and indicates how to warp pixels of a block (e.g., blocks 802A-802D) of the video frame 800 to generate a prediction block that can be used to encode or decode the block. The frame header 814 may include one or more parameterized motion models, each model corresponding to a segment of the video frame 800.
For example, parameterized motion model 816 corresponds to a first motion model of a first segment associated with a first reference frame. Parameterized motion model 818 corresponds to a second motion model of a second segment associated with the first reference frame. The parameterized motion model 820 corresponds to a first motion model of a first segment associated with a second reference frame. The parameterized motion model 822 corresponds to a second motion model of a second segment associated with a second reference frame. Parameterized motion model 824 corresponds to a third motion model of a third segment associated with a second reference frame. Parameterized motion model 826 corresponds to a first motion model of a first segment associated with a third reference frame. Parameterized motion model 828 corresponds to a second motion model of a second segment associated with a third reference frame.
The parameterized motion model associated with the reference frame may correspond to one or more motion model types. For example, parameterized motion model 816 and parameterized motion model 818 may correspond to a homographic motion model and an affine motion model, respectively, of the first reference frame. In some implementations, each reference frame can be associated with multiple parameterized motion models of a single motion model type. For example, parameterized motion model 816 and parameterized motion model 818 may both correspond to different homographic motion models. However, in some embodiments, the reference frame may be limited to one motion model for each motion model type. Furthermore, in some embodiments, the reference frame may be limited to a single motion model entirely. In this case, the motion model may be replaced in certain situations, such as where a new motion model results in a lower prediction error.
The parametric motion model may indicate global motion within a plurality of frames of the video sequence. As such, the parameterized motion model encoded within the frame header 814 may be used to generate prediction blocks for a plurality of blocks in a plurality of frames of the video sequence. The reference frames associated with the parameterized motion model in the frame header 814 may be selected from a reference frame buffer, such as by using bits encoded into the frame header 814. For example, the bits encoded into the header 814 may point to the virtual index position of the reference frame in the reference frame buffer.
Fig. 9A-D are illustrations of an example of warping pixels of a block of a video frame according to a parameterized motion model, according to an embodiment of the present disclosure. The parametric motion model for pixels of the block in the warped frame may correspond to a motion model type. The motion model type corresponding to the parameterized motion model may be a homographic motion model type, an affine motion model type, a similarity motion model type, or a translation motion model type. The parameterized motion model to be used may be indicated by data associated with a reference frame, such as within a frame header of the encoded bitstream.
9A-D depict different motion model types for warping patches (patch) used to project pixels of a block into a reference frame. The warped patches may be used to generate a prediction block for encoding or decoding the block. The parametric motion model indicates how to scale, rotate, or otherwise move the pixels of the block when projected into the reference frame. The data representing the pixel projections may be used to identify parameterized motion models corresponding to respective motion models. The number and function of the parameters of the parameterized motion model depend on the particular projection used.
In fig. 9A, the pixels of block 902A are projected to warped patch 904A of frame 900A using a homographic motion model. The homographic motion model uses eight parameters to project the pixels of block 902A to warped patch 904A. Homographic motion is not constrained by a linear transformation between the coordinates of the two spaces. As such, the eight parameters defining the homographic motion model may be used to project the pixels of block 902A to quadrilateral patches (e.g., warped patches 904A) within frame 900A. The homographic motion model thus supports translation, rotation, scaling, aspect ratio variation, clipping, and other non-parallelogram warping. The homographic motion between two spaces is defined as follows:
in these equations, (X, Y) and (X, Y) are the coordinates of two spaces, namely, the projected positions of the pixels within frame 900A and the original positions of the pixels within block 902A, respectively. Further, a, b, c, d, e, f, g, and h are homography parameters, and are real numbers representing the relationship between the positions of the respective pixels within the frame 900A and the block 902A. Among these parameters, a denotes a fixed scale factor along the x-axis with the scale of the y-axis kept constant, b denotes a scale factor along the x-axis proportional to the y-distance to the center point of the block, c denotes a translation along the x-axis, d denotes a scale factor along the y-axis proportional to the x-distance to the center point of the block, e denotes a fixed scale factor along the y-axis with the scale of the x-axis kept constant, f denotes a translation along the y-axis, g denotes a proportional factor ratio of the x-and y-axes according to a function of the x-axis, and h denotes a proportional factor ratio of the x-axis and the y-axis according to a function of the y-axis.
In FIG. 9B, the pixels of block 902B are projected to warped patch 904B of frame 900B using an affine motion model. The affine motion model uses six parameters to project the pixels of block 902B to warped patch 904B. Affine motion is a linear transformation between the coordinates of two spaces defined by six parameters. As such, the six parameters defining the affine motion model may be used to project the pixels of block 902B to a parallelogram patch (e.g., warped patch 904B) within frame 900B. Affine motion models thus support translation, rotation, scaling, aspect ratio change, and cropping. The affine projection between the two spaces is defined as follows:
x ═ a × X + b × Y + c; and Y ═ d × X + e × Y + f.
In these equations, (X, Y) and (X, Y) are the coordinates of two spaces, namely, the projected positions of the pixels within frame 900B and the original positions of the pixels within block 902B, respectively. In addition, a, B, c, d, e, and f are affine parameters, and are real numbers representing the relationship between the positions of the respective pixels within the frame 900B and the block 902B. Where a and d represent rotation or scaling factors along the x-axis, b and e represent rotation or scaling factors along the y-axis, and c and f represent translations along the x-axis and y-axis, respectively.
In fig. 9C, the pixels of block 902C are projected to warped patch 904C of frame 900C using the similarity motion model. The similarity motion model uses four parameters to project the pixels of block 902C to warped patch 904C. The semblance motion is a linear transformation between the coordinates of two spaces defined by four parameters. For example, the four parameters may be a translation along the x-axis, a translation along the y-axis, a rotation value, and a scaling value. As such, the four parameters defining the similarity motion model may be used to project the pixels of block 902C into square patches (e.g., warped patches 904C) within frame 900C. The similarity motion model thus supports a square-to-square transformation by rotation and scaling.
In fig. 9D, the pixels of block 902D are projected to warped patch 904D of frame 900D using the translational motion model. The translational motion model uses two parameters to project the pixels of block 902D to warped patch 904D. Translational motion is a linear transformation between the coordinates of two spaces defined by two parameters. For example, the two parameters may be translation along the x-axis and translation along the y-axis. As such, two parameters defining the translational motion model may be used to project the pixels of block 902D to a square patch (e.g., warped patch 904D) within frame 900D.
Returning again to FIG. 6, at 604, the process 600 determines a first motion vector for the current block based on the segmentation. As described above, a video frame may be divided into segments with respect to at least some reference frames. As such, the current block may be part of a number of slices, each slice corresponding to a reference frame. For at least some segments to which the current frame belongs, the process 600 determines respective motion vectors.
As described with reference to fig. 7, 8 and 9, a motion vector is generated between the current block and a reference frame selected based on a parameterized motion model associated with a segment of the current block. As such, the motion vector between the current block and the reference frame selected based on the parameterized motion model may be a reference to the parameterized motion model. That is, the motion vectors indicate the reference frame and the parameterized motion model.
A motion vector may be generated by warping pixels of the current block into warped patches within the reference frame according to the selected parameterized motion model. For example, the pixels of the current block are projected to warped patches within the reference frame. The shape and size of the warped patch onto which the pixels of the current block are projected depends on the motion model associated with the selected parametric motion model. The warped patch may be a rectangular patch or a non-rectangular patch. For example, if the parameterized motion model is a translational motion model type, the warped patch is a rectangular block of the same size as the current block. In another example, if the parametric motion model is a homographic motion model type, the warped patch may be any quadrilateral and of any size. The position of the warped patch also depends on the motion model. For example, the parameters of the parametric motion model indicate x-axis and/or y-axis translation of the warped patch. The parameters of the parametric motion model may further indicate changes in rotation, scaling, or other motion of the warped patch.
The warped patches may then be expanded using the motion vectors to generate a prediction block back to the current block. The prediction block may have a rectangular geometry for predicting the current block. For example, unwrapping the projected pixels of the warped patch after projecting the respective pixels to the warped patch of the reference frame may include projecting the warped patch to a rectangular block using the generated motion vector. The pixel location coordinates of the warped patch of the reference frame may be projected to the rectangular block based on respective coordinate translations to the rectangular block. The resulting rectangular block may be used to generate a prediction block.
At 606, the process 600 determines a second motion vector for the current block using block-level translational motion compensation (i.e., conventional motion compensation). That is, process 600 may determine the second motion vector using inter prediction as described above.
At 608, the process 600 encodes one of the first motion vector and the second motion vector corresponding to the smaller error for the current block. The smaller error may be an error corresponding to the optimal rate-distortion value. The rate distortion value is a ratio that balances the amount of distortion (i.e., video quality loss) and the rate (i.e., the number of bits) used for encoding. For each motion vector determined at 604 and 606, the process 600 may determine the motion vector corresponding to the best rate-distortion value.
The process 600 may encode the selected motion vector in an encoded bitstream. Where the selected motion vector is a segment-based motion vector (i.e., the motion vector determined at 604), process 600 may encode parameters of a parameterized motion model used to determine the motion vector. Alternatively, the process 600 may encode a motion model type corresponding to a parameterized motion model.
Fig. 10 is a flow diagram of a process 1000 for decoding a current block using a segment-based parameterized motion model, according to an embodiment of the present disclosure. Process 1000 receives an encoded bitstream, such as compressed bitstream 420 of fig. 5. Process 1000 may be performed by a decoder. For example, the process 1000 may be performed in whole or in part by the intra/inter prediction stage 508 of the decoder 500. Process 1000 may be performed in whole or in part during the reconstruction path of encoder 400 of fig. 4 (indicated by the dashed connecting lines). Embodiments of process 1000 may be performed by instructions stored in a memory, such as memory 204 of receiving station 106 or transmitting station 102, for execution by a processor, such as CPU 202.
At 1002, process 1000 identifies a parameterized motion model corresponding to a motion model type. The parameterized motion model may be identified based on information encoded in the header of the current frame being decoded (i.e., the frame header). The current frame being decoded is the frame containing the current block.
At 1004, in response to determining to encode the current block using the parameterized motion model, process 1000 decodes the current block using the parameterized motion model. The current block header may include an indication identifying that the current block was encoded using a parameterized motion model. For example, the current block header may include an indicator of a global motion model type used to encode the current block. For example, the indicator may indicate that global motion is used to encode the current block, or that no global motion is used to encode the current block (e.g., zero global motion).
In response to determining to encode the current block using the parameterized motion model, process 1000 decodes the current block using the parameterized motion model. In response to determining that the current block is not encoded using the parametric motion model, process 1000 decodes the current block using translational motion compensation.
A header for an inter-frame of a video sequence may include data indicating one or more parameterized motion models that may be used to encode or decode one or more blocks thereof. For example, the data encoded into the headers of the inter-frames may include parameters of a parameterized motion model. The data may also include coded flags indicating a plurality of parameterized motion models available for inter-frame.
In some embodiments, the reference frame may not have a parameterized motion model. For example, there may be too much different motion within the reference frame to identify global motion. In another example, the prediction error determined for warped pixels based on the motion model may not satisfy the threshold. In this case, zero motion may be used to encode or decode a block of frames using the reference frame. By default, a zero motion model may be encoded in the header of all or some of the inter-frames of the video sequence.
In some implementations, a current block encoded using a parametric motion model is decoded by warping pixels of an encoded block according to the parametric motion model. The warped pixels of the encoded block are then interpolated. For example, interpolation may be performed using a 6 tap (tap) × 6 tap sub-pixel filter. In another example, interpolation may be performed using bicubic interpolation. Bicubic interpolation may include interpolating sub-pixel values of a coded block using a 4 tap x 4 tap window. Bicubic interpolation may include applying horizontal clipping and vertical clipping to the encoded blocks.
Fig. 11 is an example 1100 of global motion according to an embodiment of the present disclosure. Example 1100 includes a current frame 1104 and a reference frame 1102. The current frame 1104 and the reference frame 1102 may be frames in a video sequence. Example 1100 illustrates a case where an object captured by a camera, but not a camera, is moving. As described above, camera motion may include translation, rotation, zooming, cropping, and the like.
Also as described above, the global motion of the reference frame for the current frame (i.e., the parameterized motion model) may be used to encode at least a segment of the current frame. The global motion may be encoded in a header of the current frame in the encoded bitstream such that the decoder may use the global motion to decode at least the segment of the current frame.
Fig. 12 is an example 1200 of encoding a current frame using global motion according to an embodiment of the present disclosure. Example 1200 includes a current frame 1202 to be encoded and reference frames REF _ 11204, REF _21206, and REF _ 31208. Current frame 1202 may be encoded using any one or any combination of reference frames 1204, 1206, and/or 1208. Example 1200 includes optical flows 1210, 1216, 1222, global motion 1212, 1218, 1224, aggregate residual 1230, and residual 1214, 1220, 1228. Although not shown in FIG. 12, current frame 1202, reference frames REF _ 11204, REF _21206, REF _ 31208, optical flows 1210, 1216, 1222, and global motion 1212, 1218, 1224 have the same size. The aggregate residual 1230 and residuals 1214, 1220, 1228 illustrate partial residual blocks that are enlarged for clarity and visualization purposes. That is, the aggregate residual 1230 and the residuals 1214, 1220, 1228 illustrate residuals associated with encoding a portion of the current frame 1202. As described further below, the optical flows 1210, 1216, 1222 show block-level motion; and global motions 1212, 1218, 1224 illustrate motions caused by global motions.
In a video sequence comprising a current frame 1202 and reference frames 1204, 1206, 1208, the foreground comprises the empire building and the background depicts other buildings. The video sequence illustrates an example of camera motion, where a portion of the frame (e.g., including the empire building's foreground) moves in one direction, while other portions of the frame (e.g., the background) move in another direction. The motion shown is small and may not be readily perceptible. As such, the reference frames may be similar. As such, the global motion model and residual may also be similar. For simplicity, the example 1200 is described with respect to one global motion model per reference frame. However, in the following description, it is understood that a plurality of global motion models are applied for each reference frame.
Similarly, optical flow 1216 describes the local motion between the current frame and reference frame REF _21206, while optical flow 1222 describes the local motion between the current frame and reference frame REF _ 31208. As illustrated, the optical flows 1210, 1216, 1222 indicate that these residuals are concentrated in the foreground of the current frame (i.e., the empire building).
As described above, global motion (also referred to as parametric motion models or motion models) may have a small number of parameters and may induce motion on each pixel of the reference frame. For example, assuming a right shift, each pixel of the reference frame may be associated with a right shift; and assuming rotation, each pixel will move by a distance given by the rotation.
Similarly, global motion 1218 illustrates global motion associated with reference frame REF _21206 relative to current frame 1202, and global motion 1224 illustrates global motion associated with reference frame REF _ 31208 relative to current frame 1202.
In the example 1200, the gray levels of the global motion 1212, 1218, 1212 are more similar to the gray levels of portions (e.g., background) of the optical flow 1210, 1216, 1222, respectively, as compared to other portions (e.g., empire building). Similarly applies to the optical flows 1210, 1216, 1222. That is, example 1200 is intended to depict that global motion describes the background of current frame 1202 more than the foreground (i.e., empire building). This is also illustrated using residuals 1214, 1220, 1228.
The resulting residual 1214 illustrates that the difference (i.e., residual) includes a foreground difference (i.e., empire building) taking into account the global motion from the reference frame REF _ 11204 to the current frame 1202. The global motion from the reference frame REF _ 11204 to the current frame 1202 may be one or more parametric motion models that make the reference frame REF _ 11204 and the current frame as similar as possible. Similarly, the residuals 1214, 1220 illustrate that the motion models corresponding to the global motions 1212, 1218, respectively, are more aligned with the background than the foreground of the current frame 1202. As described above, the error value is typically related to the residual. The error may be a mean square error between pixel values of a block of the current frame and pixel values of the prediction block. The error may be the sum of absolute difference errors. The error may be a frequency weighted error such that high frequency errors with higher coding costs are weighted higher than low frequency errors. Any other suitable error measure may be used. In example 1200, the computed residuals 1214, 1220, and 1228 have respective mean square errors of 127.721, 312.328, and 358.795, respectively.
As described above, predicting the current frame 1202 may include predicting each block of the current frame 1202 to determine an aggregate residual 1230. In a simple example, local motion (e.g., inter prediction) is ignored, and assuming that global motion is used to predict each block of the current frame 1202, each block of the current frame 1202 uses the block of the reference frames 1204, 1206, 1208 that corresponds to the smallest residual as its prediction block. As such, the first block of current frame 1202 may be predicted from reference frame REF _21206 along line 1232, the second block may be predicted from reference frame REF _ 31208 along line 1234, and the third block may be predicted from reference frame REF _ 11204 along line 1236. The aggregate residual 1230 has a corresponding calculated mean squared error 118.387.
As described above and in example 1200, the global motion model is computed independently. For example, a first parameterized motion model is determined that best matches a first portion of the first reference frame to a first portion of the current frame, a second parameterized motion model is determined that best matches a second portion of the first reference frame to a second portion of the current frame, and so on. Calculating or determining a parameterized motion model for the reference frame may answer the following questions: what is the best way to match the reference frame to the current frame? Given a set of reference frames, one or more parameterized motion models are computed (e.g., identified, etc.) for one reference frame in the set of reference frames without using the results of computing the parameterized motion models for any of the other reference frames.
In some embodiments, and as described further below, the global motion of the reference frame may be jointly computed (i.e., a parameterized motion model). As described above, the jointly computed global motion model may be referred to as a diversified global motion.
For example, if a first global motion associated with a first reference frame (e.g., reference frame REF _ 11204) accurately (or with sufficient accuracy) describes a first portion (e.g., background) of a current frame (e.g., current frame 1202), then determining global motions associated with other reference frames (e.g., reference frames REF _21206 and REF _ 31208) may ignore the first portion of the current frame and focus on determining a global motion model that best describes other portions (e.g., foreground) of the current frame. Similarly, the global motion associated with the second reference frame (e.g., reference frame REF _ 21206) may be a good descriptor for the second portion of the current frame (e.g., the first portion of the foreground), and the global motion associated with the third reference frame (e.g., reference frame REF _ 31208) may be a good descriptor for the third portion of the current frame (e.g., the second portion of the foreground).
As such, by diversifying the global motion, and as further described with reference to fig. 13, given a set of reference frames, any of the jointly determined global motions associated with the reference frames may not be a good global descriptor of the global motion of the current frame by itself. However, in general, jointly determined global motion (i.e., diverse motion) may better describe the global motion of the current frame, because each jointly determined global motion may be the best descriptor for a portion of the current frame.
Fig. 13 is an example 1300 of a diversified global motion according to an embodiment of the present disclosure. In the example 1300, elements that are the same or similar to those of FIG. 12 are denoted by the same reference numerals. Example 1300 shows, for current frame 1202, a jointly determined global motion (i.e., a diverse motion) associated with each reference frame REF _ 11204, REF _21206, REF _ 31208.
Residual 1314 is the residual resulting from predicting all blocks of current frame 1202 using global motion 1312. The Mean Square Error (MSE) of the residual 1314 is 121.045. Residual 1320 is the residual resulting from predicting all blocks of current frame 1202 using global motion 1318. The Mean Square Error (MSE) of the residual 1320 is 565.076. Residual 1328 is the residual resulting from predicting all blocks of current frame 1202 using global motion 1324. The Mean Square Error (MSE) of residual 1328 is 636.596. The lighter areas of the residuals 1314, 1320, 1328 represent higher residual values than the residuals associated with the darker areas of the residuals 1314, 1320, 1328. Global motion 1318, 1324 generates a high residual to the background, as shown by residuals 1320, 1328, respectively.
Aggregate residual 1330 is the residual resulting from predicting the current frame using the diversified global motion. That is, the aggregate residual 1330 is the residual resulting from predicting the current frame 1202 using the jointly determined global motion 1312, 1318, 1324. Jointly predicting the current frame 1202 using global motion 1312, 1318, 1324 means that for each block of the current frame 1202, the best global motion 1312, 1318, 1324 (or other prediction) is selected to predict that block, as explained above.
Using diversified global motion to predict the current frame 1202 results in a better prediction (MSE 59.309) than using independently determined global motion (MSE 118.387 of fig. 12).
That is, when not considered jointly, the global motion 1318 (MSE-565.076) associated with the reference frame REF _21206 performs worse than the global motion 1218 (MSE-312.328) of fig. 12; and the global motion 1324 associated with the reference frame REF _ 31286 (MSE-636.596) performs worse than the global motion 1224 (MSE-358.795). When considered separately, the global motion 1318, 1324 results in a poor score (i.e., high residual) for the entire current frame 1202. However, the global movements 1318, 1324 deal well with the foreground (e.g., empire building), thereby improving compression. As such, when predicting the current frame 1202, global motion 1312 may be used to predict blocks in the background, and one of global motion 1318, 1324 may be used to predict blocks in the foreground.
Fig. 14 is a flow diagram of a process 1400 for encoding a current frame of video using diverse motion according to an embodiment of the disclosure. Given a set of available reference frames for encoding the current frame, the process 1400 jointly determines respective motion models for encoding the current frame for at least a subset of the available reference frames. As described above, the determined motion model is a global motion model (i.e., a parameterized motion model). The process 1400 then encodes the block of the current frame using the jointly determined motion model.
At 1402, the process 1400 jointly determines respective motion models for reference frames that can be used to encode the current frame. As used herein, the term "determining" may refer to creating, constructing, forming, producing, generating, or determining in any way. The process 1400 may determine respective motion models for all or a subset of the available reference frames. For example, the process 1400 may determine respective motion models for three of the available reference frames. In an example, the three available reference frames may be a golden frame, an alternative reference frame, and a last reference frame. In the following, with reference to fig. 15, 16 and 18, examples are provided for jointly determining respective motion models of reference frames.
At 1404, the process encodes the current frame using the respective motion models. Encoding the current frame using the respective motion model may include encoding the current block of the current frame using a global motion model associated with the current block. The associated motion model may be as described with reference to fig. 15, 17 or 18. In some implementations, the current block may be encoded using an associated global motion model or a motion vector determined using conventional motion compensation. That is, the current block may be encoded using the associated motion model and one of the motion vectors generated by conventional motion compensation that results in a smaller residual. Encoding a current block may refer to encoding a current block in an encoded bitstream, such as compressed bitstream 420 of fig. 4.
In some embodiments, process 1400 may include additional steps or operations. For example, the process 1400 may encode the determined motion model in an encoded bitstream (such as in the header of the current frame). The process 1400 may encode the determined parameters of the motion model in an encoded bitstream. In an example, and as described with reference to fig. 16, four different candidate motion models may be calculated for each of the three reference frames. One of the four calculated candidate motion models for each reference frame may be selected for encoding the current frame. As such, the parameters of the selected candidate motion model may be encoded in the compressed bitstream. The selected candidate motion models may be such that they jointly cover the current frame. That is, each block of the current frame may be associated with one of the candidate motion models.
In some embodiments, other steps or operations not presented and described herein may be used in process 1400. Moreover, not all illustrated steps or operations may be required to implement a technique in accordance with the subject matter of this disclosure.
FIG. 15 is a flow diagram of a process 1500 for jointly determining a motion model according to an embodiment of the present disclosure. Process 1500 may be implemented at 1402 of process 1400.
The process 1500 may generate several candidate motion models for each reference frame. The process 1500 then selects the best combination of motion models. The best combination is the combination of motion models that yields the smallest aggregate residual for the current frame. In an example, the combination of motion models includes one candidate motion model for each reference frame. In another example, the combination of motion models includes zero or more candidate motion models for each reference frame. Each combination of motion models corresponds to a diverse motion model. The best combination of motion models is the diversified motion model selected for encoding the current frame.
For simplicity of illustration, the process 1500 is described with respect to three reference frames and four candidate motion models for each reference frame. However, more or fewer reference frames may be used, and more or fewer candidate motion models may be generated (e.g., identified, calculated, etc.) for each reference frame. The same or different number of candidate motion models may be generated for each reference frame. The candidate motion model is not necessarily the optimal motion model. That is, the candidate motion model does not necessarily have to result in the minimum MSE for the current frame as a whole.
At 1502, the process 1500 determines respective aggregate residuals for the combination of candidate motion models.
In an example, each of the combinations of candidate motion models consists of one respective candidate motion model for each reference frame. In another example, each of the combinations of candidate motion models consists of zero or more respective candidate motion models for each reference frame. More generally, the combination of candidate motion models may include any number of candidate motion models, and the number of candidate motion models is not limited to the number of reference frames.
Determining respective aggregate residuals for the combination of candidate motion models may include generating respective candidate motion models for the reference frame independent of the candidate motion models of other reference frames.
In some implementations of process 1500, a combination of candidate motion models that does not include one respective candidate motion model for each reference frame may be determined at 1502. For example, and using the example of fig. 16, a combination comprising two instead of three candidate motion models may be determined.
Fig. 16 is an example 1600 of a combination of motion models according to an embodiment of the present disclosure. In the example 1600, four candidate motion models, namely, candidate motion model 1602-1608, are generated for the reference frame REF _ 11204; generating four candidate motion models for the reference frame REF _21206, namely candidate motion model 1612 and 1618; and four candidate motion models, candidate motion model 1622-.
The candidate motion models may be generated in any number of ways. For example, the candidate motion models for the reference frame may each correspond to a motion model type (e.g., a translational motion model type, a similarity motion model type, an affine motion model type, and a homographic motion model type). For example, each candidate motion model may be generated using a different error advantage. For example, parametric motion models associated with different segments of the current frame may be used as candidate motion models for the reference frame. For example, the current frame may be divided into a plurality of segments, and a parametric motion model may be determined for each segment. The number of segments may correspond to the number of candidate motion models to be generated. The blocks of the current frame may be assigned to the segments in any manner. In an example, a first set of consecutive blocks (e.g., 25% of the blocks) is assigned to the first set, a next set of consecutive blocks is assigned to the second set, and so on.
In an embodiment, determining the respective aggregated residuals for the combination of candidate motion models may include generating the respective residuals for some or all of the candidate motion models of the reference frame. For example, for each of the generated candidate motion models, the process 1500 may generate a residual corresponding to predicting the current frame using the candidate motion model and the corresponding reference frame.
Fig. 16 shows the respective residuals. The residuals 1603-. The residuals 1613-1619 are residuals resulting from predicting the current frame 1202 using the candidate motion models 1612-1618, respectively. The residuals 1623-. Predicting the current frame using the candidate motion model may include dividing the current frame into fixed blocks of size N × M and predicting each block using the candidate motion model. N and M may be integer values selected from the set comprising values 4, 8, 16, 32, 64, and 128.
The process 1500 may jointly perform an exhaustive search over the reference frames (e.g., the three reference frames REF _1, REF _2, and REF _3) to determine which combination of candidate motion models minimizes the reconstruction error of the current frame. In fig. 16, because four candidate motion models are generated per reference frame, process 1500 may determine an aggregate residual for each of the combinations of 64 (i.e., 4 x 64) candidate motion models. An exemplary combination of candidate motion models includes: candidate motion models 1602, 1612, 1622, candidate motion models 1602, 1612, 1624, candidate motion models 1602, 1612, 1626, candidate motion models 1602, 1612, 1628, candidate motion models 1604, 1612, 1622, candidate motion models 1604, 1612, 1624, candidate motion models 1604, 1612, 1626, candidate motion models 1604, 1612, 1628, and the like.
As described above, in an example, a combination of candidate motion models may include zero or more candidate motion models per reference frame. As such, 220 combinations of candidate motion models are possible assuming that each combination of candidate motion models includes three candidate motion models. The 220 possible combinations correspond to selecting any three of the total 12 (i.e., the candidate motion models 1602, 1612, 1618, and 1622, 1628) available candidate motion models. An exemplary combination of candidate motion models includes: candidate motion models 1602, 1604, 1606; candidate motion models 1602, 1604, 1622; candidate motion models 1606, 1616, 1628, and so on. In an example where each combination of candidate motion models includes four candidate motion models, examples of combinations of candidate motion models include: candidate motion models 1602, 1604, 1606, 1612; candidate motion models 1602, 1604, 1606, 1608; candidate motion models 1602, 1604, 1612, 1628, and so on.
Although for ease of illustration, the present disclosure may refer to selecting a respective candidate motion model for each reference frame, it should be understood that, as described above, a combination of candidate motion models may include zero or more candidate motion models for each reference frame and/or the number of motion models in a combination of motion models is not limited to the number of reference frames.
The number of motion models for each combination may depend on the number of predetermined desired motion models for each combination. For example, the predetermined desired number may be an input or a configuration. In another example, a process for encoding a current frame of video using diverse motion (such as process 1400) may use a series of desired number of motion models. For example, given the range [3-5], the process may generate a combination of 3 motion models, a combination of 4 motion models, and a combination of 5 motion models. As described herein, the optimal combination of motion models may be used to encode a video frame. The number of combinations of motion models may also be inputs, configurations, etc.
In an embodiment, determining the respective aggregated residuals for the combination of candidate motion models may include, for the current block, determining a first residual block using the first candidate motion model, determining a second residual block using the second candidate motion model, determining a third residual block using the third candidate motion model, and adding one of the first, second, and third residual blocks corresponding to the smallest residual block to the aggregated residuals of the current frame.
For example, for a combination including the candidate motion models 1608, 1616, 1624, for each block of size 8 × 8 in the current frame, the process 1500 may select one of the (1608, reference frame REF _ 11204), (1616, reference frame REF _ 21206), (1624, reference frame REF _ 31208) pairs that provides the best prediction (i.e., the smallest residual). The minimum residual for each block is added to the combined aggregate residual of the candidate motion models. Residual 1630 is an example of an aggregate residual for a combination of candidate motion models.
As described above, the process 1500 may associate a respective aggregate residual with each combination of candidate motion models. At 1504, the process 1500 may select a combination of candidate motion models corresponding to the smallest aggregate residual. As such, each block of the current frame may be associated with a global motion model corresponding to a minimum aggregate error for the current frame.
The diverse motion described herein does not assume or rely on the fact that reference frames are similar to each other. The diverse motions according to embodiments of the present disclosure may be used with any set of reference frames.
In some embodiments, the one or more reference frames used to determine the diversified motion model for the current frame may not be in the video sequence, including a displayable frame of the current frame.
For example, if the video sequence includes an object of interest (e.g., a famous person, a landmark, etc.), a frame or image of the object of interest may be included in the video sequence as a reference image. These reference pictures are non-displayable frames because they are not themselves part of the video sequence. However, these reference pictures may be used as reference frames to provide a better prediction for those frames (or parts of frames) of the video sequence that comprise the object of interest. Such reference frames may be used to predict at least aspects of the object of interest. For example, such reference frames may be used to better predict the hairstyle (e.g., appearance) of a person (e.g., an object of interest).
In some implementations, the one or more reference frames used to determine the diversified motion model for the current frame may be frames that are temporally distant from the current frame. In general, a reference frame used for predicting a current frame is a frame temporally adjacent to the current frame. However, the reference frames used to determine the diversified motion model may be temporally distant frames.
For example, assume that a panoramic video sequence of a landscape is shot. The panoramic video sequence includes a first frame that includes an appearance of a landscape (e.g., a rock formation). The second frame taken after 10 minutes and the third frame taken after 12 minutes also included the same appearance. In a typical prediction technique, the first frame will not be used as a reference frame for predicting the second and/or third frames. However, using the diversified motion model according to the embodiments of the present disclosure, even if the first frame is temporally distant from the second and third frames, the first frame may be used as a reference frame for predicting the appearance of a landscape in the second and third frames. The first frame may be marked (e.g., identified, selected, etc.) as a reference frame for predicting the frame that includes the appearance.
In jointly determining the motion model 1500, candidate motion models for the reference frame are selected independently of candidate motion models for other reference frames. In other examples, and as further described with reference to fig. 17-18, jointly determining a motion model may use available information about candidate motion models of other reference frames to generate one or more candidate motion models of the reference frame. That is, all reference frames (e.g., candidate motion models for all reference frames) may be used simultaneously to determine the diversified motion model.
FIG. 17 is a flow diagram of a process 1700 for jointly determining a motion model according to another embodiment of the present disclosure. The process 1700 is described with respect to three reference frames. However, any number of reference frames may be used.
In an example, for each reference frame, the process 1700 assigns a respective initial motion model. In another example, zero or more initial motion models may be assigned to a reference frame. The process 1700 then iteratively refines one of the motion models by fixing the other two motion models. The process 1700 iterates until the objective function is satisfied. Depending on the semantics of the objective function, "until the objective function is satisfied" may mean until the objective function is satisfied, when the objective function is satisfied, until the objective function is not satisfied, when the objective function is not satisfied, or other semantics. The objective function may be any one or more conditions, processes, evaluations, etc. that cause the process 1700 to proceed from 1706 to the end of the current iteration. At the end of each iteration (and before the first iteration), the current diversified motion model is determined. As such, the current diversified motion model associates a global motion model with each block of the current frame.
At 1702, the process 1700 assigns respective initial motion models to the motion models of the reference frames. Given the reference frames REF _1, REF _2, and REF _3, the process 1700 may assign initial motion models M1, M2, and M3, respectively. As described above, zero or more initial motion models may be assigned to a reference frame. For example, M1 and M2 may correspond to REF _1, and M3 may correspond to REF _ 3. As such, it should be understood that the number of initial motion models is not limited by the number of reference frames. For example, the initial motion models may be M1, M2, M3, and M4, where M1 and M3 correspond to REF _1, M2 corresponds to REF _2, and M4 corresponds to REF _ 3.
The initial motion model may be any motion model generated as described above. The initial motion model need not necessarily be the best motion model associated with the corresponding reference frame. For example, the simplest model type may be used to generate the initial motion model. For example, the initial motion model may be generated ignoring the error dominance associated with the initial motion model. The combination of motion models M1, M2, M3 constitutes the current diversified motion model.
At 1704, the process 1700 determines a cost of encoding the current frame using the motion model (i.e., using the current diversified motion model). The process 1700 determines the encoding cost for each block of the current frame as the minimum cost in M1 in REF _1, M2 in REF _2, and M3 in REF _ 3. For example, the process 1700 may determine residual values (MSE) associated with encoding blocks of the current frame using the initial motion model (M1, M2, M3).
At 1706, the process 1700 determines whether the objective function is satisfied. If so, the process 1700 ends. Otherwise, the process 1700 proceeds to 1708 to perform an iteration to improve the current motion model (i.e., the current diversified motion model). That is, the process 1700 iteratively refines the motion models M1, M2, M3 (by performing 1708-1724) until an objective function is satisfied in order to reduce the cost of encoding the current frame using the current diverse motion model. When the objective function is satisfied, the current diversified motion model associates each block of the current frame with a global motion model, so that the associated motion model can be used to encode the block of the current frame (e.g., at 1404 of fig. 14). In the following, the scaling functions are further illustrated.
The motion model may be improved in any number of ways. For example, improving a motion model may refer to generating another parameterized motion model for the same block using the motion model, as described above. Another parameterized motion model may, for example, use a different type of motion model than the motion model. For example, improving the motion model may refer to generating a motion model for a subset of blocks of the current frame. For example, a subset may include all but one, two, or any number of blocks. In another example, RANSAC over all blocks of the current frame, weighted by the respective encoding costs of the blocks, may be used. That is, RANSAC may be biased towards those blocks of the current frame for which the current diversified motion model is not optimal. That is, RANSAC may be biased towards those blocks associated with the highest error.
At 1708, the process fixes the motion models M2 and M3. That is, the process 1700 uses the last determined motion models M2 and M3 without changing. As such, in the first iteration, the initial motion models assigned to motion models M2 and M3 at 1702 are used.
At 1710, the process 1700 generates a candidate motion model M 'for REF _ 1'1. In an example, four candidate motion models M 'may be generated'1(e.g., M'1,1、M'1,2、M'1,3、M'1,4). However, any number of candidate motion models may be generated.
For each candidate motion model, the process 1700 determines (not shown in fig. 17) a respective error for encoding the current frame using M2, M3, and the motion model candidate. In an example, the respective errors may be determined similar to 1502 of fig. 15. For example, determine the combination (M'1,1、M2、M3)、(M'1,2、M2、M3)、(M'1,3M2, M3) and (M'1,4M2, M3).
At 1712, process 1700 sets M1 as the best candidate motion model M'1Which yields the smallest aggregate residual, similar to 1504 of fig. 15. For example, suppose M 'is selected'1,2Then new combination M1 (set to M'1,2) M2, M3 constitute the current diversified motion model.
At 1714-'2(e.g., M'2,1、M'2,2、M'2,3、M'2,4) And setting M2 as the best candidate motion model M'2. M1 is fixed to candidate motion model M 'of 1712'1。
At 1720-'3(e.g., M'3,1、M'3,2、M'3,3、M'3,4) And setting M3 as the best candidate motion model M'3. Candidate motion model M 'with M1 fixed to 1712'1. Candidate motion model M 'with M2 fixed to 1718'2。
To summarize the operations 1708-, 1714-, 1718-, and 1720-, 1724, the process 1700 generates a first motion model candidate (e.g., M 'for the first reference frame (e.g., REF _ 2)'2,1、M'2,2、M'2,3、M'2,4) And for each first motion model candidate (e.g., M'2,1、M'2,2、M'2,3、M'2,4) Respective errors for encoding the current frame using respective motion models (e.g., M1, M3) and motion model candidates for other frames (e.g., REF _1, REF _3) are determined. The process 1700 then sets the motion model (e.g., M2) of (i.e., associated with) the first reference frame (e.g., REF _2) to the first motion model candidate (e.g., M 'corresponding to the smallest aggregate residual error)'2,1、M'2,2、M'2,3、M'2,4) One of them.
The process 1700 then returns to 1706 to determine whether the objective function is satisfied. In an example, the objective function may be a number of iterations. The number of iterations may be any number (e.g., 2, 3, 4, etc.). As such, the process 1700 ends when the process 1700 executes 1708- > 1724 for the number of iterations.
In another example, the objective function may relate to an error in encoding the current frame using a motion model. The error threshold may relate to the error associated with encoding the current frame using the motion models M1, M2, and M3 set during the iterations of 1708-1724. That is, the error threshold may relate to an error associated with encoding the current frame using the current diversified motion model. The error may be a mean square error between pixel values of the current block and pixel values of a prediction block of the reference frame. The error may be the sum of absolute difference errors. Any other suitable error measure may be used.
In an example, the objective function may be such that the error must improve by a certain error threshold from one iteration to the next. As such, if the iteration cannot improve the error by at least the error threshold, the process 1700 proceeds from 1706 to end. For example, an objective function may be considered satisfied when an iteration cannot improve the coding of a given percentage of blocks of a current frame. In other words, the objective function is such that each iteration has to improve the coding of a given percentage of blocks of the current frame. In an example, the percentage may be 1/3 blocks of the frame. In another example, a 10% block improvement is necessary. Any percentage threshold may be used.
In some cases, it may not be possible to identify (i.e., determine, generate, etc.) the best motion model for some pixels or blocks of the current frame within a reasonable number of iterations or amount of computational resources. As such, errors associated with these blocks or pixels are excluded from the error calculation because these blocks do not provide an improved metric between iterations. Thus, the robustness measure may be added to the objective function by adding a condition that excludes these blocks from the error (e.g., aggregate error) determination. Excluding some blocks from the aggregate error calculation may result in a high cost of encoding these blocks (i.e., sub-optimal diverse motion models). However, excluding such blocks prevents the situation where such blocks may skew or bias an otherwise optimal diverse motion model for the rest of the current frame.
In an example, blocks in the current frame that do not meet the block-wise error threshold are excluded from the determination of the error. In an example, the block-wise error threshold may be "in the 95 th percentile of error. That is, the blocks in the current frame associated with the 5% highest error are excluded from the determination of the error to be used in the objective function.
In another example, block errors that exceed a threshold may be excluded from the determination of errors. For example, assuming a threshold of 200, if the error (e.g., MSE) associated with predicting the block of the current frame is greater than 200, the error is not added to the aggregate residual.
FIG. 18 is a flow diagram of a process 1800 for jointly determining a motion model according to yet another embodiment of the present disclosure. Process 1800 is described with respect to three reference frames. However, any number of reference frames may be used.
The process 1800 updates the motion models M1, M2, and M3 and the blocks in the current frame that are assigned the motion models M1, M2, and M3 at the same time. As such, process 1800 optimizes two correlation quantities: motion models (which are determined based on the blocks assigned to the motion models) and block allocations (i.e., which blocks are assigned to which motion models). As used herein, "a block of a current frame is assigned to a motion model" means that the motion model can be used to predict the block. As used herein, "a motion model is assigned to a block of a current frame" means that the motion model can be used to predict the block.
At 1802, process 1800 makes an initial assignment of an initial motion model for a block of a current frame. In an example, process 1800 may divide the current frame into a number of block groups equal to the number of reference frames. Each group of blocks constitutes a subset of the blocks of the current frame.
An initial motion model may be generated for each group relative to the respective reference frame. For example, given three reference frames REF _1, REF _2, and REF _3 and three groups G1, G2, and G3, the process 1800 may generate a motion model M1 for predicting the blocks of group G1 from the reference frame REF _1, a motion model M2 for predicting the blocks of group G2 from the reference frame REF _2, and a motion model M3 for predicting the blocks of group G3 from the reference frame REF _ 3. The motion models M1, M2, and M3 constitute respective motion models of the reference frames and constitute the current diversified motion model.
The current frame may be divided into a plurality of groups in any of a variety of ways. For example, each block of the current frame may be randomly assigned to a group. In another example, a contiguous (e.g., in raster scan order) percentage of blocks may be assigned to each group. For example, the first third of the chunks may be assigned to group G1, the second 1/3 of the chunks may be assigned to group G2, and the third 1/3 of the chunks may be assigned to group G3. Other ways of dividing the current frame are also possible. A segment may include the same or a different number of blocks.
At 1804, process 1800 determines whether the objective function is satisfied. If so, process 1800 ends. Otherwise, process 1800 proceeds to 1806 to perform an iteration to improve the current motion model and block allocation. The objective function may be as described with reference to 1706 of fig. 17.
At 1806, the process 1800 refines the respective motion models of the reference frames assigned to the respective tile groups of the current frame. As such, process 1800 updates (or improves) motion models M1, M2, and M3 with the block allocation fixed. The motion model may be refined as described with reference to fig. 17.
The motion models M1, M2, and M3 are refined using the same set of blocks (e.g., G1, G2, and G3) assigned to each motion model. That is, during the improvement operation, each motion model is improved while considering only the blocks assigned to the motion model, so that the total encoding cost of the blocks assigned to the motion model is reduced using the motion model.
As such, improving the respective motion models of the reference frames assigned to the respective tile groups of the current frame may include assigning a first motion model of a first reference frame in the reference frames to a first tile group of the current frame, assigning a second motion model of a second reference frame in the reference frames to a second tile group of the current frame, and updating the first motion model and the second motion model to minimize a cost of encoding the first tile group and the second tile group.
At 1808, the process 1800 updates the assignment of the respective motion models to the blocks of the current frame. That is, the block allocation of all blocks of the current frame may be updated with all motion models (i.e., the improved motion models M1, M2, and M3 of 1806) fixed. The improved motion models M1, M2, M3 correspond to the current diversified motion models. The process 1800 may update the block allocation by choosing (i.e., associating, etc.) one of the improved motion models M1, M2, and M3 for each block of the current block that minimizes the block encoding cost. As such, block 1808 may result in a different block being included in each of groups G1, G2, and G3. At block 1806, the updated groups G1, G2, and G3 are used in the next iteration. As such, it can be said that the blocks included in a group among the groups G1-G3 are associated with the motion model of the group. It should be noted that updating the motion models of the blocks at 1808 may result in some motion models not being assigned to any block of the current frame. Thus, when the process 1800 ends in response to the query at 1804, some motion models may not be assigned to any block of the current frame.
As such, updating the allocation of the respective motion models to the blocks of the current frame may include reassigning the first motion model and the second motion model to the blocks in the first block set and the second block set to minimize a cost of encoding the blocks in the first block set and the second block set.
Fig. 19 is a flow diagram of a process 1900 for decoding a current frame, according to an embodiment of the disclosure. Process 1900 receives an encoded bitstream, such as compressed bitstream 420 of fig. 5. Process 1900 may be performed by a decoder. For example, the process 1900 may be performed in whole or in part by the intra/inter prediction stage 508 of the decoder 500. Process 1900 may be performed in whole or in part during a reconstruction path (shown by dashed connecting lines) of encoder 400 of fig. 4. For example, embodiments of process 1900 may be performed by instructions stored in a memory, such as memory 204 of receiving station 106 or transmitting station 102, for execution by a processor, such as CPU 202.
At 1902, the process 1900 decodes, from the encoded bitstream, respective motion models for reference frames of the current frame. The respective motion models of the reference frames constitute a diversified motion model for predicting the current frame. The motion models of the reference frames are encoded in the bitstream by an encoder (such as encoder 400 of fig. 4), which jointly determines the respective motion models. The encoder may jointly determine the respective motion models using one of processes 1500, 1700, or 1800.
At 1904, in response to determining to encode the current block of the current frame using at least one of the respective motion models, the process 1900 decodes the current block using at least one of the respective motion models.
For simplicity of explanation, processes 600, 1000, 1400, 1500, 1700, 1800, and 1900 are depicted and described as a series of steps or operations. However, steps or operations in accordance with the present disclosure may occur in various orders and/or concurrently. In addition, other steps or operations not presented and described herein may be used. Moreover, not all illustrated steps or operations may be required to implement a methodology in accordance with the disclosed subject matter.
The above described encoding and decoding aspects illustrate some examples of encoding and decoding techniques. However, it should be understood that encoding and decoding (as those terms are used in the claims) may refer to compression, decompression, transformation, or any other processing or alteration of data.
The word "example" is used herein to mean serving as an example, instance, or illustration. Any aspect or design described herein as "exemplary" is not necessarily to be construed as preferred or advantageous over other aspects or designs. Rather, use of the word "example" is intended to present concepts in a concrete fashion. As used in this application, the term "or" is intended to mean an inclusive "or" rather than an exclusive "or". That is, unless stated otherwise or clear from context, it is stated that "X comprises a or B" is intended to mean any of the natural inclusive permutations. That is, if X comprises A; x comprises B; or X includes both A and B, then "X includes A or B" is satisfied under any of the foregoing circumstances. In addition, the articles "a" and "an" as used in this application and the appended claims should generally be construed to mean "one or more" unless specified otherwise or clear from context to be directed to a singular form. Furthermore, unless so described, the use of the term "embodiment" or the term "one embodiment" throughout this disclosure is not intended to refer to the same embodiment or implementation.
Implementations of transmitting station 102 and/or receiving station 106 (as well as algorithms, methods, instructions, etc. stored thereon and/or executed thereby, including by encoder 400 and decoder 500) may be implemented in hardware, software, or any combination thereof. The hardware may include, for example, a computer, an Intellectual Property (IP) core, an Application Specific Integrated Circuit (ASIC), a programmable logic array, an optical processor, a programmable logic controller, microcode, a microcontroller, a server, a microprocessor, a digital signal processor, or any other suitable circuitry. In the claims, the term "processor" should be understood to encompass any of the foregoing hardware, alone or in combination. The terms "signal" and "data" are used interchangeably. Furthermore, portions of transmitting station 102 and receiving station 106 need not necessarily be implemented in the same manner.
Further, in an aspect, transmitting station 102 or receiving station 106 may be implemented, for example, using a general purpose computer or a general purpose processor having a computer program that, when executed, performs any of the respective methods, algorithms, and/or instructions described herein. Additionally, or alternatively, for example, a special purpose computer/processor may be used, which may contain other hardware for performing any of the methods, algorithms, or instructions described herein.
Transmitting station 102 and receiving station 106 may be implemented, for example, on computers in a videoconferencing system. Alternatively, transmitting station 102 may be implemented on a server, and receiving station 106 may be implemented on a device separate from the server, such as a handheld communication device. In this case, transmitting station 102 may encode the content into an encoded video signal using encoder 400 and transmit the encoded video signal to a communication device. The communication device may then, in turn, decode the encoded video signal using the decoder 500. Alternatively, the communication device may decode content stored locally on the communication device, e.g., content not transmitted by transmitting station 102. Other suitable transmission and reception implementations are available. For example, the receiving station 106 may be a generally stationary personal computer rather than a portable communication device and/or a device including the encoder 400 may also include the decoder 500.
Furthermore, all or portions of the embodiments of the present disclosure may take the form of a computer program product accessible from, for example, a computer-usable or computer-readable medium. A computer-usable or computer-readable medium may be, for example, any apparatus that tangibly embodies, stores, communicates or transmits the program for use by or in connection with any processor. The medium may be, for example, an electronic, magnetic, optical, electromagnetic, or semiconductor device. Other suitable media are also available.
The above-described embodiments, embodiments and aspects have been described in order to facilitate understanding of the present disclosure, and do not limit the present disclosure. On the contrary, the disclosure is intended to cover various modifications and equivalent arrangements included within the scope of the appended claims, which scope is to be accorded the broadest interpretation so as to encompass all such modifications and equivalent arrangements as is permitted under the law.
Claims (22)
1. A method for encoding a current frame of video, the method comprising:
jointly determining a motion model for a reference frame of the current frame; and
encoding the current frame using the motion models such that different portions of the current frame are encoded using different ones of the motion models.
2. The method of claim 1, wherein:
jointly determining the motion models comprises jointly determining respective motion models for reference frames used to encode the current frame by:
determining respective aggregate residuals for combinations of candidate motion models; and
selecting a combination of candidate motion models corresponding to a smallest aggregate residual, wherein the respective motion models correspond to the candidate motion models of the selected combination; and encoding the current frame comprises encoding the current frame using the respective motion models.
3. The method of claim 2, wherein determining respective aggregate residuals for combinations of candidate motion models comprises:
respective residuals for some of the candidate motion models for the reference frame are generated.
4. The method of claim 2, wherein the first and second light sources are selected from the group consisting of,
wherein a combination of the combinations of candidate motion models comprises a first candidate motion model for a first reference frame, a second candidate motion model for a second reference frame, and a third candidate motion model for a third reference frame, an
Wherein determining respective aggregate residuals for combinations of candidate motion models comprises:
for a current block, determining a first residual block using the first candidate motion model;
for the current block, determining a second residual block using the second candidate motion model;
determining, for the current block, a third residual block using the third candidate motion model; and
adding one of the first, second, and third residual blocks corresponding to a minimum residual block to an aggregate residual of the current frame.
5. The method of claim 3, wherein generating respective residuals for some of the candidate motion models for the reference frame comprises:
dividing the current frame into blocks; and
predicting the block using one of the candidate motion models of the reference frame.
6. The method of any of claims 1 to 5, wherein four candidate motion models are available for each of the reference frames of the current frame.
7. The method of any of claims 1-6, wherein the reference frame of the current frame comprises a reference image that is a non-displayable frame of the video.
8. The method of any of claims 1 to 7, wherein jointly determining the motion model comprises:
respective candidate motion models for the reference frames are generated independently of the candidate motion models of the other reference frames.
9. The method of any of claims 1-8, wherein the current frame comprises a first tile group and a second tile group, and wherein jointly determining the motion model comprises:
generating a candidate motion model for the first block group; and
determining an aggregate residual of the current frame using the candidate motion model for the first block group and using the other motion models for the second block group.
10. A device for encoding a current frame of video, the device comprising:
a processor configured to execute instructions stored in a non-transitory storage medium to:
jointly determining a motion model for a reference frame of the current frame; and
encoding the current frame using the motion models such that different portions of the current frame are encoded using different ones of the motion models.
11. The apparatus of claim 10, wherein the instructions to jointly determine the motion model comprise instructions to:
assigning respective initial motion models to the motion models;
determining a cost of encoding the current frame using the motion model; and
iteratively improving the motion models until an objective function is satisfied to reduce a cost of encoding the current frame using the respective initial motion models.
12. The apparatus of claim 11, wherein the reference frames comprise a first reference frame and other reference frames, and wherein the instructions to iteratively refine the motion models until the objective function is satisfied to reduce a cost of encoding the current frame using the respective initial motion models comprise instructions to:
generating motion model candidates for the first reference frame;
for each of the motion model candidates, determining a respective error for encoding the current frame using the respective motion model of the other frame and the motion model candidate; and
setting the motion model of the first reference frame to one of the first motion model candidates corresponding to a smallest aggregate residual.
13. Apparatus according to claim 11 or 12, wherein the objective function relates to an error of encoding the current frame using the motion model.
14. The apparatus of claim 13, wherein the error used to encode the current frame using the motion model is a mean square error.
15. The apparatus of claim 12 or any claim dependent on claim 12, wherein the instructions to determine the respective error for encoding the current frame using the respective motion model comprise instructions to:
excluding blocks of the current frame that satisfy a block-wise error threshold from determining the respective errors.
16. The apparatus of claim 12 or any claim dependent on claim 12, wherein the instructions to determine the respective error for encoding the current frame using the respective motion model comprise instructions to:
excluding block errors exceeding a threshold from determining the respective errors.
17. The apparatus of claim 11 or 12, wherein the objective function is a number of iterations.
18. The apparatus of claim 10, wherein the instructions to jointly determine the motion model comprise instructions to:
improving the respective motion models of the reference frames assigned to the respective block groups of the current frame; and
updating the assignment of the respective motion models to the blocks of the current frame.
19. The apparatus of claim 18, wherein said instructions for improving the respective motion models of the reference frames assigned to respective tile groups of the current frame comprise instructions for:
assigning a first motion model of a first one of the reference frames to a first block group of the current frame;
assigning a second motion model of a second one of the reference frames to a second group of blocks of the current frame; and
updating the first motion model and the second motion model to minimize a cost of encoding blocks in the first block set and the second block set.
20. The apparatus of claim 19, wherein the instructions to update the assignment of the respective motion models to blocks of the current frame comprise instructions to:
reassigning the first and second motion models to blocks of the first and second block sets to minimize a cost of encoding blocks of the first and second block sets.
21. A method for decoding a current frame, comprising:
decoding respective motion models for reference frames of the current frame from the encoded bitstream, wherein the encoder jointly determines the respective motion models; and
in response to determining to encode a current block of the current frame using at least one of the respective motion models, decode the current block using the at least one of the respective motion models.
22. A device for decoding a current frame of video, the device comprising:
a processor configured to execute instructions stored in a non-transitory storage medium to:
decoding respective motion models for reference frames of the current frame from the encoded bitstream, wherein the encoder jointly determines the respective motion models; and
in response to determining to encode a current block of the current frame using at least one of the respective motion models, decode the current block using the at least one of the respective motion models.
Applications Claiming Priority (5)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201762587025P | 2017-11-16 | 2017-11-16 | |
US62/587,025 | 2017-11-16 | ||
US16/016,857 | 2018-06-25 | ||
US16/016,857 US10681374B2 (en) | 2017-11-16 | 2018-06-25 | Diversified motion using multiple global motion models |
PCT/US2018/047209 WO2019099083A1 (en) | 2017-11-16 | 2018-08-21 | Diversified motion using multiple global motion models |
Publications (2)
Publication Number | Publication Date |
---|---|
CN110692241A true CN110692241A (en) | 2020-01-14 |
CN110692241B CN110692241B (en) | 2023-07-18 |
Family
ID=66433713
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201880035987.7A Active CN110692241B (en) | 2017-11-16 | 2018-08-21 | Diversified motions using multiple global motion models |
Country Status (4)
Country | Link |
---|---|
US (2) | US10681374B2 (en) |
EP (1) | EP3711293A1 (en) |
CN (1) | CN110692241B (en) |
WO (1) | WO2019099083A1 (en) |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN113556551A (en) * | 2020-04-23 | 2021-10-26 | 上海高德威智能交通系统有限公司 | Encoding and decoding methods, devices and equipment |
Families Citing this family (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11070813B2 (en) * | 2018-06-29 | 2021-07-20 | Intel Corporation | Global motion estimation and modeling for accurate global motion compensation for efficient video processing or coding |
EP3959886A4 (en) * | 2019-04-25 | 2022-06-22 | OP Solutions, LLC | Signaling of global motion vector in picture header |
MX2021013065A (en) * | 2019-04-25 | 2022-04-12 | Op Solutions Llc | Global motion constrained motion vector in inter prediction. |
BR112021021334A2 (en) * | 2019-04-25 | 2022-01-18 | Op Solutions Llc | Global motion vector flag in image header |
Citations (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5657087A (en) * | 1994-06-15 | 1997-08-12 | Samsung Electronics Co., Ltd. | Motion compensation encoding method and apparatus adaptive to motion amount |
EP1351510A1 (en) * | 2001-09-14 | 2003-10-08 | NTT DoCoMo, Inc. | Coding method,decoding method,coding apparatus,decoding apparatus,image processing system,coding program,and decoding program |
US20080107179A1 (en) * | 2005-03-14 | 2008-05-08 | Nilsson Michael E | Global Motion Estimation |
US20080240247A1 (en) * | 2007-03-29 | 2008-10-02 | Samsung Electronics Co., Ltd. | Method of encoding and decoding motion model parameters and video encoding and decoding method and apparatus using motion model parameters |
US20090086814A1 (en) * | 2007-09-28 | 2009-04-02 | Dolby Laboratories Licensing Corporation | Treating video information |
CN104396244A (en) * | 2012-04-16 | 2015-03-04 | 诺基亚公司 | An apparatus, a method and a computer program for video coding and decoding |
US20150264391A1 (en) * | 2014-03-14 | 2015-09-17 | Imagination Technologies Limited | Error Tracking and Mitigation for Motion Compensation-Based Video Compression |
CN105850133A (en) * | 2013-12-27 | 2016-08-10 | 英特尔公司 | Content adaptive dominant motion compensated prediction for next generation video coding |
US9438910B1 (en) * | 2014-03-11 | 2016-09-06 | Google Inc. | Affine motion prediction in video coding |
Family Cites Families (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP3183155B2 (en) | 1996-03-18 | 2001-07-03 | 株式会社日立製作所 | Image decoding apparatus and image decoding method |
US7321626B2 (en) | 2002-03-08 | 2008-01-22 | Sharp Laboratories Of America, Inc. | System and method for predictive motion estimation using a global motion predictor |
US8411750B2 (en) * | 2009-10-30 | 2013-04-02 | Qualcomm Incorporated | Global motion parameter estimation using block-based motion vectors |
US20170337711A1 (en) * | 2011-03-29 | 2017-11-23 | Lyrical Labs Video Compression Technology, LLC | Video processing and encoding |
EP2683165B1 (en) | 2012-07-04 | 2015-10-14 | Thomson Licensing | Method for coding and decoding a block of pixels from a motion model |
US9819965B2 (en) * | 2012-11-13 | 2017-11-14 | Intel Corporation | Content adaptive transform coding for next generation video |
US9715903B2 (en) * | 2014-06-16 | 2017-07-25 | Qualcomm Incorporated | Detection of action frames of a video stream |
US20180295375A1 (en) * | 2017-04-05 | 2018-10-11 | Lyrical Labs Video Compression Technology, LLC | Video processing and encoding |
-
2018
- 2018-06-25 US US16/016,857 patent/US10681374B2/en active Active
- 2018-08-21 CN CN201880035987.7A patent/CN110692241B/en active Active
- 2018-08-21 WO PCT/US2018/047209 patent/WO2019099083A1/en unknown
- 2018-08-21 EP EP18765773.9A patent/EP3711293A1/en active Pending
-
2020
- 2020-04-29 US US16/861,299 patent/US11115678B2/en active Active
Patent Citations (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5657087A (en) * | 1994-06-15 | 1997-08-12 | Samsung Electronics Co., Ltd. | Motion compensation encoding method and apparatus adaptive to motion amount |
EP1351510A1 (en) * | 2001-09-14 | 2003-10-08 | NTT DoCoMo, Inc. | Coding method,decoding method,coding apparatus,decoding apparatus,image processing system,coding program,and decoding program |
US20080107179A1 (en) * | 2005-03-14 | 2008-05-08 | Nilsson Michael E | Global Motion Estimation |
US20080240247A1 (en) * | 2007-03-29 | 2008-10-02 | Samsung Electronics Co., Ltd. | Method of encoding and decoding motion model parameters and video encoding and decoding method and apparatus using motion model parameters |
US20090086814A1 (en) * | 2007-09-28 | 2009-04-02 | Dolby Laboratories Licensing Corporation | Treating video information |
CN104396244A (en) * | 2012-04-16 | 2015-03-04 | 诺基亚公司 | An apparatus, a method and a computer program for video coding and decoding |
CN105850133A (en) * | 2013-12-27 | 2016-08-10 | 英特尔公司 | Content adaptive dominant motion compensated prediction for next generation video coding |
US9438910B1 (en) * | 2014-03-11 | 2016-09-06 | Google Inc. | Affine motion prediction in video coding |
US20150264391A1 (en) * | 2014-03-14 | 2015-09-17 | Imagination Technologies Limited | Error Tracking and Mitigation for Motion Compensation-Based Video Compression |
Non-Patent Citations (1)
Title |
---|
李礼: "面向高性能视频编码的码率控制与仿射预测研究", 《中国优秀硕士学位论文全文数据库 电子期刊》 * |
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN113556551A (en) * | 2020-04-23 | 2021-10-26 | 上海高德威智能交通系统有限公司 | Encoding and decoding methods, devices and equipment |
CN113556551B (en) * | 2020-04-23 | 2023-06-23 | 上海高德威智能交通系统有限公司 | Encoding and decoding method, device and equipment |
Also Published As
Publication number | Publication date |
---|---|
EP3711293A1 (en) | 2020-09-23 |
US20200260112A1 (en) | 2020-08-13 |
CN110692241B (en) | 2023-07-18 |
WO2019099083A1 (en) | 2019-05-23 |
US11115678B2 (en) | 2021-09-07 |
US20190149841A1 (en) | 2019-05-16 |
US10681374B2 (en) | 2020-06-09 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN110741640B (en) | Optical flow estimation for motion compensated prediction in video coding | |
CN110692241B (en) | Diversified motions using multiple global motion models | |
JP6163674B2 (en) | Content adaptive bi-directional or functional predictive multi-pass pictures for highly efficient next-generation video coding | |
CN111757106B (en) | Method and apparatus for coding a current block in a video stream using multi-level compound prediction | |
US10506249B2 (en) | Segmentation-based parameterized motion models | |
US11284107B2 (en) | Co-located reference frame interpolation using optical flow estimation | |
JP6605726B2 (en) | Motion vector partitioning of previous frame | |
CN111614956B (en) | DC coefficient sign coding scheme | |
US11876974B2 (en) | Block-based optical flow estimation for motion compensated prediction in video coding | |
CN110741641B (en) | Method and apparatus for video compression | |
CN110692246B (en) | Method and apparatus for motion compensated prediction | |
CN110169059B (en) | Composite Prediction for Video Coding | |
CN115486068A (en) | Method and apparatus for inter-frame prediction based on deep neural network in video coding | |
US20220148131A1 (en) | Image/video super resolution | |
US10225573B1 (en) | Video coding using parameterized motion models | |
WO2023160717A1 (en) | Method, apparatus, and medium for video processing | |
US20210144364A1 (en) | Motion Field Estimation Based on Motion Trajectory Derivation | |
WO2023226951A1 (en) | Method, apparatus, and medium for video processing | |
WO2023205371A1 (en) | Motion refinement for a co-located reference frame | |
CN118055253A (en) | Optical flow estimation for motion compensated prediction in video coding | |
WO2023287417A1 (en) | Warped motion compensation with explicitly signaled extended rotations |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |