US20230274100A1 - Techniques and Models for Multilingual Text Rewriting - Google Patents
Techniques and Models for Multilingual Text Rewriting Download PDFInfo
- Publication number
- US20230274100A1 US20230274100A1 US17/682,282 US202217682282A US2023274100A1 US 20230274100 A1 US20230274100 A1 US 20230274100A1 US 202217682282 A US202217682282 A US 202217682282A US 2023274100 A1 US2023274100 A1 US 2023274100A1
- Authority
- US
- United States
- Prior art keywords
- input text
- style
- text sequence
- module
- input
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000000034 method Methods 0.000 title claims abstract description 30
- 238000013519 translation Methods 0.000 claims abstract description 82
- 238000012549 training Methods 0.000 claims abstract description 37
- 238000000605 extraction Methods 0.000 claims abstract description 10
- 239000013598 vector Substances 0.000 claims description 67
- 238000013528 artificial neural network Methods 0.000 claims description 35
- 230000015654 memory Effects 0.000 claims description 21
- 238000012545 processing Methods 0.000 claims description 13
- 230000006870 function Effects 0.000 claims description 10
- 230000008859 change Effects 0.000 claims description 8
- 238000011176 pooling Methods 0.000 claims description 7
- 230000003750 conditioning effect Effects 0.000 claims description 5
- 239000000284 extract Substances 0.000 claims description 5
- 238000010801 machine learning Methods 0.000 claims description 5
- 238000013459 approach Methods 0.000 abstract description 32
- 238000005516 engineering process Methods 0.000 abstract description 27
- 230000008569 process Effects 0.000 abstract description 6
- 230000014616 translation Effects 0.000 description 71
- 238000012546 transfer Methods 0.000 description 43
- 238000012360 testing method Methods 0.000 description 21
- 238000010606 normalization Methods 0.000 description 16
- 238000011156 evaluation Methods 0.000 description 13
- 230000007935 neutral effect Effects 0.000 description 11
- 238000012552 review Methods 0.000 description 9
- 238000002474 experimental method Methods 0.000 description 7
- 230000007246 mechanism Effects 0.000 description 6
- 230000009466 transformation Effects 0.000 description 6
- 238000000844 transformation Methods 0.000 description 5
- 238000011161 development Methods 0.000 description 4
- 238000003058 natural language processing Methods 0.000 description 4
- 238000004321 preservation Methods 0.000 description 4
- 238000005070 sampling Methods 0.000 description 4
- 238000013515 script Methods 0.000 description 4
- 238000004891 communication Methods 0.000 description 3
- 238000007781 pre-processing Methods 0.000 description 3
- 238000010361 transduction Methods 0.000 description 3
- 230000026683 transduction Effects 0.000 description 3
- 238000002679 ablation Methods 0.000 description 2
- 230000002411 adverse Effects 0.000 description 2
- 230000006399 behavior Effects 0.000 description 2
- 230000015556 catabolic process Effects 0.000 description 2
- 230000001143 conditioned effect Effects 0.000 description 2
- 238000013527 convolutional neural network Methods 0.000 description 2
- 238000006731 degradation reaction Methods 0.000 description 2
- 230000000306 recurrent effect Effects 0.000 description 2
- 238000011160 research Methods 0.000 description 2
- 235000014347 soups Nutrition 0.000 description 2
- 238000000551 statistical hypothesis test Methods 0.000 description 2
- 229920002803 thermoplastic polyurethane Polymers 0.000 description 2
- 238000012952 Resampling Methods 0.000 description 1
- 230000003044 adaptive effect Effects 0.000 description 1
- 230000009286 beneficial effect Effects 0.000 description 1
- 230000008901 benefit Effects 0.000 description 1
- 230000002457 bidirectional effect Effects 0.000 description 1
- 238000004422 calculation algorithm Methods 0.000 description 1
- 239000003795 chemical substances by application Substances 0.000 description 1
- 238000004590 computer program Methods 0.000 description 1
- 230000021615 conjugation Effects 0.000 description 1
- 238000010586 diagram Methods 0.000 description 1
- 238000004836 empirical method Methods 0.000 description 1
- 230000001747 exhibiting effect Effects 0.000 description 1
- 238000001914 filtration Methods 0.000 description 1
- 235000013305 food Nutrition 0.000 description 1
- 239000012634 fragment Substances 0.000 description 1
- PCHJSUWPFVWCPO-UHFFFAOYSA-N gold Chemical compound [Au] PCHJSUWPFVWCPO-UHFFFAOYSA-N 0.000 description 1
- 239000010931 gold Substances 0.000 description 1
- 229910052737 gold Inorganic materials 0.000 description 1
- 238000007689 inspection Methods 0.000 description 1
- 230000003993 interaction Effects 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000001537 neural effect Effects 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 230000006403 short-term memory Effects 0.000 description 1
- 238000001228 spectrum Methods 0.000 description 1
- 238000010408 sweeping Methods 0.000 description 1
- 231100000331 toxic Toxicity 0.000 description 1
- 230000002588 toxic effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/40—Processing or translation of natural language
- G06F40/58—Use of machine translation, e.g. for multi-lingual retrieval, for server-side translation for client devices or for real-time translation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/30—Semantic analysis
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/10—Text processing
- G06F40/166—Editing, e.g. inserting or deleting
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/10—Text processing
- G06F40/197—Version control
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/253—Grammatical analysis; Style critique
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/40—Processing or translation of natural language
- G06F40/42—Data-driven translation
- G06F40/44—Statistical methods, e.g. probability models
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/40—Processing or translation of natural language
- G06F40/51—Translation evaluation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/40—Processing or translation of natural language
- G06F40/55—Rule-based translation
- G06F40/56—Natural language generation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
- G06N3/0455—Auto-encoder networks; Encoder-decoder networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/047—Probabilistic or stochastic networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/096—Transfer learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/0464—Convolutional networks [CNN, ConvNet]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/088—Non-supervised learning, e.g. competitive learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/09—Supervised learning
Definitions
- Text translation into different languages is an important area that supports a wide variety of different tools and applications.
- the quality of results can differ widely depending on the specific languages, models used in translations, and how such models are trained.
- Certain models may be trained using zero-shot or few-shot approaches in limited translation instances but may not be sufficiently effective in multilingual translations or other text rewriting situations. For instance, it can be challenging to properly convey a particular style or level of formality in translations across various languages. It is also possible for “accidental translations” to occur, in which a model provides a correct answer but in the wrong language. There may also be significant resource tradeoffs between using larger models versus incurring excessive memory costs. These and other issues can adversely affect multilingual translation in conventional approaches.
- aspects of the technology provide a model-based approach for multilingual text rewriting that is applicable across many languages and across different styles that may include various formality levels or other attributes of the text.
- the approach enables performance of attribute-controlled machine translation (e.g., dialect-aware translation, formality-controlled translations), as well as the ability to adapt text to a given domain or locale.
- attribute-controlled machine translation e.g., dialect-aware translation, formality-controlled translations
- One aspect involves leveraging a large pretrained multilingual model and fine-tuning it to perform general-purpose multilingual attribute transfer.
- Multilingual models can endow text with additional attributes in one language using exemplars of that attribute from another language, according to a zero-shot style transfer.
- the model presented is capable of manipulating both language and textual attributes jointly.
- This approach can support zero-shot formality-sensitive translation, with no labeled data in the target language.
- a system is configured for a multilingual text rewriting model.
- the system comprises memory configured to store a set of text exemplars in a source language and a set of rewritten texts in a plurality of languages different from the source language, and one or more processing elements operatively coupled to the memory.
- the one or more processing elements implement a multilingual text rewriter as a neural network having a number of modules including: a corruption module configured to generate a corrupted version of an input text sequence based on the set of text exemplars in the source language; an encoder module comprising an encoder neural network configured to receive the corrupted version of the input text sequence and to generate a set of encoded representations of the corrupted version of the input text sequence; a style extractor module configured to extract a set of style vector representations associated with the input text sequence; and a decoder module comprising a decoder neural network configured to receive the set of encoded representations and the set of style vector representations and to output the set of rewritten texts in the plurality of languages.
- a corruption module configured to generate a corrupted version of an input text sequence based on the set of text exemplars in the source language
- an encoder module comprising an encoder neural network configured to receive the corrupted version of the input text sequence and to generate a set of encoded representations
- Each style vector representation of the set is added element-wise to one of the set of encoded representations.
- a set of model weights is shared by the encoder module and the style extractor module, and a unique token is appended to the input text sequence for style extraction instead of mean-pooling all of the style vector representations in the set.
- the system is configured to provide rewritten text in selected ones of the plurality of languages according to a change in a least a sentiment or a formality of the input text sequence.
- the encoder module, style extractor module and the decoder module are configured as transformer stacks initialized from a common pretrained language model.
- both the encoder module and the decoder module are attention-based neural network modules.
- the corruption module employs a corruption function C for a given pair of non-overlapping spans (s1, s2) of the input text sequence, so that the model is capable of reconstructing span s2 from C(s2) and Vs1, where Vs1 is an attribute vector of span s1.
- the style extractor module may include a set of style extractor elements including a first subset configured to operate on different style exemplars and a second subset configured to operate on the input text sequence prior to corruption by the corruption module.
- the corruption module may be configured to employ at least one of token-level corruption or style-aware back-translation corruption.
- the set of model weights may be initialized with the weights of a pretrained text-to-text model.
- the model weights for the style extractor module may not be tied to the weights of the encoder module during training.
- the system may be configured to extract pairs of random non-overlapping spans of tokens from each line of text in a given text exemplar, and configured to use a first-occurring span in the text as an exemplar of attributes of a second span in the text.
- a cross-lingual learning signal can be added to a training objective for training the model.
- the encoder module may be configured to use negation of a true exemplar vector associated with the input text sequence as an attribute vector in a forward pass operation.
- the decoder module may be configured to receive a set of stochastic tuning ranges that provide conditioning for the decoder module.
- the set of style vector representations corresponds to a set of attributes associated with the input text sequence
- the system is configured to input a set of exemplars illustrating defined attributes and to extract corresponding attribute vectors for use in rewriting the input text sequence into the plurality of languages according to the defined attributes.
- the system may be configured to form an attribute delta vector including a scale factor, in which the attribute delta vector is added to the set of encoded representations before processing by the decoder module.
- a computer-implemented method for providing multilingual text rewriting according to a machine learning model comprises: obtaining an input text sequence based on a set of text exemplars in a source language; generating, by a corruption module, a corrupted version of the input text sequence; receiving, by an encoder neural network of an encoder module, the corrupted version of the input text sequence; generating, by the encoder module, a set of encoded representations of the corrupted version of the input text sequence; extracting, by a style extractor module, a set of style vector representations associated with the input text sequence; receiving, by a decoder neural network of a decoder module, the set of encoded representations and the set of style vector representations, in which each style vector representation of the set is added element-wise to one of the set of encoded representations; outputting, by the decoder module, a set of rewritten texts in a plurality of languages different from the source language; and storing the re
- generating the corrupted version of the input text sequence is performed according to a corruption function C for a given pair of non-overlapping spans (s1, s2) of the input text sequence, so that the model is capable of reconstructing span s2 from C(s2) and Vs1, where Vs1 is an attribute vector of span s1.
- the method further comprising: extracting pairs of random non-overlapping spans of tokens from each line of text in a given text exemplar; and using a first-occurring span in the text as an exemplar of attributes of a second span in the text.
- the method further comprises adding a cross-lingual learning signal to a training objective for training the model.
- the method further comprises applying a set of stochastic tuning ranges to selectively condition the decoder module.
- the set of style vector representations corresponds to a set of attributes associated with the input text sequence.
- outputting the set of rewritten texts includes generating one or more versions of the input text sequence in selected ones of the plurality of languages according to the set of attributes.
- FIG. 1 illustrates an example of rewriting text to exhibit new attributes in accordance with aspects of the technology.
- FIG. 2 illustrates a Transformer-type architecture for use in accordance with aspects of the technology.
- FIGS. 3 A-B illustrates a style extractor architecture for use in accordance with aspects of the technology.
- FIG. 4 illustrates a table showing BLEU scores of various supervised and unsupervised models for low-resource language pairs in accordance with aspects of the technology.
- FIG. 5 illustrates another table showing examples of a multilingual rewriting model performing sentiment transfer in accordance with aspects of the technology.
- FIGS. 6 A-B illustrate examples sets of positive and negative exemplars, respectively.
- FIG. 7 illustrates how model behavior can depend on the inference-time parameter.
- FIGS. 8 A-L measure content preservation versus transfer accuracy as both language and sentiment are changed at the same time, in accordance with aspects of the technology.
- FIGS. 9 A-B illustrate formal and informal exemplars for testing aspects of the technology.
- FIGS. 10 A-C illustrate tables of different levels of formality in accordance with aspects of the technology.
- FIG. 11 presents a table showing average formality and translation quality scores for a human evaluation of system operation.
- FIG. 12 presents a table showing the T-V distinction accuracy for formality-aware Spanish translations in accordance with aspects of the technology.
- FIGS. 13 A-B illustrate a system for use with aspects of the technology.
- FIG. 14 illustrates a method in accordance with aspects of the technology.
- the technology provides an encoder-decoder architectural approach with attribute extraction to train rewriter models that can be used in “universal” textual rewriting across many different languages.
- a cross-lingual learning signal is incorporated into the training approach. Certain training processes do not employ any exemplars. This approach enables not just straight translation, but also the ability to create new sentences with different attributes.
- FIG. 1 illustrates a general example 100 , which enables rewriting text to exhibit new attributes.
- multilingual rewriter module 102 receives an input sentence 102 in Chinese having one type of (negative) sentiment (“This soup isn't very good”).
- the rewriter module 102 is able to produce an output sentence 104 in Spanish having a different type of (positive) sentiment (“This soup is delicious!”).
- the model employed by the multilingual rewriter 102 enables rewriting text to exhibit new attributes, such as changes in language, sentiment or formality.
- the target language (Spanish) 108 is signaled by a unique language code.
- Other attributes are controlled through few-shot exemplars 110 , which may leverage distinct languages from the input and the target.
- transfer scale ( ⁇ ) 112 modulates the strength of the attribute transfer.
- the transfer scale helps control how much preference the model can give to the desired style versus preserving the input. At one extreme, a very large scale would lead to the model completely ignoring the input and producing a random sentence in the target style. However, control of this parameter can enable the system to attain all values in the spectrum of a given style. For example, for the sentence “The movie was okay”, if there is a goal for a more positive sentence, once could potentially attain “The movie was good”, “The movie was great”, and “The movie was amazing!” by using larger and large transfer scales.
- aspects of the technology are used to test the conjecture that large models trained on massive multilingual corpora should be able to “identify” a textual attribute (e.g., formality) given just a few exemplars in one language (e.g., English), and “apply” the underlying concept to a generative task in another language in a zero-shot fashion (e.g., formality transfer in Chinese).
- a textual attribute e.g., formality
- a generative task in another language e.g., formality transfer in Chinese.
- attribute and “styles” are used interchangeably.
- the model can go beyond traditional tasks associated with style transfer, for example extending to translation (language transfer) and style-conditioned translation.
- exemplars is used to denote inputs show-casing a particular attribute.
- the machine translation rewrite model discussed herein may employ a neural network such as a convolutional neural network (CNN) or a recurrent neural network (RNN), e.g., a bidirectional long short-term memory (Bi-LSTM) RNN.
- a neural network such as a convolutional neural network (CNN) or a recurrent neural network (RNN), e.g., a bidirectional long short-term memory (Bi-LSTM) RNN.
- the model may employ a self-attention architecture. This may include an approach such as the Transformer neural network encoder-decoder architecture.
- An exemplary Transformer-type architecture is shown in FIG. 2 , which is based on the arrangement shown in U.S. Pat. No. 10,452,878, entitled “Attention-based sequence transduction neural networks”, the entire disclosure of which is incorporated herein by reference.
- System 200 of FIG. 2 is implementable as computer programs by processors of one or more computers in one or more locations.
- the system 200 receives an input sequence 202 and processes the input sequence 202 to transduce the input sequence 202 into an output sequence 204 .
- the input sequence 202 has a respective network input at each of multiple input positions in an input order and the output sequence 204 has a respective network output at each of multiple output positions in an output order.
- System 200 can perform any of a variety of tasks that require processing sequential inputs to generate sequential outputs.
- System 200 includes an attention-based sequence transduction neural network 206 , which in turn includes an encoder neural network 208 and a decoder neural network 210 .
- the encoder neural network 208 is configured to receive the input sequence 202 and generate a respective encoded representation of each of the network inputs in the input sequence.
- an encoded representation is a vector or other ordered collection of numeric values.
- the decoder neural network 210 is then configured to use the encoded representations of the network inputs to generate the output sequence 204 .
- both the encoder 208 and the decoder 210 are attention-based.
- the encoder neural network 208 includes an embedding layer (input embedding) 212 and a sequence of one or more encoder subnetworks 214 .
- the encoder neural 208 network may N encoder subnetworks 214 .
- the embedding layer 212 is configured, for each network input in the input sequence, to map the network input to a numeric representation of the network input in an embedding space, e.g., into a vector in the embedding space.
- the embedding layer 212 then provides the numeric representations of the network inputs to the first subnetwork in the sequence of encoder subnetworks 214 .
- the embedding layer 212 may be configured to map each network input to an embedded representation of the network input and then combine, e.g., sum or average, the embedded representation of the network input with a positional embedding of the input position of the network input in the input order to generate a combined embedded representation of the network input.
- the positional embeddings are learned. As used herein, “learned” means that an operation or a value has been adjusted during the training of the sequence transduction neural network 206 . In other cases, the positional embeddings may be fixed and are different for each position.
- Each of the encoder subnetworks 214 is configured to receive a respective encoder subnetwork input for each of the plurality of input positions and to generate a respective subnetwork output for each of the plurality of input positions.
- the encoder subnetwork outputs generated by the last encoder subnetwork in the sequence are then used as the encoded representations of the network inputs.
- the encoder subnetwork input is the numeric representations generated by the embedding layer 212
- the encoder subnetwork input is the encoder subnetwork output of the preceding encoder subnetwork in the sequence.
- Each encoder subnetwork 214 includes an encoder self-attention sub-layer 216 .
- the encoder self-attention sub-layer 216 is configured to receive the subnetwork input for each of the plurality of input positions and, for each particular input position in the input order, apply an attention mechanism over the encoder subnetwork inputs at the input positions using one or more queries derived from the encoder subnetwork input at the particular input position to generate a respective output for the particular input position.
- the attention mechanism is a multi-head attention mechanism as shown.
- each of the encoder subnetworks 214 may also include a residual connection layer that combines the outputs of the encoder self-attention sub-layer with the inputs to the encoder self-attention sub-layer to generate an encoder self-attention residual output and a layer normalization layer that applies layer normalization to the encoder self-attention residual output.
- a residual connection layer that combines the outputs of the encoder self-attention sub-layer with the inputs to the encoder self-attention sub-layer to generate an encoder self-attention residual output
- a layer normalization layer that applies layer normalization to the encoder self-attention residual output.
- Some or all of the encoder subnetworks can also include a position-wise feed-forward layer 218 that is configured to operate on each position in the input sequence separately.
- the feed-forward layer 218 is configured receive an input at the input position and apply a sequence of transformations to the input at the input position to generate an output for the input position.
- the inputs received by the position-wise feed-forward layer 218 can be the outputs of the layer normalization layer when the residual and layer normalization layers are included or the outputs of the encoder self-attention sub-layer 216 when the residual and layer normalization layers are not included.
- the transformations applied by the layer 218 will generally be the same for each input position (but different feed-forward layers in different subnetworks may apply different transformations).
- the encoder subnetwork can also include a residual connection layer that combines the outputs of the position-wise feed-forward layer with the inputs to the position-wise feed-forward layer to generate an encoder position-wise residual output and a layer normalization layer that applies layer normalization to the encoder position-wise residual output.
- these two layers are also collectively referred to as an “Add & Norm” operation.
- the outputs of this layer normalization layer can then be used as the outputs of the encoder subnetwork 214 .
- the decoder neural network 210 is configured to generate the output sequence in an auto-regressive manner. That is, the decoder neural network 210 generates the output sequence, by at each of a plurality of generation time steps, generating a network output for a corresponding output position conditioned on (i) the encoded representations and (ii) network outputs at output positions preceding the output position in the output order. In particular, for a given output position, the decoder neural network generates an output that defines a probability distribution over possible network outputs at the given output position. The decoder neural network can then select a network output for the output position by sampling from the probability distribution or by selecting the network output with the highest probability.
- the decoder neural network 210 is auto-regressive, at each generation time step, the decoder network 210 operates on the network outputs that have already been generated before the generation time step, i.e., the network outputs at output positions preceding the corresponding output position in the output order. In some implementations, to ensure this is the case during both inference and training, at each generation time step the decoder neural network 210 shifts the already generated network outputs right by one output order position (i.e., introduces a one position offset into the already generated network output sequence) and (as will be described in more detail below) masks certain operations so that positions can only attend to positions up to and including that position in the output sequence (and not subsequent positions).
- the decoder neural network 210 includes an embedding layer (output embedding) 220 , a sequence of decoder subnetworks 222 , a linear layer 224 , and a softmax layer 226 .
- the decoder neural network can include N decoder subnetworks 222 .
- the embedding layer 220 is configured to, at each generation time step, for each network output at an output position that precedes the current output position in the output order, map the network output to a numeric representation of the network output in the embedding space. The embedding layer 220 then provides the numeric representations of the network outputs to the first subnetwork 222 in the sequence of decoder subnetworks.
- the embedding layer 220 is configured to map each network output to an embedded representation of the network output and combine the embedded representation of the network output with a positional embedding of the output position of the network output in the output order to generate a combined embedded representation of the network output.
- the combined embedded representation is then used as the numeric representation of the network output.
- the embedding layer 220 generates the combined embedded representation in the same manner as described above with reference to the embedding layer 212 .
- Each decoder subnetwork 222 is configured to, at each generation time step, receive a respective decoder subnetwork input for each of the plurality of output positions preceding the corresponding output position and to generate a respective decoder subnetwork output for each of the plurality of output positions preceding the corresponding output position (or equivalently, when the output sequence has been shifted right, each network output at a position up to and including the current output position).
- each decoder subnetwork 222 includes two different attention sub-layers: a decoder self-attention sub-layer 228 and an encoder-decoder attention sub-layer 230 .
- Each decoder self-attention sub-layer 228 is configured to, at each generation time step, receive an input for each output position preceding the corresponding output position and, for each of the particular output positions, apply an attention mechanism over the inputs at the output positions preceding the corresponding position using one or more queries derived from the input at the particular output position to generate a updated representation for the particular output position. That is, the decoder self-attention sub-layer 228 applies an attention mechanism that is masked so that it does not attend over or otherwise process any data that is not at a position preceding the current output position in the output sequence.
- Each encoder-decoder attention sub-layer 230 is configured to, at each generation time step, receive an input for each output position preceding the corresponding output position and, for each of the output positions, apply an attention mechanism over the encoded representations at the input positions using one or more queries derived from the input for the output position to generate an updated representation for the output position.
- the encoder-decoder attention sub-layer 230 applies attention over encoded representations while the decoder self-attention sub-layer 228 applies attention over inputs at output positions.
- the decoder self-attention sub-layer 228 is shown as being before the encoder-decoder attention sub-layer in the processing order within the decoder subnetwork 222 . In other examples, however, the decoder self-attention sub-layer 228 may be after the encoder-decoder attention sub-layer 230 in the processing order within the decoder subnetwork 222 or different subnetworks may have different processing orders.
- each decoder subnetwork 222 includes, after the decoder self-attention sub-layer 228 , after the encoder-decoder attention sub-layer 230 , or after each of the two sub-layers, a residual connection layer that combines the outputs of the attention sub-layer with the inputs to the attention sub-layer to generate a residual output and a layer normalization layer that applies layer normalization to the residual output.
- a residual connection layer that combines the outputs of the attention sub-layer with the inputs to the attention sub-layer to generate a residual output
- a layer normalization layer that applies layer normalization to the residual output.
- the decoder subnetwork 170 also include a position-wise feed-forward layer 232 that is configured to operate in a similar manner as the position-wise feed-forward layer 218 from the encoder 208 .
- the layer 232 is configured to, at each generation time step: for each output position preceding the corresponding output position: receive an input at the output position, and apply a sequence of transformations to the input at the output position to generate an output for the output position.
- the inputs received by the position-wise feed-forward layer 232 can be the outputs of the layer normalization layer (following the last attention sub-layer in the subnetwork 222 ) when the residual and layer normalization layers are included or the outputs of the last attention sub-layer in the subnetwork 222 when the residual and layer normalization layers are not included.
- the decoder subnetwork can also include a residual connection layer that combines the outputs of the position-wise feed-forward layer with the inputs to the position-wise feed-forward layer to generate a decoder position-wise residual output and a layer normalization layer that applies layer normalization to the decoder position-wise residual output.
- These two layers are also collectively referred to as an “Add & Norm” operation.
- the outputs of this layer normalization layer can then be used as the outputs of the decoder subnetwork 222 .
- the linear layer 224 applies a learned linear transformation to the output of the last decoder subnetwork 222 in order to project the output of the last decoder subnetwork 222 into the appropriate space for processing by the softmax layer 226 .
- the softmax layer 226 then applies a softmax function over the outputs of the linear layer 224 to generate the probability distribution (output probabilities) 234 over the possible network outputs at the generation time step.
- the decoder 210 can then select a network output from the possible network outputs using the probability distribution.
- One baseline configuration that can be employed with aspects of the technology employs an English-language few-shot style transfer model encoder decoder Transformer, which is equipped with an additional “style extractor” that is a variation of the encoder neural network 208 discussed above.
- This configuration removes the need for training labels, and offers a single model that can target an unrestricted set of style attributes. Aspects of this approach are discussed in detail in “TextSETTR: Few-Shot Text Style Extraction and Tunable Targeted Restyling”, by Riley et al., Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ⁇ 2021, the entire disclosure of which is incorporated herein by reference.
- the style extractor takes exemplars of a style as input and extracts a fixed-width “style vector” by mean-pooling the output embeddings across tokens. These style vectors (and combinations thereof) can then be added element-wise to the encoded representations of a given input before being passed to the decoder in order to induce targeted style changes.
- FIG. 3 A illustrates an example training configuration 300
- FIG. 3 B illustrates an example inference configuration 320 .
- the training configuration 300 shows an architecture for few-shot style transfer.
- Encoder 302 , decoder 304 and style extractor 306 are transformer stacks initialized from a pretrained language model such as T5, which is trained using teacher forcing.
- the model reconstructs a corrupted input provided by corruption block 308 , conditioned on a fixed-width “style vector” extracted from the preceding sentence.
- the corruption block 308 may employ two types of corruption: token-level and style-aware back-translation. In the former, a noise function is applied independently to each token, with a probability of either dropping a token, replacing it with one of the examples in the current batch at the same position or keeping it as is.
- the model is used in inference mode to change the style of the input to match the style of a random sentence in the batch, and the resulting output is used as a corrupted version of the input.
- the corruption block 308 corrupts the input by (i) dropping, (ii) replacing, and/or (iii) shuffling tokens, which may be performed in that order.
- the training occurs in an unsupervised fashion, so the model can handle arbitrary styles at inference time. To compensate for the lack of training labels, this architecture relies on the intuition that style is a “slow-moving feature”, consistent across long spans of text.
- One or more stochastic tuning ranges 310 can be used to provide extra conditioning for the decoder 304 , and enable fine-grained control of inference. This can include different “add” and “delete” rates.
- the system can calculate the proportions of tokens that were added and deleted.
- the “add rate” is the proportion of output tokens absent from the input
- the “delete rate” is the proportion of input tokens absent from the output.
- These rates can be provided to the decoder as ranges covering but not necessarily centered on the “true” rates. For instance, each range width may be sampled uniformly from [0,1], and uniformly sample the “alignment” of the true rate within the range.
- the style vector is added to each of the final encoder hidden states.
- the weights of the model may be initialized with those of a pretrained (e.g., T5) model. Both the style extractor and the encoder can be initialized from the pretrained encoder, but the weights need not be tied during training.
- style extractor block 322 has a set of style extractor elements 324 , which may operate on different style exemplars (e.g., styles “A” and “B”), and also on the input prior to corruption.
- style exemplars e.g., styles “A” and “B”
- ⁇ is the delta scale, which can be an important hyperparameter to tune. It has been found that values in the range [1.0, 10.0] work well, with the best values depending on the attribute and the exemplars in question.
- Stochastic tuning ranges 326 may be the same or different from those used during training. With this approach, at inference time a new style vector is formed via “targeted restyling”, which adds a directional delta to the extracted style of the input text.
- one aspect of the technology includes not separating the attribute extraction module (e.g., style extractor 306 in FIG. 3 A ) from the encoder-decoder architecture. Instead, all the weights are shared between this module and the encoder. In order to distinguish the two, the system prepends the text with a unique token when performing attribute extraction, and takes the representation of this token from the encoder's output as the fixed-width vector representation, instead of mean-pooling all the representations. This allows the multilingual architecture to leverage larger models without incurring excessive memory costs.
- the attribute extraction module e.g., style extractor 306 in FIG. 3 A
- sentence-splitting during preprocessing of non-English text may discard any data that does not include ASCII punctuation.
- ASCII punctuation For languages like Chinese and Thai, which do not typically use ASCII punctuation, this would filter out most well-formed text, leaving mainly text in the wrong language, or a non-natural language (e.g., Javascript code). This could adversely affect system operation, so a more language-agnostic approach has been adopted.
- the system extracts pairs of random non-overlapping spans of tokens from each line of text, and uses the first-occurring span in the text as an exemplar of the attributes of the second span. This approach allows the system to retain all data, is independent of language, and still exploits the “slow-moving feature” intuition discussed above.
- the system starts with an mT5 checkpoint as the initial model, for instance the XL variant.
- an additional cross-lingual learning signal is added to the training objective by forcing the model to not only perform an attribute transfer procedure but also to translate the sentence to a baseline language such as English.
- English is particularly suitable due to the availability of parallel data between English and many other languages.
- the system can use the negation of the “true” exemplar vector associated with the input (as opposed to leveraging a random exemplar) as the attribute vector in the forward pass.
- V s1 is the attribute vector of the first span
- the model is tasked with reconstructing s 2 from C(s 2 ) and V s1 .
- the intuition behind the negative exemplar is that it is desirable for the model to learn that the changes induced by an attribute vector should be undone when applying the same procedure with the negative of this attribute vector.
- the system can perform the decoding operation using sampling, e.g., with a temperature of 1.5.
- the approach also includes translation as another cross-lingual learning objective by leveraging English-centric translation data for some language pairs. For this task, exemplars are not used.
- attribute vectors there are multiple ways one could use the attribute vectors to perform rewriting.
- the system is provided with sets of exemplars illustrating attributes A and B, which are used to extract attribute vectors V A and V B , respectively. Given an input x with attribute A, the system should rewrite it to exhibit attribute B.
- One inference strategy is to first extract the attribute vector V x of x, then form the following attribute delta vector:
- V A,B,x V x + ⁇ ( V B ⁇ V A )
- ⁇ is a scale factor, which may be chosen by the user.
- the resulting vector V A,B,x then gets added to the encoded representation of x before being passed to the decoder. For within-language tasks, where the output should be in the same language as the input, the system can use this inference strategy.
- the system does not include the vector V x in the computation of V A,B,x for cross-language tasks, and instead uses the following equation:
- the system drew monolingual data from mC4, the same dataset used to train mT5.
- the “-Latin” languages were removed due to their small data size.
- Parallel data for 46 of these languages with English was leveraged, by taking all language pairs with more than 500,000 lines of parallel data from the OPUS 100 (dataset, with the exception of Hebrew, Wegn, Laun (because their language codes in OPUS 100 were not present in the language codes of mC4), and Japanese. Japanese was excluded as it appears in the sentiment evaluation, and it was desirable to test the model's ability to perform zero-shot attribute transfer in languages where no parallel data is available. As Japanese has a unique script and no genealogical relation with other languages under consideration, it poses a particularly challenging case.
- the two-character language codes for the utilized languages are as follows: ar, bg, bn, ca, cs, da, de, el, es, et, eu, fa, fi, fr, gl, hi, hu, id, is, it, ko, lt, lv, mg, ink, ml, ms, mt, nl, no, pl, pt, ro, ru, si, sk, sl, sq, sr, sv, th, tr, uk, ur, vi, and zh.
- the monolingual data was preprocessed into input-exemplar pairs, using the approach described above. In addition, any pair was discarded where either element is shorter than five tokens. In the case of parallel data, no exemplars were leveraged, and instead used a vector of zeros. For both data sources, any training example with either input, exemplar or target (in the case of parallel data) longer than 64 tokens was discarded. Language-specific tokens were pre-pended to the input to prime the model for translations. Instead of introducing new tokens, the tokens associated to the target language names in English were reused.
- the models were trained in JAX (discussed in “JAX: composable transformations of Python+NumPy programs” by Bradbury et al., 2018).
- the Adafactor optimizer was used (discussed in “Adafactor: Adaptive learning rates with sublinear memory cost”, in Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 4596-4604).
- the accumulated optimizer states were reused from the original mT5 training.
- a constant learning rate of 1e-3 was used and trained for 25,000 steps, using batches of 512 inputs. Both the Jax and Adafactor references are incorporated herein by reference.
- BLEU scores For machine translation, the system utilized BLEU scores (as discussed in “Bleu: a method for automatic evaluation of machine translation”, 2002, in Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311-318). The scores were computed through the sacreBLEU5 library (see “A call for clarity in reporting BLEU scores”, 2018, in Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186-191). The output was tokenized for the Indic languages using the tokenizer provided by the Indic-NLP library (see “Indic NLP Library, A unified approach to NLP for Indian languages by Anoop Kunchukuttan, 2020). For decoding, a beam search was used, with a beam size of 5 for machine translation, within-language sentiment transfer and formality-aware machine translation, and beam size of 1 for cross-language sentiment transfer. Each of the identified references is incorporated herein by reference.
- the baselines include no parallel data (-para); no language tokens (-lang tokens); no exemplars in training (-exemplars); and no back-translation (-BT).
- the “-para” setting tests whether the additional supervision signal obtained from the parallel data is necessary or even useful, and the “-lang tokens” setting addresses a similar question regarding language tokens.
- FIG. 4 illustrates Table 1, which presents BLEU scores of various supervised and unsupervised models for low-resource language pairs.
- Table 1 compares the instant Universal Rewriter model with the bilingual unsupervised translation models and large multilingual unsupervised translation models of certain prior approaches and their analogous supervised variants.
- Table 1 compares the instant Universal Rewriter model with the bilingual unsupervised translation models and large multilingual unsupervised translation models of certain prior approaches and their analogous supervised variants.
- the rewriter model attains comparable performance with the external baselines, despite not being directly trained for unsupervised translation. It is interesting to note that the baseline with no parallel data collapses, failing to produce the correct language and giving low BLEU scores. A similar pattern for the baseline is seen without language tokens (-lang tokens) when translating into languages without parallel data. On the other hand, the remaining baselines that leverage parallel data manage to attain reasonable scores (even for language pairs without parallel data), justifying the inclusion of the parallel data. Finally, it is observed that removing the ability to control text attributes via exemplars (-exemplars) confers no meaningful benefit for translation quality. This finding supports the view that a single general model can be well-suited to adjust both language and other textual attributes.
- a specific multilingual style transfer evaluation was developed for testing, crafted from an existing dataset for multilingual classification.
- this evaluation leverages the multilingual Amazon reviews dataset, which was presented by Prettenhofer and Stein.
- This dataset consists of Amazon reviews in English, French, German and Japanese with review ratings in the range 1-5.
- the approach treated reviews rated 4 or 5 as positive, and reviews rated 1 or 2 as negative, and dropped reviews rated 3.
- mBERT was fine-tuned (the multilingual variant of BERT) on the remaining reviews and treated it as an oracle sentiment classifier, splitting the reviews into training and development sets. This model achieved 80.1% accuracy on this development set.
- the Universal Rewriter model was compared with various ablation models on the task of transferring negative to positive sentiment. For evaluation, examples longer than 61 tokens were removed, samples were disregarded that disagreed with the oracle classifier, only keeping those which it assigned above 80% probability of being the correct class. Prepending two tokens to every example was done to control the language, and one end-of-sentence token. This gave a maximum of 64 tokens, which is the maximum length that the models were trained on. Development and test sets were constructed from the remaining examples, consisting of 800 examples, 200 from each language, equally balanced in positive and negative reviews.
- FIGS. 6 A and 6 B illustrate the sets of positive and negative exemplars, respectively.
- the ⁇ parameter allows control of the content preservation vs. transfer accuracy trade-off, accurately changing attributes while still preserving content and language.
- the general shape of the baseline without language tokens (-lang tokens) 710 is similar to Universal Rewriter 702 , only shifted to the left. Upon inspection, it was found this to be due to wrong-language errors, which negatively affected the self-BLEU score.
- FIGS. 8 A-L measure content preservation vs. transfer accuracy as both language and sentiment are changed at the same time. For this task, ⁇ was varied between 0.5 and 5.0 with 0.5 increments. The “-lang tokens” baseline was omitted since there was no explicit way of controlling both language and style for this baseline. As in the within-language case, the Universal Rewriter outperformed all the ablations, as presented in the various results in FIGS. 8 A-L .
- FIGS. 8 A-F show plots of different unsupervised language pairings, comparing transfer accuracy (Y axis) to self-BLEU values (X axis).
- FIGS. 8 G-H show zero-shot language pairs, evaluating transfer accuracy (Y axis) to self-BLEU values (X axis).
- FIGS. 81 -L show supervised language pairs, again evaluating transfer accuracy (Y axis) to self-BLEU values (X axis).
- formality-sensitive translation can also be evaluated.
- Such a task requires the model to translate input text into another language, while simultaneously controlling the formality level of the output text. This type of control is useful when the desired formality level is known ahead of time, and may not match that of the source text—for example, when providing translations for second language learners, or for use within an assistive agent.
- Tables 3A-C of FIGS. 10 A-C show a successful example of Universal Rewriter translating an input sentence into four languages at three levels of formality.
- Table 3A of FIG. 10 A is an informal level
- table 3B of FIG. 10 B is a neutral amount of formality.
- table 3C of FIG. 10 C is a more formal level.
- Native speaker consultants confirmed that, while not every translation is perfectly natural, they are all understandable, and the formality level clearly increases going from informal to neutral to formal. Interestingly, these differences manifest in both lexical and grammatical choices, including the use of formal vs. informal pronouns in Chinese, Russian and Spanish, despite this distinction being absent in English.
- an overall approach for training the multilingual text rewriting model can include obtaining one or more text strings for every language to be supported, extracting 2 sub-spans of text and tokenizing them. The process can randomly replace tokens to corrupt them.
- the spans are passed through the encoder to reuse mT5 model (or equivalent model). Untouched spans are passed through the encoder to extract the style.
- the style is effectively an attribute, which can encompass any one (or more) of: formality, positive v, negative, dialect, author style, magazine style, food items, movie items, or even something that could be changed about a sentence while leaving other attributes the same.
- the style vector is added to the outputs of encoder model, and that combination is input to the decoder model (in which or more stochastic tuning ranges provide extra conditioning).
- the resultant multilingual model can handle not only sentences, but even random spans of text. It consumes exemplars in one language and extracts results in a different language,
- FIGS. 13 A and 13 B are pictorial and functional diagrams, respectively, of an example system 1300 that includes a plurality of computing devices and databases connected via a network.
- computing device(s) 1302 may be a cloud-based server system.
- Databases 1304 , 1306 and 1308 may store, e.g., a corpus of source text (e.g., sentences, sentence fragment, or larger text samples), a corpus of translated output text for each source text element in one or more different languages, and one or more machine translation models for text rewriting, respectively.
- the server system may access the databases via network 1310 .
- One or more user devices or systems may include a computing system 1312 and a desktop computer 1314 , for instance to provide source text segment and/or other information to the computing device(s) 1302 .
- each of the computing devices 1302 and 1312 - 1314 may include one or more processors, memory, data and instructions.
- the memory stores information accessible by the one or more processors, including instructions and data (e.g., machine translation model(s), corpus information, style extractors, corruption types, exemplars, etc.) that may be executed or otherwise used by the processor(s).
- the memory may be of any type capable of storing information accessible by the processor(s), including a computing device-readable medium.
- the memory is a non-transitory medium such as a hard-drive, memory card, optical disk, solid-state, etc. Systems may include different combinations of the foregoing, whereby different portions of the instructions and data are stored on different types of media.
- the instructions may be any set of instructions to be executed directly (such as machine code) or indirectly (such as scripts) by the processor(s).
- the instructions may be stored as computing device code on the computing device-readable medium.
- the terms “instructions”, “modules” and “programs” may be used interchangeably herein.
- the instructions may be stored in object code format for direct processing by the processor, or in any other computing device language including scripts or collections of independent source code modules that are interpreted on demand or compiled in advance.
- the processors may be any conventional processors, such as commercially available GPUs, CPUs, TPUs, etc.
- each processor may be a dedicated device such as an ASIC or other hardware-based processor.
- FIG. 13 B functionally illustrates the processors, memory, and other elements of a given computing device as being within the same block, such devices may actually include multiple processors, computing devices, or memories that may or may not be stored within the same physical housing.
- the memory may be a hard drive or other storage media located in a housing different from that of the processor(s), for instance in a cloud computing system of server 1302 . Accordingly, references to a processor or computing device will be understood to include references to a collection of processors or computing devices or memories that may or may not operate in parallel.
- the data such as source text files or text segment and/or translated output text in multiple languages, may be operated on by the system to train one or more models. This can include filtering or curating the input dataset(s).
- the trained models may be used on textual input to provide translated text to one or more users, for instance users of computers 1312 and/or 1314 .
- Such computers may include all of the components normally used in connection with a computing device such as the processor and memory described above as well as a user interface subsystem for receiving input from a user and presenting information to the user (e.g., text, imagery and/or other graphical elements).
- the user interface subsystem may include one or more user inputs (e.g., at least one front (user) facing camera, a mouse, keyboard, touch screen and/or microphone) and one or more display devices (e.g., a monitor having a screen or any other electrical device that is operable to display information (e.g., text, imagery and/or other graphical elements).
- Other output devices, such as speaker(s) may also provide information to users.
- the user-related computing devices may communicate with a back-end computing system (e.g., server 1302 ) via one or more networks, such as network 1310 .
- the network 1310 and intervening nodes, may include various configurations and protocols including short range communication protocols such as BluetoothTM, Bluetooth LETM, the Internet, World Wide Web, intranets, virtual private networks, wide area networks, local networks, private networks using communication protocols proprietary to one or more companies, Ethernet, WiFi and HTTP, and various combinations of the foregoing.
- short range communication protocols such as BluetoothTM, Bluetooth LETM, the Internet, World Wide Web, intranets, virtual private networks, wide area networks, local networks, private networks using communication protocols proprietary to one or more companies, Ethernet, WiFi and HTTP, and various combinations of the foregoing.
- Such communication may be facilitated by any device capable of transmitting data to and from other computing devices, such as modems and wireless interfaces.
- computing device 1302 may include one or more server computing devices having a plurality of computing devices, e.g., a load balanced server farm or cloud computing system, that exchange information with different nodes of a network for the purpose of receiving, processing and transmitting the data to and from other computing devices.
- computing device 1302 may include one or more server computing devices that are capable of communicating with any of the computing devices 1312 - 1314 via the network 1310 .
- FIG. 14 illustrates a process 1400 in accordance with aspects of the technology, which involves a computer-implemented method for providing multilingual text rewriting according to a machine learning model.
- the method comprising obtaining an input text sequence based on a set of text exemplars in a source language, and at block 1404 it comprises generating, by a corruption module, a corrupted version of the input text sequence.
- it includes receiving, by an encoder neural network of an encoder module, the corrupted version of the input text sequence.
- it includes generating, by the encoder module, a set of encoded representations of the corrupted version of the input text sequence.
- the method includes extracting, by a style extractor module, a set of style vector representations associated with the input text sequence.
- the method includes outputting, by the decoder module, a set of rewritten texts in a plurality of languages different from the source language, and at block 1416 it includes storing the rewritten text in selected ones of the plurality of languages according to a change in a least a sentiment or a formality of the input text sequence.
- a set of model weights is shared by the encoder module and the style extractor module, and a unique token is appended to the input text sequence for style extraction instead of mean-pooling all of the style vector representations in the set.
- the universal rewriter models discussed herein are extremely robust and are suitable for use with a number of computer-focused applications. This can include translation services from one language to another (or many), healthcare applications that support patients who may be most comfortable communicating in their native language, video streaming services that provide subtitles in a number of selectable languages, and videoconferencing services that may support a real-time closed-captioning feature for the users.
Abstract
Description
- Text translation into different languages is an important area that supports a wide variety of different tools and applications. The quality of results can differ widely depending on the specific languages, models used in translations, and how such models are trained. Certain models may be trained using zero-shot or few-shot approaches in limited translation instances but may not be sufficiently effective in multilingual translations or other text rewriting situations. For instance, it can be challenging to properly convey a particular style or level of formality in translations across various languages. It is also possible for “accidental translations” to occur, in which a model provides a correct answer but in the wrong language. There may also be significant resource tradeoffs between using larger models versus incurring excessive memory costs. These and other issues can adversely affect multilingual translation in conventional approaches.
- Aspects of the technology provide a model-based approach for multilingual text rewriting that is applicable across many languages and across different styles that may include various formality levels or other attributes of the text. The approach enables performance of attribute-controlled machine translation (e.g., dialect-aware translation, formality-controlled translations), as well as the ability to adapt text to a given domain or locale. One aspect involves leveraging a large pretrained multilingual model and fine-tuning it to perform general-purpose multilingual attribute transfer.
- The techniques and tools described herein establish that few-shot approaches can naturally extend to a multilingual setting. Multilingual models can endow text with additional attributes in one language using exemplars of that attribute from another language, according to a zero-shot style transfer. The model presented is capable of manipulating both language and textual attributes jointly. This approach can support zero-shot formality-sensitive translation, with no labeled data in the target language. There are many computer-based applications in which this technology can be greatly beneficial, including translation services, healthcare apps, video streaming services, videoconferencing, creative writing apps, etc.
- According to one aspect, a system is configured for a multilingual text rewriting model. The system comprises memory configured to store a set of text exemplars in a source language and a set of rewritten texts in a plurality of languages different from the source language, and one or more processing elements operatively coupled to the memory. The one or more processing elements implement a multilingual text rewriter as a neural network having a number of modules including: a corruption module configured to generate a corrupted version of an input text sequence based on the set of text exemplars in the source language; an encoder module comprising an encoder neural network configured to receive the corrupted version of the input text sequence and to generate a set of encoded representations of the corrupted version of the input text sequence; a style extractor module configured to extract a set of style vector representations associated with the input text sequence; and a decoder module comprising a decoder neural network configured to receive the set of encoded representations and the set of style vector representations and to output the set of rewritten texts in the plurality of languages. Each style vector representation of the set is added element-wise to one of the set of encoded representations. A set of model weights is shared by the encoder module and the style extractor module, and a unique token is appended to the input text sequence for style extraction instead of mean-pooling all of the style vector representations in the set. The system is configured to provide rewritten text in selected ones of the plurality of languages according to a change in a least a sentiment or a formality of the input text sequence.
- In one example, the encoder module, style extractor module and the decoder module are configured as transformer stacks initialized from a common pretrained language model. Alternatively or additionally, both the encoder module and the decoder module are attention-based neural network modules. Alternatively or additionally, the corruption module employs a corruption function C for a given pair of non-overlapping spans (s1, s2) of the input text sequence, so that the model is capable of reconstructing span s2 from C(s2) and Vs1, where Vs1 is an attribute vector of span s1. Here, the corruption function C may be as follow: C:=ƒ(⋅, source language, −Vs1). Alternatively or additionally, the style extractor module may include a set of style extractor elements including a first subset configured to operate on different style exemplars and a second subset configured to operate on the input text sequence prior to corruption by the corruption module.
- During training, the corruption module may be configured to employ at least one of token-level corruption or style-aware back-translation corruption. Alternatively or additionally, the set of model weights may be initialized with the weights of a pretrained text-to-text model. Alternatively or additionally, the model weights for the style extractor module may not be tied to the weights of the encoder module during training.
- The system may be configured to extract pairs of random non-overlapping spans of tokens from each line of text in a given text exemplar, and configured to use a first-occurring span in the text as an exemplar of attributes of a second span in the text. Alternatively or additionally, a cross-lingual learning signal can be added to a training objective for training the model. Alternatively or additionally, the encoder module may be configured to use negation of a true exemplar vector associated with the input text sequence as an attribute vector in a forward pass operation. Alternatively or additionally, the decoder module may be configured to receive a set of stochastic tuning ranges that provide conditioning for the decoder module.
- In an example, the set of style vector representations corresponds to a set of attributes associated with the input text sequence, and the system is configured to input a set of exemplars illustrating defined attributes and to extract corresponding attribute vectors for use in rewriting the input text sequence into the plurality of languages according to the defined attributes. In this case, the system may be configured to form an attribute delta vector including a scale factor, in which the attribute delta vector is added to the set of encoded representations before processing by the decoder module.
- According to another aspect, a computer-implemented method for providing multilingual text rewriting according to a machine learning model is provided. The method comprises: obtaining an input text sequence based on a set of text exemplars in a source language; generating, by a corruption module, a corrupted version of the input text sequence; receiving, by an encoder neural network of an encoder module, the corrupted version of the input text sequence; generating, by the encoder module, a set of encoded representations of the corrupted version of the input text sequence; extracting, by a style extractor module, a set of style vector representations associated with the input text sequence; receiving, by a decoder neural network of a decoder module, the set of encoded representations and the set of style vector representations, in which each style vector representation of the set is added element-wise to one of the set of encoded representations; outputting, by the decoder module, a set of rewritten texts in a plurality of languages different from the source language; and storing the rewritten text in selected ones of the plurality of languages according to a change in a least a sentiment or a formality of the input text sequence. A set of model weights is shared by the encoder module and the style extractor module, and a unique token is appended to the input text sequence for style extraction instead of mean-pooling all of the style vector representations in the set.
- In one example, generating the corrupted version of the input text sequence is performed according to a corruption function C for a given pair of non-overlapping spans (s1, s2) of the input text sequence, so that the model is capable of reconstructing span s2 from C(s2) and Vs1, where Vs1 is an attribute vector of span s1. Alternatively or additionally to any of the above, the method further comprising: extracting pairs of random non-overlapping spans of tokens from each line of text in a given text exemplar; and using a first-occurring span in the text as an exemplar of attributes of a second span in the text. Alternatively or additionally, the method further comprises adding a cross-lingual learning signal to a training objective for training the model. Alternatively or additionally, the method further comprises applying a set of stochastic tuning ranges to selectively condition the decoder module. In another example, the set of style vector representations corresponds to a set of attributes associated with the input text sequence. Here, outputting the set of rewritten texts includes generating one or more versions of the input text sequence in selected ones of the plurality of languages according to the set of attributes.
-
FIG. 1 illustrates an example of rewriting text to exhibit new attributes in accordance with aspects of the technology. -
FIG. 2 illustrates a Transformer-type architecture for use in accordance with aspects of the technology. -
FIGS. 3A-B illustrates a style extractor architecture for use in accordance with aspects of the technology. -
FIG. 4 illustrates a table showing BLEU scores of various supervised and unsupervised models for low-resource language pairs in accordance with aspects of the technology. -
FIG. 5 illustrates another table showing examples of a multilingual rewriting model performing sentiment transfer in accordance with aspects of the technology. -
FIGS. 6A-B illustrate examples sets of positive and negative exemplars, respectively. -
FIG. 7 illustrates how model behavior can depend on the inference-time parameter. -
FIGS. 8A-L measure content preservation versus transfer accuracy as both language and sentiment are changed at the same time, in accordance with aspects of the technology. -
FIGS. 9A-B illustrate formal and informal exemplars for testing aspects of the technology. -
FIGS. 10A-C illustrate tables of different levels of formality in accordance with aspects of the technology. -
FIG. 11 presents a table showing average formality and translation quality scores for a human evaluation of system operation. -
FIG. 12 presents a table showing the T-V distinction accuracy for formality-aware Spanish translations in accordance with aspects of the technology. -
FIGS. 13A-B illustrate a system for use with aspects of the technology. -
FIG. 14 illustrates a method in accordance with aspects of the technology. - The technology provides an encoder-decoder architectural approach with attribute extraction to train rewriter models that can be used in “universal” textual rewriting across many different languages. A cross-lingual learning signal is incorporated into the training approach. Certain training processes do not employ any exemplars. This approach enables not just straight translation, but also the ability to create new sentences with different attributes.
-
FIG. 1 illustrates a general example 100, which enables rewriting text to exhibit new attributes. For instance, in this examplemultilingual rewriter module 102 receives aninput sentence 102 in Chinese having one type of (negative) sentiment (“This soup isn't very good”). Therewriter module 102 is able to produce anoutput sentence 104 in Spanish having a different type of (positive) sentiment (“This soup is delicious!”). The model employed by themultilingual rewriter 102 enables rewriting text to exhibit new attributes, such as changes in language, sentiment or formality. Here, the target language (Spanish) 108 is signaled by a unique language code. Other attributes are controlled through few-shotexemplars 110, which may leverage distinct languages from the input and the target. In addition, transfer scale (λ) 112 modulates the strength of the attribute transfer. The transfer scale helps control how much preference the model can give to the desired style versus preserving the input. At one extreme, a very large scale would lead to the model completely ignoring the input and producing a random sentence in the target style. However, control of this parameter can enable the system to attain all values in the spectrum of a given style. For example, for the sentence “The movie was okay”, if there is a goal for a more positive sentence, once could potentially attain “The movie was good”, “The movie was great”, and “The movie was amazing!” by using larger and large transfer scales. - Aspects of the technology are used to test the conjecture that large models trained on massive multilingual corpora should be able to “identify” a textual attribute (e.g., formality) given just a few exemplars in one language (e.g., English), and “apply” the underlying concept to a generative task in another language in a zero-shot fashion (e.g., formality transfer in Chinese). This can be tested by leveraging a large pretrained multilingual model and fine-tuning it to perform general-purpose multilingual attribute transfer.
- Note that in the following discussion, the terms “attributes” and “styles” are used interchangeably. The model can go beyond traditional tasks associated with style transfer, for example extending to translation (language transfer) and style-conditioned translation. The term “exemplars” is used to denote inputs show-casing a particular attribute.
- The machine translation rewrite model discussed herein may employ a neural network such as a convolutional neural network (CNN) or a recurrent neural network (RNN), e.g., a bidirectional long short-term memory (Bi-LSTM) RNN. Alternatively or additionally, the model may employ a self-attention architecture. This may include an approach such as the Transformer neural network encoder-decoder architecture. An exemplary Transformer-type architecture is shown in
FIG. 2 , which is based on the arrangement shown in U.S. Pat. No. 10,452,878, entitled “Attention-based sequence transduction neural networks”, the entire disclosure of which is incorporated herein by reference. -
System 200 ofFIG. 2 is implementable as computer programs by processors of one or more computers in one or more locations. Thesystem 200 receives aninput sequence 202 and processes theinput sequence 202 to transduce theinput sequence 202 into an output sequence 204. Theinput sequence 202 has a respective network input at each of multiple input positions in an input order and the output sequence 204 has a respective network output at each of multiple output positions in an output order. -
System 200 can perform any of a variety of tasks that require processing sequential inputs to generate sequential outputs.System 200 includes an attention-based sequence transductionneural network 206, which in turn includes an encoderneural network 208 and a decoderneural network 210. The encoderneural network 208 is configured to receive theinput sequence 202 and generate a respective encoded representation of each of the network inputs in the input sequence. Generally, an encoded representation is a vector or other ordered collection of numeric values. The decoderneural network 210 is then configured to use the encoded representations of the network inputs to generate the output sequence 204. Generally, both theencoder 208 and thedecoder 210 are attention-based. In some cases, neither the encoder nor the decoder includes any convolutional layers or any recurrent layers. The encoderneural network 208 includes an embedding layer (input embedding) 212 and a sequence of one ormore encoder subnetworks 214. The encoder neural 208 network mayN encoder subnetworks 214. - The embedding
layer 212 is configured, for each network input in the input sequence, to map the network input to a numeric representation of the network input in an embedding space, e.g., into a vector in the embedding space. The embeddinglayer 212 then provides the numeric representations of the network inputs to the first subnetwork in the sequence ofencoder subnetworks 214. The embeddinglayer 212 may be configured to map each network input to an embedded representation of the network input and then combine, e.g., sum or average, the embedded representation of the network input with a positional embedding of the input position of the network input in the input order to generate a combined embedded representation of the network input. In some cases, the positional embeddings are learned. As used herein, “learned” means that an operation or a value has been adjusted during the training of the sequence transductionneural network 206. In other cases, the positional embeddings may be fixed and are different for each position. - The combined embedded representation is then used as the numeric representation of the network input. Each of the
encoder subnetworks 214 is configured to receive a respective encoder subnetwork input for each of the plurality of input positions and to generate a respective subnetwork output for each of the plurality of input positions. The encoder subnetwork outputs generated by the last encoder subnetwork in the sequence are then used as the encoded representations of the network inputs. For the first encoder subnetwork in the sequence, the encoder subnetwork input is the numeric representations generated by the embeddinglayer 212, and, for each encoder subnetwork other than the first encoder subnetwork in the sequence, the encoder subnetwork input is the encoder subnetwork output of the preceding encoder subnetwork in the sequence. - Each
encoder subnetwork 214 includes an encoder self-attention sub-layer 216. The encoder self-attention sub-layer 216 is configured to receive the subnetwork input for each of the plurality of input positions and, for each particular input position in the input order, apply an attention mechanism over the encoder subnetwork inputs at the input positions using one or more queries derived from the encoder subnetwork input at the particular input position to generate a respective output for the particular input position. In some cases, the attention mechanism is a multi-head attention mechanism as shown. In some implementations, each of theencoder subnetworks 214 may also include a residual connection layer that combines the outputs of the encoder self-attention sub-layer with the inputs to the encoder self-attention sub-layer to generate an encoder self-attention residual output and a layer normalization layer that applies layer normalization to the encoder self-attention residual output. These two layers are collectively referred to as an “Add & Norm” operation inFIG. 2 . - Some or all of the encoder subnetworks can also include a position-wise feed-
forward layer 218 that is configured to operate on each position in the input sequence separately. In particular, for each input position, the feed-forward layer 218 is configured receive an input at the input position and apply a sequence of transformations to the input at the input position to generate an output for the input position. The inputs received by the position-wise feed-forward layer 218 can be the outputs of the layer normalization layer when the residual and layer normalization layers are included or the outputs of the encoder self-attention sub-layer 216 when the residual and layer normalization layers are not included. The transformations applied by thelayer 218 will generally be the same for each input position (but different feed-forward layers in different subnetworks may apply different transformations). - In cases where an
encoder subnetwork 214 includes a position-wise feed-forward layer 218 as shown, the encoder subnetwork can also include a residual connection layer that combines the outputs of the position-wise feed-forward layer with the inputs to the position-wise feed-forward layer to generate an encoder position-wise residual output and a layer normalization layer that applies layer normalization to the encoder position-wise residual output. As noted above, these two layers are also collectively referred to as an “Add & Norm” operation. The outputs of this layer normalization layer can then be used as the outputs of theencoder subnetwork 214. - Once the encoder
neural network 208 has generated the encoded representations, the decoderneural network 210 is configured to generate the output sequence in an auto-regressive manner. That is, the decoderneural network 210 generates the output sequence, by at each of a plurality of generation time steps, generating a network output for a corresponding output position conditioned on (i) the encoded representations and (ii) network outputs at output positions preceding the output position in the output order. In particular, for a given output position, the decoder neural network generates an output that defines a probability distribution over possible network outputs at the given output position. The decoder neural network can then select a network output for the output position by sampling from the probability distribution or by selecting the network output with the highest probability. - Because the decoder
neural network 210 is auto-regressive, at each generation time step, thedecoder network 210 operates on the network outputs that have already been generated before the generation time step, i.e., the network outputs at output positions preceding the corresponding output position in the output order. In some implementations, to ensure this is the case during both inference and training, at each generation time step the decoderneural network 210 shifts the already generated network outputs right by one output order position (i.e., introduces a one position offset into the already generated network output sequence) and (as will be described in more detail below) masks certain operations so that positions can only attend to positions up to and including that position in the output sequence (and not subsequent positions). While the remainder of the description below describes that, when generating a given output at a given output position, various components of thedecoder 210 operate on data at output positions preceding the given output positions (and not on data at any other output positions), it will be understood that this type of conditioning can be effectively implemented using shifting. - The decoder
neural network 210 includes an embedding layer (output embedding) 220, a sequence ofdecoder subnetworks 222, alinear layer 224, and asoftmax layer 226. In particular, the decoder neural network can includeN decoder subnetworks 222. However, while the example ofFIG. 2 shows theencoder 208 and thedecoder 210 including the same number of subnetworks, in some cases theencoder 208 and thedecoder 210 include different numbers of subnetworks. The embeddinglayer 220 is configured to, at each generation time step, for each network output at an output position that precedes the current output position in the output order, map the network output to a numeric representation of the network output in the embedding space. The embeddinglayer 220 then provides the numeric representations of the network outputs to thefirst subnetwork 222 in the sequence of decoder subnetworks. - In some implementations, the embedding
layer 220 is configured to map each network output to an embedded representation of the network output and combine the embedded representation of the network output with a positional embedding of the output position of the network output in the output order to generate a combined embedded representation of the network output. The combined embedded representation is then used as the numeric representation of the network output. The embeddinglayer 220 generates the combined embedded representation in the same manner as described above with reference to the embeddinglayer 212. - Each
decoder subnetwork 222 is configured to, at each generation time step, receive a respective decoder subnetwork input for each of the plurality of output positions preceding the corresponding output position and to generate a respective decoder subnetwork output for each of the plurality of output positions preceding the corresponding output position (or equivalently, when the output sequence has been shifted right, each network output at a position up to and including the current output position). In particular, eachdecoder subnetwork 222 includes two different attention sub-layers: a decoder self-attention sub-layer 228 and an encoder-decoder attention sub-layer 230. Each decoder self-attention sub-layer 228 is configured to, at each generation time step, receive an input for each output position preceding the corresponding output position and, for each of the particular output positions, apply an attention mechanism over the inputs at the output positions preceding the corresponding position using one or more queries derived from the input at the particular output position to generate a updated representation for the particular output position. That is, the decoder self-attention sub-layer 228 applies an attention mechanism that is masked so that it does not attend over or otherwise process any data that is not at a position preceding the current output position in the output sequence. - Each encoder-
decoder attention sub-layer 230, on the other hand, is configured to, at each generation time step, receive an input for each output position preceding the corresponding output position and, for each of the output positions, apply an attention mechanism over the encoded representations at the input positions using one or more queries derived from the input for the output position to generate an updated representation for the output position. Thus, the encoder-decoder attention sub-layer 230 applies attention over encoded representations while the decoder self-attention sub-layer 228 applies attention over inputs at output positions. - In the example of
FIG. 2 , the decoder self-attention sub-layer 228 is shown as being before the encoder-decoder attention sub-layer in the processing order within thedecoder subnetwork 222. In other examples, however, the decoder self-attention sub-layer 228 may be after the encoder-decoder attention sub-layer 230 in the processing order within thedecoder subnetwork 222 or different subnetworks may have different processing orders. In some implementations, eachdecoder subnetwork 222 includes, after the decoder self-attention sub-layer 228, after the encoder-decoder attention sub-layer 230, or after each of the two sub-layers, a residual connection layer that combines the outputs of the attention sub-layer with the inputs to the attention sub-layer to generate a residual output and a layer normalization layer that applies layer normalization to the residual output. These two layers being inserted after each of the two sub-layers, both referred to as an “Add & Norm” operation. - Some or all of the decoder subnetwork 170 also include a position-wise feed-
forward layer 232 that is configured to operate in a similar manner as the position-wise feed-forward layer 218 from theencoder 208. In particular, thelayer 232 is configured to, at each generation time step: for each output position preceding the corresponding output position: receive an input at the output position, and apply a sequence of transformations to the input at the output position to generate an output for the output position. The inputs received by the position-wise feed-forward layer 232 can be the outputs of the layer normalization layer (following the last attention sub-layer in the subnetwork 222) when the residual and layer normalization layers are included or the outputs of the last attention sub-layer in thesubnetwork 222 when the residual and layer normalization layers are not included. In cases where adecoder subnetwork 222 includes a position-wise feed-forward layer 232, the decoder subnetwork can also include a residual connection layer that combines the outputs of the position-wise feed-forward layer with the inputs to the position-wise feed-forward layer to generate a decoder position-wise residual output and a layer normalization layer that applies layer normalization to the decoder position-wise residual output. These two layers are also collectively referred to as an “Add & Norm” operation. The outputs of this layer normalization layer can then be used as the outputs of thedecoder subnetwork 222. - At each generation time step, the
linear layer 224 applies a learned linear transformation to the output of thelast decoder subnetwork 222 in order to project the output of thelast decoder subnetwork 222 into the appropriate space for processing by thesoftmax layer 226. Thesoftmax layer 226 then applies a softmax function over the outputs of thelinear layer 224 to generate the probability distribution (output probabilities) 234 over the possible network outputs at the generation time step. Thedecoder 210 can then select a network output from the possible network outputs using the probability distribution. - One baseline configuration that can be employed with aspects of the technology employs an English-language few-shot style transfer model encoder decoder Transformer, which is equipped with an additional “style extractor” that is a variation of the encoder
neural network 208 discussed above. This configuration removes the need for training labels, and offers a single model that can target an unrestricted set of style attributes. Aspects of this approach are discussed in detail in “TextSETTR: Few-Shot Text Style Extraction and Tunable Targeted Restyling”, by Riley et al., Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, © 2021, the entire disclosure of which is incorporated herein by reference. - Here, the style extractor takes exemplars of a style as input and extracts a fixed-width “style vector” by mean-pooling the output embeddings across tokens. These style vectors (and combinations thereof) can then be added element-wise to the encoded representations of a given input before being passed to the decoder in order to induce targeted style changes.
FIG. 3A illustrates anexample training configuration 300, andFIG. 3B illustrates anexample inference configuration 320. - The
training configuration 300 shows an architecture for few-shot style transfer.Encoder 302,decoder 304 andstyle extractor 306 are transformer stacks initialized from a pretrained language model such as T5, which is trained using teacher forcing. During training, the model reconstructs a corrupted input provided bycorruption block 308, conditioned on a fixed-width “style vector” extracted from the preceding sentence. Thecorruption block 308 may employ two types of corruption: token-level and style-aware back-translation. In the former, a noise function is applied independently to each token, with a probability of either dropping a token, replacing it with one of the examples in the current batch at the same position or keeping it as is. In the latter, the model is used in inference mode to change the style of the input to match the style of a random sentence in the batch, and the resulting output is used as a corrupted version of the input. Thecorruption block 308 corrupts the input by (i) dropping, (ii) replacing, and/or (iii) shuffling tokens, which may be performed in that order. The training occurs in an unsupervised fashion, so the model can handle arbitrary styles at inference time. To compensate for the lack of training labels, this architecture relies on the intuition that style is a “slow-moving feature”, consistent across long spans of text. - One or more stochastic tuning ranges 310 can be used to provide extra conditioning for the
decoder 304, and enable fine-grained control of inference. This can include different “add” and “delete” rates. By way of example, for every input/output pair during training, the system can calculate the proportions of tokens that were added and deleted. The “add rate” is the proportion of output tokens absent from the input, and the “delete rate” is the proportion of input tokens absent from the output. These rates can be provided to the decoder as ranges covering but not necessarily centered on the “true” rates. For instance, each range width may be sampled uniformly from [0,1], and uniformly sample the “alignment” of the true rate within the range. In this approach, the final ranges are clipped to [0,1], and a vector containing the upper and lower bound of each range is prepended to the encoder hidden state sequence. This approach provides more flexibility at inference time, so the system can enforce tight or loose constraints on each rate. - To incorporate the style vector into the rest of the model, it is added to each of the final encoder hidden states. The weights of the model may be initialized with those of a pretrained (e.g., T5) model. Both the style extractor and the encoder can be initialized from the pretrained encoder, but the weights need not be tied during training.
- As shown in the
example inference configuration 320 ofFIG. 3B ,style extractor block 322 has a set ofstyle extractor elements 324, which may operate on different style exemplars (e.g., styles “A” and “B”), and also on the input prior to corruption. λ is the delta scale, which can be an important hyperparameter to tune. It has been found that values in the range [1.0, 10.0] work well, with the best values depending on the attribute and the exemplars in question. Stochastic tuning ranges 326 may be the same or different from those used during training. With this approach, at inference time a new style vector is formed via “targeted restyling”, which adds a directional delta to the extracted style of the input text. - In order to make the model robust and applicable to “universal” text rewriting, one aspect of the technology includes not separating the attribute extraction module (e.g.,
style extractor 306 inFIG. 3A ) from the encoder-decoder architecture. Instead, all the weights are shared between this module and the encoder. In order to distinguish the two, the system prepends the text with a unique token when performing attribute extraction, and takes the representation of this token from the encoder's output as the fixed-width vector representation, instead of mean-pooling all the representations. This allows the multilingual architecture to leverage larger models without incurring excessive memory costs. - While one approach that is focused on a particular language employs a sentence-splitting algorithm that is based on detecting English punctuation, it has been found that such an approach can lead to problems for some languages. In particular, sentence-splitting during preprocessing of non-English text may discard any data that does not include ASCII punctuation. For languages like Chinese and Thai, which do not typically use ASCII punctuation, this would filter out most well-formed text, leaving mainly text in the wrong language, or a non-natural language (e.g., Javascript code). This could adversely affect system operation, so a more language-agnostic approach has been adopted. Here, the system extracts pairs of random non-overlapping spans of tokens from each line of text, and uses the first-occurring span in the text as an exemplar of the attributes of the second span. This approach allows the system to retain all data, is independent of language, and still exploits the “slow-moving feature” intuition discussed above.
- In one scenario, the system starts with an mT5 checkpoint as the initial model, for instance the XL variant. While it is possible to use style-aware back-translation, according to one aspect of the technology an additional cross-lingual learning signal is added to the training objective by forcing the model to not only perform an attribute transfer procedure but also to translate the sentence to a baseline language such as English. English is particularly suitable due to the availability of parallel data between English and many other languages. To make this objective more similar to the cycle-consistency found in traditional back-translation, the system can use the negation of the “true” exemplar vector associated with the input (as opposed to leveraging a random exemplar) as the attribute vector in the forward pass. More explicitly, let ƒ be the function which takes a triplet of (text, language, attribute vector), then proceed to pass the text through the encoder, add the attribute vector element-wise to the encoded representations, then decodes the result into the given language through sampling. For a given pair (s1, s2) of non-overlapping spans, one can define the corruption function C as follow:
-
C:=ƒ(⋅, English, −V s1) - where Vs1 is the attribute vector of the first span, and the model is tasked with reconstructing s2 from C(s2) and Vs1. The intuition behind the negative exemplar is that it is desirable for the model to learn that the changes induced by an attribute vector should be undone when applying the same procedure with the negative of this attribute vector. Note that the system can perform the decoding operation using sampling, e.g., with a temperature of 1.5.
- In addition to the cross-lingual treatment of the style-aware back-translation objective, the approach also includes translation as another cross-lingual learning objective by leveraging English-centric translation data for some language pairs. For this task, exemplars are not used.
- There are multiple ways one could use the attribute vectors to perform rewriting. Suppose the system is provided with sets of exemplars illustrating attributes A and B, which are used to extract attribute vectors VA and VB, respectively. Given an input x with attribute A, the system should rewrite it to exhibit attribute B. One inference strategy is to first extract the attribute vector Vx of x, then form the following attribute delta vector:
-
V A,B,x :=V x+λ(V B −V A) - In this case, λ is a scale factor, which may be chosen by the user. The resulting vector VA,B,x then gets added to the encoded representation of x before being passed to the decoder. For within-language tasks, where the output should be in the same language as the input, the system can use this inference strategy.
- However, in certain cross-language experiments, it was found the model was more reluctant to change languages when using this strategy. For this reason, according to one aspect of the technology the system does not include the vector Vx in the computation of VA,B,x for cross-language tasks, and instead uses the following equation:
-
V A,B,x:=λ(V B −V A) - This section describes certain experiments using the multilingual architecture. First is a discussion of the selection of languages and data sources considered, as well as data preprocessing steps. Next, the performance of the models is measured in the challenging tasks of low-resource and unsupervised machine translation which is viewed as rewriting text in a different language. After this experiment, a multilingual variant of the sentiment transfer task is introduced, which allows for exploration of the interaction between multilinguality and style transfer. Finally, consider the problem of multiple attribute rewriting through two cross-lingual sentence rewriting tasks and zero-shot formality-aware machine translation task for the specific language pair English-Spanish.
- According to the experimental setup, the system drew monolingual data from mC4, the same dataset used to train mT5. The “-Latin” languages were removed due to their small data size. Parallel data for 46 of these languages with English was leveraged, by taking all language pairs with more than 500,000 lines of parallel data from the OPUS 100 (dataset, with the exception of Hebrew, Croatian, Bosnian (because their language codes in
OPUS 100 were not present in the language codes of mC4), and Japanese. Japanese was excluded as it appears in the sentiment evaluation, and it was desirable to test the model's ability to perform zero-shot attribute transfer in languages where no parallel data is available. As Japanese has a unique script and no genealogical relation with other languages under consideration, it poses a particularly challenging case. The two-character language codes for the utilized languages are as follows: ar, bg, bn, ca, cs, da, de, el, es, et, eu, fa, fi, fr, gl, hi, hu, id, is, it, ko, lt, lv, mg, ink, ml, ms, mt, nl, no, pl, pt, ro, ru, si, sk, sl, sq, sr, sv, th, tr, uk, ur, vi, and zh. - Both parallel and monolingual datasets were used for training the rewriter models. For each batch, a random selection was made to select to sample from either the monolingual or parallel datasets (with equal likelihood), and then uniformly sample from all available datasets in the chosen category.
- The monolingual data was preprocessed into input-exemplar pairs, using the approach described above. In addition, any pair was discarded where either element is shorter than five tokens. In the case of parallel data, no exemplars were leveraged, and instead used a vector of zeros. For both data sources, any training example with either input, exemplar or target (in the case of parallel data) longer than 64 tokens was discarded. Language-specific tokens were pre-pended to the input to prime the model for translations. Instead of introducing new tokens, the tokens associated to the target language names in English were reused.
- The models were trained in JAX (discussed in “JAX: composable transformations of Python+NumPy programs” by Bradbury et al., 2018). The Adafactor optimizer was used (discussed in “Adafactor: Adaptive learning rates with sublinear memory cost”, in Proceedings of the 35th International Conference on Machine Learning,
volume 80 of Proceedings of Machine Learning Research, pages 4596-4604). The accumulated optimizer states were reused from the original mT5 training. A constant learning rate of 1e-3 was used and trained for 25,000 steps, using batches of 512 inputs. Both the Jax and Adafactor references are incorporated herein by reference. - For machine translation, the system utilized BLEU scores (as discussed in “Bleu: a method for automatic evaluation of machine translation”, 2002, in Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311-318). The scores were computed through the sacreBLEU5 library (see “A call for clarity in reporting BLEU scores”, 2018, in Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186-191). The output was tokenized for the Indic languages using the tokenizer provided by the Indic-NLP library (see “Indic NLP Library, A unified approach to NLP for Indian languages by Anoop Kunchukuttan, 2020). For decoding, a beam search was used, with a beam size of 5 for machine translation, within-language sentiment transfer and formality-aware machine translation, and beam size of 1 for cross-language sentiment transfer. Each of the identified references is incorporated herein by reference.
- Aspects of the technology introduce several baselines, which follow a similar training recipe as the general rewriter model discussed herein, but with some features removed. The baselines include no parallel data (-para); no language tokens (-lang tokens); no exemplars in training (-exemplars); and no back-translation (-BT). The “-para” setting tests whether the additional supervision signal obtained from the parallel data is necessary or even useful, and the “-lang tokens” setting addresses a similar question regarding language tokens. The “-exemplars” setting tests whether we suffer any degradation in translation quality by introducing the ability to leverage exemplars at inference time. Finally, given the expensive nature of back-translation, the “-BT” setting tests whether this objective is truly necessary.
- Parallel data only accounts for a fraction of the languages available in mC4, yet there is interest in rewriting across all languages in mC4, potentially changing languages and attributes at the same time. To ensure the model is capable of such cross-lingual tasks, the translation quality is inspected on a selection of low-resource languages. Aspects of the technology study both languages with parallel data as well as those without it, a setting referred to as “unsupervised machine translation by language transfer” or “multilingual unsupervised machine translation”. In the latter setting, there is a consideration of the translation quality for low-resource English-centric pairs where the associated low-resource languages have no parallel data at all, with English or otherwise.
- According to one set of tests, Turkish, Gujarati, and Kazakh were selected as low-resource languages, using the available newstest test sets from WMT, using newstest2018 for Turkish and newstest2019 for Gujarati and Kazakh. Of these three languages, in one example the model only sees parallel data for the language pair English-Turkish.
FIG. 4 illustrates Table 1, which presents BLEU scores of various supervised and unsupervised models for low-resource language pairs. - Table 1 compares the instant Universal Rewriter model with the bilingual unsupervised translation models and large multilingual unsupervised translation models of certain prior approaches and their analogous supervised variants. To control the output language in the baseline with no language tokens, during testing the system leveraged 214 (16,384) examples of monolingual data for each language with λ=0.5, chosen from {0.5, 1.0, 1.5} by looking at devset performance. For testing, this data was randomly drawn from the newscrawl datasets.
- Notably, the rewriter model attains comparable performance with the external baselines, despite not being directly trained for unsupervised translation. It is interesting to note that the baseline with no parallel data collapses, failing to produce the correct language and giving low BLEU scores. A similar pattern for the baseline is seen without language tokens (-lang tokens) when translating into languages without parallel data. On the other hand, the remaining baselines that leverage parallel data manage to attain reasonable scores (even for language pairs without parallel data), justifying the inclusion of the parallel data. Finally, it is observed that removing the ability to control text attributes via exemplars (-exemplars) confers no meaningful benefit for translation quality. This finding supports the view that a single general model can be well-suited to adjust both language and other textual attributes.
- Next, turning to style transfer in a multilingual setting, it is demonstrated for the first time the possibility of performing attribute transfer in one language using only exemplars from another language. Specifically, it is observed that Universal Rewriter model discussed herein can transfer sentiment in French, German and Japanese (as shown in Table 2 of
FIG. 5 ), despite having seen no sentiment labels in those languages, only four sentiment-labeled in English, and no labeled data whatsoever in Japanese. Here, for this test the system used λ=5.0, and defined sentiment using just four English exemplars: {I loved it./The movie was great./I hated it./The movie was awful.} - It is noted that previous work on multilingual style transfer has been limited to a less ambitious setting where (i) models are trained for transferring one specific attribute, (ii) a large corpus of labels is provided at training time, and (iii) the labels cover all target languages. By contrast, the current approach is attribute-agnostic (one model can transfer arbitrary attributes), few-shot (only using labels at inference time), and zero-shot across languages.
- A specific multilingual style transfer evaluation was developed for testing, crafted from an existing dataset for multilingual classification. In particular, this evaluation leverages the multilingual Amazon reviews dataset, which was presented by Prettenhofer and Stein. In “Cross-language text classification using structural correspondence learning”, 2010, in Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1118-1127 (incorporated herein by reference). This dataset consists of Amazon reviews in English, French, German and Japanese with review ratings in the range 1-5. For testing, the approach treated reviews rated 4 or 5 as positive, and reviews rated 1 or 2 as negative, and dropped reviews rated 3. mBERT was fine-tuned (the multilingual variant of BERT) on the remaining reviews and treated it as an oracle sentiment classifier, splitting the reviews into training and development sets. This model achieved 80.1% accuracy on this development set.
- The Universal Rewriter model was compared with various ablation models on the task of transferring negative to positive sentiment. For evaluation, examples longer than 61 tokens were removed, samples were disregarded that disagreed with the oracle classifier, only keeping those which it assigned above 80% probability of being the correct class. Prepending two tokens to every example was done to control the language, and one end-of-sentence token. This gave a maximum of 64 tokens, which is the maximum length that the models were trained on. Development and test sets were constructed from the remaining examples, consisting of 800 examples, 200 from each language, equally balanced in positive and negative reviews.
- For this experiment, there were three questions of interest: (1) Are English exemplars sufficient to perform sentiment transfer in non-English languages? (2) Is the Universal Rewriter model able to simultaneously transfer sentiment and language (multiple attribute rewriting? And (3) Is the use of parallel data necessary to achieve either of the above goals? To study the first question, the system leverage twenty (total) handcrafted English exemplars, exhibiting positive and negative sentiment for all experiments.
FIGS. 6A and 6B illustrate the sets of positive and negative exemplars, respectively. - To address the third question, the testing purposely excluded English-Japanese parallel data. Given Japanese's unique script, it was believed performance on Japanese sentiment transfer and translation is a reasonable proxy for the low-resource languages within mC4 for which there may be difficulties obtaining parallel data. Since model behavior depends on the inference-time parameter λ, results are shown in chart 700 of
FIG. 7 for a wide array of A values (sweeping from 0.5 to 9.0 in increments of 0.5) to examine the influence of this parameter. For this task, the following two metrics were used: (i) self-BLEU, defined as the BLEU score using the inputs as references and the model's outputs as hypotheses, and (ii) transfer accuracy, defined as the average sentiment transfer accuracy as judged by our classifier. As shown inFIG. 7 , the λ parameter allows control of the content preservation vs. transfer accuracy trade-off, accurately changing attributes while still preserving content and language. Note that the general shape of the baseline without language tokens (-lang tokens) 710 is similar toUniversal Rewriter 702, only shifted to the left. Upon inspection, it was found this to be due to wrong-language errors, which negatively affected the self-BLEU score. - Whether the model is capable of rewriting multiple attributes in one pass can be evaluated by assessing its ability to perform sentiment transfer and translation simultaneously. For such testing, the same exemplars, development and test sets were reused as in the previous experiment. Here, instead of simply transferring sentiment, the system also varies the target language to produce translations into each other language. In this cross-lingual setting, defining content preservation is more challenging, since self-BLEU is no longer a meaningful measure as the input and target could be of potentially very different languages. Given the strong performance of the universal rewriter model on translation as shown above, “neutral” translations were generated using λ=0 and treating those as gold labels. Then self-BLEU was redefined to be the BLEU score computed using these predictions as references. As λ increases one can expect to see more changes relative to the neutral translations, but also an increase in transfer accuracy.
- Three levels of supervision were considered: (1) unsupervised translation for the language pairs Japanese↔{English, French, German}, (2) zero-shot translation for the language pairs French↔German, which lack parallel data but both languages have parallel data with English, and (3) supervised translation for the language pairs {French, German}↔English, which have parallel data. Note that for the zero-shot (and some of the unsupervised) pairs, exemplars are in a different language from both the input and output, which acts as an additional test of the cross-linguality of the model.
-
FIGS. 8A-L measure content preservation vs. transfer accuracy as both language and sentiment are changed at the same time. For this task, λ was varied between 0.5 and 5.0 with 0.5 increments. The “-lang tokens” baseline was omitted since there was no explicit way of controlling both language and style for this baseline. As in the within-language case, the Universal Rewriter outperformed all the ablations, as presented in the various results inFIGS. 8A-L . Here,FIGS. 8A-F show plots of different unsupervised language pairings, comparing transfer accuracy (Y axis) to self-BLEU values (X axis).FIGS. 8G-H show zero-shot language pairs, evaluating transfer accuracy (Y axis) to self-BLEU values (X axis). AndFIGS. 81 -L show supervised language pairs, again evaluating transfer accuracy (Y axis) to self-BLEU values (X axis). - As a more practical test of the model's ability to transfer multiple attributes in one pass, formality-sensitive translation can also be evaluated. Such a task requires the model to translate input text into another language, while simultaneously controlling the formality level of the output text. This type of control is useful when the desired formality level is known ahead of time, and may not match that of the source text—for example, when providing translations for second language learners, or for use within an assistive agent.
- For this test, assume labels are only available in English, and in limited quantity. Specifically, 10 English exemplars were constructed (total) of formal and informal language, as shown in
FIGS. 9A and 9B , respectively. - Tables 3A-C of
FIGS. 10A-C show a successful example of Universal Rewriter translating an input sentence into four languages at three levels of formality. Table 3A ofFIG. 10A is an informal level, table 3B ofFIG. 10B is a neutral amount of formality. And table 3C ofFIG. 10C is a more formal level. Native speaker consultants confirmed that, while not every translation is perfectly natural, they are all understandable, and the formality level clearly increases going from informal to neutral to formal. Interestingly, these differences manifest in both lexical and grammatical choices, including the use of formal vs. informal pronouns in Chinese, Russian and Spanish, despite this distinction being absent in English. - For a more thorough examination, focus on the language pair English-Spanish. Here, in one test 50 sentences were first randomly sampled (in which sentences containing toxic words or ungrammatical text, as well as those longer than 64 tokens or shorter than 3 tokens were manually removed), containing 2nd person pronouns from the English News-Crawl datasets. Next, informal, neutral, and formal translations were generated of these 50 sentences, yielding a total of 150 sentences. Then bilingual speakers were asked to assess the formality and quality of the translations. Speakers rated formality on a scale of 0-4 (using the labels: Very informal, Informal, Neutral, Formal, and Very formal), and the quality on a scale of 0-6 (with the labels: No meaning preserved, Some meaning preserved, Most meaning preserved and few grammar mistakes, and Perfect meaning and grammar for the
values - To encourage high-quality translations, beam search was used with a beam size of 5. λ=2:3, 0, and 1.5 to generate the informal, neutral, and formal translations respectively. Items with an asterisk are statistically significantly different from the analogous result for the neutral. The average formality and translation quality scores are presented in Table 4 of
FIG. 11 , showing the resultant human evaluations for zero-shot formality-aware machine translation according to the universal rewriting model described above. - It can be seen that on average, the universal rewriting model is successful in increasing perceived formality when moving from informal to neutral to formal translations. While the formal and informal outputs exhibit a slight degradation in translation quality (as compared to neutral outputs), on average raters still judge that most of the meaning of the input is preserved. Given the relatively small number of sentences used in this experiment (3 sets of 50 translations), statistical significance tests performed using paired bootstrap resampling (as described by Philipp Koehn (2004) in “Statistical significance tests for machine translation evaluation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing), pages 388-395, incorporated herein by reference) with 1000 bootstrapped datasets. The average formality scores for the informal (resp. formal) translations were verified as being lower (resp. higher) than the scores associated with the neutral translations with at least 95% statistical significance. Here, “resp. lower” or “resp. higher” mean that the formality scores for the stylized-translations were either higher (when making them formal) or lower (when making them informal) than the base translation. This demonstrates that the model is able to control the formality of the generated text.
- An additional evaluation of formality was performed by assessing whether the model successfully uses the T-V distinction in Spanish to convey formality (described by Brown and Gilman (1960), “The pronouns of power and solidarity”, in Thomas A. Sebeok, editor, Style in Language, pages 253-276, incorporated herein by reference). This involved manually inspecting each translation to assess whether the model used tu (informal) or usted (formal), either explicitly or implicitly through verb conjugations. The results are presented in Table 5 of
FIG. 12 . - It can be seen that for the informal translations, the model almost exclusively uses tu. By contrast, when generating formal translations, the outputs are more varied, with the model often avoiding second person forms entirely. For example, the rewriter's formal translation for the sentence “This will allow you to turn on certain features: is “Esto permitirá activar determinadas funciones”, which translates to “This allows one to activate certain features.” Nevertheless, it can be seen that the automatic evaluation shows the same trend observed in the human evaluation.
- Overall, these results demonstrate that the multilingual (universal) rewriting model is capable of zero-shot transfer of the (broadly construed) notion of formality provided by English exemplars onto other languages.
- In view of the above discussion, an overall approach for training the multilingual text rewriting model can include obtaining one or more text strings for every language to be supported, extracting 2 sub-spans of text and tokenizing them. The process can randomly replace tokens to corrupt them. The spans are passed through the encoder to reuse mT5 model (or equivalent model). Untouched spans are passed through the encoder to extract the style. Note that the style is effectively an attribute, which can encompass any one (or more) of: formality, positive v, negative, dialect, author style, magazine style, food items, movie items, or even something that could be changed about a sentence while leaving other attributes the same. The style vector is added to the outputs of encoder model, and that combination is input to the decoder model (in which or more stochastic tuning ranges provide extra conditioning). The resultant multilingual model can handle not only sentences, but even random spans of text. It consumes exemplars in one language and extracts results in a different language,
- TPU, GPU, CPU or other computing architectures can be employed to implement aspects of the technology in accordance with the features disclosed herein. One example computing architecture is shown in
FIGS. 13A and 13B . In particular,FIGS. 13A and 13B are pictorial and functional diagrams, respectively, of anexample system 1300 that includes a plurality of computing devices and databases connected via a network. For instance, computing device(s) 1302 may be a cloud-based server system.Databases network 1310. One or more user devices or systems may include acomputing system 1312 and adesktop computer 1314, for instance to provide source text segment and/or other information to the computing device(s) 1302. - As shown in
FIG. 13B , each of thecomputing devices 1302 and 1312-1314 may include one or more processors, memory, data and instructions. The memory stores information accessible by the one or more processors, including instructions and data (e.g., machine translation model(s), corpus information, style extractors, corruption types, exemplars, etc.) that may be executed or otherwise used by the processor(s). The memory may be of any type capable of storing information accessible by the processor(s), including a computing device-readable medium. The memory is a non-transitory medium such as a hard-drive, memory card, optical disk, solid-state, etc. Systems may include different combinations of the foregoing, whereby different portions of the instructions and data are stored on different types of media. The instructions may be any set of instructions to be executed directly (such as machine code) or indirectly (such as scripts) by the processor(s). For example, the instructions may be stored as computing device code on the computing device-readable medium. In that regard, the terms “instructions”, “modules” and “programs” may be used interchangeably herein. The instructions may be stored in object code format for direct processing by the processor, or in any other computing device language including scripts or collections of independent source code modules that are interpreted on demand or compiled in advance. - The processors may be any conventional processors, such as commercially available GPUs, CPUs, TPUs, etc. Alternatively, each processor may be a dedicated device such as an ASIC or other hardware-based processor. Although
FIG. 13B functionally illustrates the processors, memory, and other elements of a given computing device as being within the same block, such devices may actually include multiple processors, computing devices, or memories that may or may not be stored within the same physical housing. Similarly, the memory may be a hard drive or other storage media located in a housing different from that of the processor(s), for instance in a cloud computing system ofserver 1302. Accordingly, references to a processor or computing device will be understood to include references to a collection of processors or computing devices or memories that may or may not operate in parallel. - The data, such as source text files or text segment and/or translated output text in multiple languages, may be operated on by the system to train one or more models. This can include filtering or curating the input dataset(s). The trained models may be used on textual input to provide translated text to one or more users, for instance users of
computers 1312 and/or 1314. - Such computers may include all of the components normally used in connection with a computing device such as the processor and memory described above as well as a user interface subsystem for receiving input from a user and presenting information to the user (e.g., text, imagery and/or other graphical elements). The user interface subsystem may include one or more user inputs (e.g., at least one front (user) facing camera, a mouse, keyboard, touch screen and/or microphone) and one or more display devices (e.g., a monitor having a screen or any other electrical device that is operable to display information (e.g., text, imagery and/or other graphical elements). Other output devices, such as speaker(s) may also provide information to users.
- The user-related computing devices (e.g., 1312-1314) may communicate with a back-end computing system (e.g., server 1302) via one or more networks, such as
network 1310. Thenetwork 1310, and intervening nodes, may include various configurations and protocols including short range communication protocols such as Bluetooth™, Bluetooth LE™, the Internet, World Wide Web, intranets, virtual private networks, wide area networks, local networks, private networks using communication protocols proprietary to one or more companies, Ethernet, WiFi and HTTP, and various combinations of the foregoing. Such communication may be facilitated by any device capable of transmitting data to and from other computing devices, such as modems and wireless interfaces. - In one example,
computing device 1302 may include one or more server computing devices having a plurality of computing devices, e.g., a load balanced server farm or cloud computing system, that exchange information with different nodes of a network for the purpose of receiving, processing and transmitting the data to and from other computing devices. For instance,computing device 1302 may include one or more server computing devices that are capable of communicating with any of the computing devices 1312-1314 via thenetwork 1310. -
FIG. 14 illustrates a process 1400 in accordance with aspects of the technology, which involves a computer-implemented method for providing multilingual text rewriting according to a machine learning model. Atblock 1402, the method comprising obtaining an input text sequence based on a set of text exemplars in a source language, and atblock 1404 it comprises generating, by a corruption module, a corrupted version of the input text sequence. Atblock 1406 it includes receiving, by an encoder neural network of an encoder module, the corrupted version of the input text sequence. Atblock 1408 it includes generating, by the encoder module, a set of encoded representations of the corrupted version of the input text sequence. Atblock 1410 the method includes extracting, by a style extractor module, a set of style vector representations associated with the input text sequence. Atblock 1412 it includes receiving, by a decoder neural network of a decoder module, the set of encoded representations and the set of style vector representations, in which each style vector representation of the set is added element-wise to one of the set of encoded representations. Atblock 1414 the method includes outputting, by the decoder module, a set of rewritten texts in a plurality of languages different from the source language, and atblock 1416 it includes storing the rewritten text in selected ones of the plurality of languages according to a change in a least a sentiment or a formality of the input text sequence. In this method, a set of model weights is shared by the encoder module and the style extractor module, and a unique token is appended to the input text sequence for style extraction instead of mean-pooling all of the style vector representations in the set. - The universal rewriter models discussed herein are extremely robust and are suitable for use with a number of computer-focused applications. This can include translation services from one language to another (or many), healthcare applications that support patients who may be most comfortable communicating in their native language, video streaming services that provide subtitles in a number of selectable languages, and videoconferencing services that may support a real-time closed-captioning feature for the users.
- Although the technology herein has been described with reference to particular embodiments, it is to be understood that these embodiments are merely illustrative of the principles and applications of the present technology. It is therefore to be understood that numerous modifications may be made to the illustrative embodiments and that other arrangements may be devised without departing from the spirit and scope of the present technology as defined by the appended claims.
Claims (25)
C:=ƒ(⋅, source language, −V s1).
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/682,282 US20230274100A1 (en) | 2022-02-28 | 2022-02-28 | Techniques and Models for Multilingual Text Rewriting |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/682,282 US20230274100A1 (en) | 2022-02-28 | 2022-02-28 | Techniques and Models for Multilingual Text Rewriting |
Publications (1)
Publication Number | Publication Date |
---|---|
US20230274100A1 true US20230274100A1 (en) | 2023-08-31 |
Family
ID=87761862
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US17/682,282 Pending US20230274100A1 (en) | 2022-02-28 | 2022-02-28 | Techniques and Models for Multilingual Text Rewriting |
Country Status (1)
Country | Link |
---|---|
US (1) | US20230274100A1 (en) |
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN117076614A (en) * | 2023-10-13 | 2023-11-17 | 中山大学深圳研究院 | Cross-language text retrieval method and terminal equipment based on transfer learning |
US20240054282A1 (en) * | 2022-08-15 | 2024-02-15 | International Business Machines Corporation | Elucidated natural language artifact recombination with contextual awareness |
Citations (35)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20030203343A1 (en) * | 2002-04-25 | 2003-10-30 | International Business Machines Corporation | Foreign language teaching tool |
US20050075880A1 (en) * | 2002-01-22 | 2005-04-07 | International Business Machines Corporation | Method, system, and product for automatically modifying a tone of a message |
US20070294077A1 (en) * | 2006-05-22 | 2007-12-20 | Shrikanth Narayanan | Socially Cognizant Translation by Detecting and Transforming Elements of Politeness and Respect |
US20100114556A1 (en) * | 2008-10-31 | 2010-05-06 | International Business Machines Corporation | Speech translation method and apparatus |
US20100217582A1 (en) * | 2007-10-26 | 2010-08-26 | Mobile Technologies Llc | System and methods for maintaining speech-to-speech translation in the field |
US20150254238A1 (en) * | 2007-10-26 | 2015-09-10 | Facebook, Inc. | System and Methods for Maintaining Speech-To-Speech Translation in the Field |
US20190108212A1 (en) * | 2017-10-09 | 2019-04-11 | International Business Machines Corporation | Fault Injection in Human-Readable Information |
US20190115008A1 (en) * | 2017-10-17 | 2019-04-18 | International Business Machines Corporation | Automatic answer rephrasing based on talking style |
US10423722B2 (en) * | 2016-08-18 | 2019-09-24 | At&T Intellectual Property I, L.P. | Communication indicator |
US20200057798A1 (en) * | 2018-08-20 | 2020-02-20 | International Business Machines Corporation | Cognitive clipboard |
US20200097554A1 (en) * | 2018-09-26 | 2020-03-26 | Huawei Technologies Co., Ltd. | Systems and methods for multilingual text generation field |
US20200110797A1 (en) * | 2018-10-04 | 2020-04-09 | International Business Machines Corporation | Unsupervised text style transfer system for improved online social media experience |
US20200159826A1 (en) * | 2018-11-19 | 2020-05-21 | Genesys Telecommunications Laboratories, Inc. | Method and System for Sentiment Analysis |
US20200210772A1 (en) * | 2018-12-31 | 2020-07-02 | Charles University Faculty of Mathematics and Physics | A Computer-Implemented Method of Creating a Translation Model for Low Resource Language Pairs and a Machine Translation System using this Translation Model |
US20200311195A1 (en) * | 2019-04-01 | 2020-10-01 | International Business Machines Corporation | Controllable Style-Based Text Transformation |
US20200356634A1 (en) * | 2019-05-09 | 2020-11-12 | Adobe Inc. | Systems and methods for transferring stylistic expression in machine translation of sequence data |
US10891435B1 (en) * | 2018-02-20 | 2021-01-12 | Interactions Llc | Bootstrapping multilingual natural language understanding via machine translation |
US20210020161A1 (en) * | 2018-03-14 | 2021-01-21 | Papercup Technologies Limited | Speech Processing System And A Method Of Processing A Speech Signal |
US20210027761A1 (en) * | 2019-07-26 | 2021-01-28 | International Business Machines Corporation | Automatic translation using deep learning |
US20210034705A1 (en) * | 2019-07-30 | 2021-02-04 | Adobe Inc. | Converting tone of digital content |
US20210165960A1 (en) * | 2019-12-02 | 2021-06-03 | Asapp, Inc. | Modifying text according to a specified attribute |
US11068654B2 (en) * | 2018-11-15 | 2021-07-20 | International Business Machines Corporation | Cognitive system for declarative tone modification |
US20210303803A1 (en) * | 2020-03-25 | 2021-09-30 | International Business Machines Corporation | Text style transfer using reinforcement learning |
US11194971B1 (en) * | 2020-03-05 | 2021-12-07 | Alexander Dobranic | Vision-based text sentiment analysis and recommendation system |
US20210383074A1 (en) * | 2020-06-03 | 2021-12-09 | PM Labs, Inc. | System and method for reinforcement learning based controlled natural language generation |
US11202131B2 (en) * | 2019-03-10 | 2021-12-14 | Vidubly Ltd | Maintaining original volume changes of a character in revoiced media stream |
US20210390270A1 (en) * | 2020-06-16 | 2021-12-16 | Baidu Usa Llc | Cross-lingual unsupervised classification with multi-view transfer learning |
US20210397793A1 (en) * | 2020-06-17 | 2021-12-23 | Microsoft Technology Licensing, Llc | Intelligent Tone Detection and Rewrite |
US20220121879A1 (en) * | 2020-10-16 | 2022-04-21 | Adobe Inc. | Multi-dimensional language style transfer |
US20220198157A1 (en) * | 2020-12-22 | 2022-06-23 | Microsoft Technology Licensing, Llc | Multilingual Model Training Using Parallel Corpora, Crowdsourcing, and Accurate Monolingual Models |
US11429785B2 (en) * | 2020-11-23 | 2022-08-30 | Pusan National University Industry-University Cooperation Foundation | System and method for generating test document for context sensitive spelling error correction |
US11574132B2 (en) * | 2018-07-26 | 2023-02-07 | International Business Machines Corporation | Unsupervised tunable stylized text transformations |
US11587561B2 (en) * | 2019-10-25 | 2023-02-21 | Mary Lee Weir | Communication system and method of extracting emotion data during translations |
US11645478B2 (en) * | 2020-11-04 | 2023-05-09 | Adobe Inc. | Multi-lingual tagging for digital images |
US20230289529A1 (en) * | 2020-03-26 | 2023-09-14 | Grammarly, Inc. | Detecting the tone of text |
-
2022
- 2022-02-28 US US17/682,282 patent/US20230274100A1/en active Pending
Patent Citations (67)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20050075880A1 (en) * | 2002-01-22 | 2005-04-07 | International Business Machines Corporation | Method, system, and product for automatically modifying a tone of a message |
US20030203343A1 (en) * | 2002-04-25 | 2003-10-30 | International Business Machines Corporation | Foreign language teaching tool |
US7085707B2 (en) * | 2002-04-25 | 2006-08-01 | International Business Machines Corporation | Foreign language teaching tool |
US8032355B2 (en) * | 2006-05-22 | 2011-10-04 | University Of Southern California | Socially cognizant translation by detecting and transforming elements of politeness and respect |
US20070294077A1 (en) * | 2006-05-22 | 2007-12-20 | Shrikanth Narayanan | Socially Cognizant Translation by Detecting and Transforming Elements of Politeness and Respect |
US20100217582A1 (en) * | 2007-10-26 | 2010-08-26 | Mobile Technologies Llc | System and methods for maintaining speech-to-speech translation in the field |
US20150254238A1 (en) * | 2007-10-26 | 2015-09-10 | Facebook, Inc. | System and Methods for Maintaining Speech-To-Speech Translation in the Field |
US20100114556A1 (en) * | 2008-10-31 | 2010-05-06 | International Business Machines Corporation | Speech translation method and apparatus |
US9342509B2 (en) * | 2008-10-31 | 2016-05-17 | Nuance Communications, Inc. | Speech translation method and apparatus utilizing prosodic information |
US10423722B2 (en) * | 2016-08-18 | 2019-09-24 | At&T Intellectual Property I, L.P. | Communication indicator |
US20190108212A1 (en) * | 2017-10-09 | 2019-04-11 | International Business Machines Corporation | Fault Injection in Human-Readable Information |
US20190108213A1 (en) * | 2017-10-09 | 2019-04-11 | International Business Machines Corporation | Fault Injection in Human-Readable Information |
US10606943B2 (en) * | 2017-10-09 | 2020-03-31 | International Business Machines Corporation | Fault injection in human-readable information |
US10418023B2 (en) * | 2017-10-17 | 2019-09-17 | International Business Machines Corporation | Automatic answer rephrasing based on talking style |
US20190392813A1 (en) * | 2017-10-17 | 2019-12-26 | International Business Machines Corporation | Automatic answer rephrasing based on talking style |
US11030990B2 (en) * | 2017-10-17 | 2021-06-08 | International Business Machines Corporation | Automatic answer rephrasing based on talking style |
US20190115008A1 (en) * | 2017-10-17 | 2019-04-18 | International Business Machines Corporation | Automatic answer rephrasing based on talking style |
US10891435B1 (en) * | 2018-02-20 | 2021-01-12 | Interactions Llc | Bootstrapping multilingual natural language understanding via machine translation |
US20210020161A1 (en) * | 2018-03-14 | 2021-01-21 | Papercup Technologies Limited | Speech Processing System And A Method Of Processing A Speech Signal |
US11574132B2 (en) * | 2018-07-26 | 2023-02-07 | International Business Machines Corporation | Unsupervised tunable stylized text transformations |
US20200057798A1 (en) * | 2018-08-20 | 2020-02-20 | International Business Machines Corporation | Cognitive clipboard |
US10902188B2 (en) * | 2018-08-20 | 2021-01-26 | International Business Machines Corporation | Cognitive clipboard |
US20200097554A1 (en) * | 2018-09-26 | 2020-03-26 | Huawei Technologies Co., Ltd. | Systems and methods for multilingual text generation field |
US11151334B2 (en) * | 2018-09-26 | 2021-10-19 | Huawei Technologies Co., Ltd. | Systems and methods for multilingual text generation field |
US20200110797A1 (en) * | 2018-10-04 | 2020-04-09 | International Business Machines Corporation | Unsupervised text style transfer system for improved online social media experience |
US11068654B2 (en) * | 2018-11-15 | 2021-07-20 | International Business Machines Corporation | Cognitive system for declarative tone modification |
US11586828B2 (en) * | 2018-11-19 | 2023-02-21 | Genesys Telecommunications Laboratories, Inc. | Method, system and computer program product for sentiment analysis |
US20200410171A1 (en) * | 2018-11-19 | 2020-12-31 | Genesys Telecommunications Laboratories, Inc. | Method, system and computer program product for sentiment analysis |
US20200410172A1 (en) * | 2018-11-19 | 2020-12-31 | Genesys Telecommunications Laboratories, Inc. | Method, system and computer program product for sentiment analysis |
US20200387674A1 (en) * | 2018-11-19 | 2020-12-10 | Genesys Telecommunications Laboratories, Inc. | Method, system and computer program product for sentiment analysis |
US11551011B2 (en) * | 2018-11-19 | 2023-01-10 | Genesys Telecommunications Laboratories, Inc. | Method, system and computer program product for sentiment analysis |
US11562148B2 (en) * | 2018-11-19 | 2023-01-24 | Genesys Telecommunications Laboratories, Inc. | Method, system and computer program product for sentiment analysis |
US20200159826A1 (en) * | 2018-11-19 | 2020-05-21 | Genesys Telecommunications Laboratories, Inc. | Method and System for Sentiment Analysis |
US10789430B2 (en) * | 2018-11-19 | 2020-09-29 | Genesys Telecommunications Laboratories, Inc. | Method and system for sentiment analysis |
US11037028B2 (en) * | 2018-12-31 | 2021-06-15 | Charles University Faculty of Mathematics and Physics | Computer-implemented method of creating a translation model for low resource language pairs and a machine translation system using this translation model |
US20200210772A1 (en) * | 2018-12-31 | 2020-07-02 | Charles University Faculty of Mathematics and Physics | A Computer-Implemented Method of Creating a Translation Model for Low Resource Language Pairs and a Machine Translation System using this Translation Model |
US11202131B2 (en) * | 2019-03-10 | 2021-12-14 | Vidubly Ltd | Maintaining original volume changes of a character in revoiced media stream |
US20200311195A1 (en) * | 2019-04-01 | 2020-10-01 | International Business Machines Corporation | Controllable Style-Based Text Transformation |
US10977439B2 (en) * | 2019-04-01 | 2021-04-13 | International Business Machines Corporation | Controllable style-based text transformation |
US11210477B2 (en) * | 2019-05-09 | 2021-12-28 | Adobe Inc. | Systems and methods for transferring stylistic expression in machine translation of sequence data |
US20200356634A1 (en) * | 2019-05-09 | 2020-11-12 | Adobe Inc. | Systems and methods for transferring stylistic expression in machine translation of sequence data |
US20220075965A1 (en) * | 2019-05-09 | 2022-03-10 | Adobe Inc. | Systems and methods for transferring stylistic expression in machine translation of sequence data |
US20210027761A1 (en) * | 2019-07-26 | 2021-01-28 | International Business Machines Corporation | Automatic translation using deep learning |
US11200881B2 (en) * | 2019-07-26 | 2021-12-14 | International Business Machines Corporation | Automatic translation using deep learning |
US20210034705A1 (en) * | 2019-07-30 | 2021-02-04 | Adobe Inc. | Converting tone of digital content |
US11475223B2 (en) * | 2019-07-30 | 2022-10-18 | Adobe Inc. | Converting tone of digital content |
US11587561B2 (en) * | 2019-10-25 | 2023-02-21 | Mary Lee Weir | Communication system and method of extracting emotion data during translations |
US11610061B2 (en) * | 2019-12-02 | 2023-03-21 | Asapp, Inc. | Modifying text according to a specified attribute |
US20210165960A1 (en) * | 2019-12-02 | 2021-06-03 | Asapp, Inc. | Modifying text according to a specified attribute |
US11194971B1 (en) * | 2020-03-05 | 2021-12-07 | Alexander Dobranic | Vision-based text sentiment analysis and recommendation system |
US11630959B1 (en) * | 2020-03-05 | 2023-04-18 | Delta Campaigns, Llc | Vision-based text sentiment analysis and recommendation system |
US20210303803A1 (en) * | 2020-03-25 | 2021-09-30 | International Business Machines Corporation | Text style transfer using reinforcement learning |
US11314950B2 (en) * | 2020-03-25 | 2022-04-26 | International Business Machines Corporation | Text style transfer using reinforcement learning |
US11763085B1 (en) * | 2020-03-26 | 2023-09-19 | Grammarly, Inc. | Detecting the tone of text |
US20230289529A1 (en) * | 2020-03-26 | 2023-09-14 | Grammarly, Inc. | Detecting the tone of text |
US11586830B2 (en) * | 2020-06-03 | 2023-02-21 | PM Labs, Inc. | System and method for reinforcement learning based controlled natural language generation |
US20210383074A1 (en) * | 2020-06-03 | 2021-12-09 | PM Labs, Inc. | System and method for reinforcement learning based controlled natural language generation |
US20210390270A1 (en) * | 2020-06-16 | 2021-12-16 | Baidu Usa Llc | Cross-lingual unsupervised classification with multi-view transfer learning |
US11694042B2 (en) * | 2020-06-16 | 2023-07-04 | Baidu Usa Llc | Cross-lingual unsupervised classification with multi-view transfer learning |
US20210397793A1 (en) * | 2020-06-17 | 2021-12-23 | Microsoft Technology Licensing, Llc | Intelligent Tone Detection and Rewrite |
US20220121879A1 (en) * | 2020-10-16 | 2022-04-21 | Adobe Inc. | Multi-dimensional language style transfer |
US20220414400A1 (en) * | 2020-10-16 | 2022-12-29 | Adobe Inc. | Multi-dimensional language style transfer |
US11741190B2 (en) * | 2020-10-16 | 2023-08-29 | Adobe Inc. | Multi-dimensional language style transfer |
US11487971B2 (en) * | 2020-10-16 | 2022-11-01 | Adobe Inc. | Multi-dimensional language style transfer |
US11645478B2 (en) * | 2020-11-04 | 2023-05-09 | Adobe Inc. | Multi-lingual tagging for digital images |
US11429785B2 (en) * | 2020-11-23 | 2022-08-30 | Pusan National University Industry-University Cooperation Foundation | System and method for generating test document for context sensitive spelling error correction |
US20220198157A1 (en) * | 2020-12-22 | 2022-06-23 | Microsoft Technology Licensing, Llc | Multilingual Model Training Using Parallel Corpora, Crowdsourcing, and Accurate Monolingual Models |
Non-Patent Citations (17)
Title |
---|
Alex York, The Best 10 Wordtune Alternatives for Rewriting Content in 2024, December 19, 2023, 38 pages, https://clickup.com/blog/wordtune-alternatives/ (Year: 2023) * |
Anumanchipalli, et al., "Intent Transfer in Speech-to-Speech Machine Translation", In IEEE Spoken Language Technology Workshop, Dec. 2, 2012, 6 pages. (Year: 2012) * |
Bandel, Elron et al. "SimpleStyle: An Adaptable Style Transfer Approach." ArXiv abs/2212.10498 (Dec. 2022), 12 pages (Year: 2022) * |
Jin, Di et al. "Deep Learning for Text Style Transfer: A Survey." Computational Linguistics 48 (2020): pg. 155-205. https://arxiv.org/pdf/2011.00416.pdf (Year: 2020) * |
Juncen Li; "Delete, Retrieve, Generate: A Simple Approach to Sentiment and Style Transfer"; 17 Apr 2018, 12 pages, https://doi.org/10.48550/arXiv.1804.06437 (Year: 2018) * |
Lample, Guillaume et al. "Multiple-Attribute Text Rewriting." International Conference on Learning Representations, 27 September 2018, 20 pages, https://openreview.net/pdf?id=H1g2NhC5KQ. (Year: 2018) * |
MohammadiBaghmolaei, Rezvan and Ali Ahmadi. "TET: Text emotion transfer." Knowledge-Based Systems, 1 December 2022, DOI:10.1016/j.knosys.2022.110236 (Year: 2022) * |
Parker Riley et al., "TextSETTR: Label-Free Text Style Extraction and Tunable Targeted Restyling", Version 1, 8 Oct 2020, 15 pages, https://doi.org/10.48550/arXiv.2010.03802, arXiv:2010.03802v1 (Year: 2020) * |
Parker Riley et al., "TextSETTR: Label-Free Text Style Extraction and Tunable Targeted Restyling", Version 2, 29 Dec 2020, 16 pages, https://doi.org/10.48550/arXiv.2010.03802, arXiv:2010.03802v2 (Year: 2020) * |
Parker Riley, "Text Rewriting with Missing Supervision", PhD Thesis; 2021,129 pages file:///C:/Users/bsmith3/Downloads/Riley_rochester_0188E_12322.pdf (Year: 2021) * |
Reif, Emily et al. "A Recipe for Arbitrary Text Style Transfer with Large Language Models." ArXiv abs/2109.03910 ( 8 Sep 2021): n. pag. 12 pages (Year: 2021) * |
Riley, Parker et al. "FRMT: A Benchmark for Few-Shot Region-Aware Machine Translation." ArXiv abs/2210.00193 (1 October 2022): 15 pages (Year: 2022) * |
Sandeep Subramanian, "Multiple-Attribute Text Style Transfer", 1 Nov 2018, 20 pages https://doi.org/10.48550/arXiv.1811.00552 (Year: 2018) * |
Shrimai Prabhumoye, "Style Transfer Through Back-Translation", In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 866–876, Melbourne, Australia. 24 Apr 2018 (Year: 2018) * |
Sudhakar, Akhilesh et al. ""Transforming" Delete, Retrieve, Generate Approach for Controlled Text Style Transfer." Conference on Empirical Methods in Natural Language Processing, 25 August 2019, 11 pages, DOI:10.18653/v1/D19-1322 (Year: 2019) * |
Syed, Bakhtiyar et al. "Adapting Language Models for Non-Parallel Author-Stylized Rewriting." AAAI Conference on Artificial Intelligence, 22 September 2019, 8 pages; DOI:10.1609/AAAI.V34I05.6433 (Year: 2019) * |
Zhiqiang Hu, "Text Style Transfer: A Review and Experimental Evaluation" 24 Oct 2020, 32 pages, https://doi.org/10.48550/arXiv.2010.12742 (Year: 2020) * |
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20240054282A1 (en) * | 2022-08-15 | 2024-02-15 | International Business Machines Corporation | Elucidated natural language artifact recombination with contextual awareness |
CN117076614A (en) * | 2023-10-13 | 2023-11-17 | 中山大学深圳研究院 | Cross-language text retrieval method and terminal equipment based on transfer learning |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
Naveed et al. | A comprehensive overview of large language models | |
Li et al. | Data augmentation approaches in natural language processing: A survey | |
Song et al. | Mass: Masked sequence to sequence pre-training for language generation | |
Alvarez-Melis et al. | A causal framework for explaining the predictions of black-box sequence-to-sequence models | |
Haslwanter | An introduction to statistics with python | |
US20230274100A1 (en) | Techniques and Models for Multilingual Text Rewriting | |
WO2019060353A1 (en) | System and method for translating chat messages | |
Dirix et al. | How well do word recognition measures correlate? Effects of language context and repeated presentations | |
Abadani et al. | Parsquad: machine translated squad dataset for Persian question answering | |
Pelicon et al. | Investigating cross-lingual training for offensive language detection | |
Sun et al. | Contextual text denoising with masked language models | |
Basu et al. | Med-easi: Finely annotated dataset and models for controllable simplification of medical texts | |
Liu et al. | Document-level event argument extraction with self-augmentation and a cross-domain joint training mechanism | |
Bensalah et al. | Transformer model and convolutional neural networks (CNNs) for Arabic to English machine translation | |
Siu | Revolutionizing translation with AI: Unravelling neural machine translation and generative pre-trained large language models | |
Kumar et al. | Addressing domain shift in neural machine translation via reinforcement learning | |
Mukherjee et al. | Balancing the style-content trade-off in sentiment transfer using polarity-aware denoising | |
Chatterjee | Automatic post-editing for machine translation | |
Liu et al. | Correcting verb selection errors for ESL with the perceptron | |
Rikters et al. | Combining machine translated sentence chunks from multiple MT systems | |
Beyer et al. | Embedding space correlation as a measure of domain similarity | |
Poncelas | Improving transductive data selection algorithms for machine translation | |
Huot et al. | Text-blueprint: An interactive platform for plan-based conditional generation | |
Sagirova et al. | Complexity of symbolic representation in working memory of Transformer correlates with the complexity of a task | |
Lazareva et al. | Technology for mastering russian vocabulary by chinese students in the field of international trade |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:GARCIA, XAVIER EDUARDO;FIRAT, ORHAN;CONSTANT, NOAH;AND OTHERS;SIGNING DATES FROM 20220228 TO 20220302;REEL/FRAME:059159/0838 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: FINAL REJECTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE AFTER FINAL ACTION FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: ADVISORY ACTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:RILEY, PARKER;REEL/FRAME:067195/0601Effective date: 20240422 |