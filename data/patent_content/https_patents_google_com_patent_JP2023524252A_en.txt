JP2023524252A - Generative nonlinear human shape model - Google Patents
Generative nonlinear human shape model Download PDFInfo
- Publication number
- JP2023524252A JP2023524252A JP2022566221A JP2022566221A JP2023524252A JP 2023524252 A JP2023524252 A JP 2023524252A JP 2022566221 A JP2022566221 A JP 2022566221A JP 2022566221 A JP2022566221 A JP 2022566221A JP 2023524252 A JP2023524252 A JP 2023524252A
- Authority
- JP
- Japan
- Prior art keywords
- shape
- model
- learned
- pose
- training
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
- 238000012549 training Methods 0.000 claims abstract description 94
- 230000037237 body shape Effects 0.000 claims abstract description 25
- 230000008921 facial expression Effects 0.000 claims description 64
- 238000000034 method Methods 0.000 claims description 54
- 230000003068 static effect Effects 0.000 claims description 43
- 230000008569 process Effects 0.000 claims description 24
- 230000001815 facial effect Effects 0.000 claims description 23
- 239000013598 vector Substances 0.000 claims description 9
- 238000012545 processing Methods 0.000 claims description 6
- 230000014509 gene expression Effects 0.000 claims description 4
- 239000000203 mixture Substances 0.000 claims description 2
- 238000001914 filtration Methods 0.000 claims 2
- 238000013135 deep learning Methods 0.000 abstract description 4
- 230000015572 biosynthetic process Effects 0.000 abstract description 2
- 238000003786 synthesis reaction Methods 0.000 abstract description 2
- 230000006870 function Effects 0.000 description 21
- 210000003128 head Anatomy 0.000 description 20
- 230000033001 locomotion Effects 0.000 description 18
- 230000015654 memory Effects 0.000 description 14
- 238000013528 artificial neural network Methods 0.000 description 13
- 238000011156 evaluation Methods 0.000 description 11
- 238000012937 correction Methods 0.000 description 9
- 230000009466 transformation Effects 0.000 description 9
- 238000010586 diagram Methods 0.000 description 8
- 239000008186 active pharmaceutical agent Substances 0.000 description 7
- 230000008901 benefit Effects 0.000 description 7
- 230000007935 neutral effect Effects 0.000 description 6
- 238000002474 experimental method Methods 0.000 description 5
- 238000010276 construction Methods 0.000 description 4
- 230000000694 effects Effects 0.000 description 4
- 238000009877 rendering Methods 0.000 description 4
- 210000004872 soft tissue Anatomy 0.000 description 4
- 238000000844 transformation Methods 0.000 description 4
- 238000004891 communication Methods 0.000 description 3
- 238000005457 optimization Methods 0.000 description 3
- 230000000306 recurrent effect Effects 0.000 description 3
- 230000000007 visual effect Effects 0.000 description 3
- 230000009471 action Effects 0.000 description 2
- 230000004913 activation Effects 0.000 description 2
- 238000007792 addition Methods 0.000 description 2
- 238000013527 convolutional neural network Methods 0.000 description 2
- 238000005516 engineering process Methods 0.000 description 2
- 210000000887 face Anatomy 0.000 description 2
- 239000011159 matrix material Substances 0.000 description 2
- 238000002156 mixing Methods 0.000 description 2
- 238000012986 modification Methods 0.000 description 2
- 230000004048 modification Effects 0.000 description 2
- 210000003205 muscle Anatomy 0.000 description 2
- 230000008439 repair process Effects 0.000 description 2
- 238000012360 testing method Methods 0.000 description 2
- 241000282412 Homo Species 0.000 description 1
- 210000000577 adipose tissue Anatomy 0.000 description 1
- 238000004458 analytical method Methods 0.000 description 1
- 238000013459 approach Methods 0.000 description 1
- 210000000746 body region Anatomy 0.000 description 1
- 210000000988 bone and bone Anatomy 0.000 description 1
- 210000005252 bulbus oculi Anatomy 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 238000007405 data analysis Methods 0.000 description 1
- 238000013499 data model Methods 0.000 description 1
- 230000001419 dependent effect Effects 0.000 description 1
- 238000013461 design Methods 0.000 description 1
- 238000001514 detection method Methods 0.000 description 1
- 230000004069 differentiation Effects 0.000 description 1
- 238000006073 displacement reaction Methods 0.000 description 1
- 238000010195 expression analysis Methods 0.000 description 1
- 210000001508 eye Anatomy 0.000 description 1
- 210000000744 eyelid Anatomy 0.000 description 1
- 230000003993 interaction Effects 0.000 description 1
- 238000002955 isolation Methods 0.000 description 1
- 210000001847 jaw Anatomy 0.000 description 1
- 210000005067 joint tissue Anatomy 0.000 description 1
- 230000002045 lasting effect Effects 0.000 description 1
- 230000007787 long-term memory Effects 0.000 description 1
- 238000010801 machine learning Methods 0.000 description 1
- 238000013507 mapping Methods 0.000 description 1
- 210000003739 neck Anatomy 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 238000011176 pooling Methods 0.000 description 1
- 230000011218 segmentation Effects 0.000 description 1
- 230000035945 sensitivity Effects 0.000 description 1
- 238000013179 statistical model Methods 0.000 description 1
- 238000012800 visualization Methods 0.000 description 1
- 230000037303 wrinkles Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T17/00—Three dimensional [3D] modelling, e.g. data description of 3D objects
- G06T17/20—Finite element generation, e.g. wire-frame surface description, tesselation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T17/00—Three dimensional [3D] modelling, e.g. data description of 3D objects
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
- G06N3/0455—Auto-encoder networks; Encoder-decoder networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T19/00—Manipulating 3D models or images for computer graphics
- G06T19/20—Editing of 3D images, e.g. changing shapes or colours, aligning objects or positioning parts
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/70—Determining position or orientation of objects or cameras
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V40/00—Recognition of biometric, human-related or animal-related patterns in image or video data
- G06V40/10—Human or animal bodies, e.g. vehicle occupants or pedestrians; Body parts, e.g. hands
- G06V40/16—Human faces, e.g. facial parts, sketches or expressions
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/20—Special algorithmic details
- G06T2207/20081—Training; Learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2219/00—Indexing scheme for manipulating 3D models or images for computer graphics
- G06T2219/20—Indexing scheme for editing of 3D models
- G06T2219/2021—Shape modification
Abstract
本開示は、十分にトレーニング可能なモジュール式の深層学習フレームワーク内の、統計学的、多関節3D人間形状モデリングパイプラインを提供する。詳細には、本開示の態様は、トレーニングデータのセット上でエンドツーエンドで一緒にトレーニングされる少なくとも顔および体の形状コンポーネントを有する機械学習済み3D人間形状モデルを対象とする。モデルコンポーネント(たとえば、顔、手、および体の残部のコンポーネントをともに含む)のジョイントトレーニングが、生成された顔形状と体形状との間の向上した整合性の合成を可能にする。The present disclosure provides a statistical, articulated 3D human shape modeling pipeline within a fully trainable and modular deep learning framework. In particular, aspects of this disclosure are directed to machine-learned 3D human shape models having at least face and body shape components that are jointly trained end-to-end on a set of training data. Joint training of model components (eg, including face, hand, and rest of the body components together) enables synthesis of improved consistency between generated face and body shapes.
Description
本開示は、一般に、3次元(3D)人間形状モデリングパイプラインに関する。より詳細には、本開示は、エンドツーエンドで一緒にトレーニングされた少なくとも顔および体の形状コンポーネントを有する、機械学習済み3次元人間形状モデルを含むまたは使用するシステムおよび方法に関する。 The present disclosure relates generally to three-dimensional (3D) human shape modeling pipelines. More particularly, the present disclosure relates to systems and methods that include or use machine-learned 3D human shape models having at least face and body shape components that have been jointly trained end-to-end.
人間の動き、行為、および表現は、様々な技術分野に中心的な実用上の重要性があり、継続的に関心の的になっている。例として、画像および映像における創造的キャプチャ、没入型写真、ならびに物理的3D空間推論はすべて、人体の向上したモデリングから恩恵を受けるのに役立つ技術分野である。したがって、ポーズ、形状、顔の表情、および/または手の操作のレベルで全身の詳細を正確に表すことができるモデルが、3Dでのみ十分に理解され得る微細なインタラクションをキャプチャし、深く分析するために不可欠である。 Human movements, actions, and expressions are of central practical importance and of continuing interest in a variety of technical fields. By way of example, creative capture in images and videos, immersive photography, and physical 3D spatial reasoning are all areas of technology that help benefit from improved modeling of the human body. Therefore, a model that can accurately represent full-body detail at the level of pose, shape, facial expression, and/or hand manipulation captures and deeply analyzes microscopic interactions that can only be fully understood in 3D. is essential for
画像および映像中の人間のスティックフィギュアの場所を突き止めること、およびいくつかの条件下では、等価な3D骨格および基本形状までのリフティングにはかなりの進歩があったが、3D物理空間を土台としている、意味的に重要な表面のレベルで人体の正確なモデルを再構築する全般的な探求は、依然として著しい革新を行っている分野である。 Although considerable progress has been made in locating human stick figures in images and videos and, under some conditions, lifting to equivalent 3D skeletons and primitive shapes, it is based on 3D physical space. The general quest to reconstruct an accurate model of the human body at the semantically significant surface level remains an area of significant innovation.
少なくとも中期的にはモデル構成進歩の可能性は、直感物理的および意味論的人間モデリングと、大規模なキャプチャデータセットとの間の発生時にあると思われる。顔、手、および体についての多くの興味深い、強力なモデルが、時間とともに構成されたが、すべてではないとしても、大部分は、完全な人体という状況ではなく、分離して構築されてきた。したがって、必然的にそれらは、深層学習との関連で最近出現した、大規模なデータ分析およびモデル構成プロセスを利用していない。 The potential for model construction advances, at least in the medium term, appears to lie between intuitive physical and semantic human modeling and large-scale capture datasets. Many interesting and powerful models of faces, hands, and bodies have been constructed over time, but most, if not all, have been built in isolation rather than in the context of a complete human body. Consequently, they necessarily do not take advantage of the large-scale data analysis and model construction processes that have recently emerged in the context of deep learning.
例として、Adam、Frank、またはSMPL-Xのような、いくつかの最近の全身モデルは、顔、体、および手について旧来のコンポーネントを組み合わせるが、通常、すでに学習されたコンポーネントの上に適切なスケーリングを用いて整合性のある、関節パラメタリゼーション(joint parameterization)を構成することに焦点を当てる。これにより、すべてのデータにおける構造を同時に十分に活用し、コンポーネントの代替表現または異なる損失で実験し、最終的な影響を評価し、革新することが困難になる。 As an example, some recent full-body models, such as Adam, Frank, or SMPL-X, combine classical components for the face, body, and hands, but usually use appropriate models on top of the already learned components. We focus on constructing consistent joint parameterizations with scaling. This makes it difficult to fully exploit the structure in all the data simultaneously, experiment with alternative representations or different losses of components, assess the final impact, and innovate.
本開示の実施形態の態様および利点が、以下の説明において部分的に記載され、または説明から学ぶことができ、または実施形態の実践を通して学ぶことができる。 Aspects and advantages of embodiments of the disclosure are set forth in part in the following description, or may be learned from the description, or may be learned through practice of the embodiments.
本開示の1つの例示的な態様は、エンドツーエンドで一緒にトレーニングされた少なくとも顔および体の形状コンポーネントを有する、機械学習済み3次元人間形状モデルを特徴とするコンピューティングシステムを対象とする。このコンピューティングシステムは、1つまたは複数のプロセッサと、機械学習済み3次元人間形状モデルをまとめて記憶する1つまたは複数の非一時的コンピュータ可読媒体とを含む。機械学習済み3次元人間形状モデルは、人体についての顔表情データを生成するために、人体に関連する顔表情埋込みを処理するようにトレーニングされた機械学習済み顔表情デコーダモデル(machine-learned facial expression decoder model)を含む。機械学習済み3次元人間形状モデルは、人体についてのポーズによる形状調整を生成するために、ポーズパラメータのセットを処理するようにトレーニングされた機械学習済みポーズ空間変形(pose space deformation)モデルを含む。機械学習済み3次元人間形状モデルは、人体についての識別ベースの静止形状データを生成するために、人体に関連する静止形状(rest shape)埋込みを処理するようにトレーニングされた機械学習済み形状デコーダモデルを含む。機械学習済み3次元人間形状モデルは、顔表情データ、ポーズによる形状調整、および識別ベースの静止形状データに少なくとも部分的に基づいて、人体のポーズメッシュ(posed mesh)を生成するようにトレーニングされる。機械学習済み顔表情デコーダモデル、機械学習済みポーズ空間変形モデル、および機械学習済み形状デコーダモデルのすべてが、トレーニングボディについて機械学習済み3次元人間形状モデルによって生成されたトレーニングポーズメッシュを、トレーニングボディの1つまたは複数のグランドトゥルース登録の形状スキャンと比較する、再構成損失関数(reconstructive loss function)に少なくとも部分的に基づいて、エンドツーエンドで一緒にトレーニングされている。 One exemplary aspect of the present disclosure is directed to a computing system featuring a machine-learned 3D human shape model having at least face and body shape components that are jointly trained end-to-end. The computing system includes one or more processors and one or more non-transitory computer-readable media collectively storing a machine-learned three-dimensional human geometry model. A machine-learned 3D human shape model is a machine-learned facial expression decoder model trained to process facial expression embeddings associated with the human body to generate facial expression data about the human body. decoder model). The machine-learned 3D human shape model includes a machine-learned pose space deformation model trained to process a set of pose parameters to generate pose-based shape adjustments for the human body. A machine-learned three-dimensional human shape model is a machine-learned shape decoder model trained to process rest shape embeddings associated with the human body to generate discrimination-based rest shape data for the human body. including. A machine-learned 3D human shape model is trained to generate a posed mesh of the human body based at least in part on facial expression data, pose-based shape fitting, and discrimination-based static shape data. . The machine-learned facial decoder model, the machine-learned pose space deformation model, and the machine-learned shape decoder model all apply the training pose mesh generated by the machine-learned 3D human shape model to the training body's It is jointly trained end-to-end based, at least in part, on a reconstructive loss function that compares shape scans of one or more ground truth registrations.
本開示の別の例示的な態様は、機械学習済み3次元人間形状モデルをエンドツーエンドで一緒にトレーニングするためのコンピュータ実装方法を対象とする。この方法は、1つまたは複数のコンピューティングデバイスを含むコンピューティングシステムによって、トレーニングボディの1つまたは複数のグランドトゥルース登録の形状スキャンを取得するステップを含み、トレーニングボディの1つまたは複数のグランドトゥルース登録の形状スキャンは、任意のポーズを有する少なくともグランドトゥルース登録の全身スキャンと、グランドトゥルース登録の顔詳細スキャンとを含む。この方法は、コンピューティングシステムによって、形状エンコーダモデルを使用して、トレーニングボディに関連する静止形状埋込みを取得するために、静止しているポーズとともに推定登録全身スキャンを符号化するステップを含む。この方法は、トレーニングボディについての識別ベースの静止形状データを取得するために、コンピューティングシステムによって、形状デコーダモデルを使用して、静止形状埋込みを復号するステップを含む。この方法は、トレーニングボディに関連する顔表情埋込みを取得するために、コンピューティングシステムによって、顔のエンコーダモデルを使用して、グランドトゥルース登録の顔詳細スキャンから導出されたデータを符号化するステップを含む。この方法は、トレーニングボディについての顔表情データを取得するために、コンピューティングシステムによって、顔デコーダモデルを使用して、顔表情埋込みを復号するステップを含む。この方法は、コンピューティングシステムによって、識別ベースの静止形状データ、顔表情データ、および任意のポーズに対応するポーズパラメータのセットに少なくとも部分的に基づいて、トレーニングボディについてのトレーニングポーズメッシュを生成するステップを含む。この方法は、トレーニングボディについて生成されたトレーニングポーズメッシュを、任意のポーズを有するグランドトゥルース登録の全身スキャンおよびグランドトゥルース登録の顔詳細スキャンと比較する再構成損失関数を評価するステップを含む。この方法は、形状エンコーダモデルと、形状デコーダモデルと、顔エンコーダモデルと、顔デコーダモデルとを、再構成損失に少なくとも部分的に基づいて、一緒にトレーニングするステップを含む。 Another exemplary aspect of the present disclosure is directed to a computer-implemented method for end-to-end joint training of machine-learned 3D human shape models. The method includes acquiring shape scans of one or more ground truth registrations of the training body, by a computing system including one or more computing devices, wherein the one or more ground truth registrations of the training body are obtained. The registration shape scan includes at least a ground truth registration whole body scan with arbitrary poses and a ground truth registration face detail scan. The method includes encoding, by a computing system, an estimated registered whole body scan with a stationary pose to obtain a stationary shape embedding associated with a training body using a shape encoder model. The method includes decoding static shape embeddings using a shape decoder model by a computing system to obtain identity-based static shape data for training bodies. The method comprises encoding, by a computing system, data derived from face detail scans for ground truth registration using a face encoder model to obtain facial expression embeddings associated with training bodies. include. The method includes decoding facial expression embeddings using a facial decoder model by a computing system to obtain facial expression data for training bodies. The method includes, by a computing system, generating a training pose mesh for the training body based at least in part on identity-based static shape data, facial expression data, and a set of pose parameters corresponding to a given pose. including. The method includes evaluating a reconstruction loss function that compares a training pose mesh generated for a training body with a ground truth registration whole body scan and a ground truth registration face detail scan with arbitrary poses. The method includes jointly training a shape encoder model, a shape decoder model, a face encoder model, and a face decoder model based at least in part on the reconstruction loss.
本開示の別の例示的な態様は、人体についての顔表情データを生成するために、人体に関連する顔表情埋込みを処理するようにトレーニングされた機械学習済み顔表情デコーダモデルと、人体についての識別ベースの形状データを生成するために、人体に関連する形状埋込みを処理するようにトレーニングされた機械学習済み形状デコーダモデルとを含む、機械学習済み3次元人間形状モデルをまとめて記憶する、1つまたは複数の非一時的コンピュータ可読媒体を対象とする。機械学習済み3次元人間形状モデルは、顔表情データ、ポーズパラメータのセット、および識別ベースの形状データに少なくとも部分的に基づいて、人体のポーズメッシュを生成するようにトレーニングされる。機械学習済み顔表情デコーダモデルおよび機械学習済み形状デコーダモデルは、トレーニングボディについての機械学習済み3次元人間形状モデルによって生成されたトレーニングポーズメッシュを、トレーニングボディの1つまたは複数のグランドトゥルース登録の形状スキャンと比較する再構成損失関数に少なくとも部分的に基づいて、エンドツーエンドで一緒にトレーニングされている。 Another exemplary aspect of the present disclosure is a machine-learned facial expression decoder model trained to process facial expression embeddings associated with the human body to generate facial expression data for the human body; storing together a machine-learned 3D human shape model including a machine-learned shape decoder model trained to process shape embeddings associated with the human body to generate discrimination-based shape data; It is directed to one or more non-transitory computer-readable media. A machine-learned 3D human shape model is trained to generate a pose mesh of the human body based at least in part on the facial expression data, the set of pose parameters, and the identity-based shape data. The machine-learned facial decoder model and the machine-learned shape decoder model convert the training pose mesh generated by the machine-learned 3D human shape model for the training body into the shape of one or more ground truth registrations of the training body. The scans are jointly trained end-to-end based, at least in part, on the reconstruction loss function that they compare to.
本開示の他の態様は、様々なシステム、装置、非一時的コンピュータ可読媒体、ユーザインターフェース、および電子デバイスを対象とする。 Other aspects of this disclosure are directed to various systems, apparatus, non-transitory computer-readable media, user interfaces, and electronic devices.
本開示の様々な実施形態のこれらおよび他の特徴、態様、および利点は、以下の説明および添付の特許請求の範囲を参照すると、より良く理解されよう。本明細書に組み込まれるとともにその一部をなす添付の図面は、本開示の例示的な実施形態を示しており、説明とともに、関連原理を説明するのに役立つ。 These and other features, aspects, and advantages of various embodiments of the present disclosure will become better understood with reference to the following description and appended claims. BRIEF DESCRIPTION OF THE DRAWINGS The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate exemplary embodiments of the present disclosure and, together with the description, serve to explain related principles.
当業者を対象とする実施形態の詳細な説明が本明細書に記載され、本明細書は添付の図を参照する。 Detailed descriptions of embodiments directed to those skilled in the art are provided herein and reference is made to the accompanying figures.
複数の図にわたって繰り返される参照番号は、様々な実装形態において同じ特徴を識別することが意図されている。 Reference numbers that are repeated across multiple figures are intended to identify the same features in different implementations.
1.序論
一般に、本開示は、十分にトレーニング可能なモジュール式の深層学習フレームワーク内の、統計学的、多関節3D人間形状モデリングパイプラインを対象とする。詳細には、本開示の態様は、トレーニングデータのセット上でエンドツーエンドで一緒にトレーニングされる少なくとも顔および体の形状コンポーネントを有する機械学習済み3D人間形状モデルを対象とする。モデルコンポーネント(たとえば、顔のコンポーネントと体のコンポーネントの両方を含む)のジョイントトレーニングが、生成された顔形状と体形状との間の向上した整合性の合成を可能にする。
1. Introduction In general, the present disclosure is directed to a statistical, articulated 3D human shape modeling pipeline within a fully trainable and modular deep learning framework. In particular, aspects of this disclosure are directed to machine-learned 3D human shape models having at least face and body shape components that are jointly trained end-to-end on a set of training data. Joint training of model components (eg, including both facial and body components) enables synthesis of improved consistency between generated facial and body shapes.
より詳細には、いくつかの実装形態において、トレーニングデータのセットは、様々なポーズでキャプチャされた人間の高解像度の完全3Dボディスキャンを、任意選択で頭部および顔の表情および/または手の関節の追加のクローズアップとともに、含むことができる。1つの例示的なトレーニングデータセットは、34,000を超える多様な人間の構成を含むことができる。いくつかの実装形態では、これらのスキャンの各々が、1つまたは複数の初期の、アーティストが設計した、性別による区別のないリグ入り(rigged)クワッドメッシュに登録されて、人体の登録された形状スキャンを得ることができる。 More specifically, in some implementations, the training data set consists of high-resolution full 3D body scans of humans captured in various poses, optionally with head and facial expressions and/or hands. Can be included with additional close-ups of joints. One exemplary training data set can contain over 34,000 diverse human constructs. In some implementations, each of these scans is registered to one or more initial, artist-designed, gender-neutral, rigged quad meshes to obtain the registered shape of the human body. You can get a scan.
本明細書で説明するいくつかの例示的な機械学習済み3D人間形状モデルは、いくつかのサブモデルまたは他のモジュール式コンポーネントを含むことができる。例として、人間形状モデルは、人体についての顔表情データを生成するために人体に関連する顔表情埋込みを処理するようにトレーニングされた機械学習済み顔表情デコーダモデル、人体についてのポーズによる形状調整を生成するためにポーズパラメータのセットを処理するようにトレーニングされた機械学習済みポーズ空間変形モデル、および/または人体についての識別ベースの静止形状データを生成するために人体に関連する静止形状埋込みを処理するようにトレーニングされた機械学習済み形状デコーダモデルを含むことができる。たとえば、いくつかの実装形態では、デコーダモデルは、入力形状メッシュ(たとえば、顔のメッシュまたは体の静止形状メッシュ)を受け取り、それぞれの埋込み(たとえば、顔表情埋込みまたは静止形状埋込み)を生成するようにトレーニングされたそれぞれのオートエンコーダ(たとえば、変分オートエンコーダ)の一部分としてトレーニングされ得る。 Some example machine-learned 3D human shape models described herein may include several sub-models or other modular components. As examples, the human shape model is a machine-learned facial decoder model trained to process facial expression embeddings related to the human body to generate facial expression data about the human body; A machine-learned pose space deformation model trained to process a set of pose parameters to generate and/or process static shape embeddings related to the human body to generate discrimination-based static shape data about the human body It can include a machine-learned shape decoder model trained to do so. For example, in some implementations, the decoder model receives an input shape mesh (e.g., a face mesh or a body static shape mesh) and generates respective embeddings (e.g., facial expression embeddings or static shape embeddings). can be trained as part of each autoencoder (eg, a variational autoencoder) trained to .
機械学習済み3D人間形状モデルに含まれ得る追加の例示的なモデルは、人体の骨格表現の複数の関節について複数の予測される関節中心を生成するために識別ベースの静止形状データを処理するようにトレーニングされた機械学習済み関節中心予測モデル、ならびに/または人体のポーズメッシュを生成するために、顔表情データ、ポーズによる形状調整、識別ベースの静止形状データ、および1つもしくは複数の予測される関節中心を処理するようにトレーニングされた機械学習済みブレンドスキニングモデルを含むことができる。 An additional exemplary model that may be included in the machine-learned 3D human shape model is to process discrimination-based static shape data to generate multiple predicted joint centers for multiple joints of a skeletal representation of the human body. and/or facial expression data, pose-based shape adjustment, discrimination-based static shape data, and one or more predicted It can include a machine-learned blended skinning model trained to handle joint centers.
本開示の一態様によれば、上記で説明した、またはさもなければ機械学習済み3D人間形状モデルに含まれる、モデルの一部または全部が、共有される損失関数上でエンドツーエンドで一緒にトレーニングされ得る。したがって、いくつかの実装形態では、変分オートエンコーダ、ポーズ空間変形補正、骨格関節中心予測器、および/またはブレンドスキニング関数に基づく非線形形状空間を含むすべてのモデルパラメータが、単一の整合性のある学習ループにおいてトレーニングされ得る。 According to one aspect of the present disclosure, some or all of the models described above or otherwise included in the machine-learned 3D human shape model are combined end-to-end on a shared loss function. can be trained. Therefore, in some implementations, all model parameters, including nonlinear shape space based variational autoencoders, pose space deformation corrections, skeletal joint center predictors, and/or blended skinning functions, are combined into a single consistent It can be trained in some learning loop.
3D動的スキャンデータ(たとえば、34,000を超える多様な人間の構成)に関してモデルのすべてを同時にトレーニングすると、相関関係をキャプチャするモデルの能力全体が向上し、様々なコンポーネント(たとえば、モデル化された顔、体、および/または手)の整合性が確保され得る。言い方を変えれば、顔、体、および/または手のスキャンを含むことができるトレーニングデータに顔、体、および/または手のコンポーネントのジョイントトレーニングを使用することにより、得られるモデルは、顔表情分析、ならびに体(詳細な手を含む)形状およびポーズ推定をより自然にかつ矛盾なくサポートすることができる。 Training all of the models simultaneously on 3D dynamic scan data (e.g., over 34,000 diverse human configurations) improves the overall ability of the model to capture correlations, and improves the overall ability of the model to capture correlations and analyzes of various components (e.g., modeled faces). , body, and/or hand) integrity can be ensured. In other words, by using joint training of the facial, body, and/or hand components on training data, which can include facial, body, and/or hand scans, the resulting model can be used for facial expression analysis. , and body (including detailed hands) shape and pose estimation more naturally and consistently.
本開示は、本明細書で説明するように構築され、トレーニングされ、かつ、2つの異なる解像度、すなわち10,168個の頂点からなる中解像度GHUM、および2,852個の頂点の低解像度GHUML(ite)を有する、2つの例示的な十分にトレーニング可能な、性別による区別のない一般的な人間モデルを提供する。例示的な実験データもまた、これらの2つの例示的な人間モデルに提供され、誤差が減少して向上した品質および整合性を示す。例として、図1は、GHS3DからのデータについてのGHUMおよびGHUMLの例示的な評価を、左側の両モデルのヒートマップとともに提供する。レンダリングは、対象(後列)の異なる体のポーズの登録、ならびに中列および前列にそれぞれGHUMLフィットおよびGHUMフィットを示す。両方のモデルが、優れた品質推定を示し、GHUMでは誤差がより小さい。 The present disclosure was constructed and trained as described herein and has two different resolutions: medium resolution GHUM of 10,168 vertices and low resolution GHUML(ite) of 2,852 vertices. , we provide two exemplary well-trainable, gender-neutral generic human models. Exemplary experimental data are also provided for these two exemplary human models, showing improved quality and consistency with reduced error. As an example, Figure 1 provides an exemplary evaluation of GHUM and GHUML on data from GHS3D, with heatmaps of both models on the left. Renderings show the registration of different body poses of the subject (back row) and the GHUML and GHUM fits in the middle and front rows, respectively. Both models show excellent quality estimates, with smaller errors for GHUM.
したがって、本開示の態様は、体の形状、ならびに顔の表情および/または手の動きを作動させることができる全身の、統計学的な人間の形状およびポーズモデルを構成するためのエンドツーエンドの学習パイプラインを対象とする。コンピューティングシステムが深層学習を実施できるようにし、解剖学的に関節角度制約がある最小限の人間の骨格パラメタリゼーションの状況において、非線形形状空間、ポーズ空間変形補正、骨格関節中心推定器、および/またはブレンドスキニング関数を含む、すべてのモデルコンポーネントの同時トレーニングが可能になる、エンドツーエンドのパイプラインおよび統一損失関数が提供される。このモデルは、体の部分コンポーネント間の最大限の詳細および設計整合性を確保するために、高解像度の全身スキャン、ならびに動いている顔および/または手のクローズアップを用いてトレーニングされ得る。 Aspects of the present disclosure thus provide an end-to-end method for constructing a full-body, statistical human shape and pose model that can drive body shape, as well as facial expressions and/or hand movements. Target the learning pipeline. Enables a computing system to perform deep learning, in the context of minimal human skeletal parameterization with anatomically joint angle constraints, nonlinear shape space, pose space deformation correction, skeletal joint center estimator, and/or An end-to-end pipeline and unified loss function is provided that allows simultaneous training of all model components, including blended skinning functions. The model can be trained with high-resolution full-body scans and close-ups of the face and/or hands in motion to ensure maximum detail and design consistency between body part components.
加えて、一般的な人間の形状の新しく収集された3Dデータセット、GHS3Dが記述され、30,000を超える写真のように現実的な動的人体スキャンで構成される。例示的な実施形態はまた、Caesarからの4,000を超える全身スキャンを使用する。中程度の解像度のモデル、GHUMと、特別に設計された(ダウンサンプリングされていない)低解像度のモデル、GHUMLの両方が提供され、それらの相対的なパフォーマンスが、異なる線形および非線形モデル(体の形状および顔の表情ではPCAまたは変分オートエンコーダ)の下で、登録および拘束付き3D表面フィッティングについて評価される。画像からの形状およびポーズの復元も、図示されている。 In addition, a newly collected 3D dataset of common human shapes, GHS3D, is described and consists of over 30,000 photorealistic dynamic human body scans. Exemplary embodiments also use over 4,000 full body scans from Caesar. Both a medium-resolution model, GHUM, and a specially designed (non-downsampled) low-resolution model, GHUML, are provided and their relative performance can be compared with different linear and nonlinear models (body It is evaluated for registration and constrained 3D surface fitting under PCA or variational autoencoders for shape and facial expression. Reconstruction of shape and pose from images is also illustrated.
本明細書で説明するシステムおよび方法は、いくつかの技術的効果および利益を提供する。1つの例示的な技術的効果として、本開示のシステムおよび方法は、体、顔、および/または手の特徴間の整合性が改善した人間の形状を含む、人間の形状のより現実的な2次元または3次元レンダリングまたはモデルを提供することができる。具体的には、モデルコンポーネントは、互いに整合するように学習し、また領域間共通の(cross-domain)パターンまたは関係を学習することができる。したがって、本開示のシステムおよび方法は、コンピューティングシステムが向上した人間モデリング機能を実施できるようにすることができる。 The systems and methods described herein provide several technical effects and benefits. As one exemplary technical effect, the systems and methods of the present disclosure provide a more realistic representation of human shapes, including human shapes with improved consistency between body, facial, and/or hand features. Dimensional or 3D renderings or models can be provided. Specifically, model components can learn to align with each other and learn cross-domain patterns or relationships. Accordingly, the systems and methods of the present disclosure can enable computing systems to perform enhanced human modeling functions.
別の例示的な技術的効果は、複数のモデルコンポーネントを一緒にトレーニングすることによって、人間の形状モデルを生成するために必要なトレーニング時間の総量を減らすことができる。より詳細には、以前の手法は、顔および体のコンポーネントを別々にトレーニングし、次いでそれらを事後に結合しようとし、2つの異なるトレーニングプロセスと、さらに組合せを容易にするための追加の作業となり、その組合せはそれでもなお整合性がない場合がある。提案する手法は、1つのエンドツーエンドプロセスですべてのモデルコンポーネントを一緒にトレーニングし、それによってトレーニングおよび得られるモデルをより整合性のあるものにする。 Another exemplary technical effect can reduce the total amount of training time required to generate a human shape model by training multiple model components together. More specifically, previous approaches trained the facial and body components separately and then attempted to combine them post-hoc, resulting in two different training processes and additional work to facilitate the combination, The combination may still be inconsistent. The proposed method trains all model components together in one end-to-end process, thereby making the trained and resulting models more consistent.
次に図を参照しながら、本開示の例示的な実施形態についてさらに詳細に説明する。 Exemplary embodiments of the present disclosure will now be described in greater detail with reference to the figures.
2.例示的な実装形態の概要
構造化されていないポイントクラウド
登録されたグランドトゥルースメッシュX*は次いで、モデルパラメータαが関節と形状調整の両方の結果として入力を厳密にマッチさせる出力を生成するように調整されるエンドツーエンドのトレーニングネットワークにフィードされ得る。反復最近傍点(ICP: iterative closest point)ロス(登録に使用されるものと同じ)による、またはプロキシメッシュX*に合わせた、ポイントクラウドへの直接モデルパラメータ調整を含む、様々な技法を使用することができる。同じモデルトポロジーのターゲット入力メッシュX*として有して、プロセスを大幅により速くし、トレーニング損失をより良いふるまいにする。 The registered ground truth mesh X * can then be fed into an end-to-end training network where the model parameter α is adjusted to produce an output that closely matches the input as a result of both joint and shape adjustments. Using a variety of techniques, including direct model parameter adjustment to the point cloud, by iterative closest point (ICP) loss (same as used for registration) or fitted to proxy mesh X * can be done. Having the same model topology as the target input mesh X * makes the process significantly faster and the training loss behaves better.
したがって、図2は、例示的なエンドツーエンドの統計学的3D多関節人間形状モデル構成の概要を示す。トレーニング入力は、静止している(または「A」)ポーズと、様々な関節および軟組織の変形を表す任意のポーズの両方を含む、高解像度の3Dボディスキャンのセットを含むことができる。加えて、異なるジェスチャーおよびオブジェクトグラブ(object grab)をキャプチャするために、詳細な顔の表情の頭部クローズアップスキャンおよび/または手のクローズアップスキャンが収集され得る。これらのスキャンは、Yに概略的に示されている。 Accordingly, FIG. 2 outlines an exemplary end-to-end statistical 3D articulated human shape model construction. The training input can include a set of high-resolution 3D body scans, including both static (or "A") poses and arbitrary poses representing various joint and soft tissue deformations. In addition, detailed facial expression head close-up scans and/or hand close-up scans may be collected to capture different gestures and object grabs. These scans are shown schematically in Y.
ボディランドマークが、複数の仮想視点(たとえばこれは、データを収集するために使用されたカメラの当初のセットとは異なっていてもよい)からデータ(たとえば、多視点ステレオ三角測量技法を使用して取得される)の写真のように現実的な3D再構成をレンダリングし、生成された画像においてそれらのランドマークを検出し、それらの対応する3d再構成を取得するために画像中のランドマーク検出を三角測量することによって、自動的に識別され得る。アーティストが設計した全身多関節メッシュが、先のできる限り等角の表面の下で、(たとえば、メッシュファセット距離にポイントスキャンとして実装される)疎なランドマーク対応と密な反復最近傍点(IPC)残余部を組み合わせる損失を使用して、ポイントクラウドに次第に登録され得る。登録されたグランドトゥルース形状スキャンは、X*に概略的に示されている。 Body landmarks are identified using data (e.g., using multi-view stereo triangulation techniques) from multiple virtual viewpoints (e.g., which may be different from the original set of cameras used to collect the data). ), detect those landmarks in the generated image, and extract the landmarks in the image to obtain their corresponding 3d reconstruction. It can be automatically identified by triangulating the detection. An artist-designed full-body articulated mesh under a surface that is as conformal as possible, with sparse landmark correspondences (e.g., implemented as point scans to mesh facet distances) and dense iterative nearest neighbor points (IPC) The residuals can be progressively registered to the point cloud using loss combining. The registered ground truth shape scan is shown schematically in X * .
図2に示す例示的な人間形状モデルは、体φbにはディープ変分オートエンコーダ(VAE)および顔の表情φfにはオフセットVAEとして実装される非線形形状空間を有し得る。例示的な人間形状モデルはまた、J個の関節を有する骨格Kによって変調される、トレーニング可能なポーズ空間変形関数Dと、中心予測器Cと、ブレンドスキニング関数Mとを含む。 The exemplary human shape model shown in FIG. 2 may have a nonlinear shape space implemented as a deep variational autoencoder (VAE) for the body φ b and an offset VAE for the facial expression φ f . The exemplary human shape model also includes a trainable pose space transformation function D, a center predictor C, and a blend skinning function M modulated by a J-jointed skeleton K.
いくつかの実装形態では、トレーニング中に、同じ対象のすべての高解像度スキャン(全身と顔および手のクローズアップの両方)が、フィルタFによって適切にマスキングされた残余部とともに、使用され得る(図3参照)。モデル構成について、N個のキャプチャされた対象が、B個の全身スキャン、F個のクローズアップの手のスキャン、およびH個のクローズアップの頭部スキャンとともに使用され得る。いくつかの実装形態では、学習中に、トレーニングアルゴリズムは、各スキャンθにおけるポーズ推定に関して損失関数を最小化することと、他のモデルパラメータ(φ,γ,ψ,ω)に関して損失関数を最適化することとを交互に行うことができる。 In some implementations, during training, all high-resolution scans of the same subject (both full-body and close-ups of face and hands) can be used, with residuals appropriately masked by filter F (Fig. 3). For model construction, N captured subjects may be used with B whole body scans, F close-up hand scans, and H close-up head scans. In some implementations, during learning, the training algorithm minimizes the loss function with respect to the pose estimate at each scan θ and optimizes the loss function with respect to the other model parameters (φ, γ, ψ, ω). You can alternate between
動作中、ポーズおよび形状推定のために、モデルは、φ=(φf,φb)によって与えられるエンコーダデコーダを用いて、体形および顔の表情β=(βf,βb)についての運動学的ポーズθおよびVAE潜在空間を含むパラメータα=(θ,β)によって制御され得る。 During motion, for pose and shape estimation, the model uses the encoder-decoder given by φ=(φ f ,φ b ) to calculate kinematics for body shape and facial expression β=(β f ,β b ). It can be controlled by the parameters α=(θ,β), including the target pose θ and the VAE latent space.
2.1例示的な人間モデル表現
本開示の例示的な実装形態は、人間モデルを、J個の関節を有する骨格K、および関節の動きを明示的に符号化するように線形ブレンディングスキニング(LBS)で変形された皮膚によって指定される、多関節メッシュとして表すことができる。骨格の関節の動きに加えて、顔の表情を導出するために、非線形モデルが使用され得る。J個の関節を有するモデルXを、M(α=(θ,β),φ,γ,ω)として、詳細には、
本明細書ではGHUMおよびGHUMLと呼ぶ、提案する人間モデルの2つの例示的な実装形態は、それぞれ、アーティストが定義するリグ入りテンプレートメッシュ(Vghum=10,168、Vghuml=2852、J=63)を使用して生成される。GHUMとGHUMLの両方について、図2に示すパイプラインは、すべてのパラメータ(θ,φ,γ,ψ,ω)を推定したが、メッシュトポロジーおよび関節階層(joint hierarchy)Kは、固定的であると考えられる。生体力学的な関節角度限界もまた最適化中に活用され得るように、階層は、解剖学的に(したがって最小限に)パラメータ化される。頂点xi∈Xは、
3.例示的なエンドツーエンドの統計モデル学習
この節では、スキニングウェイトωを最適化する例示的なエンドツーエンドのニューラルネットワークベースのパイプラインの説明を行い、静止形状埋込み
3.1例示的な変分体形オートエンコーダ
いくつかの例示的な実装形態では、複数の対象の形状スキャンは、静止またはニュートラルの「A」ポーズにおいて、Caesarデータセット(4,329対象)ならびにGHS3Dのキャプチャされたスキャンにモデルを登録することによって取得され得る。一例として、図3は、頭部および顔のスキャンのクローズアップを示す。いくつかの例示的な実装形態は、ボディスキャンならびにクローズアップの手および頭部スキャンを融合することによって、ニュートラルのAポーズで全身形状を推定する。単一のボディスキャンからの体形推定と比較して、これらの例示的な実装形態は、追加の頭部および手の形状の詳細を利用することができる。
3.1 Exemplary Variant Shape Autoencoders In some exemplary implementations, shape scans of multiple objects were captured in the Caesar dataset (4,329 objects) as well as GHS3D in static or neutral 'A' poses. It can be obtained by registering the model to the scan. As an example, Figure 3 shows a close-up of a head and face scan. Some example implementations estimate the full-body shape in a neutral A-pose by fusing body scans and close-up hand and head scans. Compared to body shape estimation from a single body scan, these example implementations can take advantage of additional head and hand shape details.
複数の対象について推定される静止形状
いくつかの例示的な実装形態では、エンコーダおよびデコーダは、パラメトリックReLU活性化関数を使用することができる。それらはいくつかのパラメータに対して、恒等変換(identity transformation)または標準的なReLUのいずれかをモデル化することができるからである。標準的な実行として、変分エンコーダは、平均および分散(μ,Σ)を出力することができ、これらは、サンプリングされたコードβbを取得するために、再パラメータ化トリックにより潜在空間に変換することができる。いくつかの実装形態では、単純な分布、N(0,I)を使用することができ、潜在空間を正則化するために損失関数にカルバックライブラーダイバージェンスを融合させることができる。したがって、1つの例示的な公式は、次の通りである。
3.2例示的な変分顔表情オートエンコーダ
変分体形オートエンコーダは、顔の形状の分散を含む、様々な体形を表すことができる。(単なる人体計測の頭部および顔の静止変形とは対照的に)複雑な顔の表情をさらにサポートするために、任意選択で追加の顔のモデリングを導入することができる。たとえば、モデルは、GHS3Dの数千の顔の表情の動きシーケンススキャンから構築することができる。3-DOFの関節顎、2つの2-DOFのまぶた、および2つの2-DOFの眼球に加えて、スキニングウェイトおよびポーズ空間変形を含む、頭部の関節接合部(articulated joint)のパラメータは、パイプラインの残りとともに更新することができる。
3.2 Exemplary Variational Facial Expression Autoencoders Variational body shape autoencoders can represent a variety of body shapes, including variations in facial shape. Additional facial modeling can optionally be introduced to further support complex facial expressions (as opposed to just anthropometric head and face static deformations). For example, models can be built from motion sequence scans of thousands of facial expressions in GHS3D. The parameters of the articulated joint of the head, including the 3-DOF articulated jaw, the two 2-DOF eyelids, and the two 2-DOF eyeballs, plus skinning weights and pose-space deformations, are It can be updated with the rest of the pipeline.
関節ではない表情による顔の動きについては、非線形埋込みβfが、変分体形オートエンコーダと同じネットワーク構造内に構築され得る。VAEへの入力は、すべての関節接合部の動き(首、頭部、眼、および顎を含む)を除去することによって、ニュートラルの頭部ポーズでの顔の表情
3.3例示的なスキニングモデル
非線形形状および顔の表情モデルを適用した後、最適なスキニング関数が、複数の対象および複数のポーズのメッシュデータから統計学的に推定され得る。具体的には、(4)の場合と同じデータ用語が使用され得るが、ここで最適化変数は、関節中心推定器のパラメータ
骨格関節中心の1つの可能な選択は、それらを、関節によって最大限に影響を受ける2つのメッシュコンポーネント(セグメンテーション)を結合する境界頂点のリング上の平均的位置に配置することである。境界頂点、
いくつかの実装形態では、すべてのメッシュ頂点にわたってプールすることによって大域的に関節中心を学習する代わりに、関節によってスキニングされたそれらの頂点から単に局所的に推定を行うことができる。これは、かなり少ないトレーニング可能なパラメータとなり、3N×3Jから3N×3Iに減少し、実際にはたとえばI=4となる。スパース性もまた、L1正則化、また骨方向をテンプレートに合わせることにより助長され得る。特異点を回避し、関節中心が表面の外側に動くことを防ぐために、中心補正の大きさ
いくつかの実装形態では、複雑な軟組織の変形に起因するスキニングアーチファクトを修復するために、データ駆動型のポーズによる補正器(PSD: pose-dependent corrector)
一例として、図5は、PSDの利点を示す、ここでは非受動的関節点、たとえば右股関節部(hip)および大腿部、ならびに胸部および腋窩の周りの、例示的なポーズ空間変形アーキテクチャスケッチおよび図を示す。説明を簡単にするために、ここではθは、
さらに、いくつかの事例では
高周波ローカルPSDは、多くの場合望ましくなく、過学習に起因する可能性が高い。したがって、以下を用いた、なめらかなポーズ空間変形が推奨され得る。
PSD正則化器および数を減らしたトレーニング可能な重みを用いても、依然として過学習が発生し得る。ポーズ空間変形が特にいくつかの領域(体および手)のみに対して構築されたSMPLまたはMANOとは異なり、PSDモデルは、本開示のいくつかの実装形態において、高解像度の体、手、および頭部のデータクローズアップに基づいて一緒にトレーニングされた、人間モデル全体に対して構成される。結果として、体のデータは、手および頭部の動きに限られた変化を有するが、頭部および手のデータには、体の残部についての動きがない。したがって、すべての関節が損失への影響なしに動くことができる大きい関節空間がある。これは望ましくない。過学習を防ぐために、入力ポーズ特徴ベクトルは、頭部、体、左手、および右手の関節をとらえる、4つの特徴ベクトルに、フィルタリングまたはマスクキングされ得る。各特徴ベクトルは、同じReLUレイヤに取り込むことができ、出力は、次のリグレッサの前に合計され得る。したがって、1つの例示的な損失は、次の通りである。
いくつかの実装形態では、パイプラインの終わりにスキニングウェイトを推定するために、線形ブレンディングスキニングレイヤを使用することができ、所与のポーズθおよび顔の表情付きのポーズ補正された静止形状
最終のスキンメッシュXはまた、以下のように加算によってなめらかになるように弱く正則化され得る。
ポーズ推定器。体形推定および現在のスキニングパラメータを仮定すると、ポーズθは、トレーニングセットによって再最適化され得る。探索空間を制限し、整合性を強化し、不自然な局所的最小値を回避するために、人体計測の骨格で利用可能な解剖学的関節角度制限を活用することができる。問題は、箱型制約(box constraint)があるL-BFGSソルバー、およびTensorFlowの自動微分によって評価される勾配を使用して効率的に解決され得る。 Pose estimator. Given the body shape estimate and current skinning parameters, the pose θ can be reoptimized by the training set. Anatomical joint angle limits available in the anthropometric skeleton can be exploited to limit the search space, enforce consistency, and avoid unnatural local minima. The problem can be efficiently solved using the L-BFGS solver with box constraints and gradients evaluated by automatic differentiation in TensorFlow.
4.例示的な実験
この節では、本明細書で説明するシステムおよび方法の例示的な実装形態で行われる例示的な実験を説明する。
4. Exemplary Experiments This section describes exemplary experiments performed with exemplary implementations of the systems and methods described herein.
データセット。多様な体および顔形状(4,329対象)を含んでいるCaesarに加えて、本明細書で説明する例示的な実験は、55の体のポーズ、60の手のポーズ、および40の顔の表情の動きシーケンスとともに32の対象(女性16および男性16)をキャプチャするために、60Hzで動作する複数の3dmdシステムも使用した。対象は、BMIが17.5～39の範囲、身長152cm～192cm、年齢22～47である。すべての複数のポーズデータに対して、一般的にトレーニングセットにないポーズを含むフリースタイルの動きシーケンスに基づいて、評価に3対象、およびテストに4対象を使用する。各顔キャプチャシーケンスは、ニュートラルの顔から開始して、指定された顔の表情になり、各シーケンスが約2s続く。 data set. In addition to Caesar, which contains a wide variety of body and face shapes (4,329 subjects), the exemplary experiment described here includes 55 body poses, 60 hand poses, and 40 facial expressions. Multiple 3dmd systems operating at 60 Hz were also used to capture 32 subjects (16 female and 16 male) with motion sequences. Subjects ranged in BMI from 17.5 to 39, were 152 cm to 192 cm tall, and were 22 to 47 years old. For all multiple pose data, we typically use 3 subjects for evaluation and 4 subjects for testing, based on freestyle motion sequences that include poses not in the training set. Each face capture sequence starts with a neutral face to a specified facial expression, each sequence lasting about 2 s.
データからの登録サンプルが、図6に示されている。具体的には、図6は、Caesar(左上)ならびにGHS3Dからのデータのサンプル登録を示す。顔の詳細、および関節の結果としての他の体の部分の軟組織の変形をキャプチャする登録の質に注目されたい。 A sample enrollment from the data is shown in FIG. Specifically, Figure 6 shows a sample registration of data from Caesar (top left) as well as GHS3D. Note the quality of the registration that captures facial details and soft tissue deformations in other body parts as a result of joints.
登録。表1は、ICPおよび(拡張された)Chamfer距離を使用したポイントクラウドへの登録を報告する。ICP誤差は、最も近い登録されたメッシュファセットまでの点と平面との距離として測定されるが、Chamferは、点から点で双方向に推定される。 register. Table 1 reports the registration to the point cloud using ICP and the (extended) Chamfer distance. The ICP error is measured as the point-to-plane distance to the nearest registered mesh facet, while Chamfer is estimated bidirectionally from point to point.
Table 1(表1):GHUMおよびGHUMLでのCaesarおよびGHS3D(顔、手、および体の残部の詳細を含む)上の登録誤差。
提案する登録技法は、誤差が小さく、局所的なポイントクラウドの詳細を保存する(図6)。 The proposed registration technique has small errors and preserves local point cloud details (Fig. 6).
モデル評価。フル解像度と低解像度の両方の人間モデル(GHUMおよびGHUML)が、エンドツーエンドのパイプラインを使用して構築された。両方のモデルが、骨格関節の同じセットを共有するが、10,168個のメッシュ頂点に対して2,852個のメッシュ頂点(顔の表情について1,932個の頂点に対して585個の頂点がある)を有する。両方のモデルについて、例示的な実験は、テストデータ上で登録X*までのメッシュXの頂点に基づく平均ユークリッド距離を評価した。数字はTable 2(表2)に報告され、視覚化は、図4、図1、および図9に示されている(Sup. Matで手の評価を検索されたい)。両方のモデルの出力を、それらの対応するトポロジーの下で登録されたメッシュと比較する。両方のモデルは、多様な体形(たとえば、VAEとしてモデル化される)を厳密に表し、自然な顔の表情(たとえば、顔のVAEとして表現される)を生成し、様々な体形およびポーズに対して顕著なスキニングアーチファクトなしになめらかかつ自然にポーズをとることができる。 model evaluation. Both full-resolution and low-resolution human models (GHUM and GHUML) were built using an end-to-end pipeline. Both models share the same set of skeletal joints, but have 2,852 mesh vertices versus 10,168 mesh vertices (585 versus 1,932 vertices for facial expressions). For both models, an exemplary experiment evaluated the vertex-based mean Euclidean distance of mesh X to registration X * on the test data. Numbers are reported in Table 2 and visualizations are shown in Figures 4, 1 and 9 (search for hand ratings in Sup. Mat). Compare the outputs of both models with meshes registered under their corresponding topologies. Both models rigorously represent a variety of body shapes (e.g., modeled as VAEs), generate natural facial expressions (e.g., represented as facial VAEs), and perform well for a variety of body shapes and poses. poses smoothly and naturally without noticeable skinning artifacts.
Table 2(表2):登録するための、頂点に基づく平均ユークリッド誤差(mm)。
GHUM対GHUML。低解像度モデルは、体形の大域的特徴を保存し、体および顔の動きを正確にスキニングする。GHUMと比較して、GHUMLは、唇の変形、腕および指における筋肉の隆起、脂肪組織に起因するしわについてのいくらかの詳細を失うことが観測され得る。パフォーマンスに関しては、GHUMLは、フィードフォワード評価モードで、GHUMよりも2.5倍高速である。 GHUM vs. GHUML. The low-resolution model preserves global features of body shape and accurately skins body and face movements. Compared to GHUM, it can be observed that GHUML loses some detail on lip deformation, muscle bulges in arms and fingers, and wrinkles due to adipose tissue. In terms of performance, GHUML is 2.5 times faster than GHUM in feedforward evaluation mode.
図4は、Caesarでの評価を示す。図4の左側は、GHUMおよびGHUMLについての登録までの頂点ごとのユークリッド距離誤差を示す。図4の右側は、上から下に、登録、GHUM、およびGHUMLを示す。VAEベースのモデルは、体形を極めてよく表すことができる。GHUMLと比較して、さらなる筋肉またはウエスト軟組織の詳細が、GHUMによって保存される。 Figure 4 shows the evaluation in Caesar. The left side of FIG. 4 shows the vertex-wise Euclidean distance error to registration for GHUM and GHUML. The right side of Figure 4 shows, from top to bottom, registrations, GHUM, and GHUML. VAE-based models can represent body shape very well. More muscle or waist soft tissue detail is preserved by GHUM compared to GHUML.
図9は、GHUMおよびGHUMLの手の再構成を強調して、図1のように評価およびレンダリングを示す。図1の場合と同様の結論が成り立つ。手のひらの屈曲領域の周りのさらなる変形の詳細がGHUMLよりもGHUMによって保存されていることに注目されたい。 Figure 9 shows the evaluation and rendering as in Figure 1, highlighting the GHUM and GHUML hand reconstructions. A similar conclusion holds as in the case of FIG. Note that more deformation detail around the palm flexion region is preserved by GHUM than by GHUML.
VAE評価。体形について、提案するVAEは、16-dimと61-dimの両方の潜在表現をサポートし、前者は1.72倍高い再構成誤差を有する(Table 2(表2)の報告および数字は、64-dim表現に基づいている)。いくつかの例では、20-dimの埋込みが、顔の表情VAEに使用されることがある。 VAE rating. For body shape, the proposed VAE supports both 16-dim and 61-dim latent representations, with the former having a 1.72-fold higher reconstruction error (reports and figures in Table 2 are for 64-dim expression). In some examples, a 20-dim embedding may be used for facial expression VAE.
図7は、VAEとPCAの両方について、潜在次元の関数として顔の表情の再構成誤差を示す。20次元のVAEは、96線形PCA基底を使用するものと同様の再構成誤差を有し、パフォーマンスが2.9倍遅くなる。具体的には、図7は、低次元体制における非線形性の利点を説明する、VAEおよびPCAモデルの分析を示す。 Figure 7 shows the facial expression reconstruction error as a function of latent dimension for both VAE and PCA. A 20-dimensional VAE has a similar reconstruction error to that using 96 linear PCA basis, with a 2.9 times slower performance. Specifically, FIG. 7 shows analyzes of VAE and PCA models that illustrate the benefits of nonlinearity in the low-dimensional regime.
GHUM対SMPL。図8では、GHUMおよびSMPLが、視覚的品質について比較される。詳細には、図8は、2つのポーズの各々について、左から右に、登録、GHUM、およびSMPLを示す。この動きシーケンスには骨盤アーチファクトがより少ないことに注目されたいが、GHUMは、比較可能な視覚的品質のポージングを生成する。 GHUM vs SMPL. In Figure 8, GHUM and SMPL are compared for visual quality. Specifically, FIG. 8 shows, from left to right, registration, GHUM, and SMPL for each of the two poses. Note that this motion sequence has less pelvic artifacts, but GHUM produces poses of comparable visual quality.
GHUMは、SMPLからの異なるメッシュおよび骨格トポロジーを有し、SMPLには、手および顔の関節がない。比較するために、GHS3Dからのキャプチャされた動きシーケンス(我々のトレーニングデータセットにはない、すべてのポーズ)が採用され、キャプチャされたシーケンスは、SMPLおよびGHUMメッシュそれぞれとともに登録される。誤差が評価されるとき、1対1の点から平面のユークリッド距離が(たとえば、登録中の表面スライディングに過敏になるのを防ぐために)使用され、誤差は、SMPLとの公正な比較のために体領域についてのみ評価される。GHUMからの平均再構成誤差は、4.4mmであるが、SMPLは、5.37mmの誤差を有し、GHUMの視覚的スキニング品質は、SMPLと同等であると観測される。 GHUM has a different mesh and skeletal topology from SMPL, which lacks hand and facial joints. For comparison, captured motion sequences from GHS3D (all poses not in our training dataset) are taken and the captured sequences are registered with SMPL and GHUM meshes, respectively. When the error is evaluated, the one-to-one point-to-plane Euclidean distance is used (e.g., to prevent sensitivity to surface sliding during registration), and the error is calculated for fair comparison with the SMPL Only body regions are evaluated. The average reconstruction error from GHUM is 4.4 mm, while SMPL has an error of 5.37 mm, and the visual skinning quality of GHUM is observed to be comparable to SMPL.
単眼画像からの3Dポーズおよび形状再構成。この節では、GHUMを用いた画像推論について説明する。この場合、(両手、および顔を除く体の残部についての)モデルの運動事前知識(kinematic prior)が、Human3.6M、CMU、およびGHS3Dからのデータでトレーニングされている。画像予測器が、ポーズおよび形状に使用されなかった。代わりに、初期設定は、6つの異なる運動構成で行われ、αパラメータは、解剖学的関節角度制限の下で最適化された。損失として、骨格関節再投影誤差および意味論的身体部分アライメント(semantic body-part alignment)が使用された。結果は、図10に示されている。具体的には、図10は、意味論的身体部分アライメント損失下で非線形ポーズおよび形状最適化に依存することによるGHUMでの単眼3D人間ポーズおよび形状再構成を示している。 3D pose and shape reconstruction from monocular images. This section describes image inference using GHUM. In this case, the model's kinematic priors (for the rest of the body, excluding hands and face) have been trained with data from Human3.6M, CMU, and GHS3D. No image predictor was used for pose and shape. Instead, initial setup was performed with 6 different motion configurations and the α parameter was optimized under anatomical joint angle limits. Skeletal joint reprojection error and semantic body-part alignment were used as losses. Results are shown in FIG. Specifically, Fig. 10 shows monocular 3D human pose and shape reconstruction in GHUM by relying on nonlinear pose and shape optimization under semantic body part alignment loss.
5.例示的なデバイスおよびシステム
図11Aは、本開示の例示的実施形態による例示的コンピューティングシステム100のブロック図を示す。システム100は、ネットワーク180を介して通信可能に結合されている、ユーザコンピューティングデバイス102、サーバコンピューティングシステム130、およびトレーニング用コンピューティングシステム150を含む。
5. Exemplary Devices and Systems FIG. 11A shows a block diagram of an
ユーザコンピューティングデバイス102は、たとえば、パーソナルコンピューティングデバイス(たとえば、ラップトップまたはデスクトップ)、モバイルコンピューティングデバイス(たとえば、スマートフォンまたはタブレット)、ゲーミングコンソールもしくはコントローラ、ウェアラブルコンピューティングデバイス、埋込み型コンピューティングデバイス、あるいは任意の他のタイプのコンピューティングデバイスなどの、任意のタイプのコンピューティングデバイスであってもよい。 User computing devices 102 are, for example, personal computing devices (eg, laptops or desktops), mobile computing devices (eg, smartphones or tablets), gaming consoles or controllers, wearable computing devices, embedded computing devices, Or it may be any type of computing device, such as any other type of computing device.
ユーザコンピューティングデバイス102は、1つまたは複数のプロセッサ112と、メモリ114とを含む。1つまたは複数のプロセッサ112は、任意の適切な処理デバイス(たとえば、プロセッサコア、マイクロプロセッサ、ASIC、FPGA、コントローラ、マイクロコントローラなど)であってもよく、1つのプロセッサまたは動作可能に接続されている複数のプロセッサであってよい。メモリ114は、RAM、ROM、EEPROM、EPROM、フラッシュメモリデバイス、磁気ディスクなど、およびそれらの組合せのような、1つまたは複数の非一時的コンピュータ可読記憶媒体を含み得る。メモリ114は、データ116と、ユーザコンピューティングデバイス102に動作を実施させるようにプロセッサ112によって実行される命令118とを記憶することができる。
User computing device 102 includes one or more processors 112 and
いくつかの実装形態では、ユーザコンピューティングシステム102は、1つまたは複数の機械学習済みモデル120を記憶するか、または含むことができる。たとえば、機械学習済みモデル120は、ニューラルネットワーク(たとえば、ディープニューラルネットワーク)または非線形モデルおよび/もしくは線形モデルを含む他のタイプの機械学習済みモデルなど、様々な機械学習済みモデルであってよく、またはそうでなければ、それらの機械学習済みモデルを含むことができる。ニューラルネットワークは、フィードフォワードニューラルネットワーク、回帰型ニューラルネットワーク(たとえば、長短期メモリ回帰型ニューラルネットワーク)、畳み込みニューラルネットワーク、または他の形のニューラルネットワークを含み得る。例示的機械学習済みモデル120については、図2を参照して論じる。
In some implementations, user computing system 102 may store or include one or more machine-learned
いくつかの実装形態では、1つまたは複数の機械学習済みモデル120は、ネットワーク180を介してサーバコンピューティングシステム130から受信され、ユーザコンピューティングデバイスメモリ114に記憶され、次いで、1つまたは複数のプロセッサ112によって使われ、またはそうでなければ実装され得る。いくつかの実装形態では、ユーザコンピューティングデバイス102は、単一機械学習済みモデル120の複数の並列インスタンスを実装することができる。
In some implementations, one or more machine-learned
追加または代替として、1つまたは複数の機械学習済みモデル140は、クライアント-サーバ関係に従ってユーザコンピューティングデバイス102と通信するサーバコンピューティングシステム130に含まれ、またはそうでなければ、サーバコンピューティングシステム130によって記憶され、実装され得る。たとえば、機械学習済みモデル140は、ウェブサービス(たとえば、体形モデリングおよび/またはレンダリングサービス)の一部分として、サーバコンピューティングシステム130によって実装され得る。したがって、1つまたは複数のモデル120が、ユーザコンピューティングデバイス102において記憶され、実装されてよく、かつ/または1つもしくは複数のモデル140が、サーバコンピューティングシステム130において記憶され、実装されてよい。
Additionally or alternatively, the one or more machine-learned
ユーザコンピューティングデバイス102は、ユーザ入力を受信する1つまたは複数のユーザ入力構成要素122も含み得る。たとえば、ユーザ入力構成要素122は、ユーザ入力オブジェクト(たとえば、指またはスタイラス)のタッチに敏感な、タッチ感応構成要素(たとえば、タッチ感応表示画面またはタッチパッド)であってよい。タッチ感応構成要素は、仮想キーボードを実装するのに役立ち得る。他の例示的ユーザ入力構成要素は、マイクロフォン、従来のキーボード、またはユーザがユーザ入力を与えることができる他の手段を含む。
User computing device 102 may also include one or more
サーバコンピューティングシステム130は、1つまたは複数のプロセッサ132と、メモリ134とを含む。1つまたは複数のプロセッサ132は、任意の適切な処理デバイス(たとえば、プロセッサコア、マイクロプロセッサ、ASIC、FPGA、コントローラ、マイクロコントローラなど)であってもよく、1つのプロセッサまたは動作可能に接続されている複数のプロセッサであってよい。メモリ134は、RAM、ROM、EEPROM、EPROM、フラッシュメモリデバイス、磁気ディスクなど、およびそれらの組合せのような、1つまたは複数の非一時的コンピュータ可読記憶媒体を含み得る。メモリ134は、データ136と、サーバコンピューティングシステム130に動作を実施させるようにプロセッサ132によって実行される命令138とを記憶することができる。
いくつかの実装形態では、サーバコンピューティングシステム130は、1つまたは複数のサーバコンピューティングデバイスを含むか、またはそうでなければ、1つまたは複数のサーバコンピューティングデバイスによって実装される。サーバコンピューティングシステム130が複数のサーバコンピューティングデバイスを含む事例では、そのようなサーバコンピューティングデバイスは、順次コンピューティングアーキテクチャ、並列コンピューティングアーキテクチャ、またはそれらの何らかの組合せに従って動作することができる。
In some implementations, server-
上述したように、サーバコンピューティングシステム130は、1つまたは複数の機械学習済みモデル140を記憶するか、またはそうでなければ含むことができる。たとえば、モデル140は、様々な機械学習済みモデルであってよく、または、そうでなければそれらを含んでよい。例示的な機械学習済みモデルは、ニューラルネットワークまたは他のマルチレイヤ非線形モデルを含む。例示的なニューラルネットワークは、フィードフォワードニューラルネットワーク、ディープニューラルネットワーク、回帰型ニューラルネットワーク、および畳み込みニューラルネットワークを含む。例示的なモデル140については、図2を参照して説明する。
As noted above,
ユーザコンピューティングデバイス102および/またはサーバコンピューティングシステム130は、ネットワーク180を介して通信可能に結合されるトレーニング用コンピューティングシステム150との対話により、モデル120および/または140をトレーニングすることができる。トレーニング用コンピューティングシステム150は、サーバコンピューティングシステム130とは別個であってよく、またはサーバコンピューティングシステム130の一部分であってよい。
User computing device 102 and/or
トレーニング用コンピューティングシステム150は、1つまたは複数のプロセッサ152と、メモリ154とを含む。1つまたは複数のプロセッサ152は、任意の適切な処理デバイス(たとえば、プロセッサコア、マイクロプロセッサ、ASIC、FPGA、コントローラ、マイクロコントローラなど)であってもよく、1つのプロセッサまたは動作可能に接続されている複数のプロセッサであってよい。メモリ154は、RAM、ROM、EEPROM、EPROM、フラッシュメモリデバイス、磁気ディスクなど、およびそれらの組合せのような、1つまたは複数の非一時的コンピュータ可読記憶媒体を含み得る。メモリ154は、データ156と、トレーニング用コンピューティングシステム150に動作を実施させるようにプロセッサ152によって実行される命令158とを記憶することができる。いくつかの実装形態では、トレーニング用コンピューティングシステム150は、1つまたは複数のサーバコンピューティングデバイスを含むか、またはそうでなければ、サーバコンピューティングデバイスによって実装される。
トレーニング用コンピューティングシステム150は、ユーザコンピューティングデバイス102および/またはサーバコンピューティングシステム130において記憶された機械学習済みモデル120および/または140を、たとえば、誤差逆伝播など、様々なトレーニングまたは学習技法を使ってトレーニングするモデル訓練器160を含み得る。たとえば、損失関数は、(たとえば、損失関数の勾配に基づいて)モデルの1つまたは複数のパラメータを更新するために、モデルを通して逆伝搬され得る。平均2乗誤差、尤度損失、交差エントロピー損失、ヒンジ損失、および/または様々な他の損失関数など、様々な損失関数が使用され得る。勾配降下技法は、いくつかのトレーニング反復に対してパラメータを反復的に更新するために使用され得る。
The
いくつかの実装形態では、誤差逆伝播を実行することは、打ち切り型通時的逆伝播を実行することを含み得る。モデル訓練器160は、トレーニングされるモデルの汎化能力を向上するために、いくつかの汎化技法(たとえば、重み減衰、ドロップアウトなど)を実施することができる。
In some implementations, performing backpropagation may include performing truncated backpropagation.
特に、モデル訓練器160は、トレーニングデータのセット162に基づいて、機械学習済みモデル120および/または140をトレーニングすることができる。トレーニングデータ162は、たとえば、全身、手、および/もしくは顔のスキャンならびに/またはそのようなスキャンのグランドトゥルース登録を含むことができる。
In particular,
いくつかの実装形態では、ユーザが承諾を与えた場合、トレーニング例は、ユーザコンピューティングデバイス102によって提供され得る。したがって、そのような実装形態では、ユーザコンピューティングデバイス102に提供されるモデル120は、ユーザコンピューティングデバイス102から受信されるユーザ固有データに対してトレーニング用コンピューティングシステム150によってトレーニングされ得る。場合によっては、このプロセスは、モデルの個人化と呼ばれることがある。
In some implementations, training examples may be provided by the user computing device 102 if the user has given consent. Accordingly, in such implementations,
モデル訓練器160は、所望の機能性を提供するために使用されるコンピュータ論理を含む。モデル訓練器160は、汎用プロセッサを制御するハードウェア、ファームウェア、および/またはソフトウェアに実装することができる。たとえば、いくつかの実装形態では、モデル訓練器160は、記憶デバイス上に記憶され、メモリにロードされ、1つまたは複数のプロセッサによって実行されるプログラムファイルを含む。他の実装形態では、モデル訓練器160は、RAMハードディスクまたは光学もしくは磁気媒体などの有形コンピュータ可読記憶媒体に記憶されるコンピュータ実行可能命令の1つまたは複数のセットを含む。
ネットワーク180は、ローカルエリアネットワーク(たとえば、イントラネット)、ワイドエリアネットワーク(たとえば、インターネット)、またはそれらの何らかの組合せなどの、任意のタイプの通信ネットワークであってもよく、任意の数のワイヤードまたはワイヤレスリンクを含むことができる。一般に、ネットワーク180を介した通信は、多種多様な通信プロトコル(たとえば、TCP/IP、HTTP、SMTP、FTP)、符号化もしくはフォーマット(たとえば、HTML、XML)、および/または保護方式(たとえば、VPN、セキュアHTTP、SSL)を使用して、任意のタイプのワイヤードおよび/またはワイヤレス接続を介して搬送され得る。
図11Aは、本開示を実装するために使用され得る1つの例示的コンピューティングシステムを示す。他のコンピューティングシステムが同様に使用されてもよい。たとえば、いくつかの実装形態では、ユーザコンピューティングデバイス102は、モデル訓練器160およびトレーニングデータセット162を含み得る。そのような実装形態では、モデル120は、ユーザコンピューティングデバイス102においてローカルにトレーニングされることと使われることの両方が可能である。そのような実装形態のいくつかでは、ユーザコンピューティングデバイス102は、ユーザ固有データに基づいて、モデル訓練器160を実装して、モデル120を個人化し得る。
FIG. 11A shows one exemplary computing system that may be used to implement this disclosure. Other computing systems may be used as well. For example, in some implementations, user computing device 102 may include
図11Bは、本開示の例示的な実施形態に従って実行する例示的なコンピューティングデバイス10のブロック図を示す。コンピューティングデバイス10は、ユーザコンピューティングデバイスまたはサーバコンピューティングデバイスであってよい。
FIG. 11B shows a block diagram of an
コンピューティングデバイス10は、いくつかのアプリケーション(たとえば、アプリケーション1～N)を含む。各アプリケーションは、それ自体の機械学習ライブラリおよび機械学習済みモデルを含む。たとえば、各アプリケーションは、機械学習済みモデルを含み得る。例示的アプリケーションは、テキストメッセージングアプリケーション、eメールアプリケーション、ディクテーションアプリケーション、仮想キーボードアプリケーション、ブラウザアプリケーションなどを含む。
図11Bに示すように、各アプリケーションは、コンピューティングデバイスのいくつかの他の構成要素、たとえば、1つもしくは複数のセンサー、コンテキストマネージャ、デバイス状態構成要素、および/または追加構成要素などと通信することができる。いくつかの実装形態では、各アプリケーションは、API(たとえば、パブリックAPI)を使って、各デバイス構成要素と通信することができる。いくつかの実装形態では、各アプリケーションによって使用されるAPIは、そのアプリケーションに固有である。 As shown in FIG. 11B, each application communicates with several other components of the computing device, such as one or more sensors, a context manager, a device state component, and/or additional components. be able to. In some implementations, each application can use APIs (eg, public APIs) to communicate with each device component. In some implementations, the API used by each application is specific to that application.
図11Cは、本開示の例示的な実施形態に従って実行する例示的なコンピューティングデバイス50のブロック図を示す。コンピューティングデバイス50は、ユーザコンピューティングデバイスまたはサーバコンピューティングデバイスであってよい。
FIG. 11C shows a block diagram of an
コンピューティングデバイス50は、いくつかのアプリケーション(たとえば、アプリケーション1～N)を含む。各アプリケーションは、中央インテリジェンスレイヤと通信する。例示的なアプリケーションは、テキストメッセージングアプリケーション、eメールアプリケーション、ディクテーションアプリケーション、仮想キーボードアプリケーション、ブラウザアプリケーションなどを含む。いくつかの実装形態では、各アプリケーションは、API(たとえば、すべてのアプリケーションにわたる共通API)を使って、中央インテリジェンスレイヤ(およびその中に記憶されるモデル)と通信することができる。
中央インテリジェンスレイヤは、いくつかの機械学習済みモデルを含む。たとえば、図11Cに示すように、それぞれの機械学習済みモデル(たとえば、モデル)が、各アプリケーションに与えられ、中央インテリジェンスレイヤによって管理され得る。他の実装形態では、2つ以上のアプリケーションが、単一の機械学習済みモデルを共有することができる。たとえば、いくつかの実装形態では、中央インテリジェンスレイヤは、アプリケーションすべてに単一モデル(たとえば、単一モデル)を提供することができる。いくつかの実装形態では、中央インテリジェンスレイヤは、コンピューティングデバイス50のオペレーティングシステム内に含まれるか、またはそうでなければ、オペレーティングシステムによって実装される。
The central intelligence layer contains several machine-learned models. For example, as shown in FIG. 11C, a respective machine-learned model (eg, model) may be provided for each application and managed by a central intelligence layer. In other implementations, two or more applications can share a single machine-learned model. For example, in some implementations, the central intelligence layer can provide a single model (eg, single model) for all applications. In some implementations, the central intelligence layer is included within or otherwise implemented by the operating system of
中央インテリジェンスレイヤは、中央デバイスデータレイヤと通信することができる。中央デバイスデータレイヤは、コンピューティングデバイス50向けのデータの集中型リポジトリであってよい。図11Cに示すように、中央デバイスデータレイヤは、コンピューティングデバイスのいくつかの他の構成要素、たとえば、1つまたは複数のセンサー、コンテキストマネージャ、デバイス状態構成要素、および/または追加構成要素などと通信することができる。いくつかの実装形態では、中央デバイスデータレイヤは、API(たとえば、プライベートAPI)を使用して、各デバイス構成要素と通信することができる。
A central intelligence layer may communicate with a central device data layer. A central device data layer may be a centralized repository of data for
6.追加の開示
本明細書で説明した技術は、サーバ、データベース、ソフトウェアアプリケーション、および他のコンピュータベースのシステム、ならびに行われるアクションおよびそのようなシステムとの間で送られる情報を参照する。コンピュータベースのシステムの固有の柔軟性は、構成要素の間でのタスクおよび機能の多種多様な可能な構成、組合せ、および分割を可能にする。たとえば、本明細書で説明されるプロセスは、単一のデバイスもしくは構成要素または組合せて働く複数のデバイスもしくは構成要素を使用して実装され得る。データベースおよびアプリケーションは、単一のシステム上で実装されるか、または複数のシステムに分散されてよい。分散構成要素は、順次、または並行して動作することができる。
6. Additional Disclosure The technology described herein refers to servers, databases, software applications, and other computer-based systems, as well as actions taken and information sent to and from such systems. The inherent flexibility of computer-based systems allows for a wide variety of possible configurations, combinations, and divisions of tasks and functions between components. For example, the processes described herein can be implemented using a single device or component or multiple devices or components working in combination. The database and application may be implemented on a single system or distributed across multiple systems. Distributed components can operate sequentially or in parallel.
本主題を、その様々な特定の例示的な実施形態に関して詳細に説明したが、各例は、本開示の限定ではなく、説明として与えられる。当業者は、上記を理解すると、そのような実施形態の代替、変形、および等価物を容易に作り出すことができる。したがって、本開示は、当業者には容易に明らかになるように、本主題へのそのような修正、変形および/または追加を含めることを排除しない。たとえば、1つの実施形態の一部として示され、または説明される特徴は、またさらなる実施形態をもたらすために、別の実施形態とともに使用され得る。したがって、本開示がそのような代替、変形、および等価物を包含することが意図されている。 Although the present subject matter has been described in detail in terms of various specific exemplary embodiments thereof, each example is provided by way of explanation, not limitation of the disclosure. Those of ordinary skill in the art, once they understand the above, can easily devise alternatives, variations, and equivalents of such embodiments. Accordingly, this disclosure does not exclude inclusion of such modifications, variations and/or additions to the present subject matter, as would be readily apparent to those skilled in the art. For example, features illustrated or described as part of one embodiment can be used with another embodiment to yield a still further embodiment. Accordingly, it is intended that this disclosure covers such alternatives, modifications and equivalents.
50 コンピューティングデバイス
100 コンピューティングシステム
102 ユーザコンピューティングデバイス
112 プロセッサ
114 メモリ
116 データ
118 命令
120 機械学習済みモデル
122 ユーザ入力構成要素
130 サーバコンピューティングシステム
132 プロセッサ
134 メモリ
136 データｗ
138 命令
140 機械学習済みモデル
150 トレーニング用コンピューティングシステム
152 プロセッサ
154 メモリ
156 データ
158 命令
160 モデル訓練器
162 トレーニングデータ
180 ネットワーク
50 computing devices
100 Computing Systems
102 User Computing Devices
112 processors
114 memory
116 data
118 instructions
120 machine-learned models
122 User Input Component
130 server computing system
132 processors
134 memory
136 data
138 instructions
140 machine-learned models
150 Training Computing Systems
152 processors
154 memories
156 data
158 instructions
160 Model Trainer
162 training data
180 network
Claims (20)
1つまたは複数のコンピューティングデバイスを含むコンピューティングシステムによって、トレーニングボディの1つまたは複数のグランドトゥルース登録の形状スキャンを取得するステップであり、前記トレーニングボディの前記1つまたは複数のグランドトゥルース登録の形状スキャンが、任意のポーズを有する少なくともグランドトゥルース登録の全身スキャンと、グランドトゥルース登録の顔詳細スキャンとを含む、取得するステップと、
前記コンピューティングシステムによって、形状エンコーダモデルを使用して、前記トレーニングボディに関連する静止形状埋込みを取得するために、静止しているポーズとともに推定登録全身スキャンを符号化するステップと、
前記トレーニングボディについての識別ベースの静止形状データを取得するために、前記コンピューティングシステムによって、形状デコーダモデルを使用して、前記静止形状埋込みを復号するステップと、
前記トレーニングボディに関連する顔表情埋込みを取得するために、前記コンピューティングシステムによって、顔のエンコーダモデルを使用して、前記グランドトゥルース登録の顔詳細スキャンから導出されたデータを符号化するステップと、
前記トレーニングボディについての顔表情データを取得するために、前記コンピューティングシステムによって、顔デコーダモデルを使用して、前記顔表情埋込みを復号するステップと、
前記コンピューティングシステムによって、前記識別ベースの静止形状データ、前記顔表情データ、および前記任意のポーズに対応するポーズパラメータのセットに少なくとも部分的に基づいて、前記トレーニングボディのトレーニングポーズメッシュを生成するステップと、
前記トレーニングボディについて生成された前記トレーニングポーズメッシュを、前記任意のポーズを有する前記グランドトゥルース登録の全身スキャンおよび前記グランドトゥルース登録の顔詳細スキャンと比較する再構成損失関数を評価するステップと、
前記形状エンコーダモデルと、前記形状デコーダモデルと、前記顔エンコーダモデルと、前記顔デコーダモデルとを、前記再構成損失に少なくとも部分的に基づいて、一緒にトレーニングするステップと、
少なくとも前記形状デコーダモデルと、前記顔デコーダモデルとを含む前記機械学習済み3次元人間形状モデルを提供するステップと
を含む、コンピュータ実装方法。 A computer-implemented method of training a machine-learned three-dimensional human shape model together in an end-to-end pipeline, comprising, for one or more training iterations,
obtaining shape scans of one or more ground truth registrations of a training body, by a computing system comprising one or more computing devices, wherein said one or more ground truth registrations of said training body; obtaining, wherein the shape scan comprises at least a ground truth registration whole body scan with arbitrary pose and a ground truth registration face detail scan;
encoding, by the computing system, an estimated registered whole body scan with a stationary pose to obtain a stationary shape embedding associated with the training body using a shape encoder model;
decoding, by the computing system, the static shape embeddings using a shape decoder model to obtain identity-based static shape data for the training bodies;
encoding, by the computing system, data derived from facial detail scans of the ground truth registration using a facial encoder model to obtain facial expression embeddings associated with the training body;
decoding, by the computing system, the facial expression embedding using a facial decoder model to obtain facial expression data for the training body;
generating, by the computing system, a training pose mesh for the training body based at least in part on the identity-based static shape data, the facial expression data, and a set of pose parameters corresponding to the arbitrary pose; and,
evaluating a reconstruction loss function that compares the training pose mesh generated for the training body with a full-body scan of the ground truth registration with the arbitrary pose and a facial detail scan of the ground truth registration;
jointly training the shape encoder model, the shape decoder model, the face encoder model, and the face decoder model based at least in part on the reconstruction loss;
providing the machine-learned 3D human shape model including at least the shape decoder model and the face decoder model.
前記ポーズ空間変形モデルが、前記再構成損失に少なくとも部分的に基づいて、前記形状エンコーダモデル、前記形状デコーダモデル、前記顔エンコーダモデル、および前記顔デコーダモデルとともに一緒にトレーニングされる、
請求項1に記載のコンピュータ実装方法。 The step of generating, by the computing system, the training pose mesh for the training body comprises: generating poses for generating pose-based shape adjustments for the training body, using a pose space deformation model, by the computing system; processing said set of parameters;
the pose space deformation model is jointly trained with the shape encoder model, the shape decoder model, the face encoder model, and the face decoder model based at least in part on the reconstruction loss;
The computer-implemented method of Claim 1.
前記トレーニングボディの骨格表現の複数の関節について複数の予測関節中心を生成するために、前記コンピューティングシステムによって、関節中心予測モデルを使用して、前記識別ベースの静止形状データを処理するステップと、
前記トレーニングボディの前記トレーニングポーズメッシュを生成するために、前記コンピューティングシステムによって、ブレンドスキニングモデルを使用して、前記顔表情データ、前記ポーズによる形状調整、前記識別ベースの静止形状データ、および前記1つまたは複数の予測関節中心を処理するステップと
を含み、
前記関節中心予測モデルおよび前記ブレンドスキニングモデルが、前記再構成損失に少なくとも部分的に基づいて、前記形状エンコーダモデル、前記形状デコーダモデル、前記顔エンコーダモデル、前記顔デコーダモデル、および前記ポーズ空間変形モデルとともに一緒にトレーニングされる、
請求項2に記載のコンピュータ実装方法。 generating, by the computing system, the training pose mesh for the training body;
processing the identity-based static shape data using a joint center prediction model by the computing system to generate a plurality of predicted joint centers for a plurality of joints of the skeletal representation of the training body;
The facial expression data, the pose-based shape adjustment, the identity-based static shape data, and the one, using a blended skinning model, by the computing system to generate the training pose mesh of the training body. and processing one or more predicted joint centers;
The joint-center prediction model and the blended skinning model are adapted, based at least in part on the reconstruction loss, to the shape encoder model, the shape decoder model, the face encoder model, the face decoder model, and the pose space deformation model. trained together with
3. The computer-implemented method of Claim 2.
1つまたは複数のプロセッサと、
機械学習済み3次元人間形状モデルをまとめて記憶する1つまたは複数の非一時的コンピュータ可読媒体であって、前記機械学習済み3次元人間形状モデルが、
人体についての顔表情データを生成するために、前記人体に関連する顔表情埋込みを処理するようにトレーニングされた機械学習済み顔表情デコーダモデルと、
前記人体についてのポーズによる形状調整を生成するために、ポーズパラメータのセットを処理するようにトレーニングされた機械学習済みポーズ空間変形モデルと、
前記人体についての識別ベースの静止形状データを生成するために、前記人体に関連する静止形状埋込みを処理するようにトレーニングされた機械学習済み形状デコーダモデルと
を含む、1つまたは複数の非一時的コンピュータ可読媒体と
を備え、
前記機械学習済み3次元人間形状モデルが、前記顔表情データ、前記ポーズによる形状調整、および前記識別ベースの静止形状データに少なくとも部分的に基づいて、前記人体のポーズメッシュを生成するようにトレーニングされており、
前記機械学習済み顔表情デコーダモデル、前記機械学習済みポーズ空間変形モデル、および前記機械学習済み形状デコーダモデルのすべてが、前記トレーニングボディについて前記機械学習済み3次元人間形状モデルによって生成されたトレーニングポーズメッシュを、前記トレーニングボディの1つまたは複数のグランドトゥルース登録の形状スキャンと比較する再構成損失関数に少なくとも部分的に基づいて、エンドツーエンドで一緒にトレーニングされている、コンピューティングシステム。 A computing system comprising a machine-learned three-dimensional human shape model having at least face and body shape components trained together in an end-to-end pipeline, comprising:
one or more processors;
One or more non-transitory computer-readable media collectively storing a machine-learned three-dimensional human shape model, said machine-learned three-dimensional human shape model comprising:
a machine-learned facial expression decoder model trained to process facial expression embeddings associated with said human body to generate facial expression data about said human body;
a machine-learned pose-space deformation model trained to process a set of pose parameters to generate pose-based shape adjustments for the human body;
a machine-learned shape decoder model trained to process static shape embeddings associated with said human body to generate discrimination-based static shape data for said human body. a computer readable medium;
The machine-learned 3D human shape model is trained to generate a pose mesh of the human body based at least in part on the facial expression data, the pose-based shape adjustment, and the discrimination-based static shape data. and
The machine-learned facial decoder model, the machine-learned pose space deformation model, and the machine-learned shape decoder model are all training pose meshes generated by the machine-learned 3D human shape model for the training body. is jointly trained end-to-end based at least in part on a reconstruction loss function that compares a shape scan of one or more ground truth registrations of said training body.
前記人体の骨格表現の複数の関節について複数の予測関節中心を生成するために、前記識別ベースの静止形状データを処理するようにトレーニングされた機械学習済み関節中心予測モデルと、
前記人体の前記ポーズメッシュを生成するために、前記顔表情データ、前記ポーズによる形状調整、前記識別ベースの静止形状データ、および前記1つまたは複数の予測関節中心を処理するようにトレーニングされた機械学習済みブレンドスキニングモデルと
をさらに含み、
前記機械学習済み関節中心予測モデルおよび前記機械学習済みブレンドスキニングモデルが、前記再構成損失に少なくとも部分的に基づいて、前記機械学習済み顔表情デコーダモデル、前記機械学習済みポーズ空間変形モデル、および前記機械学習済み形状デコーダモデルとともに、エンドツーエンドで一緒にトレーニングされている、請求項8に記載のコンピューティングシステム。 The machine-learned three-dimensional human shape model is
a machine-learned joint center prediction model trained to process the discrimination-based static shape data to generate a plurality of predicted joint centers for a plurality of joints of the skeletal representation of the human body;
A machine trained to process the facial expression data, the pose-based shape adjustment, the identity-based static shape data, and the one or more predicted joint centers to generate the pose mesh of the human body. and a trained blend skinning model,
The machine-learned joint-centered prediction model and the machine-learned blended skinning model are configured, based at least in part on the reconstruction loss, to perform the machine-learned facial expression decoder model, the machine-learned pose space deformation model, and the 9. The computing system of claim 8, co-trained end-to-end with a machine-learned shape decoder model.
新しい人体に関連する新しい顔表情埋込み、ポーズパラメータの新しいセット、および新しい静止形状埋込みを取得することと、
前記新しい顔表情埋込み、ポーズパラメータの前記新しいセット、および前記新しい静止形状埋込みに少なくとも部分的に基づいて、前記新しい人体の新しいポーズメッシュを生成することと
を含む、請求項8から17のいずれか一項に記載のコンピューティングシステム。 The one or more non-transitory computer-readable media further store instructions that, when executed by the one or more processors, cause the computing system to perform operations, the operations comprising:
obtaining a new facial expression embedding associated with the new human body, a new set of pose parameters, and a new static shape embedding;
generating a new pose mesh for the new human body based at least in part on the new facial expression embedding, the new set of pose parameters, and the new static shape embedding. A computing system according to any one of the preceding paragraphs.
前記新しい人体を示す単眼画像から、前記新しい顔表情埋込み、ポーズパラメータの前記新しいセット、および前記新しい静止形状埋込みを生成すること
を含む、請求項18に記載のコンピューティングシステム。 obtaining the new facial expression embedding, the new set of pose parameters, and the new static shape embedding;
19. The computing system of claim 18, comprising generating the new facial expression embeddings, the new set of pose parameters, and the new static shape embeddings from a monocular image showing the new human body.
人体についての顔表情データを生成するために、前記人体に関連する顔表情埋込みを処理するようにトレーニングされた機械学習済み顔表情デコーダモデルと、
前記人体についての識別ベースの形状データを生成するために、前記人体に関連する形状埋込みを処理するようにトレーニングされた機械学習済み形状デコーダモデルと
を含み、
前記機械学習済み3次元人間形状モデルが、前記顔表情データ、ポーズパラメータのセット、前記識別ベースの形状データに少なくとも部分的に基づいて、前記人体のポーズメッシュを生成するようにトレーニングされ、
前記機械学習済み顔表情デコーダモデルおよび前記機械学習済み形状デコーダモデルが、トレーニングボディについての前記機械学習済み3次元人間形状モデルによって生成されたトレーニングポーズメッシュを、前記トレーニングボディの1つまたは複数のグランドトゥルース登録の形状スキャンと比較する再構成損失関数に少なくとも部分的に基づいて、エンドツーエンドで一緒にトレーニングされている、1つまたは複数の非一時的コンピュータ可読媒体。 One or more non-transitory computer-readable media collectively storing a machine-learned three-dimensional human shape model, said machine-learned three-dimensional human shape model comprising:
a machine-learned facial expression decoder model trained to process facial expression embeddings associated with said human body to generate facial expression data about said human body;
a machine-learned shape decoder model trained to process shape embeddings associated with said human body to generate identity-based shape data for said human body;
training the machine-learned 3D human shape model to generate a pose mesh of the human body based at least in part on the facial expression data, the set of pose parameters, and the identity-based shape data;
The machine-learned facial decoder model and the machine-learned shape decoder model convert a training pose mesh generated by the machine-learned 3D human shape model for a training body to one or more grounds of the training body. One or more non-transitory computer-readable media that have been jointly trained end-to-end based, at least in part, on a reconstruction loss function that compares the shape scans of the truth registration.
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2020/030712 WO2021221657A1 (en) | 2020-04-30 | 2020-04-30 | Generative nonlinear human shape models |
Publications (2)
Publication Number | Publication Date |
---|---|
JP2023524252A true JP2023524252A (en) | 2023-06-09 |
JP7378642B2 JP7378642B2 (en) | 2023-11-13 |
Family
ID=70802929
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2022566221A Active JP7378642B2 (en) | 2020-04-30 | 2020-04-30 | Generative nonlinear human shape model |
Country Status (6)
Country | Link |
---|---|
US (1) | US20230169727A1 (en) |
EP (1) | EP4128168A1 (en) |
JP (1) | JP7378642B2 (en) |
KR (1) | KR20230004837A (en) |
CN (1) | CN115461785A (en) |
WO (1) | WO2021221657A1 (en) |
Families Citing this family (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN111260774B (en) * | 2020-01-20 | 2023-06-23 | 北京百度网讯科技有限公司 | Method and device for generating 3D joint point regression model |
CN114333069B (en) * | 2022-03-03 | 2022-05-17 | 腾讯科技（深圳）有限公司 | Object posture processing method, device, equipment and storage medium |
CN116434347B (en) * | 2023-06-12 | 2023-10-13 | 中山大学 | Skeleton sequence identification method and system based on mask pattern self-encoder |
CN116452755B (en) * | 2023-06-15 | 2023-09-22 | 成就医学科技(天津)有限公司 | Skeleton model construction method, system, medium and equipment |
CN116740820B (en) * | 2023-08-16 | 2023-10-31 | 南京理工大学 | Single-view point cloud three-dimensional human body posture and shape estimation method based on automatic augmentation |
Family Cites Families (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2018139203A1 (en) | 2017-01-26 | 2018-08-02 | ソニー株式会社 | Information processing device, information processing method, and program |
JP6244593B1 (en) | 2017-01-30 | 2017-12-13 | 株式会社コロプラ | Information processing method, apparatus, and program for causing computer to execute information processing method |
US20200401794A1 (en) | 2018-02-16 | 2020-12-24 | Nippon Telegraph And Telephone Corporation | Nonverbal information generation apparatus, nonverbal information generation model learning apparatus, methods, and programs |
-
2020
- 2020-04-30 WO PCT/US2020/030712 patent/WO2021221657A1/en unknown
- 2020-04-30 KR KR1020227041960A patent/KR20230004837A/en not_active Application Discontinuation
- 2020-04-30 EP EP20727760.9A patent/EP4128168A1/en active Pending
- 2020-04-30 CN CN202080100318.0A patent/CN115461785A/en active Pending
- 2020-04-30 US US17/922,160 patent/US20230169727A1/en active Pending
- 2020-04-30 JP JP2022566221A patent/JP7378642B2/en active Active
Also Published As
Publication number | Publication date |
---|---|
WO2021221657A1 (en) | 2021-11-04 |
JP7378642B2 (en) | 2023-11-13 |
KR20230004837A (en) | 2023-01-06 |
US20230169727A1 (en) | 2023-06-01 |
CN115461785A (en) | 2022-12-09 |
EP4128168A1 (en) | 2023-02-08 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
Saito et al. | SCANimate: Weakly supervised learning of skinned clothed avatar networks | |
JP7378642B2 (en) | Generative nonlinear human shape model | |
Pishchulin et al. | Building statistical shape spaces for 3d human modeling | |
CN109584353B (en) | Method for reconstructing three-dimensional facial expression model based on monocular video | |
Hasler et al. | A statistical model of human pose and body shape | |
Gall et al. | Optimization and filtering for human motion capture: A multi-layer framework | |
Kähler et al. | Geometry-based muscle modeling for facial animation | |
JP2023526566A (en) | fast and deep facial deformation | |
CN110310285B (en) | Accurate burn area calculation method based on three-dimensional human body reconstruction | |
Lei et al. | Cadex: Learning canonical deformation coordinate space for dynamic surface representation via neural homeomorphism | |
CN109829972B (en) | Three-dimensional human standard skeleton extraction method for continuous frame point cloud | |
CN112233222A (en) | Human body parametric three-dimensional model deformation method based on neural network joint point estimation | |
Taylor et al. | VR props: an end-to-end pipeline for transporting real objects into virtual and augmented environments | |
Li et al. | Remodeling of mannequins based on automatic binding of mesh to anthropometric parameters | |
Madadi et al. | Deep unsupervised 3D human body reconstruction from a sparse set of landmarks | |
Garcia-D’Urso et al. | Accurate estimation of parametric models of the human body from 3D point clouds | |
US20210110001A1 (en) | Machine learning for animatronic development and optimization | |
Saint et al. | 3dbooster: 3d body shape and texture recovery | |
Duveau et al. | Cage-based motion recovery using manifold learning | |
CA3177593A1 (en) | Transformer-based shape models | |
Xu et al. | Human body reshaping and its application using multiple RGB-D sensors | |
Li et al. | MultiResGNet: Approximating Nonlinear Deformation via Multi‐Resolution Graphs | |
Huang et al. | Detail-preserving controllable deformation from sparse examples | |
Capell et al. | Physically based rigging for deformable characters | |
Diao et al. | Combating Spurious Correlations in Loose‐fitting Garment Animation Through Joint‐Specific Feature Learning |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20221226 |
|
A621 | Written request for application examination |
Free format text: JAPANESE INTERMEDIATE CODE: A621Effective date: 20221226 |
|
TRDD | Decision of grant or rejection written | ||
A01 | Written decision to grant a patent or to grant a registration (utility model) |
Free format text: JAPANESE INTERMEDIATE CODE: A01Effective date: 20231002 |
|
A61 | First payment of annual fees (during grant procedure) |
Free format text: JAPANESE INTERMEDIATE CODE: A61Effective date: 20231031 |
|
R150 | Certificate of patent or registration of utility model |
Ref document number: 7378642Country of ref document: JPFree format text: JAPANESE INTERMEDIATE CODE: R150 |