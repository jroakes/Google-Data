CN114503439A - Compressing data exhibiting mixed compressibility - Google Patents
Compressing data exhibiting mixed compressibility Download PDFInfo
- Publication number
- CN114503439A CN114503439A CN201980101171.4A CN201980101171A CN114503439A CN 114503439 A CN114503439 A CN 114503439A CN 201980101171 A CN201980101171 A CN 201980101171A CN 114503439 A CN114503439 A CN 114503439A
- Authority
- CN
- China
- Prior art keywords
- data
- computing device
- bits
- transform
- computing system
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- H—ELECTRICITY
- H03—ELECTRONIC CIRCUITRY
- H03M—CODING; DECODING; CODE CONVERSION IN GENERAL
- H03M7/00—Conversion of a code where information is represented by a given sequence or number of digits to a code where the same, similar or subset of information is represented by a different sequence or number of digits
- H03M7/14—Conversion to or from non-weighted codes
- H03M7/24—Conversion to or from floating-point codes
-
- H—ELECTRICITY
- H03—ELECTRONIC CIRCUITRY
- H03M—CODING; DECODING; CODE CONVERSION IN GENERAL
- H03M7/00—Conversion of a code where information is represented by a given sequence or number of digits to a code where the same, similar or subset of information is represented by a different sequence or number of digits
- H03M7/30—Compression; Expansion; Suppression of unnecessary data, e.g. redundancy reduction
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F7/00—Methods or arrangements for processing data by operating upon the order or content of the data handled
- G06F7/38—Methods or arrangements for performing computations using exclusively denominational number representation, e.g. using binary, ternary, decimal representation
- G06F7/48—Methods or arrangements for performing computations using exclusively denominational number representation, e.g. using binary, ternary, decimal representation using non-contact-making devices, e.g. tube, solid state device; using unspecified devices
- G06F7/483—Computations with numbers represented by a non-linear combination of denominational numbers, e.g. rational numbers, logarithmic number system or floating-point numbers
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F7/00—Methods or arrangements for processing data by operating upon the order or content of the data handled
- G06F7/74—Selecting or encoding within a word the position of one or more bits having a specified value, e.g. most or least significant one or zero detection, priority encoders
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/06—Physical realisation, i.e. hardware implementation of neural networks, neurons or parts of neurons
- G06N3/063—Physical realisation, i.e. hardware implementation of neural networks, neurons or parts of neurons using electronic means
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
-
- H—ELECTRICITY
- H03—ELECTRONIC CIRCUITRY
- H03M—CODING; DECODING; CODE CONVERSION IN GENERAL
- H03M7/00—Conversion of a code where information is represented by a given sequence or number of digits to a code where the same, similar or subset of information is represented by a different sequence or number of digits
- H03M7/30—Compression; Expansion; Suppression of unnecessary data, e.g. redundancy reduction
- H03M7/3059—Digital compression and data reduction techniques where the original information is represented by a subset or similar information, e.g. lossy compression
-
- H—ELECTRICITY
- H03—ELECTRONIC CIRCUITRY
- H03M—CODING; DECODING; CODE CONVERSION IN GENERAL
- H03M7/00—Conversion of a code where information is represented by a given sequence or number of digits to a code where the same, similar or subset of information is represented by a different sequence or number of digits
- H03M7/30—Compression; Expansion; Suppression of unnecessary data, e.g. redundancy reduction
- H03M7/60—General implementation details not specific to a particular type of compression
- H03M7/6064—Selection of Compressor
- H03M7/607—Selection between different types of compressors
-
- H—ELECTRICITY
- H03—ELECTRONIC CIRCUITRY
- H03M—CODING; DECODING; CODE CONVERSION IN GENERAL
- H03M7/00—Conversion of a code where information is represented by a given sequence or number of digits to a code where the same, similar or subset of information is represented by a different sequence or number of digits
- H03M7/30—Compression; Expansion; Suppression of unnecessary data, e.g. redundancy reduction
- H03M7/70—Type of the data to be coded, other than image and sound
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/10—Interfaces, programming languages or software development kits, e.g. for simulating neural networks
Abstract
Systems and methods for compressing data exhibiting mixed compressibility, such as floating point data, are provided. As one example, aspects of the present disclosure may be used to compress floating point data representing parameter values of a machine learning model. Accordingly, aspects of the present disclosure may be used to compress machine learning models (e.g., to reduce storage requirements associated with the models, reduce bandwidth consumed to transmit the models, etc.).
Description
Technical Field
The present disclosure relates generally to systems and methods for compressing data exhibiting hybrid compressibility. In particular, the present disclosure relates to compressing floating point data, such as, for example, floating point data encoding machine learning model parameter values.
Background
Information stored in a computing system may be stored in memory as data. For example, information may be stored as "bits" of data, where each bit is equal to 1 or 0.
It may be desirable to reduce the amount of data and/or memory used to store information (e.g., reduce the number of bits required to store data). Techniques for reducing memory usage for storing information are commonly referred to as compression techniques.
Lossless compression refers to a compression technique that allows accurate reproduction of information to be obtained after compression. Lossy compression refers to a compression technique that allows only an estimation of information to be recreated after compression.
In some cases, the data may exhibit mixed compressibility. For example, a first portion of data may be highly suitable for one or more compression techniques (i.e., may have a high "compressibility"), while a second portion may not be suitable for a compression technique (i.e., may have a low "compressibility").
For example, if application of a compression technique results in a significant reduction in storage usage and/or only zero or small information loss, the data may have high compressibility whereas if application of a compression technique results in only a small reduction in storage usage and/or a large information loss, the data may have low compressibility.
Disclosure of Invention
Aspects and advantages of embodiments of the present disclosure will be set forth in part in the description which follows or may be learned by practice of the embodiments.
One example aspect of the present disclosure relates to a computing system configured to compress data. The computing system includes one or more processors and one or more non-transitory computer-readable media storing instructions that, when executed by the one or more processors, cause the computing system to perform operations. The operation includes: subject data is obtained. The operation includes: a first compression technique is performed to compress a first portion of data and obtain first compressed data, wherein a second portion of data remains uncompressed after performing the first compression technique. The operation includes: a second portion of the data is transformed to obtain transformed data. The operation includes: the transform data is quantized to obtain quantized data. The operation includes: the first compressed data and the quantized data are stored.
Another example aspect of the present disclosure relates to a computer-implemented method of transmitting compressed data. The method comprises the following steps: subject data is received at a first computing device. The method comprises the following steps: a first portion and a second portion of the subject data are determined by the first computing device. The method comprises the following steps: the second portion is transformed by the first computing device to obtain transformed data. The method comprises the following steps: the transform data is quantized by a first computing device to obtain quantized data. The method comprises the following steps: the quantized data from the first computing device is transmitted to a second computing device.
Another example aspect of the present disclosure is directed to one or more non-transitory computer-readable media storing instructions that, when executed by one or more computing devices, cause the one or more computing devices to compress a machine learning model by performing operations. The operation includes: model parameter data is obtained that respectively includes a plurality of floating point numbers for a plurality of parameters of a machine learning model, wherein each of the plurality of floating point numbers includes one or more sign bits, a plurality of exponent bits, and a plurality of mantissa bits. The operation includes: for each of a plurality of floating point numbers: generating first compressed data from one or more most significant mantissa bits of the one or more sign bits, the plurality of exponent bits, and the plurality of mantissa bits to obtain first compressed data; performing a Kashin decomposition on one or more remaining least significant mantissa bits to obtain transformed data, the one or more remaining least significant mantissa bits comprising a plurality of mantissa bits not included in one or more most significant mantissa bits from which the first compressed data is generated; performing uniform quantization on the transform data to obtain quantized data; and storing the first compressed data and the quantized data.
Other aspects of the disclosure relate to various systems, apparatuses, non-transitory computer-readable media, user interfaces, and electronic devices.
These and other features, aspects, and advantages of various embodiments of the present disclosure will become better understood with reference to the following description and appended claims. The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate exemplary embodiments of the disclosure and together with the description, serve to explain the relevant principles.
Drawings
A detailed discussion of embodiments directed to one of ordinary skill in the art is set forth in the specification, which makes reference to the appended drawings, in which:
fig. 1A depicts a block diagram of an example computing system, according to an example embodiment of the present disclosure.
Fig. 1B depicts a block diagram of an example computing device, according to an example embodiment of the present disclosure.
Fig. 1C depicts a block diagram of an example computing device, according to an example embodiment of the present disclosure.
Fig. 2 depicts a block diagram of an example machine learning model, according to an example embodiment of the present disclosure.
Fig. 3 depicts an example block diagram of example data, according to an example embodiment of the present disclosure.
Fig. 4 depicts a flowchart of an example method of transforming data, according to an example embodiment of the present disclosure.
FIG. 5 depicts a flowchart of an example method of storing compressed data, according to an example embodiment of the disclosure.
Fig. 6 depicts a flowchart of an example method of transmitting compressed data according to an example embodiment of the disclosure.
Reference numerals repeated throughout the several figures are intended to identify identical features in the various embodiments.
Detailed Description
SUMMARY
In general, the present disclosure relates to systems and methods for compressing data (such as floating point data) exhibiting mixed compressibility. As one example, aspects of the present disclosure may be used to compress floating point data representing parameter values of a machine learning model. Accordingly, aspects of the present disclosure may be used to compress machine learning models (e.g., to reduce storage requirements associated with the models, reduce bandwidth consumed by transmitting the models, etc.). In particular, one aspect of the present disclosure relates to a compression technique in which a first portion of floating point data exhibiting high compressibility is compressed using a prediction technique, while a second remaining portion of floating point data exhibiting low compressibility is compressed by decomposition (e.g., applying Kashin decomposition) in combination with quantization (e.g., uniform quantization, bit truncation, etc.).
More specifically, the information may be stored as data in the computing system. The data may represent words, numbers, or other suitable types of information in a format that can be processed and/or otherwise interpreted by the computing system. For example, data may be represented as a collection of numbers (e.g., bits) in a storage device. Example memory devices include, but are not limited to, non-transitory computer-readable storage media such as RAM, ROM, EEPROM, EPROM, flash memory devices, disks, and the like, and combinations thereof.
It may be desirable to reduce the amount of memory used to store information. For example, it may be desirable to reduce the size of the data used to store information. Techniques for reducing the size of data are referred to as data compression techniques. Lossless compression refers to compression techniques that allow data to be compressed without loss of information and then decompressed. Lossy compression refers to compression techniques that introduce unrecoverable errors into the data after compression and/or decompression, resulting in a loss of information. For example, lossy compression techniques may slightly increase and/or decrease the value represented by the data. As another example, lossy compression techniques can result in a loss of precision associated with the data. As another example, lossy compression techniques may introduce blurring or other distortions into the image.
While it may be desirable to reduce or eliminate information loss, lossy compression may allow for a significant reduction in data size compared to lossless compression, so that some applications may choose to use lossy compression rather than lossless compression. For example, in many cases, errors introduced by lossy compression do not significantly affect the functionality of the data.
It may be desirable to use compression techniques to store and/or transmit data sets having larger sizes, such as data sets having a large number of data items and/or a large number of digits per data item. As one example, a tensor with a large number of model parameter values (such as millions of corresponding values of model parameters) may be used to represent a machine learning model. In other words, the parameters of the machine learning model may be represented as tensors with a large number (millions) of component values.
As another example, measurement data (such as time series measurement data) may have a large number of samples. The time series measurement data may include sensor data, such as environmental sensor data (e.g., seismic data, temperature data, etc.), biosensor data, and/or any other data collected over time. Additionally, in many cases, the model parameters and/or measurement data may be represented in the form of floating point data, which may require storage of a relatively large amount of data. For example, in some cases, each entry in a floating point data class set (e.g., each floating point number) may be 32, 64, or 128 bits, which may significantly contribute to the size of the data.
Additionally, in some cases, a significant portion of the data may exhibit low compressibility. For example, low compressibility data may appear to be limited by the absence of statistical redundancy on the data set, such that lossless compression techniques do not significantly reduce the size of the data. In the remainder of the disclosure, the term "incompressible" is generally used to refer to data having low compressibility. The term does not necessarily mean that the data cannot be compressed at all, but is used to refer to data that is not suitable for conventional compression techniques and/or that conventional compression techniques do not provide a satisfactory compromise between computational expense, information loss and compression gain. As one example, data may be incompressible if the data undergoes a size reduction less than a threshold amount (e.g., 10%) when compressed according to conventional compression methods.
As an example, floating point data may be defined by a sign component, an exponent component, and a mantissa component (also referred to as a significand (signed) component). In some cases, a small portion (e.g., about one to about three bits or numbers) of the sign component, exponent component, and/or mantissa component may have some statistical redundancy across the data set, allowing them to be efficiently compressed according to lossless compression techniques. However, in some cases, a large number of mantissa components may be incompressible. This may limit the effectiveness of lossless compression techniques, in particular, where the mantissa component is the largest component in the floating point data. For example, in some cases, the mantissa component may have a size of about 75% to about 80% of the total size of the floating point data. Similarly, in some cases, the size of the incompressible portion of data may be about 75% to about 80% of the total size of the data.
One approach to this problem is to quantize at least a portion of the data. For example, the quantization technique may map a set of values to "indices" in a compression stage and map the indices to representative values of the entire set in a decompression stage. By doing this, precision may be removed because the entire set of values is mapped to a single element (e.g., via an index). Thus, quantization techniques may reduce the size of data by reducing the precision of the data (e.g., by removing less significant (significant) numbers from the data and truncating or rounding to the nearest lower precision value), at the expense of losing any information at the original precision. This can introduce errors into the data, which can adversely affect the functionality of the system that utilizes the data. This problem can be made worse by the fact that: errors introduced by quantization may affect each data item in the data non-uniformly. For example, if the data includes parameters that define a machine learning model, some parameters may be more susceptible to errors than some other parameters, which may negatively impact the training and/or prediction tasks of the machine learning model.
Systems and methods according to example aspects of the present disclosure may provide solutions to these and other problems. According to an example aspect of the disclosure, one or more computing devices may receive subject data, such as a data set. A data set may include a plurality of data items. For example, each of the data items may define a value, such as a vocabulary (lexical) and/or a numerical value. The data may be stored according to any suitable format. For example, the data may be stored as one or more tensors. For example, each component of the one or more tensors may correspond to a data item of the plurality of data items.
The subject data may be stored in any suitable manner in accordance with example aspects of the present disclosure. For example, the data may be stored in any suitable memory device, such as one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, a disk, and the like, and combinations thereof.
In some embodiments, the data may include one or more numbers. For example, each of the one or more data items may include one or more numbers. For example, the one or more digits may include one or more bits. In some embodiments, one or more bits may represent computer-readable information. For example, a bit may encode information (e.g., numerical information, such as base-10 numerical information, lexical information, or any other suitable information) into a binary data string (e.g., base-2 numerical data) such that the information may be processed or otherwise interpreted by a computing system.
In some embodiments, the subject data may be floating point data. For example, the data may represent floating point numbers. For example, data may be defined and/or interpreted according to a floating point standard (such as, but not limited to, IEEE 754). For example, the floating point criterion may define a real number value according to:
where s is the value of the sign component, b is the base value, e is the value of the exponent component, o is the offset value, p is the precision value, and m isiIs the value of the ith digit of the mantissa component.
In some embodiments, the floating point standard may define a base (base) or a radix (radix). The base number may represent the number of values that may be represented by a single number. For example, the base may be two for a binary representation or ten for a decimal representation. Additionally and/or alternatively, the floating point standard may define precision. For example, the precision may define the maximum number of digits of the mantissa component. Additionally and/or alternatively, the floating point criteria may define an exponent range. The exponent range may define a set of allowed values for the exponent component of the floating point data. Additionally and/or alternatively, the index range may define a deviation or offset of the index components and/or a numerical number. Additionally and/or alternatively, the floating point criteria may define one or more reserved values. For example, the reserved value may represent a special case or other anomaly of the format, such as, but not limited to, infinity or a value that is infinity on the representation (e.g., infinity, negative infinity), NaN (not a number), or any other suitable reserved value.
In some embodiments, floating point data may include a sign component. For example, the sign component may define whether a number represented by floating point data is a positive or negative number. For example, a sign component may include a bit that can have one of two values. A first one of the two values may indicate that the number is a positive number and a second one of the two values may indicate that the number is a negative number. For example, a logic low (e.g., zero) may indicate that the number is a positive number, while a logic high (e.g., one) may indicate that the number is a negative number. For example, the sign component may be based on (-1)sThe sign of the number is defined, where s is the sign component.
In some embodiments, the floating point data may include an exponent component. The exponent component may represent the power to which the base is raised. For example, the exponential component may define the order of magnitude of the number represented. As an example, the exponential component may define an order of magnitude as b(e-o)Where b is a base value, e is the value of the index component, and o is an offset value. For example, the offset value may be selected such that the smallest possible index representable by the criterion is the smallest value (e.g., zero or one).
In some embodiments, floating point data may include a mantissa component (or a significand component). The mantissa component may define the significand of a floating point number. The value of the mantissa component gives the value of the floating point number when multiplied by the value of the sign component and/or raised to the base value of the exponent component. In some embodiments, the mantissa component may be stored such that the first number of the floating point number has a known value. For example, in a binary representation, the mantissa component may be defined as the value of the trailing decimal point. In some cases, the value of the number immediately preceding the decimal point is defined by the standard. For example, in a binary representation, the value of the number immediately preceding the decimal point may always be zero or always be one.
According to an example aspect of the disclosure, one or more computing devices may obtain a first portion and/or a second portion of subject data. For example, the first portion and/or the second portion may be a subset of the data. For example, the first portion and/or the second portion may include one or more numbers. In some embodiments, the first portion and/or the second portion is defined for each of a plurality of data items in the subject data. For example, each of the plurality of data items may be divided into a first portion and/or a second portion. In some embodiments, each of the plurality of data items may be split at the same point. For example, in one example embodiment, the first portion is a compressible portion and the second portion is an incompressible portion.
In some embodiments, the first portion may be a compressible portion of the subject data. The first (e.g., compressible) portion may be efficiently compressed according to lossless compression techniques. For example, the first (e.g., compressible) portion may be effectively compressed without loss of information. For example, the size of the first (e.g., compressible) portion may be significantly reduced without loss of information.
The first (e.g., compressible) portion may exhibit statistical redundancy such that at least a portion of the compressible portion may be predicted or otherwise recreated based on the statistical redundancy. One example of statistical redundancy includes having a continuously recurring (recurring) value of a portion (e.g., one or more numbers) of a data item across multiple data items in the subject data. For example, portions of the plurality of data items may remain unchanged or otherwise change infrequently between the plurality of data items. The compressible portion may exhibit statistical redundancy that allows the compressible portion to be compressed by any suitable compression technique, such as, but not limited to, a Lempel-Zip-Markov chain algorithm (LZMA), a Burrows-Wheeler algorithm (e.g., bzip2), a DEFLATE algorithm (e.g., gzip), a machine learning compression technique, or any other suitable compression technique. For example, the successive reproduction value may be expressed as a value and the number of successive reproductions of the value. Such a representation may require less memory to store and/or transmit than uniquely representing a value at each iteration.
In some embodiments, the second portion may be an incompressible portion of data. That is, the second (e.g., non-compressible) portion may be a portion of the data that may not be efficiently compressed according to lossless compression techniques. For example, the second (e.g., non-compressible) portion may appear to be limited to no statistical redundancy, such that the size of the second (e.g., non-compressible) portion may be significantly reduced without loss of information. For example, the second (e.g., non-compressible) portion may comprise portions of the plurality of data items such that the values of the portions exhibit little or no duplication, particularly consecutive duplication, among the plurality of data items. For example, the second (e.g., non-compressible) portion may experience a size reduction of less than about 10% when compressed according to conventional compression methods. However, indices can generally benefit from LZMA/bzip or similar compression methods even when non-quantized values are difficult to compress.
In some embodiments, the subject data may include a plurality of data blocks. The data blocks may be sequential. The data blocks may have similar or identical formats. In some embodiments where the subject data includes multiple data blocks, each data block may be compressed as described herein. For example, the respective data included in each data block may include a first (e.g., compressible) portion and a second (e.g., incompressible) portion. As described herein, the first and second portions of each respective data block may be compressed using first and second techniques.
In some embodiments, the first portion and/or the second portion may be portions of floating point data. For example, the first portion may be a compressible portion of floating point data and the second portion may be an incompressible portion of floating point data. For example, the first portion may comprise at least a portion of a sign component and/or an exponent component and/or a mantissa component. For example, at least a portion of the mantissa component may include about one to about three digits of the mantissa component. In some embodiments, the about one to about three digits may be about one to about three of the most significant digits of the mantissa component. If a portion of the mantissa component is present in the first portion, the second portion may include the mantissa component or a remaining portion of the mantissa component. The first portion and/or the second portion may comprise a plurality of data items. In other embodiments, other allocations of different components of data or subcomponents thereof to two or more portions may be used (e.g., in addition to the example allocations described above).
As one example application, in some cases, floating point data defining machine learning model parameters may be stored as tensors. That is, the topic data may be a tensor, such as a tensor that stores machine learning model parameters. Each component in the tensor can correspond to a floating point number that defines a unique machine learning parameter. Each component in the tensor can thus have an associated sign component, exponent component, and/or mantissa component. In some cases, a portion of the tensor component may be compressible. For example, a portion of the tensor components (e.g., the sign and exponent components) may exhibit statistical redundancy across some aspects of the tensor, such as across one or more dimensions of the tensor. In some cases, a portion of the tensor component may be incompressible. For example, a portion of a tensor component (e.g., a mantissa component or portion thereof) may be uniquely defined at each tensor component.
According to an example aspect of the disclosure, one or more computing devices may transform a portion of subject data to obtain transformed data. For example, one or more computing devices may transform an incompressible portion of the subject data. For example, one or more computing devices may transform the portion of the subject data such that a norm of the portion of the subject data is minimized.
For example, in some embodiments, transforming portions of the subject data may include: linearizing the subject data and/or linearizing portions of the subject data. For example, linearizing the subject data and/or portions of the subject data may include: the topic data and/or portions of the topic data are represented as vectors. For example, in some cases, the topic data may be represented as a multidimensional matrix or tensor. In this case, the subject data and/or portions (e.g., non-compressible portions) of the subject data can be linearized into a vector representation. Any suitable linearization method may be employed in accordance with the present disclosure.
One example transformation according to an example aspect of the present disclosure is Kashin decomposition. The Kashin decomposition transforms a frame representation of a vector in a complex frame with N dimensions into a frame extension with N dimensions, N > N, where the coefficients have the smallest possible dynamic range, referred to herein as the Kashin representation. For example, the dynamic range of the coefficients may be approximately
Transforming a portion of the subject data according to Kashin's decomposition may provide several advantages. For example, transforming the subject data according to a Kashin decomposition may spread the information contained in portions of the subject data approximately evenly and/or evenly among the plurality of coefficients. For example, each iterative step of truncating the frame representation and determining the frame representation of the residual may conceptually "scrape" the information in x at larger coefficients and "redistribute" the information at smaller coefficients. This may be repeated until the error is spread evenly in the desired manner. Thus, if an error is introduced into the plurality of coefficients (e.g., from quantizing the plurality of coefficients), the error is associated with the plurality of coefficients, rather than with the portion of the subject data. Thus, if portions of subject data are recreated from transformed data having errors, the errors are approximately evenly and/or uniformly spread among the recreated data, which may better preserve the portions of subject data in the recreated data.
According to example aspects of the present disclosure, one or more computing devices may quantize transform data to obtain quantized data. The transform data may be quantized to reduce the precision associated with the transform data. For example, the quantized data may have a smaller number of digits than the transform data. By reducing the number of digits, the quantized data may have a smaller size in memory than the transform data. According to example aspects of the disclosure, any suitable method of quantization may be employed. The quantization method may include a compression technique and/or the quantized data may be further compressed after quantization.
In some embodiments, the transform data may be quantized by uniform quantization. For example, the transform data may be rounded to the nearest value with lower precision (e.g., a smaller number of digits) than the transform data to obtain quantized data. For example, the quantized data may define a plurality of quantization levels, wherein the transform data has a value substantially between two quantization levels. In one embodiment, for example, if a transform data item has a value between two quantization levels and less than a midpoint defined between the two quantization levels, then the value of the corresponding quantized data item is selected as the lower of the two quantization levels. Otherwise, the value of the corresponding quantized data item is selected as the higher of the two quantization levels.
In some embodiments, the transform data may be quantized by truncating at least a portion of the transform data. For example, less significant bits (e.g., bits having a relatively low associated magnitude) may be truncated from the transform data. In other words, the values of the transform data may be rounded down to the nearest of the multiple quantization levels, resulting in rounded down data having one or more trailing zeros. One or more trailing zeros may be omitted from storage and/or restored at a later point in the computation to reduce memory requirements for storing quantized data as compared to transformed data.
In some embodiments, the transform data may be quantized by mapping the transform data to one of a plurality of index values. For example, in one example embodiment, the transform data is rounded, rounded down, or the like to a closest value associated with one of the plurality of index values, and the index value associated with the closest value is stored, for example, as a substitute item for storing the transform data.
Quantizing the transform data to obtain quantized data may introduce quantization errors into the quantized data. However, transforming the raw data (e.g., by Kashin decomposition) prior to quantizing the data may allow quantization errors to be spread or "smeared" between the raw data. In other words, the quantization error is not concentrated on specific terms in the original data, but is scattered among the terms. Thus, while still introducing error into the quantized data, the error can be better handled in the application of the data. For example, a machine learning model with parameters defined by data may be less affected by errors in the data than conventional compression.
According to example aspects of the disclosure, one or more computing devices may store quantized data. For example, one or more computing devices may store the quantized data in one or more memory devices. Example memory devices include, but are not limited to, non-transitory computer-readable storage media such as RAM, ROM, EEPROM, EPROM, flash memory devices, disks, and the like, and combinations thereof.
Additionally and/or alternatively, one or more computing devices can store the compressed portion of the subject data. For example, the compressed portion may be a compressible portion of the subject data that is compressed according to any suitable compression technique (particularly lossless compression techniques). For example, the compressed portion may be compressed by any suitable compression technique, such as, but not limited to, a Lempel-Zip-Markov chain algorithm (LZMA), a Burrows-Wheeler algorithm (e.g., bzip2), a DEFLATE algorithm (e.g., gzip), a machine learning compression technique, or any other suitable compression technique.
Additionally and/or alternatively, one or more computing devices may transmit quantized data. For example, the quantized data may be transmitted from a first computing device to a second computing device. As another example, the quantized data may be transmitted from a first location (e.g., a first memory address) to a second location (e.g., a second memory address). The quantized data may be transmitted by any suitable transmission method. For example, the quantized data may be transmitted via wired transmission (e.g., ethernet) and/or via wireless transmission (e.g., bluetooth, ZigBee, WLAN, IEEE 802.11) or via any other suitable transmission method or technique.
Additionally and/or alternatively, one or more computing devices may transmit the compressed portion of the subject data. For example, the compressed portion may be a compressible portion of the subject data that is compressed according to any suitable compression technique (particularly lossless compression techniques). For example, the compressed portion may be compressed by any suitable compression technique, such as, but not limited to, a Lempel-Zip-Markov chain algorithm (LZMA), a Burrows-Wheeler algorithm (e.g., bzip2), a DEFLATE algorithm (e.g., gzip), a machine learning compression technique, or any other suitable compression technique.
In some embodiments, after storing and/or transmitting the quantized data, the one or more computing devices may reconstruct the subject data from the compressed data and/or the quantized data. For example, one or more computing devices may decompress compressed data to obtain compressible portions from the compressed data. For example, the compressible portion may be obtained from compressed data without loss of information.
Additionally and/or alternatively, the one or more computing devices may reconstruct the non-compressible portion from the quantized data. For example, one or more computing devices may reconstruct the transform data from the quantized data. In some embodiments, the transform data cannot be accurately reconstructed from the quantized data due to quantization errors introduced during quantization. In these cases, reconstructing the transformed data may include: transform data is estimated based on the quantized data. As another example, the quantized data may include an index that maps to a corresponding value of the transform data.
For example, the quantized data may be mapped to corresponding values of the transform data with less precision than the corresponding values. Therefore, reconstructing the transform data may require increasing the accuracy of the quantized data. For example, the accuracy can be improved by adding a number to the quantized data. As one example, if the transform data is a 32-bit number and the quantized data is a 24-bit number, the quantized data is 8 bits less accurate than the transform data. In some embodiments, the most significant 24 bits of the transform data are the same as the 24 bits of the quantized data. Therefore, in order to estimate transform data from quantized data, 8 trailing (i.e., least significant) bits may be appended to 24 bits of the quantized data. In some embodiments, the trailing bit may be zero. In some embodiments, the quantized data may be dithered so that trailing bits may have random values. Dithering may allow the reconstructed transform data to be offset from the quantization values by introducing controlled noise into the reconstructed data.
Additionally and/or alternatively, the incompressible portion may be reconstructed from the transform data (such as reconstructed transform data). For example, an inverse transform may be applied to the transformed data to undo the transform and restore the original data. For example, in the case where the incompressible part is transformed by Kashin decomposition, the incompressible part can be reconstructed from the frame expansion of the incompressible part. For example, if the non-compressible portion is represented as having frames
Various advantages may be realized by storing and/or transmitting the subject data as a compressed data portion and a transformed and quantized data portion. For example, the compressed data portion may be transmitted without loss of information. In addition, since a high level of compression can be achieved, compressed data can be efficiently transmitted. In addition, although errors are introduced in the transformed and quantized data, the errors have less impact on the application of the transformed and quantized data than data that is quantized only.
For example, one technical effect in accordance with example aspects of the present disclosure is to provide systems and methods for lossy data compression of low-compressibility data such that compression errors necessarily resulting from compression of the low-compressibility data are interspersed among the low-compressibility data. For example, compression errors may be spread among multiple data items in low compressibility data. By spreading the compression error, certain characteristics of the data set may be better preserved after compression and decompression than in conventional approaches. For example, if the low compressibility data defines parameters of the machine learning model, the tuning characteristics of the parameters may be better preserved. As another example, if the low compressibility data defines measurements, the measurements and/or the relationship between the measured values may be better preserved. As another example, errors may be more easily compensated by having a more uniform effect across low compressibility data.
Another technical effect according to aspects of the present disclosure is to provide techniques that allow a higher level of data compression than conventional approaches without adversely affecting systems that use the data. For example, where the topic data defines parameters of a machine learning model, the parameters may be compressed to a higher level of compression without adversely affecting tuning of the machine learning model. For example, systems and methods according to example aspects of the present disclosure may be particularly beneficial in embodiments that are more sensitive to error and/or value compression levels than the speed of compression and/or decompression. Furthermore, some lossy compression may even increase the robustness of the model.
For example, systems and methods according to the present disclosure may be applied to federal learning (fed learning) techniques of machine learning models to reduce the amount of data transmitted and/or improve the accuracy of the machine learning models. In a federated learning technique, a machine learning model may be trained at one or more computing devices using data found at the one or more computing devices. The machine learning model may then be transmitted to a second computing device (e.g., a device seeking to utilize the machine learning model). Benefits of federal learning include, for example, the ability to train models using training data and use models elsewhere without necessarily requiring transmission of training data. This is particularly useful in situations where the training data is private or otherwise not intended for transmission.
More specifically, in some example applications, a server may send a machine learning model (e.g., partially trained, possibly "blank") to several devices. The model may be compressed generally as described herein. Such a version of the model may be referred to as
As another example application with technical benefits, the compression/decompression scheme described herein may be used in a model sharing framework, where a compression model is sent from a first device (e.g., a server) to a second device (e.g., a client, such as a user's device). As another example, the first device and the second device may be peer devices, rather than servers/clients. The first device may also provide a codec (also referred to as a compressor/decompressor) to the second device, which may use the codec to decompress and/or recompress or update the model. The first device may provide a new or updated codec to the second device over time.
As another example technical effect and benefit, transmission bandwidth may be reduced. However, in particular, transmitting the machine learning model may require a large amount of network bandwidth, since the machine learning model may include a large amount of data. Furthermore, lossless compression techniques may have limited effectiveness due to the uniqueness of the model parameters. In addition, conventional lossy compression techniques may introduce errors into the model parameters, particularly unevenly, which may affect tuning of the machine learning model. However, by employing systems and methods according to the present disclosure, the model may be transmitted using less data than lossless compression, while having less negative impact on the tuning of the machine learning model.
Example apparatus and System
Fig. 1A depicts a block diagram of an example computing system 100, according to an example embodiment of the present disclosure. The system 100 includes a user computing device 102, a server computing system 130, and a training computing system 150 communicatively coupled through a network 180.
The user computing device 102 may be any type of computing device, such as, for example, a personal computing device (e.g., a laptop or desktop computer), a mobile computing device (e.g., a smartphone or tablet), a gaming console or controller, a wearable computing device, an embedded computing device, or any other type of computing device.
The user computing device 102 includes one or more processors 112 and memory 114. The one or more processors 112 may be any suitable processing device (e.g., processor cores, microprocessors, ASICs, FPGAs, controllers, microcontrollers, etc.) and may be one processor or a plurality of processors operably connected. The memory 114 may include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, disks, and the like, and combinations thereof. The memory 114 may store data 116 and instructions 118 that, when executed by the processor 112, cause the user computing device 102 to operate.
In some implementations, the user computing device 102 can store or include one or more machine learning models 120. For example, the machine learning model 120 may be or may otherwise include various machine learning models, such as a neural network (e.g., a deep neural network) or other types of machine learning models, including non-linear models and/or linear models. The neural network may include a feed-forward neural network, a recurrent neural network (e.g., a long-short term memory recurrent neural network), a convolutional neural network, or other form of neural network. An example machine learning model 120 is discussed with reference to fig. 2.
In some implementations, the one or more machine learning models 120 can be received from the server computing system 130 over the network 180, stored in the user computing device memory 114, and then used or otherwise implemented by the one or more processors 112. For example, the model 120 may be stored in the user computing device memory 114 according to the method for storing compressed data illustrated in FIG. 4. As another example, the model 120 may be transmitted between the user computing device 102 and/or the server computing system 130 according to the method for transmitting compressed data illustrated in fig. 5. In some implementations, the user computing device 102 can implement multiple parallel instances of a single machine learning model 120 (e.g., performing parallel prediction tasks across multiple instances of relevant and/or irrelevant input features).
Additionally or alternatively, one or more machine learning models 140 can be included in the server computing system 130 or otherwise stored and implemented by the server computing system 130, the server computing system 130 communicating with the user computing device 102 according to a client-server relationship. For example, the machine learning model 140 may be implemented by the server computing system 140 as part of a web service (e.g., a machine learning service). Accordingly, one or more models 120 may be stored and implemented at the user computing device 102 and/or one or more models 140 may be stored and implemented at the server computing system 130.
The user computing device 102 may also include one or more user input components 122 that receive user input. For example, the user input component 122 may be a touch sensitive component (e.g., a touch sensitive display screen or a touchpad) that is sensitive to touch by a user input object (e.g., a finger or a stylus). The touch sensitive component may be used to implement a virtual keyboard. Other example user input components include a microphone, a conventional keyboard, or other means by which a user may provide user input.
The user computing device 102 may include one or more compressors/decompressors 124. Compressor/decompressor 124 may compress and/or decompress data in accordance with any of the compression techniques described herein. The compressor/decompressor 124 may be included in a particular application or may be implemented by an operating system of the user computing device 102. The compressor/decompressor 124 includes computer logic for providing the desired functionality. The compressor/decompressor 124 may be implemented in hardware, firmware and/or software controlling a general purpose processor.
The server computing system 130 includes one or more processors 132 and memory 134. The one or more processors 132 may be any suitable processing device (e.g., processor cores, microprocessors, ASICs, FPGAs, controllers, microcontrollers, etc.) and may be one processor or a plurality of processors operably connected. Memory 134 may include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, disks, and the like, and combinations thereof. The memory 134 may store data 136 and instructions 138 that, when executed by the processor 132, cause the server computing system 130 to operate.
In some implementations, the server computing system 130 includes or is otherwise implemented by one or more server computing devices. Where the server computing system 130 includes multiple server computing devices, such server computing devices may operate according to a sequential computing architecture, a parallel computing architecture, or some combination thereof.
As described above, the server computing system 130 may store or otherwise include one or more machine learning models 140. For example, the model 140 may be or may otherwise include various machine learning models. Example machine learning models include neural networks or other multi-layered nonlinear models. Example neural networks include feed-forward neural networks, deep neural networks, recurrent neural networks, and convolutional neural networks. An example model 140 is discussed with reference to FIG. 2.
The server computing device 130 may include one or more compressor/decompressors 142. Compressor/decompressor 142 may compress and/or decompress data in accordance with any of the compression techniques described herein. Compressor/decompressor 142 may be included in a particular application or may be implemented by an operating system of server computing system 130. Compressor/decompressor 142 includes computer logic for providing the desired functionality. Compressor/decompressor 142 may be implemented in hardware, firmware, and/or software controlling a general purpose processor.
The user computing device 102 and/or the server computing system 130 may train the models 120 and/or 140 via interaction with a training computing system 150 communicatively coupled through a network 180. The training computing system 150 may be separate from the server computing system 130 or may be part of the server computing system 130. For example, the models 120 and/or 140 may be trained at the training computing system 150 and transmitted to the user computing device 102 and/or the server computing system 130. For example, models 120 and/or 140 may be transmitted according to the method for transmitting compressed data illustrated in FIG. 5.
In some embodiments, counter-propagating the error may include: a reverse propagation of the truncation across time is performed. The model trainer 160 may perform a number of generalization techniques (e.g., weight decay, descent (dropouts), etc.) to improve the generalization capability of the trained model.
In particular, the model trainer 160 may train the machine learning models 120 and/or 140 based on a set of training data 162. The training data 162 may include, for example, pairs of input features and associated category labels. For example, the input features may include features for natural language processing, such as raw or processed language information. As another example, the input features may include image features, such as raw or processed images. As another example, the input features may include features for a content recommendation service, such as web usage or other suitable information. The input features may have associated labels. For example, the labels may indicate desired aspects of training, such as real categories associated with some or all of the input features.
In some implementations, the training examples may be provided by the user computing device 102 if the user has provided consent. Thus, in such an implementation, the model 120 provided to the user computing device 102 may be trained by training the computing system 150 based on user-specific data received from the user computing device 102. In some cases, this process may be referred to as a personalization model.
The model trainer 160 includes computer logic for providing the desired functionality. Model trainer 160 may be implemented in hardware, firmware, and/or software that controls a general purpose processor. For example, in some embodiments, model trainer 160 includes program files stored on a storage device, loaded into memory, and executed by one or more processors. In other implementations, the model trainer 160 includes one or more sets of computer-executable instructions stored in a tangible computer-readable storage medium (such as a RAM hard disk or optical or magnetic medium). In some implementations, the model trainer 160 may perform any of the compression/decompression techniques described herein.
FIG. 1A illustrates one example computing system that can be used to implement the present disclosure. Other computing systems may also be used. For example, in some implementations, the user computing device 102 may include a model trainer 160 and a training data set 162. In such an implementation, the model 120 may be trained and used locally at the user computing device 102. In some such implementations, the user computing device 102 may implement the model trainer 160 to personalize the model 120 based on user-specific data. As another example, in some implementations, the training computing system 150 may include a compressor and/or a decompressor (e.g., as described at 124 and 142). By having a compressor/decompressor at the training computing system 150, the training computing system 150 may participate in the example federated learning application described above. For example, all communications across network 180 may occur in compressed form.
Fig. 1B depicts a block diagram of an example computing device 100, conducted in accordance with an example embodiment of the present disclosure. Computing device 10 may be a user computing device or a server computing device.
As illustrated in fig. 1B, each application may communicate with several other components of the computing device, such as, for example, one or more sensors, a context manager, a device state component, and/or additional components. In some implementations, each application can communicate with each device component using an API (e.g., a public API). In some embodiments, the APIs used by each application are specific to that application.
Fig. 1C depicts a block diagram of an example computing device 50, conducted in accordance with an example embodiment of the present disclosure. Computing device 50 may be a user computing device or a server computing device.
The central smart tier includes a number of machine learning models. For example, as illustrated in fig. 1C, a respective machine learning model (e.g., model) may be provided for each application and managed by a central intelligence layer. In other implementations, two or more applications may share a single machine learning model. For example, in some implementations, the central smart inlay may provide a single model (e.g., a single model) for all applications. In some implementations, the central smart inlay is included within or otherwise implemented by the operating system of the computing device 50.
The central smart inlay may communicate with the central device data plane. The central device data layer may be a centralized repository for data of the computing device 50. As illustrated in fig. 1C, the central device data layer may communicate with several other components of the computing device, such as, for example, one or more sensors, a context manager, a device status component, and/or additional components. In some implementations, the central device data layer can communicate with each device component using an API (e.g., a private API).
Fig. 2 depicts a block diagram of an example machine learning model 200, according to an example embodiment of the present disclosure. In some implementations, the machine learning model 200 is trained to receive a set of input data 204 that describes input features, and as a result of receiving the input data 204, to provide output data 206 indicative of one or more outputs (e.g., predictions). One of ordinary skill in the art will recognize that systems and methods according to example aspects of the present disclosure may be extended to any suitable machine learning task.
Fig. 3 depicts a block diagram of example data 300, according to an example embodiment of the present disclosure. Data 300 may be floating point data. For example, data 300 includes a sign component 302, an exponent component 304, and a mantissa component 306. For purposes of illustration, the data 300 is a 16-bit floating point number having one number in the sign component 302, six numbers in the exponent component 304, and nine numbers in the mantissa component 306. Those of ordinary skill in the art will appreciate that data having any suitable number of digits in each of the sign component 302, exponent component 304, and mantissa component 306 may be used in accordance with the present disclosure. For example, 32-bit floating point numbers having one, eight, and twenty-three digits in the sign, exponent, and mantissa components, respectively, may be used according to the present disclosure. As another example, 64-bit "double precision" floating point numbers having one, eleven, and fifty-two digits in the sign, exponent, and mantissa components, respectively, may be used according to the present disclosure.
The data 300 may include a first portion 310 and a second portion 312. As illustrated in fig. 3, the portions 310, 312 are divided into left or right sides of the line 311. However, one of ordinary skill in the art will appreciate that any suitable number of any of the sign component 302, the exponent component 304, and/or the mantissa component 306 may be included in the first portion 310 and/or the second portion 312. For example, line 311 may be located between any two digits in data 300. As another example, the numbers in the data 300 may be split separately, such as without regard to location within the data 300.
In some embodiments, the first portion 310 may include numbers that are compressible across a data class set (e.g., a data set that includes the data 300). For example, as illustrated in fig. 3, the entire sign component 302 and exponent component 304 are included in a first portion 310 along with two digits of the mantissa component 306. In some cases, these numbers may exhibit statistical redundancy across multiple data (e.g., a data set including data 300), allowing them to be efficiently compressed according to conventional compression techniques.
In some embodiments, the second portion 312 may include numbers that are not compressible across a data class set (e.g., a data set including the data 300). For example, as illustrated in fig. 3, a portion of the digits of mantissa component 306 are included in second portion 312. In some cases, these numbers may appear to be limited to no statistical redundancy across multiple data (e.g., a data set including data 300), thereby preventing their efficient compression according to conventional compression techniques.
Example method
Fig. 4 depicts a flowchart of an example method for transforming data, according to an example embodiment of the present disclosure. Although fig. 4 depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particular illustrated order or arrangement. Various steps of the method 400 may be omitted, rearranged, combined, and/or adjusted without departing from the scope of the present disclosure.
In 402, a computing system may receive vector data. For example, the vector data may include data representing a vector. As another example, the vector data may be represented in a vector format. For example, in some embodiments, the vector data may represent a linearized multidimensional matrix having a single dimension. For example, the vector data may be a vector x having n dimensions.
In some embodiments, an initial Kashin frame representation and/or an initial truncation level may be initialized. For example, the initial Kashin frame representation may be initialized to one or more coefficients aiWherein i is 1, … …, N. In some embodiments, each of the coefficients may be initialized to zero. As another example, an initial truncation level may be initialized to
In 404, the computing system may determine a vector frame representation of the vector data. For example, it can be in a frame with N dimensions
In 406, the computing system may truncate the vector frame representation to obtain a truncated frame representation. E.g. vectorsThe frame representation may be in levels
In 408, the computing system may adjust one or more coefficients of the Kashin frame representation based on the truncated frame representation. For example, in some embodiments, one or more truncation coefficients may be added to the Kashin frame coefficients. In some embodiments, one or more of the truncation coefficients may be factored (e.g., by
In 410, the computing system may determine a residual. For example, the residual may be the difference (e.g., x-x') between vectors represented by the frame representation before truncation and after truncation.
At 412, the computing system may determine a residual frame representation of the residual. For example, the frame may be based on satisfying the uncertainty principle
FIG. 5 depicts a flowchart of an example method for storing compressed data, according to an example embodiment of the disclosure. Although fig. 5 depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particular illustrated order or arrangement. Various steps of the method 500 may be omitted, rearranged, combined, and/or adjusted without departing from the scope of the present disclosure.
In 502, a computing system may receive topic data. For example, the subject data may include a data set. The subject data and/or data set can include a plurality of data items. For example, each of the data items may define a value, such as a vocabulary and/or a numerical value. The subject data may be stored according to any suitable format. For example, in some embodiments, the subject data may be floating point data. In some embodiments, floating point data may include a sign component, an exponent component, and/or a mantissa component.
At 504, the computing system may obtain the incompressible portion from the subject data. For example, the non-compressible portion may be a portion of the subject data that may not be efficiently compressed according to lossless compression techniques. For example, the incompressible portion may appear to be limited to no statistical redundancy so that the size of the incompressible portion may be significantly reduced without loss of information. For example, the non-compressible portion may comprise portions of the plurality of data items such that the values of the portions exhibit little or no duplication, particularly consecutive duplication, across the plurality of data items.
In 506, the computing system may transform the non-compressible portion to obtain transformed data. For example, the computing system may transform the incompressible portion such that the norm of the incompressible portion is minimized. For example, the computing system may transform the incompressible portion according to a Kashin decomposition.
In 508, the computing system may quantize the transform data to obtain quantized data. For example, the transform data may be quantized to reduce the precision associated with the transform data. For example, the quantized data may have a smaller number of digits than the transform data. Any suitable quantization method may be employed to quantize the transform data. For example, the transform data may be quantized by uniform quantization. As another example, the transform data may be quantized by truncating at least a portion of the transform data. For example, the less significant bits may be truncated from the transformed data. As another example, the transform data may be quantized by mapping the transform data to one of a plurality of index values.
In 510, the computing system may store the quantized data. For example, one or more computing devices may store the quantized data in one or more memory devices. Example memory devices include, but are not limited to, non-transitory computer-readable storage media such as RAM, ROM, EEPROM, EPROM, flash memory devices, disks, and the like, and combinations thereof.
Additionally and/or alternatively, one or more computing devices can store the compressed portion of the subject data. For example, the compressed portion may be a compressible portion of the subject data that is compressed according to any suitable compression technique (particularly lossless compression techniques). For example, a compressible portion of compressed data may be a portion of compressed data that is separate from an incompressible portion. For example, the compressible portion may be data in the compressible portion that is not in the incompressible portion. For example, the compressed portion may be compressed by any suitable compression technique, such as, but not limited to, a Lempel-Zip-Markov chain algorithm (LZMA), a Burrows-Wheeler algorithm (e.g., bzip2), a DEFLATE algorithm (e.g., gzip), a machine learning compression technique, or any other suitable compression technique.
In some implementations, the quantized data may be stored in association with a compressed portion of the subject data. For example, storing two forms of data in association with each other may include: both forms of data are stored in the same data file. As another example, storing two forms of data in association with each other may include: both forms of data are stored such that the data are logically related to each other and/or refer to each other or to each other's storage locations. As another example, storing two forms of data in association with each other may include: the two forms of data are stored within a data structure, where the data structure contains a logical connection between the two forms of data (e.g., a pointer to another pointer or a lookup table within the database).
Fig. 6 depicts a flowchart of an example method for transmitting compressed data, according to an example embodiment of the disclosure. Although fig. 6 depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particular illustrated order or arrangement. Various steps of the method 600 may be omitted, rearranged, combined, and/or adapted without departing from the scope of the present disclosure.
In 602, a computing system may receive topic data. For example, the subject data may include a data set. The subject data and/or data set can include a plurality of data items. For example, each of the data items may define a value, such as a vocabulary and/or a numerical value. The subject data may be stored according to any suitable format. For example, in some embodiments, the subject data may be floating point data. In some embodiments, floating point data may include a sign component, an exponent component, and/or a mantissa component.
In 604, the computing device may obtain a first portion and a second portion of the topic data. For example, in some embodiments, the second portion may be an incompressible portion of the subject data. For example, the second portion may be a portion of the subject data that may not be efficiently compressed according to lossless compression techniques. For example, the second portion may appear to be limited to no statistical redundancy, such that the size of the second portion may be significantly reduced without loss of information. For example, the second portion may comprise portions of the plurality of data items such that the values of the portions exhibit little or no duplication, particularly consecutive duplication, among the plurality of data items.
Additionally and/or alternatively, in some embodiments, the first portion may be a compressible portion of the subject data. The first portion may be efficiently compressed according to lossless compression techniques. For example, the first portion may be efficiently compressed without loss of information. For example, the size of the first portion may be significantly reduced without loss of information. For example, the first portion may exhibit statistical redundancy such that at least a portion of the first portion may be predicted or otherwise recreated based on the statistical redundancy.
In 606, the computing system may transform the second portion to obtain transformed data. For example, the computing system may transform the second portion such that a norm of the second portion is minimized. For example, the computing system may transform the second portion according to a Kashin decomposition.
In 608, the computing system may quantize the transform data to obtain quantized data. For example, the transform data may be quantized to reduce the precision associated with the transform data. For example, the quantized data may have a smaller number of digits than the transform data. Any suitable quantization method may be employed to quantize the transform data. For example, the transform data may be quantized by uniform quantization. As another example, the transform data may be quantized by truncating at least a portion of the transform data. For example, the less significant bits may be truncated from the transformed data. As another example, the transform data may be quantized by mapping the transform data to one of a plurality of index values.
In 610, the computing system may transmit quantized data. For example, the computing system may transmit the quantized data to a second computing system. The quantized data may be transmitted by any suitable transmission method. For example, the quantized data may be transmitted via wired transmission (e.g., ethernet) and/or via wireless transmission (e.g., bluetooth, ZigBee, WLAN, IEEE 802.11) or via any other suitable transmission method or technique.
Additionally and/or alternatively, one or more computing devices may transmit the compressed portion of the subject data. For example, the compressed portion may be a first portion of the subject data that is compressed according to any suitable compression technique, particularly a lossless compression technique. For example, the compressed portion may be compressed by any suitable compression technique, such as, but not limited to, a Lempel-Zip-Markov chain algorithm (LZMA), a Burrows-Wheeler algorithm (e.g., bzip2), a DEFLATE algorithm (e.g., gzip), a machine learning compression technique, or any other suitable compression technique. For example, the computing device may compress the first portion to obtain compressed data. Additionally and/or alternatively, the computing device may transmit the compressed data (e.g., to a second computing device). Additionally and/or alternatively, the compressed data may be decompressed (e.g., by the second computing device) to obtain the first portion.
Additional disclosure
The techniques discussed herein make reference to servers, databases, software applications, and other computer-based systems and actions taken and information sent to and from such systems. The inherent flexibility of computer-based systems allows for a variety of possible configurations, combinations, and divisions of tasks and functionality between and among components. For example, the processes discussed herein may be implemented using a single device or component or multiple devices or components operating in combination. Databases and applications may be implemented on a single system or distributed across multiple systems. The distributed components may run sequentially or in parallel.
While the present subject matter has been described in detail with respect to various specific example embodiments thereof, each example is provided by way of illustration, and not limitation, of the present disclosure. Alterations, modifications, and equivalents of such embodiments may readily occur to those skilled in the art having the benefit of the teachings presented in the foregoing description. Accordingly, the subject disclosure does not preclude inclusion of such modifications, variations and/or additions to the present subject matter as would be readily apparent to one of ordinary skill in the art. For instance, features illustrated or described as part of one embodiment, can be used with another embodiment to yield a still further embodiment. Accordingly, the present disclosure is intended to embrace such alterations, modifications, and equivalents.
Claims (20)
1. A computing system configured to compress data, the computing system comprising:
one or more processors; and
one or more non-transitory computer-readable media storing instructions that, when executed by the one or more processors, cause the computing system to perform operations comprising:
obtaining subject data;
performing a first compression technique to compress a first portion of the data and obtain first compressed data, wherein a second portion of the data remains uncompressed after performing the first compression technique;
transforming the second portion of the data to obtain transformed data;
quantizing the transform data to obtain quantized data; and
storing the first compressed data and the quantized data.
2. The computing system of claim 1, wherein the data comprises floating point data.
3. The computing system of claim 2, wherein:
the floating point data includes one or more sign bits, a plurality of exponent bits, and a plurality of mantissa bits;
the first portion of the data comprises the one or more sign bits, the plurality of exponent bits, and one or more most significant mantissa bits of the plurality of mantissa bits; and
the second portion includes one or more least significant mantissa bits of the plurality of mantissa bits.
4. The computing system of claim 2 or 3, wherein the floating point data encodes parameter values for parameters of a machine learning model.
5. The computing system of any of the preceding claims, wherein transforming the second portion of the data comprises: performing a Kashin decomposition on the second portion of the data.
6. The computing system of any of the preceding claims, wherein quantizing the transform data comprises: applying uniform quantization to the transform data.
7. The computing system of any of the preceding claims, wherein quantizing the transform data comprises: truncating at least a portion of the transformed data.
8. The computing system of any of the preceding claims, further comprising:
reconstructing the transform data from the quantized data; and
reconstructing the second portion from the transformed data.
9. The computing system of claim 8, wherein to reconstruct the transform data comprises to: dithering the quantized data.
10. A computer-implemented method of transmitting compressed data, the method comprising:
receiving, at a first computing device, subject data;
determining, by the first computing device, a first portion and a second portion of the topic data;
transforming, by the first computing device, the second portion to obtain transformed data;
quantizing, by the first computing device, the transform data to obtain quantized data; and
transmitting the quantized data from the first computing device to a second computing device.
11. The computer-implemented method of claim 10, wherein the subject data comprises floating point data.
12. The computer-implemented method of claim 11, wherein the floating point data includes a sign component, an exponent component, and a mantissa component.
13. The computer-implemented method of claim 12, wherein the second portion comprises at least a portion of the mantissa component.
14. The computer-implemented method of claim 12 or 13, wherein the first portion comprises the sign component, the exponent component, and at least a portion of the mantissa component.
15. The computer-implemented method of any of claims 10 to 14, wherein transforming the second portion comprises: performing a Kashin decomposition on the second portion.
16. The computer-implemented method of any of claims 10 to 15, wherein quantizing the transform data comprises: performing uniform quantization on the transform data.
17. The computer-implemented method of any of claims 10 to 16, wherein quantizing the transform data comprises: truncating at least a portion of the transformed data.
18. The computer-implemented method of any of claims 10 to 17, further comprising:
reconstructing, by the second computing device, the transform data from the quantized data; and
reconstructing, by the second computing device, the second portion from the transformed data.
19. The method of any of claims 10 to 18, further comprising:
compressing, by the first computing device, the first portion to obtain compressed data;
transmitting the compressed data from the first computing device to the second computing device; and
decompressing, by the second computing device, the compressed data to obtain the first portion.
20. One or more non-transitory computer-readable media storing instructions that, when executed by one or more computing devices, cause the one or more computing devices to compress a machine learning model by performing operations comprising:
obtaining model parameter data that respectively includes a plurality of floating point numbers for a plurality of parameters of the machine learning model, wherein each of the plurality of floating point numbers includes one or more sign bits, a plurality of exponent bits, and a plurality of mantissa bits; and
for each of the plurality of floating point numbers:
generating first compressed data from the one or more sign bits, the plurality of exponent bits, and one or more most significant mantissa bits of the plurality of mantissa bits, thereby obtaining first compressed data;
performing a Kashin decomposition on one or more remaining least significant mantissa bits to obtain transformed data, the one or more remaining least significant mantissa bits comprising the plurality of mantissa bits not included in the one or more most significant mantissa bits from which the first compressed data was generated;
performing uniform quantization on the transform data to obtain quantized data; and
storing the first compressed data and the quantized data.
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2019/050205 WO2021050039A1 (en) | 2019-09-09 | 2019-09-09 | Compression of data that exhibits mixed compressibility |
Publications (1)
Publication Number | Publication Date |
---|---|
CN114503439A true CN114503439A (en) | 2022-05-13 |
Family
ID=68063048
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201980101171.4A Pending CN114503439A (en) | 2019-09-09 | 2019-09-09 | Compressing data exhibiting mixed compressibility |
Country Status (4)
Country | Link |
---|---|
US (2) | US11843397B2 (en) |
EP (1) | EP4022773A1 (en) |
CN (1) | CN114503439A (en) |
WO (1) | WO2021050039A1 (en) |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN117440154A (en) * | 2023-12-21 | 2024-01-23 | 之江实验室 | Depth map sequence compression method considering floating point digital splitting |
Family Cites Families (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
MX354337B (en) * | 2013-04-15 | 2018-02-27 | Rossato Luca | Hybrid backward-compatible signal encoding and decoding. |
EP4012928B1 (en) | 2015-05-21 | 2023-06-14 | Zeropoint Technologies AB | Methods, devices and systems for semantic-value data compression and decompression |
-
2019
- 2019-09-09 US US17/620,448 patent/US11843397B2/en active Active
- 2019-09-09 EP EP19774006.1A patent/EP4022773A1/en active Pending
- 2019-09-09 CN CN201980101171.4A patent/CN114503439A/en active Pending
- 2019-09-09 WO PCT/US2019/050205 patent/WO2021050039A1/en unknown
-
2023
- 2023-10-27 US US18/496,120 patent/US20240080038A1/en active Pending
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN117440154A (en) * | 2023-12-21 | 2024-01-23 | 之江实验室 | Depth map sequence compression method considering floating point digital splitting |
CN117440154B (en) * | 2023-12-21 | 2024-04-19 | 之江实验室 | Depth map sequence compression method considering floating point digital splitting |
Also Published As
Publication number | Publication date |
---|---|
US20220368343A1 (en) | 2022-11-17 |
US20240080038A1 (en) | 2024-03-07 |
EP4022773A1 (en) | 2022-07-06 |
WO2021050039A1 (en) | 2021-03-18 |
US11843397B2 (en) | 2023-12-12 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
Lindstrom et al. | Fast and efficient compression of floating-point data | |
JP2020532777A (en) | Deep neural network execution method, execution device, learning method, learning device and program | |
US11907818B2 (en) | Compression of machine-learned models via entropy penalized weight reparameterization | |
US11018692B2 (en) | Floating point data set compression | |
US20240080038A1 (en) | Compression of Data that Exhibits Mixed Compressibility | |
EP4008057B1 (en) | Lossless exponent and lossy mantissa weight compression for training deep neural networks | |
EP3738080A1 (en) | Learning compressible features | |
CN110753225A (en) | Video compression method and device and terminal equipment | |
Tao et al. | Exploration of pattern-matching techniques for lossy compression on cosmology simulation data sets | |
Klöwer et al. | Compressing atmospheric data into its real information content | |
US20230299788A1 (en) | Systems and Methods for Improved Machine-Learned Compression | |
Al-Bahadili | A novel lossless data compression scheme based on the error correcting Hamming codes | |
CN113038134B (en) | Picture processing method, intelligent terminal and storage medium | |
WO2020177863A1 (en) | Training of algorithms | |
EP4309083A1 (en) | Efficient compression of activation functions | |
Wu et al. | FedComp: A Federated Learning Compression Framework for Resource-Constrained Edge Computing Devices | |
US20210357789A1 (en) | Systems and Methods for Compressing Floating Point Tensors | |
Wilkins et al. | Efficient Communication in Federated Learning Using Floating-Point Lossy Compression | |
Tola | Comparative study of compression functions in modern web programming languages | |
EL Asnaoui et al. | An application of linear algebra to image compression | |
Agrawal et al. | Near lossless time series data compression methods using statistics and deviation | |
Sruthi et al. | An Integrated Semi-Supervised Learning Framework For Image Compression Using Dct, Huffman Encoding, and Lzw Coding | |
Liu et al. | An improved fast encoding algorithm for vector quantization | |
WO2023073067A1 (en) | Method and data processing system for lossy image or video encoding, transmission and decoding | |
WO2020240687A1 (en) | Calculation processing method, calculation processing device, and program |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |