CN114902649A - Non-occlusion video overlay - Google Patents
Non-occlusion video overlay Download PDFInfo
- Publication number
- CN114902649A CN114902649A CN202080060945.6A CN202080060945A CN114902649A CN 114902649 A CN114902649 A CN 114902649A CN 202080060945 A CN202080060945 A CN 202080060945A CN 114902649 A CN114902649 A CN 114902649A
- Authority
- CN
- China
- Prior art keywords
- video
- location
- overlay content
- content
- frame
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N5/00—Details of television systems
- H04N5/222—Studio circuitry; Studio devices; Studio equipment
- H04N5/262—Studio circuits, e.g. for mixing, switching-over, change of character of image, other special effects ; Cameras specially adapted for the electronic generation of special effects
- H04N5/272—Means for inserting a foreground image in a background image, i.e. inlay, outlay
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/43—Processing of content or additional data, e.g. demultiplexing additional data from a digital video stream; Elementary client operations, e.g. monitoring of home network or synchronising decoder's clock; Client middleware
- H04N21/431—Generation of visual interfaces for content selection or interaction; Content or additional data rendering
- H04N21/4312—Generation of visual interfaces for content selection or interaction; Content or additional data rendering involving specific graphical features, e.g. screen layout, special fonts or colors, blinking icons, highlights or animations
- H04N21/4316—Generation of visual interfaces for content selection or interaction; Content or additional data rendering involving specific graphical features, e.g. screen layout, special fonts or colors, blinking icons, highlights or animations for displaying supplemental content in a region of the screen, e.g. an advertisement in a separate window
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/70—Determining position or orientation of objects or cameras
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/70—Determining position or orientation of objects or cameras
- G06T7/73—Determining position or orientation of objects or cameras using feature-based methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/40—Extraction of image or video features
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/74—Image or video pattern matching; Proximity measures in feature spaces
- G06V10/761—Proximity, similarity or dissimilarity measures
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/40—Scenes; Scene-specific elements in video content
- G06V20/41—Higher-level, semantic clustering, classification or understanding of video scenes, e.g. detection, labelling or Markovian modelling of sport events or news items
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/40—Scenes; Scene-specific elements in video content
- G06V20/46—Extracting features or characteristics from the video content, e.g. video fingerprints, representative shots or key frames
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V40/00—Recognition of biometric, human-related or animal-related patterns in image or video data
- G06V40/10—Human or animal bodies, e.g. vehicle occupants or pedestrians; Body parts, e.g. hands
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V40/00—Recognition of biometric, human-related or animal-related patterns in image or video data
- G06V40/10—Human or animal bodies, e.g. vehicle occupants or pedestrians; Body parts, e.g. hands
- G06V40/16—Human faces, e.g. facial parts, sketches or expressions
- G06V40/161—Detection; Localisation; Normalisation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V40/00—Recognition of biometric, human-related or animal-related patterns in image or video data
- G06V40/20—Movements or behaviour, e.g. gesture recognition
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/43—Processing of content or additional data, e.g. demultiplexing additional data from a digital video stream; Elementary client operations, e.g. monitoring of home network or synchronising decoder's clock; Client middleware
- H04N21/44—Processing of video elementary streams, e.g. splicing a video clip retrieved from local storage with an incoming video stream, rendering scenes according to MPEG-4 scene graphs
- H04N21/44008—Processing of video elementary streams, e.g. splicing a video clip retrieved from local storage with an incoming video stream, rendering scenes according to MPEG-4 scene graphs involving operations for analysing video streams, e.g. detecting features or characteristics in the video stream
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/10—Image acquisition modality
- G06T2207/10016—Video; Image sequence
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/10—Image acquisition modality
- G06T2207/10024—Color image
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/30—Subject of image; Context of image processing
- G06T2207/30196—Human being; Person
- G06T2207/30201—Face
Abstract
Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for overlaying content on a video stream. In one aspect, a category of video is identified. Confidence scores are determined, each confidence score indicating a likelihood that a location in the frame includes a feature of a feature type. A weight for each feature type is determined based on the category, the weight reflecting the importance of not occluding features of the feature type. The confidence score is adjusted for each feature type based on the weight of the feature type to generate an adjusted confidence score. The adjusted confidence scores are aggregated for each location of each frame to generate an aggregated and adjusted confidence score. Determining a location at which to place the overlay content during the video display based on the aggregated and adjusted confidence scores. The overlay content is provided for display at the determined location in the video.
Description
This specification relates generally to data processing and overlaying content over different types of video streams while avoiding regions of the video streams featuring particular types of content.
The video streamed to the user may include additional content superimposed on top of the original video stream. The overlay content may be provided to the user within a rectangular area that overlays and blocks a portion of the original video screen. In some methods, the area for providing the superimposed content is located at an arbitrary position of the video screen, for example, the middle-lower portion of the screen. If the important content of the original video stream is located in the lower middle portion of the video screen, it may be blocked or obstructed by the overlay content.
Disclosure of Invention
In general, one innovative aspect of the subject matter described in this specification can be embodied in methods that include the following operations: identifying, for a video with which overlay content is to be displayed, a video category of the video from a set of predefined video categories; for each video frame in a set of sampled video frames of the video: determining, for each video feature type in a set of video feature types and for each location in a plurality of locations in the video frame, a confidence score that indicates a likelihood that the location in the video frame includes a feature of the video feature type; determining a weight for each video feature type based on the video category, the weight reflecting an importance of not obscuring video features of the video feature type when displaying video of the video category; and for each video feature type in the set of video feature types, adjusting the confidence scores for the plurality of locations in the video frame based on the weights determined for the video feature type to generate an adjusted confidence score; aggregating the adjusted confidence scores for each location of each video frame in the set of sampled video frames to generate an aggregated and adjusted confidence score; determining a location at which the overlay content is to be placed during the video display based on the aggregated and adjusted confidence scores; and providing the overlay content for display at the determined location in the video. Other embodiments of this aspect include corresponding methods, apparatus, and computer programs, encoded on computer storage devices, configured to perform the actions of the methods. These and other embodiments may each include one or more of the following features.
In some implementations, the video feature types can include changes in human faces, human torso, text, moving objects, or color variances.
In some implementations, at least some confidence scores for different video feature types may be determined in parallel.
In some implementations, adjusting the confidence score for the plurality of locations in the video frame for each video feature type in the set of video feature types may include adjusting the confidence score for locations in the video frame that are within a predefined proximity of a center of the video frame.
In some implementations, determining the location at which the overlay content will be placed during the video display based on the aggregated and adjusted confidence scores may further include determining the location at which the overlay content will be placed during the video display based on a specified size of the overlay content and a frame duration or number of frames within which the overlay content will be provided within the video.
In some implementations, determining a location at which the overlay content is to be placed can include determining a plurality of locations, providing the overlay content at the determined location including providing the overlay content for display at one of the plurality of locations.
In some implementations, the method may further sample, by the video processing system, the video based on a sampling rate to obtain the set of sampled video frames. The sampling rate may be determined based on the processing power of the video processing system.
Particular embodiments of the subject matter described in this specification can be implemented to realize one or more of the following advantages.
The techniques described in this specification provide resource-efficient techniques for displaying video content and overlay content displayed with the video content. For example, when a user is watching a video stream that fills the video screen, content within the video screen area that is important to the user may not fill the entire area of the video screen. Important content may include video features that are likely to be of most interest to the user. For example, important features, such as faces, text, or salient objects (such as foreground objects or moving objects) may only occupy a portion of the video screen area. Thus, there is an opportunity to display additional content to the user in the form of overlay content that does not obstruct the portion of the video screen area that contains the underlying important content.
Aspects of the present disclosure provide advantages in identifying locations in a video frame that contain features from which overlay content is excluded because overlay content above these locations can block or obscure important content (e.g., content classified as important) included in an underlying video stream, which can result in wasted computing resources by delivering video to a user when the user cannot perceive important content, thereby making the delivery of video incomplete or ineffective. In some cases, a machine learning engine (such as a bayesian classifier, an optical character recognition system, or a neural network) may identify important features within the video stream, such as faces or other human parts, text, or other salient objects (such as foreground objects or moving objects). Areas encompassing these important features can be identified; the overlay content may then be displayed outside of these identified regions, for example, at the location(s) that have been determined to have no significant features (or at least a minimum likelihood of having significant features). As a result, the user can receive the overlay content without obstructing the important content of the underlying video stream, thereby not wasting the computational resources required to deliver the video. This results in a more efficient video distribution system that prevents computing system resources (e.g., network bandwidth, memory, processor cycles, and limited client device display space) from being wasted by delivering video where important content is occluded or otherwise imperceptible to the user.
This solution has the further advantage of increasing the efficiency of the screen area in terms of the bandwidth of important content delivered to the viewer. If a user is watching a video, where the important content of the video typically occupies only a portion of the viewing area, the available bandwidth to deliver the important content to the viewer is underutilized. By using a machine learning system to identify the portion of the viewing area that contains the underlying important content of the video stream, aspects of the present disclosure provide for superimposing additional content outside of that portion of the viewing area, resulting in more efficient utilization of the screen area to deliver the important content to the viewer. In other words, more content is delivered for the same or similar resource overhead.
In some approaches, the overlay content includes a box or other icon that the viewer can click on to remove the overlay content, for example, if the overlay content obstructs important content in the underlying video. A further advantage of the present disclosure is that because the overlay content is less likely to obstruct important content in the underlying video, there is less disruption in the viewing experience and a greater likelihood that the viewer will not "click away" from the already displayed overlay content.
Moreover, identifying important features to avoid occlusion may be further refined and customized based on the category of the video, which may lead to a change in each of the technical advantages described above. For example, different video features may be more important for different video categories. For example, not obscuring certain types of video features may be more important for some video categories, and identifying the most important video features for a category may improve the efficiency of content distribution (e.g., by not wasting resources by distributing videos for which important content is obscured). As another example, for some video categories, certain video features may be less important (or not important), and viewing areas that include low importance content or non-important content may be used to overlay content, thereby improving efficient utilization of screen area to deliver important content to users.
Further, the techniques described in this specification provide various ways to reduce the processing time (and use of resources) for determining the location at which the superimposed content is to be displayed. For example, various downsampling and parallelization methods may be used. Thus, the identification of the appropriate location at which the overlay content will be displayed may be performed using fewer computing resources, while still benefiting from other technical advantages of the present invention that arise when overlay content is displayed at the identified location(s).
The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Drawings
FIG. 1 is a block diagram of an example environment in which digital content is distributed and provided for display on a client device.
Fig. 2 shows an example video frame.
Fig. 3 is a block diagram of an example environment in which a video processing system determines the location of superimposed content within a video.
FIG. 4 illustrates an example heatmap.
FIG. 5 illustrates an example confidence score visualization.
FIG. 6 is a flow diagram of an example process for determining a location within a video at which to display overlay content.
FIG. 7 is a block diagram of an example computer system that may be used to perform the described operations.
Detailed Description
This specification relates generally to overlaying content on top of different types of video streams while avoiding areas of the video streams having important content.
As summarized below and described in more detail throughout this document, video systems typically display overlay content (e.g., text, images, etc.) at a predetermined location within the video that is static, such as in the middle-lower portion of the video. However, for some videos (or for some types of videos), displaying the overlay content at a static, predetermined location may obscure important content of the video. For example, a video related to sports may display a scoreboard in a lower middle portion of the video, and overlay content displayed at a preconfigured location (e.g., lower middle portion) may obscure the scoreboard, which may frustrate the user and cause the underlying video content to be of less importance to the user.
The techniques described herein provide a video system that may be configured to identify portions of a video at which overlay content may be provided without obscuring important video content. Important content may include, for example, video features such as text, a human face, a human torso, a moving object, or portions of the video that undergo a change in color variance between video frames. Important content may include the type of video feature on which displaying the overlaid content would be undesirable to the user because the user wishes to view the content and find it interesting. In some implementations, a video system can be configured to automatically identify video features of a video feature type corresponding to important content within a set of frames of a video. The video system may automatically determine the location at which the overlay content is to be displayed by identifying locations at which video features are not identified. For example, a confidence score may be determined for each video feature type and for each of a plurality of locations in a video frame. Each confidence score may indicate a likelihood that a location in a video frame includes a feature of a particular video feature type. The confidence scores may be aggregated and used to determine the location at which the superimposed content is to be displayed.
In some implementations, depending on the type or category of video, certain features of the video may be important because superimposing content on those features may interfere with the viewing of the video, while other features may be less important because superimposing content on those features may not interfere with the viewing of the video. Thus, the video system may be configured to adjust the confidence values of different features based on the category or type of video. In some implementations, for different types/categories of video, the video system may be configured to assign a different importance level (e.g., a weight on a numerical scale from 1 to 5) to each detected feature type. A high importance/weight for a particular feature type indicates that the particular feature type should not be occluded by overlay content, while a low importance/weight indicates that the particular feature type can be occluded by overlay content. For example, for talk show videos, high importance may be assigned for not being superimposed on the portion of the video that includes the human face (e.g., a weight of 4 in the range of 1 to 5). For sports-related videos, high importance may be assigned (e.g., a weight of 4 or 5 in the range of 1 to 5) for not being superimposed on textual portions of the video (e.g., the portions that include scores) and for not being superimposed on portions of the video that are moving there.
Based on the weight assigned to each feature type, the confidence values for the various feature types may be adjusted based on the video category weight, thereby reducing the likelihood of superimposing content on feature types that are more important to the video category. Based on the adjusted confidence values for the different feature types, the video system may identify one or more locations at which the overlay content may be displayed. The video system may then provide the overlay content for display at one of these identified locations.
These and additional features are described in further detail below with reference to fig. 1-7.
In addition to the description throughout this document, a user may be provided with controls that allow the user to make selections as to whether and when the systems, programs, or features described herein are capable of collecting user information (e.g., information about the user's social network, social behavior or activity, profession, the user's preferences, or the user's current location) and whether to send content or communications from a server to the user. Further, certain data may be processed in one or more ways prior to storage or use, such that personally identifiable information is removed. For example, the identity of the user may be processed such that no personal identity information can be determined for the user, or the geographic location of the user may be summarized where location information is obtained (such as at a city, zip code, or state level) such that no particular location of the user can be determined. Thus, the user may have control over which information is collected by the user, how the information is used, and which information is provided to the user.
Fig. 1 is a block diagram of an example environment 100 in which digital content is distributed and provided for display on a client device 100. The example environment 100 includes a network 104. The network 104 may include a Local Area Network (LAN), a Wide Area Network (WAN), the Internet, or a combination thereof. The network 104 may also include any type of wired and/or wireless network, satellite network, cable network, Wi-Fi network, mobile communication network (e.g., 3G, 4G, etc.), or any combination thereof. Network 104 may utilize communication protocols, including packet-based and/or datagram-based protocols such as Internet Protocol (IP), Transmission Control Protocol (TCP), User Datagram Protocol (UDP), or other types of protocols. The network 104 may also include a plurality of devices, such as switches, routers, gateways, access points, firewalls, base stations, repeaters, or combinations thereof, that facilitate network communications and/or form the basis of network hardware.
The network 104 connects the client device 102, the content platform 106, the content provider 108, and the video processing system 110. The example environment 100 may include many different content platforms 106, video processing systems 110, client devices 102, and content providers 108.
In some implementations, the content platform 106 can store certain information about the client device (e.g., device preference information, content consumption information, etc.). Such user information may be used by the content platform to, for example, customize content provided to the client device 102 or enable convenient access to particular content frequently accessed by the client device 102. In some implementations, content platform 106 may not store such device information on the platform; however, content platform 106 may still provide such information for storage on a particular server (separate from the content platform). Thus, content platform 106 (also referred to herein as content platform/server 106 or simply server) refers to a content platform that stores such device information or a server (separate from the content platform) that stores such device information.
In some implementations, the content platform 106 is a video service through which users can view streaming video content. The video streamed to the user may include additional content (e.g., provided by the content provider 108) superimposed over the original video stream. It would generally be desirable to provide overlay content on an underlying video stream to provide additional content to viewers of the video stream and to increase the amount of content transmitted within the viewing area for a given video stream bandwidth. In addition to or instead of a video stream scene, the content platform 106 may include a video processor that processes video files to modify the video files to include overlay content, wherein the processed video files with overlay content are provided to the client device 102 for display on the client device 102.
However, there is a technical problem of determining how to place the overlay content so that it does not obscure important content in the underlying video. This is a particularly difficult problem in the context of overlaying content on a video, as the location of important content in a video can change rapidly over time. Thus, even if a particular location within a video is a good candidate for overlay content at one point in time (e.g., in one frame), that location may become a poor candidate for overlay content at a later point in time/in a subsequent frame (e.g., due to movement of a character within the video).
Using the location information of important content determined by the video processing system 110, the content platform 106 can overlay the content on top of the video stream while avoiding areas of the video screen with important content in the underlying video stream, e.g., areas of the original video stream containing faces, text, or salient objects (such as moving objects). The video processing system 110 can, for example, include a machine learning method and engine that can identify locations in a video that are unlikely to include important content, such that overlay content displayed at those locations is unlikely to obstruct important content in the underlying video. As described in more detail below, different types (e.g., categories) of video may include different types of important content. Thus, the video processing system 110 can prioritize the locations of the overlay content based on avoiding video features that are particularly important for the video category that has been determined for the video.
Further structural and operational aspects of these components of the example environment 100 are described with reference to fig. 3.
Fig. 2 shows an example video frame 200. The video frame 200 includes various features, such as a person 202 and various objects (e.g., a cup 204, a laptop 206, a sign 208, a text tag 209, and social media posts 210). In addition to text tag 209, some other objects included in video frame 200 may include other text. For example, the cup 204 has a text label, the laptop 206 has a sticker 212 that includes text, the placard 208 includes text 214, and the social media post 210 includes text 216.
When overlay content is displayed over a video frame 200, it may be desirable not to obscure important features displayed in the frame 200. Some features may be more important to the user than others. In some cases, the feature size may determine importance. For example, based on the face of the person 202 being larger than the face 218, the face of the person 202 may be more important in social media posts than the face 218. As another example, the face of person 202 may be more important than face 218 due to movement of person 202 during the video.
As another example, feature importance may be based on the category of the video. As described below with respect to fig. 3, the category of the video may be determined. For example, video frame 200 may be a frame from a talk show video, and the video may have a talk show or entertainment category. The person 202 may be, for example, a talk show presenter.
For talk show videos, certain types of features, such as a human face or torso, may be particularly important. Other types of features, such as text, may also be more important, but less important than human features. Talk show videos may give different types of importance to certain features than to other video categories. For example, for sports videos, static text such as scoreboards and moving objects (e.g., players) may be of highest importance. In sports (e.g., baseball) videos, small text items such as ball counts, hit counts, and out counts may be considered important, while for talk show videos, small text items such as text on the cup 204 may not be considered particularly important (although larger text items such as text 214 on the placard 208 or text 216 in the social media post 210 may still be of significant significance).
As described in more detail below, the video processing system 110 can determine one or more locations at which to display overlay content in a video to avoid obscuring features important to a video category (e.g., a talk show) of the video comprising the video frames 200 and based on detected locations of features important to the video type.
Fig. 3 is a block diagram of an example environment 300 in which the video processing system 110 determines a location within a video at which to display overlay content. The video processing system 110 may receive or access the input video 302. For example, the video processing system 110 may receive a video file or video stream, or a link to a video file or video stream. In some implementations, the video processing system 110 can process multiple video inputs in parallel (and generate a respective output for each video input).
In some implementations, the video processing system 110 includes a pre-processor 304. The preprocessor 304 may, for example, preprocess the input video 302 to provide consistency of one or more of frame rate, video size, video quality, video resolution, or video format. The output of the pre-processor 304 may be a video stream in a standard format for further processing by other sub-engines of the video processing system 110.
The video classifier 306 may identify a category of the input video 302 from a set of predefined video categories. The video categories may include entertainment, games, lifestyle, sports, knowledge, and society, to name a few. The video classifier 306 may determine the category of the input video 302 based on, for example, metadata of the input video 302. The metadata may include a video title or may include a video category or genre. The categories or genres included in the video metadata may be mapped to one of the predefined video categories. The metadata 302 may be received with the input video 302 or may be accessed by the video classifier 302 as part of a video classification. In some implementations, the video category is provided as input (e.g., with the input video 302) to the video processing system 110. Other types of video classification may be performed, such as techniques involving analyzing a subset of the frames of the input video 302 and determining a video category based on the content of the analyzed frames.
The video sampler 308 may capture a subset of the frames of the input video 302. For example, the video sampler 308 may determine a sampling rate, which may be, for example, a number of frames per second or a number of seconds per frame. For example, the video sampler 308 may capture three frames per second, one frame per second, or one frame per three seconds. The sampling rate may be determined based on various factors. In some cases, the sampling rate may be determined based on the video category. For example, some types of video content may be more dynamic (e.g., in terms of content changes between frames) than other types of content that are more static. For example, previous analysis may indicate that entertainment videos are generally more dynamic than knowledge videos. Thus, the sampling rate for entertainment video may be higher than the sampling rate for knowledge video.
In some cases, the sampling rate may be determined based on the processing power of the video processing system 110. For example, a particular instance of the video processing system 110 receiving a given input video 302 may have preconfigured processing capabilities. As another example, processing capacity may be determined dynamically, such as based on current (or recent) resource utilization. For example, recent resource utilization may be based on recent processing of other input videos by the video processing system 110. In some embodiments, the sampling rate may be a parameter that is configurable by an administrator. Reducing the sampling rate may lead to resource efficiency by reducing the number of frames processed.
The sampled frames may be processed by a set of feature detectors 310. The feature detector 310 includes a face detector 310a, a human joint detector 310b, a body contour detector 310c, a text detector 310d, a moving object detector 310e, and a color signal variance detector 310 f. In some embodiments, feature detectors 310 are run at least partially in parallel. Separate tasks may be initiated for each feature type, such as coordination and monitoring where the main task performs parallel tasks. Some or all of the feature detectors 310 may perform certain optimizations that are specific to a given feature detector 310. For example, for some (or all) types of feature detection, the frame image may be downsampled (e.g., lower resolution).
Each feature detector 310 may determine a confidence score for each location of each sampled frame, where a given confidence score indicates the confidence that a given location in a given frame includes a feature corresponding to the given feature detector 310. The confidence score may range from 0 to 1, for example, where 0 indicates the lowest confidence in the feature detection and 1 indicates the strongest confidence in the feature detection.
In some approaches, the detected features may be distinguished by limiting feature detection to larger features, i.e., features that are in the foreground and closer to the video viewpoint than the background features. For example, a large human face corresponding to a person in the foreground of the video may be detected by the face detector 310a, while a smaller human face corresponding to a person in the background of the video (such as a face in a crowd of people) may be excluded from detection by the face detector 310a or may receive a lower confidence score than the larger face.
The confidence scores may be stored in a confidence score map or matrix. For example, the matrix dimensions and addressing may correspond to the size and location of the video frames. For example, the location may correspond to a single pixel or a block of multiple pixels. For example, the location may be a 10 x 10 block of pixels. The block size may be a configurable or dynamically determined parameter. Larger or smaller block sizes may be configured based on, for example, processing power or desired accuracy or processing time. As mentioned above, the processing capability may be determined based on a pre-configured resource description or based on dynamic performance. For example, higher processing power or a higher level of desired precision may result in smaller block sizes. For example, lower processing power or less desired processing time may result in larger block sizes.
The face detector 310 may be configured to detect a human face in the sample frame. The face detector 310a may detect a blurred face, a side contour, and a closed-eye face. The face detector 310a may create a confidence map for each sampled frame where the confidence score for regions within the detected face are non-zero (where the higher the confidence of the detected face, the higher the non-zero confidence score) and the confidence score for regions of the frame outside the detected face are zero. The face detector 310a may be or include a computer vision system, such as a machine learning system, e.g., a bayesian image classifier or a Convolutional Neural Network (CNN) image classifier. For efficiency purposes, the face detector 310a may find frame locations where human faces appear in the frame without actually recognizing the identity of the people displayed within those locations (e.g., recognizing the faces of particular people displayed within those areas).
The human joint detector 310b, which may be or include a machine learning computer vision engine, may detect other important human body parts such as the neck, shoulders, elbows, wrists or hands. In some embodiments, certain body parts (such as those of the upper body) may be treated more importantly than lower body parts. In some cases, the human joint detector 310b is configured to weight the upper body part more importantly, while in other embodiments the human joint detector 310b may exclude the lower body part from detection. Once a joint is detected, an area around the joint (e.g., a circle) may be identified, and the area within the circle may be considered to include the location of the important human body part. The size of the circle may be based on the detected size of the person to which the joint belongs (e.g., a larger person in the frame may have a larger area of the circle around the detected joint). The size of the circle (or the ratio of the size of the person to the size of the circle) may be a configurable parameter. Different joints may have different sized circles compared to other types of joints. For example, a shoulder joint may be considered more important than an elbow joint, and thus a larger circle may be used for the shoulder than the elbow (or as another example, a shoulder joint may be represented by a larger circle due to being generally a larger joint than an elbow joint). The joint detector 310b may create a confidence map for each sampled frame where the confidence score for regions within the circle associated with the joint is non-zero (where the higher the confidence of the detected joint, the higher the non-zero confidence score) and the confidence score for regions of the frame outside the circle associated with the joint is zero.
The body contour detector 310c may determine the contour of the person detected in the sampled frame. The body contour detector 310c may be or include CNN. In some implementations, in addition to the face detector 310a and the human joint detector 310b, the body contour detector 310c can be used to identify any region of the frame that is occupied by a human body and not just important human body parts (such as faces or important joints). In some implementations, the body contour detector 310c generates a confidence map, but includes a score whose weighting is less than the high confidence score generated by the face detector 310a or the human joint detector 310 b. For example, the face detector 310a may score the location with the detected face with a confidence score of 1 for high confidence of the detected face, the human joint detector 310b may score the location with the detected joint with a confidence score of 0.8 for high confidence of the detected joint, and the body contour detector 310c may score the location within the body contour with a confidence score of 0.6. The net result of differing scoring weights between the face detector 310a, the human joint detector 310b, and the body contour detector 310c may be to process avoiding occlusion of important body parts according to body part priorities, where the face is treated most cautiously and important joints are treated more cautiously than body parts other than the face and important joints. Finally, whether to place the overlay content at a given frame location is based on the aggregated (and adjusted) confidence scores aggregated across the feature detectors 310, as described in more detail below.
The text detector 310d may detect text in the sample frame to avoid obscuring important text content, such as subtitles, game scores, or captions. Text detector 310d may detect text features within video frames, such as text appearing on product labels, signposts, on-screen whiteboards in school lecture videos, and so forth. The text detector 310d may detect text in multiple languages and may recognize various sizes of text or distorted text. Notably, the text features detected within the frame are part of the video stream itself, as compared to the overlay content that is separate from the video stream.
Other types of important features that a user may prefer not to be occluded may be moving objects. For example, moving objects are generally more likely to convey important content to a viewer than static objects, and thus are generally less suitable for being occluded by superimposed content. The moving object detector 310e may detect a moving object by detecting movement between adjacent sampling frames based on a color space difference between the frames. Processing (where the calculation yields a confidence score for the current frame) may be performed on each frame (except the first frame and the last frame) using the previous frame, the current frame, and the next frame. Moving object detection may include calculating an average color value of pixels within a frame location (e.g., when a location corresponds to multiple pixels), and calculating a euclidean difference between the average color value of the location and the average color values calculated for corresponding locations in the previous and next frames. A higher difference may correspond to a higher confidence score indicating the likelihood that the location corresponds to moving (and therefore important) content. The moving object detection may be based on the following equation:
Where S is all integer pairs i, j e [ x: x +10), [ y: y +10)
Wherein A is t RGB matrix representing a frame at time t
Wherein | M 0 [x,y,t 0 ]-M 0 [x,y,t 1 ]RGB euclidean distance between two 3 x 1 vectors where
And wherein i, j, C is the color value C at coordinate i, j.
Color variance may be another type of feature. The color variance between frames of a location may indicate important content, such as foreground content. Low or no color variance between frames may indicate less important content, such as background regions (e.g., walls, sky, etc.). The color signal variance detector 310f may determine the color variance between frames and calculate a confidence score for each location in each respective frame that indicates whether the location is subject to the color variance of the frame relative to the preceding and subsequent frame(s). As described above, the position with respect to the color variance position may be a pixel block. The color variance can be calculated according to the following formula:
where S is all integer pairs i, j e [ x, x +50 ], [ y, y +50)
Wherein A is t RGB matrix representing a frame at time t
The confidence value adjuster 312 may adjust the confidence value generated by the feature detector 310 based on the video category determined by the video classifier 306. The confidence value adjuster 312 may identify or determine a weight for each feature type based on the video category. As mentioned previously, some feature types are more important for some video categories. Each feature type weight of a category may be a value between 0 and 1, for example. A weight value of 1 may indicate that the feature type is important for the video category. A weight value of a feature type for a video category less than 1 (e.g., 0.5) may indicate that the feature type is less important for the video category. For example, for talk show video, the weight of a human facial feature may be 1 and the weight of a text feature may be 0.8. The sports video category may have a weight of 1 for moving object features, 1 for text features (e.g., for scoreboards or statistics), and 0.8 for human facial features. In some embodiments, the confidence value adjuster 312 uses a lookup table to retrieve the weight for a given category. The look-up table may store weight values for each of the various features for each category. The look-up table may include weight values that are pre-configured by an administrator, or the look-up table may include weight values that are dynamically determined or adjusted by a machine learning system that is trained to determine weight adjustments for different video categories.
The confidence value aggregator 314 may aggregate the adjusted confidence scores that have been adjusted by the confidence value adjuster 312 to generate an aggregated and adjusted confidence score for each location of each sample frame. The aggregated and adjusted confidence scores may be stored in a matrix for each sampled frame. For each location and for each sample frame, the aggregation may include summing the confidence values calculated for that location by each feature detector 310. The confidence value aggregator 314 may limit the sum to 1 if the sum is greater than 1 for a position of the sample frame. Thus, the aggregated and adjusted confidence score may have a value between 0 and 1, where a value of 1 indicates the highest confidence of the significant feature at a location and a value of 0 indicates the lowest confidence of the significant feature at that location.
The confidence value aggregator 314 (or in some embodiments, the confidence value adjuster 312) may further adjust the confidence score to improve the confidence score at a location at or near the center of the frame. Based on the general understanding that important content of a video is likely to occupy the center region of a frame (and that overlay content placed in the center may result in too much distraction to the user from the main content of the video), center-based adjustments may be made to make it less likely that a center position will be selected for the overlay content. For example, a constant two-dimensional gaussian distribution may be applied as a multiplier to the aggregated and adjusted confidence score, e.g., according to the following formula:
Wherein x, y e [ -width, width), [ -height, height)
The aggregated and adjusted confidence scores may be used for various purposes. For example, in some implementations and as shown below with respect to fig. 4, the visualizer 316 may generate and present a heat map for each of the one or more sampled frames. The heat map of the frame may display confidence values that appear as different colors. For example, red may indicate a high confidence value and green may indicate a low confidence value. As another example, a heat map of a frame may be combined with (e.g., superimposed on) a frame image and presented as a merged image, as shown below with respect to fig. 5. The separate and/or superimposed heatmaps may be viewed by an administrator or developer for monitoring, troubleshooting, or debugging purposes, for example.
As another example, in some implementations, the aggregated and adjusted confidence scores may be used to calculate a quality score for one or more existing (e.g., in-use) content slots (slots) currently used to display the overlay content on the input video 302. Whether to continue displaying the overlay content in the existing content slot may be determined based on the calculated quality score of the existing content slot (e.g., a higher quality score may indicate that the existing content slot is suitable for displaying the overlay content, while a lower quality score may indicate that the existing content slot is not ideal for displaying the overlay content).
The aggregated and adjusted confidence scores may be used by the overlay location identifier 318 to determine one or more outputs 320 based on, for example, one or more overlay content and size inputs 322 received as inputs, each output 320 including a recommended overlay content location 320a and a corresponding time offset 320b for the input video 302. Each overlay content and size input 322 can specify a desired size and display duration for an overlay content item to be overlaid on the input video 302 for a specified display duration.
The detected features (as represented by the aggregated and adjusted confidence scores) may be located at different locations within different frames that occupy a particular duration of the input video 302. The overlay content location 320a may be determined such that overlay content that begins to be displayed at the time offset 320b for a specified duration including multiple frames is not located anywhere the detected feature spans the multiple frames. The identified overlay content location 320a may be a location that is generally or substantially outside of a location that includes the identified feature in a series of video frames corresponding to the input duration. The location that includes the identified feature may be a location that should not be occluded (if possible) by the overlay content (e.g., to ensure that important content of the underlying input video 302 is not occluded). As another example, the overlay content location 320a may be a location corresponding to a confidence score below a predetermined threshold.
The overlay position identifier 318 may determine candidate overlay content slots for a given overlay content item size and duration (e.g., a content item duration corresponding to a particular number of consecutive/contiguous video frames in the input video 302 based on the content item size and the content item duration). The overlay position identifier 318 may determine a quality score for each candidate slot for each frame in the series for each of the different series of consecutive frames corresponding to the desired duration. The quality score of a candidate slot of a frame may be the sum of the aggregated and adjusted confidence scores for all positions located within the candidate slot. The total quality score may be calculated for a series of candidate slots by calculating the sum of all quality scores for the candidate slots of all frames in the series. The series may be identified by a time offset of the first frame of the series. A set of one or more highest ranked overall quality scores may be identified.
Based on the one or more highest ranked overall quality scores, the overlay location identifier 318 may identify one or more outputs 320, where each output 320 includes an overlay content location 320a (e.g., location and size) of a candidate slot having one of the highest ranked quality scores, and a corresponding time offset 320b indicating a starting time offset at which the overlay content item is to be displayed.
Output(s) 320 may be provided to, for example, content platform 106 to enable content platform 106 to display the overlay content item(s) corresponding to the output(s) at the recommended overlay content location(s) 320a, starting at the corresponding time offset(s) 320b during playback of input video 302. As another example, the output(s) 320 may be provided to the visualizer 316, such as during development or troubleshooting, and the visualizer 316 may present, for example, an overlay content outline (or, in some cases, actual overlay content) for monitoring or troubleshooting purposes, e.g., for viewing by an administrator or developer.
Fig. 4 illustrates an example heatmap 400. The heat map 400 that may be generated by the visualizer 316 of fig. 3 may be used to visualize the aggregated and adjusted confidence scores generated by the confidence value aggregator 314. For example, the heatmap 400 may be used by an administrator or developer for troubleshooting or informative purposes. The heat map 400 is a visualization of the matrix of aggregated and adjusted confidence scores for each sampled video frame 200 described above with respect to fig. 2.
Different colors may be used to display different confidence levels. For example, red may indicate a high confidence value and green may indicate a low confidence value. The red-colorable regions 402a-402f of the heat map 400 correspond to high aggregated and adjusted confidence scores that are based on the confidence scores generated by the face detector 310a, the human joint detector 310b, and/or the body contour detector 310 c. The regions 402a-402f may have a red color (e.g., corresponding to a high confidence score) based on the adjustment/weighting performed by the confidence value adjuster 312 according to the talk show video category determined by the video classifier 306. Regions 404a-404e of heat map 400 may also be colored in red (e.g., shaded in the same or lighter red (s)) based on a (relatively) high aggregated and adjusted confidence score corresponding to text detector 310d detecting text on text label 209, social media post 210, cup 204, sticker 212, and sign 208. Other regions of the heat map 400, such as region 406 and region 408, may be colored in green (e.g., green shading) to reflect a lower aggregated and adjusted confidence score. As described above and shown in fig. 5, the overlay location identifier 318 may determine the location(s) corresponding to a low confidence score that persists across a group of frames for displaying the location of the overlay content for a group of frames equal to the desired duration.
Fig. 5 illustrates an example confidence score visualization 500. The confidence score visualization 500 illustrates the presentation of at least a portion of the heat map 400 of fig. 4 over the video frame 200 of fig. 2. As with the heat map 400, a confidence score visualization 500 may optionally be generated (e.g., by the visualizer 316 of fig. 3) for troubleshooting, monitoring, or other informative purposes. For example, the visualization 500 includes a colored region 502a corresponding to a feature of the person 202 that has been identified by the face detector 310a, the human joint detector 310b, and/or the body contour detector 310 c. Colored region 502b corresponds to the detection of face 202 in social media text 210. The shaded text regions 504a-504e correspond to the text detector 310d detecting the text label 209, the text 216 on the social media post 210, the text on the cup 204, the text on the sticker 212, and the text 214 on the sign 208. Other colored regions may be displayed, or colored regions corresponding to confidence scores above a predetermined threshold may be displayed (e.g., so that only the region with the highest confidence score is displayed).
A content slot 506 located outside of the colored region corresponding to the high confidence score corresponds to a recommended position for displaying the overlaid content item for a specified duration beginning with (or at least including) the time offset of the talk show video corresponding to the video frame 200. The content slot 506 may have a size equal to the desired overlay content size. For informational purposes, a quality score 508 of the content slot 506 is displayed, which may correspond to the highest ranked quality score of the frame 200 determined by the overlay position identifier 318. As mentioned, the location information describing the location of the content slot 506 and the time offset corresponding to the frame 200 may be provided to the content platform 106 for displaying the overlay content item at that location, for example, when the end user selects to view a talk show video.
Although a single frame and a single content slot visualization are shown, multiple frame visualizations are generated (e.g., as a collection of still images or as a modified video stream) and displayed to an administrator, developer, or content operator, for example, by visualizer 216. Further, each frame visualization may include a presentation of more than one candidate content slot, such as the top N-ranked content slot identified for the current frame.
FIG. 6 is a flow diagram of an example process 600 for determining a location within a video at which overlay content will be displayed. The operations of process 600 are described below as being performed by components of the systems described and depicted in fig. 1 and 3. The operation of process 600 is described below for illustrative purposes only. The operations of process 600 may be performed by any suitable device or system (e.g., any suitable data processing apparatus). The operations of process 600 may also be implemented as instructions stored on a computer-readable medium, which may be non-transitory. Execution of the instructions causes one or more data processing apparatus to perform the operations of process 600.
The video processing system 110 identifies (at 602) a video category for the video with which the overlay content is to be displayed (e.g., on which the overlay content is to be displayed) from a set of predefined video categories for the video. For example, as described above with reference to fig. 3, the video classifier 306 may identify a video category of the input video 302. The video categories may include entertainment, games, lifestyle, sports, knowledge and society, and other categories.
The video processing system 110 performs processing (at 604) on each video frame in a set of sampled video frames of the video. For example, the video processing system 110 determines a confidence score for each video feature type in a set of video feature types and for each of a plurality of locations in a video frame, the confidence score indicating a likelihood that the location in the video frame includes a feature of the video feature type (at 606). The video feature types may include human faces, human torso, text, moving objects, or changes in color variance. For example, as described above with reference to fig. 3, each of the feature detectors 310 including the face detector 310a, the human joint detector 310b, the body contour detector 310c, the text detector 310d, the moving object detector 310e, and the color signal variance detector 310f may determine a confidence score for each location of each frame. The location within the frame may be a pixel or a block of pixels. At least some confidence scores for different video feature types may be determined in parallel. For example, two or more of the feature detectors 310 may be run in parallel.
The video processing system 110 determines a weight for each video feature type based on the video category that reflects the importance of not obscuring the video features of that video feature type when displaying video of that video category (at 608). For example, as described above with reference to fig. 3, the confidence value adjuster 312 may determine a weight for each feature type of the features associated with the feature detector 310 based on the video category determined by the video classifier 306.
The video processing system 110 adjusts the confidence scores for the plurality of locations in the video frame for each video feature type in the set of video feature types based on the weights determined for that video feature type to generate adjusted confidence scores (at 610). For example, as described above with reference to fig. 3, the confidence value adjuster 312 adjusts the confidence scores determined by the feature detector 310 based on the corresponding weights of the video feature types determined from the video categories determined by the video classifier 306. In some implementations, the confidence score for a location in the video frame within a predefined proximity to the center of the video frame can be further adjusted (e.g., boosted) to prevent the overlay content from being positioned at the center of the frame or appended.
The video processing system 110 aggregates the adjusted confidence scores for each location of each video frame in the set of sampled video frames to generate an aggregated and adjusted confidence score (at 612). For example, as described above with reference to fig. 3, the confidence value aggregator 314 aggregates the adjusted confidence scores produced by the confidence score adjuster 312 to generate an aggregated and adjusted confidence score.
The video processing system 110 determines (at 614) a location at which the overlay content will be displayed during the video display based on the aggregated and adjusted confidence scores. For example, as described above with reference to fig. 3, the overlay position identifier 312 may determine the overlay content position 320a and the time offset 320b of the input video 302 based on the size of the overlay content and the desired display duration, beginning to display the overlay content at the time offset 320b as an overlay over the input video 302. Determining the location at which the overlay content will be displayed during the video display may include determining the location based on a specified size of the overlay content and a frame duration or number of frames within which the overlay content will be provided in the video. For example, with respect to fig. 3, the overlay location identifier may determine the overlay location based on the overlay content size and duration input 322.
The video processing system 110 provides the overlay content for display at the determined location in the video (at 616). For example, as described above with reference to fig. 3, the video processing system 110 may provide the overlay content position 320a and the corresponding time offset 320b of the overlay content item (in some implementations, the overlay content item itself) to the content platform 106 to cause the content platform 106 to present the overlay content item at the overlay content position 320a as an overlay on the input video 302 starting at the time offset 320 b.
Thus, in this manner, the video processing system 110 and/or the content platform 106 can display the overlay content item over the video content without obscuring important video content and importantly avoiding content that is most important to the video of the video category determined for the video content.
FIG. 7 is a block diagram of an example computer system 700 that may be used to perform the operations described above. The system 700 includes a processor 710, a memory 720, a storage device 730, and an input/output device 740. Each of the components 710, 720, 730, and 740 can be interconnected, for example, using a system bus 750. Processor 710 is capable of processing instructions for execution within system 700. In some implementations, the processor 710 is a single-threaded processor. In another implementation, the processor 710 is a multi-threaded processor. The processor 710 is capable of processing instructions stored in the memory 720 or on the storage device 730.
Memory 720 stores information within system 700. In one implementation, the memory 720 is a computer-readable medium. In some implementations, the memory 720 is a volatile memory unit or units. In another implementation, the memory 720 is a non-volatile memory unit or units.
The storage device 730 is capable of providing mass storage for the system 700. In some implementations, the storage device 730 is a computer-readable medium. In various different implementations, the storage device 730 may include, for example, a hard disk device, an optical disk device, a storage device shared by multiple computing devices over a network (e.g., a cloud storage device), or some other mass storage device.
Input/output device 740 provides input/output operations for system 700. In some implementations, the input/output devices 740 can include one or more of a network interface device, such as an ethernet card, a serial communication device (e.g., an RS-232 port), and/or a wireless interface device (e.g., an 802.11 card). In another embodiment, the input/output devices may include driver devices, such as keyboards, printers, and display devices, configured to receive input data and transmit output data to the peripheral devices 760. However, other implementations may also be used, such as mobile computing devices, mobile communication devices, set-top box television client devices, and so forth.
Although an example processing system has been described in fig. 7, implementations of the subject matter and the functional operations described in this specification can be implemented in other types of digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
Embodiments of the subject matter and the operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions, encoded on multiple computer storage media (or one computer storage medium) for execution by, or to control the operation of, data processing apparatus. Alternatively or additionally, program instructions may be encoded on an artificially generated propagated signal (e.g., a machine-generated electrical, optical, or electromagnetic signal) that is generated to encode information for transmission to suitable receiver apparatus for execution by data processing apparatus. The computer storage medium may be or be included in a computer-readable storage device, a computer-readable storage substrate, a random or serial access memory array or device, or a combination of one or more of them. Further, although the computer storage medium is not a propagated signal, the computer storage medium can be a source or destination of computer program instructions encoded in an artificially generated propagated signal. The computer storage medium may also be, or be included in, one or more separate physical components or media (e.g., multiple CDs, disks, or other storage devices).
The operations described in this specification may be implemented as operations performed by data processing apparatus on data stored on one or more computer-readable storage devices or received from other sources.
The term "data processing apparatus" encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, a system on a chip, or a plurality or combination of the foregoing. The apparatus can comprise special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). The apparatus can include, in addition to hardware, code that creates a runtime environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, a cross-platform runtime environment, a virtual machine, or a combination of one or more of them. The apparatus and operating environment can implement a variety of different computing model infrastructures, such as web services, distributed computing and grid computing infrastructures.
A computer program (also known as a program, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment. A computer program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code). A computer program can be deployed to be run on one computer or on multiple computers at one site or distributed across multiple sites and interconnected by a communication network.
The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform actions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors. Generally, a processor will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a processor for performing actions in accordance with the instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data (e.g., magnetic, magneto-optical disks, or optical disks). However, a computer need not have such devices. Further, the computer may be embedded in another device, e.g., a mobile phone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device (e.g., a Universal Serial Bus (USB) flash drive), to name a few. Devices suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including: for example, semiconductor memory devices, such as EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device (e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor) for displaying information to the user and a keyboard and a pointing device (e.g., a mouse or a trackball) by which the user can provide input to the computer. Other kinds of devices may also be used to provide for interaction with the user; for example, feedback provided to the user can be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback; input from the user may be received in any form, including acoustic, speech, or tactile input. Further, the computer may interact with the user by sending and receiving documents to and from the device used by the user; for example, by sending a web page to a web browser on a user's client device in response to a request received from the web browser.
Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface or a web browser through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back end, middleware, or front end components. The components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include local area networks ("LANs") and wide area networks ("WANs"), the internet (e.g., the internet), and peer-to-peer networks (e.g., ad hoc peer-to-peer networks).
The computing system may include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, the server sends data (e.g., an HTML page) to the client device (e.g., for displaying data to a user interacting with the client device and receiving user input from the user). Data generated at the client device (e.g., a result of the user interaction) may be received at the server from the client device.
While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any invention or of what may be claimed, but rather as descriptions of features specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous. Moreover, the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
Thus, particular embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. In some cases, the actions recited in the claims can be performed in a different order and still achieve desirable results. Further, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some embodiments, multitasking and parallel processing may be advantageous.
Claims (20)
1. A computer-implemented method, comprising:
identifying, for a video with which overlay content is to be displayed, a video category of the video from a set of predefined video categories;
for each video frame in a set of sampled video frames of the video:
determining, for each video feature type in a set of video feature types and for each location in a plurality of locations in the video frame, a confidence score indicating a likelihood that the location in the video frame includes a feature of the video feature type;
determining a weight for each video feature type based on the video category, the weight reflecting an importance of not obscuring video features of the video feature type when displaying video of the video category; and
for each video feature type in the set of video feature types, adjusting the confidence scores for the plurality of locations in the video frame based on the weights determined for the video feature type to generate an adjusted confidence score;
aggregating the adjusted confidence scores for each location of each video frame in the set of sampled video frames to generate an aggregated and adjusted confidence score;
determining a location at which the overlay content is to be placed during the video display based on the aggregated and adjusted confidence scores; and
providing overlay content for display at the determined location in the video.
2. The computer-implemented method of claim 1, wherein a video feature type comprises a change in a human face, a human torso, text, a moving object, or a color variance.
3. The computer-implemented method of claim 1 or 2, wherein at least some confidence scores for different video feature types are determined in parallel.
4. The computer-implemented method of any preceding claim, wherein adjusting the confidence scores for the plurality of locations in the video frame for each video feature type in the set of video feature types comprises adjusting the confidence scores for locations in the video frame that are within a predefined proximity of a center of the video frame.
5. The computer-implemented method of any preceding claim, wherein determining a location at which the overlay content is to be placed during the video display based on the aggregated and adjusted confidence scores further comprises determining a location at which the overlay content is to be placed during the video display based on a specified size of the overlay content and a frame duration or number of frames within which the overlay content is to be provided within the video.
6. The computer-implemented method of any preceding claim, wherein determining a location at which the overlay content is to be placed comprises determining a plurality of locations, and providing the overlay content at the determined location comprises providing the overlay content for display at one of the plurality of locations.
7. The computer-implemented method of any preceding claim, further comprising sampling, by the video processing system, the video based on a sampling rate to obtain the set of sampled video frames, wherein the sampling rate is determined based on processing capabilities of the video processing system.
8. A system, comprising:
one or more memory devices storing instructions; and
one or more data processing apparatus configured to interact with the one or more memory devices and, when executing instructions, perform operations comprising:
identifying, for a video with which overlay content is to be displayed, a video category of the video from a set of predefined video categories;
for each video frame in a set of sampled video frames of the video:
determining, for each video feature type in a set of video feature types and for each location in a plurality of locations in the video frame, a confidence score indicating a likelihood that the location in the video frame includes a feature of the video feature type;
determining a weight for each video feature type based on the video category, the weight reflecting an importance of not obscuring video features of the video feature type when displaying video of the video category; and
for each video feature type in the set of video feature types, adjusting the confidence scores for the plurality of locations in the video frame based on the weights determined for the video feature type to generate an adjusted confidence score; aggregating the adjusted confidence scores for each location of each video frame in the set of sampled video frames to generate an aggregated and adjusted confidence score;
determining a location at which the overlay content is to be placed during the video display based on the aggregated and adjusted confidence scores; and
providing overlay content for display at the determined location in the video.
9. The system of claim 8, wherein the video feature type comprises a human face, a human torso, text, a moving object, or a change in color variance.
10. The system of claim 8 or 9, wherein at least some confidence scores for different video feature types are determined in parallel.
11. The system of any preceding claim, wherein adjusting the confidence scores for the plurality of locations in the video frame for each video feature type in the set of video feature types comprises adjusting the confidence scores for locations in the video frame that are within a predefined proximity of a center of the video frame.
12. The system of any preceding claim, wherein determining a location at which the overlay content is to be placed during the video display based on the aggregated and adjusted confidence scores further comprises determining a location at which the overlay content is to be placed during the video display based on a specified size of the overlay content and a frame duration or number of frames within which the overlay content is to be provided within the video.
13. The system of any preceding claim, wherein determining a location at which the overlay content is to be placed comprises determining a plurality of locations, and providing the overlay content at the determined location comprises providing the overlay content for display at one of the plurality of locations.
14. The system of any preceding claim, wherein the operations further comprise sampling, by the video processing system, the video based on a sampling rate to obtain the set of sampled video frames, wherein the sampling rate is determined based on a processing capability of the video processing system.
15. A computer-readable medium storing instructions that, when executed by one or more data processing apparatus, cause the one or more data processing apparatus to perform operations comprising:
identifying, for a video with which overlay content is to be displayed, a video category of the video from a set of predefined video categories;
for each video frame in a set of sampled video frames of the video:
determining, for each video feature type in a set of video feature types and for each location in a plurality of locations in the video frame, a confidence score indicating a likelihood that the location in the video frame includes a feature of the video feature type;
determining a weight for each video feature type based on the video category, the weight reflecting an importance of not obscuring video features of the video feature type when displaying video of the video category; and
for each video feature type in the set of video feature types, adjusting the confidence scores for the plurality of locations in the video frame based on the weights determined for the video feature type to generate an adjusted confidence score;
aggregating the adjusted confidence scores for each location of each video frame in the set of sampled video frames to generate an aggregated and adjusted confidence score;
determining a location at which the overlay content is to be placed during the video display based on the aggregated and adjusted confidence scores; and
providing overlay content for display at the determined location in the video.
16. The computer-readable medium of claim 15, wherein the video feature type comprises a change in a human face, a human torso, text, a moving object, or a color variance.
17. The computer readable medium of claim 15 or 16, wherein at least some confidence scores for different video feature types are determined in parallel.
18. The computer-readable medium of any preceding claim, wherein adjusting the confidence scores for the plurality of locations in the video frame for each video feature type in the set of video feature types comprises adjusting the confidence scores for locations in the video frame that are within a predefined proximity of a center of the video frame.
19. The computer-readable medium of any preceding claim, wherein determining a location at which the overlay content is to be placed during the video display based on the aggregated and adjusted confidence score further comprises determining a location at which the overlay content is to be placed during the video display based on a specified size of the overlay content and a frame duration or number of frames within which the overlay content is to be provided within the video.
20. The computer-readable medium of any preceding claim, wherein determining a location at which the overlay content is to be placed comprises determining a plurality of locations, and providing the overlay content at the determined location comprises providing the overlay content for display at one of the plurality of locations.
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2020/058087 WO2022093252A1 (en) | 2020-10-30 | 2020-10-30 | Non-occluding video overlays |
Publications (1)
Publication Number | Publication Date |
---|---|
CN114902649A true CN114902649A (en) | 2022-08-12 |
Family
ID=73544355
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202080060945.6A Pending CN114902649A (en) | 2020-10-30 | 2020-10-30 | Non-occlusion video overlay |
Country Status (6)
Country | Link |
---|---|
US (2) | US11758216B2 (en) |
EP (1) | EP4011061A1 (en) |
JP (1) | JP7367187B2 (en) |
KR (1) | KR102625760B1 (en) |
CN (1) | CN114902649A (en) |
WO (1) | WO2022093252A1 (en) |
Families Citing this family (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20220207804A1 (en) * | 2020-12-30 | 2022-06-30 | Snap Inc. | Automated content curation for generating composite augmented reality content |
US20230140042A1 (en) * | 2021-11-04 | 2023-05-04 | Tencent America LLC | Method and apparatus for signaling occlude-free regions in 360 video conferencing |
Family Cites Families (12)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2006041886A (en) | 2004-07-27 | 2006-02-09 | Sony Corp | Information processor and method, recording medium, and program |
US20090016449A1 (en) | 2007-07-11 | 2009-01-15 | Gene Cheung | Providing placement information to a user of a video stream of content to be overlaid |
KR101153262B1 (en) * | 2009-08-13 | 2012-06-05 | 주식회사 케이티 | System and method for inserting/extracting watermark in video contents by piling on each other |
US9111287B2 (en) * | 2009-09-30 | 2015-08-18 | Microsoft Technology Licensing, Llc | Video content-aware advertisement placement |
JP5465620B2 (en) | 2010-06-25 | 2014-04-09 | Ｋｄｄｉ株式会社 | Video output apparatus, program and method for determining additional information area to be superimposed on video content |
US9467750B2 (en) * | 2013-05-31 | 2016-10-11 | Adobe Systems Incorporated | Placing unobtrusive overlays in video content |
EP3236655A1 (en) | 2014-02-07 | 2017-10-25 | Sony Interactive Entertainment America LLC | Scheme for determining the locations and timing of advertisements and other insertions in media |
WO2016012875A1 (en) | 2014-07-23 | 2016-01-28 | Comigo Ltd. | Reducing interference of an overlay with underlying content |
JP6352126B2 (en) | 2014-09-17 | 2018-07-04 | ヤフー株式会社 | Advertisement display device, advertisement display method, and advertisement display program |
US10706889B2 (en) * | 2016-07-07 | 2020-07-07 | Oath Inc. | Selective content insertion into areas of media objects |
JP7118966B2 (en) * | 2016-12-13 | 2022-08-16 | ロヴィ ガイズ， インコーポレイテッド | Systems and methods for minimizing obstruction of media assets by overlays by predicting the path of movement of an object of interest of the media asset and avoiding placement of overlays in the path of movement |
KR20200076968A (en) * | 2018-12-20 | 2020-06-30 | 주식회사 케이티 | Contents providing server, user device and method for mapping overlay contents and location related contents |
-
2020
- 2020-10-30 WO PCT/US2020/058087 patent/WO2022093252A1/en unknown
- 2020-10-30 CN CN202080060945.6A patent/CN114902649A/en active Pending
- 2020-10-30 US US17/637,355 patent/US11758216B2/en active Active
- 2020-10-30 KR KR1020227006392A patent/KR102625760B1/en active IP Right Grant
- 2020-10-30 JP JP2022512820A patent/JP7367187B2/en active Active
- 2020-10-30 EP EP20811874.5A patent/EP4011061A1/en active Pending
-
2023
- 2023-08-03 US US18/230,028 patent/US20240007703A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
WO2022093252A1 (en) | 2022-05-05 |
KR20220058892A (en) | 2022-05-10 |
JP7367187B2 (en) | 2023-10-23 |
JP2023503764A (en) | 2023-02-01 |
US11758216B2 (en) | 2023-09-12 |
US20240007703A1 (en) | 2024-01-04 |
US20220368979A1 (en) | 2022-11-17 |
EP4011061A1 (en) | 2022-06-15 |
KR102625760B1 (en) | 2024-01-16 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US10810434B2 (en) | Movement and transparency of comments relative to video frames | |
US20240007703A1 (en) | Non-occluding video overlays | |
EP3516882B1 (en) | Content based stream splitting of video data | |
US9892324B1 (en) | Actor/person centric auto thumbnail | |
US20160360267A1 (en) | Process for increasing the quality of experience for users that watch on their terminals a high definition video stream | |
CN108010037B (en) | Image processing method, device and storage medium | |
US11416546B2 (en) | Content type detection in videos using multiple classifiers | |
US20130223537A1 (en) | Video Bit Stream Transmission System | |
US20140023341A1 (en) | Annotating General Objects in Video | |
EP3955584A1 (en) | Digital media system | |
CN113965777A (en) | Method and system for combining digital video content | |
US11049273B2 (en) | Systems and methods for generating a visibility counts per pixel of a texture atlas associated with a viewer telemetry data | |
US20230206632A1 (en) | Computerized system and method for fine-grained video frame classification and content creation therefrom | |
US20220353435A1 (en) | System, Device, and Method for Enabling High-Quality Object-Aware Zoom-In for Videos | |
US20220417586A1 (en) | Non-occluding video overlays | |
US20150382065A1 (en) | Method, system and related selection device for navigating in ultra high resolution video content | |
JP6623905B2 (en) | Server device, information processing method and program | |
US11115718B2 (en) | Virtual reality imaging system | |
CN117809001A (en) | VR-based stadium management event viewing method, device and equipment | |
CN113793410A (en) | Video processing method and device, electronic equipment and storage medium | |
EP2071511A1 (en) | Method and device for generating a sequence of images of reduced size |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |