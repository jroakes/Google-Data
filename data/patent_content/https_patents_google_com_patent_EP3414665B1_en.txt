EP3414665B1 - Profiling cache replacement - Google Patents
Profiling cache replacement Download PDFInfo
- Publication number
- EP3414665B1 EP3414665B1 EP16826601.3A EP16826601A EP3414665B1 EP 3414665 B1 EP3414665 B1 EP 3414665B1 EP 16826601 A EP16826601 A EP 16826601A EP 3414665 B1 EP3414665 B1 EP 3414665B1
- Authority
- EP
- European Patent Office
- Prior art keywords
- pages
- page
- memory
- cache
- main memory
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 230000015654 memory Effects 0.000 claims description 473
- 238000000034 method Methods 0.000 claims description 79
- 238000013508 migration Methods 0.000 claims description 9
- 230000005012 migration Effects 0.000 claims description 9
- 238000010586 diagram Methods 0.000 description 20
- 238000004891 communication Methods 0.000 description 10
- 238000007796 conventional method Methods 0.000 description 8
- 238000007726 management method Methods 0.000 description 7
- 230000008569 process Effects 0.000 description 6
- 230000004888 barrier function Effects 0.000 description 3
- 230000003247 decreasing effect Effects 0.000 description 3
- 238000012545 processing Methods 0.000 description 3
- 230000009471 action Effects 0.000 description 2
- 230000008859 change Effects 0.000 description 2
- 238000013500 data storage Methods 0.000 description 2
- 230000000670 limiting effect Effects 0.000 description 2
- 238000013507 mapping Methods 0.000 description 2
- 230000002829 reductive effect Effects 0.000 description 2
- 230000003466 anti-cipated effect Effects 0.000 description 1
- 230000006399 behavior Effects 0.000 description 1
- 230000000903 blocking effect Effects 0.000 description 1
- 230000001413 cellular effect Effects 0.000 description 1
- 238000004883 computer application Methods 0.000 description 1
- 238000004590 computer program Methods 0.000 description 1
- 230000000977 initiatory effect Effects 0.000 description 1
- 230000002452 interceptive effect Effects 0.000 description 1
- 230000007246 mechanism Effects 0.000 description 1
- 238000012544 monitoring process Methods 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 230000002093 peripheral effect Effects 0.000 description 1
- 230000002085 persistent effect Effects 0.000 description 1
- 230000008054 signal transmission Effects 0.000 description 1
- 230000003068 static effect Effects 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 238000013519 translation Methods 0.000 description 1
- 230000001960 triggered effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/0223—User address space allocation, e.g. contiguous or non contiguous base addressing
- G06F12/0284—Multiple user address space allocation, e.g. using different base addresses
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
- G06F12/0802—Addressing of a memory level in which the access to the desired data or data block requires associative addressing means, e.g. caches
- G06F12/0804—Addressing of a memory level in which the access to the desired data or data block requires associative addressing means, e.g. caches with main memory updating
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
- G06F12/0802—Addressing of a memory level in which the access to the desired data or data block requires associative addressing means, e.g. caches
- G06F12/0844—Multiple simultaneous or quasi-simultaneous cache accessing
- G06F12/0855—Overlapped cache accessing, e.g. pipeline
- G06F12/0859—Overlapped cache accessing, e.g. pipeline with reload from main memory
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
- G06F12/0802—Addressing of a memory level in which the access to the desired data or data block requires associative addressing means, e.g. caches
- G06F12/0866—Addressing of a memory level in which the access to the desired data or data block requires associative addressing means, e.g. caches for peripheral storage systems, e.g. disk cache
- G06F12/0868—Data transfer between cache memory and other subsystems, e.g. storage devices or host systems
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
- G06F12/0802—Addressing of a memory level in which the access to the desired data or data block requires associative addressing means, e.g. caches
- G06F12/0893—Caches characterised by their organisation or structure
- G06F12/0897—Caches characterised by their organisation or structure with two or more cache hierarchy levels
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
- G06F12/12—Replacement control
- G06F12/121—Replacement control using replacement algorithms
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
- G06F12/12—Replacement control
- G06F12/121—Replacement control using replacement algorithms
- G06F12/122—Replacement control using replacement algorithms of the least frequently used [LFU] type, e.g. with individual count value
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
- G06F12/12—Replacement control
- G06F12/121—Replacement control using replacement algorithms
- G06F12/123—Replacement control using replacement algorithms with age lists, e.g. queue, most recently used [MRU] list or least recently used [LRU] list
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/06—Digital input from, or digital output to, record carriers, e.g. RAID, emulated record carriers or networked record carriers
- G06F3/0601—Interfaces specially adapted for storage systems
- G06F3/0602—Interfaces specially adapted for storage systems specifically adapted to achieve a particular effect
- G06F3/0604—Improving or facilitating administration, e.g. storage management
- G06F3/0605—Improving or facilitating administration, e.g. storage management by facilitating the interaction with a user or administrator
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/06—Digital input from, or digital output to, record carriers, e.g. RAID, emulated record carriers or networked record carriers
- G06F3/0601—Interfaces specially adapted for storage systems
- G06F3/0602—Interfaces specially adapted for storage systems specifically adapted to achieve a particular effect
- G06F3/061—Improving I/O performance
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/06—Digital input from, or digital output to, record carriers, e.g. RAID, emulated record carriers or networked record carriers
- G06F3/0601—Interfaces specially adapted for storage systems
- G06F3/0628—Interfaces specially adapted for storage systems making use of a particular technique
- G06F3/0638—Organizing or formatting or addressing of data
- G06F3/064—Management of blocks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/06—Digital input from, or digital output to, record carriers, e.g. RAID, emulated record carriers or networked record carriers
- G06F3/0601—Interfaces specially adapted for storage systems
- G06F3/0628—Interfaces specially adapted for storage systems making use of a particular technique
- G06F3/0646—Horizontal data movement in storage systems, i.e. moving data in between storage devices or systems
- G06F3/0647—Migration mechanisms
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/06—Digital input from, or digital output to, record carriers, e.g. RAID, emulated record carriers or networked record carriers
- G06F3/0601—Interfaces specially adapted for storage systems
- G06F3/0628—Interfaces specially adapted for storage systems making use of a particular technique
- G06F3/0662—Virtualisation aspects
- G06F3/0665—Virtualisation aspects at area level, e.g. provisioning of virtual or logical volumes
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/06—Digital input from, or digital output to, record carriers, e.g. RAID, emulated record carriers or networked record carriers
- G06F3/0601—Interfaces specially adapted for storage systems
- G06F3/0668—Interfaces specially adapted for storage systems adopting a particular infrastructure
- G06F3/0671—In-line storage system
- G06F3/0673—Single storage device
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
- G06F12/12—Replacement control
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2212/00—Indexing scheme relating to accessing, addressing or allocation within memory systems or architectures
- G06F2212/10—Providing a specific technical effect
- G06F2212/1016—Performance improvement
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2212/00—Indexing scheme relating to accessing, addressing or allocation within memory systems or architectures
- G06F2212/10—Providing a specific technical effect
- G06F2212/1016—Performance improvement
- G06F2212/1021—Hit rate improvement
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2212/00—Indexing scheme relating to accessing, addressing or allocation within memory systems or architectures
- G06F2212/15—Use in a specific computing environment
- G06F2212/152—Virtualized environment, e.g. logically partitioned system
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2212/00—Indexing scheme relating to accessing, addressing or allocation within memory systems or architectures
- G06F2212/25—Using a specific main memory architecture
- G06F2212/251—Local memory within processor subsystem
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2212/00—Indexing scheme relating to accessing, addressing or allocation within memory systems or architectures
- G06F2212/25—Using a specific main memory architecture
- G06F2212/253—Centralized memory
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2212/00—Indexing scheme relating to accessing, addressing or allocation within memory systems or architectures
- G06F2212/60—Details of cache memory
- G06F2212/601—Reconfiguration of cache memory
Definitions
- a cache is a block of memory used for temporary storage of frequently accessed data, and allows future requests for cached data to be more quickly serviced than requests for non-cached data. If requested data is contained in the cache (a scenario known as a "cache hit”), the request can be served by simply reading the cache, which is comparably faster than accessing the data from main memory. Conversely, if the requested data is not contained in the cache (a scenario known as a "cache miss”), the data is recomputed or, in conventional techniques, filled into the cache from its original storage location, which is slower than simply reading the data from the cache. Overall system performance is thus improved, in terms of speed, when a larger portion of data requests are serviced from cache memory.
- cache replacement algorithms are employed.
- Conventional cache replacement algorithms include least recently used (LRU) algorithms, most recently used (MRU) algorithms, least frequently used (LFU) algorithms, random replacement algorithms, and so on.
- LRU least recently used
- MRU most recently used
- LFU least frequently used
- cache replacement algorithms are a set of optimizing instructions that a computer program or a hardware-maintained structure implements for managing the cache memory.
- cache replacement algorithms select which information in the cache memory to evict in order to make room for information from main memory.
- EP 0362880 A2 discloses "A main storage management process in a computer system which executes a program in a multiple manner under control of a virtual storage type operating system. A first portion area of a real storage area, allocated to a first program and used later is maintained in that state while the contents of a second portion area of the real storage area is saved in an auxiliary storage to release the second portion area when the first program is to be released. The saved contents of the second portion area from the auxiliary storage is loaded on the real storage when the first program is to be swapped in.”
- EP 1505506 A1 discloses "a method of data caching which uses both the FIFO and LRU cache replacement algorithms. In addition an auxiliary FIFO queue is used for the cache management. As a result the cache hit rate increases which positively impacts overall system performance.”
- US 2012/017039 A1 discloses "a method for caching in a processor system having virtual memory”. The method comprises: “monitoring slow memory in the processor system to determine frequently accessed pages; for a frequently accessed page in slow memory: copy the frequently accessed page from slow memory to a location in fast memory; and update virtual address page tables to reflect the location of the frequently accessed page in fast memory.”
- WO 2011/104741 A1 discloses "a management system for a storage system, the storage system including a first storage subsystem and a second storage subsystem each including logical storage areas for storing data to be processed by a host computer, the logical storage areas in each of the first and second storage subsystems having storage tiers associated respectively with a plurality of storage area characteristic information pieces which are information pieces characterizing the corresponding logical storage areas and which are different from each other, the management system comprising a data migration management part, wherein when the data is migrated from the first storage subsystem to the second storage subsystem, the data migration management part: acquires a configuration of the storage tiers of the logical storage areas of the first storage subsystem in which the data of a migration target is stored; compares the configuration with a configuration of the storage tiers of the logical storage areas of the second storage subsystem; and then migrates the migration target data stored in the logical storage areas of the first storage subsystem to the logical storage areas of the second storage subsystem in accordance with a result of the
- the present disclosure provides a method for managing data migration between a main memory and a cache memory as set out in claim 1, a system as set out in claim 7, and one or more computer-readable storage media as set out in claim 13.
- Profiling cache replacement is a technique for managing data migration between a main memory and a cache memory to improve overall system performance.
- Both the cache and main memories are configured to store pages of data-the cache memory being smaller than the main memory and thus capable of maintaining fewer pages than the main memory.
- the cache memory has at least one of lower latency, higher bandwidth, or lower power usage. Consequently, system performance improves when a larger portion of data access requests can be serviced from the cache memory.
- profiling cache replacement caches highly-requested pages in the cache memory and migrates (or leaves) less-requested pages in the main memory.
- profiling cache replacement employs a profiler to maintain counters that count memory requests for access to not only the pages maintained in the cache memory, but also the pages maintained in the main memory. Based on the information collected by the profiler (e.g., about memory access requests), a mover moves pages between the main and cache memories.
- the mover can swap highly-requested pages of the main memory, such as a most-requested page of the main memory, with little-requested pages of the cache memory, such as a least-requested page of the cache memory.
- the mover can do so, for instance, when the counters indicate that the number of page access requests for highly-requested pages of the main memory is greater than the number of page access requests for little-requested pages of the cache memory.
- the requests made by the memory users are not blocked for cache misses, and the mover performs the page swapping in the background.
- the non-blocking behavior when a page access request results in a cache miss, the requested page is not immediately loaded into the cache memory so that the request can be serviced from the cache memory. Instead, the request is serviced directly from the main memory.
- priority is given to servicing requests made by memory users over the page swapping performed by the mover. To do so, the mover is limited to swapping pages at predetermined time intervals, such as once every microsecond ( ⁇ s).
- the mover determines whether the number of page access requests for a highly-requested page of the main memory exceeds the number of page access requests for a little-requested page of the cache memory. If so, the mover swaps the main memory's highly-requested page with the cache memory's little-requested page. In so doing, profiling cache replacement optimizes the pages with which the cache memory is filled, and does so without interfering with operations of memory users, the result being improved system performance.
- This document describes techniques using, and devices enabling, profiling cache replacement.
- data migration between a main memory and a cache memory is managed in a manner that improves system performance over conventional cache replacement techniques, such as least recently used (LRU) algorithms, most recently used (MRU) algorithms, least frequently used (LFU) algorithms, random replacement algorithms, and so on.
- LRU least recently used
- MRU most recently used
- LFU least frequently used
- the improved performance results, at least partially, from reducing an amount of "thrashing” that occurs, relative to conventional techniques, in conjunction with migrating data between the main and cache memories.
- thrashing refers to a condition caused by excessive fill and eviction traffic that can be generated when a size of a working set of data exceeds a size of the cache memory.
- Thrashing can result in a dramatic increase in a number of cache misses, which cause the system to spend more time performing cache fill and eviction for the resulting misses than performing originally requested computing operations for the working set of data.
- profiling cache replacement data in cache and main memories can be managed in a manner that reduces thrashing and improves overall system performance.
- a memory user can request access to a particular page loaded in memory.
- the client application requests access to the particular page from memory as part of an initialization process.
- the client application may request access to the particular page many times.
- the particular page can be loaded into cache memory, e.g., so that requests for the particular page can be serviced more quickly than if serviced from main memory.
- the client application may not need to access the particular page, or may access it little relative to other pages.
- the client application may instead request access to other pages more, such that the requests to access the other pages eventually exceed the requests for the particular page-since the initialization process has completed and the particular page is no longer requested.
- Efficiency of the client application's normal operation may thus be improved by servicing the requests for the other pages from cache memory, e.g ., since the cache memory, although smaller in size than the main memory, has a lower latency, higher bandwidth, or lower power usage than the main memory. In other words, efficiency may be improved by swapping one of the other pages with the particular page.
- “swapping” refers to an action or series of actions that enables data corresponding to the one other page to be filled into the cache memory and evicting the data corresponding to the particular page from the cache memory.
- the data corresponding to the one other page is simply exchanged with the data corresponding to the particular page, such that the data of the one other page takes the place of the particular page's data in the cache memory and the particular page's data takes the place of the data of the one other page in the main memory.
- the data corresponding to the one other page is simply exchanged with the data corresponding to the particular page, such that the data of the one other page takes the place of the particular page's data in the cache memory and the particular page's data takes the place of the data of the one other page in the main memory.
- a page when a page is cached, its data resides solely in the cache memory, and not in the main memory.
- At least some data for each page in the memory exists in the main memory regardless of whether a page corresponding to the data is cached or not.
- a page in the cache memory and a page in the main memory together provide the valid data of the page.
- some of the pages in the cache memory may not contain valid data, because those pages are not filled.
- Some of the pages in the main memory also may not contain valid data, such as when an entirety of those pages' data has been modified in the cache memory. Under these scenarios, swapping the particular page's data with the data corresponding to the one other page is not simply an exchange of data as in the first described scenario.
- swapping involves evicting the data corresponding to the particular page from the cache memory to a respective page in the main memory.
- the portions of the data that were modified while in the cache memory are copied to the respective page in the main memory-the copying being limited to the modified blocks, since the respective page still maintains the same data for the unmodified blocks.
- the page of cache memory from which the data was evicted is filled with the data of the one other page from the main memory.
- the respective page (e.g ., the page into which the evicted data is placed) and the one other page e.g ., the main-memory data that was used to fill the available cache-memory page) do not correspond to the same pages, however.
- profiling cache replacement counts accesses to the pages of data maintained in both the main and cache memories.
- accesses to the pages in "addressable memory” are tracked with counters.
- addressable memory refers to the portions of memory recognized by memory users. In the first scenario described just above, the size of addressable memory corresponds to a size of the main memory plus the size of the cache memory. In the other scenarios, however, the size of addressable memory corresponds to the size of just the main memory.
- a number of counters are maintained that corresponds to a number of pages in the main memory.
- a profiler can maintain counters for each of the pages in the addressable memory and when access to one of those pages is requested, increment the counter for the page.
- a counter is maintained for each page in addressable memory, e.g ., a counter is maintained for each page in the main memory since the cached pages are also represented by data in the main memory.
- a mover that is configured to move pages from main memory to cache memory, and vice versa, can swap one of the other pages that is more requested than the particular page with the particular page.
- the mover can move the more-requested page from main memory into cache memory, and can move the particular page from cache memory into main memory.
- requests made by the client application are not blocked, and the act of swapping pages to optimize the data maintained in the cache memory is performed in the background.
- the requested page is not immediately loaded into the cache memory so that the request can be serviced from the cache memory. Instead, the request is serviced directly from the main memory.
- the mover is limited to initiating page swaps at predetermined time intervals, e.g., once every microsecond ( ⁇ s). As a result, the mover does not compete with the client application for access to the main or cache memories at other times. Using such techniques, interruption of computing operations may be avoided and thrashing reduced, thereby improving the overall performance of the system.
- Fig. 1 is an illustration of an example environment 100 in which profiling cache replacement can be employed.
- Environment 100 illustrates a memory-profiling computing device 102 having profiled memory 104.
- the memory-profiling computing device 102 is configured as a smartphone, however, other configurations are contemplated. Other configurations of the memory-profiling computing device 102 that are capable of optimizing memory using profiling cache replacement are illustrated in later figures.
- the Environment 100 also illustrates components of the profiled memory 104.
- the profiled memory 104 includes cache memory 106 and main memory 108.
- the cache memory 106 and the main memory 108 are capable of storing data for access by memory users, such as by an operating system, client applications, and so on.
- the cache memory 106 and the main memory 108 are capable of storing pages of data.
- pages refers to same-sized blocks of data, e.g ., 4-kilobyte (KB) blocks of data.
- the cache memory 106 is smaller-it has less storage and is thus capable of storing fewer pages of data than the main memory 108.
- the cache memory Although smaller in terms of storage than the main memory 108, the cache memory has at least one of lower latency, higher bandwidth, or lower power usage than the main memory 108. Due to these characteristics of the cache memory 106, servicing a larger portion of data access requests with cached data rather than with un-cached data results in more efficient request servicing, in terms of speed, power, or some other measure of system efficiency, for the memory-profiling computing device 102.
- the profiled memory 104 also includes mapper 110, profiler 112, and mover 114.
- the mapper 110, the profiler 112, and the mover 114 represent functionality to optimize the performance of the profiled memory 104 by caching highly-used pages and leaving less-used pages in the main memory 108.
- the mapper 110 is used for each memory access.
- the mapper 110 maps an input address to an address in the cache memory 106 (e.g., a cache hit) or to an address in the main memory 108 (e.g., a cache miss).
- the requested page is not immediately loaded into the cache memory 106 so that the request can be serviced from the cache memory 106. Instead, the request is serviced directly from the main memory 108, e.g., the requested page is provided to a memory user that requested access to the page directly from the main memory 108.
- the profiler 112 represents functionality to collect information about memory accesses.
- the profiler 112 tracks numbers of page accesses, such as a number of requests to access the pages in the profiled memory 104.
- numbers of page accesses such as a number of requests to access the pages in the profiled memory 104.
- the profiler 112 may track memory access in different ways without departing from the scope of the technique described herein.
- the profiler 112 can count bytes instead of the number of requests.
- the profiler 112 can treat larger requests to access pages in memory as multiple requests.
- memory access can be tracked without counting each request to access a page. Rather, read requests for a page can be tracked but write requests not tracked. Likewise, write requests for a page can be tracked while read requests are not tracked.
- the profiler 112 can maintain a counter for each page in the cache memory 106 and each page in the main memory 108. When access to a page is requested, the profiler 112 can increment the counter for that page thereby tracking the page accesses. In one or more implementations, however, the profiler 112 maintains fewer counters than one per page of memory. In so doing, the profiler 112 can reduce an amount of memory used to store tracking information that describes page accesses of the profiled memory 104. Details of the manner in which the profiler 112 uses fewer than one counter for each page of memory are discussed herein below.
- the mover 114 represents functionality to move pages between the cache memory 106 and the main memory 108.
- the mover 114 is capable of swapping highly-requested pages of the main memory 108 with little-requested pages of the cache memory 106.
- the term "highly" requested page or pages refers to pages that are requested more than others in the portion of memory under consideration, and may correspond to a most-requested page in the portion under consideration, a top ten percent requested page in the portion, and so on.
- a highly-requested page in the main memory 108 may be a page for which the number of requests ranks in the top ten percent among the pages in the main memory 108, or it may be the most requested page in the main memory 108.
- the term “little” requested page or pages refers to pages that are requested less than others in the portion of memory under consideration, and may correspond to a least-requested page in the portion under consideration, a bottom ten percent requested page in the portion, and so on.
- a little-requested page in the cache memory 106 may be a page for which the number of requests ranks in the bottom ten percent among the pages in the cache memory 106, or it may be the least requested page in the cache memory. It should be noted, however, that "highly” and “little” are used in conjunction with the portion of memory corresponding to the pages.
- a highly-requested page in the main memory 108 can have a fewer number of requests for access than a little-requested page in the cache memory 108, or may have a similar number of requests. However, a highly-requested page in the main memory 108 has a greater number of requests for access than a little requested page in the main memory 108. Additionally, a highly-requested page in the profiled memory 104 ( e.g ., among the pages in the main memory 108 and the cache memory 106) has a greater number of requests for access than little-requested pages in the profiled memory 104.
- the mover 114 may swap highly-requested pages of the main memory 108 with little-requested pages of the cache memory 106, responsive to a determination that the highly-requested pages of the main memory 108 are requested for access more than the little-requested pages of the cache memory 106.
- the mover 114 also represents functionality to make determinations as to whether pages in the main memory 108 are accessed more than the pages in the cache memory 106. The mover 114 may do so by checking the information collected by the profiler 112, e.g ., the counters.
- the mover 114 moves pages (e.g ., swaps a page in the cache memory 106 with a page in the main memory 108), the mover 114 updates the address information used by the mapper 110, so that future memory access requests are mapped to the correct address in the cache memory 106 or the main memory 108. It should be noted that responsive to a determination that the highly-requested pages of the main memory 108 are not requested for access more than the little-requested pages of the cache memory 106, the mover 114 does not swap pages.
- FIGs 2 and 3 illustrate diagrams of example page counter values for main and cache memories.
- Diagram 200 of Fig. 2 shows the example page counter values for the main and cache memories at a first time
- diagram 300 of Fig. 3 shows the example page counter values for the main and cache memories at a second time, subsequent to the first time.
- the mover 114 checks the information about requested page accesses.
- the mover 114 also initiates page swaps at a predetermined interval of time, e.g., once every microsecond ( ⁇ s).
- the first time corresponding to the diagram 200 may be before the occurrence of a particular such predetermined time interval while the second time corresponding to the diagram 300 is after the occurrence of the particular predetermined time interval.
- the diagram 200 includes a first axis 202 that corresponds to counter values which indicate the number of page accesses.
- a second axis 204 of the diagram 200 represents the pages maintained in memory.
- the diagram 200 also includes dividing line 206. The bars illustrated to the left of the dividing line 206 represent counter values of pages in the cache memory 106 at the first time, while the bars illustrated to the right of the dividing line represent counter values of pages in the main memory 108 at the first time.
- the cache memory 106 is 128 megabytes (128 MB)
- the main memory 108 is 4 gigabytes (4 GB)
- each page of data is 4 kilobytes (4 KB).
- the bars to the left of the dividing line 206 represent 128 MB worth of 4-KB pages, while the bars to the right of the dividing line 206 represent 4 GB worth of 4-KB pages (1 million pages). It should be appreciated that these are merely exemplary sizes and that sizes of the cache memory 106, the main memory 108, and the pages maintained therein can vary from the sizes used in the example without departing from the scope of the techniques described herein.
- Figs 2 and 3 have been illustrated to represent the above described scenario in which data corresponding to pages in the main memory 108 is simply exchanged with the data corresponding to pages in the cache memory 106, such that when a page is cached, its data is in cache memory 106, but not in the main memory 108.
- memory users recognize addressable memory as the combination of 4 GB and 128 MB (e.g., a combination of the main memory 108 and the cache memory 106).
- the main memory 108 and the cache memory 106 may be configured according to the other memory configuration and swapping scenarios described above.
- a page in the cache memory 106 and a page in the main memory 108 together provide the valid data of a cached page.
- at least some data corresponding to the cached pages may be in the cache memory 106 and at least some data corresponding to the cached pages may also be in the main memory 108.
- some of the pages in the cache memory 106 may not contain valid data, because those pages are not filled.
- Some of the pages in the main memory 108 also may not contain valid data, such as when an entirety of those pages' data has been modified in the cache memory 106.
- the mover 114 may not fill each portion of the page's 4 KB of data into the cache memory 106.
- the mover 114 may instead fill 1 KB of the page's data into the cache memory 106.
- the mover may fill the 1 KB of data that is close to a request address, such as if there is some uncertainty as to whether the other 3 KB will even be used.
- the mover 114 may not fill the page's data that will be overwritten into the cache memory 106. Rather, the mover 114 can fill into the cache memory 106 information to indicate where to find each 64-byte portion of the page.
- the bars that represent the pages are ordered from left to right in descending counter value order, and also within the bounds of the cache memory 106 and the main memory 108.
- bar 208 which represents the number of page accesses of the most-accessed page in the cache memory 106 corresponds to a greater counter value than bar 210, which represents the number of page accesses for the second-most-accessed page in the cache memory 106.
- bar 212 which represents the number of page accesses of the second-least-accessed page in the main memory 108, corresponds to a greater counter value than bar 214, which represents the number of page accesses of the least-accessed page in the main memory 108.
- bars 216, 218, which represent a least-requested page in the cache memory 106 and a most-requested page in the main memory 108, respectively, at the first time.
- the bar 216 is smaller than the bar 218, and thus represents that access to the least-requested page in the cache memory 106 has, at the first time, been requested less than access to the most-requested page in the main memory 108.
- the diagram 300 is similar to the diagram 200, it includes axes that represent counter values and pages maintained in memory, the dividing line between the pages maintained in the cache memory 106 and the main memory 108, and so on.
- the diagram 300 differs from the diagram 200 in a notable respect, however.
- the bars 216, 218 are swapped, representing a swap of the corresponding pages between the cache memory 106 and the main memory 108.
- the diagram 300 illustrates a scenario in which the page represented by the bar 216 is moved into the main memory 108 ( e.g ., by the mover 114) and the page represented by the bar 218 is moved into the cache memory 106 ( e.g ., by the mover 114).
- the diagram 200 corresponds to a first time and the diagram 300 corresponds to a second time that is subsequent to the first time.
- the counter values for the pages represented in the diagrams 200, 300 are the same at both times.
- a moving period the time during which the mover 114 swaps pages between the cache memory 106 and the main memory 108, is assumed to have occurred.
- the first time and the second time may thus represent slices of time, respectively, directly before and directly after the moving period.
- the profiling cache replacement techniques are applied by the mover 114 to optimize the cached pages for the memory-profiling computing device 102.
- the memory-profiling computing device 102 can be one or a combination of various devices, here illustrated with six examples: a smartphone 102-1, a computing watch 102-2, a digital camera 102-3, a laptop 102-4, a tablet computer 102-5, and a desktop computer 102-6 though other computing devices and systems, such as a netbook, a gaming console, or a set-top box may also be used.
- the techniques operate, at least in part, through a remote computing device.
- the remote computing device can be configured as a server, for example. In such cases, some computing can be forgone locally, e.g., by communicating data enabling the computing through a communication device having limited computing operations or even communicating the data enabling the computing directly from memory-profiling computing devices 102 to the server.
- the memory-profiling computing device 102 includes or is able to communicate with a display 402 (five are shown in Fig. 4 ), a transceiver 404, one or more processors 406, and computer-readable storage media 408 (CRM 408).
- the transceiver 404 is capable of sending and receiving data directly or through a communication network, such as client application data from devices 102 through a local area, wide area, personal area, cellular, or near-field network.
- the cache memory 106, the main memory 108, the profiler 112, the mapper 110, and the mover 114 are embodied on the CRM 408.
- the cache memory 106 includes cached pages 410 and the main memory includes main-memory loaded pages 412 (MM-loaded pages 412).
- the profiler 112 includes memory access information 414, which is collected by the profiler 112 about memory accesses.
- the memory access information 414 includes counters that indicate numbers of requests to access the cached pages 410 and the MM-loaded pages 412.
- the CRM 408 also includes input address mapping 416, which maps input addresses (such as those provided by a memory user to access pages of information in the cache or main memories) to an address of one of the cached pages 410 in the cache memory 106 (a cache hit), or one of the MM-loaded pages 412 in the main memory 108 (a cache miss).
- input addresses such as those provided by a memory user to access pages of information in the cache or main memories
- the mapper 110 is used for each memory access, and represents functionality to map an input address to an address in the cache memory 106 or the main memory 108.
- the mapper 110 may refer to the input address mapping 416 and return a corresponding address of the cache memory 106 or of the main memory 108.
- the profiler 112 is also employed with each memory access.
- the profiler tracks the number of accesses to the cached pages 410 and the MM-loaded pages 412.
- the profiler 112 maintains, as part of the memory access information 414, respective counters for each of the cached pages 410 and each of the MM-loaded pages 412. In this scenario, when one of the cached pages 410 or one of the MM-loaded pages 412 is accessed, the profiler 112 increments the respective counter to indicate the access. In some implementations, however, maintaining an incrementable counter for each of the cached pages 410 and each of the MM-loaded pages 412 may consume too much storage space.
- the profiler 112 uses 8-bit counters, the main memory is 4 GB, and each of the MM-loaded pages 412 is 4 KB, for example, then 1 MB of memory is used simply to store the counters for one million pages-which may not be suitable in some implementations, e.g., when the memory access information 414 is stored in static random-access memory (SRAM). Accordingly, the profiler 112 can track and maintain the memory access information 414 in manners that utilize less storage.
- the profiler 112 may, for instance, implement counters that reduce total counter storage through dynamic expansion of a range of the counters or such that there are fewer counters than one for each page of memory.
- the profiler 112 may in one or more default implementations use 8-bit counters, and in one or more other implementations use dynamically expanding counters.
- access counts of the pages in the cache memory 106 and the main memory 108 have a high dynamic range, e.g., highly-accessed pages can be accessed significantly more than little-accessed pages, and the number of accesses of highly-accessed pages can continue to increase during system operation.
- the data in memory can be divided into sets.
- the cached pages 410 and the MM-loaded pages 412 can be divided into sets of pages, such that each set includes some of the cached pages 410 and some of the MM-loaded pages 412.
- the main memory 108 is 4GB
- each page is 4 KB
- the pages can be divided into sets such that each set includes 512 of the MM-loaded pages 412 and 16 of the cached pages 410.
- the mover 114 can swap the cached pages 410 in a set with the MM-loaded pages 412 that are also in the set. Consequently, when checking counters, the mover 114 may, in this example, check 512 counters to determine the most- and least-requested pages. The mover 114 does not, however, swap the cached pages 410 in the set with the MM-loaded pages 412 from other sets.
- the profiler 112 can keep a common scale S for each set of pages, and an N-bit counter C for each page in a set.
- the profiler can implement the common scale S for a set of pages using 6 bits.
- the profiler 112 can use a common scale when pages are divided into sets because counter values are compared within a set, e.g ., since solely the pages in the cache memory 106 and the main memory 108 of a same set are swapped. Having the common scale S and the N-bit counter C , the profiler 112 can maintain counters such that their values equal C ⁇ 2 S .
- the profiler 112 increases the page's counter C with a probability of 1 2 S . This allows the profiler 112 to generate S random bits, and then increase the counter solely when each of the S bits is zero.
- the profiler 112 can increase the common scale S for the page's set by one, and divide each of the counter values for the particular page causing the overflow as well as for the other pages of the set by two.
- K represents a significand, which is a part of a floating point number consisting of its significant digits
- E represents the exponent of the base (2 is the base).
- a 7-bit counter C can represent page access values in a range of [0, 15 ⁇ 2 7 ].
- the profiler 112 encodes an individual counter C in a different manner.
- a counter can store values in a range of [0,1984].
- a number of counters can also be reduced in some storage-sensitive implementations, e.g ., from one counter per page of memory to less than one counter per page of memory.
- profiling cache replacement involves caching the highly-requested pages of the main memory 108 into the cache memory 106 rather than simply caching pages into the cache memory 106 simply because an access to those pages is requested, numbers of accesses of rarely requested pages in the main memory 108 are largely irrelevant for the techniques described herein. Accordingly, the profiler 112 may maintain counters for the pages that are accessed more often.
- tags may be used to identify a page with which each of the counters is associated.
- the profiler 112 updates the counter in one of the manners described above to indicate the access.
- one of the counters that is already used to track accesses to a different page may be disassociated with the different page and associated with the requested but previously unassociated page.
- profiling cache replacement involves applying one or more modified counter tagging techniques to disassociate counters with tracked pages and associated them with requested but previously unassociated pages. These modified counter tagging techniques can reduce thrashing in comparison to conventional techniques.
- the profiler 112 applies the modified counter tagging techniques by maintaining a number of counters N as part of the memory access information 414.
- Each counter comprises a data pair representing a page tag that identifies a page with which the counter is associated and a count associated with the page, e.g., in the form ⁇ page, count ⁇ .
- the profiler 112 checks to see if there is a counter ⁇ X, C ⁇ that is associated with the particular page X. If there is a counter associated with the particular page X, e.g., ⁇ X, C ⁇ exists, then the profiler 112 increments the count C by one.
- the profiler 112 finds a counter ⁇ Y, C ⁇ for a page Y having a smallest count C. The profiler 112 then replaces the value of the counter so that it is associated with the particular page X and indicates one access of the particular page X, e.g., the profiler adjusts the pair of values of ⁇ Y, C ⁇ to ⁇ X, 1 ⁇ . This is different from conventional techniques which inherit a previous count. In other words, instead of replacing C with 1 as the modified counter tagging techniques do, conventional techniques replace C with C+1. By counting page accesses with the modified counter tagging techniques, a number of counters N with the largest counts correspond to the top-n pages.
- the mover 114 represents functionality to check the counters maintained in the memory access information 414 for determining whether to swap pages between the main memory 108 and the cache memory 106. As mentioned above, the mover 114 performs these checks to make the determinations to initiate page swaps at a predetermined interval of time, such as every microsecond (1 ⁇ s). Although the examples discussed herein refer to the interval of time being predetermined and corresponding to 1 ⁇ s, the interval of time may be different without departing from the scope of the techniques described herein. By way of example, the predetermined interval can also be determined randomly, based on a number of accesses as discussed in more detail below (e.g., a total number of accesses requested for pages from the profiled memory 104), and so on.
- the mover 114 can make the determinations in accordance with the techniques described herein. Rather than using an absolute time (e.g ., the predetermined interval of time), the mover 114 can alternately make the determinations every N memory accesses per set of pages. In this way, the techniques described herein can control a percentage of the background bandwidth used instead of an absolute value of the background bandwidth used.
- the mover 114 is configured to determine whether highly-requested pages of the main memory 108 are requested more than little-requested pages in the cache memory 106.
- the highly-requested pages of the main memory 108 are highly-requested relative to the MM-loaded pages 412, and the little-requested pages in the cache memory 106 are little-requested relative to the cached pages 410.
- the number of requests for the highly-requested main memory pages may be substantially similar, however, to the number of requests for the little-requested cache memory pages.
- the mover 114 swaps the highly-requested pages of the main memory 108 with the little-requested pages of the cache memory 106. To do so, the mover 114 evicts the little-requested pages from the cache memory 106 and fills the highly-requested pages of the main memory into the cache memory 106.
- the mover 114 Since operation of the mover 114 competes with demand requests from memory users (e.g ., client applications), limiting performance of the mover 114's determining and swapping operations to the moving period reduces the mover 114's impact on the memory users-reducing the latency of memory access for the memory profiling computing device 102.
- the mover 114 thus operates in the background while memory users access pages of data from the cache memory 106 and the main memory 108.
- page access requests made by memory users are given priority over page access requests made for the purpose of optimizing memory, e.g. , page swaps requested by the mover 114.
- the requests made to access pages of data from the cache memory 106 and the main memory 108 can generally be divided into two types-demand requests and background requests.
- the term "demand request” refers to a request made by a memory user, such as a client application, for a page of data from the cache memory 106 or the main memory 108.
- background request refers to a fill or eviction request triggered indirectly by the demand request, such as the fill and eviction requests made by the mover 114 in conjunction with swapping pages between the main memory 108 and the cache memory 106.
- the techniques described herein give priority to fulfilling the demand requests over fulfilling the background requests in a couple ways.
- the techniques may allow a limited number of pending background requests, such as by maintaining the pending requests in a queue configured to hold the limited number of pending background requests. If the queue is full when a background request is made, the request is simply dropped, e.g. , the request is not added to the queue. Indeed, other background requests are not added to the queue until at least one of the pending background requests in the full queue is serviced.
- the second way in which the techniques described herein give priority to fulfilling the demand requests over fulfilling the background requests involves an implementation in which a dynamic random-access memory (DRAM) request queue is used, and in which the background requests are allowed to fill a limited amount (e.g ., half) of the DRAM request queue. If, for example, a fill level of the DRAM request queue is more than half of the queue's maximum fill level, then the techniques described herein limit incoming requests to demand requests.
- background requests can be stalled when the fill level of the DRAM request queue is more than half of the queue's maximum fill level. For instance, a background request initiated by the mover 114 is dropped if there are too many mover-requested background requests pending.
- the background requests initiated by the mover 114 not dropped are disassembled into DRAM requests and sent to the DRAM request queue. If the DRAM request queue surpasses a particular threshold (e.g ., half of the queue's maximum fill level), indicating that the DRAM request queue is too busy to currently handle incoming DRAM requests, further background requests are not sent to the DRAM request queue. Instead, the background DRAM requests are held until the DRAM request queue level is lower than the threshold. Once the DRAM request queue is able to again handle background DRAM requests, the held requests are sent to the DRAM request queue.
- a particular threshold e.g ., half of the queue's maximum fill level
- the workload that memory users impose on the profiled memory 104 can change over time.
- the highly-requested pages can also change.
- pages cached and highly-requested in the main memory 108 while one client application operates may not be the same as the pages cached and highly-requested in the main memory 108 while another client application operates.
- the profiler 112 can decay values of the page counters. By automatically decaying counter values at some decay interval, pages that were once heavily accessed can be evicted from the cache memory 106 when their use wanes.
- the profiler 112 can simply divide each of the counter values of a set in half at the decay interval.
- the decay interval corresponds to a predefined threshold for a total number of accesses requested for the pages in a set, e.g., 2 ⁇ 14 accesses.
- the profiler 112 divides the counter values of the set by two.
- the profiler 112 decays the counters by decreasing the common scale S of a set by one when the set's common scale S is greater than zero.
- the profiler 112 divides each of the set's counters by two.
- the decay interval corresponds to some predetermined absolute time rather than the predefined number of accesses.
- the counters can be decayed in manners other than being divided by two without departing from the scope of the techniques described herein. By way of example, rather than being divided by two the counters can be multiplied by a factor such as two-thirds ( 2 3 ).
- counters for the little-requested pages of the cache memory 106 and the highly-requested pages of the main memory 108 can have the same or very similar values.
- the cached pages of the set that are little-requested (compared to other cached pages) and the set's highly-requested pages of the main memory 108 can have the same or very similar values. Difficulties can arise when the mover 114 is to determine which pages to cache when cached pages have similar counter values to pages that are candidates for being cached. Consider an example in which a first page of a set is accessed slightly more than other pages of the set.
- the mover 114 can cache the first page.
- a second page of the set becomes accessed slightly more than the first page.
- the mover 114 replaces the first page with the second page, thereby evicting the first page from the cache memory 106.
- slightly more accesses of the first page again can cause the mover 114 to replace the second page in the cache memory, and so on. This scenario can cause thrashing for these pages, and can be especially problematic when decaying counters are used and the decaying interval is short.
- the mover 114 can add a barrier of entry before determining to swap the highly-requested pages of the main memory 108 with the little-requested pages of the cache memory 106.
- the mover 114 can instead determine to swap pages when the number of requested accesses of the highly-requested main memory pages is greater than number of accesses of the little-requested cached pages and the entry barrier.
- the mover 114 can determine to swap pages when the following is true: AccessCount high MM page > AccessCount little CM page + ⁇
- ⁇ represents the entry barrier and can correspond to a predetermined integer, such as an integer that offsets a resource cost (e.g., time, power, etc.) to swap the pages.
- a resource cost e.g., time, power, etc.
- Profiling cache replacement can be implemented in still other ways without departing from the scope of the techniques described herein.
- One example of an alternate implementation is to use time-multiplexed counters. Instead of maintaining counters solely for the pages that are accessed most, the profiler 112 can maintain counters for each of the pages. At any given time, however, the profiler 112 may have access to a limited number of the counters.
- the profiler 112 When access is requested to a page for which the counter is in the SRAM, the profiler 112 simply increments the counter. When access is requested to a page for which the counter is in the DRAM, however, the profiler 112 ignores the request.
- the mover 114 can be configured to check the counters in both the DRAM and SRAM.
- Another example of an alternate embodiment is to bundle the counters in a page table data structure.
- the profiler 112 can bundle the counters in a page table data structure.
- management of the counters may follow a same flow as the page table and a translation lookaside buffer (TLB).
- counter decaying can be implemented by decreasing the common scale S gradually, resulting in a fractional S. Instead of decaying each counter value at the predetermined decaying interval, the represented counter values can be decayed gradually by decreasing the common scale S gradually.
- Figs. 5-7 depict methods enabling or using profiling cache replacement. These methods are shown as sets of blocks that specify operations performed but are not necessarily limited to the order or combinations shown for performing the operations by the respective blocks. In portions of the following discussion reference may be made to environment 100 of Fig. 1 and entities detailed in Fig. 4 , reference to which is made for example only. The techniques are not limited to performance by one entity or multiple entities operating on one device.
- Fig. 5 depicts method 500, which describes manners in which to replace pages in cache memory according to a profiling algorithm used to manage data migration between the cache memory and a main memory.
- counters maintained in association with pages of the data maintained in the main and cache memories are updated.
- the counters are configured to indicate a number of requests to access the pages maintained in the main and cache memories.
- the profiler 112 maintains counters in the memory access information 414. These counters indicate a number of requests to access the cached pages 410 and the MM-loaded pages 412. Responsive to requests to access one of the cached pages 410 or one of the MM-loaded pages 412, for instance, the profiler 112 increments corresponding counters in the memory access information 414.
- profiling memory replacement is performed in a background of servicing requests made by the memory for the pages maintained in the cache memory 106 and the main memory 108.
- performance of the method steps 504 and 506 can be limited to being performed at a predetermined time interval, such as every microsecond ( ⁇ s).
- a determination is made at the predetermined time interval as to whether a number of page access requests for a highly-requested page of the main memory is greater than a number of page access requests for a little-requested page of the cache memory according to the counters.
- the highly-requested page of the main memory is highly-requested relative to the pages loaded in the main memory, and the little-requested page of the cache memory is little-requested relative to the pages loaded in the cache memory.
- the number of requests for the highly-requested main memory page may be substantially similar, however, to the number of requests for the little-requested cache memory page.
- the mover 114 checks the counters maintained by the profiler 112 in the memory access information 414. Every microsecond, for instance, the mover 114 determines a little-requested cached page 410 ( e.g ., that is requested less than other pages in the cache memory 106) and a highly-requested MM-loaded page 412 ( e.g ., that is requested more than other pages in the main memory 108) according to the counters. The mover 114 compares the respective counter values to determine whether the highly-requested page of the main memory 108 is requested more than the little-requested page of the cache memory 106.
- a little-requested cached page 410 e.g ., that is requested less than other pages in the cache memory 106
- a highly-requested MM-loaded page 412 e.g ., that is requested more than other pages in the main memory 108 according to the counters.
- the mover 114 compares the respective counter values to determine whether the highly-re
- the mover 114 determines at step 504 that the highly-requested page from the main memory 108 is requested more than the little-requested page of the cache memory 106. Responsive to this determination, the mover 114 swaps the highly-requested page from the main memory 108 with the little-requested page of the cache memory 106. In other words, the mover 114 evicts the little-requested cache memory page from the cache memory 106 and loads that page into the main memory 108. The mover 114 also caches the highly-requested main memory page.
- Fig. 6 depicts method 600, which describes manners in which page accesses are counted for profiling cache replacement using reduced-space counters.
- common scales are maintained for sets of pages in memory.
- the data maintained in the profiled memory 104 is divided into sets of pages, such that each set includes a plurality of pages from the cache memory 106 and a plurality of pages from the main memory 108.
- the pages can be divided into sets as described in more detail above.
- the profiler 112 For each set of data, the profiler 112 maintains a common scale S, which indicates a base count or base number of accesses for the pages of the set. In one or more implementations, the profiler 112 maintains 4-bit common scales for each set of pages. It should be appreciated that different sized common scales (in terms of number of bits) may be used without departing from the scope of the techniques describe herein.
- a counter is maintained for each of the pages in a set and that indicates a number of page accesses relative to other pages in the set.
- the profiler 112 maintains an N-bit counter for each page in a set, as described in more detail above.
- the N-bit counter for a page together with the common scale S of that page's set, indicates the number of accesses for the page.
- a corresponding counter and common scale are updated according to the access.
- the profiler 112 updates an N-bit counter associated with the requested page and also updates the common scale S associated with the requested page's set.
- the profiler 112 updates N-bit counters and common scales S as described in more detail above. It should be appreciated that the techniques described herein can utilize method 600 in one or more storage-sensitive implementations, e.g., when the memory access information 414 is stored in SRAM.
- Fig. 7 depicts method 700, which describes manners in which page accesses are counted for profiling cache replacement using fewer than one counter per page of memory. Like the method 600, the method 700 can also be utilized for one or more storage-sensitive implementations.
- n number of counters are associated with the top- n accessed pages in memory.
- the profiler 112 associates n counters with the top-n accessed pages in the profiled memory 104.
- counters are associated with pages of the profiled memory 104 that are cached, leaving remaining counters to associate with the next most-accessed pages in the main memory 108.
- a request is received to access a page maintained in memory.
- a request is received to access a page of data maintained in the profiled memory 104, such as to access one of the cached pages 410 or one of the MM-loaded pages 412.
- a determination is made as to whether the requested page is associated with one of the counters, e.g ., whether one of the counters has an attribute identifying the requested page.
- the profiler 112 determines whether the requested page is associated with one of the counters maintained as part of the memory access information 414.
- the counter associated with the requested page is updated.
- the profiler 112 increments a counter value C for a counter associated with the requested page from C to C +1. If, however, a determination is made that the requested page is not associated with one of the counters (e.g., "no" at 706), then at 710, the least-accessed page associated with a counter is determined.
- the profiler 112 determines a least-accessed page associated with a counter in a same set of pages as the requested page. Alternately, the profiler 112 simply determines the least-accessed page of the pages associated with counters. The profiler 112 can determine the least-accessed page by checking the counter values of the counters.
- the counter of the least-accessed page is associated with the requested page.
- the profiler 112 disassociates the least-accessed page with the counter and then associates the counter with the requested page, e.g., by changing a tag of the counter to identify the requested page.
- a counter value of the counter is adjusted to reflect the access request.
- the profiler 112 adjusts a counter value C (which indicates a number of accesses of the page previously associated with the counter) by setting the counter value to one. Setting the counter value to one contrasts with some conventional techniques which involve setting the counter value C to C +1. In so doing, the method 700 can reduce trashing.
- Fig. 8 illustrates various components of example computing system 800 that can be implemented as any type of client, server, and/or computing device as described with reference to the previous Figs. 1-7 to implement profiling cache replacement.
- computing system 800 can be implemented as one or a combination of a wired and/or wireless wearable device, System-on-Chip (SoC), and/or as another type of device or portion thereof.
- Computing system 800 may also be associated with a user (e.g ., a person) and/or an entity that operates the device such that a device describes logical devices that include users, software, firmware, and/or a combination of devices.
- SoC System-on-Chip
- Computing system 800 includes communication devices 802 that enable wired and/or wireless communication of device data 804 (e.g ., received data, data that is being received, data scheduled for broadcast, data packets of the data, etc.).
- Device data 804 or other device content can include configuration settings of the device, media content stored on the device, and/or information associated with a user of the device.
- Media content stored on computing system 800 can include any type of audio, video, and/or image data, including complex or detailed results of profiling cache replacement acts.
- Computing system 800 includes one or more data inputs 806 via which any type of data, media content, and/or inputs can be received, such as human utterances, user-selectable inputs (explicit or implicit), messages, music, television media content, recorded video content, and any other type of audio, video, and/or image data received from any content and/or data source.
- data inputs 806 via which any type of data, media content, and/or inputs can be received, such as human utterances, user-selectable inputs (explicit or implicit), messages, music, television media content, recorded video content, and any other type of audio, video, and/or image data received from any content and/or data source.
- Computing system 800 also includes communication interfaces 808, which can be implemented as any one or more of a serial and/or parallel interface, a wireless interface, any type of network interface, a modem, and as any other type of communication interface.
- Communication interfaces 808 provide a connection and/or communication links between computing system 800 and a communication network by which other electronic, computing, and communication devices communicate data with computing system 800.
- Computing system 800 includes one or more processors 810 (e.g ., any of microprocessors, controllers, and the like), which process various computer-executable instructions to control the operation of computing system 800 and to enable techniques for, or in which can be embodied, profiling cache replacement.
- processors 810 e.g ., any of microprocessors, controllers, and the like
- computing system 800 can be implemented with any one or combination of hardware, firmware, or fixed logic circuitry that is implemented in connection with processing and control circuits which are generally identified at 812.
- computing system 800 can include a system bus or data transfer system that couples the various components within the device.
- a system bus can include any one or combination of different bus structures, such as a memory bus or memory controller, a peripheral bus, a universal serial bus, and/or a processor or local bus that utilizes any of a variety of bus architectures.
- Computing system 800 also includes computer-readable media 814, such as one or more memory devices in addition to the profiled memory 104 that enable persistent and/or non-transitory data storage (i.e ., in contrast to mere signal transmission), examples of which include random access memory (RAM), non-volatile memory (e.g ., any one or more of a read-only memory (ROM), flash memory, EPROM, EEPROM, etc.), and a disk storage device.
- RAM random access memory
- non-volatile memory e.g ., any one or more of a read-only memory (ROM), flash memory, EPROM, EEPROM, etc.
- a disk storage device may be implemented as any type of magnetic or optical storage device, such as a hard disk drive, a recordable and/or rewriteable compact disc (CD), any type of a digital versatile disc (DVD), and the like.
- Computing system 800 can also include a mass storage media device 816.
- the computer-readable media 814 also includes a profiled memory 104.
- Computer-readable media 814 provides data storage mechanisms to store device data 804, as well as various device applications 818 and any other types of information and/or data related to operational aspects of computing system 800.
- an operating system 820 can be maintained as a computer application with computer-readable media 814 and executed on processors 810.
- Device applications 818 may include a device manager, such as any form of a control application, software application, signal-processing and control module, code that is native to a particular device, a hardware abstraction layer for a particular device, and so on.
- Device applications 818 also include any system components, engines, or managers to implement the techniques.
Description
- In computing, a cache is a block of memory used for temporary storage of frequently accessed data, and allows future requests for cached data to be more quickly serviced than requests for non-cached data. If requested data is contained in the cache (a scenario known as a "cache hit"), the request can be served by simply reading the cache, which is comparably faster than accessing the data from main memory. Conversely, if the requested data is not contained in the cache (a scenario known as a "cache miss"), the data is recomputed or, in conventional techniques, filled into the cache from its original storage location, which is slower than simply reading the data from the cache. Overall system performance is thus improved, in terms of speed, when a larger portion of data requests are serviced from cache memory.
- Since cache memory is typically smaller than main memory, data previously filled into the cache may need to be replaced by data used more recently. To do so, cache replacement algorithms are employed. Conventional cache replacement algorithms include least recently used (LRU) algorithms, most recently used (MRU) algorithms, least frequently used (LFU) algorithms, random replacement algorithms, and so on. Broadly speaking, cache replacement algorithms are a set of optimizing instructions that a computer program or a hardware-maintained structure implements for managing the cache memory. In particular, cache replacement algorithms select which information in the cache memory to evict in order to make room for information from main memory.
- Many of the conventional cache replacement algorithms do not maintain information about data blocks that are not currently in cache memory. As a result, when a size of a working set of data exceeds a size of the cache memory, excessive fill and eviction traffic can be generated. This excessive fill and eviction traffic can cause a condition known as "thrashing", in which a number of cache misses increases dramatically and time spent performing cache fill and eviction as a result of the misses can surpass the time spent performing the originally requested computing operations for the working set of data. Accordingly, conventional cache replacement algorithms have drawbacks which are capable of hampering computing operations.
-
EP 0362880 A2 discloses "A main storage management process in a computer system which executes a program in a multiple manner under control of a virtual storage type operating system. A first portion area of a real storage area, allocated to a first program and used later is maintained in that state while the contents of a second portion area of the real storage area is saved in an auxiliary storage to release the second portion area when the first program is to be released. The saved contents of the second portion area from the auxiliary storage is loaded on the real storage when the first program is to be swapped in." -
EP 1505506 A1 discloses "a method of data caching which uses both the FIFO and LRU cache replacement algorithms. In addition an auxiliary FIFO queue is used for the cache management. As a result the cache hit rate increases which positively impacts overall system performance." -
US 2012/017039 A1 discloses "a method for caching in a processor system having virtual memory". The method comprises: "monitoring slow memory in the processor system to determine frequently accessed pages; for a frequently accessed page in slow memory: copy the frequently accessed page from slow memory to a location in fast memory; and update virtual address page tables to reflect the location of the frequently accessed page in fast memory." -
WO 2011/104741 A1 discloses "a management system for a storage system, the storage system including a first storage subsystem and a second storage subsystem each including logical storage areas for storing data to be processed by a host computer, the logical storage areas in each of the first and second storage subsystems having storage tiers associated respectively with a plurality of storage area characteristic information pieces which are information pieces characterizing the corresponding logical storage areas and which are different from each other, the management system comprising a data migration management part, wherein when the data is migrated from the first storage subsystem to the second storage subsystem, the data migration management part: acquires a configuration of the storage tiers of the logical storage areas of the first storage subsystem in which the data of a migration target is stored; compares the configuration with a configuration of the storage tiers of the logical storage areas of the second storage subsystem; and then migrates the migration target data stored in the logical storage areas of the first storage subsystem to the logical storage areas of the second storage subsystem in accordance with a result of the comparison." - The present disclosure provides a method for managing data migration between a main memory and a cache memory as set out in claim 1, a system as set out in claim 7, and one or more computer-readable storage media as set out in claim 13.
- This document describes profiling cache replacement. Profiling cache replacement is a technique for managing data migration between a main memory and a cache memory to improve overall system performance. Both the cache and main memories are configured to store pages of data-the cache memory being smaller than the main memory and thus capable of maintaining fewer pages than the main memory. Compared to the main memory, however, the cache memory has at least one of lower latency, higher bandwidth, or lower power usage. Consequently, system performance improves when a larger portion of data access requests can be serviced from the cache memory. To increase the portion of data access requests serviced from the cache memory, profiling cache replacement caches highly-requested pages in the cache memory and migrates (or leaves) less-requested pages in the main memory.
- Unlike conventional cache replacement techniques, profiling cache replacement employs a profiler to maintain counters that count memory requests for access to not only the pages maintained in the cache memory, but also the pages maintained in the main memory. Based on the information collected by the profiler (e.g., about memory access requests), a mover moves pages between the main and cache memories. By way of example, the mover can swap highly-requested pages of the main memory, such as a most-requested page of the main memory, with little-requested pages of the cache memory, such as a least-requested page of the cache memory. The mover can do so, for instance, when the counters indicate that the number of page access requests for highly-requested pages of the main memory is greater than the number of page access requests for little-requested pages of the cache memory.
- So as not to impede the operations of memory users (e.g., client applications), the requests made by the memory users are not blocked for cache misses, and the mover performs the page swapping in the background. With regard to the non-blocking behavior, when a page access request results in a cache miss, the requested page is not immediately loaded into the cache memory so that the request can be serviced from the cache memory. Instead, the request is serviced directly from the main memory. With regard to performing page swapping in the background, priority is given to servicing requests made by memory users over the page swapping performed by the mover. To do so, the mover is limited to swapping pages at predetermined time intervals, such as once every microsecond (µs). At the predetermined time interval, the mover determines whether the number of page access requests for a highly-requested page of the main memory exceeds the number of page access requests for a little-requested page of the cache memory. If so, the mover swaps the main memory's highly-requested page with the cache memory's little-requested page. In so doing, profiling cache replacement optimizes the pages with which the cache memory is filled, and does so without interfering with operations of memory users, the result being improved system performance.
- This summary is provided to introduce simplified concepts concerning the techniques, which are further described below in the Detailed Description. This summary is not intended to identify essential features of the claimed subject matter, nor is it intended for use in determining the scope of the claimed subject matter.
- Embodiments of techniques and devices for profiling cache replacement are described with reference to the following drawings. The same numbers are used throughout the drawings to reference like features and components:
-
Fig. 1 illustrates an example environment in which the techniques can be implemented. -
Fig. 2 illustrates a diagram showing example page counter values for main and cache memories at a first time. -
Fig. 3 illustrates a diagram showing example page counter values for the main and cache memories at a second time, subsequent to the first time. -
Fig. 4 illustrates an example memory-profiling computing device ofFig. 1 . -
Fig. 5 illustrates a method to replace pages in cache memory according to a profiling algorithm. -
Fig. 6 illustrates a method to count page accesses for profiling cache replacement using reduced-space counters. -
Fig. 7 illustrates a method to count page accesses for profiling cache replacement using fewer than one counter per page of memory. -
Fig. 8 illustrates an example computing system embodying, or in which techniques may be implemented that enable use of, profiling cache replacement. - This document describes techniques using, and devices enabling, profiling cache replacement. Through use of these techniques and devices, data migration between a main memory and a cache memory is managed in a manner that improves system performance over conventional cache replacement techniques, such as least recently used (LRU) algorithms, most recently used (MRU) algorithms, least frequently used (LFU) algorithms, random replacement algorithms, and so on. The improved performance results, at least partially, from reducing an amount of "thrashing" that occurs, relative to conventional techniques, in conjunction with migrating data between the main and cache memories. The term "thrashing" refers to a condition caused by excessive fill and eviction traffic that can be generated when a size of a working set of data exceeds a size of the cache memory. Thrashing can result in a dramatic increase in a number of cache misses, which cause the system to spend more time performing cache fill and eviction for the resulting misses than performing originally requested computing operations for the working set of data. Through application of profiling cache replacement, data in cache and main memories can be managed in a manner that reduces thrashing and improves overall system performance.
- By way of example, a memory user, such as a client application, can request access to a particular page loaded in memory. In this example, assume that the client application requests access to the particular page from memory as part of an initialization process. During the initialization process, the client application may request access to the particular page many times. In order to most efficiently service the client application's requests, the particular page can be loaded into cache memory, e.g., so that requests for the particular page can be serviced more quickly than if serviced from main memory. After completion of the initialization process, the client application may not need to access the particular page, or may access it little relative to other pages. During normal operation, the client application may instead request access to other pages more, such that the requests to access the other pages eventually exceed the requests for the particular page-since the initialization process has completed and the particular page is no longer requested.
- Efficiency of the client application's normal operation may thus be improved by servicing the requests for the other pages from cache memory, e.g., since the cache memory, although smaller in size than the main memory, has a lower latency, higher bandwidth, or lower power usage than the main memory. In other words, efficiency may be improved by swapping one of the other pages with the particular page. As used herein "swapping" refers to an action or series of actions that enables data corresponding to the one other page to be filled into the cache memory and evicting the data corresponding to the particular page from the cache memory. In some scenarios, the data corresponding to the one other page is simply exchanged with the data corresponding to the particular page, such that the data of the one other page takes the place of the particular page's data in the cache memory and the particular page's data takes the place of the data of the one other page in the main memory. In such scenarios, when a page is cached, its data resides solely in the cache memory, and not in the main memory.
- In other scenarios, however, at least some data for each page in the memory exists in the main memory regardless of whether a page corresponding to the data is cached or not. In these scenarios, when a page is cached, a page in the cache memory and a page in the main memory together provide the valid data of the page. In some instances, however, some of the pages in the cache memory may not contain valid data, because those pages are not filled. Some of the pages in the main memory also may not contain valid data, such as when an entirety of those pages' data has been modified in the cache memory. Under these scenarios, swapping the particular page's data with the data corresponding to the one other page is not simply an exchange of data as in the first described scenario. Rather, swapping involves evicting the data corresponding to the particular page from the cache memory to a respective page in the main memory. At the respective page, the portions of the data that were modified while in the cache memory are copied to the respective page in the main memory-the copying being limited to the modified blocks, since the respective page still maintains the same data for the unmodified blocks. After the eviction, the page of cache memory from which the data was evicted is filled with the data of the one other page from the main memory. The respective page (e.g., the page into which the evicted data is placed) and the one other page (e.g., the main-memory data that was used to fill the available cache-memory page) do not correspond to the same pages, however.
- Regardless of the manner in which the main and cache memories are configured and thus the manner in which their pages are swapped, after the swapping the requests for the one other page can be serviced from the cache memory. To determine when and which pages are swapped between main and cache memories, profiling cache replacement counts accesses to the pages of data maintained in both the main and cache memories. In particular, accesses to the pages in "addressable memory" are tracked with counters. The term "addressable memory" refers to the portions of memory recognized by memory users. In the first scenario described just above, the size of addressable memory corresponds to a size of the main memory plus the size of the cache memory. In the other scenarios, however, the size of addressable memory corresponds to the size of just the main memory. Thus, a number of counters are maintained that corresponds to a number of pages in the main memory. To do so, a profiler can maintain counters for each of the pages in the addressable memory and when access to one of those pages is requested, increment the counter for the page. In the above-discussed other memory configuration and swapping scenarios, a counter is maintained for each page in addressable memory, e.g., a counter is maintained for each page in the main memory since the cached pages are also represented by data in the main memory.
- Returning to the discussion of the client application that requests access to the particular page many times during initialization, does not request access or requests access little to the particular page after initialization, and requests access to other pages more during normal operation. The counters maintained by the profiler at some point can indicate that access to at least one of the other pages has been requested more than access to the particular page. Once this is the case, a mover that is configured to move pages from main memory to cache memory, and vice versa, can swap one of the other pages that is more requested than the particular page with the particular page. In particular, the mover can move the more-requested page from main memory into cache memory, and can move the particular page from cache memory into main memory. To keep operation of the client application from being interrupted, requests made by the client application are not blocked, and the act of swapping pages to optimize the data maintained in the cache memory is performed in the background. In particular, when a page access request results in a cache miss, the requested page is not immediately loaded into the cache memory so that the request can be serviced from the cache memory. Instead, the request is serviced directly from the main memory. Further, the mover is limited to initiating page swaps at predetermined time intervals, e.g., once every microsecond (µs). As a result, the mover does not compete with the client application for access to the main or cache memories at other times. Using such techniques, interruption of computing operations may be avoided and thrashing reduced, thereby improving the overall performance of the system.
- This is but one simple example of ways in which profiling cache replacement can be performed, other examples and details are provided below. This document now turns to an example environment that references diagrams showing page counter values, after which devices and methods, and an example computing system are described.
-
Fig. 1 is an illustration of anexample environment 100 in which profiling cache replacement can be employed.Environment 100 illustrates a memory-profilingcomputing device 102 having profiled memory 104. In the particular example ofFig. 1 , the memory-profilingcomputing device 102 is configured as a smartphone, however, other configurations are contemplated. Other configurations of the memory-profilingcomputing device 102 that are capable of optimizing memory using profiling cache replacement are illustrated in later figures. -
Environment 100 also illustrates components of the profiled memory 104. The profiled memory 104 includescache memory 106 andmain memory 108. Thecache memory 106 and themain memory 108 are capable of storing data for access by memory users, such as by an operating system, client applications, and so on. For example, thecache memory 106 and themain memory 108 are capable of storing pages of data. As used herein, the term "pages" refers to same-sized blocks of data, e.g., 4-kilobyte (KB) blocks of data. Relative to themain memory 108, thecache memory 106 is smaller-it has less storage and is thus capable of storing fewer pages of data than themain memory 108. Although smaller in terms of storage than themain memory 108, the cache memory has at least one of lower latency, higher bandwidth, or lower power usage than themain memory 108. Due to these characteristics of thecache memory 106, servicing a larger portion of data access requests with cached data rather than with un-cached data results in more efficient request servicing, in terms of speed, power, or some other measure of system efficiency, for the memory-profilingcomputing device 102. - The profiled memory 104 also includes
mapper 110,profiler 112, andmover 114. Themapper 110, theprofiler 112, and themover 114 represent functionality to optimize the performance of the profiled memory 104 by caching highly-used pages and leaving less-used pages in themain memory 108. Themapper 110 is used for each memory access. Themapper 110 maps an input address to an address in the cache memory 106 (e.g., a cache hit) or to an address in the main memory 108 (e.g., a cache miss). In contrast to conventional techniques, when a page access request results in a cache miss, the requested page is not immediately loaded into thecache memory 106 so that the request can be serviced from thecache memory 106. Instead, the request is serviced directly from themain memory 108, e.g., the requested page is provided to a memory user that requested access to the page directly from themain memory 108. - The
profiler 112 represents functionality to collect information about memory accesses. By way of example, theprofiler 112 tracks numbers of page accesses, such as a number of requests to access the pages in the profiled memory 104. Although the techniques are described herein using the example in which the number of requests to access each page is tracked, theprofiler 112 may track memory access in different ways without departing from the scope of the technique described herein. When sizes of requests are not uniform across each request, for instance, such as when some requests are for 64 B and other requests are for 128 B, theprofiler 112 can count bytes instead of the number of requests. Alternately, theprofiler 112 can treat larger requests to access pages in memory as multiple requests. In other examples, memory access can be tracked without counting each request to access a page. Rather, read requests for a page can be tracked but write requests not tracked. Likewise, write requests for a page can be tracked while read requests are not tracked. - Regardless of the unit used to track memory access to pages, the
profiler 112 can maintain a counter for each page in thecache memory 106 and each page in themain memory 108. When access to a page is requested, theprofiler 112 can increment the counter for that page thereby tracking the page accesses. In one or more implementations, however, theprofiler 112 maintains fewer counters than one per page of memory. In so doing, theprofiler 112 can reduce an amount of memory used to store tracking information that describes page accesses of the profiled memory 104. Details of the manner in which theprofiler 112 uses fewer than one counter for each page of memory are discussed herein below. - The
mover 114 represents functionality to move pages between thecache memory 106 and themain memory 108. For example, themover 114 is capable of swapping highly-requested pages of themain memory 108 with little-requested pages of thecache memory 106. As used herein, the term "highly" requested page or pages refers to pages that are requested more than others in the portion of memory under consideration, and may correspond to a most-requested page in the portion under consideration, a top ten percent requested page in the portion, and so on. For example, a highly-requested page in themain memory 108 may be a page for which the number of requests ranks in the top ten percent among the pages in themain memory 108, or it may be the most requested page in themain memory 108. Similarly, the term "little" requested page or pages refers to pages that are requested less than others in the portion of memory under consideration, and may correspond to a least-requested page in the portion under consideration, a bottom ten percent requested page in the portion, and so on. By way of example, a little-requested page in thecache memory 106 may be a page for which the number of requests ranks in the bottom ten percent among the pages in thecache memory 106, or it may be the least requested page in the cache memory. It should be noted, however, that "highly" and "little" are used in conjunction with the portion of memory corresponding to the pages. Thus, a highly-requested page in themain memory 108 can have a fewer number of requests for access than a little-requested page in thecache memory 108, or may have a similar number of requests. However, a highly-requested page in themain memory 108 has a greater number of requests for access than a little requested page in themain memory 108. Additionally, a highly-requested page in the profiled memory 104 (e.g., among the pages in themain memory 108 and the cache memory 106) has a greater number of requests for access than little-requested pages in the profiled memory 104. - In any case, the
mover 114 may swap highly-requested pages of themain memory 108 with little-requested pages of thecache memory 106, responsive to a determination that the highly-requested pages of themain memory 108 are requested for access more than the little-requested pages of thecache memory 106. In addition to moving pages between thecache memory 106 and themain memory 108, themover 114 also represents functionality to make determinations as to whether pages in themain memory 108 are accessed more than the pages in thecache memory 106. Themover 114 may do so by checking the information collected by theprofiler 112, e.g., the counters. Once themover 114 moves pages (e.g., swaps a page in thecache memory 106 with a page in the main memory 108), themover 114 updates the address information used by themapper 110, so that future memory access requests are mapped to the correct address in thecache memory 106 or themain memory 108. It should be noted that responsive to a determination that the highly-requested pages of themain memory 108 are not requested for access more than the little-requested pages of thecache memory 106, themover 114 does not swap pages. - For context, consider
Figs 2 and3 , which illustrate diagrams of example page counter values for main and cache memories. Diagram 200 ofFig. 2 shows the example page counter values for the main and cache memories at a first time, and diagram 300 ofFig. 3 shows the example page counter values for the main and cache memories at a second time, subsequent to the first time. As discussed in more detail below, themover 114 checks the information about requested page accesses. Themover 114 also initiates page swaps at a predetermined interval of time, e.g., once every microsecond (µs). With reference to the predetermined interval of time, the first time corresponding to the diagram 200 may be before the occurrence of a particular such predetermined time interval while the second time corresponding to the diagram 300 is after the occurrence of the particular predetermined time interval. - The diagram 200 includes a
first axis 202 that corresponds to counter values which indicate the number of page accesses. Asecond axis 204 of the diagram 200 represents the pages maintained in memory. The diagram 200 also includes dividingline 206. The bars illustrated to the left of thedividing line 206 represent counter values of pages in thecache memory 106 at the first time, while the bars illustrated to the right of the dividing line represent counter values of pages in themain memory 108 at the first time. In the particular example illustrated byFigs 2 and3 , thecache memory 106 is 128 megabytes (128 MB), themain memory 108 is 4 gigabytes (4 GB), and each page of data is 4 kilobytes (4 KB). Thus, the bars to the left of thedividing line 206 represent 128 MB worth of 4-KB pages, while the bars to the right of thedividing line 206 represent 4 GB worth of 4-KB pages (1 million pages). It should be appreciated that these are merely exemplary sizes and that sizes of thecache memory 106, themain memory 108, and the pages maintained therein can vary from the sizes used in the example without departing from the scope of the techniques described herein. - For greater ease in understanding the explained concepts,
Figs 2 and3 have been illustrated to represent the above described scenario in which data corresponding to pages in themain memory 108 is simply exchanged with the data corresponding to pages in thecache memory 106, such that when a page is cached, its data is incache memory 106, but not in themain memory 108. In these scenarios, memory users recognize addressable memory as the combination of 4 GB and 128 MB (e.g., a combination of themain memory 108 and the cache memory 106). In implementation, however, when thecache memory 106 is small relative to the main memory 108 (128 MB versus 4 GB), themain memory 108 and thecache memory 106 may be configured according to the other memory configuration and swapping scenarios described above. - In the other memory configuration and swapping scenarios described above, a page in the
cache memory 106 and a page in themain memory 108 together provide the valid data of a cached page. In these other scenarios, when pages are cached, at least some data corresponding to the cached pages may be in thecache memory 106 and at least some data corresponding to the cached pages may also be in themain memory 108. In some instances, however, some of the pages in thecache memory 106 may not contain valid data, because those pages are not filled. Some of the pages in themain memory 108 also may not contain valid data, such as when an entirety of those pages' data has been modified in thecache memory 106. In accordance with these other configuration and swapping scenarios, and given the example in which themain memory 108 is 4 GB and the cache memory is 128 MB, memory users recognize the addressable memory as simply 4 GB (e.g., solely the main memory 108). When filling a 4-KB page of data from themain memory 108 into thecache memory 106, themover 114 may not fill each portion of the page's 4 KB of data into thecache memory 106. For example, themover 114 may instead fill 1 KB of the page's data into thecache memory 106. The mover may fill the 1 KB of data that is close to a request address, such as if there is some uncertainty as to whether the other 3 KB will even be used. When it is anticipated that most of the requests to a page will be write requests, themover 114 may not fill the page's data that will be overwritten into thecache memory 106. Rather, themover 114 can fill into thecache memory 106 information to indicate where to find each 64-byte portion of the page. - Returning to the illustrated example of
Figs 2 and3 , the bars that represent the pages are ordered from left to right in descending counter value order, and also within the bounds of thecache memory 106 and themain memory 108. Thus,bar 208, which represents the number of page accesses of the most-accessed page in thecache memory 106 corresponds to a greater counter value thanbar 210, which represents the number of page accesses for the second-most-accessed page in thecache memory 106. Similarly,bar 212, which represents the number of page accesses of the second-least-accessed page in themain memory 108, corresponds to a greater counter value thanbar 214, which represents the number of page accesses of the least-accessed page in themain memory 108. - Of particular note are
bars cache memory 106 and a most-requested page in themain memory 108, respectively, at the first time. In the illustrated example, thebar 216 is smaller than thebar 218, and thus represents that access to the least-requested page in thecache memory 106 has, at the first time, been requested less than access to the most-requested page in themain memory 108. - The diagram 300 is similar to the diagram 200, it includes axes that represent counter values and pages maintained in memory, the dividing line between the pages maintained in the
cache memory 106 and themain memory 108, and so on. The diagram 300 differs from the diagram 200 in a notable respect, however. In the diagram 300 thebars cache memory 106 and themain memory 108. In other words, the diagram 300 illustrates a scenario in which the page represented by thebar 216 is moved into the main memory 108 (e.g., by the mover 114) and the page represented by thebar 218 is moved into the cache memory 106 (e.g., by the mover 114). - As mentioned above, the diagram 200 corresponds to a first time and the diagram 300 corresponds to a second time that is subsequent to the first time. For the purpose of clarity, it may be assumed in
Figs. 2 and3 that requests to access the pages represented by the illustrated bars have not been made between the first and second times. In other words, the counter values for the pages represented in the diagrams 200, 300 are the same at both times. Between the first time and the second time, however, a moving period, the time during which themover 114 swaps pages between thecache memory 106 and themain memory 108, is assumed to have occurred. The first time and the second time may thus represent slices of time, respectively, directly before and directly after the moving period. Between the first and second times, therefore, the profiling cache replacement techniques are applied by themover 114 to optimize the cached pages for the memory-profilingcomputing device 102. - With regard to the example memory-profiling
computing device 102 ofFig. 1 , consider a detailed illustration inFig. 4 . The memory-profilingcomputing device 102 can be one or a combination of various devices, here illustrated with six examples: a smartphone 102-1, a computing watch 102-2, a digital camera 102-3, a laptop 102-4, a tablet computer 102-5, and a desktop computer 102-6 though other computing devices and systems, such as a netbook, a gaming console, or a set-top box may also be used. As noted above, in some embodiments the techniques operate, at least in part, through a remote computing device. The remote computing device can be configured as a server, for example. In such cases, some computing can be forgone locally, e.g., by communicating data enabling the computing through a communication device having limited computing operations or even communicating the data enabling the computing directly from memory-profilingcomputing devices 102 to the server. - The memory-profiling
computing device 102 includes or is able to communicate with a display 402 (five are shown inFig. 4 ), atransceiver 404, one ormore processors 406, and computer-readable storage media 408 (CRM 408). Thetransceiver 404 is capable of sending and receiving data directly or through a communication network, such as client application data fromdevices 102 through a local area, wide area, personal area, cellular, or near-field network. - In one or more implementations, the
cache memory 106, themain memory 108, theprofiler 112, themapper 110, and themover 114 are embodied on theCRM 408. Thecache memory 106 includes cached pages 410 and the main memory includes main-memory loaded pages 412 (MM-loaded pages 412). Theprofiler 112 includesmemory access information 414, which is collected by theprofiler 112 about memory accesses. By way of example, thememory access information 414 includes counters that indicate numbers of requests to access the cached pages 410 and the MM-loadedpages 412. TheCRM 408 also includesinput address mapping 416, which maps input addresses (such as those provided by a memory user to access pages of information in the cache or main memories) to an address of one of the cached pages 410 in the cache memory 106 (a cache hit), or one of the MM-loadedpages 412 in the main memory 108 (a cache miss). - As discussed above, the
mapper 110 is used for each memory access, and represents functionality to map an input address to an address in thecache memory 106 or themain memory 108. When themapper 110 receives an input address (e.g., for requesting access to a page of data from memory), themapper 110 may refer to theinput address mapping 416 and return a corresponding address of thecache memory 106 or of themain memory 108. - The
profiler 112 is also employed with each memory access. In particular, the profiler tracks the number of accesses to the cached pages 410 and the MM-loadedpages 412. In one or more implementations, theprofiler 112 maintains, as part of thememory access information 414, respective counters for each of the cached pages 410 and each of the MM-loadedpages 412. In this scenario, when one of the cached pages 410 or one of the MM-loadedpages 412 is accessed, theprofiler 112 increments the respective counter to indicate the access. In some implementations, however, maintaining an incrementable counter for each of the cached pages 410 and each of the MM-loadedpages 412 may consume too much storage space. If theprofiler 112 uses 8-bit counters, the main memory is 4 GB, and each of the MM-loadedpages 412 is 4 KB, for example, then 1 MB of memory is used simply to store the counters for one million pages-which may not be suitable in some implementations, e.g., when thememory access information 414 is stored in static random-access memory (SRAM). Accordingly, theprofiler 112 can track and maintain thememory access information 414 in manners that utilize less storage. Theprofiler 112 may, for instance, implement counters that reduce total counter storage through dynamic expansion of a range of the counters or such that there are fewer counters than one for each page of memory. - With regard to reducing total counter storage through dynamic expansion, the
profiler 112 may in one or more default implementations use 8-bit counters, and in one or more other implementations use dynamically expanding counters. To implement counters for which the range expands dynamically, floating point representations can be used. Broadly speaking, access counts of the pages in thecache memory 106 and themain memory 108 have a high dynamic range, e.g., highly-accessed pages can be accessed significantly more than little-accessed pages, and the number of accesses of highly-accessed pages can continue to increase during system operation. - In one or more implementations, including implementations in which dynamically expanding counters are employed, the data in memory can be divided into sets. In other words, the cached pages 410 and the MM-loaded
pages 412 can be divided into sets of pages, such that each set includes some of the cached pages 410 and some of the MM-loadedpages 412. In the continuing example in which thecache memory 106 is 128 MB, themain memory 108 is 4GB, and each page is 4 KB, for instance, the pages can be divided into sets such that each set includes 512 of the MM-loadedpages 412 and 16 of the cached pages 410. When the data in memory is divided into sets, themover 114 can swap the cached pages 410 in a set with the MM-loadedpages 412 that are also in the set. Consequently, when checking counters, themover 114 may, in this example, check 512 counters to determine the most- and least-requested pages. Themover 114 does not, however, swap the cached pages 410 in the set with the MM-loadedpages 412 from other sets. - In dynamically expanding counter implementations, the
profiler 112 can keep a common scale S for each set of pages, and an N-bit counter C for each page in a set. By way of example, the profiler can implement the common scale S for a set of pages using 6 bits. Broadly speaking, theprofiler 112 can use a common scale when pages are divided into sets because counter values are compared within a set, e.g., since solely the pages in thecache memory 106 and themain memory 108 of a same set are swapped. Having the common scale S and the N-bit counter C, theprofiler 112 can maintain counters such that their values equal C × 2 S. - In contrast to default implementations of profiling cache replacement in which the
profiler 112 increments the counter value by 1 for each access of a page, in implementations in which dynamically expanding counters are used, theprofiler 112 increases the page's counter C with a probability ofprofiler 112 to generate S random bits, and then increase the counter solely when each of the S bits is zero. When the counter C of a page overflows (e.g., when the previous N-bits are not enough to represent page accesses), theprofiler 112 can increase the common scale S for the page's set by one, and divide each of the counter values for the particular page causing the overflow as well as for the other pages of the set by two. - Consider a scenario in which this scheme for dynamically expanding counters is employed. The
profiler 112 can store each counter value in a variety of different ways. For example, theprofiler 112 can store a counter value equal to C, in which C is simply a binary integer. Theprofiler 112 can also store a counter value equal to C × 2 S , where C is again a binary integer. Theprofiler 112 can also increase a dynamic range of individual counters using a simple floating point representation for C. The counter value stored by theprofiler 112 can still equal C × 2 S , however, theprofiler 112 can encode C as: -
- If, in this scenario, it is assumed that the
profiler 112 uses a 4-bit significand K (so that it can range in value from 0-15), and 3 bits for the exponent (so that it can range in value from 0-7), a 7-bit counter C can represent page access values in a range of [0, 15 × 27]. - Consider an alternate scenario for employing dynamically expanding counters. In this alternate scenario, the
profiler 112 encodes an individual counter C in a different manner. In particular, the encoding depends on a number of bits allotted for the significand K, which is represented herein as nK, and a number of bits allotted for the exponent E, which is represented herein as nE. If the exponent E is equal to zero, then theprofiler 112 simply encodes the value of the counter C so that it is equal to the significand K, such that C=K. If the exponent E is greater than zero, however, theprofiler 112 encodes the value of the counter C as follows: - If, in this alternate scenario, it is assumed that the number of bits allotted for the significand nK is 4 bits and the number of bits allotted for the exponent nE is 3 bits, then a counter can store values in a range of [0,1984]. As mentioned above, in addition to reducing a size (in terms of number of bits) of individual counters, a number of counters can also be reduced in some storage-sensitive implementations, e.g., from one counter per page of memory to less than one counter per page of memory.
- With regard to using fewer than one counter per page of memory, doing so is based on an observation that working sets of data used by memory users (e.g., client applications) in conjunction with common workloads are unlikely to equal or exceed a size of the
main memory 108. Furthermore, since profiling cache replacement involves caching the highly-requested pages of themain memory 108 into thecache memory 106 rather than simply caching pages into thecache memory 106 simply because an access to those pages is requested, numbers of accesses of rarely requested pages in themain memory 108 are largely irrelevant for the techniques described herein. Accordingly, theprofiler 112 may maintain counters for the pages that are accessed more often. - To reduce the number of counters used to track the pages in the
cache memory 106 and themain memory 108, tags may be used to identify a page with which each of the counters is associated. When access is requested to a page that is associated with a counter, theprofiler 112 updates the counter in one of the manners described above to indicate the access. When access is requested to a page that is not associated with a counter (e.g., a page for which accesses are not currently being tracked by a counter), however, one of the counters that is already used to track accesses to a different page may be disassociated with the different page and associated with the requested but previously unassociated page. - Some conventional techniques for disassociating counters with tracked pages and associating them with requested but previously unassociated pages can cause thrashing. In one or more implementations, profiling cache replacement involves applying one or more modified counter tagging techniques to disassociate counters with tracked pages and associated them with requested but previously unassociated pages. These modified counter tagging techniques can reduce thrashing in comparison to conventional techniques.
- The
profiler 112 applies the modified counter tagging techniques by maintaining a number of counters N as part of thememory access information 414. Each counter comprises a data pair representing a page tag that identifies a page with which the counter is associated and a count associated with the page, e.g., in the form {page, count}. When access to a particular page X is requested, theprofiler 112 checks to see if there is a counter {X, C} that is associated with the particular page X. If there is a counter associated with the particular page X, e.g., {X, C} exists, then theprofiler 112 increments the count C by one. If there is no counter associated with the particular page X, theprofiler 112 finds a counter {Y, C} for a page Y having a smallest count C. Theprofiler 112 then replaces the value of the counter so that it is associated with the particular page X and indicates one access of the particular page X, e.g., the profiler adjusts the pair of values of {Y, C} to {X, 1}. This is different from conventional techniques which inherit a previous count. In other words, instead of replacing C with 1 as the modified counter tagging techniques do, conventional techniques replace C with C+1. By counting page accesses with the modified counter tagging techniques, a number of counters N with the largest counts correspond to the top-n pages. - Regardless of how the counters are implemented, the
mover 114 represents functionality to check the counters maintained in thememory access information 414 for determining whether to swap pages between themain memory 108 and thecache memory 106. As mentioned above, themover 114 performs these checks to make the determinations to initiate page swaps at a predetermined interval of time, such as every microsecond (1 µs). Although the examples discussed herein refer to the interval of time being predetermined and corresponding to 1 µs, the interval of time may be different without departing from the scope of the techniques described herein. By way of example, the predetermined interval can also be determined randomly, based on a number of accesses as discussed in more detail below (e.g., a total number of accesses requested for pages from the profiled memory 104), and so on. - At the predetermined interval of time (each microsecond), the
mover 114 can make the determinations in accordance with the techniques described herein. Rather than using an absolute time (e.g., the predetermined interval of time), themover 114 can alternately make the determinations every N memory accesses per set of pages. In this way, the techniques described herein can control a percentage of the background bandwidth used instead of an absolute value of the background bandwidth used. - Regardless of frequency, during the moving period, the
mover 114 is configured to determine whether highly-requested pages of themain memory 108 are requested more than little-requested pages in thecache memory 106. The highly-requested pages of themain memory 108 are highly-requested relative to the MM-loadedpages 412, and the little-requested pages in thecache memory 106 are little-requested relative to the cached pages 410. The number of requests for the highly-requested main memory pages may be substantially similar, however, to the number of requests for the little-requested cache memory pages. - If the highly-requested pages in the
main memory 108 are requested more than the little-requested pages in thecache memory 106, themover 114 swaps the highly-requested pages of themain memory 108 with the little-requested pages of thecache memory 106. To do so, themover 114 evicts the little-requested pages from thecache memory 106 and fills the highly-requested pages of the main memory into thecache memory 106. Since operation of themover 114 competes with demand requests from memory users (e.g., client applications), limiting performance of themover 114's determining and swapping operations to the moving period reduces themover 114's impact on the memory users-reducing the latency of memory access for the memoryprofiling computing device 102. Themover 114 thus operates in the background while memory users access pages of data from thecache memory 106 and themain memory 108. - In addition to limiting operation of the
mover 114 to the background, page access requests made by memory users are given priority over page access requests made for the purpose of optimizing memory, e.g., page swaps requested by themover 114. The requests made to access pages of data from thecache memory 106 and themain memory 108 can generally be divided into two types-demand requests and background requests. The term "demand request" refers to a request made by a memory user, such as a client application, for a page of data from thecache memory 106 or themain memory 108. The term "background request" refers to a fill or eviction request triggered indirectly by the demand request, such as the fill and eviction requests made by themover 114 in conjunction with swapping pages between themain memory 108 and thecache memory 106. - The techniques described herein give priority to fulfilling the demand requests over fulfilling the background requests in a couple ways. First, the techniques may allow a limited number of pending background requests, such as by maintaining the pending requests in a queue configured to hold the limited number of pending background requests. If the queue is full when a background request is made, the request is simply dropped, e.g., the request is not added to the queue. Indeed, other background requests are not added to the queue until at least one of the pending background requests in the full queue is serviced.
- The second way in which the techniques described herein give priority to fulfilling the demand requests over fulfilling the background requests, involves an implementation in which a dynamic random-access memory (DRAM) request queue is used, and in which the background requests are allowed to fill a limited amount (e.g., half) of the DRAM request queue. If, for example, a fill level of the DRAM request queue is more than half of the queue's maximum fill level, then the techniques described herein limit incoming requests to demand requests. Here, background requests can be stalled when the fill level of the DRAM request queue is more than half of the queue's maximum fill level. For instance, a background request initiated by the
mover 114 is dropped if there are too many mover-requested background requests pending. Nonetheless, the background requests initiated by themover 114 not dropped are disassembled into DRAM requests and sent to the DRAM request queue. If the DRAM request queue surpasses a particular threshold (e.g., half of the queue's maximum fill level), indicating that the DRAM request queue is too busy to currently handle incoming DRAM requests, further background requests are not sent to the DRAM request queue. Instead, the background DRAM requests are held until the DRAM request queue level is lower than the threshold. Once the DRAM request queue is able to again handle background DRAM requests, the held requests are sent to the DRAM request queue. - During operation, the workload that memory users impose on the profiled memory 104 can change over time. In conjunction with changing workloads, the highly-requested pages can also change. In other words, pages cached and highly-requested in the
main memory 108 while one client application operates may not be the same as the pages cached and highly-requested in themain memory 108 while another client application operates. To ensure that thecache memory 106 is filled with pages that correspond to the current operations of the memory-profilingcomputing device 102, theprofiler 112 can decay values of the page counters. By automatically decaying counter values at some decay interval, pages that were once heavily accessed can be evicted from thecache memory 106 when their use wanes. - In one or more implementations, the
profiler 112 can simply divide each of the counter values of a set in half at the decay interval. Consider an example in which the decay interval corresponds to a predefined threshold for a total number of accesses requested for the pages in a set, e.g., 2^14 accesses. In this example, when the total number of accesses for pages in a set exceeds 2^14 accesses, theprofiler 112 divides the counter values of the set by two. With reference back to implementations that use the common scale S, theprofiler 112 decays the counters by decreasing the common scale S of a set by one when the set's common scale S is greater than zero. If the set's common scale S is already zero, however, theprofiler 112 divides each of the set's counters by two. In yet other implementations, the decay interval corresponds to some predetermined absolute time rather than the predefined number of accesses. It should be appreciated that the counters can be decayed in manners other than being divided by two without departing from the scope of the techniques described herein. By way of example, rather than being divided by two the counters can be multiplied by a factor such as two-thirds ( - With further regard to further scenarios that arise during operation of the memory-profiling
computing device 102, in some cases counters for the little-requested pages of thecache memory 106 and the highly-requested pages of themain memory 108 can have the same or very similar values. With reference to a set of pages, the cached pages of the set that are little-requested (compared to other cached pages) and the set's highly-requested pages of themain memory 108 can have the same or very similar values. Difficulties can arise when themover 114 is to determine which pages to cache when cached pages have similar counter values to pages that are candidates for being cached. Consider an example in which a first page of a set is accessed slightly more than other pages of the set. As a result of having more accesses, themover 114 can cache the first page. Consider also, however, that later a second page of the set becomes accessed slightly more than the first page. As a result of the subsequent accesses, themover 114 replaces the first page with the second page, thereby evicting the first page from thecache memory 106. In a set of pages that is cyclically referenced, slightly more accesses of the first page again can cause themover 114 to replace the second page in the cache memory, and so on. This scenario can cause thrashing for these pages, and can be especially problematic when decaying counters are used and the decaying interval is short. - To prevent such thrashing, the
mover 114 can add a barrier of entry before determining to swap the highly-requested pages of themain memory 108 with the little-requested pages of thecache memory 106. Thus instead of swapping the highly-requested pages of themain memory 108 with the little-requested pages of thecache memory 106 whenever a number of requested accesses of the highly-requested main memory pages is greater than the number of requested accesses of the little-requested cached pages, themover 114 can instead determine to swap pages when the number of requested accesses of the highly-requested main memory pages is greater than number of accesses of the little-requested cached pages and the entry barrier. By way of example, themover 114 can determine to swap pages when the following is true: - In this expression, Δ represents the entry barrier and can correspond to a predetermined integer, such as an integer that offsets a resource cost (e.g., time, power, etc.) to swap the pages.
- Profiling cache replacement can be implemented in still other ways without departing from the scope of the techniques described herein. One example of an alternate implementation is to use time-multiplexed counters. Instead of maintaining counters solely for the pages that are accessed most, the
profiler 112 can maintain counters for each of the pages. At any given time, however, theprofiler 112 may have access to a limited number of the counters. Consider a scenario in which one sixteenth of the counters are maintained in SRAM and the other fifteen sixteenths are maintained in DRAM. When access is requested to a page for which the counter is in the SRAM, theprofiler 112 simply increments the counter. When access is requested to a page for which the counter is in the DRAM, however, theprofiler 112 ignores the request. Periodically, these time-multiplexed counter techniques flush the counters in the SRAM back to DRAM, and load another one sixteenth of counters from DRAM into SRAM. It is assumed that by doing so over some period of time each counter will have approximately one sixteenth of the actual access count value. Themover 114 can be configured to check the counters in both the DRAM and SRAM. - Another example of an alternate embodiment is to bundle the counters in a page table data structure. Thus, instead of maintaining separate counters, the
profiler 112 can bundle the counters in a page table data structure. In so doing, management of the counters may follow a same flow as the page table and a translation lookaside buffer (TLB). In yet another example of an alternate embodiment, counter decaying can be implemented by decreasing the common scale S gradually, resulting in a fractional S. Instead of decaying each counter value at the predetermined decaying interval, the represented counter values can be decayed gradually by decreasing the common scale S gradually. - These and other capabilities, as well as ways in which entities of
Figs. 1 and4 act and interact, are set forth in greater detail below. These entities may be further divided, combined, and so on. Theenvironment 100 ofFig. 1 and the detailed illustrations ofFig. 4 illustrate some of many possible environments capable of employing the described techniques. -
Figs. 5-7 depict methods enabling or using profiling cache replacement. These methods are shown as sets of blocks that specify operations performed but are not necessarily limited to the order or combinations shown for performing the operations by the respective blocks. In portions of the following discussion reference may be made toenvironment 100 ofFig. 1 and entities detailed inFig. 4 , reference to which is made for example only. The techniques are not limited to performance by one entity or multiple entities operating on one device. -
Fig. 5 depictsmethod 500, which describes manners in which to replace pages in cache memory according to a profiling algorithm used to manage data migration between the cache memory and a main memory. - At 502, counters maintained in association with pages of the data maintained in the main and cache memories are updated. The counters are configured to indicate a number of requests to access the pages maintained in the main and cache memories. By way of example, the
profiler 112 maintains counters in thememory access information 414. These counters indicate a number of requests to access the cached pages 410 and the MM-loadedpages 412. Responsive to requests to access one of the cached pages 410 or one of the MM-loadedpages 412, for instance, theprofiler 112 increments corresponding counters in thememory access information 414. - So as not to interfere with memory accesses of memory users, such as client applications, profiling memory replacement is performed in a background of servicing requests made by the memory for the pages maintained in the
cache memory 106 and themain memory 108. In so doing, performance of the method steps 504 and 506 can be limited to being performed at a predetermined time interval, such as every microsecond (µs). At 504, a determination is made at the predetermined time interval as to whether a number of page access requests for a highly-requested page of the main memory is greater than a number of page access requests for a little-requested page of the cache memory according to the counters. The highly-requested page of the main memory is highly-requested relative to the pages loaded in the main memory, and the little-requested page of the cache memory is little-requested relative to the pages loaded in the cache memory. The number of requests for the highly-requested main memory page may be substantially similar, however, to the number of requests for the little-requested cache memory page. - By way of example, the
mover 114 checks the counters maintained by theprofiler 112 in thememory access information 414. Every microsecond, for instance, themover 114 determines a little-requested cached page 410 (e.g., that is requested less than other pages in the cache memory 106) and a highly-requested MM-loaded page 412 (e.g., that is requested more than other pages in the main memory 108) according to the counters. Themover 114 compares the respective counter values to determine whether the highly-requested page of themain memory 108 is requested more than the little-requested page of thecache memory 106. - Responsive to a determination that a number of page access requests for the highly-requested main memory page is greater than the number of page access requests for the little-requested cache memory page, at 506, the highly-requested main memory page and the little-requested cache memory page are swapped. By way of example, the
mover 114 determines atstep 504 that the highly-requested page from themain memory 108 is requested more than the little-requested page of thecache memory 106. Responsive to this determination, themover 114 swaps the highly-requested page from themain memory 108 with the little-requested page of thecache memory 106. In other words, themover 114 evicts the little-requested cache memory page from thecache memory 106 and loads that page into themain memory 108. Themover 114 also caches the highly-requested main memory page. -
Fig. 6 depictsmethod 600, which describes manners in which page accesses are counted for profiling cache replacement using reduced-space counters. - At 602, common scales are maintained for sets of pages in memory. By way of example, the data maintained in the profiled memory 104 is divided into sets of pages, such that each set includes a plurality of pages from the
cache memory 106 and a plurality of pages from themain memory 108. In particular, the pages can be divided into sets as described in more detail above. For each set of data, theprofiler 112 maintains a common scale S, which indicates a base count or base number of accesses for the pages of the set. In one or more implementations, theprofiler 112 maintains 4-bit common scales for each set of pages. It should be appreciated that different sized common scales (in terms of number of bits) may be used without departing from the scope of the techniques describe herein. - At 604, a counter is maintained for each of the pages in a set and that indicates a number of page accesses relative to other pages in the set. By way of example, the
profiler 112 maintains an N-bit counter for each page in a set, as described in more detail above. The N-bit counter for a page, together with the common scale S of that page's set, indicates the number of accesses for the page. Responsive to a request to access a page, at 606, a corresponding counter and common scale are updated according to the access. By way of example, responsive to a request to access a page, theprofiler 112 updates an N-bit counter associated with the requested page and also updates the common scale S associated with the requested page's set. Theprofiler 112 updates N-bit counters and common scales S as described in more detail above. It should be appreciated that the techniques described herein can utilizemethod 600 in one or more storage-sensitive implementations, e.g., when thememory access information 414 is stored in SRAM. -
Fig. 7 depictsmethod 700, which describes manners in which page accesses are counted for profiling cache replacement using fewer than one counter per page of memory. Like themethod 600, themethod 700 can also be utilized for one or more storage-sensitive implementations. - At 702, n number of counters are associated with the top-n accessed pages in memory. By way of example, the
profiler 112 associates n counters with the top-n accessed pages in the profiled memory 104. In particular, counters are associated with pages of the profiled memory 104 that are cached, leaving remaining counters to associate with the next most-accessed pages in themain memory 108. - At 704, a request is received to access a page maintained in memory. By way of example, a request is received to access a page of data maintained in the profiled memory 104, such as to access one of the cached pages 410 or one of the MM-loaded
pages 412. At 706, a determination is made as to whether the requested page is associated with one of the counters, e.g., whether one of the counters has an attribute identifying the requested page. By way of example, theprofiler 112 determines whether the requested page is associated with one of the counters maintained as part of thememory access information 414. - If a determination is made that the requested page is associated with one of the counters (e.g., "yes" at 706), then at 708, the counter associated with the requested page is updated. By way of example, the
profiler 112 increments a counter value C for a counter associated with the requested page from C to C+1. If, however, a determination is made that the requested page is not associated with one of the counters (e.g., "no" at 706), then at 710, the least-accessed page associated with a counter is determined. By way of example, theprofiler 112 determines a least-accessed page associated with a counter in a same set of pages as the requested page. Alternately, theprofiler 112 simply determines the least-accessed page of the pages associated with counters. Theprofiler 112 can determine the least-accessed page by checking the counter values of the counters. - At 712, the counter of the least-accessed page is associated with the requested page. By way of example, the
profiler 112 disassociates the least-accessed page with the counter and then associates the counter with the requested page, e.g., by changing a tag of the counter to identify the requested page. At 714, a counter value of the counter is adjusted to reflect the access request. By way of example, theprofiler 112 adjusts a counter value C (which indicates a number of accesses of the page previously associated with the counter) by setting the counter value to one. Setting the counter value to one contrasts with some conventional techniques which involve setting the counter value C to C+1. In so doing, themethod 700 can reduce trashing. - The preceding discussion describes methods relating to profiling cache replacement. Aspects of these methods may be implemented in hardware (e.g., fixed logic circuitry), firmware, software, manual processing, or any combination thereof. These techniques may be embodied on one or more of the entities shown in
Figs. 1 ,4 , and8 (computing system 800 is described inFig. 8 below), which may be further divided, combined, and so on. Thus, these figures illustrate some of the many possible systems or apparatuses capable of employing the described techniques. The entities of these figures generally represent software, firmware, hardware, whole devices or networks, or a combination thereof. -
Fig. 8 illustrates various components ofexample computing system 800 that can be implemented as any type of client, server, and/or computing device as described with reference to the previousFigs. 1-7 to implement profiling cache replacement. In embodiments,computing system 800 can be implemented as one or a combination of a wired and/or wireless wearable device, System-on-Chip (SoC), and/or as another type of device or portion thereof.Computing system 800 may also be associated with a user (e.g., a person) and/or an entity that operates the device such that a device describes logical devices that include users, software, firmware, and/or a combination of devices. -
Computing system 800 includescommunication devices 802 that enable wired and/or wireless communication of device data 804 (e.g., received data, data that is being received, data scheduled for broadcast, data packets of the data, etc.).Device data 804 or other device content can include configuration settings of the device, media content stored on the device, and/or information associated with a user of the device. Media content stored oncomputing system 800 can include any type of audio, video, and/or image data, including complex or detailed results of profiling cache replacement acts.Computing system 800 includes one ormore data inputs 806 via which any type of data, media content, and/or inputs can be received, such as human utterances, user-selectable inputs (explicit or implicit), messages, music, television media content, recorded video content, and any other type of audio, video, and/or image data received from any content and/or data source. -
Computing system 800 also includescommunication interfaces 808, which can be implemented as any one or more of a serial and/or parallel interface, a wireless interface, any type of network interface, a modem, and as any other type of communication interface. Communication interfaces 808 provide a connection and/or communication links betweencomputing system 800 and a communication network by which other electronic, computing, and communication devices communicate data withcomputing system 800. -
Computing system 800 includes one or more processors 810 (e.g., any of microprocessors, controllers, and the like), which process various computer-executable instructions to control the operation ofcomputing system 800 and to enable techniques for, or in which can be embodied, profiling cache replacement. Alternatively or in addition,computing system 800 can be implemented with any one or combination of hardware, firmware, or fixed logic circuitry that is implemented in connection with processing and control circuits which are generally identified at 812. Although not shown,computing system 800 can include a system bus or data transfer system that couples the various components within the device. A system bus can include any one or combination of different bus structures, such as a memory bus or memory controller, a peripheral bus, a universal serial bus, and/or a processor or local bus that utilizes any of a variety of bus architectures. -
Computing system 800 also includes computer-readable media 814, such as one or more memory devices in addition to the profiled memory 104 that enable persistent and/or non-transitory data storage (i.e., in contrast to mere signal transmission), examples of which include random access memory (RAM), non-volatile memory (e.g., any one or more of a read-only memory (ROM), flash memory, EPROM, EEPROM, etc.), and a disk storage device. A disk storage device may be implemented as any type of magnetic or optical storage device, such as a hard disk drive, a recordable and/or rewriteable compact disc (CD), any type of a digital versatile disc (DVD), and the like.Computing system 800 can also include a massstorage media device 816. In this example, the computer-readable media 814 also includes a profiled memory 104. - Computer-
readable media 814 provides data storage mechanisms to storedevice data 804, as well asvarious device applications 818 and any other types of information and/or data related to operational aspects ofcomputing system 800. For example, anoperating system 820 can be maintained as a computer application with computer-readable media 814 and executed onprocessors 810.Device applications 818 may include a device manager, such as any form of a control application, software application, signal-processing and control module, code that is native to a particular device, a hardware abstraction layer for a particular device, and so on. -
Device applications 818 also include any system components, engines, or managers to implement the techniques. - Although embodiments of techniques using, and apparatuses enabling, profiling cache replacement have been described in language specific to features and/or methods, it is to be understood that the subject of the appended claims is not necessarily limited to the specific features or methods described. Rather, the specific features and methods are disclosed as example implementations of these techniques.
Claims (13)
- A method for managing data migration between a main memory (108) and a cache memory (106), the method comprising:maintaining (502) counters in association with pages of the data maintained in the main and cache memories, the counters indicating numbers of requests to access the pages, the pages of data maintained in the main memory and the cache memory are divided into sets of pages, each set of pages including a plurality of pages in the cache memory and a plurality of pages in the main memory; andat a predetermined time interval:determining (504) whether a number of page access requests for a highly-requested page of the main memory in a given set is greater than a number of page access requests for a little-requested page of the cache memory in a given set according to the counters; andresponsive (506) to a determination that the number of page access requests for the highly-requested page of the main memory in the given set is greater than the number of page access requests for the little-requested page of the cache memory in the given set, swapping the highly-requested page of the main memory in the given set with the little-requested page of the cache memory in the given set.
- The method as described in claim 1, wherein the little-requested page of the cache memory corresponds to a least-requested page (216) of the cache memory (106), and the highly-requested page (218) of the main memory (108) corresponds to a most-requested page of the main memory.
- The method as described in claim 1, wherein the maintaining includes incrementing a counter associated with a given page responsive to a request to access the given page.
- The method as described in claim 1, wherein the determining and the swapping are performed in a background of servicing requests made by memory users to access the pages maintained in the main and cache memories.
- The method as described in claim 1, further comprising:receiving a request to access at least one of the pages maintained in the main memory; andservicing the request for the at least one requested page from the main memory without first filling the at least one requested page into the cache memory.
- The method as described in claim 1, wherein pages of the main memory (108) are not swapped with pages of the cache memory (106) in different sets.
- A system comprising:main (108) and cache (106) memories configured to maintain pages of data, the cache memory configured to maintain fewer of the pages than the main memory, and having at least one of lower latency, higher bandwidth, or lower power usage than the main memory,wherein the pages of data maintained in the main memory and the cache memory are divided into sets of pages, each set of pages including a plurality of pages in the cache memory and a plurality of pages in the main memory;a profiler (112) configured to maintain (502) counters in association with the pages maintained in the main and cache memories, the counters configured to indicate numbers of requests to access the pages; anda mover (114) configured to:make a determination (504) as to whether a number of page access requests for a highly-requested page in the main memory in a given set is greater than a number of page access requests for a little-requested page in the cache memory in the given set according to the counters; andresponsive (506) to a determination that the number of page access requests for the highly-requested page in the main memory in the given set is greater than the number of page access requests for the little-requested page in the cache memory in the given set, swap the highly-requested page in the main memory in the given set with the little-requested page in the cache memory in the given set.
- The system as described in claim 7, wherein the profiler (112) is further configured to maintain a respective counter for each of the pages in the main memory.
- The system as described in claim 7, wherein the profiler (112) is further configured to maintain fewer of the counters than one counter per page of data maintained in the main memory.
- The system as described in claim 7, wherein the mover (114) is further configured to make the determination at a predetermined time interval.
- The system as described in claim 7, wherein the mover (114) is further configured to check a subset of the counters at the predetermined time interval as part of making the determination.
- The system as described in claim 7, further comprising a mapper (110) configured to map an input address associated with a memory access request to a corresponding page in the main memory or the cache memory effective to service the memory access request.
- One or more computer-readable storage media comprising:main (108) and cache (106) memories configured to maintain pages of data, the cache memory configured to maintain fewer of the pages than the main memory, and having at least one of lower latency, higher bandwidth, or lower power usage than the main memory,wherein the pages of data maintained in the main memory and the cache memory are divided into sets of pages, each set of pages including a plurality of pages in the cache memory and a plurality of pages in the main memory; andinstructions that, responsive to being executed on one or more processors fill pages into the cache memory according to a profiling cache replacement technique by performing operations in which:counters are maintained (502) in association with the pages of the main and cache memories to indicate numbers of requests to access the pages;a determination (504) is made at a predetermined time interval as to whether a number of page access requests for a highly-requested page of the main memory in a given set is greater than a number of page access requests for a little-requested page of the cache memory in the given set according to the counters; andthe highly-requested page of the main memory in the given set is swapped (506) with the little-requested page of the cache memory in the given set responsive to a determination that the number of page access requests for the highly-requested page of the main memory in the given set is greater than the number of page access requests for the little-requested page of the cache memory in the given set.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201662293688P | 2016-02-10 | 2016-02-10 | |
US15/097,177 US10387329B2 (en) | 2016-02-10 | 2016-04-12 | Profiling cache replacement |
PCT/US2016/068625 WO2017139037A1 (en) | 2016-02-10 | 2016-12-26 | Profiling cache replacement |
Publications (2)
Publication Number | Publication Date |
---|---|
EP3414665A1 EP3414665A1 (en) | 2018-12-19 |
EP3414665B1 true EP3414665B1 (en) | 2021-11-10 |
Family
ID=57799876
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
EP16826601.3A Active EP3414665B1 (en) | 2016-02-10 | 2016-12-26 | Profiling cache replacement |
Country Status (9)
Country | Link |
---|---|
US (1) | US10387329B2 (en) |
EP (1) | EP3414665B1 (en) |
JP (1) | JP6613375B2 (en) |
KR (1) | KR102043886B1 (en) |
CN (1) | CN107066397B (en) |
DE (2) | DE202016107157U1 (en) |
GB (1) | GB2547306B (en) |
TW (1) | TWI684099B (en) |
WO (1) | WO2017139037A1 (en) |
Families Citing this family (15)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10387329B2 (en) | 2016-02-10 | 2019-08-20 | Google Llc | Profiling cache replacement |
US9826932B2 (en) | 2016-04-19 | 2017-11-28 | Google Llc | Automated abdominojugular reflux testing |
CN107992434A (en) * | 2017-11-24 | 2018-05-04 | 郑州云海信息技术有限公司 | Lower brush method, apparatus and storage medium for distributed layer storage system |
CN108089998A (en) * | 2017-12-13 | 2018-05-29 | 郑州云海信息技术有限公司 | A kind of Linux pagings replacement method and system |
CN108829344A (en) * | 2018-05-24 | 2018-11-16 | 北京百度网讯科技有限公司 | Date storage method, device and storage medium |
US11475014B2 (en) * | 2018-12-20 | 2022-10-18 | AVAST Software s.r.o. | Updating a toplist for a continuous data stream |
KR20200091199A (en) * | 2019-01-22 | 2020-07-30 | 에스케이하이닉스 주식회사 | Storage device, computing system having the storage device and operating method thereof |
US10908821B2 (en) | 2019-02-28 | 2021-02-02 | Micron Technology, Inc. | Use of outstanding command queues for separate read-only cache and write-read cache in a memory sub-system |
US11288199B2 (en) | 2019-02-28 | 2022-03-29 | Micron Technology, Inc. | Separate read-only cache and write-read cache in a memory sub-system |
US11106609B2 (en) * | 2019-02-28 | 2021-08-31 | Micron Technology, Inc. | Priority scheduling in queues to access cache data in a memory sub-system |
US10970222B2 (en) | 2019-02-28 | 2021-04-06 | Micron Technology, Inc. | Eviction of a cache line based on a modification of a sector of the cache line |
US11526290B2 (en) * | 2019-06-29 | 2022-12-13 | Intel Corporation | System and method to track physical address accesses by a CPU or device |
US11237981B1 (en) | 2019-09-30 | 2022-02-01 | Amazon Technologies, Inc. | Memory scanner to accelerate page classification |
US11899589B2 (en) | 2021-06-22 | 2024-02-13 | Samsung Electronics Co., Ltd. | Systems, methods, and devices for bias mode management in memory systems |
CN113489572B (en) * | 2021-08-23 | 2022-12-20 | 杭州安恒信息技术股份有限公司 | Request sending method, device, equipment and storage medium |
Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
EP1098250A1 (en) * | 1999-11-05 | 2001-05-09 | Emc Corporation | Cache using multiple logical ring units |
US20030093621A1 (en) * | 2001-11-09 | 2003-05-15 | International Business Machines Corporation | Different caching treatment of memory contents based on memory region |
WO2011104741A1 (en) * | 2010-02-23 | 2011-09-01 | Hitachi, Ltd. | Management system for storage system and method for managing storage system |
US20120017039A1 (en) * | 2010-07-16 | 2012-01-19 | Plx Technology, Inc. | Caching using virtual memory |
Family Cites Families (22)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP3226525B2 (en) | 1988-10-07 | 2001-11-05 | 株式会社日立製作所 | Main memory management method |
US5247687A (en) | 1990-08-31 | 1993-09-21 | International Business Machines Corp. | Method and apparatus for determining and using program paging characteristics to optimize system productive cpu time |
US6134602A (en) | 1997-09-24 | 2000-10-17 | Microsoft Corporation | Application programming interface enabling application programs to group code and data to control allocation of physical memory in a virtual memory system |
EP1505506A1 (en) | 2003-08-05 | 2005-02-09 | Sap Ag | A method of data caching |
US20050108478A1 (en) | 2003-11-13 | 2005-05-19 | International Business Machines Corporation | Dynamic frequent instruction line cache |
JP2008090554A (en) | 2006-09-29 | 2008-04-17 | Toshiba Corp | Information processor, controller, and memory management method |
US8271729B2 (en) * | 2009-09-18 | 2012-09-18 | International Business Machines Corporation | Read and write aware cache storing cache lines in a read-often portion and a write-often portion |
JP5183650B2 (en) * | 2010-02-17 | 2013-04-17 | 株式会社日立製作所 | Computer system, backup method and program in computer system |
US10169087B2 (en) * | 2011-01-28 | 2019-01-01 | International Business Machines Corporation | Technique for preserving memory affinity in a non-uniform memory access data processing system |
WO2012111113A1 (en) | 2011-02-16 | 2012-08-23 | 富士通株式会社 | Memory management program, memory management method, information processing device, and computer-readable recording medium upon which memory management program has been recorded |
JP5376681B2 (en) | 2011-02-28 | 2013-12-25 | エヌイーシーコンピュータテクノ株式会社 | Information processing apparatus and error correction support method |
US20130091331A1 (en) * | 2011-10-11 | 2013-04-11 | Iulian Moraru | Methods, apparatus, and articles of manufacture to manage memory |
US8832411B2 (en) * | 2011-12-14 | 2014-09-09 | Microsoft Corporation | Working set swapping using a sequentially ordered swap file |
US9098406B2 (en) * | 2012-01-23 | 2015-08-04 | Empire Technology Development Llc | Managing addressable memory in heterogeneous multicore processors |
US9116792B2 (en) * | 2012-05-18 | 2015-08-25 | Silicon Motion, Inc. | Data storage device and method for flash block management |
US9235353B2 (en) * | 2012-10-12 | 2016-01-12 | Hitachi, Ltd. | Storage apparatus and management method including determination of migration of data from one storage area to another based on access frequency |
US9940286B2 (en) | 2013-03-14 | 2018-04-10 | Nvidia Corporation | PCIE traffic tracking hardware in a unified virtual memory system |
KR102094163B1 (en) | 2013-08-28 | 2020-03-27 | 삼성전자 주식회사 | Apparatus and method for managing cache in memory system based on hybrid chache, and the memory system |
US9472248B2 (en) * | 2014-03-28 | 2016-10-18 | Intel Corporation | Method and apparatus for implementing a heterogeneous memory subsystem |
GB2527788A (en) * | 2014-07-02 | 2016-01-06 | Ibm | Scheduling applications in a clustered computer system |
US9727466B2 (en) * | 2014-08-26 | 2017-08-08 | Arm Limited | Interconnect and method of managing a snoop filter for an interconnect |
US10387329B2 (en) | 2016-02-10 | 2019-08-20 | Google Llc | Profiling cache replacement |
-
2016
- 2016-04-12 US US15/097,177 patent/US10387329B2/en active Active
- 2016-12-08 GB GB1620880.3A patent/GB2547306B/en active Active
- 2016-12-16 TW TW105141709A patent/TWI684099B/en active
- 2016-12-20 DE DE202016107157.7U patent/DE202016107157U1/en active Active
- 2016-12-20 DE DE102016225545.2A patent/DE102016225545A1/en not_active Withdrawn
- 2016-12-26 JP JP2018524413A patent/JP6613375B2/en active Active
- 2016-12-26 KR KR1020187011277A patent/KR102043886B1/en active IP Right Grant
- 2016-12-26 EP EP16826601.3A patent/EP3414665B1/en active Active
- 2016-12-26 WO PCT/US2016/068625 patent/WO2017139037A1/en active Application Filing
- 2016-12-28 CN CN201611234178.8A patent/CN107066397B/en active Active
Patent Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
EP1098250A1 (en) * | 1999-11-05 | 2001-05-09 | Emc Corporation | Cache using multiple logical ring units |
US20030093621A1 (en) * | 2001-11-09 | 2003-05-15 | International Business Machines Corporation | Different caching treatment of memory contents based on memory region |
WO2011104741A1 (en) * | 2010-02-23 | 2011-09-01 | Hitachi, Ltd. | Management system for storage system and method for managing storage system |
US20120017039A1 (en) * | 2010-07-16 | 2012-01-19 | Plx Technology, Inc. | Caching using virtual memory |
Also Published As
Publication number | Publication date |
---|---|
CN107066397A (en) | 2017-08-18 |
EP3414665A1 (en) | 2018-12-19 |
GB2547306B (en) | 2019-10-09 |
JP2018537770A (en) | 2018-12-20 |
DE102016225545A1 (en) | 2017-08-10 |
KR20180056736A (en) | 2018-05-29 |
WO2017139037A1 (en) | 2017-08-17 |
GB201620880D0 (en) | 2017-01-25 |
JP6613375B2 (en) | 2019-11-27 |
CN107066397B (en) | 2020-12-04 |
KR102043886B1 (en) | 2019-12-02 |
GB2547306A (en) | 2017-08-16 |
DE202016107157U1 (en) | 2017-06-06 |
TW201732603A (en) | 2017-09-16 |
TWI684099B (en) | 2020-02-01 |
US20170228322A1 (en) | 2017-08-10 |
US10387329B2 (en) | 2019-08-20 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
EP3414665B1 (en) | Profiling cache replacement | |
CN107102955B (en) | Association and atomic write-back cache system and method for storage subsystems | |
US10409725B2 (en) | Management of shared pipeline resource usage based on level information | |
US7711902B2 (en) | Area effective cache with pseudo associative memory | |
CN105740164B (en) | Multi-core processor supporting cache consistency, reading and writing method, device and equipment | |
US10007614B2 (en) | Method and apparatus for determining metric for selective caching | |
CN111602377B (en) | Resource adjusting method in cache, data access method and device | |
US9063862B2 (en) | Expandable data cache | |
US11106600B2 (en) | Cache replacement based on translation lookaside buffer evictions | |
US9501419B2 (en) | Apparatus, systems, and methods for providing a memory efficient cache | |
US7197605B2 (en) | Allocating cache lines | |
US20070079070A1 (en) | Cache controller | |
CN113342265B (en) | Cache management method and device, processor and computer device | |
US11507519B2 (en) | Data compression and encryption based on translation lookaside buffer evictions | |
US20180113815A1 (en) | Cache entry replacement based on penalty of memory access | |
US10261915B2 (en) | Intelligently partitioning data cache to allocate space for translation entries | |
CN113039530A (en) | Free space management for compressed storage systems | |
CN113138851B (en) | Data management method, related device and system | |
US10452548B2 (en) | Preemptive cache writeback with transaction support | |
KR101976320B1 (en) | Last level cache memory and data management method thereof | |
KR102665339B1 (en) | Cache replacement based on conversion index buffer eviction | |
CN111344684A (en) | Multi-level cache placement mechanism |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: UNKNOWN |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: THE INTERNATIONAL PUBLICATION HAS BEEN MADE |
|
PUAI | Public reference made under article 153(3) epc to a published international application that has entered the european phase |
Free format text: ORIGINAL CODE: 0009012 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: REQUEST FOR EXAMINATION WAS MADE |
|
17P | Request for examination filed |
Effective date: 20180611 |
|
AK | Designated contracting states |
Kind code of ref document: A1Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
AX | Request for extension of the european patent |
Extension state: BA ME |
|
DAV | Request for validation of the european patent (deleted) | ||
DAX | Request for extension of the european patent (deleted) | ||
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: EXAMINATION IS IN PROGRESS |
|
17Q | First examination report despatched |
Effective date: 20190711 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: EXAMINATION IS IN PROGRESS |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R079Ref document number: 602016066122Country of ref document: DEFree format text: PREVIOUS MAIN CLASS: G06F0012123000Ipc: G06F0012085500 |
|
GRAP | Despatch of communication of intention to grant a patent |
Free format text: ORIGINAL CODE: EPIDOSNIGR1 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: GRANT OF PATENT IS INTENDED |
|
RIC1 | Information provided on ipc code assigned before grant |
Ipc: G06F 12/0855 20160101AFI20210312BHEPIpc: G06F 12/123 20160101ALI20210312BHEPIpc: G06F 12/02 20060101ALI20210312BHEPIpc: G06F 12/08 20160101ALI20210312BHEPIpc: G06F 12/121 20160101ALI20210312BHEPIpc: G06F 12/0897 20160101ALI20210312BHEPIpc: G06F 12/0868 20160101ALI20210312BHEPIpc: G06F 12/12 20160101ALI20210312BHEP |
|
INTG | Intention to grant announced |
Effective date: 20210420 |
|
GRAJ | Information related to disapproval of communication of intention to grant by the applicant or resumption of examination proceedings by the epo deleted |
Free format text: ORIGINAL CODE: EPIDOSDIGR1 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: EXAMINATION IS IN PROGRESS |
|
GRAP | Despatch of communication of intention to grant a patent |
Free format text: ORIGINAL CODE: EPIDOSNIGR1 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: GRANT OF PATENT IS INTENDED |
|
INTC | Intention to grant announced (deleted) | ||
INTG | Intention to grant announced |
Effective date: 20210729 |
|
GRAS | Grant fee paid |
Free format text: ORIGINAL CODE: EPIDOSNIGR3 |
|
GRAA | (expected) grant |
Free format text: ORIGINAL CODE: 0009210 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: THE PATENT HAS BEEN GRANTED |
|
AK | Designated contracting states |
Kind code of ref document: B1Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
RBV | Designated contracting states (corrected) |
Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
REG | Reference to a national code |
Ref country code: ATRef legal event code: REFRef document number: 1446728Country of ref document: ATKind code of ref document: TEffective date: 20211115Ref country code: CHRef legal event code: EP |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R096Ref document number: 602016066122Country of ref document: DE |
|
REG | Reference to a national code |
Ref country code: IERef legal event code: FG4D |
|
REG | Reference to a national code |
Ref country code: LTRef legal event code: MG9D |
|
REG | Reference to a national code |
Ref country code: NLRef legal event code: MPEffective date: 20211110 |
|
REG | Reference to a national code |
Ref country code: ATRef legal event code: MK05Ref document number: 1446728Country of ref document: ATKind code of ref document: TEffective date: 20211110 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: RSFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211110Ref country code: LTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211110Ref country code: FIFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211110Ref country code: BGFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20220210Ref country code: ATFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211110 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ISFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20220310Ref country code: SEFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211110Ref country code: PTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20220310Ref country code: PLFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211110Ref country code: NOFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20220210Ref country code: NLFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211110Ref country code: LVFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211110Ref country code: HRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211110Ref country code: GRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20220211Ref country code: ESFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211110 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: SMFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211110Ref country code: SKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211110Ref country code: ROFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211110Ref country code: EEFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211110Ref country code: DKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211110Ref country code: CZFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211110 |
|
REG | Reference to a national code |
Ref country code: CHRef legal event code: PL |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R097Ref document number: 602016066122Country of ref document: DE |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: MCFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211110 |
|
PLBE | No opposition filed within time limit |
Free format text: ORIGINAL CODE: 0009261 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: NO OPPOSITION FILED WITHIN TIME LIMIT |
|
REG | Reference to a national code |
Ref country code: BERef legal event code: MMEffective date: 20211231 |
|
26N | No opposition filed |
Effective date: 20220811 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: LUFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20211226Ref country code: IEFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20211226Ref country code: ALFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211110 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: SIFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211110Ref country code: FRFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20220110Ref country code: BEFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20211231 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: LIFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20211231Ref country code: CHFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20211231 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ITFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211110 |
|
P01 | Opt-out of the competence of the unified patent court (upc) registered |
Effective date: 20230505 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: CYFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211110 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: HUFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMIT; INVALID AB INITIOEffective date: 20161226 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: MKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211110 |
|
PGFP | Annual fee paid to national office [announced via postgrant information from national office to epo] |
Ref country code: DEPayment date: 20231229Year of fee payment: 8 |