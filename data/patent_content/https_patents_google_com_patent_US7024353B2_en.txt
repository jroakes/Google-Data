US7024353B2 - Distributed speech recognition with back-end voice activity detection apparatus and method - Google Patents
Distributed speech recognition with back-end voice activity detection apparatus and method Download PDFInfo
- Publication number
- US7024353B2 US7024353B2 US10/215,810 US21581002A US7024353B2 US 7024353 B2 US7024353 B2 US 7024353B2 US 21581002 A US21581002 A US 21581002A US 7024353 B2 US7024353 B2 US 7024353B2
- Authority
- US
- United States
- Prior art keywords
- speech
- features
- cepstral
- speech recognition
- input
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Expired - Lifetime, expires
Links
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/20—Speech recognition techniques specially adapted for robustness in adverse environments, e.g. in noise, of stress induced speech
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/04—Segmentation; Word boundary detection
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/78—Detection of presence or absence of voice signals
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/28—Constructional details of speech recognition systems
- G10L15/30—Distributed recognition, e.g. in client-server systems, for mobile phones or network applications
Definitions
- This invention relates generally to speech recognition and more particularly to distributed speech recognition.
- Speech recognition is known in the art.
- speech audio input is digitized and then processed to facilitate identification of specific spoken words contained in the speech input.
- features are extracted from the digitized speech and then compared against previously stored patterns to enable such recognition of the speech content.
- ETSI European Technical Standards Institute
- Standard ES 201 108, Ver. 1.1.2, April 2000 a distributed speech recognition system where a portable device such as a cellular handset executes the feature extraction function and transmits the resultant features to a fixed-end platform that then facilitates the pattern matching function.
- pattern matching can be more successfully accomplished when the input can be accurately characterized as being either speech or non-speech audio input.
- the input can be accurately characterized as being either speech or non-speech audio input.
- information is available to identify a given segment of audio input as being non-speech, that information can be used to beneficially influence the functionality of the pattern matching activity by, for example, eliminating pattern matching for that particular non-speech segment.
- voice activity detection is not ordinarily available in distributed speech recognition systems such as the ETSI standard noted above. Instead, the speech content features are transmitted sans any voice activity detection information to the remote pattern matching platform.
- FIG. 1 comprises a prior art depiction of the front-end feature extractor of a distributed speech recognition system
- FIG. 2 comprises a block diagram depiction of a back-end pattern matching platform having voice activity detection as configured in accordance with various embodiments of the invention.
- FIG. 3 comprises a block diagram of a voice activity detector as configured in accordance with an embodiment of the invention.
- a plurality of speech recognition features are processed to provide at least an approximation of precursor information that originally yielded the speech recognition features.
- This precursor information is then processed to detect portions that likely correspond to speech and to provide a corresponding identification thereof.
- voice detection information is then used to facilitate the recognition processing of the speech recognition features.
- the speech recognition features include Mel frequency cepstral coefficients.
- the speech recognition features are processed by means of an inverse discrete cosine transformation to provide resultant values that are used to provide the approximation of the precursor information. If desired, these resultant values can themselves be processed by means of exponentiation to provide the precursor information.
- the speech recognition features can be processed to ascertain signal to noise information, which information is used, alone or with other voice activity detection information, to aid the recognition processing of the speech recognition features.
- the pattern matching activity in a distributed speech recognition system can benefit from voice activity detection information regardless of the fact that no such voice activity detection information was initially forwarded by the features-extraction front-end. This can result in improved recognition and/or reduced power and/or processing requirements.
- audio input such as speech is digitized in an analog to digital converter 11 (optionally, the digitized speech can then pass through a DC-offset removal filter (not shown) as understood in the art).
- a framing unit 12 then parses the digitized speech into corresponding frames.
- the frame-size will ordinarily be dependent on the sampling frequency. For example, the ETSI distributed speech recognition standard referenced earlier accommodates three different sampling frequencies, these being 8, 11, and 16 KHz.
- a suitable frame-size for these three sampling frequencies would be 200, 256, and 400 samples, respectively.
- a log energy unit 13 computes the natural logarithm of the overall energy for each frame to provide a Log-E parameter, which comprises one of the speech recognition features that will eventually be provided to the back-end of the distributed speech recognition system.
- the framed information is provided to a filter and fast Fourier transform (FFT) unit 14 .
- FFT filter and fast Fourier transform
- a pre-emphasis filter emphasizes the higher frequency components of the speech content.
- These pre-emphasized frames are then windowed by a Hamming window of the same size as the frames.
- the windowed frames are then transformed into the frequency domain by a fast Fourier transform.
- the size of the FFT depends on the sampling frequency, viz. 256 for 8 and 11 KHz and 512 for 16 KHz.
- the FFT magnitudes in the frequency range between 64 Hz and F s /2, where F s is the sampling frequency, are then Mel-filtered 15 .
- the Mel-filter bank outputs are then subjected to a (natural) logarithm function 16 .
- the 23 log values are then transformed by a discrete cosine transform (DCT) 17 to obtain the 13 Mel frequency cepstral coefficient values C 0 through C 12 .
- DCT discrete cosine transform
- the values C 13 through C 22 are discarded, i.e., not computed, as they are not going to be transmitted or otherwise provided to the back-end pattern matching activity.
- the Mel frequency cepstral coefficient parameters and the log-E parameter are then quantized and otherwise appropriately coded in a coder 18 and provided to a wireless transmitter of choice for transmission to a remote back-end pattern matching platform
- the above feature extraction functionality can be readily provided in, for example, a wireless transceiver platform such as a cellular handset. So configured, it can be seen that audible speech as provided to the handset can have speech recognition features extracted therefrom for subsequent remote processing.
- the speech recognition features include Mel frequency cepstral coefficients and a log-E parameter. It should be understood that this particular example has been provided for purposes of illustration only, however, and to provide a useful basis for now providing a detailed description of some embodiments for carrying out the invention. There are numerous other speech recognition features that could be extracted, either in addition to those suggested or in lieu thereof The tenants of this invention are applicable to such alternative embodiments as well.
- FIG. 2 provides a block diagram general overview of a back-end pattern matching platform suitable for use with the above described front-end feature extraction platform.
- An appropriate wireless receiver 21 receives the speech recognition feature information as transmitted by the front-end platform described above.
- a decoder 22 decodes the received information to specifically recover the speech recognition features described above.
- the filter bank outputs F 0 through F 22 obtained as above are, of course, only an approximation of the original filter bank outputs computed at the front-end because of the earlier truncation operation (i.e., the dropping of the values C 13 through C 22 ) and the quantization of the Mel frequency cepstral values C 0 through C 12 .
- These filter bank outputs represent an approximation of the precursor information that was used to develop the speech recognition features from which they are now derived.
- This precursor information is provided to a voice activity detector 25 that serves to detect whether the precursor information likely includes at least a significant quantity of speech or not.
- a segmentation unit 26 uses this information to provide one or more signals to a pattern matching unit 27 which signals identify which segments being provided to the pattern matching unit 27 by the decoder 22 likely include speech content.
- the pattern matching unit 27 can process the speech recognition feature information accordingly, thereby likely increasing recognition quality and accuracy and/or reducing power/processing needs to achieve like results.
- the voice activity detector 25 can also process the recovered precursor information to develop a signal representing the signal to noise ratio (SNR) as corresponds to the original audio input. Such information is also potentially usable by the pattern matching unit 27 for similar purposes as those already mentioned above.
- SNR signal to noise ratio
- the filter bank outputs F 0 through F 22 mentioned above may be regarded as average spectral magnitude estimates at the different frequency bands or channels for the current input frame.
- F(m,i) we will denote the filter bank output for the m th frame and i th channel by F(m,i), and when the specific channel is not important, we will denote the set of all filter bank outputs for the m th frame by F(m).
- a channel energy estimator 30 uses these values as input, a channel energy estimator 30 provides a smoothed estimate of the channel energies as follows.
- E ch (m,i) is the smoothed channel energy estimate for the m th frame and the i th channel
- E min 5000
- the value of the correction factor ⁇ 1 (for 8 kHz sampling frequency) is given by the i th value in the 23-element table: ⁇ 3.2811, 2.2510, 1.4051, 1.1038, 0.8867, 0.6487, 0.5482, 0.4163, 0.3234, 0.2820, 0.2505, 0.2036, 0.1680, 0.1397, 0.1179, 0.1080, 0.0931, 0.0763, 0.0674, 0.0636, 0.0546, 0.0478, 0.0046 ⁇ .
- the channel noise energy estimate (defined below), in a preferred embodiment, is initialized as follows:
- the channel energy estimate and the channel noise energy estimate for the current frame m, and all the 23 channels are denoted by E ch (m) and E n (m) respectively.
- the channel energy estimate E ch (m) for the current frame is also used as input to a spectral deviation estimator 34 , which estimates the spectral deviation ⁇ E (m) for the current frame as follows.
- the average long-term log energy spectrum is initialized as follows:
- E _ dB ⁇ ( m + 1 , i ) ⁇ 0.9 ⁇ E _ dB ⁇ ( m , i ) + 0.1 ⁇ E dB ⁇ ( m , i ) ; V ⁇ ( m ) > SIG_THLD ⁇ ( m ) 0.7 ⁇ E _ dB ⁇ ( m , i ) + 0.3 ⁇ E dB ⁇ ( m , i ) ; V ⁇ ( m ) ⁇ SIG_THLD ⁇ ( m ) where the parameter SIG_THLD(M) depends on the quantized signal SNR described next.
- SNR inst max(0.0, 10 log 10 ( E ts,inst ( m )/ E tn ( m ))). From the instantaneous SNR, the smoothed SNR is estimated as:
- the quantized signal SNR is used to determine different threshold values.
- the signal threshold for the next frame SIG_THLD(m+1) is determined using SNR q (m) as an index into the 20-element table ⁇ 36, 43, 52, 62, 73, 86, 101, 117, 134, 153, 173, 194, 217, 242, 268, 295, 295, 295, 295, 295 ⁇ in a preferred embodiment.
- the voice metric V(m), the spectral deviation ⁇ E (m), the peak-to-average ratio P2A(m), and the quantized signal SNR SNR q (m) are input to an update decision determiner 36 .
- the logic shown below in pseudo-code demonstrates how the noise estimate update decision can be made. It also demonstrates how a forced update decision is made (a forced update mechanism allows the voice activity detector to recover from wrong classification of background noise as speech whenever there is a sudden increase in background noise level).
- the update threshold for the current frame UPDATE_THLD(m) is determined using SNR q (m) as an index into a 20-element table given by ⁇ 31, 32, 33, 34, 35, 36, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 38, 38, 38, 38 ⁇ in a preferred embodiment.
- the update decision determination process begins by clearing the update flag (update_flag) and the forced update flag (fupdate_flag). These flags are set if certain conditions are satisfied as illustrated in the pseudo-code below:
- the updated channel noise estimate is stored in noise energy estimate storage 38 for all future frames until the next update occurs.
- the output of the noise energy estimate storage 38 E n (m) is used as an input to the channel SNR estimator 32 as described earlier.
- the voice metric V(m) and the quantized signal SNR value SNR q (m) serve as inputs to the voice activity determiner 39 .
- the output of the voice activity determiner denoted by VAD_FLAG is set to FALSE since these frames are assumed to be noise-only frames.
- the voice activity determiner operates as follows.
- the voice activity of the current frame is determined by testing if the voice metric exceeds the voice metric threshold. If the output of this test is TRUE, then the current frame is declared “voice-active.” Otherwise, the hangover count variable (hangover_count) is tested to find out if it is greater than or equal to zero. If the output of this test is TRUE, then the current frame is also declared “voice-active.” If the outputs of both tests are FALSE, then the current frame is declared “voice-inactive.”
- the “hangover” mechanism is generally used to cover slowly decaying speech that might otherwise be classified as noise, and to bridge over small gaps or pauses in speech that may be degraded by aggressive voice activity detection.
- the hangover mechanism is activated if the number of consecutive “voice-active” frames (counted by the burst_count variable) is at least equal to B cnt , the burst count threshold. To activate the mechanism, the number of hangover frames is set to H cnt , the hangover count threshold.
- the pseudo-code for the voice activity determiner is shown below:
- the variable VAD_FLAG from 310 is output as the voice activity information for each frame.
- the unquantized SNR value SNR(m) from 306 is also output.
- an approximation of precursor information as derived from speech recognition features that have been provided by a remote front-end for a distributed speech recognition system can be developed and used to aid in identifying portions of the speech recognition feature information that itself corresponds to speech and non-speech.
- This information in turn is readily used, in accordance with well understood prior art technique, to inform the activities and functionality of a back-end pattern matching process to facilitate improved recognition and/or improved logistical operation.
- the benefits of voice activity detection are provided to the back-end of a distributed voice recognition system notwithstanding that such information was not developed in the first instance at the front-end and then provided to the back-end. Instead, information as provided by the front-end is at least partially reverse processed to at least approximate information as it originally appeared at the front-end to then facilitate the voice activity detection activity.
Abstract
Description
The warped frequency range is then divided into 23 equal-sized, half-overlapping bands (a.k.a. channels or bins). For example, if Fs=8000 Hz, the frequency range between 64 Hz and 4000 Hz is warped into the Mel-frequency range between 98.6 and 2146.1 and divided into 23 bands with each band 170.6 wide and the center of each band 85.3 apart. The center of the first band is located at 98.6+85.3=183.9, and that of the last band is located at 2146.1−85.3=2060.8. These centers are then un-warped and rounded to the nearest FFT bin frequencies. In the linear frequency domain, the 23 bands are no longer equal-sized but the size of each band generally increases as the frequency increases. The FFT magnitudes falling inside each band are then combined using a triangular weighting window (with the weight at the center equal to 1.0 and at either end approaching 0.0).
(Notice that in the above equation the unavailable Cepstral Coefficients C13 through C22 are assumed to be zero.) The resulting D1 values are then exponentiated in
F 1=exp(D 1); i=0, 1, . . . , 22.
E ch(m,i)=max {E mm,αch(m)E ch(m−1,i)+(1−αch(m))(λ1 F(m,i))2 }; i=0, 1, . . . , 22
where Ech(m,i) is the smoothed channel energy estimate for the mth frame and the ith channel, Emin is the minimum allowable channel energy, {λ1, i=0, 1, . . . , 22} are the correction factors to compensate for the effect of the pre-emphasis filter and the varying widths of the triangular weighting windows used in Mel-filtering, and αch(m) is the channel energy smoothing factor defined as:
which means that αch assumes a value of zero for the first frame (m=1) and a value of 0.45 for all subsequent frames. This allows the channel energy estimate to be initialized to the unfiltered channel energy of the first frame. In a preferred embodiment, Emin=5000, and the value of the correction factor λ1 (for 8 kHz sampling frequency) is given by the ith value in the 23-element table: {3.2811, 2.2510, 1.4051, 1.1038, 0.8867, 0.6487, 0.5482, 0.4163, 0.3234, 0.2820, 0.2505, 0.2036, 0.1680, 0.1397, 0.1179, 0.1080, 0.0931, 0.0763, 0.0674, 0.0636, 0.0546, 0.0478, 0.0046}. From the channel energy estimate, a peak to
if ((m ≦ INIT_FRAMES) OR (fupdate_flag == TRUE)) | ||
{ |
if (P2A(m) < PEAK_TO_AVE_THLD) | |
{ | |
|
} | |
else | |
{ |
En(m,i) = Emin; 0 ≦ i ≦ 22; |
} |
} | ||
where En(m,i) is the smoothed noise energy estimate for the mth frame and the ith channel, INIT_FRAMES is the number of initial frames that are assumed to be noise-only frames, and fupdate_flag is a forced update flag defined below. In a preferred embodiment, INIT_FRAMES=10, and PEAK_TO_AVE_THLD=10.
where the values {σq(m, i), i=0, 1, . . . , 22} are constrained to be between 0 and 89 both inclusive.
where v(k) is the kth value of the 90-element voice metric table v defined as:
E dB(m,i)=10 log10(E ch(m,i)); i=0, 1, . . . , 22.
Next, the spectral deviation ΔE(m) is estimated as the sum of the absolute difference between the current log energy spectrum and an average long-term log energy spectrum denoted by ĒdB(m), that is:
The average long-term log energy spectrum is initialized as follows:
if ((m ≦ INIT_FRAMES) OR (fupdate_flag == TRUE)) | ||
{ | ||
ĒdB (m,i) = EdB (m,i); 0 ≦ i ≦ 22; | ||
} | ||
The average long-term log energy spectrum is updated as follows:
where the parameter SIG_THLD(M) depends on the quantized signal SNR described next.
Next, the instantaneous total signal energy Ets,inst(m) is computed as follows:
if (V(m) > SIG_THLD(m)) | ||
{ | ||
|
} | ||
The instantaneous total signal energy Ets,inst(m) is updated as above only if the current frame is determined to be a signal frame by checking whether V(m)>SIG_THLD(m). It is also clear that the signal energy estimated above is really (signal+noise) energy in the strict sense. Initialization of Ets,inst(m) is performed as follows.
if ((m≦INIT_FRAMES) OR (fupdate_flag == TRUE)) | ||
{ | ||
Ets,inst(m) = INIT_SIG_ENRG; | ||
} | ||
where the value of INIT_SIG_ENRG=1.0E+09 in a preferred embodiment.
SNR inst=max(0.0, 10 log10(E ts,inst(m)/E tn(m))).
From the instantaneous SNR, the smoothed SNR is estimated as:
if ((m ≦ INIT_FRAMES) OR (fupdate_flag == TRUE)) | ||
{ | ||
SNR(m) = SNRinst(m); | ||
} | ||
else | ||
{ | ||
if (V(m) > SIG_THLD(m)) | ||
{ | ||
SNR(m) = βSNR(m−1) + (1−β) SNRinst(m); | ||
β = min(β+0.003, HI_BETA); | ||
} | ||
else | ||
{ | ||
β = max(β−0.003, LO_BETA); | ||
} | ||
} | ||
In a preferred embodiment, the lower and upper limits of the smoothing factor β are respectively LO_BETA=0.950 and HI_BETA=0.998. Because we estimate the SNR as the ratio between (signal+noise) energy to signal energy, the lowest value of the estimated SNR is zero. The estimate is more accurate for higher values of SNR and it becomes less and less accurate as the SNR value decreases. The signal SNR is then quantized to 20 different values as:
SNR q(m)=max(0,min(round(SNR(m)/1.5),19)).
update_flag = FALSE; |
fupdate_flag = FALSE; |
if ((m > INIT_FRAMES) AND (V(m) < UPDATE_THLD(m)) AND |
(P2A(m) < PEAK_TO_AVE_THLD) |
{ |
update_flag = TRUE; |
update_cnt = 0; |
} |
else |
{ |
if ((P2A(m) < PEAK_TO_AVE_THLD) AND |
(ΔE(m) < DEV_THLD)) |
{ |
update_cnt = update_cnt + 1; |
if (update_cnt ≧ UPDATE_CNT_THLD) |
{ |
update_flag = TRUE; |
fupdate_flag = TRUE; |
} |
} |
} |
In order to avoid long term “creeping” of the update counter (update_cnt) setting the forced update flag (fupdate_flag) falsely in the above pseudo-code, hysteresis logic is implemented as shown below:
if (update_cnt == last_update_cnt) | ||
{ | ||
hyster_cnt = hyster_cnt + 1; | ||
} | ||
else | ||
{ | ||
hyster_cnt = 0; | ||
last_update_cnt = update_cnt; | ||
} | ||
if (hyster_cnt > HYSTER_CNT_THLD) | ||
{ | ||
update_cnt = 0; | ||
} | ||
In a preferred embodiment, the values of (previously undefined) constants used above are as follows:
E n(m+1,i)=0.9E n(m,i)+0.1E ch(m,i)); i=0 1, . . . , 22.
The updated channel noise estimate is stored in noise
V th(m)=V table [SNR q(m)], H cnt(m)=H table [SNR q(m)], B cnt(m)=B table [SNR q(m)],
where SNRq(m) is used as an index into the respective tables. In a preferred embodiment, these tables are defined by:
V table={33, 35, 36, 37, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 58, 58, 58, 58, 58},
H table={54, 52, 50, 48, 46, 44, 42, 40, 38, 36, 34, 32, 30, 28, 26, 24, 22, 20, 18, 16}, and
B table={3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6}.
if (V(m) > Vth(m)) | ||
{ | ||
VAD_LOCAL = TRUE; | ||
burst_count = burst_count + 1; | ||
if (burst_count >= Bcnt(m)) | ||
{ | ||
hangover_count = Hcnt(m); | ||
} | ||
} | ||
else | ||
{ | ||
VAD_LOCAL = FALSE: | ||
burst_count = 0; | ||
if (hangover_count >= 0) | ||
{ | ||
hangover_count = hangover_count − 1; | ||
} | ||
} | ||
if ((VAD_LOCAL == TRUE) OR (hangover_count >= 0)) | ||
{ | ||
VAD_FLAG = TRUE; | ||
} | ||
else | ||
{ | ||
VAD_FLAG = FALSE; | ||
} | ||
The variable VAD_FLAG from 310 is output as the voice activity information for each frame. Optionally, the unquantized SNR value SNR(m) from 306 is also output.
Claims (20)
Priority Applications (10)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US10/215,810 US7024353B2 (en) | 2002-08-09 | 2002-08-09 | Distributed speech recognition with back-end voice activity detection apparatus and method |
AU2003254288A AU2003254288A1 (en) | 2002-08-09 | 2003-08-01 | Distributed speech recognition with back-end voice activity detection apparatus and method |
JP2004527699A JP2005535920A (en) | 2002-08-09 | 2003-08-01 | Distributed speech recognition and method with back-end speech detection device |
MXPA05001593A MXPA05001593A (en) | 2002-08-09 | 2003-08-01 | Distributed speech recognition with back-end voice activity detection apparatus and method. |
EP03784867A EP1540645A4 (en) | 2002-08-09 | 2003-08-01 | Distributed speech recognition with back-end voice activity detection apparatus and method |
RU2005106251/09A RU2005106251A (en) | 2002-08-09 | 2003-08-01 | DEVICE AND METHOD FOR DISTRIBUTED RECOGNITION OF SPEECH USING SPEECH ACTIVITY IN THE INTERNAL INTERFACE |
PCT/US2003/024040 WO2004015685A2 (en) | 2002-08-09 | 2003-08-01 | Distributed speech recognition with back-end voice activity detection apparatus and method |
KR1020057002294A KR20060007363A (en) | 2002-08-09 | 2003-08-01 | Distributed speech recognition with back-end voice activity detection apparatus and method |
CNA038194147A CN1675684A (en) | 2002-08-09 | 2003-08-01 | Distributed speech recognition with back-end voice activity detection apparatus and method |
ZA200500792A ZA200500792B (en) | 2002-08-09 | 2005-01-26 | Distributed speech recognition with back-end voice activity detection apparatus and method |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US10/215,810 US7024353B2 (en) | 2002-08-09 | 2002-08-09 | Distributed speech recognition with back-end voice activity detection apparatus and method |
Publications (2)
Publication Number | Publication Date |
---|---|
US20040030544A1 US20040030544A1 (en) | 2004-02-12 |
US7024353B2 true US7024353B2 (en) | 2006-04-04 |
Family
ID=31494940
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US10/215,810 Expired - Lifetime US7024353B2 (en) | 2002-08-09 | 2002-08-09 | Distributed speech recognition with back-end voice activity detection apparatus and method |
Country Status (10)
Country | Link |
---|---|
US (1) | US7024353B2 (en) |
EP (1) | EP1540645A4 (en) |
JP (1) | JP2005535920A (en) |
KR (1) | KR20060007363A (en) |
CN (1) | CN1675684A (en) |
AU (1) | AU2003254288A1 (en) |
MX (1) | MXPA05001593A (en) |
RU (1) | RU2005106251A (en) |
WO (1) | WO2004015685A2 (en) |
ZA (1) | ZA200500792B (en) |
Cited By (12)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20060080096A1 (en) * | 2004-09-29 | 2006-04-13 | Trevor Thomas | Signal end-pointing method and system |
US20060173678A1 (en) * | 2005-02-02 | 2006-08-03 | Mazin Gilbert | Method and apparatus for predicting word accuracy in automatic speech recognition systems |
US20090036170A1 (en) * | 2007-07-30 | 2009-02-05 | Texas Instruments Incorporated | Voice activity detector and method |
USRE41130E1 (en) * | 1999-10-22 | 2010-02-16 | Bruce Fette | Radio communication system and method of operation |
WO2011053428A1 (en) | 2009-10-29 | 2011-05-05 | General Instrument Corporation | Voice detection for triggering of call release |
US20120130713A1 (en) * | 2010-10-25 | 2012-05-24 | Qualcomm Incorporated | Systems, methods, and apparatus for voice activity detection |
US9037455B1 (en) * | 2014-01-08 | 2015-05-19 | Google Inc. | Limiting notification interruptions |
US9165567B2 (en) | 2010-04-22 | 2015-10-20 | Qualcomm Incorporated | Systems, methods, and apparatus for speech feature detection |
US9451584B1 (en) | 2012-12-06 | 2016-09-20 | Google Inc. | System and method for selection of notification techniques in an electronic device |
US9691413B2 (en) * | 2015-10-06 | 2017-06-27 | Microsoft Technology Licensing, Llc | Identifying sound from a source of interest based on multiple audio feeds |
US10304478B2 (en) * | 2014-03-12 | 2019-05-28 | Huawei Technologies Co., Ltd. | Method for detecting audio signal and apparatus |
US11430461B2 (en) * | 2010-12-24 | 2022-08-30 | Huawei Technologies Co., Ltd. | Method and apparatus for detecting a voice activity in an input audio signal |
Families Citing this family (20)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
KR100714721B1 (en) * | 2005-02-04 | 2007-05-04 | 삼성전자주식회사 | Method and apparatus for detecting voice region |
CA2612903C (en) * | 2005-06-20 | 2015-04-21 | Telecom Italia S.P.A. | Method and apparatus for transmitting speech data to a remote device in a distributed speech recognition system |
KR100791349B1 (en) * | 2005-12-08 | 2008-01-07 | 한국전자통신연구원 | Method and Apparatus for coding speech signal in Distributed Speech Recognition system |
WO2008108239A1 (en) * | 2007-02-27 | 2008-09-12 | Nec Corporation | Voice recognition system, method, and program |
US20090150144A1 (en) * | 2007-12-10 | 2009-06-11 | Qnx Software Systems (Wavemakers), Inc. | Robust voice detector for receive-side automatic gain control |
JP5454469B2 (en) * | 2008-05-09 | 2014-03-26 | 富士通株式会社 | Speech recognition dictionary creation support device, processing program, and processing method |
US9037474B2 (en) * | 2008-09-06 | 2015-05-19 | Huawei Technologies Co., Ltd. | Method for classifying audio signal into fast signal or slow signal |
EP2816560A1 (en) * | 2009-10-19 | 2014-12-24 | Telefonaktiebolaget L M Ericsson (PUBL) | Method and background estimator for voice activity detection |
KR101251373B1 (en) | 2011-10-27 | 2013-04-05 | 한국과학기술연구원 | Sound classification apparatus and method thereof |
CN104715761B (en) * | 2013-12-16 | 2018-03-30 | 深圳市梦网百科信息技术有限公司 | A kind of audio valid data detection method and system |
CN103778914B (en) * | 2014-01-27 | 2017-02-15 | 华南理工大学 | Anti-noise voice identification method and device based on signal-to-noise ratio weighing template characteristic matching |
EP3117210A4 (en) * | 2014-03-12 | 2017-11-01 | University Of Virginia Patent Foundation | Compositions and methods for treating eye infections and disease |
US9489958B2 (en) * | 2014-07-31 | 2016-11-08 | Nuance Communications, Inc. | System and method to reduce transmission bandwidth via improved discontinuous transmission |
US10070220B2 (en) * | 2015-10-30 | 2018-09-04 | Dialog Semiconductor (Uk) Limited | Method for equalization of microphone sensitivities |
CN105513589B (en) * | 2015-12-18 | 2020-04-28 | 百度在线网络技术（北京）有限公司 | Speech recognition method and device |
US10090005B2 (en) * | 2016-03-10 | 2018-10-02 | Aspinity, Inc. | Analog voice activity detection |
CN107919130B (en) * | 2017-11-06 | 2021-12-17 | 百度在线网络技术（北京）有限公司 | Cloud-based voice processing method and device |
US10861484B2 (en) * | 2018-12-10 | 2020-12-08 | Cirrus Logic, Inc. | Methods and systems for speech detection |
CN112489692A (en) * | 2020-11-03 | 2021-03-12 | 北京捷通华声科技股份有限公司 | Voice endpoint detection method and device |
CN113345473B (en) * | 2021-06-24 | 2024-02-13 | 中国科学技术大学 | Voice endpoint detection method, device, electronic equipment and storage medium |
Citations (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5706394A (en) * | 1993-11-30 | 1998-01-06 | At&T | Telecommunications speech signal improvement by reduction of residual noise |
US6104993A (en) | 1997-02-26 | 2000-08-15 | Motorola, Inc. | Apparatus and method for rate determination in a communication system |
US6182032B1 (en) * | 1997-09-10 | 2001-01-30 | U.S. Philips Corporation | Terminal switching to a lower speech codec rate when in a non-acoustically coupled speech path communication mode |
US6453289B1 (en) * | 1998-07-24 | 2002-09-17 | Hughes Electronics Corporation | Method of noise reduction for speech codecs |
US20030018475A1 (en) * | 1999-08-06 | 2003-01-23 | International Business Machines Corporation | Method and apparatus for audio-visual speech detection and recognition |
US20030061036A1 (en) * | 2001-05-17 | 2003-03-27 | Harinath Garudadri | System and method for transmitting speech activity in a distributed voice recognition system |
US20030061042A1 (en) * | 2001-06-14 | 2003-03-27 | Harinanth Garudadri | Method and apparatus for transmitting speech activity in distributed voice recognition systems |
US6633839B2 (en) * | 2001-02-02 | 2003-10-14 | Motorola, Inc. | Method and apparatus for speech reconstruction in a distributed speech recognition system |
US20030204394A1 (en) * | 2002-04-30 | 2003-10-30 | Harinath Garudadri | Distributed voice recognition system utilizing multistream network feature processing |
Family Cites Families (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6003004A (en) * | 1998-01-08 | 1999-12-14 | Advanced Recognition Technologies, Inc. | Speech recognition method and system using compressed speech data |
JP2001236085A (en) * | 2000-02-25 | 2001-08-31 | Matsushita Electric Ind Co Ltd | Sound domain detecting device, stationary noise domain detecting device, nonstationary noise domain detecting device and noise domain detecting device |
JP4244514B2 (en) * | 2000-10-23 | 2009-03-25 | セイコーエプソン株式会社 | Speech recognition method and speech recognition apparatus |
-
2002
- 2002-08-09 US US10/215,810 patent/US7024353B2/en not_active Expired - Lifetime
-
2003
- 2003-08-01 WO PCT/US2003/024040 patent/WO2004015685A2/en active Application Filing
- 2003-08-01 MX MXPA05001593A patent/MXPA05001593A/en not_active Application Discontinuation
- 2003-08-01 EP EP03784867A patent/EP1540645A4/en not_active Withdrawn
- 2003-08-01 AU AU2003254288A patent/AU2003254288A1/en not_active Abandoned
- 2003-08-01 RU RU2005106251/09A patent/RU2005106251A/en not_active Application Discontinuation
- 2003-08-01 KR KR1020057002294A patent/KR20060007363A/en not_active Application Discontinuation
- 2003-08-01 CN CNA038194147A patent/CN1675684A/en active Pending
- 2003-08-01 JP JP2004527699A patent/JP2005535920A/en active Pending
-
2005
- 2005-01-26 ZA ZA200500792A patent/ZA200500792B/en unknown
Patent Citations (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5706394A (en) * | 1993-11-30 | 1998-01-06 | At&T | Telecommunications speech signal improvement by reduction of residual noise |
US6104993A (en) | 1997-02-26 | 2000-08-15 | Motorola, Inc. | Apparatus and method for rate determination in a communication system |
US6182032B1 (en) * | 1997-09-10 | 2001-01-30 | U.S. Philips Corporation | Terminal switching to a lower speech codec rate when in a non-acoustically coupled speech path communication mode |
US6453289B1 (en) * | 1998-07-24 | 2002-09-17 | Hughes Electronics Corporation | Method of noise reduction for speech codecs |
US20030018475A1 (en) * | 1999-08-06 | 2003-01-23 | International Business Machines Corporation | Method and apparatus for audio-visual speech detection and recognition |
US6633839B2 (en) * | 2001-02-02 | 2003-10-14 | Motorola, Inc. | Method and apparatus for speech reconstruction in a distributed speech recognition system |
US20030061036A1 (en) * | 2001-05-17 | 2003-03-27 | Harinath Garudadri | System and method for transmitting speech activity in a distributed voice recognition system |
US20030061042A1 (en) * | 2001-06-14 | 2003-03-27 | Harinanth Garudadri | Method and apparatus for transmitting speech activity in distributed voice recognition systems |
US20030204394A1 (en) * | 2002-04-30 | 2003-10-30 | Harinath Garudadri | Distributed voice recognition system utilizing multistream network feature processing |
Non-Patent Citations (1)
Title |
---|
ETSI ES 201 108 v1.1.2 Standard, "Speech Processing, Transmission and Quality Aspects (STQ); Distributed Speech Recognition; Front-end Feature Extraction Algorithm; Compression Algorithms", European Telecommunications Standards Institute, Apr. 2000, 20 pages. |
Cited By (20)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
USRE41130E1 (en) * | 1999-10-22 | 2010-02-16 | Bruce Fette | Radio communication system and method of operation |
US20060080096A1 (en) * | 2004-09-29 | 2006-04-13 | Trevor Thomas | Signal end-pointing method and system |
US20060080099A1 (en) * | 2004-09-29 | 2006-04-13 | Trevor Thomas | Signal end-pointing method and system |
US20060173678A1 (en) * | 2005-02-02 | 2006-08-03 | Mazin Gilbert | Method and apparatus for predicting word accuracy in automatic speech recognition systems |
US8175877B2 (en) * | 2005-02-02 | 2012-05-08 | At&T Intellectual Property Ii, L.P. | Method and apparatus for predicting word accuracy in automatic speech recognition systems |
US8538752B2 (en) * | 2005-02-02 | 2013-09-17 | At&T Intellectual Property Ii, L.P. | Method and apparatus for predicting word accuracy in automatic speech recognition systems |
US20090036170A1 (en) * | 2007-07-30 | 2009-02-05 | Texas Instruments Incorporated | Voice activity detector and method |
US8374851B2 (en) * | 2007-07-30 | 2013-02-12 | Texas Instruments Incorporated | Voice activity detector and method |
WO2011053428A1 (en) | 2009-10-29 | 2011-05-05 | General Instrument Corporation | Voice detection for triggering of call release |
US9165567B2 (en) | 2010-04-22 | 2015-10-20 | Qualcomm Incorporated | Systems, methods, and apparatus for speech feature detection |
US8898058B2 (en) * | 2010-10-25 | 2014-11-25 | Qualcomm Incorporated | Systems, methods, and apparatus for voice activity detection |
US20120130713A1 (en) * | 2010-10-25 | 2012-05-24 | Qualcomm Incorporated | Systems, methods, and apparatus for voice activity detection |
US11430461B2 (en) * | 2010-12-24 | 2022-08-30 | Huawei Technologies Co., Ltd. | Method and apparatus for detecting a voice activity in an input audio signal |
US9451584B1 (en) | 2012-12-06 | 2016-09-20 | Google Inc. | System and method for selection of notification techniques in an electronic device |
US9037455B1 (en) * | 2014-01-08 | 2015-05-19 | Google Inc. | Limiting notification interruptions |
US10304478B2 (en) * | 2014-03-12 | 2019-05-28 | Huawei Technologies Co., Ltd. | Method for detecting audio signal and apparatus |
US20190279657A1 (en) * | 2014-03-12 | 2019-09-12 | Huawei Technologies Co., Ltd. | Method for Detecting Audio Signal and Apparatus |
US10818313B2 (en) * | 2014-03-12 | 2020-10-27 | Huawei Technologies Co., Ltd. | Method for detecting audio signal and apparatus |
US11417353B2 (en) * | 2014-03-12 | 2022-08-16 | Huawei Technologies Co., Ltd. | Method for detecting audio signal and apparatus |
US9691413B2 (en) * | 2015-10-06 | 2017-06-27 | Microsoft Technology Licensing, Llc | Identifying sound from a source of interest based on multiple audio feeds |
Also Published As
Publication number | Publication date |
---|---|
EP1540645A2 (en) | 2005-06-15 |
AU2003254288A1 (en) | 2004-02-25 |
RU2005106251A (en) | 2005-10-10 |
JP2005535920A (en) | 2005-11-24 |
MXPA05001593A (en) | 2005-09-20 |
AU2003254288A8 (en) | 2004-02-25 |
EP1540645A4 (en) | 2006-05-31 |
ZA200500792B (en) | 2006-07-26 |
WO2004015685A3 (en) | 2004-07-15 |
WO2004015685A2 (en) | 2004-02-19 |
US20040030544A1 (en) | 2004-02-12 |
CN1675684A (en) | 2005-09-28 |
KR20060007363A (en) | 2006-01-24 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US7024353B2 (en) | Distributed speech recognition with back-end voice activity detection apparatus and method | |
US7337107B2 (en) | Perceptual harmonic cepstral coefficients as the front-end for speech recognition | |
EP1738355B1 (en) | Signal encoding | |
US5611019A (en) | Method and an apparatus for speech detection for determining whether an input signal is speech or nonspeech | |
US8050911B2 (en) | Method and apparatus for transmitting speech activity in distributed voice recognition systems | |
EP0950239B1 (en) | Method and recognizer for recognizing a sampled sound signal in noise | |
US6453289B1 (en) | Method of noise reduction for speech codecs | |
Karray et al. | Towards improving speech detection robustness for speech recognition in adverse conditions | |
US20110153326A1 (en) | System and method for computing and transmitting parameters in a distributed voice recognition system | |
US20030061036A1 (en) | System and method for transmitting speech activity in a distributed voice recognition system | |
US10074384B2 (en) | State estimating apparatus, state estimating method, and state estimating computer program | |
US9280982B1 (en) | Nonstationary noise estimator (NNSE) | |
US20040148160A1 (en) | Method and apparatus for noise suppression within a distributed speech recognition system | |
Morris et al. | The full combination sub-bands approach to noise robust HMM/ANN based ASR | |
US7451082B2 (en) | Noise-resistant utterance detector | |
CA2305652A1 (en) | Method for instrumental voice quality evaluation | |
JP2001177416A (en) | Method and device for acquiring voice coded parameter | |
JP3413862B2 (en) | Voice section detection method | |
US7260528B2 (en) | System and method for obtaining reliable speech recognition coefficients in noisy environment | |
KR100284772B1 (en) | Voice activity detecting device and method therof | |
Kim et al. | Speech enhancement of noisy speech using log-spectral amplitude estimator and harmonic tunneling | |
Brummer | Speaker recognition over HF radio after automatic speaker segmentation | |
Kim et al. | Enhancement of noisy speech for noise robust front-end and speech reconstruction at back-end of DSR system. | |
Wang et al. | A voice activity detection algorithm based on perceptual wavelet packet transform and teager energy operator | |
Farsi et al. | A novel method to modify VAD used in ITU-T G. 729B for low SNRs |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: MOTOROLA, INC., ILLINOISFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:RAMABADRAN, TENKASI;REEL/FRAME:013183/0649Effective date: 20020808 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
FPAY | Fee payment |
Year of fee payment: 4 |
|
AS | Assignment |
Owner name: MOTOROLA MOBILITY, INC, ILLINOISFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:MOTOROLA, INC;REEL/FRAME:025673/0558Effective date: 20100731 |
|
AS | Assignment |
Owner name: MOTOROLA MOBILITY LLC, ILLINOISFree format text: CHANGE OF NAME;ASSIGNOR:MOTOROLA MOBILITY, INC.;REEL/FRAME:029216/0282Effective date: 20120622 |
|
FPAY | Fee payment |
Year of fee payment: 8 |
|
AS | Assignment |
Owner name: GOOGLE TECHNOLOGY HOLDINGS LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:MOTOROLA MOBILITY LLC;REEL/FRAME:034420/0001Effective date: 20141028 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 12TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1553)Year of fee payment: 12 |