CN117556885A - Training large-scale visual transducer neural networks with variable tile sizes - Google Patents
Training large-scale visual transducer neural networks with variable tile sizes Download PDFInfo
- Publication number
- CN117556885A CN117556885A CN202311577935.1A CN202311577935A CN117556885A CN 117556885 A CN117556885 A CN 117556885A CN 202311577935 A CN202311577935 A CN 202311577935A CN 117556885 A CN117556885 A CN 117556885A
- Authority
- CN
- China
- Prior art keywords
- image
- training
- tile
- embedding
- neural network
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000012549 training Methods 0.000 title claims abstract description 164
- 238000013528 artificial neural network Methods 0.000 title claims abstract description 100
- 230000000007 visual effect Effects 0.000 title abstract description 7
- 238000000034 method Methods 0.000 claims abstract description 73
- 238000012545 processing Methods 0.000 claims abstract description 33
- 230000008569 process Effects 0.000 claims description 42
- 238000009826 distribution Methods 0.000 claims description 20
- 238000003860 storage Methods 0.000 claims description 8
- 238000009966 trimming Methods 0.000 claims 1
- 238000009827 uniform distribution Methods 0.000 claims 1
- 238000004590 computer program Methods 0.000 description 15
- 238000001514 detection method Methods 0.000 description 6
- 238000010801 machine learning Methods 0.000 description 6
- 230000009471 action Effects 0.000 description 5
- 238000004891 communication Methods 0.000 description 5
- 239000011159 matrix material Substances 0.000 description 5
- 238000010586 diagram Methods 0.000 description 4
- 230000006870 function Effects 0.000 description 4
- 230000011218 segmentation Effects 0.000 description 4
- 238000013527 convolutional neural network Methods 0.000 description 3
- 230000003993 interaction Effects 0.000 description 3
- 230000007246 mechanism Effects 0.000 description 3
- 241000282472 Canis lupus familiaris Species 0.000 description 2
- 241000282326 Felis catus Species 0.000 description 2
- 238000012986 modification Methods 0.000 description 2
- 230000004048 modification Effects 0.000 description 2
- 230000003287 optical effect Effects 0.000 description 2
- 238000005457 optimization Methods 0.000 description 2
- 230000004044 response Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- ORILYTVJVMAKLC-UHFFFAOYSA-N Adamantane Natural products C1C(C2)CC3CC1CC2C3 ORILYTVJVMAKLC-UHFFFAOYSA-N 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 230000001149 cognitive effect Effects 0.000 description 1
- 239000003086 colorant Substances 0.000 description 1
- 230000003247 decreasing effect Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000004519 manufacturing process Methods 0.000 description 1
- 238000005259 measurement Methods 0.000 description 1
- 238000010606 normalization Methods 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 238000005070 sampling Methods 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/09—Supervised learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/0464—Convolutional networks [CNN, ConvNet]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
Abstract
The present disclosure relates to training a large-scale visual transducer neural network with variable tile sizes. A method of training the neural network includes, at each training step: obtaining a plurality of training images; obtaining a corresponding target output of each training image; selecting an image tile generation scheme from a plurality of image tile generation schemes, wherein each image tile generation scheme generates a different number of tiles for a given input image, and wherein each tile comprises a respective subset of pixels of the given input image; for each training image: generating a plurality of image tiles by applying the selected image tile generation scheme to the training image; and processing the plurality of image tiles using a neural network to generate a network output; and training the neural network on an objective function that measures, for each training image, a difference between a network output of the training image and a target network output of the training image.
Description
Technical Field
The present description relates to training neural networks.
Background
Neural networks are machine learning models that employ one or more layers of nonlinear units to predict output for receiving inputs. Some neural networks include one or more hidden layers in addition to the output layer. The output of each hidden layer is used as an input for the next layer in the network, the next hidden layer or output layer. Each layer of the network generates an output from the received inputs based on the current values of the respective parameter sets.
Disclosure of Invention
The present specification describes a system implemented as a computer program on one or more computers at one or more locations that trains a visual transformer neural network (ViT). ViT is a neural network that processes an input comprising an image, i.e. it processes intensity values of pixels of the image, to generate an output of the image, e.g. a classification or regression output, and that comprises one or more self-attention layers and one or more output layers.
The subject matter described in this specification can be implemented in specific embodiments to realize one or more of the following advantages.
Some neural networks, such as visual transducers (ViT), convert images into sequences by slicing the images into tiles (patches). The size of these tiles controls the speed/accuracy tradeoff, smaller tiles result in higher accuracy at greater computational cost, but changing tile sizes typically requires retraining the model. This specification generally describes techniques for repeatedly adjusting tile sizes of image tiles during training of a neural network. This results in a single trained neural network, i.e., a neural network with a single set of weights, that performs well across a wide range of tile sizes, making it possible to customize the model at deployment time according to different computational budgets.
In other words, the present specification describes modifications to the architecture, training, or both of ViT. The described modifications include changing the image tile generation scheme for each step of the training process and allowing the system to pre-train ViT the neural network, which then achieves the most advanced performance on any of a variety of downstream tasks with adjustable accuracy and computational cost. In particular, when a variable image tile generation scheme is employed during the training process, the deployed ViT may operate across a range of image tile sizes and act as a backbone for the model for fine-tuning of a particular downstream task.
The details of one or more embodiments of the subject matter of the specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Drawings
FIG. 1 is a diagram of an example neural network training system with a variable image tile generation system.
Figure 2 shows the processing of a single image through trained ViT.
Fig. 3 is a flow chart of a process of training ViT with a variable image tile generation system.
FIG. 4 illustrates the processing of an image by a variable tile generation system.
Fig. 5 illustrates processing of image tiles by an image tile embedding system having variable tile sizes.
Fig. 6 depicts the ability of ViT trained with variable tile sizes to process multiple tile sizes with high precision.
Fig. 7 demonstrates the accuracy of ViT trained on multiple image tile sizes as compared to the accuracy of ViT trained on fixed image tile sizes.
Like reference numbers and designations in the various drawings indicate like elements.
Detailed Description
The present specification describes a system implemented as a computer program on one or more computers at one or more locations that trains a visual transformer neural network (ViT).
FIG. 1 is a schematic diagram of an example neural network training system 100. The neural network training system 100 is an example of a system implemented as a computer program on one or more computers in one or more locations, in which the systems, components, and techniques described below may be implemented.
The neural network training system 100 is configured to train a vision transformer (ViT) neural network 110.
ViT 110 is configured to process an input image to generate an output for the input image.
ViT 110 contains an image tile embedding system 160 that processes a plurality of image tiles from an input image to generate an input sequence comprising a respective input element ("tile embedding") at each of a plurality of positions. Each tile is a respective spatial region within the input image.
Typically, the input sequence includes a respective input element corresponding to each of the plurality of image tiles.
In some implementations, for each image tile, a respective input element corresponding to the image tile is generated from a combination of an embedding of intensity values of pixels in the image and an embedding of their respective locations within the image ("location embedding"). Position embedding is a learned embedding that contains information about the location of a tile relative to other tiles in an image.
Further details regarding the image tile embedding system are described in greater detail below with reference to FIG. 5.
ViT 110 the input sequence is processed by the plurality of self-attention neural network blocks to generate an output sequence comprising a respective output element at each location, i.e., for each input element in the input sequence. These blocks are referred to as "self-attention" neural network blocks because each block includes one or more self-attention layers 130, each self-attention layer applying a self-attention mechanism to elements in the input sequence (as received by the block) as part of updating the input sequence.
VIT 110 then uses one or more output layers 140 to process one or more of the output elements to generate an output for the image, e.g., a classification output for a different task or a different output.
In other words, the output of ViT 100 can take several forms. The classification output typically includes a respective score corresponding to each of the plurality of categories. The score of a category indicates the likelihood that the image belongs to that category. In some cases, a category may be a class of objects (e.g., dogs, cats, people, etc.), and an image may belong to the category if the image depicts objects included in the class of objects corresponding to the class. In some cases, a category may represent a global image attribute (e.g., whether an image depicts a day or night scene, or whether an image depicts a summer or winter scene), and if an image has a global attribute corresponding to the category, the image may belong to the category.
An example configuration of ViT will be described in more detail below.
To train ViT, the system 100 obtains first training data 120.
The first training data 120 includes a plurality of training images and a respective target output for each training image. For example, the target output may be a real-valued image tag from a standard supervised learning process. As another example, the target output may be the output of a larger, pre-trained "teacher" neural network used to train the image. For example, the pre-trained neural network may be ViT with more parameters than ViT 110.
The system 100 then trains ViT the neural network 110, e.g., by supervised learning, on the first training data 120 to minimize appropriate classification losses, e.g., a loss function comprising cross entropy losses between the target classification output of the given training image and the classification output generated by ViT for the given training image, and optionally one or more regularization terms.
After this training, the system 100 may use the trained ViT110 to perform classification tasks on the new input image or provide data specifying the trained ViT for performing classification tasks.
Alternatively or additionally, the system 100 may train the downstream neural network including the self-attention block 130 in conjunction with another set of output layers on different downstream tasks, e.g. on different classification tasks or on regression tasks, i.e. on training data of the downstream tasks.
As a particular example, the neural network training system 100 may configure a downstream neural network that includes a self-attention block and another set of output layers to generate a classification output. The classification output may include a respective score corresponding to each of a plurality of categories different from those used in the initial training. This is referred to as fine tuning the pre-trained ViT110 on new data related to a particular downstream task that may or may not already exist in the initial training data.
The original task and the downstream task may be any of a variety of computer vision tasks.
For example, the original task, the downstream task, or both may be classified tasks, i.e., tasks in which the network output may be classified output. The classification output typically includes a respective score corresponding to each of the plurality of categories. The score of a category indicates the likelihood that the image belongs to that category. In some cases, a category may be a class of objects (e.g., dogs, cats, people, etc.), and an image may belong to the category if the image depicts objects included in the class of objects corresponding to the class. In some cases, a category may represent a global image attribute (e.g., whether an image depicts a day or night scene, or whether an image depicts a summer or winter scene), and if an image has a global attribute corresponding to the category, the image may belong to the category.
As another example, the tasks may include one or more object detection tasks. In the object detection task, the output generated by the neural network identifies the location in the input image where the object type is depicted, such as a bounding box or other region.
As another example, the tasks may include one or more instance segmentation tasks. In an instance segmentation task, the output generated by the neural network identifies, for each pixel in the image that belongs to a particular object type, the object instance to which that pixel corresponds.
As another example, the tasks may include one or more semantic segmentation tasks. In the semantic segmentation task, the output generated by the neural network identifies, for each pixel in the image, to which of a plurality of categories the pixel belongs.
As another example, the tasks may include one or more depth prediction tasks. In a depth prediction task, an output generated by a neural network identifies, for each pixel in an image, a predicted depth of a scene at that pixel.
As another example, the tasks may include one or more surface normal prediction tasks. In a surface normal prediction task, the output generated by the neural network identifies, for each pixel in the image, a predicted surface normal of the scene at that pixel.
In some implementations, for downstream task training or raw training performed by the system 100, the neural network is trained with a text processing neural network to perform multi-modal tasks that require processing of both text and image inputs. That is, the target output to be generated for a given image depends on one or more outputs generated by the text processing neural network for one or more corresponding text inputs. Examples of such tasks include open vocabulary image classification, open vocabulary object detection, image specification text, text-based image search, image-based retrieval, and so forth.
After this training, the system 100 may use the trained downstream neural network to perform downstream tasks on the new input image or provide data specifying the trained downstream neural network for performing classification tasks.
The size of the image tiles provided as input to ViT 110 controls ViT the tradeoff between accuracy and speed of 110. Smaller tile sizes result in ViT 110 being executed with higher accuracy at higher computational cost. On the other hand, when the neural network training system 100 uses a larger tile size, i.e., the input image is sliced into larger sub-images, the trained ViT 110 is performed with lower accuracy and reduced computational cost.
When deployed, a particular downstream application may require higher resolution (smaller tile size) than other downstream applications when the model is used to make predictions or perform downstream tasks. In particular, some downstream tasks, such as object detection, may require higher resolution (smaller tile size) in some environments or for detecting some object classes. Systems that perform object detection on low resolution images (a small number of pixels) or a class of objects with large distinguishing features may require a corresponding low resolution image tile generation scheme (a larger tile size) to maintain high accuracy. Alternatively, a system performing object detection on a high resolution image (a large number of pixels) or on a class of objects with small distinguishing features may require a corresponding high resolution image tile generation scheme (a smaller tile size) to capture the distinguishing details. When the tile size is too large, the tiling process may average out small or subtle features that distinguish one object from another.
As another example, a particular downstream application may require greater computational efficiency (and thus a larger tile size) than other downstream applications.
The ability to dynamically alter tile sizes at deployment time provides more flexibility to adjust performance and computational cost depending on the particular downstream task.
However, training ViT using the fixed tile size neural network training system 100 during training, the ViT 110 achieves high accuracy only in deployments within small tile size ranges approaching the fixed tile size selected for training. Thus, the ViT is trained using a fixed tile size neural network training system 100, which ViT must be deployed at the same fixed tile size for all downstream applications to maintain maximum accuracy.
To illustrate this, in some implementations, the neural network training system 100 is designed to generate image tiles during training by using the variable image tile generation system 150.
The variable image tile generation system 150 selects an image tile generation scheme from a plurality of image tile generation schemes for each training step such that tile sizes are different across different training steps.
Each image tile generation scheme of the plurality of image tile generation schemes generates a different number of tiles of the input image given the input image, wherein each tile includes a respective subset of pixels of the input image, i.e., a non-overlapping or overlapping subset of pixels of various sizes.
For example, the plurality of image tile generation schemes may divide an image into tiles having corresponding stride (stride), wherein the corresponding stride is different for each of the plurality of image tile generation schemes.
As another example, each of the plurality of image tile generation schemes may divide the image into non-overlapping tiles, each tile having the same size, but different sizes for different schemes.
In general, the variable image tile generation system 150 selects a scheme such that a different scheme is selected at different training steps, i.e., such that any given training step will have a different scheme than a subset of the other training steps.
As a result of training using the variable image tile generation system 150, the trained ViT 110 maintains maximum accuracy and can operate efficiently across multiple tile sizes.
Further details regarding the variable image tile generation system 150 are described in greater detail below with reference to FIG. 4.
Fig. 2 is a diagram depicting the processing of an image 202 by ViT 110.
First, the variable image tile generation system 150 processes an image, as depicted in FIG. 4.
The variable image tile generation system 150 outputs a plurality of output image tiles 222a-n.
The variable image tile generation system 150 feeds the output image tiles 222a-n into the ViT 110.
The image tile embedding system 160 processes each image tile 222a-n.
Further, the image tile embedding system 160 accepts details regarding which image tile generation scheme 223 was selected, such as the corresponding tile size, in order to appropriately adjust the size of the embedding parameters, as discussed in detail below. The ViT uses the self-attention sub-network 240 to process a sequence comprising a plurality of embeddings 232a-n corresponding to the input image tiles 222a-n to generate a first image tile output set 242a-n. As described above, the self-attention sub-network 240 includes a sequence of layer blocks, each layer block applying a self-attention mechanism to elements in an input sequence (as received by the block) as part of an update input sequence.
ViT110 then processes one or more of the first set of image tile outputs 242a-n using the sequence of output neural network layers 140 to generate the classification output 262. Alternatively or additionally, viT may produce additional outputs depending on the desired output of the downstream task, e.g., regression outputs or outputs of auxiliary losses used during training.
Fig. 3 is a flow chart of an example process 300 for training ViT using a neural network training system. For convenience, process 300 will be described as being performed by a system of one or more computers located at one or more locations. For example, a neural network training system, such as the neural network training system 100 depicted in fig. 1, suitably programmed in accordance with the present description, may perform the process 300. The system trains ViT over multiple training steps.
During each of the plurality of training steps, the system obtains a plurality of training images and corresponding target outputs 302 for the training step.
Next, the system selects an image tile generation scheme 304 from a plurality of available image tile generation schemes.
The system applies the selected image tile generation scheme to the plurality of images corresponding to the training step 306 to generate a plurality of image tiles for each training image.
For each image, the system then processes the plurality of image tiles through a plurality of neural network layers, which may include a self-attention layer and an additional output layer.
The system then generates an output 308, such as a classification or regression output, for each training image and measures the accuracy of the target output relative to the input training data 310.
The system calculates an adjustment to the weight ("parameter") of ViT and adjusts the network parameter 312 accordingly. For example, the system may calculate a gradient of the network parameter with respect to a loss function that includes one or more terms of measurement accuracy, and then apply an optimizer, such as Adam, adamW, rmsProp, adafactor, etc., to the gradient to update the network parameter.
The system uses the updated parameters to go to the next training step and continues to adjust the parameters of the neural network to minimize the loss function.
Fig. 4 is a diagram depicting how the variable image tile generation system 150 processes an image before ViT 110 processes the image.
The neural network training system 100 may configure the image tile generation system 150 to apply the image tile generation scheme to the input image 202 in various ways.
In particular, the system 150 performs the image tile generation scheme selection 300 to select the image tile generation scheme 230.
Conventionally, the system 100 would select the same image tile generation scheme 230 for each training step. The strategy generates image tiles of a fixed size across all training steps. This is the standard means of training ViT.
Rather, in some implementations, for each training step, the system 150 samples the image tile generation scheme 230 from a random distribution of predefined allowed schemes.
In some other implementations, for each training step, the system 150 samples the image tile generation scheme 230 from a particular non-uniform probability distribution. For example, the system 150 may sample the image tile generation scheme 230 for each training step from a gaussian probability distribution with a predefined standard deviation and center point such that a range of schemes towards the distribution center is more likely to be selected than a range of schemes towards the distribution edge.
In some other implementations, for each training step, the system 150 samples the image tile generation scheme 230 from a randomly selected non-uniform probability distribution. For example, for each training step, the system 150 may first sample from a set of allowed probability distributions (i.e., gaussian, uniformly random, or linear). The system 150 may then sample the image tile selection scheme 230 from the selected probability distribution of the training step.
In some other implementations, for each training step, the system 150 samples the image tile generation scheme 230 from a probability distribution, wherein the probability distribution is one of a sequence of probability distributions, each probability distribution associated with a respective subset of the plurality of training steps.
In some other implementations, the system 150 may select the image tile generation scheme 230 using any manner of selecting a probability distribution for each training step and sampling from the probability distribution to select the image tile generation scheme 230.
The system 150 applies the selected image tile scheme 230 to the input image. The variable image tile generation scheme 150 sends the image tiles 222a-n to ViT 110 along with details about the selected image tile scheme 223, including the number of image tiles for the selected image tile generation scheme 230.
The variable image tile generation system 150 processes the plurality of input images 112a-n independently, i.e., sequentially or in parallel. The variable image tile generation system 150 generates a sequence of s tiles 122a-n for each input image 112a-n. The number of image tiles in each sequence depends on the tile size defined by the selected image tile generation scheme 230. In some implementations, the number of image tiles in each image tile sequence 122a-n may be written as
Where h is the height of the input images 112a-n, w is the width of the input images 112a-n, and p is the tile size corresponding to the selected image tile generation scheme 230. The height and width of the input images 112a-n are generally uniform, but in some embodiments the variable image tile generation system 150 may be configured to employ a normalization strategy to create a sequence of tiles 122a-n across all non-uniform images 112a-n in a training step with a single image tile generation scheme 230. The variable image tile generation system 150 sends the image tiles 122a-n to the image tile embedding system 160.
FIG. 5 illustrates an image tile embedding system 160 that processes each of n image tiles 222 a-n.
Each image tile embed 232a-n represents a pixel of a corresponding image tile 222a-n and may be generated by processing the pixel of the corresponding image tile 222. In this specification, embedding is an ordered set of values representing inputs in a particular embedding space. For example, the embedding may be a floating point vector or other numerical value having a fixed dimension.
In some implementations in which each image tile 222a-n is represented as a two-dimensional sub-image of the image 202, each image tile embedding 232a-n is a remodeled version of the corresponding image tile 222 a-n. For example, the image tile embedding system 160 may "planarize" (flat) each image tile 222a-n to generate an image tile embedding 232a-n, the image tile embedding 232a-n being a one-dimensional tensor comprising each pixel in the image tile 222 a-n. As a particular example, if each image tile 222a-n has a dimension lxwxc, where C represents the channel number of the image (e.g., c=3 for an RGB image), the image tile embedding system 160 may generate an image tile embedding 232a-n having a dimension 1× (l·w·c).
In some other implementations, the image tile embedding system 160 may process one-dimensional tensors of pixels comprising the image tiles 222a-n (e.g., flattened versions of the image tiles 222 a-n) to generate the corresponding image tile embeddings 232a-n. As described in more detail below, the image tile embeddings 232a-n will be processed by the ViT 110, viT that the training has been configured to accept input in a particular format, e.g., a particular size and shape. Thus, the image tile embedding system 160 may project each image tile 222a-n into a coordinate space having the dimensions required by ViT 110.
For example, the image tile embedding system 160 may process each image tile 222a-n using linear projection:
z i ＝x i E i +b i
wherein,is the i-th image tile embedding 232a-n, D is the input dimension required by ViT 110,is a bagOne-dimensional tensor including the ith image tile 222a-N, N being the number of pixels in the ith image tile 222a-N, ei ε RN D being the projection matrix, #>Is an optional linear bias term.
In some implementations, the image tile embedding system 160 uses a respective different projection matrix E i To generate each image tile embedding 232a-n; in some other implementations, the image tile embedding system 160 uses the same projection matrix E to generate each image tile embedding 232a-n. Similarly, in some implementations, the image tile embedding system 160 uses a respective different bias term b i To generate each image tile embedding 232a-n; in some other implementations, the image tile embedding system 160 uses the same bias term b i To generate each image tile embedding 232a-n.
In some embodiments, the linear projection is machine-learned. For example, during training of ViT 110, the neural network training system 100 may update parameters of the linear projections (e.g., projection matrix E i And bias term b i Parameters of (c) are set forth). As a specific example, the training system may update the parameters of the linear projection by: the training error of ViT is counter-propagated through ViT 110 to the image tile embedding system 160 and the random gradient descent on the counter-propagated error is used during training of the first training data 120 to determine the update.
Instead of, or in addition to, processing the one-dimensional tensors corresponding to the image tiles 222a-n with linear projections, the image tile embedding system 160 may use an embedded neural network to process the one-dimensional tensors. For example, the embedded system 160 may be considered a component of ViT 110. That is, the embedding system 160 may be an embedding sub-network of ViT 110 that includes one or more neural network layers configured to process one-dimensional tensors and generate the image tile embeddings 232a-n.
For example, the embedded neural network may include one or more feed-forward neural network layers configured to process one-dimensional tensors corresponding to the image tiles 222a-n.
As another example, the embedded neural network may include one or more self-attention neural network layers configured to concurrently process each one-dimensional tensor corresponding to a respective image tile 222a-n using a self-attention mechanism.
As another example, the embedded neural network may include one or more convolutional neural network layers configured to process the image tiles 222a-n using convolutional filters. As a particular example, if the image tiles 222a-n are represented as two-dimensional images, the image tile embedding system 160 may process each (non-planarized) image tile 222a-n using one or more convolutional neural network layers to generate a feature map for the image tile 222a-n. The image tile embedding system 160 may then planarize the feature map and process the planarized feature map using linear projection as described above to generate corresponding image tile embeddings 232a-n.
As another particular example, the image tile embedding system 160 may process the entire image 202 using one or more convolutional neural network layers to generate a feature map of the image 202. The feature map may be two-dimensional (or, like image 202, may be two-dimensional, with each element having multiple channels). The neural network training system 100 may then determine n tiles of the feature map of the image 202, wherein each tile includes one or more elements of the feature map. That is, instead of dividing the image 202 itself into image tiles 222a-n, the variable image tile generation system 150 may divide the feature map of the image 202 generated by the embedded neural network of the image tile embedding system 160. As a particular example, each tile may include a single element of a feature map. The image tile embedding system 160 may then generate image tile embeddings 232a-n from the n tiles of the feature map, for example, by applying linear projections to the tiles of the feature map as described above.
After the image tile embedding system 160 generates the image tile embeddings 232a-n, the neural network training system 100 may generate an input sequence from the image tile embeddings 232a-n to be provided as an input to the ViT 110. Typically, the input sequence includes one or more input elements corresponding to respective image tile embeddings 232a-n. For example, the input sequence may include a respective input element corresponding to each of the n image tile embeddings 232a-n. As a particular example, the input elements corresponding to the n image tile embeddings 232a-n may be ordered in the input sequence in the raster order of the corresponding image tiles 222 a-n.
In some implementations, the input elements in the input sequence corresponding to the image tile embeddings 232a-n are equal to the image tile embeddings 232a-n themselves.
In some other implementations, to generate input elements of the input sequence corresponding to the image tile embeddings 232a-n, the neural network training system 100 may combine (i) the image tile embeddings 232a-n and (ii) a location embedment representing locations of the image tiles 222a-n within the image 202 corresponding to the image tile embeddings 232a-n. For example, the neural network training system 100 may append location embeddings to the image tile embeddings 232a-n. By incorporating location embedding, the neural network training system 100 may encode spatial information, such as the relative positioning of each image tile in the image, which may be leveraged by ViT110 to generate the classification output 262.
In some implementations, the location embeddings corresponding to each image tile 222a-n of the image 202 are integers. For example, a first image tile located in the upper left corner of the image 202 may have a position embedded "1", a second image tile immediately to the right of the first image tile may have a position embedded "2", and so on.
In some other implementations, the location embedding is machine-learned. For example, during training of ViT110, the training system may concurrently learn position embedding by counter-propagating ViT training errors of position embedding via ViT 110. In some such implementations, the training system may generate a respective different location embedment for each image tile (e.g., assuming that each image 202 received by the neural network training system 100 is segmented into the same number of tiles).
In some other implementations, the training system may incorporate two-dimensional information into the location embeddings by learning, for both dimensions of the image 202, the respective location embeddings for each coordinate along that dimension. For example, if the image 202 is segmented into a two-dimensional grid of image tiles 222a-n, the training system may generate two sets of location embeddings: the first set includes a respective position embedding for each index along a vertical axis of the grid, and the second set includes a respective embedding for each index along a horizontal axis of the grid. To generate a positional embedding of a particular image tile 222a-n, the neural network training system may be combined, for example, by connecting: (i) A position embedding corresponding to the index of the particular image tile 222a-n along the vertical axis, and (ii) a position embedding corresponding to the index of the particular image tile 222a-n along the horizontal axis.
In some implementations, one or more input elements in the input sequence do not correspond to any image tiles 222a-n of the image 202. For example, the input sequence may include the same class embedding for all received images 102. For example, the class embeddings may be tensors having the same dimensions as the image tile embeddings 232 a-n. As a specific example, the class embedding may be a tensor of all '0' or all '1'.
Class embedding can be inserted anywhere in the input sequence; for example, the class embedding may be the first input element of the input sequence, or the last input element of the input sequence.
In some implementations, class embedding is machine-learned. For example, during training of ViT 110, the training system may concurrently learn parameters for class embedding by back-propagating ViT training errors to class embedding via ViT 110.
In embodiments in which the input elements corresponding to each image tile 222a-n include a position embedding corresponding to the image tile 222a-n, the neural network training system 100 may also append the position embedding to a class embedding, such as a machine-learned position embedding or a predetermined position embedding (e.g., a position embedding of all '0's or all '1's).
However, adding class embedding to an input sequence requires ViT 110 to handle longer input sequences. Given the configuration of modern hardware and the large number of parameters of ViT 110, this can increase the memory overhead of ViT, for example, due to the token (token) filling required by modern machine learning accelerators.
In some implementations, to remove this memory overhead, the sequence does not include class embedding, i.e., each element in the input sequence corresponds to a different tile of the image 202, and the sequence does not include any elements that do not correspond to tiles.
After generating the input sequence, the neural network training system 100 provides the input sequence as an input to ViT 110. ViT 110 can process the input sequence to generate a classification output 262.
As described above, the neural network training system 100 configures the image tile embedding system 160 to accept image tiles 222a-n having tile sizes that vary between training steps.
Thus, the image tile embedding system 160 includes an image tile embedding sizer 231 that is adapted to a variable tile size corresponding to the image tile generation scheme 223 selected from the variable image tile generation system 150.
In some implementations, the tile and position embedding parameters depend on the dimensions of the input image tiles 122 a-n. Thus, the image tile embedding system 160 adjusts the size of the corresponding parameters according to the dimensions of the image tiles 122a-n corresponding to the selected image tile generation scheme 223.
The image tile embedding sizer 231 may use a variety of numerical techniques to resize the embedding parameters according to the dimensions of the image tiles 122a-n corresponding to the selected image tile generation scheme 232.
In particular, the embedding system saves a set of original image tile embedding parameters ("weights") and a set of location embedding parameters ("weights") that are shared across the tile size and training steps. At each training step, the sizer 231 resizes the shared set of tile embedding parameters and the set of position embedding parameters to match the image tile generation scheme 223 that has been selected for the training step.
Thus, after training or for downstream training with a fixed tile size, the sizer 231 may adjust the size of the shared set of trained position embedding parameters and image tile embedding parameters to match the fixed tile size.
In some implementations, the tile embedding resizer 231 uses a linear resizing transform, which may be represented by a linear transform:
wherein,is an arbitrary input, and->The tile embedding resizer 231 independently adjusts the sizes of the channels (i.e., corresponding to multiple colors) o of the multi-channel input. In order to find a new set of image tile embedding weights, where the embedding of the resized tile matches the embedding of the original tile, the tile embedding resizer 231 must solve the optimization problem:
Wherein,embedding weights representing resizing, +.>And χ is some distribution over the tile. In case the tile size increases by p +.p, the resized insert can be written +.>Wherein p=b (B T B) -1 ＝(B T ) + Is B T Is the pseudo-inverse of (a):
reducing p in tile size * In the case of < p, the solution to the above-described optimization equation generally depends on the tile distribution χ. In this example, the pseudo-inverse may be written as
Thus, for both increasing tile size and decreasing tile size, the tile embedding resizer 231 implements a pseudo-inverse resizing transform. The expression of the pseudo-inverse resizing transform can be written as:
wherein,is a matrix corresponding to the pseudo-inverse resizing transform.
The tile embedding system 160 adjusts the size of the position embeddings to match the parameters of the selected tile generation scheme 223. When the tile size changes, the shared position embedment may not correspond to the correct position of each tile because the number of tiles per training step is different. When the selected tile generation scheme 223 changes, the tile embedding system 160 may adjust the position embeddings to reflect the new tile size.
In some implementations, the tile embedding system 160 adjusts the size of the shared location embeddings that are shared among all possible tile generation schemes. The tile embedding system 160 performs an interpolation process, such as bilinear interpolation, to scale the shared location embeddings to reflect the exact location of the image tile relative to other image tile generation schemes. Further, the image tile embedding system 160 may combine the resized tile embedding of the image with an interpolation location embedding corresponding to the location of the image tile within the image as an input to the self-attention neural network 130. By combining the position embedding and the tile embedding, both the features of the pixels within the tile and the position of the tile in the image are encoded in the input aggregate embedding.
In some implementations, after the system 100 trains ViT110, the system 100 or a different neural network training system trains the resulting pre-trained ViT to perform downstream tasks. Thus, the training system may utilize the same tile generation scheme for pre-trained ViT110 or a different tile generation scheme than the tile generation scheme for training pre-trained ViT110, with training inputs specific to downstream tasks to fine tune the pre-trained neural network.
For example, the training system may utilize a fixed tile size to fine tune the pre-trained ViT110. The trimmed neural network exhibits the same performance across multiple tile sizes even though it is trimmed with a fixed tile size. Because the pre-trained ViT110 is trained with variable tile sizes, the trimmed neural network maintains its high performance across multiple tile sizes.
In some implementations, the neural network training system pre-trains ViT110 with the image tile generation scheme S1. The neural network training system may utilize the image tile generation scheme S2 to fine tune the pre-trained ViT110. The image tile generation scheme S3 may be utilized to deploy fine-tuned instances of the downstream task-specific pre-training ViT110. The image tile generation schemes S1, S2, S3 may or may not be identical, of the same type (fixed or variable), or related. S1, S2, and S3 may generate more or fewer image tiles that are consistent with the respective image tile generation schemes than the other image tile generation schemes.
Fig. 6 shows a flow of a single image selected from the plurality of images 102 by ViT110. The neural network training system 100 trains ViT110 using the variable image tile generation system 150. Fig. 6 demonstrates the selection of two image tile generation schemes 232 selected from a plurality of available image tile generation schemes. Fig. 6 demonstrates two selected image tile generation schemes 232, wherein one selected image tile generation scheme 232 consists of four image tiles and one selected image tile generation scheme 232 consists of 9 image tiles. The image tile generation scheme 232, which consists of four image tiles, is less computationally expensive while performing downstream tasks with less precision. The image tile generation scheme 232, which consists of nine image tiles, is more computationally expensive while achieving performance on downstream tasks. After the "chunking" process, FIG. 6 depicts a resizing of shared embedding weights independent of tile size. The image tile embedding sizer 231 resizes the embedding weights according to the number of tiles in the selected image tile generation scheme 232.
The image tile embedding system 160 sends the resized image tile embeddings to the ViT trained using the variable image tile generation system 150. ViT110 the details of the "chunking" process or how many tiles are in the input image need not be known. ViT110 trained with the variable image tile generation system 150 maintains a high degree of accuracy over multiple tile sizes, as depicted in fig. 6.
Fig. 7 demonstrates the tile size dependence for a ViT model trained using a neural network training system 100 with a fixed tile size, as compared to a neural network training system 100 with a variable tile size. When deployed, the pre-trained ViT 110 can select which tile size of the input image to process. When deployed, pre-trained ViT 100 trained using fixed tile sizes (ViT-B/16 and ViT-B/30) delivers peak output classification performance matching its training tile sizes (16 and 30, respectively). Alternatively, pre-trained ViT (FlexiVit-B) trained using variable tile sizes delivers peak output classification performance across a wide range of tile sizes. Decoupling of tile size and performance is advantageous because tile size is a convenient leverage for controlling the overall accuracy and computational cost of a deployed ViT 110 neural network.
The present specification generally describes that the neural network trained with variable tile sizes is ViT. More generally, however, the described techniques for resizing tiles during training may be used during training of any neural network having any suitable architecture that receives a set of image tiles as input, i.e., rather than directly operating on the entire input image.
The term "configured" is used in this specification in connection with systems and computer program components. A system for one or more computers configured to perform a particular operation or action means that the system has installed thereon software, firmware, hardware, or a combination thereof that, in operation, causes the system to perform the operation or action. For one or more computer programs configured to perform particular operations or actions, it is meant that the one or more programs include instructions that, when executed by a data processing apparatus, cause the apparatus to perform the operations or actions.
Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly embodied computer software or firmware, in computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible, non-transitory storage medium for execution by, or to control the operation of, data processing apparatus. The computer storage medium may be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them. Alternatively or additionally, the program instructions may be encoded on a manually generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by data processing apparatus.
The term "data processing apparatus" refers to data processing hardware and encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. The apparatus may also be or further comprise dedicated logic circuitry, for example an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). In addition to hardware, the apparatus may optionally include code that creates an execution environment for the computer program, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
A computer program, also known as a program, software application, module, software module, script, or code, can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages; and the computer program may be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data, e.g., one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, e.g., files that store one or more modules, sub programs, or portions of code. A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, or in combination with, special purpose logic circuitry, e.g., an FPGA or ASIC, and one or more programmed computers.
A computer suitable for executing a computer program may be based on a general-purpose or special-purpose microprocessor or both, or any other kind of central processing unit. Typically, a central processing unit will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a central processing unit for carrying out or executing instructions and one or more memory devices for storing instructions and data. The central processing unit and the memory may be supplemented by, or incorporated in, special purpose logic circuitry. Typically, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, such devices are not required for a computer. Furthermore, the computer may be embedded in another device, such as a mobile phone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, such as a Universal Serial Bus (USB) flash drive, to name a few.
Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices including, for example: semiconductor memory devices such as EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disk; and CD ROM and DVD-ROM discs.
To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other types of devices may also be used to provide interaction with a user; for example, feedback provided to the user may be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input. In addition, the computer may interact with the user by: transmitting and receiving documents to and from devices used by the user; for example, a Web page is sent to a Web browser on a user device in response to a request received from the Web browser. Further, the computer may interact with the user by sending text messages or other forms of messages to a personal device, such as a smart phone running a messaging application, and receiving response messages as feedback from the user.
The data processing means for implementing the machine learning model may also comprise, for example, dedicated hardware accelerator units for handling public and computationally intensive parts of machine learning training or production, i.e. inference, workload.
The machine learning model can be implemented and deployed using a machine learning framework such as a TensorFlow framework, microsoft cognitive toolkit framework, apache Single framework, or Apache MXNet framework.
Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, such as a data server, or that includes a middleware component, such as an application server, or that includes a front-end component, such as a client computer having a graphical user interface, a Web browser, or an application through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include local area networks ("LANs") and wide area networks ("WANs") -such as the internet.
The computing system may include clients and servers. The client and server are typically remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, the server sends data, such as HTML pages, to the user device, e.g., for the purpose of displaying data to and receiving user input from a user interacting with the device as a client. Data generated at the user device, e.g., results of a user interaction, may be received at the server from the device.
While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any invention or of what may be claimed, but rather as descriptions of features specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Furthermore, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, although operations are depicted in the drawings and described in the claims in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous. Moreover, the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated in a single software product or packaged into multiple software products.
Specific embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying drawings do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous.
Claims (23)
1. A method performed by one or more computers to train a neural network configured to process an input image to generate a network output of the input image, the method comprising, at each training step of a plurality of training steps:
Obtaining a plurality of training images for the training step;
for each training image of the plurality of training images, obtaining a respective target output;
selecting an image tile generation scheme for the training step from a plurality of image tile generation schemes, wherein each image tile generation scheme of the plurality of image tile generation schemes generates a different number of tiles of the input image given the input image, and wherein each tile comprises a respective subset of pixels of the input image;
for each training image of the plurality of training images:
generating a plurality of image tiles of the training image by applying the selected image tile generation scheme to the training image; and
processing the plurality of image tiles using the neural network to generate a network output for the training image; and
the neural network is trained on an objective function that measures, for each training image, a difference between a network output of the training image and a target network output of the training image.
2. The method of claim 1, wherein each of the plurality of image tile generation schemes divides the input image into non-overlapping tiles of a corresponding size, and wherein the corresponding size is different for each of the plurality of image tile generation schemes.
3. The method of claim 1, wherein each of the plurality of image tile generation schemes divides the input image into tiles having a corresponding stride, and wherein the corresponding stride is different for each of the plurality of image tile generation schemes.
4. The method of claim 1, wherein the respective target output of each training image of the plurality of training images is a true value target output generated based on a label of the training image.
5. The method of claim 1, wherein the respective target output of each training image of the plurality of training images is an output generated by a trained teacher neural network by processing the training image.
6. The method of claim 1, further comprising:
before training the neural network, parameter values of the neural network are initialized based on trained parameter values of a trained teacher neural network.
7. The method of claim 1, wherein the neural network comprises an embedded sub-network, a self-attention sub-network, and an output sub-network, and wherein processing the plurality of image tiles using the neural network to generate the network output of the training image comprises:
Processing the plurality of image tiles using the embedding sub-network to generate a respective embedding for each image tile of the plurality of image tiles;
processing the respective embedded input sequence comprising each image tile of the plurality of image tiles using the self-attention sub-network to generate a self-attention output of the training image; and
the self-attention sub-network is processed using the output sub-network to generate the network output of the training image.
8. The method of claim 7, wherein processing the plurality of image tiles using the embedding sub-network to generate a respective embedding of each image tile of the plurality of image tiles comprises:
for each image tile, a set of tile embedding weights is applied to intensity values of pixels in the image tile to generate an initial embedding of the image tile.
9. The method of claim 8, wherein the embedding sub-network is with a single set of tile embedding weights having a first size and shared among the plurality of image tile generation schemes, and wherein applying the set of tile embedding weights comprises:
Resizing the set of tile embedding weights from the first size to have a size that matches a size of the image tile; and
a set of resized tile embedding weights is applied to the intensity values of the pixels in the image tile to generate an initial embedding of the image tile.
10. The method of claim 9, wherein training the neural network comprises adjusting the set of tile embedding weights shared between the plurality of image tile generation schemes.
11. The method of claim 8, wherein processing the plurality of image tiles using the embedding sub-network to generate a respective embedding of each image tile of the plurality of image tiles comprises:
the initial embedding of the image tile is combined with a learned positional embedding to generate the embedding of the image tile, the learned positional embedding corresponding to a position of the image tile within the training image.
12. The method of claim 10, wherein the embedding sub-network has a single set of location embeddings shared among the plurality of image tile generation schemes and including a respective location embedment for each location in a first set of locations, and wherein applying the set of tile embedding weights comprises:
Generating an interpolated position embedding corresponding to the position of the image tile from the single set of position embeddings; and
the initial embedding of the image tile is combined with the interpolation position embedding corresponding to the position of the image tile within the training image to generate the embedding of the image tile.
13. The method of claim 12, wherein training the neural network comprises adjusting the single set of location embeddings shared between the plurality of image tile generation schemes.
14. The method of any of claims 1 to 13, wherein selecting an image tile generation scheme for the training step from a plurality of image tile generation schemes comprises:
an image tile generation scheme is sampled from a probability distribution over the plurality of tile generation schemes for the training step.
15. The method of claim 14, wherein the probability distribution is a uniform distribution over the plurality of tile generation schemes.
16. The method of claim 14, wherein the probability distribution of the training step is one of a sequence of probability distributions, each probability distribution being associated with a respective subset of the plurality of training steps.
17. The method of claim 1, wherein the respective target output of each training image of the plurality of training images is generated based on an output generated by a text processing neural network.
18. A method performed by one or more computers, the method comprising:
obtaining data specifying a pre-trained neural network that has been trained by performing the respective operations of the method of any of claims 1 to 17; and
the pre-trained neural network is trimmed to perform downstream tasks.
19. The method of claim 18, wherein fine-tuning the pre-trained neural network to perform a downstream task includes processing training input of the downstream task using only a single fixed image tile generation scheme.
20. The method of claim 19, further comprising:
after trimming the pre-trained neural network to perform the downstream task, the downstream task is performed using the trimmed neural network by processing new inputs for the downstream task using a second image tile generation scheme different from the fixed image tile generation scheme.
21. The method of claim 20, wherein the fixed image tile generation scheme generates a fewer number of image tiles than the second image tile generation scheme.
22. A system comprising one or more computers and one or more storage devices storing instructions that, when executed by the one or more computers, cause the one or more computers to perform the operations of the method of any one of claims 1-21.
23. One or more computer-readable storage media storing instructions that, when executed by one or more computers, cause the one or more computers to perform the operations of the method of any one of claims 1-21.
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202263427762P | 2022-11-23 | 2022-11-23 | |
US63/427,762 | 2022-11-23 |
Publications (1)
Publication Number | Publication Date |
---|---|
CN117556885A true CN117556885A (en) | 2024-02-13 |
Family
ID=88965110
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202311577935.1A Pending CN117556885A (en) | 2022-11-23 | 2023-11-23 | Training large-scale visual transducer neural networks with variable tile sizes |
Country Status (1)
Country | Link |
---|---|
CN (1) | CN117556885A (en) |
-
2023
- 2023-11-23 CN CN202311577935.1A patent/CN117556885A/en active Pending
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN109863537B (en) | Stylized input image | |
US10115032B2 (en) | Universal correspondence network | |
CN110622177B (en) | Instance partitioning | |
US11847541B2 (en) | Training neural networks using data augmentation policies | |
US11636668B2 (en) | Bilateral convolution layer network for processing point clouds | |
WO2019232099A1 (en) | Neural architecture search for dense image prediction tasks | |
CN110622169A (en) | Neural network system for motion recognition in video | |
WO2019089578A1 (en) | Font identification from imagery | |
CN109934792B (en) | Electronic device and control method thereof | |
CN112734641A (en) | Training method and device of target detection model, computer equipment and medium | |
AU2021354030B2 (en) | Processing images using self-attention based neural networks | |
US11257217B2 (en) | Image segmentation using neural networks | |
CN112771578A (en) | Image generation using subdivision scaling and depth upscaling | |
CN116843901A (en) | Medical image segmentation model training method and medical image segmentation method | |
CN113837965A (en) | Image definition recognition method and device, electronic equipment and storage medium | |
US20230410465A1 (en) | Real time salient object detection in images and videos | |
CN114913339B (en) | Training method and device for feature map extraction model | |
CN117556885A (en) | Training large-scale visual transducer neural networks with variable tile sizes | |
CN110717405A (en) | Face feature point positioning method, device, medium and electronic equipment | |
US11983903B2 (en) | Processing images using self-attention based neural networks | |
KR20230164314A (en) | Learning method and device for domain generalization | |
CN116758554A (en) | Test question information extraction method and device, electronic equipment and storage medium | |
EP4115341A2 (en) | Cross-transformer neural network system for few-shot similarity determination and classification | |
CN116245160A (en) | System and method for clipping filters in deep neural networks |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |