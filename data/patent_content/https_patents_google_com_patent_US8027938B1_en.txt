US8027938B1 - Discriminative training in machine learning - Google Patents
Discriminative training in machine learning Download PDFInfo
- Publication number
- US8027938B1 US8027938B1 US12/055,967 US5596708A US8027938B1 US 8027938 B1 US8027938 B1 US 8027938B1 US 5596708 A US5596708 A US 5596708A US 8027938 B1 US8027938 B1 US 8027938B1
- Authority
- US
- United States
- Prior art keywords
- updates
- model
- worker
- workers
- training data
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
Definitions
- This specification relates to training machine learning systems.
- Machine learning typically uses statistical techniques to iteratively process training data in order to improve the accuracy of one or more predictive functions.
- One type of machine learning is a classification system that generates a function mapping inputs to desired outputs. The system learns parameters of a function that maps a vector into one of several classes by looking at several input-output examples.
- Machine learning can be used in a number of applications including natural language processing, syntactic pattern recognition, and speech recognition.
- Structured prediction refers to a generalization of multi-class classification to problems where the predicted outputs describe a configuration over components with possible dependencies between components.
- these types of problems involve exponentially large output sets, require computationally intensive decoding algorithms, and call for the optimization of complicated loss functions.
- Some approaches to these types of problems e.g., maximum margin approaches or voted perceptron in natural language processing, do not scale well to very large numbers of training examples or to models having a very large number of features.
- a method includes distributing a parameterized model to each worker of a plurality of workers, the plurality of workers being arranged in a hierarchy of workers, the parameterized model including a plurality of feature functions representing a plurality of features and corresponding model parameters, processing a portion of training data at each worker of the plurality of workers according to the parameterized model, the training data including a number of training examples, to calculate updates to model parameters, for each worker at a lowest level of the hierarchy of workers, sending the calculated updates to a next higher level worker, for each other worker in the hierarchy of workers, combining updates of the respective worker with updates received from one or more lower level workers, collecting all updates from the workers at a master to generate real updates to the model parameters, generating an updated model using the real updates to the model parameters, and distributing the updated model to each worker when performing a next iteration.
- Implementations can include one or more of the following features.
- the training data can include a plurality of example input output pairs, each example input-output pair including an input and a correct output. Identifying updates can include providing a first input to the model from a first example input-output pair, calculating a difference between an output generated by the model according to the input and the correct output of the first example input-output pair, determining which model features fire in response to the example input, and calculating an update to one or more model parameters associated with the firing model features.
- the aspect can further include receiving training data and assigning a distinct portion of the training data to each worker.
- the aspect can further include dynamically defining the hierarchy of workers according to a size of the received training data.
- Generating real updates can include calculating a quotient of combined updates for a parameter and the number of workers contributing to the combined updates.
- Each worker can independently process the respective portion of the training data in an online manner to generate worker level updates to model parameters.
- Generating the updated model can further include comparing parameter updates for each feature to a threshold contribution score and when the parameter updates for a particular feature are below a threshold contribution score, updating the model without updating the parameter value for the particular feature.
- a method in one aspect, includes receiving training data, the training data including source and target language pairs, distributing portions of the training data to each worker of a plurality of workers where the plurality of workers are arranged in a hierarchy of workers, identifying a parameterized model that includes a plurality of features and corresponding parameters, the features including lexical n-gram features, training the model, and distributing an updated model to each worker when performing a next iteration.
- Training the model includes distributing the model to each worker of the hierarchy of workers, processing a portion of training data at each worker of the plurality of workers according to the parameterized model to identify updates to the model parameters, for each worker at a lowest level of the hierarchy of workers, sending the calculated parameter updates to a next higher level worker, for each other worker in the hierarchy of workers, combining parameter updates of the respective worker with parameter updates received from one or more lower level workers, collecting all updates from the workers at a master to generate real updates to the model parameters, and generating an updated model using the real updates to the parameters.
- Implementations can include one or more of the following features.
- Generating real updates can include calculating a quotient of combined updates for a parameter and the number of workers contributing to the combined updates.
- Generating the updated model can further include comparing parameter updates for each feature to a threshold contribution score and when the parameter updates for a particular feature are below a threshold contribution score, updating the model without updating the parameter value for the particular feature.
- the aspect can further include performing a plurality of iterations, each iteration including distributing an updated model, the updated model generated during a previous iteration, to each worker and generating parameter updates by applying the portion of the training data assigned to each worker to the updated model of the iteration.
- the aspect can further include, when no more iterations are to be performed, using the last updated model to process input text to provide translated output text.
- the system can learn parameters (e.g., feature weights) for millions of model features over hundreds of thousands to virtually unlimited numbers of training data examples.
- the system improves robustness of a large margin online learning algorithm without sacrificing convergence rates.
- the distributed architecture is designed to take advantage of inexpensive hardware components (e.g., computer clusters) by allowing efficient failure recovery.
- FIG. 1 shows an example distributed architecture for discriminative training
- FIG. 2 is a flowchart of an example process for discriminative training
- FIG. 3 is a flowchart of an example process for parallel online training of model parameters.
- FIG. 4 is a schematic diagram of a generic computer system.
- FIG. 1 shows an example distributed architecture 100 for discriminative training.
- the architecture 100 includes a master process (master 102 ) and multiple worker processes (workers 104 and 106 ).
- the workers can be hierarchically organized.
- distributed architecture 100 includes first level workers 104 and second level workers 106 .
- Each second level worker 106 is in communication with a first level worker 106 .
- one second level worker 106 “worker 6 ” is shown as in communication with first level worker 104 “worker 2 ”.
- each first level worker 104 is in communication with the master 102 .
- a specified computation operation e.g., a particular model
- updates can be sent up in the reverse direction level-by-level to the master 102 .
- Updates from lower level workers can be combined at each level according to one or more specified combination operations and then passed up to the next level.
- the distributed architecture 100 is shown as a binary tree, other architectures can have other configurations.
- the number and arrangement of workers is dynamically generated before processing particular training data, for example, to minimize communication costs.
- the distributed architecture 100 can be scaled as necessary to include many worker at each level and many levels of workers all having the master operating as a root node for the tree structure.
- the master 102 maintains the tree structure of the workers 104 and 106 . Additionally, the master 102 initializes the computation operation (e.g., the model) at a beginning of a discriminative learning operation. The master 102 broadcasts the model to the participating workers along with the tree structure. Knowledge of the tree structure allows the workers to determine whether updates have been received from all lower level workers and which upper level process (e.g., next level worker or the master) to send updates to.
- the computation operation e.g., the model
- the master 102 broadcasts the model to the participating workers along with the tree structure. Knowledge of the tree structure allows the workers to determine whether updates have been received from all lower level workers and which upper level process (e.g., next level worker or the master) to send updates to.
- the master 102 generates a new model using the updates received from the workers 104 and 106 . Once launched, the master 102 drives the computations until one or more specified stopping criteria are satisfied (e.g., a specified number of iterations, a specified result).
- Data for performing the discriminative learning process can be separated into disjoint parts and processed independently, and in parallel, by the workers.
- Each worker carries out the computation (e.g., according to the received model) for a portion of the training data assigned to the respective worker.
- the workers then send results (e.g., model parameter updates) to the master either directly or indirectly depending on the level of the particular worker. For example, a particular worker can perform a local combination of results from lower level workers in communication with the worker.
- the distributed architecture 100 allows for independent processing of parts of the training data by the workers 104 and 106 while also allowing independent combination of results from all data parts to produce a global view of the results.
- the master 102 uses the global view of results to generate a new, updated model, which can then be distributed to, and used for another processing iteration by, the workers 104 and 106 .
- the distributed architecture 100 can be used for batch learning techniques as well as online learning techniques.
- batch learning techniques can be directly used with the distributed architecture 100 while online learning techniques can require approximation in order to be performed according to the distributed architecture 100 .
- a batch technique keeps model parameters constant while calculating an error associated with each input training example.
- an online technique constantly updates its parameters following each training example such that its error calculation uses different weights for each input.
- the distributed architecture 100 also allows for recovery from failure of any particular workers. For example, if a particular worker fails, a different worker can be assigned by the master. The master can instruct the worker as to its position within the tree structure such that the worker knows which data part to process as well as which workers to send updates to or receive updates from during a processing iteration.
- FIG. 2 is a flowchart of an example process 200 for discriminative training.
- the process 200 will be described with respect to a system (e.g., a discriminative training system) that performs the process 200 .
- the system receives 202 a parameterized model.
- the parameterized model is used to compute a best scoring output.
- a given linear model is used to compute the one or more parameters such that the likelihood of a given input resulting in a correct output is maximized according to particular scoring criteria.
- the model can be trained to compute parameters to maximize the likelihood that a given input sentence in a first language is correctly translated into a target language sentence.
- the scoring criteria used to estimate the maximum likelihood can be, for example, a Bilingual Evaluation Understudy (“BLEU”) score
- BLEU is a method for evaluating the quality of text which has been translated from one natural language to another using machine translation.
- the BLEU score provides a measure of the statistical closeness of machine translations to reference translations.
- the BLEU score is described, for example, in Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu, “BLEU: a Method for Automatic Evaluation of Machine Translation,” Proceedings of the 40th Annual Meeting on the Association for Computational Linguistics, pages 311-318, July 2002.
- the BLEU score is a geometric mean of a ratio of matching n-grams of length one to four between a candidate translation and a group of reference translations, along with a length term penalizing short sentences.
- the sufficient statistics of the BLEU score are the number of matching n-grams (i.e., n-gram precisions for the group of reference translations), the candidate translation length, and the effective length of the reference translations of the group.
- a particular linear model parameterized by w can compute a best scoring output according to:
- y ⁇ ⁇ ( x , w ) arg ⁇ ⁇ max y ⁇ GEN ⁇ ( x ) ⁇ w ⁇ ⁇ ⁇ ( x , y )
- GEN(x) is a set of possible outputs given an input x
- ⁇ (x, y) is a set of feature functions defined on (x, y) where each (x, y) is an input (x) output (y) pair.
- the number of features defined by the set of feature functions can be very large (e.g., millions).
- w represents a vector of model parameters (also referred to as feature weights). Each entry in the vector corresponds to a particular feature such that the length of the vector is equal to the total number of features.
- the features can include individual n-grams and their associated probabilities.
- An n-gram is a sequence of n consecutive words.
- An n-gram has an order, which is the number of words in the n-gram. For example, a 1-gram (or unigram) includes one word; a 2-gram (or bigram) includes two words.
- a translation system uses the technique to train other forms of language model features, e.g., long-distance language model features, phrase table features, or syntactic features.
- ⁇ (x, w) represents estimated, or most likely, y outputs as a function of an input x, feature functions associated with the input x, and corresponding model parameters (e.g., feature weights) w.
- the parameters w are calculated to provide maximum likelihood output y over all possible inputs x as a function one or more feature functions defined on (x, y).
- a loss function L(y*, y) is often defined for an output y with respect to the correct output y*.
- the loss function represents a user-defined quality measure of the output.
- the system receives 204 a set of training data including a number of training examples.
- the training examples are particular known correct input-output pairs.
- M ⁇ (x, y*) ⁇ each input x has a corresponding correct output y*.
- each input x in the training examples can be a source language sentence having a corresponding output y* representing a known target language sentence translation of input x.
- the system learns 206 parameter values of w to provide best scoring output from model.
- the system learns the parameter values, for example, by training, in parallel, the model in an iterative process using an approximation of online discriminative training
- the system uses 208 the model including the learned parameter values to process input data.
- the trained model can be used to translate a given input sentence in a first natural language into a translated output sentence in another natural language.
- query expansion the model input can be one or more terms in a first natural language and the model output can be one or more terms in the same natural language.
- Other model can be applied to other statistical machine learning applications including classification and regression.
- FIG. 3 is a flowchart of an example process 300 for parallel online training of model parameters.
- the process 200 will be described with respect to a system (e.g., a discriminative training system) that performs the process 200 .
- the system separates 302 training data among workers.
- the training data can include a set of correct input-output pairs (e.g., (x, y*) pairs).
- the system assigns the training data into parts based on, for example, the number of available workers.
- the master identifies available workers and dynamically defines a tree structure relating the workers to the master.
- the training data is evenly divided among the workers, e.g., data parts of equal size or equal numbers of training examples per worker.
- the training data can be separated into data parts of various sizes. Each worker becomes responsible for processing the assigned data part from the training data.
- the system identifies 304 a current model.
- the current model can be generated by the system according to specified parameters or user input. Alternatively, the current model can be received (e.g., from a storage location or from a remote location).
- the model provides a voted perceptron technique in which parameter values converge in a finite number of iterations.
- a particular update rule can be used to update the parameter vector w provided by: w ⁇ w+ ⁇ [ ⁇ ( x,y *) ⁇ ( x, ⁇ )].
- ⁇ is a learning rate and ⁇ (x, y*) ⁇ (x, ⁇ ) represents the difference between the feature functions for the predicted value ⁇ and the correct value y*.
- the update is performed each time a misclassification occurs, which is whenever
- y ⁇ ⁇ ( x , w ) arg ⁇ ⁇ max y ⁇ GEN ⁇ ( x ) ⁇ w ⁇ ⁇ ⁇ ( x , y ) is different from y*.
- a particular parameter is updated when the feature functions do not provide the correct output for a given input.
- the loss function is factored into the update rule, for example in slack-rescaled support vector machines, resulting in an update defined by: w ⁇ w+ ⁇ [ ⁇ ( x, y *) ⁇ ( x, ⁇ )] L ( y*, y ).
- the update is performed based on the predicted output of:
- y ⁇ ⁇ ( x , w ) arg ⁇ ⁇ max y ⁇ GEN ⁇ ( x ) ⁇ ⁇ 1 + w ⁇ [ ⁇ ⁇ ( x , y * ) - ⁇ ⁇ ( x , y ) ] ⁇ ⁇ L ⁇ ( y * , y ) .
- the score is rescaled by the loss with respect to the correct output.
- the system broadcasts 306 the identified model to the workers.
- the master broadcasts the model to the workers.
- the model is directly broadcast to each worker.
- the model is broadcast to the first level of workers, which in turn broadcasts the model to the next level of workers.
- the model is broadcast level-by-level until all workers include the model.
- the system independently processes 308 assigned data parts of the training data at each worker according to the received model.
- the part of the training data assigned to each particular worker is processed according to an online technique (e.g., the perceptron or online slack-scaled SVM techniques described above).
- an online technique e.g., the perceptron or online slack-scaled SVM techniques described above.
- the inputs are applied to the model to generate particular outputs.
- the outputs generated from the model for the training data of the particular workers are used to generate updates to the model parameters.
- the workers generate 310 updates for one or more model parameters.
- the updates are generated according to the model and type of update rules used. For example, for a perceptron or online slack-scaled SVM techniques there are particular update rules as described above.
- the update rule identifies parameter values for particular features using the difference between the predicted output value and the correct output value for a given input.
- the system combines 312 updates from lower level workers and sends the updates up to a next higher level that the particular worker is in communication with. For example, if a particular worker is at a lowest level (e.g., worker 3 of FIG. 2 ), there are no lower level workers to combine updates with. Consequently, worker 3 sends its updates up to the next level worker according to the tree structure (e.g., to worker 1 ).
- a lowest level e.g., worker 3 of FIG. 2
- worker 3 sends its updates up to the next level worker according to the tree structure (e.g., to worker 1 ).
- worker 2 receives updates from one or more lower level workers according to the tree structure.
- the tree structure identifies both which worker processes are to send updates up to worker 2 as well as the worker to which worker 2 sends its combined updates. For example, worker 2 receives updates from worker 3 and worker 4 and combines them with any of its own updates.
- each worker waits until update are received from all lower level workers that are sending updates before combining the updates.
- Combining the updates includes aggregating the updates from all data parts being combined at the worker.
- the updates are aggregated for each feature. Aggregation can include summing, identifying a maximum, or other combination of the updates.
- the highest level workers receive aggregated updates from all lower workers operating on a data part, the highest level workers send 314 the updates to the master.
- a counter is maintained.
- the counter counts the number of times a given feature fires in the respective data part.
- a feature is firing in a data part if it is present in an output involved in the updates for that data part. In other words, the number of times particular features associated with an output result that triggers an update process is counted.
- the combined updates (along with the counter values) are then sent up to the next level worker (or to the master) according to the tree structure.
- the master then generates 316 a new model using the updates received from one or more workers.
- the master calculates real updates.
- the real updates are calculated by dividing the update for each feature by the counter value associated with that feature (e.g., the number of data parts in which the feature was updated). For example, in a system having 10 workers, if the feature was updated in two data parts, the real update is calculated by summing the updates and then dividing by the number of data parts updated (i.e., 2).
- Other techniques can be used for calculating the real updates. For example, the updates can be averaged over all workers instead of just firing workers. However, this tends to penalize the updates, slowing the calculation of optimal parameter values.
- the real updates to the model parameters are then added to the current model to generate the new updated model.
- the master optionally selects particular features to update in the model at each iteration. For example, when the real updates are generated for each feature, the updates can be compared to some threshold contribution value. If the contribution is small, it can be considered noise and the parameter value can be left the same. If the contribution is above the threshold, the parameter value for the feature is updated. Selecting features as opposed to updating all features can result in more iterations being required before obtaining the optimal parameter values. However, overfitting of the data can be reduced.
- the system determines 318 whether additional iterations are to be performed.
- the number of iterations performed can be a specified number or in response to one or more conditions being satisfied. For example, a specified number (e.g., 20) of iterations can be performed regardless of the updates to the model between iterations.
- the specified number can be specified, for example, based on empirical data regarding the convergence of the particular model type.
- the iterations continue until a specified condition is met.
- the condition can be a degree of change from a prior model to the next model due to the updates.
- the condition can be a threshold number of features that are being updated. The iterations can continue until the degree or number of changes are below a threshold amount.
- a combination of specified number of iterations and conditions are used. For example, a specified condition can be used that is capped by a specified maximum number of iterations.
- the training of the model is complete 320 and can be used by a system to process input data to generate output data. However, when there are other iterations to perform, the process repeats beginning with the master broadcasting 306 the new model to the workers having a data part of the training data.
- each worker process works independently on a particular part of the training data.
- online updates are generated. After each iteration, each worker receives an updated model that includes parameter updates from all other workers.
- an online technique can be parallelized for a distributed system.
- the master synchronizes the learning process.
- the master initiates each subsequent iteration using an updated model only after receiving updates from all workers.
- This provides an approximation of online learning.
- the approximation allows for the combination of parallel updates that closely mimic online learning.
- an approximation of online learning can be performed on very large data sets distributed over multiple workers.
- the parallel updates generated by the online learning at each worker are combined to identify real updates for the model.
- the online discriminative training model is used to train a statistical machine translation system.
- a log-linear or linear model can be used for discriminative training of a machine translation system.
- the features of such a model are typically components of a generative model. For example, if f 1 J and e 1 I are two sentences in source and target languages having lengths J and I respectively, then the probability of the source sentence occurring given the target sentence, P(f 1 J
- the model of a machine translations system can be trained to maximize an evaluation metric for decoded sentences.
- the system can be trained to maximize a BLEU score.
- training to maximize BLEU score can be performed using an iterative online search algorithm.
- a N-best re-ranking approach can be used. For example, when lexical n-grams are used as features, the number of features increase dramatically such that an N-best re-ranking approach can be used.
- the N-best list re-ranking can be treated as a structured prediction problem where a set of candidate outputs is approximated by N-best lists decoded using a baseline linear model w b .
- Both the voted perception and slack-scaled supported vector processes described above can be used in the machine translation framework with some modification.
- GEN(x) is approximated by the N-best list for x.
- y* can be replaced by an oracle candidate in the N-best list.
- the oracle candidate is the candidate having the least lost.
- This N-best version provides a particular instance of the model for use with machine translation and does not modify the process for generating and combining updates to perform iterations of updating the model parameters described above.
- Training a machine translation system can include receiving training data derived from one or more parallel text collections.
- the NIST 2006 machine translation evaluation and the LDC named entity list (“LDC2005T34”) can be used to identify training data.
- a feature selection technique can be used.
- the training adapts to certain stylistic preferences in the training corpus.
- the overfit parameter values might not improve and might even deteriorate the quality of the translations.
- the master When the feature updates from all worker processes are received at the master, the master generates the real updates for each feature. The value of each feature update can be compared to a threshold value. If the feature has an update below the threshold it is not added to the updated model. This provides a control on the number of features selected at each iteration. The number and particular feature function updates included in the updated model can vary for each iteration.
- FIG. 4 is a schematic diagram of a generic computer system 400 .
- the system 400 can be used for practicing operations described, for example in association with the method 300 of FIG. 3 , in one embodiment.
- the system 400 can include a processor 410 , a memory 420 , a storage device 430 , and input/output devices 440 . Each of the components 410 , 420 , 430 , and 440 are interconnected using a system bus 450 .
- the processor 410 is capable of processing instructions for execution within the system 400 .
- the processor 410 is a single-threaded processor.
- the processor 410 is a multi-threaded processor.
- the processor 410 is capable of processing instructions stored in the memory 420 or on the storage device 430 to display graphical information for a user interface on the input/output device 440 .
- the memory 420 is a computer readable medium such as volatile or non volatile that stores information within the system 400 .
- the storage device 430 is capable of providing persistent storage for the system 400 .
- the storage device 430 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device, or other suitable persistent storage means.
- the input/output device 440 provides input/output operations for the system 400 .
- the input/output device 440 includes a keyboard and/or pointing device.
- the input/output device 440 includes a display unit for displaying graphical user interfaces.
- Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- Embodiments of the subject matter described in this specification can be implemented as one or more computer program products, i.e., one or more modules of computer program instructions encoded on a computer readable medium for execution by, or to control the operation of, data processing apparatus.
- the computer readable medium can be a machine-readable storage device, a machine-readable storage substrate, a memory device, a composition of matter effecting a machine-readable propagated signal, or a combination of one or more of them.
- data processing apparatus encompasses all apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers.
- the apparatus can include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- a propagated signal is an artificially generated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus.
- a computer program (also known as a program, software, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, and it can be deployed in any form, including as a stand alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.
- a computer program does not necessarily correspond to a file in a file system.
- a program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code).
- a computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- the processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer.
- a processor will receive instructions and data from a read only memory or a random access memory or both.
- the essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- a computer need not have such devices.
- a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio player, a Global Positioning System (GPS) receiver, to name just a few.
- Computer readable media suitable for storing computer program instructions and data include all forms of non volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks.
- the processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described is this specification, or any combination of one or more such back end, middleware, or front end components.
- the components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (“LAN”) and a wide area network (“WAN”), e.g., the Internet.
- LAN local area network
- WAN wide area network
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network.
- the relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
Abstract
Description
w←w+α[Φ(x,y*)−Φ(x,ŷ)].
is different from y*. In other words, a particular parameter is updated when the feature functions do not provide the correct output for a given input.
w←w+α[Φ(x, y*)−Φ(x, ŷ)]L(y*, y).
Essentially, instead of using a top scoring output y, the score is rescaled by the loss with respect to the correct output.
Claims (39)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US12/055,967 US8027938B1 (en) | 2007-03-26 | 2008-03-26 | Discriminative training in machine learning |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US92024307P | 2007-03-26 | 2007-03-26 | |
US12/055,967 US8027938B1 (en) | 2007-03-26 | 2008-03-26 | Discriminative training in machine learning |
Publications (1)
Publication Number | Publication Date |
---|---|
US8027938B1 true US8027938B1 (en) | 2011-09-27 |
Family
ID=44652576
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US12/055,967 Active 2030-02-01 US8027938B1 (en) | 2007-03-26 | 2008-03-26 | Discriminative training in machine learning |
Country Status (1)
Country | Link |
---|---|
US (1) | US8027938B1 (en) |
Cited By (24)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20110252382A1 (en) * | 2010-04-07 | 2011-10-13 | International Business Machines Corporation | Process performance using a people cloud |
US20120330647A1 (en) * | 2011-06-24 | 2012-12-27 | Microsoft Corporation | Hierarchical models for language modeling |
US20130110491A1 (en) * | 2011-10-28 | 2013-05-02 | Microsoft Corporation | Discriminative learning of feature functions of generative type in speech translation |
US8849974B2 (en) | 2010-04-14 | 2014-09-30 | International Business Machines Corporation | Social network based information discovery about network data processing systems |
CN105956021A (en) * | 2016-04-22 | 2016-09-21 | 华中科技大学 | Automated task parallel method suitable for distributed machine learning and system thereof |
CN106034146A (en) * | 2015-03-12 | 2016-10-19 | 阿里巴巴集团控股有限公司 | Information interaction method and system |
WO2017156791A1 (en) * | 2016-03-18 | 2017-09-21 | Microsoft Technology Licensing, Llc | Method and apparatus for training a learning machine |
CN107784364A (en) * | 2016-08-25 | 2018-03-09 | 微软技术许可有限责任公司 | The asynchronous training of machine learning model |
CN108009642A (en) * | 2016-10-31 | 2018-05-08 | 腾讯科技（深圳）有限公司 | Distributed machines learning method and system |
EP3370166A4 (en) * | 2015-11-16 | 2018-10-31 | Huawei Technologies Co., Ltd. | Method and apparatus for model parameter fusion |
CN111105016A (en) * | 2019-12-06 | 2020-05-05 | 浪潮电子信息产业股份有限公司 | Data processing method and device, electronic equipment and readable storage medium |
US10769383B2 (en) * | 2017-10-23 | 2020-09-08 | Alibaba Group Holding Limited | Cluster-based word vector processing method, device, and apparatus |
US10805353B2 (en) | 2018-09-26 | 2020-10-13 | Bank Of America Corporation | Security tool |
US10846483B2 (en) | 2017-11-14 | 2020-11-24 | Advanced New Technologies Co., Ltd. | Method, device, and apparatus for word vector processing based on clusters |
US11087234B2 (en) * | 2016-01-29 | 2021-08-10 | Verizon Media Inc. | Method and system for distributed deep machine learning |
US11132602B1 (en) * | 2016-08-11 | 2021-09-28 | Twitter, Inc. | Efficient online training for machine learning |
EP3129920B1 (en) * | 2014-04-11 | 2022-06-08 | Google LLC | Parallelizing the training of convolutional neural networks |
DE102021107675A1 (en) | 2021-03-26 | 2022-09-29 | Bayerische Motoren Werke Aktiengesellschaft | System and method for implementing intelligent vehicle functions |
US11481210B2 (en) * | 2020-12-29 | 2022-10-25 | X Development Llc | Conditioning autoregressive language model to improve code migration |
WO2022227792A1 (en) * | 2021-04-30 | 2022-11-03 | International Business Machines Corporation | Federated training of machine learning models |
US11531912B2 (en) * | 2019-04-12 | 2022-12-20 | Samsung Electronics Co., Ltd. | Electronic apparatus and server for refining artificial intelligence model, and method of refining artificial intelligence model |
US11734614B1 (en) * | 2020-03-26 | 2023-08-22 | Amazon Technologies, Inc. | Training service for an aggregated machine learning model |
US11954611B2 (en) | 2020-08-27 | 2024-04-09 | International Business Machines Corporation | Tensor comparison across a distributed machine learning environment |
US11983623B1 (en) * | 2018-02-27 | 2024-05-14 | Workday, Inc. | Data validation for automatic model building and release |
Citations (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6381563B1 (en) * | 1999-01-22 | 2002-04-30 | Cadence Design Systems, Inc. | System and method for simulating circuits using inline subcircuits |
US6848084B1 (en) * | 2002-07-02 | 2005-01-25 | Cadence Design Systems, Inc. | Method and apparatus for verification of memories at multiple abstraction levels |
US7124377B2 (en) * | 2003-04-04 | 2006-10-17 | Interniversitair Microelektronica Centrum (Imec) | Design method for essentially digital systems and components thereof and essentially digital systems made in accordance with the method |
US7263674B2 (en) * | 2003-12-05 | 2007-08-28 | Coventor, Inc. | System and method for three-dimensional visualization and postprocessing of a system model |
US7272575B2 (en) * | 2001-07-13 | 2007-09-18 | Lilly Mae Vega | Method and system for facilitating service transactions |
US7599799B2 (en) * | 1998-10-27 | 2009-10-06 | Microsoft Corporation | Methods for using co-regulated genesets to enhance detection and classification of gene expression patterns |
US7606403B2 (en) * | 2002-10-17 | 2009-10-20 | Intel Corporation | Model-based fusion of scanning probe microscopic images for detection and identification of molecular structures |
US7620743B2 (en) * | 2004-04-01 | 2009-11-17 | Lsi Corporation | System and method for implementing multiple instantiated configurable peripherals in a circuit design |
US7634393B1 (en) * | 2005-01-04 | 2009-12-15 | United States Of America | Technique for coupling meteorology to acoustics in forests |
US7694249B2 (en) * | 2005-10-07 | 2010-04-06 | Sonics, Inc. | Various methods and apparatuses for estimating characteristics of an electronic system's design |
-
2008
- 2008-03-26 US US12/055,967 patent/US8027938B1/en active Active
Patent Citations (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7599799B2 (en) * | 1998-10-27 | 2009-10-06 | Microsoft Corporation | Methods for using co-regulated genesets to enhance detection and classification of gene expression patterns |
US6381563B1 (en) * | 1999-01-22 | 2002-04-30 | Cadence Design Systems, Inc. | System and method for simulating circuits using inline subcircuits |
US7272575B2 (en) * | 2001-07-13 | 2007-09-18 | Lilly Mae Vega | Method and system for facilitating service transactions |
US6848084B1 (en) * | 2002-07-02 | 2005-01-25 | Cadence Design Systems, Inc. | Method and apparatus for verification of memories at multiple abstraction levels |
US7606403B2 (en) * | 2002-10-17 | 2009-10-20 | Intel Corporation | Model-based fusion of scanning probe microscopic images for detection and identification of molecular structures |
US7124377B2 (en) * | 2003-04-04 | 2006-10-17 | Interniversitair Microelektronica Centrum (Imec) | Design method for essentially digital systems and components thereof and essentially digital systems made in accordance with the method |
US7263674B2 (en) * | 2003-12-05 | 2007-08-28 | Coventor, Inc. | System and method for three-dimensional visualization and postprocessing of a system model |
US7620743B2 (en) * | 2004-04-01 | 2009-11-17 | Lsi Corporation | System and method for implementing multiple instantiated configurable peripherals in a circuit design |
US7634393B1 (en) * | 2005-01-04 | 2009-12-15 | United States Of America | Technique for coupling meteorology to acoustics in forests |
US7694249B2 (en) * | 2005-10-07 | 2010-04-06 | Sonics, Inc. | Various methods and apparatuses for estimating characteristics of an electronic system's design |
Non-Patent Citations (23)
Title |
---|
"MPI: A Message-Passing Interface Standard," Message Passing Interface Forum, University of Tennessee, Knoxville, TN, Nov. 15, 2003, 238 pages. |
An analytical model for designing memory hierarchies, Jacob, B.L.; Chen, P.M.; Silverman, S.R.; Mudge, T.N.; Computers, IEEE Transactions on vol. 45 , Issue: 10 Digital Object Identifier: 10.1109/12.543711 Publication Year: 1996 , pp. 1180-1194. * |
Breiman, Leo, "Bagging Predictors," Machine Learning, 24, 1996, pp. 123-140. |
Collins et al., "Incremental Parsing with the Perceptron Algorithm," 42nd Annual Meeting of the Association for Computational Linguistics, Jul. 21-26, 2004, Barcelona, Spain, 9 pages. |
Collins, Michael, "Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms," Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, Jul. 6-7, 2002, 10 pages. |
Collins, Michael, "Parameter Estimation For Statistical Parsing Models: Theory and Practice Of Distribution-Free Methods," Chapter 2, H. Bunt et al. (eds.), New Technologies in Parsing Technology, 2004, pp. 19-55. |
Daume et al., "Learning as Search Optimization: Approximate Large Margin Methods for Structured Prediction," Proceedings, Twenty-Second International Conference on Machine Learning, 2005, 11 pages. |
Koehn, Philipp, "Pharaoh: A Beam Search Decoder for Phrase-Based Statistical Machine Translation Models," Frederking et al. (Eds.): AMTA 2004, LNAI 3265, pp. 115-124. |
Large hierarchical object recognition using libraries of parameterized model sub-parts, Ettinger, G.J.; Computer Vision and Pattern Recognition, 1988. Proceedings CVPR '88., Computer Society Conference on Digital Object Identifier: 10.1109/CVPR.1988.196212 Publication Year: 1988 , pp. 32-41. * |
Liang et al., "An End-to-End Discriminative Approach to Machine Translation," Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, Sydney, Jul. 2006, pp. 761-768. |
Liu et al., "On The Limited Memory BFGS Method For Large Scale Optimization," Mathematical Programming 45, 1989, pp. 503-528. |
Model-checking problems as a basis for parameterized intractability, Flum, J.; Grohe, M.; Logic in Computer Science, 2004. Proceedings of the 19th Annual IEEE Symposium on Digital Object Identifier: 10.1109/LICS.2004.1319633 Publication Year: 2004 , pp. 388-397. * |
Modeling complex information systems using parameterized Petri nets (PPNs), Srinivasan, P.; Gracanin, D.; Brockhaus, D.; Circuits and Systems, 1994., Proceedings of the 37th Midwest Symposium on vol. 1 Digital Object Identifier: 10.1109/MWSCAS.1994.519395 Publication Year: 1994 , pp. 735-738. * |
Och, Franz Josef, "Minimum Error Rate Training in Statistical Machine Translation," Proceedings of the 415' Annual Meeting of the Association for Computational Linguistics, Sapporo, Japan, pp. 160-167. |
Papineni et al., "BLEU: a Method for Automatic Evaluation of Machine Translation," Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, Jul. 2002, pp. 311-318. |
Ratliff et al., "(Online) Subgradient Methods for Structured Prediction," Jan. 1, 2007, Eleventh International Conference on Artificial Intelligence and Statistics, San Juan, Puerto Rico, 9 pages. |
Roark et al., "Discriminative Language Modeling with Conditional Random Fields and the Perceptron Algorithm," 42nd Annual Meeting of the Association for Computational Linguistics, Jul. 21-26, 2004, Barcelona, Spain, 9 pages. |
Taskar et al., "Max-Margin Markov Networks," Advances in Neural Information Processing Systems 16, Copyright 2004 Massachusetts Institute of Technology, 11 pages. |
Tsochantaridis et al., "Large Margin Methods for Structured and Interdependent Output Variables," Journal of Machine Learning Research 6, 2005, pp. 1453-1484. |
Vishwanathan et al., "Accelerated Training of Conditional Random Fields with Stochastic Gradient Methods," In Proceedings of the International Conference on Machine Learning, 2006, 8 pages. |
Vishwanathan et al., "Step Size Adaptation in Reproducing Kernel Hilbert Space," Journal of Machine Learning Research 7, 2006, pp. 1107-1133. |
Yamada et al., "Reranking for Large-Scale Statistical Machine Translation," Neural Information Processing Systems, 2006, Machine Translation Workshop, Vancouver, Canada, 20 pages. |
Zanni et al., "Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems*," Journal of Machine Learning Research 7, 2006, pp. 1467-1492. |
Cited By (35)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20110252382A1 (en) * | 2010-04-07 | 2011-10-13 | International Business Machines Corporation | Process performance using a people cloud |
US8849974B2 (en) | 2010-04-14 | 2014-09-30 | International Business Machines Corporation | Social network based information discovery about network data processing systems |
US8977537B2 (en) * | 2011-06-24 | 2015-03-10 | Microsoft Technology Licensing, Llc | Hierarchical models for language modeling |
US20120330647A1 (en) * | 2011-06-24 | 2012-12-27 | Microsoft Corporation | Hierarchical models for language modeling |
US20130110491A1 (en) * | 2011-10-28 | 2013-05-02 | Microsoft Corporation | Discriminative learning of feature functions of generative type in speech translation |
EP3129920B1 (en) * | 2014-04-11 | 2022-06-08 | Google LLC | Parallelizing the training of convolutional neural networks |
CN106034146B (en) * | 2015-03-12 | 2019-10-22 | 阿里巴巴集团控股有限公司 | Information interacting method and system |
CN106034146A (en) * | 2015-03-12 | 2016-10-19 | 阿里巴巴集团控股有限公司 | Information interaction method and system |
US11373116B2 (en) | 2015-11-16 | 2022-06-28 | Huawei Technologies Co., Ltd. | Model parameter fusion method and apparatus |
EP3370166A4 (en) * | 2015-11-16 | 2018-10-31 | Huawei Technologies Co., Ltd. | Method and apparatus for model parameter fusion |
EP3745284A1 (en) * | 2015-11-16 | 2020-12-02 | Huawei Technologies Co., Ltd. | Model parameter fusion method and apparatus |
US11087234B2 (en) * | 2016-01-29 | 2021-08-10 | Verizon Media Inc. | Method and system for distributed deep machine learning |
WO2017156791A1 (en) * | 2016-03-18 | 2017-09-21 | Microsoft Technology Licensing, Llc | Method and apparatus for training a learning machine |
CN108431794A (en) * | 2016-03-18 | 2018-08-21 | 微软技术许可有限责任公司 | Method and apparatus for training learning machine |
US11334814B2 (en) | 2016-03-18 | 2022-05-17 | Microsoft Technology Licensing, Llc | Method and apparatus for training a learning machine |
CN105956021B (en) * | 2016-04-22 | 2019-05-21 | 华中科技大学 | A kind of automation task suitable for distributed machines study parallel method and its system |
CN105956021A (en) * | 2016-04-22 | 2016-09-21 | 华中科技大学 | Automated task parallel method suitable for distributed machine learning and system thereof |
US11132602B1 (en) * | 2016-08-11 | 2021-09-28 | Twitter, Inc. | Efficient online training for machine learning |
CN107784364A (en) * | 2016-08-25 | 2018-03-09 | 微软技术许可有限责任公司 | The asynchronous training of machine learning model |
CN108009642B (en) * | 2016-10-31 | 2021-12-14 | 腾讯科技（深圳）有限公司 | Distributed machine learning method and system |
CN108009642A (en) * | 2016-10-31 | 2018-05-08 | 腾讯科技（深圳）有限公司 | Distributed machines learning method and system |
US10769383B2 (en) * | 2017-10-23 | 2020-09-08 | Alibaba Group Holding Limited | Cluster-based word vector processing method, device, and apparatus |
US10846483B2 (en) | 2017-11-14 | 2020-11-24 | Advanced New Technologies Co., Ltd. | Method, device, and apparatus for word vector processing based on clusters |
US11983623B1 (en) * | 2018-02-27 | 2024-05-14 | Workday, Inc. | Data validation for automatic model building and release |
US10805353B2 (en) | 2018-09-26 | 2020-10-13 | Bank Of America Corporation | Security tool |
US11531912B2 (en) * | 2019-04-12 | 2022-12-20 | Samsung Electronics Co., Ltd. | Electronic apparatus and server for refining artificial intelligence model, and method of refining artificial intelligence model |
CN111105016A (en) * | 2019-12-06 | 2020-05-05 | 浪潮电子信息产业股份有限公司 | Data processing method and device, electronic equipment and readable storage medium |
CN111105016B (en) * | 2019-12-06 | 2023-04-28 | 浪潮电子信息产业股份有限公司 | Data processing method and device, electronic equipment and readable storage medium |
US11734614B1 (en) * | 2020-03-26 | 2023-08-22 | Amazon Technologies, Inc. | Training service for an aggregated machine learning model |
US11954611B2 (en) | 2020-08-27 | 2024-04-09 | International Business Machines Corporation | Tensor comparison across a distributed machine learning environment |
US11656867B2 (en) | 2020-12-29 | 2023-05-23 | Google Llc | Conditioning autoregressive language model to improve code migration |
US11481210B2 (en) * | 2020-12-29 | 2022-10-25 | X Development Llc | Conditioning autoregressive language model to improve code migration |
DE102021107675A1 (en) | 2021-03-26 | 2022-09-29 | Bayerische Motoren Werke Aktiengesellschaft | System and method for implementing intelligent vehicle functions |
WO2022227792A1 (en) * | 2021-04-30 | 2022-11-03 | International Business Machines Corporation | Federated training of machine learning models |
GB2620539A (en) * | 2021-04-30 | 2024-01-10 | Ibm | Federated training of machine learning models |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US8027938B1 (en) | Discriminative training in machine learning | |
US10860808B2 (en) | Method and system for generation of candidate translations | |
US11113479B2 (en) | Utilizing a gated self-attention memory network model for predicting a candidate answer match to a query | |
CN102272754B (en) | Custom language models | |
Ling et al. | Latent predictor networks for code generation | |
US20220171942A1 (en) | Natural Language Processing with an N-Gram Machine | |
JP7031101B2 (en) | Methods, systems and tangible computer readable devices | |
EP2137639B1 (en) | Large language models in machine translation | |
US9836457B2 (en) | Machine translation method for performing translation between languages | |
US11061805B2 (en) | Code dependency influenced bug localization | |
US20190130249A1 (en) | Sequence-to-sequence prediction using a neural network model | |
US8645119B2 (en) | Minimum error rate training with a large number of features for machine learning | |
Mairesse et al. | Stochastic language generation in dialogue using factored language models | |
US8185375B1 (en) | Word alignment with bridge languages | |
CN109074517B (en) | Global normalized neural network | |
US10346548B1 (en) | Apparatus and method for prefix-constrained decoding in a neural machine translation system | |
Fried et al. | Incorporating both distributional and relational semantics in word representations | |
Mathur et al. | Online learning approaches in computer assisted translation | |
Tilk et al. | Event participant modelling with neural networks | |
Le et al. | Measuring the influence of long range dependencies with neural network language models | |
Burdisso et al. | τ-SS3: A text classifier with dynamic n-grams for early risk detection over text streams | |
US20200151398A1 (en) | Sequence transduction neural networks | |
Plank et al. | Importance weighting and unsupervised domain adaptation of POS taggers: a negative result | |
US20150006151A1 (en) | Model learning method | |
Blain et al. | Exploring hypotheses spaces in neural machine translation |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:XU, PENG;TSOCHANDARIDIS, IOANNIS;REEL/FRAME:020874/0765Effective date: 20080424 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
FEPP | Fee payment procedure |
Free format text: PAYOR NUMBER ASSIGNED (ORIGINAL EVENT CODE: ASPN); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
FPAY | Fee payment |
Year of fee payment: 4 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044101/0405Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 8 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 12TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1553); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 12 |