US11210475B2 - Enhanced attention mechanisms - Google Patents
Enhanced attention mechanisms Download PDFInfo
- Publication number
- US11210475B2 US11210475B2 US16/518,518 US201916518518A US11210475B2 US 11210475 B2 US11210475 B2 US 11210475B2 US 201916518518 A US201916518518 A US 201916518518A US 11210475 B2 US11210475 B2 US 11210475B2
- Authority
- US
- United States
- Prior art keywords
- sequence
- encodings
- output
- attention
- computers
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/40—Processing or translation of natural language
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/34—Browsing; Visualisation therefor
- G06F16/345—Summarisation for human users
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
-
- G06N3/0445—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G06N3/0454—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/047—Probabilistic or stochastic networks
Definitions
- the present specification relates to attention mechanisms for sequence-to-sequence models.
- Sequence-to-sequence models with a soft attention mechanism have been applied to numerous sequence transduction problems. This type of model may be used to infer an alignment between an input sequence and an output sequence.
- MoChA Monotonic Chunkwise Attention
- a method is performed by one or more computers, for example, one or more computers of an automated language processing system.
- the method includes: receiving, by the one or more computers, data indicating an input sequence; processing, by the one or more computers, the data using an encoder neural network to generate a sequence of encodings; determining, by the one or more computers, a series of attention outputs with respect to the encodings using one or more attender modules, wherein determining each of the attention outputs includes (i) selecting an encoding from the sequence of encodings and (ii) determining attention over a proper subset of the sequence of encodings, wherein the proper subset of encodings is determined based on a position of the selected encoding in the sequence of encodings, and wherein the selections of encodings are monotonic through the sequence of encodings; generating, by the one or more computers, an output sequence by processing the attention outputs using a decoder neural network; and providing, by the one or more computers
- each of the proper subsets includes a same number of encodings.
- selecting the encoding for each of the attention outputs includes constraining the selection of each selected encoding to be monotonic along the sequence of encodings with respect to an immediately previously selected encoding.
- each of the attention outputs corresponds to a different one of multiple output steps, where, for each output step, the selected encoding is the encoding at (i) a same position in the sequence of encodings as a particular selected encoding for the immediately previous output step or (ii) a position in the sequence of encodings that is subsequent to the position in the sequence of encodings as a particular selected encoding for the immediately previous output step.
- the proper subset is a window of encodings bounded by the selected encoding for the attention output.
- the proper subsets are different positions of a sliding window that advances monotonically along the sequence of encodings for generating the series of attention outputs.
- the sliding window has a fixed size for each of the attention outputs.
- selecting the encoding from the sequence of encodings includes determining a hard monotonic attention output.
- determining attention over each proper subset of the sequence of encodings includes: determining, based on the corresponding hard monotonic attention output, a soft attention output for a region of the encodings occurring before a location corresponding to the hard monotonic attention output.
- the one or more attender modules are configured to determine, for each of the encodings, a soft attention output for a fixed, predetermined number of segments of the encodings.
- the one or more attender modules are configured to compute hard attention outputs, such that (i) for each of the encodings, the decoder neural network attends to only a single encoded state of the encodings, and (ii) the alignment between the input sequence and output sequence is forced to be strictly monotonic.
- the one or more attender modules are configured to compute soft attention for each of the encodings, over multiple different segments of the encodings selected with respect to adaptively set locations encodings.
- the one or more computers are part of an automated speech recognition system; receiving the data indicating the input sequence includes receiving a sequence of feature vectors that indicate audio characteristics of an utterance; generating the output sequence includes generating a sequence of distributions over a set of symbols that represents at least one of linguistic states, phones, characters, word pieces, or words; and providing the output that indicates the language sequence includes providing, an output that indicates a transcription for the utterance.
- the automated speech recognizer performs online recognition of the utterance, such that an initial portion of the utterance is recognized and indicated while the speaker is speaking a later portion of the utterance.
- the encoder neural network includes at least one convolutional layer, at least one convolutional long short-term memory (LSTM) layer, and at least one unidirectional LSTM layer; and the decoder includes a unidirectional LSTM layer.
- LSTM convolutional long short-term memory
- the one or more computers are part of an automated document summarization system; receiving the data indicating the input sequence includes receiving a sequence of inputs indicating text of at least a portion of a first text; generating the output sequence includes generating a sequence of distributions over a predetermined word vocabulary; and providing the output that indicates the language sequence includes providing, an output that indicates generated natural language text representing an automatically summarized version of at least a portion of the first text.
- the encoder neural network includes a bidirectional LSTM layer; and the decoder neural network includes a unidirectional LSTM layer and a softmax layer that outputs a distribution over the predetermined word vocabulary.
- FIGS. 1A-1C are diagrams of schematics of attention mechanisms, according to an implementation of the present disclosure.
- FIGS. 2A-2D are attention alignments plots and speech utterance feature sequence for the speech recognition task, according to an implementation of the present disclosure.
- FIG. 3 is a chart of running time requirements, illustrating computational complexity of different modelling approaches.
- FIG. 4 is a diagram illustrating an example of a system 400 using monotonic chunkwise attention (MoChA) for speech recognition.
- MoChA monotonic chunkwise attention
- the present disclosure introduces a novel attention mechanism, which retains the online and linear-time advantages of hard monotonic attention while allowing for the flexibility and accuracy improvements of soft attention alignments.
- a sequence-to-sequence model can process an input sequence with an encoder recurrent neural network (RNN) to produce a sequence of hidden states, referred to as a memory.
- RNN encoder recurrent neural network
- a decoder RNN then autoregressively produces the output sequence.
- the decoder is directly conditioned by an attention mechanism, which allows the decoder to refer back to entries in the encoder's hidden state sequence.
- FIGS. 1A-1C are diagrams of schematics of several attention mechanisms.
- each node represents the possibility of the model attending to a given memory entry (horizontal axis) at a given output step (vertical axis).
- the memory entries are labeled h 0 to h 7 in order from left to right, and the outputs for the different steps are labeled y 0 to y 4 .
- Each row of nodes represents the same set of memory entries ⁇ h 0 , . . . , h 7 ⁇ .
- the shading of the different rows represents the different attention computed for this set of memory entries.
- the top row represents the attention used on the set of memory entries to calculate the output y 0
- the row below represents the attention used on the same set of memory entries to calculate the output y 1 , and so on.
- FIG. 1A illustrates soft attention, in which the model assigns a probability (represented by the shade of gray of each node, with darker shades representing higher probabilities) to each memory entry at each output step.
- the context vector is computed as the weighted average of the memory, weighted by these probabilities.
- a darker shading represents a higher weighting of the content of the memory entry.
- attention is computed over the entire set of memory entries ⁇ h 0 , . . . , h 7 ⁇ .
- FIG. 1B shows an example of monotonic attention.
- monotonic attention inspects memory entries from left-to-right, choosing whether to move on to the next memory entry (where nodes with X show memory entries inspected and skipped) or stop and attend (shown as black nodes).
- the context vector is hard-assigned to the memory entry that was attended to. In other words, the single memory entry where attention is placed can be used as the context vector.
- the monotonic attention process starts inspecting memory entries from the position selected for the previous output time step. For example, after selecting memory entry h 1 to attend to for generating output y 0 , the monotonic attention mechanism begins with inspection of the same memory entry h 1 when determining attention for generating output y 1 .
- the attention mechanism progresses through the memory entries in the same direction each time, e.g., from left to right in the example, until reaching a single memory entry to attend to.
- the attention mechanism inspects but moves beyond memory entries h 1 and h 2 to attend to memory entry h 3 for providing the context vector used to generate output y 1 .
- the monotonic nature of the attention mechanism can be seen in the way the index value for the memory entries attended to does not decrease (e.g., stays the same or increases) from each output step to the next (e.g., h 1 is selected for computing y 0 , h 3 is selected for computing y 1 , h 4 is selected for computing y 2 , and so on).
- FIG. 1C shows a technique called “monotonic chunkwise attention” (MoChA), which enables performance of soft attention over small chunks of the memory preceding where a hard monotonic attention mechanism has chosen to attend.
- MoChA utilizes a hard monotonic attention mechanism to choose the endpoint (shown as nodes with bold borders) of the chunk over which it attends. This endpoint is the right-most node in each chunk.
- the chunk boundaries (here, with a window size of 3) are shown as dashed lines.
- the model performs soft attention (with attention weighting shown as the different shades of gray) over the chunk of the memory set by the window position for the time step, and computes the context vector as the chunk's weighted average.
- the chunk the memory, h that is used to determine the context vector for a given time step is set by the window size, which is fixed in this example, and the endpoint position, which is selected according to monotonic constraints.
- the attention mechanism computes hard attention to select memory entry h 1 .
- This sets the right-most boundary of the subset of memory entries that will be used for soft attention calculation.
- the sliding window of size three thus includes the selected memory entry h 1 at the right-most boundary extends to include up to two additional memory entries that occur earlier in the sequence of memory entries. In this case, there is only one prior memory entry, h 0 , so the window includes only memory entries h 0 and h 1 .
- the attention mechanism proceeds monotonically through the memory to select a memory entry as a boundary for the sliding window.
- the monotonic constraint requires the mechanism to select either the memory entry h 1 that was selected for the previous output step or the memory entry for a subsequent output step.
- attention mechanism begins consideration at the previous selected memory entry, h 1 , then considers the next memory entry, h 2 , then considers the next memory entry, h 3 , and selects memory entry h 3 as the hard attention result. From this selection, the sliding window is set with memory entry h 3 as the right-most memory entry included in the sliding window, and the window size of three encompasses the memory entries h 2 and h 1 also.
- the attention mechanism computes soft attention over the set of memory entries h 3 , h 2 , and h 1 and the attention output is used to generate output y 1 .
- the process continues in a similar manner with the hard attention selection for computing attention for generating output y 2 beginning with consideration of memory entry h 3 , and so on.
- the attention mechanism can progress through the memory in single direction, as with hard attention.
- a memory entry is selected not as a single memory entry to use, but as the boundary of a region comprising multiple memory entries that will be attended to.
- the set of memory entries that the sliding window encompasses can vary from one time step to another, but moves only monotonically, e.g., in a single direction, through the memory.
- each attention determination can be based on a subset of memory entries (e.g., encodings or encoded states), where a boundary of the subset is set by the memory entry selected using hard attention.
- the sliding window has a fixed size or maximum size, so that the same number of values or memory entries are included in each subset.
- the window size is three memory entries, as shown by the dashed boxes drawn over the memory states. This means that up to three memory entries are selected in the subset for determining each attention output, e.g., the memory entry at the far right of the subset that was selected as a result of hard attention and up to two immediately previous memory entries in the memory.
- the subset includes only two memory entries.
- the memory entries h 0 -h 7 can represent encodings for different input steps.
- the input steps can represent a progression along a dimension, such as (i) steps in time (e.g., through audio or video data), (ii) steps through the content of a document or data set (e.g., positions along a document from beginning to end), (iii) steps along a spatial dimension, (iv) steps along an index, and so on.
- the memory entries h 0 -h 7 can represent a series of encodings that an encoder neural network generated based on processing a series of acoustic feature vectors representing successive frames or segments of audio data to be analyzed, e.g., for the purpose of keyword spotting or speech recognition.
- each memory entry h0-h7 may represent a different output of an encoder, with each encoding being produced in response to a different frame of input data to the encoder.
- Memory entry h 0 can represent the encoding for frame f 0
- memory entry h 1 can represent the encoding for frame f 1 , and so on.
- the memory entries h 0 -h 7 can indicate encodings generated for different successive sections of a document being analyzed, e.g., for document summarization or machine translation.
- the memory entries h 0 -h 7 can represent encodings generated for different portions of an image being analyzed, e.g., for object recognition or other image processing.
- the memory entries h 0 -h 7 can represent encodings for phones or text elements to be used in text-to-speech synthesis.
- the outputs y 0 -y 4 represent outputs for different output steps.
- the output steps are not required to correspond to the input steps. In other words, there may be multiple input steps for each output step, and vice versa.
- FIGS. 1A-1C do not represent the actual content of the output y 0 -y 4 , but instead illustrate the principles used to create the attention outputs that would be used to generate the respective outputs y 0 -y 4 .
- the content of the outputs y0-y4 can represent any appropriate sequence of outputs for a sequence-to-sequence model. As an example, for speech recognition, the outputs y0-y4 may represent a series of predictions of phone, grapheme, word piece, or words.
- the predictions may be in the form of a probability distribution over a predetermined set of output targets.
- the outputs may represent likelihoods whether a predetermined keyword has occurred given the values evaluated in the memory.
- the outputs may represent indications of a grapheme, word piece, or word to be included in a document summary or translation.
- the outputs may represent predictions of sounds, speech units, prosody, acoustic parameters, or other aspects of synthesized speech.
- the model includes a training procedure, which allows it to be straightforwardly applied to existing sequence-to-sequence models and trained with standard backpropagation. It is experimentally shown that MoChA effectively closes the gap between monotonic and soft attention on online speech recognition and provides a 20% relative improvement over monotonic attention on document summarization (a task which does not exhibit monotonic alignments). The advantages incur only a modest increase in the number of parameters and computational cost.
- MoChA can be considered a generalization of monotonic attention, the approach is re-derived and some of its shortcomings are pointed out. It is shown how soft attention over chunks can be straightforwardly added to hard monotonic attention, giving us the MoChA attention mechanism. It is also shown how MoChA can be trained efficiently with respect to the mechanism's expected output, which allows the use of standard backpropagation.
- h j EncoderRNN( x j ,h j ⁇ 1) (1)
- the parameter s i represents the decoder's state and c i represents a “context” vector, which is computed as a function of the encoder hidden state sequence h.
- the context vector c i is the sole conduit through which the decoder has access to information about the input sequence.
- the context vector is computed as a simple weighted average of h, weighted by ⁇ i:
- the soft attention mechanism is illustrated in FIG. 1A . Note that in order to compute c i for any output time step i, all of the encoder hidden states h j for j ⁇ 1, . . . , T ⁇ are calculated. The soft attention mechanism is not applicable to online/real-time sequence transduction problems, because it needs to have observed the entire input sequence before producing any output. Furthermore, producing each context vector c i involves computing T energy scalar terms and weighting values. While the operations can typically be parallelized, the soft attention mechanism is associated with increased O(TU) cost in time and space.
- ⁇ i , j p i , j ⁇ ( ( 1 - p i , j - 1 ) ⁇ ⁇ i , j - 1 p i , j - 1 + ⁇ i - 1 , j )
- Equation (11) can be explained by observing that (1 ⁇ p i,j-1 ) ⁇ i,j-1 /p i,j-1 is the probability of attending to memory entry j ⁇ 1 at the current output time step ( ⁇ i,j-1 ) corrected for the fact that the model did not attend to memory entry j (by multiplying by (1 ⁇ p i,j-1 ) and dividing by p i,j-1 ).
- the addition of ⁇ i-1,j represents the additional possibility that the model attended to entry j at the previous output time step, and finally multiplying it all by pi,j reflects the probability that the model selected memory item j at the current output time step i.
- MonotonicEnergy ⁇ ( s i - 1 , h j ) g ⁇ ⁇ T ⁇ ⁇ ⁇ ⁇ tanh ⁇ ( W s ⁇ s i - 1 + W h ⁇ h j + b ) + r
- Parameters g, r are learnable scalars and v, W s , W h , b are as in equation (5).
- MoChA Monotonic Chunkwise Attention
- the core of the idea is to allow the attention mechanism to perform soft attention over small “chunks” of memory preceding where a hard monotonic attention mechanism decides to stop. Some degree of softness is facilitated in the input-output alignment, while retaining the online decoding and linear-time complexity advantages.
- the hard monotonic attention process is followed in order to determine t i (the location where the hard monotonic attention mechanism decides to stop scanning the memory at output time step i).
- the model uses the expected value of c i based on MoChA's induced probability distribution (denoted as ⁇ i,j ). This can be computed as:
- This function can be computed efficiently, for example, by convolving x with a length-(f+b ⁇ 1) sequence of 1s and truncating appropriately.
- the parameter ⁇ i can be computed as:
- ⁇ i , : exp ⁇ ⁇ ( u i , : ) ⁇ ⁇ MovingSum ⁇ ⁇ ( ⁇ i , : MovingSum ⁇ ⁇ ( exp ⁇ ⁇ ( u i , : ) , w , 1 ) , 1 , w ) ( 19 )
- Equations (20) to (23) reflect the (unchanged) computation of the monotonic attention probability distribution
- equations (24) and (25) compute MoChA's probability distribution
- equation (26) computes the expected value of the context vector c i .
- a novel attention mechanism was developed, which allows computing soft attention over small chunks of the memory, whose locations are set adaptively. The mechanism has an efficient training-time algorithm and enjoys online and linear-time decoding at test time.
- the encoder processes the input sequence to produce a sequence of encoder outputs over which attention is performed.
- the attention module produces output for each time step, e.g., for each new item in the input sequence or each new output of the encoder.
- a hard attention determination is used by the attention module at each timestep, as the endpoint for the soft attention window.
- the hard attention value used may not change at each timestep however, depending on the results of the hard attention calculation.
- the model doesn't produce a hard attention value for each of the encoder states. Rather, the decoder starts from where it left off at the previous output timestep, and continues until it assigns an attention weight of 1 to some entry of the input.
- MoChA in its natural setting, i.e. a domain where roughly monotonic alignments are expected.
- the goal in this task is to produce the sequence of words spoken in a recorded speech utterance.
- RNN-based models can be unidirectional in order to satisfy the online requirement.
- the network ingests the spoken utterance as a mel-filterbank spectrogram, which is passed to an encoder consisting of convolution layers, convolutional LSTM layers, and unidirectional LSTM layers.
- the decoder is a single unidirectional LSTM, which attends to the encoder state sequence via either MoChA or a standard soft attention mechanism.
- the decoder produces a sequence of distributions over character and word-delimiter tokens. Performance is measured in terms of word error rate (WER) after segmenting characters output by the model into words based on the produced word delimiter tokens. None of the models reported integrated a separate language model.
- WER word error rate
- the goal of the task is to produce a sequence of “highlight” sentences from a news article.
- the “pointer-generator” network (without the coverage penalty) was chosen.
- Input words are converted to a learned embedding and passed into the model's encoder, consisting of a single bidirectional LSTM layer.
- the decoder is a unidirectional LSTM with an attention mechanism whose state is passed to a softmax layer which produces a sequence of distributions over the vocabulary.
- the model is augmented with a copy mechanism, which interpolates linearly between using the softmax output layer's word distribution, or a distribution of word IDs weighted by the attention distribution at a given output time step.
- the MoChA technique provides an attention mechanism which performs soft attention over adaptively-located chunks of the input sequence.
- MoChA allows for online and linear-time decoding, while also facilitating local input-output reorderings.
- the MoChA framework can be applied to additional problems with approximately monotonic alignments, such as speech synthesis and morphological inflection.
- the chunk size w may also vary adaptively in some implementations.
- Speech utterances can be represented as mel-scaled spectrograms with 80 coefficients, along with delta and delta-delta coefficients.
- Feature sequences were first fed into two convolutional layers, each with 3 ⁇ 3 filters and a 2 ⁇ 2 stride with 32 filters per layer. Each convolution was followed by batch normalization prior to a ReLU nonlinearity. The output of the convolutional layers was fed into a convolutional LSTM layer, using 1 ⁇ 3 filters. This was followed by an additional 3 ⁇ 3 convolutional layer with 32 filters and a stride of 1 ⁇ 1.
- the encoder had three additional unidirectional LSTM layers with a hidden state size of 256, each followed by a dense layer with a 256-dimensional output with batch normalization and a ReLU nonlinearity.
- the decoder was a single unidirectional LSTM layer with a hidden state size of 256. Its input consisted of a 64-dimensional learned embedding of the previously output symbol and the 256-dimensional context vector produced by the attention mechanism. The attention energy function had a hidden dimensionality d of 128. The softmax output layer took as input the concatenation of the attention context vector and the decoder's state.
- the initial learning rate 0.001 was dropped by a factor of 10 after 600,000, 800,000, and 1,000,000 steps. Inputs were fed into the network in batches of 8 utterances, using teacher forcing. Localized label smoothing was applied to the target outputs with weights [0.015, 0.035, 0.035, 0.015] for neighbors at [ ⁇ 2, ⁇ 1, 1, 2].
- FIG. 4 illustrates an example of a system 400 using monotonic chunkwise attention (MoChA) for speech recognition.
- an electronic device 410 stores and uses a speech recognition model 402 to generate a transcription for a voice input.
- the speech recognition model 402 includes an attention mechanism, represented by attender 406 , that implements the MoChA techniques discussed above.
- the electronic device 410 can be any appropriate computing device, for example, a mobile phone, a laptop computer, a desktop computer, a navigation device, a wearable device, a home automation device, an appliance, a smart speaker, a digital conversational assistant device, an entertainment device, etc.
- the electronic device 410 includes a microphone that detects an utterance from a user 411 and generates audio data 414 representing the utterance spoken by user 412 .
- a feature extraction module 416 processes the audio data 414 to extract (e.g., generate) a set of feature values that are indicative of acoustic characteristics of the utterance.
- the feature values may be mel-frequency cepstral coefficients.
- Sets of extracted feature values e.g., a sequence of feature vectors, are then provided as input to the speech recognition model 402 .
- Each set of feature values (e.g., feature vector) can represent acoustic properties of a different portion of the utterance.
- the speech recognition model 402 can be an end-to-end model, for example, a model that includes functions of an acoustic model, language model, and pronunciation model.
- the speech recognition model 402 may thus be configured to receive acoustic information, e.g., as waveform samples or extracted features, and provide output indicative of likelihoods of language units, e.g., phonetic units (e.g., phones, context dependent phones, etc.) or orthographic units (e.g., graphemes, word pieces that may include multiple graphemes, and/or whole words).
- phonetic units e.g., phones, context dependent phones, etc.
- orthographic units e.g., graphemes, word pieces that may include multiple graphemes, and/or whole words.
- the speech recognition model 402 can be implemented as one or more neural networks, which can be jointly trained. In some implementations, the one or more neural networks can be trained together as a single model or single neural network. In some implementations, the speech recognition model 402 includes an encoder 404 , an attender 406 , and a decoder 408 . The speech recognition model 402 can include a softmax layer, which may be integrated with the decoder 408 or may be a separate layer that receives output from the decoder 408 . Each of the encoder 404 , the attender 406 , and the decoder 408 , as well as a softmax layer, may be implemented using one or more neural network layers.
- the encoder 404 , the attender 406 , and/or the decoder 408 is implemented using one or more recurrent neural network layers, such as long short-term memory (LSTM) layers.
- LSTM long short-term memory
- the speech recognition model 402 can be implemented as a listen, attend, and spell (LAS) model or as another type of model.
- the extracted feature values are provided as inputs to the encoder 404 of the speech recognition model 402 .
- the encoder 404 generates an encoded feature representation as an output. This encoder output is often referred to as an encoding or encoder state, and can be represented by the symbol h enc or h u .
- the encoder 404 may generate an encoding for each input feature vector provided as input to the encoder 404 .
- the encoder module 404 can perform a function similar to an acoustic model, by receiving input features and mapping them to a higher-level feature representation, h enc . This process of generating an encoded feature representation, h enc , can be done for each of multiple input frames, representing different input time steps.
- the inputs can be provided over a set of input steps ⁇ 0, 1, 2, 3, . . . u ⁇ , each corresponding to an input feature vector ⁇ v 0 , v 1 , v 2 , v 3 , . . . v u ⁇ , which results in a corresponding set of encoding outputs ⁇ h 0 , h 1 , h 2 , h 3 , . . . h u ⁇ .
- the output of the encoder 404 is processed using the attender 406 to generate a context vector c i , as discussed above.
- the attender 406 can perform monotonic chunkwise attention as discussed above. In general, the attender 406 determines which features in the encodings from the encoder 404 should be attended to in order to predict the next output symbol.
- the output symbol, or an output distribution representing likelihoods of the output symbol can be represented by output y i , discussed above.
- the attender 406 can generate a context output c i for each of multiple output steps i, where each output step represents a different prediction of the speech recognition model 402 .
- each output step i can represent the prediction of a different output element of an utterance being recognized, where the output elements are graphemes (e.g., characters), wordpieces, and/or whole words.
- the attender 406 can compute attention based on the encodings for one or more input steps u, e.g., the encoding for the current input step as well as encodings for previous input steps.
- the attender 406 can generate an attention context output a over the set of all the encoder outputs of the utterance, e.g., the entire set ⁇ h 0 , h 1 , h 2 , h 3 , . . . h u ⁇ .
- the attention context vector c can be a vector representing a weighted summary of the current and previous encodings for frames (e.g., portions) of the utterance being recognized.
- the input time steps u and the output time steps i can be different, for example, so that there are multiple input time steps for each output time step, but this is not required.
- the decoder 408 receives the context vector c as an input and uses it to generate an output representation, such as a distribution indicating of likelihoods of word elements.
- the decoder 408 can also receive and process, along with the context vector c i , one or more other inputs, such as an output of the decoder 408 for the immediately previous output time step (e.g., time step i ⁇ 1) and/or an output of a softmax layer for the immediately previous output time step.
- the decoder 408 can process inputs including context vector c i as well as output y i-1 to generate decoder output used to create output y i .
- the output distribution y i is a function of the decoder state s i and context vector c i .
- the decoder state s i is a function of the previous decoder state, s i-1 , the previously emitted character, as indicated by the previous output distribution y i-1 , and the previous context vector c i-1 . More generally, the decoder state at any given output step can be based on a state or output of at least some portion of the model 402 for the immediately previous output step. This is illustrated conceptually by feedback arrow 409 , and represents that the decoder 408 may receive as input output of a subsequent softmax layer or even a predicted output label determined by the beam search module to assist in generating the next output.
- the decoder 408 can receive the attention context vector c output by the attender 406 , as well as an embedding for the previous prediction, y i-1 , and process these in order to produce a decoder output.
- the model 400 may include a softmax layer that receives output of the decoder 408 .
- the softmax layer is integrated with the decoder 408 , so that the output y i represents both the output of the softmax layer and the decoder 408 .
- the output y i represents the output of the softmax layer that is separate from the decoder 408 .
- the output y i can be a probability distribution, P(y i
- information indicating specific selections of output labels can be used.
- the scores in output y i indicate likelihoods for each element in a set of output labels representing different word elements.
- the decoder can provide a probability distribution that indicates posterior probabilities for each of a set of output labels.
- the decoder 408 and/or an associated softmax layer may trained to output a set of values indicative of the likelihood of occurrence of each of a predetermined set of output labels.
- This set of values can be a vector and can indicate a probability distribution over the set of output labels.
- the output labels are graphemes (e.g., individual characters, and potentially punctuation and other symbols), but the set of output labels is not so limited.
- the set of output labels can include wordpieces and/or entire words, in addition to or instead of graphemes.
- the output distribution of the decoder 408 and/or the softmax layer can include a posterior probability value for each of the different output labels.
- the output y i of the decoder or the output of a softmax layer that receives and processes the output y i can include 100 different probability values, one for each output label.
- the probability distribution can then be used to select and assign scores to candidate orthographic elements (e.g., graphemes, wordpieces, and/or words) in a beam search process for determining the transcription.
- candidate orthographic elements e.g., graphemes, wordpieces, and/or words
- the outputs y i , y i-1 , . . . y 0 may each represent a selection of a specific output label rather than a distribution over all possible output labels.
- Generating the transcription output for the utterance can include using beam search processing to generate one or more candidate transcriptions based on the output label scores representing probabilities of occurrence for different word elements.
- the system 400 also includes a beam search module 418 that performs beam search decoding to generate the candidate transcriptions from which a final transcription 420 is generated as an output of the ASR system 400 .
- the electronic device 410 can perform any of various actions. For example, the electronic device 410 can analyze the transcription 420 to detect a hotword or command in the utterance received from user 412 . In some implementations, the electronic device 410 determines whether one or more predetermined commands are present in the transcription 420 , and when the command is identified the electronic device performs an action corresponding to the identified command.
- the system 400 can identify and execute a particular command (e.g., activate a virtual assistant, play a song, set a timer, add an item to a list, and so on), change an operating mode of the electronic device 410 , send the transcription 420 as a request or query to a server, provide search results generated using the transcription 420 as a query, display the transcription 420 of the utterance, or enter the transcription 420 into a text area of a user interface (e.g., during a dictation mode).
- a particular command e.g., activate a virtual assistant, play a song, set a timer, add an item to a list, and so on
- change an operating mode of the electronic device 410 e.g., send the transcription 420 as a request or query to a server, provide search results generated using the transcription 420 as a query, display the transcription 420 of the utterance, or enter the transcription 420 into a text area of a user interface (e.g., during a
- the attender 406 at each time step i the attender 406 generates a context vector, c i , encapsulating the information in the acoustic signal needed to generate the next character.
- the attention model is content-based, so the contents of the decoder state s i are matched to the contents of h u representing time step u of encoding h, to generate an attention vector ⁇ i . Then vector ⁇ i is used to linearly blend vectors h u to create context vector c i .
- the attention mechanism can compute the scalar energy e i,u for each time step u, using vector h u ⁇ h and si.
- the scalar energy e i,u is converted into a probability distribution over time steps (or attention) a using a softmax function. This is used to create the context vector c i by linearly blending the listener features or encoder outputs, h u , at different time steps, for example, using the equations shown below.
- the MoCha attention technique can be used in a wide variety of other applications, such as keyword spotting, text-to-speech synthesis, image classification, object detection, document summarization, machine translation, and so on.
- Embodiments of the invention and all of the functional operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- Embodiments of the invention can be implemented as one or more computer program products, e.g., one or more modules of computer program instructions encoded on a computer readable medium for execution by, or to control the operation of, data processing apparatus.
- the computer readable medium can be a machine-readable storage device, a machine-readable storage substrate, a memory device, a composition of matter effecting a machine-readable propagated signal, or a combination of one or more of them.
- data processing apparatus encompasses all apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers.
- the apparatus can include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- a propagated signal is an artificially generated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus.
- a computer program (also known as a program, software, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, and it can be deployed in any form, including as a stand alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.
- a computer program does not necessarily correspond to a file in a file system.
- a program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code).
- a computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- the processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer.
- a processor will receive instructions and data from a read only memory or a random access memory or both.
- the essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- a computer need not have such devices.
- a computer can be embedded in another device, e.g., a tablet computer, a mobile telephone, a personal digital assistant (PDA), a mobile audio player, a Global Positioning System (GPS) receiver, to name just a few.
- Computer readable media suitable for storing computer program instructions and data include all forms of non volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks.
- the processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- embodiments of the invention can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.
- Embodiments of the invention can be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the invention, or any combination of one or more such back end, middleware, or front end components.
- the components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (“LAN”) and a wide area network (“WAN”), e.g., the Internet.
- LAN local area network
- WAN wide area network
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network.
- the relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- HTML file In each instance where an HTML file is mentioned, other file types or formats may be substituted. For instance, an HTML file may be replaced by an XML, JSON, plain text, or other types of files. Moreover, where a table or hash table is mentioned, other data structures (such as spreadsheets, relational databases, or structured files) may be used.
Abstract
Description
h j=EncoderRNN(x j ,h j−1) (1)
s i=DecoderRNN(y i-1 ,s i-1 ,c i) (2)
y i=Output(s i ,c i) (3)
e i,j=Energy(h j ,s i-1)
Energy(h j ·s i-1):=v T tanh(W h h j +W s s i-1 +b)
e i,j=MonotonicEnergy(s i-1 ,h j)
p i,j=σ(e i,j)
z i,j˜Bernoulli(p i,j)
Algorithm 1 MoChA decoding process (test time). During training, lines 4-19 are replaced with |
eqs. (20) to (26) and yi−1 is replaced with the ground-truth output at timestep i − 1. |
1: | Input: memory h of length T, chunk size w |
2: | State: s0 = {right arrow over (0)}, t0 = 1, i = 1, y0 = StartOfSequence |
3: | while yi−1 ≠ EndOfSequence do // Produce output tokens until end-of-sequence token is produced |
4: | for j = ti−1 to T do // Start inspecting memory entries hj left-to-right from where we left off |
5: | ei,j = MonotonicEnergy(si−1, hj) // Compute attention energy for hj |
6: | pi,j = σ(ei,j) // Compute probability of choosing hj |
7: | if pi,j ≥ 0.5 then // If pi,j is larger than 0.5, we stop scanning the memory |
8: | v = j − w + 1 // Set chunk start location |
9: | for k = v to j do // Compute chunkwise softmax energies over a size-w chunk before j |
10: | ui,k = ChunkEnergy(si−1, hk) |
11: | end for |
12: |
|
13: | ti = j // Remember where we left off for the next output timestep |
14: | break // Stop scanning the memory |
15: | end if |
16: | end for |
17: | if pi,j < 0.5, ∀j ∈ {ti−1, ti−1 + 1, . . . , T} then |
18: | ci = {right arrow over (0)} // If we scanned the entire memory without stopping, set ci to a vector of zeros |
19: | end if |
20: | si = DecoderRNN(si−1, yi−1, ci) // Update output RNN state based on the new context vector |
21: | yi = Output(si, ci) // Output a new symbol using the softmax output layer |
22: | i = i + 1 |
23: | end while |
v=t i −w+1 (14)
u i,k=ChunkEnergy(s i-1 ,h k),k∈{v,v+1, . . . ,t i} (15)
TABLE 1 |
Word error rate on the Wall Street Journal test set. |
The results reflect the statistics of 8 trials. |
Attention Mechanism | Best WER | Average WER | |
Soft Attention (offline) | 14.2% | 14.6 ± 0.3% | |
MoChA, w = 2 | 13.9% | 15.0 ± 0.6% | |
TABLE 2 |
ROUGE F-scores for document summarization |
on the CNN/Daily Mail dataset. |
Mechanism | R-1 | R-2 | |
Soft Attention (offline) | 39.11 | 15.76 | |
Hard Monotonic Attention | 31.14 | 11.16 | |
MoChA, w = 8 | 35.46 | 13.55 | |
Claims (20)
Priority Applications (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US16/518,518 US11210475B2 (en) | 2018-07-23 | 2019-07-22 | Enhanced attention mechanisms |
US17/456,958 US20220083743A1 (en) | 2018-07-23 | 2021-11-30 | Enhanced attention mechanisms |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201862702049P | 2018-07-23 | 2018-07-23 | |
US16/518,518 US11210475B2 (en) | 2018-07-23 | 2019-07-22 | Enhanced attention mechanisms |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US17/456,958 Continuation US20220083743A1 (en) | 2018-07-23 | 2021-11-30 | Enhanced attention mechanisms |
Publications (2)
Publication Number | Publication Date |
---|---|
US20200026760A1 US20200026760A1 (en) | 2020-01-23 |
US11210475B2 true US11210475B2 (en) | 2021-12-28 |
Family
ID=69163016
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/518,518 Active 2040-05-04 US11210475B2 (en) | 2018-07-23 | 2019-07-22 | Enhanced attention mechanisms |
US17/456,958 Pending US20220083743A1 (en) | 2018-07-23 | 2021-11-30 | Enhanced attention mechanisms |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US17/456,958 Pending US20220083743A1 (en) | 2018-07-23 | 2021-11-30 | Enhanced attention mechanisms |
Country Status (1)
Country | Link |
---|---|
US (2) | US11210475B2 (en) |
Families Citing this family (18)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11210475B2 (en) * | 2018-07-23 | 2021-12-28 | Google Llc | Enhanced attention mechanisms |
US10380997B1 (en) * | 2018-07-27 | 2019-08-13 | Deepgram, Inc. | Deep learning internal state index-based search and classification |
US10997277B1 (en) * | 2019-03-26 | 2021-05-04 | Amazon Technologies, Inc. | Multinomial distribution on an integrated circuit |
US10930301B1 (en) * | 2019-08-27 | 2021-02-23 | Nec Corporation | Sequence models for audio scene recognition |
US11790227B1 (en) * | 2020-01-16 | 2023-10-17 | Educational Testing Service | Systems and methods for neural content scoring |
CN111091839B (en) * | 2020-03-20 | 2020-06-26 | 深圳市友杰智新科技有限公司 | Voice awakening method and device, storage medium and intelligent device |
US11526678B2 (en) * | 2020-05-14 | 2022-12-13 | Naver Corporation | Attention over common-sense network for natural language inference |
CN111626293A (en) * | 2020-05-21 | 2020-09-04 | 咪咕文化科技有限公司 | Image text recognition method and device, electronic equipment and storage medium |
CN111625634B (en) * | 2020-05-25 | 2023-08-22 | 泰康保险集团股份有限公司 | Word slot recognition method and device, computer readable storage medium and electronic equipment |
CN111857728B (en) * | 2020-07-22 | 2021-08-31 | 中山大学 | Code abstract generation method and device |
CN112133304B (en) * | 2020-09-18 | 2022-05-06 | 中科极限元(杭州)智能科技股份有限公司 | Low-delay speech recognition model based on feedforward neural network and training method |
CN112257911B (en) * | 2020-10-13 | 2024-03-26 | 杭州电子科技大学 | TCN multivariate time sequence prediction method based on parallel space-time attention mechanism |
GB2600987B (en) * | 2020-11-16 | 2024-04-03 | Toshiba Kk | Speech Recognition Systems and Methods |
US20220284891A1 (en) * | 2021-03-03 | 2022-09-08 | Google Llc | Noisy student teacher training for robust keyword spotting |
CN112668338B (en) | 2021-03-22 | 2021-06-08 | 中国人民解放军国防科技大学 | Clarification problem generation method and device and electronic equipment |
CN113570135B (en) * | 2021-07-27 | 2023-08-01 | 天津大学 | Parallel hybrid network-based grotto temple rock mass crack development control method and device |
KR102494422B1 (en) * | 2022-06-24 | 2023-02-06 | 주식회사 액션파워 | Method for detecting spoken voice in audio data including ars voice |
CN116186702B (en) * | 2023-02-24 | 2024-02-13 | 中国科学院信息工程研究所 | Malicious software classification method and device based on cooperative attention |
Citations (19)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5588091A (en) | 1989-05-17 | 1996-12-24 | Environmental Research Institute Of Michigan | Dynamically stable associative learning neural network system |
US6601052B1 (en) | 1999-06-16 | 2003-07-29 | Korea Advanced Institute Of Science And Technology | Selective attention method using neural network |
US9705904B1 (en) | 2016-07-21 | 2017-07-11 | Cylance Inc. | Neural attention mechanisms for malware analysis |
US20180060301A1 (en) * | 2016-08-31 | 2018-03-01 | Microsoft Technology Licensing, Llc | End-to-end learning of dialogue agents for information access |
US20180082171A1 (en) * | 2016-09-22 | 2018-03-22 | Salesforce.Com, Inc. | Pointer sentinel mixture architecture |
US20180144208A1 (en) | 2016-11-18 | 2018-05-24 | Salesforce.Com, Inc. | Adaptive attention model for image captioning |
US20180330718A1 (en) * | 2017-05-11 | 2018-11-15 | Mitsubishi Electric Research Laboratories, Inc. | System and Method for End-to-End speech recognition |
US20180329884A1 (en) * | 2017-05-12 | 2018-11-15 | Rsvp Technologies Inc. | Neural contextual conversation learning |
US20180336884A1 (en) * | 2017-05-19 | 2018-11-22 | Baidu Usa Llc | Cold fusing sequence-to-sequence models with language models |
US20180336880A1 (en) | 2017-05-19 | 2018-11-22 | Baidu Usa Llc | Systems and methods for multi-speaker neural text-to-speech |
US10169656B2 (en) | 2016-08-29 | 2019-01-01 | Nec Corporation | Video system using dual stage attention based recurrent neural network for future event prediction |
US20190073353A1 (en) * | 2017-09-07 | 2019-03-07 | Baidu Usa Llc | Deep compositional frameworks for human-like language acquisition in virtual environments |
US20190122651A1 (en) | 2017-10-19 | 2019-04-25 | Baidu Usa Llc | Systems and methods for neural text-to-speech using convolutional sequence learning |
US20190189115A1 (en) | 2017-12-15 | 2019-06-20 | Mitsubishi Electric Research Laboratories, Inc. | Method and Apparatus for Open-Vocabulary End-to-End Speech Recognition |
US20190205761A1 (en) | 2017-12-28 | 2019-07-04 | Adeptmind Inc. | System and method for dynamic online search result generation |
US10346524B1 (en) * | 2018-03-29 | 2019-07-09 | Sap Se | Position-dependent word salience estimation |
US20190332919A1 (en) * | 2017-02-24 | 2019-10-31 | Google Llc | Sequence processing using online attention |
US20190354808A1 (en) * | 2018-05-18 | 2019-11-21 | Google Llc | Augmentation of Audiographic Images for Improved Machine Learning |
US20200026760A1 (en) * | 2018-07-23 | 2020-01-23 | Google Llc | Enhanced attention mechanisms |
Family Cites Families (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7054850B2 (en) * | 2000-06-16 | 2006-05-30 | Canon Kabushiki Kaisha | Apparatus and method for detecting or recognizing pattern by employing a plurality of feature detecting elements |
CN107845385B (en) * | 2016-09-19 | 2021-07-13 | 南宁富桂精密工业有限公司 | Coding and decoding method and system for information hiding |
US10811000B2 (en) * | 2018-04-13 | 2020-10-20 | Mitsubishi Electric Research Laboratories, Inc. | Methods and systems for recognizing simultaneous speech by multiple speakers |
US11657799B2 (en) * | 2020-04-03 | 2023-05-23 | Microsoft Technology Licensing, Llc | Pre-training with alignments for recurrent neural network transducer based end-to-end speech recognition |
-
2019
- 2019-07-22 US US16/518,518 patent/US11210475B2/en active Active
-
2021
- 2021-11-30 US US17/456,958 patent/US20220083743A1/en active Pending
Patent Citations (22)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5588091A (en) | 1989-05-17 | 1996-12-24 | Environmental Research Institute Of Michigan | Dynamically stable associative learning neural network system |
US6601052B1 (en) | 1999-06-16 | 2003-07-29 | Korea Advanced Institute Of Science And Technology | Selective attention method using neural network |
US9705904B1 (en) | 2016-07-21 | 2017-07-11 | Cylance Inc. | Neural attention mechanisms for malware analysis |
US10169656B2 (en) | 2016-08-29 | 2019-01-01 | Nec Corporation | Video system using dual stage attention based recurrent neural network for future event prediction |
US20180060301A1 (en) * | 2016-08-31 | 2018-03-01 | Microsoft Technology Licensing, Llc | End-to-end learning of dialogue agents for information access |
US10546066B2 (en) * | 2016-08-31 | 2020-01-28 | Microsoft Technology Licensing, Llc | End-to-end learning of dialogue agents for information access |
US20180082171A1 (en) * | 2016-09-22 | 2018-03-22 | Salesforce.Com, Inc. | Pointer sentinel mixture architecture |
US20180144208A1 (en) | 2016-11-18 | 2018-05-24 | Salesforce.Com, Inc. | Adaptive attention model for image captioning |
US20190332919A1 (en) * | 2017-02-24 | 2019-10-31 | Google Llc | Sequence processing using online attention |
US20180330718A1 (en) * | 2017-05-11 | 2018-11-15 | Mitsubishi Electric Research Laboratories, Inc. | System and Method for End-to-End speech recognition |
US20180329884A1 (en) * | 2017-05-12 | 2018-11-15 | Rsvp Technologies Inc. | Neural contextual conversation learning |
US20180336880A1 (en) | 2017-05-19 | 2018-11-22 | Baidu Usa Llc | Systems and methods for multi-speaker neural text-to-speech |
US20180336884A1 (en) * | 2017-05-19 | 2018-11-22 | Baidu Usa Llc | Cold fusing sequence-to-sequence models with language models |
US10867595B2 (en) * | 2017-05-19 | 2020-12-15 | Baidu Usa Llc | Cold fusing sequence-to-sequence models with language models |
US20190073353A1 (en) * | 2017-09-07 | 2019-03-07 | Baidu Usa Llc | Deep compositional frameworks for human-like language acquisition in virtual environments |
US10366166B2 (en) * | 2017-09-07 | 2019-07-30 | Baidu Usa Llc | Deep compositional frameworks for human-like language acquisition in virtual environments |
US20190122651A1 (en) | 2017-10-19 | 2019-04-25 | Baidu Usa Llc | Systems and methods for neural text-to-speech using convolutional sequence learning |
US20190189115A1 (en) | 2017-12-15 | 2019-06-20 | Mitsubishi Electric Research Laboratories, Inc. | Method and Apparatus for Open-Vocabulary End-to-End Speech Recognition |
US20190205761A1 (en) | 2017-12-28 | 2019-07-04 | Adeptmind Inc. | System and method for dynamic online search result generation |
US10346524B1 (en) * | 2018-03-29 | 2019-07-09 | Sap Se | Position-dependent word salience estimation |
US20190354808A1 (en) * | 2018-05-18 | 2019-11-21 | Google Llc | Augmentation of Audiographic Images for Improved Machine Learning |
US20200026760A1 (en) * | 2018-07-23 | 2020-01-23 | Google Llc | Enhanced attention mechanisms |
Non-Patent Citations (37)
Also Published As
Publication number | Publication date |
---|---|
US20200026760A1 (en) | 2020-01-23 |
US20220083743A1 (en) | 2022-03-17 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11210475B2 (en) | Enhanced attention mechanisms | |
US11900915B2 (en) | Multi-dialect and multilingual speech recognition | |
US11676579B2 (en) | Deep learning internal state index-based search and classification | |
US20230410796A1 (en) | Encoder-decoder models for sequence to sequence mapping | |
US11922932B2 (en) | Minimum word error rate training for attention-based sequence-to-sequence models | |
US9786270B2 (en) | Generating acoustic models | |
US11961511B2 (en) | System and method for disambiguation and error resolution in call transcripts | |
US11205444B2 (en) | Utilizing bi-directional recurrent encoders with multi-hop attention for speech emotion recognition | |
US20180174576A1 (en) | Acoustic-to-word neural network speech recognizer | |
CN113168828A (en) | Session proxy pipeline trained based on synthetic data | |
US20220238116A1 (en) | A Method Of Sequence To Sequence Data Processing And A System For Sequence To Sequence Data Processing | |
Hinsvark et al. | Accented speech recognition: A survey | |
CN114023300A (en) | Chinese speech synthesis method based on diffusion probability model | |
Baas et al. | Transfusion: Transcribing speech with multinomial diffusion | |
CN112289299B (en) | Training method and device of speech synthesis model, storage medium and electronic equipment | |
Sharan et al. | ASR for Speech based Search in Hindi using Attention based Model |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
FEPP | Fee payment procedure |
Free format text: ENTITY STATUS SET TO UNDISCOUNTED (ORIGINAL EVENT CODE: BIG.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:CHIU, CHUNG-CHENG;RAFFEL, COLIN ABRAHAM;SIGNING DATES FROM 20180917 TO 20180925;REEL/FRAME:049825/0603 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: AWAITING TC RESP., ISSUE FEE NOT PAID |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: AWAITING TC RESP., ISSUE FEE NOT PAID |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: AWAITING TC RESP, ISSUE FEE PAYMENT VERIFIED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: PUBLICATIONS -- ISSUE FEE PAYMENT VERIFIED |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |