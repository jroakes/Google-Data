US7319994B1 - Document compression scheme that supports searching and partial decompression - Google Patents
Document compression scheme that supports searching and partial decompression Download PDFInfo
- Publication number
- US7319994B1 US7319994B1 US10/444,761 US44476103A US7319994B1 US 7319994 B1 US7319994 B1 US 7319994B1 US 44476103 A US44476103 A US 44476103A US 7319994 B1 US7319994 B1 US 7319994B1
- Authority
- US
- United States
- Prior art keywords
- term
- terms
- documents
- occurrence
- occurrences
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/33—Querying
- G06F16/3331—Query processing
- G06F16/334—Query execution
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/31—Indexing; Data structures therefor; Storage structures
- G06F16/316—Indexing structures
-
- Y—GENERAL TAGGING OF NEW TECHNOLOGICAL DEVELOPMENTS; GENERAL TAGGING OF CROSS-SECTIONAL TECHNOLOGIES SPANNING OVER SEVERAL SECTIONS OF THE IPC; TECHNICAL SUBJECTS COVERED BY FORMER USPC CROSS-REFERENCE ART COLLECTIONS [XRACs] AND DIGESTS
- Y10—TECHNICAL SUBJECTS COVERED BY FORMER USPC
- Y10S—TECHNICAL SUBJECTS COVERED BY FORMER USPC CROSS-REFERENCE ART COLLECTIONS [XRACs] AND DIGESTS
- Y10S707/00—Data processing: database and file management or data structures
- Y10S707/99931—Database or file accessing
- Y10S707/99932—Access augmentation or optimizing
-
- Y—GENERAL TAGGING OF NEW TECHNOLOGICAL DEVELOPMENTS; GENERAL TAGGING OF CROSS-SECTIONAL TECHNOLOGIES SPANNING OVER SEVERAL SECTIONS OF THE IPC; TECHNICAL SUBJECTS COVERED BY FORMER USPC CROSS-REFERENCE ART COLLECTIONS [XRACs] AND DIGESTS
- Y10—TECHNICAL SUBJECTS COVERED BY FORMER USPC
- Y10S—TECHNICAL SUBJECTS COVERED BY FORMER USPC CROSS-REFERENCE ART COLLECTIONS [XRACs] AND DIGESTS
- Y10S707/00—Data processing: database and file management or data structures
- Y10S707/99931—Database or file accessing
- Y10S707/99933—Query processing, i.e. searching
-
- Y—GENERAL TAGGING OF NEW TECHNOLOGICAL DEVELOPMENTS; GENERAL TAGGING OF CROSS-SECTIONAL TECHNOLOGIES SPANNING OVER SEVERAL SECTIONS OF THE IPC; TECHNICAL SUBJECTS COVERED BY FORMER USPC CROSS-REFERENCE ART COLLECTIONS [XRACs] AND DIGESTS
- Y10—TECHNICAL SUBJECTS COVERED BY FORMER USPC
- Y10S—TECHNICAL SUBJECTS COVERED BY FORMER USPC CROSS-REFERENCE ART COLLECTIONS [XRACs] AND DIGESTS
- Y10S707/00—Data processing: database and file management or data structures
- Y10S707/99941—Database schema or data structure
- Y10S707/99942—Manipulating data structure, e.g. compression, compaction, compilation
Definitions
- the present invention relates to techniques for storing documents in computer systems. More specifically, the present invention relates to a document compression scheme that supports both searching and efficient decompression of portions of documents.
- Search engines typically operate by identifying web pages containing occurrences of specific terms (i.e. words) within these documents. For example, a search engine might search for all web pages containing the terms “military” and “industrial”. A search engine can also search for web pages containing a specific phrase, such as “flash in the pan”.
- An inverted index is a lookup structure that specifies where a given term occurs in the set of documents. For example, an entry for a given term in the inverted index may contain identifiers for documents in which the term occurs, as well as offsets of the occurrences within the documents. This allows documents containing the given term to be rapidly identified.
- a search engine 112 generally operates by receiving a query 113 from a user 111 through a web browser 114 .
- This query 113 specifies a number of terms to be searched for in the set of documents.
- search engine 112 uses inverted index 110 to identify documents that satisfy the query.
- Search engine 112 then returns a response 115 through web browser 114 , wherein the response 115 contains references to the identified documents.
- Documents can also be stored in compressed form in a separate compressed repository 106 . This allows documents or portions of documents (snippets) to be easily retrieved by search engine 112 and to be displayed to user 111 through web browser 114 .
- web crawler 104 continually retrieves new documents from web 102 . These new documents feed through a compressor 105 , which compresses the new documents before they are stored in compressed repository 106 . The new documents also feed through indexer 108 , which adds terms from the new documents into inverted index 110 .
- the inverted index 110 illustrated in FIG. 1 can be used to efficiently identify specific terms in documents. However, because the inverted index 110 loses the ordering of the terms, searches that match multi-word portions of the document (as in a phrase search) would require position information (offsets) for the individual terms to be retrieved and aligned in order to match the proper ordering required by the query. This process can be time consuming.
- One embodiment of the present invention provides a system that facilitates accessing a compressed representation of a set of documents, wherein the compressed representation supports searching and partial decompression.
- the system receives a search request containing terms to be searched for in the set of documents.
- the system identifies occurrences of the terms in the set of documents by following pointers through the compressed representation.
- This compressed representation encodes occurrences of a term as a pointer to the next occurrence of the term to facilitate rapid enumeration of the occurrences of the term.
- the compressed representation maintains sequential ordering between adjacent terms in the set of documents, which allows fast access to neighboring terms.
- the terms in the search request are received in textual form.
- the system converts the terms from textual form into corresponding term identifiers.
- identifying occurrences of a term involves using a corresponding term identifier to look up a pointer to a first occurrence of the term in the compressed representation. It also involves following a chain of pointers starting at the first occurrence to identify other occurrences of the term in the compressed representation.
- every K th occurrence of a term in the compressed representation includes a term identifier, which allows the term identifier to be obtained by following at most K (on average K/ 2 ) pointers given any occurrence of the term in the compressed representation.
- a corresponding term identifier is included on average every Kth occurrence of the term in the compressed representation.
- a corresponding term identifier is included with an occurrence of a term based on the position of the occurrence in the compressed representation.
- the system looks up occurrences of a least frequent term in the phrase, and then detects phrase matches by decoding neighboring terms for each occurrence of the least frequent term.
- the system generates the compressed representation by encoding at least some occurrences of a term as a byte difference to the next occurrence of the term. In doing so, the system maintains sequential ordering of terms in the set of documents.
- the system also includes a corresponding term identifier with every K th occurrence of the term, whereby the parameter K can be varied to trade off space with decoding performance.
- the system additionally constructs a term offset table containing an offset of a first occurrence of each term in the compressed representation.
- the system decompresses a region surrounding an occurrence of a term, by scanning through neighboring terms in the compressed representation, and following pointers between occurrences of the neighboring terms to locate term identifiers for the neighboring terms. If the textual form is desired for the neighboring terms, the system uses the term identifiers to look up a textual form for each of the neighboring terms in a term dictionary.
- the neighboring terms can also be used for document scoring purposes (i.e. for assigning a relevance score given a query).
- the system decompresses a document in the set of documents, by scanning through occurrences of terms in the compressed representation of the document, and following pointers from the occurrences of the terms to locate term identifiers for the terms.
- the system uses the term identifiers to look up a textual form for each of the terms in the document in a term dictionary.
- the system uses a document offset table to identify documents associated with occurrences of terms.
- a pointer between occurrences of a term is encoded as a byte difference between the occurrences.
- the system speeds up the sequential processing of documents and terms in the compressed representation by keeping track of the next occurrence of each term, thus avoiding the decoding cost (i.e. K/ 2 hops on the average case) paid for random access. Note that it is possible to keep track of the next occurrence of only a limited number of the most popular terms in order to get most of the speed up while using significantly less storage (for the next occurrences).
- the present invention introduces a technique for compressing the documents, in which the compressed representation is also an inverted index, and in which the documents can be efficiently and partially decompressed.
- An immediate consequence of this fact is a savings in space over existing systems that maintain a separate inverted index in addition to a compressed representation of full document contents.
- a secondary consequence is that the performance characteristics are quite different from existing systems that use a separate inverted index.
- the present invention allows fast access to neighboring terms (terms before and/or after the term). This speed is due to the “partial” decompressibility of the representation.
- the inverted-index feature of the data structure can be used to locate the query terms, and the nearby (potentially non-query) terms can be decoded to provide additional information to be used in scoring.
- the present invention allows neighboring terms to be readily available to a document-scoring algorithm for a query.
- FIG. 1 illustrates a document searching system
- FIG. 2 illustrates a document searching system in accordance with an embodiment of the present invention.
- FIG. 3 illustrates how an interlaced repository for documents is structured in accordance with an embodiment of the present invention.
- FIG. 4 illustrates a number of data structures used in the document searching process in accordance with an embodiment of the present invention.
- FIG. 5 presents a flow chart illustrating how data structures involved in the document searching process are constructed in accordance with an embodiment of the present invention.
- FIG. 6 presents a flow chart illustrating how a query is processed in accordance with an embodiment of the present invention.
- FIG. 7 presents a flow chart illustrating how a query involving a phrase match is processed in accordance with an embodiment of the present invention.
- FIG. 8 illustrates how a document is decompressed in accordance with an embodiment of the present invention.
- FIG. 9 illustrates how an inverted index can be combined with repositories in accordance with an embodiment of the present invention.
- a computer readable storage medium which may be any device or medium that can store code and/or data for use by a computer system.
- the transmission medium may include a communications network, such as the Internet.
- FIG. 2 illustrates a document searching system in accordance with an embodiment of the present invention.
- the system illustrated in FIG. 2 stores documents in a single “interlaced” repository 208 .
- This interlaced repository 208 functions as an inverted index, which allows search engine 210 to efficiently locate occurrences of specific terms in interlaced repository 208 .
- interlaced repository 208 maintains ordering between adjacent terms in each document. This allows documents or portions of documents to be easily decoded, which facilitates rapid phrase searches. Note that decoding a term simply requires following at most K pointers to locate the term identifier (which is encoded on every K th occurrence).
- the system illustrated in FIG. 2 includes a web crawler 104 , which continually retrieves new documents from web 102 . These new documents feed through compressor/indexer 205 to be added to interlaced repository 208 .
- a “term” can include any contiguous string of characters or punctuation marks within a document.
- a term can include a word, an HTML tag or a punctuation mark.
- a term can also be referred to as a “token.”
- FIG. 3 illustrates how an interlaced repository for documents is structured in accordance with an embodiment of the present invention.
- Interlaced repository 208 contains a collection of documents 301 - 304 (which are represented as rectangles in FIG. 3 ). Each term in a document is represented as a pointer to the next occurrence of the term. For example, in FIG. 3 , a first occurrence of the term “I” in document 301 points to a second occurrence in document 301 , which points to an occurrence in document 302 , which points to an occurrence in a following document. Similarly, the occurrence of the term “the” in document 301 points to an occurrence in document 302 , which points to an occurrence in document 303 , which points to an occurrence in a following document. Finally, the occurrence of the term “need” in document 301 points to an occurrence in document 302 , which points to an occurrence in document 304 , which points to an occurrence in a following document.
- FIG. 4 illustrates data structures involved in the document searching process in accordance with an embodiment of the present invention. These data structures include term identifier table 402 , dictionary 404 , term offset table 406 , document offset table 408 , interlaced repository 410 and attribute table 412 .
- Term identifier table 402 can generally include any type of lookup structure that can be used to map the textual form of a term to a corresponding term identifier.
- term identifier table 402 is implemented as a hash table.
- Dictionary 404 performs the inverse mapping. It maps term identifiers to corresponding term text.
- dictionary 404 is implemented as an array of pointers, wherein each pointer references a string containing the corresponding term text.
- the term identifier is simply the array location of the corresponding pointer for the term. In this way, the term identifier can be used to lookup the pointer for the term, and the pointer can be used to obtain the term text.
- the array entries may be sorted by frequency of the corresponding terms. In this way, more frequently occurring terms have smaller term identifiers. This allows the relative frequencies of terms to be easily obtained by comparing term identifiers. This can also save significant space since term identifiers are encoded (on every Kth occurrence) as part of the interlaced repository and more frequent terms would have lower term identifiers and thus take less space to encode.
- Interlaced repository 410 contains a set of documents in compressed form. Each term in interlaced repository 410 is encoded as a pointer to the next occurrence of the same term, which can be located in the same document or a following document. In one embodiment of the present invention, these pointers are encoded as byte differences between consecutive occurrences of the term. Note that more frequently occurring terms generally have smaller byte differences, which allows them to be encoded with fewer bytes in a variable-length code.
- Term offset table 406 maps term identifiers to the offset of the first occurrence of each term in interlaced repository 410 . This allows a first occurrence of a term to be easily identified/located.
- document offset table 408 maps document identifiers to the first byte/term of a corresponding document in interlaced repository 410 .
- attribute table 412 maps offsets in interlaced repository 410 to corresponding document attributes, such as font size, underlining, or any other document attribute. This allows more per-token information to be available for scoring a document given a query.
- interlaced repository 410 preserves the ordering of terms within documents, and because consecutive terms tend to have the same attributes (e.g. font size), we can alternatively compress the attribute information within interlaced repository 410 using run-length encoding. In this case, determining the attributes for a given term simply involves performing a binary search on the decompressed form of this structure.
- FIG. 5 presents a flow chart illustrating how data structures involved in the document searching process are constructed in accordance with an embodiment of the present invention.
- the system starts by constructing a dictionary of terms for the set of documents, such that each term is mapped to a dense range (e.g. 0 to N where N is the number of unique terms) (step 502 ). It is preferable but not necessary for this dictionary to be sorted by frequency so that more frequent terms are given a smaller value in the dense range (e.g. the word “the” may have a value of 0). The value in this dense range is called the “term identifier.”
- the system scans over a subset of the set of documents to construct a base dictionary roughly a few million terms in size.
- the base dictionary contains a 32-bit integer term identifier as well as the number of occurrences for the term.
- the final base lexicon is sorted by the occurrence counts so that more frequent terms have lower term identifiers.
- the N th most popular term gets a term identifier of N. This frequency ordering helps save space, especially if the K parameter is low. This ordering is also intuitive and may be helpful at scoring time.
- the base dictionary is used for encoding other portions of the set of documents. Terms that are not found in the base lexicon are added to a “secondary lexicon,” wherein term identifiers in the secondary lexicon are offset by the last termid in the base lexicon. This is reasonable because we expect these exceptional terms to be very rare, which justifies the high term identifiers.
- the system starts encoding backwards from the end of the set of documents, and encodes each document term as a byte difference to the next occurrence of the term in the compressed representation, which may be in the same document or in a different document (step 504 ).
- a special value of 0 may be used to indicate this special case, since 0 cannot be a valid delta otherwise.
- the encoding used for the individual term occurrences is a byte-aligned encoding scheme. It is conceivable to use other bit-aligned encoding schemes that would use bit deltas instead, but we chose byte-alignment for both decoding efficiency and conceptual simplicity. Also note that if the encoding scheme is decodable backwards, it makes it possible to explore the neighborhood surrounding a term occurrence by moving backward as well as forward.
- every K occurrences of a term we encode the term itself as determined from the dictionary. Note that a frequency-sorted dictionary would give better average-case space complexity for encoding the term identifiers, though such frequency-sorted dictionary need not be used.
- K K
- a few rarely-occurring terms may be searched for much more than others, in which case it may make sense to tilt the tradeoff towards decoding speed for just those terms. Because these terms are already rare, decreasing K just for these terms would not hurt overall space usage very much, but can potentially speed up the system for the average user.
- the system constructs a term offset table 406 which points to the first occurrence of each term (step 506 ).
- FIG. 6 presents a flow chart illustrating how a query is processed in accordance with an embodiment of the present invention.
- the system uses term identifier table 402 to generate term identifiers for the terms in the query (step 604 ).
- the system uses the term identifiers to lookup offsets for the first occurrence of each term in term offset table 406 (step 606 ).
- the system follows pointers between each occurrence of the terms of interest (step 608 ).
- the system can use document offset table 408 to determine which document is associated with each occurrence. Given the offset of an occurrence of a term, we can determine the corresponding document identifier by performing a moving binary search in document offset table 408 . Note that for popular terms, skip tables can be generated in order to avoid hopping through all occurrences of the token in order to reach a particular occurrences in the repository.
- an OR search involves identifying documents that contain any of the terms in the query
- an AND search involves identifying documents that contain all of the terms in the query.
- An alternative approach for an AND search which may become feasible if K is very small (e.g. 2), is to actually search for the other more frequent terms within the documents that contain the less frequent terms. The cost of this operation may be acceptable for very low K, and of course depending on how popular the individual terms are.
- FIG. 7 presents a flow chart illustrating how a query involving a phrase match is processed in accordance with an embodiment of the present invention.
- the system starts by receiving a request to search for a phrase containing a number of terms (step 702 ).
- the system then converts the terms in the phrase into term identifiers (step 704 ).
- the system looks up occurrences of the least frequent term in the phrase (step 706 ).
- the system detects phrase matches by decoding neighboring terms (step 708 ). Note that because interlaced repository 208 maintains ordering of terms, it is possible to quickly identify neighboring terms (by following their pointers up to K times to locate the term identifiers) to determine if there exists a phrase match.
- FIG. 8 illustrates how a document (or a portion of a document) is decompressed in accordance with an embodiment of the present invention.
- the system starts by scanning through occurrences of terms in the compressed representation of the document in interlaced repository 208 (step 802 ). Note that a pointer to the document can be obtained by performing a lookup in document offset table 408 . Next, the system follows pointers from the occurrences of terms in the compressed representation to locate corresponding term identifiers for the terms (step 804 ). Finally, the system uses the term identifiers to look up the textual form of each term (step 806 ). The system can additionally access attribute table 412 to determine attributes for the terms.
- each document can be “decompressed” partially (term by term) and efficiently (with K/ 2 memory lookups on average).
- this compression scheme is “continuously adaptive” since the difference between the occurrences would be smaller (thus more compact to encode) for more frequent terms.
- the present invention allows documents to be decoded efficiently and partially (term by term). Note that because decoding a term involves hops through the data structure, a storage medium that is fast for random access, such as RAM, is efficient for decompression.
- the parameter K provides an easily adjustable trade-off between space and decoding time. A lower K gives better decompression performance while a higher K requires less space since the term identifier is encoded less often.
- the above-described compressed representation may allow much better compression of term attributes such as font type and size because per-term attributes tend to be the same for consecutive terms.
- the compressed representation can be extended easily by adding new documents at the end. This only requires a modification on the previous global last occurrence of each term, and perhaps an expansion of the dictionary if some of the terms are not found in the existing dictionary. To get to the last global occurrence, we can follow the encoded deltas, or we can maintain a separate “last occurrence table” similar to the above-mentioned term offset table 406 , which points to first occurrences.
- This data structure can be used for not only searching (as described below), but also for the sequential processing of the documents for other purposes that can work with a pre-parsed representation of the documents. Note that the K/ 2 hops on average for decoding each term would not be necessary for simple sequential processing if we also keep track of the next occurrence of each term.
- This type of a multi-purpose data structure is particularly attractive for a search engine research (and development) environment where researchers/developers will routinely want to process the documents sequentially, and can do so over the same data structure that they use for searching.
- the performance bottleneck is the time it takes to decode the occurrences (which are typically delta encoded to save space, and thus have to be followed from the beginning) of the most frequently occurring term, especially if this term is a so-called stop-word such as “the”.
- the present invention allows the search engine to simply look at the documents that contain the least popular term (by decoding its occurrences). This allows the search engine to decode other terms in the document to see if any of the other terms match more popular terms in the query. If so, the search engine can simply advance to that position in the “linked list of occurrences” for the other (more popular) terms.
- the worst case we would decode all other terms in the document, and if we do not find the other query terms, we would simply move on to the next occurrence of the least-popular term.
- the worst-case time complexity would be improved.
- the average-time complexity would be O(K*L*D*N) where K is the term identifier encoding frequency (discussed earlier), L is the length of the query, D is the average number of unique terms in a document, and N is the number of occurrences of the least frequent term.
- K is the term identifier encoding frequency (discussed earlier)
- L the length of the query
- D is the average number of unique terms in a document
- N is the number of occurrences of the least frequent term.
- the “hidden constant” would depend primarily on the properties of the language of the documents and of course the query.
- phrase matches would become much faster since we would only need to decode a limited number of terms that are immediately after or before the least-popular term.
- This operation would have the time complexity O(K*L*N) where K is the term identifier encoding frequency (discussed earlier), L is the length of the phrase, and N is the number of occurrences of the least-frequent term in the phrase.
- This scheme also has certain advantages for handling complex scoring schemes that may or may not look at other (non-query) terms in the document. Note that because we can decompress the document term-by-term, the search engine can employ lazy, incremental decompression that would work well for a multi-layer scoring algorithm that requires more information at each layer. The lowest layers of this algorithm would perform the least-costly operations that are typically viewed as the “document selection” activity in a search engine, while higher layers perform increasingly complex scoring of the document for the given query.
- the lowest layer may perform the AND operation that is commonly used in today's search engines, while a second (higher) layer may use a complex machine learning algorithm that requires a lot of CPU. Only documents that pass the AND-layer would be scored at the second layer, thus saving time in the average case, assuming the second layer takes more time than the first and the document does not always pass the first layer. Note that any test (e.g. AND, OR, etc) on the query terms would be very fast since we already know the query terms and thus we would not have to go through the K/ 2 hops necessary for decoding unknown terms in the average case.
- any test e.g. AND, OR, etc
- the above-described dictionary-based compression scheme is used in combination with techniques used in existing search engines, such as an exemplary search engine described in the article S. Brin and L. Page, “The Anatomy of a Large-Scale Hypertextual Search Engine,” Seventh International World Wide Web Conference, Brisbane, Australia and in U.S. Pat. No. 6,285,999 (both incorporated herein by reference).
- an exemplary search engine described in the article S. Brin and L. Page, “The Anatomy of a Large-Scale Hypertextual Search Engine,” Seventh International World Wide Web Conference, Brisbane, Australia and in U.S. Pat. No. 6,285,999 (both incorporated herein by reference).
- a single repository 208 may be used.
- Each repository may represent, for example, a subset of the documents to be stored, such as a set of documents grouped according to similarity in topic or concept, and/or grouped using known information retrieval techniques (such as term frequency, inverse document frequency etc.).
- a repository may also represent a single document, a portion of a document, portions of several documents, or any other arrangement of documents, portions of documents, terms, tokens, etc.
- an inverted index 902 may be combined with repositories 908 ; for example, inverted index 902 may be used to index a term 904 into documents (or e.g. groups of documents) 908 by identifying the documents (or e.g. groups of documents) in which the term appears.
- This indexing may be accomplished using document identifiers 906 , or by any other method.
- the repositories 908 e.g. document, or group of documents
- the methods described previously may be used to encode terms, such as term 904 , as pointers to the next occurrence of the terms.
- terms may be identified and encoded according to the techniques discussed with reference to FIG. 4 .
- Documents (or e.g. groups of documents) 906 in which term 904 appears may thus be identified without the need to traverse an entire repository 208 by following pointers.
- inverted index 902 may store document (or e.g. group of documents) identifying information 906 in adjacent memory locations, data locality may be improved with respect to a lookup for term 904 .
- a “document” is to be broadly interpreted to include any machine-readable or machine-storable work product.
- a document may be a file, a combination of files, one or more files with embedded links to other files, etc.
- the files may be of any type, such as text, audio, image, 20 video, etc.
- a common document is a Web page.
Abstract
Description
Claims (42)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US10/444,761 US7319994B1 (en) | 2003-05-23 | 2003-05-23 | Document compression scheme that supports searching and partial decompression |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US10/444,761 US7319994B1 (en) | 2003-05-23 | 2003-05-23 | Document compression scheme that supports searching and partial decompression |
Publications (1)
Publication Number | Publication Date |
---|---|
US7319994B1 true US7319994B1 (en) | 2008-01-15 |
Family
ID=38921079
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US10/444,761 Active 2025-01-06 US7319994B1 (en) | 2003-05-23 | 2003-05-23 | Document compression scheme that supports searching and partial decompression |
Country Status (1)
Country | Link |
---|---|
US (1) | US7319994B1 (en) |
Cited By (12)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20060215291A1 (en) * | 2005-03-24 | 2006-09-28 | Jaquette Glen A | Data string searching |
US20080059488A1 (en) * | 2006-08-23 | 2008-03-06 | Giridharan Iyengar | System and method for positional representation of content for efficient indexing, search, retrieval, and compression |
US20080072134A1 (en) * | 2006-09-19 | 2008-03-20 | Sreeram Viswanath Balakrishnan | Annotating token sequences within documents |
US20080133565A1 (en) * | 2006-11-08 | 2008-06-05 | Tomohiro Yasuda | Device and method for constructing inverted indexes |
US20080243715A1 (en) * | 2007-04-02 | 2008-10-02 | Bank Of America Corporation | Financial Account Information Management and Auditing |
US20090248400A1 (en) * | 2008-04-01 | 2009-10-01 | International Business Machines Corporation | Rule Based Apparatus for Modifying Word Annotations |
US20100169304A1 (en) * | 2008-12-31 | 2010-07-01 | Thomson Reuters Global Resources | System and method to retrieve relevant information |
US20110302148A1 (en) * | 2010-06-02 | 2011-12-08 | Yahoo! Inc. | System and Method for Indexing Food Providers and Use of the Index in Search Engines |
US20120221540A1 (en) * | 2011-02-24 | 2012-08-30 | A9.Com, Inc. | Encoding of variable-length data with group unary formats |
US20140032714A1 (en) * | 2012-07-27 | 2014-01-30 | Interdigital Patent Holdings, Inc. | Method and apparatus for publishing location information for a content object |
US9442905B1 (en) * | 2013-06-28 | 2016-09-13 | Google Inc. | Detecting neighborhoods from geocoded web documents |
US20180129696A1 (en) * | 2016-11-04 | 2018-05-10 | Sap Se | Storage and pruning for faster access of a document store |
Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5953723A (en) * | 1993-04-02 | 1999-09-14 | T.M. Patents, L.P. | System and method for compressing inverted index files in document search/retrieval system |
US6493707B1 (en) * | 1999-10-29 | 2002-12-10 | Verizon Laboratories Inc. | Hypervideo: information retrieval using realtime buffers |
US20030061025A1 (en) * | 2001-03-16 | 2003-03-27 | Eli Abir | Content conversion method and apparatus |
US20040126026A1 (en) * | 2002-12-26 | 2004-07-01 | Chun-Xiang He | Apparatus and method for data compressibility test |
US6907598B2 (en) * | 2002-06-05 | 2005-06-14 | Microsoft Corporation | Method and system for compressing program code and interpreting compressed program code |
-
2003
- 2003-05-23 US US10/444,761 patent/US7319994B1/en active Active
Patent Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5953723A (en) * | 1993-04-02 | 1999-09-14 | T.M. Patents, L.P. | System and method for compressing inverted index files in document search/retrieval system |
US6493707B1 (en) * | 1999-10-29 | 2002-12-10 | Verizon Laboratories Inc. | Hypervideo: information retrieval using realtime buffers |
US20030061025A1 (en) * | 2001-03-16 | 2003-03-27 | Eli Abir | Content conversion method and apparatus |
US6907598B2 (en) * | 2002-06-05 | 2005-06-14 | Microsoft Corporation | Method and system for compressing program code and interpreting compressed program code |
US20040126026A1 (en) * | 2002-12-26 | 2004-07-01 | Chun-Xiang He | Apparatus and method for data compressibility test |
Cited By (22)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20060215291A1 (en) * | 2005-03-24 | 2006-09-28 | Jaquette Glen A | Data string searching |
US7747629B2 (en) * | 2006-08-23 | 2010-06-29 | International Business Machines Corporation | System and method for positional representation of content for efficient indexing, search, retrieval, and compression |
US20080059488A1 (en) * | 2006-08-23 | 2008-03-06 | Giridharan Iyengar | System and method for positional representation of content for efficient indexing, search, retrieval, and compression |
US20080072134A1 (en) * | 2006-09-19 | 2008-03-20 | Sreeram Viswanath Balakrishnan | Annotating token sequences within documents |
US20080133565A1 (en) * | 2006-11-08 | 2008-06-05 | Tomohiro Yasuda | Device and method for constructing inverted indexes |
US8321485B2 (en) * | 2006-11-08 | 2012-11-27 | Hitachi, Ltd. | Device and method for constructing inverted indexes |
US20080243715A1 (en) * | 2007-04-02 | 2008-10-02 | Bank Of America Corporation | Financial Account Information Management and Auditing |
US8099345B2 (en) * | 2007-04-02 | 2012-01-17 | Bank Of America Corporation | Financial account information management and auditing |
US20090248400A1 (en) * | 2008-04-01 | 2009-10-01 | International Business Machines Corporation | Rule Based Apparatus for Modifying Word Annotations |
US9208140B2 (en) | 2008-04-01 | 2015-12-08 | International Business Machines Corporation | Rule based apparatus for modifying word annotations |
US8433560B2 (en) * | 2008-04-01 | 2013-04-30 | International Business Machines Corporation | Rule based apparatus for modifying word annotations |
US20100169304A1 (en) * | 2008-12-31 | 2010-07-01 | Thomson Reuters Global Resources | System and method to retrieve relevant information |
US9710542B2 (en) * | 2008-12-31 | 2017-07-18 | Thomson Reuters Global Resources Unlimited Company | System and method to retrieve relevant information |
US20110302148A1 (en) * | 2010-06-02 | 2011-12-08 | Yahoo! Inc. | System and Method for Indexing Food Providers and Use of the Index in Search Engines |
US8903800B2 (en) * | 2010-06-02 | 2014-12-02 | Yahoo!, Inc. | System and method for indexing food providers and use of the index in search engines |
US20120221540A1 (en) * | 2011-02-24 | 2012-08-30 | A9.Com, Inc. | Encoding of variable-length data with group unary formats |
US9195675B2 (en) | 2011-02-24 | 2015-11-24 | A9.Com, Inc. | Decoding of variable-length data with group formats |
US9336225B2 (en) * | 2011-02-24 | 2016-05-10 | A9.Com, Inc. | Encoding of variable-length data with unary formats |
US20140032714A1 (en) * | 2012-07-27 | 2014-01-30 | Interdigital Patent Holdings, Inc. | Method and apparatus for publishing location information for a content object |
US9442905B1 (en) * | 2013-06-28 | 2016-09-13 | Google Inc. | Detecting neighborhoods from geocoded web documents |
US20180129696A1 (en) * | 2016-11-04 | 2018-05-10 | Sap Se | Storage and pruning for faster access of a document store |
US10860571B2 (en) * | 2016-11-04 | 2020-12-08 | Sap Se | Storage and pruning for faster access of a document store |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US9619565B1 (en) | Generating content snippets using a tokenspace repository | |
EP1779273B1 (en) | Multi-stage query processing system and method for use with tokenspace repository | |
Ziviani et al. | Compression: A key for next-generation text retrieval systems | |
US8914380B2 (en) | Search index format optimizations | |
US8838551B2 (en) | Multi-level database compression | |
US7882107B2 (en) | Method and system for processing a text search query in a collection of documents | |
US7634468B2 (en) | Expanded inverted index | |
US5963954A (en) | Method for mapping an index of a database into an array of files | |
Ferragina et al. | Compressing and searching XML data via two zips | |
US7319994B1 (en) | Document compression scheme that supports searching and partial decompression | |
US20080133565A1 (en) | Device and method for constructing inverted indexes | |
US7984036B2 (en) | Processing a text search query in a collection of documents | |
Claude et al. | Universal indexes for highly repetitive document collections | |
Bahle et al. | Compaction Techniques for Nextword Indexes. | |
US6947932B2 (en) | Method of performing a search of a numerical document object model | |
Bell et al. | The MG retrieval system: compressing for space and speed | |
Martynov et al. | An indexing algorithm for text retrieval | |
Arroyuelo et al. | To index or not to index: Time–space trade-offs for positional ranking functions in search engines | |
Lam et al. | Entry pairing in inverted file | |
Zhang et al. | Modified LZW algorithm for efficient compressed text retrieval |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE, INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:SERCINOGLU, OLCAN;REEL/FRAME:015813/0517Effective date: 20030502 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
FPAY | Fee payment |
Year of fee payment: 4 |
|
FPAY | Fee payment |
Year of fee payment: 8 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044695/0115Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 12TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1553); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 12 |