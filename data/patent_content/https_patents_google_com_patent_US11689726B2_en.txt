US11689726B2 - Hybrid motion-compensated neural network with side-information based video coding - Google Patents
Hybrid motion-compensated neural network with side-information based video coding Download PDFInfo
- Publication number
- US11689726B2 US11689726B2 US16/516,784 US201916516784A US11689726B2 US 11689726 B2 US11689726 B2 US 11689726B2 US 201916516784 A US201916516784 A US 201916516784A US 11689726 B2 US11689726 B2 US 11689726B2
- Authority
- US
- United States
- Prior art keywords
- encoder
- side information
- neural network
- decoder
- source data
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/134—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or criterion affecting or controlling the adaptive coding
- H04N19/146—Data rate or code amount at the encoder output
- H04N19/147—Data rate or code amount at the encoder output according to rate distortion criteria
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/60—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using transform coding
- H04N19/61—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using transform coding in combination with predictive coding
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/082—Learning methods modifying the architecture, e.g. adding, deleting or silencing nodes or connections
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/088—Non-supervised learning, e.g. competitive learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T9/00—Image coding
- G06T9/002—Image coding using neural networks
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/102—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or selection affected or controlled by the adaptive coding
- H04N19/103—Selection of coding mode or of prediction mode
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/102—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or selection affected or controlled by the adaptive coding
- H04N19/119—Adaptive subdivision aspects, e.g. subdivision of a picture into rectangular or non-rectangular coding blocks
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/134—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or criterion affecting or controlling the adaptive coding
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/169—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding
- H04N19/17—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being an image region, e.g. an object
- H04N19/176—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being an image region, e.g. an object the region being a block, e.g. a macroblock
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/169—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding
- H04N19/184—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being bits, e.g. of the compressed video stream
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/189—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the adaptation method, adaptation tool or adaptation type used for the adaptive coding
- H04N19/19—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the adaptation method, adaptation tool or adaptation type used for the adaptive coding using optimisation based on Lagrange multipliers
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/30—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using hierarchical techniques, e.g. scalability
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/46—Embedding additional information in the video signal during the compression process
- H04N19/463—Embedding additional information in the video signal during the compression process by compressing encoding parameters before transmission
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/50—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding
- H04N19/59—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding involving spatial sub-sampling or interpolation, e.g. alteration of picture size or resolution
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/06—Physical realisation, i.e. hardware implementation of neural networks, neurons or parts of neurons
- G06N3/063—Physical realisation, i.e. hardware implementation of neural networks, neurons or parts of neurons using electronic means
Definitions
- Digital video streams may represent video using a sequence of frames or still images.
- Digital video can be used for various applications, including, for example, video conferencing, high-definition video entertainment, video advertisements, or sharing of user-generated videos.
- a digital video stream can contain a large amount of data and consume a significant amount of computing or communication resources of a computing device for processing, transmission, or storage of the video data.
- Various approaches have been proposed to reduce the amount of data in video streams, including compression and other encoding techniques.
- One aspect of the disclosed implementations is a first encoder comprising a neural network having at least one hidden layer, wherein the neural network receives source data from the video stream at a first hidden layer of the at least one hidden layer, receives side information correlated with the source data at the first hidden layer, and generates guided information using the source data and the side information.
- the first encoder outputs the guided information and the side information to a decoder for reconstruction of the source data.
- a method for coding a video stream described herein includes providing source data from the video stream to a first encoder including a neural network, generating, using the source data, side information, inputting the side information to the neural network for encoding the source data, and transmitting the source data and the side information from the first encoder to a decoder.
- Another hybrid apparatus for coding a video stream described herein includes a first encoder and a first decoder comprising a neural network having a plurality of hidden layers.
- the neural network receives source data from the video stream at a first hidden layer of the encoder, receives side information correlated with the source data at the first hidden layer of the encoder, generates guided information using the source data and the side information, and receives the guided information and the side information at a first hidden layer of the first decoder for reconstruction of the source data.
- FIG. 1 is a schematic of a video encoding and decoding system.
- FIG. 2 is a block diagram of an example of a computing device that can implement a transmitting station or a receiving station.
- FIG. 3 is a diagram of a video stream to be encoded and subsequently decoded.
- FIG. 4 is a block diagram of an encoder according to implementations of this disclosure.
- FIG. 5 is a block diagram of a decoder according to implementations of this disclosure.
- FIG. 6 is a block diagram of a representation of a portion of a frame according to implementations of this disclosure.
- FIG. 7 is a block diagram of an example of a quad-tree representation of a block according to implementations of this disclosure.
- FIG. 8 is a flowchart of a process for searching for a best mode to code a block.
- FIG. 9 is a block diagram of a process of estimating the rate and distortion costs of coding an image block by using a prediction mode.
- FIG. 10 is a flowchart of a process for encoding a block of a video stream according to implementations of this disclosure.
- FIG. 11 is a block diagram of an example of a codec comprising a neural network with side information according to implementations of this disclosure.
- FIG. 12 is a block diagram of an example of a neural network that can be used to implement the codec of FIG. 11 .
- FIG. 13 is a block diagram of another example of a neural network that can be used to implement the codec of FIG. 11 .
- FIG. 14 is a block diagram of a variation in the example of the neural network of FIG. 13 .
- FIG. 15 is a block diagram of an alternative example of a codec comprising a neural network with side information according to implementations of this disclosure.
- Encoding techniques may be designed to maximize coding efficiency.
- Coding efficiency can mean encoding a video at the lowest possible bit rate while minimizing distortion (e.g., while maintaining a certain level of video quality). Coding efficiency is typically measured in terms of both rate and distortion.
- Rate refers to the number of bits required for encoding (such as encoding a block, a frame, etc.).
- Distortion measures the quality loss between, for example, a source video block and a reconstructed version of the source video block. For example, the distortion may be calculated as a mean-square error between pixel values of the source block and those of the reconstructed block.
- a video codec optimizes the amount of distortion against the rate required to encode the video.
- Modern video codecs (e.g., H.264, which is also known as MPEG-4 AVC; VP9; H.265, which is also known as HEVC; AVS2; and AV1) define and use a large number of tools and configurations (e.g., parameters) to improve coding efficiency.
- a video encoder can use a mode decision to examine (e.g., test, evaluate, etc.) at least some of the valid combinations of parameters to select a combination that results in a relatively low rate-distortion value.
- An example of a mode decision is an intra-prediction mode decision, which determines the best intra-prediction mode for coding a block.
- a mode decision is a partition decision, which determines an optimal sub-partitioning of a coding unit (also known as a coding tree unit or CTU).
- a mode decision includes a decision as to a transform type to use in transforming a block (such as a residual or an image block) from the pixel domain to the frequency domain to form a transform block that includes transform coefficients.
- a metric can be computed for each of the examined combinations and the respective metrics compared.
- the metric can combine the rate and distortion described above to produce a rate-distortion (RD) value or cost.
- the RD value or cost may be a single scalar value.
- Quantization parameters in video codecs can be used to control the tradeoff between rate and distortion.
- a larger quantization parameter means higher quantization (such as of transform coefficients) resulting in a lower rate but higher distortion; and a smaller quantization parameter means lower quantization resulting in a higher rate but a lower distortion.
- the variables QP, q, and Q may be used interchangeably in this disclosure to refer to a quantization parameter.
- the value of the quantization parameter can be fixed.
- an encoder can use one quantization parameter value to encode all frames and/or all blocks of a video.
- the quantization parameter can change, for example, from frame to frame.
- the encoder can change the quantization parameter value(s) based on fluctuations in network bandwidth.
- the quantization parameter can be used to control the tradeoff between rate and distortion
- the quantization parameter can be used to calculate the RD cost associated with a respective combination of parameters.
- the combination resulting in the lowest cost (e.g., lowest RD cost) can be used for encoding, for example, a block or a frame in a compressed bitstream. That is, whenever an encoder decision (e.g., a mode decision) is based on the RD cost, the QP value may be used to determine the RD cost.
- the QP can be used to derive a multiplier that is used to combine the rate and distortion values into one metric.
- Some codecs may refer to the multiplier as the Lagrange multiplier (denoted ⁇ mode ); other codecs may use a similar multiplier that is referred as rdmult. Each codec may have a different method of calculating the multiplier. Unless the context makes clear, the multiplier is referred to herein, regardless of the codec, as the Lagrange multiplier or Lagrange parameter.
- the Lagrange multiplier can be used to evaluate the RD costs of competing modes (i.e., competing combinations of parameters). Specifically, let r m denote the rate (in bits) resulting from using a mode m and let d m denote the resulting distortion. The rate distortion cost of selecting the mode m can be computed as a scalar value: d m + ⁇ mode r m . By using the Lagrange parameter ⁇ mode , it is then possible to compare the cost of two modes and select one with the lower combined RD cost. This technique of evaluating rate distortion cost is a basis of mode decision processes in at least some video codecs.
- Different video codecs may use different techniques to compute the Lagrange multipliers from the quantization parameters. This is due in part to the fact that the different codecs may have different meanings (e.g., definitions, semantics, etc.) for, and method of use of, quantization parameters.
- Codecs (referred to herein as HEVC codecs) that implement the HEVC standard may use a formula that is similar to the formula (1).
- the multiplier has a non-linear relationship to the quantization parameter.
- the multiplier In the cases of HEVC and H.264, the multiplier has an exponential relationship to the QP; and in the cases of H.263, VP9, and AV1, the multiplier has a quadratic relationship to the QP.
- the multipliers may undergo further changes before being used in the respective codecs to account for additional side information included in a compressed bitstream by the encoder. Examples of side information include picture type (e.g., intra vs. inter predicted frame), color components (e.g., luminance or chrominance), and/or region of interest. In an example, such additional changes can be linear changes to the multipliers.
- the RD cost associated with a specific mode may be determined by performing at least a subset of the encoding steps of the encoder.
- the subset of the encoding steps can include, depending on the mode for which an RD cost is to be determined, at least one of determining a prediction block, determining a residual block, determining a transform type, determining an interpolation filter, quantizing a transform block, entropy-encoding (such as using a hypothetical encoder), and so on.
- Machine learning can be well suited to address the computational complexity problem in video coding. For example, instead of performing all of the encoding steps (i.e., a brute-force or exhaustive approach) for determining a rate and a distortion (or, equivalently, an RD cost) associated with mode, a machine-learning model can be used to estimate the rate and the distortion, or to estimate the RD cost, associated with the mode. Then, the best mode may be selected based on the, e.g., lowest, RD cost.
- the machine-learning model may be trained using the vast amount of training data that is available from an encoder performing standard encoding techniques, such as those described with respect to FIGS. 4 and 6 - 9 . More specifically, the training data can be used during the learning phase of machine learning to derive (e.g., learn, infer, etc.) the machine-learning model that is (e.g., defines, constitutes, etc.) a mapping from the input data to an output, in this example a RD cost that can be used to make one or more mode decisions.
- the training data can be used during the learning phase of machine learning to derive (e.g., learn, infer, etc.) the machine-learning model that is (e.g., defines, constitutes, etc.) a mapping from the input data to an output, in this example a RD cost that can be used to make one or more mode decisions.
- the predictive capabilities (i.e., accuracy) of a machine-learning model are as good as the inputs used to train the machine-learning model and the inputs presented to the machine-learning model to predict a result (e.g., the best mode).
- the model computes the output as a deterministic function of its input.
- the machine-learning model can be a neural network model, which can be a convolutional neural-network (CNN). Further details of a neural network model, including a CNN, will be discussed below in regards to FIGS. 12 - 14 .
- a machine-learning model can be used to decide (e.g., select, choose, etc.) a mode from among multiple available modes in a coding process for a block, such as an image block, a prediction block, or a transform.
- This can be a powerful tool for image compression.
- video compression relies heavily on exploiting temporal redundancies between frames, hence introducing a third dimension—time and hence movement—to the horizontal and vertical dimensions of the pixels. Learning motion fields from a three-dimensional volume of data using machine learning is possible, but an additional degree of complexity is involved.
- information e.g., motion information
- information derived from conventional encoding methods may be made available for reconstruction of video data compressed, at least in part, using machine learning. This is achieved using a deep neural network having structural constraints that enforce the availability of the information at the decoder.
- the neural network is described herein first with reference to a block-based codec with the teachings may be incorporated.
- a block-based codec is described as an example, other codecs may be used with the present teachings, including a feature-based codec.
- FIG. 1 is a schematic of a video encoding and decoding system 100 .
- a transmitting station 102 can be, for example, a computer having an internal configuration of hardware, such as that described with respect to FIG. 2 .
- the processing of the transmitting station 102 can be distributed among multiple devices.
- a network 104 can connect the transmitting station 102 and a receiving station 106 for encoding and decoding of the video stream.
- the video stream can be encoded in the transmitting station 102
- the encoded video stream can be decoded in the receiving station 106 .
- the network 104 can be, for example, the Internet.
- the network 104 can also be a local area network (LAN), wide area network (WAN), virtual private network (VPN), cellular telephone network, or any other means of transferring the video stream from the transmitting station 102 to, in this example, the receiving station 106 .
- LAN local area network
- WAN wide area network
- VPN virtual private network
- the receiving station 106 can be a computer having an internal configuration of hardware, such as that described with respect to FIG. 2 .
- the processing of the receiving station 106 can be distributed among multiple devices.
- an implementation can omit the network 104 .
- a video stream can be encoded and then stored for transmission at a later time to the receiving station 106 or any other device having memory.
- the receiving station 106 receives (e.g., via the network 104 , a computer bus, and/or some communication pathway) the encoded video stream and stores the video stream for later decoding.
- a real-time transport protocol RTP
- a transport protocol other than RTP e.g., a Hypertext Transfer Protocol (HTTP)-based video streaming protocol, may be used.
- HTTP Hypertext Transfer Protocol
- the transmitting station 102 and/or the receiving station 106 may include the ability to both encode and decode a video stream as described below.
- the receiving station 106 could be a video conference participant who receives an encoded video bitstream from a video conference server (e.g., the transmitting station 102 ) to decode and view and further encodes and transmits its own video bitstream to the video conference server for decoding and viewing by other participants.
- FIG. 2 is a block diagram of an example of a computing device 200 that can implement a transmitting station or a receiving station.
- the computing device 200 can implement one or both of the transmitting station 102 and the receiving station 106 of FIG. 1 .
- the computing device 200 can be in the form of a computing system including multiple computing devices, or in the form of a single computing device, for example, a mobile phone, a tablet computer, a laptop computer, a notebook computer, a desktop computer, and the like.
- a CPU 202 in the computing device 200 can be a central processing unit.
- the CPU 202 can be any other type of device, or multiple devices, now-existing or hereafter developed, capable of manipulating or processing information.
- the disclosed implementations can be practiced with a single processor as shown (e.g., the CPU 202 ), advantages in speed and efficiency can be achieved by using more than one processor.
- a memory 204 in the computing device 200 can be a read-only memory (ROM) device or a random-access memory (RAM) device. Any other suitable type of storage device can be used as the memory 204 .
- the memory 204 can include code and data 206 that is accessed by the CPU 202 using a bus 212 .
- the memory 204 can further include an operating system 208 and application programs 210 , the application programs 210 including at least one program that permits the CPU 202 to perform the methods described herein.
- the application programs 210 can include applications 1 through N, which further include a video coding application that performs the methods described herein.
- the computing device 200 can also include a secondary storage 214 , which can, for example, be a memory card used with a computing device 200 that is mobile. Because the video communication sessions may contain a significant amount of information, they can be stored in whole or in part in the secondary storage 214 and loaded into the memory 204 as needed for processing.
- a secondary storage 214 can, for example, be a memory card used with a computing device 200 that is mobile. Because the video communication sessions may contain a significant amount of information, they can be stored in whole or in part in the secondary storage 214 and loaded into the memory 204 as needed for processing.
- the computing device 200 can also include one or more output devices, such as a display 218 .
- the display 218 may be, in one example, a touch-sensitive display that combines a display with a touch-sensitive element that is operable to sense touch inputs.
- the display 218 can be coupled to the CPU 202 via the bus 212 .
- Other output devices that permit a user to program or otherwise use the computing device 200 can be provided in addition to or as an alternative to the display 218 .
- the output device is or includes a display
- the display can be implemented in various ways, including as a liquid crystal display (LCD); a cathode-ray tube (CRT) display; or a light-emitting diode (LED) display, such as an organic LED (OLED) display.
- LCD liquid crystal display
- CRT cathode-ray tube
- LED light-emitting diode
- OLED organic LED
- the computing device 200 can also include or be in communication with an image-sensing device 220 , for example, a camera, or any other image-sensing device, now existing or hereafter developed, that can sense an image, such as the image of a user operating the computing device 200 .
- the image-sensing device 220 can be positioned such that it is directed toward the user operating the computing device 200 .
- the position and optical axis of the image-sensing device 220 can be configured such that the field of vision includes an area that is directly adjacent to the display 218 and from which the display 218 is visible.
- the computing device 200 can also include or be in communication with a sound-sensing device 222 , for example, a microphone, or any other sound-sensing device, now existing or hereafter developed, that can sense sounds near the computing device 200 .
- the sound-sensing device 222 can be positioned such that it is directed toward the user operating the computing device 200 and can be configured to receive sounds, for example, speech or other utterances, made by the user while the user operates the computing device 200 .
- FIG. 2 depicts the CPU 202 and the memory 204 of the computing device 200 as being integrated into a single unit, other configurations can be utilized.
- the operations of the CPU 202 can be distributed across multiple machines (each machine having one or more processors) that can be coupled directly or across a local area or other network.
- the memory 204 can be distributed across multiple machines, such as a network-based memory or memory in multiple machines performing the operations of the computing device 200 .
- the bus 212 of the computing device 200 can be composed of multiple buses.
- the secondary storage 214 can be directly coupled to the other components of the computing device 200 or can be accessed via a network and can comprise a single integrated unit, such as a memory card, or multiple units, such as multiple memory cards.
- the computing device 200 can thus be implemented in a wide variety of configurations.
- FIG. 3 is a diagram of an example of a video stream 300 to be encoded and subsequently decoded.
- the video stream 300 includes a video sequence 302 .
- the video sequence 302 includes a number of adjacent frames 304 . While three frames are depicted as the adjacent frames 304 , the video sequence 302 can include any number of adjacent frames 304 .
- the adjacent frames 304 can then be further subdivided into individual frames, for example, a frame 306 .
- the frame 306 can be divided into a series of segments 308 or planes.
- the segments 308 can be subsets of frames that permit parallel processing, for example.
- the segments 308 can also be subsets of frames that can separate the video data into separate colors.
- the frame 306 of color video data can include a luminance plane and two chrominance planes.
- the segments 308 may be sampled at different resolutions.
- the frame 306 may be further subdivided into blocks 310 , which can contain data corresponding to, for example, 16 ⁇ 16 pixels in the frame 306 .
- the blocks 310 can also be arranged to include data from one or more segments 308 of pixel data.
- the blocks 310 can also be of any other suitable size, such as 4 ⁇ 4 pixels, 8 ⁇ 8 pixels, 16 ⁇ 8 pixels, 8 ⁇ 16 pixels, 16 ⁇ 16 pixels, or larger.
- FIG. 4 is a block diagram of an encoder 400 in accordance with implementations of this disclosure.
- the encoder 400 can be implemented, as described above, in the transmitting station 102 , such as by providing a computer software program stored in memory, for example, the memory 204 .
- the computer software program can include machine instructions that, when executed by a processor, such as the CPU 202 , cause the transmitting station 102 to encode video data in manners described herein.
- the encoder 400 can also be implemented as specialized hardware included in, for example, the transmitting station 102 .
- the encoder 400 has the following stages to perform the various functions in a forward path (shown by the solid connection lines) to produce an encoded or compressed bitstream 420 using the video stream 300 as input: an intra/inter-prediction stage 402 , a transform stage 404 , a quantization stage 406 , and an entropy encoding stage 408 .
- the encoder 400 may also include a reconstruction path (shown by the dotted connection lines) to reconstruct a frame for encoding of future blocks.
- the encoder 400 has the following stages to perform the various functions in the reconstruction path: a dequantization stage 410 , an inverse transform stage 412 , a reconstruction stage 414 , and a loop filtering stage 416 .
- Other structural variations of the encoder 400 can be used to encode the video stream 300 .
- a block can be encoded using intra-frame prediction (also called intra-prediction) or inter-frame prediction (also called inter-prediction), or a combination of both.
- intra-frame prediction also called intra-prediction
- inter-frame prediction also called inter-prediction
- a prediction block can be formed.
- intra-prediction all or part of a prediction block may be formed from samples in the current frame that have been previously encoded and reconstructed.
- inter-prediction all or part of a prediction block may be formed from samples in one or more previously constructed reference frames determined using motion vectors.
- the prediction block can be subtracted from the current block at the intra/inter-prediction stage 402 to produce a residual block (also called a residual).
- the transform stage 404 transforms the residual into transform coefficients in, for example, the frequency domain using block-based transforms.
- block-based transforms i.e., transform types
- DCT Discrete Cosine Transform
- ADST Asymmetric Discrete Sine Transform
- Other block-based transforms are possible.
- combinations of different transforms may be applied to a single residual.
- the DCT transforms the residual block into the frequency domain where the transform coefficient values are based on spatial frequency.
- the lowest frequency (DC) coefficient is at the top-left of the matrix, and the highest frequency coefficient is at the bottom-right of the matrix. It is worth noting that the size of a prediction block, and hence the resulting residual block, may be different from the size of the transform block. For example, the prediction block may be split into smaller blocks to which separate transforms are applied.
- the quantization stage 406 converts the transform coefficients into discrete quantum values, which are referred to as quantized transform coefficients, using a quantizer value or a quantization level. For example, the transform coefficients may be divided by the quantizer value and truncated.
- the quantized transform coefficients are then entropy encoded by the entropy encoding stage 408 . Entropy coding may be performed using any number of techniques, including token and binary trees.
- the entropy-encoded coefficients, together with other information used to decode the block (which may include, for example, the type of prediction used, transform type, motion vectors, and quantizer value), are then output to the compressed bitstream 420 .
- the information to decode the block may be entropy coded into block, frame, slice, and/or section headers within the compressed bitstream 420 .
- the compressed bitstream 420 can also be referred to as an encoded video stream or encoded video bitstream; these terms will be used interchangeably herein.
- the reconstruction path in FIG. 4 can be used to ensure that both the encoder 400 and a decoder 500 (described below) use the same reference frames and blocks to decode the compressed bitstream 420 .
- the reconstruction path performs functions that are similar to functions that take place during the decoding process and that are discussed in more detail below, including dequantizing the quantized transform coefficients at the dequantization stage 410 and inverse transforming the dequantized transform coefficients at the inverse transform stage 412 to produce a derivative residual block (also called a derivative residual).
- the prediction block that was predicted at the intra/inter-prediction stage 402 can be added to the derivative residual to create a reconstructed block.
- the loop filtering stage 416 can be applied to the reconstructed block to reduce distortion, such as blocking artifacts.
- encoder 400 can be used to encode the compressed bitstream 420 .
- a non-transform based encoder 400 can quantize the residual signal directly without the transform stage 404 for certain blocks or frames.
- an encoder 400 can have the quantization stage 406 and the dequantization stage 410 combined into a single stage.
- FIG. 5 is a block diagram of a decoder 500 in accordance with implementations of this disclosure.
- the decoder 500 can be implemented in the receiving station 106 , for example, by providing a computer software program stored in the memory 204 .
- the computer software program can include machine instructions that, when executed by a processor, such as the CPU 202 , cause the receiving station 106 to decode video data in the manners described below.
- the decoder 500 can also be implemented in hardware included in, for example, the transmitting station 102 or the receiving station 106 .
- the decoder 500 similar to the reconstruction path of the encoder 400 discussed above, includes in one example the following stages to perform various functions to produce an output video stream 516 from the compressed bitstream 420 : an entropy decoding stage 502 , a dequantization stage 504 , an inverse transform stage 506 , an intra/inter-prediction stage 508 , a reconstruction stage 510 , a loop filtering stage 512 , and a post filtering stage 514 .
- Other structural variations of the decoder 500 can be used to decode the compressed bitstream 420 .
- the data elements within the compressed bitstream 420 can be decoded by the entropy decoding stage 502 to produce a set of quantized transform coefficients.
- the dequantization stage 504 dequantizes the quantized transform coefficients (e.g., by multiplying the quantized transform coefficients by the quantizer value), and the inverse transform stage 506 inverse transforms the dequantized transform coefficients using the selected transform type to produce a derivative residual that can be identical to that created by the inverse transform stage 412 in the encoder 400 .
- the decoder 500 can use the intra/inter-prediction stage 508 to create the same prediction block as was created in the encoder 400 , for example, at the intra/inter-prediction stage 402 .
- the prediction block can be added to the derivative residual to create a reconstructed block.
- the loop filtering stage 512 can be applied to the reconstructed block to reduce blocking artifacts. Other filtering can be applied to the reconstructed block.
- the post filtering stage 514 is applied to the reconstructed block to reduce blocking distortion, and the result is output as an output video stream 516 .
- the output video stream 516 can also be referred to as a decoded video stream; these terms will be used interchangeably herein.
- the decoder 500 can be used to decode the compressed bitstream 420 .
- the decoder 500 can produce the output video stream 516 without the post filtering stage 514 .
- the post filtering stage 514 is applied after the loop filtering stage 512 .
- the loop filtering stage 512 can include an optional deblocking filtering stage.
- the encoder 400 includes an optional deblocking filtering stage in the loop filtering stage 416 .
- FIG. 6 is a block diagram of a representation of a portion 600 of a frame, such as the frame 306 of FIG. 3 , according to implementations of this disclosure.
- the portion 600 of the frame includes four 64 ⁇ 64 blocks 610 , which may be referred to as superblocks, in two rows and two columns in a matrix or Cartesian plane.
- a superblock can have a larger or a smaller size. While FIG. 6 is explained with respect to a superblock of size 64 ⁇ 64, the description is easily extendable to larger (e.g., 128 ⁇ 128) or smaller superblock sizes.
- a superblock can be a basic or maximum coding unit (CU).
- Each CU can include four 32 ⁇ 32 blocks 620 .
- Each 32 ⁇ 32 block 620 can include four 16 ⁇ 16 blocks 630 .
- Each 16 ⁇ 16 block 630 can include four 8 ⁇ 8 blocks 640 .
- Each 8 ⁇ 8 block 640 can include four 4 ⁇ 4 blocks 650 .
- Each 4 ⁇ 4 block 650 can include 16 pixels, which can be represented in four rows and four columns in each respective block in the Cartesian plane or matrix. The pixels can include information representing an image captured in the frame, such as luminance information, color information, and location information.
- a block such as a 16 ⁇ 16-pixel block as shown, can include a luminance block 660 , which can include luminance pixels 662 ; and two chrominance blocks 670 / 680 , such as a U or Cb chrominance block 670 , and a V or Cr chrominance block 680 .
- the chrominance blocks 670 / 680 can include chrominance pixels 690 .
- the luminance block 660 can include 16 ⁇ 16 luminance pixels 662
- each chrominance block 670 / 680 can include 8 ⁇ 8 chrominance pixels 690 , as shown. Although one arrangement of blocks is shown, any arrangement can be used.
- N ⁇ N blocks in some implementations, N ⁇ M, where N ⁇ M, blocks can be used.
- N ⁇ M 32 ⁇ 64 blocks, 64 ⁇ 32 blocks, 16 ⁇ 32 blocks, 32 ⁇ 16 blocks, or any other size blocks can be used.
- N ⁇ 2N blocks, 2N ⁇ N blocks, or a combination thereof can be used.
- Video coding can include ordered block-level coding.
- Ordered block-level coding can include coding blocks of a frame in an scan order, such as raster scan order, wherein blocks can be identified and processed starting with a block in the upper left corner of the frame, or a portion of the frame, and proceeding along rows from left to right and from the top row to the bottom row, identifying each block in turn for processing.
- the CU in the top row and left column of a frame can be the first block coded
- the CU immediately to the right of the first block can be the second block coded.
- the second row from the top can be the second row coded, such that the CU in the left column of the second row can be coded after the CU in the rightmost column of the first row.
- coding a block can include using quad-tree coding, which can include coding smaller block units with a block in raster-scan order.
- quad-tree coding can include coding smaller block units with a block in raster-scan order.
- the 64 ⁇ 64 superblock shown in the bottom-left corner of the portion of the frame shown in FIG. 6 can be coded using quad-tree coding in which the top-left 32 ⁇ 32 block can be coded, then the top-right 32 ⁇ 32 block can be coded, then the bottom-left 32 ⁇ 32 block can be coded, and then the bottom-right 32 ⁇ 32 block can be coded.
- Each 32 ⁇ 32 block can be coded using quad-tree coding in which the top-left 16 ⁇ 16 block can be coded, then the top-right 16 ⁇ 16 block can be coded, then the bottom-left 16 ⁇ 16 block can be coded, and then the bottom-right 16 ⁇ 16 block can be coded.
- Each 16 ⁇ 16 block can be coded using quad-tree coding in which the top-left 8 ⁇ 8 block can be coded, then the top-right 8 ⁇ 8 block can be coded, then the bottom-left 8 ⁇ 8 block can be coded, and then the bottom-right 8 ⁇ 8 block can be coded.
- Each 8 ⁇ 8 block can be coded using quad-tree coding in which the top-left 4 ⁇ 4 block can be coded, then the top-right 4 ⁇ 4 block can be coded, then the bottom-left 4 ⁇ 4 block can be coded, and then the bottom-right 4 ⁇ 4 block can be coded.
- 8 ⁇ 8 blocks can be omitted for a 16 ⁇ 16 block, and the 16 ⁇ 16 block can be coded using quad-tree coding in which the top-left 4 ⁇ 4 block can be coded, and then the other 4 ⁇ 4 blocks in the 16 ⁇ 16 block can be coded in raster-scan order.
- Video coding can include compressing the information included in an original, or input, frame by omitting some of the information in the original frame from a corresponding encoded frame.
- coding can include reducing spectral redundancy, reducing spatial redundancy, reducing temporal redundancy, or a combination thereof.
- reducing spectral redundancy can include using a color model based on a luminance component (Y) and two chrominance components (U and V or Cb and Cr), which can be referred to as the YUV or YCbCr color model or color space.
- YUV color model can include using a relatively large amount of information to represent the luminance component of a portion of a frame and using a relatively small amount of information to represent each corresponding chrominance component for the portion of the frame.
- a portion of a frame can be represented by a high-resolution luminance component, which can include a 16 ⁇ 16 block of pixels, and by two lower resolution chrominance components, each of which representing the portion of the frame as an 8 ⁇ 8 block of pixels.
- a pixel can indicate a value (e.g., a value in the range from 0 to 255) and can be stored or transmitted using, for example, eight bits.
- Reducing spatial redundancy can include intra prediction of the block and transforming the residual block into the frequency domain as described above.
- a unit of an encoder such as the transform stage 404 of FIG. 4 , can perform a DCT using transform coefficient values based on spatial frequency after intra/inter-prediction stage 402 .
- Reducing temporal redundancy can include using similarities between frames to encode a frame using a relatively small amount of data based on one or more reference frames, which can be previously encoded, decoded, and reconstructed frames of the video stream.
- a block or a pixel of a current frame can be similar to a spatially corresponding block or pixel of a reference frame.
- a block or a pixel of a current frame can be similar to a block or a pixel of a reference frame at a different spatial location.
- reducing temporal redundancy can include generating motion information indicating the spatial difference (e.g., a translation between the location of the block or the pixel in the current frame and the corresponding location of the block or the pixel in the reference frame). This is referred to as inter prediction above.
- Reducing temporal redundancy can include identifying a block or a pixel in a reference frame, or a portion of the reference frame, that corresponds with a current block or pixel of a current frame.
- a reference frame, or a portion of a reference frame, which can be stored in memory can be searched for the best block or pixel to use for encoding a current block or pixel of the current frame.
- the search may identify the block of the reference frame for which the difference in pixel values between the reference block and the current block is minimized, and can be referred to as motion searching.
- the portion of the reference frame searched can be limited.
- the portion of the reference frame searched which can be referred to as the search area, can include a limited number of rows of the reference frame.
- identifying the reference block can include calculating a cost function, such as a sum of absolute differences (SAD), between the pixels of the blocks in the search area and the pixels of the current block.
- SAD sum of absolute differences
- the spatial difference between the location of the reference block in the reference frame and the current block in the current frame can be represented as a motion vector.
- the difference in pixel values between the reference block and the current block can be referred to as differential data, residual data, or as a residual block.
- generating motion vectors can be referred to as motion estimation, and a pixel of a current block can be indicated based on location using Cartesian coordinates such as f x,y .
- a pixel of the search area of the reference frame can be indicated based on a location using Cartesian coordinates such as r x,y .
- a motion vector (MV) for the current block can be determined based on, for example, a SAD between the pixels of the current frame and the corresponding pixels of the reference frame.
- a CU or block may be coded using quad-tree partitioning or coding as shown in the example of FIG. 7 .
- the example shows quad-tree partitioning of a block 700 .
- the block 700 can be partitioned differently, such as by an encoder (e.g., the encoder 400 of FIG. 4 ) or a machine-learning model as described herein.
- the block 700 is partitioned into four blocks, namely, the blocks 700 - 1 , 700 - 2 , 700 - 3 , and 700 - 4 .
- the block 700 - 2 is further partitioned into the blocks 702 - 1 , 702 - 2 , 702 - 3 , and 702 - 4 .
- the blocks 700 - 1 , 700 - 2 , 700 - 3 , and 700 - 4 are each of size N/2 ⁇ N/2 (e.g., 64 ⁇ 64), and the blocks 702 - 1 , 702 - 2 , 702 - 3 , and 702 - 4 are each of size N/4 ⁇ N/4 (e.g., 32 ⁇ 32). If a block is partitioned, it is partitioned into four equally sized, non-overlapping square sub-blocks.
- a quad-tree data representation is used to describe how the block 700 is partitioned into sub-blocks, such as blocks 700 - 1 , 700 - 2 , 700 - 3 , 700 - 4 , 702 - 1 , 702 - 2 , 702 - 3 , and 702 - 4 .
- a quad-tree 704 of the partition of the block 700 is shown. Each node of the quad-tree 704 is assigned a flag of “1” if the node is further split into four sub-nodes and assigned a flag of “0” if the node is not split.
- the flag can be referred to as a split bit (e.g., 1) or a stop bit (e.g., 0) and is coded in a compressed bitstream.
- a split bit e.g., 1
- a stop bit e.g., 0
- a node either has four child nodes or has no child nodes.
- a node that has no child nodes corresponds to a block that is not split further.
- Each of the child nodes of a split block corresponds to a sub-block.
- each node corresponds to a sub-block of the block 700 .
- the corresponding sub-block is shown between parentheses.
- a node 704 - 1 which has a value of 0, corresponds to the block 700 - 1 .
- a root node 704 - 0 corresponds to the block 700 .
- the value of the root node 704 - 0 is the split bit (e.g., 1).
- the flags indicate whether a sub-block of the block 700 is further split into four sub-sub-blocks.
- a node 704 - 2 includes a flag of “1” because the block 700 - 2 is split into the blocks 702 - 1 , 702 - 2 , 702 - 3 , and 702 - 4 .
- Each of nodes 704 - 1 , 704 - 3 , and 704 - 4 includes a flag of “0” because the corresponding blocks are not split.
- no flag of “0” or “1” is necessary for these nodes. That the blocks 702 - 5 , 702 - 6 , 702 - 7 , and 702 - 8 are not split further can be inferred from the absence of additional flags corresponding to these blocks.
- the smallest sub-block is 32 ⁇ 32 pixels, but further partitioning is possible.
- the quad-tree data representation for the quad-tree 704 can be represented by the binary data of “10100,” where each bit represents a node of the quad-tree 704 .
- the binary data indicates the partitioning of the block 700 to the encoder and decoder.
- the encoder can encode the binary data in a compressed bitstream, such as the compressed bitstream 420 of FIG. 4 , in a case where the encoder needs to communicate the binary data to a decoder, such as the decoder 500 of FIG. 5 .
- the blocks corresponding to the leaf nodes of the quad-tree 704 can be used as the bases for prediction. That is, prediction can be performed for each of the blocks 700 - 1 , 702 - 1 , 702 - 2 , 702 - 3 , 702 - 4 , 700 - 3 , and 700 - 4 , referred to herein as coding blocks.
- the coding block can be a luminance block or a chrominance block. It is noted that, in an example, the block partitioning can be determined with respect to luminance blocks. The same partition, or a different partition, can be used with the chrominance blocks.
- a prediction type (e.g., intra- or inter-prediction) is determined at the coding block. That is, a coding block is the decision point for prediction.
- a mode decision process determines the partitioning of a coding block, such as the block 700 .
- the partition decision process calculates the RD costs of different combinations of coding parameters. That is, for example, different combinations of prediction blocks and predictions (e.g., intra-prediction, inter-prediction, etc.) are examined to determine an optimal partitioning.
- the machine-learning model can be used to generate estimates of the RD costs associated with respective modes, which are in turn used in the mode decision. That is, implementations according to this disclosure can be used for cases where a best mode is typically selected from among a set of possible modes, using RDO processes.
- FIG. 8 is a flowchart of a process 800 for searching for a best mode to code a block.
- the process 800 is an illustrative, high level process of a mode decision process that determines a best mode of multiple available modes. For ease of description, the process 800 is described with respect to selecting an intra-prediction mode for encoding a prediction block. Other examples of best modes that can be determined by processes similar to the process 800 include determining a transform type and determining a transform size.
- the process 800 can be implemented by an encoder, such as the encoder 400 of FIG. 4 , using a brute-force approach to the mode decision.
- the process 800 receives a block.
- the block can be a prediction unit.
- each of the leaf node coding blocks e.g., a block 700 - 1 , 702 - 1 , 702 - 2 , 702 - 3 , 702 - 4 , 700 - 3 , or 700 - 4
- the block can be one such prediction unit.
- the process 800 determines (e.g., selects, calculates, chooses, etc.) a list of modes.
- the list of modes can include K modes, where K is an integer number.
- the list of modes can be denoted ⁇ m 1 , m 2 , . . . , m k ⁇ .
- the encoder can have a list of available intra-prediction modes.
- the list of available intra-prediction modes can be ⁇ DC_PRED, V_PRED, H_PRED, D45_PRED, D135_PRED, D117_PRED, D153_PRED, D207_PRED, D63_PRED, SMOOTH_PRED, SMOOTH_V_PRED, and SMOOTH_H_PRED, PAETH_PRED ⁇ .
- a description of these intra-prediction modes is omitted as the description is irrelevant to the understanding of this disclosure.
- the list of modes determined at 804 can be any subset of the list of available intra-prediction modes.
- the process 800 initializes a BEST_COST variable to a high value (e.g., INT_MAX, which may be equal to 2,147,483,647) and initializes a loop variable i to 1, which corresponds to the first mode to be examined.
- a high value e.g., INT_MAX, which may be equal to 2,147,483,647
- the process 800 computes or calculates an RD_COST i for the mode i .
- the process 800 proceeds back to 808 ; otherwise the process 800 proceeds to 818 .
- the process 800 outputs the index of the best mode, BEST_MODE. Outputting the best mode can mean returning the best mode to a caller of the process 800 . Outputting the best mode can mean encoding the image using the best mode. Outputting the best mode can have other semantics. The process 800 then terminates.
- FIG. 9 is a block diagram of a process 900 of estimating the rate and distortion costs of coding an image block X by using a coding mode m i .
- the process 900 can be performed by an encoder, such as the encoder 400 of FIG. 4 .
- the process 900 includes coding of the image block X using the coding mode m i to determine the RD cost of encoding the block. More specifically, the process 900 computes the number of bits (RATE) required to encode the image block X.
- the example 900 also calculates a distortion (DISTORTION) based on a difference between the image block X and a reconstructed version of the image block X d .
- the process 900 can be used by the process 800 at 808 .
- the coding mode m i is a prediction mode.
- a prediction using the mode m i , is determined.
- the prediction can be determined as described with respect to intra/inter-prediction stage 402 of FIG. 4 .
- a residual is determined as a difference between the image block 902 and the prediction.
- the residual is transformed and quantized, such as described, respectively, with respect to the transform stage 404 and the quantization stage 406 of FIG. 4 .
- the rate (RATE) is calculated by a rate estimator 912 , which performs the hypothetical encoding.
- the rate estimator 912 can perform entropy encoding, such as described with respect to the entropy encoding stage 408 of FIG. 4 .
- the quantized residual is dequantized at 914 (such as described, for example, with respect to the dequantization stage 410 of FIG. 4 ), inverse transformed at 916 (such as described, for example, with respect to the inverse transform stage 412 of FIG. 4 ), and reconstructed at 918 (such as described, for example, with respect to the reconstruction stage 414 of FIG. 4 ) to generate a reconstructed block.
- a distortion estimator 920 calculates the distortion between the image block X and the reconstructed block.
- the distortion can be a mean square error between pixel values of the image block X and the reconstructed block.
- the distortion can be a sum of absolute differences error between pixel values of the image block X and the reconstructed block. Any other suitable distortion measure can be used.
- the rate, RATE, and distortion, DISTORTION are then combined into a scalar value (i.e., the RD cost) by using the Lagrange multiplier as shown in formula (5) DISTORTION+ ⁇ mode ⁇ RATE, (5)
- the Lagrange multiplier ⁇ mode of the formula 5 can vary (e.g., depending on the encoder performing the operations of the process 900 ).
- FIGS. 8 and 9 illustrate an approach to mode decisions in a block-based encoder that is largely a serial process that essentially codes an image block X by using candidate modes to determine the mode with the best cost.
- Techniques have been used to reduce the complexity in mode decisions. For example, early termination techniques have been used to terminate the loop of the process 800 of FIG. 8 as soon as certain conditions are met, such as, for example, that the rate distortion cost is lower than a threshold.
- Other techniques include selecting, for example based on heuristics, a subset of the available candidate modes or using multi-passes over the candidate modes.
- FIG. 10 is a flowchart of a process 1000 for encoding, using a machine-learning model, a block of a video stream according to implementations of this disclosure.
- the process 1000 includes two phases: a training phase and an inference phase.
- the training and inference phases are shown as phases of one process (i.e., the process 1000 ).
- the training and inference phases are often separate processes.
- the process 1000 trains the machine-learning (ML) mode.
- the ML model can be trained using training data 1004 as input.
- the training data 1004 is a set of training data. Each training datum is indicated by a subscript i.
- Each training datum of the training data 1004 can include a video block (i.e., a training block i ) that was encoded by traditional encoding methods (e.g., by a block-based encoder), such as described with respect to FIGS. 4 and 6 - 9 ; one or more modes, used by the encoder for encoding the training block i : and the resulting encoding cost i , as determined by the encoder, of encoding the training block, using the mode i .
- parameters of the ML model are generated such that, for at least some of the training data 1004 , the ML model can infer, for a training datum, the mode i , encoding cost i , or both.
- the ML model learns (e.g., trains, builds, derives, etc.) a mapping (i.e., a function) from the inputs to the outputs.
- the mode can be a partition decision, or any other mode decision for compression or reconstruction in video coding.
- the mode can include a combination of mode decisions.
- the block may be an image block, a prediction block, or a transform block, for example, of a source frame.
- the block can be a residual block, that is, the difference between a source image block and a prediction block.
- the encoding mode can be related to any of these blocks.
- the encoding mode can include a partition mode, an intra- or inter-prediction mode, a transform mode, etc.
- the encoding cost can be the cost of encoding a block using the encoding mode.
- the input data can include block features of the training block, during the training phase. Which block features are calculated (e.g. generated) and used as input to the machine-learning model can depend on the encoding mode. For example, different block features can be extracted (e.g., calculated, determined, etc.) for an encoding mode related to a transform block than an encoding mode related to a prediction block.
- the encoding cost can include two separate values; namely, a rate and a distortion from which a RD cost can be calculated as described above.
- the encoding cost can include, or can be, the RD cost value itself.
- the ML model can then be used by the process 1000 during an inference phase.
- the inference phase includes the operations 1020 and 1022 .
- a separation 1010 indicates that the training phase and the inference phase can be separated in time.
- the inferencing phase can be performed using a different encoder than that used to train the machine-learning model at 1002 . In an example, the same encoder is used. In either case, the inference phase uses a machine-learning model that is trained as described with respect to 1002 .
- the process 1000 receives a source block for which a best mode for encoding the block in a bitstream is to be determined.
- the best mode can be the partitioning that minimizes encoding cost.
- the best mode can be a mode that relates to a block, such as a transform type or a transform size.
- the best mode can be a mode that relates to an intra-prediction block, such as intra-prediction mode.
- the best mode can be a mode that relates to an inter-prediction block, such as an interpolation filter type.
- the best mode can be a combination of modes for encoding and optionally reconstructing a source block.
- the source block is presented to the model that is trained as described with respect to 1002 .
- the process 1000 obtains (e.g., generates, calculates, selects, determines, etc.) the mode decision that minimizes encoding cost (e.g., the best mode) as the output of the machine-learning model.
- the process 1000 encodes, in a compressed bitstream, the block using the best mode.
- Information that is derived from the source block during the inference phase of the encoding process 1000 is not readily available to different mode decisions of the encoder or to a decoder. Also, the process 1000 is well-adapted for image compression, but is more difficult to apply to video compression. For at least these reasons, while neural network encoders (e.g., those implementing a machine-learning model) may be better in representing and restoring high frequency information and residuals, conventional encoders are often better at capturing simple motion and coding low frequencies.
- neural network encoders e.g., those implementing a machine-learning model
- FIG. 11 is a block diagram of an example of a codec 1100 comprising a neural network with side information.
- This arrangement may be considered a modification of learned image compression, where the network learns (through training) how to get close to the optimum rate-distortion function of the source X.
- the side information Y may be used in the neural network with guide information for guided restoration.
- 11 and its variations is to, given a source image represented by the source X and a degraded image represented by the input Y (also referred to as degraded source data), send minimal guide information from the source X that allows the side information Y to be transformed to X d , where X d is closer to the source X than to the side information Y.
- a conventional encoder pipeline may encode a bitstream, which produces a base layer reconstruction. The base layer reconstruction may be used as the side information Y, while separate guide information provided by the source X yields a restored signal X d (also referred to as the reconstructed source data).
- the source (e.g., the input) X 1102 is input to an encoder 1104 that incorporates a decoder 1108 for reconstruction of the source X 1102 , which is the reconstructed source or output X d 1110 .
- the encoder 1104 and the decoder 1108 may comprise one or more neural networks that embody a machine-learning model that can be developed according to the teachings herein.
- the encoder may be referred to a neural network encoder 1104
- the decoder 1108 may be referred to as a neural network decoder 1108 .
- the machine-learning model may be trained to get close to the optimum rate distortion function of the source information, such as a source block.
- the neural network(s) may be trained so that the reconstructed source X d 1110 is substantially similar to the source X 1102 .
- the reconstructed source X d 1110 is substantially similar to the source X 1102 when an encoding cost is minimized.
- the encoding cost may be a rate-distortion value in some implementations.
- the objective function R X/Y (D) to which the neural network(s) are trained is the rate R to transmit the source X 1102 given known side information Y at a distortion D.
- the codec 1100 can produce an output, or compressed, bitstream for transmission to a decoder, or for storage.
- the compressed bitstream may be generated by quantizing block residuals from the encoder 1104 using the quantizer 1106 , and entropy coding the quantized residuals using the entropy coder 1112 .
- the block residuals may or may not be transformed.
- the quantizer 1106 may operate similarly to the quantization stage 406 of FIG. 4 .
- the entropy coder 1112 may operate similarly to the entropy encoding stage 408 of FIG. 4 .
- the bitstream from the entropy coder 1112 may be transmitted to a decoder, such as one structured similarly to the decoder 1108 .
- the codec 1100 receives side information Y as input, examples of which are described below.
- side information is information that is correlated with the source X, and is available to both an encoder and decoder without modification by the neural network(s) thereof.
- the available side information is provided to the neural network to derive guided information that, together with the side information, can reconstruct the source. In this way, the guided information may be considered enhancement information.
- the structure of the codec 1100 provides a powerful framework that can achieve many hybrid video encoding architectures by changing the side information Y.
- FIG. 12 is a block diagram of a neural network that can be used to implement the codec of FIG. 11 .
- the neural network may comprise a CNN and/or a fully-connected neural network.
- constraints are added to the neural network structure so as to pass through the side information Y from the input to the encoder and to a single (e.g., the first) layer of the decoder.
- the machine-learning model such as a classification deep-learning model, includes two main portions: a feature-extraction portion and a classification portion.
- the feature-extraction portion detects features of the model.
- the classification portion attempts to classify the detected features into a desired response.
- Each of the portions can include one or more layers and/or one or more operations.
- classification is used herein to refer to the one or more of the layers that outputs one or more values from the model.
- the output may be a discrete value, such as a class or a category.
- the output may be a continuous value (e.g., a rate value, a distortion value, a RD cost value).
- the classification portion may be appropriately termed a regression portion.
- a CNN is an example of a machine-learning model.
- the feature-extraction portion often includes a set of convolutional operations.
- the convolution operations may be a series of filters that are used to filter an input image based on a filter (typically a square of size k, without loss of generality). For example, and in the context of machine vision, these filters can be used to find features in an input image.
- the features can include, for example, edges, corners, endpoints, and so on.
- later convolutional operations can find higher-level features. It is noted that the term “features” is used in two different contexts within this disclosure.
- features can be extracted, from an input image or block, by the feature-extraction portion of a CNN.
- features can be calculated (e.g., derived) from an input block and used as inputs to a machine-learning model. Context makes clear which use of the term “features” is intended.
- the classification (e.g., regression) portion can be a set of fully connected layers.
- the fully connected layers can be thought of as looking at all the input features of an image in order to generate a high-level classifier.
- stages e.g., a series
- of high-level classifiers eventually generate the desired regression output.
- a CNN may be composed of a number of convolutional operations (e.g., the feature-extraction portion) followed by a number of fully connected layers.
- the number of operations of each type and their respective sizes may be determined during the training phase of the machine learning.
- additional layers and/or operations can be included in each portion. For example, combinations of Pooling, MaxPooling, Dropout, Activation, Normalization, BatchNormalization, and other operations can be grouped with convolution operations (i.e., in the feature-extraction portion) and/or the fully connected operation (i.e., in the classification portion).
- the fully connected layers may be referred to as Dense operations.
- a convolution operation can use a SeparableConvolution2D or Convolution2D operation.
- a convolution layer can be a group of operations starting with a Convolution2D or SeparableConvolution2D operation followed by zero or more operations (e.g., Pooling, Dropout, Activation, Normalization, BatchNormalization, other operations, or a combination thereof), until another convolutional layer, a Dense operation, or the output of the CNN is reached.
- operations e.g., Pooling, Dropout, Activation, Normalization, BatchNormalization, other operations, or a combination thereof
- a Dense layer can be a group of operations or layers starting with a Dense operation (i.e., a fully connected layer) followed by zero or more operations (e.g., Pooling, Dropout, Activation, Normalization, BatchNormalization, other operations, or a combination thereof) until another convolution layer, another Dense layer, or the output of the network is reached.
- a Dense operation i.e., a fully connected layer
- zero or more operations e.g., Pooling, Dropout, Activation, Normalization, BatchNormalization, other operations, or a combination thereof
- the boundary between feature extraction based on convolutional networks and a feature classification using Dense operations can be marked by a Flatten operation, which flattens the multidimensional matrix from the feature extraction into a vector.
- Each of the fully connected operations is a linear operation in which every input is connected to every output by a weight.
- a fully connected layer with N number of inputs and M outputs can have a total of N ⁇ M weights.
- a Dense operation may be followed by a non-linear activation function to generate an output of that layer.
- the first hidden layer 1200 A may be a feature-extraction layer
- the second hidden layer 1200 B and third hidden layer 1200 C may be classification layers.
- Data of the source X comprises input data from the video stream.
- the input data can include pixel data, such as luma or chroma data, position data, such as x- and y-coordinates, etc.
- the side information Y is provided to the first hidden layer 1200 A for feature extraction.
- the resulting extracted features are then used for classification at the second hidden layer 1200 B.
- the output to the quantizers 1106 comprises block residuals (e.g., for the luma and each of the chroma blocks) that may or may not be transformed as described previously. This is by example only, and other information needed to reconstruct the blocks may also be transmitted (e.g., the partitioning, etc.).
- the encoder 1104 passes through the side information Y to a single, here the first, layer (e.g., the third hidden layer 1200 C) of the decoder 1108 . That is, the side information Y passes through the layers of the encoder 1104 , after being used for feature extraction in the first hidden layer 1200 A, so as to be used in reconstruction in the first layer of the decoder 1108 .
- the disclosure herein means transmitting the side information Y, or whatever information is needed to recreate the side information Y, from the encoder 1104 to the decoder 1108 .
- the side information Y passes through the hidden layers of the encoder 1104 to the decoder 1108 .
- the side information Y may jump (or bypass) one or more layers of the encoder 1104 as described below in regards to FIG. 13 .
- the neural network may be referred to as a constrained network because the neural network is constrained by the side information Y. That is, a layer in each of the encoder 1104 and the decoder 1108 relies upon the side information Y.
- FIG. 13 is a block diagram of another neural network that can be used to implement the codec of FIG. 11 .
- the neural network may comprise a CNN and/or a fully-connected neural network similar to that described with regard to FIG. 12 .
- constraints are added to the neural network structure so as to pass through the side information Y from the input to the encoder and to the first layer of the decoder.
- the first hidden layer 1300 A may be a feature-extraction layer
- the second hidden layer 1300 B and third hidden layer 1300 C may be classification layers.
- Data of the source X comprises input data from the video stream.
- the input data can include pixel data, such as luma or chroma data, position data, such as x- and y-coordinates, etc.
- the side information Y is provided to the first hidden layer 1300 A for feature extraction.
- the resulting extracted features are then used for classification at the second hidden layer 1300 B.
- the output to the quantizers 1106 may comprise block residuals (e.g., for the luma and each of the chroma blocks) that may or may not be transformed as described previously. Information needed to reconstruct the blocks may also be transmitted (e.g., the partitioning, etc.).
- the encoder 1104 passes the side information Y to the first layer (e.g., the third hidden layer 1300 C) of the decoder 1108 .
- the side information Y passes through one or more hidden layers after the first layer of the encoder 1104 so as to be used in reconstruction in the first layer of the decoder 1108 .
- the side information Y bypasses one or more hidden layers of the encoder 1104 of FIG. 13 so as to pass through the side information Y, or information needed to recreate the side information Y, to the first layer of the decoder 1108 .
- the third hidden layer 1300 C uses the side information Y, together with the output of the quantizers 1106 , as input, and provides the reconstructed source or output X d 1110 .
- FIG. 13 is another example of a constrained network, in that a layer of each of the encoder 1104 and the decoder 1108 relies upon the side information Y.
- a neural network includes at least one, and often multiple layers, hidden or otherwise. Accordingly, more than three layers or less than three layers may be used in either implementation.
- the number of layers used by each of the encoder 1104 and decoder 1108 may differ from that shown.
- the decoder 1108 may include one or more additional layers subsequent to the third hidden layer 1300 C, where the subsequent layers do not receive the side information Y as input, and instead receive input from the third hidden layer 1200 C.
- FIG. 14 A block diagram of a variation in the example of the neural network of FIG. 13 in shown in FIG. 14 .
- the neural network in FIG. 14 includes at least four hidden layers 1400 A, 1400 B, 1400 C, and 1400 D.
- the first hidden layer 1400 A may be a feature-extraction layer
- the second hidden layer 1400 B, the third hidden layer 1400 C, and the fourth hidden layer 1400 D may be classification layers.
- convolution operations are used for each of feature extraction and classification, instead of using fully-connected layers for classification. Hence, this may be referred to as a convolution-only neural network. This is not required, and fully-connected layers may also or alternatively be used for classification.
- Data of the source X comprises input data from the video stream.
- the input data 1102 can include pixel data, such as luma or chroma data, position data, such as x- and y-coordinates, etc.
- the side information Y is provided to the first hidden layer 1400 A for feature extraction.
- the resulting extracted features are then used for classification at the second hidden layer 1400 B.
- the output to the quantizers 1106 may comprise block residuals (e.g., for the luma and each of the chroma blocks) that may or may not be transformed as described previously. Information needed to reconstruct the blocks may also be transmitted (e.g., the partitioning, etc.).
- the encoder 1104 passes the side information Y to a single layer (e.g., the first layer) of the decoder 1108 . More specifically, and as in FIG. 13 , the side information Y bypasses one or more hidden layers of the encoder 1104 of FIG. 14 so as to pass through the side information Y, or information needed to recreate the side information Y, to the decoder 1108 .
- FIG. 14 differs from FIG. 13 in that, among other things, FIG. 14 includes an expander layer 1410 that may be considered part of the decoder 1108 .
- the expander layer 1410 may be another hidden, convolution layer.
- the expander layer 1410 receives the output from the quantizers 1106 as input, and expands the information from that output for use as input to the third hidden layer 1300 C.
- the guided information obtained from the source X for input to the decoder 1108 is desirably minimized through the training of the codec, such as the codec of FIG. 14 .
- the expander layer 1410 may be incorporated to allow the encoder 1104 to send less information than the codec of FIG. 12 or 13 .
- the expander layer 1410 receives the output from the quantizers 1106 , and that information is expanded, that is, the amount of data is increased through convolution operations to form the guided information for input to the first layer (e.g., the third hidden layer 1400 C) of the decoder 1108 .
- the amount of data may be increased by increasing the resolution of the guided information from the encoder 1104 .
- the expander layer 1410 may be referred to as part of the decoder 1108 , it is not considered to be the first layer of the decoder 1108 . Instead, it is considered a pre-layer that generates the guided information for input to the first layer of the decoder.
- the first layer of the decoder is the first layer where both the guided information and the side information Y are input.
- the third hidden layer 1400 C uses the side information Y, together with the output of the expander layer 1410 , as input, and performs convolution operations to provide output to the fourth hidden layer 1400 D.
- the fourth hidden layer 1400 D performs convolution operations on the output from the third hidden layer 1400 C to output the reconstructed source or output X d 1110 .
- FIG. 14 may be described as a constrained network because a layer of each of the encoder 1104 and the decoder 1108 relies upon (i.e., is constrained by) the side information Y.
- the expander layer 1410 may perform upscaling.
- the expander layer 1410 may be referred to as an upscaling layer.
- the presence of the layer 1410 illustrates that the side information Y and the input data 1102 (or correspondingly the output of the quantizers 1106 ) may not be of the same resolution.
- the layer 1410 may instead be a reducing layer, that is, a layer that reduces the amount of data through convolution operations to form the guided information for input to the first layer (e.g., the third hidden layer 1400 C) of the decoder 1108 .
- the amount of data may be decreased by decreasing the resolution of the guided information from the encoder 1104 .
- the layer 1410 may be referred to as a downscaling layer. More generally, the layer 1410 may be referred to as a resolution adjustment layer or normalizing layer.
- the resolution adjustment layer may be a trained layer (implemented using machine learning).
- the resolution adjustment layer may perform any one or more of standard algorithms to change the resolution of its input in accordance with the resolution of the side information Y. It is also possible for the layer 1410 to be omitted.
- the expanding or reducing functions otherwise performed by the layer 1410 may be performed by the third hidden layer 1400 C by training that layer using the two data sources at different resolutions as input with the output to the fourth hidden layer 1400 D being at full or reduced resolution, whichever is indicated for the reconstructed source or output X d 1110 .
- each of FIGS. 11 to 14 provides for end-to-end training of the neural network. In this way, the network automatically learns how to restore Y to get Xd as well as what minimal information to send as guide.
- FIG. 15 is a block diagram of an alternative example of a codec 1500 comprising a neural network with side information according to implementations of this disclosure.
- the codec 1500 is similar to the codec 1100 except for the structure of the neural network that forms the encoder 1504 and the decoder 1508 .
- the input which may be the same source X 1102 of FIG. 11 , is input to an encoder 1504 that incorporates a decoder 1508 for reconstruction of the source X 1102 , which is the reconstructed output or source X d 1110 .
- the encoder 1504 and the decoder 1508 may comprise one or more neural networks that embody a machine-learning model that can be developed according to the teachings herein.
- the encoder 1504 may be referred to as a neural network encoder
- the decoder 1508 may be referred to as a neural network decoder.
- the codec 1500 produces the output, or compressed, bitstream R X/Y (D) for transmission to a decoder, or for storage.
- the compressed bitstream R X/Y (D) may be generated by quantizing block residuals from the encoder 1504 using the quantizer 1106 , and entropy coding the quantized residuals using the entropy coder 1112 .
- the block residuals may or may not be transformed.
- the quantizer 1106 may operate similarly to the quantization stage 406 of FIG. 4 .
- the entropy coder 1112 may operate similarly to the entropy encoding stage 408 of FIG. 4 .
- the codec 1500 receives side information Y as input, examples of which are described below.
- structural constraints are not imposed on the neural network to take account of the side information Y.
- a simplified model is used whereby side information is used as input by generating a difference X ⁇ T(Y) that is coded using the neural network formed of the encoder 1504 and the decoder 1508 . That is, the difference X ⁇ T(Y) is used as input to the first layer of the encoder 1504 .
- the machine-learning model may be trained to get close to the optimum rate distortion function of the difference.
- the neural network(s) may be trained so that the output of the decoder 1508 is substantially similar to the difference X ⁇ T(Y). For example, the output is substantially similar to the difference X ⁇ T(Y) when an encoding cost is minimized.
- the encoding cost may be a rate-distortion value in some implementations.
- the side information T(Y) is defined by a deterministic transformation of Y such that the information T(Y) is at the same resolution as the source X 1102 . Accordingly, the side information T(Y) may be used to generate the reconstructed source or output X d 1110 by adding the side information T(Y) to the output of the decoder 1508 .
- the codec 1500 may have a similar structure to the codec 1100 as shown in the examples of FIGS. 12 - 14 , except for the inclusion of structure allowing the pass through of the side information Y.
- An advantage of the codec 1500 over the codec 1100 is that the structure of the neural network is easier.
- a variety of neural network application programming interfaces (APIs) may be used (e.g., trained) due to the lack of side information constraints on the neural network structure.
- the codec 1500 may have reduced performance (e.g., higher rate-distortion values) due to loss of the structure in the side information Y.
- another decoder having a similar structure to that of the decoder 1108 or 1508 may be used to reconstruct the source data/information using the side information and source data as inputs into (e.g., at least a first hidden layer of) the decoder.
- the side information Y is described as being derived from conventional encoding methods.
- the side information Y may be any side information that is correlated with the source information.
- the side information Y may be a product of the encoder itself, such as where only portions of the encoder are implemented via a neural network. That is, the encoder may be a hybrid encoder that includes certain block-based components as described with the example of FIGS. 3 - 5 , or object-based components as are known to those skilled in the art, combined with one or more neural networks. In this example, the hybrid encoder may itself produce (and use) the side information.
- the side information Y may also be determined by a second encoder, where the second encoder provides the side information Y to the first (e.g., neural network) encoder.
- the side information Y (or a function thereof) is used with a deep neural network that may have structural constraints that enforce the availability of the side information Y on the decoder side.
- the information derived from the neural network layer(s) may be considered guided or enhancement information for the video being coded, as described briefly above. Many variations of the side information Y, and hence the enhancement information, are possible.
- the side information Y may be used in prediction residue (residual) coding and reconstruction.
- the side information Y may be a full resolution predictor or prediction signal from a traditional motion-based predictor (e.g., a prediction block from inter prediction, such as performed at intra/inter-prediction stage 402 ).
- the neural network may learn, for example, the optimal residue transform (i.e., the transform that produces the lowest rate-distortion value) for the residual resulting from the source X (e.g., the block) and the full resolution predictor.
- the full resolution predictor may also improve reconstruction by the decoder, due to the availability of the structure (i.e., the features) of the full resolution predictor.
- a hybrid video encoder results where only the residue coding in a conventional encoder is modified. Because conventional video compression codes the prediction residue independently of the prediction, the inclusion of the prediction as side information with the neural network may provide a better reconstruction of the original video data.
- the neural network may be used for restoration with guided information.
- a conventional encoder may be used to encode a bitstream, which may be referred to as a base layer.
- the base layer reconstruction may be used as the side information Y to refine the source X in a separate guided layer. In this way, a form of scalable encoding is formed.
- a conventional encoder may encode a base layer bitstream at reduced resolution.
- the reduced-resolution reconstruction from the conventional base layer e.g., the per-frame reduced resolution reconstruction
- side information Y e.g., the per-frame reduced resolution reconstruction
- This process generates a form of spatial scalable encoding where the motion information is at the reduced resolution only. Even in such a design, it is expected that advantages (e.g., in rate-distortion values) will result for small reduction ratios, low bitrates, or both.
- the base layer bitstream may be decoded independently of the enhancement neural network layer. Some loss in coding efficiency is expected because of the loss of precision in the motion information. However, this allows a design whereby the training of the neural networks can be open-looped. This may be implemented with two encoders as described above, for example.
- in-loop super-resolution may be implemented. That is, the output of the full-resolution enhancement neural network layer may be used to refresh frame buffers used to encode subsequent frames.
- the reference frame buffers may always be at full resolution (e.g., the reference frame buffers are able to store full-resolution reference frames).
- the coded frames at lower resolution can be use scaled motion compensation.
- This in-loop design in contrast to the open-looped design above, may be implemented using a single hybrid encoder.
- the artificial neural networks with side information may also be used to generate multimode predictors.
- source X may be the data to predict
- side information Y may be the data used to predict the source X.
- the side information Y may include neighboring pixels for intra prediction.
- the neural network would then be trained by using the results of the exhaustive analysis previously described as input, where the results comprise the most efficient intra-prediction mode for respective training blocks (e.g., the intra-prediction mode resulting in the lowest encoding cost).
- Fully-connected layers may be desirable in this design for classification (e.g., instead of convolution layers), because such a design may be used to more tightly control the size (e.g., the number of parameters) of the layers.
- the available modes in the multimode predictor may be considered to limit the passing of just a few bits of information.
- the codecs 1100 and 1500 of FIGS. 11 - 15 include both an encoder and a decoder.
- This arrangement represents a structure for training the neural network, and may represent an encoder at a transmitting station 102 .
- the decoder may correspond in structure to a trained implementation of the decoder 1108 or the decoder 1508 , together with another decoder or portions of another decoder, such as the decoder 500 of FIG. 5 .
- the decoder can use the output of the entropy coder 1112 to generate the side information for inclusion in the first layer of the neural network portion of the decoder, or can receive the side information as signals separate from the signals from the entropy coder 1112 for inclusion in the first layer of the neural network portion of the decoder.
- An artificial neural network with side information as described above provides a powerful framework that can address many use cases of interest.
- the neural network has structural constraints that enforce availability of the side information on the decoder side.
- the side information may be derived from conventional encoding methods or any other degrading process (i.e., one that degrades the source). Many variations are possible depending on what the side information is.
- example or “implementation” are used herein to mean serving as an example, instance, or illustration. Any aspect or design described herein as “example” or “implementation” is not necessarily to be construed as being preferred or advantageous over other aspects or designs. Rather, use of the words “example” or “implementation” is intended to present concepts in a concrete fashion.
- the term “or” is intended to mean an inclusive “or” rather than an exclusive “or.” That is, unless specified otherwise or clearly indicated otherwise by the context, “X includes A or B” is intended to mean any of the natural inclusive permutations thereof.
- Implementations of the transmitting station 102 and/or the receiving station 106 can be realized in hardware, software, or any combination thereof.
- the hardware can include, for example, computers, intellectual property (IP) cores, application-specific integrated circuits (ASICs), programmable logic arrays, optical processors, programmable logic controllers, microcode, microcontrollers, servers, microprocessors, digital signal processors, or any other suitable circuit.
- IP intellectual property
- ASICs application-specific integrated circuits
- programmable logic arrays optical processors
- programmable logic controllers programmable logic controllers
- microcode microcontrollers
- servers microprocessors, digital signal processors, or any other suitable circuit.
- signal processors should be understood as encompassing any of the foregoing hardware, either singly or in combination.
- signals and “data” are used interchangeably. Further, portions of the transmitting station 102 and the receiving station 106 do not necessarily have to be implemented in the same manner.
- the transmitting station 102 or the receiving station 106 can be implemented using a general-purpose computer or general-purpose processor with a computer program that, when executed, carries out any of the respective methods, algorithms, and/or instructions described herein.
- a special-purpose computer/processor which can contain other hardware for carrying out any of the methods, algorithms, or instructions described herein, can be utilized.
- the transmitting station 102 and the receiving station 106 can, for example, be implemented on computers in a video conferencing system.
- the transmitting station 102 can be implemented on a server, and the receiving station 106 can be implemented on a device separate from the server, such as a handheld communications device.
- the transmitting station 102 using an encoder 400 , can encode content into an encoded video signal and transmit the encoded video signal to the communications device.
- the communications device can then decode the encoded video signal using a decoder 500 .
- the communications device can decode content stored locally on the communications device, for example, content that was not transmitted by the transmitting station 102 .
- Other transmitting station 102 and receiving station 106 implementation schemes are available.
- the receiving station 106 can be a generally stationary personal computer rather than a portable communications device, and/or a device including an encoder 400 may also include a decoder 500 .
- implementations of the present disclosure can take the form of a computer program product accessible from, for example, a tangible computer-usable or computer-readable medium.
- a computer-usable or computer-readable medium can be any device that can, for example, tangibly contain, store, communicate, or transport the program for use by or in connection with any processor.
- the medium can be, for example, an electronic, magnetic, optical, electromagnetic, or semiconductor device. Other suitable mediums are also available.
Abstract
Description
λmode=0.85×2(QP-12)/3 (1)
λmode=0.85·Q H263 2 (2)
rdmult=88·q 2/24 (3)
λmode=0.12·Q AV1 2/256 (4)
DISTORTION+λmode×RATE, (5)
Claims (20)
Priority Applications (4)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US16/516,784 US11689726B2 (en) | 2018-12-05 | 2019-07-19 | Hybrid motion-compensated neural network with side-information based video coding |
PCT/US2019/059597 WO2020117412A1 (en) | 2018-12-05 | 2019-11-04 | Hybrid motion-compensated neural network with side-information based video coding |
CN201980015244.8A CN111801945A (en) | 2018-12-05 | 2019-11-04 | Hybrid motion compensated neural network with side information based video coding |
EP19835893.9A EP3744096A1 (en) | 2018-12-05 | 2019-11-04 | Hybrid motion-compensated neural network with side-information based video coding |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201862775481P | 2018-12-05 | 2018-12-05 | |
US16/516,784 US11689726B2 (en) | 2018-12-05 | 2019-07-19 | Hybrid motion-compensated neural network with side-information based video coding |
Publications (2)
Publication Number | Publication Date |
---|---|
US20200186809A1 US20200186809A1 (en) | 2020-06-11 |
US11689726B2 true US11689726B2 (en) | 2023-06-27 |
Family
ID=70972675
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/516,784 Active US11689726B2 (en) | 2018-12-05 | 2019-07-19 | Hybrid motion-compensated neural network with side-information based video coding |
Country Status (4)
Country | Link |
---|---|
US (1) | US11689726B2 (en) |
EP (1) | EP3744096A1 (en) |
CN (1) | CN111801945A (en) |
WO (1) | WO2020117412A1 (en) |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20200327702A1 (en) * | 2020-06-26 | 2020-10-15 | Intel Corporation | Video codec assisted real-time video enhancement using deep learning |
Families Citing this family (20)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
GB2575628A (en) * | 2018-07-09 | 2020-01-22 | Nokia Technologies Oy | Video processing |
KR20200082227A (en) * | 2018-12-28 | 2020-07-08 | 한국전자통신연구원 | Method and device for determining loss function for audio signal |
US10886943B2 (en) | 2019-03-18 | 2021-01-05 | Samsung Electronics Co., Ltd | Method and apparatus for variable rate compression with a conditional autoencoder |
CN112449140B (en) * | 2019-08-29 | 2021-09-14 | 华为技术有限公司 | Video super-resolution processing method and device |
US11140422B2 (en) * | 2019-09-25 | 2021-10-05 | Microsoft Technology Licensing, Llc | Thin-cloud system for live streaming content |
CN111161316B (en) * | 2019-12-18 | 2023-08-01 | 深圳云天励飞技术有限公司 | Target object tracking method and device and terminal equipment |
US11948090B2 (en) * | 2020-03-06 | 2024-04-02 | Tencent America LLC | Method and apparatus for video coding |
US11496151B1 (en) * | 2020-04-24 | 2022-11-08 | Tencent America LLC | Neural network model compression with block partitioning |
CN111860597B (en) * | 2020-06-17 | 2021-09-07 | 腾讯科技（深圳）有限公司 | Video information processing method and device, electronic equipment and storage medium |
US20220067509A1 (en) * | 2020-09-02 | 2022-03-03 | Alibaba Group Holding Limited | System and method for learning from partial compressed representation |
CN112365896B (en) * | 2020-10-15 | 2022-06-14 | 武汉大学 | Object-oriented encoding method based on stack type sparse self-encoder |
US11582442B1 (en) * | 2020-12-03 | 2023-02-14 | Amazon Technologies, Inc. | Video encoding mode selection by a hierarchy of machine learning models |
WO2022159073A1 (en) * | 2021-01-19 | 2022-07-28 | Google Llc | Video coding with guided machine learning restoration |
US11496746B2 (en) * | 2021-02-02 | 2022-11-08 | Qualcomm Incorporated | Machine learning based rate-distortion optimizer for video compression |
US11582443B1 (en) * | 2021-02-18 | 2023-02-14 | Meta Platforms, Inc. | Architecture to adapt cumulative distribution functions for mode decision in video encoding |
CN117441336A (en) * | 2021-06-11 | 2024-01-23 | Oppo广东移动通信有限公司 | Video encoding and decoding method, device, system and storage medium |
US11909956B2 (en) * | 2021-06-15 | 2024-02-20 | Tencent America LLC | DNN-based cross component prediction |
US20230090440A1 (en) * | 2021-09-21 | 2023-03-23 | Qualcomm Incorporated | Lossy Compressed Feedback For Multiple Incremental Redundancy Scheme (MIRS) |
WO2023051654A1 (en) * | 2021-09-29 | 2023-04-06 | Beijing Bytedance Network Technology Co., Ltd. | Method, apparatus, and medium for video processing |
CN114363632B (en) * | 2021-12-10 | 2023-05-16 | 浙江大华技术股份有限公司 | Intra-frame prediction method, encoding and decoding method, encoder and decoder, system, electronic device, and storage medium |
Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20150016510A1 (en) * | 2013-07-10 | 2015-01-15 | Microsoft Corporation | Region-of-Interest Aware Video Coding |
US20170264902A1 (en) | 2016-03-09 | 2017-09-14 | Sony Corporation | System and method for video processing based on quantization parameter |
US20180307984A1 (en) * | 2017-04-24 | 2018-10-25 | Intel Corporation | Dynamic distributed training of machine learning models |
US20190294980A1 (en) * | 2016-05-26 | 2019-09-26 | Ogma Intelligent Systems Corp. | Processing time-varying data streams using sparse predictive hierarchies |
US20200059669A1 (en) * | 2017-04-25 | 2020-02-20 | Panasonic Intellectual Property Corporation Of America | Encoder, decoder, encoding method, and decoding method |
Family Cites Families (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
GB2533109B (en) * | 2014-12-09 | 2018-11-28 | Gurulogic Microsystems Oy | Encoder, decoder and method for data |
US10034005B2 (en) * | 2015-06-05 | 2018-07-24 | Sony Corporation | Banding prediction for video encoding |
CN107925762B (en) * | 2015-09-03 | 2020-11-27 | 联发科技股份有限公司 | Video coding and decoding processing method and device based on neural network |
US10347294B2 (en) * | 2016-06-30 | 2019-07-09 | Google Llc | Generating moving thumbnails for videos |
-
2019
- 2019-07-19 US US16/516,784 patent/US11689726B2/en active Active
- 2019-11-04 EP EP19835893.9A patent/EP3744096A1/en active Pending
- 2019-11-04 WO PCT/US2019/059597 patent/WO2020117412A1/en unknown
- 2019-11-04 CN CN201980015244.8A patent/CN111801945A/en active Pending
Patent Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20150016510A1 (en) * | 2013-07-10 | 2015-01-15 | Microsoft Corporation | Region-of-Interest Aware Video Coding |
US20170264902A1 (en) | 2016-03-09 | 2017-09-14 | Sony Corporation | System and method for video processing based on quantization parameter |
US20190294980A1 (en) * | 2016-05-26 | 2019-09-26 | Ogma Intelligent Systems Corp. | Processing time-varying data streams using sparse predictive hierarchies |
US20180307984A1 (en) * | 2017-04-24 | 2018-10-25 | Intel Corporation | Dynamic distributed training of machine learning models |
US20200059669A1 (en) * | 2017-04-25 | 2020-02-20 | Panasonic Intellectual Property Corporation Of America | Encoder, decoder, encoding method, and decoding method |
Non-Patent Citations (18)
Title |
---|
"Introduction to Video Coding Part 1: Transform Coding", Mozilla, Mar. 2012, 171 pp. |
"Overview VP7 Data Format and Decoder", Version 1.5, On2 Technologies, Inc., Mar. 28, 2005, 65 pp. |
"VP6 Bitstream and Decoder Specification", Version 1.02, On2 Technologies, Inc., Aug. 17, 2006, 88 pp. |
"VP6 Bitstream and Decoder Specification", Version 1.03, On2 Technologies, Inc., Oct. 29, 2007, 95 pp. |
"VP8 Data Format and Decoding Guide, WebM Project", Google On2, Dec. 1, 2010, 103 pp. |
Bankoski et al., "VP8 Data Format and Decoding Guide draft-bankoski-vp8-bitstream-02", Network Working Group, Internet-Draft, May 18, 2011, 288 pp. |
Bankoski et al., "VP8 Data Format and Decoding Guide", Independent Submission RFC 6389, Nov. 2011, 305 pp. |
Bankoski, et al., "Technical Overview of VP8, An Open Source Video Codec for the Web", Jul. 11, 2011, 6 pp. |
International Search Report and Written Opinion of International Application No. PCT/US2019059597 dated Mar. 13, 2020; 15 pages. |
Pfaff J et al.; "Neural netowrk based intra prediction for video coding"; Proceedings of SPIE; issue 0277-786x vol. 10752; Sep. 17, 2018; pp. 1075213. |
Series H: Audiovisual and Multimedia Systems, Coding of moving video: Implementors Guide for H.264: Advanced video coding for generic audiovisual services, International Telecommunication Union, Jul. 30, 2010, 15 pp. |
Series H: Audiovisual and Multimedia Systems, Infrastructure of audiovisual services—Coding of moving video, Advanced video coding for generic audiovisual services, Amendment 1: Support of additional colour spaces and removal of the High 4:4:4 Profile, International Telecommunication Union, Jun. 2006, 16 pp. |
Series H: Audiovisual and Multimedia Systems, Infrastructure of audiovisual services—Coding of moving video, Advanced video coding for generic audiovisual services, International Telecommunication Union, Version 11, Mar. 2009. 670 pp. |
Series H: Audiovisual and Multimedia Systems, Infrastructure of audiovisual services—Coding of moving video, Advanced video coding for generic audiovisual services, International Telecommunication Union, Version 12, Mar. 2010, 676 pp. |
Series H: Audiovisual and Multimedia Systems, Infrastructure of audiovisual services—Coding of moving video, Advanced video coding for generic audiovisual services, Version 1, International Telecommunication Union, May 2003, 282 pp. |
Series H: Audiovisual and Multimedia Systems, Infrastructure of audiovisual services—Coding of moving video, Advanced video coding for generic audiovisual services, Version 3, International Telecommunication Union, Mar. 2005, 343 pp. |
Series H: Audiovisual and Multimedia Systems, Infrastructure of audiovisual services—Coding of moving video, Advanced video coding for generic audiovisual services, Version 8, International Telecommunication Union, Nov. 1, 2007, 564 pp. |
Series H: Audiovisual and Multimedia Systems, Infrastructure of audiovisual services—Coding of moving video, Amendment 2: New profiles for professional applications, International Telecommunication Union, Apr. 2007, 75 pp. |
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20200327702A1 (en) * | 2020-06-26 | 2020-10-15 | Intel Corporation | Video codec assisted real-time video enhancement using deep learning |
US11889096B2 (en) * | 2020-06-26 | 2024-01-30 | Intel Corporation | Video codec assisted real-time video enhancement using deep learning |
Also Published As
Publication number | Publication date |
---|---|
CN111801945A (en) | 2020-10-20 |
EP3744096A1 (en) | 2020-12-02 |
WO2020117412A1 (en) | 2020-06-11 |
US20200186809A1 (en) | 2020-06-11 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11689726B2 (en) | Hybrid motion-compensated neural network with side-information based video coding | |
US11310501B2 (en) | Efficient use of quantization parameters in machine-learning models for video coding | |
US11310498B2 (en) | Receptive-field-conforming convolutional models for video coding | |
US10848765B2 (en) | Rate/distortion/RDcost modeling with machine learning | |
US11025907B2 (en) | Receptive-field-conforming convolution models for video coding | |
US10142652B2 (en) | Entropy coding motion vector residuals obtained using reference motion vectors | |
US10009625B2 (en) | Low-latency two-pass video coding | |
US11956447B2 (en) | Using rate distortion cost as a loss function for deep learning | |
US11343528B2 (en) | Compound prediction for video coding | |
US10582212B2 (en) | Warped reference motion vectors for video compression | |
US20220415039A1 (en) | Systems and Techniques for Retraining Models for Video Quality Assessment and for Transcoding Using the Retrained Models | |
US10419777B2 (en) | Non-causal overlapped block prediction in variable block size video coding | |
US11849113B2 (en) | Quantization constrained neural image coding | |
US20230007284A1 (en) | Ultra Light Models and Decision Fusion for Fast Video Coding | |
KR20200005748A (en) | Complex Motion-Compensation Prediction |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
FEPP | Fee payment procedure |
Free format text: ENTITY STATUS SET TO UNDISCOUNTED (ORIGINAL EVENT CODE: BIG.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:MUKHERJEE, DEBARGHA;JOSHI, URVANG;CHEN, YUE;AND OTHERS;SIGNING DATES FROM 20181204 TO 20181218;REEL/FRAME:049826/0419 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: FINAL REJECTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE AFTER FINAL ACTION FORWARDED TO EXAMINER |
|
STCV | Information on status: appeal procedure |
Free format text: EXAMINER'S ANSWER TO APPEAL BRIEF MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: FINAL REJECTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: ADVISORY ACTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |