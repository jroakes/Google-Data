CN111602148B - Regularized neural network architecture search - Google Patents
Regularized neural network architecture search Download PDFInfo
- Publication number
- CN111602148B CN111602148B CN201980008348.6A CN201980008348A CN111602148B CN 111602148 B CN111602148 B CN 111602148B CN 201980008348 A CN201980008348 A CN 201980008348A CN 111602148 B CN111602148 B CN 111602148B
- Authority
- CN
- China
- Prior art keywords
- architecture
- neural network
- training
- candidate
- data
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 238000013528 artificial neural network Methods 0.000 title claims abstract description 199
- 238000012549 training Methods 0.000 claims abstract description 127
- 238000000034 method Methods 0.000 claims abstract description 52
- 238000010801 machine learning Methods 0.000 claims abstract description 40
- 230000035772 mutation Effects 0.000 claims description 34
- 238000012545 processing Methods 0.000 claims description 23
- 238000003058 natural language processing Methods 0.000 claims description 3
- 238000005457 optimization Methods 0.000 description 22
- 230000008569 process Effects 0.000 description 21
- 238000004590 computer program Methods 0.000 description 16
- 230000009471 action Effects 0.000 description 11
- 238000004891 communication Methods 0.000 description 8
- 230000009467 reduction Effects 0.000 description 8
- 238000012360 testing method Methods 0.000 description 7
- 238000012986 modification Methods 0.000 description 5
- 230000004048 modification Effects 0.000 description 5
- 238000011176 pooling Methods 0.000 description 5
- 230000004044 response Effects 0.000 description 5
- 230000032683 aging Effects 0.000 description 4
- 230000000306 recurrent effect Effects 0.000 description 4
- 230000003993 interaction Effects 0.000 description 3
- 230000004913 activation Effects 0.000 description 2
- 238000013459 approach Methods 0.000 description 2
- 230000008901 benefit Effects 0.000 description 2
- 239000003795 chemical substances by application Substances 0.000 description 2
- 239000000470 constituent Substances 0.000 description 2
- 238000013527 convolutional neural network Methods 0.000 description 2
- 230000006870 function Effects 0.000 description 2
- 230000003287 optical effect Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 238000010200 validation analysis Methods 0.000 description 2
- 238000004458 analytical method Methods 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 239000000872 buffer Substances 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 230000001149 cognitive effect Effects 0.000 description 1
- 238000013461 design Methods 0.000 description 1
- 230000010339 dilation Effects 0.000 description 1
- 230000008451 emotion Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 238000013518 transcription Methods 0.000 description 1
- 230000035897 transcription Effects 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 238000013519 translation Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/086—Learning methods using evolutionary algorithms, e.g. genetic algorithms or genetic programming
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/082—Learning methods modifying the architecture, e.g. adding, deleting or silencing nodes or connections
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
Abstract
A method is described for receiving training data for training a Neural Network (NN) to perform machine learning tasks and for determining an optimized NN architecture for performing ML tasks using the training data. Determining an optimized NN architecture includes: maintaining global data including, for each candidate architecture in the population of candidate architectures, (i) data defining the candidate architecture, and (ii) data specifying how recently a neural network having the candidate architecture was trained when determining an optimized neural network architecture; and repeatedly performing a plurality of operations using each of the plurality of worker computing units to generate a new candidate architecture based on the selected candidate architecture having the best fitness measure, adding the new candidate architecture to the population, and removing the least recently trained candidate architecture from the population.
Description
Cross Reference to Related Applications
The present application claims priority from U.S. provisional application Ser. No. 62/625,923 filed on day 28, 2 in 2018. The disclosure of the previous application is considered to be part of the disclosure of the present application and is incorporated by reference in the disclosure of the present application.
Background
The present description relates to determining architecture of a neural network.
Neural networks are machine learning models that employ one or more layers of nonlinear units to predict output for a received input. Some neural networks include one or more hidden layers in addition to the output layer. The output of each hidden layer serves as an input to the next layer in the network, i.e., the next hidden layer or output layer. Each layer of the network generates an output from the received inputs based on the current values of the respective parameter sets.
Some neural networks are recurrent neural networks. A recurrent neural network is a neural network that receives an input sequence and generates an output sequence from the input sequence. In particular, the recurrent neural network may use some or all of the network internal states from the previous time step in calculating the output at the current time step. An example of a recurrent neural network is a Long Short Term (LSTM) neural network, which includes one or more LSTM memory blocks. Each LSTM memory block may include one or more cells, each cell including an input gate, a forget gate, and an output gate, allowing the cell to store the previous state of the cell, e.g., for generating the current activation or other components provided to the LSTM neural network.
Disclosure of Invention
In general, one innovative aspect of the subject matter described in this specification can be embodied in methods for determining an optimized neural network architecture for a neural network configured to perform machine learning tasks. The method comprises the following steps: receiving training data for training a neural network to perform a machine learning task, the training data comprising a plurality of training examples and a respective target output for each training example; and determining an optimized neural network architecture for performing the machine learning task using the training data, comprising: maintaining global data, the global data comprising, for each candidate architecture in a population of candidate architectures, (i) data defining the candidate architecture, and (ii) data specifying how recently a neural network having the candidate architecture was trained when determining an optimized neural network architecture; and repeatedly performing the following using each of the plurality of worker computing units that each operate asynchronously with each of the other worker computing units: selecting, by the worker computing unit, a plurality of candidate architectures from the population; training, for each selected candidate architecture and by the worker computing unit, a new neural network having the candidate architecture on a training subset of the training data to determine training values for parameters of the new neural network; determining, for each selected candidate architecture and by the worker computing unit, a fitness measure by evaluating performance of the trained new neural network on a validated subset of the training data; generating, by the worker computing unit, a new candidate architecture based on the selected candidate architecture having the best fitness measure; adding the new candidate architecture to the population; and removing the least recently trained candidate architecture from the population.
The method may further include providing data specifying an optimization architecture. The method may further include determining training values for parameters of the neural network having the optimized neural network architecture. The method may further include providing data specifying training parameters. The operations may further include training the new candidate architecture to determine trained parameter values; and associating training values of the parameters with the new candidate architectures in the overall data. Determining training values for parameters of a neural network having an optimized neural network architecture may include selecting training values associated with an architecture having an associated best-fit measure as training values for parameters of a neural network having an optimized neural network architecture. Determining training values for parameters of the neural network having the optimized neural network architecture may include further training the neural network having the optimized neural network architecture on more training data to determine training values. Training, for each selected candidate architecture and by the worker computing unit, a new neural network having the candidate architecture on a training subset of training data to determine training values for parameters of the new neural network may include training the new neural network starting from values associated with the candidate architecture in the overall data. The method may further include initializing a population with a plurality of default candidate architectures. For each candidate architecture, the data defining the candidate architecture may identify an architecture of one or more units, each unit being repeated multiple times to generate the candidate architecture. Generating, by the worker computing unit, a new candidate architecture based on the selected candidate architecture having the best fitness measure may include modifying the architecture for at least one of the units in the candidate architecture having the best fitness measure. The architecture of the modification unit may include: randomly selecting mutations from the set of mutations; and applying the randomly selected mutation to the architecture of the cell. Modifying the architecture of the cell may include using the mutant neural network to process data specifying a candidate architecture having a best fitness measure, wherein the mutant neural network has been trained to process network inputs including data for generating new candidate architectures. Determining an optimized neural network architecture for performing a machine learning task using training data may include selecting a candidate architecture with a best fit in the population as the optimized architecture. The machine learning task may be one or more of the following: image processing, image classification, speech recognition, and natural language processing.
Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods. A system of one or more computers may be configured to perform particular operations or actions by way of software, firmware, hardware, or any combination thereof installed on the system that, when operated, may cause the system to perform the actions. One or more computer programs may be configured to perform particular operations or actions by virtue of including instructions that, when executed by a data processing apparatus, cause the apparatus to perform the actions.
The subject matter described in this specification can be implemented in specific embodiments such that one or more of the following advantages are realized. By determining the architecture of the neural network using the aging evolution techniques described in this specification, the system can determine a network architecture that achieves or even exceeds the most advanced performance on any of a variety of machine learning tasks, e.g., image classification or another image processing task. In particular, the technique uses each of a plurality of worker computing units to generate a new architecture by mutating the corresponding candidate architecture with the best fitness measure at each iteration, adding the new architecture to the population of candidate architectures, and removing the least recently trained old architecture from the population. By removing the least recently trained architecture (or the oldest architecture) at each iteration, the system allows all candidate architectures in the population to have a short lifetime. Thus, the population is often fully updated, resulting in more diversity and more exploration, which results in better architecture search results while maintaining the efficiency of the system due to the simplicity of aging evolution. Furthermore, because only the best candidate architecture is selected for mutation, the described techniques allow the population of candidate architectures to improve significantly over time. Thus, the resulting optimized architecture for a neural network has better performance (e.g., better accuracy) when performing particular machine learning tasks than architectures generated by existing neural network architecture search methods. These techniques can determine such an optimized architecture while requiring minimal over-parameters and minimal to no user input. In addition, the system may determine this architecture in a manner that utilizes distributed processing, i.e., by distributing training operations among multiple worker computing units running asynchronously, to more quickly determine an optimized architecture. Operations for determining an optimized architecture have been adapted such that operations may be performed asynchronously and in parallel by distributed worker computing units to more efficiently determine an optimized architecture.
The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Drawings
FIG. 1 illustrates an architecture of an example neural network architecture optimization system.
Fig. 2A and 2B illustrate example candidate architectures for neural networks for performing machine learning tasks.
Fig. 3 shows an example architecture of a unit.
Fig. 4A and 4B show examples of mutations.
FIG. 5 is a flowchart of an example process for determining an optimized neural network architecture for performing machine learning tasks.
Like reference numbers and designations in the various drawings indicate like elements.
Detailed Description
The specification describes a neural network architecture optimization system implemented as a computer program on one or more computers at one or more locations that determines an optimal network architecture for a neural network configured to perform a particular machine learning task. Depending on the task, the neural network may be configured to receive any type of digital data input and generate any type of score, classification, or regression output based on the input.
For example, if the input to the neural network is an image or a feature that has been extracted from an image, the output that the neural network generates for a given image may be a score for each object class in a set of object classes (object classes), each score representing an estimated likelihood that the image contains an image of an object belonging to that class.
As another example, if the input to the neural network is an internet resource (e.g., a web page), a document, or a portion of a document, or a feature extracted from an internet resource, a document, or a portion of a document, the output generated by the neural network for a given internet resource, document, or portion of a document may be a score for each topic in the set of topics, each score representing an estimated likelihood that the internet resource, document, or portion of a document is related to that topic.
As another example, if the input to the neural network is a characteristic of the impression context of a particular advertisement, the output generated by the neural network may be a score representing an estimated likelihood that the particular advertisement will be clicked.
As another example, if the input to the neural network is a characteristic of a personalized recommendation for the user, e.g., a characteristic that characterizes the context of the recommendation, e.g., a characteristic that characterizes a previous action taken by the user, the output generated by the neural network may be a score for each content item in the set of content items, each score representing an estimated likelihood that the user will respond favorably to the recommended content item.
As another example, if the input to the neural network is a sequence of text in one language, the output generated by the neural network may be a score for each text segment in a set of text segments in another language, each score representing an estimated likelihood that the text segment in the other language is a correct translation of the input text into the other language.
As another example, if the input to the neural network is a sequence representing a spoken utterance, the output generated by the neural network may be a score for each text segment in the set of text segments, each score representing an estimated likelihood that the text segment is a correct transcription of the utterance.
FIG. 1 illustrates an example neural network architecture optimization system 100. The neural network architecture optimization system 100 is an example of a system implemented as a computer program on one or more computers in one or more locations, where the systems, components, and techniques described below may be implemented.
The neural network architecture optimization system 100 is the following: that is, training data 102 for training the neural network to perform the machine learning task is received from a user of the system, an optimal neural network architecture for performing the machine learning task is determined using the training data 102, and the neural network having the optimal neural network architecture is trained to determine training values for parameters of the neural network.
The training data 102 generally includes a plurality of training examples and a respective target output for each training example. The target output for a given training example is the output that should be generated by the trained neural network by processing the given training example. The system 100 divides the received training data into training subsets, validation subsets, and test subsets (optional).
The system 100 may receive the training data 102 in any of a variety of ways. For example, the system 100 may receive training data as an upload from a remote user of the system over a data communications network, for example, using an Application Programming Interface (API) provided by the system 100. As another example, the system 100 may receive input from a user specifying which data the system 100 has maintained should be used as training data 102.
The neural network architecture optimization system 100 uses the training data 102 to generate data 152 specifying a trained neural network. The data 152 specifies the optimal architecture of the trained neural network and the training values of the parameters of the trained neural network with the optimal architecture.
Once the neural network architecture optimization system 100 has generated the data 152, the neural network architecture optimization system 100 may instantiate a trained neural network using the trained neural network data 152 and process the newly received input using the trained neural network to perform machine learning tasks, for example, through an API provided by the system. That is, the system 100 may receive input to be processed, process the input using a trained neural network, and provide output generated by the trained neural network or data derived from the generated output in response to the received input. Alternatively or additionally, the system 100 may store the trained neural network data 152 for later use in instantiating the trained neural network, or may transmit the trained neural network data 152 to another system for use in instantiating the trained neural network, or output the data 152 to a user submitting the training data.
Machine learning tasks are tasks specified by a user who submits training data 102 to system 100.
In some implementations, the user explicitly defines the task by submitting data identifying the task to the neural network architecture optimization system 100 along with the training data 102. For example, the system 100 may present a user interface on a user device of the user that allows the user to select tasks from a list of tasks supported by the system 100. That is, the neural network architecture optimization system 100 may maintain a list of machine learning tasks, e.g., image processing tasks such as image classification, speech recognition tasks, natural language processing tasks such as emotion analysis, and so forth. The system 100 may allow a user to select one of the maintained tasks as the task for which training data is to be used by selecting one of the tasks in the user interface.
In some other implementations, the user-submitted training data 102 specifies machine learning tasks. That is, the neural network architecture optimization system 100 defines tasks as tasks that process inputs having the same format and structure as the training examples in the training data 102 in order to generate outputs having the same format and structure as the target outputs for the training examples. For example, if the training example is an image with a particular resolution and the target output is a one-thousand-dimension vector, the system 100 may identify the task as a task that maps the image with the particular resolution to the one-thousand-dimension vector. For example, a thousand-dimensional target output vector may have a single element that is non-zero. The location of the non-zero value indicates which of the 1000 categories the training example image belongs to. In this example, the system 100 may identify that the task is to map the image to a one-thousand-dimensional probability vector. Each element represents a probability that the image belongs to a respective category. The CIFAR-1000 dataset is an example of such training data 102, consisting of 50000 training examples paired with a target output classification selected from 1000 possible categories. CIFAR-10 is a related dataset in which the classification is one of ten possible categories. Another example of suitable training data 102 is an MNIST dataset, where the training example is an image of handwritten numbers and the target output is numbers represented by those images. The target output may be represented as a ten-dimensional vector having a single non-zero value, where the location of the non-zero value indicates the corresponding number.
The neural network architecture optimization system 100 includes an overall repository 110 and a plurality of worker computing units (also referred to as "workers") 120A-N that operate independently of one another to update data stored in the overall repository 110.
At any given time during training, the ensemble store 110 is implemented as one or more storage devices in one or more physical locations and stores data specifying the current ensemble of candidate neural network architectures. The system 100 may utilize one or more default neural network architectures to initialize the overall memory bank.
The ensemble store 110 stores ensemble data that includes, for each candidate architecture in the ensemble of candidate architectures, (i) data defining the candidate architecture, and (ii) data specifying how recently a neural network having the candidate architecture was trained. Optionally, for each candidate architecture, the overall repository 110 may also store an instance of the neural network with that architecture, the current values of parameters of the neural network with that architecture, or additional metadata characterizing that architecture.
Each candidate architecture in the population includes a stack of multiple units. In some cases, the candidate architecture includes one or more other neural network layers, e.g., an output layer and/or one or more other types of layers, in addition to the stack of cells. For example, a candidate architecture may include a stack of elements followed by a softmax sorting neural network layer. Examples of candidate architectures in the population are described in more detail below with reference to fig. 2.
Typically, a cell is a fully-convolutional neural network configured to receive a cell input and generate a cell output for the cell input. In some implementations, each unit in the stack of units of the candidate architecture can receive direct input from a previous unit and skip input from a unit preceding the previous unit.
The unit includes a plurality of operation blocks, for example, three, five, or ten operation blocks. Each operational block in the cell receives one or more respective input hidden states and constructs a respective output hidden state from the input hidden states using respective pairs of combinations. In particular, the pairwise combination applies a first operation to the first input hidden state, applies a second operation to the second hidden state, and combines outputs of the first operation and the second operation to generate an output hidden state.
A given candidate architecture in the population of candidate architectures is specified by a plurality of pairwise combinations that form units that are repeated through the candidate architecture. In some embodiments in which each candidate architecture in the population includes a stack of different types of units, a given candidate architecture may be specified by a respective pair-wise combination for each type of unit.
For example, a given candidate architecture may include a stack of two different types of units: a general unit and a protocol unit. Candidate architectures may be specified by five pairwise combinations of constituent generic units and five pairwise combinations of constituent specification units.
An example architecture of cell and pairwise combinations is described in more detail below with reference to fig. 3.
Each of the workers 120A-120N is implemented as one or more computer programs and data that are deployed for execution on a respective computing unit. The computing units are configured such that they can operate independently of each other. In some implementations, for example, because the workers share some resources, only partial operational independence is achieved. The computing unit may be, for example, a computer, a core within a computer having multiple cores, or other hardware or software within a computer capable of independently performing computations for a worker.
Each of the workers 120A-120N iteratively updates the population of possible neural network architectures in the population store 110 to improve the fit of the population. Each worker operates independently and asynchronously from the other.
In particular, in each iteration, a given worker 120A-120N (e.g., worker 120A) selects a plurality of candidate architectures (e.g., candidate architecture 122) from the population.
The given worker then trains a new neural network with candidate architecture on the training subset of training data 102 for each selected candidate architecture to determine training values for parameters of the new neural network. The worker may train a new neural network starting from the parameter values associated with the candidate architectures in the overall data.
The worker determines, for each selected candidate architecture, a fitness measure by evaluating the performance of the trained new neural network over a validated subset of training data. The fitness measure may be any measure that is suitable for a machine learning task and that measures the performance of the neural network on the machine learning task. For example, the fitness measure may include various classification errors, cross-ratio measures, rewards or rewards indicators, and so forth.
The worker generates a new candidate architecture (e.g., new candidate architecture 124) based on the selected candidate architecture with the best fitness measure. In particular, the worker modifies the architecture of at least one cell in the candidate architecture having the best fitness measure.
In some implementations, the worker mutates the selected candidate architecture by processing data specifying the selected candidate architecture via a mutating neural network. The mutant neural network is the following neural network: which has been trained to receive an input comprising data of a specified architecture and to generate an output defining another architecture different from the input architecture.
In some other implementations, the worker maintains data identifying a set of possible mutations that may be applied to the input architecture. For example, the worker may randomly select mutations from the set of mutations and apply the randomly selected mutations to the selected candidate structure.
The set of possible mutations may include any of a variety of architectural modifications that represent the addition, removal, or modification of components from an architecture, or variations in hyper-parameters for training a neural network having the architecture. Examples of mutations are described in detail below with reference to fig. 4.
After generating a new candidate architecture from the selected candidate architecture with the best fitness measure, the worker then adds the new candidate architecture to the population and removes the least recently trained candidate architecture from the population. By removing the least recently trained architecture (or the oldest architecture) in each iteration, the system 100 allows all candidate architectures in the population to have a short lifetime. Thus, the population is often fully updated, resulting in more diversity and more exploration, which results in better architecture search results while maintaining system efficiency. The process for selecting candidate architectures from the population, adding new candidate architectures generated based on the selected candidate architecture with the best fitness measure, and removing the oldest candidate architecture from the population may be referred to as "aged evolution" or "regularized evolution". Aging evolution is simple because it has few meta-parameters, most of which do not need to be adjusted, thus reducing computational costs associated with searching (an agent/controller is typically itself a neural network with many weights compared to other techniques that require training of agents/controllers, and it optimizes with more meta-parameters to adjust: learning rate schedules, greedy, batch, replay buffers, etc.) furthermore, because only the candidate architecture with the best fitness measure at each iteration is selected for mutation, the regularized evolution process allows the population of candidate architectures to improve over time, resulting in better performance (e.g., better accuracy) of the optimized neural network in performing specific machine learning tasks compared to existing neural network architecture search methods.
Once termination criteria for training have been met (e.g., after a number of iterations that exceed a threshold have been performed, or after the fitness of the best fit candidate neural network in the population store exceeds a threshold), the neural network architecture optimization system 100 selects the best neural network architecture from among the architectures remaining in the population, or in some cases, from all architectures in the population at any point in time during training.
In particular, in some embodiments, the neural network architecture optimization system 100 selects an architecture in the population that has a best fitness measure. In other embodiments, the neural network architecture optimization system 100 tracks the fitness measures of the architectures even after removing the architectures from the population and uses the tracked fitness measures to select the architecture with the best fitness measure.
To generate data 152 specifying a trained neural network, in some embodiments, the neural network architecture optimization system 100 obtains training values for parameters of the trained neural network having an optimal neural network architecture from the overall repository 110. In some other embodiments, the system 100 trains the neural network with the optimized architecture, for example, starting from scratch or fine-tuning parameter values generated as a result of determining the optimized architecture of the neural network. The system may further train a neural network having an optimized neural network architecture on more training data to determine a final training value for the trained neural network.
The system 100 then uses the trained neural network to process the requests received by the user, for example, through an API provided by the system. In other embodiments, the system may provide data specifying an optimization architecture to the user, and optionally trained parameter values, in response to receiving training data 102, for example, over a data communications network.
In embodiments where the system 100 generates a test subset from the training data, the system also tests the performance of the trained neural network with the optimized neural network architecture on the test subset to determine a fitness measure of the trained neural network on the user-specified machine learning task. The system 100 may then provide the fitness measure for presentation to a user submitting training data or store the fitness measure in association with training values of parameters of the trained neural network.
Fig. 2A and 2B illustrate an example architecture of a neural network 200 for performing machine learning tasks.
The neural network 200 includes a stack 201 of cells. The stack 201 includes a plurality of units stacked one after another.
In some embodiments, the cells in stack 201 have the same cell type, i.e., all cells have the same architecture, but may have different parameter values. In some embodiments, the cells in stack 201 have different cell types. For example, as shown in fig. 2A, stack 201 includes two types of units: a general unit and a protocol unit. In particular, the stack 201 includes a stack 204 of N common units, a reduction unit 206, a stack 208 of N common units, a reduction unit 210, and a stack 212 of N common units, wherein the stack 204 of N common units is followed by the reduction unit 206, the reduction unit 206 is followed by the stack 208 of N common units, the stack 208 of N common units is followed by the reduction unit 210, and the reduction unit 210 is followed by the stack 212 of N common units. All the common units have the same structure, as are the specification units. The architecture of the generic unit is independent of the architecture of the specification unit. The general and protocol units are described in more detail in 2018, CVPR, B.Zoph, V.Vasudevan, J.Shlens and q.v. le., "learn transferable architecture for scalable image recognition (Learning transferable architectures for scalable image recognition)", the web site of which is https:// arxiv. Org/pdf/1707.07012.Pdf.
Typically, each cell in stack 201 is configured to receive as input one or more outputs of one or more previous cells and process the input to generate an output for the input. For example, each cell is configured to receive a direct input from a previous cell (immediately preceding the current cell) and a skip input from a cell preceding the previous cell. As shown in the example of fig. 2B, each cell has two input activation tensors and one output. The first unit in stack 201 receives as input two copies of network input 202. Each unit following the first unit receives as input the outputs of the first two units.
In some embodiments, to reduce the computational cost associated with processing the input image, each application of the reduction unit may be followed by a stride 2 convolution operation that reduces the image size of the output of the reduction unit. The normal unit may preserve the image size.
The neural network 200 includes a sub-network 214 that follows the cell stack 201. The subnetwork 214 is configured to receive the output of the cell stack 201 as an input and process the output of the cell stack 201 to generate the network output 216. For example, the subnetwork 214 includes a soft-max classification neural network layer.
Fig. 3 shows an example architecture of a unit 300.
The unit 300 includes a plurality of operation blocks: blocks 302, 304, 306, 308, and 310. The unit 300 receives two input tensors, which are considered as hidden states "0" and "1". The more hidden states of the cell 300 are then constructed by pairwise combining. For example, pairwise combining applies a first operation to a first hidden state, applies a second operation to a second hidden state, and combines (by adding or concatenating) the outputs of the first and second operations to generate a new hidden state. The first operation and the second operation are selected from a predetermined set of possible operations including, for example, convolution, pooling layers.
As shown in fig. 3, the first pairwise combination of blocks 302 applies a 3 x 3 average pooling operation to hidden state 0 and a 3 x 3 maximum pooling operation to hidden state 1 to produce hidden state 2. The next pairwise combination may be selected from hidden states 0, 1, and 2 to produce hidden state 3 (in this example, the pairwise combination of block 304 selects hidden states 0 and 1 as inputs), and so on. After five pairwise combinations corresponding to five blocks, any hidden states that remain unused (e.g., hidden states 5 and 6 in fig. 3) are concatenated to form the output of cell 300 (hidden state 7).
Fig. 4A and 4B illustrate examples of mutations.
In particular, FIG. 4A illustrates hidden state mutation, which includes a random selection of whether to modify a common or reduced element of a given candidate architecture. Once the cells are selected, the hidden state mutation randomly and uniformly selects one of the five pairwise combinations. The hidden state mutation then randomly and uniformly selects one of the two operations 402 and 404 of the selected pairwise combination and replaces the hidden state associated with the selected operation with another hidden state from within the cell. As shown in fig. 4A, the selected operation 404 has a hidden state of 3. The mutation replaces this hidden state with the hidden state 4 within the cell. To preserve the feedforward nature of the convolutional neural network architecture of the cell, the hidden state associated with the selected operation may be replaced without any loop-forming constraints.
FIG. 4B illustrates an operation mutation that operates similarly to a hidden state mutation in terms of selecting one of two units, one of five pairwise combinations, and one of two operations of the selected pairwise combination. Instead of modifying the hidden state associated with the selected operation, the operation mutation modifies the selected operation itself. In particular, the abrupt change in operation replaces the selected operation with an operation randomly selected from a predetermined operation set. For example, the predetermined set of operations may include, but is not limited to, a 3x3 depth separable convolution, a 5x5 depth separable convolution, a 7x7 depth separable convolution, a 1x7 followed by a 7x1 convolution, an identification, a 3x3 average pooling, a 3x3 maximum pooling, and a 3x3 dilation convolution.
As shown in fig. 4B, operation 406 is replaced by no operation 408 (e.g., allowing the input through the identity operation to pass without changing the input, i.e., the output of the identity operation is the same as the input).
FIG. 5 is a flowchart of an example process for determining an optimized neural network architecture for performing machine learning tasks. For convenience, process 500 is described as being performed by a system of one or more computers located in one or more locations. For example, a neural network architecture optimization system appropriately programmed according to the specification, e.g., the neural network architecture optimization system 100 of fig. 1 appropriately programmed according to the specification, may perform the process 500.
The system receives training data for training a neural network to perform a machine learning task (step 501). The training data includes a plurality of training examples and a respective target output for each training example. The system divides the received training data into training subsets, validation subsets, and test subsets (optional).
The system maintains the aggregate data in an aggregate library (step 502). The system may utilize one or more default neural network architectures to initialize the overall store. For each candidate architecture in the population of candidate architectures, the population data includes (i) data defining the candidate architecture, and (ii) data specifying how recently a neural network having the candidate architecture was trained when determining an optimized neural network architecture.
The system repeatedly performs the following steps 504-514 using each of the plurality of worker computing units until the termination criteria for training have been met. Each worker operates asynchronously with each other worker.
The system selects a plurality of candidate architectures from the population by the worker computing unit (step 504).
The system trains a new neural network with the candidate architecture on a training subset of the training data for each selected candidate architecture and by the worker computing unit to determine training values for parameters of the new neural network (step 506). The worker may train a new neural network starting from the parameter values associated with the candidate architectures in the overall data.
The system determines a fitness measure for each selected candidate architecture by evaluating the performance of the trained new neural network on a validated subset of the training data and by the worker computing unit (step 508). The fitness measure may be any measure that is suitable for a machine learning task and that may measure the performance of the neural network on the machine learning task. For example, the fitness measure may include various classification errors, cross-ratio measures, rewards or rewards indicators, and the like.
The system generates a new candidate architecture based on the selected candidate architecture with the best fit measure by the worker computing unit (step 510). In particular, the worker modifies the architecture of at least one cell in the candidate architecture having the best fitness measure.
In some implementations, the worker mutates the selected candidate architecture by processing data specifying the selected candidate architecture via a mutating neural network. The mutant neural network is the following neural network: it may be trained to receive an input comprising data of a specified architecture and to generate an output defining another architecture different from the input architecture.
In some other implementations, the worker maintains data identifying a set of possible mutations that can be applied to the input architecture. For example, the worker may randomly select mutations from the set of mutations and apply the randomly selected mutations to the selected candidate structure.
The set of possible mutations may include any of a variety of architectural modifications that represent the addition, removal, or modification of components from an architecture, or variations in hyper-parameters for training a neural network having the architecture.
The system adds the new candidate architecture to the population (step 512).
The system removes the least recently trained candidate architecture from the population (step 514). By removing the least recently trained architecture (or the oldest architecture) in each iteration of the aged evolution process, the system allows all candidate architectures in the population to have a short lifetime. Thus, the population is often updated throughout, resulting in more diversity and more exploration, which results in better neural network architecture search results, while maintaining the efficiency of the music system (due to the simplicity of aging evolution).
The system provides data specifying the optimization architecture (step 516). In particular, the system selects the best fit candidate neural network architecture as the optimized neural network architecture for performing the machine learning task. That is, once the worker has completed performing the iteration and has met the termination criteria, for example, after a number of iterations that have been performed that exceeds a threshold, or after the degree of fit of the best-fit candidate neural network in the overall store exceeds a threshold, the system selects the best-fit candidate neural network architecture as the final neural network architecture for performing the machine learning task.
In some embodiments, the system obtains training values for parameters of a trained neural network having an optimized neural network architecture from an overall repository. In some other embodiments, the system trains the neural network with the optimized architecture, for example, starting from scratch or fine-tuning parameter values generated as a result of determining the optimized architecture for the neural network. The system may further train a neural network having an optimized neural network architecture on more training data to determine a final training value for the trained neural network.
The system then uses the trained neural network to process the user-received requests, for example, through an API provided by the system. In other embodiments, the system may provide data specifying an optimization architecture to the user, and optionally trained parameter values, in response to receiving training data, e.g., over a data communications network.
In embodiments in which the system generates a test subset from the training data, the system also measures performance of the trained neural network with the optimized neural network architecture over the test subset to determine a fitness measure of the trained neural network over the user-specified machine learning task. The system may then provide the fitness measure for presentation to a user submitting training data or store the fitness measure in association with training values of parameters of the trained neural network.
Because only the candidate architecture with the best fitness measure at each iteration is selected for mutation, the above-described approach allows the population of candidate architectures to improve over time, resulting in better performance (e.g., better accuracy) of the optimized neural network in performing a particular machine learning task than existing neural network architecture search approaches.
Furthermore, using the described methods, the system can automatically generate a final trained neural network that can achieve performance that competes with and exceeds the most advanced manual design model on machine learning tasks, while requiring little or no input from the neural network designer.
The term "configured" is used throughout the specification in systems and computer program components. For a system of one or more computers to be configured to perform particular operations or actions, it is meant that the system has installed thereon software, firmware, hardware, or a combination thereof that, when operated, causes the system to perform the operations or actions. By one or more computer programs to be configured to perform particular operations or actions, it is meant that the one or more programs comprise instructions which, when executed by a data processing apparatus, cause the apparatus to perform the operations or actions.
Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly embodied computer software or firmware, in computer hardware (including the structures disclosed in this specification and their equivalents), or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible, non-transitory storage medium, to perform or control the operation of data processing apparatus. The computer storage medium may be a machine-readable storage device, a machine-readable storage substrate, a random or serial access storage device, or a combination of one or more of them. Alternatively or additionally, the program instructions may be encoded on a manually-generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by data processing apparatus.
The term "data processing apparatus" refers to data processing hardware and includes all types of apparatus, devices, and machines for processing data, including for example a programmable processor, a computer, or multiple processors or computers. The apparatus may also be or further comprise a dedicated logic circuit, for example an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). In addition to hardware, the apparatus may optionally include code that creates an execution environment for the computer program, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
A computer program can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can also be referred to or described as a program, software application, module, software module, script or code; it may be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub-programs, or portions of code) that store other programs or data. A computer program can be deployed to be executed on one computer or on multiple computers at one site or distributed across multiple sites and interconnected by a data communication network.
In this specification, the term "database" is used broadly to refer to any collection of data: the data need not be structured or fully structured in any particular way and may be stored in storage devices in one or more locations. Thus, for example, an index database may include multiple data sets, each of which may be organized and accessed differently.
Similarly, in this specification, the term "engine" is used broadly to refer to a software-based system, subsystem, or process that is programmed to perform one or more particular functions. Typically, the engine will be implemented as one or more software modules or components installed on one or more computers in one or more locations. In some cases, one or more computers will be dedicated to a particular engine. In other cases, multiple engines may be installed and run on the same computer or computers.
The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, or combination of, special purpose logic circuitry (e.g., an FPGA or ASIC) and one or more programmed computers.
A computer suitable for executing a computer program may be based on a general purpose or special purpose microprocessor or both, or on any other type of central processing unit. Typically, a central processing unit will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a central processing unit for executing instructions and one or more memory devices for storing instructions and data. The central processing unit and the memory may be supplemented by, or incorporated in, special purpose logic circuitry. Typically, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, a computer need not have such devices. Furthermore, a computer may be embedded in another device, such as a mobile phone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, such as a Universal Serial Bus (USB) flash drive, to name a few.
Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and storage devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disk; CD ROM and DVD-ROM discs.
To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device (e.g., a CRT (cathode ray tube) or LCD (liquid crystal display)) monitor for displaying information to the user and a keyboard and a pointing device (e.g., a mouse or a trackball) by which the user can provide input to the computer. Other kinds of devices may also be used to provide for interaction with a user; for example, feedback provided to the user may be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input. In addition, the computer may interact with the user by sending and receiving documents to and from the device used by the user; for example, the user may be interacted with by sending a web page to a web browser on the user device in response to a request received from the web browser. Moreover, the computer may interact with the user by sending text messages or other forms of messages to a personal device (e.g., a smart phone running a messaging application) and in turn receiving response messages from the user.
The data processing apparatus for implementing the machine learning model may also include, for example, a dedicated hardware accelerator unit for handling the common and computationally intensive parts of the machine learning training or process, i.e., reasoning, workload.
The machine learning model can be implemented and deployed using a machine learning framework, such as a TensorFlow framework, microsoft Cognitive Toolkit framework, apache Single framework, or Apache MXNet framework.
Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component (e.g., as a data server) or that includes a middleware component (e.g., an application server) or that includes a front-end component (e.g., a client computer having a graphical user interface, a web browser, or an application through which a user can interact with an implementation of the subject matter described in this specification), or any combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include a Local Area Network (LAN) and a Wide Area Network (WAN), such as the internet.
The computing system may include clients and servers. The client and server are typically remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, the server sends data, such as HTML pages, to the user device, e.g., for displaying data to and receiving user input from a user interacting with the device acting as a client. Data generated at the user device, e.g., results of a user interaction, may be received at the server from the device.
While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any invention or of what may be claimed, but rather as descriptions of features specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Furthermore, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, although operations are depicted in the drawings and described in the claims in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous. Furthermore, the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
Specific embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or serial order, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous.
Claims (14)
1. A method for determining an architecture of a neural network, comprising:
receiving training data for training a neural network to perform a machine learning task, the training data comprising a plurality of training examples and a respective target output for each training example; and
determining an optimized neural network architecture for performing the machine learning task using the training data, comprising:
maintaining global data including, for each candidate architecture in a population of candidate architectures, (i) data defining the candidate architecture and (ii) data specifying how recently a neural network having the candidate architecture was trained when determining the optimized neural network architecture, and
the following operations are repeatedly performed using each of a plurality of worker computing units each running asynchronously with each other worker computing unit:
Selecting, by the worker computing unit, a plurality of candidate architectures from the population,
training, for each selected candidate architecture and by the worker computing unit, a new neural network having the candidate architecture on a training subset of the training data to determine training values for parameters of the new neural network;
by evaluating the performance of the trained new neural network on a validated subset of the training data, a fitness measure is determined for each selected candidate architecture and by the worker computing unit,
generating, by the worker computing unit, a new candidate architecture based on the selected candidate architecture having the best fitness measure, wherein generating the new candidate architecture comprises:
randomly selecting mutations from a set of mutations, the set of mutations comprising a cryptic mutation, wherein:
the hidden state mutation randomly selects one of a plurality of units of the selected candidate architecture, the selected unit being associated with one or more pairwise combinations of the plurality of units, each of the one or more pairwise combinations comprising two network operations,
the hidden state mutations randomly and uniformly select one of the one or more pairwise combinations,
The hidden state mutation randomly and uniformly selects one of the two network operations of the selected pairwise combination, an
The hidden state mutation replaces the hidden state associated with the selected network operation with another existing hidden state from within the selected cell, and
adding the new candidate architecture to the population, and
the least recently trained candidate architecture is removed from the population.
2. The method of claim 1, further comprising:
data specifying the optimized neural network architecture is provided.
3. The method of claim 1, further comprising:
training values of parameters of a neural network having the optimized neural network architecture are determined.
4. A method according to claim 3, further comprising:
data specifying training parameters is provided.
5. The method of claim 3, wherein the operations performed by each worker computing unit further comprise:
training the new candidate architecture to determine a training value of the parameter; and
the training values of the parameters are associated with new candidate architectures in the overall data.
6. The method of claim 5, wherein determining training values for parameters of a neural network having the optimized neural network architecture comprises:
Training values associated with an architecture associated with the best fitness measure are selected as training values for parameters of the neural network having the optimized neural network architecture.
7. The method of claim 5, wherein determining training values for parameters of a neural network having the optimized neural network architecture comprises:
the neural network with the optimized neural network architecture is further trained on more training data to determine the training values.
8. The method of claim 5, wherein training, for each selected candidate architecture and by the worker computing unit, a new neural network having the candidate architecture on a training subset of the training data to determine training values for parameters of the new neural network comprises: the new neural network is trained starting from training values of the parameters associated with the candidate architectures in the overall data.
9. The method of claim 1, further comprising:
the population is initialized with a plurality of default candidate architectures.
10. The method of claim 1, wherein, for each candidate architecture, the data defining the candidate architecture identifies an architecture of one or more units, each unit being repeated multiple times to generate the candidate architecture.
11. The method of any of claims 1-10, wherein determining an optimized neural network architecture for performing the machine learning task using the training data further comprises:
the candidate architecture with the best fitness measure in the population is selected as the optimized neural network architecture.
12. The method of any of claims 1-10, wherein the machine learning task is one or more of: image processing, image classification, speech recognition, and natural language processing.
13. A system for determining architecture of a neural network, the system comprising one or more computers and one or more storage devices storing instructions that, when executed by the one or more computers, cause the one or more computers to perform the method of any one of claims 1-12.
14. A non-transitory computer-readable storage medium encoded with instructions that, when executed by one or more computers, cause the one or more computers to perform the method of any one of claims 1-12.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201862625923P | 2018-02-02 | 2018-02-02 | |
US62/625,923 | 2018-02-02 | ||
PCT/US2019/016515 WO2019152929A1 (en) | 2018-02-02 | 2019-02-04 | Regularized neural network architecture search |
Publications (2)
Publication Number | Publication Date |
---|---|
CN111602148A CN111602148A (en) | 2020-08-28 |
CN111602148B true CN111602148B (en) | 2024-04-02 |
Family
ID=66286954
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201980008348.6A Active CN111602148B (en) | 2018-02-02 | 2019-02-04 | Regularized neural network architecture search |
Country Status (4)
Country | Link |
---|---|
US (3) | US11144831B2 (en) |
EP (1) | EP3711000B1 (en) |
CN (1) | CN111602148B (en) |
WO (1) | WO2019152929A1 (en) |
Families Citing this family (19)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10685286B1 (en) * | 2019-07-30 | 2020-06-16 | SparkCognition, Inc. | Automated neural network generation using fitness estimation |
CN110782034A (en) | 2019-10-31 | 2020-02-11 | 北京小米智能科技有限公司 | Neural network training method, device and storage medium |
US11593627B2 (en) | 2019-12-31 | 2023-02-28 | X Development Llc | Artificial neural network architectures based on synaptic connectivity graphs |
US11625611B2 (en) | 2019-12-31 | 2023-04-11 | X Development Llc | Training artificial neural networks based on synaptic connectivity graphs |
US11620487B2 (en) * | 2019-12-31 | 2023-04-04 | X Development Llc | Neural architecture search based on synaptic connectivity graphs |
US11593617B2 (en) | 2019-12-31 | 2023-02-28 | X Development Llc | Reservoir computing neural networks based on synaptic connectivity graphs |
US11568201B2 (en) | 2019-12-31 | 2023-01-31 | X Development Llc | Predicting neuron types based on synaptic connectivity graphs |
US11631000B2 (en) | 2019-12-31 | 2023-04-18 | X Development Llc | Training artificial neural networks based on synaptic connectivity graphs |
WO2021159095A1 (en) * | 2020-02-07 | 2021-08-12 | Google Llc | Population-based black-box optimization |
CN113361680B (en) * | 2020-03-05 | 2024-04-12 | 华为云计算技术有限公司 | Neural network architecture searching method, device, equipment and medium |
CN111488971B (en) * | 2020-04-09 | 2023-10-24 | 北京百度网讯科技有限公司 | Neural network model searching method and device, and image processing method and device |
US11544561B2 (en) * | 2020-05-15 | 2023-01-03 | Microsoft Technology Licensing, Llc | Task-aware recommendation of hyperparameter configurations |
EP4207857A4 (en) * | 2020-10-21 | 2023-10-18 | Huawei Technologies Co., Ltd. | Data transmission method and communication apparatus |
US20220172038A1 (en) * | 2020-11-30 | 2022-06-02 | International Business Machines Corporation | Automated deep learning architecture selection for time series prediction with user interaction |
CN112560985B (en) * | 2020-12-25 | 2024-01-12 | 北京百度网讯科技有限公司 | Neural network searching method and device and electronic equipment |
WO2022251719A1 (en) | 2021-05-28 | 2022-12-01 | Google Llc | Granular neural network architecture search over low-level primitives |
CN117396885A (en) * | 2021-05-28 | 2024-01-12 | 渊慧科技有限公司 | Enhancing population-based training of neural networks |
US20220035877A1 (en) * | 2021-10-19 | 2022-02-03 | Intel Corporation | Hardware-aware machine learning model search mechanisms |
CN114665979B (en) * | 2022-02-23 | 2024-04-19 | 国网浙江省电力有限公司信息通信分公司 | Ultra-long distance light transmission method and system based on population evolution iteration |
Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2010035690A2 (en) * | 2008-09-24 | 2010-04-01 | 日本電気株式会社 | Structure training device, structure training system, structure training method, program, and recording medium |
CN105637540A (en) * | 2013-10-08 | 2016-06-01 | 谷歌公司 | Methods and apparatus for reinforcement learning |
CN105938559A (en) * | 2015-03-04 | 2016-09-14 | 埃森哲环球服务有限公司 | Digital image processing using convolutional neural networks |
Family Cites Families (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7685080B2 (en) * | 2005-09-28 | 2010-03-23 | Honda Motor Co., Ltd. | Regularized least squares classification or regression with leave-one-out (LOO) error |
-
2019
- 2019-02-04 CN CN201980008348.6A patent/CN111602148B/en active Active
- 2019-02-04 WO PCT/US2019/016515 patent/WO2019152929A1/en unknown
- 2019-02-04 EP EP19719658.7A patent/EP3711000B1/en active Active
-
2020
- 2020-06-19 US US16/906,034 patent/US11144831B2/en active Active
-
2021
- 2021-09-14 US US17/475,137 patent/US11669744B2/en active Active
-
2023
- 2023-04-27 US US18/140,442 patent/US20230259784A1/en active Pending
Patent Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2010035690A2 (en) * | 2008-09-24 | 2010-04-01 | 日本電気株式会社 | Structure training device, structure training system, structure training method, program, and recording medium |
CN105637540A (en) * | 2013-10-08 | 2016-06-01 | 谷歌公司 | Methods and apparatus for reinforcement learning |
CN105938559A (en) * | 2015-03-04 | 2016-09-14 | 埃森哲环球服务有限公司 | Digital image processing using convolutional neural networks |
Non-Patent Citations (4)
Title |
---|
A Genetic Programming Approach to Designing Convolutional Neural Network Architectures;Masanori Suganuma et al;《arXiv》;20170811;第1-5页 * |
Evolutionary Computation and Its Applications in Neural and Fuzzy Systems;Biaobiao Zhang et al;《Applied Computational Intelligence and Soft Computing》;20110101;第3-10页 * |
Learning Transferable Architectures for Scalable Image Recognition;Barret Zoph et al;《arXiv》;20170721;第1-10页 * |
一种基于框架语义的专项新闻检索方法研究;谢兴生等;《中国科学技术大学学报》;20160315;第46卷(第03期);第253-256页 * |
Also Published As
Publication number | Publication date |
---|---|
EP3711000A1 (en) | 2020-09-23 |
WO2019152929A1 (en) | 2019-08-08 |
US20220004879A1 (en) | 2022-01-06 |
US11144831B2 (en) | 2021-10-12 |
US11669744B2 (en) | 2023-06-06 |
US20200320399A1 (en) | 2020-10-08 |
US20230259784A1 (en) | 2023-08-17 |
CN111602148A (en) | 2020-08-28 |
EP3711000B1 (en) | 2023-06-21 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN111602148B (en) | Regularized neural network architecture search | |
US20210334624A1 (en) | Neural architecture search using a performance prediction neural network | |
US11544536B2 (en) | Hybrid neural architecture search | |
US11934956B2 (en) | Regularizing machine learning models | |
US20210150355A1 (en) | Training machine learning models using task selection policies to increase learning progress | |
CN110366734B (en) | Optimizing neural network architecture | |
US20210004677A1 (en) | Data compression using jointly trained encoder, decoder, and prior neural networks | |
US20200104679A1 (en) | Learning observation representations by predicting the future in latent space | |
KR102170199B1 (en) | Classify input examples using comparison sets | |
US20210019599A1 (en) | Adaptive neural architecture search | |
EP3542319B1 (en) | Training neural networks using a clustering loss | |
US11922281B2 (en) | Training machine learning models using teacher annealing | |
US20220292329A1 (en) | Neural architecture search with weight sharing | |
EP3673419A1 (en) | Population based training of neural networks | |
EP3602419A1 (en) | Neural network optimizer search | |
US20220383119A1 (en) | Granular neural network architecture search over low-level primitives | |
CN114861873A (en) | Multi-stage computationally efficient neural network inference | |
CN115398446A (en) | Machine learning algorithm search using symbolic programming | |
CN115485694A (en) | Machine learning algorithm search | |
US20240152809A1 (en) | Efficient machine learning model architecture selection | |
JP2024519265A (en) | Neural network with feedforward spatial transformation units | |
CN117121014A (en) | Neural network with feedforward space transformation unit |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |